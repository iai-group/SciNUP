{"id": "0912.1310", "contents": "Title: Automatic creation of urban velocity fields from aerial video Abstract: In this paper, we present a system for modelling vehicle motion in an urban\nscene from low frame-rate aerial video. In particular, the scene is modelled as\na probability distribution over velocities at every pixel in the image.\n  We describe the complete system for acquiring this model. The video is\ncaptured from a helicopter and stabilized by warping the images to match an\northorectified image of the area. A pixel classifier is applied to the\nstabilized images, and the response is segmented to determine car locations and\norientations. The results are fed in to a tracking scheme which tracks cars for\nthree frames, creating tracklets. This allows the tracker to use a combination\nof velocity, direction, appearance, and acceleration cues to keep only tracks\nlikely to be correct. Each tracklet provides a measurement of the car velocity\nat every point along the tracklet's length, and these are then aggregated to\ncreate a histogram of vehicle velocities at every pixel in the image.\n  The results demonstrate that the velocity probability distribution prior can\nbe used to infer a variety of information about road lane directions, speed\nlimits, vehicle speeds and common trajectories, and traffic bottlenecks, as\nwell as providing a means of describing environmental knowledge about traffic\nrules that can be used in tracking. \n\n"}
{"id": "1109.4909", "contents": "Title: Sparse Online Low-Rank Projection and Outlier Rejection (SOLO) for 3-D\n  Rigid-Body Motion Registration Abstract: Motivated by an emerging theory of robust low-rank matrix representation, in\nthis paper, we introduce a novel solution for online rigid-body motion\nregistration. The goal is to develop algorithmic techniques that enable a\nrobust, real-time motion registration solution suitable for low-cost, portable\n3-D camera devices. Assuming 3-D image features are tracked via a standard\ntracker, the algorithm first utilizes Robust PCA to initialize a low-rank shape\nrepresentation of the rigid body. Robust PCA finds the global optimal solution\nof the initialization, while its complexity is comparable to singular value\ndecomposition. In the online update stage, we propose a more efficient\nalgorithm for sparse subspace projection to sequentially project new feature\nobservations onto the shape subspace. The lightweight update stage guarantees\nthe real-time performance of the solution while maintaining good registration\neven when the image sequence is contaminated by noise, gross data corruption,\noutlying features, and missing data. The state-of-the-art accuracy of the\nsolution is validated through extensive simulation and a real-world experiment,\nwhile the system enjoys one to two orders of magnitude speed-up compared to\nwell-established RANSAC solutions. The new algorithm will be released online to\naid peer evaluation. \n\n"}
{"id": "1203.0781", "contents": "Title: Posterior Mean Super-Resolution with a Compound Gaussian Markov Random\n  Field Prior Abstract: This manuscript proposes a posterior mean (PM) super-resolution (SR) method\nwith a compound Gaussian Markov random field (MRF) prior. SR is a technique to\nestimate a spatially high-resolution image from observed multiple\nlow-resolution images. A compound Gaussian MRF model provides a preferable\nprior for natural images that preserves edges. PM is the optimal estimator for\nthe objective function of peak signal-to-noise ratio (PSNR). This estimator is\nnumerically determined by using variational Bayes (VB). We then solve the\nconjugate prior problem on VB and the exponential-order calculation cost\nproblem of a compound Gaussian MRF prior with simple Taylor approximations. In\nexperiments, the proposed method roughly overcomes existing methods. \n\n"}
{"id": "1203.2210", "contents": "Title: Fixed-Rank Representation for Unsupervised Visual Learning Abstract: Subspace clustering and feature extraction are two of the most commonly used\nunsupervised learning techniques in computer vision and pattern recognition.\nState-of-the-art techniques for subspace clustering make use of recent advances\nin sparsity and rank minimization. However, existing techniques are\ncomputationally expensive and may result in degenerate solutions that degrade\nclustering performance in the case of insufficient data sampling. To partially\nsolve these problems, and inspired by existing work on matrix factorization,\nthis paper proposes fixed-rank representation (FRR) as a unified framework for\nunsupervised visual learning. FRR is able to reveal the structure of multiple\nsubspaces in closed-form when the data is noiseless. Furthermore, we prove that\nunder some suitable conditions, even with insufficient observations, FRR can\nstill reveal the true subspace memberships. To achieve robustness to outliers\nand noise, a sparse regularizer is introduced into the FRR framework. Beyond\nsubspace clustering, FRR can be used for unsupervised feature extraction. As a\nnon-trivial byproduct, a fast numerical solver is developed for FRR.\nExperimental results on both synthetic data and real applications validate our\ntheoretical analysis and demonstrate the benefits of FRR for unsupervised\nvisual learning. \n\n"}
{"id": "1208.4391", "contents": "Title: Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev\n  Descent Abstract: We present a method to track the precise shape of an object in video based on\nnew modeling and optimization on a new Riemannian manifold of parameterized\nregions.\n  Joint dynamic shape and appearance models, in which a template of the object\nis propagated to match the object shape and radiance in the next frame, are\nadvantageous over methods employing global image statistics in cases of complex\nobject radiance and cluttered background. In cases of 3D object motion and\nviewpoint change, self-occlusions and dis-occlusions of the object are\nprominent, and current methods employing joint shape and appearance models are\nunable to adapt to new shape and appearance information, leading to inaccurate\nshape detection. In this work, we model self-occlusions and dis-occlusions in a\njoint shape and appearance tracking framework.\n  Self-occlusions and the warp to propagate the template are coupled, thus a\njoint problem is formulated. We derive a coarse-to-fine optimization scheme,\nadvantageous in object tracking, that initially perturbs the template by coarse\nperturbations before transitioning to finer-scale perturbations, traversing all\nscales, seamlessly and automatically. The scheme is a gradient descent on a\nnovel infinite-dimensional Riemannian manifold that we introduce. The manifold\nconsists of planar parameterized regions, and the metric that we introduce is a\nnovel Sobolev-type metric defined on infinitesimal vector fields on regions.\nThe metric has the property of resulting in a gradient descent that\nautomatically favors coarse-scale deformations (when they reduce the energy)\nbefore moving to finer-scale deformations.\n  Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and\nbackground show that occlusion/dis-occlusion modeling leads to superior shape\naccuracy compared to recent methods employing joint shape/appearance models or\nemploying global statistics. \n\n"}
{"id": "1208.5365", "contents": "Title: A Missing and Found Recognition System for Hajj and Umrah Abstract: This note describes an integrated recognition system for identifying missing\nand found objects as well as missing, dead, and found people during Hajj and\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\nreach 20 millions during the next decade. The ultimate goal of this system is\nto integrate facial recognition and object identification solutions into the\nHajj and Umrah rituals. The missing and found computerized system is part of\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\nsafety. \n\n"}
{"id": "1212.0433", "contents": "Title: Compressive Schlieren Deflectometry Abstract: Schlieren deflectometry aims at characterizing the deflections undergone by\nrefracted incident light rays at any surface point of a transparent object. For\nsmooth surfaces, each surface location is actually associated with a sparse\ndeflection map (or spectrum). This paper presents a novel method to\ncompressively acquire and reconstruct such spectra. This is achieved by\naltering the way deflection information is captured in a common Schlieren\nDeflectometer, i.e., the deflection spectra are indirectly observed by the\nprinciple of spread spectrum compressed sensing. These observations are\nrealized optically using a 2-D Spatial Light Modulator (SLM) adjusted to the\ncorresponding sensing basis and whose modulations encode the light deviation\nsubsequently recorded by a CCD camera. The efficiency of this approach is\ndemonstrated experimentally on the observation of few test objects. Further,\nusing a simple parametrization of the deflection spectra we show that relevant\nkey parameters can be directly computed using the measurements, avoiding full\nreconstruction. \n\n"}
{"id": "1212.3530", "contents": "Title: A Multi-Orientation Analysis Approach to Retinal Vessel Tracking Abstract: This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications. \n\n"}
{"id": "1301.3323", "contents": "Title: Auto-pooling: Learning to Improve Invariance of Image Features from\n  Image Sequences Abstract: Learning invariant representations from images is one of the hardest\nchallenges facing computer vision. Spatial pooling is widely used to create\ninvariance to spatial shifting, but it is restricted to convolutional models.\nIn this paper, we propose a novel pooling method that can learn soft clustering\nof features from image sequences. It is trained to improve the temporal\ncoherence of features, while keeping the information loss at minimum. Our\nmethod does not use spatial information, so it can be used with\nnon-convolutional models too. Experiments on images extracted from natural\nvideos showed that our method can cluster similar features together. When\ntrained by convolutional features, auto-pooling outperformed traditional\nspatial pooling on an image classification task, even though it does not use\nthe spatial topology of features. \n\n"}
{"id": "1301.3461", "contents": "Title: Factorized Topic Models Abstract: In this paper we present a modification to a latent topic model, which makes\nthe model exploit supervision to produce a factorized representation of the\nobserved data. The structured parameterization separately encodes variance that\nis shared between classes from variance that is private to each class by the\nintroduction of a new prior over the topic space. The approach allows for a\nmore eff{}icient inference and provides an intuitive interpretation of the data\nin terms of an informative signal together with structured noise. The\nfactorized representation is shown to enhance inference performance for image,\ntext, and video classification. \n\n"}
{"id": "1307.1437", "contents": "Title: Toward Guaranteed Illumination Models for Non-Convex Objects Abstract: Illumination variation remains a central challenge in object detection and\nrecognition. Existing analyses of illumination variation typically pertain to\nconvex, Lambertian objects, and guarantee quality of approximation in an\naverage case sense. We show that it is possible to build V(vertex)-description\nconvex cone models with worst-case performance guarantees, for non-convex\nLambertian objects. Namely, a natural verification test based on the angle to\nthe constructed cone guarantees to accept any image which is sufficiently\nwell-approximated by an image of the object under some admissible lighting\ncondition, and guarantees to reject any image that does not have a sufficiently\ngood approximation. The cone models are generated by sampling point\nilluminations with sufficient density, which follows from a new perturbation\nbound for point images in the Lambertian model. As the number of point images\nrequired for guaranteed verification may be large, we introduce a new\nformulation for cone preserving dimensionality reduction, which leverages tools\nfrom sparse and low-rank decomposition to reduce the complexity, while\ncontrolling the approximation error with respect to the original cone. \n\n"}
{"id": "1309.2074", "contents": "Title: Learning Transformations for Clustering and Classification Abstract: A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification. \n\n"}
{"id": "1309.6301", "contents": "Title: Solving OSCAR regularization problems by proximal splitting algorithms Abstract: The OSCAR (octagonal selection and clustering algorithm for regression)\nregularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible for\nits grouping behavior) and was proposed to encourage group sparsity in\nscenarios where the groups are a priori unknown. The OSCAR regularizer has a\nnon-trivial proximity operator, which limits its applicability. We reformulate\nthis regularizer as a weighted sorted L_1 norm, and propose its grouping\nproximity operator (GPO) and approximate proximity operator (APO), thus making\nstate-of-the-art proximal splitting algorithms (PSAs) available to solve\ninverse problems with OSCAR regularization. The GPO is in fact the APO followed\nby additional grouping and averaging operations, which are costly in time and\nstorage, explaining the reason why algorithms with APO are much faster than\nthat with GPO. The convergences of PSAs with GPO are guaranteed since GPO is an\nexact proximity operator. Although convergence of PSAs with APO is may not be\nguaranteed, we have experimentally found that APO behaves similarly to GPO when\nthe regularization parameter of the pair-wise L_inf norm is set to an\nappropriately small value. Experiments on recovery of group-sparse signals\n(with unknown groups) show that PSAs with APO are very fast and accurate. \n\n"}
{"id": "1310.4891", "contents": "Title: Dictionary Learning and Sparse Coding on Grassmann Manifolds: An\n  Extrinsic Solution Abstract: Recent advances in computer vision and machine learning suggest that a wide\nrange of problems can be addressed more appropriately by considering\nnon-Euclidean geometry. In this paper we explore sparse dictionary learning\nover the space of linear subspaces, which form Riemannian structures known as\nGrassmann manifolds. To this end, we propose to embed Grassmann manifolds into\nthe space of symmetric matrices by an isometric mapping, which enables us to\ndevise a closed-form solution for updating a Grassmann dictionary, atom by\natom. Furthermore, to handle non-linearity in data, we propose a kernelised\nversion of the dictionary learning algorithm. Experiments on several\nclassification tasks (face recognition, action recognition, dynamic texture\nclassification) show that the proposed approach achieves considerable\nimprovements in discrimination accuracy, in comparison to state-of-the-art\nmethods such as kernelised Affine Hull Method and graph-embedding Grassmann\ndiscriminant analysis. \n\n"}
{"id": "1311.2901", "contents": "Title: Visualizing and Understanding Convolutional Networks Abstract: Large Convolutional Network models have recently demonstrated impressive\nclassification performance on the ImageNet benchmark. However there is no clear\nunderstanding of why they perform so well, or how they might be improved. In\nthis paper we address both issues. We introduce a novel visualization technique\nthat gives insight into the function of intermediate feature layers and the\noperation of the classifier. We also perform an ablation study to discover the\nperformance contribution from different model layers. This enables us to find\nmodel architectures that outperform Krizhevsky \\etal on the ImageNet\nclassification benchmark. We show our ImageNet model generalizes well to other\ndatasets: when the softmax classifier is retrained, it convincingly beats the\ncurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets. \n\n"}
{"id": "1312.4740", "contents": "Title: Learning High-level Image Representation for Image Retrieval via\n  Multi-Task DNN using Clickthrough Data Abstract: Image retrieval refers to finding relevant images from an image database for\na query, which is considered difficult for the gap between low-level\nrepresentation of images and high-level representation of queries. Recently\nfurther developed Deep Neural Network sheds light on automatically learning\nhigh-level image representation from raw pixels. In this paper, we proposed a\nmulti-task DNN learned for image retrieval, which contains two parts, i.e.,\nquery-sharing layers for image representation computation and query-specific\nlayers for relevance estimation. The weights of multi-task DNN are learned on\nclickthrough data by Ring Training. Experimental results on both simulated and\nreal dataset show the effectiveness of the proposed method. \n\n"}
{"id": "1312.6034", "contents": "Title: Deep Inside Convolutional Networks: Visualising Image Classification\n  Models and Saliency Maps Abstract: This paper addresses the visualisation of image classification models, learnt\nusing deep Convolutional Networks (ConvNets). We consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The first one generates an image, which maximises the class\nscore [Erhan et al., 2009], thus visualising the notion of the class, captured\nby a ConvNet. The second technique computes a class saliency map, specific to a\ngiven image and class. We show that such maps can be employed for weakly\nsupervised object segmentation using classification ConvNets. Finally, we\nestablish the connection between the gradient-based ConvNet visualisation\nmethods and deconvolutional networks [Zeiler et al., 2013]. \n\n"}
{"id": "1312.6199", "contents": "Title: Intriguing properties of neural networks Abstract: Deep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. While\ntheir expressiveness is the reason they succeed, it also causes them to learn\nuninterpretable solutions that could have counter-intuitive properties. In this\npaper we report two such properties.\n  First, we find that there is no distinction between individual high level\nunits and random linear combinations of high level units, according to various\nmethods of unit analysis. It suggests that it is the space, rather than the\nindividual units, that contains of the semantic information in the high layers\nof neural networks.\n  Second, we find that deep neural networks learn input-output mappings that\nare fairly discontinuous to a significant extend. We can cause the network to\nmisclassify an image by applying a certain imperceptible perturbation, which is\nfound by maximizing the network's prediction error. In addition, the specific\nnature of these perturbations is not a random artifact of learning: the same\nperturbation can cause a different network, that was trained on a different\nsubset of the dataset, to misclassify the same input. \n\n"}
{"id": "1312.6885", "contents": "Title: Deep learning for class-generic object detection Abstract: We investigate the use of deep neural networks for the novel task of class\ngeneric object detection. We show that neural networks originally designed for\nimage recognition can be trained to detect objects within images, regardless of\ntheir class, including objects for which no bounding box labels have been\nprovided. In addition, we show that bounding box labels yield a 1% performance\nincrease on the ImageNet recognition challenge. \n\n"}
{"id": "1402.0170", "contents": "Title: Collaborative Receptive Field Learning Abstract: The challenge of object categorization in images is largely due to arbitrary\ntranslations and scales of the foreground objects. To attack this difficulty,\nwe propose a new approach called collaborative receptive field learning to\nextract specific receptive fields (RF's) or regions from multiple images, and\nthe selected RF's are supposed to focus on the foreground objects of a common\ncategory. To this end, we solve the problem by maximizing a submodular function\nover a similarity graph constructed by a pool of RF candidates. However,\nmeasuring pairwise distance of RF's for building the similarity graph is a\nnontrivial problem. Hence, we introduce a similarity metric called\npyramid-error distance (PED) to measure their pairwise distances through\nsumming up pyramid-like matching errors over a set of low-level features.\nBesides, in consistent with the proposed PED, we construct a simple\nnonparametric classifier for classification. Experimental results show that our\nmethod effectively discovers the foreground objects in images, and improves\nclassification performance. \n\n"}
{"id": "1402.0240", "contents": "Title: Graph Cuts with Interacting Edge Costs - Examples, Approximations, and\n  Algorithms Abstract: We study an extension of the classical graph cut problem, wherein we replace\nthe modular (sum of edge weights) cost function by a submodular set function\ndefined over graph edges. Special cases of this problem have appeared in\ndifferent applications in signal processing, machine learning, and computer\nvision. In this paper, we connect these applications via the generic\nformulation of \"cooperative graph cuts\", for which we study complexity,\nalgorithms, and connections to polymatroidal network flows. Finally, we compare\nthe proposed algorithms empirically. \n\n"}
{"id": "1402.0595", "contents": "Title: Scene Labeling with Contextual Hierarchical Models Abstract: Scene labeling is the problem of assigning an object label to each pixel. It\nunifies the image segmentation and object recognition problems. The importance\nof using contextual information in scene labeling frameworks has been widely\nrealized in the field. We propose a contextual framework, called contextual\nhierarchical model (CHM), which learns contextual information in a hierarchical\nframework for scene labeling. At each level of the hierarchy, a classifier is\ntrained based on downsampled input images and outputs of previous levels. Our\nmodel then incorporates the resulting multi-resolution contextual information\ninto a classifier to segment the input image at original resolution. This\ntraining strategy allows for optimization of a joint posterior probability at\nmultiple resolutions through the hierarchy. Contextual hierarchical model is\npurely based on the input image patches and does not make use of any fragments\nor shape examples. Hence, it is applicable to a variety of problems such as\nobject segmentation and edge detection. We demonstrate that CHM outperforms\nstate-of-the-art on Stanford background and Weizmann horse datasets. It also\noutperforms state-of-the-art edge detection methods on NYU depth dataset and\nachieves state-of-the-art on Berkeley segmentation dataset (BSDS 500). \n\n"}
{"id": "1402.3849", "contents": "Title: Scalable Kernel Clustering: Approximate Kernel k-means Abstract: Kernel-based clustering algorithms have the ability to capture the non-linear\nstructure in real world data. Among various kernel-based clustering algorithms,\nkernel k-means has gained popularity due to its simple iterative nature and\nease of implementation. However, its run-time complexity and memory footprint\nincrease quadratically in terms of the size of the data set, and hence, large\ndata sets cannot be clustered efficiently. In this paper, we propose an\napproximation scheme based on randomization, called the Approximate Kernel\nk-means. We approximate the cluster centers using the kernel similarity between\na few sampled points and all the points in the data set. We show that the\nproposed method achieves better clustering performance than the traditional low\nrank kernel approximation based clustering schemes. We also demonstrate that\nits running time and memory requirements are significantly lower than those of\nkernel k-means, with only a small reduction in the clustering quality on\nseveral public domain large data sets. We then employ ensemble clustering\ntechniques to further enhance the performance of our algorithm. \n\n"}
{"id": "1402.5077", "contents": "Title: Group-sparse Matrix Recovery Abstract: We apply the OSCAR (octagonal selection and clustering algorithms for\nregression) in recovering group-sparse matrices (two-dimensional---2D---arrays)\nfrom compressive measurements. We propose a 2D version of OSCAR (2OSCAR)\nconsisting of the $\\ell_1$ norm and the pair-wise $\\ell_{\\infty}$ norm, which\nis convex but non-differentiable. We show that the proximity operator of 2OSCAR\ncan be computed based on that of OSCAR. The 2OSCAR problem can thus be\nefficiently solved by state-of-the-art proximal splitting algorithms.\nExperiments on group-sparse 2D array recovery show that 2OSCAR regularization\nsolved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm\n(with debiasing) yields the most accurate results. \n\n"}
{"id": "1404.3596", "contents": "Title: Face Detection with a 3D Model Abstract: This paper presents a part-based face detection approach where the spatial\nrelationship between the face parts is represented by a hidden 3D model with\nsix parameters. The computational complexity of the search in the six\ndimensional pose space is addressed by proposing meaningful 3D pose candidates\nby image-based regression from detected face keypoint locations. The 3D pose\ncandidates are evaluated using a parameter sensitive classifier based on\ndifference features relative to the 3D pose. A compatible subset of candidates\nis then obtained by non-maximal suppression. Experiments on two standard face\ndetection datasets show that the proposed 3D model based approach obtains\nresults comparable to or better than state of the art. \n\n"}
{"id": "1406.1943", "contents": "Title: Structured Dictionary Learning for Classification Abstract: Sparsity driven signal processing has gained tremendous popularity in the\nlast decade. At its core, the assumption is that the signal of interest is\nsparse with respect to either a fixed transformation or a signal dependent\ndictionary. To better capture the data characteristics, various dictionary\nlearning methods have been proposed for both reconstruction and classification\ntasks. For classification particularly, most approaches proposed so far have\nfocused on designing explicit constraints on the sparse code to improve\nclassification accuracy while simply adopting $l_0$-norm or $l_1$-norm for\nsparsity regularization. Motivated by the success of structured sparsity in the\narea of Compressed Sensing, we propose a structured dictionary learning\nframework (StructDL) that incorporates the structure information on both group\nand task levels in the learning process. Its benefits are two-fold: (i) the\nlabel consistency between dictionary atoms and training data are implicitly\nenforced; and (ii) the classification performance is more robust in the cases\nof a small dictionary size or limited training data than other techniques.\nUsing the subspace model, we derive the conditions for StructDL to guarantee\nthe performance and show theoretically that StructDL is superior to $l_0$-norm\nor $l_1$-norm regularized dictionary learning for classification. Extensive\nexperiments have been performed on both synthetic simulations and real world\napplications, such as face recognition and object classification, to\ndemonstrate the validity of the proposed DL framework. \n\n"}
{"id": "1409.0473", "contents": "Title: Neural Machine Translation by Jointly Learning to Align and Translate Abstract: Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition. \n\n"}
{"id": "1409.6070", "contents": "Title: Spatially-sparse convolutional neural networks Abstract: Convolutional neural networks (CNNs) perform well on problems such as\nhandwriting recognition and image classification. However, the performance of\nthe networks is often limited by budget and time constraints, particularly when\ntrying to train deep networks.\n  Motivated by the problem of online handwriting recognition, we developed a\nCNN for processing spatially-sparse inputs; a character drawn with a one-pixel\nwide pen on a high resolution grid looks like a sparse matrix. Taking advantage\nof the sparsity allowed us more efficiently to train and test large, deep CNNs.\nOn the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test\nerror of 3.82%.\n  Although pictures are not sparse, they can be thought of as sparse by adding\npadding. Applying a deep convolutional network using sparsity has resulted in a\nsubstantial reduction in test error on the CIFAR small picture datasets: 6.28%\non CIFAR-10 and 24.30% for CIFAR-100. \n\n"}
{"id": "1410.5263", "contents": "Title: Building pattern recognition applications with the SPARE library Abstract: This paper presents the SPARE C++ library, an open source software tool\nconceived to build pattern recognition and soft computing systems. The library\nfollows the requirement of the generality: most of the implemented algorithms\nare able to process user-defined input data types transparently, such as\nlabeled graphs and sequences of objects, as well as standard numeric vectors.\nHere we present a high-level picture of the SPARE library characteristics,\nfocusing instead on the specific practical possibility of constructing pattern\nrecognition systems for different input data types. In particular, as a proof\nof concept, we discuss two application instances involving clustering of\nreal-valued multidimensional sequences and classification of labeled graphs. \n\n"}
{"id": "1410.6264", "contents": "Title: Capturing spatial interdependence in image features: the counting grid,\n  an epitomic representation for bags of features Abstract: In recent scene recognition research images or large image regions are often\nrepresented as disorganized \"bags\" of features which can then be analyzed using\nmodels originally developed to capture co-variation of word counts in text.\nHowever, image feature counts are likely to be constrained in different ways\nthan word counts in text. For example, as a camera pans upwards from a building\nentrance over its first few floors and then further up into the sky Fig. 1,\nsome feature counts in the image drop while others rise -- only to drop again\ngiving way to features found more often at higher elevations. The space of all\npossible feature count combinations is constrained both by the properties of\nthe larger scene and the size and the location of the window into it. To\ncapture such variation, in this paper we propose the use of the counting grid\nmodel. This generative model is based on a grid of feature counts, considerably\nlarger than any of the modeled images, and considerably smaller than the real\nestate needed to tile the images next to each other tightly. Each modeled image\nis assumed to have a representative window in the grid in which the feature\ncounts mimic the feature distribution in the image. We provide a learning\nprocedure that jointly maps all images in the training set to the counting grid\nand estimates the appropriate local counts in it. Experimentally, we\ndemonstrate that the resulting representation captures the space of feature\ncount combinations more accurately than the traditional models, not only when\nthe input images come from a panning camera, but even when modeling images of\ndifferent scenes from the same category. \n\n"}
{"id": "1411.0126", "contents": "Title: Detection of texts in natural images Abstract: A framework that makes use of Connected components and supervised Support\nmachine to recognise texts is proposed. The image is preprocessed and and edge\ngraph is calculated using a probabilistic framework to compensate for\nphotometric noise. Connected components over the resultant image is calculated,\nwhich is bounded and then pruned using geometric constraints. Finally a Gabor\nFeature based SVM is used to classify the presence of text in the candidates.\nThe proposed method was tested with ICDAR 10 dataset and few other images\navailable on the internet. It resulted in a recall and precision metric of 0.72\nand 0.88 comfortably better than the benchmark Eiphstein's algorithm. The\nproposed method recorded a 0.70 and 0.74 in natural images which is\nsignificantly better than current methods on natural images. The proposed\nmethod also scales almost linearly for high resolution, cluttered images. \n\n"}
{"id": "1411.2861", "contents": "Title: Computational Baby Learning Abstract: Intuitive observations show that a baby may inherently possess the capability\nof recognizing a new visual concept (e.g., chair, dog) by learning from only\nvery few positive instances taught by parent(s) or others, and this recognition\ncapability can be gradually further improved by exploring and/or interacting\nwith the real instances in the physical world. Inspired by these observations,\nwe propose a computational model for slightly-supervised object detection,\nbased on prior knowledge modelling, exemplar learning and learning with video\ncontexts. The prior knowledge is modeled with a pre-trained Convolutional\nNeural Network (CNN). When very few instances of a new concept are given, an\ninitial concept detector is built by exemplar learning over the deep features\nfrom the pre-trained CNN. Simulating the baby's interaction with physical\nworld, the well-designed tracking solution is then used to discover more\ndiverse instances from the massive online unlabeled videos. Once a positive\ninstance is detected/identified with high score in each video, more variable\ninstances possibly from different view-angles and/or different distances are\ntracked and accumulated. Then the concept detector can be fine-tuned based on\nthese new instances. This process can be repeated again and again till we\nobtain a very mature concept detector. Extensive experiments on Pascal\nVOC-07/10/12 object detection datasets well demonstrate the effectiveness of\nour framework. It can beat the state-of-the-art full-training based\nperformances by learning from very few samples for each object category, along\nwith about 20,000 unlabeled videos. \n\n"}
{"id": "1411.3159", "contents": "Title: Part Detector Discovery in Deep Convolutional Neural Networks Abstract: Current fine-grained classification approaches often rely on a robust\nlocalization of object parts to extract localized feature representations\nsuitable for discrimination. However, part localization is a challenging task\ndue to the large variation of appearance and pose. In this paper, we show how\npre-trained convolutional neural networks can be used for robust and efficient\nobject part discovery and localization without the necessity to actually train\nthe network on the current dataset. Our approach called \"part detector\ndiscovery\" (PDD) is based on analyzing the gradient maps of the network outputs\nand finding activation centers spatially related to annotated semantic parts or\nbounding boxes.\n  This allows us not just to obtain excellent performance on the CUB200-2011\ndataset, but in contrast to previous approaches also to perform detection and\nbird classification jointly without requiring a given bounding box annotation\nduring testing and ground-truth parts during training. The code is available at\nhttp://www.inf-cv.uni-jena.de/part_discovery and\nhttps://github.com/cvjena/PartDetectorDisovery. \n\n"}
{"id": "1411.3230", "contents": "Title: Sparse Modeling for Image and Vision Processing Abstract: In recent years, a large amount of multi-disciplinary research has been\nconducted on sparse models and their applications. In statistics and machine\nlearning, the sparsity principle is used to perform model selection---that is,\nautomatically selecting a simple model among a large collection of them. In\nsignal processing, sparse coding consists of representing data with linear\ncombinations of a few dictionary elements. Subsequently, the corresponding\ntools have been widely adopted by several scientific communities such as\nneuroscience, bioinformatics, or computer vision. The goal of this monograph is\nto offer a self-contained view of sparse modeling for visual recognition and\nimage processing. More specifically, we focus on applications where the\ndictionary is learned and adapted to data, yielding a compact representation\nthat has been successful in various contexts. \n\n"}
{"id": "1411.4952", "contents": "Title: From Captions to Visual Concepts and Back Abstract: This paper presents a novel approach for automatically generating image\ndescriptions: visual detectors, language models, and multimodal similarity\nmodels learnt directly from a dataset of image captions. We use multiple\ninstance learning to train visual detectors for words that commonly occur in\ncaptions, including many different parts of speech such as nouns, verbs, and\nadjectives. The word detector outputs serve as conditional inputs to a\nmaximum-entropy language model. The language model learns from a set of over\n400,000 image descriptions to capture the statistics of word usage. We capture\nglobal semantics by re-ranking caption candidates using sentence-level features\nand a deep multimodal similarity model. Our system is state-of-the-art on the\nofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When\nhuman judges compare the system captions to ones written by other people on our\nheld-out test set, the system captions have equal or better quality 34% of the\ntime. \n\n"}
{"id": "1411.5654", "contents": "Title: Learning a Recurrent Visual Representation for Image Caption Generation Abstract: In this paper we explore the bi-directional mapping between images and their\nsentence-based descriptions. We propose learning this mapping using a recurrent\nneural network. Unlike previous approaches that map both sentences and images\nto a common embedding, we enable the generation of novel sentences given an\nimage. Using the same model, we can also reconstruct the visual features\nassociated with an image given its visual description. We use a novel recurrent\nvisual memory that automatically learns to remember long-term visual concepts\nto aid in both sentence generation and visual feature reconstruction. We\nevaluate our approach on several tasks. These include sentence generation,\nsentence retrieval and image retrieval. State-of-the-art results are shown for\nthe task of generating novel image descriptions. When compared to human\ngenerated captions, our automatically generated captions are preferred by\nhumans over $19.8\\%$ of the time. Results are better than or comparable to\nstate-of-the-art results on the image and sentence retrieval tasks for methods\nusing similar visual features. \n\n"}
{"id": "1412.1265", "contents": "Title: Deeply learned face representations are sparse, selective, and robust Abstract: This paper designs a high-performance deep convolutional network (DeepID2+)\nfor face recognition. It is learned with the identification-verification\nsupervisory signal. By increasing the dimension of hidden representations and\nadding supervision to early convolutional layers, DeepID2+ achieves new\nstate-of-the-art on LFW and YouTube Faces benchmarks. Through empirical\nstudies, we have discovered three properties of its deep neural activations\ncritical for the high performance: sparsity, selectiveness and robustness. (1)\nIt is observed that neural activations are moderately sparse. Moderate sparsity\nmaximizes the discriminative power of the deep net as well as the distance\nbetween images. It is surprising that DeepID2+ still can achieve high\nrecognition accuracy even after the neural responses are binarized. (2) Its\nneurons in higher layers are highly selective to identities and\nidentity-related attributes. We can identify different subsets of neurons which\nare either constantly excited or inhibited when different identities or\nattributes are present. Although DeepID2+ is not taught to distinguish\nattributes during training, it has implicitly learned such high-level concepts.\n(3) It is much more robust to occlusions, although occlusion patterns are not\nincluded in the training set. \n\n"}
{"id": "1412.4102", "contents": "Title: Representing Data by a Mixture of Activated Simplices Abstract: We present a new model which represents data as a mixture of simplices.\nSimplices are geometric structures that generalize triangles. We give a simple\ngeometric understanding that allows us to learn a simplicial structure\nefficiently. Our method requires that the data are unit normalized (and thus\nlie on the unit sphere). We show that under this restriction, building a model\nwith simplices amounts to constructing a convex hull inside the sphere whose\nboundary facets is close to the data. We call the boundary facets of the convex\nhull that are close to the data Activated Simplices. While the total number of\nbases used to build the simplices is a parameter of the model, the dimensions\nof the individual activated simplices are learned from the data. Simplices can\nhave different dimensions, which facilitates modeling of inhomogeneous data\nsources. The simplicial structure is bounded --- this is appropriate for\nmodeling data with constraints, such as human elbows can not bend more than 180\ndegrees. The simplices are easy to interpret and extremes within the data can\nbe discovered among the vertices. The method provides good reconstruction and\nregularization. It supports good nearest neighbor classification and it allows\nrealistic generative models to be constructed. It achieves state-of-the-art\nresults on benchmark datasets, including 3D poses and digits. \n\n"}
{"id": "1412.4438", "contents": "Title: Fixed Point Algorithm Based on Quasi-Newton Method for Convex\n  Minimization Problem with Application to Image Deblurring Abstract: Solving an optimization problem whose objective function is the sum of two\nconvex functions has received considerable interests in the context of image\nprocessing recently. In particular, we are interested in the scenario when a\nnon-differentiable convex function such as the total variation (TV) norm is\nincluded in the objective function due to many variational models established\nin image processing have this nature. In this paper, we propose a fast fixed\npoint algorithm based on the quasi-Newton method for solving this class of\nproblem, and apply it in the field of TV-based image deblurring. The novel\nmethod is derived from the idea of the quasi-Newton method, and the fixed-point\nalgorithms based on the proximity operator, which were widely investigated very\nrecently. Utilizing the non-expansion property of the proximity operator we\nfurther investigate the global convergence of the proposed algorithm. Numerical\nexperiments on image deblurring problem with additive or multiplicative noise\nare presented to demonstrate that the proposed algorithm is superior to the\nrecently developed fixed-point algorithm in the computational efficiency. \n\n"}
{"id": "1412.6279", "contents": "Title: Non-parametric PSF estimation from celestial transit solar images using\n  blind deconvolution Abstract: Context: Characterization of instrumental effects in astronomical imaging is\nimportant in order to extract accurate physical information from the\nobservations. The measured image in a real optical instrument is usually\nrepresented by the convolution of an ideal image with a Point Spread Function\n(PSF). Additionally, the image acquisition process is also contaminated by\nother sources of noise (read-out, photon-counting). The problem of estimating\nboth the PSF and a denoised image is called blind deconvolution and is\nill-posed.\n  Aims: We propose a blind deconvolution scheme that relies on image\nregularization. Contrarily to most methods presented in the literature, our\nmethod does not assume a parametric model of the PSF and can thus be applied to\nany telescope.\n  Methods: Our scheme uses a wavelet analysis prior model on the image and weak\nassumptions on the PSF. We use observations from a celestial transit, where the\nocculting body can be assumed to be a black disk. These constraints allow us to\nretain meaningful solutions for the filter and the image, eliminating trivial,\ntranslated and interchanged solutions. Under an additive Gaussian noise\nassumption, they also enforce noise canceling and avoid reconstruction\nartifacts by promoting the whiteness of the residual between the blurred\nobservations and the cleaned data.\n  Results: Our method is applied to synthetic and experimental data. The PSF is\nestimated for the SECCHI/EUVI instrument using the 2007 Lunar transit, and for\nSDO/AIA using the 2012 Venus transit. Results show that the proposed\nnon-parametric blind deconvolution method is able to estimate the core of the\nPSF with a similar quality to parametric methods proposed in the literature. We\nalso show that, if these parametric estimations are incorporated in the\nacquisition model, the resulting PSF outperforms both the parametric and\nnon-parametric methods. \n\n"}
{"id": "1412.6856", "contents": "Title: Object Detectors Emerge in Deep Scene CNNs Abstract: With the success of new computational architectures for visual processing,\nsuch as convolutional neural networks (CNN) and access to image databases with\nmillions of labeled examples (e.g., ImageNet, Places), the state of the art in\ncomputer vision is advancing rapidly. One important factor for continued\nprogress is to understand the representations that are learned by the inner\nlayers of these deep architectures. Here we show that object detectors emerge\nfrom training CNNs to perform scene classification. As scenes are composed of\nobjects, the CNN for scene classification automatically discovers meaningful\nobjects detectors, representative of the learned scene categories. With object\ndetectors emerging as a result of learning to recognize scenes, our work\ndemonstrates that the same network can perform both scene recognition and\nobject localization in a single forward-pass, without ever having been\nexplicitly taught the notion of objects. \n\n"}
{"id": "1412.8070", "contents": "Title: Functional correspondence by matrix completion Abstract: In this paper, we consider the problem of finding dense intrinsic\ncorrespondence between manifolds using the recently introduced functional\nframework. We pose the functional correspondence problem as matrix completion\nwith manifold geometric structure and inducing functional localization with the\n$L_1$ norm. We discuss efficient numerical procedures for the solution of our\nproblem. Our method compares favorably to the accuracy of state-of-the-art\ncorrespondence algorithms on non-rigid shape matching benchmarks, and is\nespecially advantageous in settings when only scarce data is available. \n\n"}
{"id": "1412.8419", "contents": "Title: Simple Image Description Generator via a Linear Phrase-Based Approach Abstract: Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\non the recently release Microsoft COCO dataset. \n\n"}
{"id": "1412.8659", "contents": "Title: Deep Roto-Translation Scattering for Object Classification Abstract: Dictionary learning algorithms or supervised deep convolution networks have\nconsiderably improved the efficiency of predefined feature representations such\nas SIFT. We introduce a deep scattering convolution network, with predefined\nwavelet filters over spatial and angular variables. This representation brings\nan important improvement to results previously obtained with predefined\nfeatures over object image databases such as Caltech and CIFAR. The resulting\naccuracy is comparable to results obtained with unsupervised deep learning and\ndictionary based representations. This shows that refining image\nrepresentations by using geometric priors is a promising direction to improve\nimage classification and its understanding. \n\n"}
{"id": "1501.06202", "contents": "Title: Robust Subjective Visual Property Prediction from Crowdsourced Pairwise\n  Labels Abstract: The problem of estimating subjective visual properties from image and video\nhas attracted increasing interest. A subjective visual property is useful\neither on its own (e.g. image and video interestingness) or as an intermediate\nrepresentation for visual recognition (e.g. a relative attribute). Due to its\nambiguous nature, annotating the value of a subjective visual property for\nlearning a prediction model is challenging. To make the annotation more\nreliable, recent studies employ crowdsourcing tools to collect pairwise\ncomparison labels because human annotators are much better at ranking two\nimages/videos (e.g. which one is more interesting) than giving an absolute\nvalue to each of them separately. However, using crowdsourced data also\nintroduces outliers. Existing methods rely on majority voting to prune the\nannotation outliers/errors. They thus require large amount of pairwise labels\nto be collected. More importantly as a local outlier detection method, majority\nvoting is ineffective in identifying outliers that can cause global ranking\ninconsistencies. In this paper, we propose a more principled way to identify\nannotation outliers by formulating the subjective visual property prediction\ntask as a unified robust learning to rank problem, tackling both the outlier\ndetection and learning to rank jointly. Differing from existing methods, the\nproposed method integrates local pairwise comparison labels together to\nminimise a cost that corresponds to global inconsistency of ranking order. This\nnot only leads to better detection of annotation outliers but also enables\nlearning with extremely sparse annotations. Extensive experiments on various\nbenchmark datasets demonstrate that our new approach significantly outperforms\nstate-of-the-arts alternatives. \n\n"}
{"id": "1501.07738", "contents": "Title: Co-Regularized Deep Representations for Video Summarization Abstract: Compact keyframe-based video summaries are a popular way of generating\nviewership on video sharing platforms. Yet, creating relevant and compelling\nsummaries for arbitrarily long videos with a small number of keyframes is a\nchallenging task. We propose a comprehensive keyframe-based summarization\nframework combining deep convolutional neural networks and restricted Boltzmann\nmachines. An original co-regularization scheme is used to discover meaningful\nsubject-scene associations. The resulting multimodal representations are then\nused to select highly-relevant keyframes. A comprehensive user study is\nconducted comparing our proposed method to a variety of schemes, including the\nsummarization currently in use by one of the most popular video sharing\nwebsites. The results show that our method consistently outperforms the\nbaseline schemes for any given amount of keyframes both in terms of\nattractiveness and informativeness. The lead is even more significant for\nsmaller summaries. \n\n"}
{"id": "1502.02734", "contents": "Title: Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image\n  Segmentation Abstract: Deep convolutional neural networks (DCNNs) trained on a large number of\nimages with strong pixel-level annotations have recently significantly pushed\nthe state-of-art in semantic image segmentation. We study the more challenging\nproblem of learning DCNNs for semantic image segmentation from either (1)\nweakly annotated training data such as bounding boxes or image-level labels or\n(2) a combination of few strongly labeled and many weakly labeled images,\nsourced from one or multiple datasets. We develop Expectation-Maximization (EM)\nmethods for semantic image segmentation model training under these weakly\nsupervised and semi-supervised settings. Extensive experimental evaluation\nshows that the proposed techniques can learn models delivering competitive\nresults on the challenging PASCAL VOC 2012 image segmentation benchmark, while\nrequiring significantly less annotation effort. We share source code\nimplementing the proposed system at\nhttps://bitbucket.org/deeplab/deeplab-public. \n\n"}
{"id": "1502.04569", "contents": "Title: Image Specificity Abstract: For some images, descriptions written by multiple people are consistent with\neach other. But for other images, descriptions across people vary considerably.\nIn other words, some images are specific $-$ they elicit consistent\ndescriptions from different people $-$ while other images are ambiguous.\nApplications involving images and text can benefit from an understanding of\nwhich images are specific and which ones are ambiguous. For instance, consider\ntext-based image retrieval. If a query description is moderately similar to the\ncaption (or reference description) of an ambiguous image, that query may be\nconsidered a decent match to the image. But if the image is very specific, a\nmoderate similarity between the query and the reference description may not be\nsufficient to retrieve the image.\n  In this paper, we introduce the notion of image specificity. We present two\nmechanisms to measure specificity given multiple descriptions of an image: an\nautomated measure and a measure that relies on human judgement. We analyze\nimage specificity with respect to image content and properties to better\nunderstand what makes an image specific. We then train models to automatically\npredict the specificity of an image from image features alone without requiring\ntextual descriptions of the image. Finally, we show that modeling image\nspecificity leads to improvements in a text-based image retrieval application. \n\n"}
{"id": "1502.05243", "contents": "Title: SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks Abstract: The task of classifying videos of natural dynamic scenes into appropriate\nclasses has gained lot of attention in recent years. The problem especially\nbecomes challenging when the camera used to capture the video is dynamic. In\nthis paper, we analyse the performance of statistical aggregation (SA)\ntechniques on various pre-trained convolutional neural network(CNN) models to\naddress this problem. The proposed approach works by extracting CNN activation\nfeatures for a number of frames in a video and then uses an aggregation scheme\nin order to obtain a robust feature descriptor for the video. We show through\nresults that the proposed approach performs better than the-state-of-the arts\nfor the Maryland and YUPenn dataset. The final descriptor obtained is powerful\nenough to distinguish among dynamic scenes and is even capable of addressing\nthe scenario where the camera motion is dominant and the scene dynamics are\ncomplex. Further, this paper shows an extensive study on the performance of\nvarious aggregation methods and their combinations. We compare the proposed\napproach with other dynamic scene classification algorithms on two publicly\navailable datasets - Maryland and YUPenn to demonstrate the superior\nperformance of the proposed approach. \n\n"}
{"id": "1502.06464", "contents": "Title: Rectified Factor Networks Abstract: We propose rectified factor networks (RFNs) to efficiently construct very\nsparse, non-linear, high-dimensional representations of the input. RFN models\nidentify rare and small events in the input, have a low interference between\ncode units, have a small reconstruction error, and explain the data covariance\nstructure. RFN learning is a generalized alternating minimization algorithm\nderived from the posterior regularization method which enforces non-negative\nand normalized posterior means. We proof convergence and correctness of the RFN\nlearning algorithm. On benchmarks, RFNs are compared to other unsupervised\nmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to\nprevious sparse coding methods, RFNs yield sparser codes, capture the data's\ncovariance structure more precisely, and have a significantly smaller\nreconstruction error. We test RFNs as pretraining technique for deep networks\non different vision datasets, where RFNs were superior to RBMs and\nautoencoders. On gene expression data from two pharmaceutical drug discovery\nstudies, RFNs detected small and rare gene modules that revealed highly\nrelevant new biological insights which were so far missed by other unsupervised\nmethods. \n\n"}
{"id": "1503.01224", "contents": "Title: Temporal Pyramid Pooling Based Convolutional Neural Networks for Action\n  Recognition Abstract: Encouraged by the success of Convolutional Neural Networks (CNNs) in image\nclassification, recently much effort is spent on applying CNNs to video based\naction recognition problems. One challenge is that video contains a varying\nnumber of frames which is incompatible to the standard input format of CNNs.\nExisting methods handle this issue either by directly sampling a fixed number\nof frames or bypassing this issue by introducing a 3D convolutional layer which\nconducts convolution in spatial-temporal domain.\n  To solve this issue, here we propose a novel network structure which allows\nan arbitrary number of frames as the network input. The key of our solution is\nto introduce a module consisting of an encoding layer and a temporal pyramid\npooling layer. The encoding layer maps the activation from previous layers to a\nfeature vector suitable for pooling while the temporal pyramid pooling layer\nconverts multiple frame-level activations into a fixed-length video-level\nrepresentation. In addition, we adopt a feature concatenation layer which\ncombines appearance information and motion information. Compared with the frame\nsampling strategy, our method avoids the risk of missing any important frames.\nCompared with the 3D convolutional method which requires a huge video dataset\nfor network training, our model can be learned on a small target dataset\nbecause we can leverage the off-the-shelf image-level CNN for model parameter\ninitialization. Experiments on two challenging datasets, Hollywood2 and HMDB51,\ndemonstrate that our method achieves superior performance over state-of-the-art\nmethods while requiring much fewer training data. \n\n"}
{"id": "1503.01228", "contents": "Title: Bethe Learning of Conditional Random Fields via MAP Decoding Abstract: Many machine learning tasks can be formulated in terms of predicting\nstructured outputs. In frameworks such as the structured support vector machine\n(SVM-Struct) and the structured perceptron, discriminative functions are\nlearned by iteratively applying efficient maximum a posteriori (MAP) decoding.\nHowever, maximum likelihood estimation (MLE) of probabilistic models over these\nsame structured spaces requires computing partition functions, which is\ngenerally intractable. This paper presents a method for learning discrete\nexponential family models using the Bethe approximation to the MLE. Remarkably,\nthis problem also reduces to iterative (MAP) decoding. This connection emerges\nby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a\nconvex dual objective which circumvents the intractable partition function. The\nresult is a new single loop algorithm MLE-Struct, which is substantially more\nefficient than previous double-loop methods for approximate maximum likelihood\nestimation. Our algorithm outperforms existing methods in experiments involving\nimage segmentation, matching problems from vision, and a new dataset of\nuniversity roommate assignments. \n\n"}
{"id": "1503.01521", "contents": "Title: Jointly Learning Multiple Measures of Similarities from Triplet\n  Comparisons Abstract: Similarity between objects is multi-faceted and it can be easier for human\nannotators to measure it when the focus is on a specific aspect. We consider\nthe problem of mapping objects into view-specific embeddings where the distance\nbetween them is consistent with the similarity comparisons of the form \"from\nthe t-th view, object A is more similar to B than to C\". Our framework jointly\nlearns view-specific embeddings exploiting correlations between views.\nExperiments on a number of datasets, including one of multi-view crowdsourced\ncomparison on bird images, show the proposed method achieves lower triplet\ngeneralization error when compared to both learning embeddings independently\nfor each view and all views pooled into one view. Our method can also be used\nto learn multiple measures of similarity over input features taking class\nlabels into account and compares favorably to existing approaches for\nmulti-task metric learning on the ISOLET dataset. \n\n"}
{"id": "1503.02302", "contents": "Title: DESAT: an SSW tool for SDO/AIA image de-saturation Abstract: Saturation affects a significant rate of images recorded by the Atmospheric\nImaging Assembly on the Solar Dynamics Observatory. This paper describes a\ncomputational method and a technological pipeline for the de-saturation of such\nimages, based on several mathematical ingredients like Expectation\nMaximization, image correlation and interpolation. An analysis of the\ncomputational properties and demands of the pipeline, together with an\nassessment of its reliability are performed against a set of data recorded from\nthe Feburary 25 2014 flaring event. \n\n"}
{"id": "1503.07790", "contents": "Title: Transductive Multi-label Zero-shot Learning Abstract: Zero-shot learning has received increasing interest as a means to alleviate\nthe often prohibitive expense of annotating training data for large scale\nrecognition problems. These methods have achieved great success via learning\nintermediate semantic representations in the form of attributes and more\nrecently, semantic word vectors. However, they have thus far been constrained\nto the single-label case, in contrast to the growing popularity and importance\nof more realistic multi-label data. In this paper, for the first time, we\ninvestigate and formalise a general framework for multi-label zero-shot\nlearning, addressing the unique challenge therein: how to exploit multi-label\ncorrelation at test time with no training data for those classes? In\nparticular, we propose (1) a multi-output deep regression model to project an\nimage into a semantic word space, which explicitly exploits the correlations in\nthe intermediate semantic layer of word vectors; (2) a novel zero-shot learning\nalgorithm for multi-label data that exploits the unique compositionality\nproperty of semantic word vector representations; and (3) a transductive\nlearning strategy to enable the regression model learned from seen classes to\ngeneralise well to unseen classes. Our zero-shot learning experiments on a\nnumber of standard multi-label datasets demonstrate that our method outperforms\na variety of baselines. \n\n"}
{"id": "1504.04943", "contents": "Title: Weakly Supervised Fine-Grained Image Categorization Abstract: In this paper, we categorize fine-grained images without using any object /\npart annotation neither in the training nor in the testing stage, a step\ntowards making it suitable for deployments. Fine-grained image categorization\naims to classify objects with subtle distinctions. Most existing works heavily\nrely on object / part detectors to build the correspondence between object\nparts by using object or object part annotations inside training images. The\nneed for expensive object annotations prevents the wide usage of these methods.\nInstead, we propose to select useful parts from multi-scale part proposals in\nobjects, and use them to compute a global image representation for\ncategorization. This is specially designed for the annotation-free fine-grained\ncategorization task, because useful parts have shown to play an important role\nin existing annotation-dependent works but accurate part detectors can be\nhardly acquired. With the proposed image representation, we can further detect\nand visualize the key (most discriminative) parts in objects of different\nclasses. In the experiment, the proposed annotation-free method achieves better\naccuracy than that of state-of-the-art annotation-free and most existing\nannotation-dependent methods on two challenging datasets, which shows that it\nis not always necessary to use accurate object / part annotations in\nfine-grained image categorization. \n\n"}
{"id": "1504.05241", "contents": "Title: Convolutional Neural Network-Based Image Representation for Visual Loop\n  Closure Detection Abstract: Deep convolutional neural networks (CNN) have recently been shown in many\ncomputer vision and pattern recog- nition applications to outperform by a\nsignificant margin state- of-the-art solutions that use traditional\nhand-crafted features. However, this impressive performance is yet to be fully\nexploited in robotics. In this paper, we focus one specific problem that can\nbenefit from the recent development of the CNN technology, i.e., we focus on\nusing a pre-trained CNN model as a method of generating an image representation\nappropriate for visual loop closure detection in SLAM (simultaneous\nlocalization and mapping). We perform a comprehensive evaluation of the outputs\nat the intermediate layers of a CNN as image descriptors, in comparison with\nstate-of-the-art image descriptors, in terms of their ability to match images\nfor detecting loop closures. The main conclusions of our study include: (a)\nCNN-based image representations perform comparably to state-of-the-art hand-\ncrafted competitors in environments without significant lighting change, (b)\nthey outperform state-of-the-art competitors when lighting changes\nsignificantly, and (c) they are also significantly faster to extract than the\nstate-of-the-art hand-crafted features even on a conventional CPU and are two\norders of magnitude faster on an entry-level GPU. \n\n"}
{"id": "1504.06779", "contents": "Title: Computational Cost Reduction in Learned Transform Classifications Abstract: We present a theoretical analysis and empirical evaluations of a novel set of\ntechniques for computational cost reduction of classifiers that are based on\nlearned transform and soft-threshold. By modifying optimization procedures for\ndictionary and classifier training, as well as the resulting dictionary\nentries, our techniques allow to reduce the bit precision and to replace each\nfloating-point multiplication by a single integer bit shift. We also show how\nthe optimization algorithms in some dictionary training methods can be modified\nto penalize higher-energy dictionaries. We applied our techniques with the\nclassifier Learning Algorithm for Soft-Thresholding, testing on the datasets\nused in its original paper. Our results indicate it is feasible to use solely\nsums and bit shifts of integers to classify at test time with a limited\nreduction of the classification accuracy. These low power operations are a\nvaluable trade off in FPGA implementations as they increase the classification\nthroughput while decrease both energy consumption and manufacturing cost. \n\n"}
{"id": "1504.07488", "contents": "Title: Speeding Up Neural Networks for Large Scale Classification using WTA\n  Hashing Abstract: In this paper we propose to use the Winner Takes All hashing technique to\nspeed up forward propagation and backward propagation in fully connected layers\nin convolutional neural networks. The proposed technique reduces significantly\nthe computational complexity, which in turn, allows us to train layers with a\nlarge number of kernels with out the associated time penalty.\n  As a consequence we are able to train convolutional neural network on a very\nlarge number of output classes with only a small increase in the computational\ncost. To show the effectiveness of the technique we train a new output layer on\na pretrained network using both the regular multiplicative approach and our\nproposed hashing methodology. Our results showed no drop in performance and\ndemonstrate, with our implementation, a 7 fold speed up during the training. \n\n"}
{"id": "1504.07907", "contents": "Title: A Flexible Tensor Block Coordinate Ascent Scheme for Hypergraph Matching Abstract: The estimation of correspondences between two images resp. point sets is a\ncore problem in computer vision. One way to formulate the problem is graph\nmatching leading to the quadratic assignment problem which is NP-hard. Several\nso called second order methods have been proposed to solve this problem. In\nrecent years hypergraph matching leading to a third order problem became\npopular as it allows for better integration of geometric information. For most\nof these third order algorithms no theoretical guarantees are known. In this\npaper we propose a general framework for tensor block coordinate ascent methods\nfor hypergraph matching. We propose two algorithms which both come along with\nthe guarantee of monotonic ascent in the matching score on the set of discrete\nassignment matrices. In the experiments we show that our new algorithms\noutperform previous work both in terms of achieving better matching scores and\nmatching accuracy. This holds in particular for very challenging settings where\none has a high number of outliers and other forms of noise. \n\n"}
{"id": "1504.07918", "contents": "Title: Robust hyperspectral image classification with rejection fields Abstract: In this paper we present a novel method for robust hyperspectral image\nclassification using context and rejection. Hyperspectral image classification\nis generally an ill-posed image problem where pixels may belong to unknown\nclasses, and obtaining representative and complete training sets is costly.\nFurthermore, the need for high classification accuracies is frequently greater\nthan the need to classify the entire image.\n  We approach this problem with a robust classification method that combines\nclassification with context with classification with rejection. A rejection\nfield that will guide the rejection is derived from the classification with\ncontextual information obtained by using the SegSALSA algorithm. We validate\nour method in real hyperspectral data and show that the performance gains\nobtained from the rejection fields are equivalent to an increase the dimension\nof the training sets. \n\n"}
{"id": "1505.00066", "contents": "Title: Pose Induction for Novel Object Categories Abstract: We address the task of predicting pose for objects of unannotated object\ncategories from a small seed set of annotated object classes. We present a\ngeneralized classifier that can reliably induce pose given a single instance of\na novel category. In case of availability of a large collection of novel\ninstances, our approach then jointly reasons over all instances to improve the\ninitial estimates. We empirically validate the various components of our\nalgorithm and quantitatively show that our method produces reliable pose\nestimates. We also show qualitative results on a diverse set of classes and\nfurther demonstrate the applicability of our system for learning shape models\nof novel object classes. \n\n"}
{"id": "1505.00571", "contents": "Title: Higher Order Maximum Persistency and Comparison Theorems Abstract: We address combinatorial problems that can be formulated as minimization of a\npartially separable function of discrete variables (energy minimization in\ngraphical models, weighted constraint satisfaction, pseudo-Boolean\noptimization, 0-1 polynomial programming). For polyhedral relaxations of such\nproblems it is generally not true that variables integer in the relaxed\nsolution will retain the same values in the optimal discrete solution. Those\nwhich do are called persistent. Such persistent variables define a part of a\nglobally optimal solution. Once identified, they can be excluded from the\nproblem, reducing its size.\n  To any polyhedral relaxation we associate a sufficient condition proving\npersistency of a subset of variables. We set up a specially constructed linear\nprogram which determines the set of persistent variables maximal with respect\nto the relaxation. The condition improves as the relaxation is tightened and\npossesses all its invariances. The proposed framework explains a variety of\nexisting methods originating from different areas of research and based on\ndifferent principles. A theoretical comparison is established that relates\nthese methods to the standard linear relaxation and proves that the proposed\ntechnique identifies same or larger set of persistent variables. \n\n"}
{"id": "1505.00687", "contents": "Title: Unsupervised Learning of Visual Representations using Videos Abstract: Is strong supervision necessary for learning a good visual representation? Do\nwe really need millions of semantically-labeled images to train a Convolutional\nNeural Network (CNN)? In this paper, we present a simple yet surprisingly\npowerful approach for unsupervised learning of CNN. Specifically, we use\nhundreds of thousands of unlabeled videos from the web to learn visual\nrepresentations. Our key idea is that visual tracking provides the supervision.\nThat is, two patches connected by a track should have similar visual\nrepresentation in deep feature space since they probably belong to the same\nobject or object part. We design a Siamese-triplet network with a ranking loss\nfunction to train this CNN representation. Without using a single image from\nImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train\nan ensemble of unsupervised networks that achieves 52% mAP (no bounding box\nregression). This performance comes tantalizingly close to its\nImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We\nalso show that our unsupervised network can perform competitively in other\ntasks such as surface-normal estimation. \n\n"}
{"id": "1505.01953", "contents": "Title: The structure of optimal parameters for image restoration problems Abstract: We study the qualitative properties of optimal regularisation parameters in\nvariational models for image restoration. The parameters are solutions of\nbilevel optimisation problems with the image restoration problem as constraint.\nA general type of regulariser is considered, which encompasses total variation\n(TV), total generalized variation (TGV) and infimal-convolution total variation\n(ICTV). We prove that under certain conditions on the given data optimal\nparameters derived by bilevel optimisation problems exist. A crucial point in\nthe existence proof turns out to be the boundedness of the optimal parameters\naway from $0$ which we prove in this paper. The analysis is done on the\noriginal -- in image restoration typically non-smooth variational problem -- as\nwell as on a smoothed approximation set in Hilbert space which is the one\nconsidered in numerical computations. For the smoothed bilevel problem we also\nprove that it $\\Gamma$ converges to the original problem as the smoothing\nvanishes. All analysis is done in function spaces rather than on the\ndiscretised learning problem. \n\n"}
{"id": "1505.02120", "contents": "Title: Bilevel approaches for learning of variational imaging models Abstract: We review some recent learning approaches in variational imaging, based on\nbilevel optimisation, and emphasize the importance of their treatment in\nfunction space. The paper covers both analytical and numerical techniques.\nAnalytically, we include results on the existence and structure of minimisers,\nas well as optimality conditions for their characterisation. Based on this\ninformation, Newton type methods are studied for the solution of the problems\nat hand, combining them with sampling techniques in case of large databases.\nThe computational verification of the developed techniques is extensively\ndocumented, covering instances with different type of regularisers, several\nnoise models, spatially dependent weights and large image databases. \n\n"}
{"id": "1505.04467", "contents": "Title: Exploring Nearest Neighbor Approaches for Image Captioning Abstract: We explore a variety of nearest neighbor baseline approaches for image\ncaptioning. These approaches find a set of nearest neighbor images in the\ntraining set from which a caption may be borrowed for the query image. We\nselect a caption for the query image by finding the caption that best\nrepresents the \"consensus\" of the set of candidate captions gathered from the\nnearest neighbor images. When measured by automatic evaluation metrics on the\nMS COCO caption evaluation server, these approaches perform as well as many\nrecent approaches that generate novel captions. However, human studies show\nthat a method that generates novel captions is still preferred over the nearest\nneighbor approach. \n\n"}
{"id": "1505.05286", "contents": "Title: Measuring Visibility using Atmospheric Transmission and Digital Surface\n  Model Abstract: Reliable and exact assessment of visibility is essential for safe air\ntraffic. In order to overcome the drawbacks of the currently subjective reports\nfrom human observers, we present an approach to automatically derive visibility\nmeasures by means of image processing. It first exploits image based estimation\nof the atmospheric transmission describing the portion of the light that is not\nscattered by atmospheric phenomena (e.g., haze, fog, smoke) and reaches the\ncamera. Once the atmospheric transmission is estimated, a 3D representation of\nthe vicinity (digital surface model: DMS) is used to compute depth measurements\nfor the haze-free pixels and then derive a global visibility estimation for the\nairport. Results on foggy images demonstrate the validity of the proposed\nmethod. \n\n"}
{"id": "1505.05641", "contents": "Title: Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with\n  Rendered 3D Model Views Abstract: Object viewpoint estimation from 2D images is an essential task in computer\nvision. However, two issues hinder its progress: scarcity of training data with\nviewpoint annotations, and a lack of powerful features. Inspired by the growing\navailability of 3D models, we propose a framework to address both issues by\ncombining render-based image synthesis and CNNs. We believe that 3D models have\nthe potential in generating a large number of images of high variation, which\ncan be well exploited by deep CNN with a high learning capacity. Towards this\ngoal, we propose a scalable and overfit-resistant image synthesis pipeline,\ntogether with a novel CNN specifically tailored for the viewpoint estimation\ntask. Experimentally, we show that the viewpoint estimation from our pipeline\ncan significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark. \n\n"}
{"id": "1505.06250", "contents": "Title: Efficient Large Scale Video Classification Abstract: Video classification has advanced tremendously over the recent years. A large\npart of the improvements in video classification had to do with the work done\nby the image classification community and the use of deep convolutional\nnetworks (CNNs) which produce competitive results with hand- crafted motion\nfeatures. These networks were adapted to use video frames in various ways and\nhave yielded state of the art classification results. We present two methods\nthat build on this work, and scale it up to work with millions of videos and\nhundreds of thousands of classes while maintaining a low computational cost. In\nthe context of large scale video processing, training CNNs on video frames is\nextremely time consuming, due to the large number of frames involved. We\npropose to avoid this problem by training CNNs on either YouTube thumbnails or\nFlickr images, and then using these networks' outputs as features for other\nhigher level classifiers. We discuss the challenges of achieving this and\npropose two models for frame-level and video-level classification. The first is\na highly efficient mixture of experts while the latter is based on long short\nterm memory neural networks. We present results on the Sports-1M video dataset\n(1 million videos, 487 classes) and on a new dataset which has 12 million\nvideos and 150,000 labels. \n\n"}
{"id": "1506.00333", "contents": "Title: Learning to Answer Questions From Image Using Convolutional Neural\n  Network Abstract: In this paper, we propose to employ the convolutional neural network (CNN)\nfor the image question answering (QA). Our proposed CNN provides an end-to-end\nframework with convolutional architectures for learning not only the image and\nquestion representations, but also their inter-modal interactions to produce\nthe answer. More specifically, our model consists of three CNNs: one image CNN\nto encode the image content, one sentence CNN to compose the words of the\nquestion, and one multimodal convolution layer to learn their joint\nrepresentation for the classification in the space of candidate answer words.\nWe demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA\ndatasets, which are two benchmark datasets for the image QA, with the\nperformances significantly outperforming the state-of-the-art. \n\n"}
{"id": "1506.01066", "contents": "Title: Visualizing and Understanding Neural Models in NLP Abstract: While neural networks have been successfully applied to many NLP tasks the\nresulting vector-based models are very difficult to interpret. For example it's\nnot clear how they achieve {\\em compositionality}, building sentence meaning\nfrom the meanings of words and phrases. In this paper we describe four\nstrategies for visualizing compositionality in neural models for NLP, inspired\nby similar work in computer vision. We first plot unit values to visualize\ncompositionality of negation, intensification, and concessive clauses, allow us\nto see well-known markedness asymmetries in negation. We then introduce three\nsimple and straightforward methods for visualizing a unit's {\\em salience}, the\namount it contributes to the final composed meaning: (1) gradient\nback-propagation, (2) the variance of a token from the average word node, (3)\nLSTM-style gates that measure information flow. We test our methods on\nsentiment using simple recurrent nets and LSTMs. Our general-purpose methods\nmay have wide applications for understanding compositionality and other\nsemantic properties of deep networks , and also shed light on why LSTMs\noutperform simple recurrent nets, \n\n"}
{"id": "1506.02078", "contents": "Title: Visualizing and Understanding Recurrent Networks Abstract: Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study. \n\n"}
{"id": "1506.03995", "contents": "Title: Technical Report: Image Captioning with Semantically Similar Images Abstract: This report presents our submission to the MS COCO Captioning Challenge 2015.\nThe method uses Convolutional Neural Network activations as an embedding to\nfind semantically similar images. From these images, the most typical caption\nis selected based on unigram frequencies. Although the method received low\nscores with automated evaluation metrics and in human assessed average\ncorrectness, it is competitive in the ratio of captions which pass the Turing\ntest and which are assessed as better or equal to human captions. \n\n"}
{"id": "1506.04579", "contents": "Title: ParseNet: Looking Wider to See Better Abstract: We present a technique for adding global context to deep convolutional\nnetworks for semantic segmentation. The approach is simple, using the average\nfeature for a layer to augment the features at each location. In addition, we\nstudy several idiosyncrasies of training, significantly increasing the\nperformance of baseline networks (e.g. from FCN). When we add our proposed\nglobal feature, and a technique for learning normalization parameters, accuracy\nincreases consistently even over our improved versions of the baselines. Our\nproposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow\nand PASCAL-Context with small additional computational cost over baselines, and\nnear current state-of-the-art performance on PASCAL VOC 2012 semantic\nsegmentation with a simple approach. Code is available at\nhttps://github.com/weiliu89/caffe/tree/fcn . \n\n"}
{"id": "1506.04954", "contents": "Title: A Tensor-Based Dictionary Learning Approach to Tomographic Image\n  Reconstruction Abstract: We consider tomographic reconstruction using priors in the form of a\ndictionary learned from training images. The reconstruction has two stages:\nfirst we construct a tensor dictionary prior from our training data, and then\nwe pose the reconstruction problem in terms of recovering the expansion\ncoefficients in that dictionary. Our approach differs from past approaches in\nthat a) we use a third-order tensor representation for our images and b) we\nrecast the reconstruction problem using the tensor formulation. The dictionary\nlearning problem is presented as a non-negative tensor factorization problem\nwith sparsity constraints. The reconstruction problem is formulated in a convex\noptimization framework by looking for a solution with a sparse representation\nin the tensor dictionary. Numerical results show that our tensor formulation\nleads to very sparse representations of both the training images and the\nreconstructions due to the ability of representing repeated features compactly\nin the dictionary. \n\n"}
{"id": "1506.05163", "contents": "Title: Deep Convolutional Networks on Graph-Structured Data Abstract: Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate. \n\n"}
{"id": "1506.05439", "contents": "Title: Learning with a Wasserstein Loss Abstract: Learning to predict multi-label outputs is challenging, but in many problems\nthere is a natural metric on the outputs that can be used to improve\npredictions. In this paper we develop a loss function for multi-label learning,\nbased on the Wasserstein distance. The Wasserstein distance provides a natural\nnotion of dissimilarity for probability measures. Although optimizing with\nrespect to the exact Wasserstein distance is costly, recent work has described\na regularized approximation that is efficiently computed. We describe an\nefficient learning algorithm based on this regularization, as well as a novel\nextension of the Wasserstein distance from probability measures to unnormalized\nmeasures. We also describe a statistical learning bound for the loss. The\nWasserstein loss can encourage smoothness of the predictions with respect to a\nchosen metric on the output space. We demonstrate this property on a real-data\ntag prediction problem, using the Yahoo Flickr Creative Commons dataset,\noutperforming a baseline that doesn't use the metric. \n\n"}
{"id": "1506.06579", "contents": "Title: Understanding Neural Networks Through Deep Visualization Abstract: Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup. \n\n"}
{"id": "1506.06868", "contents": "Title: Learning Discriminative Bayesian Networks from High-dimensional\n  Continuous Neuroimaging Data Abstract: Due to its causal semantics, Bayesian networks (BN) have been widely employed\nto discover the underlying data relationship in exploratory studies, such as\nbrain research. Despite its success in modeling the probability distribution of\nvariables, BN is naturally a generative model, which is not necessarily\ndiscriminative. This may cause the ignorance of subtle but critical network\nchanges that are of investigation values across populations. In this paper, we\npropose to improve the discriminative power of BN models for continuous\nvariables from two different perspectives. This brings two general\ndiscriminative learning frameworks for Gaussian Bayesian networks (GBN). In the\nfirst framework, we employ Fisher kernel to bridge the generative models of GBN\nand the discriminative classifiers of SVMs, and convert the GBN parameter\nlearning to Fisher kernel learning via minimizing a generalization error bound\nof SVMs. In the second framework, we employ the max-margin criterion and build\nit directly upon GBN models to explicitly optimize the classification\nperformance of the GBNs. The advantages and disadvantages of the two frameworks\nare discussed and experimentally compared. Both of them demonstrate strong\npower in learning discriminative parameters of GBNs for neuroimaging based\nbrain network analysis, as well as maintaining reasonable representation\ncapacity. The contributions of this paper also include a new Directed Acyclic\nGraph (DAG) constraint with theoretical guarantee to ensure the graph validity\nof GBN. \n\n"}
{"id": "1507.01581", "contents": "Title: Joint Calibration for Semantic Segmentation Abstract: Semantic segmentation is the task of assigning a class-label to each pixel in\nan image. We propose a region-based semantic segmentation framework which\nhandles both full and weak supervision, and addresses three common problems:\n(1) Objects occur at multiple scales and therefore we should use regions at\nmultiple scales. However, these regions are overlapping which creates\nconflicting class predictions at the pixel-level. (2) Class frequencies are\nhighly imbalanced in realistic datasets. (3) Each pixel can only be assigned to\na single class, which creates competition between classes. We address all three\nproblems with a joint calibration method which optimizes a multi-class loss\ndefined over the final pixel-level output labeling, as opposed to simply region\nclassification. Our method outperforms the state-of-the-art on the popular SIFT\nFlow [18] dataset in both the fully and weakly supervised setting by a\nconsiderably margin (+6% and +10%, respectively). \n\n"}
{"id": "1507.02772", "contents": "Title: Riemannian Dictionary Learning and Sparse Coding for Positive Definite\n  Matrices Abstract: Data encoded as symmetric positive definite (SPD) matrices frequently arise\nin many areas of computer vision and machine learning. While these matrices\nform an open subset of the Euclidean space of symmetric matrices, viewing them\nthrough the lens of non-Euclidean Riemannian geometry often turns out to be\nbetter suited in capturing several desirable data properties. However,\nformulating classical machine learning algorithms within such a geometry is\noften non-trivial and computationally expensive. Inspired by the great success\nof dictionary learning and sparse coding for vector-valued data, our goal in\nthis paper is to represent data in the form of SPD matrices as sparse conic\ncombinations of SPD atoms from a learned dictionary via a Riemannian geometric\napproach. To that end, we formulate a novel Riemannian optimization objective\nfor dictionary learning and sparse coding in which the representation loss is\ncharacterized via the affine invariant Riemannian metric. We also present a\ncomputationally simple algorithm for optimizing our model. Experiments on\nseveral computer vision datasets demonstrate superior classification and\nretrieval performance using our approach when compared to sparse coding via\nalternative non-Riemannian formulations. \n\n"}
{"id": "1508.01108", "contents": "Title: Evaluating color texture descriptors under large variations of\n  controlled lighting conditions Abstract: The recognition of color texture under varying lighting conditions is still\nan open issue. Several features have been proposed for this purpose, ranging\nfrom traditional statistical descriptors to features extracted with neural\nnetworks. Still, it is not completely clear under what circumstances a feature\nperforms better than the others. In this paper we report an extensive\ncomparison of old and new texture features, with and without a color\nnormalization step, with a particular focus on how they are affected by small\nand large variation in the lighting conditions. The evaluation is performed on\na new texture database including 68 samples of raw food acquired under 46\nconditions that present single and combined variations of light color,\ndirection and intensity. The database allows to systematically investigate the\nrobustness of texture descriptors across a large range of variations of imaging\nconditions. \n\n"}
{"id": "1508.05306", "contents": "Title: Exemplar Based Deep Discriminative and Shareable Feature Learning for\n  Scene Image Classification Abstract: In order to encode the class correlation and class specific information in\nimage representation, we propose a new local feature learning approach named\nDeep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to\nhierarchically learn feature transformation filter banks to transform raw pixel\nimage patches to features. The learned filter banks are expected to: (1) encode\ncommon visual patterns of a flexible number of categories; (2) encode\ndiscriminative information; and (3) hierarchically extract patterns at\ndifferent visual levels. Particularly, in each single layer of DDSFL, shareable\nfilters are jointly learned for classes which share the similar patterns.\nDiscriminative power of the filters is achieved by enforcing the features from\nthe same category to be close, while features from different categories to be\nfar away from each other. Furthermore, we also propose two exemplar selection\nmethods to iteratively select training data for more efficient and effective\nlearning. Based on the experimental results, DDSFL can achieve very promising\nperformance, and it also shows great complementary effect to the\nstate-of-the-art Caffe features. \n\n"}
{"id": "1508.06535", "contents": "Title: Deep Convolutional Neural Networks for Smile Recognition Abstract: This thesis describes the design and implementation of a smile detector based\non deep convolutional neural networks. It starts with a summary of neural\nnetworks, the difficulties of training them and new training methods, such as\nRestricted Boltzmann Machines or autoencoders. It then provides a literature\nreview of convolutional neural networks and recurrent neural networks. In order\nto select databases for smile recognition, comprehensive statistics of\ndatabases popular in the field of facial expression recognition were generated\nand are summarized in this thesis. It then proposes a model for smile\ndetection, of which the main part is implemented. The experimental results are\ndiscussed in this thesis and justified based on a comprehensive model selection\nperformed. All experiments were run on a Tesla K40c GPU benefiting from a\nspeedup of up to factor 10 over the computations on a CPU. A smile detection\ntest accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous\nFacial Action (DISFA) database, significantly outperforming existing approaches\nwith accuracies ranging from 65.55% to 79.67%. This experiment is re-run under\nvarious variations, such as retaining less neutral images or only the low or\nhigh intensities, of which the results are extensively compared. \n\n"}
{"id": "1508.06708", "contents": "Title: Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose\n  Estimation Abstract: This paper focuses on structured-output learning using deep neural networks\nfor 3D human pose estimation from monocular images. Our network takes an image\nand 3D pose as inputs and outputs a score value, which is high when the\nimage-pose pair matches and low otherwise. The network structure consists of a\nconvolutional neural network for image feature extraction, followed by two\nsub-networks for transforming the image features and pose into a joint\nembedding. The score function is then the dot-product between the image and\npose embeddings. The image-pose embedding and score function are jointly\ntrained using a maximum-margin cost function. Our proposed framework can be\ninterpreted as a special form of structured support vector machines where the\njoint feature space is discriminatively learned using deep neural networks. We\ntest our framework on the Human3.6m dataset and obtain state-of-the-art results\ncompared to other recent methods. Finally, we present visualizations of the\nimage-pose embedding space, demonstrating the network has learned a high-level\nembedding of body-orientation and pose-configuration. \n\n"}
{"id": "1509.01602", "contents": "Title: Object Recognition from Short Videos for Robotic Perception Abstract: Deep neural networks have become the primary learning technique for object\nrecognition. Videos, unlike still images, are temporally coherent which makes\nthe application of deep networks non-trivial. Here, we investigate how motion\ncan aid object recognition in short videos. Our approach is based on Long\nShort-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs,\nwe implement each gate as a convolution. We show that convolutional-based LSTM\nmodels are capable of learning motion dependencies and are able to improve the\nrecognition accuracy when more frames in a sequence are available. We evaluate\nour approach on the Washington RGBD Object dataset and on the Washington RGBD\nScenes dataset. Our approach outperforms deep nets applied to still images and\nsets a new state-of-the-art in this domain. \n\n"}
{"id": "1509.01788", "contents": "Title: Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM)\n  for unsupervised RGB-D image segmentation Abstract: Recent advances in depth imaging sensors provide easy access to the\nsynchronized depth with color, called RGB-D image. In this paper, we propose an\nunsupervised method for indoor RGB-D image segmentation and analysis. We\nconsider a statistical image generation model based on the color and geometry\nof the scene. Our method consists of a joint color-spatial-directional\nclustering method followed by a statistical planar region merging method. We\nevaluate our method on the NYU depth database and compare it with existing\nunsupervised RGB-D segmentation methods. Results show that, it is comparable\nwith the state of the art methods and it needs less computation time. Moreover,\nit opens interesting perspectives to fuse color and geometry in an unsupervised\nmanner. \n\n"}
{"id": "1509.06825", "contents": "Title: Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700\n  Robot Hours Abstract: Current learning-based robot grasping approaches exploit human-labeled\ndatasets for training the models. However, there are two problems with such a\nmethodology: (a) since each object can be grasped in multiple ways, manually\nlabeling grasp locations is not a trivial task; (b) human labeling is biased by\nsemantics. While there have been attempts to train robots using trial-and-error\nexperiments, the amount of data used in such experiments remains substantially\nlow and hence makes the learner prone to over-fitting. In this paper, we take\nthe leap of increasing the available training data to 40 times more than prior\nwork, leading to a dataset size of 50K data points collected over 700 hours of\nrobot grasping attempts. This allows us to train a Convolutional Neural Network\n(CNN) for the task of predicting grasp locations without severe overfitting. In\nour formulation, we recast the regression problem to an 18-way binary\nclassification over image patches. We also present a multi-stage learning\napproach where a CNN trained in one stage is used to collect hard negatives in\nsubsequent stages. Our experiments clearly show the benefit of using\nlarge-scale datasets (and multi-stage training) for the task of grasping. We\nalso compare to several baselines and show state-of-the-art performance on\ngeneralization to unseen objects for grasping. \n\n"}
{"id": "1509.08067", "contents": "Title: Online Object Tracking, Learning and Parsing with And-Or Graphs Abstract: This paper presents a method, called AOGTracker, for simultaneously tracking,\nlearning and parsing (TLP) of unknown objects in video sequences with a\nhierarchical and compositional And-Or graph (AOG) representation. %The AOG\ncaptures both structural and appearance variations of a target object in a\nprincipled way. The TLP method is formulated in the Bayesian framework with a\nspatial and a temporal dynamic programming (DP) algorithms inferring object\nbounding boxes on-the-fly. During online learning, the AOG is discriminatively\nlearned using latent SVM to account for appearance (e.g., lighting and partial\nocclusion) and structural (e.g., different poses and viewpoints) variations of\na tracked object, as well as distractors (e.g., similar objects) in background.\nThree key issues in online inference and learning are addressed: (i)\nmaintaining purity of positive and negative examples collected online, (ii)\ncontroling model complexity in latent structure learning, and (iii) identifying\ncritical moments to re-learn the structure of AOG based on its intrackability.\nThe intrackability measures uncertainty of an AOG based on its score maps in a\nframe. In experiments, our AOGTracker is tested on two popular tracking\nbenchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks,\nand the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery\ntracking). In the former, our AOGTracker outperforms state-of-the-art tracking\nalgorithms including two trackers based on deep convolutional network. In the\nlatter, our AOGTracker outperforms all other trackers in VOT2013 and is\ncomparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015. \n\n"}
{"id": "1510.03743", "contents": "Title: Wide-Area Image Geolocalization with Aerial Reference Imagery Abstract: We propose to use deep convolutional neural networks to address the problem\nof cross-view image geolocalization, in which the geolocation of a ground-level\nquery image is estimated by matching to georeferenced aerial images. We use\nstate-of-the-art feature representations for ground-level images and introduce\na cross-view training approach for learning a joint semantic feature\nrepresentation for aerial images. We also propose a network architecture that\nfuses features extracted from aerial images at multiple spatial scales. To\nsupport training these networks, we introduce a massive database that contains\npairs of aerial and ground-level images from across the United States. Our\nmethods significantly out-perform the state of the art on two benchmark\ndatasets. We also show, qualitatively, that the proposed feature\nrepresentations are discriminative at both local and continental spatial\nscales. \n\n"}
{"id": "1510.03979", "contents": "Title: Better Exploiting OS-CNNs for Better Event Recognition in Images Abstract: Event recognition from still images is one of the most important problems for\nimage understanding. However, compared with object recognition and scene\nrecognition, event recognition has received much less research attention in\ncomputer vision community. This paper addresses the problem of cultural event\nrecognition in still images and focuses on applying deep learning methods on\nthis problem. In particular, we utilize the successful architecture of\nObject-Scene Convolutional Neural Networks (OS-CNNs) to perform event\nrecognition. OS-CNNs are composed of object nets and scene nets, which transfer\nthe learned representations from the pre-trained models on large-scale object\nand scene recognition datasets, respectively. We propose four types of\nscenarios to explore OS-CNNs for event recognition by treating them as either\n\"end-to-end event predictors\" or \"generic feature extractors\". Our experimental\nresults demonstrate that the global and local representations of OS-CNNs are\ncomplementary to each other. Finally, based on our investigation of OS-CNNs, we\ncome up with a solution for the cultural event recognition track at the ICCV\nChaLearn Looking at People (LAP) challenge 2015. Our team secures the third\nplace at this challenge and our result is very close to the best performance. \n\n"}
{"id": "1511.00438", "contents": "Title: Semantic Summarization of Egocentric Photo Stream Events Abstract: With the rapid increase of users of wearable cameras in recent years and of\nthe amount of data they produce, there is a strong need for automatic retrieval\nand summarization techniques. This work addresses the problem of automatically\nsummarizing egocentric photo streams captured through a wearable camera by\ntaking an image retrieval perspective. After removing non-informative images by\na new CNN-based filter, images are ranked by relevance to ensure semantic\ndiversity and finally re-ranked by a novelty criterion to reduce redundancy. To\nassess the results, a new evaluation metric is proposed which takes into\naccount the non-uniqueness of the solution. Experimental results applied on a\ndatabase of 7,110 images from 6 different subjects and evaluated by experts\ngave 95.74% of experts satisfaction and a Mean Opinion Score of 4.57 out of\n5.0. Source code is available at\nhttps://github.com/imatge-upc/egocentric-2017-lta \n\n"}
{"id": "1511.02251", "contents": "Title: Learning Visual Features from Large Weakly Supervised Data Abstract: Convolutional networks trained on large supervised dataset produce visual\nfeatures which form the basis for the state-of-the-art in many computer-vision\nproblems. Further improvements of these visual features will likely require\neven larger manually labeled data sets, which severely limits the pace at which\nprogress can be made. In this paper, we explore the potential of leveraging\nmassive, weakly-labeled image collections for learning good visual features. We\ntrain convolutional networks on a dataset of 100 million Flickr photos and\ncaptions, and show that these networks produce features that perform well in a\nrange of vision problems. We also show that the networks appropriately capture\nword similarity, and learn correspondences between different languages. \n\n"}
{"id": "1511.03055", "contents": "Title: Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing Abstract: A typical image retrieval pipeline starts with the comparison of global\ndescriptors from a large database to find a short list of candidate matches. A\ngood image descriptor is key to the retrieval pipeline and should reconcile two\ncontradictory requirements: providing recall rates as high as possible and\nbeing as compact as possible for fast matching. Following the recent successes\nof Deep Convolutional Neural Networks (DCNN) for large scale image\nclassification, descriptors extracted from DCNNs are increasingly used in place\nof the traditional hand crafted descriptors such as Fisher Vectors (FV) with\nbetter retrieval performances. Nevertheless, the dimensionality of a typical\nDCNN descriptor --extracted either from the visual feature pyramid or the\nfully-connected layers-- remains quite high at several thousands of scalar\nvalues. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully\nunsupervised method to compute extremely compact binary hashes --in the 32-256\nbits range-- from high-dimensional global descriptors. UTH consists of two\nsuccessive deep learning steps. First, Stacked Restricted Boltzmann Machines\n(SRBM), a type of unsupervised deep neural nets, are used to learn binary\nembedding functions able to bring the descriptor size down to the desired\nbitrate. SRBMs are typically able to ensure a very high compression rate at the\nexpense of loosing some desirable metric properties of the original DCNN\ndescriptor space. Then, triplet networks, a rank learning scheme based on\nweight sharing nets is used to fine-tune the binary embedding functions to\nretain as much as possible of the useful metric properties of the original\nspace. A thorough empirical evaluation conducted on multiple publicly available\ndataset using DCNN descriptors shows that our method is able to significantly\noutperform state-of-the-art unsupervised schemes in the target bit range. \n\n"}
{"id": "1511.03244", "contents": "Title: TemplateNet for Depth-Based Object Instance Recognition Abstract: We present a novel deep architecture termed templateNet for depth based\nobject instance recognition. Using an intermediate template layer we exploit\nprior knowledge of an object's shape to sparsify the feature maps. This has\nthree advantages: (i) the network is better regularised resulting in structured\nfilters; (ii) the sparse feature maps results in intuitive features been learnt\nwhich can be visualized as the output of the template layer and (iii) the\nresulting network achieves state-of-the-art performance. The network benefits\nfrom this without any additional parametrization from the template layer. We\nderive the weight updates needed to efficiently train this network in an\nend-to-end manner. We benchmark the templateNet for depth based object instance\nrecognition using two publicly available datasets. The datasets present\nmultiple challenges of clutter, large pose variations and similar looking\ndistractors. Through our experiments we show that with the addition of a\ntemplate layer, a depth based CNN is able to outperform existing\nstate-of-the-art methods in the field. \n\n"}
{"id": "1511.04119", "contents": "Title: Action Recognition using Visual Attention Abstract: We propose a soft attention based model for the task of action recognition in\nvideos. We use multi-layered Recurrent Neural Networks (RNNs) with Long\nShort-Term Memory (LSTM) units which are deep both spatially and temporally.\nOur model learns to focus selectively on parts of the video frames and\nclassifies videos after taking a few glimpses. The model essentially learns\nwhich parts in the frames are relevant for the task at hand and attaches higher\nimportance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51\nand Hollywood2 datasets and analyze how the model focuses its attention\ndepending on the scene and the action being performed. \n\n"}
{"id": "1511.04587", "contents": "Title: Accurate Image Super-Resolution Using Very Deep Convolutional Networks Abstract: We present a highly accurate single-image super-resolution (SR) method. Our\nmethod uses a very deep convolutional network inspired by VGG-net used for\nImageNet classification \\cite{simonyan2015very}. We find increasing our network\ndepth shows a significant improvement in accuracy. Our final model uses 20\nweight layers. By cascading small filters many times in a deep network\nstructure, contextual information over large image regions is exploited in an\nefficient way. With very deep networks, however, convergence speed becomes a\ncritical issue during training. We propose a simple yet effective training\nprocedure. We learn residuals only and use extremely high learning rates\n($10^4$ times higher than SRCNN \\cite{dong2015image}) enabled by adjustable\ngradient clipping. Our proposed method performs better than existing methods in\naccuracy and visual improvements in our results are easily noticeable. \n\n"}
{"id": "1511.05616", "contents": "Title: Learning Structured Inference Neural Networks with Label Relations Abstract: Images of scenes have various objects as well as abundant attributes, and\ndiverse levels of visual categorization are possible. A natural image could be\nassigned with fine-grained labels that describe major components,\ncoarse-grained labels that depict high level abstraction or a set of labels\nthat reveal attributes. Such categorization at different concept layers can be\nmodeled with label graphs encoding label information. In this paper, we exploit\nthis rich information with a state-of-art deep learning framework, and propose\na generic structured model that leverages diverse label relations to improve\nimage classification performance. Our approach employs a novel stacked label\nprediction neural network, capturing both inter-level and intra-level label\nsemantics. We evaluate our method on benchmark image datasets, and empirical\nresults illustrate the efficacy of our model. \n\n"}
{"id": "1511.06085", "contents": "Title: Variable Rate Image Compression with Recurrent Neural Networks Abstract: A large fraction of Internet traffic is now driven by requests from mobile\ndevices with relatively small screens and often stringent bandwidth\nrequirements. Due to these factors, it has become the norm for modern\ngraphics-heavy websites to transmit low-resolution, low-bytecount image\npreviews (thumbnails) as part of the initial page load process to improve\napparent page responsiveness. Increasing thumbnail compression beyond the\ncapabilities of existing codecs is therefore a current research focus, as any\nbyte savings will significantly enhance the experience of mobile device users.\nToward this end, we propose a general framework for variable-rate image\ncompression and a novel architecture based on convolutional and deconvolutional\nLSTM recurrent networks. Our models address the main issues that have prevented\nautoencoder neural networks from competing with existing image compression\nalgorithms: (1) our networks only need to be trained once (not per-image),\nregardless of input image dimensions and the desired compression rate; (2) our\nnetworks are progressive, meaning that the more bits are sent, the more\naccurate the image reconstruction; and (3) the proposed architecture is at\nleast as efficient as a standard purpose-trained autoencoder for a given number\nof bits. On a large-scale benchmark of 32$\\times$32 thumbnails, our LSTM-based\napproaches provide better visual quality than (headerless) JPEG, JPEG2000 and\nWebP, with a storage size that is reduced by 10% or more. \n\n"}
{"id": "1511.06233", "contents": "Title: Towards Open Set Deep Networks Abstract: Deep networks have produced significant gains for various visual recognition\nproblems, leading to high impact academic and commercial applications. Recent\nwork in deep networks highlighted that it is easy to generate images that\nhumans would never classify as a particular object class, yet networks classify\nsuch images high confidence as that given class - deep network are easily\nfooled with images humans do not consider meaningful. The closed set nature of\ndeep networks forces them to choose from one of the known classes leading to\nsuch artifacts. Recognition in the real world is open set, i.e. the recognition\nsystem should reject unknown/unseen classes at test time. We present a\nmethodology to adapt deep networks for open set recognition, by introducing a\nnew model layer, OpenMax, which estimates the probability of an input being\nfrom an unknown class. A key element of estimating the unknown probability is\nadapting Meta-Recognition concepts to the activation patterns in the\npenultimate layer of the network. OpenMax allows rejection of \"fooling\" and\nunrelated open set images presented to the system; OpenMax greatly reduces the\nnumber of obvious errors made by a deep network. We prove that the OpenMax\nconcept provides bounded open space risk, thereby formally providing an open\nset recognition solution. We evaluate the resulting open set deep networks\nusing pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation\ndata, and thousands of fooling and open set images. The proposed OpenMax model\nsignificantly outperforms open set recognition accuracy of basic deep networks\nas well as deep networks with thresholding of SoftMax probabilities. \n\n"}
{"id": "1511.06361", "contents": "Title: Order-Embeddings of Images and Language Abstract: Hypernymy, textual entailment, and image captioning can be seen as special\ncases of a single visual-semantic hierarchy over words, sentences, and images.\nIn this paper we advocate for explicitly modeling the partial order structure\nof this hierarchy. Towards this goal, we introduce a general method for\nlearning ordered representations, and show how it can be applied to a variety\nof tasks involving images and language. We show that the resulting\nrepresentations improve performance over current approaches for hypernym\nprediction and image-caption retrieval. \n\n"}
{"id": "1511.06448", "contents": "Title: Learning Representations from EEG with Deep Recurrent-Convolutional\n  Neural Networks Abstract: One of the challenges in modeling cognitive events from electroencephalogram\n(EEG) data is finding representations that are invariant to inter- and\nintra-subject differences, as well as to inherent noise associated with such\ndata. Herein, we propose a novel approach for learning such representations\nfrom multi-channel EEG time-series, and demonstrate its advantages in the\ncontext of mental load classification task. First, we transform EEG activities\ninto a sequence of topology-preserving multi-spectral images, as opposed to\nstandard EEG analysis techniques that ignore such spatial information. Next, we\ntrain a deep recurrent-convolutional network inspired by state-of-the-art video\nclassification to learn robust representations from the sequence of images. The\nproposed approach is designed to preserve the spatial, spectral, and temporal\nstructure of EEG which leads to finding features that are less sensitive to\nvariations and distortions within each dimension. Empirical evaluation on the\ncognitive load classification task demonstrated significant improvements in\nclassification accuracy over current state-of-the-art approaches in this field. \n\n"}
{"id": "1511.06489", "contents": "Title: A Simple Hierarchical Pooling Data Structure for Loop Closure Abstract: We propose a data structure obtained by hierarchically averaging bag-of-word\ndescriptors during a sequence of views that achieves average speedups in\nlarge-scale loop closure applications ranging from 4 to 20 times on benchmark\ndatasets. Although simple, the method works as well as sophisticated\nagglomerative schemes at a fraction of the cost with minimal loss of\nperformance. \n\n"}
{"id": "1511.06951", "contents": "Title: Gradual DropIn of Layers to Train Very Deep Neural Networks Abstract: We introduce the concept of dynamically growing a neural network during\ntraining. In particular, an untrainable deep network starts as a trainable\nshallow network and newly added layers are slowly, organically added during\ntraining, thereby increasing the network's depth. This is accomplished by a new\nlayer, which we call DropIn. The DropIn layer starts by passing the output from\na previous layer (effectively skipping over the newly added layers), then\nincreasingly including units from the new layers for both feedforward and\nbackpropagation. We show that deep networks, which are untrainable with\nconventional methods, will converge with DropIn layers interspersed in the\narchitecture. In addition, we demonstrate that DropIn provides regularization\nduring training in an analogous way as dropout. Experiments are described with\nthe MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset\nwith its architecture expanded from 3 to 11 layers, and on the ImageNet dataset\nwith the AlexNet architecture expanded to 13 layers and the VGG 16-layer\narchitecture. \n\n"}
{"id": "1511.07497", "contents": "Title: Constrained Structured Regression with Convolutional Neural Networks Abstract: Convolutional Neural Networks (CNNs) have recently emerged as the dominant\nmodel in computer vision. If provided with enough training data, they predict\nalmost any visual quantity. In a discrete setting, such as classification, CNNs\nare not only able to predict a label but often predict a confidence in the form\nof a probability distribution over the output space. In continuous regression\ntasks, such a probability estimate is often lacking. We present a regression\nframework which models the output distribution of neural networks. This output\ndistribution allows us to infer the most likely labeling following a set of\nphysical or modeling constraints. These constraints capture the intricate\ninterplay between different input and output variables, and complement the\noutput of a CNN. However, they may not hold everywhere. Our setup further\nallows to learn a confidence with which a constraint holds, in the form of a\ndistribution of the constrain satisfaction. We evaluate our approach on the\nproblem of intrinsic image decomposition, and show that constrained structured\nregression significantly increases the state-of-the-art. \n\n"}
{"id": "1511.07710", "contents": "Title: Searching for Objects using Structure in Indoor Scenes Abstract: To identify the location of objects of a particular class, a passive computer\nvision system generally processes all the regions in an image to finally output\nfew regions. However, we can use structure in the scene to search for objects\nwithout processing the entire image. We propose a search technique that\nsequentially processes image regions such that the regions that are more likely\nto correspond to the query class object are explored earlier. We frame the\nproblem as a Markov decision process and use an imitation learning algorithm to\nlearn a search strategy. Since structure in the scene is essential for search,\nwe work with indoor scene images as they contain both unary scene context\ninformation and object-object context in the scene. We perform experiments on\nthe NYU-depth v2 dataset and show that the unary scene context features alone\ncan achieve a significantly high average precision while processing only\n20-25\\% of the regions for classes like bed and sofa. By considering\nobject-object context along with the scene context features, the performance is\nfurther improved for classes like counter, lamp, pillow and sofa. \n\n"}
{"id": "1511.08177", "contents": "Title: Exploring Person Context and Local Scene Context for Object Detection Abstract: In this paper we explore two ways of using context for object detection. The\nfirst model focusses on people and the objects they commonly interact with,\nsuch as fashion and sports accessories. The second model considers more general\nobject detection and uses the spatial relationships between objects and between\nobjects and scenes. Our models are able to capture precise spatial\nrelationships between the context and the object of interest, and make\neffective use of the appearance of the contextual region. On the newly released\nCOCO dataset, our models provide relative improvements of up to 5% over\nCNN-based state-of-the-art detectors, with the gains concentrated on hard cases\nsuch as small objects (10% relative improvement). \n\n"}
{"id": "1512.00486", "contents": "Title: Loss Functions for Top-k Error: Analysis and Insights Abstract: In order to push the performance on realistic computer vision tasks, the\nnumber of classes in modern benchmark datasets has significantly increased in\nrecent years. This increase in the number of classes comes along with increased\nambiguity between the class labels, raising the question if top-1 error is the\nright performance measure. In this paper, we provide an extensive comparison\nand evaluation of established multiclass methods comparing their top-k\nperformance both from a practical as well as from a theoretical perspective.\nMoreover, we introduce novel top-k loss functions as modifications of the\nsoftmax and the multiclass SVM losses and provide efficient optimization\nschemes for them. In the experiments, we compare on various datasets all of the\nproposed and established methods for top-k error optimization. An interesting\ninsight of this paper is that the softmax loss yields competitive top-k\nperformance for all k simultaneously. For a specific top-k error, our new top-k\nlosses lead typically to further improvements while being faster to train than\nthe softmax. \n\n"}
{"id": "1512.00596", "contents": "Title: The MegaFace Benchmark: 1 Million Faces for Recognition at Scale Abstract: Recent face recognition experiments on a major benchmark LFW show stunning\nperformance--a number of algorithms achieve near to perfect score, surpassing\nhuman recognition rates. In this paper, we advocate evaluations at the million\nscale (LFW includes only 13K photos of 5K people). To this end, we have\nassembled the MegaFace dataset and created the first MegaFace challenge. Our\ndataset includes One Million photos that capture more than 690K different\nindividuals. The challenge evaluates performance of algorithms with increasing\nnumbers of distractors (going from 10 to 1M) in the gallery set. We present\nboth identification and verification performance, evaluate performance with\nrespect to pose and a person's age, and compare as a function of training data\nsize (number of photos and people). We report results of state of the art and\nbaseline algorithms. Our key observations are that testing at the million scale\nreveals big performance differences (of algorithms that perform similarly well\non smaller scale) and that age invariant recognition as well as pose are still\nchallenging for most. The MegaFace dataset, baseline code, and evaluation\nscripts, are all publicly released for further experimentations at:\nmegaface.cs.washington.edu. \n\n"}
{"id": "1512.00818", "contents": "Title: Zero-Shot Event Detection by Multimodal Distributional Semantic\n  Embedding of Videos Abstract: We propose a new zero-shot Event Detection method by Multi-modal\nDistributional Semantic embedding of videos. Our model embeds object and action\nconcepts as well as other available modalities from videos into a\ndistributional semantic space. To our knowledge, this is the first Zero-Shot\nevent detection model that is built on top of distributional semantics and\nextends it in the following directions: (a) semantic embedding of multimodal\ninformation in videos (with focus on the visual modalities), (b) automatically\ndetermining relevance of concepts/attributes to a free text query, which could\nbe useful for other applications, and (c) retrieving videos by free text event\nquery (e.g., \"changing a vehicle tire\") based on their content. We embed videos\ninto a distributional semantic space and then measure the similarity between\nvideos and the event query in a free text form. We validated our method on the\nlarge TRECVID MED (Multimedia Event Detection) challenge. Using only the event\ntitle as a query, our method outperformed the state-of-the-art that uses big\ndescriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC\nmetric. It is also an order of magnitude faster. \n\n"}
{"id": "1512.01289", "contents": "Title: Predicting and visualizing psychological attributions with a deep neural\n  network Abstract: Judgments about personality based on facial appearance are strong effectors\nin social decision making, and are known to have impact on areas from\npresidential elections to jury decisions. Recent work has shown that it is\npossible to predict perception of memorability, trustworthiness, intelligence\nand other attributes in human face images. The most successful of these\napproaches require face images expertly annotated with key facial landmarks. We\ndemonstrate a Convolutional Neural Network (CNN) model that is able to perform\nthe same task without the need for landmark features, thereby greatly\nincreasing efficiency. The model has high accuracy, surpassing human-level\nperformance in some cases. Furthermore, we use a deconvolutional approach to\nvisualize important features for perception of 22 attributes and demonstrate a\nnew method for separately visualizing positive and negative features. \n\n"}
{"id": "1512.02167", "contents": "Title: Simple Baseline for Visual Question Answering Abstract: We describe a very simple bag-of-words baseline for visual question\nanswering. This baseline concatenates the word features from the question and\nCNN features from the image to predict the answer. When evaluated on the\nchallenging VQA dataset [2], it shows comparable performance to many recent\napproaches using recurrent neural networks. To explore the strength and\nweakness of the trained model, we also provide an interactive web demo and\nopen-source code. . \n\n"}
{"id": "1512.02188", "contents": "Title: Pseudo-Bayesian Robust PCA: Algorithms and Analyses Abstract: Commonly used in computer vision and other applications, robust PCA\nrepresents an algorithmic attempt to reduce the sensitivity of classical PCA to\noutliers. The basic idea is to learn a decomposition of some data matrix of\ninterest into low rank and sparse components, the latter representing unwanted\noutliers. Although the resulting optimization problem is typically NP-hard,\nconvex relaxations provide a computationally-expedient alternative with\ntheoretical support. However, in practical regimes performance guarantees break\ndown and a variety of non-convex alternatives, including Bayesian-inspired\nmodels, have been proposed to boost estimation quality. Unfortunately though,\nwithout additional a priori knowledge none of these methods can significantly\nexpand the critical operational range such that exact principal subspace\nrecovery is possible. Into this mix we propose a novel pseudo-Bayesian\nalgorithm that explicitly compensates for design weaknesses in many existing\nnon-convex approaches leading to state-of-the-art performance with a sound\nanalytical foundation. Surprisingly, our algorithm can even outperform convex\nmatrix completion despite the fact that the latter is provided with perfect\nknowledge of which entries are not corrupted. \n\n"}
{"id": "1512.02736", "contents": "Title: Window-Object Relationship Guided Representation Learning for Generic\n  Object Detections Abstract: In existing works that learn representation for object detection, the\nrelationship between a candidate window and the ground truth bounding box of an\nobject is simplified by thresholding their overlap. This paper shows\ninformation loss in this simplification and picks up the relative location/size\ninformation discarded by thresholding. We propose a representation learning\npipeline to use the relationship as supervision for improving the learned\nrepresentation in object detection. Such relationship is not limited to object\nof the target category, but also includes surrounding objects of other\ncategories. We show that image regions with multiple contexts and multiple\nrotations are effective in capturing such relationship during the\nrepresentation learning process and in handling the semantic and visual\nvariation caused by different window-object configurations. Experimental\nresults show that the representation learned by our approach can improve the\nobject detection accuracy by 6.4% in mean average precision (mAP) on\nILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved\nby our single model and it is the best among published results. On PASCAL VOC,\nit outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute\nmAP. \n\n"}
{"id": "1512.03385", "contents": "Title: Deep Residual Learning for Image Recognition Abstract: Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation. \n\n"}
{"id": "1512.04785", "contents": "Title: On Deep Representation Learning from Noisy Web Images Abstract: The keep-growing content of Web images may be the next important data source\nto scale up deep neural networks, which recently obtained a great success in\nthe ImageNet classification challenge and related tasks. This prospect,\nhowever, has not been validated on convolutional networks (convnet) -- one of\nbest performing deep models -- because of their supervised regime. While\nunsupervised alternatives are not so good as convnet in generalizing the\nlearned model to new domains, we use convnet to leverage semi-supervised\nrepresentation learning. Our approach is to use massive amounts of unlabeled\nand noisy Web images to train convnets as general feature detectors despite\nchallenges coming from data such as high level of mislabeled data, outliers,\nand data biases. Extensive experiments are conducted at several data scales,\ndifferent network architectures, and data reranking techniques. The learned\nrepresentations are evaluated on nine public datasets of various topics. The\nbest results obtained by our convnets, trained on 3.14 million Web images,\noutperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is\nclosing the gap with VGG-16. These prominent results suggest a budget solution\nto use deep learning in practice and motivate more research in semi-supervised\nrepresentation learning. \n\n"}
{"id": "1512.08086", "contents": "Title: Part-Stacked CNN for Fine-Grained Visual Categorization Abstract: In the context of fine-grained visual categorization, the ability to\ninterpret models as human-understandable visual manuals is sometimes as\nimportant as achieving high classification accuracy. In this paper, we propose\na novel Part-Stacked CNN architecture that explicitly explains the fine-grained\nrecognition process by modeling subtle differences from object parts. Based on\nmanually-labeled strong part annotations, the proposed architecture consists of\na fully convolutional network to locate multiple object parts and a two-stream\nclassification network that en- codes object-level and part-level cues\nsimultaneously. By adopting a set of sharing strategies between the computation\nof multiple object parts, the proposed architecture is very efficient running\nat 20 frames/sec during inference. Experimental results on the CUB-200-2011\ndataset reveal the effectiveness of the proposed architecture, from both the\nperspective of classification accuracy and model interpretability. \n\n"}
{"id": "1601.00022", "contents": "Title: Event Specific Multimodal Pattern Mining with Image-Caption Pairs Abstract: In this paper we describe a novel framework and algorithms for discovering\nimage patch patterns from a large corpus of weakly supervised image-caption\npairs generated from news events. Current pattern mining techniques attempt to\nfind patterns that are representative and discriminative, we stipulate that our\ndiscovered patterns must also be recognizable by humans and preferably with\nmeaningful names. We propose a new multimodal pattern mining approach that\nleverages the descriptive captions often accompanying news images to learn\nsemantically meaningful image patch patterns. The mutltimodal patterns are then\nnamed using words mined from the associated image captions for each pattern. A\nnovel evaluation framework is provided that demonstrates our patterns are 26.2%\nmore semantically meaningful than those discovered by the state of the art\nvision only pipeline, and that we can provide tags for the discovered images\npatches with 54.5% accuracy with no direct supervision. Our methods also\ndiscover named patterns beyond those covered by the existing image datasets\nlike ImageNet. To the best of our knowledge this is the first algorithm\ndeveloped to automatically mine image patch patterns that have strong semantic\nmeaning specific to high-level news events, and then evaluate these patterns\nbased on that criteria. \n\n"}
{"id": "1601.00414", "contents": "Title: Kernel Sparse Subspace Clustering on Symmetric Positive Definite\n  Manifolds Abstract: Sparse subspace clustering (SSC), as one of the most successful subspace\nclustering methods, has achieved notable clustering accuracy in computer vision\ntasks. However, SSC applies only to vector data in Euclidean space. As such,\nthere is still no satisfactory approach to solve subspace clustering by ${\\it\nself-expressive}$ principle for symmetric positive definite (SPD) matrices\nwhich is very useful in computer vision. In this paper, by embedding the SPD\nmatrices into a Reproducing Kernel Hilbert Space (RKHS), a kernel subspace\nclustering method is constructed on the SPD manifold through an appropriate\nLog-Euclidean kernel, termed as kernel sparse subspace clustering on the SPD\nRiemannian manifold (KSSCR). By exploiting the intrinsic Riemannian geometry\nwithin data, KSSCR can effectively characterize the geodesic distance between\nSPD matrices to uncover the underlying subspace structure. Experimental results\non two famous database demonstrate that the proposed method achieves better\nclustering results than the state-of-the-art approaches. \n\n"}
{"id": "1601.01750", "contents": "Title: Learning to Remove Multipath Distortions in Time-of-Flight Range Images\n  for a Robotic Arm Setup Abstract: Range images captured by Time-of-Flight (ToF) cameras are corrupted with\nmultipath distortions due to interaction between modulated light signals and\nscenes. The interaction is often complicated, which makes a model-based\nsolution elusive. We propose a learning-based approach for removing the\nmultipath distortions for a ToF camera in a robotic arm setup. Our approach is\nbased on deep learning. We use the robotic arm to automatically collect a large\namount of ToF range images containing various multipath distortions. The\ntraining images are automatically labeled by leveraging a high precision\nstructured light sensor available only in the training time. In the test time,\nwe apply the learned model to remove the multipath distortions. This allows our\nrobotic arm setup to enjoy the speed and compact form of the ToF camera without\ncompromising with its range measurement errors. We conduct extensive\nexperimental validations and compare the proposed method to several baseline\nalgorithms. The experiment results show that our method achieves 55% error\nreduction in range estimation and largely outperforms the baseline algorithms. \n\n"}
{"id": "1601.04798", "contents": "Title: Scale-aware Pixel-wise Object Proposal Networks Abstract: Object proposal is essential for current state-of-the-art object detection\npipelines. However, the existing proposal methods generally fail in producing\nresults with satisfying localization accuracy. The case is even worse for small\nobjects which however are quite common in practice. In this paper we propose a\nnovel Scale-aware Pixel-wise Object Proposal (SPOP) network to tackle the\nchallenges. The SPOP network can generate proposals with high recall rate and\naverage best overlap (ABO), even for small objects. In particular, in order to\nimprove the localization accuracy, a fully convolutional network is employed\nwhich predicts locations of object proposals for each pixel. The produced\nensemble of pixel-wise object proposals enhances the chance of hitting the\nobject significantly without incurring heavy extra computational cost. To solve\nthe challenge of localizing objects at small scale, two localization networks\nwhich are specialized for localizing objects with different scales are\nintroduced, following the divide-and-conquer philosophy. Location outputs of\nthese two networks are then adaptively combined to generate the final proposals\nby a large-/small-size weighting network. Extensive evaluations on PASCAL VOC\n2007 show the SPOP network is superior over the state-of-the-art models. The\nhigh-quality proposals from SPOP network also significantly improve the mean\naverage precision (mAP) of object detection with Fast-RCNN framework. Finally,\nthe SPOP network (trained on PASCAL VOC) shows great generalization performance\nwhen testing it on ILSVRC 2013 validation set. \n\n"}
{"id": "1601.05610", "contents": "Title: Reading Car License Plates Using Deep Convolutional Neural Networks and\n  LSTMs Abstract: In this work, we tackle the problem of car license plate detection and\nrecognition in natural scene images. Inspired by the success of deep neural\nnetworks (DNNs) in various vision applications, here we leverage DNNs to learn\nhigh-level features in a cascade framework, which lead to improved performance\non both detection and recognition.\n  Firstly, we train a $37$-class convolutional neural network (CNN) to detect\nall characters in an image, which results in a high recall, compared with\nconventional approaches such as training a binary text/non-text classifier.\nFalse positives are then eliminated by the second plate/non-plate CNN\nclassifier. Bounding box refinement is then carried out based on the edge\ninformation of the license plates, in order to improve the\nintersection-over-union (IoU) ratio. The proposed cascade framework extracts\nlicense plates effectively with both high recall and precision. Last, we\npropose to recognize the license characters as a {sequence labelling} problem.\nA recurrent neural network (RNN) with long short-term memory (LSTM) is trained\nto recognize the sequential features extracted from the whole license plate via\nCNNs. The main advantage of this approach is that it is segmentation free. By\nexploring context information and avoiding errors caused by segmentation, the\nRNN method performs better than a baseline method of combining segmentation and\ndeep CNN classification; and achieves state-of-the-art recognition accuracy. \n\n"}
{"id": "1601.05900", "contents": "Title: When is Clustering Perturbation Robust? Abstract: Clustering is a fundamental data mining tool that aims to divide data into\ngroups of similar items. Generally, intuition about clustering reflects the\nideal case -- exact data sets endowed with flawless dissimilarity between\nindividual instances.\n  In practice however, these cases are in the minority, and clustering\napplications are typically characterized by noisy data sets with approximate\npairwise dissimilarities. As such, the efficacy of clustering methods in\npractical applications necessitates robustness to perturbations.\n  In this paper, we perform a formal analysis of perturbation robustness,\nrevealing that the extent to which algorithms can exhibit this desirable\ncharacteristic is inherently limited, and identifying the types of structures\nthat allow popular clustering paradigms to discover meaningful clusters in\nspite of faulty data. \n\n"}
{"id": "1601.06608", "contents": "Title: An Unsupervised Method for Detection and Validation of The Optic Disc\n  and The Fovea Abstract: In this work, we have presented a novel method for detection of retinal image\nfeatures, the optic disc and the fovea, from colour fundus photographs of\ndilated eyes for Computer-aided Diagnosis(CAD) system. A saliency map based\nmethod was used to detect the optic disc followed by an unsupervised\nprobabilistic Latent Semantic Analysis for detection validation. The validation\nconcept is based on distinct vessels structures in the optic disc. By using the\nclinical information of standard location of the fovea with respect to the\noptic disc, the macula region is estimated. Accuracy of 100\\% detection is\nachieved for the optic disc and the macula on MESSIDOR and DIARETDB1 and 98.8\\%\ndetection accuracy on STARE dataset. \n\n"}
{"id": "1601.06615", "contents": "Title: A Taxonomy of Deep Convolutional Neural Nets for Computer Vision Abstract: Traditional architectures for solving computer vision problems and the degree\nof success they enjoyed have been heavily reliant on hand-crafted features.\nHowever, of late, deep learning techniques have offered a compelling\nalternative -- that of automatically learning problem-specific features. With\nthis new paradigm, every problem in computer vision is now being re-examined\nfrom a deep learning perspective. Therefore, it has become important to\nunderstand what kind of deep networks are suitable for a given problem.\nAlthough general surveys of this fast-moving paradigm (i.e. deep-networks)\nexist, a survey specific to computer vision is missing. We specifically\nconsider one form of deep networks widely used in computer vision -\nconvolutional neural networks (CNNs). We start with \"AlexNet\" as our base CNN\nand then examine the broad variations proposed over time to suit different\napplications. We hope that our recipe-style survey will serve as a guide,\nparticularly for novice practitioners intending to use deep-learning techniques\nfor computer vision. \n\n"}
{"id": "1602.01168", "contents": "Title: Learning Discriminative Features via Label Consistent Neural Network Abstract: Deep Convolutional Neural Networks (CNN) enforces supervised information only\nat the output layer, and hidden layers are trained by back propagating the\nprediction error from the output layer without explicit supervision. We propose\na supervised feature learning approach, Label Consistent Neural Network, which\nenforces direct supervision in late hidden layers. We associate each neuron in\na hidden layer with a particular class label and encourage it to be activated\nfor input signals from the same class. More specifically, we introduce a label\nconsistency regularization called \"discriminative representation error\" loss\nfor late hidden layers and combine it with classification error loss to build\nour overall objective function. This label consistency constraint alleviates\nthe common problem of gradient vanishing and tends to faster convergence; it\nalso makes the features derived from late hidden layers discriminative enough\nfor classification even using a simple $k$-NN classifier, since input signals\nfrom the same class will have very similar representations. Experimental\nresults demonstrate that our approach achieves state-of-the-art performances on\nseveral public benchmarks for action and object category recognition. \n\n"}
{"id": "1602.01528", "contents": "Title: EIE: Efficient Inference Engine on Compressed Deep Neural Network Abstract: State-of-the-art deep neural networks (DNNs) have hundreds of millions of\nconnections and are both computationally and memory intensive, making them\ndifficult to deploy on embedded systems with limited hardware resources and\npower budgets. While custom hardware helps the computation, fetching weights\nfrom DRAM is two orders of magnitude more expensive than ALU operations, and\ndominates the required power.\n  Previously proposed 'Deep Compression' makes it possible to fit large DNNs\n(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by\npruning the redundant connections and having multiple connections share the\nsame weight. We propose an energy efficient inference engine (EIE) that\nperforms inference on this compressed network model and accelerates the\nresulting sparse matrix-vector multiplication with weight sharing. Going from\nDRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;\nWeight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.\nEvaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to\nCPU and GPU implementations of the same DNN without compression. EIE has a\nprocessing power of 102GOPS/s working directly on a compressed network,\ncorresponding to 3TOPS/s on an uncompressed network, and processes FC layers of\nAlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is\n24,000x and 3,400x more energy efficient than a CPU and GPU respectively.\nCompared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy\nefficiency and area efficiency. \n\n"}
{"id": "1602.02410", "contents": "Title: Exploring the Limits of Language Modeling Abstract: In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon. \n\n"}
{"id": "1602.03468", "contents": "Title: Articulated Clinician Detection Using 3D Pictorial Structures on RGB-D\n  Data Abstract: Reliable human pose estimation (HPE) is essential to many clinical\napplications, such as surgical workflow analysis, radiation safety monitoring\nand human-robot cooperation. Proposed methods for the operating room (OR) rely\neither on foreground estimation using a multi-camera system, which is a\nchallenge in real ORs due to color similarities and frequent illumination\nchanges, or on wearable sensors or markers, which are invasive and therefore\ndifficult to introduce in the room. Instead, we propose a novel approach based\non Pictorial Structures (PS) and on RGB-D data, which can be easily deployed in\nreal ORs. We extend the PS framework in two ways. First, we build robust and\ndiscriminative part detectors using both color and depth images. We also\npresent a novel descriptor for depth images, called histogram of depth\ndifferences (HDD). Second, we extend PS to 3D by proposing 3D pairwise\nconstraints and a new method that makes exact inference tractable. Our approach\nis evaluated for pose estimation and clinician detection on a challenging RGB-D\ndataset recorded in a busy operating room during live surgeries. We conduct\nseries of experiments to study the different part detectors in conjunction with\nthe various 2D or 3D pairwise constraints. Our comparisons demonstrate that 3D\nPS with RGB-D part detectors significantly improves the results in a visually\nchallenging operating environment. \n\n"}
{"id": "1602.08465", "contents": "Title: Seq-NMS for Video Object Detection Abstract: Video object detection is challenging because objects that are easily\ndetected in one frame may be difficult to detect in another frame within the\nsame clip. Recently, there have been major advances for doing object detection\nin a single image. These methods typically contain three phases: (i) object\nproposal generation (ii) object classification and (iii) post-processing. We\npropose a modification of the post-processing phase that uses high-scoring\nobject detections from nearby frames to boost scores of weaker detections\nwithin the same clip. We show that our method obtains superior results to\nstate-of-the-art single image object detection techniques. Our method placed\n3rd in the video object detection (VID) task of the ImageNet Large Scale Visual\nRecognition Challenge 2015 (ILSVRC2015). \n\n"}
{"id": "1602.08977", "contents": "Title: Clustering Based Feature Learning on Variable Stars Abstract: The success of automatic classification of variable stars strongly depends on\nthe lightcurve representation. Usually, lightcurves are represented as a vector\nof many statistical descriptors designed by astronomers called features. These\ndescriptors commonly demand significant computational power to calculate,\nrequire substantial research effort to develop and do not guarantee good\nperformance on the final classification task. Today, lightcurve representation\nis not entirely automatic; algorithms that extract lightcurve features are\ndesigned by humans and must be manually tuned up for every survey. The vast\namounts of data that will be generated in future surveys like LSST mean\nastronomers must develop analysis pipelines that are both scalable and\nautomated. Recently, substantial efforts have been made in the machine learning\ncommunity to develop methods that prescind from expert-designed and manually\ntuned features for features that are automatically learned from data. In this\nwork we present what is, to our knowledge, the first unsupervised feature\nlearning algorithm designed for variable stars. Our method first extracts a\nlarge number of lightcurve subsequences from a given set of photometric data,\nwhich are then clustered to find common local patterns in the time series.\nRepresentatives of these patterns, called exemplars, are then used to transform\nlightcurves of a labeled set into a new representation that can then be used to\ntrain an automatic classifier. The proposed algorithm learns the features from\nboth labeled and unlabeled lightcurves, overcoming the bias generated when the\nlearning process is done only with labeled data. We test our method on MACHO\nand OGLE datasets; the results show that the classification performance we\nachieve is as good and in some cases better than the performance achieved using\ntraditional features, while the computational cost is significantly lower. \n\n"}
{"id": "1603.00124", "contents": "Title: Learning Multilayer Channel Features for Pedestrian Detection Abstract: Pedestrian detection based on the combination of Convolutional Neural Network\n(i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achieved\ngreat success. Generally, HOG+LUV are used to generate the candidate proposals\nand then CNN classifies these proposals. Despite its success, there is still\nroom for improvement. For example, CNN classifies these proposals by the\nfull-connected layer features while proposal scores and the features in the\ninner-layers of CNN are ignored. In this paper, we propose a unifying framework\ncalled Multilayer Channel Features (MCF) to overcome the drawback. It firstly\nintegrates HOG+LUV with each layer of CNN into a multi-layer image channels.\nBased on the multi-layer image channels, a multi-stage cascade AdaBoost is then\nlearned. The weak classifiers in each stage of the multi-stage cascade is\nlearned from the image channels of corresponding layer. With more abundant\nfeatures, MCF achieves the state-of-the-art on Caltech pedestrian dataset\n(i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves\n7.98% miss rate. As many non-pedestrian detection windows can be quickly\nrejected by the first few stages, it accelerates detection speed by 1.43 times.\nBy eliminating the highly overlapped detection windows with lower scores after\nthe first stage, it's 4.07 times faster with negligible performance loss. \n\n"}
{"id": "1603.01068", "contents": "Title: First Steps Toward Camera Model Identification with Convolutional Neural\n  Networks Abstract: Detecting the camera model used to shoot a picture enables to solve a wide\nseries of forensic problems, from copyright infringement to ownership\nattribution. For this reason, the forensic community has developed a set of\ncamera model identification algorithms that exploit characteristic traces left\non acquired images by the processing pipelines specific of each camera model.\nIn this paper, we investigate a novel approach to solve camera model\nidentification problem. Specifically, we propose a data-driven algorithm based\non convolutional neural networks, which learns features characterizing each\ncamera model directly from the acquired pictures. Results on a well-known\ndataset of 18 camera models show that: (i) the proposed method outperforms\nup-to-date state-of-the-art algorithms on classification of 64x64 color image\npatches; (ii) features learned by the proposed network generalize to camera\nmodels never used for training. \n\n"}
{"id": "1603.02078", "contents": "Title: A novel learning-based frame pooling method for Event Detection Abstract: Detecting complex events in a large video collection crawled from video\nwebsites is a challenging task. When applying directly good image-based feature\nrepresentation, e.g., HOG, SIFT, to videos, we have to face the problem of how\nto pool multiple frame feature representations into one feature representation.\nIn this paper, we propose a novel learning-based frame pooling method. We\nformulate the pooling weight learning as an optimization problem and thus our\nmethod can automatically learn the best pooling weight configuration for each\nspecific event category. Experimental results conducted on TRECVID MED 2011\nreveal that our method outperforms the commonly used average pooling and max\npooling strategies on both high-level and low-level 2D image features. \n\n"}
{"id": "1603.04467", "contents": "Title: TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\n  Systems Abstract: TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org. \n\n"}
{"id": "1603.06463", "contents": "Title: Controlling Explanatory Heatmap Resolution and Semantics via\n  Decomposition Depth Abstract: We present an application of the Layer-wise Relevance Propagation (LRP)\nalgorithm to state of the art deep convolutional neural networks and Fisher\nVector classifiers to compare the image perception and prediction strategies of\nboth classifiers with the use of visualized heatmaps. Layer-wise Relevance\nPropagation (LRP) is a method to compute scores for individual components of an\ninput image, denoting their contribution to the prediction of the classifier\nfor one particular test point. We demonstrate the impact of different choices\nof decomposition cut-off points during the LRP-process, controlling the\nresolution and semantics of the heatmap on test images from the PASCAL VOC 2007\ntest data set. \n\n"}
{"id": "1603.06568", "contents": "Title: Modelling Temporal Information Using Discrete Fourier Transform for\n  Recognizing Emotions in User-generated Videos Abstract: With the widespread of user-generated Internet videos, emotion recognition in\nthose videos attracts increasing research efforts. However, most existing works\nare based on framelevel visual features and/or audio features, which might fail\nto model the temporal information, e.g. characteristics accumulated along time.\nIn order to capture video temporal information, in this paper, we propose to\nanalyse features in frequency domain transformed by discrete Fourier transform\n(DFT features). Frame-level features are firstly extract by a pre-trained deep\nconvolutional neural network (CNN). Then, time domain features are transferred\nand interpolated into DFT features. CNN and DFT features are further encoded\nand fused for emotion classification. By this way, static image features\nextracted from a pre-trained deep CNN and temporal information represented by\nDFT features are jointly considered for video emotion recognition. Experimental\nresults demonstrate that combining DFT features can effectively capture\ntemporal information and therefore improve emotion recognition performance. Our\napproach has achieved a state-of-the-art performance on the largest video\nemotion dataset (VideoEmotion-8 dataset), improving accuracy from 51.1% to\n62.6%. \n\n"}
{"id": "1603.07141", "contents": "Title: BreakingNews: Article Annotation by Image and Text Processing Abstract: Building upon recent Deep Neural Network architectures, current approaches\nlying in the intersection of computer vision and natural language processing\nhave achieved unprecedented breakthroughs in tasks like automatic captioning or\nimage retrieval. Most of these learning methods, though, rely on large training\nsets of images associated with human annotations that specifically describe the\nvisual content. In this paper we propose to go a step further and explore the\nmore complex cases where textual descriptions are loosely related to the\nimages. We focus on the particular domain of News articles in which the textual\ncontent often expresses connotative and ambiguous relations that are only\nsuggested but not directly inferred from images. We introduce new deep learning\nmethods that address source detection, popularity prediction, article\nillustration and geolocation of articles. An adaptive CNN architecture is\nproposed, that shares most of the structure for all the tasks, and is suitable\nfor multitask and transfer learning. Deep Canonical Correlation Analysis is\ndeployed for article illustration, and a new loss function based on Great\nCircle Distance is proposed for geolocation. Furthermore, we present\nBreakingNews, a novel dataset with approximately 100K news articles including\nimages, text and captions, and enriched with heterogeneous meta-data (such as\nGPS coordinates and popularity metrics). We show this dataset to be appropriate\nto explore all aforementioned problems, for which we provide a baseline\nperformance using various Deep Learning architectures, and different\nrepresentations of the textual and visual features. We report very promising\nresults and bring to light several limitations of current state-of-the-art in\nthis kind of domain, which we hope will help spur progress in the field. \n\n"}
{"id": "1603.07475", "contents": "Title: Fine-scale Surface Normal Estimation using a Single NIR Image Abstract: We present surface normal estimation using a single near infrared (NIR)\nimage. We are focusing on fine-scale surface geometry captured with an\nuncalibrated light source. To tackle this ill-posed problem, we adopt a\ngenerative adversarial network which is effective in recovering a sharp output,\nwhich is also essential for fine-scale surface normal estimation. We\nincorporate angular error and integrability constraint into the objective\nfunction of the network to make estimated normals physically meaningful. We\ntrain and validate our network on a recent NIR dataset, and also evaluate the\ngenerality of our trained model by using new external datasets which are\ncaptured with a different camera under different environment. \n\n"}
{"id": "1603.08079", "contents": "Title: Do You See What I Mean? Visual Resolution of Linguistic Ambiguities Abstract: Understanding language goes hand in hand with the ability to integrate\ncomplex contextual information obtained via perception. In this work, we\npresent a novel task for grounded language understanding: disambiguating a\nsentence given a visual scene which depicts one of the possible interpretations\nof that sentence. To this end, we introduce a new multimodal corpus containing\nambiguous sentences, representing a wide range of syntactic, semantic and\ndiscourse ambiguities, coupled with videos that visualize the different\ninterpretations for each sentence. We address this task by extending a vision\nmodel which determines if a sentence is depicted by a video. We demonstrate how\nsuch a model can be adjusted to recognize different interpretations of the same\nunderlying sentence, allowing to disambiguate sentences in a unified fashion\nacross the different ambiguity types. \n\n"}
{"id": "1603.08367", "contents": "Title: Sparse Activity and Sparse Connectivity in Supervised Learning Abstract: Sparseness is a useful regularizer for learning in a wide range of\napplications, in particular in neural networks. This paper proposes a model\ntargeted at classification tasks, where sparse activity and sparse connectivity\nare used to enhance classification capabilities. The tool for achieving this is\na sparseness-enforcing projection operator which finds the closest vector with\na pre-defined sparseness for any given vector. In the theoretical part of this\npaper, a comprehensive theory for such a projection is developed. In\nconclusion, it is shown that the projection is differentiable almost everywhere\nand can thus be implemented as a smooth neuronal transfer function. The entire\nmodel can hence be tuned end-to-end using gradient-based methods. Experiments\non the MNIST database of handwritten digits show that classification\nperformance can be boosted by sparse activity or sparse connectivity. With a\ncombination of both, performance can be significantly better compared to\nclassical non-sparse approaches. \n\n"}
{"id": "1603.08390", "contents": "Title: A Generic Inverted Index Framework for Similarity Search on the GPU -\n  Technical Report Abstract: We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named $\\tau$-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source. \n\n"}
{"id": "1603.09439", "contents": "Title: The Open World of Micro-Videos Abstract: Micro-videos are six-second videos popular on social media networks with\nseveral unique properties. Firstly, because of the authoring process, they\ncontain significantly more diversity and narrative structure than existing\ncollections of video \"snippets\". Secondly, because they are often captured by\nhand-held mobile cameras, they contain specialized viewpoints including\nthird-person, egocentric, and self-facing views seldom seen in traditional\nproduced video. Thirdly, due to to their continuous production and publication\non social networks, aggregate micro-video content contains interesting\nopen-world dynamics that reflects the temporal evolution of tag topics. These\naspects make micro-videos an appealing well of visual data for developing\nlarge-scale models for video understanding. We analyze a novel dataset of\nmicro-videos labeled with 58 thousand tags. To analyze this data, we introduce\nviewpoint-specific and temporally-evolving models for video understanding,\ndefined over state-of-the-art motion and deep visual features. We conclude that\nour dataset opens up new research opportunities for large-scale video analysis,\nnovel viewpoints, and open-world dynamics. \n\n"}
{"id": "1603.09473", "contents": "Title: Learning Compatibility Across Categories for Heterogeneous Item\n  Recommendation Abstract: Identifying relationships between items is a key task of an online\nrecommender system, in order to help users discover items that are functionally\ncomplementary or visually compatible. In domains like clothing recommendation,\nthis task is particularly challenging since a successful system should be\ncapable of handling a large corpus of items, a huge amount of relationships\namong them, as well as the high-dimensional and semantically complicated\nfeatures involved. Furthermore, the human notion of \"compatibility\" to capture\ngoes beyond mere similarity: For two items to be compatible---whether jeans and\na t-shirt, or a laptop and a charger---they should be similar in some ways, but\nsystematically different in others.\n  In this paper we propose a novel method, Monomer, to learn complicated and\nheterogeneous relationships between items in product recommendation settings.\nRecently, scalable methods have been developed that address this task by\nlearning similarity metrics on top of the content of the products involved.\nHere our method relaxes the metricity assumption inherent in previous work and\nmodels multiple localized notions of 'relatedness,' so as to uncover ways in\nwhich related items should be systematically similar, and systematically\ndifferent. Quantitatively, we show that our system achieves state-of-the-art\nperformance on large-scale compatibility prediction tasks, especially in cases\nwhere there is substantial heterogeneity between related items. Qualitatively,\nwe demonstrate that richer notions of compatibility can be learned that go\nbeyond similarity, and that our model can make effective recommendations of\nheterogeneous content. \n\n"}
{"id": "1603.09742", "contents": "Title: Object Boundary Guided Semantic Segmentation Abstract: Semantic segmentation is critical to image content understanding and object\nlocalization. Recent development in fully-convolutional neural network (FCN)\nhas enabled accurate pixel-level labeling. One issue in previous works is that\nthe FCN based method does not exploit the object boundary information to\ndelineate segmentation details since the object boundary label is ignored in\nthe network training. To tackle this problem, we introduce a double branch\nfully convolutional neural network, which separates the learning of the\ndesirable semantic class labeling with mask-level object proposals guided by\nrelabeled boundaries. This network, called object boundary guided FCN\n(OBG-FCN), is able to integrate the distinct properties of object shape and\nclass features elegantly in a fully convolutional way with a designed masking\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\nand show that the end-to-end trainable OBG-FCN system offers great improvement\nin optimizing the target semantic segmentation quality. \n\n"}
{"id": "1604.00036", "contents": "Title: Modeling Visual Compatibility through Hierarchical Mid-level Elements Abstract: In this paper we present a hierarchical method to discover mid-level elements\nwith the objective of modeling visual compatibility between objects. At the\nbase-level, our method identifies patterns of CNN activations with the aim of\nmodeling different variations/styles in which objects of the classes of\ninterest may occur. At the top-level, the proposed method discovers patterns of\nco-occurring activations of base-level elements that define visual\ncompatibility between pairs of object classes. Experiments on the massive\nAmazon dataset show the strength of our method at describing object classes and\nthe characteristics that drive the compatibility between them. \n\n"}
{"id": "1604.00974", "contents": "Title: Writer-independent Feature Learning for Offline Signature Verification\n  using Deep Convolutional Neural Networks Abstract: Automatic Offline Handwritten Signature Verification has been researched over\nthe last few decades from several perspectives, using insights from graphology,\ncomputer vision, signal processing, among others. In spite of the advancements\non the field, building classifiers that can separate between genuine signatures\nand skilled forgeries (forgeries made targeting a particular signature) is\nstill hard. We propose approaching the problem from a feature learning\nperspective. Our hypothesis is that, in the absence of a good model of the data\ngeneration process, it is better to learn the features from data, instead of\nusing hand-crafted features that have no resemblance to the signature\ngeneration process. To this end, we use Deep Convolutional Neural Networks to\nlearn features in a writer-independent format, and use this model to obtain a\nfeature representation on another set of users, where we train writer-dependent\nclassifiers. We tested our method in two datasets: GPDS-960 and Brazilian\nPUC-PR. Our experimental results show that the features learned in a subset of\nthe users are discriminative for the other users, including across different\ndatasets, reaching close to the state-of-the-art in the GPDS dataset, and\nimproving the state-of-the-art in the Brazilian PUC-PR dataset. \n\n"}
{"id": "1604.01685", "contents": "Title: The Cityscapes Dataset for Semantic Urban Scene Understanding Abstract: Visual understanding of complex urban street scenes is an enabling factor for\na wide range of applications. Object detection has benefited enormously from\nlarge-scale datasets, especially in the context of deep learning. For semantic\nurban scene understanding, however, no current dataset adequately captures the\ncomplexity of real-world urban scenes.\n  To address this, we introduce Cityscapes, a benchmark suite and large-scale\ndataset to train and test approaches for pixel-level and instance-level\nsemantic labeling. Cityscapes is comprised of a large, diverse set of stereo\nvideo sequences recorded in streets from 50 different cities. 5000 of these\nimages have high quality pixel-level annotations; 20000 additional images have\ncoarse annotations to enable methods that leverage large volumes of\nweakly-labeled data. Crucially, our effort exceeds previous attempts in terms\nof dataset size, annotation richness, scene variability, and complexity. Our\naccompanying empirical study provides an in-depth analysis of the dataset\ncharacteristics, as well as a performance evaluation of several\nstate-of-the-art approaches based on our benchmark. \n\n"}
{"id": "1604.01753", "contents": "Title: Hollywood in Homes: Crowdsourcing Data Collection for Activity\n  Understanding Abstract: Computer vision has a great potential to help our daily lives by searching\nfor lost keys, watering flowers or reminding us to take a pill. To succeed with\nsuch tasks, computer vision methods need to be trained from real and diverse\nexamples of our daily dynamic scenes. While most of such scenes are not\nparticularly exciting, they typically do not appear on YouTube, in movies or TV\nbroadcasts. So how do we collect sufficiently many diverse but boring samples\nrepresenting our lives? We propose a novel Hollywood in Homes approach to\ncollect such data. Instead of shooting videos in the lab, we ensure diversity\nby distributing and crowdsourcing the whole process of video creation from\nscript writing to video recording and annotation. Following this procedure we\ncollect a new dataset, Charades, with hundreds of people recording videos in\ntheir own homes, acting out casual everyday activities. The dataset is composed\nof 9,848 annotated videos with an average length of 30 seconds, showing\nactivities of 267 people from three continents. Each video is annotated by\nmultiple free-text descriptions, action labels, action intervals and classes of\ninteracted objects. In total, Charades provides 27,847 video descriptions,\n66,500 temporally localized intervals for 157 action classes and 41,104 labels\nfor 46 object classes. Using this rich data, we evaluate and provide baseline\nresults for several tasks including action recognition and automatic\ndescription generation. We believe that the realism, diversity, and casual\nnature of this dataset will present unique challenges and new opportunities for\ncomputer vision community. \n\n"}
{"id": "1604.02647", "contents": "Title: Real-Time Facial Segmentation and Performance Capture from RGB Input Abstract: We introduce the concept of unconstrained real-time 3D facial performance\ncapture through explicit semantic segmentation in the RGB input. To ensure\nrobustness, cutting edge supervised learning approaches rely on large training\ndatasets of face images captured in the wild. While impressive tracking quality\nhas been demonstrated for faces that are largely visible, any occlusion due to\nhair, accessories, or hand-to-face gestures would result in significant visual\nartifacts and loss of tracking accuracy. The modeling of occlusions has been\nmostly avoided due to its immense space of appearance variability. To address\nthis curse of high dimensionality, we perform tracking in unconstrained images\nassuming non-face regions can be fully masked out. Along with recent\nbreakthroughs in deep learning, we demonstrate that pixel-level facial\nsegmentation is possible in real-time by repurposing convolutional neural\nnetworks designed originally for general semantic segmentation. We develop an\nefficient architecture based on a two-stream deconvolution network with\ncomplementary characteristics, and introduce carefully designed training\nsamples and data augmentation strategies for improved segmentation accuracy and\nrobustness. We adopt a state-of-the-art regression-based facial tracking\nframework with segmented face images as training, and demonstrate accurate and\nuninterrupted facial performance capture in the presence of extreme occlusion\nand even side views. Furthermore, the resulting segmentation can be directly\nused to composite partial 3D face models on the input images and enable\nseamless facial manipulation tasks, such as virtual make-up or face\nreplacement. \n\n"}
{"id": "1604.02946", "contents": "Title: Kernel-based Sensor Fusion with Application to Audio-Visual Voice\n  Activity Detection Abstract: In this paper, we address the problem of multiple view data fusion in the\npresence of noise and interferences. Recent studies have approached this\nproblem using kernel methods, by relying particularly on a product of kernels\nconstructed separately for each view. From a graph theory point of view, we\nanalyze this fusion approach in a discrete setting. More specifically, based on\na statistical model for the connectivity between data points, we propose an\nalgorithm for the selection of the kernel bandwidth, a parameter, which, as we\nshow, has important implications on the robustness of this fusion approach to\ninterferences. Then, we consider the fusion of audio-visual speech signals\nmeasured by a single microphone and by a video camera pointed to the face of\nthe speaker. Specifically, we address the task of voice activity detection,\ni.e., the detection of speech and non-speech segments, in the presence of\nstructured interferences such as keyboard taps and office noise. We propose an\nalgorithm for voice activity detection based on the audio-visual signal.\nSimulation results show that the proposed algorithm outperforms competing\nfusion and voice activity detection approaches. In addition, we demonstrate\nthat a proper selection of the kernel bandwidth indeed leads to improved\nperformance. \n\n"}
{"id": "1604.03635", "contents": "Title: Online Multi-Target Tracking Using Recurrent Neural Networks Abstract: We present a novel approach to online multi-target tracking based on\nrecurrent neural networks (RNNs). Tracking multiple objects in real-world\nscenes involves many challenges, including a) an a-priori unknown and\ntime-varying number of targets, b) a continuous state estimation of all present\ntargets, and c) a discrete combinatorial problem of data association. Most\nprevious methods involve complex models that require tedious tuning of\nparameters. Here, we propose for the first time, an end-to-end learning\napproach for online multi-target tracking. Existing deep learning methods are\nnot designed for the above challenges and cannot be trivially applied to the\ntask. Our solution addresses all of the above points in a principled way.\nExperiments on both synthetic and real data show promising results obtained at\n~300 Hz on a standard CPU, and pave the way towards future research in this\ndirection. \n\n"}
{"id": "1604.04970", "contents": "Title: Deep Aesthetic Quality Assessment with Semantic Information Abstract: Human beings often assess the aesthetic quality of an image coupled with the\nidentification of the image's semantic content. This paper addresses the\ncorrelation issue between automatic aesthetic quality assessment and semantic\nrecognition. We cast the assessment problem as the main task among a multi-task\ndeep model, and argue that semantic recognition task offers the key to address\nthis problem. Based on convolutional neural networks, we employ a single and\nsimple multi-task framework to efficiently utilize the supervision of aesthetic\nand semantic labels. A correlation item between these two tasks is further\nintroduced to the framework by incorporating the inter-task relationship\nlearning. This item not only provides some useful insight about the correlation\nbut also improves assessment accuracy of the aesthetic task. Particularly, an\neffective strategy is developed to keep a balance between the two tasks, which\nfacilitates to optimize the parameters of the framework. Extensive experiments\non the challenging AVA dataset and Photo.net dataset validate the importance of\nsemantic recognition in aesthetic quality assessment, and demonstrate that\nmulti-task deep models can discover an effective aesthetic representation to\nachieve state-of-the-art results. \n\n"}
{"id": "1604.05417", "contents": "Title: Triplet Probabilistic Embedding for Face Verification and Clustering Abstract: Despite significant progress made over the past twenty five years,\nunconstrained face verification remains a challenging problem. This paper\nproposes an approach that couples a deep CNN-based approach with a\nlow-dimensional discriminative embedding learned using triplet probability\nconstraints to solve the unconstrained face verification problem. Aside from\nyielding performance improvements, this embedding provides significant\nadvantages in terms of memory and for post-processing operations like subject\nspecific clustering. Experiments on the challenging IJB-A dataset show that the\nproposed algorithm performs comparably or better than the state of the art\nmethods in verification and identification metrics, while requiring much less\ntraining data and training time. The superior performance of the proposed\nmethod on the CFP dataset shows that the representation learned by our deep CNN\nis robust to extreme pose variation. Furthermore, we demonstrate the robustness\nof the deep features to challenges including age, pose, blur and clutter by\nperforming simple clustering experiments on both IJB-A and LFW datasets. \n\n"}
{"id": "1604.05495", "contents": "Title: Deep Saliency with Encoded Low level Distance Map and High Level\n  Features Abstract: Recent advances in saliency detection have utilized deep learning to obtain\nhigh level features to detect salient regions in a scene. These advances have\ndemonstrated superior results over previous works that utilize hand-crafted low\nlevel features for saliency detection. In this paper, we demonstrate that\nhand-crafted features can provide complementary information to enhance\nperformance of saliency detection that utilizes only high level features. Our\nmethod utilizes both high level and low level features for saliency detection\nunder a unified deep learning framework. The high level features are extracted\nusing the VGG-net, and the low level features are compared with other parts of\nan image to form a low level distance map. The low level distance map is then\nencoded using a convolutional neural network(CNN) with multiple 1X1\nconvolutional and ReLU layers. We concatenate the encoded low level distance\nmap and the high level features, and connect them to a fully connected neural\nnetwork classifier to evaluate the saliency of a query region. Our experiments\nshow that our method can further improve the performance of state-of-the-art\ndeep learning-based saliency detection methods. \n\n"}
{"id": "1604.05865", "contents": "Title: Estimating 3D Trajectories from 2D Projections via Disjunctive Factored\n  Four-Way Conditional Restricted Boltzmann Machines Abstract: Estimation, recognition, and near-future prediction of 3D trajectories based\non their two dimensional projections available from one camera source is an\nexceptionally difficult problem due to uncertainty in the trajectories and\nenvironment, high dimensionality of the specific trajectory states, lack of\nenough labeled data and so on. In this article, we propose a solution to solve\nthis problem based on a novel deep learning model dubbed Disjunctive Factored\nFour-Way Conditional Restricted Boltzmann Machine (DFFW-CRBM). Our method\nimproves state-of-the-art deep learning techniques for high dimensional\ntime-series modeling by introducing a novel tensor factorization capable of\ndriving forth order Boltzmann machines to considerably lower energy levels, at\nno computational costs. DFFW-CRBMs are capable of accurately estimating,\nrecognizing, and performing near-future prediction of three-dimensional\ntrajectories from their 2D projections while requiring limited amount of\nlabeled data. We evaluate our method on both simulated and real-world data,\nshowing its effectiveness in predicting and classifying complex ball\ntrajectories and human activities. \n\n"}
{"id": "1605.00052", "contents": "Title: InterActive: Inter-Layer Activeness Propagation Abstract: An increasing number of computer vision tasks can be tackled with deep\nfeatures, which are the intermediate outputs of a pre-trained Convolutional\nNeural Network. Despite the astonishing performance, deep features extracted\nfrom low-level neurons are still below satisfaction, arguably because they\ncannot access the spatial context contained in the higher layers. In this\npaper, we present InterActive, a novel algorithm which computes the activeness\nof neurons and network connections. Activeness is propagated through a neural\nnetwork in a top-down manner, carrying high-level context and improving the\ndescriptive power of low-level and mid-level neurons. Visualization indicates\nthat neuron activeness can be interpreted as spatial-weighted neuron responses.\nWe achieve state-of-the-art classification performance on a wide range of image\ndatasets. \n\n"}
{"id": "1605.01177", "contents": "Title: A metric on the space of finite sets of trajectories for evaluation of\n  multi-target tracking algorithms Abstract: In this paper, we propose a metric on the space of finite sets of\ntrajectories for assessing multi-target tracking algorithms in a mathematically\nsound way. The main use of the metric is to compare estimates of trajectories\nfrom different algorithms with the ground truth of trajectories. The proposed\nmetric includes intuitive costs associated to localization error for properly\ndetected targets, missed and false targets and track switches at each time\nstep. The metric computation is based on solving a multi-dimensional assignment\nproblem. We also propose a lower bound for the metric, which is also a metric\nfor sets of trajectories and is computable in polynomial time using linear\nprogramming. We also extend the proposed metrics on sets of trajectories to\nrandom finite sets of trajectories. \n\n"}
{"id": "1605.02057", "contents": "Title: Robust Bayesian Method for Simultaneous Block Sparse Signal Recovery\n  with Applications to Face Recognition Abstract: In this paper, we present a novel Bayesian approach to recover simultaneously\nblock sparse signals in the presence of outliers. The key advantage of our\nproposed method is the ability to handle non-stationary outliers, i.e. outliers\nwhich have time varying support. We validate our approach with empirical\nresults showing the superiority of the proposed method over competing\napproaches in synthetic data experiments as well as the multiple measurement\nface recognition problem. \n\n"}
{"id": "1605.03705", "contents": "Title: Movie Description Abstract: Audio Description (AD) provides linguistic descriptions of movies and allows\nvisually impaired people to follow a movie along with their peers. Such\ndescriptions are by design mainly visual and thus naturally form an interesting\ndata source for computer vision and computational linguistics. In this work we\npropose a novel dataset which contains transcribed ADs, which are temporally\naligned to full length movies. In addition we also collected and aligned movie\nscripts used in prior work and compare the two sources of descriptions. In\ntotal the Large Scale Movie Description Challenge (LSMDC) contains a parallel\ncorpus of 118,114 sentences and video clips from 202 movies. First we\ncharacterize the dataset by benchmarking different approaches for generating\nvideo descriptions. Comparing ADs to scripts, we find that ADs are indeed more\nvisual and describe precisely what is shown rather than what should happen\naccording to the scripts created prior to movie production. Furthermore, we\npresent and compare the results of several teams who participated in a\nchallenge organized in the context of the workshop \"Describing and\nUnderstanding Video & The Large Scale Movie Description Challenge (LSMDC)\", at\nICCV 2015. \n\n"}
{"id": "1605.04603", "contents": "Title: Improving the Neural Algorithm of Artistic Style Abstract: In this work we investigate different avenues of improving the Neural\nAlgorithm of Artistic Style (by Leon A. Gatys, Alexander S. Ecker and Matthias\nBethge, arXiv:1508.06576).\n  While showing great results when transferring homogeneous and repetitive\npatterns, the original style representation often fails to capture more complex\nproperties, like having separate styles of foreground and background. This\nleads to visual artifacts and undesirable textures appearing in unexpected\nregions when performing style transfer.\n  We tackle this issue with a variety of approaches, mostly by modifying the\nstyle representation in order for it to capture more information and impose a\ntighter constraint on the style transfer result.\n  In our experiments, we subjectively evaluate our best method as producing\nfrom barely noticeable to significant improvements in the quality of style\ntransfer. \n\n"}
{"id": "1605.04770", "contents": "Title: Automatic Image Annotation via Label Transfer in the Semantic Space Abstract: Automatic image annotation is among the fundamental problems in computer\nvision and pattern recognition, and it is becoming increasingly important in\norder to develop algorithms that are able to search and browse large-scale\nimage collections. In this paper, we propose a label propagation framework\nbased on Kernel Canonical Correlation Analysis (KCCA), which builds a latent\nsemantic space where correlation of visual and textual features are well\npreserved into a semantic embedding. The proposed approach is robust and can\nwork either when the training set is well annotated by experts, as well as when\nit is noisy such as in the case of user-generated tags in social media. We\nreport extensive results on four popular datasets. Our results show that our\nKCCA-based framework can be applied to several state-of-the-art label transfer\nmethods to obtain significant improvements. Our approach works even with the\nnoisy tags of social users, provided that appropriate denoising is performed.\nExperiments on a large scale setting show that our method can provide some\nbenefits even when the semantic space is estimated on a subset of training\nimages. \n\n"}
{"id": "1605.05440", "contents": "Title: Beyond Caption To Narrative: Video Captioning With Multiple Sentences Abstract: Recent advances in image captioning task have led to increasing interests in\nvideo captioning task. However, most works on video captioning are focused on\ngenerating single input of aggregated features, which hardly deviates from\nimage captioning process and does not fully take advantage of dynamic contents\npresent in videos. We attempt to generate video captions that convey richer\ncontents by temporally segmenting the video with action localization,\ngenerating multiple captions from multiple frames, and connecting them with\nnatural language processing techniques, in order to generate a story-like\ncaption. We show that our proposed method can generate captions that are richer\nin contents and can compete with state-of-the-art method without explicitly\nusing video-level features as input. \n\n"}
{"id": "1605.06083", "contents": "Title: Stereotyping and Bias in the Flickr30K Dataset Abstract: An untested assumption behind the crowdsourced descriptions of the images in\nthe Flickr30K dataset (Young et al., 2014) is that they \"focus only on the\ninformation that can be obtained from the image alone\" (Hodosh et al., 2013, p.\n859). This paper presents some evidence against this assumption, and provides a\nlist of biases and unwarranted inferences that can be found in the Flickr30K\ndataset. Finally, it considers methods to find examples of these, and discusses\nhow we should deal with stereotype-driven descriptions in future applications. \n\n"}
{"id": "1605.06215", "contents": "Title: Efficient Feature-based Image Registration by Mapping Sparsified\n  Surfaces Abstract: With the advancement in the digital camera technology, the use of high\nresolution images and videos has been widespread in the modern society. In\nparticular, image and video frame registration is frequently applied in\ncomputer graphics and film production. However, conventional registration\napproaches usually require long computational time for high resolution images\nand video frames. This hinders the application of the registration approaches\nin the modern industries. In this work, we first propose a new image\nrepresentation method to accelerate the registration process by triangulating\nthe images effectively. For each high resolution image or video frame, we\ncompute an optimal coarse triangulation which captures the important features\nof the image. Then, we apply a surface registration algorithm to obtain a\nregistration map which is used to compute the registration of the high\nresolution image. Experimental results suggest that our overall algorithm is\nefficient and capable to achieve a high compression rate while the accuracy of\nthe registration is well retained when compared with the conventional\ngrid-based approach. Also, the computational time of the registration is\nsignificantly reduced using our triangulation-based approach. \n\n"}
{"id": "1605.08247", "contents": "Title: cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey Abstract: The \"cvpaper.challenge\" is a group composed of members from AIST, Tokyo Denki\nUniv. (TDU), and Univ. of Tsukuba that aims to systematically summarize papers\non computer vision, pattern recognition, and related fields. For this\nparticular review, we focused on reading the ALL 602 conference papers\npresented at the CVPR2015, the premier annual computer vision event held in\nJune 2015, in order to grasp the trends in the field. Further, we are proposing\n\"DeepSurvey\" as a mechanism embodying the entire process from the reading\nthrough all the papers, the generation of ideas, and to the writing of paper. \n\n"}
{"id": "1605.09782", "contents": "Title: Adversarial Feature Learning Abstract: The ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily\ncomplex data distributions has been demonstrated empirically, with compelling\nresults showing that the latent space of such generators captures semantic\nvariation in the data distribution. Intuitively, models trained to predict\nthese semantic latent representations given data may serve as useful feature\nrepresentations for auxiliary problems where semantics are relevant. However,\nin their existing form, GANs have no means of learning the inverse mapping --\nprojecting data back into the latent space. We propose Bidirectional Generative\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\ndemonstrate that the resulting learned feature representation is useful for\nauxiliary supervised discrimination tasks, competitive with contemporary\napproaches to unsupervised and self-supervised feature learning. \n\n"}
{"id": "1606.00185", "contents": "Title: A Survey on Learning to Hash Abstract: Nearest neighbor search is a problem of finding the data points from the\ndatabase such that the distances from them to the query point are the smallest.\nLearning to hash is one of the major solutions to this problem and has been\nwidely studied recently. In this paper, we present a comprehensive survey of\nthe learning to hash algorithms, categorize them according to the manners of\npreserving the similarities into: pairwise similarity preserving, multiwise\nsimilarity preserving, implicit similarity preserving, as well as quantization,\nand discuss their relations. We separate quantization from pairwise similarity\npreserving as the objective function is very different though quantization, as\nwe show, can be derived from preserving the pairwise similarities. In addition,\nwe present the evaluation protocols, and the general performance analysis, and\npoint out that the quantization algorithms perform superiorly in terms of\nsearch accuracy, search time cost, and space cost. Finally, we introduce a few\nemerging topics. \n\n"}
{"id": "1606.02407", "contents": "Title: Structured Convolution Matrices for Energy-efficient Deep learning Abstract: We derive a relationship between network representation in energy-efficient\nneuromorphic architectures and block Toplitz convolutional matrices. Inspired\nby this connection, we develop deep convolutional networks using a family of\nstructured convolutional matrices and achieve state-of-the-art trade-off\nbetween energy efficiency and classification accuracy for well-known image\nrecognition tasks. We also put forward a novel method to train binary\nconvolutional networks by utilising an existing connection between\nnoisy-rectified linear units and binary activations. \n\n"}
{"id": "1606.03498", "contents": "Title: Improved Techniques for Training GANs Abstract: We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes. \n\n"}
{"id": "1606.04189", "contents": "Title: Inverting face embeddings with convolutional neural networks Abstract: Deep neural networks have dramatically advanced the state of the art for many\nareas of machine learning. Recently they have been shown to have a remarkable\nability to generate highly complex visual artifacts such as images and text\nrather than simply recognize them.\n  In this work we use neural networks to effectively invert low-dimensional\nface embeddings while producing realistically looking consistent images. Our\ncontribution is twofold, first we show that a gradient ascent style approaches\ncan be used to reproduce consistent images, with a help of a guiding image.\nSecond, we demonstrate that we can train a separate neural network to\neffectively solve the minimization problem in one pass, and generate images in\nreal-time. We then evaluate the loss imposed by using a neural network instead\nof the gradient descent by comparing the final values of the minimized loss\nfunction. \n\n"}
{"id": "1606.04446", "contents": "Title: Attend Refine Repeat: Active Box Proposal Generation via In-Out\n  Localization Abstract: The problem of computing category agnostic bounding box proposals is utilized\nas a core component in many computer vision tasks and thus has lately attracted\na lot of attention. In this work we propose a new approach to tackle this\nproblem that is based on an active strategy for generating box proposals that\nstarts from a set of seed boxes, which are uniformly distributed on the image,\nand then progressively moves its attention on the promising image areas where\nit is more likely to discover well localized bounding box proposals. We call\nour approach AttractioNet and a core component of it is a CNN-based category\nagnostic object location refinement module that is capable of yielding accurate\nand robust bounding box predictions regardless of the object category.\n  We extensively evaluate our AttractioNet approach on several image datasets\n(i.e. COCO, PASCAL, ImageNet detection and NYU-Depth V2 datasets) reporting on\nall of them state-of-the-art results that surpass the previous work in the\nfield by a significant margin and also providing strong empirical evidence that\nour approach is capable to generalize to unseen categories. Furthermore, we\nevaluate our AttractioNet proposals in the context of the object detection task\nusing a VGG16-Net based detector and the achieved detection performance on COCO\nmanages to significantly surpass all other VGG16-Net based detectors while even\nbeing competitive with a heavily tuned ResNet-101 based detector. Code as well\nas box proposals computed for several datasets are available at::\nhttps://github.com/gidariss/AttractioNet. \n\n"}
{"id": "1606.05328", "contents": "Title: Conditional Image Generation with PixelCNN Decoders Abstract: This work explores conditional image generation with a new image density\nmodel based on the PixelCNN architecture. The model can be conditioned on any\nvector, including descriptive labels or tags, or latent embeddings created by\nother networks. When conditioned on class labels from the ImageNet database,\nthe model is able to generate diverse, realistic scenes representing distinct\nanimals, objects, landscapes and structures. When conditioned on an embedding\nproduced by a convolutional network given a single image of an unseen face, it\ngenerates a variety of new portraits of the same person with different facial\nexpressions, poses and lighting conditions. We also show that conditional\nPixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,\nthe gated convolutional layers in the proposed model improve the log-likelihood\nof PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,\nwith greatly reduced computational cost. \n\n"}
{"id": "1606.07461", "contents": "Title: LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in\n  Recurrent Neural Networks Abstract: Recurrent neural networks, and in particular long short-term memory (LSTM)\nnetworks, are a remarkably effective tool for sequence modeling that learn a\ndense black-box hidden representation of their sequential input. Researchers\ninterested in better understanding these models have studied the changes in\nhidden state representations over time and noticed some interpretable patterns\nbut also significant noise. In this work, we present LSTMVIS, a visual analysis\ntool for recurrent neural networks with a focus on understanding these hidden\nstate dynamics. The tool allows users to select a hypothesis input range to\nfocus on local state changes, to match these states changes to similar patterns\nin a large data set, and to align these results with structural annotations\nfrom their domain. We show several use cases of the tool for analyzing specific\nhidden state properties on dataset containing nesting, phrase structure, and\nchord progressions, and demonstrate how the tool can be used to isolate\npatterns for further statistical analysis. We characterize the domain, the\ndifferent stakeholders, and their goals and tasks. \n\n"}
{"id": "1607.00971", "contents": "Title: Can we unify monocular detectors for autonomous driving by using the\n  pixel-wise semantic segmentation of CNNs? Abstract: Autonomous driving is a challenging topic that requires complex solutions in\nperception tasks such as recognition of road, lanes, traffic signs or lights,\nvehicles and pedestrians. Through years of research, computer vision has grown\ncapable of tackling these tasks with monocular detectors that can provide\nremarkable detection rates with relatively low processing times. However, the\nrecent appearance of Convolutional Neural Networks (CNNs) has revolutionized\nthe computer vision field and has made possible approaches to perform full\npixel-wise semantic segmentation in times close to real time (even on hardware\nthat can be carried on a vehicle). In this paper, we propose to use full image\nsegmentation as an approach to simplify and unify most of the detection tasks\nrequired in the perception module of an autonomous vehicle, analyzing major\nconcerns such as computation time and detection performance. \n\n"}
{"id": "1607.01059", "contents": "Title: Improving Sparse Representation-Based Classification Using Local\n  Principal Component Analysis Abstract: Sparse representation-based classification (SRC), proposed by Wright et al.,\nseeks the sparsest decomposition of a test sample over the dictionary of\ntraining samples, with classification to the most-contributing class. Because\nit assumes test samples can be written as linear combinations of their\nsame-class training samples, the success of SRC depends on the size and\nrepresentativeness of the training set. Our proposed classification algorithm\nenlarges the training set by using local principal component analysis to\napproximate the basis vectors of the tangent hyperplane of the class manifold\nat each training sample. The dictionary in SRC is replaced by a local\ndictionary that adapts to the test sample and includes training samples and\ntheir corresponding tangent basis vectors. We use a synthetic data set and\nthree face databases to demonstrate that this method can achieve higher\nclassification accuracy than SRC in cases of sparse sampling, nonlinear class\nmanifolds, and stringent dimension reduction. \n\n"}
{"id": "1607.02241", "contents": "Title: Overcoming Challenges in Fixed Point Training of Deep Convolutional\n  Networks Abstract: It is known that training deep neural networks, in particular, deep\nconvolutional networks, with aggressively reduced numerical precision is\nchallenging. The stochastic gradient descent algorithm becomes unstable in the\npresence of noisy gradient updates resulting from arithmetic with limited\nnumeric precision. One of the well-accepted solutions facilitating the training\nof low precision fixed point networks is stochastic rounding. However, to the\nbest of our knowledge, the source of the instability in training neural\nnetworks with noisy gradient updates has not been well investigated. This work\nis an attempt to draw a theoretical connection between low numerical precision\nand training algorithm stability. In doing so, we will also propose and verify\nthrough experiments methods that are able to improve the training performance\nof deep convolutional networks in fixed point. \n\n"}
{"id": "1607.03856", "contents": "Title: Deep Structured-Output Regression Learning for Computational Color\n  Constancy Abstract: Computational color constancy that requires esti- mation of illuminant colors\nof images is a fundamental yet active problem in computer vision, which can be\nformulated into a regression problem. To learn a robust regressor for color\nconstancy, obtaining meaningful imagery features and capturing latent\ncorrelations across output variables play a vital role. In this work, we\nintroduce a novel deep structured-output regression learning framework to\nachieve both goals simultaneously. By borrowing the power of deep convolutional\nneural networks (CNN) originally designed for visual recognition, the proposed\nframework can automatically discover strong features for white balancing over\ndifferent illumination conditions and learn a multi-output regressor beyond\nunderlying relationships between features and targets to find the complex\ninterdependence of dif- ferent dimensions of target variables. Experiments on\ntwo public benchmarks demonstrate that our method achieves competitive\nperformance in comparison with the state-of-the-art approaches. \n\n"}
{"id": "1607.05194", "contents": "Title: HeMIS: Hetero-Modal Image Segmentation Abstract: We introduce a deep learning image segmentation framework that is extremely\nrobust to missing imaging modalities. Instead of attempting to impute or\nsynthesize missing data, the proposed approach learns, for each modality, an\nembedding of the input image into a single latent vector space for which\narithmetic operations (such as taking the mean) are well defined. Points in\nthat space, which are averaged over modalities available at inference time, can\nthen be further processed to yield the desired segmentation. As such, any\ncombinatorial subset of available modalities can be provided as input, without\nhaving to learn a combinatorial number of imputation models. Evaluated on two\nneurological MRI datasets (brain tumors and MS lesions), the approach yields\nstate-of-the-art segmentation results when provided with all modalities;\nmoreover, its performance degrades remarkably gracefully when modalities are\nremoved, significantly more so than alternative mean-filling or other synthesis\napproaches. \n\n"}
{"id": "1607.07646", "contents": "Title: Emotion-Based Crowd Representation for Abnormality Detection Abstract: In crowd behavior understanding, a model of crowd behavior need to be trained\nusing the information extracted from video sequences. Since there is no\nground-truth available in crowd datasets except the crowd behavior labels, most\nof the methods proposed so far are just based on low-level visual features.\nHowever, there is a huge semantic gap between low-level motion/appearance\nfeatures and high-level concept of crowd behaviors. In this paper we propose an\nattribute-based strategy to alleviate this problem. While similar strategies\nhave been recently adopted for object and action recognition, as far as we\nknow, we are the first showing that the crowd emotions can be used as\nattributes for crowd behavior understanding. The main idea is to train a set of\nemotion-based classifiers, which can subsequently be used to represent the\ncrowd motion. For this purpose, we collect a big dataset of video clips and\nprovide them with both annotations of \"crowd behaviors\" and \"crowd emotions\".\nWe show the results of the proposed method on our dataset, which demonstrate\nthat the crowd emotions enable the construction of more descriptive models for\ncrowd behaviors. We aim at publishing the dataset with the article, to be used\nas a benchmark for the communities. \n\n"}
{"id": "1607.07988", "contents": "Title: ATGV-Net: Accurate Depth Super-Resolution Abstract: In this work we present a novel approach for single depth map\nsuper-resolution. Modern consumer depth sensors, especially Time-of-Flight\nsensors, produce dense depth measurements, but are affected by noise and have a\nlow lateral resolution. We propose a method that combines the benefits of\nrecent advances in machine learning based single image super-resolution, i.e.\ndeep convolutional networks, with a variational method to recover accurate\nhigh-resolution depth maps. In particular, we integrate a variational method\nthat models the piecewise affine structures apparent in depth data via an\nanisotropic total generalized variation regularization term on top of a deep\nnetwork. We call our method ATGV-Net and train it end-to-end by unrolling the\noptimization procedure of the variational method. To train deep networks, a\nlarge corpus of training data with accurate ground-truth is required. We\ndemonstrate that it is feasible to train our method solely on synthetic data\nthat we generate in large quantities for this task. Our evaluations show that\nwe achieve state-of-the-art results on three different benchmarks, as well as\non a challenging Time-of-Flight dataset, all without utilizing an additional\nintensity image as guidance. \n\n"}
{"id": "1607.08811", "contents": "Title: Can a CNN Recognize Catalan Diet? Abstract: Nowadays, we can find several diseases related to the unhealthy diet habits\nof the population, such as diabetes, obesity, anemia, bulimia and anorexia. In\nmany cases, these diseases are related to the food consumption of people.\nMediterranean diet is scientifically known as a healthy diet that helps to\nprevent many metabolic diseases. In particular, our work focuses on the\nrecognition of Mediterranean food and dishes. The development of this\nmethodology would allow to analise the daily habits of users with wearable\ncameras, within the topic of lifelogging. By using automatic mechanisms we\ncould build an objective tool for the analysis of the patient's behaviour,\nallowing specialists to discover unhealthy food patterns and understand the\nuser's lifestyle.\n  With the aim to automatically recognize a complete diet, we introduce a\nchallenging multi-labeled dataset related to Mediterranean diet called FoodCAT.\nThe first type of label provided consists of 115 food classes with an average\nof 400 images per dish, and the second one consists of 12 food categories with\nan average of 3800 pictures per class. This dataset will serve as a basis for\nthe development of automatic diet recognition. In this context, deep learning\nand more specifically, Convolutional Neural Networks (CNNs), currently are\nstate-of-the-art methods for automatic food recognition. In our work, we\ncompare several architectures for image classification, with the purpose of\ndiet recognition. Applying the best model for recognising food categories, we\nachieve a top-1 accuracy of 72.29\\%, and top-5 of 97.07\\%. In a complete diet\nrecognition of dishes from Mediterranean diet, enlarged with the Food-101\ndataset for international dishes recognition, we achieve a top-1 accuracy of\n68.07\\%, and top-5 of 89.53\\%, for a total of 115+101 food classes. \n\n"}
{"id": "1608.01866", "contents": "Title: Fusing Deep Convolutional Networks for Large Scale Visual Concept\n  Classification Abstract: Deep learning architectures are showing great promise in various computer\nvision domains including image classification, object detection, event\ndetection and action recognition. In this study, we investigate various aspects\nof convolutional neural networks (CNNs) from the big data perspective. We\nanalyze recent studies and different network architectures both in terms of\nrunning time and accuracy. We present extensive empirical information along\nwith best practices for big data practitioners. Using these best practices we\npropose efficient fusion mechanisms both for single and multiple network\nmodels. We present state-of-the art results on benchmark datasets while keeping\ncomputational costs at a lower level. Another contribution of our paper is that\nthese state-of-the-art results can be reached without using extensive data\naugmentation techniques. \n\n"}
{"id": "1608.02236", "contents": "Title: Bootstrapping Face Detection with Hard Negative Examples Abstract: Recently significant performance improvement in face detection was made\npossible by deeply trained convolutional networks. In this report, a novel\napproach for training state-of-the-art face detector is described. The key is\nto exploit the idea of hard negative mining and iteratively update the Faster\nR-CNN based face detector with the hard negatives harvested from a large set of\nbackground examples. We demonstrate that our face detector outperforms\nstate-of-the-art detectors on the FDDB dataset, which is the de facto standard\nfor evaluating face detection algorithms. \n\n"}
{"id": "1608.03914", "contents": "Title: When was that made? Abstract: In this paper, we explore deep learning methods for estimating when objects\nwere made. Automatic methods for this task could potentially be useful for\nhistorians, collectors, or any individual interested in estimating when their\nartifact was created. Direct applications include large-scale data organization\nor retrieval. Toward this goal, we utilize features from existing deep networks\nand also fine-tune new networks for temporal estimation. In addition, we create\ntwo new datasets of 67,771 dated clothing items from Flickr and museum\ncollections. Our method outperforms both a color-based baseline and previous\nstate of the art methods for temporal estimation. We also provide several\nanalyses of what our networks have learned, and demonstrate applications to\nidentifying temporal inspiration in fashion collections. \n\n"}
{"id": "1608.04188", "contents": "Title: Face Alignment In-the-Wild: A Survey Abstract: Over the last two decades, face alignment or localizing fiducial facial\npoints has received increasing attention owing to its comprehensive\napplications in automatic face analysis. However, such a task has proven\nextremely challenging in unconstrained environments due to many confounding\nfactors, such as pose, occlusions, expression and illumination. While numerous\ntechniques have been developed to address these challenges, this problem is\nstill far away from being solved. In this survey, we present an up-to-date\ncritical review of the existing literatures on face alignment, focusing on\nthose methods addressing overall difficulties and challenges of this topic\nunder uncontrolled conditions. Specifically, we categorize existing face\nalignment techniques, present detailed descriptions of the prominent algorithms\nwithin each category, and discuss their advantages and disadvantages.\nFurthermore, we organize special discussions on the practical aspects of face\nalignment in-the-wild, towards the development of a robust face alignment\nsystem. In addition, we show performance statistics of the state of the art,\nand conclude this paper with several promising directions for future research. \n\n"}
{"id": "1608.04959", "contents": "Title: Frame- and Segment-Level Features and Candidate Pool Evaluation for\n  Video Caption Generation Abstract: We present our submission to the Microsoft Video to Language Challenge of\ngenerating short captions describing videos in the challenge dataset. Our model\nis based on the encoder--decoder pipeline, popular in image and video\ncaptioning systems. We propose to utilize two different kinds of video\nfeatures, one to capture the video content in terms of objects and attributes,\nand the other to capture the motion and action information. Using these diverse\nfeatures we train models specializing in two separate input sub-domains. We\nthen train an evaluator model which is used to pick the best caption from the\npool of candidates generated by these domain expert models. We argue that this\napproach is better suited for the current video captioning task, compared to\nusing a single model, due to the diversity in the dataset.\n  Efficacy of our method is proven by the fact that it was rated best in MSR\nVideo to Language Challenge, as per human evaluation. Additionally, we were\nranked second in the automatic evaluation metrics based table. \n\n"}
{"id": "1608.05167", "contents": "Title: AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene\n  Classification Abstract: Aerial scene classification, which aims to automatically label an aerial\nimage with a specific semantic category, is a fundamental problem for\nunderstanding high-resolution remote sensing imagery. In recent years, it has\nbecome an active task in remote sensing area and numerous algorithms have been\nproposed for this task, including many machine learning and data-driven\napproaches. However, the existing datasets for aerial scene classification like\nUC-Merced dataset and WHU-RS19 are with relatively small sizes, and the results\non them are already saturated. This largely limits the development of scene\nclassification algorithms. This paper describes the Aerial Image Dataset (AID):\na large-scale dataset for aerial scene classification. The goal of AID is to\nadvance the state-of-the-arts in scene classification of remote sensing images.\nFor creating AID, we collect and annotate more than ten thousands aerial scene\nimages. In addition, a comprehensive review of the existing aerial scene\nclassification techniques as well as recent widely-used deep learning methods\nis given. Finally, we provide a performance analysis of typical aerial scene\nclassification and deep learning approaches on AID, which can be served as the\nbaseline results on this benchmark. \n\n"}
{"id": "1608.05180", "contents": "Title: A Holistic Approach for Data-Driven Object Cutout Abstract: Object cutout is a fundamental operation for image editing and manipulation,\nyet it is extremely challenging to automate it in real-world images, which\ntypically contain considerable background clutter. In contrast to existing\ncutout methods, which are based mainly on low-level image analysis, we propose\na more holistic approach, which considers the entire shape of the object of\ninterest by leveraging higher-level image analysis and learnt global shape\npriors. Specifically, we leverage a deep neural network (DNN) trained for\nobjects of a particular class (chairs) for realizing this mechanism. Given a\nrectangular image region, the DNN outputs a probability map (P-map) that\nindicates for each pixel inside the rectangle how likely it is to be contained\ninside an object from the class of interest. We show that the resulting P-maps\nmay be used to evaluate how likely a rectangle proposal is to contain an\ninstance of the class, and further process good proposals to produce an\naccurate object cutout mask. This amounts to an automatic end-to-end pipeline\nfor catergory-specific object cutout. We evaluate our approach on segmentation\nbenchmark datasets, and show that it significantly outperforms the\nstate-of-the-art on them. \n\n"}
{"id": "1608.05461", "contents": "Title: We Can \"See\" You via Wi-Fi - WiFi Action Recognition via Vision-based\n  Methods Abstract: Recently, Wi-Fi has caught tremendous attention for its ubiquity, and,\nmotivated by Wi-Fi's low cost and privacy preservation, researchers have been\nputting lots of investigation into its potential on action recognition and even\nperson identification. In this paper, we offer an comprehensive overview on\nthese two topics in Wi-Fi. Also, through looking at these two topics from an\nunprecedented perspective, we could achieve generality instead of designing\nspecific ad-hoc features for each scenario. Observing the great resemblance of\nChannel State Information (CSI, a fine-grained information captured from the\nreceived Wi-Fi signal) to texture, we proposed a brand-new framework based on\ncomputer vision methods. To minimize the effect of location dependency embedded\nin CSI, we propose a novel de-noising method based on Singular Value\nDecomposition (SVD) to eliminate the background energy and effectively extract\nthe channel information of signals reflected by human bodies. From the\nexperiments conducted, we demonstrate the feasibility and efficacy of the\nproposed methods. Also, we conclude factors that would affect the performance\nand highlight a few promising issues that require further deliberation. \n\n"}
{"id": "1608.06197", "contents": "Title: CrowdNet: A Deep Convolutional Network for Dense Crowd Counting Abstract: Our work proposes a novel deep learning framework for estimating crowd\ndensity from static images of highly dense crowds. We use a combination of deep\nand shallow, fully convolutional networks to predict the density map for a\ngiven crowd image. Such a combination is used for effectively capturing both\nthe high-level semantic information (face/body detectors) and the low-level\nfeatures (blob detectors), that are necessary for crowd counting under large\nscale variations. As most crowd datasets have limited training samples (<100\nimages) and deep learning based approaches require large amounts of training\ndata, we perform multi-scale data augmentation. Augmenting the training samples\nin such a manner helps in guiding the CNN to learn scale invariant\nrepresentations. Our method is tested on the challenging UCF_CC_50 dataset, and\nshown to outperform the state of the art methods. \n\n"}
{"id": "1608.06770", "contents": "Title: Automatic Synchronization of Multi-User Photo Galleries Abstract: In this paper we address the issue of photo galleries synchronization, where\npictures related to the same event are collected by different users. Existing\nsolutions to address the problem are usually based on unrealistic assumptions,\nlike time consistency across photo galleries, and often heavily rely on\nheuristics, limiting therefore the applicability to real-world scenarios. We\npropose a solution that achieves better generalization performance for the\nsynchronization task compared to the available literature. The method is\ncharacterized by three stages: at first, deep convolutional neural network\nfeatures are used to assess the visual similarity among the photos; then, pairs\nof similar photos are detected across different galleries and used to construct\na graph; eventually, a probabilistic graphical model is used to estimate the\ntemporal offset of each pair of galleries, by traversing the minimum spanning\ntree extracted from this graph. The experimental evaluation is conducted on\nfour publicly available datasets covering different types of events,\ndemonstrating the strength of our proposed method. A thorough discussion of the\nobtained results is provided for a critical assessment of the quality in\nsynchronization. \n\n"}
{"id": "1608.07639", "contents": "Title: Learning to generalize to new compositions in image understanding Abstract: Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure. \n\n"}
{"id": "1609.01571", "contents": "Title: Best-Buddies Similarity - Robust Template Matching using Mutual Nearest\n  Neighbors Abstract: We propose a novel method for template matching in unconstrained\nenvironments. Its essence is the Best-Buddies Similarity (BBS), a useful,\nrobust, and parameter-free similarity measure between two sets of points. BBS\nis based on counting the number of Best-Buddies Pairs (BBPs)--pairs of points\nin source and target sets, where each point is the nearest neighbor of the\nother. BBS has several key features that make it robust against complex\ngeometric deformations and high levels of outliers, such as those arising from\nbackground clutter and occlusions. We study these properties, provide a\nstatistical analysis that justifies them, and demonstrate the consistent\nsuccess of BBS on a challenging real-world dataset while using different types\nof features. \n\n"}
{"id": "1609.01882", "contents": "Title: Polysemous codes Abstract: This paper considers the problem of approximate nearest neighbor search in\nthe compressed domain. We introduce polysemous codes, which offer both the\ndistance estimation quality of product quantization and the efficient\ncomparison of binary codes with Hamming distance. Their design is inspired by\nalgorithms introduced in the 90's to construct channel-optimized vector\nquantizers. At search time, this dual interpretation accelerates the search.\nMost of the indexed vectors are filtered out with Hamming distance, letting\nonly a fraction of the vectors to be ranked with an asymmetric distance\nestimator.\n  The method is complementary with a coarse partitioning of the feature space\nsuch as the inverted multi-index. This is shown by our experiments performed on\nseveral public benchmarks such as the BIGANN dataset comprising one billion\nvectors, for which we report state-of-the-art results for query times below\n0.3\\,millisecond per core. Last but not least, our approach allows the\napproximate computation of the k-NN graph associated with the Yahoo Flickr\nCreative Commons 100M, described by CNN image descriptors, in less than 8 hours\non a single machine. \n\n"}
{"id": "1609.02452", "contents": "Title: End-to-End Eye Movement Detection Using Convolutional Neural Networks Abstract: Common computational methods for automated eye movement detection - i.e. the\ntask of detecting different types of eye movement in a continuous stream of\ngaze data - are limited in that they either involve thresholding on\nhand-crafted signal features, require individual detectors each only detecting\na single movement, or require pre-segmented data. We propose a novel approach\nfor eye movement detection that only involves learning a single detector\nend-to-end, i.e. directly from the continuous gaze data stream and\nsimultaneously for different eye movements without any manual feature crafting\nor segmentation. Our method is based on convolutional neural networks (CNN)\nthat recently demonstrated superior performance in a variety of tasks in\ncomputer vision, signal processing, and machine learning. We further introduce\na novel multi-participant dataset that contains scripted and free-viewing\nsequences of ground-truth annotated saccades, fixations, and smooth pursuits.\nWe show that our CNN-based method outperforms state-of-the-art baselines by a\nlarge margin on this challenging dataset, thereby underlining the significant\npotential of this approach for holistic, robust, and accurate eye movement\nprotocol analysis. \n\n"}
{"id": "1609.04116", "contents": "Title: Joint Gender Classification and Age Estimation by Nearly Orthogonalizing\n  Their Semantic Spaces Abstract: In human face-based biometrics, gender classification and age estimation are\ntwo typical learning tasks. Although a variety of approaches have been proposed\nto handle them, just a few of them are solved jointly, even so, these joint\nmethods do not yet specifically concern the semantic difference between human\ngender and age, which is intuitively helpful for joint learning, consequently\nleaving us a room of further improving the performance. To this end, in this\nwork we firstly propose a general learning framework for jointly estimating\nhuman gender and age by specially attempting to formulate such semantic\nrelationships as a form of near-orthogonality regularization and then\nincorporate it into the objective of the joint learning framework. In order to\nevaluate the effectiveness of the proposed framework, we exemplify it by\nrespectively taking the widely used binary-class SVM for gender classification,\nand two threshold-based ordinal regression methods (i.e., the discriminant\nlearning for ordinal regression and support vector ordinal regression) for age\nestimation, and crucially coupling both through the proposed semantic\nformulation. Moreover, we develop its kernelized nonlinear counterpart by\nderiving a representer theorem for the joint learning strategy. Finally,\nthrough extensive experiments on three aging datasets FG-NET, Morph Album I and\nMorph Album II, we demonstrate the effectiveness and superiority of the\nproposed joint learning strategy. \n\n"}
{"id": "1609.04453", "contents": "Title: A Large Contextual Dataset for Classification, Detection and Counting of\n  Cars with Deep Learning Abstract: We have created a large diverse set of cars from overhead images, which are\nuseful for training a deep learner to binary classify, detect and count them.\nThe dataset and all related material will be made publically available. The set\ncontains contextual matter to aid in identification of difficult targets. We\ndemonstrate classification and detection on this dataset using a neural network\nwe call ResCeption. This network combines residual learning with\nInception-style layers and is used to count cars in one look. This is a new way\nto count objects rather than by localization or density estimation. It is\nfairly accurate, fast and easy to implement. Additionally, the counting method\nis not car or scene specific. It would be easy to train this method to count\nother kinds of objects and counting over new scenes requires no extra set up or\nassumptions about object locations. \n\n"}
{"id": "1609.05119", "contents": "Title: Deep Impression: Audiovisual Deep Residual Networks for Multimodal\n  Apparent Personality Trait Recognition Abstract: Here, we develop an audiovisual deep residual network for multimodal apparent\npersonality trait recognition. The network is trained end-to-end for predicting\nthe Big Five personality traits of people from their videos. That is, the\nnetwork does not require any feature engineering or visual analysis such as\nface detection, face landmark alignment or facial expression recognition.\nRecently, the network won the third place in the ChaLearn First Impressions\nChallenge with a test accuracy of 0.9109. \n\n"}
{"id": "1609.05672", "contents": "Title: Multi-Residual Networks: Improving the Speed and Accuracy of Residual\n  Networks Abstract: In this article, we take one step toward understanding the learning behavior\nof deep residual networks, and supporting the observation that deep residual\nnetworks behave like ensembles. We propose a new convolutional neural network\narchitecture which builds upon the success of residual networks by explicitly\nexploiting the interpretation of very deep networks as an ensemble. The\nproposed multi-residual network increases the number of residual functions in\nthe residual blocks. Our architecture generates models that are wider, rather\nthan deeper, which significantly improves accuracy. We show that our model\nachieves an error rate of 3.73% and 19.45% on CIFAR-10 and CIFAR-100\nrespectively, that outperforms almost all of the existing models. We also\ndemonstrate that our model outperforms very deep residual networks by 0.22%\n(top-1 error) on the full ImageNet 2012 classification dataset. Additionally,\ninspired by the parallel structure of multi-residual networks, a model\nparallelism technique has been investigated. The model parallelism method\ndistributes the computation of residual blocks among the processors, yielding\nup to 15% computational complexity improvement. \n\n"}
{"id": "1609.08740", "contents": "Title: Scalable Discrete Supervised Hash Learning with Asymmetric Matrix\n  Factorization Abstract: Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, and it has received a broad attention due to its low storage cost and\nfast retrieval speed. However, the existing limitations make the present\nalgorithms difficult to deal with large-scale datasets: (1) discrete\nconstraints are involved in the learning of the hash function; (2) pairwise or\ntriplet similarity is adopted to generate efficient hashcodes, resulting both\ntime and space complexity are greater than O(n^2). To address these issues, we\npropose a novel discrete supervised hash learning framework which can be\nscalable to large-scale datasets. First, the discrete learning procedure is\ndecomposed into a binary classifier learning scheme and binary codes learning\nscheme, which makes the learning procedure more efficient. Second, we adopt the\nAsymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based\nBatch Coordinate Descent method, such that the time and space complexity is\nreduced to O(n). The proposed framework also provides a flexible paradigm to\nincorporate with arbitrary hash function, including deep neural networks and\nkernel methods. Experiments on large-scale datasets demonstrate that the\nproposed method is superior or comparable with state-of-the-art hashing\nalgorithms. \n\n"}
{"id": "1609.09143", "contents": "Title: Recurrent Convolutional Networks for Pulmonary Nodule Detection in CT\n  Imaging Abstract: Computed tomography (CT) generates a stack of cross-sectional images covering\na region of the body. The visual assessment of these images for the\nidentification of potential abnormalities is a challenging and time consuming\ntask due to the large amount of information that needs to be processed. In this\narticle we propose a deep artificial neural network architecture, ReCTnet, for\nthe fully-automated detection of pulmonary nodules in CT scans. The\narchitecture learns to distinguish nodules and normal structures at the pixel\nlevel and generates three-dimensional probability maps highlighting areas that\nare likely to harbour the objects of interest. Convolutional and recurrent\nlayers are combined to learn expressive image representations exploiting the\nspatial dependencies across axial slices. We demonstrate that leveraging\nintra-slice dependencies substantially increases the sensitivity to detect\npulmonary nodules without inflating the false positive rate. On the publicly\navailable LIDC/IDRI dataset consisting of 1,018 annotated CT scans, ReCTnet\nreaches a detection sensitivity of 90.5% with an average of 4.5 false positives\nper scan. Comparisons with a competing multi-channel convolutional neural\nnetwork for multi-slice segmentation and other published methodologies using\nthe same dataset provide evidence that ReCTnet offers significant performance\ngains. \n\n"}
{"id": "1609.09220", "contents": "Title: CNN-aware Binary Map for General Semantic Segmentation Abstract: In this paper we introduce a novel method for general semantic segmentation\nthat can benefit from general semantics of Convolutional Neural Network (CNN).\nOur segmentation proposes visually and semantically coherent image segments. We\nuse binary encoding of CNN features to overcome the difficulty of the\nclustering on the high-dimensional CNN feature space. These binary codes are\nvery robust against noise and non-semantic changes in the image. These binary\nencoding can be embedded into the CNN as an extra layer at the end of the\nnetwork. This results in real-time segmentation. To the best of our knowledge\nour method is the first attempt on general semantic image segmentation using\nCNN. All the previous papers were limited to few number of category of the\nimages (e.g. PASCAL VOC). Experiments show that our segmentation algorithm\noutperform the state-of-the-art non-semantic segmentation methods by large\nmargin. \n\n"}
{"id": "1609.09365", "contents": "Title: Deep Tracking on the Move: Learning to Track the World from a Moving\n  Vehicle using Recurrent Neural Networks Abstract: This paper presents an end-to-end approach for tracking static and dynamic\nobjects for an autonomous vehicle driving through crowded urban environments.\nUnlike traditional approaches to tracking, this method is learned end-to-end,\nand is able to directly predict a full unoccluded occupancy grid map from raw\nlaser input data. Inspired by the recently presented DeepTracking approach\n[Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the\ntemporal evolution of the state of the environment, and propose to use Spatial\nTransformer modules to exploit estimates of the egomotion of the vehicle. Our\nresults demonstrate the ability to track a range of objects, including cars,\nbuses, pedestrians, and cyclists through occlusion, from both moving and\nstationary platforms, using a single learned model. Experimental results\ndemonstrate that the model can also predict the future states of objects from\ncurrent inputs, with greater accuracy than previous work. \n\n"}
{"id": "1609.09430", "contents": "Title: CNN Architectures for Large-Scale Audio Classification Abstract: Convolutional Neural Networks (CNNs) have proven very effective in image\nclassification and show promise for audio. We use various CNN architectures to\nclassify the soundtracks of a dataset of 70M training videos (5.24 million\nhours) with 30,871 video-level labels. We examine fully connected Deep Neural\nNetworks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We\ninvestigate varying the size of both training set and label vocabulary, finding\nthat analogs of the CNNs used in image classification do well on our audio\nclassification task, and larger training and label sets help up to a point. A\nmodel using embeddings from these classifiers does much better than raw\nfeatures on the Audio Set [5] Acoustic Event Detection (AED) classification\ntask. \n\n"}
{"id": "1610.01644", "contents": "Title: Understanding intermediate layers using linear classifier probes Abstract: Neural network models have a reputation for being black boxes. We propose to\nmonitor the features at every layer of a model and measure how suitable they\nare for classification. We use linear classifiers, which we refer to as\n\"probes\", trained entirely independently of the model itself.\n  This helps us better understand the roles and dynamics of the intermediate\nlayers. We demonstrate how this can be used to develop a better intuition about\nmodels and to diagnose potential problems.\n  We apply this technique to the popular models Inception v3 and Resnet-50.\nAmong other things, we observe experimentally that the linear separability of\nfeatures increase monotonically along the depth of the model. \n\n"}
{"id": "1610.01852", "contents": "Title: Compressive Imaging with Iterative Forward Models Abstract: We propose a new compressive imaging method for reconstructing 2D or 3D\nobjects from their scattered wave-field measurements. Our method relies on a\nnovel, nonlinear measurement model that can account for the multiple scattering\nphenomenon, which makes the method preferable in applications where linear\nmeasurement models are inaccurate. We construct the measurement model by\nexpanding the scattered wave-field with an accelerated-gradient method, which\nis guaranteed to converge and is suitable for large-scale problems. We provide\nexplicit formulas for computing the gradient of our measurement model with\nrespect to the unknown image, which enables image formation with a sparsity-\ndriven numerical optimization algorithm. We validate the method both\nanalytically and with numerical simulations. \n\n"}
{"id": "1610.02357", "contents": "Title: Xception: Deep Learning with Depthwise Separable Convolutions Abstract: We present an interpretation of Inception modules in convolutional neural\nnetworks as being an intermediate step in-between regular convolution and the\ndepthwise separable convolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable convolution can\nbe understood as an Inception module with a maximally large number of towers.\nThis observation leads us to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules have been replaced\nwith depthwise separable convolutions. We show that this architecture, dubbed\nXception, slightly outperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and significantly outperforms Inception V3 on a\nlarger image classification dataset comprising 350 million images and 17,000\nclasses. Since the Xception architecture has the same number of parameters as\nInception V3, the performance gains are not due to increased capacity but\nrather to a more efficient use of model parameters. \n\n"}
{"id": "1610.02391", "contents": "Title: Grad-CAM: Visual Explanations from Deep Networks via Gradient-based\n  Localization Abstract: We propose a technique for producing \"visual explanations\" for decisions from\na large class of CNN-based models, making them more transparent. Our approach -\nGradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of\nany target concept, flowing into the final convolutional layer to produce a\ncoarse localization map highlighting important regions in the image for\npredicting the concept. Grad-CAM is applicable to a wide variety of CNN\nmodel-families: (1) CNNs with fully-connected layers, (2) CNNs used for\nstructured outputs, (3) CNNs used in tasks with multimodal inputs or\nreinforcement learning, without any architectural changes or re-training. We\ncombine Grad-CAM with fine-grained visualizations to create a high-resolution\nclass-discriminative visualization and apply it to off-the-shelf image\nclassification, captioning, and visual question answering (VQA) models,\nincluding ResNet-based architectures. In the context of image classification\nmodels, our visualizations (a) lend insights into their failure modes, (b) are\nrobust to adversarial images, (c) outperform previous methods on localization,\n(d) are more faithful to the underlying model and (e) help achieve\ngeneralization by identifying dataset bias. For captioning and VQA, we show\nthat even non-attention based models can localize inputs. We devise a way to\nidentify important neurons through Grad-CAM and combine it with neuron names to\nprovide textual explanations for model decisions. Finally, we design and\nconduct human studies to measure if Grad-CAM helps users establish appropriate\ntrust in predictions from models and show that Grad-CAM helps untrained users\nsuccessfully discern a 'stronger' nodel from a 'weaker' one even when both make\nidentical predictions. Our code is available at\nhttps://github.com/ramprs/grad-cam/, along with a demo at\nhttp://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E. \n\n"}
{"id": "1610.06421", "contents": "Title: Mixed Neural Network Approach for Temporal Sleep Stage Classification Abstract: This paper proposes a practical approach to addressing limitations posed by\nuse of single active electrodes in applications for sleep stage classification.\nElectroencephalography (EEG)-based characterizations of sleep stage progression\ncontribute the diagnosis and monitoring of the many pathologies of sleep.\nSeveral prior reports have explored ways of automating the analysis of sleep\nEEG and of reducing the complexity of the data needed for reliable\ndiscrimination of sleep stages in order to make it possible to perform sleep\nstudies at lower cost in the home (rather than only in specialized clinical\nfacilities). However, these reports have involved recordings from electrodes\nplaced on the cranial vertex or occiput, which can be uncomfortable or\ndifficult for subjects to position. Those that have utilized single EEG\nchannels which contain less sleep information, have showed poor classification\nperformance. We have taken advantage of Rectifier Neural Network for feature\ndetection and Long Short-Term Memory (LSTM) network for sequential data\nlearning to optimize classification performance with single electrode\nrecordings. After exploring alternative electrode placements, we found a\ncomfortable configuration of a single-channel EEG on the forehead and have\nshown that it can be integrated with additional electrodes for simultaneous\nrecording of the electroocuolgram (EOG). Evaluation of data from 62 people\n(with 494 hours sleep) demonstrated better performance of our analytical\nalgorithm for automated sleep classification than existing approaches using\nvertex or occipital electrode placements. Use of this recording configuration\nwith neural network deconvolution promises to make clinically indicated home\nsleep studies practical. \n\n"}
{"id": "1610.07629", "contents": "Title: A Learned Representation For Artistic Style Abstract: The diversity of painting styles represents a rich visual vocabulary for the\nconstruction of an image. The degree to which one may learn and parsimoniously\ncapture this visual vocabulary measures our understanding of the higher level\nfeatures of paintings, if not images in general. In this work we investigate\nthe construction of a single, scalable deep network that can parsimoniously\ncapture the artistic style of a diversity of paintings. We demonstrate that\nsuch a network generalizes across a diversity of artistic styles by reducing a\npainting to a point in an embedding space. Importantly, this model permits a\nuser to explore new painting styles by arbitrarily combining the styles learned\nfrom individual paintings. We hope that this work provides a useful step\ntowards building rich models of paintings and offers a window on to the\nstructure of the learned representation of artistic style. \n\n"}
{"id": "1610.09001", "contents": "Title: SoundNet: Learning Sound Representations from Unlabeled Video Abstract: We learn rich natural sound representations by capitalizing on large amounts\nof unlabeled sound data collected in the wild. We leverage the natural\nsynchronization between vision and sound to learn an acoustic representation\nusing two-million unlabeled videos. Unlabeled video has the advantage that it\ncan be economically acquired at massive scales, yet contains useful signals\nabout natural sound. We propose a student-teacher training procedure which\ntransfers discriminative visual knowledge from well established visual\nrecognition models into the sound modality using unlabeled video as a bridge.\nOur sound representation yields significant performance improvements over the\nstate-of-the-art results on standard benchmarks for acoustic scene/object\nclassification. Visualizations suggest some high-level semantics automatically\nemerge in the sound network, even though it is trained without ground truth\nlabels. \n\n"}
{"id": "1610.09003", "contents": "Title: Cross-Modal Scene Networks Abstract: People can recognize scenes across many different modalities beyond natural\nimages. In this paper, we investigate how to learn cross-modal scene\nrepresentations that transfer across modalities. To study this problem, we\nintroduce a new cross-modal scene dataset. While convolutional neural networks\ncan categorize scenes well, they also learn an intermediate representation not\naligned across modalities, which is undesirable for cross-modal transfer\napplications. We present methods to regularize cross-modal convolutional neural\nnetworks so that they have a shared representation that is agnostic of the\nmodality. Our experiments suggest that our scene representation can help\ntransfer representations across modalities for retrieval. Moreover, our\nvisualizations suggest that units emerge in the shared representation that tend\nto activate on consistent concepts independently of the modality. \n\n"}
{"id": "1611.00471", "contents": "Title: Dual Attention Networks for Multimodal Reasoning and Matching Abstract: We propose Dual Attention Networks (DANs) which jointly leverage visual and\ntextual attention mechanisms to capture fine-grained interplay between vision\nand language. DANs attend to specific regions in images and words in text\nthrough multiple steps and gather essential information from both modalities.\nBased on this framework, we introduce two types of DANs for multimodal\nreasoning and matching, respectively. The reasoning model allows visual and\ntextual attentions to steer each other during collaborative inference, which is\nuseful for tasks such as Visual Question Answering (VQA). In addition, the\nmatching model exploits the two attention mechanisms to estimate the similarity\nbetween images and sentences by focusing on their shared semantics. Our\nextensive experiments validate the effectiveness of DANs in combining vision\nand language, achieving the state-of-the-art performance on public benchmarks\nfor VQA and image-text matching. \n\n"}
{"id": "1611.00850", "contents": "Title: Optical Flow Estimation using a Spatial Pyramid Network Abstract: We learn to compute optical flow by combining a classical spatial-pyramid\nformulation with deep learning. This estimates large motions in a\ncoarse-to-fine approach by warping one image of a pair at each pyramid level by\nthe current flow estimate and computing an update to the flow. Instead of the\nstandard minimization of an objective function at each pyramid level, we train\none deep network per level to compute the flow update. Unlike the recent\nFlowNet approach, the networks do not need to deal with large motions; these\nare dealt with by the pyramid. This has several advantages. First, our Spatial\nPyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms\nof model parameters. This makes it more efficient and appropriate for embedded\napplications. Second, since the flow at each pyramid level is small (< 1\npixel), a convolutional approach applied to pairs of warped images is\nappropriate. Third, unlike FlowNet, the learned convolution filters appear\nsimilar to classical spatio-temporal filters, giving insight into the method\nand how to improve it. Our results are more accurate than FlowNet on most\nstandard benchmarks, suggesting a new direction of combining classical flow\nmethods with deep learning. \n\n"}
{"id": "1611.01990", "contents": "Title: Hamiltonian operator for spectral shape analysis Abstract: Many shape analysis methods treat the geometry of an object as a metric space\nthat can be captured by the Laplace-Beltrami operator. In this paper, we\npropose to adapt the classical Hamiltonian operator from quantum mechanics to\nthe field of shape analysis. To this end we study the addition of a potential\nfunction to the Laplacian as a generator for dual spaces in which shape\nprocessing is performed. We present a general optimization approach for solving\nvariational problems involving the basis defined by the Hamiltonian using\nperturbation theory for its eigenvectors. The suggested operator is shown to\nproduce better functional spaces to operate with, as demonstrated on different\nshape analysis tasks. \n\n"}
{"id": "1611.03530", "contents": "Title: Understanding deep learning requires rethinking generalization Abstract: Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple depth two neural networks already have perfect finite sample\nexpressivity as soon as the number of parameters exceeds the number of data\npoints as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models. \n\n"}
{"id": "1611.03673", "contents": "Title: Learning to Navigate in Complex Environments Abstract: Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities. \n\n"}
{"id": "1611.04413", "contents": "Title: Automatic discovery of discriminative parts as a quadratic assignment\n  problem Abstract: Part-based image classification consists in representing categories by small\nsets of discriminative parts upon which a representation of the images is\nbuilt. This paper addresses the question of how to automatically learn such\nparts from a set of labeled training images. The training of parts is cast as a\nquadratic assignment problem in which optimal correspondences between image\nregions and parts are automatically learned. The paper analyses different\nassignment strategies and thoroughly evaluates them on two public datasets:\nWillow actions and MIT 67 scenes. State-of-the art results are obtained on\nthese datasets. \n\n"}
{"id": "1611.05128", "contents": "Title: Designing Energy-Efficient Convolutional Neural Networks using\n  Energy-Aware Pruning Abstract: Deep convolutional neural networks (CNNs) are indispensable to\nstate-of-the-art computer vision algorithms. However, they are still rarely\ndeployed on battery-powered mobile devices, such as smartphones and wearable\ngadgets, where vision algorithms can enable many revolutionary real-world\napplications. The key limiting factor is the high energy consumption of CNN\nprocessing due to its high computational complexity. While there are many\nprevious efforts that try to reduce the CNN model size or amount of\ncomputation, we find that they do not necessarily result in lower energy\nconsumption, and therefore do not serve as a good metric for energy cost\nestimation.\n  To close the gap between CNN design and energy consumption optimization, we\npropose an energy-aware pruning algorithm for CNNs that directly uses energy\nconsumption estimation of a CNN to guide the pruning process. The energy\nestimation methodology uses parameters extrapolated from actual hardware\nmeasurements that target realistic battery-powered system setups. The proposed\nlayer-by-layer pruning algorithm also prunes more aggressively than previously\nproposed pruning methods by minimizing the error in output feature maps instead\nof filter weights. For each layer, the weights are first pruned and then\nlocally fine-tuned with a closed-form least-square solution to quickly restore\nthe accuracy. After all layers are pruned, the entire network is further\nglobally fine-tuned using back-propagation. With the proposed pruning method,\nthe energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x,\nrespectively, with less than 1% top-5 accuracy loss. Finally, we show that\npruning the AlexNet with a reduced number of target classes can greatly\ndecrease the number of weights but the energy reduction is limited.\n  Energy modeling tool and energy-aware pruned models available at\nhttp://eyeriss.mit.edu/energy.html \n\n"}
{"id": "1611.05138", "contents": "Title: S3Pool: Pooling with Stochastic Spatial Sampling Abstract: Feature pooling layers (e.g., max pooling) in convolutional neural networks\n(CNNs) serve the dual purpose of providing increasingly abstract\nrepresentations as well as yielding computational savings in subsequent\nconvolutional layers. We view the pooling operation in CNNs as a two-step\nprocedure: first, a pooling window (e.g., $2\\times 2$) slides over the feature\nmap with stride one which leaves the spatial resolution intact, and second,\ndownsampling is performed by selecting one pixel from each non-overlapping\npooling window in an often uniform and deterministic (e.g., top-left) manner.\nOur starting point in this work is the observation that this regularly spaced\ndownsampling arising from non-overlapping windows, although intuitive from a\nsignal processing perspective (which has the goal of signal reconstruction), is\nnot necessarily optimal for \\emph{learning} (where the goal is to generalize).\nWe study this aspect and propose a novel pooling strategy with stochastic\nspatial sampling (S3Pool), where the regular downsampling is replaced by a more\ngeneral stochastic version. We observe that this general stochasticity acts as\na strong regularizer, and can also be seen as doing implicit data augmentation\nby introducing distortions in the feature maps. We further introduce a\nmechanism to control the amount of distortion to suit different datasets and\narchitectures. To demonstrate the effectiveness of the proposed approach, we\nperform extensive experiments on several popular image classification\nbenchmarks, observing excellent improvements over baseline models. Experimental\ncode is available at https://github.com/Shuangfei/s3pool. \n\n"}
{"id": "1611.05418", "contents": "Title: VisualBackProp: efficient visualization of CNNs Abstract: This paper proposes a new method, that we call VisualBackProp, for\nvisualizing which sets of pixels of the input image contribute most to the\npredictions made by the convolutional neural network (CNN). The method heavily\nhinges on exploring the intuition that the feature maps contain less and less\nirrelevant information to the prediction decision when moving deeper into the\nnetwork. The technique we propose was developed as a debugging tool for\nCNN-based systems for steering self-driving cars and is therefore required to\nrun in real-time, i.e. it was designed to require less computations than a\nforward propagation. This makes the presented visualization method a valuable\ndebugging tool which can be easily used during both training and inference. We\nfurthermore justify our approach with theoretical arguments and theoretically\nconfirm that the proposed method identifies sets of input pixels, rather than\nindividual pixels, that collaboratively contribute to the prediction. Our\ntheoretical findings stand in agreement with the experimental results. The\nempirical evaluation shows the plausibility of the proposed approach on the\nroad video data as well as in other applications and reveals that it compares\nfavorably to the layer-wise relevance propagation approach, i.e. it obtains\nsimilar visualization results and simultaneously achieves order of magnitude\nspeed-ups. \n\n"}
{"id": "1611.05664", "contents": "Title: Learning to detect and localize many objects from few examples Abstract: The current trend in object detection and localization is to learn\npredictions with high capacity deep neural networks trained on a very large\namount of annotated data and using a high amount of processing power. In this\nwork, we propose a new neural model which directly predicts bounding box\ncoordinates. The particularity of our contribution lies in the local\ncomputations of predictions with a new form of local parameter sharing which\nkeeps the overall amount of trainable parameters low. Key components of the\nmodel are spatial 2D-LSTM recurrent layers which convey contextual information\nbetween the regions of the image. We show that this model is more powerful than\nthe state of the art in applications where training data is not as abundant as\nin the classical configuration of natural images and Imagenet/Pascal VOC tasks.\nWe particularly target the detection of text in document images, but our method\nis not limited to this setting. The proposed model also facilitates the\ndetection of many objects in a single image and can deal with inputs of\nvariable sizes without resizing. \n\n"}
{"id": "1611.05720", "contents": "Title: Hard-Aware Deeply Cascaded Embedding Abstract: Riding on the waves of deep neural networks, deep metric learning has also\nachieved promising results in various tasks using triplet network or Siamese\nnetwork. Though the basic goal of making images from the same category closer\nthan the ones from different categories is intuitive, it is hard to directly\noptimize due to the quadratic or cubic sample size. To solve the problem, hard\nexample mining which only focuses on a subset of samples that are considered\nhard is widely used. However, hard is defined relative to a model, where\ncomplex models treat most samples as easy ones and vice versa for simple\nmodels, and both are not good for training. Samples are also with different\nhard levels, it is hard to define a model with the just right complexity and\nchoose hard examples adequately. This motivates us to ensemble a set of models\nwith different complexities in cascaded manner and mine hard examples\nadaptively, a sample is judged by a series of models with increasing\ncomplexities and only updates models that consider the sample as a hard case.\nWe evaluate our method on CARS196, CUB-200-2011, Stanford Online Products,\nVehicleID and DeepFashion datasets. Our method outperforms state-of-the-art\nmethods by a large margin. \n\n"}
{"id": "1611.05777", "contents": "Title: DeeperBind: Enhancing Prediction of Sequence Specificities of DNA\n  Binding Proteins Abstract: Transcription factors (TFs) are macromolecules that bind to\n\\textit{cis}-regulatory specific sub-regions of DNA promoters and initiate\ntranscription. Finding the exact location of these binding sites (aka motifs)\nis important in a variety of domains such as drug design and development. To\naddress this need, several \\textit{in vivo} and \\textit{in vitro} techniques\nhave been developed so far that try to characterize and predict the binding\nspecificity of a protein to different DNA loci. The major problem with these\ntechniques is that they are not accurate enough in prediction of the binding\naffinity and characterization of the corresponding motifs. As a result,\ndownstream analysis is required to uncover the locations where proteins of\ninterest bind. Here, we propose DeeperBind, a long short term recurrent\nconvolutional network for prediction of protein binding specificities with\nrespect to DNA probes. DeeperBind can model the positional dynamics of probe\nsequences and hence reckons with the contributions made by individual\nsub-regions in DNA sequences, in an effective way. Moreover, it can be trained\nand tested on datasets containing varying-length sequences. We apply our\npipeline to the datasets derived from protein binding microarrays (PBMs), an\nin-vitro high-throughput technology for quantification of protein-DNA binding\npreferences, and present promising results. To the best of our knowledge, this\nis the most accurate pipeline that can predict binding specificities of DNA\nsequences from the data produced by high-throughput technologies through\nutilization of the power of deep learning for feature generation and positional\ndynamics modeling. \n\n"}
{"id": "1611.06224", "contents": "Title: ModelHub: Towards Unified Data and Lifecycle Management for Deep\n  Learning Abstract: Deep learning has improved state-of-the-art results in many important fields,\nand has been the subject of much research in recent years, leading to the\ndevelopment of several systems for facilitating deep learning. Current systems,\nhowever, mainly focus on model building and training phases, while the issues\nof data management, model sharing, and lifecycle management are largely\nignored. Deep learning modeling lifecycle generates a rich set of data\nartifacts, such as learned parameters and training logs, and comprises of\nseveral frequently conducted tasks, e.g., to understand the model behaviors and\nto try out new models. Dealing with such artifacts and tasks is cumbersome and\nlargely left to the users. This paper describes our vision and implementation\nof a data and lifecycle management system for deep learning. First, we\ngeneralize model exploration and model enumeration queries from commonly\nconducted tasks by deep learning modelers, and propose a high-level domain\nspecific language (DSL), inspired by SQL, to raise the abstraction level and\naccelerate the modeling process. To manage the data artifacts, especially the\nlarge amount of checkpointed float parameters, we design a novel model\nversioning system (dlv), and a read-optimized parameter archival storage system\n(PAS) that minimizes storage footprint and accelerates query workloads without\nlosing accuracy. PAS archives versioned models using deltas in a\nmulti-resolution fashion by separately storing the less significant bits, and\nfeatures a novel progressive query (inference) evaluation algorithm. Third, we\nshow that archiving versioned models using deltas poses a new dataset\nversioning problem and we develop efficient algorithms for solving it. We\nconduct extensive experiments over several real datasets from computer vision\ndomain to show the efficiency of the proposed techniques. \n\n"}
{"id": "1611.06453", "contents": "Title: Fast Video Classification via Adaptive Cascading of Deep Models Abstract: Recent advances have enabled \"oracle\" classifiers that can classify across\nmany classes and input distributions with high accuracy without retraining.\nHowever, these classifiers are relatively heavyweight, so that applying them to\nclassify video is costly. We show that day-to-day video exhibits highly skewed\nclass distributions over the short term, and that these distributions can be\nclassified by much simpler models. We formulate the problem of detecting the\nshort-term skews online and exploiting models based on it as a new sequential\ndecision making problem dubbed the Online Bandit Problem, and present a new\nalgorithm to solve it. When applied to recognizing faces in TV shows and\nmovies, we realize end-to-end classification speedups of 2.4-7.8x/2.6-11.2x (on\nGPU/CPU) relative to a state-of-the-art convolutional neural network, at\ncompetitive accuracy. \n\n"}
{"id": "1611.07245", "contents": "Title: Single-View and Multi-View Depth Fusion Abstract: Dense and accurate 3D mapping from a monocular sequence is a key technology\nfor several applications and still an open research area. This paper leverages\nrecent results on single-view CNN-based depth estimation and fuses them with\nmulti-view depth estimation. Both approaches present complementary strengths.\nMulti-view depth is highly accurate but only in high-texture areas and\nhigh-parallax cases. Single-view depth captures the local structure of\nmid-level regions, including texture-less areas, but the estimated depth lacks\nglobal coherence. The single and multi-view fusion we propose is challenging in\nseveral aspects. First, both depths are related by a deformation that depends\non the image content. Second, the selection of multi-view points of high\naccuracy might be difficult for low-parallax configurations. We present\ncontributions for both problems. Our results in the public datasets of NYUv2\nand TUM shows that our algorithm outperforms the individual single and\nmulti-view approaches. A video showing the key aspects of mapping in our Single\nand Multi-view depth proposal is available at https://youtu.be/ipc5HukTb4k \n\n"}
{"id": "1611.07485", "contents": "Title: Scene Labeling using Gated Recurrent Units with Explicit Long Range\n  Conditioning Abstract: Recurrent neural network (RNN), as a powerful contextual dependency modeling\nframework, has been widely applied to scene labeling problems. However, this\nwork shows that directly applying traditional RNN architectures, which unfolds\na 2D lattice grid into a sequence, is not sufficient to model structure\ndependencies in images due to the \"impact vanishing\" problem. First, we give an\nempirical analysis about the \"impact vanishing\" problem. Then, a new RNN unit\nnamed Recurrent Neural Network with explicit long range conditioning (RNN-ELC)\nis designed to alleviate this problem. A novel neural network architecture is\nbuilt for scene labeling tasks where one of the variants of the new RNN unit,\nGated Recurrent Unit with Explicit Long-range Conditioning (GRU-ELC), is used\nto model multi scale contextual dependencies in images. We validate the use of\nGRU-ELC units with state-of-the-art performance on three standard scene\nlabeling datasets. Comprehensive experiments demonstrate that the new GRU-ELC\nunit benefits scene labeling problem a lot as it can encode longer contextual\ndependencies in images more effectively than traditional RNN units. \n\n"}
{"id": "1611.07715", "contents": "Title: Deep Feature Flow for Video Recognition Abstract: Deep convolutional neutral networks have achieved great success on image\nrecognition tasks. Yet, it is non-trivial to transfer the state-of-the-art\nimage recognition networks to videos as per-frame evaluation is too slow and\nunaffordable. We present deep feature flow, a fast and accurate framework for\nvideo recognition. It runs the expensive convolutional sub-network only on\nsparse key frames and propagates their deep feature maps to other frames via a\nflow field. It achieves significant speedup as flow computation is relatively\nfast. The end-to-end training of the whole architecture significantly boosts\nthe recognition accuracy. Deep feature flow is flexible and general. It is\nvalidated on two recent large scale video datasets. It makes a large step\ntowards practical video recognition. \n\n"}
{"id": "1611.08002", "contents": "Title: Semantic Compositional Networks for Visual Captioning Abstract: A Semantic Compositional Network (SCN) is developed for image captioning, in\nwhich semantic concepts (i.e., tags) are detected from the image, and the\nprobability of each tag is used to compose the parameters in a long short-term\nmemory (LSTM) network. The SCN extends each weight matrix of the LSTM to an\nensemble of tag-dependent weight matrices. The degree to which each member of\nthe ensemble is used to generate an image caption is tied to the\nimage-dependent probability of the corresponding tag. In addition to captioning\nimages, we also extend the SCN to generate captions for video clips. We\nqualitatively analyze semantic composition in SCNs, and quantitatively evaluate\nthe algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text.\nExperimental results show that the proposed method significantly outperforms\nprior state-of-the-art approaches, across multiple evaluation metrics. \n\n"}
{"id": "1611.08657", "contents": "Title: Convolutional Experts Constrained Local Model for Facial Landmark\n  Detection Abstract: Constrained Local Models (CLMs) are a well-established family of methods for\nfacial landmark detection. However, they have recently fallen out of favor to\ncascaded regression-based approaches. This is in part due to the inability of\nexisting CLM local detectors to model the very complex individual landmark\nappearance that is affected by expression, illumination, facial hair, makeup,\nand accessories. In our work, we present a novel local detector --\nConvolutional Experts Network (CEN) -- that brings together the advantages of\nneural architectures and mixtures of experts in an end-to-end framework. We\nfurther propose a Convolutional Experts Constrained Local Model (CE-CLM)\nalgorithm that uses CEN as local detectors. We demonstrate that our proposed\nCE-CLM algorithm outperforms competitive state-of-the-art baselines for facial\nlandmark detection by a large margin on four publicly-available datasets. Our\napproach is especially accurate and robust on challenging profile images. \n\n"}
{"id": "1611.08788", "contents": "Title: SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial\n  Networks Abstract: Autonomous driving is one of the most recent topics of interest which is\naimed at replicating human driving behavior keeping in mind the safety issues.\nWe approach the problem of learning synthetic driving using generative neural\nnetworks. The main idea is to make a controller trainer network using images\nplus key press data to mimic human learning. We used the architecture of a\nstable GAN to make predictions between driving scenes using key presses. We\ntrain our model on one video game (Road Rash) and tested the accuracy and\ncompared it by running the model on other maps in Road Rash to determine the\nextent of learning. \n\n"}
{"id": "1611.09051", "contents": "Title: Deep, Dense, and Low-Rank Gaussian Conditional Random Fields Abstract: In this work we introduce a fully-connected graph structure in the Deep\nGaussian Conditional Random Field (G-CRF) model. For this we express the\npairwise interactions between pixels as the inner-products of low-dimensional\nembeddings, delivered by a new subnetwork of a deep architecture. We\nefficiently minimize the resulting energy by solving the resulting low-rank\nlinear system with conjugate gradients, and derive an analytic expression for\nthe gradient of our embeddings which allows us to train them end-to-end with\nbackpropagation.\n  We demonstrate the merit of our approach by achieving state of the art\nresults on three challenging Computer Vision benchmarks, namely semantic\nsegmentation, human parts segmentation, and saliency estimation. Our\nimplementation is fully GPU based, built on top of the Caffe library, and will\nbe made publicly available. \n\n"}
{"id": "1611.09325", "contents": "Title: What Is Around The Camera? Abstract: How much does a single image reveal about the environment it was taken in? In\nthis paper, we investigate how much of that information can be retrieved from a\nforeground object, combined with the background (i.e. the visible part of the\nenvironment). Assuming it is not perfectly diffuse, the foreground object acts\nas a complexly shaped and far-from-perfect mirror. An additional challenge is\nthat its appearance confounds the light coming from the environment with the\nunknown materials it is made of. We propose a learning-based approach to\npredict the environment from multiple reflectance maps that are computed from\napproximate surface normals. The proposed method allows us to jointly model the\nstatistics of environments and material properties. We train our system from\nsynthesized training data, but demonstrate its applicability to real-world\ndata. Interestingly, our analysis shows that the information obtained from\nobjects made out of multiple materials often is complementary and leads to\nbetter performance. \n\n"}
{"id": "1611.09587", "contents": "Title: Surveillance Video Parsing with Single Frame Supervision Abstract: Surveillance video parsing, which segments the video frames into several\nlabels, e.g., face, pants, left-leg, has wide applications.\nHowever,pixel-wisely annotating all frames is tedious and inefficient. In this\npaper, we develop a Single frame Video Parsing (SVP) method which requires only\none labeled frame per video in training stage. To parse one particular frame,\nthe video segment preceding the frame is jointly considered. SVP (1) roughly\nparses the frames within the video segment, (2) estimates the optical flow\nbetween frames and (3) fuses the rough parsing results warped by optical flow\nto produce the refined parsing result. The three components of SVP, namely\nframe parsing, optical flow estimation and temporal fusion are integrated in an\nend-to-end manner. Experimental results on two surveillance video datasets show\nthe superiority of SVP over state-of-the-arts. \n\n"}
{"id": "1611.09969", "contents": "Title: High-Resolution Image Inpainting using Multi-Scale Neural Patch\n  Synthesis Abstract: Recent advances in deep learning have shown exciting promise in filling large\nholes in natural images with semantically plausible and context aware details,\nimpacting fundamental image manipulation tasks such as object removal. While\nthese learning-based methods are significantly more effective in capturing\nhigh-level features than prior techniques, they can only handle very\nlow-resolution inputs due to memory limitations and difficulty in training.\nEven for slightly larger images, the inpainted regions would appear blurry and\nunpleasant boundaries become visible. We propose a multi-scale neural patch\nsynthesis approach based on joint optimization of image content and texture\nconstraints, which not only preserves contextual structures but also produces\nhigh-frequency details by matching and adapting patches with the most similar\nmid-layer feature correlations of a deep classification network. We evaluate\nour method on the ImageNet and Paris Streetview datasets and achieved\nstate-of-the-art inpainting accuracy. We show our approach produces sharper and\nmore coherent results than prior methods, especially for high-resolution\nimages. \n\n"}
{"id": "1612.00005", "contents": "Title: Plug & Play Generative Networks: Conditional Iterative Generation of\n  Images in Latent Space Abstract: Generating high-resolution, photo-realistic images has been a long-standing\ngoal in machine learning. Recently, Nguyen et al. (2016) showed one interesting\nway to synthesize novel images by performing gradient ascent in the latent\nspace of a generator network to maximize the activations of one or multiple\nneurons in a separate classifier network. In this paper we extend this method\nby introducing an additional prior on the latent code, improving both sample\nquality and sample diversity, leading to a state-of-the-art generative model\nthat produces high quality images at higher resolutions (227x227) than previous\ngenerative models, and does so for all 1000 ImageNet categories. In addition,\nwe provide a unified probabilistic interpretation of related activation\nmaximization methods and call the general class of models \"Plug and Play\nGenerative Networks\". PPGNs are composed of 1) a generator network G that is\ncapable of drawing a wide range of image types and 2) a replaceable \"condition\"\nnetwork C that tells the generator what to draw. We demonstrate the generation\nof images conditioned on a class (when C is an ImageNet or MIT Places\nclassification network) and also conditioned on a caption (when C is an image\ncaptioning network). Our method also improves the state of the art of\nMultifaceted Feature Visualization, which generates the set of synthetic inputs\nthat activate a neuron in order to better understand how deep neural networks\noperate. Finally, we show that our model performs reasonably well at the task\nof image inpainting. While image models are used in this paper, the approach is\nmodality-agnostic and can be applied to many types of data. \n\n"}
{"id": "1612.00940", "contents": "Title: End-to-end learning of brain tissue segmentation from imperfect labeling Abstract: Segmenting a structural magnetic resonance imaging (MRI) scan is an important\npre-processing step for analytic procedures and subsequent inferences about\nlongitudinal tissue changes. Manual segmentation defines the current gold\nstandard in quality but is prohibitively expensive. Automatic approaches are\ncomputationally intensive, incredibly slow at scale, and error prone due to\nusually involving many potentially faulty intermediate steps. In order to\nstreamline the segmentation, we introduce a deep learning model that is based\non volumetric dilated convolutions, subsequently reducing both processing time\nand errors. Compared to its competitors, the model has a reduced set of\nparameters and thus is easier to train and much faster to execute. The contrast\nin performance between the dilated network and its competitors becomes obvious\nwhen both are tested on a large dataset of unprocessed human brain volumes. The\ndilated network consistently outperforms not only another state-of-the-art deep\nlearning approach, the up convolutional network, but also the ground truth on\nwhich it was trained. Not only can the incredible speed of our model make large\nscale analyses much easier but we also believe it has great potential in a\nclinical setting where, with little to no substantial delay, a patient and\nprovider can go over test results. \n\n"}
{"id": "1612.00992", "contents": "Title: Mining Spatio-temporal Data on Industrialization from Historical\n  Registries Abstract: Despite the growing availability of big data in many fields, historical data\non socioevironmental phenomena are often not available due to a lack of\nautomated and scalable approaches for collecting, digitizing, and assembling\nthem. We have developed a data-mining method for extracting tabulated, geocoded\ndata from printed directories. While scanning and optical character recognition\n(OCR) can digitize printed text, these methods alone do not capture the\nstructure of the underlying data. Our pipeline integrates both page layout\nanalysis and OCR to extract tabular, geocoded data from structured text. We\ndemonstrate the utility of this method by applying it to scanned manufacturing\nregistries from Rhode Island that record 41 years of industrial land use. The\nresulting spatio-temporal data can be used for socioenvironmental analyses of\nindustrialization at a resolution that was not previously possible. In\nparticular, we find strong evidence for the dispersion of manufacturing from\nthe urban core of Providence, the state's capital, along the Interstate 95\ncorridor to the north and south. \n\n"}
{"id": "1612.01601", "contents": "Title: Superpixels: An Evaluation of the State-of-the-Art Abstract: Superpixels group perceptually similar pixels to create visually meaningful\nentities while heavily reducing the number of primitives for subsequent\nprocessing steps. As of these properties, superpixel algorithms have received\nmuch attention since their naming in 2003. By today, publicly available\nsuperpixel algorithms have turned into standard tools in low-level vision. As\nsuch, and due to their quick adoption in a wide range of applications,\nappropriate benchmarks are crucial for algorithm selection and comparison.\nUntil now, the rapidly growing number of algorithms as well as varying\nexperimental setups hindered the development of a unifying benchmark. We\npresent a comprehensive evaluation of 28 state-of-the-art superpixel algorithms\nutilizing a benchmark focussing on fair comparison and designed to provide new\ninsights relevant for applications. To this end, we explicitly discuss\nparameter optimization and the importance of strictly enforcing connectivity.\nFurthermore, by extending well-known metrics, we are able to summarize\nalgorithm performance independent of the number of generated superpixels,\nthereby overcoming a major limitation of available benchmarks. Furthermore, we\ndiscuss runtime, robustness against noise, blur and affine transformations,\nimplementation details as well as aspects of visual quality. Finally, we\npresent an overall ranking of superpixel algorithms which redefines the\nstate-of-the-art and enables researchers to easily select appropriate\nalgorithms and the corresponding implementations which themselves are made\npublicly available as part of our benchmark at\ndavidstutz.de/projects/superpixel-benchmark/. \n\n"}
{"id": "1612.01928", "contents": "Title: Invariant Representations for Noisy Speech Recognition Abstract: Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training. \n\n"}
{"id": "1612.02575", "contents": "Title: Filter sharing: Efficient learning of parameters for volumetric\n  convolutions Abstract: Typical convolutional neural networks (CNNs) have several millions of\nparameters and require a large amount of annotated data to train them. In\nmedical applications where training data is hard to come by, these\nsophisticated machine learning models are difficult to train. In this paper, we\npropose a method to reduce the inherent complexity of CNNs during training by\nexploiting the significant redundancy that is noticed in the learnt CNN\nfilters. Our method relies on finding a small set of filters and mixing\ncoefficients to derive every filter in each convolutional layer at the time of\ntraining itself, thereby reducing the number of parameters to be trained. We\nconsider the problem of 3D lung nodule segmentation in CT images and\ndemonstrate the effectiveness of our method in achieving good results with only\nfew training examples. \n\n"}
{"id": "1612.02709", "contents": "Title: Predicting Ground-Level Scene Layout from Aerial Imagery Abstract: We introduce a novel strategy for learning to extract semantically meaningful\nfeatures from aerial imagery. Instead of manually labeling the aerial imagery,\nwe propose to predict (noisy) semantic features automatically extracted from\nco-located ground imagery. Our network architecture takes an aerial image as\ninput, extracts features using a convolutional neural network, and then applies\nan adaptive transformation to map these features into the ground-level\nperspective. We use an end-to-end learning approach to minimize the difference\nbetween the semantic segmentation extracted directly from the ground image and\nthe semantic segmentation predicted solely based on the aerial image. We show\nthat a model learned using this strategy, with no additional training, is\nalready capable of rough semantic labeling of aerial imagery. Furthermore, we\ndemonstrate that by finetuning this model we can achieve more accurate semantic\nsegmentation than two baseline initialization strategies. We use our network to\naddress the task of estimating the geolocation and geoorientation of a ground\nimage. Finally, we show how features extracted from an aerial image can be used\nto hallucinate a plausible ground-level panorama. \n\n"}
{"id": "1612.02954", "contents": "Title: A series of maximum entropy upper bounds of the differential entropy Abstract: We present a series of closed-form maximum entropy upper bounds for the\ndifferential entropy of a continuous univariate random variable and study the\nproperties of that series. We then show how to use those generic bounds for\nupper bounding the differential entropy of Gaussian mixture models. This\nrequires to calculate the raw moments and raw absolute moments of Gaussian\nmixtures in closed-form that may also be handy in statistical machine learning\nand information theory. We report on our experiments and discuss on the\ntightness of those bounds. \n\n"}
{"id": "1612.04526", "contents": "Title: Astronomical image reconstruction with convolutional neural networks Abstract: State of the art methods in astronomical image reconstruction rely on the\nresolution of a regularized or constrained optimization problem. Solving this\nproblem can be computationally intensive and usually leads to a quadratic or at\nleast superlinear complexity w.r.t. the number of pixels in the image. We\ninvestigate in this work the use of convolutional neural networks for image\nreconstruction in astronomy. With neural networks, the computationally\nintensive tasks is the training step, but the prediction step has a fixed\ncomplexity per pixel, i.e. a linear complexity. Numerical experiments show that\nour approach is both computationally efficient and competitive with other state\nof the art methods in addition to being interpretable. \n\n"}
{"id": "1612.04647", "contents": "Title: UnrealStereo: Controlling Hazardous Factors to Analyze Stereo Vision Abstract: A reliable stereo algorithm is critical for many robotics applications. But\ntextureless and specular regions can easily cause failure by making feature\nmatching difficult. Understanding whether an algorithm is robust to these\nhazardous regions is important. Although many stereo benchmarks have been\ndeveloped to evaluate performance, it is hard to quantify the effect of\nhazardous regions in real images because the location and severity of these\nregions are unknown. In this paper, we develop a synthetic image generation\ntool enabling to control hazardous factors, such as making objects more\nspecular or transparent, to produce hazardous regions at different degrees. The\ndensely controlled sampling strategy in virtual worlds enables to effectively\nstress test stereo algorithms by varying the types and degrees of the hazard.\nWe generate a large synthetic image dataset with automatically computed\nhazardous regions and analyze algorithms on these regions. The observations\nfrom synthetic images are further validated by annotating hazardous regions in\nreal-world datasets Middlebury and KITTI (which gives a sparse sampling of the\nhazards). Our synthetic image generation tool is based on a game engine Unreal\nEngine 4 and will be open-source along with the virtual scenes in our\nexperiments. Many publicly available realistic game contents can be used by our\ntool to provide an enormous resource for development and evaluation of\nalgorithms. \n\n"}
{"id": "1612.05050", "contents": "Title: Towards Score Following in Sheet Music Images Abstract: This paper addresses the matching of short music audio snippets to the\ncorresponding pixel location in images of sheet music. A system is presented\nthat simultaneously learns to read notes, listens to music and matches the\ncurrently played music to its corresponding notes in the sheet. It consists of\nan end-to-end multi-modal convolutional neural network that takes as input\nimages of sheet music and spectrograms of the respective audio snippets. It\nlearns to predict, for a given unseen audio snippet (covering approximately one\nbar of music), the corresponding position in the respective score line. Our\nresults suggest that with the use of (deep) neural networks -- which have\nproven to be powerful image processing models -- working with sheet music\nbecomes feasible and a promising future research direction. \n\n"}
{"id": "1612.05086", "contents": "Title: Coupling Adaptive Batch Sizes with Learning Rates Abstract: Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available. \n\n"}
{"id": "1612.05234", "contents": "Title: Visual Compiler: Synthesizing a Scene-Specific Pedestrian Detector and\n  Pose Estimator Abstract: We introduce the concept of a Visual Compiler that generates a scene specific\npedestrian detector and pose estimator without any pedestrian observations.\nGiven a single image and auxiliary scene information in the form of camera\nparameters and geometric layout of the scene, the Visual Compiler first infers\ngeometrically and photometrically accurate images of humans in that scene\nthrough the use of computer graphics rendering. Using these renders we learn a\nscene-and-region specific spatially-varying fully convolutional neural network,\nfor simultaneous detection, pose estimation and segmentation of pedestrians. We\ndemonstrate that when real human annotated data is scarce or non-existent, our\ndata generation strategy can provide an excellent solution for bootstrapping\nhuman detection and pose estimation. Experimental results show that our\napproach outperforms off-the-shelf state-of-the-art pedestrian detectors and\npose estimators that are trained on real data. \n\n"}
{"id": "1612.05441", "contents": "Title: A Message Passing Algorithm for the Minimum Cost Multicut Problem Abstract: We propose a dual decomposition and linear program relaxation of the NP -hard\nminimum cost multicut problem. Unlike other polyhedral relaxations of the\nmulticut polytope, it is amenable to efficient optimization by message passing.\nLike other polyhedral elaxations, it can be tightened efficiently by cutting\nplanes. We define an algorithm that alternates between message passing and\nefficient separation of cycle- and odd-wheel inequalities. This algorithm is\nmore efficient than state-of-the-art algorithms based on linear programming,\nincluding algorithms written in the framework of leading commercial software,\nas we show in experiments with large instances of the problem from applications\nin computer vision, biomedical image analysis and data mining. \n\n"}
{"id": "1612.06851", "contents": "Title: Beyond Skip Connections: Top-Down Modulation for Object Detection Abstract: In recent years, we have seen tremendous progress in the field of object\ndetection. Most of the recent improvements have been achieved by targeting\ndeeper feedforward networks. However, many hard object categories such as\nbottle, remote, etc. require representation of fine details and not just\ncoarse, semantic representations. But most of these fine details are lost in\nthe early convolutional layers. What we need is a way to incorporate finer\ndetails from lower layers into the detection architecture. Skip connections\nhave been proposed to combine high-level and low-level features, but we argue\nthat selecting the right features from low-level requires top-down contextual\ninformation. Inspired by the human visual pathway, in this paper we propose\ntop-down modulations as a way to incorporate fine details into the detection\nframework. Our approach supplements the standard bottom-up, feedforward ConvNet\nwith a top-down modulation (TDM) network, connected using lateral connections.\nThese connections are responsible for the modulation of lower layer filters,\nand the top-down network handles the selection and integration of contextual\ninformation and low-level features. The proposed TDM architecture provides a\nsignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,\n35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any\nbells and whistles (e.g., multi-scale, iterative box refinement, etc.). \n\n"}
{"id": "1612.09346", "contents": "Title: Rotation equivariant vector field networks Abstract: In many computer vision tasks, we expect a particular behavior of the output\nwith respect to rotations of the input image. If this relationship is\nexplicitly encoded, instead of treated as any other variation, the complexity\nof the problem is decreased, leading to a reduction in the size of the required\nmodel. In this paper, we propose the Rotation Equivariant Vector Field Networks\n(RotEqNet), a Convolutional Neural Network (CNN) architecture encoding rotation\nequivariance, invariance and covariance. Each convolutional filter is applied\nat multiple orientations and returns a vector field representing magnitude and\nangle of the highest scoring orientation at every spatial location. We develop\na modified convolution operator relying on this representation to obtain deep\narchitectures. We test RotEqNet on several problems requiring different\nresponses with respect to the inputs' rotation: image classification,\nbiomedical image segmentation, orientation estimation and patch matching. In\nall cases, we show that RotEqNet offers extremely compact models in terms of\nnumber of parameters and provides results in line to those of networks orders\nof magnitude larger. \n\n"}
{"id": "1701.02477", "contents": "Title: Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic\n  Speech Recognition Abstract: Multi-task learning (MTL) involves the simultaneous training of two or more\nrelated tasks over shared representations. In this work, we apply MTL to\naudio-visual automatic speech recognition(AV-ASR). Our primary task is to learn\na mapping between audio-visual fused features and frame labels obtained from\nacoustic GMM/HMM model. This is combined with an auxiliary task which maps\nvisual features to frame labels obtained from a separate visual GMM/HMM model.\nThe MTL model is tested at various levels of babble noise and the results are\ncompared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate\nthat MTL is especially useful at higher level of noise. Compared to base-line,\nupto 7\\% relative improvement in WER is reported at -3 SNR dB \n\n"}
{"id": "1701.02870", "contents": "Title: Context-aware Captions from Context-agnostic Supervision Abstract: We introduce an inference technique to produce discriminative context-aware\nimage captions (captions that describe differences between images or visual\nconcepts) using only generic context-agnostic training data (captions that\ndescribe a concept or an image in isolation). For example, given images and\ncaptions of \"siamese cat\" and \"tiger cat\", we generate language that describes\nthe \"siamese cat\" in a way that distinguishes it from \"tiger cat\". Our key\nnovelty is that we show how to do joint inference over a language model that is\ncontext-agnostic and a listener which distinguishes closely-related concepts.\nWe first apply our technique to a justification task, namely to describe why an\nimage contains a particular fine-grained category as opposed to another\nclosely-related category of the CUB-200-2011 dataset. We then study\ndiscriminative image captioning to generate language that uniquely refers to\none of two semantically-similar images in the COCO dataset. Evaluations with\ndiscriminative ground truth for justification and human studies for\ndiscriminative image captioning reveal that our approach outperforms baseline\ngenerative and speaker-listener approaches for discrimination. \n\n"}
{"id": "1701.03151", "contents": "Title: Guaranteed Parameter Estimation for Discrete Energy Minimization Abstract: Structural learning, a method to estimate the parameters for discrete energy\nminimization, has been proven to be effective in solving computer vision\nproblems, especially in 3D scene parsing. As the complexity of the models\nincreases, structural learning algorithms turn to approximate inference to\nretain tractability. Unfortunately, such methods often fail because the\napproximation can be arbitrarily poor. In this work, we propose a method to\novercome this limitation through exploiting the properties of the joint problem\nof training time inference and learning. With the help of the learning\nframework, we transform the inapproximable inference problem into a polynomial\ntime solvable one, thereby enabling tractable exact inference while still\nallowing an arbitrary graph structure and full potential interactions. Our\nlearning algorithm is guaranteed to return a solution with a bounded error to\nthe global optimal within the feasible parameter space. We demonstrate the\neffectiveness of this method on two point cloud scene parsing datasets. Our\napproach runs much faster and solves a problem that is intractable for\nprevious, well-known approaches. \n\n"}
{"id": "1701.03551", "contents": "Title: Cost-Effective Active Learning for Deep Image Classification Abstract: Recent successes in learning-based image classification, however, heavily\nrely on the large number of annotated training samples, which may require\nconsiderable human efforts. In this paper, we propose a novel active learning\nframework, which is capable of building a competitive classifier with optimal\nfeature representation via a limited amount of labeled training instances in an\nincremental learning manner. Our approach advances the existing active learning\nmethods in two aspects. First, we incorporate deep convolutional neural\nnetworks into active learning. Through the properly designed framework, the\nfeature representation and the classifier can be simultaneously updated with\nprogressively annotated informative samples. Second, we present a\ncost-effective sample selection strategy to improve the classification\nperformance with less manual annotations. Unlike traditional methods focusing\non only the uncertain samples of low prediction confidence, we especially\ndiscover the large amount of high confidence samples from the unlabeled set for\nfeature learning. Specifically, these high confidence samples are automatically\nselected and iteratively assigned pseudo-labels. We thus call our framework\n\"Cost-Effective Active Learning\" (CEAL) standing for the two\nadvantages.Extensive experiments demonstrate that the proposed CEAL framework\ncan achieve promising results on two challenging image classification datasets,\ni.e., face recognition on CACD database [1] and object categorization on\nCaltech-256 [2]. \n\n"}
{"id": "1701.03916", "contents": "Title: On H\\\"older projective divergences Abstract: We describe a framework to build distances by measuring the tightness of\ninequalities, and introduce the notion of proper statistical divergences and\nimproper pseudo-divergences. We then consider the H\\\"older ordinary and reverse\ninequalities, and present two novel classes of H\\\"older divergences and\npseudo-divergences that both encapsulate the special case of the Cauchy-Schwarz\ndivergence. We report closed-form formulas for those statistical\ndissimilarities when considering distributions belonging to the same\nexponential family provided that the natural parameter space is a cone (e.g.,\nmultivariate Gaussians), or affine (e.g., categorical distributions). Those new\nclasses of H\\\"older distances are invariant to rescaling, and thus do not\nrequire distributions to be normalized. Finally, we show how to compute\nstatistical H\\\"older centroids with respect to those divergences, and carry out\ncenter-based clustering toy experiments on a set of Gaussian distributions that\ndemonstrate empirically that symmetrized H\\\"older divergences outperform the\nsymmetric Cauchy-Schwarz divergence. \n\n"}
{"id": "1701.04249", "contents": "Title: Geometric features for voxel-based surface recognition Abstract: We introduce a library of geometric voxel features for CAD surface\nrecognition/retrieval tasks. Our features include local versions of the\nintrinsic volumes (the usual 3D volume, surface area, integrated mean and\nGaussian curvature) and a few closely related quantities. We also compute Haar\nwavelet and statistical distribution features by aggregating raw voxel\nfeatures. We apply our features to object classification on the ESB data set\nand demonstrate accurate results with a small number of shallow decision trees. \n\n"}
{"id": "1702.01005", "contents": "Title: Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear\n  Subspace Learning Abstract: Principal Component Analysis (PCA) and Kernel Principal Component Analysis\n(KPCA) are fundamental methods in machine learning for dimensionality\nreduction. The former is a technique for finding this approximation in finite\ndimensions and the latter is often in an infinite dimensional Reproducing\nKernel Hilbert-space (RKHS). In this paper, we present a geometric framework\nfor computing the principal linear subspaces in both situations as well as for\nthe robust PCA case, that amounts to computing the intrinsic average on the\nspace of all subspaces: the Grassmann manifold. Points on this manifold are\ndefined as the subspaces spanned by $K$-tuples of observations. The intrinsic\nGrassmann average of these subspaces are shown to coincide with the principal\ncomponents of the observations when they are drawn from a Gaussian\ndistribution. We show similar results in the RKHS case and provide an efficient\nalgorithm for computing the projection onto the this average subspace. The\nresult is a method akin to KPCA which is substantially faster. Further, we\npresent a novel online version of the KPCA using our geometric framework.\nCompetitive performance of all our algorithms are demonstrated on a variety of\nreal and synthetic data sets. \n\n"}
{"id": "1702.02549", "contents": "Title: Backpropagation Training for Fisher Vectors within Neural Networks Abstract: Fisher-Vectors (FV) encode higher-order statistics of a set of multiple local\ndescriptors like SIFT features. They already show good performance in\ncombination with shallow learning architectures on visual recognitions tasks.\nCurrent methods using FV as a feature descriptor in deep architectures assume\nthat all original input features are static. We propose a framework to jointly\nlearn the representation of original features, FV parameters and parameters of\nthe classifier in the style of traditional neural networks. Our proof of\nconcept implementation improves the performance of FV on the Pascal Voc 2007\nchallenge in a multi-GPU setting in comparison to a default SVM setting. We\ndemonstrate that FV can be embedded into neural networks at arbitrary\npositions, allowing end-to-end training with back-propagation. \n\n"}
{"id": "1702.04114", "contents": "Title: Graph Based Over-Segmentation Methods for 3D Point Clouds Abstract: Over-segmentation, or super-pixel generation, is a common preliminary stage\nfor many computer vision applications. New acquisition technologies enable the\ncapturing of 3D point clouds that contain color and geometrical information.\nThis 3D information introduces a new conceptual change that can be utilized to\nimprove the results of over-segmentation, which uses mainly color information,\nand to generate clusters of points we call super-points. We consider a variety\nof possible 3D extensions of the Local Variation (LV) graph based\nover-segmentation algorithms, and compare them thoroughly. We consider\ndifferent alternatives for constructing the connectivity graph, for assigning\nthe edge weights, and for defining the merge criterion, which must now account\nfor the geometric information and not only color. Following this evaluation, we\nderive a new generic algorithm for over-segmentation of 3D point clouds. We\ncall this new algorithm Point Cloud Local Variation (PCLV). The advantages of\nthe new over-segmentation algorithm are demonstrated on both outdoor and\ncluttered indoor scenes. Performance analysis of the proposed approach compared\nto state-of-the-art 2D and 3D over-segmentation algorithms shows significant\nimprovement according to the common performance measures. \n\n"}
{"id": "1702.04179", "contents": "Title: Structured Deep Hashing with Convolutional Neural Networks for Fast\n  Person Re-identification Abstract: Given a pedestrian image as a query, the purpose of person re-identification\nis to identify the correct match from a large collection of gallery images\ndepicting the same person captured by disjoint camera views. The critical\nchallenge is how to construct a robust yet discriminative feature\nrepresentation to capture the compounded variations in pedestrian appearance.\nTo this end, deep learning methods have been proposed to extract hierarchical\nfeatures against extreme variability of appearance. However, existing methods\nin this category generally neglect the efficiency in the matching stage whereas\nthe searching speed of a re-identification system is crucial in real-world\napplications. In this paper, we present a novel deep hashing framework with\nConvolutional Neural Networks (CNNs) for fast person re-identification.\nTechnically, we simultaneously learn both CNN features and hash functions/codes\nto get robust yet discriminative features and similarity-preserving hash codes.\nThereby, person re-identification can be resolved by efficiently computing and\nranking the Hamming distances between images. A structured loss function\ndefined over positive pairs and hard negatives is proposed to formulate a novel\noptimization problem so that fast convergence and more stable optimized\nsolution can be obtained. Extensive experiments on two benchmarks CUHK03\n\\cite{FPNN} and Market-1501 \\cite{Market1501} show that the proposed deep\narchitecture is efficacy over state-of-the-arts. \n\n"}
{"id": "1702.04267", "contents": "Title: On Detecting Adversarial Perturbations Abstract: Machine learning and deep learning in particular has advanced tremendously on\nperceptual tasks in recent years. However, it remains vulnerable against\nadversarial perturbations of the input that have been crafted specifically to\nfool the system while being quasi-imperceptible to a human. In this work, we\npropose to augment deep neural networks with a small \"detector\" subnetwork\nwhich is trained on the binary classification task of distinguishing genuine\ndata from data containing adversarial perturbations. Our method is orthogonal\nto prior work on addressing adversarial perturbations, which has mostly focused\non making the classification network itself more robust. We show empirically\nthat adversarial perturbations can be detected surprisingly well even though\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\ntrained to detect only a specific adversary, they generalize to similar and\nweaker adversaries. In addition, we propose an adversarial attack that fools\nboth the classifier and the detector and a novel training procedure for the\ndetector that counteracts this attack. \n\n"}
{"id": "1702.04869", "contents": "Title: Improving automated multiple sclerosis lesion segmentation with a\n  cascaded 3D convolutional neural network approach Abstract: In this paper, we present a novel automated method for White Matter (WM)\nlesion segmentation of Multiple Sclerosis (MS) patient images. Our approach is\nbased on a cascade of two 3D patch-wise convolutional neural networks (CNN).\nThe first network is trained to be more sensitive revealing possible candidate\nlesion voxels while the second network is trained to reduce the number of\nmisclassified voxels coming from the first network. This cascaded CNN\narchitecture tends to learn well from small sets of training data, which can be\nvery interesting in practice, given the difficulty to obtain manual label\nannotations and the large amount of available unlabeled Magnetic Resonance\nImaging (MRI) data. We evaluate the accuracy of the proposed method on the\npublic MS lesion segmentation challenge MICCAI2008 dataset, comparing it with\nrespect to other state-of-the-art MS lesion segmentation tools. Furthermore,\nthe proposed method is also evaluated on two private MS clinical datasets,\nwhere the performance of our method is also compared with different recent\npublic available state-of-the-art MS lesion segmentation methods. At the time\nof writing this paper, our method is the best ranked approach on the MICCAI2008\nchallenge, outperforming the rest of 60 participant methods when using all the\navailable input modalities (T1-w, T2-w and FLAIR), while still in the top-rank\n(3rd position) when using only T1-w and FLAIR modalities. On clinical MS data,\nour approach exhibits a significant increase in the accuracy segmenting of WM\nlesions when compared with the rest of evaluated methods, highly correlating\n($r \\ge 0.97$) also with the expected lesion volume. \n\n"}
{"id": "1702.05089", "contents": "Title: Improving Text Proposals for Scene Images with Fully Convolutional\n  Networks Abstract: Text Proposals have emerged as a class-dependent version of object proposals\n- efficient approaches to reduce the search space of possible text object\nlocations in an image. Combined with strong word classifiers, text proposals\ncurrently yield top state of the art results in end-to-end scene text\nrecognition. In this paper we propose an improvement over the original Text\nProposals algorithm of Gomez and Karatzas (2016), combining it with Fully\nConvolutional Networks to improve the ranking of proposals. Results on the\nICDAR RRC and the COCO-text datasets show superior performance over current\nstate-of-the-art. \n\n"}
{"id": "1702.05596", "contents": "Title: Brain Inspired Cognitive Model with Attention for Self-Driving Cars Abstract: Perception-driven approach and end-to-end system are two major vision-based\nframeworks for self-driving cars. However, it is difficult to introduce\nattention and historical information of autonomous driving process, which are\nthe essential factors for achieving human-like driving into these two methods.\nIn this paper, we propose a novel model for self-driving cars named\nbrain-inspired cognitive model with attention (CMA). This model consists of\nthree parts: a convolutional neural network for simulating human visual cortex,\na cognitive map built to describe relationships between objects in complex\ntraffic scene and a recurrent neural network that combines with the real-time\nupdated cognitive map to implement attention mechanism and long-short term\nmemory. The benefit of our model is that can accurately solve three tasks\nsimultaneously:1) detection of the free space and boundaries of the current and\nadjacent lanes. 2)estimation of obstacle distance and vehicle attitude, and 3)\nlearning of driving behavior and decision making from human driver. More\nsignificantly, the proposed model could accept external navigating instructions\nduring an end-to-end driving process. For evaluation, we build a large-scale\nroad-vehicle dataset which contains more than forty thousand labeled road\nimages captured by three cameras on our self-driving car. Moreover, human\ndriving activities and vehicle states are recorded in the meanwhile. \n\n"}
{"id": "1702.07963", "contents": "Title: Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning\n  Techniques Abstract: In this paper, we proposed using a hybrid method that utilises deep\nconvolutional and recurrent neural networks for accurate delineation of skin\nlesion of images supplied with ISBI 2017 lesion segmentation challenge. The\nproposed method was trained using 1800 images and tested on 150 images from\nISBI 2017 challenge. \n\n"}
{"id": "1702.08319", "contents": "Title: Visual Translation Embedding Network for Visual Relation Detection Abstract: Visual relations, such as \"person ride bike\" and \"bike next to car\", offer a\ncomprehensive scene understanding of an image, and have already shown their\ngreat utility in connecting computer vision and natural language. However, due\nto the challenging combinatorial complexity of modeling\nsubject-predicate-object relation triplets, very little work has been done to\nlocalize and predict visual relations. Inspired by the recent advances in\nrelational representation learning of knowledge bases and convolutional object\ndetection networks, we propose a Visual Translation Embedding network (VTransE)\nfor visual relation detection. VTransE places objects in a low-dimensional\nrelation space where a relation can be modeled as a simple vector translation,\ni.e., subject + predicate $\\approx$ object. We propose a novel feature\nextraction layer that enables object-relation knowledge transfer in a\nfully-convolutional fashion that supports training and inference in a single\nforward/backward pass. To the best of our knowledge, VTransE is the first\nend-to-end relation detection network. We demonstrate the effectiveness of\nVTransE over other state-of-the-art methods on two large-scale datasets: Visual\nRelationship and Visual Genome. Note that even though VTransE is a purely\nvisual model, it is still competitive to the Lu's multi-modal model with\nlanguage priors. \n\n"}
{"id": "1702.08608", "contents": "Title: Towards A Rigorous Science of Interpretable Machine Learning Abstract: As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning. \n\n"}
{"id": "1702.08646", "contents": "Title: Boundary Flow: A Siamese Network that Predicts Boundary Motion without\n  Training on Motion Abstract: Using deep learning, this paper addresses the problem of joint object\nboundary detection and boundary motion estimation in videos, which we named\nboundary flow estimation. Boundary flow is an important mid-level visual cue as\nboundaries characterize objects spatial extents, and the flow indicates objects\nmotions and interactions. Yet, most prior work on motion estimation has focused\non dense object motion or feature points that may not necessarily reside on\nboundaries. For boundary flow estimation, we specify a new fully convolutional\nSiamese network (FCSN) that jointly estimates object-level boundaries in two\nconsecutive frames. Boundary correspondences in the two frames are predicted by\nthe same FCSN with a new, unconventional deconvolution approach. Finally, the\nboundary flow estimate is improved with an edgelet-based filtering. Evaluation\nis conducted on three tasks: boundary detection in videos, boundary flow\nestimation, and optical flow estimation. On boundary detection, we achieve the\nstate-of-the-art performance on the benchmark VSB100 dataset. On boundary flow\nestimation, we present the first results on the Sintel training dataset. For\noptical flow estimation, we run the recent approach CPMFlow but on the\naugmented input with our boundary-flow matches, and achieve significant\nperformance improvement on the Sintel benchmark. \n\n"}
{"id": "1702.08891", "contents": "Title: Predicting Slice-to-Volume Transformation in Presence of Arbitrary\n  Subject Motion Abstract: This paper aims to solve a fundamental problem in intensity-based 2D/3D\nregistration, which concerns the limited capture range and need for very good\ninitialization of state-of-the-art image registration methods. We propose a\nregression approach that learns to predict rotation and translations of\narbitrary 2D image slices from 3D volumes, with respect to a learned canonical\natlas co-ordinate system. To this end, we utilize Convolutional Neural Networks\n(CNNs) to learn the highly complex regression function that maps 2D image\nslices into their correct position and orientation in 3D space. Our approach is\nattractive in challenging imaging scenarios, where significant subject motion\ncomplicates reconstruction performance of 3D volumes from 2D slice data. We\nextensively evaluate the effectiveness of our approach quantitatively on\nsimulated MRI brain data with extreme random motion. We further demonstrate\nqualitative results on fetal MRI where our method is integrated into a full\nreconstruction and motion compensation pipeline. With our CNN regression\napproach we obtain an average prediction error of 7mm on simulated data, and\nconvincing reconstruction quality of images of very young fetuses where\nprevious methods fail. We further discuss applications to Computed Tomography\nand X-ray projections. Our approach is a general solution to the 2D/3D\ninitialization problem. It is computationally efficient, with prediction times\nper slice of a few milliseconds, making it suitable for real-time scenarios. \n\n"}
{"id": "1703.00792", "contents": "Title: Robust Spatial Filtering with Graph Convolutional Neural Networks Abstract: Convolutional Neural Networks (CNNs) have recently led to incredible\nbreakthroughs on a variety of pattern recognition problems. Banks of finite\nimpulse response filters are learned on a hierarchy of layers, each\ncontributing more abstract information than the previous layer. The simplicity\nand elegance of the convolutional filtering process makes them perfect for\nstructured problems such as image, video, or voice, where vertices are\nhomogeneous in the sense of number, location, and strength of neighbors. The\nvast majority of classification problems, for example in the pharmaceutical,\nhomeland security, and financial domains are unstructured. As these problems\nare formulated into unstructured graphs, the heterogeneity of these problems,\nsuch as number of vertices, number of connections per vertex, and edge\nstrength, cannot be tackled with standard convolutional techniques. We propose\na novel neural learning framework that is capable of handling both homogeneous\nand heterogeneous data, while retaining the benefits of traditional CNN\nsuccesses.\n  Recently, researchers have proposed variations of CNNs that can handle graph\ndata. In an effort to create learnable filter banks of graphs, these methods\neither induce constraints on the data or require preprocessing. As opposed to\nspectral methods, our framework, which we term Graph-CNNs, defines filters as\npolynomials of functions of the graph adjacency matrix. Graph-CNNs can handle\nboth heterogeneous and homogeneous graph data, including graphs having entirely\ndifferent vertex or edge sets. We perform experiments to validate the\napplicability of Graph-CNNs to a variety of structured and unstructured\nclassification problems and demonstrate state-of-the-art results on document\nand molecule classification problems. \n\n"}
{"id": "1703.01040", "contents": "Title: Learning Robot Activities from First-Person Human Videos Using\n  Convolutional Future Regression Abstract: We design a new approach that allows robot learning of new activities from\nunlabeled human example videos. Given videos of humans executing the same\nactivity from a human's viewpoint (i.e., first-person videos), our objective is\nto make the robot learn the temporal structure of the activity as its future\nregression network, and learn to transfer such model for its own motor\nexecution. We present a new deep learning model: We extend the state-of-the-art\nconvolutional object detection network for the representation/estimation of\nhuman hands in training videos, and newly introduce the concept of using a\nfully convolutional network to regress (i.e., predict) the intermediate scene\nrepresentation corresponding to the future frame (e.g., 1-2 seconds later).\nCombining these allows direct prediction of future locations of human hands and\nobjects, which enables the robot to infer the motor control plan using our\nmanipulation network. We experimentally confirm that our approach makes\nlearning of robot activities from unlabeled human interaction videos possible,\nand demonstrate that our robot is able to execute the learned collaborative\nactivities in real-time directly based on its camera input. \n\n"}
{"id": "1703.01365", "contents": "Title: Axiomatic Attribution for Deep Networks Abstract: We study the problem of attributing the prediction of a deep network to its\ninput features, a problem previously studied by several other works. We\nidentify two fundamental axioms---Sensitivity and Implementation Invariance\nthat attribution methods ought to satisfy. We show that they are not satisfied\nby most known attribution methods, which we consider to be a fundamental\nweakness of those methods. We use the axioms to guide the design of a new\nattribution method called Integrated Gradients. Our method requires no\nmodification to the original network and is extremely simple to implement; it\njust needs a few calls to the standard gradient operator. We apply this method\nto a couple of image models, a couple of text models and a chemistry model,\ndemonstrating its ability to debug networks, to extract rules from a network,\nand to enable users to engage with models better. \n\n"}
{"id": "1703.02291", "contents": "Title: Triple Generative Adversarial Nets Abstract: Generative Adversarial Nets (GANs) have shown promise in image generation and\nsemi-supervised learning (SSL). However, existing GANs in SSL have two\nproblems: (1) the generator and the discriminator (i.e. the classifier) may not\nbe optimal at the same time; and (2) the generator cannot control the semantics\nof the generated samples. The problems essentially arise from the two-player\nformulation, where a single discriminator shares incompatible roles of\nidentifying fake samples and predicting labels and it only estimates the data\nwithout considering the labels. To address the problems, we present triple\ngenerative adversarial net (Triple-GAN), which consists of three players---a\ngenerator, a discriminator and a classifier. The generator and the classifier\ncharacterize the conditional distributions between images and labels, and the\ndiscriminator solely focuses on identifying fake image-label pairs. We design\ncompatible utilities to ensure that the distributions characterized by the\nclassifier and the generator both converge to the data distribution. Our\nresults on various datasets demonstrate that Triple-GAN as a unified model can\nsimultaneously (1) achieve the state-of-the-art classification results among\ndeep generative models, and (2) disentangle the classes and styles of the input\nand transfer smoothly in the data space via interpolation in the latent space\nclass-conditionally. \n\n"}
{"id": "1703.02437", "contents": "Title: PathTrack: Fast Trajectory Annotation with Path Supervision Abstract: Progress in Multiple Object Tracking (MOT) has been historically limited by\nthe size of the available datasets. We present an efficient framework to\nannotate trajectories and use it to produce a MOT dataset of unprecedented\nsize. In our novel path supervision the annotator loosely follows the object\nwith the cursor while watching the video, providing a path annotation for each\nobject in the sequence. Our approach is able to turn such weak annotations into\ndense box trajectories. Our experiments on existing datasets prove that our\nframework produces more accurate annotations than the state of the art, in a\nfraction of the time. We further validate our approach by crowdsourcing the\nPathTrack dataset, with more than 15,000 person trajectories in 720 sequences.\nTracking approaches can benefit training on such large-scale datasets, as did\nobject recognition. We prove this by re-training an off-the-shelf person\nmatching network, originally trained on the MOT15 dataset, almost halving the\nmisclassification rate. Additionally, training on our data consistently\nimproves tracking results, both on our dataset and on MOT15. On the latter, we\nimprove the top-performing tracker (NOMT) dropping the number of IDSwitches by\n18% and fragments by 5%. \n\n"}
{"id": "1703.02442", "contents": "Title: Detecting Cancer Metastases on Gigapixel Pathology Images Abstract: Each year, the treatment decisions for more than 230,000 breast cancer\npatients in the U.S. hinge on whether the cancer has metastasized away from the\nbreast. Metastasis detection is currently performed by pathologists reviewing\nlarge expanses of biological tissues. This process is labor intensive and\nerror-prone. We present a framework to automatically detect and localize tumors\nas small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x\n100,000 pixels. Our method leverages a convolutional neural network (CNN)\narchitecture and obtains state-of-the-art results on the Camelyon16 dataset in\nthe challenging lesion-level tumor detection task. At 8 false positives per\nimage, we detect 92.4% of the tumors, relative to 82.7% by the previous best\nautomated approach. For comparison, a human pathologist attempting exhaustive\nsearch achieved 73.2% sensitivity. We achieve image-level AUC scores above 97%\non both the Camelyon16 test set and an independent set of 110 slides. In\naddition, we discover that two slides in the Camelyon16 training set were\nerroneously labeled normal. Our approach could considerably reduce false\nnegative rates in metastasis detection. \n\n"}
{"id": "1703.02563", "contents": "Title: Flow Fields: Dense Correspondence Fields for Highly Accurate Large\n  Displacement Optical Flow Estimation Abstract: Modern large displacement optical flow algorithms usually use an\ninitialization by either sparse descriptor matching techniques or dense\napproximate nearest neighbor fields. While the latter have the advantage of\nbeing dense, they have the major disadvantage of being very outlier-prone as\nthey are not designed to find the optical flow, but the visually most similar\ncorrespondence. In this article we present a dense correspondence field\napproach that is much less outlier-prone and thus much better suited for\noptical flow estimation than approximate nearest neighbor fields. Our approach\ndoes not require explicit regularization, smoothing (like median filtering) or\na new data term. Instead we solely rely on patch matching techniques and a\nnovel multi-scale matching strategy. We also present enhancements for outlier\nfiltering. We show that our approach is better suited for large displacement\noptical flow estimation than modern descriptor matching techniques. We do so by\ninitializing EpicFlow with our approach instead of their originally used\nstate-of-the-art descriptor matching technique. We significantly outperform the\noriginal EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this\nextended article of our former conference publication we further improve our\napproach in matching accuracy as well as runtime and present more experiments\nand insights. \n\n"}
{"id": "1703.02635", "contents": "Title: A Computational Model of a Single-Photon Avalanche Diode Sensor for\n  Transient Imaging Abstract: Single-Photon Avalanche Diodes (SPAD) are affordable photodetectors, capable\nto collect extremely fast low-energy events, due to their single-photon\nsensibility. This makes them very suitable for time-of-flight-based range\nimaging systems, allowing to reduce costs and power requirements, without\nsacrifizing much temporal resolution. In this work we describe a computational\nmodel to simulate the behaviour of SPAD sensors, aiming to provide a realistic\ncamera model for time-resolved light transport simulation, with applications on\nprototyping new reconstructions techniques based on SPAD time-of-flight data.\nOur model accounts for the major effects of the sensor on the incoming signal.\nWe compare our model against real-world measurements, and apply it to a variety\nof scenarios, including complex multiply-scattered light transport. \n\n"}
{"id": "1703.04071", "contents": "Title: A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification\n  and Domain Adaptation Abstract: Recently, DNN model compression based on network architecture design, e.g.,\nSqueezeNet, attracted a lot attention. No accuracy drop on image classification\nis observed on these extremely compact networks, compared to well-known models.\nAn emerging question, however, is whether these model compression techniques\nhurt DNN's learning ability other than classifying images on a single dataset.\nOur preliminary experiment shows that these compression methods could degrade\ndomain adaptation (DA) ability, though the classification performance is\npreserved. Therefore, we propose a new compact network architecture and\nunsupervised DA method in this paper. The DNN is built on a new basic module\nConv-M which provides more diverse feature extractors without significantly\nincreasing parameters. The unified framework of our DA method will\nsimultaneously learn invariance across domains, reduce divergence of feature\nrepresentations, and adapt label prediction. Our DNN has 4.1M parameters, which\nis only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN\nobtains GoogLeNet-level accuracy both on classification and DA, and our DA\nmethod slightly outperforms previous competitive ones. Put all together, our DA\nstrategy based on our DNN achieves state-of-the-art on sixteen of total\neighteen DA tasks on popular Office-31 and Office-Caltech datasets. \n\n"}
{"id": "1703.04364", "contents": "Title: Deep Learning for Skin Lesion Classification Abstract: Melanoma, a malignant form of skin cancer is very threatening to life.\nDiagnosis of melanoma at an earlier stage is highly needed as it has a very\nhigh cure rate. Benign and malignant forms of skin cancer can be detected by\nanalyzing the lesions present on the surface of the skin using dermoscopic\nimages. In this work, an automated skin lesion detection system has been\ndeveloped which learns the representation of the image using Google's\npretrained CNN model known as Inception-v3 \\cite{cnn}. After obtaining the\nrepresentation vector for our input dermoscopic images we have trained two\nlayer feed forward neural network to classify the images as malignant or\nbenign. The system also classifies the images based on the cause of the cancer\neither due to melanocytic or non-melanocytic cells using a different neural\nnetwork. These classification tasks are part of the challenge organized by\nInternational Skin Imaging Collaboration (ISIC) 2017. Our system learns to\nclassify the images based on the model built using the training images given in\nthe challenge and the experimental results were evaluated using validation and\ntest sets. Our system has achieved an overall accuracy of 65.8\\% for the\nvalidation set. \n\n"}
{"id": "1703.04665", "contents": "Title: Geometry-Based Region Proposals for Real-Time Robot Detection of\n  Tabletop Objects Abstract: We present a novel object detection pipeline for localization and recognition\nin three dimensional environments. Our approach makes use of an RGB-D sensor\nand combines state-of-the-art techniques from the robotics and computer vision\ncommunities to create a robust, real-time detection system. We focus\nspecifically on solving the object detection problem for tabletop scenes, a\ncommon environment for assistive manipulators. Our detection pipeline locates\nobjects in a point cloud representation of the scene. These clusters are\nsubsequently used to compute a bounding box around each object in the RGB\nspace. Each defined patch is then fed into a Convolutional Neural Network (CNN)\nfor object recognition. We also demonstrate that our region proposal method can\nbe used to develop novel datasets that are both large and diverse enough to\ntrain deep learning models, and easy enough to collect that end-users can\ndevelop their own datasets. Lastly, we validate the resulting system through an\nextensive analysis of the accuracy and run-time of the full pipeline. \n\n"}
{"id": "1703.04835", "contents": "Title: A Proximity-Aware Hierarchical Clustering of Faces Abstract: In this paper, we propose an unsupervised face clustering algorithm called\n\"Proximity-Aware Hierarchical Clustering\" (PAHC) that exploits the local\nstructure of deep representations. In the proposed method, a similarity measure\nbetween deep features is computed by evaluating linear SVM margins. SVMs are\ntrained using nearest neighbors of sample data, and thus do not require any\nexternal training data. Clusters are then formed by thresholding the similarity\nscores. We evaluate the clustering performance using three challenging\nunconstrained face datasets, including Celebrity in Frontal-Profile (CFP),\nIARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3)\ndatasets. Experimental results demonstrate that the proposed approach can\nachieve significant improvements over state-of-the-art methods. Moreover, we\nalso show that the proposed clustering algorithm can be applied to curate a set\nof large-scale and noisy training dataset while maintaining sufficient amount\nof images and their variations due to nuisance factors. The face verification\nperformance on JANUS CS3 improves significantly by finetuning a DCNN model with\nthe curated MS-Celeb-1M dataset which contains over three million face images. \n\n"}
{"id": "1703.05128", "contents": "Title: DeepVel: deep learning for the estimation of horizontal velocities at\n  the solar surface Abstract: Many phenomena taking place in the solar photosphere are controlled by plasma\nmotions. Although the line-of-sight component of the velocity can be estimated\nusing the Doppler effect, we do not have direct spectroscopic access to the\ncomponents that are perpendicular to the line-of-sight. These components are\ntypically estimated using methods based on local correlation tracking. We have\ndesigned DeepVel, an end-to-end deep neural network that produces an estimation\nof the velocity at every single pixel and at every time step and at three\ndifferent heights in the atmosphere from just two consecutive continuum images.\nWe confront DeepVel with local correlation tracking, pointing out that they\ngive very similar results in the time- and spatially-averaged cases. We use the\nnetwork to study the evolution in height of the horizontal velocity field in\nfragmenting granules, supporting the buoyancy-braking mechanism for the\nformation of integranular lanes in these granules. We also show that DeepVel\ncan capture very small vortices, so that we can potentially expand the scaling\ncascade of vortices to very small sizes and durations. \n\n"}
{"id": "1703.05593", "contents": "Title: Convolutional neural network architecture for geometric matching Abstract: We address the problem of determining correspondences between two images in\nagreement with a geometric model such as an affine or thin-plate spline\ntransformation, and estimating its parameters. The contributions of this work\nare three-fold. First, we propose a convolutional neural network architecture\nfor geometric matching. The architecture is based on three main components that\nmimic the standard steps of feature extraction, matching and simultaneous\ninlier detection and model parameter estimation, while being trainable\nend-to-end. Second, we demonstrate that the network parameters can be trained\nfrom synthetically generated imagery without the need for manual annotation and\nthat our matching layer significantly increases generalization capabilities to\nnever seen before images. Finally, we show that the same model can perform both\ninstance-level and category-level matching giving state-of-the-art results on\nthe challenging Proposal Flow dataset. \n\n"}
{"id": "1703.06189", "contents": "Title: TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals Abstract: Temporal Action Proposal (TAP) generation is an important problem, as fast\nand accurate extraction of semantically important (e.g. human actions) segments\nfrom untrimmed videos is an important step for large-scale video analysis. We\npropose a novel Temporal Unit Regression Network (TURN) model. There are two\nsalient aspects of TURN: (1) TURN jointly predicts action proposals and refines\nthe temporal boundaries by temporal coordinate regression; (2) Fast computation\nis enabled by unit feature reuse: a long untrimmed video is decomposed into\nvideo units, which are reused as basic building blocks of temporal proposals.\nTURN outperforms the state-of-the-art methods under average recall (AR) by a\nlarge margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames\nper second (FPS) on a TITAN X GPU. We further apply TURN as a proposal\ngeneration stage for existing temporal action localization pipelines, it\noutperforms state-of-the-art performance on THUMOS-14 and ActivityNet. \n\n"}
{"id": "1703.07131", "contents": "Title: Knowledge distillation using unlabeled mismatched images Abstract: Current approaches for Knowledge Distillation (KD) either directly use\ntraining data or sample from the training data distribution. In this paper, we\ndemonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for\nimage classification networks. For illustration, we consider scenarios where\nthis is a complete absence of training data, or mismatched stimulus has to be\nused for augmenting a small amount of training data. We demonstrate that\nstimulus complexity is a key factor for distillation's good performance. Our\nexamples include use of various datasets for stimulating MNIST and CIFAR\nteachers. \n\n"}
{"id": "1703.07330", "contents": "Title: License Plate Detection and Recognition Using Deeply Learned\n  Convolutional Neural Networks Abstract: This work details Sighthounds fully automated license plate detection and\nrecognition system. The core technology of the system is built using a sequence\nof deep Convolutional Neural Networks (CNNs) interlaced with accurate and\nefficient algorithms. The CNNs are trained and fine-tuned so that they are\nrobust under different conditions (e.g. variations in pose, lighting,\nocclusion, etc.) and can work across a variety of license plate templates (e.g.\nsizes, backgrounds, fonts, etc). For quantitative analysis, we show that our\nsystem outperforms the leading license plate detection and recognition\ntechnology i.e. ALPR on several benchmarks. Our system is available to\ndevelopers through the Sighthound Cloud API at\nhttps://www.sighthound.com/products/cloud \n\n"}
{"id": "1703.07469", "contents": "Title: RobustFill: Neural Program Learning under Noisy I/O Abstract: The problem of automatically generating a computer program from some\nspecification has been studied since the early days of AI. Recently, two\ncompeting approaches for automatic program learning have received significant\nattention: (1) neural program synthesis, where a neural network is conditioned\non input/output (I/O) examples and learns to generate a program, and (2) neural\nprogram induction, where a neural network generates new outputs directly using\na latent program representation.\n  Here, for the first time, we directly compare both approaches on a\nlarge-scale, real-world learning task. We additionally contrast to rule-based\nprogram synthesis, which uses hand-crafted semantics to guide the program\ngeneration. Our neural models use a modified attention RNN to allow encoding of\nvariable-sized sets of I/O pairs. Our best synthesis model achieves 92%\naccuracy on a real-world test set, compared to the 34% accuracy of the previous\nbest neural synthesis approach. The synthesis model also outperforms a\ncomparable induction model on this task, but we more importantly demonstrate\nthat the strength of each approach is highly dependent on the evaluation metric\nand end-user application. Finally, we show that we can train our neural models\nto remain very robust to the type of noise expected in real-world data (e.g.,\ntypos), while a highly-engineered rule-based system fails entirely. \n\n"}
{"id": "1703.07834", "contents": "Title: Large Pose 3D Face Reconstruction from a Single Image via Direct\n  Volumetric CNN Regression Abstract: 3D face reconstruction is a fundamental Computer Vision problem of\nextraordinary difficulty. Current systems often assume the availability of\nmultiple facial images (sometimes from the same subject) as input, and must\naddress a number of methodological challenges such as establishing dense\ncorrespondences across large facial poses, expressions, and non-uniform\nillumination. In general these methods require complex and inefficient\npipelines for model building and fitting. In this work, we propose to address\nmany of these limitations by training a Convolutional Neural Network (CNN) on\nan appropriate dataset consisting of 2D images and 3D facial models or scans.\nOur CNN works with just a single 2D facial image, does not require accurate\nalignment nor establishes dense correspondence between images, works for\narbitrary facial poses and expressions, and can be used to reconstruct the\nwhole 3D facial geometry (including the non-visible parts of the face)\nbypassing the construction (during training) and fitting (during testing) of a\n3D Morphable Model. We achieve this via a simple CNN architecture that performs\ndirect regression of a volumetric representation of the 3D facial geometry from\na single 2D image. We also demonstrate how the related task of facial landmark\nlocalization can be incorporated into the proposed framework and help improve\nreconstruction quality, especially for the cases of large poses and facial\nexpressions. Testing code will be made available online, along with pre-trained\nmodels http://aaronsplace.co.uk/papers/jackson2017recon \n\n"}
{"id": "1703.08132", "contents": "Title: Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling Abstract: We present an approach for weakly supervised learning of human actions. Given\na set of videos and an ordered list of the occurring actions, the goal is to\ninfer start and end frames of the related action classes within the video and\nto train the respective action classifiers without any need for hand labeled\nframe boundaries. To address this task, we propose a combination of a\ndiscriminative representation of subactions, modeled by a recurrent neural\nnetwork, and a coarse probabilistic model to allow for a temporal alignment and\ninference over long sequences. While this system alone already generates good\nresults, we show that the performance can be further improved by approximating\nthe number of subactions to the characteristics of the different action\nclasses. To this end, we adapt the number of subaction classes by iterating\nrealignment and reestimation during training. The proposed system is evaluated\non two benchmark datasets, the Breakfast and the Hollywood extended dataset,\nshowing a competitive performance on various weak learning tasks such as\ntemporal action segmentation and action alignment. \n\n"}
{"id": "1703.09554", "contents": "Title: Lucid Data Dreaming for Video Object Segmentation Abstract: Convolutional networks reach top quality in pixel-level video object\nsegmentation but require a large amount of training data (1k~100k) to deliver\nsuch results. We propose a new training strategy which achieves\nstate-of-the-art results across three evaluation datasets while using 20x~1000x\nless annotated data than competing methods. Our approach is suitable for both\nsingle and multiple object segmentation. Instead of using large training sets\nhoping to generalize across domains, we generate in-domain training data using\nthe provided annotation on the first frame of each video to synthesize (\"lucid\ndream\") plausible future video frames. In-domain per-video training data allows\nus to train high quality appearance- and motion-based models, as well as tune\nthe post-processing stage. This approach allows to reach competitive results\neven when training from only a single annotated frame, without ImageNet\npre-training. Our results indicate that using a larger training set is not\nautomatically better, and that for the video object segmentation task a smaller\ntraining set that is closer to the target domain is more effective. This\nchanges the mindset regarding how many training samples and general\n\"objectness\" knowledge are required for the video object segmentation task. \n\n"}
{"id": "1703.10239", "contents": "Title: SeGAN: Segmenting and Generating the Invisible Abstract: Objects often occlude each other in scenes; Inferring their appearance beyond\ntheir visible parts plays an important role in scene understanding, depth\nestimation, object interaction and manipulation. In this paper, we study the\nchallenging problem of completing the appearance of occluded objects. Doing so\nrequires knowing which pixels to paint (segmenting the invisible parts of\nobjects) and what color to paint them (generating the invisible parts). Our\nproposed novel solution, SeGAN, jointly optimizes for both segmentation and\ngeneration of the invisible parts of objects. Our experimental results show\nthat: (a) SeGAN can learn to generate the appearance of the occluded parts of\nobjects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the\ninvisible parts of objects; (c) trained on synthetic photo realistic images,\nSeGAN can reliably segment natural images; (d) by reasoning about occluder\noccludee relations, our method can infer depth layering. \n\n"}
{"id": "1703.10553", "contents": "Title: Learning Convolutional Networks for Content-weighted Image Compression Abstract: Lossy image compression is generally formulated as a joint rate-distortion\noptimization to learn encoder, quantizer, and decoder. However, the quantizer\nis non-differentiable, and discrete entropy estimation usually is required for\nrate control. These make it very challenging to develop a convolutional network\n(CNN)-based image compression system. In this paper, motivated by that the\nlocal information content is spatially variant in an image, we suggest that the\nbit rate of the different parts of the image should be adapted to local\ncontent. And the content aware bit rate is allocated under the guidance of a\ncontent-weighted importance map. Thus, the sum of the importance map can serve\nas a continuous alternative of discrete entropy estimation to control\ncompression rate. And binarizer is adopted to quantize the output of encoder\ndue to the binarization scheme is also directly defined by the importance map.\nFurthermore, a proxy function is introduced for binary operation in backward\npropagation to make it differentiable. Therefore, the encoder, decoder,\nbinarizer and importance map can be jointly optimized in an end-to-end manner\nby using a subset of the ImageNet database. In low bit rate image compression,\nexperiments show that our system significantly outperforms JPEG and JPEG 2000\nby structural similarity (SSIM) index, and can produce the much better visual\nresult with sharp edges, rich textures, and fewer artifacts. \n\n"}
{"id": "1703.10902", "contents": "Title: Fast Predictive Multimodal Image Registration Abstract: We introduce a deep encoder-decoder architecture for image deformation\nprediction from multimodal images. Specifically, we design an image-patch-based\ndeep network that jointly (i) learns an image similarity measure and (ii) the\nrelationship between image patches and deformation parameters. While our method\ncan be applied to general image registration formulations, we focus on the\nLarge Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By\npredicting the initial momentum of the shooting formulation of LDDMM, we\npreserve its mathematical properties and drastically reduce the computation\ntime, compared to optimization-based approaches. Furthermore, we create a\nBayesian probabilistic version of the network that allows evaluation of\nregistration uncertainty via sampling of the network at test time. We evaluate\nour method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our\nexperiments show that our method generates accurate predictions and that\nlearning the similarity measure leads to more consistent registrations than\nrelying on generic multimodal image similarity measures, such as mutual\ninformation. Our approach is an order of magnitude faster than\noptimization-based LDDMM. \n\n"}
{"id": "1703.10908", "contents": "Title: Quicksilver: Fast Predictive Image Registration - a Deep Learning\n  Approach Abstract: This paper introduces Quicksilver, a fast deformable image registration\nmethod. Quicksilver registration for image-pairs works by patch-wise prediction\nof a deformation model based directly on image appearance. A deep\nencoder-decoder network is used as the prediction model. While the prediction\nstrategy is general, we focus on predictions for the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) model. Specifically, we predict the\nmomentum-parameterization of LDDMM, which facilitates a patch-wise prediction\nstrategy while maintaining the theoretical properties of LDDMM, such as\nguaranteed diffeomorphic mappings for sufficiently strong regularization. We\nalso provide a probabilistic version of our prediction network which can be\nsampled during the testing time to calculate uncertainties in the predicted\ndeformations. Finally, we introduce a new correction network which greatly\nincreases the prediction accuracy of an already existing prediction network. We\nshow experimental results for uni-modal atlas-to-image as well as uni- / multi-\nmodal image-to-image registrations. These experiments demonstrate that our\nmethod accurately predicts registrations obtained by numerical optimization, is\nvery fast, achieves state-of-the-art registration results on four standard\nvalidation datasets, and can jointly learn an image similarity measure.\nQuicksilver is freely available as an open-source software. \n\n"}
{"id": "1704.00260", "contents": "Title: Aligned Image-Word Representations Improve Inductive Transfer Across\n  Vision-Language Tasks Abstract: An important goal of computer vision is to build systems that learn visual\nrepresentations over time that can be applied to many tasks. In this paper, we\ninvestigate a vision-language embedding as a core representation and show that\nit leads to better cross-task transfer than standard multi-task learning. In\nparticular, the task of visual recognition is aligned to the task of visual\nquestion answering by forcing each to use the same word-region embeddings. We\nshow this leads to greater inductive transfer from recognition to VQA than\nstandard multitask learning. Visual recognition also improves, especially for\ncategories that have relatively few recognition training labels but appear\noften in the VQA setting. Thus, our paper takes a small step towards creating\nmore general vision systems by showing the benefit of interpretable, flexible,\nand trainable core representations. \n\n"}
{"id": "1704.00389", "contents": "Title: Hidden Two-Stream Convolutional Networks for Action Recognition Abstract: Analyzing videos of human actions involves understanding the temporal\nrelationships among video frames. State-of-the-art action recognition\napproaches rely on traditional optical flow estimation methods to pre-compute\nmotion information for CNNs. Such a two-stage approach is computationally\nexpensive, storage demanding, and not end-to-end trainable. In this paper, we\npresent a novel CNN architecture that implicitly captures motion information\nbetween adjacent frames. We name our approach hidden two-stream CNNs because it\nonly takes raw video frames as input and directly predicts action classes\nwithout explicitly computing optical flow. Our end-to-end approach is 10x\nfaster than its two-stage baseline. Experimental results on four challenging\naction recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show\nthat our approach significantly outperforms the previous best real-time\napproaches. \n\n"}
{"id": "1704.00509", "contents": "Title: Truncating Wide Networks using Binary Tree Architectures Abstract: Recent study shows that a wide deep network can obtain accuracy comparable to\na deeper but narrower network. Compared to narrower and deeper networks, wide\nnetworks employ relatively less number of layers and have various important\nbenefits, such that they have less running time on parallel computing devices,\nand they are less affected by gradient vanishing problems. However, the\nparameter size of a wide network can be very large due to use of large width of\neach layer in the network. In order to keep the benefits of wide networks\nmeanwhile improve the parameter size and accuracy trade-off of wide networks,\nwe propose a binary tree architecture to truncate architecture of wide networks\nby reducing the width of the networks. More precisely, in the proposed\narchitecture, the width is continuously reduced from lower layers to higher\nlayers in order to increase the expressive capacity of network with a less\nincrease on parameter size. Also, to ease the gradient vanishing problem,\nfeatures obtained at different layers are concatenated to form the output of\nour architecture. By employing the proposed architecture on a baseline wide\nnetwork, we can construct and train a new network with same depth but\nconsiderably less number of parameters. In our experimental analyses, we\nobserve that the proposed architecture enables us to obtain better parameter\nsize and accuracy trade-off compared to baseline networks using various\nbenchmark image classification datasets. The results show that our model can\ndecrease the classification error of baseline from 20.43% to 19.22% on\nCifar-100 using only 28% of parameters that baseline has. Code is available at\nhttps://github.com/ZhangVision/bitnet. \n\n"}
{"id": "1704.00642", "contents": "Title: Local nearest neighbour classification with applications to\n  semi-supervised learning Abstract: We derive a new asymptotic expansion for the global excess risk of a\nlocal-$k$-nearest neighbour classifier, where the choice of $k$ may depend upon\nthe test point. This expansion elucidates conditions under which the dominant\ncontribution to the excess risk comes from the decision boundary of the optimal\nBayes classifier, but we also show that if these conditions are not satisfied,\nthen the dominant contribution may arise from the tails of the marginal\ndistribution of the features. Moreover, we prove that, provided the\n$d$-dimensional marginal distribution of the features has a finite $\\rho$th\nmoment for some $\\rho > 4$ (as well as other regularity conditions), a local\nchoice of $k$ can yield a rate of convergence of the excess risk of\n$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard\n$k$-nearest neighbour classifier, our theory would require $d \\geq 5$ and $\\rho\n> 4d/(d-4)$ finite moments to achieve this rate. These results motivate a new\n$k$-nearest neighbour classifier for semi-supervised learning problems, where\nthe unlabelled data are used to obtain an estimate of the marginal feature\ndensity, and fewer neighbours are used for classification when this density\nestimate is small. Our worst-case rates are complemented by a minimax lower\nbound, which reveals that the local, semi-supervised $k$-nearest neighbour\nclassifier attains the minimax optimal rate over our classes for the excess\nrisk, up to a subpolynomial factor in $n$. These theoretical improvements over\nthe standard $k$-nearest neighbour classifier are also illustrated through a\nsimulation study. \n\n"}
{"id": "1704.00717", "contents": "Title: It Takes Two to Tango: Towards Theory of AI's Mind Abstract: Theory of Mind is the ability to attribute mental states (beliefs, intents,\nknowledge, perspectives, etc.) to others and recognize that these mental states\nmay differ from one's own. Theory of Mind is critical to effective\ncommunication and to teams demonstrating higher collective performance. To\neffectively leverage the progress in Artificial Intelligence (AI) to make our\nlives more productive, it is important for humans and AI to work well together\nin a team. Traditionally, there has been much emphasis on research to make AI\nmore accurate, and (to a lesser extent) on having it better understand human\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\nmore human-like and having it develop a theory of our minds. In this work, we\nargue that for human-AI teams to be effective, humans must also develop a\ntheory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,\nand quirks. We instantiate these ideas within the domain of Visual Question\nAnswering (VQA). We find that using just a few examples (50), lay people can be\ntrained to better predict responses and oncoming failures of a complex VQA\nmodel. We further evaluate the role existing explanation (or interpretability)\nmodalities play in helping humans build ToAIM. Explainable AI has received\nconsiderable scientific and popular attention in recent times. Surprisingly, we\nfind that having access to the model's internal states - its confidence in its\ntop-k predictions, explicit or implicit attention maps which highlight regions\nin the image (and words in the question) the model is looking at (and listening\nto) while answering a question about an image - do not help people better\npredict its behavior. \n\n"}
{"id": "1704.00758", "contents": "Title: Unsupervised Action Proposal Ranking through Proposal Recombination Abstract: Recently, action proposal methods have played an important role in action\nrecognition tasks, as they reduce the search space dramatically. Most\nunsupervised action proposal methods tend to generate hundreds of action\nproposals which include many noisy, inconsistent, and unranked action\nproposals, while supervised action proposal methods take advantage of\npredefined object detectors (e.g., human detector) to refine and score the\naction proposals, but they require thousands of manual annotations to train.\n  Given the action proposals in a video, the goal of the proposed work is to\ngenerate a few better action proposals that are ranked properly. In our\napproach, we first divide action proposal into sub-proposal and then use\nDynamic Programming based graph optimization scheme to select the optimal\ncombinations of sub-proposals from different proposals and assign each new\nproposal a score. We propose a new unsupervised image-based actioness detector\nthat leverages web images and employs it as one of the node scores in our graph\nformulation. Moreover, we capture motion information by estimating the number\nof motion contours within each action proposal patch. The proposed method is an\nunsupervised method that neither needs bounding box annotations nor video level\nlabels, which is desirable with the current explosion of large-scale action\ndatasets. Our approach is generic and does not depend on a specific action\nproposal method. We evaluate our approach on several publicly available trimmed\nand un-trimmed datasets and obtain better performance compared to several\nproposal ranking methods. In addition, we demonstrate that properly ranked\nproposals produce significantly better action detection as compared to\nstate-of-the-art proposal based methods. \n\n"}
{"id": "1704.01155", "contents": "Title: Feature Squeezing: Detecting Adversarial Examples in Deep Neural\n  Networks Abstract: Although deep neural networks (DNNs) have achieved great success in many\ntasks, they can often be fooled by \\emph{adversarial examples} that are\ngenerated by adding small but purposeful distortions to natural examples.\nPrevious studies to defend against adversarial examples mostly focused on\nrefining the DNN models, but have either shown limited success or required\nexpensive computation. We propose a new strategy, \\emph{feature squeezing},\nthat can be used to harden DNN models by detecting adversarial examples.\nFeature squeezing reduces the search space available to an adversary by\ncoalescing samples that correspond to many different feature vectors in the\noriginal space into a single sample. By comparing a DNN model's prediction on\nthe original input with that on squeezed inputs, feature squeezing detects\nadversarial examples with high accuracy and few false positives. This paper\nexplores two feature squeezing methods: reducing the color bit depth of each\npixel and spatial smoothing. These simple strategies are inexpensive and\ncomplementary to other defenses, and can be combined in a joint detection\nframework to achieve high detection rates against state-of-the-art attacks. \n\n"}
{"id": "1704.01248", "contents": "Title: A Computational Approach to Relative Aesthetics Abstract: Computational visual aesthetics has recently become an active research area.\nExisting state-of-art methods formulate this as a binary classification task\nwhere a given image is predicted to be beautiful or not. In many applications\nsuch as image retrieval and enhancement, it is more important to rank images\nbased on their aesthetic quality instead of binary-categorizing them.\nFurthermore, in such applications, it may be possible that all images belong to\nthe same category. Hence determining the aesthetic ranking of the images is\nmore appropriate. To this end, we formulate a novel problem of ranking images\nwith respect to their aesthetic quality. We construct a new dataset of image\npairs with relative labels by carefully selecting images from the popular AVA\ndataset. Unlike in aesthetics classification, there is no single threshold\nwhich would determine the ranking order of the images across our entire\ndataset. We propose a deep neural network based approach that is trained on\nimage pairs by incorporating principles from relative learning. Results show\nthat such relative training procedure allows our network to rank the images\nwith a higher accuracy than a state-of-art network trained on the same set of\nimages using binary labels. \n\n"}
{"id": "1704.01444", "contents": "Title: Learning to Generate Reviews and Discovering Sentiment Abstract: We explore the properties of byte-level recurrent language models. When given\nsufficient amounts of capacity, training data, and compute time, the\nrepresentations learned by these models include disentangled features\ncorresponding to high-level concepts. Specifically, we find a single unit which\nperforms sentiment analysis. These representations, learned in an unsupervised\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\nTreebank. They are also very data efficient. When using only a handful of\nlabeled examples, our approach matches the performance of strong baselines\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\ninfluence on the generative process of the model. Simply fixing its value to be\npositive or negative generates samples with the corresponding positive or\nnegative sentiment. \n\n"}
{"id": "1704.01716", "contents": "Title: Action Representation Using Classifier Decision Boundaries Abstract: Most popular deep learning based models for action recognition are designed\nto generate separate predictions within their short temporal windows, which are\noften aggregated by heuristic means to assign an action label to the full video\nsegment. Given that not all frames from a video characterize the underlying\naction, pooling schemes that impose equal importance to all frames might be\nunfavorable. In an attempt towards tackling this challenge, we propose a novel\npooling scheme, dubbed SVM pooling, based on the notion that among the bag of\nfeatures generated by a CNN on all temporal windows, there is at least one\nfeature that characterizes the action. To this end, we learn a decision\nhyperplane that separates this unknown yet useful feature from the rest.\nApplying multiple instance learning in an SVM setup, we use the parameters of\nthis separating hyperplane as a descriptor for the video. Since these\nparameters are directly related to the support vectors in a max-margin\nframework, they serve as robust representations for pooling of the CNN\nfeatures. We devise a joint optimization objective and an efficient solver that\nlearns these hyperplanes per video and the corresponding action classifiers\nover the hyperplanes. Showcased experiments on the standard HMDB and UCF101\ndatasets demonstrate state-of-the-art performance. \n\n"}
{"id": "1704.02304", "contents": "Title: It Takes (Only) Two: Adversarial Generator-Encoder Networks Abstract: We present a new autoencoder-type architecture that is trainable in an\nunsupervised mode, sustains both generation and inference, and has the quality\nof conditional and unconditional samples boosted by adversarial learning.\nUnlike previous hybrids of autoencoders and adversarial networks, the\nadversarial game in our approach is set up directly between the encoder and the\ngenerator, and no external mappings are trained in the process of learning. The\ngame objective compares the divergences of each of the real and the generated\ndata distributions with the prior distribution in the latent space. We show\nthat direct generator-vs-encoder game leads to a tight coupling of the two\ncomponents, resulting in samples and reconstructions of a comparable quality to\nsome recently-proposed more complex architectures. \n\n"}
{"id": "1704.02422", "contents": "Title: A Deep Cascade of Convolutional Neural Networks for Dynamic MR Image\n  Reconstruction Abstract: Inspired by recent advances in deep learning, we propose a framework for\nreconstructing dynamic sequences of 2D cardiac magnetic resonance (MR) images\nfrom undersampled data using a deep cascade of convolutional neural networks\n(CNNs) to accelerate the data acquisition process. In particular, we address\nthe case where data is acquired using aggressive Cartesian undersampling.\nFirstly, we show that when each 2D image frame is reconstructed independently,\nthe proposed method outperforms state-of-the-art 2D compressed sensing\napproaches such as dictionary learning-based MR image reconstruction, in terms\nof reconstruction error and reconstruction speed. Secondly, when reconstructing\nthe frames of the sequences jointly, we demonstrate that CNNs can learn\nspatio-temporal correlations efficiently by combining convolution and data\nsharing approaches. We show that the proposed method consistently outperforms\nstate-of-the-art methods and is capable of preserving anatomical structure more\nfaithfully up to 11-fold undersampling. Moreover, reconstruction is very fast:\neach complete dynamic sequence can be reconstructed in less than 10s and, for\nthe 2D case, each image frame can be reconstructed in 23ms, enabling real-time\napplications. \n\n"}
{"id": "1704.03058", "contents": "Title: CERN: Confidence-Energy Recurrent Network for Group Activity Recognition Abstract: This work is about recognizing human activities occurring in videos at\ndistinct semantic levels, including individual actions, interactions, and group\nactivities. The recognition is realized using a two-level hierarchy of Long\nShort-Term Memory (LSTM) networks, forming a feed-forward deep architecture,\nwhich can be trained end-to-end. In comparison with existing architectures of\nLSTMs, we make two key contributions giving the name to our approach as\nConfidence-Energy Recurrent Network -- CERN. First, instead of using the common\nsoftmax layer for prediction, we specify a novel energy layer (EL) for\nestimating the energy of our predictions. Second, rather than finding the\ncommon minimum-energy class assignment, which may be numerically unstable under\nuncertainty, we specify that the EL additionally computes the p-values of the\nsolutions, and in this way estimates the most confident energy minimum. The\nevaluation on the Collective Activity and Volleyball datasets demonstrates: (i)\nadvantages of our two contributions relative to the common softmax and\nenergy-minimization formulations and (ii) a superior performance relative to\nthe state-of-the-art approaches. \n\n"}
{"id": "1704.03114", "contents": "Title: Detecting Visual Relationships with Deep Relational Networks Abstract: Relationships among objects play a crucial role in image understanding.\nDespite the great success of deep learning techniques in recognizing individual\nobjects, reasoning about the relationships among objects remains a challenging\ntask. Previous methods often treat this as a classification problem,\nconsidering each type of relationship (e.g. \"ride\") or each distinct visual\nphrase (e.g. \"person-ride-horse\") as a category. Such approaches are faced with\nsignificant difficulties caused by the high diversity of visual appearance for\neach kind of relationships or the large number of distinct visual phrases. We\npropose an integrated framework to tackle this problem. At the heart of this\nframework is the Deep Relational Network, a novel formulation designed\nspecifically for exploiting the statistical dependencies between objects and\ntheir relationships. On two large datasets, the proposed method achieves\nsubstantial improvement over state-of-the-art. \n\n"}
{"id": "1704.03285", "contents": "Title: Online Video Deblurring via Dynamic Temporal Blending Network Abstract: State-of-the-art video deblurring methods are capable of removing non-uniform\nblur caused by unwanted camera shake and/or object motion in dynamic scenes.\nHowever, most existing methods are based on batch processing and thus need\naccess to all recorded frames, rendering them computationally demanding and\ntime consuming and thus limiting their practical use. In contrast, we propose\nan online (sequential) video deblurring method based on a spatio-temporal\nrecurrent network that allows for real-time performance. In particular, we\nintroduce a novel architecture which extends the receptive field while keeping\nthe overall size of the network small to enable fast execution. In doing so,\nour network is able to remove even large blur caused by strong camera shake\nand/or fast moving objects. Furthermore, we propose a novel network layer that\nenforces temporal consistency between consecutive frames by dynamic temporal\nblending which compares and adaptively (at test time) shares features obtained\nat different time steps. We show the superiority of the proposed method in an\nextensive experimental evaluation. \n\n"}
{"id": "1704.03296", "contents": "Title: Interpretable Explanations of Black Boxes by Meaningful Perturbation Abstract: As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks \"look\" in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations. \n\n"}
{"id": "1704.03604", "contents": "Title: Instance-Level Salient Object Segmentation Abstract: Image saliency detection has recently witnessed rapid progress due to deep\nconvolutional neural networks. However, none of the existing methods is able to\nidentify object instances in the detected salient regions. In this paper, we\npresent a salient instance segmentation method that produces a saliency mask\nwith distinct object instance labels for an input image. Our method consists of\nthree steps, estimating saliency map, detecting salient object contours and\nidentifying salient object instances. For the first two steps, we propose a\nmultiscale saliency refinement network, which generates high-quality salient\nregion masks and salient object contours. Once integrated with multiscale\ncombinatorial grouping and a MAP-based subset optimization framework, our\nmethod can generate very promising salient object instance segmentation\nresults. To promote further research and evaluation of salient instance\nsegmentation, we also construct a new database of 1000 images and their\npixelwise salient instance annotations. Experimental results demonstrate that\nour proposed method is capable of achieving state-of-the-art performance on all\npublic benchmarks for salient region detection as well as on our new dataset\nfor salient instance segmentation. \n\n"}
{"id": "1704.04131", "contents": "Title: Neural Face Editing with Intrinsic Image Disentangling Abstract: Traditional face editing methods often require a number of sophisticated and\ntask specific algorithms to be applied one after the other --- a process that\nis tedious, fragile, and computationally intensive. In this paper, we propose\nan end-to-end generative adversarial network that infers a face-specific\ndisentangled representation of intrinsic face properties, including shape (i.e.\nnormals), albedo, and lighting, and an alpha matte. We show that this network\ncan be trained on \"in-the-wild\" images by incorporating an in-network\nphysically-based image formation module and appropriate loss functions. Our\ndisentangling latent representation allows for semantically relevant edits,\nwhere one aspect of facial appearance can be manipulated while keeping\northogonal properties fixed, and we demonstrate its use for a number of facial\nediting applications. \n\n"}
{"id": "1704.04886", "contents": "Title: Multi-View Image Generation from a Single-View Abstract: This paper addresses a challenging problem -- how to generate multi-view\ncloth images from only a single view input. To generate realistic-looking\nimages with different views from the input, we propose a new image generation\nmodel termed VariGANs that combines the strengths of the variational inference\nand the Generative Adversarial Networks (GANs). Our proposed VariGANs model\ngenerates the target image in a coarse-to-fine manner instead of a single pass\nwhich suffers from severe artifacts. It first performs variational inference to\nmodel global appearance of the object (e.g., shape and color) and produce a\ncoarse image with a different view. Conditioned on the generated low resolution\nimages, it then proceeds to perform adversarial learning to fill details and\ngenerate images of consistent details with the input. Extensive experiments\nconducted on two clothing datasets, MVC and DeepFashion, have demonstrated that\nimages of a novel view generated by our model are more plausible than those\ngenerated by existing approaches, in terms of more consistent global appearance\nas well as richer and sharper details. \n\n"}
{"id": "1704.05643", "contents": "Title: Skeleton Boxes: Solving skeleton based action detection with a single\n  deep convolutional neural network Abstract: Action recognition from well-segmented 3D skeleton video has been intensively\nstudied. However, due to the difficulty in representing the 3D skeleton video\nand the lack of training data, action detection from streaming 3D skeleton\nvideo still lags far behind its recognition counterpart and image based object\ndetection. In this paper, we propose a novel approach for this problem, which\nleverages both effective skeleton video encoding and deep regression based\nobject detection from images. Our framework consists of two parts:\nskeleton-based video image mapping, which encodes a skeleton video to a color\nimage in a temporal preserving way, and an end-to-end trainable fast skeleton\naction detector (Skeleton Boxes) based on image detection. Experimental results\non the latest and largest PKU-MMD benchmark dataset demonstrate that our method\noutperforms the state-of-the-art methods with a large margin. We believe our\nidea would inspire and benefit future research in this important area. \n\n"}
{"id": "1704.05693", "contents": "Title: Unsupervised Creation of Parameterized Avatars Abstract: We study the problem of mapping an input image to a tied pair consisting of a\nvector of parameters and an image that is created using a graphical engine from\nthe vector of parameters. The mapping's objective is to have the output image\nas similar as possible to the input image. During training, no supervision is\ngiven in the form of matching inputs and outputs.\n  This learning problem extends two literature problems: unsupervised domain\nadaptation and cross domain transfer. We define a generalization bound that is\nbased on discrepancy, and employ a GAN to implement a network solution that\ncorresponds to this bound. Experimentally, our method is shown to solve the\nproblem of automatically creating avatars. \n\n"}
{"id": "1704.05796", "contents": "Title: Network Dissection: Quantifying Interpretability of Deep Visual\n  Representations Abstract: We propose a general framework called Network Dissection for quantifying the\ninterpretability of latent representations of CNNs by evaluating the alignment\nbetween individual hidden units and a set of semantic concepts. Given any CNN\nmodel, the proposed method draws on a broad data set of visual concepts to\nscore the semantics of hidden units at each intermediate convolutional layer.\nThe units with semantics are given labels across a range of objects, parts,\nscenes, textures, materials, and colors. We use the proposed method to test the\nhypothesis that interpretability of units is equivalent to random linear\ncombinations of units, then we apply our method to compare the latent\nrepresentations of various networks when trained to solve different supervised\nand self-supervised training tasks. We further analyze the effect of training\niterations, compare networks trained with different initializations, examine\nthe impact of network depth and width, and measure the effect of dropout and\nbatch normalization on the interpretability of deep visual representations. We\ndemonstrate that the proposed method can shed light on characteristics of CNN\nmodels and training methods that go beyond measurements of their discriminative\npower. \n\n"}
{"id": "1704.05817", "contents": "Title: Learn to Model Motion from Blurry Footages Abstract: It is difficult to recover the motion field from a real-world footage given a\nmixture of camera shake and other photometric effects. In this paper we propose\na hybrid framework by interleaving a Convolutional Neural Network (CNN) and a\ntraditional optical flow energy. We first conduct a CNN architecture using a\nnovel learnable directional filtering layer. Such layer encodes the angle and\ndistance similarity matrix between blur and camera motion, which is able to\nenhance the blur features of the camera-shake footages. The proposed CNNs are\nthen integrated into an iterative optical flow framework, which enable the\ncapability of modelling and solving both the blind deconvolution and the\noptical flow estimation problems simultaneously. Our framework is trained\nend-to-end on a synthetic dataset and yields competitive precision and\nperformance against the state-of-the-art approaches. \n\n"}
{"id": "1704.06228", "contents": "Title: Temporal Action Detection with Structured Segment Networks Abstract: Detecting actions in untrimmed videos is an important yet challenging task.\nIn this paper, we present the structured segment network (SSN), a novel\nframework which models the temporal structure of each action instance via a\nstructured temporal pyramid. On top of the pyramid, we further introduce a\ndecomposed discriminative model comprising two classifiers, respectively for\nclassifying actions and determining completeness. This allows the framework to\neffectively distinguish positive proposals from background or incomplete ones,\nthus leading to both accurate recognition and localization. These components\nare integrated into a unified network that can be efficiently trained in an\nend-to-end fashion. Additionally, a simple yet effective temporal action\nproposal scheme, dubbed temporal actionness grouping (TAG) is devised to\ngenerate high quality action proposals. On two challenging benchmarks, THUMOS14\nand ActivityNet, our method remarkably outperforms previous state-of-the-art\nmethods, demonstrating superior accuracy and strong adaptivity in handling\nactions with various temporal structures. \n\n"}
{"id": "1704.06305", "contents": "Title: Efficient Gender Classification Using a Deep LDA-Pruned Net Abstract: Many real-time tasks, such as human-computer interaction, require fast and\nefficient facial gender classification. Although deep CNN nets have been very\neffective for a multitude of classification tasks, their high space and time\ndemands make them impractical for personal computers and mobile devices without\na powerful GPU. In this paper, we develop a 16-layer, yet lightweight, neural\nnetwork which boosts efficiency while maintaining high accuracy. Our net is\npruned from the VGG-16 model starting from the last convolutional (conv) layer\nwhere we find neuron activations are highly uncorrelated given the gender.\nThrough Fisher's Linear Discriminant Analysis (LDA), we show that this high\ndecorrelation makes it safe to discard directly last conv layer neurons with\nhigh within-class variance and low between-class variance. Combined with either\nSupport Vector Machines (SVM) or Bayesian classification, the reduced CNNs are\ncapable of achieving comparable (or even higher) accuracies on the LFW and\nCelebA datasets than the original net with fully connected layers. On LFW, only\nfour Conv5_3 neurons are able to maintain a comparably high recognition\naccuracy, which results in a reduction of total network size by a factor of 70X\nwith a 11 fold speedup. Comparisons with a state-of-the-art pruning method as\nwell as two smaller nets in terms of accuracy loss and convolutional layers\npruning rate are also provided. \n\n"}
{"id": "1704.06382", "contents": "Title: Hierarchical 3D fully convolutional networks for multi-organ\n  segmentation Abstract: Recent advances in 3D fully convolutional networks (FCN) have made it\nfeasible to produce dense voxel-wise predictions of full volumetric images. In\nthis work, we show that a multi-class 3D FCN trained on manually labeled CT\nscans of seven abdominal structures (artery, vein, liver, spleen, stomach,\ngallbladder, and pancreas) can achieve competitive segmentation results, while\navoiding the need for handcrafting features or training organ-specific models.\nTo this end, we propose a two-stage, coarse-to-fine approach that trains an FCN\nmodel to roughly delineate the organs of interest in the first stage (seeing\n$\\sim$40% of the voxels within a simple, automatically generated binary mask of\nthe patient's body). We then use these predictions of the first-stage FCN to\ndefine a candidate region that will be used to train a second FCN. This step\nreduces the number of voxels the FCN has to classify to $\\sim$10% while\nmaintaining a recall high of $>$99%. This second-stage FCN can now focus on\nmore detailed segmentation of the organs. We respectively utilize training and\nvalidation sets consisting of 281 and 50 clinical CT images. Our hierarchical\napproach provides an improved Dice score of 7.5 percentage points per organ on\naverage in our validation set. We furthermore test our models on a completely\nunseen data collection acquired at a different hospital that includes 150 CT\nscans with three anatomical labels (liver, spleen, and pancreas). In such\nchallenging organs as the pancreas, our hierarchical approach improves the mean\nDice score from 68.5 to 82.2%, achieving the highest reported average score on\nthis dataset. \n\n"}
{"id": "1704.06843", "contents": "Title: On the Two-View Geometry of Unsynchronized Cameras Abstract: We present new methods for simultaneously estimating camera geometry and time\nshift from video sequences from multiple unsynchronized cameras. Algorithms for\nsimultaneous computation of a fundamental matrix or a homography with unknown\ntime shift between images are developed. Our methods use minimal correspondence\nsets (eight for fundamental matrix and four and a half for homography) and\ntherefore are suitable for robust estimation using RANSAC. Furthermore, we\npresent an iterative algorithm that extends the applicability on sequences\nwhich are significantly unsynchronized, finding the correct time shift up to\nseveral seconds. We evaluated the methods on synthetic and wide range of real\nworld datasets and the results show a broad applicability to the problem of\ncamera synchronization. \n\n"}
{"id": "1704.07754", "contents": "Title: Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical\n  Segmentation Abstract: Deep learning models such as convolutional neural net- work have been widely\nused in 3D biomedical segmentation and achieve state-of-the-art performance.\nHowever, most of them often adapt a single modality or stack multiple\nmodalities as different input channels. To better leverage the multi-\nmodalities, we propose a deep encoder-decoder structure with cross-modality\nconvolution layers to incorporate different modalities of MRI data. In\naddition, we exploit convolutional LSTM to model a sequence of 2D slices, and\njointly learn the multi-modalities and convolutional LSTM in an end-to-end\nmanner. To avoid converging to the certain labels, we adopt a re-weighting\nscheme and two-phase training to handle the label imbalance. Experimental\nresults on BRATS-2015 show that our method outperforms state-of-the-art\nbiomedical segmentation approaches. \n\n"}
{"id": "1704.08141", "contents": "Title: Compact Descriptors for Video Analysis: the Emerging MPEG Standard Abstract: This paper provides an overview of the on-going compact descriptors for video\nanalysis standard (CDVA) from the ISO/IEC moving pictures experts group (MPEG).\nMPEG-CDVA targets at defining a standardized bitstream syntax to enable\ninteroperability in the context of video analysis applications. During the\ndevelopments of MPEGCDVA, a series of techniques aiming to reduce the\ndescriptor size and improve the video representation ability have been\nproposed. This article describes the new standard that is being developed and\nreports the performance of these key technical contributions. \n\n"}
{"id": "1704.08331", "contents": "Title: Joint Semantic and Motion Segmentation for dynamic scenes using Deep\n  Convolutional Networks Abstract: Dynamic scene understanding is a challenging problem and motion segmentation\nplays a crucial role in solving it. Incorporating semantics and motion enhances\nthe overall perception of the dynamic scene. For applications of outdoor\nrobotic navigation, joint learning methods have not been extensively used for\nextracting spatio-temporal features or adding different priors into the\nformulation. The task becomes even more challenging without stereo information\nbeing incorporated. This paper proposes an approach to fuse semantic features\nand motion clues using CNNs, to address the problem of monocular semantic\nmotion segmentation. We deduce semantic and motion labels by integrating\noptical flow as a constraint with semantic features into dilated convolution\nnetwork. The pipeline consists of three main stages i.e Feature extraction,\nFeature amplification and Multi Scale Context Aggregation to fuse the semantics\nand flow features. Our joint formulation shows significant improvements in\nmonocular motion segmentation over the state of the art methods on challenging\nKITTI tracking dataset. \n\n"}
{"id": "1704.08881", "contents": "Title: Improving Small Object Proposals for Company Logo Detection Abstract: Many modern approaches for object detection are two-staged pipelines. The\nfirst stage identifies regions of interest which are then classified in the\nsecond stage. Faster R-CNN is such an approach for object detection which\ncombines both stages into a single pipeline. In this paper we apply Faster\nR-CNN to the task of company logo detection. Motivated by its weak performance\non small object instances, we examine in detail both the proposal and the\nclassification stage with respect to a wide range of object sizes. We\ninvestigate the influence of feature map resolution on the performance of those\nstages.\n  Based on theoretical considerations, we introduce an improved scheme for\ngenerating anchor proposals and propose a modification to Faster R-CNN which\nleverages higher-resolution feature maps for small objects. We evaluate our\napproach on the FlickrLogos dataset improving the RPN performance from 0.52 to\n0.71 (MABO) and the detection performance from 0.52 to 0.67 (mAP). \n\n"}
{"id": "1704.08908", "contents": "Title: Unbiased Shape Compactness for Segmentation Abstract: We propose to constrain segmentation functionals with a dimensionless,\nunbiased and position-independent shape compactness prior, which we solve\nefficiently with an alternating direction method of multipliers (ADMM).\nInvolving a squared sum of pairwise potentials, our prior results in a\nchallenging high-order optimization problem, which involves dense (fully\nconnected) graphs. We split the problem into a sequence of easier sub-problems,\neach performed efficiently at each iteration: (i) a sparse-matrix inversion\nbased on Woodbury identity, (ii) a closed-form solution of a cubic equation and\n(iii) a graph-cut update of a sub-modular pairwise sub-problem with a sparse\ngraph. We deploy our prior in an energy minimization, in conjunction with a\nsupervised classifier term based on CNNs and standard regularization\nconstraints. We demonstrate the usefulness of our energy in several medical\napplications. In particular, we report comprehensive evaluations of our fully\nautomated algorithm over 40 subjects, showing a competitive performance for the\nchallenging task of abdominal aorta segmentation in MRI. \n\n"}
{"id": "1704.08944", "contents": "Title: Object Discovery via Cohesion Measurement Abstract: Color and intensity are two important components in an image. Usually, groups\nof image pixels, which are similar in color or intensity, are an informative\nrepresentation for an object. They are therefore particularly suitable for\ncomputer vision tasks, such as saliency detection and object proposal\ngeneration. However, image pixels, which share a similar real-world color, may\nbe quite different since colors are often distorted by intensity. In this\npaper, we reinvestigate the affinity matrices originally used in image\nsegmentation methods based on spectral clustering. A new affinity matrix, which\nis robust to color distortions, is formulated for object discovery. Moreover, a\nCohesion Measurement (CM) for object regions is also derived based on the\nformulated affinity matrix. Based on the new Cohesion Measurement, a novel\nobject discovery method is proposed to discover objects latent in an image by\nutilizing the eigenvectors of the affinity matrix. Then we apply the proposed\nmethod to both saliency detection and object proposal generation. Experimental\nresults on several evaluation benchmarks demonstrate that the proposed CM based\nmethod has achieved promising performance for these two tasks. \n\n"}
{"id": "1705.00034", "contents": "Title: Deep Multi-view Models for Glitch Classification Abstract: Non-cosmic, non-Gaussian disturbances known as \"glitches\", show up in\ngravitational-wave data of the Advanced Laser Interferometer Gravitational-wave\nObservatory, or aLIGO. In this paper, we propose a deep multi-view\nconvolutional neural network to classify glitches automatically. The primary\npurpose of classifying glitches is to understand their characteristics and\norigin, which facilitates their removal from the data or from the detector\nentirely. We visualize glitches as spectrograms and leverage the\nstate-of-the-art image classification techniques in our model. The suggested\nclassifier is a multi-view deep neural network that exploits four different\nviews for classification. The experimental results demonstrate that the\nproposed model improves the overall accuracy of the classification compared to\ntraditional single view algorithms. \n\n"}
{"id": "1705.00389", "contents": "Title: Adversarial PoseNet: A Structure-aware Convolutional Network for Human\n  Pose Estimation Abstract: For human pose estimation in monocular images, joint occlusions and\noverlapping upon human bodies often result in deviated pose predictions. Under\nthese circumstances, biologically implausible pose predictions may be produced.\nIn contrast, human vision is able to predict poses by exploiting geometric\nconstraints of joint inter-connectivity. To address the problem by\nincorporating priors about the structure of human bodies, we propose a novel\nstructure-aware convolutional network to implicitly take such priors into\naccount during training of the deep network. Explicit learning of such\nconstraints is typically challenging. Instead, we design discriminators to\ndistinguish the real poses from the fake ones (such as biologically implausible\nones). If the pose generator (G) generates results that the discriminator fails\nto distinguish from real ones, the network successfully learns the priors. \n\n"}
{"id": "1705.01352", "contents": "Title: Optical Flow in Mostly Rigid Scenes Abstract: The optical flow of natural scenes is a combination of the motion of the\nobserver and the independent motion of objects. Existing algorithms typically\nfocus on either recovering motion and structure under the assumption of a\npurely static world or optical flow for general unconstrained scenes. We\ncombine these approaches in an optical flow algorithm that estimates an\nexplicit segmentation of moving objects from appearance and physical\nconstraints. In static regions we take advantage of strong constraints to\njointly estimate the camera motion and the 3D structure of the scene over\nmultiple frames. This allows us to also regularize the structure instead of the\nmotion. Our formulation uses a Plane+Parallax framework, which works even under\nsmall baselines, and reduces the motion estimation to a one-dimensional search\nproblem, resulting in more accurate estimation. In moving regions the flow is\ntreated as unconstrained, and computed with an existing optical flow method.\nThe resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art\nresults on both the MPI-Sintel and KITTI-2015 benchmarks. \n\n"}
{"id": "1705.02315", "contents": "Title: ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on\n  Weakly-Supervised Classification and Localization of Common Thorax Diseases Abstract: The chest X-ray is one of the most commonly accessible radiological\nexaminations for screening and diagnosis of many lung diseases. A tremendous\nnumber of X-ray imaging studies accompanied by radiological reports are\naccumulated and stored in many modern hospitals' Picture Archiving and\nCommunication Systems (PACS). On the other side, it is still an open question\nhow this type of hospital-size knowledge database containing invaluable imaging\ninformatics (i.e., loosely labeled) can be used to facilitate the data-hungry\ndeep learning paradigms in building truly large-scale high precision\ncomputer-aided diagnosis (CAD) systems.\n  In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\",\nwhich comprises 108,948 frontal-view X-ray images of 32,717 unique patients\nwith the text-mined eight disease image labels (where each image can have\nmulti-labels), from the associated radiological reports using natural language\nprocessing. Importantly, we demonstrate that these commonly occurring thoracic\ndiseases can be detected and even spatially-located via a unified\nweakly-supervised multi-label image classification and disease localization\nframework, which is validated using our proposed dataset. Although the initial\nquantitative results are promising as reported, deep convolutional neural\nnetwork based \"reading chest X-rays\" (i.e., recognizing and locating the common\ndisease patterns trained with only image-level labels) remains a strenuous task\nfor fully-automated high precision CAD systems. Data download link:\nhttps://nihcc.app.box.com/v/ChestXray-NIHCC \n\n"}
{"id": "1705.02953", "contents": "Title: Temporal Segment Networks for Action Recognition in Videos Abstract: Deep convolutional networks have achieved great success for image\nrecognition. However, for action recognition in videos, their advantage over\ntraditional methods is not so evident. We present a general and flexible\nvideo-level framework for learning action models in videos. This method, called\ntemporal segment network (TSN), aims to model long-range temporal structures\nwith a new segment-based sampling and aggregation module. This unique design\nenables our TSN to efficiently learn action models by using the whole action\nvideos. The learned models could be easily adapted for action recognition in\nboth trimmed and untrimmed videos with simple average pooling and multi-scale\ntemporal window integration, respectively. We also study a series of good\npractices for the instantiation of TSN framework given limited training\nsamples. Our approach obtains the state-the-of-art performance on four\nchallenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%),\nTHUMOS14 (80.1%), and ActivityNet v1.2 (89.6%). Using the proposed RGB\ndifference for motion models, our method can still achieve competitive accuracy\non UCF101 (91.0%) while running at 340 FPS. Furthermore, based on the temporal\nsegment networks, we won the video classification track at the ActivityNet\nchallenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and\nthe proposed good practices. \n\n"}
{"id": "1705.03550", "contents": "Title: CORe50: a New Dataset and Benchmark for Continuous Object Recognition Abstract: Continuous/Lifelong learning of high-dimensional data streams is a\nchallenging research problem. In fact, fully retraining models each time new\ndata become available is infeasible, due to computational and storage issues,\nwhile na\\\"ive incremental strategies have been shown to suffer from\ncatastrophic forgetting. In the context of real-world object recognition\napplications (e.g., robotic vision), where continuous learning is crucial, very\nfew datasets and benchmarks are available to evaluate and compare emerging\ntechniques. In this work we propose a new dataset and benchmark CORe50,\nspecifically designed for continuous object recognition, and introduce baseline\napproaches for different continuous learning scenarios. \n\n"}
{"id": "1705.04228", "contents": "Title: Incremental Learning Through Deep Adaptation Abstract: Given an existing trained neural network, it is often desirable to learn new\ncapabilities without hindering performance of those already learned. Existing\napproaches either learn sub-optimal solutions, require joint training, or incur\na substantial increment in the number of parameters for each added domain,\ntypically as many as the original network. We propose a method called\n\\emph{Deep Adaptation Networks} (DAN) that constrains newly learned filters to\nbe linear combinations of existing ones. DANs precisely preserve performance on\nthe original domain, require a fraction (typically 13\\%, dependent on network\narchitecture) of the number of parameters compared to standard fine-tuning\nprocedures and converge in less cycles of training to a comparable or better\nlevel of performance. When coupled with standard network quantization\ntechniques, we further reduce the parameter cost to around 3\\% of the original\nwith negligible or no loss in accuracy. The learned architecture can be\ncontrolled to switch between various learned representations, enabling a single\nnetwork to solve a task from multiple different domains. We conduct extensive\nexperiments showing the effectiveness of our method on a range of image\nclassification tasks and explore different aspects of its behavior. \n\n"}
{"id": "1705.04258", "contents": "Title: Probabilistic Image Colorization Abstract: We develop a probabilistic technique for colorizing grayscale natural images.\nIn light of the intrinsic uncertainty of this task, the proposed probabilistic\nframework has numerous desirable properties. In particular, our model is able\nto produce multiple plausible and vivid colorizations for a given grayscale\nimage and is one of the first colorization models to provide a proper\nstochastic sampling scheme. Moreover, our training procedure is supported by a\nrigorous theoretical framework that does not require any ad hoc heuristics and\nallows for efficient modeling and learning of the joint pixel color\ndistribution. We demonstrate strong quantitative and qualitative experimental\nresults on the CIFAR-10 dataset and the challenging ILSVRC 2012 dataset. \n\n"}
{"id": "1705.04281", "contents": "Title: SEAGLE: Sparsity-Driven Image Reconstruction under Multiple Scattering Abstract: Multiple scattering of an electromagnetic wave as it passes through an object\nis a fundamental problem that limits the performance of current imaging\nsystems. In this paper, we describe a new technique-called Series Expansion\nwith Accelerated Gradient Descent on Lippmann-Schwinger Equation (SEAGLE)-for\nrobust imaging under multiple scattering based on a combination of a new\nnonlinear forward model and a total variation (TV) regularizer. The proposed\nforward model can account for multiple scattering, which makes it advantageous\nin applications where linear models are inaccurate. Specifically, it\ncorresponds to a series expansion of the scattered wave with an\naccelerated-gradient method. This expansion guarantees the convergence even for\nstrongly scattering objects. One of our key insights is that it is possible to\nobtain an explicit formula for computing the gradient of our nonlinear forward\nmodel with respect to the unknown object, thus enabling fast image\nreconstruction with the state-of-the-art fast iterative shrinkage/thresholding\nalgorithm (FISTA). The proposed method is validated on both simulated and\nexperimentally measured data. \n\n"}
{"id": "1705.04352", "contents": "Title: Reconfiguring the Imaging Pipeline for Computer Vision Abstract: Advancements in deep learning have ignited an explosion of research on\nefficient hardware for embedded computer vision. Hardware vision acceleration,\nhowever, does not address the cost of capturing and processing the image data\nthat feeds these algorithms. We examine the role of the image signal processing\n(ISP) pipeline in computer vision to identify opportunities to reduce\ncomputation and save energy. The key insight is that imaging pipelines should\nbe designed to be configurable: to switch between a traditional photography\nmode and a low-power vision mode that produces lower-quality image data\nsuitable only for computer vision. We use eight computer vision algorithms and\na reversible pipeline simulation tool to study the imaging system's impact on\nvision performance. For both CNN-based and classical vision algorithms, we\nobserve that only two ISP stages, demosaicing and gamma compression, are\ncritical for task performance. We propose a new image sensor design that can\ncompensate for skipping these stages. The sensor design features an adjustable\nresolution and tunable analog-to-digital converters (ADCs). Our proposed\nimaging system's vision mode disables the ISP entirely and configures the\nsensor to produce subsampled, lower-precision image data. This vision mode can\nsave ~75% of the average energy of a baseline photography mode while having\nonly a small impact on vision task accuracy. \n\n"}
{"id": "1705.06676", "contents": "Title: MUTAN: Multimodal Tucker Fusion for Visual Question Answering Abstract: Bilinear models provide an appealing framework for mixing and merging\ninformation in Visual Question Answering (VQA) tasks. They help to learn high\nlevel associations between question meaning and visual concepts in the image,\nbut they suffer from huge dimensionality issues. We introduce MUTAN, a\nmultimodal tensor-based Tucker decomposition to efficiently parametrize\nbilinear interactions between visual and textual representations. Additionally\nto the Tucker framework, we design a low-rank matrix-based decomposition to\nexplicitly constrain the interaction rank. With MUTAN, we control the\ncomplexity of the merging scheme while keeping nice interpretable fusion\nrelations. We show how our MUTAN model generalizes some of the latest VQA\narchitectures, providing state-of-the-art results. \n\n"}
{"id": "1705.07137", "contents": "Title: Deep De-Aliasing for Fast Compressive Sensing MRI Abstract: Fast Magnetic Resonance Imaging (MRI) is highly in demand for many clinical\napplications in order to reduce the scanning cost and improve the patient\nexperience. This can also potentially increase the image quality by reducing\nthe motion artefacts and contrast washout. However, once an image field of view\nand the desired resolution are chosen, the minimum scanning time is normally\ndetermined by the requirement of acquiring sufficient raw data to meet the\nNyquist-Shannon sampling criteria. Compressive Sensing (CS) theory has been\nperfectly matched to the MRI scanning sequence design with much less required\nraw data for the image reconstruction. Inspired by recent advances in deep\nlearning for solving various inverse problems, we propose a conditional\nGenerative Adversarial Networks-based deep learning framework for de-aliasing\nand reconstructing MRI images from highly undersampled data with great promise\nto accelerate the data acquisition process. By coupling an innovative content\nloss with the adversarial loss our de-aliasing results are more realistic.\nFurthermore, we propose a refinement learning procedure for training the\ngenerator network, which can stabilise the training with fast convergence and\nless parameter tuning. We demonstrate that the proposed framework outperforms\nstate-of-the-art CS-MRI methods, in terms of reconstruction error and\nperceptual image quality. In addition, our method can reconstruct each image in\n0.22ms--0.37ms, which is promising for real-time applications. \n\n"}
{"id": "1705.07208", "contents": "Title: PixColor: Pixel Recursive Colorization Abstract: We propose a novel approach to automatically produce multiple colorized\nversions of a grayscale image. Our method results from the observation that the\ntask of automated colorization is relatively easy given a low-resolution\nversion of the color image. We first train a conditional PixelCNN to generate a\nlow resolution color for a given grayscale image. Then, given the generated\nlow-resolution color image and the original grayscale image as inputs, we train\na second CNN to generate a high-resolution colorization of an image. We\ndemonstrate that our approach produces more diverse and plausible colorizations\nthan existing methods, as judged by human raters in a \"Visual Turing Test\". \n\n"}
{"id": "1705.07263", "contents": "Title: Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection\n  Methods Abstract: Neural networks are known to be vulnerable to adversarial examples: inputs\nthat are close to natural inputs but classified incorrectly. In order to better\nunderstand the space of adversarial examples, we survey ten recent proposals\nthat are designed for detection and compare their efficacy. We show that all\ncan be defeated by constructing new loss functions. We conclude that\nadversarial examples are significantly harder to detect than previously\nappreciated, and the properties believed to be intrinsic to adversarial\nexamples are in fact not. Finally, we propose several simple guidelines for\nevaluating future proposed defenses. \n\n"}
{"id": "1705.08086", "contents": "Title: Universal Style Transfer via Feature Transforms Abstract: Universal style transfer aims to transfer arbitrary visual styles to content\nimages. Existing feed-forward based methods, while enjoying the inference\nefficiency, are mainly limited by inability of generalizing to unseen styles or\ncompromised visual quality. In this paper, we present a simple yet effective\nmethod that tackles these limitations without training on any pre-defined\nstyles. The key ingredient of our method is a pair of feature transforms,\nwhitening and coloring, that are embedded to an image reconstruction network.\nThe whitening and coloring transforms reflect a direct matching of feature\ncovariance of the content image to a given style image, which shares similar\nspirits with the optimization of Gram matrix based cost in neural style\ntransfer. We demonstrate the effectiveness of our algorithm by generating\nhigh-quality stylized images with comparisons to a number of recent methods. We\nalso analyze our method by visualizing the whitened features and synthesizing\ntextures via simple feature coloring. \n\n"}
{"id": "1705.08168", "contents": "Title: Look, Listen and Learn Abstract: We consider the question: what can be learnt by looking at and listening to a\nlarge number of unlabelled videos? There is a valuable, but so far untapped,\nsource of information contained in the video itself -- the correspondence\nbetween the visual and the audio streams, and we introduce a novel\n\"Audio-Visual Correspondence\" learning task that makes use of this. Training\nvisual and audio networks from scratch, without any additional supervision\nother than the raw unconstrained videos themselves, is shown to successfully\nsolve this task, and, more interestingly, result in good visual and audio\nrepresentations. These features set the new state-of-the-art on two sound\nclassification benchmarks, and perform on par with the state-of-the-art\nself-supervised approaches on ImageNet classification. We also demonstrate that\nthe network is able to localize objects in both modalities, as well as perform\nfine-grained recognition tasks. \n\n"}
{"id": "1705.08264", "contents": "Title: Isomorphism between Differential and Moment Invariants under Affine\n  Transform Abstract: The invariant is one of central topics in science, technology and\nengineering. The differential invariant is essential in understanding or\ndescribing some important phenomena or procedures in mathematics, physics,\nchemistry, biology or computer science etc. The derivation of differential\ninvariants is usually difficult or complicated. This paper reports a discovery\nthat under the affine transform, differential invariants have similar\nstructures with moment invariants up to a scalar function of transform\nparameters. If moment invariants are known, relative differential invariants\ncan be obtained by the substitution of moments by derivatives with the same\norder. Whereas moment invariants can be calculated by multiple integrals, this\nmethod provides a simple way to derive differential invariants without the need\nto resolve any equation system. Since the definition of moments on different\nmanifolds or in different dimension of spaces is well established, differential\ninvariants on or in them will also be well defined. Considering that moments\nhave a strong background in mathematics and physics, this technique offers a\nnew view angle to the inner structure of invariants. Projective differential\ninvariants can also be found in this way with a screening process. \n\n"}
{"id": "1705.08293", "contents": "Title: An Invariant Model of the Significance of Different Body Parts in\n  Recognizing Different Actions Abstract: In this paper, we show that different body parts do not play equally\nimportant roles in recognizing a human action in video data. We investigate to\nwhat extent a body part plays a role in recognition of different actions and\nhence propose a generic method of assigning weights to different body points.\nThe approach is inspired by the strong evidence in the applied perception\ncommunity that humans perform recognition in a foveated manner, that is they\nrecognize events or objects by only focusing on visually significant aspects.\nAn important contribution of our method is that the computation of the weights\nassigned to body parts is invariant to viewing directions and camera parameters\nin the input data. We have performed extensive experiments to validate the\nproposed approach and demonstrate its significance. In particular, results show\nthat considerable improvement in performance is gained by taking into account\nthe relative importance of different body parts as defined by our approach. \n\n"}
{"id": "1705.08302", "contents": "Title: Anatomically Constrained Neural Networks (ACNN): Application to Cardiac\n  Image Enhancement and Segmentation Abstract: Incorporation of prior knowledge about organ shape and location is key to\nimprove performance of image analysis approaches. In particular, priors can be\nuseful in cases where images are corrupted and contain artefacts due to\nlimitations in image acquisition. The highly constrained nature of anatomical\nobjects can be well captured with learning based techniques. However, in most\nrecent and promising techniques such as CNN based segmentation it is not\nobvious how to incorporate such prior knowledge. State-of-the-art methods\noperate as pixel-wise classifiers where the training objectives do not\nincorporate the structure and inter-dependencies of the output. To overcome\nthis limitation, we propose a generic training strategy that incorporates\nanatomical prior knowledge into CNNs through a new regularisation model, which\nis trained end-to-end. The new framework encourages models to follow the global\nanatomical properties of the underlying anatomy (e.g. shape, label structure)\nvia learned non-linear representations of the shape. We show that the proposed\napproach can be easily adapted to different analysis tasks (e.g. image\nenhancement, segmentation) and improve the prediction accuracy of the\nstate-of-the-art models. The applicability of our approach is shown on\nmulti-modal cardiac datasets and public benchmarks. Additionally, we\ndemonstrate how the learned deep models of 3D shapes can be interpreted and\nused as biomarkers for classification of cardiac pathologies. \n\n"}
{"id": "1705.08421", "contents": "Title: AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual\n  Actions Abstract: This paper introduces a video dataset of spatio-temporally localized Atomic\nVisual Actions (AVA). The AVA dataset densely annotates 80 atomic visual\nactions in 430 15-minute video clips, where actions are localized in space and\ntime, resulting in 1.58M action labels with multiple labels per person\noccurring frequently. The key characteristics of our dataset are: (1) the\ndefinition of atomic visual actions, rather than composite actions; (2) precise\nspatio-temporal annotations with possibly multiple annotations for each person;\n(3) exhaustive annotation of these atomic actions over 15-minute video clips;\n(4) people temporally linked across consecutive segments; and (5) using movies\nto gather a varied set of action representations. This departs from existing\ndatasets for spatio-temporal action recognition, which typically provide sparse\nannotations for composite actions in short video clips. We will release the\ndataset publicly.\n  AVA, with its realistic scene and action complexity, exposes the intrinsic\ndifficulty of action recognition. To benchmark this, we present a novel\napproach for action localization that builds upon the current state-of-the-art\nmethods, and demonstrates better performance on JHMDB and UCF101-24 categories.\nWhile setting a new state of the art on existing datasets, the overall results\non AVA are low at 15.6% mAP, underscoring the need for developing new\napproaches for video understanding. \n\n"}
{"id": "1705.09307", "contents": "Title: Direct Multitype Cardiac Indices Estimation via Joint Representation and\n  Regression Learning Abstract: Cardiac indices estimation is of great importance during identification and\ndiagnosis of cardiac disease in clinical routine. However, estimation of\nmultitype cardiac indices with consistently reliable and high accuracy is still\na great challenge due to the high variability of cardiac structures and\ncomplexity of temporal dynamics in cardiac MR sequences. While efforts have\nbeen devoted into cardiac volumes estimation through feature engineering\nfollowed by a independent regression model, these methods suffer from the\nvulnerable feature representation and incompatible regression model. In this\npaper, we propose a semi-automated method for multitype cardiac indices\nestimation. After manual labelling of two landmarks for ROI cropping, an\nintegrated deep neural network Indices-Net is designed to jointly learn the\nrepresentation and regression models. It comprises two tightly-coupled\nnetworks: a deep convolution autoencoder (DCAE) for cardiac image\nrepresentation, and a multiple output convolution neural network (CNN) for\nindices regression. Joint learning of the two networks effectively enhances the\nexpressiveness of image representation with respect to cardiac indices, and the\ncompatibility between image representation and indices regression, thus leading\nto accurate and reliable estimations for all the cardiac indices.\n  When applied with five-fold cross validation on MR images of 145 subjects,\nIndices-Net achieves consistently low estimation error for LV wall thicknesses\n(1.44$\\pm$0.71mm) and areas of cavity and myocardium (204$\\pm$133mm$^2$). It\noutperforms, with significant error reductions, segmentation method (55.1% and\n17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall\nthicknesses and areas, respectively. These advantages endow the proposed method\na great potential in clinical cardiac function assessment. \n\n"}
{"id": "1705.09765", "contents": "Title: Deep Matching and Validation Network -- An End-to-End Solution to\n  Constrained Image Splicing Localization and Detection Abstract: Image splicing is a very common image manipulation technique that is\nsometimes used for malicious purposes. A splicing detec- tion and localization\nalgorithm usually takes an input image and produces a binary decision\nindicating whether the input image has been manipulated, and also a\nsegmentation mask that corre- sponds to the spliced region. Most existing\nsplicing detection and localization pipelines suffer from two main\nshortcomings: 1) they use handcrafted features that are not robust against\nsubsequent processing (e.g., compression), and 2) each stage of the pipeline is\nusually optimized independently. In this paper we extend the formulation of the\nunderlying splicing problem to consider two input images, a query image and a\npotential donor image. Here the task is to estimate the probability that the\ndonor image has been used to splice the query image, and obtain the splicing\nmasks for both the query and donor images. We introduce a novel deep\nconvolutional neural network architecture, called Deep Matching and Validation\nNetwork (DMVN), which simultaneously localizes and detects image splicing. The\nproposed approach does not depend on handcrafted features and uses raw input\nimages to create deep learned representations. Furthermore, the DMVN is\nend-to-end op- timized to produce the probability estimates and the\nsegmentation masks. Our extensive experiments demonstrate that this approach\noutperforms state-of-the-art splicing detection methods by a large margin in\nterms of both AUC score and speed. \n\n"}
{"id": "1705.09780", "contents": "Title: Deep Metric Learning and Image Classification with Nearest Neighbour\n  Gaussian Kernels Abstract: We present a Gaussian kernel loss function and training algorithm for\nconvolutional neural networks that can be directly applied to both distance\nmetric learning and image classification problems. Our method treats all\ntraining features from a deep neural network as Gaussian kernel centres and\ncomputes loss by summing the influence of a feature's nearby centres in the\nfeature embedding space. Our approach is made scalable by treating it as an\napproximate nearest neighbour search problem. We show how to make end-to-end\nlearning feasible, resulting in a well formed embedding space, in which\nsemantically related instances are likely to be located near one another,\nregardless of whether or not the network was trained on those classes. Our\napproach outperforms state-of-the-art deep metric learning approaches on\nembedding learning challenges, as well as conventional softmax classification\non several datasets. \n\n"}
{"id": "1705.10546", "contents": "Title: Saliency Revisited: Analysis of Mouse Movements versus Fixations Abstract: This paper revisits visual saliency prediction by evaluating the recent\nadvancements in this field such as crowd-sourced mouse tracking-based databases\nand contextual annotations. We pursue a critical and quantitative approach\ntowards some of the new challenges including the quality of mouse tracking\nversus eye tracking for model training and evaluation. We extend quantitative\nevaluation of models in order to incorporate contextual information by\nproposing an evaluation methodology that allows accounting for contextual\nfactors such as text, faces, and object attributes. The proposed contextual\nevaluation scheme facilitates detailed analysis of models and helps identify\ntheir pros and cons. Through several experiments, we find that (1) mouse\ntracking data has lower inter-participant visual congruency and higher\ndispersion, compared to the eye tracking data, (2) mouse tracking data does not\ntotally agree with eye tracking in general and in terms of different contextual\nregions in specific, and (3) mouse tracking data leads to acceptable results in\ntraining current existing models, and (4) mouse tracking data is less reliable\nfor model selection and evaluation. The contextual evaluation also reveals\nthat, among the studied models, there is no single model that performs best on\nall the tested annotations. \n\n"}
{"id": "1705.10561", "contents": "Title: End-to-end Active Object Tracking via Reinforcement Learning Abstract: We study active object tracking, where a tracker takes as input the visual\nobservation (i.e., frame sequence) and produces the camera control signal\n(e.g., move forward, turn left, etc.). Conventional methods tackle the tracking\nand the camera control separately, which is challenging to tune jointly. It\nalso incurs many human efforts for labeling and many expensive trial-and-errors\nin realworld. To address these issues, we propose, in this paper, an end-to-end\nsolution via deep reinforcement learning, where a ConvNet-LSTM function\napproximator is adopted for the direct frame-toaction prediction. We further\npropose an environment augmentation technique and a customized reward function,\nwhich are crucial for a successful training. The tracker trained in simulators\n(ViZDoom, Unreal Engine) shows good generalization in the case of unseen object\nmoving path, unseen object appearance, unseen background, and distracting\nobject. It can restore tracking when occasionally losing the target. With the\nexperiments over the VOT dataset, we also find that the tracking ability,\nobtained solely from simulators, can potentially transfer to real-world\nscenarios. \n\n"}
{"id": "1705.10589", "contents": "Title: Jeffrey's prior sampling of deep sigmoidal networks Abstract: Neural networks have been shown to have a remarkable ability to uncover low\ndimensional structure in data: the space of possible reconstructed images form\na reduced model manifold in image space. We explore this idea directly by\nanalyzing the manifold learned by Deep Belief Networks and Stacked Denoising\nAutoencoders using Monte Carlo sampling. The model manifold forms an only\nslightly elongated hyperball with actual reconstructed data appearing\npredominantly on the boundaries of the manifold. In connection with the results\nwe present, we discuss problems of sampling high-dimensional manifolds as well\nas recent work [M. Transtrum, G. Hart, and P. Qiu, Submitted (2014)] discussing\nthe relation between high dimensional geometry and model reduction. \n\n"}
{"id": "1705.10748", "contents": "Title: Computation-Performance Optimization of Convolutional Neural Networks\n  with Redundant Kernel Removal Abstract: Deep Convolutional Neural Networks (CNNs) are widely employed in modern\ncomputer vision algorithms, where the input image is convolved iteratively by\nmany kernels to extract the knowledge behind it. However, with the depth of\nconvolutional layers getting deeper and deeper in recent years, the enormous\ncomputational complexity makes it difficult to be deployed on embedded systems\nwith limited hardware resources. In this paper, we propose two\ncomputation-performance optimization methods to reduce the redundant\nconvolution kernels of a CNN with performance and architecture constraints, and\napply it to a network for super resolution (SR). Using PSNR drop compared to\nthe original network as the performance criterion, our method can get the\noptimal PSNR under a certain computation budget constraint. On the other hand,\nour method is also capable of minimizing the computation required under a given\nPSNR drop. \n\n"}
{"id": "1706.00719", "contents": "Title: Automating Carotid Intima-Media Thickness Video Interpretation with\n  Convolutional Neural Networks Abstract: Cardiovascular disease (CVD) is the leading cause of mortality yet largely\npreventable, but the key to prevention is to identify at-risk individuals\nbefore adverse events. For predicting individual CVD risk, carotid intima-media\nthickness (CIMT), a noninvasive ultrasound method, has proven to be valuable,\noffering several advantages over CT coronary artery calcium score. However,\neach CIMT examination includes several ultrasound videos, and interpreting each\nof these CIMT videos involves three operations: (1) select three end-diastolic\nultrasound frames (EUF) in the video, (2) localize a region of interest (ROI)\nin each selected frame, and (3) trace the lumen-intima interface and the\nmedia-adventitia interface in each ROI to measure CIMT. These operations are\ntedious, laborious, and time consuming, a serious limitation that hinders the\nwidespread utilization of CIMT in clinical practice. To overcome this\nlimitation, this paper presents a new system to automate CIMT video\ninterpretation. Our extensive experiments demonstrate that the suggested system\nsignificantly outperforms the state-of-the-art methods. The superior\nperformance is attributable to our unified framework based on convolutional\nneural networks (CNNs) coupled with our informative image representation and\neffective post-processing of the CNN outputs, which are uniquely designed for\neach of the above three operations. \n\n"}
{"id": "1706.00893", "contents": "Title: Learning Person Trajectory Representations for Team Activity Analysis Abstract: Activity analysis in which multiple people interact across a large space is\nchallenging due to the interplay of individual actions and collective group\ndynamics. We propose an end-to-end approach for learning person trajectory\nrepresentations for group activity analysis. The learned representations encode\nrich spatio-temporal dependencies and capture useful motion patterns for\nrecognizing individual events, as well as characteristic group dynamics that\ncan be used to identify groups from their trajectories alone. We develop our\ndeep learning approach in the context of team sports, which provide\nwell-defined sets of events (e.g. pass, shot) and groups of people (teams).\nAnalysis of events and team formations using NHL hockey and NBA basketball\ndatasets demonstrate the generality of our approach. \n\n"}
{"id": "1706.01604", "contents": "Title: Hyperplane Clustering Via Dual Principal Component Pursuit Abstract: We extend the theoretical analysis of a recently proposed single subspace\nlearning algorithm, called Dual Principal Component Pursuit (DPCP), to the case\nwhere the data are drawn from of a union of hyperplanes. To gain insight into\nthe properties of the $\\ell_1$ non-convex problem associated with DPCP, we\ndevelop a geometric analysis of a closely related continuous optimization\nproblem. Then transferring this analysis to the discrete problem, our results\nstate that as long as the hyperplanes are sufficiently separated, the dominant\nhyperplane is sufficiently dominant and the points are uniformly distributed\ninside the associated hyperplanes, then the non-convex DPCP problem has a\nunique global solution, equal to the normal vector of the dominant hyperplane.\nThis suggests the correctness of a sequential hyperplane learning algorithm\nbased on DPCP. A thorough experimental evaluation reveals that hyperplane\nlearning schemes based on DPCP dramatically improve over the state-of-the-art\nmethods for the case of synthetic data, while are competitive to the\nstate-of-the-art in the case of 3D plane clustering for Kinect data. \n\n"}
{"id": "1706.01869", "contents": "Title: StreetStyle: Exploring world-wide clothing styles from millions of\n  photos Abstract: Each day billions of photographs are uploaded to photo-sharing services and\nsocial media platforms. These images are packed with information about how\npeople live around the world. In this paper we exploit this rich trove of data\nto understand fashion and style trends worldwide. We present a framework for\nvisual discovery at scale, analyzing clothing and fashion across millions of\nimages of people around the world and spanning several years. We introduce a\nlarge-scale dataset of photos of people annotated with clothing attributes, and\nuse this dataset to train attribute classifiers via deep learning. We also\npresent a method for discovering visually consistent style clusters that\ncapture useful visual correlations in this massive dataset. Using these tools,\nwe analyze millions of photos to derive visual insight, producing a\nfirst-of-its-kind analysis of global and per-city fashion choices and\nspatio-temporal trends. \n\n"}
{"id": "1706.02003", "contents": "Title: Deep Convolutional Decision Jungle for Image Classification Abstract: We propose a novel method called deep convolutional decision jungle (CDJ) and\nits learning algorithm for image classification. The CDJ maintains the\nstructure of standard convolutional neural networks (CNNs), i.e. multiple\nlayers of multiple response maps fully connected. Each response map-or node-in\nboth the convolutional and fully-connected layers selectively respond to class\nlabels s.t. each data sample travels via a specific soft route of those\nactivated nodes. The proposed method CDJ automatically learns features, whereas\ndecision forests and jungles require pre-defined feature sets. Compared to\nCNNs, the method embeds the benefits of using data-dependent discriminative\nfunctions, which better handles multi-modal/heterogeneous data; further,the\nmethod offers more diverse sparse network responses, which in turn can be used\nfor cost-effective learning/classification. The network is learnt by combining\nconventional softmax and proposed entropy losses in each layer. The entropy\nloss,as used in decision tree growing, measures the purity of data activation\naccording to the class label distribution. The back-propagation rule for the\nproposed loss function is derived from stochastic gradient descent (SGD)\noptimization of CNNs. We show that our proposed method outperforms\nstate-of-the-art methods on three public image classification benchmarks and\none face verification dataset. We also demonstrate the use of auxiliary data\nlabels, when available, which helps our method to learn more discriminative\nrouting and representations and leads to improved classification. \n\n"}
{"id": "1706.02042", "contents": "Title: DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and\n  Caricature Modeling Abstract: Face modeling has been paid much attention in the field of visual computing.\nThere exist many scenarios, including cartoon characters, avatars for social\nmedia, 3D face caricatures as well as face-related art and design, where\nlow-cost interactive face modeling is a popular approach especially among\namateur users. In this paper, we propose a deep learning based sketching system\nfor 3D face and caricature modeling. This system has a labor-efficient\nsketching interface, that allows the user to draw freehand imprecise yet\nexpressive 2D lines representing the contours of facial features. A novel CNN\nbased deep regression network is designed for inferring 3D face models from 2D\nsketches. Our network fuses both CNN and shape based features of the input\nsketch, and has two independent branches of fully connected layers generating\nindependent subsets of coefficients for a bilinear face representation. Our\nsystem also supports gesture based interactions for users to further manipulate\ninitial face models. Both user studies and numerical results indicate that our\nsketching system can help users create face models quickly and effectively. A\nsignificantly expanded face database with diverse identities, expressions and\nlevels of exaggeration is constructed to promote further research and\nevaluation of face modeling techniques. \n\n"}
{"id": "1706.03466", "contents": "Title: Few-Shot Image Recognition by Predicting Parameters from Activations Abstract: In this paper, we are interested in the few-shot learning problem. In\nparticular, we focus on a challenging scenario where the number of categories\nis large and the number of examples per novel category is very limited, e.g. 1,\n2, or 3. Motivated by the close relationship between the parameters and the\nactivations in a neural network associated with the same category, we propose a\nnovel method that can adapt a pre-trained neural network to novel categories by\ndirectly predicting the parameters from the activations. Zero training is\nrequired in adaptation to novel categories, and fast inference is realized by a\nsingle forward pass. We evaluate our method by doing few-shot image recognition\non the ImageNet dataset, which achieves the state-of-the-art classification\naccuracy on novel categories by a significant margin while keeping comparable\nperformance on the large-scale categories. We also test our method on the\nMiniImageNet dataset and it strongly outperforms the previous state-of-the-art\nmethods. \n\n"}
{"id": "1706.03863", "contents": "Title: Criteria Sliders: Learning Continuous Database Criteria via Interactive\n  Ranking Abstract: Large databases are often organized by hand-labeled metadata, or criteria,\nwhich are expensive to collect. We can use unsupervised learning to model\ndatabase variation, but these models are often high dimensional, complex to\nparameterize, or require expert knowledge. We learn low-dimensional continuous\ncriteria via interactive ranking, so that the novice user need only describe\nthe relative ordering of examples. This is formed as semi-supervised label\npropagation in which we maximize the information gained from a limited number\nof examples. Further, we actively suggest data points to the user to rank in a\nmore informative way than existing work. Our efficient approach allows users to\ninteractively organize thousands of data points along 1D and 2D continuous\nsliders. We experiment with datasets of imagery and geometry to demonstrate\nthat our tool is useful for quickly assessing and organizing the content of\nlarge databases. \n\n"}
{"id": "1706.03912", "contents": "Title: SEP-Nets: Small and Effective Pattern Networks Abstract: While going deeper has been witnessed to improve the performance of\nconvolutional neural networks (CNN), going smaller for CNN has received\nincreasing attention recently due to its attractiveness for mobile/embedded\napplications. It remains an active and important topic how to design a small\nnetwork while retaining the performance of large and deep CNNs (e.g., Inception\nNets, ResNets). Albeit there are already intensive studies on compressing the\nsize of CNNs, the considerable drop of performance is still a key concern in\nmany designs. This paper addresses this concern with several new contributions.\nFirst, we propose a simple yet powerful method for compressing the size of deep\nCNNs based on parameter binarization. The striking difference from most\nprevious work on parameter binarization/quantization lies at different\ntreatments of $1\\times 1$ convolutions and $k\\times k$ convolutions ($k>1$),\nwhere we only binarize $k\\times k$ convolutions into binary patterns. The\nresulting networks are referred to as pattern networks. By doing this, we show\nthat previous deep CNNs such as GoogLeNet and Inception-type Nets can be\ncompressed dramatically with marginal drop in performance. Second, in light of\nthe different functionalities of $1\\times 1$ (data projection/transformation)\nand $k\\times k$ convolutions (pattern extraction), we propose a new block\nstructure codenamed the pattern residual block that adds transformed feature\nmaps generated by $1\\times 1$ convolutions to the pattern feature maps\ngenerated by $k\\times k$ convolutions, based on which we design a small network\nwith $\\sim 1$ million parameters. Combining with our parameter binarization, we\nachieve better performance on ImageNet than using similar sized networks\nincluding recently released Google MobileNets. \n\n"}
{"id": "1706.04261", "contents": "Title: The \"something something\" video database for learning and evaluating\n  visual common sense Abstract: Neural networks trained on datasets such as ImageNet have led to major\nadvances in visual object classification. One obstacle that prevents networks\nfrom reasoning more deeply about complex scenes and situations, and from\nintegrating visual knowledge with natural language, like humans do, is their\nlack of common sense knowledge about the physical world. Videos, unlike still\nimages, contain a wealth of detailed information about the physical world.\nHowever, most labelled video datasets represent high-level concepts rather than\ndetailed physical aspects about actions and scenes. In this work, we describe\nour ongoing collection of the \"something-something\" database of video\nprediction tasks whose solutions require a common sense understanding of the\ndepicted situation. The database currently contains more than 100,000 videos\nacross 174 classes, which are defined as caption-templates. We also describe\nthe challenges in crowd-sourcing this data at scale. \n\n"}
{"id": "1706.04306", "contents": "Title: Photo-realistic Facial Texture Transfer Abstract: Style transfer methods have achieved significant success in recent years with\nthe use of convolutional neural networks. However, many of these methods\nconcentrate on artistic style transfer with few constraints on the output image\nappearance. We address the challenging problem of transferring face texture\nfrom a style face image to a content face image in a photorealistic manner\nwithout changing the identity of the original content image. Our framework for\nface texture transfer (FaceTex) augments the prior work of MRF-CNN with a novel\nfacial semantic regularization that incorporates a face prior regularization\nsmoothly suppressing the changes around facial meso-structures (e.g eyes, nose\nand mouth) and a facial structure loss function which implicitly preserves the\nfacial structure so that face texture can be transferred without changing the\noriginal identity. We demonstrate results on face images and compare our\napproach with recent state-of-the-art methods. Our results demonstrate superior\ntexture transfer because of the ability to maintain the identity of the\noriginal face image. \n\n"}
{"id": "1706.04737", "contents": "Title: Suggestive Annotation: A Deep Active Learning Framework for Biomedical\n  Image Segmentation Abstract: Image segmentation is a fundamental problem in biomedical image analysis.\nRecent advances in deep learning have achieved promising results on many\nbiomedical image segmentation benchmarks. However, due to large variations in\nbiomedical images (different modalities, image settings, objects, noise, etc),\nto utilize deep learning on a new application, it usually needs a new set of\ntraining data. This can incur a great deal of annotation effort and cost,\nbecause only biomedical experts can annotate effectively, and often there are\ntoo many instances in images (e.g., cells) to annotate. In this paper, we aim\nto address the following question: With limited effort (e.g., time) for\nannotation, what instances should be annotated in order to attain the best\nperformance? We present a deep active learning framework that combines fully\nconvolutional network (FCN) and active learning to significantly reduce\nannotation effort by making judicious suggestions on the most effective\nannotation areas. We utilize uncertainty and similarity information provided by\nFCN and formulate a generalized version of the maximum set cover problem to\ndetermine the most representative and uncertain areas for annotation. Extensive\nexperiments using the 2015 MICCAI Gland Challenge dataset and a lymph node\nultrasound image segmentation dataset show that, using annotation suggestions\nby our method, state-of-the-art segmentation performance can be achieved by\nusing only 50% of training data. \n\n"}
{"id": "1706.05726", "contents": "Title: Using Deep Networks for Drone Detection Abstract: Drone detection is the problem of finding the smallest rectangle that\nencloses the drone(s) in a video sequence. In this study, we propose a solution\nusing an end-to-end object detection model based on convolutional neural\nnetworks. To solve the scarce data problem for training the network, we propose\nan algorithm for creating an extensive artificial dataset by combining\nbackground-subtracted real images. With this approach, we can achieve precision\nand recall values both of which are high at the same time. \n\n"}
{"id": "1706.06216", "contents": "Title: Dualing GANs Abstract: Generative adversarial nets (GANs) are a promising technique for modeling a\ndistribution from samples. It is however well known that GAN training suffers\nfrom instability due to the nature of its maximin formulation. In this paper,\nwe explore ways to tackle the instability problem by dualizing the\ndiscriminator. We start from linear discriminators in which case conjugate\nduality provides a mechanism to reformulate the saddle point objective into a\nmaximization problem, such that both the generator and the discriminator of\nthis 'dualing GAN' act in concert. We then demonstrate how to extend this\nintuition to non-linear formulations. For GANs with linear discriminators our\napproach is able to remove the instability in training, while for GANs with\nnonlinear discriminators our approach provides an alternative to the commonly\nused GAN training algorithm. \n\n"}
{"id": "1706.06982", "contents": "Title: Two-Stream Convolutional Networks for Dynamic Texture Synthesis Abstract: We introduce a two-stream model for dynamic texture synthesis. Our model is\nbased on pre-trained convolutional networks (ConvNets) that target two\nindependent tasks: (i) object recognition, and (ii) optical flow prediction.\nGiven an input dynamic texture, statistics of filter responses from the object\nrecognition ConvNet encapsulate the per-frame appearance of the input texture,\nwhile statistics of filter responses from the optical flow ConvNet model its\ndynamics. To generate a novel texture, a randomly initialized input sequence is\noptimized to match the feature statistics from each stream of an example\ntexture. Inspired by recent work on image style transfer and enabled by the\ntwo-stream model, we also apply the synthesis approach to combine the texture\nappearance from one texture with the dynamics of another to generate entirely\nnovel dynamic textures. We show that our approach generates novel, high quality\nsamples that match both the framewise appearance and temporal evolution of\ninput texture. Finally, we quantitatively evaluate our texture synthesis\napproach with a thorough user study. \n\n"}
{"id": "1706.07397", "contents": "Title: Fine-Grained Categorization via CNN-Based Automatic Extraction and\n  Integration of Object-Level and Part-Level Features Abstract: Fine-grained categorization can benefit from part-based features which reveal\nsubtle visual differences between object categories. Handcrafted features have\nbeen widely used for part detection and classification. Although a recent trend\nseeks to learn such features automatically using powerful deep learning models\nsuch as convolutional neural networks (CNN), their training and possibly also\ntesting require manually provided annotations which are costly to obtain. To\nrelax these requirements, we assume in this study a general problem setting in\nwhich the raw images are only provided with object-level class labels for model\ntraining with no other side information needed. Specifically, by extracting and\ninterpreting the hierarchical hidden layer features learned by a CNN, we\npropose an elaborate CNN-based system for fine-grained categorization. When\nevaluated on the Caltech-UCSD Birds-200-2011, FGVC-Aircraft, Cars and Stanford\ndogs datasets under the setting that only object-level class labels are used\nfor training and no other annotations are available for both training and\ntesting, our method achieves impressive performance that is superior or\ncomparable to the state of the art. Moreover, it sheds some light on ingenious\nuse of the hierarchical features learned by CNN which has wide applicability\nwell beyond the current fine-grained categorization task. \n\n"}
{"id": "1706.08211", "contents": "Title: End-to-end Learning of Image based Lane-Change Decision Abstract: We propose an image based end-to-end learning framework that helps\nlane-change decisions for human drivers and autonomous vehicles. The proposed\nsystem, Safe Lane-Change Aid Network (SLCAN), trains a deep convolutional\nneural network to classify the status of adjacent lanes from rear view images\nacquired by cameras mounted on both sides of the vehicle. Rather than depending\non any explicit object detection or tracking scheme, SLCAN reads the whole\ninput image and directly decides whether initiation of the lane-change at the\nmoment is safe or not. We collected and annotated 77,273 rear side view images\nto train and test SLCAN. Experimental results show that the proposed framework\nachieves 96.98% classification accuracy although the test images are from\nunseen roadways. We also visualize the saliency map to understand which part of\nimage SLCAN looks at for correct decisions. \n\n"}
{"id": "1706.08606", "contents": "Title: Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study Abstract: Deep neural networks (DNNs) have achieved unprecedented performance on a wide\nrange of complex tasks, rapidly outpacing our understanding of the nature of\ntheir solutions. This has caused a recent surge of interest in methods for\nrendering modern neural systems more interpretable. In this work, we propose to\naddress the interpretability problem in modern DNNs using the rich history of\nproblem descriptions, theories and experimental methods developed by cognitive\npsychologists to study the human mind. To explore the potential value of these\ntools, we chose a well-established analysis from developmental psychology that\nexplains how children learn word labels for objects, and applied that analysis\nto DNNs. Using datasets of stimuli inspired by the original cognitive\npsychology experiments, we find that state-of-the-art one shot learning models\ntrained on ImageNet exhibit a similar bias to that observed in humans: they\nprefer to categorize objects according to shape rather than color. The\nmagnitude of this shape bias varies greatly among architecturally identical,\nbut differently seeded models, and even fluctuates within seeds throughout\ntraining, despite nearly equivalent classification performance. These results\ndemonstrate the capability of tools from cognitive psychology for exposing\nhidden computational properties of DNNs, while concurrently providing us with a\ncomputational model for human word learning. \n\n"}
{"id": "1706.09650", "contents": "Title: Co-salient Object Detection Based on Deep Saliency Networks and Seed\n  Propagation over an Integrated Graph Abstract: This paper presents a co-salient object detection method to find common\nsalient regions in a set of images. We utilize deep saliency networks to\ntransfer co-saliency prior knowledge and better capture high-level semantic\ninformation, and the resulting initial co-saliency maps are enhanced by seed\npropagation steps over an integrated graph. The deep saliency networks are\ntrained in a supervised manner to avoid online weakly supervised learning and\nexploit them not only to extract high-level features but also to produce both\nintra- and inter-image saliency maps. Through a refinement step, the initial\nco-saliency maps can uniformly highlight co-salient regions and locate accurate\nobject boundaries. To handle input image groups inconsistent in size, we\npropose to pool multi-regional descriptors including both within-segment and\nwithin-group information. In addition, the integrated multilayer graph is\nconstructed to find the regions that the previous steps may not detect by seed\npropagation with low-level descriptors. In this work, we utilize the useful\ncomplementary components of high-, low-level information, and several\nlearning-based steps. Our experiments have demonstrated that the proposed\napproach outperforms comparable co-saliency detection methods on widely used\npublic databases and can also be directly applied to co-segmentation tasks. \n\n"}
{"id": "1706.09887", "contents": "Title: Automatic Face Image Quality Prediction Abstract: Face image quality can be defined as a measure of the utility of a face image\nto automatic face recognition. In this work, we propose (and compare) two\nmethods for automatic face image quality based on target face quality values\nfrom (i) human assessments of face image quality (matcher-independent), and\n(ii) quality values computed from similarity scores (matcher-dependent). A\nsupport vector regression model trained on face features extracted using a deep\nconvolutional neural network (ConvNet) is used to predict the quality of a face\nimage. The proposed methods are evaluated on two unconstrained face image\ndatabases, LFW and IJB-A, which both contain facial variations with multiple\nquality factors. Evaluation of the proposed automatic face image quality\nmeasures shows we are able to reduce the FNMR at 1% FMR by at least 13% for two\nface matchers (a COTS matcher and a ConvNet matcher) by using the proposed face\nquality to select subsets of face images and video frames for matching\ntemplates (i.e., multiple faces per subject) in the IJB-A protocol. To our\nknowledge, this is the first work to utilize human assessments of face image\nquality in designing a predictor of unconstrained face quality that is shown to\nbe effective in cross-database evaluation. \n\n"}
{"id": "1707.00095", "contents": "Title: Exploring the Imposition of Synaptic Precision Restrictions For\n  Evolutionary Synthesis of Deep Neural Networks Abstract: A key contributing factor to incredible success of deep neural networks has\nbeen the significant rise on massively parallel computing devices allowing\nresearchers to greatly increase the size and depth of deep neural networks,\nleading to significant improvements in modeling accuracy. Although deeper,\nlarger, or complex deep neural networks have shown considerable promise, the\ncomputational complexity of such networks is a major barrier to utilization in\nresource-starved scenarios. We explore the synaptogenesis of deep neural\nnetworks in the formation of efficient deep neural network architectures within\nan evolutionary deep intelligence framework, where a probabilistic generative\nmodeling strategy is introduced to stochastically synthesize increasingly\nefficient yet effective offspring deep neural networks over generations,\nmimicking evolutionary processes such as heredity, random mutation, and natural\nselection in a probabilistic manner. In this study, we primarily explore the\nimposition of synaptic precision restrictions and its impact on the\nevolutionary synthesis of deep neural networks to synthesize more efficient\nnetwork architectures tailored for resource-starved scenarios. Experimental\nresults show significant improvements in synaptic efficiency (~10X decrease for\nGoogLeNet-based DetectNet) and inference speed (>5X increase for\nGoogLeNet-based DetectNet) while preserving modeling accuracy. \n\n"}
{"id": "1707.00243", "contents": "Title: Deep GrabCut for Object Selection Abstract: Most previous bounding-box-based segmentation methods assume the bounding box\ntightly covers the object of interest. However it is common that a rectangle\ninput could be too large or too small. In this paper, we propose a novel\nsegmentation approach that uses a rectangle as a soft constraint by\ntransforming it into an Euclidean distance map. A convolutional encoder-decoder\nnetwork is trained end-to-end by concatenating images with these distance maps\nas inputs and predicting the object masks as outputs. Our approach gets\naccurate segmentation results given sloppy rectangles while being general for\nboth interactive segmentation and instance segmentation. We show our network\nextends to curve-based input without retraining. We further apply our network\nto instance-level semantic segmentation and resolve any overlap using a\nconditional random field. Experiments on benchmark datasets demonstrate the\neffectiveness of the proposed approaches. \n\n"}
{"id": "1707.01058", "contents": "Title: Skeleton-aided Articulated Motion Generation Abstract: This work make the first attempt to generate articulated human motion\nsequence from a single image. On the one hand, we utilize paired inputs\nincluding human skeleton information as motion embedding and a single human\nimage as appearance reference, to generate novel motion frames, based on the\nconditional GAN infrastructure. On the other hand, a triplet loss is employed\nto pursue appearance-smoothness between consecutive frames. As the proposed\nframework is capable of jointly exploiting the image appearance space and\narticulated/kinematic motion space, it generates realistic articulated motion\nsequence, in contrast to most previous video generation methods which yield\nblurred motion effects. We test our model on two human action datasets\nincluding KTH and Human3.6M, and the proposed framework generates very\npromising results on both datasets. \n\n"}
{"id": "1707.01086", "contents": "Title: Discriminative Localization in CNNs for Weakly-Supervised Segmentation\n  of Pulmonary Nodules Abstract: Automated detection and segmentation of pulmonary nodules on lung computed\ntomography (CT) scans can facilitate early lung cancer diagnosis. Existing\nsupervised approaches for automated nodule segmentation on CT scans require\nvoxel-based annotations for training, which are labor- and time-consuming to\nobtain. In this work, we propose a weakly-supervised method that generates\naccurate voxel-level nodule segmentation trained with image-level labels only.\nBy adapting a convolutional neural network (CNN) trained for image\nclassification, our proposed method learns discriminative regions from the\nactivation maps of convolution units at different scales, and identifies the\ntrue nodule location with a novel candidate-screening framework. Experimental\nresults on the public LIDC-IDRI dataset demonstrate that, our weakly-supervised\nnodule segmentation framework achieves competitive performance compared to a\nfully-supervised CNN-based segmentation method. \n\n"}
{"id": "1707.01253", "contents": "Title: Laplacian-Steered Neural Style Transfer Abstract: Neural Style Transfer based on Convolutional Neural Networks (CNN) aims to\nsynthesize a new image that retains the high-level structure of a content\nimage, rendered in the low-level texture of a style image. This is achieved by\nconstraining the new image to have high-level CNN features similar to the\ncontent image, and lower-level CNN features similar to the style image. However\nin the traditional optimization objective, low-level features of the content\nimage are absent, and the low-level features of the style image dominate the\nlow-level detail structures of the new image. Hence in the synthesized image,\nmany details of the content image are lost, and a lot of inconsistent and\nunpleasing artifacts appear. As a remedy, we propose to steer image synthesis\nwith a novel loss function: the Laplacian loss. The Laplacian matrix\n(\"Laplacian\" in short), produced by a Laplacian operator, is widely used in\ncomputer vision to detect edges and contours. The Laplacian loss measures the\ndifference of the Laplacians, and correspondingly the difference of the detail\nstructures, between the content image and a new image. It is flexible and\ncompatible with the traditional style transfer constraints. By incorporating\nthe Laplacian loss, we obtain a new optimization objective for neural style\ntransfer named Lapstyle. Minimizing this objective will produce a stylized\nimage that better preserves the detail structures of the content image and\neliminates the artifacts. Experiments show that Lapstyle produces more\nappealing stylized images with less artifacts, without compromising their\n\"stylishness\". \n\n"}
{"id": "1707.01274", "contents": "Title: Learning-based Image Enhancement for Visual Odometry in Challenging HDR\n  Environments Abstract: One of the main open challenges in visual odometry (VO) is the robustness to\ndifficult illumination conditions or high dynamic range (HDR) environments. The\nmain difficulties in these situations come from both the limitations of the\nsensors and the inability to perform a successful tracking of interest points\nbecause of the bold assumptions in VO, such as brightness constancy. We address\nthis problem from a deep learning perspective, for which we first fine-tune a\nDeep Neural Network (DNN) with the purpose of obtaining enhanced\nrepresentations of the sequences for VO. Then, we demonstrate how the insertion\nof Long Short Term Memory (LSTM) allows us to obtain temporally consistent\nsequences, as the estimation depends on previous states. However, the use of\nvery deep networks does not allow the insertion into a real-time VO framework;\ntherefore, we also propose a Convolutional Neural Network (CNN) of reduced size\ncapable of performing faster. Finally, we validate the enhanced representations\nby evaluating the sequences produced by the two architectures in several\nstate-of-art VO algorithms, such as ORB-SLAM and DSO. \n\n"}
{"id": "1707.01400", "contents": "Title: AlignGAN: Learning to Align Cross-Domain Images with Conditional\n  Generative Adversarial Networks Abstract: Recently, several methods based on generative adversarial network (GAN) have\nbeen proposed for the task of aligning cross-domain images or learning a joint\ndistribution of cross-domain images. One of the methods is to use conditional\nGAN for alignment. However, previous attempts of adopting conditional GAN do\nnot perform as well as other methods. In this work we present an approach for\nimproving the capability of the methods which are based on conditional GAN. We\nevaluate the proposed method on numerous tasks and the experimental results\nshow that it is able to align the cross-domain images successfully in absence\nof paired samples. Furthermore, we also propose another model which conditions\non multiple information such as domain information and label information.\nConditioning on domain information and label information, we are able to\nconduct label propagation from the source domain to the target domain. A 2-step\nalternating training algorithm is proposed to learn this model. \n\n"}
{"id": "1707.01943", "contents": "Title: A causal framework for explaining the predictions of black-box\n  sequence-to-sequence models Abstract: We interpret the predictions of any black-box structured input-structured\noutput model around a specific input-output pair. Our method returns an\n\"explanation\" consisting of groups of input-output tokens that are causally\nrelated. These dependencies are inferred by querying the black-box model with\nperturbed inputs, generating a graph over tokens from the responses, and\nsolving a partitioning problem to select the most relevant components. We focus\nthe general approach on sequence-to-sequence problems, adopting a variational\nautoencoder to yield meaningful input perturbations. We test our method across\nseveral NLP sequence generation tasks. \n\n"}
{"id": "1707.02290", "contents": "Title: TasselNet: Counting maize tassels in the wild via local counts\n  regression network Abstract: Accurately counting maize tassels is important for monitoring the growth\nstatus of maize plants. This tedious task, however, is still mainly done by\nmanual efforts. In the context of modern plant phenotyping, automating this\ntask is required to meet the need of large-scale analysis of genotype and\nphenotype. In recent years, computer vision technologies have experienced a\nsignificant breakthrough due to the emergence of large-scale datasets and\nincreased computational resources. Naturally image-based approaches have also\nreceived much attention in plant-related studies. Yet a fact is that most\nimage-based systems for plant phenotyping are deployed under controlled\nlaboratory environment. When transferring the application scenario to\nunconstrained in-field conditions, intrinsic and extrinsic variations in the\nwild pose great challenges for accurate counting of maize tassels, which goes\nbeyond the ability of conventional image processing techniques. This calls for\nfurther robust computer vision approaches to address in-field variations. This\npaper studies the in-field counting problem of maize tassels. To our knowledge,\nthis is the first time that a plant-related counting problem is considered\nusing computer vision technologies under unconstrained field-based environment. \n\n"}
{"id": "1707.02477", "contents": "Title: Hyperspectral Image Restoration via Total Variation Regularized Low-rank\n  Tensor Decomposition Abstract: Hyperspectral images (HSIs) are often corrupted by a mixture of several types\nof noise during the acquisition process, e.g., Gaussian noise, impulse noise,\ndead lines, stripes, and many others. Such complex noise could degrade the\nquality of the acquired HSIs, limiting the precision of the subsequent\nprocessing. In this paper, we present a novel tensor-based HSI restoration\napproach by fully identifying the intrinsic structures of the clean HSI part\nand the mixed noise part respectively. Specifically, for the clean HSI part, we\nuse tensor Tucker decomposition to describe the global correlation among all\nbands, and an anisotropic spatial-spectral total variation (SSTV)\nregularization to characterize the piecewise smooth structure in both spatial\nand spectral domains. For the mixed noise part, we adopt the $\\ell_1$ norm\nregularization to detect the sparse noise, including stripes, impulse noise,\nand dead pixels. Despite that TV regulariztion has the ability of removing\nGaussian noise, the Frobenius norm term is further used to model heavy Gaussian\nnoise for some real-world scenarios. Then, we develop an efficient algorithm\nfor solving the resulting optimization problem by using the augmented Lagrange\nmultiplier (ALM) method. Finally, extensive experiments on simulated and\nreal-world noise HSIs are carried out to demonstrate the superiority of the\nproposed method over the existing state-of-the-art ones. \n\n"}
{"id": "1707.03194", "contents": "Title: Sensitivity Analysis for Mirror-Stratifiable Convex Functions Abstract: This paper provides a set of sensitivity analysis and activity identification\nresults for a class of convex functions with a strong geometric structure, that\nwe coined \"mirror-stratifiable\". These functions are such that there is a\nbijection between a primal and a dual stratification of the space into\npartitioning sets, called strata. This pairing is crucial to track the strata\nthat are identifiable by solutions of parametrized optimization problems or by\niterates of optimization algorithms. This class of functions encompasses all\nregularizers routinely used in signal and image processing, machine learning,\nand statistics. We show that this \"mirror-stratifiable\" structure enjoys a nice\nsensitivity theory, allowing us to study stability of solutions of optimization\nproblems to small perturbations, as well as activity identification of\nfirst-order proximal splitting-type algorithms. Existing results in the\nliterature typically assume that, under a non-degeneracy condition, the active\nset associated to a minimizer is stable to small perturbations and is\nidentified in finite time by optimization schemes. In contrast, our results do\nnot require any non-degeneracy assumption: in consequence, the optimal active\nset is not necessarily stable anymore, but we are able to track precisely the\nset of identifiable strata.We show that these results have crucial implications\nwhen solving challenging ill-posed inverse problems via regularization, a\ntypical scenario where the non-degeneracy condition is not fulfilled. Our\ntheoretical results, illustrated by numerical simulations, allow to\ncharacterize the instability behaviour of the regularized solutions, by\nlocating the set of all low-dimensional strata that can be potentially\nidentified by these solutions. \n\n"}
{"id": "1707.03548", "contents": "Title: Discriminative Block-Diagonal Representation Learning for Image\n  Recognition Abstract: Existing block-diagonal representation researches mainly focuses on casting\nblock-diagonal regularization on training data, while only little attention is\ndedicated to concurrently learning both block-diagonal representations of\ntraining and test data. In this paper, we propose a discriminative\nblock-diagonal low-rank representation (BDLRR) method for recognition. In\nparticular, the elaborate BDLRR is formulated as a joint optimization problem\nof shrinking the unfavorable representation from off-block-diagonal elements\nand strengthening the compact block-diagonal representation under the\nsemi-supervised framework of low-rank representation. To this end, we first\nimpose penalty constraints on the negative representation to eliminate the\ncorrelation between different classes such that the incoherence criterion of\nthe extra-class representation is boosted. Moreover, a constructed subspace\nmodel is developed to enhance the self-expressive power of training samples and\nfurther build the representation bridge between the training and test samples,\nsuch that the coherence of the learned intra-class representation is\nconsistently heightened. Finally, the resulting optimization problem is solved\nelegantly by employing an alternative optimization strategy, and a simple\nrecognition algorithm on the learned representation is utilized for final\nprediction. Extensive experimental results demonstrate that the proposed method\nachieves superb recognition results on four face image datasets, three\ncharacter datasets, and the fifteen scene multi-categories dataset. It not only\nshows superior potential on image recognition but also outperforms\nstate-of-the-art methods. \n\n"}
{"id": "1707.03742", "contents": "Title: Large-scale Multiview 3D Hand Pose Dataset Abstract: Accurate hand pose estimation at joint level has several uses on human-robot\ninteraction, user interfacing and virtual reality applications. Yet, it\ncurrently is not a solved problem. The novel deep learning techniques could\nmake a great improvement on this matter but they need a huge amount of\nannotated data. The hand pose datasets released so far present some issues that\nmake them impossible to use on deep learning methods such as the few number of\nsamples, high-level abstraction annotations or samples consisting in depth\nmaps. In this work, we introduce a multiview hand pose dataset in which we\nprovide color images of hands and different kind of annotations for each, i.e\nthe bounding box and the 2D and 3D location on the joints in the hand. Besides,\nwe introduce a simple yet accurate deep learning architecture for real-time\nrobust 2D hand pose estimation. \n\n"}
{"id": "1707.04318", "contents": "Title: Discriminative Optimization: Theory and Applications to Computer Vision\n  Problems Abstract: Many computer vision problems are formulated as the optimization of a cost\nfunction. This approach faces two main challenges: (i) designing a cost\nfunction with a local optimum at an acceptable solution, and (ii) developing an\nefficient numerical method to search for one (or multiple) of these local\noptima. While designing such functions is feasible in the noiseless case, the\nstability and location of local optima are mostly unknown under noise,\nocclusion, or missing data. In practice, this can result in undesirable local\noptima or not having a local optimum in the expected place. On the other hand,\nnumerical optimization algorithms in high-dimensional spaces are typically\nlocal and often rely on expensive first or second order information to guide\nthe search. To overcome these limitations, this paper proposes Discriminative\nOptimization (DO), a method that learns search directions from data without the\nneed of a cost function. Specifically, DO explicitly learns a sequence of\nupdates in the search space that leads to stationary points that correspond to\ndesired solutions. We provide a formal analysis of DO and illustrate its\nbenefits in the problem of 3D point cloud registration, camera pose estimation,\nand image denoising. We show that DO performed comparably or outperformed\nstate-of-the-art algorithms in terms of accuracy, robustness to perturbations,\nand computational efficiency. \n\n"}
{"id": "1707.04877", "contents": "Title: Optical Music Recognition with Convolutional Sequence-to-Sequence Models Abstract: Optical Music Recognition (OMR) is an important technology within Music\nInformation Retrieval. Deep learning models show promising results on OMR\ntasks, but symbol-level annotated data sets of sufficient size to train such\nmodels are not available and difficult to develop. We present a deep learning\narchitecture called a Convolutional Sequence-to-Sequence model to both move\ntowards an end-to-end trainable OMR pipeline, and apply a learning process that\ntrains on full sentences of sheet music instead of individually labeled\nsymbols. The model is trained and evaluated on a human generated data set, with\nvarious image augmentations based on real-world scenarios. This data set is the\nfirst publicly available set in OMR research with sufficient size to train and\nevaluate deep learning models. With the introduced augmentations a pitch\nrecognition accuracy of 81% and a duration accuracy of 94% is achieved,\nresulting in a note level accuracy of 80%. Finally, the model is compared to\ncommercially available methods, showing a large improvements over these\napplications. \n\n"}
{"id": "1707.05414", "contents": "Title: Wide Inference Network for Image Denoising via Learning\n  Pixel-distribution Prior Abstract: We explore an innovative strategy for image denoising by using convolutional\nneural networks (CNN) to learn similar pixel-distribution features from noisy\nimages. Many types of image noise follow a certain pixel-distribution in\ncommon, such as additive white Gaussian noise (AWGN). By increasing CNN's width\nwith larger reception fields and more channels in each layer, CNNs can reveal\nthe ability to extract more accurate pixel-distribution features. The key to\nour approach is a discovery that wider CNNs with more convolutions tend to\nlearn the similar pixel-distribution features, which reveals a new strategy to\nsolve low-level vision problems effectively that the inference mapping\nprimarily relies on the priors behind the noise property instead of deeper CNNs\nwith more stacked nonlinear layers. We evaluate our work, Wide inference\nNetworks (WIN), on AWGN and demonstrate that by learning pixel-distribution\nfeatures from images, WIN-based network consistently achieves significantly\nbetter performance than current state-of-the-art deep CNN-based methods in both\nquantitative and visual evaluations. \\textit{Code and models are available at\n\\url{https://github.com/cswin/WIN}}. \n\n"}
{"id": "1707.06375", "contents": "Title: 3D Shape Reconstruction from Sketches via Multi-view Convolutional\n  Networks Abstract: We propose a method for reconstructing 3D shapes from 2D sketches in the form\nof line drawings. Our method takes as input a single sketch, or multiple\nsketches, and outputs a dense point cloud representing a 3D reconstruction of\nthe input sketch(es). The point cloud is then converted into a polygon mesh. At\nthe heart of our method lies a deep, encoder-decoder network. The encoder\nconverts the sketch into a compact representation encoding shape information.\nThe decoder converts this representation into depth and normal maps capturing\nthe underlying surface from several output viewpoints. The multi-view maps are\nthen consolidated into a 3D point cloud by solving an optimization problem that\nfuses depth and normals across all viewpoints. Based on our experiments,\ncompared to other methods, such as volumetric networks, our architecture offers\nseveral advantages, including more faithful reconstruction, higher output\nsurface resolution, better preservation of topology and shape structure. \n\n"}
{"id": "1707.06436", "contents": "Title: cvpaper.challenge in 2016: Futuristic Computer Vision through 1,600\n  Papers Survey Abstract: The paper gives futuristic challenges disscussed in the cvpaper.challenge. In\n2015 and 2016, we thoroughly study 1,600+ papers in several\nconferences/journals such as CVPR/ICCV/ECCV/NIPS/PAMI/IJCV. \n\n"}
{"id": "1707.06923", "contents": "Title: Multi-kernel learning of deep convolutional features for action\n  recognition Abstract: Image understanding using deep convolutional network has reached human-level\nperformance, yet a closely related problem of video understanding especially,\naction recognition has not reached the requisite level of maturity. We combine\nmulti-kernels based support-vector-machines (SVM) with a multi-stream deep\nconvolutional neural network to achieve close to state-of-the-art performance\non a 51-class activity recognition problem (HMDB-51 dataset); this specific\ndataset has proved to be particularly challenging for deep neural networks due\nto the heterogeneity in camera viewpoints, video quality, etc. The resulting\narchitecture is named pillar networks as each (very) deep neural network acts\nas a pillar for the hierarchical classifiers. In addition, we illustrate that\nhand-crafted features such as improved dense trajectories (iDT) and Multi-skip\nFeature Stacking (MIFS), as additional pillars, can further supplement the\nperformance. \n\n"}
{"id": "1707.07074", "contents": "Title: What-and-Where to Match: Deep Spatially Multiplicative Integration\n  Networks for Person Re-identification Abstract: Matching pedestrians across disjoint camera views, known as person\nre-identification (re-id), is a challenging problem that is of importance to\nvisual recognition and surveillance. Most existing methods exploit local\nregions within spatial manipulation to perform matching in local\ncorrespondence. However, they essentially extract \\emph{fixed} representations\nfrom pre-divided regions for each image and perform matching based on the\nextracted representation subsequently. For models in this pipeline, local finer\npatterns that are crucial to distinguish positive pairs from negative ones\ncannot be captured, and thus making them underperformed. In this paper, we\npropose a novel deep multiplicative integration gating function, which answers\nthe question of \\emph{what-and-where to match} for effective person re-id. To\naddress \\emph{what} to match, our deep network emphasizes common local patterns\nby learning joint representations in a multiplicative way. The network\ncomprises two Convolutional Neural Networks (CNNs) to extract convolutional\nactivations, and generates relevant descriptors for pedestrian matching. This\nthus, leads to flexible representations for pair-wise images. To address\n\\emph{where} to match, we combat the spatial misalignment by performing\nspatially recurrent pooling via a four-directional recurrent neural network to\nimpose spatial dependency over all positions with respect to the entire image.\nThe proposed network is designed to be end-to-end trainable to characterize\nlocal pairwise feature interactions in a spatially aligned manner. To\ndemonstrate the superiority of our method, extensive experiments are conducted\nover three benchmark data sets: VIPeR, CUHK03 and Market-1501. \n\n"}
{"id": "1707.07165", "contents": "Title: Coarse-to-Fine Lifted MAP Inference in Computer Vision Abstract: There is a vast body of theoretical research on lifted inference in\nprobabilistic graphical models (PGMs). However, few demonstrations exist where\nlifting is applied in conjunction with top of the line applied algorithms. We\npursue the applicability of lifted inference for computer vision (CV), with the\ninsight that a globally optimal (MAP) labeling will likely have the same label\nfor two symmetric pixels. The success of our approach lies in efficiently\nhandling a distinct unary potential on every node (pixel), typical of CV\napplications. This allows us to lift the large class of algorithms that model a\nCV problem via PGM inference. We propose a generic template for coarse-to-fine\n(C2F) inference in CV, which progressively refines an initial coarsely lifted\nPGM for varying quality-time trade-offs. We demonstrate the performance of C2F\ninference by developing lifted versions of two near state-of-the-art CV\nalgorithms for stereo vision and interactive image segmentation. We find that,\nagainst flat algorithms, the lifted versions have a much superior anytime\nperformance, without any loss in final solution quality. \n\n"}
{"id": "1707.07381", "contents": "Title: Group-wise Deep Co-saliency Detection Abstract: In this paper, we propose an end-to-end group-wise deep co-saliency detection\napproach to address the co-salient object discovery problem based on the fully\nconvolutional network (FCN) with group input and group output. The proposed\napproach captures the group-wise interaction information for group images by\nlearning a semantics-aware image representation based on a convolutional neural\nnetwork, which adaptively learns the group-wise features for co-saliency\ndetection. Furthermore, the proposed approach discovers the collaborative and\ninteractive relationships between group-wise feature representation and\nsingle-image individual feature representation, and model this in a\ncollaborative learning framework. Finally, we set up a unified end-to-end deep\nlearning scheme to jointly optimize the process of group-wise feature\nrepresentation learning and the collaborative learning, leading to more\nreliable and robust co-saliency detection results. Experimental results\ndemonstrate the effectiveness of our approach in comparison with the\nstate-of-the-art approaches. \n\n"}
{"id": "1707.07815", "contents": "Title: Graph-Theoretic Spatiotemporal Context Modeling for Video Saliency\n  Detection Abstract: As an important and challenging problem in computer vision, video saliency\ndetection is typically cast as a spatiotemporal context modeling problem over\nconsecutive frames. As a result, a key issue in video saliency detection is how\nto effectively capture the intrinsical properties of atomic video structures as\nwell as their associated contextual interactions along the spatial and temporal\ndimensions. Motivated by this observation, we propose a graph-theoretic video\nsaliency detection approach based on adaptive video structure discovery, which\nis carried out within a spatiotemporal atomic graph. Through graph-based\nmanifold propagation, the proposed approach is capable of effectively modeling\nthe semantically contextual interactions among atomic video structures for\nsaliency detection while preserving spatial smoothness and temporal\nconsistency. Experiments demonstrate the effectiveness of the proposed approach\nover several benchmark datasets. \n\n"}
{"id": "1707.08301", "contents": "Title: Graph-Based Classification of Omnidirectional Images Abstract: Omnidirectional cameras are widely used in such areas as robotics and virtual\nreality as they provide a wide field of view. Their images are often processed\nwith classical methods, which might unfortunately lead to non-optimal solutions\nas these methods are designed for planar images that have different geometrical\nproperties than omnidirectional ones. In this paper we study image\nclassification task by taking into account the specific geometry of\nomnidirectional cameras with graph-based representations. In particular, we\nextend deep learning architectures to data on graphs; we propose a principled\nway of graph construction such that convolutional filters respond similarly for\nthe same pattern on different positions of the image regardless of lens\ndistortions. Our experiments show that the proposed method outperforms current\ntechniques for the omnidirectional image classification problem. \n\n"}
{"id": "1707.08559", "contents": "Title: Video Highlight Prediction Using Audience Chat Reactions Abstract: Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures. \n\n"}
{"id": "1707.09135", "contents": "Title: Learning Pixel-Distribution Prior with Wider Convolution for Image\n  Denoising Abstract: In this work, we explore an innovative strategy for image denoising by using\nconvolutional neural networks (CNN) to learn pixel-distribution from noisy\ndata. By increasing CNN's width with large reception fields and more channels\nin each layer, CNNs can reveal the ability to learn pixel-distribution, which\nis a prior existing in many different types of noise. The key to our approach\nis a discovery that wider CNNs tends to learn the pixel-distribution features,\nwhich provides the probability of that inference-mapping primarily relies on\nthe priors instead of deeper CNNs with more stacked nonlinear layers. We\nevaluate our work: Wide inference Networks (WIN) on additive white Gaussian\nnoise (AWGN) and demonstrate that by learning the pixel-distribution in images,\nWIN-based network consistently achieves significantly better performance than\ncurrent state-of-the-art deep CNN-based methods in both quantitative and visual\nevaluations. \\textit{Code and models are available at\n\\url{https://github.com/cswin/WIN}}. \n\n"}
{"id": "1707.09457", "contents": "Title: Men Also Like Shopping: Reducing Gender Bias Amplification using\n  Corpus-level Constraints Abstract: Language is increasingly being used to define rich visual recognition\nproblems with supporting image collections sourced from the web. Structured\nprediction models are used in these tasks to take advantage of correlations\nbetween co-occurring labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models trained on these datasets further amplify existing bias.\nFor example, the activity cooking is over 33% more likely to involve females\nthan males in a training set, and a trained model further amplifies the\ndisparity to 68% at test time. We propose to inject corpus-level constraints\nfor calibrating existing structured prediction models and design an algorithm\nbased on Lagrangian relaxation for collective inference. Our method results in\nalmost no performance loss for the underlying recognition task but decreases\nthe magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively. \n\n"}
{"id": "1707.09700", "contents": "Title: Scene Graph Generation from Objects, Phrases and Region Captions Abstract: Object detection, scene graph generation and region captioning, which are\nthree scene understanding tasks at different semantic levels, are tied\ntogether: scene graphs are generated on top of objects detected in an image\nwith their pairwise relationship predicted, while region captioning gives a\nlanguage description of the objects, their attributes, relations, and other\ncontext information. In this work, to leverage the mutual connections across\nsemantic levels, we propose a novel neural network model, termed as Multi-level\nScene Description Network (denoted as MSDN), to solve the three vision tasks\njointly in an end-to-end manner. Objects, phrases, and caption regions are\nfirst aligned with a dynamic graph based on their spatial and semantic\nconnections. Then a feature refining structure is used to pass messages across\nthe three levels of semantic tasks through the graph. We benchmark the learned\nmodel on three tasks, and show the joint learning across three tasks with our\nproposed method can bring mutual improvements over previous models.\nParticularly, on the scene graph generation task, our proposed method\noutperforms the state-of-art method with more than 3% margin. \n\n"}
{"id": "1708.00583", "contents": "Title: A Learning-based Framework for Hybrid Depth-from-Defocus and Stereo\n  Matching Abstract: Depth from defocus (DfD) and stereo matching are two most studied passive\ndepth sensing schemes. The techniques are essentially complementary: DfD can\nrobustly handle repetitive textures that are problematic for stereo matching\nwhereas stereo matching is insensitive to defocus blurs and can handle large\ndepth range. In this paper, we present a unified learning-based technique to\nconduct hybrid DfD and stereo matching. Our input is image triplets: a stereo\npair and a defocused image of one of the stereo views. We first apply\ndepth-guided light field rendering to construct a comprehensive training\ndataset for such hybrid sensing setups. Next, we adopt the hourglass network\narchitecture to separately conduct depth inference from DfD and stereo.\nFinally, we exploit different connection methods between the two separate\nnetworks for integrating them into a unified solution to produce high fidelity\n3D disparity maps. Comprehensive experiments on real and synthetic data show\nthat our new learning-based hybrid 3D sensing technique can significantly\nimprove accuracy and robustness in 3D reconstruction. \n\n"}
{"id": "1708.00636", "contents": "Title: Generation of High Dynamic Range Illumination from a Single Image for\n  the Enhancement of Undesirably Illuminated Images Abstract: This paper presents an algorithm that enhances undesirably illuminated images\nby generating and fusing multi-level illuminations from a single image.The\ninput image is first decomposed into illumination and reflectance components by\nusing an edge-preserving smoothing filter. Then the reflectance component is\nscaled up to improve the image details in bright areas. The illumination\ncomponent is scaled up and down to generate several illumination images that\ncorrespond to certain camera exposure values different from the original. The\nvirtual multi-exposure illuminations are blended into an enhanced illumination,\nwhere we also propose a method to generate appropriate weight maps for the tone\nfusion. Finally, an enhanced image is obtained by multiplying the equalized\nillumination and enhanced reflectance. Experiments show that the proposed\nalgorithm produces visually pleasing output and also yields comparable\nobjective results to the conventional enhancement methods, while requiring\nmodest computational loads. \n\n"}
{"id": "1708.00674", "contents": "Title: Deep Detection of People and their Mobility Aids for a Hospital Robot Abstract: Robots operating in populated environments encounter many different types of\npeople, some of whom might have an advanced need for cautious interaction,\nbecause of physical impairments or their advanced age. Robots therefore need to\nrecognize such advanced demands to provide appropriate assistance, guidance or\nother forms of support. In this paper, we propose a depth-based perception\npipeline that estimates the position and velocity of people in the environment\nand categorizes them according to the mobility aids they use: pedestrian,\nperson in wheelchair, person in a wheelchair with a person pushing them, person\nwith crutches and person using a walker. We present a fast region proposal\nmethod that feeds a Region-based Convolutional Network (Fast R-CNN). With this,\nwe speed up the object detection process by a factor of seven compared to a\ndense sliding window approach. We furthermore propose a probabilistic position,\nvelocity and class estimator to smooth the CNN's detections and account for\nocclusions and misclassifications. In addition, we introduce a new hospital\ndataset with over 17,000 annotated RGB-D images. Extensive experiments confirm\nthat our pipeline successfully keeps track of people and their mobility aids,\neven in challenging situations with multiple people from different categories\nand frequent occlusions. Videos of our experiments and the dataset are\navailable at http://www2.informatik.uni-freiburg.de/~kollmitz/MobilityAids \n\n"}
{"id": "1708.01447", "contents": "Title: Video Salient Object Detection Using Spatiotemporal Deep Features Abstract: This paper presents a method for detecting salient objects in videos where\ntemporal information in addition to spatial information is fully taken into\naccount. Following recent reports on the advantage of deep features over\nconventional hand-crafted features, we propose a new set of SpatioTemporal Deep\n(STD) features that utilize local and global contexts over frames. We also\npropose new SpatioTemporal Conditional Random Field (STCRF) to compute saliency\nfrom STD features. STCRF is our extension of CRF to the temporal domain and\ndescribes the relationships among neighboring regions both in a frame and over\nframes. STCRF leads to temporally consistent saliency maps over frames,\ncontributing to the accurate detection of salient objects' boundaries and noise\nreduction during detection. Our proposed method first segments an input video\ninto multiple scales and then computes a saliency map at each scale level using\nSTD features with STCRF. The final saliency map is computed by fusing saliency\nmaps at different scale levels. Our experiments, using publicly available\nbenchmark datasets, confirm that the proposed method significantly outperforms\nstate-of-the-art methods. We also applied our saliency computation to the video\nobject segmentation task, showing that our method outperforms existing video\nobject segmentation methods. \n\n"}
{"id": "1708.01471", "contents": "Title: Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for\n  Visual Question Answering Abstract: Visual question answering (VQA) is challenging because it requires a\nsimultaneous understanding of both the visual content of images and the textual\ncontent of questions. The approaches used to represent the images and questions\nin a fine-grained manner and questions and to fuse these multi-modal features\nplay key roles in performance. Bilinear pooling based models have been shown to\noutperform traditional linear models for VQA, but their high-dimensional\nrepresentations and high computational complexity may seriously limit their\napplicability in practice. For multi-modal feature fusion, here we develop a\nMulti-modal Factorized Bilinear (MFB) pooling approach to efficiently and\neffectively combine multi-modal features, which results in superior performance\nfor VQA compared with other bilinear pooling approaches. For fine-grained image\nand question representation, we develop a co-attention mechanism using an\nend-to-end deep network architecture to jointly learn both the image and\nquestion attentions. Combining the proposed MFB approach with co-attention\nlearning in a new network architecture provides a unified model for VQA. Our\nexperimental results demonstrate that the single MFB with co-attention model\nachieves new state-of-the-art performance on the real-world VQA dataset. Code\navailable at https://github.com/yuzcccc/mfb. \n\n"}
{"id": "1708.01663", "contents": "Title: Accelerated Image Reconstruction for Nonlinear Diffractive Imaging Abstract: The problem of reconstructing an object from the measurements of the light it\nscatters is common in numerous imaging applications. While the most popular\nformulations of the problem are based on linearizing the object-light\nrelationship, there is an increased interest in considering nonlinear\nformulations that can account for multiple light scattering. In this paper, we\npropose an image reconstruction method, called CISOR, for nonlinear diffractive\nimaging, based on a nonconvex optimization formulation with total variation\n(TV) regularization. The nonconvex solver used in CISOR is our new variant of\nfast iterative shrinkage/thresholding algorithm (FISTA). We provide fast and\nmemory-efficient implementation of the new FISTA variant and prove that it\nreliably converges for our nonconvex optimization problem. In addition, we\nsystematically compare our method with other state-of-the-art methods on\nsimulated as well as experimentally measured data in both 2D and 3D settings. \n\n"}
{"id": "1708.01885", "contents": "Title: Long Short-Term Memory Kalman Filters:Recurrent Neural Estimators for\n  Pose Regularization Abstract: One-shot pose estimation for tasks such as body joint localization, camera\npose estimation, and object tracking are generally noisy, and temporal filters\nhave been extensively used for regularization. One of the most widely-used\nmethods is the Kalman filter, which is both extremely simple and general.\nHowever, Kalman filters require a motion model and measurement model to be\nspecified a priori, which burdens the modeler and simultaneously demands that\nwe use explicit models that are often only crude approximations of reality. For\nexample, in the pose-estimation tasks mentioned above, it is common to use\nmotion models that assume constant velocity or constant acceleration, and we\nbelieve that these simplified representations are severely inhibitive. In this\nwork, we propose to instead learn rich, dynamic representations of the motion\nand noise models. In particular, we propose learning these models from data\nusing long short term memory, which allows representations that depend on all\nprevious observations and all previous states. We evaluate our method using\nthree of the most popular pose estimation tasks in computer vision, and in all\ncases we obtain state-of-the-art performance. \n\n"}
{"id": "1708.01894", "contents": "Title: EndNet: Sparse AutoEncoder Network for Endmember Extraction and\n  Hyperspectral Unmixing Abstract: Data acquired from multi-channel sensors is a highly valuable asset to\ninterpret the environment for a variety of remote sensing applications.\nHowever, low spatial resolution is a critical limitation for previous sensors\nand the constituent materials of a scene can be mixed in different fractions\ndue to their spatial interactions. Spectral unmixing is a technique that allows\nus to obtain the material spectral signatures and their fractions from\nhyperspectral data. In this paper, we propose a novel endmember extraction and\nhyperspectral unmixing scheme, so called \\textit{EndNet}, that is based on a\ntwo-staged autoencoder network. This well-known structure is completely\nenhanced and restructured by introducing additional layers and a projection\nmetric (i.e., spectral angle distance (SAD) instead of inner product) to\nachieve an optimum solution. Moreover, we present a novel loss function that is\ncomposed of a Kullback-Leibler divergence term with SAD similarity and\nadditional penalty terms to improve the sparsity of the estimates. These\nmodifications enable us to set the common properties of endmembers such as\nnon-linearity and sparsity for autoencoder networks. Lastly, due to the\nstochastic-gradient based approach, the method is scalable for large-scale data\nand it can be accelerated on Graphical Processing Units (GPUs). To demonstrate\nthe superiority of our proposed method, we conduct extensive experiments on\nseveral well-known datasets. The results confirm that the proposed method\nconsiderably improves the performance compared to the state-of-the-art\ntechniques in literature. \n\n"}
{"id": "1708.02002", "contents": "Title: Focal Loss for Dense Object Detection Abstract: The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron. \n\n"}
{"id": "1708.02136", "contents": "Title: MonoPerfCap: Human Performance Capture from Monocular Video Abstract: We present the first marker-less approach for temporally coherent 3D\nperformance capture of a human with general clothing from monocular video. Our\napproach reconstructs articulated human skeleton motion as well as medium-scale\nnon-rigid surface deformations in general scenes. Human performance capture is\na challenging problem due to the large range of articulation, potentially fast\nmotion, and considerable non-rigid deformations, even from multi-view data.\nReconstruction from monocular video alone is drastically more challenging,\nsince strong occlusions and the inherent depth ambiguity lead to a highly\nill-posed reconstruction problem. We tackle these challenges by a novel\napproach that employs sparse 2D and 3D human pose detections from a\nconvolutional neural network using a batch-based pose estimation strategy.\nJoint recovery of per-batch motion allows to resolve the ambiguities of the\nmonocular reconstruction problem based on a low dimensional trajectory\nsubspace. In addition, we propose refinement of the surface geometry based on\nfully automatically extracted silhouettes to enable medium-scale non-rigid\nalignment. We demonstrate state-of-the-art performance capture results that\nenable exciting applications such as video editing and free viewpoint video,\npreviously infeasible from monocular video. Our qualitative and quantitative\nevaluation demonstrates that our approach significantly outperforms previous\nmonocular methods in terms of accuracy, robustness and scene complexity that\ncan be handled. \n\n"}
{"id": "1708.02286", "contents": "Title: Jointly Attentive Spatial-Temporal Pooling Networks for Video-based\n  Person Re-Identification Abstract: Person Re-Identification (person re-id) is a crucial task as its applications\nin visual surveillance and human-computer interaction. In this work, we present\na novel joint Spatial and Temporal Attention Pooling Network (ASTPN) for\nvideo-based person re-identification, which enables the feature extractor to be\naware of the current input video sequences, in a way that interdependency from\nthe matching items can directly influence the computation of each other's\nrepresentation. Specifically, the spatial pooling layer is able to select\nregions from each frame, while the attention temporal pooling performed can\nselect informative frames over the sequence, both pooling guided by the\ninformation from distance matching. Experiments are conduced on the iLIDS-VID,\nPRID-2011 and MARS datasets and the results demonstrate that this approach\noutperforms existing state-of-art methods. We also analyze how the joint\npooling in both dimensions can boost the person re-id performance more\neffectively than using either of them separately. \n\n"}
{"id": "1708.02386", "contents": "Title: Learning a Repression Network for Precise Vehicle Search Abstract: The growing explosion in the use of surveillance cameras in public security\nhighlights the importance of vehicle search from large-scale image databases.\nPrecise vehicle search, aiming at finding out all instances for a given query\nvehicle image, is a challenging task as different vehicles will look very\nsimilar to each other if they share same visual attributes. To address this\nproblem, we propose the Repression Network (RepNet), a novel multi-task\nlearning framework, to learn discriminative features for each vehicle image\nfrom both coarse-grained and detailed level simultaneously. Besides, benefited\nfrom the satisfactory accuracy of attribute classification, a bucket search\nmethod is proposed to reduce the retrieval time while still maintaining\ncompetitive performance. We conduct extensive experiments on the revised\nVehcileID dataset. Experimental results show that our RepNet achieves the\nstate-of-the-art performance and the bucket search method can reduce the\nretrieval time by about 24 times. \n\n"}
{"id": "1708.02478", "contents": "Title: From Deterministic to Generative: Multi-Modal Stochastic RNNs for Video\n  Captioning Abstract: Video captioning in essential is a complex natural process, which is affected\nby various uncertainties stemming from video content, subjective judgment, etc.\nIn this paper we build on the recent progress in using encoder-decoder\nframework for video captioning and address what we find to be a critical\ndeficiency of the existing methods, that most of the decoders propagate\ndeterministic hidden states. Such complex uncertainty cannot be modeled\nefficiently by the deterministic models. In this paper, we propose a generative\napproach, referred to as multi-modal stochastic RNNs networks (MS-RNN), which\nmodels the uncertainty observed in the data using latent stochastic variables.\nTherefore, MS-RNN can improve the performance of video captioning, and generate\nmultiple sentences to describe a video considering different random factors.\nSpecifically, a multi-modal LSTM (M-LSTM) is first proposed to interact with\nboth visual and textual features to capture a high-level representation. Then,\na backward stochastic LSTM (S-LSTM) is proposed to support uncertainty\npropagation by introducing latent variables. Experimental results on the\nchallenging datasets MSVD and MSR-VTT show that our proposed MS-RNN approach\noutperforms the state-of-the-art video captioning benchmarks. \n\n"}
{"id": "1708.03416", "contents": "Title: Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose\n  Estimation Abstract: Hand pose estimation from a single depth image is an essential topic in\ncomputer vision and human computer interaction. Despite recent advancements in\nthis area promoted by convolutional neural network, accurate hand pose\nestimation is still a challenging problem. In this paper we propose a Pose\nguided structured Region Ensemble Network (Pose-REN) to boost the performance\nof hand pose estimation. The proposed method extracts regions from the feature\nmaps of convolutional neural network under the guide of an initially estimated\npose, generating more optimal and representative features for hand pose\nestimation. The extracted feature regions are then integrated hierarchically\naccording to the topology of hand joints by employing tree-structured fully\nconnections. A refined estimation of hand pose is directly regressed by the\nproposed network and the final hand pose is obtained by utilizing an iterative\ncascaded method. Comprehensive experiments on public hand pose datasets\ndemonstrate that our proposed method outperforms state-of-the-art algorithms. \n\n"}
{"id": "1708.03788", "contents": "Title: Direct-Manipulation Visualization of Deep Networks Abstract: The recent successes of deep learning have led to a wave of interest from\nnon-experts. Gaining an understanding of this technology, however, is\ndifficult. While the theory is important, it is also helpful for novices to\ndevelop an intuitive feel for the effect of different hyperparameters and\nstructural variations. We describe TensorFlow Playground, an interactive, open\nsourced visualization that allows users to experiment via direct manipulation\nrather than coding, enabling them to quickly build an intuition about neural\nnets. \n\n"}
{"id": "1708.03805", "contents": "Title: Revisiting the Effectiveness of Off-the-shelf Temporal Modeling\n  Approaches for Large-scale Video Classification Abstract: This paper describes our solution for the video recognition task of\nActivityNet Kinetics challenge that ranked the 1st place. Most of existing\nstate-of-the-art video recognition approaches are in favor of an end-to-end\npipeline. One exception is the framework of DevNet. The merit of DevNet is that\nthey first use the video data to learn a network (i.e. fine-tuning or training\nfrom scratch). Instead of directly using the end-to-end classification scores\n(e.g. softmax scores), they extract the features from the learned network and\nthen fed them into the off-the-shelf machine learning models to conduct video\nclassification. However, the effectiveness of this line work has long-term been\nignored and underestimated. In this submission, we extensively use this\nstrategy. Particularly, we investigate four temporal modeling approaches using\nthe learned features: Multi-group Shifting Attention Network, Temporal Xception\nNetwork, Multi-stream sequence Model and Fast-Forward Sequence Model.\nExperiment results on the challenging Kinetics dataset demonstrate that our\nproposed temporal modeling approaches can significantly improve existing\napproaches in the large-scale video recognition tasks. Most remarkably, our\nbest single Multi-group Shifting Attention Network can achieve 77.7% in term of\ntop-1 accuracy and 93.2% in term of top-5 accuracy on the validation set. \n\n"}
{"id": "1708.03880", "contents": "Title: Image Quality Assessment Guided Deep Neural Networks Training Abstract: For many computer vision problems, the deep neural networks are trained and\nvalidated based on the assumption that the input images are pristine (i.e.,\nartifact-free). However, digital images are subject to a wide range of\ndistortions in real application scenarios, while the practical issues regarding\nimage quality in high level visual information understanding have been largely\nignored. In this paper, in view of the fact that most widely deployed deep\nlearning models are susceptible to various image distortions, the distorted\nimages are involved for data augmentation in the deep neural network training\nprocess to learn a reliable model for practical applications. In particular, an\nimage quality assessment based label smoothing method, which aims at\nregularizing the label distribution of training images, is further proposed to\ntune the objective functions in learning the neural network. Experimental\nresults show that the proposed method is effective in dealing with both low and\nhigh quality images in the typical image classification task. \n\n"}
{"id": "1708.03985", "contents": "Title: AffectNet: A Database for Facial Expression, Valence, and Arousal\n  Computing in the Wild Abstract: Automated affective computing in the wild setting is a challenging problem in\ncomputer vision. Existing annotated databases of facial expressions in the wild\nare small and mostly cover discrete emotions (aka the categorical model). There\nare very limited annotated facial databases for affective computing in the\ncontinuous dimensional model (e.g., valence and arousal). To meet this need, we\ncollected, annotated, and prepared for public distribution a new database of\nfacial emotions in the wild (called AffectNet). AffectNet contains more than\n1,000,000 facial images from the Internet by querying three major search\nengines using 1250 emotion related keywords in six different languages. About\nhalf of the retrieved images were manually annotated for the presence of seven\ndiscrete facial expressions and the intensity of valence and arousal. AffectNet\nis by far the largest database of facial expression, valence, and arousal in\nthe wild enabling research in automated facial expression recognition in two\ndifferent emotion models. Two baseline deep neural networks are used to\nclassify images in the categorical model and predict the intensity of valence\nand arousal. Various evaluation metrics show that our deep neural network\nbaselines can perform better than conventional machine learning methods and\noff-the-shelf facial expression recognition systems. \n\n"}
{"id": "1708.04308", "contents": "Title: MHTN: Modal-adversarial Hybrid Transfer Network for Cross-modal\n  Retrieval Abstract: Cross-modal retrieval has drawn wide interest for retrieval across different\nmodalities of data. However, existing methods based on DNN face the challenge\nof insufficient cross-modal training data, which limits the training\neffectiveness and easily leads to overfitting. Transfer learning is for\nrelieving the problem of insufficient training data, but it mainly focuses on\nknowledge transfer only from large-scale datasets as single-modal source domain\nto single-modal target domain. Such large-scale single-modal datasets also\ncontain rich modal-independent semantic knowledge that can be shared across\ndifferent modalities. Besides, large-scale cross-modal datasets are very\nlabor-consuming to collect and label, so it is significant to fully exploit the\nknowledge in single-modal datasets for boosting cross-modal retrieval. This\npaper proposes modal-adversarial hybrid transfer network (MHTN), which to the\nbest of our knowledge is the first work to realize knowledge transfer from\nsingle-modal source domain to cross-modal target domain, and learn cross-modal\ncommon representation. It is an end-to-end architecture with two subnetworks:\n(1) Modal-sharing knowledge transfer subnetwork is proposed to jointly transfer\nknowledge from a large-scale single-modal dataset in source domain to all\nmodalities in target domain with a star network structure, which distills\nmodal-independent supplementary knowledge for promoting cross-modal common\nrepresentation learning. (2) Modal-adversarial semantic learning subnetwork is\nproposed to construct an adversarial training mechanism between common\nrepresentation generator and modality discriminator, making the common\nrepresentation discriminative for semantics but indiscriminative for modalities\nto enhance cross-modal semantic consistency during transfer process.\nComprehensive experiments on 4 widely-used datasets show its effectiveness and\ngenerality. \n\n"}
{"id": "1708.04370", "contents": "Title: Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a\n  Docker Container Abstract: Face detection is a very important task and a necessary pre-processing step\nfor many applications such as facial landmark detection, pose estimation,\nsentiment analysis and face recognition. Not only is face detection an\nimportant pre-processing step in computer vision applications but also in\ncomputational psychology, behavioral imaging and other fields where researchers\nmight not be initiated in computer vision frameworks and state-of-the-art\ndetection applications. A large part of existing research that includes face\ndetection as a pre-processing step uses existing out-of-the-box detectors such\nas the HoG-based dlib and the OpenCV Haar face detector which are no longer\nstate-of-the-art - they are primarily used because of their ease of use and\naccessibility. We introduce Dockerface, a very accurate Faster R-CNN face\ndetector in a Docker container which requires no training and is easy to\ninstall and use. \n\n"}
{"id": "1708.04680", "contents": "Title: Augmentor: An Image Augmentation Library for Machine Learning Abstract: The generation of artificial data based on existing observations, known as\ndata augmentation, is a technique used in machine learning to improve model\naccuracy, generalisation, and to control overfitting. Augmentor is a software\npackage, available in both Python and Julia versions, that provides a high\nlevel API for the expansion of image data using a stochastic, pipeline-based\napproach which effectively allows for images to be sampled from a distribution\nof augmented images at runtime. Augmentor provides methods for most standard\naugmentation practices as well as several advanced features such as\nlabel-preserving, randomised elastic distortions, and provides many helper\nfunctions for typical augmentation tasks used in machine learning. \n\n"}
{"id": "1708.05038", "contents": "Title: ConvNet Architecture Search for Spatiotemporal Feature Learning Abstract: Learning image representations with ConvNets by pre-training on ImageNet has\nproven useful across many visual understanding tasks including object\ndetection, semantic segmentation, and image captioning. Although any image\nrepresentation can be applied to video frames, a dedicated spatiotemporal\nrepresentation is still vital in order to incorporate motion patterns that\ncannot be captured by appearance based models alone. This paper presents an\nempirical ConvNet architecture search for spatiotemporal feature learning,\nculminating in a deep 3-dimensional (3D) Residual ConvNet. Our proposed\narchitecture outperforms C3D by a good margin on Sports-1M, UCF101, HMDB51,\nTHUMOS14, and ASLAN while being 2 times faster at inference time, 2 times\nsmaller in model size, and having a more compact representation. \n\n"}
{"id": "1708.05349", "contents": "Title: PixelNN: Example-based Image Synthesis Abstract: We present a simple nearest-neighbor (NN) approach that synthesizes\nhigh-frequency photorealistic images from an \"incomplete\" signal such as a\nlow-resolution image, a surface normal map, or edges. Current state-of-the-art\ndeep generative models designed for such conditional image synthesis lack two\nimportant things: (1) they are unable to generate a large set of diverse\noutputs, due to the mode collapse problem. (2) they are not interpretable,\nmaking it difficult to control the synthesized output. We demonstrate that NN\napproaches potentially address such limitations, but suffer in accuracy on\nsmall datasets. We design a simple pipeline that combines the best of both\nworlds: the first stage uses a convolutional neural network (CNN) to maps the\ninput to a (overly-smoothed) image, and the second stage uses a pixel-wise\nnearest neighbor method to map the smoothed output to multiple high-quality,\nhigh-frequency outputs in a controllable manner. We demonstrate our approach\nfor various input modalities, and for various domains ranging from human faces\nto cats-and-dogs to shoes and handbags. \n\n"}
{"id": "1708.05355", "contents": "Title: MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion\n  Estimation Abstract: Optical flow estimation is one of the most studied problems in computer\nvision, yet recent benchmark datasets continue to reveal problem areas of\ntoday's approaches. Occlusions have remained one of the key challenges. In this\npaper, we propose a symmetric optical flow method to address the well-known\nchicken-and-egg relation between optical flow and occlusions. In contrast to\nmany state-of-the-art methods that consider occlusions as outliers, possibly\nfiltered out during post-processing, we highlight the importance of joint\nocclusion reasoning in the optimization and show how to utilize occlusion as an\nimportant cue for estimating optical flow. The key feature of our model is to\nfully exploit the symmetry properties that characterize optical flow and\nocclusions in the two consecutive images. Specifically through utilizing\nforward-backward consistency and occlusion-disocclusion symmetry in the energy,\nour model jointly estimates optical flow in both forward and backward\ndirection, as well as consistent occlusion maps in both views. We demonstrate\nsignificant performance benefits on standard benchmarks, especially from the\nocclusion-disocclusion symmetry. On the challenging KITTI dataset we report the\nmost accurate two-frame results to date. \n\n"}
{"id": "1708.05509", "contents": "Title: Towards the Automatic Anime Characters Creation with Generative\n  Adversarial Networks Abstract: Automatic generation of facial images has been well studied after the\nGenerative Adversarial Network (GAN) came out. There exists some attempts\napplying the GAN model to the problem of generating facial images of anime\ncharacters, but none of the existing work gives a promising result. In this\nwork, we explore the training of GAN models specialized on an anime facial\nimage dataset. We address the issue from both the data and the model aspect, by\ncollecting a more clean, well-suited dataset and leverage proper, empirical\napplication of DRAGAN. With quantitative analysis and case studies we\ndemonstrate that our efforts lead to a stable and high-quality model. Moreover,\nto assist people with anime character design, we build a website\n(http://make.girls.moe) with our pre-trained model available online, which\nmakes the model easily accessible to general public. \n\n"}
{"id": "1708.05595", "contents": "Title: Self-explanatory Deep Salient Object Detection Abstract: Salient object detection has seen remarkable progress driven by deep learning\ntechniques. However, most of deep learning based salient object detection\nmethods are black-box in nature and lacking in interpretability. This paper\nproposes the first self-explanatory saliency detection network that explicitly\nexploits low- and high-level features for salient object detection. We\ndemonstrate that such supportive clues not only significantly enhances\nperformance of salient object detection but also gives better justified\ndetection results. More specifically, we develop a multi-stage saliency encoder\nto extract multi-scale features which contain both low- and high-level saliency\ncontext. Dense short- and long-range connections are introduced to reuse these\nfeatures iteratively. Benefiting from the direct access to low- and high-level\nfeatures, the proposed saliency encoder can not only model the object context\nbut also preserve the boundary. Furthermore, a self-explanatory generator is\nproposed to interpret how the proposed saliency encoder or other deep saliency\nmodels making decisions. The generator simulates the absence of interesting\nfeatures by preventing these features from contributing to the saliency\nclassifier and estimates the corresponding saliency prediction without these\nfeatures. A comparison function, saliency explanation, is defined to measure\nthe prediction changes between deep saliency models and corresponding\ngenerator. Through visualizing the differences, we can interpret the capability\nof different deep neural networks based saliency detection models and\ndemonstrate that our proposed model indeed uses more reasonable structure for\nsalient object detection. Extensive experiments on five popular benchmark\ndatasets and the visualized saliency explanation demonstrate that the proposed\nmethod provides new state-of-the-art. \n\n"}
{"id": "1708.05827", "contents": "Title: Visual Forecasting by Imitating Dynamics in Natural Sequences Abstract: We introduce a general framework for visual forecasting, which directly\nimitates visual sequences without additional supervision. As a result, our\nmodel can be applied at several semantic levels and does not require any domain\nknowledge or handcrafted features. We achieve this by formulating visual\nforecasting as an inverse reinforcement learning (IRL) problem, and directly\nimitate the dynamics in natural sequences from their raw pixel values. The key\nchallenge is the high-dimensional and continuous state-action space that\nprohibits the application of previous IRL algorithms. We address this\ncomputational bottleneck by extending recent progress in model-free imitation\nwith trainable deep feature representations, which (1) bypasses the exhaustive\nstate-action pair visits in dynamic programming by using a dual formulation and\n(2) avoids explicit state sampling at gradient computation using a deep feature\nreparametrization. This allows us to apply IRL at scale and directly imitate\nthe dynamics in high-dimensional continuous visual sequences from the raw pixel\nvalues. We evaluate our approach at three different level-of-abstraction, from\nlow level pixels to higher level semantics: future frame generation, action\nanticipation, visual story forecasting. At all levels, our approach outperforms\nexisting methods. \n\n"}
{"id": "1708.06767", "contents": "Title: Seeing Through Noise: Visually Driven Speaker Separation and Enhancement Abstract: Isolating the voice of a specific person while filtering out other voices or\nbackground noises is challenging when video is shot in noisy environments. We\npropose audio-visual methods to isolate the voice of a single speaker and\neliminate unrelated sounds. First, face motions captured in the video are used\nto estimate the speaker's voice, by passing the silent video frames through a\nvideo-to-speech neural network-based model. Then the speech predictions are\napplied as a filter on the noisy input audio. This approach avoids using\nmixtures of sounds in the learning process, as the number of such possible\nmixtures is huge, and would inevitably bias the trained model. We evaluate our\nmethod on two audio-visual datasets, GRID and TCD-TIMIT, and show that our\nmethod attains significant SDR and PESQ improvements over the raw\nvideo-to-speech predictions, and a well-known audio-only method. \n\n"}
{"id": "1708.06834", "contents": "Title: Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks Abstract: Recurrent Neural Networks (RNNs) continue to show outstanding performance in\nsequence modeling tasks. However, training RNNs on long sequences often face\nchallenges like slow inference, vanishing gradients and difficulty in capturing\nlong term dependencies. In backpropagation through time settings, these issues\nare tightly coupled with the large, sequential computational graph resulting\nfrom unfolding the RNN in time. We introduce the Skip RNN model which extends\nexisting RNN models by learning to skip state updates and shortens the\neffective size of the computational graph. This model can also be encouraged to\nperform fewer state updates through a budget constraint. We evaluate the\nproposed model on various tasks and show how it can reduce the number of\nrequired RNN updates while preserving, and sometimes even improving, the\nperformance of the baseline RNN models. Source code is publicly available at\nhttps://imatge-upc.github.io/skiprnn-2017-telecombcn/ . \n\n"}
{"id": "1708.07860", "contents": "Title: Multi-task Self-Supervised Visual Learning Abstract: We investigate methods for combining multiple self-supervised tasks--i.e.,\nsupervised tasks where data can be collected without manual labeling--in order\nto train a single visual representation. First, we provide an apples-to-apples\ncomparison of four different self-supervised tasks using the very deep\nResNet-101 architecture. We then combine tasks to jointly train a network. We\nalso explore lasso regularization to encourage the network to factorize the\ninformation in its representation, and methods for \"harmonizing\" network inputs\nin order to learn a more unified representation. We evaluate all methods on\nImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our\nresults show that deeper networks work better, and that combining tasks--even\nvia a naive multi-head architecture--always improves performance. Our best\njoint network nearly matches the PASCAL performance of a model pre-trained on\nImageNet classification, and matches the ImageNet network on NYU depth\nprediction. \n\n"}
{"id": "1708.07920", "contents": "Title: Deep Learning for Target Classification from SAR Imagery: Data\n  Augmentation and Translation Invariance Abstract: This report deals with translation invariance of convolutional neural\nnetworks (CNNs) for automatic target recognition (ATR) from synthetic aperture\nradar (SAR) imagery. In particular, the translation invariance of CNNs for SAR\nATR represents the robustness against misalignment of target chips extracted\nfrom SAR images. To understand the translation invariance of the CNNs, we\ntrained CNNs which classify the target chips from the MSTAR into the ten\nclasses under the condition of with and without data augmentation, and then\nvisualized the translation invariance of the CNNs. According to our results,\neven if we use a deep residual network, the translation invariance of the CNN\nwithout data augmentation using the aligned images such as the MSTAR target\nchips is not so large. A more important factor of translation invariance is the\nuse of augmented training data. Furthermore, our CNN using augmented training\ndata achieved a state-of-the-art classification accuracy of 99.6%. These\nresults show an importance of domain-specific data augmentation. \n\n"}
{"id": "1708.08559", "contents": "Title: DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous\n  Cars Abstract: Recent advances in Deep Neural Networks (DNNs) have led to the development of\nDNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can\ndrive without any human intervention. Most major manufacturers including Tesla,\nGM, Ford, BMW, and Waymo/Google are working on building and testing different\ntypes of autonomous vehicles. The lawmakers of several US states including\nCalifornia, Texas, and New York have passed new legislation to fast-track the\nprocess of testing and deployment of autonomous vehicles on their roads.\n  However, despite their spectacular progress, DNNs, just like traditional\nsoftware, often demonstrate incorrect or unexpected corner case behaviors that\ncan lead to potentially fatal collisions. Several such real-world accidents\ninvolving autonomous cars have already happened including one which resulted in\na fatality. Most existing testing techniques for DNN-driven vehicles are\nheavily dependent on the manual collection of test data under different driving\nconditions which become prohibitively expensive as the number of test\nconditions increases.\n  In this paper, we design, implement and evaluate DeepTest, a systematic\ntesting tool for automatically detecting erroneous behaviors of DNN-driven\nvehicles that can potentially lead to fatal crashes. First, our tool is\ndesigned to automatically generated test cases leveraging real-world changes in\ndriving conditions like rain, fog, lighting conditions, etc. DeepTest\nsystematically explores different parts of the DNN logic by generating test\ninputs that maximize the numbers of activated neurons. DeepTest found thousands\nof erroneous behaviors under different realistic driving conditions (e.g.,\nblurring, rain, fog, etc.) many of which lead to potentially fatal crashes in\nthree top performing DNNs in the Udacity self-driving car challenge. \n\n"}
{"id": "1708.08985", "contents": "Title: Limiting the Reconstruction Capability of Generative Neural Network\n  using Negative Learning Abstract: Generative models are widely used for unsupervised learning with various\napplications, including data compression and signal restoration. Training\nmethods for such systems focus on the generality of the network given limited\namount of training data. A less researched type of techniques concerns\ngeneration of only a single type of input. This is useful for applications such\nas constraint handling, noise reduction and anomaly detection. In this paper we\npresent a technique to limit the generative capability of the network using\nnegative learning. The proposed method searches the solution in the gradient\ndirection for the desired input and in the opposite direction for the undesired\ninput. One of the application can be anomaly detection where the undesired\ninputs are the anomalous data. In the results section we demonstrate the\nfeatures of the algorithm using MNIST handwritten digit dataset and latter\napply the technique to a real-world obstacle detection problem. The results\nclearly show that the proposed learning technique can significantly improve the\nperformance for anomaly detection. \n\n"}
{"id": "1708.09254", "contents": "Title: Interpretation of Mammogram and Chest X-Ray Reports Using Deep Neural\n  Networks - Preliminary Results Abstract: Radiology reports are an important means of communication between\nradiologists and other physicians. These reports express a radiologist's\ninterpretation of a medical imaging examination and are critical in\nestablishing a diagnosis and formulating a treatment plan. In this paper, we\npropose a Bi-directional convolutional neural network (Bi-CNN) model for the\ninterpretation and classification of mammograms based on breast density and\nchest radiographic radiology reports based on the basis of chest pathology. The\nproposed approach helps to organize databases of radiology reports, retrieve\nthem expeditiously, and evaluate the radiology report that could be used in an\nauditing system to decrease incorrect diagnoses. Our study revealed that the\nproposed Bi-CNN outperforms the random forest and the support vector machine\nmethods. \n\n"}
{"id": "1709.00572", "contents": "Title: XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification Abstract: In recent years, there have been numerous developments towards solving\nmultimodal tasks, aiming to learn a stronger representation than through a\nsingle modality. Certain aspects of the data can be particularly useful in this\ncase - for example, correlations in the space or time domain across modalities\n- but should be wisely exploited in order to benefit from their full predictive\npotential. We propose two deep learning architectures with multimodal\ncross-connections that allow for dataflow between several feature extractors\n(XFlow). Our models derive more interpretable features and achieve better\nperformances than models which do not exchange representations, usefully\nexploiting correlations between audio and visual data, which have a different\ndimensionality and are nontrivially exchangeable. Our work improves on existing\nmultimodal deep learning algorithms in two essential ways: (1) it presents a\nnovel method for performing cross-modality (before features are learned from\nindividual modalities) and (2) extends the previously proposed\ncross-connections which only transfer information between streams that process\ncompatible data. Illustrating some of the representations learned by the\nconnections, we analyse their contribution to the increase in discrimination\nability and reveal their compatibility with a lip-reading network intermediate\nrepresentation. We provide the research community with Digits, a new dataset\nconsisting of three data types extracted from videos of people saying the\ndigits 0-9. Results show that both cross-modal architectures outperform their\nbaselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits\ndatasets, achieving state-of-the-art results. \n\n"}
{"id": "1709.01118", "contents": "Title: WESPE: Weakly Supervised Photo Enhancer for Digital Cameras Abstract: Low-end and compact mobile cameras demonstrate limited photo quality mainly\ndue to space, hardware and budget constraints. In this work, we propose a deep\nlearning solution that translates photos taken by cameras with limited\ncapabilities into DSLR-quality photos automatically. We tackle this problem by\nintroducing a weakly supervised photo enhancer (WESPE) - a novel image-to-image\nGenerative Adversarial Network-based architecture. The proposed model is\ntrained by under weak supervision: unlike previous works, there is no need for\nstrong supervision in the form of a large annotated dataset of aligned\noriginal/enhanced photo pairs. The sole requirement is two distinct datasets:\none from the source camera, and one composed of arbitrary high-quality images\nthat can be generally crawled from the Internet - the visual content they\nexhibit may be unrelated. Hence, our solution is repeatable for any camera:\ncollecting the data and training can be achieved in a couple of hours. In this\nwork, we emphasize on extensive evaluation of obtained results. Besides\nstandard objective metrics and subjective user study, we train a virtual rater\nin the form of a separate CNN that mimics human raters on Flickr data and use\nthis network to get reference scores for both original and enhanced photos. Our\nexperiments on the DPED, KITTI and Cityscapes datasets as well as pictures from\nseveral generations of smartphones demonstrate that WESPE produces comparable\nor improved qualitative results with state-of-the-art strongly supervised\nmethods. \n\n"}
{"id": "1709.01421", "contents": "Title: Multi-label Class-imbalanced Action Recognition in Hockey Videos via 3D\n  Convolutional Neural Networks Abstract: Automatic analysis of the video is one of most complex problems in the fields\nof computer vision and machine learning. A significant part of this research\ndeals with (human) activity recognition (HAR) since humans, and the activities\nthat they perform, generate most of the video semantics. Video-based HAR has\napplications in various domains, but one of the most important and challenging\nis HAR in sports videos. Some of the major issues include high inter- and\nintra-class variations, large class imbalance, the presence of both group\nactions and single player actions, and recognizing simultaneous actions, i.e.,\nthe multi-label learning problem. Keeping in mind these challenges and the\nrecent success of CNNs in solving various computer vision problems, in this\nwork, we implement a 3D CNN based multi-label deep HAR system for multi-label\nclass-imbalanced action recognition in hockey videos. We test our system for\ntwo different scenarios: an ensemble of $k$ binary networks vs. a single\n$k$-output network, on a publicly available dataset. We also compare our\nresults with the system that was originally designed for the chosen dataset.\nExperimental results show that the proposed approach performs better than the\nexisting solution. \n\n"}
{"id": "1709.01993", "contents": "Title: Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face\n  Images Abstract: Lighting estimation from face images is an important task and has\napplications in many areas such as image editing, intrinsic image\ndecomposition, and image forgery detection. We propose to train a deep\nConvolutional Neural Network (CNN) to regress lighting parameters from a single\nface image. Lacking massive ground truth lighting labels for face images in the\nwild, we use an existing method to estimate lighting parameters, which are\ntreated as ground truth with unknown noises. To alleviate the effect of such\nnoises, we utilize the idea of Generative Adversarial Networks (GAN) and\npropose a Label Denoising Adversarial Network (LDAN) to make use of synthetic\ndata with accurate ground truth to help train a deep CNN for lighting\nregression on real face images. Experiments show that our network outperforms\nexisting methods in producing consistent lighting parameters of different faces\nunder similar lighting conditions. Moreover, our method is 100,000 times faster\nin execution time than prior optimization-based lighting estimation approaches. \n\n"}
{"id": "1709.02251", "contents": "Title: Multi-modal Conditional Attention Fusion for Dimensional Emotion\n  Prediction Abstract: Continuous dimensional emotion prediction is a challenging task where the\nfusion of various modalities usually achieves state-of-the-art performance such\nas early fusion or late fusion. In this paper, we propose a novel multi-modal\nfusion strategy named conditional attention fusion, which can dynamically pay\nattention to different modalities at each time step. Long-short term memory\nrecurrent neural networks (LSTM-RNN) is applied as the basic uni-modality model\nto capture long time dependencies. The weights assigned to different modalities\nare automatically decided by the current input features and recent history\ninformation rather than being fixed at any kinds of situation. Our experimental\nresults on a benchmark dataset AVEC2015 show the effectiveness of our method\nwhich outperforms several common fusion strategies for valence prediction. \n\n"}
{"id": "1709.02779", "contents": "Title: Machine learning \\& artificial intelligence in the quantum domain Abstract: Quantum information technologies, and intelligent learning systems, are both\nemergent technologies that will likely have a transforming impact on our\nsociety. The respective underlying fields of research -- quantum information\n(QI) versus machine learning (ML) and artificial intelligence (AI) -- have\ntheir own specific challenges, which have hitherto been investigated largely\nindependently. However, in a growing body of recent work, researchers have been\nprobing the question to what extent these fields can learn and benefit from\neach other. QML explores the interaction between quantum computing and ML,\ninvestigating how results and techniques from one field can be used to solve\nthe problems of the other. Recently, we have witnessed breakthroughs in both\ndirections of influence. For instance, quantum computing is finding a vital\napplication in providing speed-ups in ML, critical in our \"big data\" world.\nConversely, ML already permeates cutting-edge technologies, and may become\ninstrumental in advanced quantum technologies. Aside from quantum speed-up in\ndata analysis, or classical ML optimization used in quantum experiments,\nquantum enhancements have also been demonstrated for interactive learning,\nhighlighting the potential of quantum-enhanced learning agents. Finally, works\nexploring the use of AI for the very design of quantum experiments, and for\nperforming parts of genuine research autonomously, have reported their first\nsuccesses. Beyond the topics of mutual enhancement, researchers have also\nbroached the fundamental issue of quantum generalizations of ML/AI concepts.\nThis deals with questions of the very meaning of learning and intelligence in a\nworld that is described by quantum mechanics. In this review, we describe the\nmain ideas, recent developments, and progress in a broad spectrum of research\ninvestigating machine learning and artificial intelligence in the quantum\ndomain. \n\n"}
{"id": "1709.03196", "contents": "Title: Deep multi-frame face super-resolution Abstract: Face verification and recognition problems have seen rapid progress in recent\nyears, however recognition from small size images remains a challenging task\nthat is inherently intertwined with the task of face super-resolution. Tackling\nthis problem using multiple frames is an attractive idea, yet requires solving\nthe alignment problem that is also challenging for low-resolution faces. Here\nwe present a holistic system for multi-frame recognition, alignment, and\nsuperresolution of faces. Our neural network architecture restores the central\nframe of each input sequence additionally taking into account a number of\nadjacent frames and making use of sub-pixel movements. We present our results\nusing the popular dataset for video face recognition (YouTube Faces). We show a\nnotable improvement of identification score compared to several baselines\nincluding the one based on single-image super-resolution. \n\n"}
{"id": "1709.03655", "contents": "Title: Learning Gating ConvNet for Two-Stream based Methods in Action\n  Recognition Abstract: For the two-stream style methods in action recognition, fusing the two\nstreams' predictions is always by the weighted averaging scheme. This fusion\nmethod with fixed weights lacks of pertinence to different action videos and\nalways needs trial and error on the validation set. In order to enhance the\nadaptability of two-stream ConvNets and improve its performance, an end-to-end\ntrainable gated fusion method, namely gating ConvNet, for the two-stream\nConvNets is proposed in this paper based on the MoE (Mixture of Experts)\ntheory. The gating ConvNet takes the combination of feature maps from the same\nlayer of the spatial and the temporal nets as input and adopts ReLU (Rectified\nLinear Unit) as the gating output activation function. To reduce the\nover-fitting of gating ConvNet caused by the redundancy of parameters, a new\nmulti-task learning method is designed, which jointly learns the gating fusion\nweights for the two streams and learns the gating ConvNet for action\nclassification. With our gated fusion method and multi-task learning approach,\na high accuracy of 94.5% is achieved on the dataset UCF101. \n\n"}
{"id": "1709.03675", "contents": "Title: Adversarial Discriminative Heterogeneous Face Recognition Abstract: The gap between sensing patterns of different face modalities remains a\nchallenging problem in heterogeneous face recognition (HFR). This paper\nproposes an adversarial discriminative feature learning framework to close the\nsensing gap via adversarial learning on both raw-pixel space and compact\nfeature space. This framework integrates cross-spectral face hallucination and\ndiscriminative feature learning into an end-to-end adversarial network. In the\npixel space, we make use of generative adversarial networks to perform\ncross-spectral face hallucination. An elaborate two-path model is introduced to\nalleviate the lack of paired images, which gives consideration to both global\nstructures and local textures. In the feature space, an adversarial loss and a\nhigh-order variance discrepancy loss are employed to measure the global and\nlocal discrepancy between two heterogeneous distributions respectively. These\ntwo losses enhance domain-invariant feature learning and modality independent\nnoise removing. Experimental results on three NIR-VIS databases show that our\nproposed approach outperforms state-of-the-art HFR methods, without requiring\nof complex network or large-scale training dataset. \n\n"}
{"id": "1709.04093", "contents": "Title: Joint Learning of Set Cardinality and State Distribution Abstract: We present a novel approach for learning to predict sets using deep learning.\nIn recent years, deep neural networks have shown remarkable results in computer\nvision, natural language processing and other related problems. Despite their\nsuccess, traditional architectures suffer from a serious limitation in that\nthey are built to deal with structured input and output data, i.e. vectors or\nmatrices. Many real-world problems, however, are naturally described as sets,\nrather than vectors. Existing techniques that allow for sequential data, such\nas recurrent neural networks, typically heavily depend on the input and output\norder and do not guarantee a valid solution. Here, we derive in a principled\nway, a mathematical formulation for set prediction where the output is\npermutation invariant. In particular, our approach jointly learns both the\ncardinality and the state distribution of the target set. We demonstrate the\nvalidity of our method on the task of multi-label image classification and\nachieve a new state of the art on the PASCAL VOC and MS COCO datasets. \n\n"}
{"id": "1709.04108", "contents": "Title: Co-training for Demographic Classification Using Deep Learning from\n  Label Proportions Abstract: Deep learning algorithms have recently produced state-of-the-art accuracy in\nmany classification tasks, but this success is typically dependent on access to\nmany annotated training examples. For domains without such data, an attractive\nalternative is to train models with light, or distant supervision. In this\npaper, we introduce a deep neural network for the Learning from Label\nProportion (LLP) setting, in which the training data consist of bags of\nunlabeled instances with associated label distributions for each bag. We\nintroduce a new regularization layer, Batch Averager, that can be appended to\nthe last layer of any deep neural network to convert it from supervised\nlearning to LLP. This layer can be implemented readily with existing deep\nlearning packages. To further support domains in which the data consist of two\nconditionally independent feature views (e.g. image and text), we propose a\nco-training algorithm that iteratively generates pseudo bags and refits the\ndeep LLP model to improve classification accuracy. We demonstrate our models on\ndemographic attribute classification (gender and race/ethnicity), which has\nmany applications in social media analysis, public health, and marketing. We\nconduct experiments to predict demographics of Twitter users based on their\ntweets and profile image, without requiring any user-level annotations for\ntraining. We find that the deep LLP approach outperforms baselines for both\ntext and image features separately. Additionally, we find that co-training\nalgorithm improves image and text classification by 4% and 8% absolute F1,\nrespectively. Finally, an ensemble of text and image classifiers further\nimproves the absolute F1 measure by 4% on average. \n\n"}
{"id": "1709.04647", "contents": "Title: Detection of Unauthorized IoT Devices Using Machine Learning Techniques Abstract: Security experts have demonstrated numerous risks imposed by Internet of\nThings (IoT) devices on organizations. Due to the widespread adoption of such\ndevices, their diversity, standardization obstacles, and inherent mobility,\norganizations require an intelligent mechanism capable of automatically\ndetecting suspicious IoT devices connected to their networks. In particular,\ndevices not included in a white list of trustworthy IoT device types (allowed\nto be used within the organizational premises) should be detected. In this\nresearch, Random Forest, a supervised machine learning algorithm, was applied\nto features extracted from network traffic data with the aim of accurately\nidentifying IoT device types from the white list. To train and evaluate\nmulti-class classifiers, we collected and manually labeled network traffic data\nfrom 17 distinct IoT devices, representing nine types of IoT devices. Based on\nthe classification of 20 consecutive sessions and the use of majority rule, IoT\ndevice types that are not on the white list were correctly detected as unknown\nin 96% of test cases (on average), and white listed device types were correctly\nclassified by their actual types in 99% of cases. Some IoT device types were\nidentified quicker than others (e.g., sockets and thermostats were successfully\ndetected within five TCP sessions of connecting to the network). Perfect\ndetection of unauthorized IoT device types was achieved upon analyzing 110\nconsecutive sessions; perfect classification of white listed types required 346\nconsecutive sessions, 110 of which resulted in 99.49% accuracy. Further\nexperiments demonstrated the successful applicability of classifiers trained in\none location and tested on another. In addition, a discussion is provided\nregarding the resilience of our machine learning-based IoT white listing method\nto adversarial attacks. \n\n"}
{"id": "1709.04744", "contents": "Title: Subspace Clustering using Ensembles of $K$-Subspaces Abstract: Subspace clustering is the unsupervised grouping of points lying near a union\nof low-dimensional linear subspaces. Algorithms based directly on geometric\nproperties of such data tend to either provide poor empirical performance, lack\ntheoretical guarantees, or depend heavily on their initialization. We present a\nnovel geometric approach to the subspace clustering problem that leverages\nensembles of the K-subspaces (KSS) algorithm via the evidence accumulation\nclustering framework. Our algorithm, referred to as ensemble K-subspaces\n(EKSS), forms a co-association matrix whose (i,j)th entry is the number of\ntimes points i and j are clustered together by several runs of KSS with random\ninitializations. We prove general recovery guarantees for any algorithm that\nforms an affinity matrix with entries close to a monotonic transformation of\npairwise absolute inner products. We then show that a specific instance of EKSS\nresults in an affinity matrix with entries of this form, and hence our proposed\nalgorithm can provably recover subspaces under similar conditions to\nstate-of-the-art algorithms. The finding is, to the best of our knowledge, the\nfirst recovery guarantee for evidence accumulation clustering and for KSS\nvariants. We show on synthetic data that our method performs well in the\ntraditionally challenging settings of subspaces with large intersection,\nsubspaces with small principal angles, and noisy data. Finally, we evaluate our\nalgorithm on six common benchmark datasets and show that unlike existing\nmethods, EKSS achieves excellent empirical performance when there are both a\nsmall and large number of points per subspace. \n\n"}
{"id": "1709.04881", "contents": "Title: Benchmarking Super-Resolution Algorithms on Real Data Abstract: Over the past decades, various super-resolution (SR) techniques have been\ndeveloped to enhance the spatial resolution of digital images. Despite the\ngreat number of methodical contributions, there is still a lack of comparative\nvalidations of SR under practical conditions, as capturing real ground truth\ndata is a challenging task. Therefore, current studies are either evaluated 1)\non simulated data or 2) on real data without a pixel-wise ground truth.\n  To facilitate comprehensive studies, this paper introduces the publicly\navailable Super-Resolution Erlangen (SupER) database that includes real\nlow-resolution images along with high-resolution ground truth data. Our\ndatabase comprises image sequences with more than 20k images captured from 14\nscenes under various types of motions and photometric conditions. The datasets\ncover four spatial resolution levels using camera hardware binning. With this\ndatabase, we benchmark 15 single-image and multi-frame SR algorithms. Our\nexperiments quantitatively analyze SR accuracy and robustness under realistic\nconditions including independent object and camera motion or photometric\nvariations. \n\n"}
{"id": "1709.05675", "contents": "Title: Organizing Multimedia Data in Video Surveillance Systems Based on Face\n  Verification with Convolutional Neural Networks Abstract: In this paper we propose the two-stage approach of organizing information in\nvideo surveillance systems. At first, the faces are detected in each frame and\na video stream is split into sequences of frames with face region of one\nperson. Secondly, these sequences (tracks) that contain identical faces are\ngrouped using face verification algorithms and hierarchical agglomerative\nclustering. Gender and age are estimated for each cluster (person) in order to\nfacilitate the usage of the organized video collection. The particular\nattention is focused on the aggregation of features extracted from each frame\nwith the deep convolutional neural networks. The experimental results of the\nproposed approach using YTF and IJB-A datasets demonstrated that the most\naccurate and fast solution is achieved for matching of normalized average of\nfeature vectors of all frames in a track. \n\n"}
{"id": "1709.05746", "contents": "Title: Adversarial Discriminative Sim-to-real Transfer of Visuo-motor Policies Abstract: Various approaches have been proposed to learn visuo-motor policies for\nreal-world robotic applications. One solution is first learning in simulation\nthen transferring to the real world. In the transfer, most existing approaches\nneed real-world images with labels. However, the labelling process is often\nexpensive or even impractical in many robotic applications. In this paper, we\npropose an adversarial discriminative sim-to-real transfer approach to reduce\nthe cost of labelling real data. The effectiveness of the approach is\ndemonstrated with modular networks in a table-top object reaching task where a\n7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter\nthrough visual observations. The adversarial transfer approach reduced the\nlabelled real data requirement by 50%. Policies can be transferred to real\nenvironments with only 93 labelled and 186 unlabelled real images. The\ntransferred visuo-motor policies are robust to novel (not seen in training)\nobjects in clutter and even a moving target, achieving a 97.8% success rate and\n1.8 cm control accuracy. \n\n"}
{"id": "1709.05861", "contents": "Title: Continuous Multimodal Emotion Recognition Approach for AVEC 2017 Abstract: This paper reports the analysis of audio and visual features in predicting\nthe continuous emotion dimensions under the seventh Audio/Visual Emotion\nChallenge (AVEC 2017), which was done as part of a B.Tech. 2nd year internship\nproject. For visual features we used the HOG (Histogram of Gradients) features,\nFisher encodings of SIFT (Scale-Invariant Feature Transform) features based on\nGaussian mixture model (GMM) and some pretrained Convolutional Neural Network\nlayers as features; all these extracted for each video clip. For audio features\nwe used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level\ndescriptors) generated by openXBOW provided by the organisers of the event.\nThen we trained fully connected neural network regression model on the dataset\nfor all these different modalities. We applied multimodal fusion on the output\nmodels to get the Concordance correlation coefficient on Development set as\nwell as Test set. \n\n"}
{"id": "1709.05865", "contents": "Title: Depression Scale Recognition from Audio, Visual and Text Analysis Abstract: Depression is a major mental health disorder that is rapidly affecting lives\nworldwide. Depression not only impacts emotional but also physical and\npsychological state of the person. Its symptoms include lack of interest in\ndaily activities, feeling low, anxiety, frustration, loss of weight and even\nfeeling of self-hatred. This report describes work done by us for Audio Visual\nEmotion Challenge (AVEC) 2017 during our second year BTech summer internship.\nWith the increase in demand to detect depression automatically with the help of\nmachine learning algorithms, we present our multimodal feature extraction and\ndecision level fusion approach for the same. Features are extracted by\nprocessing on the provided Distress Analysis Interview Corpus-Wizard of Oz\n(DAIC-WOZ) database. Gaussian Mixture Model (GMM) clustering and Fisher vector\napproach were applied on the visual data; statistical descriptors on gaze,\npose; low level audio features and head pose and text features were also\nextracted. Classification is done on fused as well as independent features\nusing Support Vector Machine (SVM) and neural networks. The results obtained\nwere able to cross the provided baseline on validation data set by 17% on audio\nfeatures and 24.5% on video features. \n\n"}
{"id": "1709.07200", "contents": "Title: Temporal Multimodal Fusion for Video Emotion Classification in the Wild Abstract: This paper addresses the question of emotion classification. The task\nconsists in predicting emotion labels (taken among a set of possible labels)\nbest describing the emotions contained in short video clips. Building on a\nstandard framework -- lying in describing videos by audio and visual features\nused by a supervised classifier to infer the labels -- this paper investigates\nseveral novel directions. First of all, improved face descriptors based on 2D\nand 3D Convo-lutional Neural Networks are proposed. Second, the paper explores\nseveral fusion methods, temporal and multimodal, including a novel hierarchical\nmethod combining features and scores. In addition, we carefully reviewed the\ndifferent stages of the pipeline and designed a CNN architecture adapted to the\ntask; this is important as the size of the training set is small compared to\nthe difficulty of the problem, making generalization difficult. The so-obtained\nmodel ranked 4th at the 2017 Emotion in the Wild challenge with the accuracy of\n58.8 %. \n\n"}
{"id": "1709.07220", "contents": "Title: Human Pose Estimation using Global and Local Normalization Abstract: In this paper, we address the problem of estimating the positions of human\njoints, i.e., articulated pose estimation. Recent state-of-the-art solutions\nmodel two key issues, joint detection and spatial configuration refinement,\ntogether using convolutional neural networks. Our work mainly focuses on\nspatial configuration refinement by reducing variations of human poses\nstatistically, which is motivated by the observation that the scattered\ndistribution of the relative locations of joints e.g., the left wrist is\ndistributed nearly uniformly in a circular area around the left shoulder) makes\nthe learning of convolutional spatial models hard. We present a two-stage\nnormalization scheme, human body normalization and limb normalization, to make\nthe distribution of the relative joint locations compact, resulting in easier\nlearning of convolutional spatial models and more accurate pose estimation. In\naddition, our empirical results show that incorporating multi-scale supervision\nand multi-scale fusion into the joint detection network is beneficial.\nExperiment results demonstrate that our method consistently outperforms\nstate-of-the-art methods on the benchmarks. \n\n"}
{"id": "1709.07359", "contents": "Title: Class-Splitting Generative Adversarial Networks Abstract: Generative Adversarial Networks (GANs) produce systematically better quality\nsamples when class label information is provided., i.e. in the conditional GAN\nsetup. This is still observed for the recently proposed Wasserstein GAN\nformulation which stabilized adversarial training and allows considering high\ncapacity network architectures such as ResNet. In this work we show how to\nboost conditional GAN by augmenting available class labels. The new classes\ncome from clustering in the representation space learned by the same GAN model.\nThe proposed strategy is also feasible when no class information is available,\ni.e. in the unsupervised setup. Our generated samples reach state-of-the-art\nInception scores for CIFAR-10 and STL-10 datasets in both supervised and\nunsupervised setup. \n\n"}
{"id": "1709.07368", "contents": "Title: Multi-label Pixelwise Classification for Reconstruction of Large-scale\n  Urban Areas Abstract: Object classification is one of the many holy grails in computer vision and\nas such has resulted in a very large number of algorithms being proposed\nalready. Specifically in recent years there has been considerable progress in\nthis area primarily due to the increased efficiency and accessibility of deep\nlearning techniques. In fact, for single-label object classification [i.e. only\none object present in the image] the state-of-the-art techniques employ deep\nneural networks and are reporting very close to human-like performance. There\nare specialized applications in which single-label object-level classification\nwill not suffice; for example in cases where the image contains multiple\nintertwined objects of different labels.\n  In this paper, we address the complex problem of multi-label pixelwise\nclassification. We present our distinct solution based on a convolutional\nneural network (CNN) for performing multi-label pixelwise classification and\nits application to large-scale urban reconstruction. A supervised learning\napproach is followed for training a 13-layer CNN using both LiDAR and satellite\nimages. An empirical study has been conducted to determine the hyperparameters\nwhich result in the optimal performance of the CNN. Scale invariance is\nintroduced by training the network on five different scales of the input and\nlabeled data. This results in six pixelwise classifications for each different\nscale. An SVM is then trained to map the six pixelwise classifications into a\nsingle-label. Lastly, we refine boundary pixel labels using graph-cuts for\nmaximum a-posteriori (MAP) estimation with Markov Random Field (MRF) priors.\nThe resulting pixelwise classification is then used to accurately extract and\nreconstruct the buildings in large-scale urban areas. The proposed approach has\nbeen extensively tested and the results are reported. \n\n"}
{"id": "1709.07720", "contents": "Title: Can We Boost the Power of the Viola-Jones Face Detector Using\n  Pre-processing? An Empirical Study Abstract: The Viola-Jones face detection algorithm was (and still is) a quite popular\nface detector. In spite of the numerous face detection techniques that have\nbeen recently presented, there are many research works that are still based on\nthe Viola-Jones algorithm because of its simplicity. In this paper, we study\nthe influence of a set of blind pre-processing methods on the face detection\nrate using the Viola-Jones algorithm. We focus on two aspects of improvement,\nspecifically badly illuminated faces and blurred faces. Many methods for\nlighting invariant and deblurring are used in order to improve the detection\naccuracy. We want to avoid using blind pre-processing methods that may obstruct\nthe face detector. To that end, we perform two sets of experiments. The first\nset is performed to avoid any blind pre-processing method that may hurt the\nface detector. The second set is performed to study the effect of the selected\npre-processing methods on images that suffer from hard conditions. We present\ntwo manners of applying the pre-processing method to the image prior to being\nused by the Viola-Jones face detector. Four different datasets are used to draw\na coherent conclusion about the potential improvement caused by using prior\nenhanced images. The results demonstrate that some of the pre-processing\nmethods may hurt the accuracy of Viola-Jones face detection algorithm. However,\nother pre-processing methods have an evident positive impact on the accuracy of\nthe face detector. Overall, we recommend three simple and fast blind\nphotometric normalization methods as a pre-processing step in order to improve\nthe accuracy of the pre-trained Viola-Jones face detector. \n\n"}
{"id": "1709.08605", "contents": "Title: Muon Trigger for Mobile Phones Abstract: The CRAYFIS experiment proposes to use privately owned mobile phones as a\nground detector array for Ultra High Energy Cosmic Rays. Upon interacting with\nEarth's atmosphere, these events produce extensive particle showers which can\nbe detected by cameras on mobile phones. A typical shower contains\nminimally-ionizing particles such as muons. As these particles interact with\nCMOS image sensors, they may leave tracks of faintly-activated pixels that are\nsometimes hard to distinguish from random detector noise. Triggers that rely on\nthe presence of very bright pixels within an image frame are not efficient in\nthis case.\n  We present a trigger algorithm based on Convolutional Neural Networks which\nselects images containing such tracks and are evaluated in a lazy manner: the\nresponse of each successive layer is computed only if activation of the current\nlayer satisfies a continuation criterion. Usage of neural networks increases\nthe sensitivity considerably comparable with image thresholding, while the lazy\nevaluation allows for execution of the trigger under the limited computational\npower of mobile phones. \n\n"}
{"id": "1709.09345", "contents": "Title: A Read-Write Memory Network for Movie Story Understanding Abstract: We propose a novel memory network model named Read-Write Memory Network\n(RWMN) to perform question and answering tasks for large-scale, multimodal\nmovie story understanding. The key focus of our RWMN model is to design the\nread network and the write network that consist of multiple convolutional\nlayers, which enable memory read and write operations to have high capacity and\nflexibility. While existing memory-augmented network models treat each memory\nslot as an independent block, our use of multi-layered CNNs allows the model to\nread and write sequential memory cells as chunks, which is more reasonable to\nrepresent a sequential story because adjacent memory blocks often have strong\ncorrelations. For evaluation, we apply our model to all the six tasks of the\nMovieQA benchmark, and achieve the best accuracies on several tasks, especially\non the visual QA task. Our model shows a potential to better understand not\nonly the content in the story, but also more abstract information, such as\nrelationships between characters and the reasons for their actions. \n\n"}
{"id": "1709.09641", "contents": "Title: Neural Multi-Atlas Label Fusion: Application to Cardiac MR Images Abstract: Multi-atlas segmentation approach is one of the most widely-used image\nsegmentation techniques in biomedical applications. There are two major\nchallenges in this category of methods, i.e., atlas selection and label fusion.\nIn this paper, we propose a novel multi-atlas segmentation method that\nformulates multi-atlas segmentation in a deep learning framework for better\nsolving these challenges. The proposed method, dubbed deep fusion net (DFN), is\na deep architecture that integrates a feature extraction subnet and a non-local\npatch-based label fusion (NL-PLF) subnet in a single network. The network\nparameters are learned by end-to-end training for automatically learning deep\nfeatures that enable optimal performance in a NL-PLF framework. The learned\ndeep features are further utilized in defining a similarity measure for atlas\nselection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09\nfor left ventricle segmentation, our approach achieved 0.833 in averaged Dice\nmetric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation on\nLV-09 dataset, comparing favorably with the other automatic left ventricle\nsegmentation methods. We also tested our approach on Cardiac Atlas Project\n(CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method\nachieved 0.815 in ADM, ranking highest at the time of writing. \n\n"}
{"id": "1710.00166", "contents": "Title: PCANet-II: When PCANet Meets the Second Order Pooling Abstract: PCANet, as one noticeable shallow network, employs the histogram\nrepresentation for feature pooling. However, there are three main problems\nabout this kind of pooling method. First, the histogram-based pooling method\nbinarizes the feature maps and leads to inevitable discriminative information\nloss. Second, it is difficult to effectively combine other visual cues into a\ncompact representation, because the simple concatenation of various visual cues\nleads to feature representation inefficiency. Third, the dimensionality of\nhistogram-based output grows exponentially with the number of feature maps\nused. In order to overcome these problems, we propose a novel shallow network\nmodel, named as PCANet-II. Compared with the histogram-based output, the second\norder pooling not only provides more discriminative information by preserving\nboth the magnitude and sign of convolutional responses, but also dramatically\nreduces the size of output features. Thus we combine the second order\nstatistical pooling method with the shallow network, i.e., PCANet. Moreover, it\nis easy to combine other discriminative and robust cues by using the second\norder pooling. So we introduce the binary feature difference encoding scheme\ninto our PCANet-II to further improve robustness. Experiments demonstrate the\neffectiveness and robustness of our proposed PCANet-II method. \n\n"}
{"id": "1710.00870", "contents": "Title: Rethinking Feature Discrimination and Polymerization for Large-scale\n  Recognition Abstract: Feature matters. How to train a deep network to acquire discriminative\nfeatures across categories and polymerized features within classes has always\nbeen at the core of many computer vision tasks, specially for large-scale\nrecognition systems where test identities are unseen during training and the\nnumber of classes could be at million scale. In this paper, we address this\nproblem based on the simple intuition that the cosine distance of features in\nhigh-dimensional space should be close enough within one class and far away\nacross categories. To this end, we proposed the congenerous cosine (COCO)\nalgorithm to simultaneously optimize the cosine similarity among data. It\ninherits the softmax property to make inter-class features discriminative as\nwell as shares the idea of class centroid in metric learning. Unlike previous\nwork where the center is a temporal, statistical variable within one mini-batch\nduring training, the formulated centroid is responsible for clustering\ninner-class features to enforce them polymerized around the network truncus.\nCOCO is bundled with discriminative training and learned end-to-end with stable\nconvergence. Experiments on five benchmarks have been extensively conducted to\nverify the effectiveness of our approach on both small-scale classification\ntask and large-scale human recognition problem. \n\n"}
{"id": "1710.00962", "contents": "Title: GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks Abstract: Facial landmarks constitute the most compressed representation of faces and\nare known to preserve information such as pose, gender and facial structure\npresent in the faces. Several works exist that attempt to perform high-level\nface-related analysis tasks based on landmarks. In contrast, in this work, an\nattempt is made to tackle the inverse problem of synthesizing faces from their\nrespective landmarks. The primary aim of this work is to demonstrate that\ninformation preserved by landmarks (gender in particular) can be further\naccentuated by leveraging generative models to synthesize corresponding faces.\nThough the problem is particularly challenging due to its ill-posed nature, we\nbelieve that successful synthesis will enable several applications such as\nboosting performance of high-level face related tasks using landmark points and\nperforming dataset augmentation. To this end, a novel face-synthesis method\nknown as Gender Preserving Generative Adversarial Network (GP-GAN) that is\nguided by adversarial loss, perceptual loss and a gender preserving loss is\npresented. Further, we propose a novel generator sub-network UDeNet for GP-GAN\nthat leverages advantages of U-Net and DenseNet architectures. Extensive\nexperiments and comparison with recent methods are performed to verify the\neffectiveness of the proposed method. \n\n"}
{"id": "1710.01020", "contents": "Title: Learning Affinity via Spatial Propagation Networks Abstract: In this paper, we propose spatial propagation networks for learning the\naffinity matrix for vision tasks. We show that by constructing a row/column\nlinear propagation model, the spatially varying transformation matrix exactly\nconstitutes an affinity matrix that models dense, global pairwise relationships\nof an image. Specifically, we develop a three-way connection for the linear\npropagation model, which (a) formulates a sparse transformation matrix, where\nall elements can be the output from a deep CNN, but (b) results in a dense\naffinity matrix that effectively models any task-specific pairwise similarity\nmatrix. Instead of designing the similarity kernels according to image features\nof two points, we can directly output all the similarities in a purely\ndata-driven manner. The spatial propagation network is a generic framework that\ncan be applied to many affinity-related tasks, including but not limited to\nimage matting, segmentation and colorization, to name a few. Essentially, the\nmodel can learn semantically-aware affinity values for high-level vision tasks\ndue to the powerful learning capability of the deep neural network classifier.\nWe validate the framework on the task of refinement for image segmentation\nboundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic\nsegmentation tasks show that the spatial propagation network provides a\ngeneral, effective and efficient solution for generating high-quality\nsegmentation results. \n\n"}
{"id": "1710.01103", "contents": "Title: Isotropic and Steerable Wavelets in N Dimensions. A multiresolution\n  analysis framework for ITK Abstract: This document describes the implementation of the external module\nITKIsotropicWavelets, a multiresolution (MRA) analysis framework using\nisotropic and steerable wavelets in the frequency domain. This framework\nprovides the backbone for state of the art filters for denoising, feature\ndetection or phase analysis in N-dimensions. It focus on reusability, and\nhighly decoupled modules for easy extension and implementation of new filters,\nand it contains a filter for multiresolution phase analysis,\n  The backbone of the multi-scale analysis is provided by an isotropic\nband-limited wavelet pyramid, and the detection of directional features is\nprovided by coupling the pyramid with a generalized Riesz transform. The\ngeneralized Riesz transform of order N behaves like a smoothed version of the\nNth order derivatives of the signal. Also, it is steerable: its components\nimpulse responses can be rotated to any spatial orientation, reducing\ncomputation time when detecting directional features. \n\n"}
{"id": "1710.01249", "contents": "Title: A Comparative Study of CNN, BoVW and LBP for Classification of\n  Histopathological Images Abstract: Despite the progress made in the field of medical imaging, it remains a large\narea of open research, especially due to the variety of imaging modalities and\ndisease-specific characteristics. This paper is a comparative study describing\nthe potential of using local binary patterns (LBP), deep features and the\nbag-of-visual words (BoVW) scheme for the classification of histopathological\nimages. We introduce a new dataset, \\emph{KIMIA Path960}, that contains 960\nhistopathology images belonging to 20 different classes (different tissue\ntypes). We make this dataset publicly available. The small size of the dataset\nand its inter- and intra-class variability makes it ideal for initial\ninvestigations when comparing image descriptors for search and classification\nin complex medical imaging cases like histopathology. We investigate deep\nfeatures, LBP histograms and BoVW to classify the images via leave-one-out\nvalidation. The accuracy of image classification obtained using LBP was 90.62\\%\nwhile the highest accuracy using deep features reached 94.72\\%. The dictionary\napproach (BoVW) achieved 96.50\\%. Deep solutions may be able to deliver higher\naccuracies but they need extensive training with a large number of (balanced)\nimage datasets. \n\n"}
{"id": "1710.01255", "contents": "Title: Variational Grid Setting Network Abstract: We propose a new neural network architecture for automatic generation of\nmissing characters in a Chinese font set. We call the neural network\narchitecture the Variational Grid Setting Network which is based on the\nvariational autoencoder (VAE) with some tweaks. The neural network model is\nable to generate missing characters relatively large in size ($256 \\times 256$\npixels). Moreover, we show that one can use very few samples for training data\nset, and get a satisfied result. \n\n"}
{"id": "1710.01408", "contents": "Title: A Fully Convolutional Network for Semantic Labeling of 3D Point Clouds Abstract: When classifying point clouds, a large amount of time is devoted to the\nprocess of engineering a reliable set of features which are then passed to a\nclassifier of choice. Generally, such features - usually derived from the\n3D-covariance matrix - are computed using the surrounding neighborhood of\npoints. While these features capture local information, the process is usually\ntime-consuming, and requires the application at multiple scales combined with\ncontextual methods in order to adequately describe the diversity of objects\nwithin a scene. In this paper we present a 1D-fully convolutional network that\nconsumes terrain-normalized points directly with the corresponding spectral\ndata,if available, to generate point-wise labeling while implicitly learning\ncontextual features in an end-to-end fashion. Our method uses only the\n3D-coordinates and three corresponding spectral features for each point.\nSpectral features may either be extracted from 2D-georeferenced images, as\nshown here for Light Detection and Ranging (LiDAR) point clouds, or extracted\ndirectly for passive-derived point clouds,i.e. from muliple-view imagery. We\ntrain our network by splitting the data into square regions, and use a pooling\nlayer that respects the permutation-invariance of the input points. Evaluated\nusing the ISPRS 3D Semantic Labeling Contest, our method scored second place\nwith an overall accuracy of 81.6%. We ranked third place with a mean F1-score\nof 63.32%, surpassing the F1-score of the method with highest accuracy by\n1.69%. In addition to labeling 3D-point clouds, we also show that our method\ncan be easily extended to 2D-semantic segmentation tasks, with promising\ninitial results. \n\n"}
{"id": "1710.01766", "contents": "Title: DeepLesion: Automated Deep Mining, Categorization and Detection of\n  Significant Radiology Image Findings using Large-Scale Clinical Lesion\n  Annotations Abstract: Extracting, harvesting and building large-scale annotated radiological image\ndatasets is a greatly important yet challenging problem. It is also the\nbottleneck to designing more effective data-hungry computing paradigms (e.g.,\ndeep learning) for medical image analysis. Yet, vast amounts of clinical\nannotations (usually associated with disease image findings and marked using\narrows, lines, lesion diameters, segmentation, etc.) have been collected over\nseveral decades and stored in hospitals' Picture Archiving and Communication\nSystems. In this paper, we mine and harvest one major type of clinical\nannotation data - lesion diameters annotated on bookmarked images - to learn an\neffective multi-class lesion detector via unsupervised and supervised deep\nConvolutional Neural Networks (CNN). Our dataset is composed of 33,688\nbookmarked radiology images from 10,825 studies of 4,477 unique patients. For\nevery bookmarked image, a bounding box is created to cover the target lesion\nbased on its measured diameters. We categorize the collection of lesions using\nan unsupervised deep mining scheme to generate clustered pseudo lesion labels.\nNext, we adopt a regional-CNN method to detect lesions of multiple categories,\nregardless of missing annotations (normally only one lesion is annotated,\ndespite the presence of multiple co-existing findings). Our integrated mining,\ncategorization and detection framework is validated with promising empirical\nresults, as a scalable, universal or multi-purpose CAD paradigm built upon\nabundant retrospective medical data. Furthermore, we demonstrate that detection\naccuracy can be significantly improved by incorporating pseudo lesion labels\n(e.g., Liver lesion/tumor, Lung nodule/tumor, Abdomen lesions, Chest lymph node\nand others). This dataset will be made publicly available (under the open\nscience initiative). \n\n"}
{"id": "1710.02584", "contents": "Title: Bag-Level Aggregation for Multiple Instance Active Learning in Instance\n  Classification Problems Abstract: A growing number of applications, e.g. video surveillance and medical image\nanalysis, require training recognition systems from large amounts of weakly\nannotated data while some targeted interactions with a domain expert are\nallowed to improve the training process. In such cases, active learning (AL)\ncan reduce labeling costs for training a classifier by querying the expert to\nprovide the labels of most informative instances. This paper focuses on AL\nmethods for instance classification problems in multiple instance learning\n(MIL), where data is arranged into sets, called bags, that are weakly labeled.\nMost AL methods focus on single instance learning problems. These methods are\nnot suitable for MIL problems because they cannot account for the bag structure\nof data. In this paper, new methods for bag-level aggregation of instance\ninformativeness are proposed for multiple instance active learning (MIAL). The\n\\textit{aggregated informativeness} method identifies the most informative\ninstances based on classifier uncertainty, and queries bags incorporating the\nmost information. The other proposed method, called \\textit{cluster-based\naggregative sampling}, clusters data hierarchically in the instance space. The\ninformativeness of instances is assessed by considering bag labels, inferred\ninstance labels, and the proportion of labels that remain to be discovered in\nclusters. Both proposed methods significantly outperform reference methods in\nextensive experiments using benchmark data from several application domains.\nResults indicate that using an appropriate strategy to address MIAL problems\nyields a significant reduction in the number of queries needed to achieve the\nsame level of performance as single instance AL methods. \n\n"}
{"id": "1710.03337", "contents": "Title: Standard detectors aren't (currently) fooled by physical adversarial\n  stop signs Abstract: An adversarial example is an example that has been adjusted to produce the\nwrong label when presented to a system at test time. If adversarial examples\nexisted that could fool a detector, they could be used to (for example) wreak\nhavoc on roads populated with smart vehicles. Recently, we described our\ndifficulties creating physical adversarial stop signs that fool a detector.\nMore recently, Evtimov et al. produced a physical adversarial stop sign that\nfools a proxy model of a detector. In this paper, we show that these physical\nadversarial stop signs do not fool two standard detectors (YOLO and Faster\nRCNN) in standard configuration. Evtimov et al.'s construction relies on a crop\nof the image to the stop sign; this crop is then resized and presented to a\nclassifier. We argue that the cropping and resizing procedure largely\neliminates the effects of rescaling and of view angle. Whether an adversarial\nattack is robust under rescaling and change of view direction remains moot. We\nargue that attacking a classifier is very different from attacking a detector,\nand that the structure of detectors - which must search for their own bounding\nbox, and which cannot estimate that box very accurately - likely makes it\ndifficult to make adversarial patterns. Finally, an adversarial pattern on a\nphysical object that could fool a detector would have to be adversarial in the\nface of a wide family of parametric distortions (scale; view angle; box shift\ninside the detector; illumination; and so on). Such a pattern would be of great\ntheoretical and practical interest. There is currently no evidence that such\npatterns exist. \n\n"}
{"id": "1710.06507", "contents": "Title: Scene Parsing with Global Context Embedding Abstract: We present a scene parsing method that utilizes global context information\nbased on both the parametric and non- parametric models. Compared to previous\nmethods that only exploit the local relationship between objects, we train a\ncontext network based on scene similarities to generate feature representations\nfor global contexts. In addition, these learned features are utilized to\ngenerate global and spatial priors for explicit classes inference. We then\ndesign modules to embed the feature representations and the priors into the\nsegmentation network as additional global context cues. We show that the\nproposed method can eliminate false positives that are not compatible with the\nglobal context representations. Experiments on both the MIT ADE20K and PASCAL\nContext datasets show that the proposed method performs favorably against\nexisting methods. \n\n"}
{"id": "1710.06929", "contents": "Title: Unsupervised Object Discovery and Segmentation of RGBD-images Abstract: In this paper we introduce a system for unsupervised object discovery and\nsegmentation of RGBD-images. The system models the sensor noise directly from\ndata, allowing accurate segmentation without sensor specific hand tuning of\nmeasurement noise models making use of the recently introduced Statistical\nInlier Estimation (SIE) method. Through a fully probabilistic formulation, the\nsystem is able to apply probabilistic inference, enabling reliable segmentation\nin previously challenging scenarios. In addition, we introduce new methods for\nfiltering out false positives, significantly improving the signal to noise\nratio. We show that the system significantly outperform state-of-the-art in on\na challenging real-world dataset. \n\n"}
{"id": "1710.07346", "contents": "Title: Be Your Own Prada: Fashion Synthesis with Structural Coherence Abstract: We present a novel and effective approach for generating new clothing on a\nwearer through generative adversarial learning. Given an input image of a\nperson and a sentence describing a different outfit, our model \"redresses\" the\nperson as desired, while at the same time keeping the wearer and her/his pose\nunchanged. Generating new outfits with precise regions conforming to a language\ndescription while retaining wearer's body structure is a new challenging task.\nExisting generative adversarial networks are not ideal in ensuring global\ncoherence of structure given both the input photograph and language description\nas conditions. We address this challenge by decomposing the complex generative\nprocess into two conditional stages. In the first stage, we generate a\nplausible semantic segmentation map that obeys the wearer's pose as a latent\nspatial arrangement. An effective spatial constraint is formulated to guide the\ngeneration of this semantic segmentation map. In the second stage, a generative\nmodel with a newly proposed compositional mapping layer is used to render the\nfinal image with precise regions and textures conditioned on this map. We\nextended the DeepFashion dataset [8] by collecting sentence descriptions for\n79K images. We demonstrate the effectiveness of our approach through both\nquantitative and qualitative evaluations. A user study is also conducted. The\ncodes and the data are available at http://mmlab.ie.cuhk.\nedu.hk/projects/FashionGAN/. \n\n"}
{"id": "1710.08014", "contents": "Title: Deep Cropping via Attention Box Prediction and Aesthetics Assessment Abstract: We model the photo cropping problem as a cascade of attention box regression\nand aesthetic quality classification, based on deep learning. A neural network\nis designed that has two branches for predicting attention bounding box and\nanalyzing aesthetics, respectively. The predicted attention box is treated as\nan initial crop window where a set of cropping candidates are generated around\nit, without missing important information. Then, aesthetics assessment is\nemployed to select the final crop as the one with the best aesthetic quality.\nWith our network, cropping candidates share features within full-image\nconvolutional feature maps, thus avoiding repeated feature computation and\nleading to higher computation efficiency. Via leveraging rich data for\nattention prediction and aesthetics assessment, the proposed method produces\nhigh-quality cropping results, even with the limited availability of training\ndata for photo cropping. The experimental results demonstrate the competitive\nresults and fast processing speed (5 fps with all steps). \n\n"}
{"id": "1710.08798", "contents": "Title: Fast PET Scan Tumor Segmentation using Superpixels, Principal Component\n  Analysis and K-means Clustering Abstract: Positron Emission Tomography scan images are extensively used in radiotherapy\nplanning, clinical diagnosis, assessment of growth and treatment of a tumor.\nThese all rely on fidelity and speed of detection and delineation algorithm.\nDespite intensive research, segmentation remained a challenging problem due to\nthe diverse image content, resolution, shape, and noise. This paper presents a\nfast positron emission tomography tumor segmentation method in which\nsuperpixels are extracted first from the input image. Principal component\nanalysis is then applied on the superpixels and also on their average. Distance\nvector of each superpixel from the average is computed in principal components\ncoordinate system. Finally, k-means clustering is applied on distance vector to\nrecognize tumor and non-tumor superpixels. The proposed approach is implemented\nin MATLAB 2016 which resulted in an average Dice similarity of 84.2% on the\ndataset. Additionally, a very fast execution time was achieved as the number of\nsuperpixels and the size of distance vector on which clustering was done was\nvery small compared to the number of raw pixels in dataset images. \n\n"}
{"id": "1710.09282", "contents": "Title: A Survey of Model Compression and Acceleration for Deep Neural Networks Abstract: Deep neural networks (DNNs) have recently achieved great success in many\nvisual recognition tasks. However, existing deep neural network models are\ncomputationally expensive and memory intensive, hindering their deployment in\ndevices with low memory resources or in applications with strict latency\nrequirements. Therefore, a natural thought is to perform model compression and\nacceleration in deep networks without significantly decreasing the model\nperformance. During the past five years, tremendous progress has been made in\nthis area. In this paper, we review the recent techniques for compacting and\naccelerating DNN models. In general, these techniques are divided into four\ncategories: parameter pruning and quantization, low-rank factorization,\ntransferred/compact convolutional filters, and knowledge distillation. Methods\nof parameter pruning and quantization are described first, after that the other\ntechniques are introduced. For each category, we also provide insightful\nanalysis about the performance, related applications, advantages, and\ndrawbacks. Then we go through some very recent successful methods, for example,\ndynamic capacity networks and stochastic depths networks. After that, we survey\nthe evaluation matrices, the main datasets used for evaluating the model\nperformance, and recent benchmark efforts. Finally, we conclude this paper,\ndiscuss remaining the challenges and possible directions for future work. \n\n"}
{"id": "1710.10589", "contents": "Title: Automatic Knee Osteoarthritis Diagnosis from Plain Radiographs: A Deep\n  Learning-Based Approach Abstract: Knee osteoarthritis (OA) is the most common musculoskeletal disorder. OA\ndiagnosis is currently conducted by assessing symptoms and evaluating plain\nradiographs, but this process suffers from subjectivity. In this study, we\npresent a new transparent computer-aided diagnosis method based on the Deep\nSiamese Convolutional Neural Network to automatically score knee OA severity\naccording to the Kellgren-Lawrence grading scale. We trained our method using\nthe data solely from the Multicenter Osteoarthritis Study and validated it on\nrandomly selected 3,000 subjects (5,960 knees) from Osteoarthritis Initiative\ndataset. Our method yielded a quadratic Kappa coefficient of 0.83 and average\nmulticlass accuracy of 66.71\\% compared to the annotations given by a committee\nof clinical experts. Here, we also report a radiological OA diagnosis area\nunder the ROC curve of 0.93. We also present attention maps -- given as a class\nprobability distribution -- highlighting the radiological features affecting\nthe network decision. This information makes the decision process transparent\nfor the practitioner, which builds better trust toward automatic methods. We\nbelieve that our model is useful for clinical decision making and for OA\nresearch; therefore, we openly release our training codes and the data set\ncreated in this study. \n\n"}
{"id": "1710.11087", "contents": "Title: An Integrated Approach to Crowd Video Analysis: From Tracking to\n  Multi-level Activity Recognition Abstract: We present an integrated framework for simultaneous tracking, group detection\nand multi-level activity recognition in crowd videos. Instead of solving these\nproblems independently and sequentially, we solve them together in a unified\nframework to utilize the strong correlation that exists among individual\nmotion, groups, and activities. We explore the hierarchical structure hidden in\nthe video that connects individuals over time to produce tracks, connects\nindividuals to form groups and also connects groups together to form a crowd.\nWe show that estimation of this hidden structure corresponds to track\nassociation and group detection. We estimate this hidden structure under a\nlinear programming formulation. The obtained graphical representation is\nfurther explored to recognize the node values that corresponds to multi-level\nactivity recognition. This problem is solved under a structured SVM framework.\nThe results on publicly available dataset show very competitive performance at\nall levels of granularity with the state-of-the-art batch processing methods\ndespite the proposed technique being an online (causal) one. \n\n"}
{"id": "1710.11446", "contents": "Title: Clothing Retrieval with Visual Attention Model Abstract: Clothing retrieval is a challenging problem in computer vision. With the\nadvance of Convolutional Neural Networks (CNNs), the accuracy of clothing\nretrieval has been significantly improved. FashionNet[1], a recent study,\nproposes to employ a set of artificial features in the form of landmarks for\nclothing retrieval, which are shown to be helpful for retrieval. However, the\nlandmark detection module is trained with strong supervision which requires\nconsiderable efforts to obtain. In this paper, we propose a self-learning\nVisual Attention Model (VAM) to extract attention maps from clothing images.\nThe VAM is further connected to a global network to form an end-to-end network\nstructure through Impdrop connection which randomly Dropout on the feature maps\nwith the probabilities given by the attention map. Extensive experiments on\nseveral widely used benchmark clothing retrieval data sets have demonstrated\nthe promise of the proposed method. We also show that compared to the trivial\nProduct connection, the Impdrop connection makes the network structure more\nrobust when training sets of limited size are used. \n\n"}
{"id": "1711.00888", "contents": "Title: Set-to-Set Hashing with Applications in Visual Recognition Abstract: Visual data, such as an image or a sequence of video frames, is often\nnaturally represented as a point set. In this paper, we consider the\nfundamental problem of finding a nearest set from a collection of sets, to a\nquery set. This problem has obvious applications in large-scale visual\nretrieval and recognition, and also in applied fields beyond computer vision.\nOne challenge stands out in solving the problem---set representation and\nmeasure of similarity. Particularly, the query set and the sets in dataset\ncollection can have varying cardinalities. The training collection is large\nenough such that linear scan is impractical. We propose a simple representation\nscheme that encodes both statistical and structural information of the sets.\nThe derived representations are integrated in a kernel framework for flexible\nsimilarity measurement. For the query set process, we adopt a learning-to-hash\npipeline that turns the kernel representations into hash bits based on simple\nlearners, using multiple kernel learning. Experiments on two visual retrieval\ndatasets show unambiguously that our set-to-set hashing framework outperforms\nprior methods that do not take the set-to-set search setting. \n\n"}
{"id": "1711.02037", "contents": "Title: Randomized Nonnegative Matrix Factorization Abstract: Nonnegative matrix factorization (NMF) is a powerful tool for data mining.\nHowever, the emergence of `big data' has severely challenged our ability to\ncompute this fundamental decomposition using deterministic algorithms. This\npaper presents a randomized hierarchical alternating least squares (HALS)\nalgorithm to compute the NMF. By deriving a smaller matrix from the nonnegative\ninput data, a more efficient nonnegative decomposition can be computed. Our\nalgorithm scales to big data applications while attaining a near-optimal\nfactorization. The proposed algorithm is evaluated using synthetic and real\nworld data and shows substantial speedups compared to deterministic HALS. \n\n"}
{"id": "1711.02837", "contents": "Title: Revealing structure components of the retina by deep learning networks Abstract: Deep convolutional neural networks (CNNs) have demonstrated impressive\nperformance on visual object classification tasks. In addition, it is a useful\nmodel for predication of neuronal responses recorded in visual system. However,\nthere is still no clear understanding of what CNNs learn in terms of visual\nneuronal circuits. Visualizing CNN's features to obtain possible connections to\nneuronscience underpinnings is not easy due to highly complex circuits from the\nretina to higher visual cortex. Here we address this issue by focusing on\nsingle retinal ganglion cells with a simple model and electrophysiological\nrecordings from salamanders. By training CNNs with white noise images to\npredicate neural responses, we found that convolutional filters learned in the\nend are resembling to biological components of the retinal circuit. Features\nrepresented by these filters tile the space of conventional receptive field of\nretinal ganglion cells. These results suggest that CNN could be used to reveal\nstructure components of neuronal circuits. \n\n"}
{"id": "1711.03473", "contents": "Title: Making a long story short: A Multi-Importance fast-forwarding egocentric\n  videos with the emphasis on relevant objects Abstract: The emergence of low-cost high-quality personal wearable cameras combined\nwith the increasing storage capacity of video-sharing websites have evoked a\ngrowing interest in first-person videos, since most videos are composed of\nlong-running unedited streams which are usually tedious and unpleasant to\nwatch. State-of-the-art semantic fast-forward methods currently face the\nchallenge of providing an adequate balance between smoothness in visual flow\nand the emphasis on the relevant parts. In this work, we present the\nMulti-Importance Fast-Forward (MIFF), a fully automatic methodology to\nfast-forward egocentric videos facing these challenges. The dilemma of defining\nwhat is the semantic information of a video is addressed by a learning process\nbased on the preferences of the user. Results show that the proposed method\nkeeps over $3$ times more semantic content than the state-of-the-art\nfast-forward. Finally, we discuss the need of a particular video stabilization\ntechnique for fast-forward egocentric videos. \n\n"}
{"id": "1711.04161", "contents": "Title: End-to-end Video-level Representation Learning for Action Recognition Abstract: From the frame/clip-level feature learning to the video-level representation\nbuilding, deep learning methods in action recognition have developed rapidly in\nrecent years. However, current methods suffer from the confusion caused by\npartial observation training, or without end-to-end learning, or restricted to\nsingle temporal scale modeling and so on. In this paper, we build upon\ntwo-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling\n(DTPP), an end-to-end video-level representation learning approach, to address\nthese problems. Specifically, at first, RGB images and optical flow stacks are\nsparsely sampled across the whole video. Then a temporal pyramid pooling layer\nis used to aggregate the frame-level features which consist of spatial and\ntemporal cues. Lastly, the trained model has compact video-level representation\nwith multiple temporal scales, which is both global and sequence-aware.\nExperimental results show that DTPP achieves the state-of-the-art performance\non two challenging video action datasets: UCF101 and HMDB51, either by ImageNet\npre-training or Kinetics pre-training. \n\n"}
{"id": "1711.04325", "contents": "Title: Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15\n  Minutes Abstract: We demonstrate that training ResNet-50 on ImageNet for 90 epochs can be\nachieved in 15 minutes with 1024 Tesla P100 GPUs. This was made possible by\nusing a large minibatch size of 32k. To maintain accuracy with this large\nminibatch size, we employed several techniques such as RMSprop warm-up, batch\nnormalization without moving averages, and a slow-start learning rate schedule.\nThis paper also describes the details of the hardware and software of the\nsystem used to achieve the above performance. \n\n"}
{"id": "1711.04340", "contents": "Title: Data Augmentation Generative Adversarial Networks Abstract: Effective training of neural networks requires much data. In the low-data\nregime, parameters are underdetermined, and learnt networks generalise poorly.\nData Augmentation alleviates this by using existing data more effectively.\nHowever standard data augmentation produces only limited plausible alternative\ndata. Given there is potential to generate a much broader set of augmentations,\nwe design and train a generative model to do data augmentation. The model,\nbased on image conditional Generative Adversarial Networks, takes data from a\nsource domain and learns to take any data item and generalise it to generate\nother within-class data items. As this generative process does not depend on\nthe classes themselves, it can be applied to novel unseen classes of data. We\nshow that a Data Augmentation Generative Adversarial Network (DAGAN) augments\nstandard vanilla classifiers well. We also show a DAGAN can enhance few-shot\nlearning systems such as Matching Networks. We demonstrate these approaches on\nOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In\nour experiments we can see over 13% increase in accuracy in the low-data regime\nexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face\n(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%\n(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%). \n\n"}
{"id": "1711.05175", "contents": "Title: Adversarial Information Factorization Abstract: We propose a novel generative model architecture designed to learn\nrepresentations for images that factor out a single attribute from the rest of\nthe representation. A single object may have many attributes which when altered\ndo not change the identity of the object itself. Consider the human face; the\nidentity of a particular person is independent of whether or not they happen to\nbe wearing glasses. The attribute of wearing glasses can be changed without\nchanging the identity of the person. However, the ability to manipulate and\nalter image attributes without altering the object identity is not a trivial\ntask. Here, we are interested in learning a representation of the image that\nseparates the identity of an object (such as a human face) from an attribute\n(such as 'wearing glasses'). We demonstrate the success of our factorization\napproach by using the learned representation to synthesize the same face with\nand without a chosen attribute. We refer to this specific synthesis process as\nimage attribute manipulation. We further demonstrate that our model achieves\ncompetitive scores, with state of the art, on a facial attribute classification\ntask. \n\n"}
{"id": "1711.05407", "contents": "Title: MARGIN: Uncovering Deep Neural Networks using Graph Signal Analysis Abstract: Interpretability has emerged as a crucial aspect of building trust in machine\nlearning systems, aimed at providing insights into the working of complex\nneural networks that are otherwise opaque to a user. There are a plethora of\nexisting solutions addressing various aspects of interpretability ranging from\nidentifying prototypical samples in a dataset to explaining image predictions\nor explaining mis-classifications. While all of these diverse techniques\naddress seemingly different aspects of interpretability, we hypothesize that a\nlarge family of interepretability tasks are variants of the same central\nproblem which is identifying \\emph{relative} change in a model's prediction.\nThis paper introduces MARGIN, a simple yet general approach to address a large\nset of interpretability tasks MARGIN exploits ideas rooted in graph signal\nanalysis to determine influential nodes in a graph, which are defined as those\nnodes that maximally describe a function defined on the graph. By carefully\ndefining task-specific graphs and functions, we demonstrate that MARGIN\noutperforms existing approaches in a number of disparate interpretability\nchallenges. \n\n"}
{"id": "1711.05611", "contents": "Title: Interpreting Deep Visual Representations via Network Dissection Abstract: The success of recent deep convolutional neural networks (CNNs) depends on\nlearning hidden representations that can summarize the important factors of\nvariation behind the data. However, CNNs often criticized as being black boxes\nthat lack interpretability, since they have millions of unexplained model\nparameters. In this work, we describe Network Dissection, a method that\ninterprets networks by providing labels for the units of their deep visual\nrepresentations. The proposed method quantifies the interpretability of CNN\nrepresentations by evaluating the alignment between individual hidden units and\na set of visual semantic concepts. By identifying the best alignments, units\nare given human interpretable labels across a range of objects, parts, scenes,\ntextures, materials, and colors. The method reveals that deep representations\nare more transparent and interpretable than expected: we find that\nrepresentations are significantly more interpretable than they would be under a\nrandom equivalently powerful basis. We apply the method to interpret and\ncompare the latent representations of various network architectures trained to\nsolve different supervised and self-supervised training tasks. We then examine\nfactors affecting the network interpretability such as the number of the\ntraining iterations, regularizations, different initializations, and the\nnetwork depth and width. Finally we show that the interpreted units can be used\nto provide explicit explanations of a prediction given by a CNN for an image.\nOur results highlight that interpretability is an important property of deep\nneural networks that provides new insights into their hierarchical structure. \n\n"}
{"id": "1711.05702", "contents": "Title: Brain Extraction from Normal and Pathological Images: A Joint\n  PCA/Image-Reconstruction Approach Abstract: Brain extraction from images is a common pre-processing step. Many approaches\nexist, but they are frequently only designed to perform brain extraction from\nimages without strong pathologies. Extracting the brain from images with strong\npathologies, for example, the presence of a tumor or of a traumatic brain\ninjury, is challenging. In such cases, tissue appearance may deviate from\nnormal tissue and violates algorithmic assumptions for these approaches; hence,\nthe brain may not be correctly extracted. This paper proposes a brain\nextraction approach which can explicitly account for pathologies by jointly\nmodeling normal tissue and pathologies. Specifically, our model uses a\nthree-part image decomposition: (1) normal tissue appearance is captured by\nprincipal component analysis, (2) pathologies are captured via a total\nvariation term, and (3) non-brain tissue is captured by a sparse term.\nDecomposition and image registration steps are alternated to allow statistical\nmodeling in a fixed atlas space. As a beneficial side effect, the model allows\nfor the identification of potential pathologies and the reconstruction of a\nquasi-normal image in atlas space. We demonstrate the effectiveness of our\nmethod on four datasets: the IBSR and LPBA40 datasets which show normal images,\nthe BRATS dataset containing images with brain tumors and a dataset containing\nclinical TBI images. We compare the performance with other popular models:\nROBEX, BEaST, MASS, BET, BSE and a recently proposed deep learning approach.\nOur model performs better than these competing methods on all four datasets.\nSpecifically, our model achieves the best median (97.11) and mean (96.88) Dice\nscores over all datasets. The two best performing competitors, ROBEX and MASS,\nachieve scores of 96.23/95.62 and 96.67/94.25 respectively. Hence, our approach\nis an effective method for high quality brain extraction on a wide variety of\nimages. \n\n"}
{"id": "1711.06020", "contents": "Title: Global versus Localized Generative Adversarial Nets Abstract: In this paper, we present a novel localized Generative Adversarial Net (GAN)\nto learn on the manifold of real data. Compared with the classic GAN that {\\em\nglobally} parameterizes a manifold, the Localized GAN (LGAN) uses local\ncoordinate charts to parameterize distinct local geometry of how data points\ncan transform at different locations on the manifold. Specifically, around each\npoint there exists a {\\em local} generator that can produce data following\ndiverse patterns of transformations on the manifold. The locality nature of\nLGAN enables local generators to adapt to and directly access the local\ngeometry without need to invert the generator in a global GAN. Furthermore, it\ncan prevent the manifold from being locally collapsed to a dimensionally\ndeficient tangent subspace by imposing an orthonormality prior between\ntangents. This provides a geometric approach to alleviating mode collapse at\nleast locally on the manifold by imposing independence between data\ntransformations in different tangent directions. We will also demonstrate the\nLGAN can be applied to train a robust classifier that prefers locally\nconsistent classification decisions on the manifold, and the resultant\nregularizer is closely related with the Laplace-Beltrami operator. Our\nexperiments show that the proposed LGANs can not only produce diverse image\ntransformations, but also deliver superior classification performances. \n\n"}
{"id": "1711.06354", "contents": "Title: Grounded Objects and Interactions for Video Captioning Abstract: We address the problem of video captioning by grounding language generation\non object interactions in the video. Existing work mostly focuses on overall\nscene understanding with often limited or no emphasis on object interactions to\naddress the problem of video understanding. In this paper, we propose\nSINet-Caption that learns to generate captions grounded over higher-order\ninteractions between arbitrary groups of objects for fine-grained video\nunderstanding. We discuss the challenges and benefits of such an approach. We\nfurther demonstrate state-of-the-art results on the ActivityNet Captions\ndataset using our model, SINet-Caption based on this approach. \n\n"}
{"id": "1711.06505", "contents": "Title: Image Matters: Visually modeling user behaviors using Advanced Model\n  Server Abstract: In Taobao, the largest e-commerce platform in China, billions of items are\nprovided and typically displayed with their images. For better user experience\nand business effectiveness, Click Through Rate (CTR) prediction in online\nadvertising system exploits abundant user historical behaviors to identify\nwhether a user is interested in a candidate ad. Enhancing behavior\nrepresentations with user behavior images will help understand user's visual\npreference and improve the accuracy of CTR prediction greatly. So we propose to\nmodel user preference jointly with user behavior ID features and behavior\nimages. However, training with user behavior images brings tens to hundreds of\nimages in one sample, giving rise to a great challenge in both communication\nand computation. To handle these challenges, we propose a novel and efficient\ndistributed machine learning paradigm called Advanced Model Server (AMS). With\nthe well known Parameter Server (PS) framework, each server node handles a\nseparate part of parameters and updates them independently. AMS goes beyond\nthis and is designed to be capable of learning a unified image descriptor model\nshared by all server nodes which embeds large images into low dimensional high\nlevel features before transmitting images to worker nodes. AMS thus\ndramatically reduces the communication load and enables the arduous joint\ntraining process. Based on AMS, the methods of effectively combining the images\nand ID features are carefully studied, and then we propose a Deep Image CTR\nModel. Our approach is shown to achieve significant improvements in both online\nand offline evaluations, and has been deployed in Taobao display advertising\nsystem serving the main traffic. \n\n"}
{"id": "1711.06526", "contents": "Title: Multi-Label Zero-Shot Learning with Structured Knowledge Graphs Abstract: In this paper, we propose a novel deep learning architecture for multi-label\nzero-shot learning (ML-ZSL), which is able to predict multiple unseen class\nlabels for each input instance. Inspired by the way humans utilize semantic\nknowledge between objects of interests, we propose a framework that\nincorporates knowledge graphs for describing the relationships between multiple\nlabels. Our model learns an information propagation mechanism from the semantic\nlabel space, which can be applied to model the interdependencies between seen\nand unseen class labels. With such investigation of structured knowledge graphs\nfor visual reasoning, we show that our model can be applied for solving\nmulti-label classification and ML-ZSL tasks. Compared to state-of-the-art\napproaches, comparable or improved performances can be achieved by our method. \n\n"}
{"id": "1711.06704", "contents": "Title: Repeatability Is Not Enough: Learning Affine Regions via\n  Discriminability Abstract: A method for learning local affine-covariant regions is presented. We show\nthat maximizing geometric repeatability does not lead to local regions, a.k.a\nfeatures,that are reliably matched and this necessitates descriptor-based\nlearning. We explore factors that influence such learning and registration: the\nloss function, descriptor type, geometric parametrization and the trade-off\nbetween matchability and geometric accuracy and propose a novel hard\nnegative-constant loss function for learning of affine regions. The affine\nshape estimator -- AffNet -- trained with the hard negative-constant loss\noutperforms the state-of-the-art in bag-of-words image retrieval and wide\nbaseline stereo. The proposed training process does not require precisely\ngeometrically aligned patches.The source codes and trained weights are\navailable at https://github.com/ducha-aiki/affnet \n\n"}
{"id": "1711.06778", "contents": "Title: Excitation Backprop for RNNs Abstract: Deep models are state-of-the-art for many vision tasks including video action\nrecognition and video captioning. Models are trained to caption or classify\nactivity in videos, but little is known about the evidence used to make such\ndecisions. Grounding decisions made by deep networks has been studied in\nspatial visual content, giving more insight into model predictions for images.\nHowever, such studies are relatively lacking for models of spatiotemporal\nvisual content - videos. In this work, we devise a formulation that\nsimultaneously grounds evidence in space and time, in a single pass, using\ntop-down saliency. We visualize the spatiotemporal cues that contribute to a\ndeep model's classification/captioning output using the model's internal\nrepresentation. Based on these spatiotemporal cues, we are able to localize\nsegments within a video that correspond with a specific action, or phrase from\na caption, without explicitly optimizing/training for these tasks. \n\n"}
{"id": "1711.06794", "contents": "Title: Co-attending Free-form Regions and Detections with Multi-modal\n  Multiplicative Feature Embedding for Visual Question Answering Abstract: Recently, the Visual Question Answering (VQA) task has gained increasing\nattention in artificial intelligence. Existing VQA methods mainly adopt the\nvisual attention mechanism to associate the input question with corresponding\nimage regions for effective question answering. The free-form region based and\nthe detection-based visual attention mechanisms are mostly investigated, with\nthe former ones attending free-form image regions and the latter ones attending\npre-specified detection-box regions. We argue that the two attention mechanisms\nare able to provide complementary information and should be effectively\nintegrated to better solve the VQA problem. In this paper, we propose a novel\ndeep neural network for VQA that integrates both attention mechanisms. Our\nproposed framework effectively fuses features from free-form image regions,\ndetection boxes, and question representations via a multi-modal multiplicative\nfeature embedding scheme to jointly attend question-related free-form image\nregions and detection boxes for more accurate question answering. The proposed\nmethod is extensively evaluated on two publicly available datasets, COCO-QA and\nVQA, and outperforms state-of-the-art approaches. Source code is available at\nhttps://github.com/lupantech/dual-mfa-vqa. \n\n"}
{"id": "1711.06897", "contents": "Title: Single-Shot Refinement Neural Network for Object Detection Abstract: For object detection, the two-stage approach (e.g., Faster R-CNN) has been\nachieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has\nthe advantage of high efficiency. To inherit the merits of both while\novercoming their disadvantages, in this paper, we propose a novel single-shot\nbased detector, called RefineDet, that achieves better accuracy than two-stage\nmethods and maintains comparable efficiency of one-stage methods. RefineDet\nconsists of two inter-connected modules, namely, the anchor refinement module\nand the object detection module. Specifically, the former aims to (1) filter\nout negative anchors to reduce search space for the classifier, and (2)\ncoarsely adjust the locations and sizes of anchors to provide better\ninitialization for the subsequent regressor. The latter module takes the\nrefined anchors as the input from the former to further improve the regression\nand predict multi-class label. Meanwhile, we design a transfer connection block\nto transfer the features in the anchor refinement module to predict locations,\nsizes and class labels of objects in the object detection module. The\nmulti-task loss function enables us to train the whole network in an end-to-end\nway. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO\ndemonstrate that RefineDet achieves state-of-the-art detection accuracy with\nhigh efficiency. Code is available at https://github.com/sfzhang15/RefineDet \n\n"}
{"id": "1711.07235", "contents": "Title: Tracking in Aerial Hyperspectral Videos using Deep Kernelized\n  Correlation Filters Abstract: Hyperspectral imaging holds enormous potential to improve the\nstate-of-the-art in aerial vehicle tracking with low spatial and temporal\nresolutions. Recently, adaptive multi-modal hyperspectral sensors have\nattracted growing interest due to their ability to record extended data quickly\nfrom aerial platforms. In this study, we apply popular concepts from\ntraditional object tracking, namely (1) Kernelized Correlation Filters (KCF)\nand (2) Deep Convolutional Neural Network (CNN) features to aerial tracking in\nhyperspectral domain. We propose the Deep Hyperspectral Kernelized Correlation\nFilter based tracker (DeepHKCF) to efficiently track aerial vehicles using an\nadaptive multi-modal hyperspectral sensor. We address low temporal resolution\nby designing a single KCF-in-multiple Regions-of-Interest (ROIs) approach to\ncover a reasonably large area. To increase the speed of deep convolutional\nfeatures extraction from multiple ROIs, we design an effective ROI mapping\nstrategy. The proposed tracker also provides flexibility to couple with the\nmore advanced correlation filter trackers. The DeepHKCF tracker performs\nexceptionally well with deep features set up in a synthetic hyperspectral video\ngenerated by the Digital Imaging and Remote Sensing Image Generation (DIRSIG)\nsoftware. Additionally, we generate a large, synthetic, single-channel dataset\nusing DIRSIG to perform vehicle classification in the Wide Area Motion Imagery\n(WAMI) platform. This way, the high-fidelity of the DIRSIG software is proved\nand a large scale aerial vehicle classification dataset is released to support\nstudies on vehicle detection and tracking in the WAMI platform. \n\n"}
{"id": "1711.07373", "contents": "Title: Attentive Explanations: Justifying Decisions and Pointing to the\n  Evidence (Extended Abstract) Abstract: Deep models are the defacto standard in visual decision problems due to their\nimpressive performance on a wide array of visual tasks. On the other hand,\ntheir opaqueness has led to a surge of interest in explainable systems. In this\nwork, we emphasize the importance of model explanation in various forms such as\nvisual pointing and textual justification. The lack of data with justification\nannotations is one of the bottlenecks of generating multimodal explanations.\nThus, we propose two large-scale datasets with annotations that visually and\ntextually justify a classification decision for various activities, i.e. ACT-X,\nand for question answering, i.e. VQA-X. We also introduce a multimodal\nmethodology for generating visual and textual explanations simultaneously. We\nquantitatively show that training with the textual explanations not only yields\nbetter textual justification models, but also models that better localize the\nevidence that support their decision. \n\n"}
{"id": "1711.07767", "contents": "Title: Receptive Field Block Net for Accurate and Fast Object Detection Abstract: Current top-performing object detectors depend on deep CNN backbones, such as\nResNet-101 and Inception, benefiting from their powerful feature\nrepresentations but suffering from high computational costs. Conversely, some\nlightweight model based detectors fulfil real time processing, while their\naccuracies are often criticized. In this paper, we explore an alternative to\nbuild a fast and accurate detector by strengthening lightweight features using\na hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs)\nin human visual systems, we propose a novel RF Block (RFB) module, which takes\nthe relationship between the size and eccentricity of RFs into account, to\nenhance the feature discriminability and robustness. We further assemble RFB to\nthe top of SSD, constructing the RFB Net detector. To evaluate its\neffectiveness, experiments are conducted on two major benchmarks and the\nresults show that RFB Net is able to reach the performance of advanced very\ndeep detectors while keeping the real-time speed. Code is available at\nhttps://github.com/ruinmessi/RFBNet. \n\n"}
{"id": "1711.07807", "contents": "Title: Universal Denoising Networks : A Novel CNN Architecture for Image\n  Denoising Abstract: We design a novel network architecture for learning discriminative image\nmodels that are employed to efficiently tackle the problem of grayscale and\ncolor image denoising. Based on the proposed architecture, we introduce two\ndifferent variants. The first network involves convolutional layers as a core\ncomponent, while the second one relies instead on non-local filtering layers\nand thus it is able to exploit the inherent non-local self-similarity property\nof natural images. As opposed to most of the existing deep network approaches,\nwhich require the training of a specific model for each considered noise level,\nthe proposed models are able to handle a wide range of noise levels using a\nsingle set of learned parameters, while they are very robust when the noise\ndegrading the latent image does not match the statistics of the noise used\nduring training. The latter argument is supported by results that we report on\npublicly available images corrupted by unknown noise and which we compare\nagainst solutions obtained by competing methods. At the same time the\nintroduced networks achieve excellent results under additive white Gaussian\nnoise (AWGN), which are comparable to those of the current state-of-the-art\nnetwork, while they depend on a more shallow architecture with the number of\ntrained parameters being one order of magnitude smaller. These properties make\nthe proposed networks ideal candidates to serve as sub-solvers on restoration\nmethods that deal with general inverse imaging problems such as deblurring,\ndemosaicking, superresolution, etc. \n\n"}
{"id": "1711.07871", "contents": "Title: Autoencoder Node Saliency: Selecting Relevant Latent Representations Abstract: The autoencoder is an artificial neural network model that learns hidden\nrepresentations of unlabeled data. With a linear transfer function it is\nsimilar to the principal component analysis (PCA). While both methods use\nweight vectors for linear transformations, the autoencoder does not come with\nany indication similar to the eigenvalues in PCA that are paired with the\neigenvectors. We propose a novel supervised node saliency (SNS) method that\nranks the hidden nodes by comparing class distributions of latent\nrepresentations against a fixed reference distribution. The latent\nrepresentations of a hidden node can be described using a one-dimensional\nhistogram. We apply normalized entropy difference (NED) to measure the\n\"interestingness\" of the histograms, and conclude a property for NED values to\nidentify a good classifying node. By applying our methods to real data sets, we\ndemonstrate the ability of SNS to explain what the trained autoencoders have\nlearned. \n\n"}
{"id": "1711.08105", "contents": "Title: Visual Question Answering as a Meta Learning Task Abstract: The predominant approach to Visual Question Answering (VQA) demands that the\nmodel represents within its weights all of the information required to answer\nany question about any image. Learning this information from any real training\nset seems unlikely, and representing it in a reasonable number of weights\ndoubly so. We propose instead to approach VQA as a meta learning task, thus\nseparating the question answering method from the information required. At test\ntime, the method is provided with a support set of example questions/answers,\nover which it reasons to resolve the given question. The support set is not\nfixed and can be extended without retraining, thereby expanding the\ncapabilities of the model. To exploit this dynamically provided information, we\nadapt a state-of-the-art VQA model with two techniques from the recent meta\nlearning literature, namely prototypical networks and meta networks.\nExperiments demonstrate the capability of the system to learn to produce\ncompletely novel answers (i.e. never seen during training) from examples\nprovided at test time. In comparison to the existing state of the art, the\nproposed method produces qualitatively distinct results with higher recall of\nrare answers, and a better sample efficiency that allows training with little\ninitial data. More importantly, it represents an important step towards\nvision-and-language methods that can learn and reason on-the-fly. \n\n"}
{"id": "1711.08506", "contents": "Title: W-Net: A Deep Model for Fully Unsupervised Image Segmentation Abstract: While significant attention has been recently focused on designing supervised\ndeep semantic segmentation algorithms for vision tasks, there are many domains\nin which sufficient supervised pixel-level labels are difficult to obtain. In\nthis paper, we revisit the problem of purely unsupervised image segmentation\nand propose a novel deep architecture for this problem. We borrow recent ideas\nfrom supervised semantic segmentation methods, in particular by concatenating\ntwo fully convolutional networks together into an autoencoder--one for encoding\nand one for decoding. The encoding layer produces a k-way pixelwise prediction,\nand both the reconstruction error of the autoencoder as well as the normalized\ncut produced by the encoder are jointly minimized during training. When\ncombined with suitable postprocessing involving conditional random field\nsmoothing and hierarchical segmentation, our resulting algorithm achieves\nimpressive results on the benchmark Berkeley Segmentation Data Set,\noutperforming a number of competing methods. \n\n"}
{"id": "1711.08760", "contents": "Title: Boosted Cascaded Convnets for Multilabel Classification of Thoracic\n  Diseases in Chest Radiographs Abstract: Chest X-ray is one of the most accessible medical imaging technique for\ndiagnosis of multiple diseases. With the availability of ChestX-ray14, which is\na massive dataset of chest X-ray images and provides annotations for 14\nthoracic diseases; it is possible to train Deep Convolutional Neural Networks\n(DCNN) to build Computer Aided Diagnosis (CAD) systems. In this work, we\nexperiment a set of deep learning models and present a cascaded deep neural\nnetwork that can diagnose all 14 pathologies better than the baseline and is\ncompetitive with other published methods. Our work provides the quantitative\nresults to answer following research questions for the dataset: 1) What loss\nfunctions to use for training DCNN from scratch on ChestX-ray14 dataset that\ndemonstrates high class imbalance and label co occurrence? 2) How to use\ncascading to model label dependency and to improve accuracy of the deep\nlearning model? \n\n"}
{"id": "1711.08901", "contents": "Title: Supervised Hashing with End-to-End Binary Deep Neural Network Abstract: Image hashing is a popular technique applied to large scale content-based\nvisual retrieval due to its compact and efficient binary codes. Our work\nproposes a new end-to-end deep network architecture for supervised hashing\nwhich directly learns binary codes from input images and maintains good\nproperties over binary codes such as similarity preservation, independence, and\nbalancing. Furthermore, we also propose a new learning scheme that can cope\nwith the binary constrained loss function. The proposed algorithm not only is\nscalable for learning over large-scale datasets but also outperforms\nstate-of-the-art supervised hashing methods, which are illustrated throughout\nextensive experiments from various image retrieval benchmarks. \n\n"}
{"id": "1711.09405", "contents": "Title: Learning a Rotation Invariant Detector with Rotatable Bounding Box Abstract: Detection of arbitrarily rotated objects is a challenging task due to the\ndifficulties of locating the multi-angle objects and separating them\neffectively from the background. The existing methods are not robust to angle\nvaries of the objects because of the use of traditional bounding box, which is\na rotation variant structure for locating rotated objects. In this article, a\nnew detection method is proposed which applies the newly defined rotatable\nbounding box (RBox). The proposed detector (DRBox) can effectively handle the\nsituation where the orientation angles of the objects are arbitrary. The\ntraining of DRBox forces the detection networks to learn the correct\norientation angle of the objects, so that the rotation invariant property can\nbe achieved. DRBox is tested to detect vehicles, ships and airplanes on\nsatellite images, compared with Faster R-CNN and SSD, which are chosen as the\nbenchmark of the traditional bounding box based methods. The results shows that\nDRBox performs much better than traditional bounding box based methods do on\nthe given tasks, and is more robust against rotation of input image and target\nobjects. Besides, results show that DRBox correctly outputs the orientation\nangles of the objects, which is very useful for locating multi-angle objects\nefficiently. The code and models are available at\nhttps://github.com/liulei01/DRBox. \n\n"}
{"id": "1711.10448", "contents": "Title: DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer\n  Classification Abstract: Globally, in 2016, one out of eleven adults suffered from Diabetes Mellitus.\nDiabetic Foot Ulcers (DFU) are a major complication of this disease, which if\nnot managed properly can lead to amputation. Current clinical approaches to DFU\ntreatment rely on patient and clinician vigilance, which has significant\nlimitations such as the high cost involved in the diagnosis, treatment and\nlengthy care of the DFU. We collected an extensive dataset of foot images,\nwhich contain DFU from different patients. In this paper, we have proposed the\nuse of traditional computer vision features for detecting foot ulcers among\ndiabetic patients, which represent a cost-effective, remote and convenient\nhealthcare solution. Furthermore, we used Convolutional Neural Networks (CNNs)\nfor the first time in DFU classification. We have proposed a novel\nconvolutional neural network architecture, DFUNet, with better feature\nextraction to identify the feature differences between healthy skin and the\nDFU. Using 10-fold cross-validation, DFUNet achieved an AUC score of 0.962.\nThis outperformed both the machine learning and deep learning classifiers we\nhave tested. Here we present the development of a novel and highly sensitive\nDFUNet for objectively detecting the presence of DFUs. This novel approach has\nthe potential to deliver a paradigm shift in diabetic foot care. \n\n"}
{"id": "1711.10449", "contents": "Title: Multi-class Semantic Segmentation of Skin Lesions via Fully\n  Convolutional Networks Abstract: Melanoma is clinically difficult to distinguish from common benign skin\nlesions, particularly melanocytic naevus and seborrhoeic keratosis. The\ndermoscopic appearance of these lesions has huge intra-class variations and\nhigh inter-class visual similarities. Most current research is focusing on\nsingle-class segmentation irrespective of classes of skin lesions. In this\nwork, we evaluate the performance of deep learning on multi-class segmentation\nof ISIC-2017 challenge dataset, which consists of 2,750 dermoscopic images. We\npropose an end-to-end solution using fully convolutional networks (FCNs) for\nmulti-class semantic segmentation to automatically segment the melanoma,\nseborrhoeic keratosis and naevus. To improve the performance of FCNs, transfer\nlearning and a hybrid loss function are used. We evaluate the performance of\nthe deep learning segmentation methods for multi-class segmentation and lesion\ndiagnosis (with post-processing method) on the testing set of the ISIC-2017\nchallenge dataset. The results showed that the two-tier level transfer learning\nFCN-8s achieved the overall best result with \\textit{Dice} score of 78.5% in a\nnaevus category, 65.3% in melanoma, and 55.7% in seborrhoeic keratosis in\nmulti-class segmentation and Accuracy of 84.62% for recognition of melanoma in\nlesion diagnosis. \n\n"}
{"id": "1711.10703", "contents": "Title: FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors Abstract: Face Super-Resolution (SR) is a domain-specific super-resolution problem. The\nspecific facial prior knowledge could be leveraged for better super-resolving\nface images. We present a novel deep end-to-end trainable Face Super-Resolution\nNetwork (FSRNet), which makes full use of the geometry prior, i.e., facial\nlandmark heatmaps and parsing maps, to super-resolve very low-resolution (LR)\nface images without well-aligned requirement. Specifically, we first construct\na coarse SR network to recover a coarse high-resolution (HR) image. Then, the\ncoarse HR image is sent to two branches: a fine SR encoder and a prior\ninformation estimation network, which extracts the image features, and\nestimates landmark heatmaps/parsing maps respectively. Both image features and\nprior information are sent to a fine SR decoder to recover the HR image. To\nfurther generate realistic faces, we propose the Face Super-Resolution\nGenerative Adversarial Network (FSRGAN) to incorporate the adversarial loss\ninto FSRNet. Moreover, we introduce two related tasks, face alignment and\nparsing, as the new evaluation metrics for face SR, which address the\ninconsistency of classic metrics w.r.t. visual perception. Extensive benchmark\nexperiments show that FSRNet and FSRGAN significantly outperforms state of the\narts for very LR face SR, both quantitatively and qualitatively. Code will be\nmade available upon publication. \n\n"}
{"id": "1711.11155", "contents": "Title: Predicting Depression Severity by Multi-Modal Feature Engineering and\n  Fusion Abstract: We present our preliminary work to determine if patient's vocal acoustic,\nlinguistic, and facial patterns could predict clinical ratings of depression\nseverity, namely Patient Health Questionnaire depression scale (PHQ-8). We\nproposed a multi modal fusion model that combines three different modalities:\naudio, video , and text features. By training over AVEC 2017 data set, our\nproposed model outperforms each single modality prediction model, and surpasses\nthe data set baseline with ice margin. \n\n"}
{"id": "1711.11279", "contents": "Title: Interpretability Beyond Feature Attribution: Quantitative Testing with\n  Concept Activation Vectors (TCAV) Abstract: The interpretation of deep learning models is a challenge due to their size,\ncomplexity, and often opaque internal state. In addition, many systems, such as\nimage classifiers, operate on low-level features rather than high-level\nconcepts. To address these challenges, we introduce Concept Activation Vectors\n(CAVs), which provide an interpretation of a neural net's internal state in\nterms of human-friendly concepts. The key idea is to view the high-dimensional\ninternal state of a neural net as an aid, not an obstacle. We show how to use\nCAVs as part of a technique, Testing with CAVs (TCAV), that uses directional\nderivatives to quantify the degree to which a user-defined concept is important\nto a classification result--for example, how sensitive a prediction of \"zebra\"\nis to the presence of stripes. Using the domain of image classification as a\ntesting ground, we describe how CAVs may be used to explore hypotheses and\ngenerate insights for a standard image classification network as well as a\nmedical application. \n\n"}
{"id": "1712.00111", "contents": "Title: Blind Gain and Phase Calibration via Sparse Spectral Methods Abstract: Blind gain and phase calibration (BGPC) is a bilinear inverse problem\ninvolving the determination of unknown gains and phases of the sensing system,\nand the unknown signal, jointly. BGPC arises in numerous applications, e.g.,\nblind albedo estimation in inverse rendering, synthetic aperture radar\nautofocus, and sensor array auto-calibration. In some cases, sparse structure\nin the unknown signal alleviates the ill-posedness of BGPC. Recently there has\nbeen renewed interest in solutions to BGPC with careful analysis of error\nbounds. In this paper, we formulate BGPC as an eigenvalue/eigenvector problem,\nand propose to solve it via power iteration, or in the sparsity or joint\nsparsity case, via truncated power iteration. Under certain assumptions, the\nunknown gains, phases, and the unknown signal can be recovered simultaneously.\nNumerical experiments show that power iteration algorithms work not only in the\nregime predicted by our main results, but also in regimes where theoretical\nanalysis is limited. We also show that our power iteration algorithms for BGPC\ncompare favorably with competing algorithms in adversarial conditions, e.g.,\nwith noisy measurement or with a bad initial estimate. \n\n"}
{"id": "1712.00374", "contents": "Title: Precision Learning: Towards Use of Known Operators in Neural Networks Abstract: In this paper, we consider the use of prior knowledge within neural networks.\nIn particular, we investigate the effect of a known transform within the\nmapping from input data space to the output domain. We demonstrate that use of\nknown transforms is able to change maximal error bounds.\n  In order to explore the effect further, we consider the problem of X-ray\nmaterial decomposition as an example to incorporate additional prior knowledge.\nWe demonstrate that inclusion of a non-linear function known from the physical\nproperties of the system is able to reduce prediction errors therewith\nimproving prediction quality from SSIM values of 0.54 to 0.88.\n  This approach is applicable to a wide set of applications in physics and\nsignal processing that provide prior knowledge on such transforms. Also maximal\nerror estimation and network understanding could be facilitated within the\ncontext of precision learning. \n\n"}
{"id": "1712.00559", "contents": "Title: Progressive Neural Architecture Search Abstract: We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet. \n\n"}
{"id": "1712.00559", "contents": "Title: Progressive Neural Architecture Search Abstract: We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet. \n\n"}
{"id": "1712.00733", "contents": "Title: Incorporating External Knowledge to Answer Open-Domain Visual Questions\n  with Dynamic Memory Networks Abstract: Visual Question Answering (VQA) has attracted much attention since it offers\ninsight into the relationships between the multi-modal analysis of images and\nnatural language. Most of the current algorithms are incapable of answering\nopen-domain questions that require to perform reasoning beyond the image\ncontents. To address this issue, we propose a novel framework which endows the\nmodel capabilities in answering more complex questions by leveraging massive\nexternal knowledge with dynamic memory networks. Specifically, the questions\nalong with the corresponding images trigger a process to retrieve the relevant\ninformation in external knowledge bases, which are embedded into a continuous\nvector space by preserving the entity-relation structures. Afterwards, we\nemploy dynamic memory networks to attend to the large body of facts in the\nknowledge graph and images, and then perform reasoning over these facts to\ngenerate corresponding answers. Extensive experiments demonstrate that our\nmodel not only achieves the state-of-the-art performance in the visual question\nanswering task, but can also answer open-domain questions effectively by\nleveraging the external knowledge. \n\n"}
{"id": "1712.01056", "contents": "Title: CNN based Learning using Reflection and Retinex Models for Intrinsic\n  Image Decomposition Abstract: Most of the traditional work on intrinsic image decomposition rely on\nderiving priors about scene characteristics. On the other hand, recent research\nuse deep learning models as in-and-out black box and do not consider the\nwell-established, traditional image formation process as the basis of their\nintrinsic learning process. As a consequence, although current deep learning\napproaches show superior performance when considering quantitative benchmark\nresults, traditional approaches are still dominant in achieving high\nqualitative results. In this paper, the aim is to exploit the best of the two\nworlds. A method is proposed that (1) is empowered by deep learning\ncapabilities, (2) considers a physics-based reflection model to steer the\nlearning process, and (3) exploits the traditional approach to obtain intrinsic\nimages by exploiting reflectance and shading gradient information. The proposed\nmodel is fast to compute and allows for the integration of all intrinsic\ncomponents. To train the new model, an object centered large-scale datasets\nwith intrinsic ground-truth images are created. The evaluation results\ndemonstrate that the new model outperforms existing methods. Visual inspection\nshows that the image formation loss function augments color reproduction and\nthe use of gradient information produces sharper edges. Datasets, models and\nhigher resolution images are available at https://ivi.fnwi.uva.nl/cv/retinet. \n\n"}
{"id": "1712.01358", "contents": "Title: Long-Term Visual Object Tracking Benchmark Abstract: We propose a new long video dataset (called Track Long and Prosper - TLP) and\nbenchmark for single object tracking. The dataset consists of 50 HD videos from\nreal world scenarios, encompassing a duration of over 400 minutes (676K\nframes), making it more than 20 folds larger in average duration per sequence\nand more than 8 folds larger in terms of total covered duration, as compared to\nexisting generic datasets for visual tracking. The proposed dataset paves a way\nto suitably assess long term tracking performance and train better deep\nlearning architectures (avoiding/reducing augmentation, which may not reflect\nreal world behaviour). We benchmark the dataset on 17 state of the art trackers\nand rank them according to tracking accuracy and run time speeds. We further\npresent thorough qualitative and quantitative evaluation highlighting the\nimportance of long term aspect of tracking. Our most interesting observations\nare (a) existing short sequence benchmarks fail to bring out the inherent\ndifferences in tracking algorithms which widen up while tracking on long\nsequences and (b) the accuracy of trackers abruptly drops on challenging long\nsequences, suggesting the potential need of research efforts in the direction\nof long-term tracking. \n\n"}
{"id": "1712.02310", "contents": "Title: From Lifestyle Vlogs to Everyday Interactions Abstract: A major stumbling block to progress in understanding basic human\ninteractions, such as getting out of bed or opening a refrigerator, is lack of\ngood training data. Most past efforts have gathered this data explicitly:\nstarting with a laundry list of action labels, and then querying search engines\nfor videos tagged with each label. In this work, we do the reverse and search\nimplicitly: we start with a large collection of interaction-rich video data and\nthen annotate and analyze it. We use Internet Lifestyle Vlogs as the source of\nsurprisingly large and diverse interaction data. We show that by collecting the\ndata first, we are able to achieve greater scale and far greater diversity in\nterms of actions and actors. Additionally, our data exposes biases built into\ncommon explicitly gathered data. We make sense of our data by analyzing the\ncentral component of interaction -- hands. We benchmark two tasks: identifying\nsemantic object contact at the video level and non-semantic contact state at\nthe frame level. We additionally demonstrate future prediction of hands. \n\n"}
{"id": "1712.02328", "contents": "Title: Generative Adversarial Perturbations Abstract: In this paper, we propose novel generative models for creating adversarial\nexamples, slightly perturbed images resembling natural images but maliciously\ncrafted to fool pre-trained models. We present trainable deep neural networks\nfor transforming images to adversarial perturbations. Our proposed models can\nproduce image-agnostic and image-dependent perturbations for both targeted and\nnon-targeted attacks. We also demonstrate that similar architectures can\nachieve impressive results in fooling classification and semantic segmentation\nmodels, obviating the need for hand-crafting attack methods for each task.\nUsing extensive experiments on challenging high-resolution datasets such as\nImageNet and Cityscapes, we show that our perturbations achieve high fooling\nrates with small perturbation norms. Moreover, our attacks are considerably\nfaster than current iterative methods at inference time. \n\n"}
{"id": "1712.02517", "contents": "Title: Broadcasting Convolutional Network for Visual Relational Reasoning Abstract: In this paper, we propose the Broadcasting Convolutional Network (BCN) that\nextracts key object features from the global field of an entire input image and\nrecognizes their relationship with local features. BCN is a simple network\nmodule that collects effective spatial features, embeds location information\nand broadcasts them to the entire feature maps. We further introduce the\nMulti-Relational Network (multiRN) that improves the existing Relation Network\n(RN) by utilizing the BCN module. In pixel-based relation reasoning problems,\nwith the help of BCN, multiRN extends the concept of `pairwise relations' in\nconventional RNs to `multiwise relations' by relating each object with multiple\nobjects at once. This yields in O(n) complexity for n objects, which is a vast\ncomputational gain from RNs that take O(n^2). Through experiments, multiRN has\nachieved a state-of-the-art performance on CLEVR dataset, which proves the\nusability of BCN on relation reasoning problems. \n\n"}
{"id": "1712.02765", "contents": "Title: Super-FAN: Integrated facial landmark localization and super-resolution\n  of real-world low resolution faces in arbitrary poses with GANs Abstract: This paper addresses 2 challenging tasks: improving the quality of low\nresolution facial images and accurately locating the facial landmarks on such\npoor resolution images. To this end, we make the following 5 contributions: (a)\nwe propose Super-FAN: the very first end-to-end system that addresses both\ntasks simultaneously, i.e. both improves face resolution and detects the facial\nlandmarks. The novelty or Super-FAN lies in incorporating structural\ninformation in a GAN-based super-resolution algorithm via integrating a\nsub-network for face alignment through heatmap regression and optimizing a\nnovel heatmap loss. (b) We illustrate the benefit of training the two networks\njointly by reporting good results not only on frontal images (as in prior work)\nbut on the whole spectrum of facial poses, and not only on synthetic low\nresolution images (as in prior work) but also on real-world images. (c) We\nimprove upon the state-of-the-art in face super-resolution by proposing a new\nresidual-based architecture. (d) Quantitatively, we show large improvement over\nthe state-of-the-art for both face super-resolution and alignment. (e)\nQualitatively, we show for the first time good results on real-world low\nresolution images. \n\n"}
{"id": "1712.03141", "contents": "Title: Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning Abstract: Learning-based pattern classifiers, including deep networks, have shown\nimpressive performance in several application domains, ranging from computer\nvision to cybersecurity. However, it has also been shown that adversarial input\nperturbations carefully crafted either at training or at test time can easily\nsubvert their predictions. The vulnerability of machine learning to such wild\npatterns (also referred to as adversarial examples), along with the design of\nsuitable countermeasures, have been investigated in the research field of\nadversarial machine learning. In this work, we provide a thorough overview of\nthe evolution of this research area over the last ten years and beyond,\nstarting from pioneering, earlier work on the security of non-deep learning\nalgorithms up to more recent work aimed to understand the security properties\nof deep learning algorithms, in the context of computer vision and\ncybersecurity tasks. We report interesting connections between these\napparently-different lines of work, highlighting common misconceptions related\nto the security evaluation of machine-learning algorithms. We review the main\nthreat models and attacks defined to this end, and discuss the main limitations\nof current work, along with the corresponding future challenges towards the\ndesign of more secure learning algorithms. \n\n"}
{"id": "1712.03866", "contents": "Title: Using a single RGB frame for real time 3D hand pose estimation in the\n  wild Abstract: We present a method for the real-time estimation of the full 3D pose of one\nor more human hands using a single commodity RGB camera. Recent work in the\narea has displayed impressive progress using RGBD input. However, since the\nintroduction of RGBD sensors, there has been little progress for the case of\nmonocular color input. We capitalize on the latest advancements of deep\nlearning, combining them with the power of generative hand pose estimation\ntechniques to achieve real-time monocular 3D hand pose estimation in\nunrestricted scenarios. More specifically, given an RGB image and the relevant\ncamera calibration information, we employ a state-of-the-art detector to\nlocalize hands. Given a crop of a hand in the image, we run the pretrained\nnetwork of OpenPose for hands to estimate the 2D location of hand joints.\nFinally, non-linear least-squares minimization fits a 3D model of the hand to\nthe estimated 2D joint positions, recovering the 3D hand pose. Extensive\nexperimental results provide comparison to the state of the art as well as\nqualitative assessment of the method in the wild. \n\n"}
{"id": "1712.04489", "contents": "Title: Fingerprint Spoof Buster Abstract: The primary purpose of a fingerprint recognition system is to ensure a\nreliable and accurate user authentication, but the security of the recognition\nsystem itself can be jeopardized by spoof attacks. This study addresses the\nproblem of developing accurate, generalizable, and efficient algorithms for\ndetecting fingerprint spoof attacks. Specifically, we propose a deep\nconvolutional neural network based approach utilizing local patches centered\nand aligned using fingerprint minutiae. Experimental results on three\npublic-domain LivDet datasets (2011, 2013, and 2015) show that the proposed\napproach provides state-of-the-art accuracies in fingerprint spoof detection\nfor intra-sensor, cross-material, cross-sensor, as well as cross-dataset\ntesting scenarios. For example, in LivDet 2015, the proposed approach achieves\n99.03% average accuracy over all sensors compared to 95.51% achieved by the\nLivDet 2015 competition winners. Additionally, two new fingerprint presentation\nattack datasets containing more than 20,000 images, using two different\nfingerprint readers, and over 12 different spoof fabrication materials are\ncollected. We also present a graphical user interface, called Fingerprint Spoof\nBuster, that allows the operator to visually examine the local regions of the\nfingerprint highlighted as live or spoof, instead of relying on only a single\nscore as output by the traditional approaches. \n\n"}
{"id": "1712.05652", "contents": "Title: Pre-training Attention Mechanisms Abstract: Recurrent neural networks with differentiable attention mechanisms have had\nsuccess in generative and classification tasks. We show that the classification\nperformance of such models can be enhanced by guiding a randomly initialized\nmodel to attend to salient regions of the input in early training iterations.\nWe further show that, if explicit heuristics for guidance are unavailable, a\nmodel that is pretrained on an unsupervised reconstruction task can discover\ngood attention policies without supervision. We demonstrate that increased\nefficiency of the attention mechanism itself contributes to these performance\nimprovements. Based on these insights, we introduce bootstrapped glimpse\nmimicking, a simple, theoretically task-general method of more effectively\ntraining attention models. Our work draws inspiration from and parallels\nresults on human learning of attention. \n\n"}
{"id": "1712.06651", "contents": "Title: Objects that Sound Abstract: In this paper our objectives are, first, networks that can embed audio and\nvisual inputs into a common space that is suitable for cross-modal retrieval;\nand second, a network that can localize the object that sounds in an image,\ngiven the audio signal. We achieve both these objectives by training from\nunlabelled video using only audio-visual correspondence (AVC) as the objective\nfunction. This is a form of cross-modal self-supervision from video.\n  To this end, we design new network architectures that can be trained for\ncross-modal retrieval and localizing the sound source in an image, by using the\nAVC task. We make the following contributions: (i) show that audio and visual\nembeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and\nbetween-mode retrieval; (ii) explore various architectures for the AVC task,\nincluding those for the visual stream that ingest a single image, or multiple\nimages, or a single image and multi-frame optical flow; (iii) show that the\nsemantic object that sounds within an image can be localized (using only the\nsound, no motion or flow information); and (iv) give a cautionary tale on how\nto avoid undesirable shortcuts in the data preparation. \n\n"}
{"id": "1712.06651", "contents": "Title: Objects that Sound Abstract: In this paper our objectives are, first, networks that can embed audio and\nvisual inputs into a common space that is suitable for cross-modal retrieval;\nand second, a network that can localize the object that sounds in an image,\ngiven the audio signal. We achieve both these objectives by training from\nunlabelled video using only audio-visual correspondence (AVC) as the objective\nfunction. This is a form of cross-modal self-supervision from video.\n  To this end, we design new network architectures that can be trained for\ncross-modal retrieval and localizing the sound source in an image, by using the\nAVC task. We make the following contributions: (i) show that audio and visual\nembeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and\nbetween-mode retrieval; (ii) explore various architectures for the AVC task,\nincluding those for the visual stream that ingest a single image, or multiple\nimages, or a single image and multi-frame optical flow; (iii) show that the\nsemantic object that sounds within an image can be localized (using only the\nsound, no motion or flow information); and (iv) give a cautionary tale on how\nto avoid undesirable shortcuts in the data preparation. \n\n"}
{"id": "1712.06760", "contents": "Title: Mining Point Cloud Local Structures by Kernel Correlation and Graph\n  Pooling Abstract: Unlike on images, semantic learning on 3D point clouds using a deep network\nis challenging due to the naturally unordered data structure. Among existing\nworks, PointNet has achieved promising results by directly learning on point\nsets. However, it does not take full advantage of a point's local neighborhood\nthat contains fine-grained structural information which turns out to be helpful\ntowards better semantic learning. In this regard, we present two new operations\nto improve PointNet with a more efficient exploitation of local structures. The\nfirst one focuses on local 3D geometric structures. In analogy to a convolution\nkernel for images, we define a point-set kernel as a set of learnable 3D points\nthat jointly respond to a set of neighboring data points according to their\ngeometric affinities measured by kernel correlation, adapted from a similar\ntechnique for point cloud registration. The second one exploits local\nhigh-dimensional feature structures by recursive feature aggregation on a\nnearest-neighbor-graph computed from 3D positions. Experiments show that our\nnetwork can efficiently capture local information and robustly achieve better\nperformances on major datasets. Our code is available at\nhttp://www.merl.com/research/license#KCNet \n\n"}
{"id": "1712.07420", "contents": "Title: Finding Competitive Network Architectures Within a Day Using UCT Abstract: The design of neural network architectures for a new data set is a laborious\ntask which requires human deep learning expertise. In order to make deep\nlearning available for a broader audience, automated methods for finding a\nneural network architecture are vital. Recently proposed methods can already\nachieve human expert level performances. However, these methods have run times\nof months or even years of GPU computing time, ignoring hardware constraints as\nfaced by many researchers and companies. We propose the use of Monte Carlo\nplanning in combination with two different UCT (upper confidence bound applied\nto trees) derivations to search for network architectures. We adapt the UCT\nalgorithm to the needs of network architecture search by proposing two ways of\nsharing information between different branches of the search tree. In an\nempirical study we are able to demonstrate that this method is able to find\ncompetitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day.\nExtending the search time to five GPU days, we are able to outperform human\narchitectures and our competitors which consider the same types of layers. \n\n"}
{"id": "1712.07465", "contents": "Title: Recurrent Attentional Reinforcement Learning for Multi-label Image\n  Recognition Abstract: Recognizing multiple labels of images is a fundamental but challenging task\nin computer vision, and remarkable progress has been attained by localizing\nsemantic-aware image regions and predicting their labels with deep\nconvolutional neural networks. The step of hypothesis regions (region\nproposals) localization in these existing multi-label image recognition\npipelines, however, usually takes redundant computation cost, e.g., generating\nhundreds of meaningless proposals with non-discriminative information and\nextracting their features, and the spatial contextual dependency modeling among\nthe localized regions are often ignored or over-simplified. To resolve these\nissues, this paper proposes a recurrent attention reinforcement learning\nframework to iteratively discover a sequence of attentional and informative\nregions that are related to different semantic objects and further predict\nlabel scores conditioned on these regions. Besides, our method explicitly\nmodels long-term dependencies among these attentional regions that help to\ncapture semantic label co-occurrence and thus facilitate multi-label\nrecognition. Extensive experiments and comparisons on two large-scale\nbenchmarks (i.e., PASCAL VOC and MS-COCO) show that our model achieves superior\nperformance over existing state-of-the-art methods in both performance and\nefficiency as well as explicitly identifying image-level semantic labels to\nspecific object regions. \n\n"}
{"id": "1712.07778", "contents": "Title: Context-Aware Semantic Inpainting Abstract: Recently image inpainting has witnessed rapid progress due to generative\nadversarial networks (GAN) that are able to synthesize realistic contents.\nHowever, most existing GAN-based methods for semantic inpainting apply an\nauto-encoder architecture with a fully connected layer, which cannot accurately\nmaintain spatial information. In addition, the discriminator in existing GANs\nstruggle to understand high-level semantics within the image context and yield\nsemantically consistent content. Existing evaluation criteria are biased\ntowards blurry results and cannot well characterize edge preservation and\nvisual authenticity in the inpainting results. In this paper, we propose an\nimproved generative adversarial network to overcome the aforementioned\nlimitations. Our proposed GAN-based framework consists of a fully convolutional\ndesign for the generator which helps to better preserve spatial structures and\na joint loss function with a revised perceptual loss to capture high-level\nsemantics in the context. Furthermore, we also introduce two novel measures to\nbetter assess the quality of image inpainting results. Experimental results\ndemonstrate that our method outperforms the state of the art under a wide range\nof criteria. \n\n"}
{"id": "1712.08062", "contents": "Title: Note on Attacking Object Detectors with Adversarial Stickers Abstract: Deep learning has proven to be a powerful tool for computer vision and has\nseen widespread adoption for numerous tasks. However, deep learning algorithms\nare known to be vulnerable to adversarial examples. These adversarial inputs\nare created such that, when provided to a deep learning algorithm, they are\nvery likely to be mislabeled. This can be problematic when deep learning is\nused to assist in safety critical decisions. Recent research has shown that\nclassifiers can be attacked by physical adversarial examples under various\nphysical conditions. Given the fact that state-of-the-art objection detection\nalgorithms are harder to be fooled by the same set of adversarial examples,\nhere we show that these detectors can also be attacked by physical adversarial\nexamples. In this note, we briefly show both static and dynamic test results.\nWe design an algorithm that produces physical adversarial inputs, which can\nfool the YOLO object detector and can also attack Faster-RCNN with relatively\nhigh success rate based on transferability. Furthermore, our algorithm can\ncompress the size of the adversarial inputs to stickers that, when attached to\nthe targeted object, result in the detector either mislabeling or not detecting\nthe object a high percentage of the time. This note provides a small set of\nresults. Our upcoming paper will contain a thorough evaluation on other object\ndetectors, and will present the algorithm. \n\n"}
{"id": "1712.08107", "contents": "Title: A Deep Learning Interpretable Classifier for Diabetic Retinopathy\n  Disease Grading Abstract: Deep neural network models have been proven to be very successful in image\nclassification tasks, also for medical diagnosis, but their main concern is its\nlack of interpretability. They use to work as intuition machines with high\nstatistical confidence but unable to give interpretable explanations about the\nreported results. The vast amount of parameters of these models make difficult\nto infer a rationale interpretation from them. In this paper we present a\ndiabetic retinopathy interpretable classifier able to classify retine images\ninto the different levels of disease severity and of explaining its results by\nassigning a score for every point in the hidden and input space, evaluating its\ncontribution to the final classification in a linear way. The generated visual\nmaps can be interpreted by an expert in order to compare its own knowledge with\nthe interpretation given by the model. \n\n"}
{"id": "1712.08273", "contents": "Title: Recurrent Pixel Embedding for Instance Grouping Abstract: We introduce a differentiable, end-to-end trainable framework for solving\npixel-level grouping problems such as instance segmentation consisting of two\nnovel components. First, we regress pixels into a hyper-spherical embedding\nspace so that pixels from the same group have high cosine similarity while\nthose from different groups have similarity below a specified margin. We\nanalyze the choice of embedding dimension and margin, relating them to\ntheoretical results on the problem of distributing points uniformly on the\nsphere. Second, to group instances, we utilize a variant of mean-shift\nclustering, implemented as a recurrent neural network parameterized by kernel\nbandwidth. This recurrent grouping module is differentiable, enjoys convergent\ndynamics and probabilistic interpretability. Backpropagating the group-weighted\nloss through this module allows learning to focus on only correcting embedding\nerrors that won't be resolved during subsequent clustering. Our framework,\nwhile conceptually simple and theoretically abundant, is also practically\neffective and computationally efficient. We demonstrate substantial\nimprovements over state-of-the-art instance segmentation for object proposal\ngeneration, as well as demonstrating the benefits of grouping loss for\nclassification tasks such as boundary detection and semantic segmentation. \n\n"}
{"id": "1712.08315", "contents": "Title: Deep Hashing with Category Mask for Fast Video Retrieval Abstract: This paper proposes an end-to-end deep hashing framework with category mask\nfor fast video retrieval. We train our network in a supervised way by fully\nexploiting inter-class diversity and intra-class identity. Classification loss\nis optimized to maximize inter-class diversity, while intra-pair is introduced\nto learn representative intra-class identity. We investigate the binary bits\ndistribution related to categories and find out that the effectiveness of\nbinary bits is highly correlated with data categories, and some bits may\ndegrade classification performance of some categories. We then design hash code\ngeneration scheme with category mask to filter out bits with negative\ncontribution. Experimental results demonstrate the proposed method outperforms\nseveral state-of-the-arts under various evaluation metrics on public datasets. \n\n"}
{"id": "1712.09532", "contents": "Title: Consensus-based Sequence Training for Video Captioning Abstract: Captioning models are typically trained using the cross-entropy loss.\nHowever, their performance is evaluated on other metrics designed to better\ncorrelate with human assessments. Recently, it has been shown that\nreinforcement learning (RL) can directly optimize these metrics in tasks such\nas captioning. However, this is computationally costly and requires specifying\na baseline reward at each step to make training converge. We propose a fast\napproach to optimize one's objective of interest through the REINFORCE\nalgorithm. First we show that, by replacing model samples with ground-truth\nsentences, RL training can be seen as a form of weighted cross-entropy loss,\ngiving a fast, RL-based pre-training algorithm. Second, we propose to use the\nconsensus among ground-truth captions of the same video as the baseline reward.\nThis can be computed very efficiently. We call the complete proposal\nConsensus-based Sequence Training (CST). Applied to the MSRVTT video captioning\nbenchmark, our proposals train significantly faster than comparable methods and\nestablish a new state-of-the-art on the task, improving the CIDEr score from\n47.3 to 54.2. \n\n"}
{"id": "1712.09709", "contents": "Title: Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro\n  Gesture Abstract: In the research of the impact of gestures using by a lecturer, one\nchallenging task is to infer the attention of a group of audiences. Two\nimportant measurements that can help infer the level of attention are eye\nmovement data and Electroencephalography (EEG) data. Under the fundamental\nassumption that a group of people would look at the same place if they all pay\nattention at the same time, we apply a method, \"Time Warp Edit Distance\", to\ncalculate the similarity of their eye movement trajectories. Moreover, we also\ncluster eye movement pattern of audiences based on these pair-wised similarity\nmetrics. Besides, since we don't have a direct metric for the \"attention\"\nground truth, a visual assessment would be beneficial to evaluate the\ngesture-attention relationship. Thus we also implement a visualization tool. \n\n"}
{"id": "1801.00868", "contents": "Title: Panoptic Segmentation Abstract: We propose and study a task we name panoptic segmentation (PS). Panoptic\nsegmentation unifies the typically distinct tasks of semantic segmentation\n(assign a class label to each pixel) and instance segmentation (detect and\nsegment each object instance). The proposed task requires generating a coherent\nscene segmentation that is rich and complete, an important step toward\nreal-world vision systems. While early work in computer vision addressed\nrelated image/scene parsing tasks, these are not currently popular, possibly\ndue to lack of appropriate metrics or associated recognition challenges. To\naddress this, we propose a novel panoptic quality (PQ) metric that captures\nperformance for all classes (stuff and things) in an interpretable and unified\nmanner. Using the proposed metric, we perform a rigorous study of both human\nand machine performance for PS on three existing datasets, revealing\ninteresting insights about the task. The aim of our work is to revive the\ninterest of the community in a more unified view of image segmentation. \n\n"}
{"id": "1801.01198", "contents": "Title: Fingerprint Distortion Rectification using Deep Convolutional Neural\n  Networks Abstract: Elastic distortion of fingerprints has a negative effect on the performance\nof fingerprint recognition systems. This negative effect brings inconvenience\nto users in authentication applications. However, in the negative recognition\nscenario where users may intentionally distort their fingerprints, this can be\na serious problem since distortion will prevent recognition system from\nidentifying malicious users. Current methods aimed at addressing this problem\nstill have limitations. They are often not accurate because they estimate\ndistortion parameters based on the ridge frequency map and orientation map of\ninput samples, which are not reliable due to distortion. Secondly, they are not\nefficient and requiring significant computation time to rectify samples. In\nthis paper, we develop a rectification model based on a Deep Convolutional\nNeural Network (DCNN) to accurately estimate distortion parameters from the\ninput image. Using a comprehensive database of synthetic distorted samples, the\nDCNN learns to accurately estimate distortion bases ten times faster than the\ndictionary search methods used in the previous approaches. Evaluating the\nproposed method on public databases of distorted samples shows that it can\nsignificantly improve the matching performance of distorted samples. \n\n"}
{"id": "1801.02031", "contents": "Title: ReMotENet: Efficient Relevant Motion Event Detection for Large-scale\n  Home Surveillance Videos Abstract: This paper addresses the problem of detecting relevant motion caused by\nobjects of interest (e.g., person and vehicles) in large scale home\nsurveillance videos. The traditional method usually consists of two separate\nsteps, i.e., detecting moving objects with background subtraction running on\nthe camera, and filtering out nuisance motion events (e.g., trees, cloud,\nshadow, rain/snow, flag) with deep learning based object detection and tracking\nrunning on cloud. The method is extremely slow and therefore not cost\neffective, and does not fully leverage the spatial-temporal redundancies with a\npre-trained off-the-shelf object detector. To dramatically speedup relevant\nmotion event detection and improve its performance, we propose a novel network\nfor relevant motion event detection, ReMotENet, which is a unified, end-to-end\ndata-driven method using spatial-temporal attention-based 3D ConvNets to\njointly model the appearance and motion of objects-of-interest in a video.\nReMotENet parses an entire video clip in one forward pass of a neural network\nto achieve significant speedup. Meanwhile, it exploits the properties of home\nsurveillance videos, e.g., relevant motion is sparse both spatially and\ntemporally, and enhances 3D ConvNets with a spatial-temporal attention model\nand reference-frame subtraction to encourage the network to focus on the\nrelevant moving objects. Experiments demonstrate that our method can achieve\ncomparable or event better performance than the object detection based method\nbut with three to four orders of magnitude speedup (up to 20k times) on GPU\ndevices. Our network is efficient, compact and light-weight. It can detect\nrelevant motion on a 15s surveillance video clip within 4-8 milliseconds on a\nGPU and a fraction of second (0.17-0.39) on a CPU with a model size of less\nthan 1MB. \n\n"}
{"id": "1801.03049", "contents": "Title: Meta-Tracker: Fast and Robust Online Adaptation for Visual Object\n  Trackers Abstract: This paper improves state-of-the-art visual object trackers that use online\nadaptation. Our core contribution is an offline meta-learning-based method to\nadjust the initial deep networks used in online adaptation-based tracking. The\nmeta learning is driven by the goal of deep networks that can quickly be\nadapted to robustly model a particular target in future frames. Ideally the\nresulting models focus on features that are useful for future frames, and avoid\noverfitting to background clutter, small parts of the target, or noise. By\nenforcing a small number of update iterations during meta-learning, the\nresulting networks train significantly faster. We demonstrate this approach on\ntop of the high performance tracking approaches: tracking-by-detection based\nMDNet and the correlation based CREST. Experimental results on standard\nbenchmarks, OTB2015 and VOT2016, show that our meta-learned versions of both\ntrackers improve speed, accuracy, and robustness. \n\n"}
{"id": "1801.04331", "contents": "Title: Prototypicality effects in global semantic description of objects Abstract: In this paper, we introduce a novel approach for semantic description of\nobject features based on the prototypicality effects of the Prototype Theory.\nOur prototype-based description model encodes and stores the semantic meaning\nof an object, while describing its features using the semantic prototype\ncomputed by CNN-classifications models. Our method uses semantic prototypes to\ncreate discriminative descriptor signatures that describe an object\nhighlighting its most distinctive features within the category. Our experiments\nshow that: i) our descriptor preserves the semantic information used by the\nCNN-models in classification tasks; ii) our distance metric can be used as the\nobject's typicality score; iii) our descriptor signatures are semantically\ninterpretable and enables the simulation of the prototypical organization of\nobjects within a category. \n\n"}
{"id": "1801.04662", "contents": "Title: Enlarging Context with Low Cost: Efficient Arithmetic Coding with\n  Trimmed Convolution Abstract: Arithmetic coding is an essential class of coding techniques. One key issue\nof arithmetic encoding method is to predict the probability of the current\ncoding symbol from its context, i.e., the preceding encoded symbols, which\nusually can be executed by building a look-up table (LUT). However, the\ncomplexity of LUT increases exponentially with the length of context. Thus,\nsuch solutions are limited to modeling large context, which inevitably\nrestricts the compression performance. Several recent deep neural network-based\nsolutions have been developed to account for large context, but are still\ncostly in computation. The inefficiency of the existing methods are mainly\nattributed to that probability prediction is performed independently for the\nneighboring symbols, which actually can be efficiently conducted by shared\ncomputation. To this end, we propose a trimmed convolutional network for\narithmetic encoding (TCAE) to model large context while maintaining\ncomputational efficiency. As for trimmed convolution, the convolutional kernels\nare specially trimmed to respect the compression order and context dependency\nof the input symbols. Benefited from trimmed convolution, the probability\nprediction of all symbols can be efficiently performed in one single forward\npass via a fully convolutional network. Furthermore, to speed up the decoding\nprocess, a slope TCAE model is presented to divide the codes from a 3D code map\ninto several blocks and remove the dependency between the codes inner one block\nfor parallel decoding, which can 60x speed up the decoding process. Experiments\nshow that our TCAE and slope TCAE attain better compression ratio in lossless\ngray image compression, and can be adopted in CNN-based lossy image compression\nto achieve state-of-the-art rate-distortion performance with real-time encoding\nspeed. \n\n"}
{"id": "1801.04815", "contents": "Title: Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly Abstract: Learning similarity functions between image pairs with deep neural networks\nyields highly correlated activations of embeddings. In this work, we show how\nto improve the robustness of such embeddings by exploiting the independence\nwithin ensembles. To this end, we divide the last embedding layer of a deep\nnetwork into an embedding ensemble and formulate training this ensemble as an\nonline gradient boosting problem. Each learner receives a reweighted training\nsample from the previous learners. Further, we propose two loss functions which\nincrease the diversity in our ensemble. These loss functions can be applied\neither for weight initialization or during training. Together, our\ncontributions leverage large embedding sizes more effectively by significantly\nreducing correlation of the embedding and consequently increase retrieval\naccuracy of the embedding. Our method works with any differentiable loss\nfunction and does not introduce any additional parameters during test time. We\nevaluate our metric learning method on image retrieval tasks and show that it\nimproves over state-of-the-art methods on the CUB 200-2011, Cars-196, Stanford\nOnline Products, In-Shop Clothes Retrieval and VehicleID datasets. \n\n"}
{"id": "1801.05124", "contents": "Title: Localization-Aware Active Learning for Object Detection Abstract: Active learning - a class of algorithms that iteratively searches for the\nmost informative samples to include in a training dataset - has been shown to\nbe effective at annotating data for image classification. However, the use of\nactive learning for object detection is still largely unexplored as determining\ninformativeness of an object-location hypothesis is more difficult. In this\npaper, we address this issue and present two metrics for measuring the\ninformativeness of an object hypothesis, which allow us to leverage active\nlearning to reduce the amount of annotated data needed to achieve a target\nobject detection performance. Our first metric measures 'localization\ntightness' of an object hypothesis, which is based on the overlapping ratio\nbetween the region proposal and the final prediction. Our second metric\nmeasures 'localization stability' of an object hypothesis, which is based on\nthe variation of predicted object locations when input images are corrupted by\nnoise. Our experimental results show that by augmenting a conventional\nactive-learning algorithm designed for classification with the proposed\nmetrics, the amount of labeled training data required can be reduced up to 25%.\nMoreover, on PASCAL 2007 and 2012 datasets our localization-stability method\nhas an average relative improvement of 96.5% and 81.9% over the baseline method\nusing classification only. \n\n"}
{"id": "1801.05299", "contents": "Title: Autonomous Driving in Reality with Reinforcement Learning and Image\n  Translation Abstract: Supervised learning is widely used in training autonomous driving vehicle.\nHowever, it is trained with large amount of supervised labeled data.\nReinforcement learning can be trained without abundant labeled data, but we\ncannot train it in reality because it would involve many unpredictable\naccidents. Nevertheless, training an agent with good performance in virtual\nenvironment is relatively much easier. Because of the huge difference between\nvirtual and real, how to fill the gap between virtual and real is challenging.\nIn this paper, we proposed a novel framework of reinforcement learning with\nimage semantic segmentation network to make the whole model adaptable to\nreality. The agent is trained in TORCS, a car racing simulator. \n\n"}
{"id": "1801.05401", "contents": "Title: Low-Shot Learning from Imaginary Data Abstract: Humans can quickly learn new visual concepts, perhaps because they can easily\nvisualize or imagine what novel objects look like from different views.\nIncorporating this ability to hallucinate novel instances of new concepts might\nhelp machine vision systems perform better low-shot learning, i.e., learning\nconcepts from few examples. We present a novel approach to low-shot learning\nthat uses this idea. Our approach builds on recent progress in meta-learning\n(\"learning to learn\") by combining a meta-learner with a \"hallucinator\" that\nproduces additional training examples, and optimizing both models jointly. Our\nhallucinator can be incorporated into a variety of meta-learners and provides\nsignificant gains: up to a 6 point boost in classification accuracy when only a\nsingle training example is available, yielding state-of-the-art performance on\nthe challenging ImageNet low-shot classification benchmark. \n\n"}
{"id": "1801.05574", "contents": "Title: Brenier approach for optimal transportation between a quasi-discrete\n  measure and a discrete measure Abstract: Correctly estimating the discrepancy between two data distributions has\nalways been an important task in Machine Learning. Recently, Cuturi proposed\nthe Sinkhorn distance which makes use of an approximate Optimal Transport cost\nbetween two distributions as a distance to describe distribution discrepancy.\nAlthough it has been successfully adopted in various machine learning\napplications (e.g. in Natural Language Processing and Computer Vision) since\nthen, the Sinkhorn distance also suffers from two unnegligible limitations. The\nfirst one is that the Sinkhorn distance only gives an approximation of the real\nWasserstein distance, the second one is the `divide by zero' problem which\noften occurs during matrix scaling when setting the entropy regularization\ncoefficient to a small value. In this paper, we introduce a new Brenier\napproach for calculating a more accurate Wasserstein distance between two\ndiscrete distributions, this approach successfully avoids the two limitations\nshown above for Sinkhorn distance and gives an alternative way for estimating\ndistribution discrepancy. \n\n"}
{"id": "1801.06665", "contents": "Title: Visual Data Augmentation through Learning Abstract: The rapid progress in machine learning methods has been empowered by i) huge\ndatasets that have been collected and annotated, ii) improved engineering (e.g.\ndata pre-processing/normalization). The existing datasets typically include\nseveral million samples, which constitutes their extension a colossal task. In\naddition, the state-of-the-art data-driven methods demand a vast amount of\ndata, hence a standard engineering trick employed is artificial data\naugmentation for instance by adding into the data cropped and (affinely)\ntransformed images. However, this approach does not correspond to any change in\nthe natural 3D scene.\n  We propose instead to perform data augmentation through learning realistic\nlocal transformations. We learn a forward and an inverse transformation that\nmaps an image from the high-dimensional space of pixel intensities to a latent\nspace which varies (approximately) linearly with the latent space of a\nrealistically transformed version of the image. Such transformed images can be\nconsidered two successive frames in a video. Next, we utilize these\ntransformations to learn a linear model that modifies the latent spaces and\nthen use the inverse transformation to synthesize a new image. We argue that\nthe this procedure produces powerful invariant representations. We perform both\nqualitative and quantitative experiments that demonstrate our proposed method\ncreates new realistic images. \n\n"}
{"id": "1801.06734", "contents": "Title: End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars\n  with Visual Perception Abstract: Convolutional Neural Networks (CNN) have been successfully applied to\nautonomous driving tasks, many in an end-to-end manner. Previous end-to-end\nsteering control methods take an image or an image sequence as the input and\ndirectly predict the steering angle with CNN. Although single task learning on\nsteering angles has reported good performances, the steering angle alone is not\nsufficient for vehicle control. In this work, we propose a multi-task learning\nframework to predict the steering angle and speed control simultaneously in an\nend-to-end manner. Since it is nontrivial to predict accurate speed values with\nonly visual inputs, we first propose a network to predict discrete speed\ncommands and steering angles with image sequences. Moreover, we propose a\nmulti-modal multi-task network to predict speed values and steering angles by\ntaking previous feedback speeds and visual recordings as inputs. Experiments\nare conducted on the public Udacity dataset and a newly collected SAIC dataset.\nResults show that the proposed model predicts steering angles and speed values\naccurately. Furthermore, we improve the failure data synthesis methods to solve\nthe problem of error accumulation in real road tests. \n\n"}
{"id": "1801.06889", "contents": "Title: Visual Analytics in Deep Learning: An Interrogative Survey for the Next\n  Frontiers Abstract: Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains. \n\n"}
{"id": "1801.07372", "contents": "Title: Numerical Coordinate Regression with Convolutional Neural Networks Abstract: We study deep learning approaches to inferring numerical coordinates for\npoints of interest in an input image. Existing convolutional neural\nnetwork-based solutions to this problem either take a heatmap matching approach\nor regress to coordinates with a fully connected output layer. Neither of these\napproaches is ideal, since the former is not entirely differentiable, and the\nlatter lacks inherent spatial generalization. We propose our differentiable\nspatial to numerical transform (DSNT) to fill this gap. The DSNT layer adds no\ntrainable parameters, is fully differentiable, and exhibits good spatial\ngeneralization. Unlike heatmap matching, DSNT works well with low heatmap\nresolutions, so it can be dropped in as an output layer for a wide range of\nexisting fully convolutional architectures. Consequently, DSNT offers a better\ntrade-off between inference speed and prediction accuracy compared to existing\ntechniques. When used to replace the popular heatmap matching approach used in\nalmost all state-of-the-art methods for pose estimation, DSNT gives better\nprediction accuracy for all model architectures tested. \n\n"}
{"id": "1801.07580", "contents": "Title: Side Information for Face Completion: a Robust PCA Approach Abstract: Robust principal component analysis (RPCA) is a powerful method for learning\nlow-rank feature representation of various visual data. However, for certain\ntypes as well as significant amount of error corruption, it fails to yield\nsatisfactory results; a drawback that can be alleviated by exploiting\ndomain-dependent prior knowledge or information. In this paper, we propose two\nmodels for the RPCA that take into account such side information, even in the\npresence of missing values. We apply this framework to the task of UV\ncompletion which is widely used in pose-invariant face recognition. Moreover,\nwe construct a generative adversarial network (GAN) to extract side information\nas well as subspaces. These subspaces not only assist in the recovery but also\nspeed up the process in case of large-scale data. We quantitatively and\nqualitatively evaluate the proposed approaches through both synthetic data and\nfive real-world datasets to verify their effectiveness. \n\n"}
{"id": "1801.07729", "contents": "Title: The Shape of Art History in the Eyes of the Machine Abstract: How does the machine classify styles in art? And how does it relate to art\nhistorians' methods for analyzing style? Several studies have shown the ability\nof the machine to learn and predict style categories, such as Renaissance,\nBaroque, Impressionism, etc., from images of paintings. This implies that the\nmachine can learn an internal representation encoding discriminative features\nthrough its visual analysis. However, such a representation is not necessarily\ninterpretable. We conducted a comprehensive study of several of the\nstate-of-the-art convolutional neural networks applied to the task of style\nclassification on 77K images of paintings, and analyzed the learned\nrepresentation through correlation analysis with concepts derived from art\nhistory. Surprisingly, the networks could place the works of art in a smooth\ntemporal arrangement mainly based on learning style labels, without any a\npriori knowledge of time of creation, the historical time and context of\nstyles, or relations between styles. The learned representations showed that\nthere are few underlying factors that explain the visual variations of style in\nart. Some of these factors were found to correlate with style patterns\nsuggested by Heinrich W\\\"olfflin (1846-1945). The learned representations also\nconsistently highlighted certain artists as the extreme distinctive\nrepresentative of their styles, which quantitatively confirms art historian\nobservations. \n\n"}
{"id": "1801.08163", "contents": "Title: DVQA: Understanding Data Visualizations via Question Answering Abstract: Bar charts are an effective way to convey numeric information, but today's\nalgorithms cannot parse them. Existing methods fail when faced with even minor\nvariations in appearance. Here, we present DVQA, a dataset that tests many\naspects of bar chart understanding in a question answering framework. Unlike\nvisual question answering (VQA), DVQA requires processing words and answers\nthat are unique to a particular bar chart. State-of-the-art VQA algorithms\nperform poorly on DVQA, and we propose two strong baselines that perform\nconsiderably better. Our work will enable algorithms to automatically extract\nnumeric and semantic information from vast quantities of bar charts found in\nscientific publications, Internet articles, business reports, and many other\nareas. \n\n"}
{"id": "1801.08297", "contents": "Title: NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural\n  Discriminative Dimensionality Reduction Abstract: In this paper, we propose a novel Convolutional Neural Network (CNN)\nstructure for general-purpose multi-task learning (MTL), which enables\nautomatic feature fusing at every layer from different tasks. This is in\ncontrast with the most widely used MTL CNN structures which empirically or\nheuristically share features on some specific layers (e.g., share all the\nfeatures except the last convolutional layer). The proposed layerwise feature\nfusing scheme is formulated by combining existing CNN components in a novel\nway, with clear mathematical interpretability as discriminative dimensionality\nreduction, which is referred to as Neural Discriminative Dimensionality\nReduction (NDDR). Specifically, we first concatenate features with the same\nspatial resolution from different tasks according to their channel dimension.\nThen, we show that the discriminative dimensionality reduction can be fulfilled\nby 1x1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use\nof existing CNN components ensures the end-to-end training and the\nextensibility of the proposed NDDR layer to various state-of-the-art CNN\narchitectures in a \"plug-and-play\" manner. The detailed ablation analysis shows\nthat the proposed NDDR layer is easy to train and also robust to different\nhyperparameters. Experiments on different task sets with various base network\narchitectures demonstrate the promising performance and desirable\ngeneralizability of our proposed method. The code of our paper is available at\nhttps://github.com/ethanygao/NDDR-CNN. \n\n"}
{"id": "1801.08329", "contents": "Title: Using Deep Autoencoders for Facial Expression Recognition Abstract: Feature descriptors involved in image processing are generally manually\nchosen and high dimensional in nature. Selecting the most important features is\na very crucial task for systems like facial expression recognition. This paper\ninvestigates the performance of deep autoencoders for feature selection and\ndimension reduction for facial expression recognition on multiple levels of\nhidden layers. The features extracted from the stacked autoencoder outperformed\nwhen compared to other state-of-the-art feature selection and dimension\nreduction techniques. \n\n"}
{"id": "1801.08747", "contents": "Title: Weakly Supervised Object Detection with Pointwise Mutual Information Abstract: In this work a novel approach for weakly supervised object detection that\nincorporates pointwise mutual information is presented. A fully convolutional\nneural network architecture is applied in which the network learns one filter\nper object class. The resulting feature map indicates the location of objects\nin an image, yielding an intuitive representation of a class activation map.\nWhile traditionally such networks are learned by a softmax or binary logistic\nregression (sigmoid cross-entropy loss), a learning approach based on a cosine\nloss is introduced. A pointwise mutual information layer is incorporated in the\nnetwork in order to project predictions and ground truth presence labels in a\nnon-categorical embedding space. Thus, the cosine loss can be employed in this\nnon-categorical representation. Besides integrating image level annotations, it\nis shown how to integrate point-wise annotations using a Spatial Pyramid\nPooling layer. The approach is evaluated on the VOC2012 dataset for\nclassification, point localization and weakly supervised bounding box\nlocalization. It is shown that the combination of pointwise mutual information\nand a cosine loss eases the learning process and thus improves the accuracy.\nThe integration of coarse point-wise localizations further improves the results\nat minimal annotation costs. \n\n"}
{"id": "1801.08863", "contents": "Title: 3D Scanning: A Comprehensive Survey Abstract: This paper provides an overview of 3D scanning methodologies and technologies\nproposed in the existing scientific and industrial literature. Throughout the\npaper, various types of the related techniques are reviewed, which consist,\nmainly, of close-range, aerial, structure-from-motion and terrestrial\nphotogrammetry, and mobile, terrestrial and airborne laser scanning, as well as\ntime-of-flight, structured-light and phase-comparison methods, along with\ncomparative and combinational studies, the latter being intended to help make a\nclearer distinction on the relevance and reliability of the possible choices.\nMoreover, outlier detection and surface fitting procedures are discussed\nconcisely, which are necessary post-processing stages. \n\n"}
{"id": "1801.09242", "contents": "Title: Joint Voxel and Coordinate Regression for Accurate 3D Facial Landmark\n  Localization Abstract: 3D face shape is more expressive and viewpoint-consistent than its 2D\ncounterpart. However, 3D facial landmark localization in a single image is\nchallenging due to the ambiguous nature of landmarks under 3D perspective.\nExisting approaches typically adopt a suboptimal two-step strategy, performing\n2D landmark localization followed by depth estimation. In this paper, we\npropose the Joint Voxel and Coordinate Regression (JVCR) method for 3D facial\nlandmark localization, addressing it more effectively in an end-to-end fashion.\nFirst, a compact volumetric representation is proposed to encode the per-voxel\nlikelihood of positions being the 3D landmarks. The dimensionality of such a\nrepresentation is fixed regardless of the number of target landmarks, so that\nthe curse of dimensionality could be avoided. Then, a stacked hourglass network\nis adopted to estimate the volumetric representation from coarse to fine,\nfollowed by a 3D convolution network that takes the estimated volume as input\nand regresses 3D coordinates of the face shape. In this way, the 3D structural\nconstraints between landmarks could be learned by the neural network in a more\nefficient manner. Moreover, the proposed pipeline enables end-to-end training\nand improves the robustness and accuracy of 3D facial landmark localization.\nThe effectiveness of our approach is validated on the 3DFAW and AFLW2000-3D\ndatasets. Experimental results show that the proposed method achieves\nstate-of-the-art performance in comparison with existing methods. \n\n"}
{"id": "1801.09518", "contents": "Title: Learning-based Image Reconstruction via Parallel Proximal Algorithm Abstract: In the past decade, sparsity-driven regularization has led to advancement of\nimage reconstruction algorithms. Traditionally, such regularizers rely on\nanalytical models of sparsity (e.g. total variation (TV)). However, more recent\nmethods are increasingly centered around data-driven arguments inspired by deep\nlearning. In this letter, we propose to generalize TV regularization by\nreplacing the l1-penalty with an alternative prior that is trainable.\nSpecifically, our method learns the prior via extending the recently proposed\nfast parallel proximal algorithm (FPPA) to incorporate data-adaptive proximal\noperators. The proposed framework does not require additional inner iterations\nfor evaluating the proximal mappings of the corresponding learned prior.\nMoreover, our formalism ensures that the training and reconstruction processes\nshare the same algorithmic structure, making the end-to-end implementation\nintuitive. As an example, we demonstrate our algorithm on the problem of\ndeconvolution in a fluorescence microscope. \n\n"}
{"id": "1801.10341", "contents": "Title: An Infinitesimal Probabilistic Model for Principal Component Analysis of\n  Manifold Valued Data Abstract: We provide a probabilistic and infinitesimal view of how the principal\ncomponent analysis procedure (PCA) can be generalized to analysis of nonlinear\nmanifold valued data. Starting with the probabilistic PCA interpretation of the\nEuclidean PCA procedure, we show how PCA can be generalized to manifolds in an\nintrinsic way that does not resort to linearization of the data space. The\nunderlying probability model is constructed by mapping a Euclidean stochastic\nprocess to the manifold using stochastic development of Euclidean\nsemimartingales. The construction uses a connection and bundles of covariant\ntensors to allow global transport of principal eigenvectors, and the model is\nthereby an example of how principal fiber bundles can be used to handle the\nlack of global coordinate system and orientations that characterizes manifold\nvalued statistics. We show how curvature implies non-integrability of the\nequivalent of Euclidean principal subspaces, and how the stochastic flows\nprovide an alternative to explicit construction of such subspaces. We describe\nestimation procedures for inference of parameters and prediction of principal\ncomponents, and we give examples of properties of the model on embedded\nsurfaces. \n\n"}
{"id": "1802.00664", "contents": "Title: Convolutional neural network-based regression for depth prediction in\n  digital holography Abstract: Digital holography enables us to reconstruct objects in three-dimensional\nspace from holograms captured by an imaging device. For the reconstruction, we\nneed to know the depth position of the recoded object in advance. In this\nstudy, we propose depth prediction using convolutional neural network\n(CNN)-based regression. In the previous researches, the depth of an object was\nestimated through reconstructed images at different depth positions from a\nhologram using a certain metric that indicates the most focused depth position;\nhowever, such a depth search is time-consuming. The CNN of the proposed method\ncan directly predict the depth position with millimeter precision from\nholograms. \n\n"}
{"id": "1802.01218", "contents": "Title: Efficient Video Object Segmentation via Network Modulation Abstract: Video object segmentation targets at segmenting a specific object throughout\na video sequence, given only an annotated first frame. Recent deep learning\nbased approaches find it effective by fine-tuning a general-purpose\nsegmentation model on the annotated frame using hundreds of iterations of\ngradient descent. Despite the high accuracy these methods achieve, the\nfine-tuning process is inefficient and fail to meet the requirements of real\nworld applications. We propose a novel approach that uses a single forward pass\nto adapt the segmentation model to the appearance of a specific object.\nSpecifically, a second meta neural network named modulator is learned to\nmanipulate the intermediate layers of the segmentation network given limited\nvisual and spatial information of the target object. The experiments show that\nour approach is 70times faster than fine-tuning approaches while achieving\nsimilar accuracy. \n\n"}
{"id": "1802.02147", "contents": "Title: DeepTravel: a Neural Network Based Travel Time Estimation Model with\n  Auxiliary Supervision Abstract: Estimating the travel time of a path is of great importance to smart urban\nmobility. Existing approaches are either based on estimating the time cost of\neach road segment which are not able to capture many cross-segment complex\nfactors, or designed heuristically in a non-learning-based way which fail to\nutilize the existing abundant temporal labels of the data, i.e., the time stamp\nof each trajectory point. In this paper, we leverage on new development of deep\nneural networks and propose a novel auxiliary supervision model, namely\nDeepTravel, that can automatically and effectively extract different features,\nas well as make full use of the temporal labels of the trajectory data. We have\nconducted comprehensive experiments on real datasets to demonstrate the\nout-performance of DeepTravel over existing approaches. \n\n"}
{"id": "1802.03345", "contents": "Title: A Two-Stage Method for Text Line Detection in Historical Documents Abstract: This work presents a two-stage text line detection method for historical\ndocuments. Each detected text line is represented by its baseline. In a first\nstage, a deep neural network called ARU-Net labels pixels to belong to one of\nthe three classes: baseline, separator or other. The separator class marks\nbeginning and end of each text line. The ARU-Net is trainable from scratch with\nmanageably few manually annotated example images (less than 50). This is\nachieved by utilizing data augmentation strategies. The network predictions are\nused as input for the second stage which performs a bottom-up clustering to\nbuild baselines. The developed method is capable of handling complex layouts as\nwell as curved and arbitrarily oriented text lines. It substantially\noutperforms current state-of-the-art approaches. For example, for the complex\ntrack of the cBAD: ICDAR2017 Competition on Baseline Detection the F-value is\nincreased from 0.859 to 0.922. The framework to train and run the ARU-Net is\nopen source. \n\n"}
{"id": "1802.04735", "contents": "Title: Semantic Scene Completion Combining Colour and Depth: preliminary\n  experiments Abstract: Semantic scene completion is the task of producing a complete 3D voxel\nrepresentation of volumetric occupancy with semantic labels for a scene from a\nsingle-view observation. We built upon the recent work of Song et al. (CVPR\n2017), who proposed SSCnet, a method that performs scene completion and\nsemantic labelling in a single end-to-end 3D convolutional network. SSCnet uses\nonly depth maps as input, even though depth maps are usually obtained from\ndevices that also capture colour information, such as RGBD sensors and stereo\ncameras. In this work, we investigate the potential of the RGB colour channels\nto improve SSCnet. \n\n"}
{"id": "1802.04877", "contents": "Title: Learning via social awareness: Improving a deep generative sketching\n  model with facial feedback Abstract: In the quest towards general artificial intelligence (AI), researchers have\nexplored developing loss functions that act as intrinsic motivators in the\nabsence of external rewards. This paper argues that such research has\noverlooked an important and useful intrinsic motivator: social interaction. We\nposit that making an AI agent aware of implicit social feedback from humans can\nallow for faster learning of more generalizable and useful representations, and\ncould potentially impact AI safety. We collect social feedback in the form of\nfacial expression reactions to samples from Sketch RNN, an LSTM-based\nvariational autoencoder (VAE) designed to produce sketch drawings. We use a\nLatent Constraints GAN (LC-GAN) to learn from the facial feedback of a small\ngroup of viewers, by optimizing the model to produce sketches that it predicts\nwill lead to more positive facial expressions. We show in multiple independent\nevaluations that the model trained with facial feedback produced sketches that\nare more highly rated, and induce significantly more positive facial\nexpressions. Thus, we establish that implicit social feedback can improve the\noutput of a deep learning model. \n\n"}
{"id": "1802.06399", "contents": "Title: Visual-Only Recognition of Normal, Whispered and Silent Speech Abstract: Silent speech interfaces have been recently proposed as a way to enable\ncommunication when the acoustic signal is not available. This introduces the\nneed to build visual speech recognition systems for silent and whispered\nspeech. However, almost all the recently proposed systems have been trained on\nvocalised data only. This is in contrast with evidence in the literature which\nsuggests that lip movements change depending on the speech mode. In this work,\nwe introduce a new audiovisual database which is publicly available and\ncontains normal, whispered and silent speech. To the best of our knowledge,\nthis is the first study which investigates the differences between the three\nspeech modes using the visual modality only. We show that an absolute decrease\nin classification rate of up to 3.7% is observed when training and testing on\nnormal and whispered, respectively, and vice versa. An even higher decrease of\nup to 8.5% is reported when the models are tested on silent speech. This\nreveals that there are indeed visual differences between the 3 speech modes and\nthe common assumption that vocalized training data can be used directly to\ntrain a silent speech recognition system may not be true. \n\n"}
{"id": "1802.06664", "contents": "Title: Multi-task, multi-label and multi-domain learning with residual\n  convolutional networks for emotion recognition Abstract: Automated emotion recognition in the wild from facial images remains a\nchallenging problem. Although recent advances in Deep Learning have supposed a\nsignificant breakthrough in this topic, strong changes in pose, orientation and\npoint of view severely harm current approaches. In addition, the acquisition of\nlabeled datasets is costly, and current state-of-the-art deep learning\nalgorithms cannot model all the aforementioned difficulties. In this paper, we\npropose to apply a multi-task learning loss function to share a common feature\nrepresentation with other related tasks. Particularly we show that emotion\nrecognition benefits from jointly learning a model with a detector of facial\nAction Units (collective muscle movements). The proposed loss function\naddresses the problem of learning multiple tasks with heterogeneously labeled\ndata, improving previous multi-task approaches. We validate the proposal using\ntwo datasets acquired in non controlled environments, and an application to\npredict compound facial emotion expressions. \n\n"}
{"id": "1802.06898", "contents": "Title: EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based\n  Cameras Abstract: Event-based cameras have shown great promise in a variety of situations where\nframe based cameras suffer, such as high speed motions and high dynamic range\nscenes. However, developing algorithms for event measurements requires a new\nclass of hand crafted algorithms. Deep learning has shown great success in\nproviding model free solutions to many problems in the vision community, but\nexisting networks have been developed with frame based images in mind, and\nthere does not exist the wealth of labeled data for events as there does for\nimages for supervised training. To these points, we present EV-FlowNet, a novel\nself-supervised deep learning pipeline for optical flow estimation for event\nbased cameras. In particular, we introduce an image based representation of a\ngiven event stream, which is fed into a self-supervised neural network as the\nsole input. The corresponding grayscale images captured from the same camera at\nthe same time as the events are then used as a supervisory signal to provide a\nloss function at training time, given the estimated flow from the network. We\nshow that the resulting network is able to accurately predict optical flow from\nevents only in a variety of different scenes, with performance competitive to\nimage based networks. This method not only allows for accurate estimation of\ndense optical flow, but also provides a framework for the transfer of other\nself-supervised methods to the event-based domain. \n\n"}
{"id": "1802.06971", "contents": "Title: A survey on trajectory clustering analysis Abstract: This paper comprehensively surveys the development of trajectory clustering.\nConsidering the critical role of trajectory data mining in modern intelligent\nsystems for surveillance security, abnormal behavior detection, crowd behavior\nanalysis, and traffic control, trajectory clustering has attracted growing\nattention. Existing trajectory clustering methods can be grouped into three\ncategories: unsupervised, supervised and semi-supervised algorithms. In spite\nof achieving a certain level of development, trajectory clustering is limited\nin its success by complex conditions such as application scenarios and data\ndimensions. This paper provides a holistic understanding and deep insight into\ntrajectory clustering, and presents a comprehensive analysis of representative\nmethods and promising future directions. \n\n"}
{"id": "1802.07088", "contents": "Title: i-RevNet: Deep Invertible Networks Abstract: It is widely believed that the success of deep convolutional networks is\nbased on progressively discarding uninformative variability about the input\nwith respect to the problem at hand. This is supported empirically by the\ndifficulty of recovering images from their hidden representations, in most\ncommonly used network architectures. In this paper we show via a one-to-one\nmapping that this loss of information is not a necessary condition to learn\nrepresentations that generalize well on complicated problems, such as ImageNet.\nVia a cascade of homeomorphic layers, we build the i-RevNet, a network that can\nbe fully inverted up to the final projection onto the classes, i.e. no\ninformation is discarded. Building an invertible architecture is difficult, for\none, because the local inversion is ill-conditioned, we overcome this by\nproviding an explicit inverse. An analysis of i-RevNets learned representations\nsuggests an alternative explanation for the success of deep networks by a\nprogressive contraction and linear separation with depth. To shed light on the\nnature of the model learned by the i-RevNet we reconstruct linear\ninterpolations between natural image representations. \n\n"}
{"id": "1802.07789", "contents": "Title: Semantic Segmentation Refinement by Monte Carlo Region Growing of High\n  Confidence Detections Abstract: Despite recent improvements using fully convolutional networks, in general,\nthe segmentation produced by most state-of-the-art semantic segmentation\nmethods does not show satisfactory adherence to the object boundaries. We\npropose a method to refine the segmentation results generated by such deep\nlearning models. Our method takes as input the confidence scores generated by a\npixel-dense segmentation network and re-labels pixels with low confidence\nlevels. The re-labeling approach employs a region growing mechanism that\naggregates these pixels to neighboring areas with high confidence scores and\nsimilar appearance. In order to correct the labels of pixels that were\nincorrectly classified with high confidence level by the semantic segmentation\nalgorithm, we generate multiple region growing steps through a Monte Carlo\nsampling of the seeds of the regions. Our method improves the accuracy of a\nstate-of-the-art fully convolutional semantic segmentation approach on the\npublicly available COCO and PASCAL datasets, and it shows significantly better\nresults on selected sequences of the finely-annotated DAVIS dataset. \n\n"}
{"id": "1803.00068", "contents": "Title: Gotta Adapt 'Em All: Joint Pixel and Feature-Level Domain Adaptation for\n  Recognition in the Wild Abstract: Recent developments in deep domain adaptation have allowed knowledge transfer\nfrom a labeled source domain to an unlabeled target domain at the level of\nintermediate features or input pixels. We propose that advantages may be\nderived by combining them, in the form of different insights that lead to a\nnovel design and complementary properties that result in better performance. At\nthe feature level, inspired by insights from semi-supervised learning, we\npropose a classification-aware domain adversarial neural network that brings\ntarget examples into more classifiable regions of source domain. Next, we posit\nthat computer vision insights are more amenable to injection at the pixel\nlevel. In particular, we use 3D geometry and image synthesis based on a\ngeneralized appearance flow to preserve identity across pose transformations,\nwhile using an attribute-conditioned CycleGAN to translate a single source into\nmultiple target images that differ in lower-level properties such as lighting.\nBesides standard UDA benchmark, we validate on a novel and apt problem of car\nrecognition in unlabeled surveillance images using labeled images from the web,\nhandling explicitly specified, nameable factors of variation through\npixel-level and implicit, unspecified factors through feature-level adaptation. \n\n"}
{"id": "1803.00404", "contents": "Title: Deep Defense: Training DNNs with Improved Adversarial Robustness Abstract: Despite the efficacy on a variety of computer vision tasks, deep neural\nnetworks (DNNs) are vulnerable to adversarial attacks, limiting their\napplications in security-critical systems. Recent works have shown the\npossibility of generating imperceptibly perturbed image inputs (a.k.a.,\nadversarial examples) to fool well-trained DNN classifiers into making\narbitrary predictions. To address this problem, we propose a training recipe\nnamed \"deep defense\". Our core idea is to integrate an adversarial\nperturbation-based regularizer into the classification objective, such that the\nobtained models learn to resist potential attacks, directly and precisely. The\nwhole optimization problem is solved just like training a recursive network.\nExperimental results demonstrate that our method outperforms training with\nadversarial/Parseval regularizations by large margins on various datasets\n(including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code\nand models for reproducing our results are available at\nhttps://github.com/ZiangYan/deepdefense.pytorch \n\n"}
{"id": "1803.00443", "contents": "Title: Knowledge Transfer with Jacobian Matching Abstract: Classical distillation methods transfer representations from a \"teacher\"\nneural network to a \"student\" network by matching their output activations.\nRecent methods also match the Jacobians, or the gradient of output activations\nwith the input. However, this involves making some ad hoc decisions, in\nparticular, the choice of the loss function.\n  In this paper, we first establish an equivalence between Jacobian matching\nand distillation with input noise, from which we derive appropriate loss\nfunctions for Jacobian matching. We then rely on this analysis to apply\nJacobian matching to transfer learning by establishing equivalence of a recent\ntransfer learning procedure to distillation.\n  We then show experimentally on standard image datasets that Jacobian-based\npenalties improve distillation, robustness to noisy inputs, and transfer\nlearning. \n\n"}
{"id": "1803.00557", "contents": "Title: The 2018 DAVIS Challenge on Video Object Segmentation Abstract: We present the 2018 DAVIS Challenge on Video Object Segmentation, a public\ncompetition specifically designed for the task of video object segmentation. It\nbuilds upon the DAVIS 2017 dataset, which was presented in the previous edition\nof the DAVIS Challenge, and added 100 videos with multiple objects per sequence\nto the original DAVIS 2016 dataset. Motivated by the analysis of the results of\nthe 2017 edition, the main track of the competition will be the same than in\nthe previous edition (segmentation given the full mask of the objects in the\nfirst frame -- semi-supervised scenario). This edition, however, also adds an\ninteractive segmentation teaser track, where the participants will interact\nwith a web service simulating the input of a human that provides scribbles to\niteratively improve the result. \n\n"}
{"id": "1803.00702", "contents": "Title: Raw Multi-Channel Audio Source Separation using Multi-Resolution\n  Convolutional Auto-Encoders Abstract: Supervised multi-channel audio source separation requires extracting useful\nspectral, temporal, and spatial features from the mixed signals. The success of\nmany existing systems is therefore largely dependent on the choice of features\nused for training. In this work, we introduce a novel multi-channel,\nmulti-resolution convolutional auto-encoder neural network that works on raw\ntime-domain signals to determine appropriate multi-resolution features for\nseparating the singing-voice from stereo music. Our experimental results show\nthat the proposed method can achieve multi-channel audio source separation\nwithout the need for hand-crafted features or any pre- or post-processing. \n\n"}
{"id": "1803.02504", "contents": "Title: Exponential Discriminative Metric Embedding in Deep Learning Abstract: With the remarkable success achieved by the Convolutional Neural Networks\n(CNNs) in object recognition recently, deep learning is being widely used in\nthe computer vision community. Deep Metric Learning (DML), integrating deep\nlearning with conventional metric learning, has set new records in many fields,\nespecially in classification task. In this paper, we propose a replicable DML\nmethod, called Include and Exclude (IE) loss, to force the distance between a\nsample and its designated class center away from the mean distance of this\nsample to other class centers with a large margin in the exponential feature\nprojection space. With the supervision of IE loss, we can train CNNs to enhance\nthe intra-class compactness and inter-class separability, leading to great\nimprovements on several public datasets ranging from object recognition to face\nverification. We conduct a comparative study of our algorithm with several\ntypical DML methods on three kinds of networks with different capacity.\nExtensive experiments on three object recognition datasets and two face\nrecognition datasets demonstrate that IE loss is always superior to other\nmainstream DML methods and approach the state-of-the-art results. \n\n"}
{"id": "1803.02555", "contents": "Title: Object cosegmentation using deep Siamese network Abstract: Object cosegmentation addresses the problem of discovering similar objects\nfrom multiple images and segmenting them as foreground simultaneously. In this\npaper, we propose a novel end-to-end pipeline to segment the similar objects\nsimultaneously from relevant set of images using supervised learning via\ndeep-learning framework. We experiment with multiple set of object proposal\ngeneration techniques and perform extensive numerical evaluations by training\nthe Siamese network with generated object proposals. Similar objects proposals\nfor the test images are retrieved using the ANNOY (Approximate Nearest\nNeighbor) library and deep semantic segmentation is performed on them. Finally,\nwe form a collage from the segmented similar objects based on the relative\nimportance of the objects. \n\n"}
{"id": "1803.03849", "contents": "Title: Learning to Localize Sound Source in Visual Scenes Abstract: Visual events are usually accompanied by sounds in our daily lives. We pose\nthe question: Can the machine learn the correspondence between visual scene and\nthe sound, and localize the sound source only by observing sound and visual\nscene pairs like human? In this paper, we propose a novel unsupervised\nalgorithm to address the problem of localizing the sound source in visual\nscenes. A two-stream network structure which handles each modality, with\nattention mechanism is developed for sound source localization. Moreover,\nalthough our network is formulated within the unsupervised learning framework,\nit can be extended to a unified architecture with a simple modification for the\nsupervised and semi-supervised learning settings as well. Meanwhile, a new\nsound source dataset is developed for performance evaluation. Our empirical\nevaluation shows that the unsupervised method eventually go through false\nconclusion in some cases. We show that even with a few supervision, false\nconclusion is able to be corrected and the source of sound in a visual scene\ncan be localized effectively. \n\n"}
{"id": "1803.03849", "contents": "Title: Learning to Localize Sound Source in Visual Scenes Abstract: Visual events are usually accompanied by sounds in our daily lives. We pose\nthe question: Can the machine learn the correspondence between visual scene and\nthe sound, and localize the sound source only by observing sound and visual\nscene pairs like human? In this paper, we propose a novel unsupervised\nalgorithm to address the problem of localizing the sound source in visual\nscenes. A two-stream network structure which handles each modality, with\nattention mechanism is developed for sound source localization. Moreover,\nalthough our network is formulated within the unsupervised learning framework,\nit can be extended to a unified architecture with a simple modification for the\nsupervised and semi-supervised learning settings as well. Meanwhile, a new\nsound source dataset is developed for performance evaluation. Our empirical\nevaluation shows that the unsupervised method eventually go through false\nconclusion in some cases. We show that even with a few supervision, false\nconclusion is able to be corrected and the source of sound in a visual scene\ncan be localized effectively. \n\n"}
{"id": "1803.04337", "contents": "Title: Replication study: Development and validation of deep learning algorithm\n  for detection of diabetic retinopathy in retinal fundus photographs Abstract: Replication studies are essential for validation of new methods, and are\ncrucial to maintain the high standards of scientific publications, and to use\nthe results in practice. We have attempted to replicate the main method in\n'Development and validation of a deep learning algorithm for detection of\ndiabetic retinopathy in retinal fundus photographs' published in JAMA 2016;\n316(22). We re-implemented the method since the source code is not available,\nand we used publicly available data sets. The original study used non-public\nfundus images from EyePACS and three hospitals in India for training. We used a\ndifferent EyePACS data set from Kaggle. The original study used the benchmark\ndata set Messidor-2 to evaluate the algorithm's performance. We used the same\ndata set. In the original study, ophthalmologists re-graded all images for\ndiabetic retinopathy, macular edema, and image gradability. There was one\ndiabetic retinopathy grade per image for our data sets, and we assessed image\ngradability ourselves. Hyper-parameter settings were not described in the\noriginal study. But some of these were later published. We were not able to\nreplicate the original study. Our algorithm's area under the receiver operating\ncurve (AUC) of 0.94 on the Kaggle EyePACS test set and 0.80 on Messidor-2 did\nnot come close to the reported AUC of 0.99 in the original study. This may be\ncaused by the use of a single grade per image, different data, or different not\ndescribed hyper-parameter settings. This study shows the challenges of\nreplicating deep learning, and the need for more replication studies to\nvalidate deep learning methods, especially for medical image analysis.\n  Our source code and instructions are available at:\nhttps://github.com/mikevoets/jama16-retina-replication \n\n"}
{"id": "1803.05494", "contents": "Title: Improving Object Counting with Heatmap Regulation Abstract: In this paper, we propose a simple and effective way to improve one-look\nregression models for object counting from images. We use class activation map\nvisualizations to illustrate the drawbacks of learning a pure one-look\nregression model for a counting task. Based on these insights, we enhance\none-look regression counting models by regulating activation maps from the\nfinal convolution layer of the network with coarse ground-truth activation maps\ngenerated from simple dot annotations. We call this strategy heatmap regulation\n(HR). We show that this simple enhancement effectively suppresses false\ndetections generated by the corresponding one-look baseline model and also\nimproves the performance in terms of false negatives. Evaluations are performed\non four different counting datasets --- two for car counting (CARPK, PUCPR+),\none for crowd counting (WorldExpo) and another for biological cell counting\n(VGG-Cells). Adding HR to a simple VGG front-end improves performance on all\nthese benchmarks compared to a simple one-look baseline model and results in\nstate-of-the-art performance for car counting. \n\n"}
{"id": "1803.05729", "contents": "Title: Exploring Linear Relationship in Feature Map Subspace for ConvNets\n  Compression Abstract: While the research on convolutional neural networks (CNNs) is progressing\nquickly, the real-world deployment of these models is often limited by\ncomputing resources and memory constraints. In this paper, we address this\nissue by proposing a novel filter pruning method to compress and accelerate\nCNNs. Our work is based on the linear relationship identified in different\nfeature map subspaces via visualization of feature maps. Such linear\nrelationship implies that the information in CNNs is redundant. Our method\neliminates the redundancy in convolutional filters by applying subspace\nclustering to feature maps. In this way, most of the representative information\nin the network can be retained in each cluster. Therefore, our method provides\nan effective solution to filter pruning for which most existing methods\ndirectly remove filters based on simple heuristics. The proposed method is\nindependent of the network structure, thus it can be adopted by any\noff-the-shelf deep learning libraries. Experiments on different networks and\ntasks show that our method outperforms existing techniques before fine-tuning,\nand achieves the state-of-the-art results after fine-tuning. \n\n"}
{"id": "1803.05854", "contents": "Title: Development and Validation of Deep Learning Algorithms for Detection of\n  Critical Findings in Head CT Scans Abstract: Importance: Non-contrast head CT scan is the current standard for initial\nimaging of patients with head trauma or stroke symptoms.\n  Objective: To develop and validate a set of deep learning algorithms for\nautomated detection of following key findings from non-contrast head CT scans:\nintracranial hemorrhage (ICH) and its types, intraparenchymal (IPH),\nintraventricular (IVH), subdural (SDH), extradural (EDH) and subarachnoid (SAH)\nhemorrhages, calvarial fractures, midline shift and mass effect.\n  Design and Settings: We retrospectively collected a dataset containing\n313,318 head CT scans along with their clinical reports from various centers. A\npart of this dataset (Qure25k dataset) was used to validate and the rest to\ndevelop algorithms. Additionally, a dataset (CQ500 dataset) was collected from\ndifferent centers in two batches B1 & B2 to clinically validate the algorithms.\n  Main Outcomes and Measures: Original clinical radiology report and consensus\nof three independent radiologists were considered as gold standard for Qure25k\nand CQ500 datasets respectively. Area under receiver operating characteristics\ncurve (AUC) for each finding was primarily used to evaluate the algorithms.\n  Results: Qure25k dataset contained 21,095 scans (mean age 43.31; 42.87%\nfemale) while batches B1 and B2 of CQ500 dataset consisted of 214 (mean age\n43.40; 43.92% female) and 277 (mean age 51.70; 30.31% female) scans\nrespectively. On Qure25k dataset, the algorithms achieved AUCs of 0.9194,\n0.8977, 0.9559, 0.9161, 0.9288 and 0.9044 for detecting ICH, IPH, IVH, SDH, EDH\nand SAH respectively. AUCs for the same on CQ500 dataset were 0.9419, 0.9544,\n0.9310, 0.9521, 0.9731 and 0.9574 respectively. For detecting calvarial\nfractures, midline shift and mass effect, AUCs on Qure25k dataset were 0.9244,\n0.9276 and 0.8583 respectively, while AUCs on CQ500 dataset were 0.9624, 0.9697\nand 0.9216 respectively. \n\n"}
{"id": "1803.05982", "contents": "Title: Real-time Deep Pose Estimation with Geodesic Loss for Image-to-Template\n  Rigid Registration Abstract: With an aim to increase the capture range and accelerate the performance of\nstate-of-the-art inter-subject and subject-to-template 3D registration, we\npropose deep learning-based methods that are trained to find the 3D position of\narbitrarily oriented subjects or anatomy based on slices or volumes of medical\nimages. For this, we propose regression CNNs that learn to predict the\nangle-axis representation of 3D rotations and translations using image\nfeatures. We use and compare mean square error and geodesic loss to train\nregression CNNs for 3D pose estimation used in two different scenarios:\nslice-to-volume registration and volume-to-volume registration. Our results\nshow that in such registration applications that are amendable to learning, the\nproposed deep learning methods with geodesic loss minimization can achieve\naccurate results with a wide capture range in real-time (<100ms). We also\ntested the generalization capability of the trained CNNs on an expanded age\nrange and on images of newborn subjects with similar and different MR image\ncontrasts. We trained our models on T2-weighted fetal brain MRI scans and used\nthem to predict the 3D pose of newborn brains based on T1-weighted MRI scans.\nWe showed that the trained models generalized well for the new domain when we\nperformed image contrast transfer through a conditional generative adversarial\nnetwork. This indicates that the domain of application of the trained deep\nregression CNNs can be further expanded to image modalities and contrasts other\nthan those used in training. A combination of our proposed methods with\naccelerated optimization-based registration algorithms can dramatically enhance\nthe performance of automatic imaging devices and image processing methods of\nthe future. \n\n"}
{"id": "1803.06506", "contents": "Title: Learning Unsupervised Visual Grounding Through Semantic Self-Supervision Abstract: Localizing natural language phrases in images is a challenging problem that\nrequires joint understanding of both the textual and visual modalities. In the\nunsupervised setting, lack of supervisory signals exacerbate this difficulty.\nIn this paper, we propose a novel framework for unsupervised visual grounding\nwhich uses concept learning as a proxy task to obtain self-supervision. The\nsimple intuition behind this idea is to encourage the model to localize to\nregions which can explain some semantic property in the data, in our case, the\nproperty being the presence of a concept in a set of images. We present\nthorough quantitative and qualitative experiments to demonstrate the efficacy\nof our approach and show a 5.6% improvement over the current state of the art\non Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and\ncomparable to state-of-art performance on the Flickr30k dataset. \n\n"}
{"id": "1803.07955", "contents": "Title: A Cascaded Convolutional Neural Network for Single Image Dehazing Abstract: Images captured under outdoor scenes usually suffer from low contrast and\nlimited visibility due to suspended atmospheric particles, which directly\naffects the quality of photos. Despite numerous image dehazing methods have\nbeen proposed, effective hazy image restoration remains a challenging problem.\nExisting learning-based methods usually predict the medium transmission by\nConvolutional Neural Networks (CNNs), but ignore the key global atmospheric\nlight. Different from previous learning-based methods, we propose a flexible\ncascaded CNN for single hazy image restoration, which considers the medium\ntransmission and global atmospheric light jointly by two task-driven\nsubnetworks. Specifically, the medium transmission estimation subnetwork is\ninspired by the densely connected CNN while the global atmospheric light\nestimation subnetwork is a light-weight CNN. Besides, these two subnetworks are\ncascaded by sharing the common features. Finally, with the estimated model\nparameters, the haze-free image is obtained by the atmospheric scattering model\ninversion, which achieves more accurate and effective restoration performance.\nQualitatively and quantitatively experimental results on the synthetic and\nreal-world hazy images demonstrate that the proposed method effectively removes\nhaze from such images, and outperforms several state-of-the-art dehazing\nmethods. \n\n"}
{"id": "1803.08134", "contents": "Title: Task dependent Deep LDA pruning of neural networks Abstract: With deep learning's success, a limited number of popular deep nets have been\nwidely adopted for various vision tasks. However, this usually results in\nunnecessarily high complexities and possibly many features of low task utility.\nIn this paper, we address this problem by introducing a task-dependent deep\npruning framework based on Fisher's Linear Discriminant Analysis (LDA). The\napproach can be applied to convolutional, fully-connected, and module-based\ndeep network structures, in all cases leveraging the high decorrelation of\nneuron motifs found in the pre-decision space and cross-layer deconv\ndependency. Moreover, we examine our approach's potential in network\narchitecture search for specific tasks and analyze the influence of our pruning\non model robustness to noises and adversarial attacks. Experimental results on\ndatasets of generic objects (ImageNet, CIFAR100) as well as domain specific\ntasks (Adience, and LFWA) illustrate our framework's superior performance over\nstate-of-the-art pruning approaches and fixed compact nets (e.g. SqueezeNet,\nMobileNet). The proposed method successfully maintains comparable accuracies\neven after discarding most parameters (98%-99% for VGG16, up to 82% for the\nalready compact InceptionNet) and with significant FLOP reductions (83% for\nVGG16, up to 64% for InceptionNet). Through pruning, we can also derive\nsmaller, but more accurate and more robust models suitable for the task. \n\n"}
{"id": "1803.08457", "contents": "Title: Clustering-driven Deep Embedding with Pairwise Constraints Abstract: Recently, there has been increasing interest to leverage the competence of\nneural networks to analyze data. In particular, new clustering methods that\nemploy deep embeddings have been presented. In this paper, we depart from\ncentroid-based models and suggest a new framework, called Clustering-driven\ndeep embedding with PAirwise Constraints (CPAC), for non-parametric clustering\nusing a neural network. We present a clustering-driven embedding based on a\nSiamese network that encourages pairs of data points to output similar\nrepresentations in the latent space. Our pair-based model allows augmenting the\ninformation with labeled pairs to constitute a semi-supervised framework. Our\napproach is based on analyzing the losses associated with each pair to refine\nthe set of constraints. We show that clustering performance increases when\nusing this scheme, even with a limited amount of user queries. We demonstrate\nhow our architecture is adapted for various types of data and present the first\ndeep framework to cluster 3D shapes. \n\n"}
{"id": "1803.08673", "contents": "Title: Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps\n  with Accurate Object Boundaries Abstract: This paper considers the problem of single image depth estimation. The\nemployment of convolutional neural networks (CNNs) has recently brought about\nsignificant advancements in the research of this problem. However, most\nexisting methods suffer from loss of spatial resolution in the estimated depth\nmaps; a typical symptom is distorted and blurry reconstruction of object\nboundaries. In this paper, toward more accurate estimation with a focus on\ndepth maps with higher spatial resolution, we propose two improvements to\nexisting approaches. One is about the strategy of fusing features extracted at\ndifferent scales, for which we propose an improved network architecture\nconsisting of four modules: an encoder, decoder, multi-scale feature fusion\nmodule, and refinement module. The other is about loss functions for measuring\ninference errors used in training. We show that three loss terms, which measure\nerrors in depth, gradients and surface normals, respectively, contribute to\nimprovement of accuracy in an complementary fashion. Experimental results show\nthat these two improvements enable to attain higher accuracy than the current\nstate-of-the-arts, which is given by finer resolution reconstruction, for\nexample, with small objects and object boundaries. \n\n"}
{"id": "1803.08709", "contents": "Title: Pose-Driven Deep Models for Person Re-Identification Abstract: Person re-identification (re-id) is the task of recognizing and matching\npersons at different locations recorded by cameras with non-overlapping views.\nOne of the main challenges of re-id is the large variance in person poses and\ncamera angles since neither of them can be influenced by the re-id system. In\nthis work, an effective approach to integrate coarse camera view information as\nwell as fine-grained pose information into a convolutional neural network (CNN)\nmodel for learning discriminative re-id embeddings is introduced. In most\nrecent work pose information is either explicitly modeled within the re-id\nsystem or explicitly used for pre-processing, for example by pose-normalizing\nperson images. In contrast, the proposed approach shows that a direct use of\ncamera view as well as the detected body joint locations into a standard CNN\ncan be used to significantly improve the robustness of learned re-id\nembeddings. On four challenging surveillance and video re-id datasets\nsignificant improvements over the current state of the art have been achieved.\nFurthermore, a novel reordering of the MARS dataset, called X-MARS is\nintroduced to allow cross-validation of models trained for single-image re-id\non tracklet data. \n\n"}
{"id": "1803.09331", "contents": "Title: StarMap for Category-Agnostic Keypoint and Viewpoint Estimation Abstract: Semantic keypoints provide concise abstractions for a variety of visual\nunderstanding tasks. Existing methods define semantic keypoints separately for\neach category with a fixed number of semantic labels in fixed indices. As a\nresult, this keypoint representation is in-feasible when objects have a varying\nnumber of parts, e.g. chairs with varying number of legs. We propose a\ncategory-agnostic keypoint representation, which combines a multi-peak heatmap\n(StarMap) for all the keypoints and their corresponding features as 3D\nlocations in the canonical viewpoint (CanViewFeature) defined for each\ninstance. Our intuition is that the 3D locations of the keypoints in canonical\nobject views contain rich semantic and compositional information. Using our\nflexible representation, we demonstrate competitive performance in keypoint\ndetection and localization compared to category-specific state-of-the-art\nmethods. Moreover, we show that when augmented with an additional depth channel\n(DepthMap) to lift the 2D keypoints to 3D, our representation can achieve\nstate-of-the-art results in viewpoint estimation. Finally, we show that our\ncategory-agnostic keypoint representation can be generalized to novel\ncategories. \n\n"}
{"id": "1803.10567", "contents": "Title: Image Generation and Translation with Disentangled Representations Abstract: Generative models have made significant progress in the tasks of modeling\ncomplex data distributions such as natural images. The introduction of\nGenerative Adversarial Networks (GANs) and auto-encoders lead to the\npossibility of training on big data sets in an unsupervised manner. However,\nfor many generative models it is not possible to specify what kind of image\nshould be generated and it is not possible to translate existing images into\nnew images of similar domains. Furthermore, models that can perform\nimage-to-image translation often need distinct models for each domain, making\nit hard to scale these systems to multiple domain image-to-image translation.\nWe introduce a model that can do both, controllable image generation and\nimage-to-image translation between multiple domains. We split our image\nrepresentation into two parts encoding unstructured and structured information\nrespectively. The latter is designed in a disentangled manner, so that\ndifferent parts encode different image characteristics. We train an encoder to\nencode images into these representations and use a small amount of labeled data\nto specify what kind of information should be encoded in the disentangled part.\nA generator is trained to generate images from these representations using the\ncharacteristics provided by the disentangled part of the representation.\nThrough this we can control what kind of images the generator generates,\ntranslate images between different domains, and even learn unknown\ndata-generating factors while only using one single model. \n\n"}
{"id": "1803.10586", "contents": "Title: Stochastic Variational Inference with Gradient Linearization Abstract: Variational inference has experienced a recent surge in popularity owing to\nstochastic approaches, which have yielded practical tools for a wide range of\nmodel classes. A key benefit is that stochastic variational inference obviates\nthe tedious process of deriving analytical expressions for closed-form variable\nupdates. Instead, one simply needs to derive the gradient of the log-posterior,\nwhich is often much easier. Yet for certain model classes, the log-posterior\nitself is difficult to optimize using standard gradient techniques. One such\nexample are random field models, where optimization based on gradient\nlinearization has proven popular, since it speeds up convergence significantly\nand can avoid poor local optima. In this paper we propose stochastic\nvariational inference with gradient linearization (SVIGL). It is similarly\nconvenient as standard stochastic variational inference - all that is required\nis a local linearization of the energy gradient. Its benefit over stochastic\nvariational inference with conventional gradient methods is a clear improvement\nin convergence speed, while yielding comparable or even better variational\napproximations in terms of KL divergence. We demonstrate the benefits of SVIGL\nin three applications: Optical flow estimation, Poisson-Gaussian denoising, and\n3D surface reconstruction. \n\n"}
{"id": "1803.11157", "contents": "Title: Security Consideration For Deep Learning-Based Image Forensics Abstract: Recently, image forensics community has paied attention to the research on\nthe design of effective algorithms based on deep learning technology and facts\nproved that combining the domain knowledge of image forensics and deep learning\nwould achieve more robust and better performance than the traditional schemes.\nInstead of improving it, in this paper, the safety of deep learning based\nmethods in the field of image forensics is taken into account. To the best of\nour knowledge, this is a first work focusing on this topic. Specifically, we\nexperimentally find that the method using deep learning would fail when adding\nthe slight noise into the images (adversarial images). Furthermore, two kinds\nof strategys are proposed to enforce security of deep learning-based method.\nFirstly, an extra penalty term to the loss function is added, which is referred\nto the 2-norm of the gradient of the loss with respect to the input images, and\nthen an novel training method are adopt to train the model by fusing the normal\nand adversarial images. Experimental results show that the proposed algorithm\ncan achieve good performance even in the case of adversarial images and provide\na safety consideration for deep learning-based image forensics \n\n"}
{"id": "1803.11404", "contents": "Title: Cross-modal Deep Variational Hand Pose Estimation Abstract: The human hand moves in complex and high-dimensional ways, making estimation\nof 3D hand pose configurations from images alone a challenging task. In this\nwork we propose a method to learn a statistical hand model represented by a\ncross-modal trained latent space via a generative deep neural network. We\nderive an objective function from the variational lower bound of the VAE\nframework and jointly optimize the resulting cross-modal KL-divergence and the\nposterior reconstruction objective, naturally admitting a training regime that\nleads to a coherent latent space across multiple modalities such as RGB images,\n2D keypoint detections or 3D hand configurations. Additionally, it grants a\nstraightforward way of using semi-supervision. This latent space can be\ndirectly used to estimate 3D hand poses from RGB images, outperforming the\nstate-of-the art in different settings. Furthermore, we show that our proposed\nmethod can be used without changes on depth images and performs comparably to\nspecialized methods. Finally, the model is fully generative and can synthesize\nconsistent pairs of hand configurations across modalities. We evaluate our\nmethod on both RGB and depth datasets and analyze the latent space\nqualitatively. \n\n"}
{"id": "1804.00097", "contents": "Title: Adversarial Attacks and Defences Competition Abstract: To accelerate research on adversarial examples and robustness of machine\nlearning classifiers, Google Brain organized a NIPS 2017 competition that\nencouraged researchers to develop new methods to generate adversarial examples\nas well as to develop new ways to defend against them. In this chapter, we\ndescribe the structure and organization of the competition and the solutions\ndeveloped by several of the top-placing teams. \n\n"}
{"id": "1804.00112", "contents": "Title: Compare and Contrast: Learning Prominent Visual Differences Abstract: Relative attribute models can compare images in terms of all detected\nproperties or attributes, exhaustively predicting which image is fancier, more\nnatural, and so on without any regard to ordering. However, when humans compare\nimages, certain differences will naturally stick out and come to mind first.\nThese most noticeable differences, or prominent differences, are likely to be\ndescribed first. In addition, many differences, although present, may not be\nmentioned at all. In this work, we introduce and model prominent differences, a\nrich new functionality for comparing images. We collect instance-level\nannotations of most noticeable differences, and build a model trained on\nrelative attribute features that predicts prominent differences for unseen\npairs. We test our model on the challenging UT-Zap50K shoes and LFW10 faces\ndatasets, and outperform an array of baseline methods. We then demonstrate how\nour prominence model improves two vision tasks, image search and description\ngeneration, enabling more natural communication between people and vision\nsystems. \n\n"}
{"id": "1804.00113", "contents": "Title: Tagging like Humans: Diverse and Distinct Image Annotation Abstract: In this work we propose a new automatic image annotation model, dubbed {\\bf\ndiverse and distinct image annotation} (D2IA). The generative model D2IA is\ninspired by the ensemble of human annotations, which create semantically\nrelevant, yet distinct and diverse tags. In D2IA, we generate a relevant and\ndistinct tag subset, in which the tags are relevant to the image contents and\nsemantically distinct to each other, using sequential sampling from a\ndeterminantal point process (DPP) model. Multiple such tag subsets that cover\ndiverse semantic aspects or diverse semantic levels of the image contents are\ngenerated by randomly perturbing the DPP sampling process. We leverage a\ngenerative adversarial network (GAN) model to train D2IA. Extensive experiments\nincluding quantitative and qualitative comparisons, as well as human subject\nstudies, on two benchmark datasets demonstrate that the proposed model can\nproduce more diverse and distinct tags than the state-of-the-arts. \n\n"}
{"id": "1804.00216", "contents": "Title: Human Semantic Parsing for Person Re-identification Abstract: Person re-identification is a challenging task mainly due to factors such as\nbackground clutter, pose, illumination and camera point of view variations.\nThese elements hinder the process of extracting robust and discriminative\nrepresentations, hence preventing different identities from being successfully\ndistinguished. To improve the representation learning, usually, local features\nfrom human body parts are extracted. However, the common practice for such a\nprocess has been based on bounding box part detection. In this paper, we\npropose to adopt human semantic parsing which, due to its pixel-level accuracy\nand capability of modeling arbitrary contours, is naturally a better\nalternative. Our proposed SPReID integrates human semantic parsing in person\nre-identification and not only considerably outperforms its counter baseline,\nbut achieves state-of-the-art performance. We also show that by employing a\n\\textit{simple} yet effective training strategy, standard popular deep\nconvolutional architectures such as Inception-V3 and ResNet-152, with no\nmodification, while operating solely on full image, can dramatically outperform\ncurrent state-of-the-art. Our proposed methods improve state-of-the-art person\nre-identification on: Market-1501 by ~17% in mAP and ~6% in rank-1, CUHK03 by\n~4% in rank-1 and DukeMTMC-reID by ~24% in mAP and ~10% in rank-1. \n\n"}
{"id": "1804.00326", "contents": "Title: Seeing Voices and Hearing Faces: Cross-modal biometric matching Abstract: We introduce a seemingly impossible task: given only an audio clip of someone\nspeaking, decide which of two face images is the speaker. In this paper we\nstudy this, and a number of related cross-modal tasks, aimed at answering the\nquestion: how much can we infer from the voice about the face and vice versa?\nWe study this task \"in the wild\", employing the datasets that are now publicly\navailable for face recognition from static images (VGGFace) and speaker\nidentification from audio (VoxCeleb). These provide training and testing\nscenarios for both static and dynamic testing of cross-modal matching. We make\nthe following contributions: (i) we introduce CNN architectures for both binary\nand multi-way cross-modal face and audio matching, (ii) we compare dynamic\ntesting (where video information is available, but the audio is not from the\nsame video) with static testing (where only a single still image is available),\nand (iii) we use human testing as a baseline to calibrate the difficulty of the\ntask. We show that a CNN can indeed be trained to solve this task in both the\nstatic and dynamic scenarios, and is even well above chance on 10-way\nclassification of the face given the voice. The CNN matches human performance\non easy examples (e.g. different gender across faces) but exceeds human\nperformance on more challenging examples (e.g. faces with the same gender, age\nand nationality). \n\n"}
{"id": "1804.00506", "contents": "Title: Towards Explanation of DNN-based Prediction with Guided Feature\n  Inversion Abstract: While deep neural networks (DNN) have become an effective computational tool,\nthe prediction results are often criticized by the lack of interpretability,\nwhich is essential in many real-world applications such as health informatics.\nExisting attempts based on local interpretations aim to identify relevant\nfeatures contributing the most to the prediction of DNN by monitoring the\nneighborhood of a given input. They usually simply ignore the intermediate\nlayers of the DNN that might contain rich information for interpretation. To\nbridge the gap, in this paper, we propose to investigate a guided feature\ninversion framework for taking advantage of the deep architectures towards\neffective interpretation. The proposed framework not only determines the\ncontribution of each feature in the input but also provides insights into the\ndecision-making process of DNN models. By further interacting with the neuron\nof the target category at the output layer of the DNN, we enforce the\ninterpretation result to be class-discriminative. We apply the proposed\ninterpretation model to different CNN architectures to provide explanations for\nimage data and conduct extensive experiments on ImageNet and PASCAL VOC07\ndatasets. The interpretation results demonstrate the effectiveness of our\nproposed framework in providing class-discriminative interpretation for\nDNN-based prediction. \n\n"}
{"id": "1804.00630", "contents": "Title: Updating the generator in PPGN-h with gradients flowing through the\n  encoder Abstract: The Generative Adversarial Network framework has shown success in implicitly\nmodeling data distributions and is able to generate realistic samples. Its\narchitecture is comprised of a generator, which produces fake data that\nsuperficially seem to belong to the real data distribution, and a discriminator\nwhich is to distinguish fake from genuine samples. The Noiseless Joint Plug &\nPlay model offers an extension to the framework by simultaneously training\nautoencoders. This model uses a pre-trained encoder as a feature extractor,\nfeeding the generator with global information. Using the Plug & Play network as\nbaseline, we design a new model by adding discriminators to the Plug & Play\narchitecture. These additional discriminators are trained to discern real and\nfake latent codes, which are the output of the encoder using genuine and\ngenerated inputs, respectively. We proceed to investigate whether this approach\nis viable. Experiments conducted for the MNIST manifold show that this indeed\nis the case. \n\n"}
{"id": "1804.00645", "contents": "Title: Universal Planning Networks Abstract: A key challenge in complex visuomotor control is learning abstract\nrepresentations that are effective for specifying goals, planning, and\ngeneralization. To this end, we introduce universal planning networks (UPN).\nUPNs embed differentiable planning within a goal-directed policy. This planning\ncomputation unrolls a forward model in a latent space and infers an optimal\naction plan through gradient descent trajectory optimization. The\nplan-by-gradient-descent process and its underlying representations are learned\nend-to-end to directly optimize a supervised imitation learning objective. We\nfind that the representations learned are not only effective for goal-directed\nvisual imitation via gradient-based trajectory optimization, but can also\nprovide a metric for specifying goals using images. The learned representations\ncan be leveraged to specify distance-based rewards to reach new target states\nfor model-free reinforcement learning, resulting in substantially more\neffective learning when solving new tasks described via image-based goals. We\nwere able to achieve successful transfer of visuomotor planning strategies\nacross robots with significantly different morphologies and actuation\ncapabilities. \n\n"}
{"id": "1804.00792", "contents": "Title: Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks Abstract: Data poisoning is an attack on machine learning models wherein the attacker\nadds examples to the training set to manipulate the behavior of the model at\ntest time. This paper explores poisoning attacks on neural nets. The proposed\nattacks use \"clean-labels\"; they don't require the attacker to have any control\nover the labeling of training data. They are also targeted; they control the\nbehavior of the classifier on a $\\textit{specific}$ test instance without\ndegrading overall classifier performance. For example, an attacker could add a\nseemingly innocuous image (that is properly labeled) to a training set for a\nface recognition engine, and control the identity of a chosen person at test\ntime. Because the attacker does not need to control the labeling function,\npoisons could be entered into the training set simply by leaving them on the\nweb and waiting for them to be scraped by a data collection bot.\n  We present an optimization-based method for crafting poisons, and show that\njust one single poison image can control classifier behavior when transfer\nlearning is used. For full end-to-end training, we present a \"watermarking\"\nstrategy that makes poisoning reliable using multiple ($\\approx$50) poisoned\ntraining instances. We demonstrate our method by generating poisoned frog\nimages from the CIFAR dataset and using them to manipulate image classifiers. \n\n"}
{"id": "1804.00819", "contents": "Title: End-to-End Dense Video Captioning with Masked Transformer Abstract: Dense video captioning aims to generate text descriptions for all events in\nan untrimmed video. This involves both detecting and describing events.\nTherefore, all previous methods on dense video captioning tackle this problem\nby building two models, i.e. an event proposal and a captioning model, for\nthese two sub-problems. The models are either trained separately or in\nalternation. This prevents direct influence of the language description to the\nevent proposal, which is important for generating accurate descriptions. To\naddress this problem, we propose an end-to-end transformer model for dense\nvideo captioning. The encoder encodes the video into appropriate\nrepresentations. The proposal decoder decodes from the encoding with different\nanchors to form video event proposals. The captioning decoder employs a masking\nnetwork to restrict its attention to the proposal event over the encoding\nfeature. This masking network converts the event proposal to a differentiable\nmask, which ensures the consistency between the proposal and captioning during\ntraining. In addition, our model employs a self-attention mechanism, which\nenables the use of efficient non-recurrent structure during encoding and leads\nto performance improvements. We demonstrate the effectiveness of this\nend-to-end model on ActivityNet Captions and YouCookII datasets, where we\nachieved 10.12 and 6.58 METEOR score, respectively. \n\n"}
{"id": "1804.00946", "contents": "Title: Unsupervised Learning of Sequence Representations by Autoencoders Abstract: Sequence data is challenging for machine learning approaches, because the\nlengths of the sequences may vary between samples. In this paper, we present an\nunsupervised learning model for sequence data, called the Integrated Sequence\nAutoencoder (ISA), to learn a fixed-length vectorial representation by\nminimizing the reconstruction error. Specifically, we propose to integrate two\nclassical mechanisms for sequence reconstruction which takes into account both\nthe global silhouette information and the local temporal dependencies.\nFurthermore, we propose a stop feature that serves as a temporal stamp to guide\nthe reconstruction process, which results in a higher-quality representation.\nThe learned representation is able to effectively summarize not only the\napparent features, but also the underlying and high-level style information.\nTake for example a speech sequence sample: our ISA model can not only recognize\nthe spoken text (apparent feature), but can also discriminate the speaker who\nutters the audio (more high-level style). One promising application of the ISA\nmodel is that it can be readily used in the semi-supervised learning scenario,\nin which a large amount of unlabeled data is leveraged to extract high-quality\nsequence representations and thus to improve the performance of the subsequent\nsupervised learning tasks on limited labeled data. \n\n"}
{"id": "1804.02088", "contents": "Title: Question Type Guided Attention in Visual Question Answering Abstract: Visual Question Answering (VQA) requires integration of feature maps with\ndrastically different structures and focus of the correct regions. Image\ndescriptors have structures at multiple spatial scales, while lexical inputs\ninherently follow a temporal sequence and naturally cluster into semantically\ndifferent question types. A lot of previous works use complex models to extract\nfeature representations but neglect to use high-level information summary such\nas question types in learning. In this work, we propose Question Type-guided\nAttention (QTA). It utilizes the information of question type to dynamically\nbalance between bottom-up and top-down visual features, respectively extracted\nfrom ResNet and Faster R-CNN networks. We experiment with multiple VQA\narchitectures with extensive input ablation studies over the TDIUC dataset and\nshow that QTA systematically improves the performance by more than 5% across\nmultiple question type categories such as \"Activity Recognition\", \"Utility\" and\n\"Counting\" on TDIUC dataset. By adding QTA on the state-of-art model MCB, we\nachieve 3% improvement for overall accuracy. Finally, we propose a multi-task\nextension to predict question types which generalizes QTA to applications that\nlack of question type, with minimal performance loss. \n\n"}
{"id": "1804.02119", "contents": "Title: Impact of ultrasound image reconstruction method on breast lesion\n  classification with neural transfer learning Abstract: Deep learning algorithms, especially convolutional neural networks, have\nbecome a methodology of choice in medical image analysis. However, recent\nstudies in computer vision show that even a small modification of input image\nintensities may cause a deep learning model to classify the image differently.\nIn medical imaging, the distribution of image intensities is related to applied\nimage reconstruction algorithm. In this paper we investigate the impact of\nultrasound image reconstruction method on breast lesion classification with\nneural transfer learning. Due to high dynamic range raw ultrasonic signals are\ncommonly compressed in order to reconstruct B-mode images. Based on raw data\nacquired from breast lesions, we reconstruct B-mode images using different\ncompression levels. Next, transfer learning is applied for classification.\nDifferently reconstructed images are employed for training and evaluation. We\nshow that the modification of the reconstruction algorithm leads to decrease of\nclassification performance. As a remedy, we propose a method of data\naugmentation. We show that the augmentation of the training set with\ndifferently reconstructed B-mode images leads to a more robust and efficient\nclassification. Our study suggests that it is important to take into account\nimage reconstruction algorithms implemented in medical scanners during\ndevelopment of computer aided diagnosis systems. \n\n"}
{"id": "1804.03032", "contents": "Title: Approximate k-NN Graph Construction: a Generic Online Approach Abstract: Nearest neighbor search and k-nearest neighbor graph construction are two\nfundamental issues arise from many disciplines such as multimedia information\nretrieval, data-mining and machine learning. They become more and more imminent\ngiven the big data emerge in various fields in recent years. In this paper, a\nsimple but effective solution both for approximate k-nearest neighbor search\nand approximate k-nearest neighbor graph construction is presented. These two\nissues are addressed jointly in our solution. On the one hand, the approximate\nk-nearest neighbor graph construction is treated as a search task. Each sample\nalong with its k-nearest neighbors are joined into the k-nearest neighbor graph\nby performing the nearest neighbor search sequentially on the graph under\nconstruction. On the other hand, the built k-nearest neighbor graph is used to\nsupport k-nearest neighbor search. Since the graph is built online, the dynamic\nupdate on the graph, which is not possible from most of the existing solutions,\nis supported. This solution is feasible for various distance measures. Its\neffectiveness both as k-nearest neighbor construction and k-nearest neighbor\nsearch approaches is verified across different types of data in different\nscales, various dimensions and under different metrics. \n\n"}
{"id": "1804.03131", "contents": "Title: Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning Abstract: This paper tackles the problem of video object segmentation, given some user\nannotation which indicates the object of interest. The problem is formulated as\npixel-wise retrieval in a learned embedding space: we embed pixels of the same\nobject instance into the vicinity of each other, using a fully convolutional\nnetwork trained by a modified triplet loss as the embedding model. Then the\nannotated pixels are set as reference and the rest of the pixels are classified\nusing a nearest-neighbor approach. The proposed method supports different kinds\nof user input such as segmentation mask in the first frame (semi-supervised\nscenario), or a sparse set of clicked points (interactive scenario). In the\nsemi-supervised scenario, we achieve results competitive with the state of the\nart but at a fraction of computation cost (275 milliseconds per frame). In the\ninteractive scenario where the user is able to refine their input iteratively,\nthe proposed method provides instant response to each input, and reaches\ncomparable quality to competing methods with much less interaction. \n\n"}
{"id": "1804.03492", "contents": "Title: PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place\n  Recognition Abstract: Unlike its image based counterpart, point cloud based retrieval for place\nrecognition has remained as an unexplored and unsolved problem. This is largely\ndue to the difficulty in extracting local feature descriptors from a point\ncloud that can subsequently be encoded into a global descriptor for the\nretrieval task. In this paper, we propose the PointNetVLAD where we leverage on\nthe recent success of deep networks to solve point cloud based retrieval for\nplace recognition. Specifically, our PointNetVLAD is a combination/modification\nof the existing PointNet and NetVLAD, which allows end-to-end training and\ninference to extract the global descriptor from a given 3D point cloud.\nFurthermore, we propose the \"lazy triplet and quadruplet\" loss functions that\ncan achieve more discriminative and generalizable global descriptors to tackle\nthe retrieval task. We create benchmark datasets for point cloud based\nretrieval for place recognition, and the experimental results on these datasets\nshow the feasibility of our PointNetVLAD. Our code and the link for the\nbenchmark dataset downloads are available in our project website.\nhttp://github.com/mikacuy/pointnetvlad/ \n\n"}
{"id": "1804.03596", "contents": "Title: A Deep Information Sharing Network for Multi-contrast Compressed Sensing\n  MRI Reconstruction Abstract: In multi-contrast magnetic resonance imaging (MRI), compressed sensing theory\ncan accelerate imaging by sampling fewer measurements within each contrast. The\nconventional optimization-based models suffer several limitations: strict\nassumption of shared sparse support, time-consuming optimization and \"shallow\"\nmodels with difficulties in encoding the rich patterns hiding in massive MRI\ndata. In this paper, we propose the first deep learning model for\nmulti-contrast MRI reconstruction. We achieve information sharing through\nfeature sharing units, which significantly reduces the number of parameters.\nThe feature sharing unit is combined with a data fidelity unit to comprise an\ninference block. These inference blocks are cascaded with dense connections,\nwhich allows for information transmission across different depths of the\nnetwork efficiently. Our extensive experiments on various multi-contrast MRI\ndatasets show that proposed model outperforms both state-of-the-art\nsingle-contrast and multi-contrast MRI methods in accuracy and efficiency. We\nshow the improved reconstruction quality can bring great benefits for the later\nmedical image analysis stage. Furthermore, the robustness of the proposed model\nto the non-registration environment shows its potential in real MRI\napplications. \n\n"}
{"id": "1804.03619", "contents": "Title: Looking to Listen at the Cocktail Party: A Speaker-Independent\n  Audio-Visual Model for Speech Separation Abstract: We present a joint audio-visual model for isolating a single speech signal\nfrom a mixture of sounds such as other speakers and background noise. Solving\nthis task using only audio as input is extremely challenging and does not\nprovide an association of the separated speech signals with speakers in the\nvideo. In this paper, we present a deep network-based model that incorporates\nboth visual and auditory signals to solve this task. The visual features are\nused to \"focus\" the audio on desired speakers in a scene and to improve the\nspeech separation quality. To train our joint audio-visual model, we introduce\nAVSpeech, a new dataset comprised of thousands of hours of video segments from\nthe Web. We demonstrate the applicability of our method to classic speech\nseparation tasks, as well as real-world scenarios involving heated interviews,\nnoisy bars, and screaming children, only requiring the user to specify the face\nof the person in the video whose speech they want to isolate. Our method shows\nclear advantage over state-of-the-art audio-only speech separation in cases of\nmixed speech. In addition, our model, which is speaker-independent (trained\nonce, applicable to any speaker), produces better results than recent\naudio-visual speech separation methods that are speaker-dependent (require\ntraining a separate model for each speaker of interest). \n\n"}
{"id": "1804.03641", "contents": "Title: Audio-Visual Scene Analysis with Self-Supervised Multisensory Features Abstract: The thud of a bouncing ball, the onset of speech as lips open -- when visual\nand audio events occur together, it suggests that there might be a common,\nunderlying event that produced both signals. In this paper, we argue that the\nvisual and audio components of a video signal should be modeled jointly using a\nfused multisensory representation. We propose to learn such a representation in\na self-supervised way, by training a neural network to predict whether video\nframes and audio are temporally aligned. We use this learned representation for\nthree applications: (a) sound source localization, i.e. visualizing the source\nof sound in a video; (b) audio-visual action recognition; and (c) on/off-screen\naudio source separation, e.g. removing the off-screen translator's voice from a\nforeign official's speech. Code, models, and video results are available on our\nwebpage: http://andrewowens.com/multisensory \n\n"}
{"id": "1804.03821", "contents": "Title: ExFuse: Enhancing Feature Fusion for Semantic Segmentation Abstract: Modern semantic segmentation frameworks usually combine low-level and\nhigh-level features from pre-trained backbone convolutional models to boost\nperformance. In this paper, we first point out that a simple fusion of\nlow-level and high-level features could be less effective because of the gap in\nsemantic levels and spatial resolution. We find that introducing semantic\ninformation into low-level features and high-resolution details into high-level\nfeatures is more effective for the later fusion. Based on this observation, we\npropose a new framework, named ExFuse, to bridge the gap between low-level and\nhigh-level features thus significantly improve the segmentation quality by\n4.0\\% in total. Furthermore, we evaluate our approach on the challenging PASCAL\nVOC 2012 segmentation benchmark and achieve 87.9\\% mean IoU, which outperforms\nthe previous state-of-the-art results. \n\n"}
{"id": "1804.04340", "contents": "Title: Zero-Shot Object Detection Abstract: We introduce and tackle the problem of zero-shot object detection (ZSD),\nwhich aims to detect object classes which are not observed during training. We\nwork with a challenging set of object classes, not restricting ourselves to\nsimilar and/or fine-grained categories as in prior works on zero-shot\nclassification. We present a principled approach by first adapting\nvisual-semantic embeddings for ZSD. We then discuss the problems associated\nwith selecting a background class and motivate two background-aware approaches\nfor learning robust detectors. One of these models uses a fixed background\nclass and the other is based on iterative latent assignments. We also outline\nthe challenge associated with using a limited number of training classes and\npropose a solution based on dense sampling of the semantic label space using\nauxiliary data with a large number of categories. We propose novel splits of\ntwo standard detection datasets - MSCOCO and VisualGenome, and present\nextensive empirical results in both the traditional and generalized zero-shot\nsettings to highlight the benefits of the proposed methods. We provide useful\ninsights into the algorithm and conclude by posing some open questions to\nencourage further research. \n\n"}
{"id": "1804.04563", "contents": "Title: Towards integrating spatial localization in convolutional neural\n  networks for brain image segmentation Abstract: Semantic segmentation is an established while rapidly evolving field in\nmedical imaging. In this paper we focus on the segmentation of brain Magnetic\nResonance Images (MRI) into cerebral structures using convolutional neural\nnetworks (CNN). CNNs achieve good performance by finding effective high\ndimensional image features describing the patch content only. In this work, we\npropose different ways to introduce spatial constraints into the network to\nfurther reduce prediction inconsistencies.\n  A patch based CNN architecture was trained, making use of multiple scales to\ngather contextual information. Spatial constraints were introduced within the\nCNN through a distance to landmarks feature or through the integration of a\nprobability atlas. We demonstrate experimentally that using spatial information\nhelps to reduce segmentation inconsistencies. \n\n"}
{"id": "1804.04604", "contents": "Title: Discovery and usage of joint attention in images Abstract: Joint visual attention is characterized by two or more individuals looking at\na common target at the same time. The ability to identify joint attention in\nscenes, the people involved, and their common target, is fundamental to the\nunderstanding of social interactions, including others' intentions and goals.\nIn this work we deal with the extraction of joint attention events, and the use\nof such events for image descriptions. The work makes two novel contributions.\nFirst, our extraction algorithm is the first which identifies joint visual\nattention in single static images. It computes 3D gaze direction, identifies\nthe gaze target by combining gaze direction with a 3D depth map computed for\nthe image, and identifies the common gaze target. Second, we use a human study\nto demonstrate the sensitivity of humans to joint attention, suggesting that\nthe detection of such a configuration in an image can be useful for\nunderstanding the image, including the goals of the agents and their joint\nactivity, and therefore can contribute to image captioning and related tasks. \n\n"}
{"id": "1804.04687", "contents": "Title: Cross-Domain Visual Recognition via Domain Adaptive Dictionary Learning Abstract: In real-world visual recognition problems, the assumption that the training\ndata (source domain) and test data (target domain) are sampled from the same\ndistribution is often violated. This is known as the domain adaptation problem.\nIn this work, we propose a novel domain-adaptive dictionary learning framework\nfor cross-domain visual recognition. Our method generates a set of intermediate\ndomains. These intermediate domains form a smooth path and bridge the gap\nbetween the source and target domains. Specifically, we not only learn a common\ndictionary to encode the domain-shared features, but also learn a set of\ndomain-specific dictionaries to model the domain shift. The separation of the\ncommon and domain-specific dictionaries enables us to learn more compact and\nreconstructive dictionaries for domain adaptation. These dictionaries are\nlearned by alternating between domain-adaptive sparse coding and dictionary\nupdating steps. Meanwhile, our approach gradually recovers the feature\nrepresentations of both source and target data along the domain path. By\naligning all the recovered domain data, we derive the final domain-adaptive\nfeatures for cross-domain visual recognition. Extensive experiments on three\npublic datasets demonstrates that our approach outperforms most\nstate-of-the-art methods. \n\n"}
{"id": "1804.04804", "contents": "Title: Learning Deep Sketch Abstraction Abstract: Human free-hand sketches have been studied in various contexts including\nsketch recognition, synthesis and fine-grained sketch-based image retrieval\n(FG-SBIR). A fundamental challenge for sketch analysis is to deal with\ndrastically different human drawing styles, particularly in terms of\nabstraction level. In this work, we propose the first stroke-level sketch\nabstraction model based on the insight of sketch abstraction as a process of\ntrading off between the recognizability of a sketch and the number of strokes\nused to draw it. Concretely, we train a model for abstract sketch generation\nthrough reinforcement learning of a stroke removal policy that learns to\npredict which strokes can be safely removed without affecting recognizability.\nWe show that our abstraction model can be used for various sketch analysis\ntasks including: (1) modeling stroke saliency and understanding the decision of\nsketch recognition models, (2) synthesizing sketches of variable abstraction\nfor a given category, or reference object instance in a photo, and (3) training\na FG-SBIR model with photos only, bypassing the expensive photo-sketch pair\ncollection step. \n\n"}
{"id": "1804.05905", "contents": "Title: Tree Morphology for Phenotyping from Semantics-Based Mapping in Orchard\n  Environments Abstract: Measuring tree morphology for phenotyping is an essential but labor-intensive\nactivity in horticulture. Researchers often rely on manual measurements which\nmay not be accurate for example when measuring tree volume. Recent approaches\non automating the measurement process rely on LIDAR measurements coupled with\nhigh-accuracy GPS. Usually each side of a row is reconstructed independently\nand then merged using GPS information. Such approaches have two disadvantages:\n(1) they rely on specialized and expensive equipment, and (2) since the\nreconstruction process does not simultaneously use information from both sides,\nside reconstructions may not be accurate. We also show that standard loop\nclosure methods do not necessarily align tree trunks well. In this paper, we\npresent a novel vision system that employs only an RGB-D camera to estimate\nmorphological parameters. A semantics-based mapping algorithm merges the\ntwo-sides 3D models of tree rows, where integrated semantic information is\nobtained and refined by robust fitting algorithms. We focus on measuring tree\nheight, canopy volume and trunk diameter from the optimized 3D model.\nExperiments conducted in real orchards quantitatively demonstrate the accuracy\nof our method. \n\n"}
{"id": "1804.06039", "contents": "Title: Real-Time Rotation-Invariant Face Detection with Progressive Calibration\n  Networks Abstract: Rotation-invariant face detection, i.e. detecting faces with arbitrary\nrotation-in-plane (RIP) angles, is widely required in unconstrained\napplications but still remains as a challenging task, due to the large\nvariations of face appearances. Most existing methods compromise with speed or\naccuracy to handle the large RIP variations. To address this problem more\nefficiently, we propose Progressive Calibration Networks (PCN) to perform\nrotation-invariant face detection in a coarse-to-fine manner. PCN consists of\nthree stages, each of which not only distinguishes the faces from non-faces,\nbut also calibrates the RIP orientation of each face candidate to upright\nprogressively. By dividing the calibration process into several progressive\nsteps and only predicting coarse orientations in early stages, PCN can achieve\nprecise and fast calibration. By performing binary classification of face vs.\nnon-face with gradually decreasing RIP ranges, PCN can accurately detect faces\nwith full $360^{\\circ}$ RIP angles. Such designs lead to a real-time\nrotation-invariant face detector. The experiments on multi-oriented FDDB and a\nchallenging subset of WIDER FACE containing rotated faces in the wild show that\nour PCN achieves quite promising performance. \n\n"}
{"id": "1804.06248", "contents": "Title: PM-GANs: Discriminative Representation Learning for Action Recognition\n  Using Partial-modalities Abstract: Data of different modalities generally convey complimentary but heterogeneous\ninformation, and a more discriminative representation is often preferred by\ncombining multiple data modalities like the RGB and infrared features. However\nin reality, obtaining both data channels is challenging due to many\nlimitations. For example, the RGB surveillance cameras are often restricted\nfrom private spaces, which is in conflict with the need of abnormal activity\ndetection for personal security. As a result, using partial data channels to\nbuild a full representation of multi-modalities is clearly desired. In this\npaper, we propose a novel Partial-modal Generative Adversarial Networks\n(PM-GANs) that learns a full-modal representation using data from only partial\nmodalities. The full representation is achieved by a generated representation\nin place of the missing data channel. Extensive experiments are conducted to\nverify the performance of our proposed method on action recognition, compared\nwith four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset\nfor action recognition is introduced, and will be the first publicly available\naction dataset that contains paired infrared and visible spectrum. \n\n"}
{"id": "1804.06579", "contents": "Title: Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines Abstract: We present a semi-supervised co-analysis method for learning 3D shape styles\nfrom projected feature lines, achieving style patch localization with only weak\nsupervision. Given a collection of 3D shapes spanning multiple object\ncategories and styles, we perform style co-analysis over projected feature\nlines of each 3D shape and then backproject the learned style features onto the\n3D shapes. Our core analysis pipeline starts with mid-level patch sampling and\npre-selection of candidate style patches. Projective features are then encoded\nvia patch convolution. Multi-view feature integration and style clustering are\ncarried out under the framework of partially shared latent factor (PSLF)\nlearning, a multi-view feature learning scheme. PSLF achieves effective\nmulti-view feature fusion by distilling and exploiting consistent and\ncomplementary feature information from multiple views, while also selecting\nstyle patches from the candidates. Our style analysis approach supports both\nunsupervised and semi-supervised analysis. For the latter, our method accepts\nboth user-specified shape labels and style-ranked triplets as clustering\nconstraints.We demonstrate results from 3D shape style analysis and patch\nlocalization as well as improvements over state-of-the-art methods. We also\npresent several applications enabled by our style analysis. \n\n"}
{"id": "1804.07056", "contents": "Title: Now you see me: evaluating performance in long-term visual tracking Abstract: We propose a new long-term tracking performance evaluation methodology and\npresent a new challenging dataset of carefully selected sequences with many\ntarget disappearances. We perform an extensive evaluation of six long-term and\nnine short-term state-of-the-art trackers, using new performance measures,\nsuitable for evaluating long-term tracking - tracking precision, recall and\nF-score. The evaluation shows that a good model update strategy and the\ncapability of image-wide re-detection are critical for long-term tracking\nperformance. We integrated the methodology in the VOT toolkit to automate\nexperimental analysis and benchmarking and to facilitate the development of\nlong-term trackers. \n\n"}
{"id": "1804.07062", "contents": "Title: Attacking Convolutional Neural Network using Differential Evolution Abstract: The output of Convolutional Neural Networks (CNN) has been shown to be\ndiscontinuous which can make the CNN image classifier vulnerable to small\nwell-tuned artificial perturbations. That is, images modified by adding such\nperturbations(i.e. adversarial perturbations) that make little difference to\nhuman eyes, can completely alter the CNN classification results. In this paper,\nwe propose a practical attack using differential evolution(DE) for generating\neffective adversarial perturbations. We comprehensively evaluate the\neffectiveness of different types of DEs for conducting the attack on different\nnetwork structures. The proposed method is a black-box attack which only\nrequires the miracle feedback of the target CNN systems. The results show that\nunder strict constraints which simultaneously control the number of pixels\nchanged and overall perturbation strength, attacking can achieve 72.29%, 78.24%\nand 61.28% non-targeted attack success rates, with 88.68%, 99.85% and 73.07%\nconfidence on average, on three common types of CNNs. The attack only requires\nmodifying 5 pixels with 20.44, 14.76 and 22.98 pixel values distortion. Thus,\nthe result shows that the current DNNs are also vulnerable to such simpler\nblack-box attacks even under very limited attack conditions. \n\n"}
{"id": "1804.07514", "contents": "Title: An Approximate Shading Model with Detail Decomposition for Object\n  Relighting Abstract: We present an object relighting system that allows an artist to select an\nobject from an image and insert it into a target scene. Through simple\ninteractions, the system can adjust illumination on the inserted object so that\nit appears naturally in the scene. To support image-based relighting, we build\nobject model from the image, and propose a \\emph{perceptually-inspired}\napproximate shading model for the relighting. It decomposes the shading field\ninto (a) a rough shape term that can be reshaded, (b) a parametric shading\ndetail that encodes missing features from the first term, and (c) a geometric\ndetail term that captures fine-scale material properties. With this\ndecomposition, the shading model combines 3D rendering and image-based\ncomposition and allows more flexible compositing than image-based methods.\nQuantitative evaluation and a set of user studies suggest our method is a\npromising alternative to existing methods of object insertion. \n\n"}
{"id": "1804.08046", "contents": "Title: First Impressions: A Survey on Vision-Based Apparent Personality Trait\n  Analysis Abstract: Personality analysis has been widely studied in psychology, neuropsychology,\nand signal processing fields, among others. From the past few years, it also\nbecame an attractive research area in visual computing. From the computational\npoint of view, by far speech and text have been the most considered cues of\ninformation for analyzing personality. However, recently there has been an\nincreasing interest from the computer vision community in analyzing personality\nfrom visual data. Recent computer vision approaches are able to accurately\nanalyze human faces, body postures and behaviors, and use these information to\ninfer apparent personality traits. Because of the overwhelming research\ninterest in this topic, and of the potential impact that this sort of methods\ncould have in society, we present in this paper an up-to-date review of\nexisting vision-based approaches for apparent personality trait recognition. We\ndescribe seminal and cutting edge works on the subject, discussing and\ncomparing their distinctive features and limitations. Future venues of research\nin the field are identified and discussed. Furthermore, aspects on the\nsubjectivity in data labeling/evaluation, as well as current datasets and\nchallenges organized to push the research on the field are reviewed. \n\n"}
{"id": "1804.08275", "contents": "Title: Deep Semantic Hashing with Generative Adversarial Networks Abstract: Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models. \n\n"}
{"id": "1804.08758", "contents": "Title: Switchable Temporal Propagation Network Abstract: Videos contain highly redundant information between frames. Such redundancy\nhas been extensively studied in video compression and encoding, but is less\nexplored for more advanced video processing. In this paper, we propose a\nlearnable unified framework for propagating a variety of visual properties of\nvideo images, including but not limited to color, high dynamic range (HDR), and\nsegmentation information, where the properties are available for only a few\nkey-frames. Our approach is based on a temporal propagation network (TPN),\nwhich models the transition-related affinity between a pair of frames in a\npurely data-driven manner. We theoretically prove two essential factors for\nTPN: (a) by regularizing the global transformation matrix as orthogonal, the\n\"style energy\" of the property can be well preserved during propagation; (b)\nsuch regularization can be achieved by the proposed switchable TPN with\nbi-directional training on pairs of frames. We apply the switchable TPN to\nthree tasks: colorizing a gray-scale video based on a few color key-frames,\ngenerating an HDR video from a low dynamic range (LDR) video and a few HDR\nframes, and propagating a segmentation mask from the first frame in videos.\nExperimental results show that our approach is significantly more accurate and\nefficient than the state-of-the-art methods. \n\n"}
{"id": "1804.08758", "contents": "Title: Switchable Temporal Propagation Network Abstract: Videos contain highly redundant information between frames. Such redundancy\nhas been extensively studied in video compression and encoding, but is less\nexplored for more advanced video processing. In this paper, we propose a\nlearnable unified framework for propagating a variety of visual properties of\nvideo images, including but not limited to color, high dynamic range (HDR), and\nsegmentation information, where the properties are available for only a few\nkey-frames. Our approach is based on a temporal propagation network (TPN),\nwhich models the transition-related affinity between a pair of frames in a\npurely data-driven manner. We theoretically prove two essential factors for\nTPN: (a) by regularizing the global transformation matrix as orthogonal, the\n\"style energy\" of the property can be well preserved during propagation; (b)\nsuch regularization can be achieved by the proposed switchable TPN with\nbi-directional training on pairs of frames. We apply the switchable TPN to\nthree tasks: colorizing a gray-scale video based on a few color key-frames,\ngenerating an HDR video from a low dynamic range (LDR) video and a few HDR\nframes, and propagating a segmentation mask from the first frame in videos.\nExperimental results show that our approach is significantly more accurate and\nefficient than the state-of-the-art methods. \n\n"}
{"id": "1804.09235", "contents": "Title: On the effectiveness of task granularity for transfer learning Abstract: We describe a DNN for video classification and captioning, trained\nend-to-end, with shared features, to solve tasks at different levels of\ngranularity, exploring the link between granularity in a source task and the\nquality of learned features for transfer learning. For solving the new task\ndomain in transfer learning, we freeze the trained encoder and fine-tune a\nneural net on the target domain. We train on the Something-Something dataset\nwith over 220, 000 videos, and multiple levels of target granularity, including\n50 action groups, 174 fine-grained action categories and captions.\nClassification and captioning with Something-Something are challenging because\nof the subtle differences between actions, applied to thousands of different\nobject classes, and the diversity of captions penned by crowd actors. Our model\nperforms better than existing classification baselines for SomethingSomething,\nwith impressive fine-grained results. And it yields a strong baseline on the\nnew Something-Something captioning task. Experiments reveal that training with\nmore fine-grained tasks tends to produce better features for transfer learning. \n\n"}
{"id": "1804.10172", "contents": "Title: Capsule networks for low-data transfer learning Abstract: We propose a capsule network-based architecture for generalizing learning to\nnew data with few examples. Using both generative and non-generative capsule\nnetworks with intermediate routing, we are able to generalize to new\ninformation over 25 times faster than a similar convolutional neural network.\nWe train the networks on the multiMNIST dataset lacking one digit. After the\nnetworks reach their maximum accuracy, we inject 1-100 examples of the missing\ndigit into the training set, and measure the number of batches needed to return\nto a comparable level of accuracy. We then discuss the improvement in low-data\ntransfer learning that capsule networks bring, and propose future directions\nfor capsule research. \n\n"}
{"id": "1804.10805", "contents": "Title: Remote Detection of Idling Cars Using Infrared Imaging and Deep Networks Abstract: Idling vehicles waste energy and pollute the environment through exhaust\nemission. In some countries, idling a vehicle for more than a predefined\nduration is prohibited and automatic idling vehicle detection is desirable for\nlaw enforcement. We propose the first automatic system to detect idling cars,\nusing infrared (IR) imaging and deep networks.\n  We rely on the differences in spatio-temporal heat signatures of idling and\nstopped cars and monitor the car temperature with a long-wavelength IR camera.\nWe formulate the idling car detection problem as spatio-temporal event\ndetection in IR image sequences and employ deep networks for spatio-temporal\nmodeling. We collected the first IR image sequence dataset for idling car\ndetection. First, we detect the cars in each IR image using a convolutional\nneural network, which is pre-trained on regular RGB images and fine-tuned on IR\nimages for higher accuracy. Then, we track the detected cars over time to\nidentify the cars that are parked. Finally, we use the 3D spatio-temporal IR\nimage volume of each parked car as input to convolutional and recurrent\nnetworks to classify them as idling or not. We carried out an extensive\nempirical evaluation of temporal and spatio-temporal modeling approaches with\nvarious convolutional and recurrent architectures. We present promising\nexperimental results on our IR image sequence dataset. \n\n"}
{"id": "1804.11332", "contents": "Title: On the iterative refinement of densely connected representation levels\n  for semantic segmentation Abstract: State-of-the-art semantic segmentation approaches increase the receptive\nfield of their models by using either a downsampling path composed of\npoolings/strided convolutions or successive dilated convolutions. However, it\nis not clear which operation leads to best results. In this paper, we\nsystematically study the differences introduced by distinct receptive field\nenlargement methods and their impact on the performance of a novel\narchitecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a\ndensely connected backbone composed of residual networks. Following standard\nimage segmentation architectures, receptive field enlargement operations that\nchange the representation level are interleaved among residual networks. This\nallows the model to exploit the benefits of both residual and dense\nconnectivity patterns, namely: gradient flow, iterative refinement of\nrepresentations, multi-scale feature combination and deep supervision. In order\nto highlight the potential of our model, we test it on the challenging CamVid\nurban scene understanding benchmark and make the following observations: 1)\ndownsampling operations outperform dilations when the model is trained from\nscratch, 2) dilations are useful during the finetuning step of the model, 3)\ncoarser representations require less refinement steps, and 4) ResNets (by model\nconstruction) are good regularizers, since they can reduce the model capacity\nwhen needed. Finally, we compare our architecture to alternative methods and\nreport state-of-the-art result on the Camvid dataset, with at least twice fewer\nparameters. \n\n"}
{"id": "1805.00313", "contents": "Title: Neural Compatibility Modeling with Attentive Knowledge Distillation Abstract: Recently, the booming fashion sector and its huge potential benefits have\nattracted tremendous attention from many research communities. In particular,\nincreasing research efforts have been dedicated to the complementary clothing\nmatching as matching clothes to make a suitable outfit has become a daily\nheadache for many people, especially those who do not have the sense of\naesthetics. Thanks to the remarkable success of neural networks in various\napplications such as image classification and speech recognition, the\nresearchers are enabled to adopt the data-driven learning methods to analyze\nfashion items. Nevertheless, existing studies overlook the rich valuable\nknowledge (rules) accumulated in fashion domain, especially the rules regarding\nclothing matching. Towards this end, in this work, we shed light on\ncomplementary clothing matching by integrating the advanced deep neural\nnetworks and the rich fashion domain knowledge. Considering that the rules can\nbe fuzzy and different rules may have different confidence levels to different\nsamples, we present a neural compatibility modeling scheme with attentive\nknowledge distillation based on the teacher-student network scheme. Extensive\nexperiments on the real-world dataset show the superiority of our model over\nseveral state-of-the-art baselines. Based upon the comparisons, we observe\ncertain fashion insights that add value to the fashion matching study. As a\nbyproduct, we released the codes, and involved parameters to benefit other\nresearchers. \n\n"}
{"id": "1805.00324", "contents": "Title: A Deep Face Identification Network Enhanced by Facial Attributes\n  Prediction Abstract: In this paper, we propose a new deep framework which predicts facial\nattributes and leverage it as a soft modality to improve face identification\nperformance. Our model is an end to end framework which consists of a\nconvolutional neural network (CNN) whose output is fanned out into two separate\nbranches; the first branch predicts facial attributes while the second branch\nidentifies face images. Contrary to the existing multi-task methods which only\nuse a shared CNN feature space to train these two tasks jointly, we fuse the\npredicted attributes with the features from the face modality in order to\nimprove the face identification performance. Experimental results show that our\nmodel brings benefits to both face identification as well as facial attribute\nprediction performance, especially in the case of identity facial attributes\nsuch as gender prediction. We tested our model on two standard datasets\nannotated by identities and face attributes. Experimental results indicate that\nthe proposed model outperforms most of the current existing face identification\nand attribute prediction methods. \n\n"}
{"id": "1805.00329", "contents": "Title: DeepDIVA: A Highly-Functional Python Framework for Reproducible\n  Experiments Abstract: We introduce DeepDIVA: an infrastructure designed to enable quick and\nintuitive setup of reproducible experiments with a large range of useful\nanalysis functionality. Reproducing scientific results can be a frustrating\nexperience, not only in document image analysis but in machine learning in\ngeneral. Using DeepDIVA a researcher can either reproduce a given experiment\nwith a very limited amount of information or share their own experiments with\nothers. Moreover, the framework offers a large range of functions, such as\nboilerplate code, keeping track of experiments, hyper-parameter optimization,\nand visualization of data and results. To demonstrate the effectiveness of this\nframework, this paper presents case studies in the area of handwritten document\nanalysis where researchers benefit from the integrated functionality. DeepDIVA\nis implemented in Python and uses the deep learning framework PyTorch. It is\ncompletely open source, and accessible as Web Service through DIVAServices. \n\n"}
{"id": "1805.00334", "contents": "Title: Deep learning approach to Fourier ptychographic microscopy Abstract: Convolutional neural networks (CNNs) have gained tremendous success in\nsolving complex inverse problems. The aim of this work is to develop a novel\nCNN framework to reconstruct video sequence of dynamic live cells captured\nusing a computational microscopy technique, Fourier ptychographic microscopy\n(FPM). The unique feature of the FPM is its capability to reconstruct images\nwith both wide field-of-view (FOV) and high resolution, i.e. a large\nspace-bandwidth-product (SBP), by taking a series of low resolution intensity\nimages. For live cell imaging, a single FPM frame contains thousands of cell\nsamples with different morphological features. Our idea is to fully exploit the\nstatistical information provided by this large spatial ensemble so as to make\npredictions in a sequential measurement, without using any additional temporal\ndataset. Specifically, we show that it is possible to reconstruct high-SBP\ndynamic cell videos by a CNN trained only on the first FPM dataset captured at\nthe beginning of a time-series experiment. Our CNN approach reconstructs a\n12800X10800 pixels phase image using only ~25 seconds, a 50X speedup compared\nto the model-based FPM algorithm. In addition, the CNN further reduces the\nrequired number of images in each time frame by ~6X. Overall, this\nsignificantly improves the imaging throughput by reducing both the acquisition\nand computational times. The proposed CNN is based on the conditional\ngenerative adversarial network (cGAN) framework. Additionally, we also exploit\ntransfer learning so that our pre-trained CNN can be further optimized to image\nother cell types. Our technique demonstrates a promising deep learning approach\nto continuously monitor large live-cell populations over an extended time and\ngather useful spatial and temporal information with sub-cellular resolution. \n\n"}
{"id": "1805.00348", "contents": "Title: OMG - Emotion Challenge Solution Abstract: This short paper describes our solution to the 2018 IEEE World Congress on\nComputational Intelligence One-Minute Gradual-Emotional Behavior Challenge,\nwhose goal was to estimate continuous arousal and valence values from short\nvideos. We designed four base regression models using visual and audio\nfeatures, and then used a spectral approach to fuse them to obtain improved\nperformance. \n\n"}
{"id": "1805.00980", "contents": "Title: SaaS: Speed as a Supervisor for Semi-supervised Learning Abstract: We introduce the SaaS Algorithm for semi-supervised learning, which uses\nlearning speed during stochastic gradient descent in a deep neural network to\nmeasure the quality of an iterative estimate of the posterior probability of\nunknown labels. Training speed in supervised learning correlates strongly with\nthe percentage of correct labels, so we use it as an inference criterion for\nthe unknown labels, without attempting to infer the model parameters at first.\nDespite its simplicity, SaaS achieves state-of-the-art results in\nsemi-supervised learning benchmarks. \n\n"}
{"id": "1805.01369", "contents": "Title: Framewise approach in multimodal emotion recognition in OMG challenge Abstract: In this report we described our approach achieves $53\\%$ of unweighted\naccuracy over $7$ emotions and $0.05$ and $0.09$ mean squared errors for\narousal and valence in OMG emotion recognition challenge. Our results were\nobtained with ensemble of single modality models trained on voice and face data\nfrom video separately. We consider each stream as a sequence of frames. Next we\nestimated features from frames and handle it with recurrent neural network. As\naudio frame we mean short $0.4$ second spectrogram interval. For features\nestimation for face pictures we used own ResNet neural network pretrained on\nAffectNet database. Each short spectrogram was considered as a picture and\nprocessed by convolutional network too. As a base audio model we used ResNet\npretrained in speaker recognition task. Predictions from both modalities were\nfused on decision level and improve single-channel approaches by a few percent \n\n"}
{"id": "1805.01556", "contents": "Title: Pixel-wise Attentional Gating for Parsimonious Pixel Labeling Abstract: To achieve parsimonious inference in per-pixel labeling tasks with a limited\ncomputational budget, we propose a \\emph{Pixel-wise Attentional Gating} unit\n(\\emph{PAG}) that learns to selectively process a subset of spatial locations\nat each layer of a deep convolutional network. PAG is a generic,\narchitecture-independent, problem-agnostic mechanism that can be readily\n\"plugged in\" to an existing model with fine-tuning. We utilize PAG in two ways:\n1) learning spatially varying pooling fields that improve model performance\nwithout the extra computation cost associated with multi-scale pooling, and 2)\nlearning a dynamic computation policy for each pixel to decrease total\ncomputation while maintaining accuracy.\n  We extensively evaluate PAG on a variety of per-pixel labeling tasks,\nincluding semantic segmentation, boundary detection, monocular depth and\nsurface normal estimation. We demonstrate that PAG allows competitive or\nstate-of-the-art performance on these tasks. Our experiments show that PAG\nlearns dynamic spatial allocation of computation over the input image which\nprovides better performance trade-offs compared to related approaches (e.g.,\ntruncating deep models or dynamically skipping whole layers). Generally, we\nobserve PAG can reduce computation by $10\\%$ without noticeable loss in\naccuracy and performance degrades gracefully when imposing stronger\ncomputational constraints. \n\n"}
{"id": "1805.01803", "contents": "Title: Unsupervised learning for concept detection in medical images: a\n  comparative analysis Abstract: As digital medical imaging becomes more prevalent and archives increase in\nsize, representation learning exposes an interesting opportunity for enhanced\nmedical decision support systems. On the other hand, medical imaging data is\noften scarce and short on annotations. In this paper, we present an assessment\nof unsupervised feature learning approaches for images in the biomedical\nliterature, which can be applied to automatic biomedical concept detection. Six\nunsupervised representation learning methods were built, including traditional\nbags of visual words, autoencoders, and generative adversarial networks. Each\nmodel was trained, and their respective feature space evaluated using images\nfrom the ImageCLEF 2017 concept detection task. We conclude that it is possible\nto obtain more powerful representations with modern deep learning approaches,\nin contrast with previously popular computer vision methods. Although\ngenerative adversarial networks can provide good results, they are harder to\nsucceed in highly varied data sets. The possibility of semi-supervised\nlearning, as well as their use in medical information retrieval problems, are\nthe next steps to be strongly considered. \n\n"}
{"id": "1805.02279", "contents": "Title: S4ND: Single-Shot Single-Scale Lung Nodule Detection Abstract: The state of the art lung nodule detection studies rely on computationally\nexpensive multi-stage frameworks to detect nodules from CT scans. To address\nthis computational challenge and provide better performance, in this paper we\npropose S4ND, a new deep learning based method for lung nodule detection. Our\napproach uses a single feed forward pass of a single network for detection and\nprovides better performance when compared to the current literature. The whole\ndetection pipeline is designed as a single $3D$ Convolutional Neural Network\n(CNN) with dense connections, trained in an end-to-end manner. S4ND does not\nrequire any further post-processing or user guidance to refine detection\nresults. Experimentally, we compared our network with the current\nstate-of-the-art object detection network (SSD) in computer vision as well as\nthe state-of-the-art published method for lung nodule detection (3D DCNN). We\nused publically available $888$ CT scans from LUNA challenge dataset and showed\nthat the proposed method outperforms the current literature both in terms of\nefficiency and accuracy by achieving an average FROC-score of $0.897$. We also\nprovide an in-depth analysis of our proposed network to shed light on the\nunclear paradigms of tiny object detection. \n\n"}
{"id": "1805.02850", "contents": "Title: Joint Cell Nuclei Detection and Segmentation in Microscopy Images Using\n  3D Convolutional Networks Abstract: We propose a 3D convolutional neural network to simultaneously segment and\ndetect cell nuclei in confocal microscopy images. Mirroring the co-dependency\nof these tasks, our proposed model consists of two serial components: the first\npart computes a segmentation of cell bodies, while the second module identifies\nthe centers of these cells. Our model is trained end-to-end from scratch on a\nmouse parotid salivary gland stem cell nuclei dataset comprising 107 image\nstacks from three independent cell preparations, each containing several\nhundred individual cell nuclei in 3D. In our experiments, we conduct a thorough\nevaluation of both detection accuracy and segmentation quality, on two\ndifferent datasets. The results show that the proposed method provides\nsignificantly improved detection and segmentation accuracy compared to\nstate-of-the-art and benchmark algorithms. Finally, we use a previously\ndescribed test-time drop-out strategy to obtain uncertainty estimates on our\npredictions and validate these estimates by demonstrating that they are\nstrongly correlated with accuracy. \n\n"}
{"id": "1805.03096", "contents": "Title: Fast Feature Extraction with CNNs with Pooling Layers Abstract: In recent years, many publications showed that convolutional neural network\nbased features can have a superior performance to engineered features. However,\nnot much effort was taken so far to extract local features efficiently for a\nwhole image. In this paper, we present an approach to compute patch-based local\nfeature descriptors efficiently in presence of pooling and striding layers for\nwhole images at once. Our approach is generic and can be applied to nearly all\nexisting network architectures. This includes networks for all local feature\nextraction tasks like camera calibration, Patchmatching, optical flow\nestimation and stereo matching. In addition, our approach can be applied to\nother patch-based approaches like sliding window object detection and\nrecognition. We complete our paper with a speed benchmark of popular CNN based\nfeature extraction approaches applied on a whole image, with and without our\nspeedup, and example code (for Torch) that shows how an arbitrary CNN\narchitecture can be easily converted by our approach. \n\n"}
{"id": "1805.04176", "contents": "Title: A Performance Evaluation of Convolutional Neural Networks for Face Anti\n  Spoofing Abstract: In the current era, biometric based access control is becoming more popular\ndue to its simplicity and ease to use by the users. It reduces the manual work\nof identity recognition and facilitates the automatic processing. The face is\none of the most important biometric visual information that can be easily\ncaptured without user cooperation in an uncontrolled environment. Precise\ndetection of spoofed faces should be on the high priority to make face based\nidentity recognition and access control robust against possible attacks. The\nrecently evolved Convolutional Neural Network (CNN) based deep learning\ntechnique has proven as one of the excellent method to deal with the visual\ninformation very effectively. The CNN learns the hierarchical features at\nintermediate layers automatically from the data. Several CNN based methods such\nas Inception and ResNet have shown outstanding performance for image\nclassification problem. This paper does a performance evaluation of CNNs for\nface anti-spoofing. The Inception and ResNet CNN architectures are used in this\nstudy. The results are computed over benchmark MSU Mobile Face Spoofing\nDatabase. The experiments are done by considering the different aspects such as\nthe depth of the model, random weight initialization vs weight transfer, fine\ntuning vs training from scratch and different learning rate. The favorable\nresults are obtained using these CNN architectures for face anti-spoofing in\ndifferent settings. \n\n"}
{"id": "1805.04969", "contents": "Title: Learning Temporal Strategic Relationships using Generative Adversarial\n  Imitation Learning Abstract: This paper presents a novel framework for automatic learning of complex\nstrategies in human decision making. The task that we are interested in is to\nbetter facilitate long term planning for complex, multi-step events. We observe\ntemporal relationships at the subtask level of expert demonstrations, and\ndetermine the different strategies employed in order to successfully complete a\ntask. To capture the relationship between the subtasks and the overall goal, we\nutilise two external memory modules, one for capturing dependencies within a\nsingle expert demonstration, such as the sequential relationship among\ndifferent sub tasks, and a global memory module for modelling task level\ncharacteristics such as best practice employed by different humans based on\ntheir domain expertise. Furthermore, we demonstrate how the hidden state\nrepresentation of the memory can be used as a reward signal to smooth the state\ntransitions, eradicating subtle changes. We evaluate the effectiveness of the\nproposed model for an autonomous highway driving application, where we\ndemonstrate its capability to learn different expert policies and outperform\nstate-of-the-art methods. The scope in industrial applications extends to any\nrobotics and automation application which requires learning from complex\ndemonstrations containing series of subtasks. \n\n"}
{"id": "1805.05553", "contents": "Title: On Learning Associations of Faces and Voices Abstract: In this paper, we study the associations between human faces and voices.\nAudiovisual integration, specifically the integration of facial and vocal\ninformation is a well-researched area in neuroscience. It is shown that the\noverlapping information between the two modalities plays a significant role in\nperceptual tasks such as speaker identification. Through an online study on a\nnew dataset we created, we confirm previous findings that people can associate\nunseen faces with corresponding voices and vice versa with greater than chance\naccuracy. We computationally model the overlapping information between faces\nand voices and show that the learned cross-modal representation contains enough\ninformation to identify matching faces and voices with performance similar to\nthat of humans. Our representation exhibits correlations to certain demographic\nattributes and features obtained from either visual or aural modality alone. We\nrelease our dataset of audiovisual recordings and demographic annotations of\npeople reading out short text used in our studies. \n\n"}
{"id": "1805.06558", "contents": "Title: Recurrent Neural Network for Learning DenseDepth and Ego-Motion from\n  Video Abstract: Learning-based, single-view depth estimation often generalizes poorly to\nunseen datasets. While learning-based, two-frame depth estimation solves this\nproblem to some extent by learning to match features across frames, it performs\npoorly at large depth where the uncertainty is high. There exists few\nlearning-based, multi-view depth estimation methods. In this paper, we present\na learning-based, multi-view dense depth map and ego-motion estimation method\nthat uses Recurrent Neural Networks (RNN). Our model is designed for 3D\nreconstruction from video where the input frames are temporally correlated. It\nis generalizable to single- or two-view dense depth estimation. Compared to\nrecent single- or two-view CNN-based depth estimation methods, our model\nleverages more views and achieves more accurate results, especially at large\ndistances. Our method produces superior results to the state-of-the-art\nlearning-based, single- or two-view depth estimation methods on both indoor and\noutdoor benchmark datasets. We also demonstrate that our method can even work\non extremely difficult sequences, such as endoscopic video, where none of the\nassumptions (static scene, constant lighting, Lambertian reflection, etc.) from\ntraditional 3D reconstruction methods hold. \n\n"}
{"id": "1805.06725", "contents": "Title: GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training Abstract: Anomaly detection is a classical problem in computer vision, namely the\ndetermination of the normal from the abnormal when datasets are highly biased\ntowards one class (normal) due to the insufficient sample size of the other\nclass (abnormal). While this can be addressed as a supervised learning problem,\na significantly more challenging problem is that of detecting the\nunknown/unseen anomaly case that takes us instead into the space of a\none-class, semi-supervised learning paradigm. We introduce such a novel anomaly\ndetection model, by using a conditional generative adversarial network that\njointly learns the generation of high-dimensional image space and the inference\nof latent space. Employing encoder-decoder-encoder sub-networks in the\ngenerator network enables the model to map the input image to a lower dimension\nvector, which is then used to reconstruct the generated output image. The use\nof the additional encoder network maps this generated image to its latent\nrepresentation. Minimizing the distance between these images and the latent\nvectors during training aids in learning the data distribution for the normal\nsamples. As a result, a larger distance metric from this learned data\ndistribution at inference time is indicative of an outlier from that\ndistribution - an anomaly. Experimentation over several benchmark datasets,\nfrom varying domains, shows the model efficacy and superiority over previous\nstate-of-the-art approaches. \n\n"}
{"id": "1805.06960", "contents": "Title: Ask No More: Deciding when to guess in referential visual dialogue Abstract: Our goal is to explore how the abilities brought in by a dialogue manager can\nbe included in end-to-end visually grounded conversational agents. We make\ninitial steps towards this general goal by augmenting a task-oriented visual\ndialogue model with a decision-making component that decides whether to ask a\nfollow-up question to identify a target referent in an image, or to stop the\nconversation to make a guess. Our analyses show that adding a decision making\ncomponent produces dialogues that are less repetitive and that include fewer\nunnecessary questions, thus potentially leading to more efficient and less\nunnatural interactions. \n\n"}
{"id": "1805.07440", "contents": "Title: Neural Architecture Search using Deep Neural Networks and Monte Carlo\n  Tree Search Abstract: Neural Architecture Search (NAS) has shown great success in automating the\ndesign of neural networks, but the prohibitive amount of computations behind\ncurrent NAS methods requires further investigations in improving the sample\nefficiency and the network evaluation cost to get better results in a shorter\ntime. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS)\nbased NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the\nsearch efficiency by adaptively balancing the exploration and exploitation at\nthe state level, and by a Meta-Deep Neural Network (DNN) to predict network\naccuracies for biasing the search toward a promising region. To amortize the\nnetwork evaluation cost, AlphaX accelerates MCTS rollouts with a distributed\ndesign and reduces the number of epochs in evaluating a network by transfer\nlearning, which is guided with the tree structure in MCTS. In 12 GPU days and\n1000 samples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy\non CIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods\nin both the accuracy and sampling efficiency. Particularly, we also evaluate\nAlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more\nsample efficient than Random Search and Regularized Evolution in finding the\nglobal optimum. Finally, we show the searched architecture improves a variety\nof vision applications from Neural Style Transfer, to Image Captioning and\nObject Detection. \n\n"}
{"id": "1805.07509", "contents": "Title: Sparsely Grouped Multi-task Generative Adversarial Networks for Facial\n  Attribute Manipulation Abstract: Recent Image-to-Image Translation algorithms have achieved significant\nprogress in neural style transfer and image attribute manipulation tasks.\nHowever, existing approaches require exhaustively labelling training data,\nwhich is labor demanding, difficult to scale up, and hard to migrate into new\ndomains. To overcome such a key limitation, we propose Sparsely Grouped\nGenerative Adversarial Networks (SG-GAN) as a novel approach that can translate\nimages on sparsely grouped datasets where only a few samples for training are\nlabelled. Using a novel one-input multi-output architecture, SG-GAN is\nwell-suited for tackling sparsely grouped learning and multi-task learning. The\nproposed model can translate images among multiple groups using only a single\ncommonly trained model. To experimentally validate advantages of the new model,\nwe apply the proposed method to tackle a series of attribute manipulation tasks\nfor facial images. Experimental results demonstrate that SG-GAN can generate\nimage translation results of comparable quality with baselines methods on\nadequately labelled datasets and results of superior quality on sparsely\ngrouped datasets. The official implementation is publicly\navailable:https://github.com/zhangqianhui/Sparsely-Grouped-GAN. \n\n"}
{"id": "1805.07696", "contents": "Title: RGB-Depth SLAM Review Abstract: Simultaneous Localization and Mapping (SLAM) have made the real-time dense\nreconstruction possible increasing the prospects of navigation, tracking, and\naugmented reality problems. Some breakthroughs have been achieved in this\nregard during past few decades and more remarkable works are still going on.\nThis paper presents an overview of SLAM approaches that have been developed\ntill now. Kinect Fusion algorithm, its variants, and further developed\napproaches are discussed in detailed. The algorithms and approaches are\ncompared for their effectiveness in tracking and mapping based on Root Mean\nSquare error over online available datasets. \n\n"}
{"id": "1805.07706", "contents": "Title: Object Localization with a Weakly Supervised CapsNet Abstract: Inspired by CapsNet's routing-by-agreement mechanism with its ability to\nlearn object properties, we explore if those properties in turn can determine\nnew properties of the objects, such as the locations. We then propose a CapsNet\narchitecture with object coordinate atoms and a modified routing-by-agreement\nalgorithm with unevenly distributed initial routing probabilities. The model is\nbased on CapsNet but uses a routing algorithm to find the objects' approximate\npositions in the image coordinate system. We also discussed how to derive the\nproperty of translation through coordinate atoms and we show the importance of\nsparse representation. We train our model on the single moving MNIST dataset\nwith class labels. Our model can learn and derive the coordinates of the digits\nbetter than its convolution counterpart that lacks a routing-by-agreement\nalgorithm, and can also perform well when testing on the multi-digit moving\nMNIST and KTH datasets. The results show our method reaches the state-of-art\nperformance on object localization without any extra localization techniques\nand modules as in prior work. \n\n"}
{"id": "1805.07785", "contents": "Title: Conditional Inference in Pre-trained Variational Autoencoders via\n  Cross-coding Abstract: Variational Autoencoders (VAEs) are a popular generative model, but one in\nwhich conditional inference can be challenging. If the decomposition into query\nand evidence variables is fixed, conditional VAEs provide an attractive\nsolution. To support arbitrary queries, one is generally reduced to Markov\nChain Monte Carlo sampling methods that can suffer from long mixing times. In\nthis paper, we propose an idea we term cross-coding to approximate the\ndistribution over the latent variables after conditioning on an evidence\nassignment to some subset of the variables. This allows generating query\nsamples without retraining the full VAE. We experimentally evaluate three\nvariations of cross-coding showing that (i) they can be quickly optimized for\ndifferent decompositions of evidence and query and (ii) they quantitatively and\nqualitatively outperform Hamiltonian Monte Carlo. \n\n"}
{"id": "1805.07941", "contents": "Title: Quantizing Convolutional Neural Networks for Low-Power High-Throughput\n  Inference Engines Abstract: Deep learning as a means to inferencing has proliferated thanks to its\nversatility and ability to approach or exceed human-level accuracy. These\ncomputational models have seemingly insatiable appetites for computational\nresources not only while training, but also when deployed at scales ranging\nfrom data centers all the way down to embedded devices. As such, increasing\nconsideration is being made to maximize the computational efficiency given\nlimited hardware and energy resources and, as a result, inferencing with\nreduced precision has emerged as a viable alternative to the IEEE 754 Standard\nfor Floating-Point Arithmetic. We propose a quantization scheme that allows\ninferencing to be carried out using arithmetic that is fundamentally more\nefficient when compared to even half-precision floating-point. Our quantization\nprocedure is significant in that we determine our quantization scheme\nparameters by calibrating against its reference floating-point model using a\nsingle inference batch rather than (re)training and achieve end-to-end post\nquantization accuracies comparable to the reference model. \n\n"}
{"id": "1805.08090", "contents": "Title: Graph Capsule Convolutional Neural Networks Abstract: Graph Convolutional Neural Networks (GCNNs) are the most recent exciting\nadvancement in deep learning field and their applications are quickly spreading\nin multi-cross-domains including bioinformatics, chemoinformatics, social\nnetworks, natural language processing and computer vision. In this paper, we\nexpose and tackle some of the basic weaknesses of a GCNN model with a capsule\nidea presented in \\cite{hinton2011transforming} and propose our Graph Capsule\nNetwork (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve\nespecially graph classification problem which current GCNN models find\nchallenging. Through extensive experiments, we show that our proposed Graph\nCapsule Network can significantly outperforms both the existing state-of-art\ndeep learning methods and graph kernels on graph classification benchmark\ndatasets. \n\n"}
{"id": "1805.08324", "contents": "Title: Measurement-wise Occlusion in Multi-object Tracking Abstract: Handling object interaction is a fundamental challenge in practical\nmulti-object tracking, even for simple interactive effects such as one object\ntemporarily occluding another. We formalize the problem of occlusion in\ntracking with two different abstractions. In object-wise occlusion, objects\nthat are occluded by other objects do not generate measurements. In\nmeasurement-wise occlusion, a previously unstudied approach, all objects may\ngenerate measurements but some measurements may be occluded by others. While\nthe relative validity of each abstraction depends on the situation and sensor,\nmeasurement-wise occlusion fits into probabilistic multi-object tracking\nalgorithms with much looser assumptions on object interaction. Its value is\ndemonstrated by showing that it naturally derives a popular approximation for\nlidar tracking, and by an example of visual tracking in image space. \n\n"}
{"id": "1805.08492", "contents": "Title: Knowledge-based Fully Convolutional Network and Its Application in\n  Segmentation of Lung CT Images Abstract: A variety of deep neural networks have been applied in medical image\nsegmentation and achieve good performance. Unlike natural images, medical\nimages of the same imaging modality are characterized by the same pattern,\nwhich indicates that same normal organs or tissues locate at similar positions\nin the images. Thus, in this paper we try to incorporate the prior knowledge of\nmedical images into the structure of neural networks such that the prior\nknowledge can be utilized for accurate segmentation. Based on this idea, we\npropose a novel deep network called knowledge-based fully convolutional network\n(KFCN) for medical image segmentation. The segmentation function and\ncorresponding error is analyzed. We show the existence of an asymptotically\nstable region for KFCN which traditional FCN doesn't possess. Experiments\nvalidate our knowledge assumption about the incorporation of prior knowledge\ninto the convolution kernels of KFCN and show that KFCN can achieve a\nreasonable segmentation and a satisfactory accuracy. \n\n"}
{"id": "1805.08587", "contents": "Title: Deep Feature Aggregation and Image Re-ranking with Heat Diffusion for\n  Image Retrieval Abstract: Image retrieval based on deep convolutional features has demonstrated\nstate-of-the-art performance in popular benchmarks. In this paper, we present a\nunified solution to address deep convolutional feature aggregation and image\nre-ranking by simulating the dynamics of heat diffusion. A distinctive problem\nin image retrieval is that repetitive or \\emph{bursty} features tend to\ndominate final image representations, resulting in representations less\ndistinguishable. We show that by considering each deep feature as a heat\nsource, our unsupervised aggregation method is able to avoid\nover-representation of \\emph{bursty} features. We additionally provide a\npractical solution for the proposed aggregation method and further show the\nefficiency of our method in experimental evaluation. Inspired by the\naforementioned deep feature aggregation method, we also propose a method to\nre-rank a number of top ranked images for a given query image by considering\nthe query as the heat source. Finally, we extensively evaluate the proposed\napproach with pre-trained and fine-tuned deep networks on common public\nbenchmarks and show superior performance compared to previous work. \n\n"}
{"id": "1805.08841", "contents": "Title: Distribution Matching Losses Can Hallucinate Features in Medical Image\n  Translation Abstract: This paper discusses how distribution matching losses, such as those used in\nCycleGAN, when used to synthesize medical images can lead to mis-diagnosis of\nmedical conditions. It seems appealing to use these new image synthesis methods\nfor translating images from a source to a target domain because they can\nproduce high quality images and some even do not require paired data. However,\nthe basis of how these image translation models work is through matching the\ntranslation output to the distribution of the target domain. This can cause an\nissue when the data provided in the target domain has an over or under\nrepresentation of some classes (e.g. healthy or sick). When the output of an\nalgorithm is a transformed image there are uncertainties whether all known and\nunknown class labels have been preserved or changed. Therefore, we recommend\nthat these translated images should not be used for direct interpretation (e.g.\nby doctors) because they may lead to misdiagnosis of patients based on\nhallucinated image features by an algorithm that matches a distribution.\nHowever there are many recent papers that seem as though this is the goal. \n\n"}
{"id": "1805.09110", "contents": "Title: The Topology ToolKit Abstract: This system paper presents the Topology ToolKit (TTK), a software platform\ndesigned for topological data analysis in scientific visualization. TTK\nprovides a unified, generic, efficient, and robust implementation of key\nalgorithms for the topological analysis of scalar data, including: critical\npoints, integral lines, persistence diagrams, persistence curves, merge trees,\ncontour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots,\nJacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due\nto a tight integration with ParaView. It is also easily accessible to\ndevelopers through a variety of bindings (Python, VTK/C++) for fast prototyping\nor through direct, dependence-free, C++, to ease integration into pre-existing\ncomplex systems. While developing TTK, we faced several algorithmic and\nsoftware engineering challenges, which we document in this paper. In\nparticular, we present an algorithm for the construction of a discrete gradient\nthat complies to the critical points extracted in the piecewise-linear setting.\nThis algorithm guarantees a combinatorial consistency across the topological\nabstractions supported by TTK, and importantly, a unified implementation of\ntopological data simplification for multi-scale exploration and analysis. We\nalso present a cached triangulation data structure, that supports time\nefficient and generic traversals, which self-adjusts its memory usage on demand\nfor input simplicial meshes and which implicitly emulates a triangulation for\nregular grids with no memory overhead. Finally, we describe an original\nsoftware architecture, which guarantees memory efficient and direct accesses to\nTTK features, while still allowing for researchers powerful and easy bindings\nand extensions. TTK is open source (BSD license) and its code, online\ndocumentation and video tutorials are available on TTK's website. \n\n"}
{"id": "1805.10547", "contents": "Title: Using Syntax to Ground Referring Expressions in Natural Images Abstract: We introduce GroundNet, a neural network for referring expression recognition\n-- the task of localizing (or grounding) in an image the object referred to by\na natural language expression. Our approach to this task is the first to rely\non a syntactic analysis of the input referring expression in order to inform\nthe structure of the computation graph. Given a parse tree for an input\nexpression, we explicitly map the syntactic constituents and relationships\npresent in the tree to a composed graph of neural modules that defines our\narchitecture for performing localization. This syntax-based approach aids\nlocalization of \\textit{both} the target object and auxiliary supporting\nobjects mentioned in the expression. As a result, GroundNet is more\ninterpretable than previous methods: we can (1) determine which phrase of the\nreferring expression points to which object in the image and (2) track how the\nlocalization of the target object is determined by the network. We study this\nproperty empirically by introducing a new set of annotations on the GoogleRef\ndataset to evaluate localization of supporting objects. Our experiments show\nthat GroundNet achieves state-of-the-art accuracy in identifying supporting\nobjects, while maintaining comparable performance in the localization of target\nobjects. \n\n"}
{"id": "1805.10807", "contents": "Title: Fast Dynamic Routing Based on Weighted Kernel Density Estimation Abstract: Capsules as well as dynamic routing between them are most recently proposed\nstructures for deep neural networks. A capsule groups data into vectors or\nmatrices as poses rather than conventional scalars to represent specific\nproperties of target instance. Besides of pose, a capsule should be attached\nwith a probability (often denoted as activation) for its presence. The dynamic\nrouting helps capsules achieve more generalization capacity with many fewer\nmodel parameters. However, the bottleneck that prevents widespread applications\nof capsule is the expense of computation during routing. To address this\nproblem, we generalize existing routing methods within the framework of\nweighted kernel density estimation, and propose two fast routing methods with\ndifferent optimization strategies. Our methods prompt the time efficiency of\nrouting by nearly 40\\% with negligible performance degradation. By stacking a\nhybrid of convolutional layers and capsule layers, we construct a network\narchitecture to handle inputs at a resolution of $64\\times{64}$ pixels. The\nproposed models achieve a parallel performance with other leading methods in\nmultiple benchmarks. \n\n"}
{"id": "1805.10916", "contents": "Title: Online Multi-Object Tracking with Historical Appearance Matching and\n  Scene Adaptive Detection Filtering Abstract: In this paper, we propose the methods to handle temporal errors during\nmulti-object tracking. Temporal error occurs when objects are occluded or noisy\ndetections appear near the object. In those situations, tracking may fail and\nvarious errors like drift or ID-switching occur. It is hard to overcome\ntemporal errors only by using motion and shape information. So, we propose the\nhistorical appearance matching method and joint-input siamese network which was\ntrained by 2-step process. It can prevent tracking failures although objects\nare temporally occluded or last matching information is unreliable. We also\nprovide useful technique to remove noisy detections effectively according to\nscene condition. Tracking performance, especially identity consistency, is\nhighly improved by attaching our methods. \n\n"}
{"id": "1805.10938", "contents": "Title: Face hallucination using cascaded super-resolution and identity priors Abstract: In this paper we address the problem of hallucinating high-resolution facial\nimages from unaligned low-resolution inputs at high magnification factors. We\napproach the problem with convolutional neural networks (CNNs) and propose a\nnovel (deep) face hallucination model that incorporates identity priors into\nthe learning procedure. The model consists of two main parts: i) a cascaded\nsuper-resolution network that upscales the low-resolution images, and ii) an\nensemble of face recognition models that act as identity priors for the\nsuper-resolution network during training. Different from competing\nsuper-resolution approaches that typically rely on a single model for upscaling\n(even with large magnification factors), our network uses a cascade of multiple\nSR models that progressively upscale the low-resolution images using steps of\n$2\\times$. This characteristic allows us to apply supervision signals (target\nappearances) at different resolutions and incorporate identity constraints at\nmultiple-scales. Our model is able to upscale (very) low-resolution images\ncaptured in unconstrained conditions and produce visually convincing results.\nWe rigorously evaluate the proposed model on a large datasets of facial images\nand report superior performance compared to the state-of-the-art. \n\n"}
{"id": "1805.11714", "contents": "Title: Deep Video Portraits Abstract: We present a novel approach that enables photo-realistic re-animation of\nportrait videos using only an input video. In contrast to existing approaches\nthat are restricted to manipulations of facial expressions only, we are the\nfirst to transfer the full 3D head position, head rotation, face expression,\neye gaze, and eye blinking from a source actor to a portrait video of a target\nactor. The core of our approach is a generative neural network with a novel\nspace-time architecture. The network takes as input synthetic renderings of a\nparametric face model, based on which it predicts photo-realistic video frames\nfor a given target actor. The realism in this rendering-to-video transfer is\nachieved by careful adversarial training, and as a result, we can create\nmodified target videos that mimic the behavior of the synthetically-created\ninput. In order to enable source-to-target video re-animation, we render a\nsynthetic target video with the reconstructed head animation parameters from a\nsource video, and feed it into the trained network -- thus taking full control\nof the target. With the ability to freely recombine source and target\nparameters, we are able to demonstrate a large variety of video rewrite\napplications without explicitly modeling hair, body or background. For\ninstance, we can reenact the full head using interactive user-controlled\nediting, and realize high-fidelity visual dubbing. To demonstrate the high\nquality of our output, we conduct an extensive series of experiments and\nevaluations, where for instance a user study shows that our video edits are\nhard to detect. \n\n"}
{"id": "1805.11718", "contents": "Title: Random mesh projectors for inverse problems Abstract: We propose a new learning-based approach to solve ill-posed inverse problems\nin imaging. We address the case where ground truth training samples are rare\nand the problem is severely ill-posed - both because of the underlying physics\nand because we can only get few measurements. This setting is common in\ngeophysical imaging and remote sensing. We show that in this case the common\napproach to directly learn the mapping from the measured data to the\nreconstruction becomes unstable. Instead, we propose to first learn an ensemble\nof simpler mappings from the data to projections of the unknown image into\nrandom piecewise-constant subspaces. We then combine the projections to form a\nfinal reconstruction by solving a deconvolution-like problem. We show\nexperimentally that the proposed method is more robust to measurement noise and\ncorruptions not seen during training than a directly learned inverse. \n\n"}
{"id": "1805.11746", "contents": "Title: Semantic Road Layout Understanding by Generative Adversarial Inpainting Abstract: Autonomous driving is becoming a reality, yet vehicles still need to rely on\ncomplex sensor fusion to understand the scene they act in. The ability to\ndiscern static environment and dynamic entities provides a comprehension of the\nroad layout that poses constraints to the reasoning process about moving\nobjects. We pursue this through a GAN-based semantic segmentation inpainting\nmodel to remove all dynamic objects from the scene and focus on understanding\nits static components such as streets, sidewalks and buildings. We evaluate\nthis task on the Cityscapes dataset and on a novel synthetically generated\ndataset obtained with the CARLA simulator and specifically designed to\nquantitatively evaluate semantic segmentation inpaintings. We compare our\nmethods with a variety of baselines working both in the RGB and segmentation\ndomains. \n\n"}
{"id": "1805.12120", "contents": "Title: On Consensus-Optimality Trade-offs in Collaborative Deep Learning Abstract: In distributed machine learning, where agents collaboratively learn from\ndiverse private data sets, there is a fundamental tension between consensus and\noptimality. In this paper, we build on recent algorithmic progresses in\ndistributed deep learning to explore various consensus-optimality trade-offs\nover a fixed communication topology. First, we propose the incremental\nconsensus-based distributed SGD (i-CDSGD) algorithm, which involves multiple\nconsensus steps (where each agent communicates information with its neighbors)\nwithin each SGD iteration. Second, we propose the generalized consensus-based\ndistributed SGD (g-CDSGD) algorithm that enables us to navigate the full\nspectrum from complete consensus (all agents agree) to complete disagreement\n(each agent converges to individual model parameters). We analytically\nestablish convergence of the proposed algorithms for strongly convex and\nnonconvex objective functions; we also analyze the momentum variants of the\nalgorithms for the strongly convex case. We support our algorithms via\nnumerical experiments, and demonstrate significant improvements over existing\nmethods for collaborative deep learning. \n\n"}
{"id": "1805.12243", "contents": "Title: Novel Video Prediction for Large-scale Scene using Optical Flow Abstract: Making predictions of future frames is a critical challenge in autonomous\ndriving research. Most of the existing methods for video prediction attempt to\ngenerate future frames in simple and fixed scenes. In this paper, we propose a\nnovel and effective optical flow conditioned method for the task of video\nprediction with an application to complex urban scenes. In contrast with\nprevious work, the prediction model only requires video sequences and optical\nflow sequences for training and testing. Our method uses the rich\nspatial-temporal features in video sequences. The method takes advantage of the\nmotion information extracting from optical flow maps between neighbor images as\nwell as previous images. Empirical evaluations on the KITTI dataset and the\nCityscapes dataset demonstrate the effectiveness of our method. \n\n"}
{"id": "1805.12358", "contents": "Title: Light Field Denoising via Anisotropic Parallax Analysis in a CNN\n  Framework Abstract: Light field (LF) cameras provide perspective information of scenes by taking\ndirectional measurements of the focusing light rays. The raw outputs are\nusually dark with additive camera noise, which impedes subsequent processing\nand applications. We propose a novel LF denoising framework based on\nanisotropic parallax analysis (APA). Two convolutional neural networks are\njointly designed for the task: first, the structural parallax synthesis network\npredicts the parallax details for the entire LF based on a set of anisotropic\nparallax features. These novel features can efficiently capture the high\nfrequency perspective components of a LF from noisy observations. Second, the\nview-dependent detail compensation network restores non-Lambertian variation to\neach LF view by involving view-specific spatial energies. Extensive experiments\nshow that the proposed APA LF denoiser provides a much better denoising\nperformance than state-of-the-art methods in terms of visual quality and in\npreservation of parallax details. \n\n"}
{"id": "1805.12369", "contents": "Title: Reinforced Continual Learning Abstract: Most artificial intelligence models have limiting ability to solve new tasks\nfaster, without forgetting previously acquired knowledge. The recently emerging\nparadigm of continual learning aims to solve this issue, in which the model\nlearns various tasks in a sequential fashion. In this work, a novel approach\nfor continual learning is proposed, which searches for the best neural\narchitecture for each coming task via sophisticatedly designed reinforcement\nlearning strategies. We name it as Reinforced Continual Learning. Our method\nnot only has good performance on preventing catastrophic forgetting but also\nfits new tasks well. The experiments on sequential classification tasks for\nvariants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach\noutperforms existing continual learning alternatives for deep networks. \n\n"}
{"id": "1805.12549", "contents": "Title: Channel Gating Neural Networks Abstract: This paper introduces channel gating, a dynamic, fine-grained, and\nhardware-efficient pruning scheme to reduce the computation cost for\nconvolutional neural networks (CNNs). Channel gating identifies regions in the\nfeatures that contribute less to the classification result, and skips the\ncomputation on a subset of the input channels for these ineffective regions.\nUnlike static network pruning, channel gating optimizes CNN inference at\nrun-time by exploiting input-specific characteristics, which allows\nsubstantially reducing the compute cost with almost no accuracy loss. We\nexperimentally show that applying channel gating in state-of-the-art networks\nachieves 2.7-8.0$\\times$ reduction in floating-point operations (FLOPs) and\n2.0-4.4$\\times$ reduction in off-chip memory accesses with a minimal accuracy\nloss on CIFAR-10. Combining our method with knowledge distillation reduces the\ncompute cost of ResNet-18 by 2.6$\\times$ without accuracy drop on ImageNet. We\nfurther demonstrate that channel gating can be realized in hardware\nefficiently. Our approach exhibits sparsity patterns that are well-suited to\ndense systolic arrays with minimal additional hardware. We have designed an\naccelerator for channel gating networks, which can be implemented using either\nFPGAs or ASICs. Running a quantized ResNet-18 model for ImageNet, our\naccelerator achieves an encouraging speedup of 2.4$\\times$ on average, with a\ntheoretical FLOP reduction of 2.8$\\times$. \n\n"}
{"id": "1806.00186", "contents": "Title: Video Description: A Survey of Methods, Datasets and Evaluation Metrics Abstract: Video description is the automatic generation of natural language sentences\nthat describe the contents of a given video. It has applications in human-robot\ninteraction, helping the visually impaired and video subtitling. The past few\nyears have seen a surge of research in this area due to the unprecedented\nsuccess of deep learning in computer vision and natural language processing.\nNumerous methods, datasets and evaluation metrics have been proposed in the\nliterature, calling the need for a comprehensive survey to focus research\nefforts in this flourishing new direction. This paper fills the gap by\nsurveying the state of the art approaches with a focus on deep learning models;\ncomparing benchmark datasets in terms of their domains, number of classes, and\nrepository size; and identifying the pros and cons of various evaluation\nmetrics like SPICE, CIDEr, ROUGE, BLEU, METEOR, and WMD. Classical video\ndescription approaches combined subject, object and verb detection with\ntemplate based language models to generate sentences. However, the release of\nlarge datasets revealed that these methods can not cope with the diversity in\nunconstrained open domain videos. Classical approaches were followed by a very\nshort era of statistical methods which were soon replaced with deep learning,\nthe current state of the art in video description. Our survey shows that\ndespite the fast-paced developments, video description research is still in its\ninfancy due to the following reasons. Analysis of video description models is\nchallenging because it is difficult to ascertain the contributions, towards\naccuracy or errors, of the visual features and the adopted language model in\nthe final description. Existing datasets neither contain adequate visual\ndiversity nor complexity of linguistic structures. Finally, current evaluation\nmetrics ... \n\n"}
{"id": "1806.00265", "contents": "Title: Learn the new, keep the old: Extending pretrained models with new\n  anatomy and images Abstract: Deep learning has been widely accepted as a promising solution for medical\nimage segmentation, given a sufficiently large representative dataset of images\nwith corresponding annotations. With ever increasing amounts of annotated\nmedical datasets, it is infeasible to train a learning method always with all\ndata from scratch. This is also doomed to hit computational limits, e.g.,\nmemory or runtime feasible for training. Incremental learning can be a\npotential solution, where new information (images or anatomy) is introduced\niteratively. Nevertheless, for the preservation of the collective information,\nit is essential to keep some \"important\" (i.e. representative) images and\nannotations from the past, while adding new information. In this paper, we\nintroduce a framework for applying incremental learning for segmentation and\npropose novel methods for selecting representative data therein. We\ncomparatively evaluate our methods in different scenarios using MR images and\nvalidate the increased learning capacity with using our methods. \n\n"}
{"id": "1806.00365", "contents": "Title: Accurate and Efficient Similarity Search for Large Scale Face\n  Recognition Abstract: Face verification is a relatively easy task with the help of discriminative\nfeatures from deep neural networks. However, it is still a challenge to\nrecognize faces on millions of identities while keeping high performance and\nefficiency. The challenge 2 of MS-Celeb-1M is a classification task. However,\nthe number of identities is too large and it is not that elegant to treat the\ntask as an image classification task. We treat the classification task as\nsimilarity search and do experiments on different similarity search strategies.\nSimilarity search strategy accelerates the speed of searching and boosts the\naccuracy of final results. The model used for extracting features is a single\ndeep neural network pretrained on CASIA-Webface, which is not trained on the\nbase set or novel set offered by official. Finally, we rank \\textbf{3rd}, while\nthe speed of searching is 1ms/image. \n\n"}
{"id": "1806.00546", "contents": "Title: Spatially Localized Atlas Network Tiles Enables 3D Whole Brain\n  Segmentation from Limited Data Abstract: Whole brain segmentation on a structural magnetic resonance imaging (MRI) is\nessential in non-invasive investigation for neuroanatomy. Historically,\nmulti-atlas segmentation (MAS) has been regarded as the de facto standard\nmethod for whole brain segmentation. Recently, deep neural network approaches\nhave been applied to whole brain segmentation by learning random patches or 2D\nslices. Yet, few previous efforts have been made on detailed whole brain\nsegmentation using 3D networks due to the following challenges: (1) fitting\nentire whole brain volume into 3D networks is restricted by the current GPU\nmemory, and (2) the large number of targeting labels (e.g., > 100 labels) with\nlimited number of training 3D volumes (e.g., < 50 scans). In this paper, we\npropose the spatially localized atlas network tiles (SLANT) method to\ndistribute multiple independent 3D fully convolutional networks to cover\noverlapped sub-spaces in a standard atlas space. This strategy simplifies the\nwhole brain learning task to localized sub-tasks, which was enabled by combing\ncanonical registration and label fusion techniques with deep learning. To\naddress the second challenge, auxiliary labels on 5111 initially unlabeled\nscans were created by MAS for pre-training. From empirical validation, the\nstate-of-the-art MAS method achieved mean Dice value of 0.76, 0.71, and 0.68,\nwhile the proposed method achieved 0.78, 0.73, and 0.71 on three validation\ncohorts. Moreover, the computational time reduced from > 30 hours using MAS to\n~15 minutes using the proposed method. The source code is available online\nhttps://github.com/MASILab/SLANTbrainSeg \n\n"}
{"id": "1806.00685", "contents": "Title: Hierarchical Attention-Based Recurrent Highway Networks for Time Series\n  Prediction Abstract: Time series prediction has been studied in a variety of domains. However, it\nis still challenging to predict future series given historical observations and\npast exogenous data. Existing methods either fail to consider the interactions\namong different components of exogenous variables which may affect the\nprediction accuracy, or cannot model the correlations between exogenous data\nand target data. Besides, the inherent temporal dynamics of exogenous data are\nalso related to the target series prediction, and thus should be considered as\nwell. To address these issues, we propose an end-to-end deep learning model,\ni.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which\nincorporates spatio-temporal feature extraction of exogenous variables and\ntemporal dynamics modeling of target variables into a single framework.\nMoreover, by introducing the hierarchical attention mechanism, HRHN can\nadaptively select the relevant exogenous features in different semantic levels.\nWe carry out comprehensive empirical evaluations with various methods over\nseveral datasets, and show that HRHN outperforms the state of the arts in time\nseries prediction, especially in capturing sudden changes and sudden\noscillations of time series. \n\n"}
{"id": "1806.00921", "contents": "Title: Automatic catheter detection in pediatric X-ray images using a\n  scale-recurrent network and synthetic data Abstract: Catheters are commonly inserted life supporting devices. X-ray images are\nused to assess the position of a catheter immediately after placement as\nserious complications can arise from malpositioned catheters. Previous computer\nvision approaches to detect catheters on X-ray images either relied on\nlow-level cues that are not sufficiently robust or only capable of processing a\nlimited number or type of catheters. With the resurgence of deep learning,\nsupervised training approaches are begining to showing promising results.\nHowever, dense annotation maps are required, and the work of a human annotator\nis hard to scale. In this work, we proposed a simple way of synthesizing\ncatheters on X-ray images and a scale recurrent network for catheter detection.\nBy training on adult chest X-rays, the proposed network exhibits promising\ndetection results on pediatric chest/abdomen X-rays in terms of both precision\nand recall. \n\n"}
{"id": "1806.02070", "contents": "Title: Instance Segmentation and Tracking with Cosine Embeddings and Recurrent\n  Hourglass Networks Abstract: Different to semantic segmentation, instance segmentation assigns unique\nlabels to each individual instance of the same class. In this work, we propose\na novel recurrent fully convolutional network architecture for tracking such\ninstance segmentations over time. The network architecture incorporates\nconvolutional gated recurrent units (ConvGRU) into a stacked hourglass network\nto utilize temporal video information. Furthermore, we train the network with a\nnovel embedding loss based on cosine similarities, such that the network\npredicts unique embeddings for every instance throughout videos. Afterwards,\nthese embeddings are clustered among subsequent video frames to create the\nfinal tracked instance segmentations. We evaluate the recurrent hourglass\nnetwork by segmenting left ventricles in MR videos of the heart, where it\noutperforms a network that does not incorporate video information. Furthermore,\nwe show applicability of the cosine embedding loss for segmenting leaf\ninstances on still images of plants. Finally, we evaluate the framework for\ninstance segmentation and tracking on six datasets of the ISBI celltracking\nchallenge, where it shows state-of-the-art performance. \n\n"}
{"id": "1806.02612", "contents": "Title: Dimensionality-Driven Learning with Noisy Labels Abstract: Datasets with significant proportions of noisy (incorrect) class labels\npresent challenges for training accurate Deep Neural Networks (DNNs). We\npropose a new perspective for understanding DNN generalization for such\ndatasets, by investigating the dimensionality of the deep representation\nsubspace of training samples. We show that from a dimensionality perspective,\nDNNs exhibit quite distinctive learning styles when trained with clean labels\nversus when trained with a proportion of noisy labels. Based on this finding,\nwe develop a new dimensionality-driven learning strategy, which monitors the\ndimensionality of subspaces during training and adapts the loss function\naccordingly. We empirically demonstrate that our approach is highly tolerant to\nsignificant proportions of noisy labels, and can effectively learn\nlow-dimensional local subspaces that capture the data distribution. \n\n"}
{"id": "1806.02877", "contents": "Title: In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye\n  Blinking Abstract: The new developments in deep generative networks have significantly improve\nthe quality and efficiency in generating realistically-looking fake face\nvideos. In this work, we describe a new method to expose fake face videos\ngenerated with neural networks. Our method is based on detection of eye\nblinking in the videos, which is a physiological signal that is not well\npresented in the synthesized fake videos. Our method is tested over benchmarks\nof eye-blinking detection datasets and also show promising performance on\ndetecting videos generated with DeepFake. \n\n"}
{"id": "1806.02888", "contents": "Title: Correspondence of Deep Neural Networks and the Brain for Visual Textures Abstract: Deep convolutional neural networks (CNNs) trained on objects and scenes have\nshown intriguing ability to predict some response properties of visual cortical\nneurons. However, the factors and computations that give rise to such ability,\nand the role of intermediate processing stages in explaining changes that\ndevelop across areas of the cortical hierarchy, are poorly understood. We\nfocused on the sensitivity to textures as a paradigmatic example, since recent\nneurophysiology experiments provide rich data pointing to texture sensitivity\nin secondary but not primary visual cortex. We developed a quantitative\napproach for selecting a subset of the neural unit population from the CNN that\nbest describes the brain neural recordings. We found that the first two layers\nof the CNN showed qualitative and quantitative correspondence to the cortical\ndata across a number of metrics. This compatibility was reduced for the\narchitecture alone rather than the learned weights, for some other related\nhierarchical models, and only mildly in the absence of a nonlinear computation\nakin to local divisive normalization. Our results show that the CNN class of\nmodel is effective for capturing changes that develop across early areas of\ncortex, and has the potential to facilitate understanding of the computations\nthat give rise to hierarchical processing in the brain. \n\n"}
{"id": "1806.03361", "contents": "Title: A Content-Based Late Fusion Approach Applied to Pedestrian Detection Abstract: The variety of pedestrians detectors proposed in recent years has encouraged\nsome works to fuse pedestrian detectors to achieve a more accurate detection.\nThe intuition behind is to combine the detectors based on its spatial\nconsensus. We propose a novel method called Content-Based Spatial Consensus\n(CSBC), which, in addition to relying on spatial consensus, considers the\ncontent of the detection windows to learn a weighted-fusion of pedestrian\ndetectors. The result is a reduction in false alarms and an enhancement in the\ndetection. In this work, we also demonstrate that there is small influence of\nthe feature used to learn the contents of the windows of each detector, which\nenables our method to be efficient even employing simple features. The CSBC\novercomes state-of-the-art fusion methods in the ETH dataset and in the Caltech\ndataset. Particularly, our method is more efficient since fewer detectors are\nnecessary to achieve expressive results. \n\n"}
{"id": "1806.03589", "contents": "Title: Free-Form Image Inpainting with Gated Convolution Abstract: We present a generative image inpainting system to complete images with\nfree-form mask and guidance. The system is based on gated convolutions learned\nfrom millions of images without additional labelling efforts. The proposed\ngated convolution solves the issue of vanilla convolution that treats all input\npixels as valid ones, generalizes partial convolution by providing a learnable\ndynamic feature selection mechanism for each channel at each spatial location\nacross all layers. Moreover, as free-form masks may appear anywhere in images\nwith any shape, global and local GANs designed for a single rectangular mask\nare not applicable. Thus, we also present a patch-based GAN loss, named\nSN-PatchGAN, by applying spectral-normalized discriminator on dense image\npatches. SN-PatchGAN is simple in formulation, fast and stable in training.\nResults on automatic image inpainting and user-guided extension demonstrate\nthat our system generates higher-quality and more flexible results than\nprevious methods. Our system helps user quickly remove distracting objects,\nmodify image layouts, clear watermarks and edit faces. Code, demo and models\nare available at: https://github.com/JiahuiYu/generative_inpainting \n\n"}
{"id": "1806.03724", "contents": "Title: Learning Answer Embeddings for Visual Question Answering Abstract: We propose a novel probabilistic model for visual question answering (Visual\nQA). The key idea is to infer two sets of embeddings: one for the image and the\nquestion jointly and the other for the answers. The learning objective is to\nlearn the best parameterization of those embeddings such that the correct\nanswer has higher likelihood among all possible answers. In contrast to several\nexisting approaches of treating Visual QA as multi-way classification, the\nproposed approach takes the semantic relationships (as characterized by the\nembeddings) among answers into consideration, instead of viewing them as\nindependent ordinal numbers. Thus, the learned embedded function can be used to\nembed unseen answers (in the training dataset). These properties make the\napproach particularly appealing for transfer learning for open-ended Visual QA,\nwhere the source dataset on which the model is learned has limited overlapping\nwith the target dataset in the space of answers. We have also developed\nlarge-scale optimization techniques for applying the model to datasets with a\nlarge number of answers, where the challenge is to properly normalize the\nproposed probabilistic models. We validate our approach on several Visual QA\ndatasets and investigate its utility for transferring models across datasets.\nThe empirical results have shown that the approach performs well not only on\nin-domain learning but also on transfer learning. \n\n"}
{"id": "1806.04284", "contents": "Title: iParaphrasing: Extracting Visually Grounded Paraphrases via an Image Abstract: A paraphrase is a restatement of the meaning of a text in other words.\nParaphrases have been studied to enhance the performance of many natural\nlanguage processing tasks. In this paper, we propose a novel task iParaphrasing\nto extract visually grounded paraphrases (VGPs), which are different phrasal\nexpressions describing the same visual concept in an image. These extracted\nVGPs have the potential to improve language and image multimodal tasks such as\nvisual question answering and image captioning. How to model the similarity\nbetween VGPs is the key of iParaphrasing. We apply various existing methods as\nwell as propose a novel neural network-based method with image attention, and\nreport the results of the first attempt toward iParaphrasing. \n\n"}
{"id": "1806.04606", "contents": "Title: Knowledge Distillation by On-the-Fly Native Ensemble Abstract: Knowledge distillation is effective to train small and generalisable network\nmodels for meeting the low-memory and fast running requirements. Existing\noffline distillation methods rely on a strong pre-trained teacher, which\nenables favourable knowledge discovery and transfer but requires a complex\ntwo-phase training procedure. Online counterparts address this limitation at\nthe price of lacking a highcapacity teacher. In this work, we present an\nOn-the-fly Native Ensemble (ONE) strategy for one-stage online distillation.\nSpecifically, ONE trains only a single multi-branch network while\nsimultaneously establishing a strong teacher on-the- fly to enhance the\nlearning of target network. Extensive evaluations show that ONE improves the\ngeneralisation performance a variety of deep neural networks more significantly\nthan alternative methods on four image classification dataset: CIFAR10,\nCIFAR100, SVHN, and ImageNet, whilst having the computational efficiency\nadvantages. \n\n"}
{"id": "1806.05030", "contents": "Title: Visually grounded cross-lingual keyword spotting in speech Abstract: Recent work considered how images paired with speech can be used as\nsupervision for building speech systems when transcriptions are not available.\nWe ask whether visual grounding can be used for cross-lingual keyword spotting:\ngiven a text keyword in one language, the task is to retrieve spoken utterances\ncontaining that keyword in another language. This could enable searching\nthrough speech in a low-resource language using text queries in a high-resource\nlanguage. As a proof-of-concept, we use English speech with German queries: we\nuse a German visual tagger to add keyword labels to each training image, and\nthen train a neural network to map English speech to German keywords. Without\nseeing parallel speech-transcriptions or translations, the model achieves a\nprecision at ten of 58%. We show that most erroneous retrievals contain\nequivalent or semantically relevant keywords; excluding these would improve\nP@10 to 91%. \n\n"}
{"id": "1806.05285", "contents": "Title: A Flexible Convolutional Solver with Application to Photorealistic Style\n  Transfer Abstract: We propose a new flexible deep convolutional neural network (convnet) to\nperform fast visual style transfer. In contrast to existing convnets that\naddress the same task, our architecture derives directly from the structure of\nthe gradient descent originally used to solve the style transfer problem [Gatys\net al., 2016]. Like existing convnets, ours approximately solves the original\nproblem much faster than the gradient descent. However, our network is uniquely\nflexible by design: it can be manipulated at runtime to enforce new constraints\non the final solution. In particular, we show how to modify it to obtain a\nphotorealistic result with no retraining. We study the modifications made by\n[Luan et al., 2017] to the original cost function of [Gatys et al., 2016] to\nachieve photorealistic style transfer. These modifications affect directly the\ngradient descent and can be reported on-the-fly in our network. These\nmodifications are possible as the proposed architecture stems from unrolling\nthe gradient descent. \n\n"}
{"id": "1806.06193", "contents": "Title: Large Scale Fine-Grained Categorization and Domain-Specific Transfer\n  Learning Abstract: Transferring the knowledge learned from large scale datasets (e.g., ImageNet)\nvia fine-tuning offers an effective solution for domain-specific fine-grained\nvisual categorization (FGVC) tasks (e.g., recognizing bird species or car make\nand model). In such scenarios, data annotation often calls for specialized\ndomain knowledge and thus is difficult to scale. In this work, we first tackle\na problem in large scale FGVC. Our method won first place in iNaturalist 2017\nlarge scale species classification challenge. Central to the success of our\napproach is a training scheme that uses higher image resolution and deals with\nthe long-tailed distribution of training data. Next, we study transfer learning\nvia fine-tuning from large scale datasets to small scale, domain-specific FGVC\ndatasets. We propose a measure to estimate domain similarity via Earth Mover's\nDistance and demonstrate that transfer learning benefits from pre-training on a\nsource domain that is similar to the target domain by this measure. Our\nproposed transfer learning outperforms ImageNet pre-training and obtains\nstate-of-the-art results on multiple commonly used FGVC datasets. \n\n"}
{"id": "1806.06195", "contents": "Title: Show, Attend and Translate: Unsupervised Image Translation with\n  Self-Regularization and Attention Abstract: Image translation between two domains is a class of problems aiming to learn\nmapping from an input image in the source domain to an output image in the\ntarget domain. It has been applied to numerous domains, such as data\naugmentation, domain adaptation, and unsupervised training. When paired\ntraining data is not accessible, image translation becomes an ill-posed\nproblem. We constrain the problem with the assumption that the translated image\nneeds to be perceptually similar to the original image and also appears to be\ndrawn from the new domain, and propose a simple yet effective image translation\nmodel consisting of a single generator trained with a self-regularization term\nand an adversarial term. We further notice that existing image translation\ntechniques are agnostic to the subjects of interest and often introduce\nunwanted changes or artifacts to the input. Thus we propose to add an attention\nmodule to predict an attention map to guide the image translation process. The\nmodule learns to attend to key parts of the image while keeping everything else\nunaltered, essentially avoiding undesired artifacts or changes. The predicted\nattention map also opens door to applications such as unsupervised segmentation\nand saliency detection. Extensive experiments and evaluations show that our\nmodel while being simpler, achieves significantly better performance than\nexisting image translation methods. \n\n"}
{"id": "1806.06284", "contents": "Title: Latent Convolutional Models Abstract: We present a new latent model of natural images that can be learned on\nlarge-scale datasets. The learning process provides a latent embedding for\nevery image in the training dataset, as well as a deep convolutional network\nthat maps the latent space to the image space. After training, the new model\nprovides a strong and universal image prior for a variety of image restoration\ntasks such as large-hole inpainting, superresolution, and colorization. To\nmodel high-resolution natural images, our approach uses latent spaces of very\nhigh dimensionality (one to two orders of magnitude higher than previous latent\nimage models). To tackle this high dimensionality, we use latent spaces with a\nspecial manifold structure (convolutional manifolds) parameterized by a ConvNet\nof a certain architecture. In the experiments, we compare the learned latent\nmodels with latent models learned by autoencoders, advanced variants of\ngenerative adversarial networks, and a strong baseline system using simpler\nparameterization of the latent space. Our model outperforms the competing\napproaches over a range of restoration tasks. \n\n"}
{"id": "1806.06530", "contents": "Title: Segmentation of Photovoltaic Module Cells in Uncalibrated\n  Electroluminescence Images Abstract: High resolution electroluminescence (EL) images captured in the infrared\nspectrum allow to visually and non-destructively inspect the quality of\nphotovoltaic (PV) modules. Currently, however, such a visual inspection\nrequires trained experts to discern different kinds of defects, which is\ntime-consuming and expensive. Automated segmentation of cells is therefore a\nkey step in automating the visual inspection workflow. In this work, we propose\na robust automated segmentation method for extraction of individual solar cells\nfrom EL images of PV modules. This enables controlled studies on large amounts\nof data to understanding the effects of module degradation over time-a process\nnot yet fully understood. The proposed method infers in several steps a\nhigh-level solar module representation from low-level edge features. An\nimportant step in the algorithm is to formulate the segmentation problem in\nterms of lens calibration by exploiting the plumbline constraint. We evaluate\nour method on a dataset of various solar modules types containing a total of\n408 solar cells with various defects. Our method robustly solves this task with\na median weighted Jaccard index of 94.47% and an $F_1$ score of 97.62%, both\nindicating a very high similarity between automatically segmented and ground\ntruth solar cell masks. \n\n"}
{"id": "1806.06970", "contents": "Title: Deconvolving convolution neural network for cell detection Abstract: Automatic cell detection in histology images is a challenging task due to\nvarying size, shape and features of cells and stain variations across a large\ncohort. Conventional deep learning methods regress the probability of each\npixel belonging to the centre of a cell followed by detection of local maxima.\nWe present deconvolution as an alternate approach to local maxima detection.\nThe ground truth points are convolved with a mapping filter to generate\nartifical labels. A convolutional neural network (CNN) is modified to convolve\nit's output with the same mapping filter and is trained for the mapped labels.\nOutput of the trained CNN is then deconvolved to generate points as cell\ndetection. We compare our method with state-of-the-art deep learning approaches\nwhere the results show that the proposed approach detects cells with\ncomparatively high precision and F1-score. \n\n"}
{"id": "1806.07110", "contents": "Title: Modality Distillation with Multiple Stream Networks for Action\n  Recognition Abstract: Diverse input data modalities can provide complementary cues for several\ntasks, usually leading to more robust algorithms and better performance.\nHowever, while a (training) dataset could be accurately designed to include a\nvariety of sensory inputs, it is often the case that not all modalities could\nbe available in real life (testing) scenarios, where a model has to be\ndeployed. This raises the challenge of how to learn robust representations\nleveraging multimodal data in the training stage, while considering limitations\nat test time, such as noisy or missing modalities.\n  This paper presents a new approach for multimodal video action recognition,\ndeveloped within the unified frameworks of distillation and privileged\ninformation, named generalized distillation. Particularly, we consider the case\nof learning representations from depth and RGB videos, while relying on RGB\ndata only at test time. We propose a new approach to train an hallucination\nnetwork that learns to distill depth features through multiplicative\nconnections of spatiotemporal representations, leveraging soft labels and hard\nlabels, as well as distance between feature maps. We report state-of-the-art\nresults on video action classification on the largest multimodal dataset\navailable for this task, the NTU RGB+D. Code available at\nhttps://github.com/ncgarcia/modality-distillation . \n\n"}
{"id": "1806.07378", "contents": "Title: Localizing and Quantifying Damage in Social Media Images Abstract: Traditional post-disaster assessment of damage heavily relies on expensive\nGIS data, especially remote sensing image data. In recent years, social media\nhas become a rich source of disaster information that may be useful in\nassessing damage at a lower cost. Such information includes text (e.g., tweets)\nor images posted by eyewitnesses of a disaster. Most of the existing research\nexplores the use of text in identifying situational awareness information\nuseful for disaster response teams. The use of social media images to assess\ndisaster damage is limited. In this paper, we propose a novel approach, based\non convolutional neural networks and class activation maps, to locate damage in\na disaster image and to quantify the degree of the damage. Our proposed\napproach enables the use of social network images for post-disaster damage\nassessment and provides an inexpensive and feasible alternative to the more\nexpensive GIS approach. \n\n"}
{"id": "1806.07497", "contents": "Title: Fully Automatic Myocardial Segmentation of Contrast Echocardiography\n  Sequence Using Random Forests Guided by Shape Model Abstract: Myocardial contrast echocardiography (MCE) is an imaging technique that\nassesses left ventricle function and myocardial perfusion for the detection of\ncoronary artery diseases. Automatic MCE perfusion quantification is challenging\nand requires accurate segmentation of the myocardium from noisy and\ntime-varying images. Random forests (RF) have been successfully applied to many\nmedical image segmentation tasks. However, the pixel-wise RF classifier ignores\ncontextual relationships between label outputs of individual pixels. RF which\nonly utilizes local appearance features is also susceptible to data suffering\nfrom large intensity variations. In this paper, we demonstrate how to overcome\nthe above limitations of classic RF by presenting a fully automatic\nsegmentation pipeline for myocardial segmentation in full-cycle 2D MCE data.\nSpecifically, a statistical shape model is used to provide shape prior\ninformation that guide the RF segmentation in two ways. First, a novel shape\nmodel (SM) feature is incorporated into the RF framework to generate a more\naccurate RF probability map. Second, the shape model is fitted to the RF\nprobability map to refine and constrain the final segmentation to plausible\nmyocardial shapes. We further improve the performance by introducing a bounding\nbox detection algorithm as a preprocessing step in the segmentation pipeline.\nOur approach on 2D image is further extended to 2D+t sequence which ensures\ntemporal consistency in the resultant sequence segmentations. When evaluated on\nclinical MCE data, our proposed method achieves notable improvement in\nsegmentation accuracy and outperforms other state-of-the-art methods including\nthe classic RF and its variants, active shape model and image registration. \n\n"}
{"id": "1806.07908", "contents": "Title: Como funciona o Deep Learning Abstract: Deep Learning methods are currently the state-of-the-art in many problems\nwhich can be tackled via machine learning, in particular classification\nproblems. However there is still lack of understanding on how those methods\nwork, why they work and what are the limitations involved in using them. In\nthis chapter we will describe in detail the transition from shallow to deep\nnetworks, include examples of code on how to implement them, as well as the\nmain issues one faces when training a deep network. Afterwards, we introduce\nsome theoretical background behind the use of deep models, and discuss their\nlimitations. \n\n"}
{"id": "1806.07987", "contents": "Title: A Hierarchical Deep Architecture and Mini-Batch Selection Method For\n  Joint Traffic Sign and Light Detection Abstract: Traffic light and sign detectors on autonomous cars are integral for road\nscene perception. The literature is abundant with deep learning networks that\ndetect either lights or signs, not both, which makes them unsuitable for\nreal-life deployment due to the limited graphics processing unit (GPU) memory\nand power available on embedded systems. The root cause of this issue is that\nno public dataset contains both traffic light and sign labels, which leads to\ndifficulties in developing a joint detection framework. We present a deep\nhierarchical architecture in conjunction with a mini-batch proposal selection\nmechanism that allows a network to detect both traffic lights and signs from\ntraining on separate traffic light and sign datasets. Our method solves the\noverlapping issue where instances from one dataset are not labelled in the\nother dataset. We are the first to present a network that performs joint\ndetection on traffic lights and signs. We measure our network on the\nTsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small\nTraffic Lights benchmark for traffic light detection and show it outperforms\nthe existing Bosch Small Traffic light state-of-the-art method. We focus on\nautonomous car deployment and show our network is more suitable than others\nbecause of its low memory footprint and real-time image processing time.\nQualitative results can be viewed at https://youtu.be/_YmogPzBXOw \n\n"}
{"id": "1806.08037", "contents": "Title: Pixel-level Reconstruction and Classification for Noisy Handwritten\n  Bangla Characters Abstract: Classification techniques for images of handwritten characters are\nsusceptible to noise. Quadtrees can be an efficient representation for learning\nfrom sparse features. In this paper, we improve the effectiveness of\nprobabilistic quadtrees by using a pixel level classifier to extract the\ncharacter pixels and remove noise from handwritten character images. The pixel\nlevel denoiser (a deep belief network) uses the map responses obtained from a\npretrained CNN as features for reconstructing the characters eliminating noise.\nWe experimentally demonstrate the effectiveness of our approach by\nreconstructing and classifying a noisy version of handwritten Bangla Numeral\nand Basic Character datasets. \n\n"}
{"id": "1806.08169", "contents": "Title: A convex method for classification of groups of examples Abstract: There are many applications where it important to perform well on a set of\nexamples as opposed to individual examples. For example in image or video\nclassification the question is does an object appear somewhere in the image or\nvideo while there are several candidates of the object per image or video. In\nthis context, it is not important what is the performance per candidate.\nInstead the performance per group is the ultimate objective.\n  For such problems one popular approach assumes weak supervision where labels\nexist for the entire group and then multiple instance learning is utilized.\nAnother approach is to optimize per candidate, assuming each candidate is\nlabeled, in the belief that this will achieve good performance per group.\n  We will show that better results can be achieved if we offer a new\nmethodology which synthesizes the aforementioned approaches and directly\noptimizes for the final optimization objective while consisting of a convex\noptimization problem which solves the global optimization problem. The benefit\nof grouping examples is demonstrated on an image classification task for\ndetecting polyps in images from capsule endoscopy of the colon. The algorithm\nwas designed to efficiently handle hundreds of millions of examples.\nFurthermore, modifications to the penalty function of the standard SVM\nalgorithm, have proven to significantly improve performance in our test case. \n\n"}
{"id": "1806.08174", "contents": "Title: Crowd disagreement about medical images is informative Abstract: Classifiers for medical image analysis are often trained with a single\nconsensus label, based on combining labels given by experts or crowds. However,\ndisagreement between annotators may be informative, and thus removing it may\nnot be the best strategy. As a proof of concept, we predict whether a skin\nlesion from the ISIC 2017 dataset is a melanoma or not, based on crowd\nannotations of visual characteristics of that lesion. We compare using the mean\nannotations, illustrating consensus, to standard deviations and other\ndistribution moments, illustrating disagreement. We show that the mean\nannotations perform best, but that the disagreement measures are still\ninformative. We also make the crowd annotations used in this paper available at\n\\url{https://figshare.com/s/5cbbce14647b66286544}. \n\n"}
{"id": "1806.08503", "contents": "Title: Global Semantic Consistency for Zero-Shot Learning Abstract: In image recognition, there are many cases where training samples cannot\ncover all target classes. Zero-shot learning (ZSL) utilizes the class semantic\ninformation to classify samples of the unseen categories that have no\ncorresponding samples contained in the training set. In this paper, we propose\nan end-to-end framework, called Global Semantic Consistency Network (GSC-Net\nfor short), which makes complete use of the semantic information of both seen\nand unseen classes, to support effective zero-shot learning. We also adopt a\nsoft label embedding loss to further exploit the semantic relationships among\nclasses. To adapt GSC-Net to a more practical setting, Generalized Zero-shot\nLearning (GZSL), we introduce a parametric novelty detection mechanism. Our\napproach achieves the state-of-the-art performance on both ZSL and GZSL tasks\nover three visual attribute datasets, which validates the effectiveness and\nadvantage of the proposed framework. \n\n"}
{"id": "1806.08864", "contents": "Title: VUNet: Dynamic Scene View Synthesis for Traversability Estimation using\n  an RGB Camera Abstract: We present VUNet, a novel view(VU) synthesis method for mobile robots in\ndynamic environments, and its application to the estimation of future\ntraversability. Our method predicts future images for given virtual robot\nvelocity commands using only RGB images at previous and current time steps. The\nfuture images result from applying two types of image changes to the previous\nand current images: 1) changes caused by different camera pose, and 2) changes\ndue to the motion of the dynamic obstacles. We learn to predict these two types\nof changes disjointly using two novel network architectures, SNet and DNet. We\ncombine SNet and DNet to synthesize future images that we pass to our\npreviously presented method GONet to estimate the traversable areas around the\nrobot. Our quantitative and qualitative evaluation indicate that our approach\nfor view synthesis predicts accurate future images in both static and dynamic\nenvironments. We also show that these virtual images can be used to estimate\nfuture traversability correctly. We apply our view synthesis-based\ntraversability estimation method to two applications for assisted\nteleoperation. \n\n"}
{"id": "1806.09045", "contents": "Title: Variational Wasserstein Clustering Abstract: We propose a new clustering method based on optimal transportation. We solve\noptimal transportation with variational principles, and investigate the use of\npower diagrams as transportation plans for aggregating arbitrary domains into a\nfixed number of clusters. We iteratively drive centroids through target domains\nwhile maintaining the minimum clustering energy by adjusting the power\ndiagrams. Thus, we simultaneously pursue clustering and the Wasserstein\ndistances between the centroids and the target domains, resulting in a\nmeasure-preserving mapping. We demonstrate the use of our method in domain\nadaptation, remeshing, and representation learning on synthetic and real data. \n\n"}
{"id": "1806.09152", "contents": "Title: SSIMLayer: Towards Robust Deep Representation Learning via Nonlinear\n  Structural Similarity Abstract: Deeper convolutional neural networks provide more capacity to approximate\ncomplex mapping functions. However, increasing network depth imposes\ndifficulties on training and increases model complexity. This paper presents a\nnew nonlinear computational layer of considerably high capacity to the deep\nconvolutional neural network architectures. This layer performs a set of\ncomprehensive convolution operations that mimics the overall function of the\nhuman visual system (HVS) via focusing on learning structural information in\nits input. The core of its computations is evaluating the components of the\nstructural similarity metric (SSIM) in a setting that allows the kernels to\nlearn to match structural information. The proposed SSIMLayer is inherently\nnonlinear and hence, it does not require subsequent nonlinear transformations.\nExperiments conducted on CIFAR-10 benchmark demonstrates that the SSIMLayer\nprovides better convergence than the traditional convolutional layer, bypasses\nthe need for nonlinear transformations and shows more robustness against noise\nperturbations and adversarial attacks. \n\n"}
{"id": "1806.09346", "contents": "Title: Sparse 3D Point-cloud Map Upsampling and Noise Removal as a vSLAM\n  Post-processing Step: Experimental Evaluation Abstract: The monocular vision-based simultaneous localization and mapping (vSLAM) is\none of the most challenging problem in mobile robotics and computer vision. In\nthis work we study the post-processing techniques applied to sparse 3D\npoint-cloud maps, obtained by feature-based vSLAM algorithms. Map\npost-processing is split into 2 major steps: 1) noise and outlier removal and\n2) upsampling. We evaluate different combinations of known algorithms for\noutlier removing and upsampling on datasets of real indoor and outdoor\nenvironments and identify the most promising combination. We further use it to\nconvert a point-cloud map, obtained by the real UAV performing indoor flight to\n3D voxel grid (octo-map) potentially suitable for path planning. \n\n"}
{"id": "1806.09648", "contents": "Title: 3D Context Enhanced Region-based Convolutional Neural Network for\n  End-to-End Lesion Detection Abstract: Detecting lesions from computed tomography (CT) scans is an important but\ndifficult problem because non-lesions and true lesions can appear similar. 3D\ncontext is known to be helpful in this differentiation task. However, existing\nend-to-end detection frameworks of convolutional neural networks (CNNs) are\nmostly designed for 2D images. In this paper, we propose 3D context enhanced\nregion-based CNN (3DCE) to incorporate 3D context information efficiently by\naggregating feature maps of 2D images. 3DCE is easy to train and end-to-end in\ntraining and inference. A universal lesion detector is developed to detect all\nkinds of lesions in one algorithm using the DeepLesion dataset. Experimental\nresults on this challenging task prove the effectiveness of 3DCE. We have\nreleased the code of 3DCE in\nhttps://github.com/rsummers11/CADLab/tree/master/lesion_detector_3DCE. \n\n"}
{"id": "1806.10319", "contents": "Title: Exploiting Spatial-Temporal Modelling and Multi-Modal Fusion for Human\n  Action Recognition Abstract: In this report, our approach to tackling the task of ActivityNet 2018\nKinetics-600 challenge is described in detail. Though spatial-temporal\nmodelling methods, which adopt either such end-to-end framework as I3D\n\\cite{i3d} or two-stage frameworks (i.e., CNN+RNN), have been proposed in\nexisting state-of-the-arts for this task, video modelling is far from being\nwell solved. In this challenge, we propose spatial-temporal network (StNet) for\nbetter joint spatial-temporal modelling and comprehensively video\nunderstanding. Besides, given that multi-modal information is contained in\nvideo source, we manage to integrate both early-fusion and later-fusion\nstrategy of multi-modal information via our proposed improved temporal Xception\nnetwork (iTXN) for video understanding. Our StNet RGB single model achieves\n78.99\\% top-1 precision in the Kinetics-600 validation set and that of our\nimproved temporal Xception network which integrates RGB, flow and audio\nmodalities is up to 82.35\\%. After model ensemble, we achieve top-1 precision\nas high as 85.0\\% on the validation set and rank No.1 among all submissions. \n\n"}
{"id": "1806.10417", "contents": "Title: Divergence-Free Shape Interpolation and Correspondence Abstract: We present a novel method to model and calculate deformation fields between\nshapes embedded in $\\mathbb{R}^D$. Our framework combines naturally\ninterpolating the two input shapes and calculating correspondences at the same\ntime. The key idea is to compute a divergence-free deformation field\nrepresented in a coarse-to-fine basis using the Karhunen-Lo\\`eve expansion. The\nadvantages are that there is no need to discretize the embedding space and the\ndeformation is volume-preserving. Furthermore, the optimization is done on\ndownsampled versions of the shapes but the morphing can be applied to any\nresolution without a heavy increase in complexity. We show results for shape\ncorrespondence, registration, inter- and extrapolation on the TOSCA and FAUST\ndata sets. \n\n"}
{"id": "1806.10681", "contents": "Title: Dynamic texture analysis with diffusion in networks Abstract: Dynamic texture is a field of research that has gained considerable interest\nfrom computer vision community due to the explosive growth of multimedia\ndatabases. In addition, dynamic texture is present in a wide range of videos,\nwhich makes it very important in expert systems based on videos such as medical\nsystems, traffic monitoring systems, forest fire detection system, among\nothers. In this paper, a new method for dynamic texture characterization based\non diffusion in directed networks is proposed. The dynamic texture is modeled\nas a directed network. The method consists in the analysis of the dynamic of\nthis network after a series of graph cut transformations based on the edge\nweights. For each network transformation, the activity for each vertex is\nestimated. The activity is the relative frequency that one vertex is visited by\nrandom walks in balance. Then, texture descriptor is constructed by\nconcatenating the activity histograms. The main contributions of this paper are\nthe use of directed network modeling and diffusion in network to dynamic\ntexture characterization. These tend to provide better performance in dynamic\ntextures classification. Experiments with rotation and interference of the\nmotion pattern were conducted in order to demonstrate the robustness of the\nmethod. The proposed approach is compared to other dynamic texture methods on\ntwo very well know dynamic texture database and on traffic condition\nclassification, and outperform in most of the cases. \n\n"}
{"id": "1806.10748", "contents": "Title: Towards automatic initialization of registration algorithms using\n  simulated endoscopy images Abstract: Registering images from different modalities is an active area of research in\ncomputer aided medical interventions. Several registration algorithms have been\ndeveloped, many of which achieve high accuracy. However, these results are\ndependent on many factors, including the quality of the extracted features or\nsegmentations being registered as well as the initial alignment. Although\nseveral methods have been developed towards improving segmentation algorithms\nand automating the segmentation process, few automatic initialization\nalgorithms have been explored. In many cases, the initial alignment from which\na registration is initiated is performed manually, which interferes with the\nclinical workflow. Our aim is to use scene classification in endoscopic\nprocedures to achieve coarse alignment of the endoscope and a preoperative\nimage of the anatomy. In this paper, we show using simulated scenes that a\nneural network can predict the region of anatomy (with respect to a\npreoperative image) that the endoscope is located in by observing a single\nendoscopic video frame. With limited training and without any hyperparameter\ntuning, our method achieves an accuracy of 76.53 (+/-1.19)%. There are several\navenues for improvement, making this a promising direction of research. Code is\navailable at https://github.com/AyushiSinha/AutoInitialization. \n\n"}
{"id": "1806.10982", "contents": "Title: High Diversity Attribute Guided Face Generation with GANs Abstract: In this work we focused on GAN-based solution for the attribute guided face\nsynthesis. Previous works exploited GANs for generation of photo-realistic face\nimages and did not pay attention to the question of diversity of the resulting\nimages. The proposed solution in its turn introducing novel latent space of\nunit complex numbers is able to provide the diversity on the \"birthday paradox\"\nscore 3 times higher than the size of the training dataset. It is important to\nemphasize that our result is shown on relatively small dataset (20k samples vs\n200k) while preserving photo-realistic properties of generated faces on\nsignificantly higher resolution (128x128 in comparison to 32x32 of previous\nworks). \n\n"}
{"id": "1806.11146", "contents": "Title: Adversarial Reprogramming of Neural Networks Abstract: Deep neural networks are susceptible to \\emph{adversarial} attacks. In\ncomputer vision, well-crafted perturbations to images can cause neural networks\nto make mistakes such as confusing a cat with a computer. Previous adversarial\nattacks have been designed to degrade performance of models or cause machine\nlearning models to produce specific outputs chosen ahead of time by the\nattacker. We introduce attacks that instead {\\em reprogram} the target model to\nperform a task chosen by the attacker---without the attacker needing to specify\nor compute the desired output for each test-time input. This attack finds a\nsingle adversarial perturbation, that can be added to all test-time inputs to a\nmachine learning model in order to cause the model to perform a task chosen by\nthe adversary---even if the model was not trained to do this task. These\nperturbations can thus be considered a program for the new task. We demonstrate\nadversarial reprogramming on six ImageNet classification models, repurposing\nthese models to perform a counting task, as well as classification tasks:\nclassification of MNIST and CIFAR-10 examples presented as inputs to the\nImageNet model. \n\n"}
{"id": "1806.11191", "contents": "Title: CR-GAN: Learning Complete Representations for Multi-view Generation Abstract: Generating multi-view images from a single-view input is an essential yet\nchallenging problem. It has broad applications in vision, graphics, and\nrobotics. Our study indicates that the widely-used generative adversarial\nnetwork (GAN) may learn \"incomplete\" representations due to the single-pathway\nframework: an encoder-decoder network followed by a discriminator network. We\npropose CR-GAN to address this problem. In addition to the single\nreconstruction path, we introduce a generation sideway to maintain the\ncompleteness of the learned embedding space. The two learning pathways\ncollaborate and compete in a parameter-sharing manner, yielding considerably\nimproved generalization ability to \"unseen\" dataset. More importantly, the\ntwo-pathway framework makes it possible to combine both labeled and unlabeled\ndata for self-supervised learning, which further enriches the embedding space\nfor realistic generations. The experimental results prove that CR-GAN\nsignificantly outperforms state-of-the-art methods, especially when generating\nfrom \"unseen\" inputs in wild conditions. \n\n"}
{"id": "1806.11216", "contents": "Title: Adversarial and Perceptual Refinement for Compressed Sensing MRI\n  Reconstruction Abstract: Deep learning approaches have shown promising performance for compressed\nsensing-based Magnetic Resonance Imaging. While deep neural networks trained\nwith mean squared error (MSE) loss functions can achieve high peak signal to\nnoise ratio, the reconstructed images are often blurry and lack sharp details,\nespecially for higher undersampling rates. Recently, adversarial and perceptual\nloss functions have been shown to achieve more visually appealing results.\nHowever, it remains an open question how to (1) optimally combine these loss\nfunctions with the MSE loss function and (2) evaluate such a perceptual\nenhancement. In this work, we propose a hybrid method, in which a visual\nrefinement component is learnt on top of an MSE loss-based reconstruction\nnetwork. In addition, we introduce a semantic interpretability score, measuring\nthe visibility of the region of interest in both ground truth and reconstructed\nimages, which allows us to objectively quantify the usefulness of the image\nquality for image post-processing and analysis. Applied on a large cardiac MRI\ndataset simulated with 8-fold undersampling, we demonstrate significant\nimprovements ($p<0.01$) over the state-of-the-art in both a human observer\nstudy and the semantic interpretability score. \n\n"}
{"id": "1806.11475", "contents": "Title: SynNet: Structure-Preserving Fully Convolutional Networks for Medical\n  Image Synthesis Abstract: Cross modal image syntheses is gaining significant interests for its ability\nto estimate target images of a different modality from a given set of source\nimages,like estimating MR to MR, MR to CT, CT to PET etc, without the need for\nan actual acquisition.Though they show potential for applications in radiation\ntherapy planning,image super resolution, atlas construction, image segmentation\netc.The synthesis results are not as accurate as the actual acquisition.In this\npaper,we address the problem of multi modal image synthesis by proposing a\nfully convolutional deep learning architecture called the SynNet.We extend the\nproposed architecture for various input output configurations. And finally, we\npropose a structure preserving custom loss function for cross-modal image\nsynthesis.We validate the proposed SynNet and its extended framework on BRATS\ndataset with comparisons against three state-of-the art methods.And the results\nof the proposed custom loss function is validated against the traditional loss\nfunction used by the state-of-the-art methods for cross modal image synthesis. \n\n"}
{"id": "1807.00273", "contents": "Title: Photorealistic Style Transfer for Videos Abstract: Photorealistic style transfer is a technique which transfers colour from one\nreference domain to another domain by using deep learning and optimization\ntechniques. Here, we present a technique which we use to transfer style and\ncolour from a reference image to a video. \n\n"}
{"id": "1807.00284", "contents": "Title: Autonomous Deep Learning: A Genetic DCNN Designer for Image\n  Classification Abstract: Recent years have witnessed the breakthrough success of deep convolutional\nneural networks (DCNNs) in image classification and other vision applications.\nAlthough freeing users from the troublesome handcrafted feature extraction by\nproviding a uniform feature extraction-classification framework, DCNNs still\nrequire a handcrafted design of their architectures. In this paper, we propose\nthe genetic DCNN designer, an autonomous learning algorithm can generate a DCNN\narchitecture automatically based on the data available for a specific image\nclassification problem. We first partition a DCNN into multiple stacked meta\nconvolutional blocks and fully connected blocks, each containing the operations\nof convolution, pooling, fully connection, batch normalization, activation and\ndrop out, and thus convert the architecture into an integer vector. Then, we\nuse refined evolutionary operations, including selection, mutation and\ncrossover to evolve a population of DCNN architectures. Our results on the\nMNIST, Fashion-MNIST, EMNISTDigit, EMNIST-Letter, CIFAR10 and CIFAR100 datasets\nsuggest that the proposed genetic DCNN designer is able to produce\nautomatically DCNN architectures, whose performance is comparable to, if not\nbetter than, that of stateof- the-art DCNN models \n\n"}
{"id": "1807.00578", "contents": "Title: Classifying neuromorphic data using a deep learning framework for image\n  classification Abstract: In the field of artificial intelligence, neuromorphic computing has been\naround for several decades. Deep learning has however made much recent progress\nsuch that it consistently outperforms neuromorphic learning algorithms in\nclassification tasks in terms of accuracy. Specifically in the field of image\nclassification, neuromorphic computing has been traditionally using either the\ntemporal or rate code for encoding static images in datasets into spike trains.\nIt is only till recently, that neuromorphic vision sensors are widely used by\nthe neuromorphic research community, and provides an alternative to such\nencoding methods. Since then, several neuromorphic datasets as obtained by\napplying such sensors on image datasets (e.g. the neuromorphic CALTECH 101)\nhave been introduced. These data are encoded in spike trains and hence seem\nideal for benchmarking of neuromorphic learning algorithms. Specifically, we\ntrain a deep learning framework used for image classification on the CALTECH\n101 and a collapsed version of the neuromorphic CALTECH 101 datasets. We\nobtained an accuracy of 91.66% and 78.01% for the CALTECH 101 and neuromorphic\nCALTECH 101 datasets respectively. For CALTECH 101, our accuracy is close to\nthe best reported accuracy, while for neuromorphic CALTECH 101, it outperforms\nthe last best reported accuracy by over 10%. This raises the question of the\nsuitability of such datasets as benchmarks for neuromorphic learning\nalgorithms. \n\n"}
{"id": "1807.00959", "contents": "Title: SymmNet: A Symmetric Convolutional Neural Network for Occlusion\n  Detection Abstract: Detecting the occlusion from stereo images or video frames is important to\nmany computer vision applications. Previous efforts focus on bundling it with\nthe computation of disparity or optical flow, leading to a chicken-and-egg\nproblem. In this paper, we leverage convolutional neural network to liberate\nthe occlusion detection task from the interleaved, traditional calculation\nframework. We propose a Symmetric Network (SymmNet) to directly exploit\ninformation from an image pair, without estimating disparity or motion in\nadvance. The proposed network is structurally left-right symmetric to learn the\nbinocular occlusion simultaneously, aimed at jointly improving both results.\nThe comprehensive experiments show that our model achieves state-of-the-art\nresults on detecting the stereo and motion occlusion. \n\n"}
{"id": "1807.00980", "contents": "Title: MetaAnchor: Learning to Detect Objects with Customized Anchors Abstract: We propose a novel and flexible anchor mechanism named MetaAnchor for object\ndetection frameworks. Unlike many previous detectors model anchors via a\npredefined manner, in MetaAnchor anchor functions could be dynamically\ngenerated from the arbitrary customized prior boxes. Taking advantage of weight\nprediction, MetaAnchor is able to work with most of the anchor-based object\ndetection systems such as RetinaNet. Compared with the predefined anchor\nscheme, we empirically find that MetaAnchor is more robust to anchor settings\nand bounding box distributions; in addition, it also shows the potential on\ntransfer tasks. Our experiment on COCO detection task shows that MetaAnchor\nconsistently outperforms the counterparts in various scenarios. \n\n"}
{"id": "1807.01172", "contents": "Title: Accurate Weakly-Supervised Deep Lesion Segmentation using Large-Scale\n  Clinical Annotations: Slice-Propagated 3D Mask Generation from 2D RECIST Abstract: Volumetric lesion segmentation from computed tomography (CT) images is a\npowerful means to precisely assess multiple time-point lesion/tumor changes.\nHowever, because manual 3D segmentation is prohibitively time consuming,\ncurrent practices rely on an imprecise surrogate called response evaluation\ncriteria in solid tumors (RECIST). Despite their coarseness, RECIST markers are\ncommonly found in current hospital picture and archiving systems (PACS),\nmeaning they can provide a potentially powerful, yet extraordinarily\nchallenging, source of weak supervision for full 3D segmentation. Toward this\nend, we introduce a convolutional neural network (CNN) based weakly supervised\nslice-propagated segmentation (WSSS) method to 1) generate the initial lesion\nsegmentation on the axial RECIST-slice; 2) learn the data distribution on\nRECIST-slices; 3) extrapolate to segment the whole lesion slice by slice to\nfinally obtain a volumetric segmentation. To validate the proposed method, we\nfirst test its performance on a fully annotated lymph node dataset, where WSSS\nperforms comparably to its fully supervised counterparts. We then test on a\ncomprehensive lesion dataset with 32,735 RECIST marks, where we report a mean\nDice score of 92% on RECIST-marked slices and 76% on the entire 3D volumes. \n\n"}
{"id": "1807.01216", "contents": "Title: Local Gradients Smoothing: Defense against localized adversarial attacks Abstract: Deep neural networks (DNNs) have shown vulnerability to adversarial attacks,\ni.e., carefully perturbed inputs designed to mislead the network at inference\ntime. Recently introduced localized attacks, Localized and Visible Adversarial\nNoise (LaVAN) and Adversarial patch, pose a new challenge to deep learning\nsecurity by adding adversarial noise only within a specific region without\naffecting the salient objects in an image. Driven by the observation that such\nattacks introduce concentrated high-frequency changes at a particular image\nlocation, we have developed an effective method to estimate noise location in\ngradient domain and transform those high activation regions caused by\nadversarial noise in image domain while having minimal effect on the salient\nobject that is important for correct classification. Our proposed Local\nGradients Smoothing (LGS) scheme achieves this by regularizing gradients in the\nestimated noisy region before feeding the image to DNN for inference. We have\nshown the effectiveness of our method in comparison to other defense methods\nincluding Digital Watermarking, JPEG compression, Total Variance Minimization\n(TVM) and Feature squeezing on ImageNet dataset. In addition, we systematically\nstudy the robustness of the proposed defense mechanism against Back Pass\nDifferentiable Approximation (BPDA), a state of the art attack recently\ndeveloped to break defenses that transform an input sample to minimize the\nadversarial effect. Compared to other defense mechanisms, LGS is by far the\nmost resistant to BPDA in localized adversarial attack setting. \n\n"}
{"id": "1807.01670", "contents": "Title: Encoding Spatial Relations from Natural Language Abstract: Natural language processing has made significant inroads into learning the\nsemantics of words through distributional approaches, however representations\nlearnt via these methods fail to capture certain kinds of information implicit\nin the real world. In particular, spatial relations are encoded in a way that\nis inconsistent with human spatial reasoning and lacking invariance to\nviewpoint changes. We present a system capable of capturing the semantics of\nspatial relations such as behind, left of, etc from natural language. Our key\ncontributions are a novel multi-modal objective based on generating images of\nscenes from their textual descriptions, and a new dataset on which to train it.\nWe demonstrate that internal representations are robust to meaning preserving\ntransformations of descriptions (paraphrase invariance), while viewpoint\ninvariance is an emergent property of the system. \n\n"}
{"id": "1807.02257", "contents": "Title: Dynamic Multimodal Instance Segmentation guided by natural language\n  queries Abstract: We address the problem of segmenting an object given a natural language\nexpression that describes it. Current techniques tackle this task by either\n(\\textit{i}) directly or recursively merging linguistic and visual information\nin the channel dimension and then performing convolutions; or by (\\textit{ii})\nmapping the expression to a space in which it can be thought of as a filter,\nwhose response is directly related to the presence of the object at a given\nspatial coordinate in the image, so that a convolution can be applied to look\nfor the object. We propose a novel method that integrates these two insights in\norder to fully exploit the recursive nature of language. Additionally, during\nthe upsampling process, we take advantage of the intermediate information\ngenerated when downsampling the image, so that detailed segmentations can be\nobtained. We compare our method against the state-of-the-art approaches in four\nstandard datasets, in which it surpasses all previous methods in six of eight\nof the splits for this task. \n\n"}
{"id": "1807.02370", "contents": "Title: Deep Back Projection for Sparse-View CT Reconstruction Abstract: Filtered back projection (FBP) is a classical method for image reconstruction\nfrom sinogram CT data. FBP is computationally efficient but produces lower\nquality reconstructions than more sophisticated iterative methods, particularly\nwhen the number of views is lower than the number required by the Nyquist rate.\nIn this paper, we use a deep convolutional neural network (CNN) to produce\nhigh-quality reconstructions directly from sinogram data. A primary novelty of\nour approach is that we first back project each view separately to form a stack\nof back projections and then feed this stack as input into the convolutional\nneural network. These single-view back projections convert the encoding of\nsinogram data into the appropriate spatial location, which can then be\nleveraged by the spatial invariance of the CNN to learn the reconstruction\neffectively. We demonstrate the benefit of our CNN based back projection on\nsimulated sparse-view CT data over classical FBP. \n\n"}
{"id": "1807.02716", "contents": "Title: A Deep-Learning-Based Geological Parameterization for History Matching\n  Complex Models Abstract: A new low-dimensional parameterization based on principal component analysis\n(PCA) and convolutional neural networks (CNN) is developed to represent complex\ngeological models. The CNN-PCA method is inspired by recent developments in\ncomputer vision using deep learning. CNN-PCA can be viewed as a generalization\nof an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA\nentail post-processing a PCA model to better honor complex geological features.\nIn CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new\nregularization involving a set of metrics for multipoint statistics is\nintroduced. The metrics are based on summary statistics of the nonlinear filter\nresponses of geological models to a pre-trained deep CNN. In addition, in the\nCNN-PCA formulation presented here, a convolutional neural network is trained\nas an explicit transform function that can post-process PCA models quickly.\nCNN-PCA is shown to provide both unconditional and conditional realizations\nthat honor the geological features present in reference SGeMS geostatistical\nrealizations for a binary channelized system. Flow statistics obtained through\nsimulation of random CNN-PCA models closely match results for random SGeMS\nmodels for a demanding case in which O-PCA models lead to significant\ndiscrepancies. Results for history matching are also presented. In this\nassessment CNN-PCA is applied with derivative-free optimization, and a subspace\nrandomized maximum likelihood method is used to provide multiple posterior\nmodels. Data assimilation and significant uncertainty reduction are achieved\nfor existing wells, and physically reasonable predictions are also obtained for\nnew wells. Finally, the CNN-PCA method is extended to a more complex\nnon-stationary bimodal deltaic fan system, and is shown to provide high-quality\nrealizations for this challenging example. \n\n"}
{"id": "1807.02758", "contents": "Title: Image Super-Resolution Using Very Deep Residual Channel Attention\n  Networks Abstract: Convolutional neural network (CNN) depth is of crucial importance for image\nsuper-resolution (SR). However, we observe that deeper networks for image SR\nare more difficult to train. The low-resolution inputs and features contain\nabundant low-frequency information, which is treated equally across channels,\nhence hindering the representational ability of CNNs. To solve these problems,\nwe propose the very deep residual channel attention networks (RCAN).\nSpecifically, we propose a residual in residual (RIR) structure to form very\ndeep network, which consists of several residual groups with long skip\nconnections. Each residual group contains some residual blocks with short skip\nconnections. Meanwhile, RIR allows abundant low-frequency information to be\nbypassed through multiple skip connections, making the main network focus on\nlearning high-frequency information. Furthermore, we propose a channel\nattention mechanism to adaptively rescale channel-wise features by considering\ninterdependencies among channels. Extensive experiments show that our RCAN\nachieves better accuracy and visual improvements against state-of-the-art\nmethods. \n\n"}
{"id": "1807.02802", "contents": "Title: Revisiting Distillation and Incremental Classifier Learning Abstract: One of the key differences between the learning mechanism of humans and\nArtificial Neural Networks (ANNs) is the ability of humans to learn one task at\na time. ANNs, on the other hand, can only learn multiple tasks simultaneously.\nAny attempts at learning new tasks incrementally cause them to completely\nforget about previous tasks. This lack of ability to learn incrementally,\ncalled Catastrophic Forgetting, is considered a major hurdle in building a true\nAI system. In this paper, our goal is to isolate the truly effective existing\nideas for incremental learning from those that only work under certain\nconditions. To this end, we first thoroughly analyze the current state of the\nart (iCaRL) method for incremental learning and demonstrate that the good\nperformance of the system is not because of the reasons presented in the\nexisting literature. We conclude that the success of iCaRL is primarily due to\nknowledge distillation and recognize a key limitation of knowledge\ndistillation, i.e, it often leads to bias in classifiers. Finally, we propose a\ndynamic threshold moving algorithm that is able to successfully remove this\nbias. We demonstrate the effectiveness of our algorithm on CIFAR100 and MNIST\ndatasets showing near-optimal results. Our implementation is available at\nhttps://github.com/Khurramjaved96/incremental-learning. \n\n"}
{"id": "1807.02925", "contents": "Title: Vehicle Image Generation Going Well with The Surroundings Abstract: Since the generative neural networks have made a breakthrough in the image\ngeneration problem, lots of researches on their applications have been studied\nsuch as image restoration, style transfer and image completion. However, there\nhas been few research generating objects in uncontrolled real-world\nenvironments. In this paper, we propose a novel approach for vehicle image\ngeneration in real-world scenes. Using a subnetwork based on a precedent work\nof image completion, our model makes the shape of an object. Details of objects\nare trained by an additional colorization and refinement subnetwork, resulting\nin a better quality of generated objects. Unlike many other works, our method\ndoes not require any segmentation layout but still makes a plausible vehicle in\nthe image. We evaluate our method by using images from Berkeley Deep Drive\n(BDD) and Cityscape datasets, which are widely used for object detection and\nimage segmentation problems. The adequacy of the generated images by the\nproposed method has also been evaluated using a widely utilized object\ndetection algorithm and the FID score. \n\n"}
{"id": "1807.02975", "contents": "Title: Polarimetric Convolutional Network for PolSAR Image Classification Abstract: The approaches for analyzing the polarimetric scattering matrix of\npolarimetric synthetic aperture radar (PolSAR) data have always been the focus\nof PolSAR image classification. Generally, the polarization coherent matrix and\nthe covariance matrix obtained by the polarimetric scattering matrix only show\na limited number of polarimetric information. In order to solve this problem,\nwe propose a sparse scattering coding way to deal with polarimetric scattering\nmatrix and obtain a close complete feature. This encoding mode can also\nmaintain polarimetric information of scattering matrix completely. At the same\ntime, in view of this encoding way, we design a corresponding classification\nalgorithm based on convolution network to combine this feature. Based on sparse\nscattering coding and convolution neural network, the polarimetric\nconvolutional network is proposed to classify PolSAR images by making full use\nof polarimetric information. We perform the experiments on the PolSAR images\nacquired by AIRSAR and RADARSAT-2 to verify the proposed method. The\nexperimental results demonstrate that the proposed method get better results\nand has huge potential for PolSAR data classification. Source code for sparse\nscattering coding is available at\nhttps://github.com/liuxuvip/Polarimetric-Scattering-Coding. \n\n"}
{"id": "1807.03057", "contents": "Title: Deriving Neural Network Architectures using Precision Learning:\n  Parallel-to-fan beam Conversion Abstract: In this paper, we derive a neural network architecture based on an analytical\nformulation of the parallel-to-fan beam conversion problem following the\nconcept of precision learning. The network allows to learn the unknown\noperators in this conversion in a data-driven manner avoiding interpolation and\npotential loss of resolution. Integration of known operators results in a small\nnumber of trainable parameters that can be estimated from synthetic data only.\nThe concept is evaluated in the context of Hybrid MRI/X-ray imaging where\ntransformation of the parallel-beam MRI projections to fan-beam X-ray\nprojections is required. The proposed method is compared to a traditional\nrebinning method. The results demonstrate that the proposed method is superior\nto ray-by-ray interpolation and is able to deliver sharper images using the\nsame amount of parallel-beam input projections which is crucial for\ninterventional applications. We believe that this approach forms a basis for\nfurther work uniting deep learning, signal processing, physics, and traditional\npattern recognition. \n\n"}
{"id": "1807.03343", "contents": "Title: Complex Fully Convolutional Neural Networks for MR Image Reconstruction Abstract: Undersampling the k-space data is widely adopted for acceleration of Magnetic\nResonance Imaging (MRI). Current deep learning based approaches for supervised\nlearning of MRI image reconstruction employ real-valued operations and\nrepresentations by treating complex valued k-space/spatial-space as real\nvalues. In this paper, we propose complex dense fully convolutional neural\nnetwork ($\\mathbb{C}$DFNet) for learning to de-alias the reconstruction\nartifacts within undersampled MRI images. We fashioned a densely-connected\nfully convolutional block tailored for complex-valued inputs by introducing\ndedicated layers such as complex convolution, batch normalization,\nnon-linearities etc. $\\mathbb{C}$DFNet leverages the inherently complex-valued\nnature of input k-space and learns richer representations. We demonstrate\nimproved perceptual quality and recovery of anatomical structures through\n$\\mathbb{C}$DFNet in contrast to its real-valued counterparts. \n\n"}
{"id": "1807.03380", "contents": "Title: An Attention Model for group-level emotion recognition Abstract: In this paper we propose a new approach for classifying the global emotion of\nimages containing groups of people. To achieve this task, we consider two\ndifferent and complementary sources of information: i) a global representation\nof the entire image (ii) a local representation where only faces are\nconsidered. While the global representation of the image is learned with a\nconvolutional neural network (CNN), the local representation is obtained by\nmerging face features through an attention mechanism. The two representations\nare first learned independently with two separate CNN branches and then fused\nthrough concatenation in order to obtain the final group-emotion classifier.\nFor our submission to the EmotiW 2018 group-level emotion recognition\nchallenge, we combine several variations of the proposed model into an\nensemble, obtaining a final accuracy of 64.83% on the test set and ranking 4th\namong all challenge participants. \n\n"}
{"id": "1807.03776", "contents": "Title: CIRL: Controllable Imitative Reinforcement Learning for Vision-based\n  Self-driving Abstract: Autonomous urban driving navigation with complex multi-agent dynamics is\nunder-explored due to the difficulty of learning an optimal driving policy. The\ntraditional modular pipeline heavily relies on hand-designed rules and the\npre-processing perception system while the supervised learning-based models are\nlimited by the accessibility of extensive human experience. We present a\ngeneral and principled Controllable Imitative Reinforcement Learning (CIRL)\napproach which successfully makes the driving agent achieve higher success\nrates based on only vision inputs in a high-fidelity car simulator. To\nalleviate the low exploration efficiency for large continuous action space that\noften prohibits the use of classical RL on challenging real tasks, our CIRL\nexplores over a reasonably constrained action space guided by encoded\nexperiences that imitate human demonstrations, building upon Deep Deterministic\nPolicy Gradient (DDPG). Moreover, we propose to specialize adaptive policies\nand steering-angle reward designs for different control signals (i.e. follow,\nstraight, turn right, turn left) based on the shared representations to improve\nthe model capability in tackling with diverse cases. Extensive experiments on\nCARLA driving benchmark demonstrate that CIRL substantially outperforms all\nprevious methods in terms of the percentage of successfully completed episodes\non a variety of goal-directed driving tasks. We also show its superior\ngeneralization capability in unseen environments. To our knowledge, this is the\nfirst successful case of the learned driving policy through reinforcement\nlearning in the high-fidelity simulator, which performs better-than supervised\nimitation learning. \n\n"}
{"id": "1807.04200", "contents": "Title: With Friends Like These, Who Needs Adversaries? Abstract: The vulnerability of deep image classification networks to adversarial attack\nis now well known, but less well understood. Via a novel experimental analysis,\nwe illustrate some facts about deep convolutional networks for image\nclassification that shed new light on their behaviour and how it connects to\nthe problem of adversaries. In short, the celebrated performance of these\nnetworks and their vulnerability to adversarial attack are simply two sides of\nthe same coin: the input image-space directions along which the networks are\nmost vulnerable to attack are the same directions which they use to achieve\ntheir classification performance in the first place. We develop this result in\ntwo main steps. The first uncovers the fact that classes tend to be associated\nwith specific image-space directions. This is shown by an examination of the\nclass-score outputs of nets as functions of 1D movements along these\ndirections. This provides a novel perspective on the existence of universal\nadversarial perturbations. The second is a clear demonstration of the tight\ncoupling between classification performance and vulnerability to adversarial\nattack within the spaces spanned by these directions. Thus, our analysis\nresolves the apparent contradiction between accuracy and vulnerability. It\nprovides a new perspective on much of the prior art and reveals profound\nimplications for efforts to construct neural nets that are both accurate and\nrobust to adversarial attack. \n\n"}
{"id": "1807.04465", "contents": "Title: Competitive Analysis System for Theatrical Movie Releases Based on Movie\n  Trailer Deep Video Representation Abstract: Audience discovery is an important activity at major movie studios. Deep\nmodels that use convolutional networks to extract frame-by-frame features of a\nmovie trailer and represent it in a form that is suitable for prediction are\nnow possible thanks to the availability of pre-built feature extractors trained\non large image datasets. Using these pre-built feature extractors, we are able\nto process hundreds of publicly available movie trailers, extract\nframe-by-frame low level features (e.g., a face, an object, etc) and create\nvideo-level representations. We use the video-level representations to train a\nhybrid Collaborative Filtering model that combines video features with\nhistorical movie attendance records. The trained model not only makes accurate\nattendance and audience prediction for existing movies, but also successfully\nprofiles new movies six to eight months prior to their release. \n\n"}
{"id": "1807.04834", "contents": "Title: Optimal Strategies for Matching and Retrieval Problems by Comparing\n  Covariates Abstract: In many retrieval problems, where we must retrieve one or more entries from a\ngallery in response to a probe, it is common practice to learn to do by\ndirectly comparing the probe and gallery entries to one another. In many\nsituations the gallery and probe have common covariates -- external variables\nthat are common to both. In principle it is possible to perform the retrieval\nbased merely on these covariates. The process, however, becomes gated by our\nability to recognize the covariates for the probe and gallery entries\ncorrectly.\n  In this paper we analyze optimal strategies for retrieval based only on\nmatching covariates, when the recognition of the covariates is itself\ninaccurate. We investigate multiple problems: recovering one item from a\ngallery of $N$ entries, matching pairs of instances, and retrieval from large\ncollections. We verify our analytical formulae through experiments to verify\ntheir correctness in practical settings. \n\n"}
{"id": "1807.05119", "contents": "Title: Learning-based Natural Geometric Matching with Homography Prior Abstract: Geometric matching is a key step in computer vision tasks. Previous\nlearning-based methods for geometric matching concentrate more on improving\nalignment quality, while we argue the importance of naturalness issue\nsimultaneously. To deal with this, firstly, Pearson correlation is applied to\nhandle large intra-class variations of features in feature matching stage.\nThen, we parametrize homography transformation with 9 parameters in full\nconnected layer of our network, to better characterize large viewpoint\nvariations compared with affine transformation. Furthermore, a novel loss\nfunction with Gaussian weights guarantees the model accuracy and efficiency in\ntraining procedure. Finally, we provide two choices for different purposes in\ngeometric matching. When compositing homography with affine transformation, the\nalignment accuracy improves and all lines are preserved, which results in a\nmore natural transformed image. When compositing homography with non-rigid\nthin-plate-spline transformation, the alignment accuracy further improves.\nExperimental results on Proposal Flow dataset show that our method outperforms\nstate-of-the-art methods, both in terms of alignment accuracy and naturalness. \n\n"}
{"id": "1807.05511", "contents": "Title: Object Detection with Deep Learning: A Review Abstract: Due to object detection's close relationship with video analysis and image\nunderstanding, it has attracted much research attention in recent years.\nTraditional object detection methods are built on handcrafted features and\nshallow trainable architectures. Their performance easily stagnates by\nconstructing complex ensembles which combine multiple low-level image features\nwith high-level context from object detectors and scene classifiers. With the\nrapid development in deep learning, more powerful tools, which are able to\nlearn semantic, high-level, deeper features, are introduced to address the\nproblems existing in traditional architectures. These models behave differently\nin network architecture, training strategy and optimization function, etc. In\nthis paper, we provide a review on deep learning based object detection\nframeworks. Our review begins with a brief introduction on the history of deep\nlearning and its representative tool, namely Convolutional Neural Network\n(CNN). Then we focus on typical generic object detection architectures along\nwith some modifications and useful tricks to improve detection performance\nfurther. As distinct specific detection tasks exhibit different\ncharacteristics, we also briefly survey several specific tasks, including\nsalient object detection, face detection and pedestrian detection. Experimental\nanalyses are also provided to compare various methods and draw some meaningful\nconclusions. Finally, several promising directions and tasks are provided to\nserve as guidelines for future work in both object detection and relevant\nneural network based learning systems. \n\n"}
{"id": "1807.05636", "contents": "Title: Cross Pixel Optical Flow Similarity for Self-Supervised Learning Abstract: We propose a novel method for learning convolutional neural image\nrepresentations without manual supervision. We use motion cues in the form of\noptical flow, to supervise representations of static images. The obvious\napproach of training a network to predict flow from a single image can be\nneedlessly difficult due to intrinsic ambiguities in this prediction task. We\ninstead propose a much simpler learning goal: embed pixels such that the\nsimilarity between their embeddings matches that between their optical flow\nvectors. At test time, the learned deep network can be used without access to\nvideo or flow information and transferred to tasks such as image\nclassification, detection, and segmentation. Our method, which significantly\nsimplifies previous attempts at using motion for self-supervision, achieves\nstate-of-the-art results in self-supervision using motion cues, competitive\nresults for self-supervision in general, and is overall state of the art in\nself-supervised pretraining for semantic image segmentation, as demonstrated on\nstandard benchmarks. \n\n"}
{"id": "1807.05933", "contents": "Title: Visual Graphs from Motion (VGfM): Scene understanding with object\n  geometry reasoning Abstract: Recent approaches on visual scene understanding attempt to build a scene\ngraph -- a computational representation of objects and their pairwise\nrelationships. Such rich semantic representation is very appealing, yet\ndifficult to obtain from a single image, especially when considering complex\nspatial arrangements in the scene. Differently, an image sequence conveys\nuseful information using the multi-view geometric relations arising from camera\nmotion. Indeed, in such cases, object relationships are naturally related to\nthe 3D scene structure. To this end, this paper proposes a system that first\ncomputes the geometrical location of objects in a generic scene and then\nefficiently constructs scene graphs from video by embedding such geometrical\nreasoning. Such compelling representation is obtained using a new model where\ngeometric and visual features are merged using an RNN framework. We report\nresults on a dataset we created for the task of 3D scene graph generation in\nmultiple views. \n\n"}
{"id": "1807.06009", "contents": "Title: ActiveStereoNet: End-to-End Self-Supervised Learning for Active Stereo\n  Systems Abstract: In this paper we present ActiveStereoNet, the first deep learning solution\nfor active stereo systems. Due to the lack of ground truth, our method is fully\nself-supervised, yet it produces precise depth with a subpixel precision of\n$1/30th$ of a pixel; it does not suffer from the common over-smoothing issues;\nit preserves the edges; and it explicitly handles occlusions. We introduce a\nnovel reconstruction loss that is more robust to noise and texture-less\npatches, and is invariant to illumination changes. The proposed loss is\noptimized using a window-based cost aggregation with an adaptive support weight\nscheme. This cost aggregation is edge-preserving and smooths the loss function,\nwhich is key to allow the network to reach compelling results. Finally we show\nhow the task of predicting invalid regions, such as occlusions, can be trained\nend-to-end without ground-truth. This component is crucial to reduce blur and\nparticularly improves predictions along depth discontinuities. Extensive\nquantitatively and qualitatively evaluations on real and synthetic data\ndemonstrate state of the art results in many challenging scenes. \n\n"}
{"id": "1807.06010", "contents": "Title: EC-Net: an Edge-aware Point set Consolidation Network Abstract: Point clouds obtained from 3D scans are typically sparse, irregular, and\nnoisy, and required to be consolidated. In this paper, we present the first\ndeep learning based edge-aware technique to facilitate the consolidation of\npoint clouds. We design our network to process points grouped in local patches,\nand train it to learn and help consolidate points, deliberately for edges. To\nachieve this, we formulate a regression component to simultaneously recover 3D\npoint coordinates and point-to-edge distances from upsampled features, and an\nedge-aware joint loss function to directly minimize distances from output\npoints to 3D meshes and to edges. Compared with previous neural network based\nworks, our consolidation is edge-aware. During the synthesis, our network can\nattend to the detected sharp edges and enable more accurate 3D reconstructions.\nAlso, we trained our network on virtual scanned point clouds, demonstrated the\nperformance of our method on both synthetic and real point clouds, presented\nvarious surface reconstruction results, and showed how our method outperforms\nthe state-of-the-arts. \n\n"}
{"id": "1807.06056", "contents": "Title: Unlimited Road-scene Synthetic Annotation (URSA) Dataset Abstract: In training deep neural networks for semantic segmentation, the main limiting\nfactor is the low amount of ground truth annotation data that is available in\ncurrently existing datasets. The limited availability of such data is due to\nthe time cost and human effort required to accurately and consistently label\nreal images on a pixel level. Modern sandbox video game engines provide open\nworld environments where traffic and pedestrians behave in a pseudo-realistic\nmanner. This caters well to the collection of a believable road-scene dataset.\nUtilizing open-source tools and resources found in single-player modding\ncommunities, we provide a method for persistent, ground truth, asset annotation\nof a game world. By collecting a synthetic dataset containing upwards of\n$1,000,000$ images, we demonstrate real-time, on-demand, ground truth data\nannotation capability of our method. Supplementing this synthetic data to\nCityscapes dataset, we show that our data generation method provides\nqualitative as well as quantitative improvements---for training networks---over\nprevious methods that use video games as surrogate. \n\n"}
{"id": "1807.06537", "contents": "Title: PIMMS: Permutation Invariant Multi-Modal Segmentation Abstract: In a research context, image acquisition will often involve a pre-defined\nstatic protocol and the data will be of high quality. If we are to build\napplications that work in hospitals without significant operational changes in\ncare delivery, algorithms should be designed to cope with the available data in\nthe best possible way. In a clinical environment, imaging protocols are highly\nflexible, with MRI sequences commonly missing appropriate sequence labeling\n(e.g. T1, T2, FLAIR). To this end we introduce PIMMS, a Permutation Invariant\nMulti-Modal Segmentation technique that is able to perform inference over sets\nof MRI scans without using modality labels. We present results which show that\nour convolutional neural network can, in some settings, outperform a baseline\nmodel which utilizes modality labels, and achieve comparable performance\notherwise. \n\n"}
{"id": "1807.07049", "contents": "Title: Robot Learning in Homes: Improving Generalization and Reducing Dataset\n  Bias Abstract: Data-driven approaches to solving robotic tasks have gained a lot of traction\nin recent years. However, most existing policies are trained on large-scale\ndatasets collected in curated lab settings. If we aim to deploy these models in\nunstructured visual environments like people's homes, they will be unable to\ncope with the mismatch in data distribution. In such light, we present the\nfirst systematic effort in collecting a large dataset for robotic grasping in\nhomes. First, to scale and parallelize data collection, we built a low cost\nmobile manipulator assembled for under 3K USD. Second, data collected using low\ncost robots suffer from noisy labels due to imperfect execution and calibration\nerrors. To handle this, we develop a framework which factors out the noise as a\nlatent variable. Our model is trained on 28K grasps collected in several houses\nunder an array of different environmental conditions. We evaluate our models by\nphysically executing grasps on a collection of novel objects in multiple unseen\nhomes. The models trained with our home dataset showed a marked improvement of\n43.7% over a baseline model trained with data collected in lab. Our\narchitecture which explicitly models the latent noise in the dataset also\nperformed 10% better than one that did not factor out the noise. We hope this\neffort inspires the robotics community to look outside the lab and embrace\nlearning based approaches to handle inaccurate cheap robots. \n\n"}
{"id": "1807.07155", "contents": "Title: Take a Look Around: Using Street View and Satellite Images to Estimate\n  House Prices Abstract: When an individual purchases a home, they simultaneously purchase its\nstructural features, its accessibility to work, and the neighborhood amenities.\nSome amenities, such as air quality, are measurable while others, such as the\nprestige or the visual impression of a neighborhood, are difficult to quantify.\nDespite the well-known impacts intangible housing features have on house\nprices, limited attention has been given to systematically quantifying these\ndifficult to measure amenities. Two issues have led to this neglect. Not only\ndo few quantitative methods exist that can measure the urban environment, but\nthat the collection of such data is both costly and subjective.\n  We show that street image and satellite image data can capture these urban\nqualities and improve the estimation of house prices. We propose a pipeline\nthat uses a deep neural network model to automatically extract visual features\nfrom images to estimate house prices in London, UK. We make use of traditional\nhousing features such as age, size, and accessibility as well as visual\nfeatures from Google Street View images and Bing aerial images in estimating\nthe house price model. We find encouraging results where learning to\ncharacterize the urban quality of a neighborhood improves house price\nprediction, even when generalizing to previously unseen London boroughs.\n  We explore the use of non-linear vs. linear methods to fuse these cues with\nconventional models of house pricing, and show how the interpretability of\nlinear models allows us to directly extract proxy variables for visual\ndesirability of neighborhoods that are both of interest in their own right, and\ncould be used as inputs to other econometric methods. This is particularly\nvaluable as once the network has been trained with the training data, it can be\napplied elsewhere, allowing us to generate vivid dense maps of the visual\nappeal of London streets. \n\n"}
{"id": "1807.07416", "contents": "Title: Image Reconstruction via Variational Network for Real-Time Hand-Held\n  Sound-Speed Imaging Abstract: Speed-of-sound is a biomechanical property for quantitative tissue\ndifferentiation, with great potential as a new ultrasound-based image modality.\nA conventional ultrasound array transducer can be used together with an\nacoustic mirror, or so-called reflector, to reconstruct sound-speed images from\ntime-of-flight measurements to the reflector collected between transducer\nelement pairs, which constitutes a challenging problem of limited-angle\ncomputed tomography. For this problem, we herein present a variational network\nbased image reconstruction architecture that is based on optimization loop\nunrolling, and provide an efficient training protocol of this network\narchitecture on fully synthetic inclusion data. Our results indicate that the\nlearned model presents good generalization ability, being able to reconstruct\nimages with significantly different statistics compared to the training set.\nComplex inclusion geometries were shown to be successfully reconstructed, also\nimproving over the prior-art by 23% in reconstruction error and by 10% in\ncontrast on synthetic data. In a phantom study, we demonstrated the detection\nof multiple inclusions that were not distinguishable by prior-art\nreconstruction, meanwhile improving the contrast by 27% for a stiff inclusion\nand by 219% for a soft inclusion. Our reconstruction algorithm takes\napproximately 10ms, enabling its use as a real-time imaging method on an\nultrasound machine, for which we are demonstrating an example preliminary setup\nherein. \n\n"}
{"id": "1807.07466", "contents": "Title: Guided Upsampling Network for Real-Time Semantic Segmentation Abstract: Semantic segmentation architectures are mainly built upon an encoder-decoder\nstructure. These models perform subsequent downsampling operations in the\nencoder. Since operations on high-resolution activation maps are\ncomputationally expensive, usually the decoder produces output segmentation\nmaps by upsampling with parameters-free operators like bilinear or\nnearest-neighbor. We propose a Neural Network named Guided Upsampling Network\nwhich consists of a multiresolution architecture that jointly exploits\nhigh-resolution and large context information. Then we introduce a new module\nnamed Guided Upsampling Module (GUM) that enriches upsampling operators by\nintroducing a learnable transformation for semantic maps. It can be plugged\ninto any existing encoder-decoder architecture with little modifications and\nlow additional computation cost. We show with quantitative and qualitative\nexperiments how our network benefits from the use of GUM module. A\ncomprehensive set of experiments on the publicly available Cityscapes dataset\ndemonstrates that Guided Upsampling Network can efficiently process\nhigh-resolution images in real-time while attaining state-of-the art\nperformances. \n\n"}
{"id": "1807.08179", "contents": "Title: Inductive Visual Localisation: Factorised Training for Superior\n  Generalisation Abstract: End-to-end trained Recurrent Neural Networks (RNNs) have been successfully\napplied to numerous problems that require processing sequences, such as image\ncaptioning, machine translation, and text recognition. However, RNNs often\nstruggle to generalise to sequences longer than the ones encountered during\ntraining. In this work, we propose to optimise neural networks explicitly for\ninduction. The idea is to first decompose the problem in a sequence of\ninductive steps and then to explicitly train the RNN to reproduce such steps.\nGeneralisation is achieved as the RNN is not allowed to learn an arbitrary\ninternal state; instead, it is tasked with mimicking the evolution of a valid\nstate. In particular, the state is restricted to a spatial memory map that\ntracks parts of the input image which have been accounted for in previous\nsteps. The RNN is trained for single inductive steps, where it produces updates\nto the memory in addition to the desired output. We evaluate our method on two\ndifferent visual recognition problems involving visual sequences: (1) text\nspotting, i.e. joint localisation and reading of text in images containing\nmultiple lines (or a block) of text, and (2) sequential counting of objects in\naerial images. We show that inductive training of recurrent models enhances\ntheir generalisation ability on challenging image datasets. \n\n"}
{"id": "1807.08259", "contents": "Title: Deep Discriminative Model for Video Classification Abstract: This paper presents a new deep learning approach for video-based scene\nclassification. We design a Heterogeneous Deep Discriminative Model (HDDM)\nwhose parameters are initialized by performing an unsupervised pre-training in\na layer-wise fashion using Gaussian Restricted Boltzmann Machines (GRBM). In\norder to avoid the redundancy of adjacent frames, we extract spatiotemporal\nvariation patterns within frames and represent them sparsely using Sparse Cubic\nSymmetrical Pattern (SCSP). Then, a pre-initialized HDDM is separately trained\nusing the videos of each class to learn class-specific models. According to the\nminimum reconstruction error from the learnt class-specific models, a weighted\nvoting strategy is employed for the classification. The performance of the\nproposed method is extensively evaluated on two action recognition datasets;\nUCF101 and Hollywood II, and three dynamic texture and dynamic scene datasets;\nDynTex, YUPENN, and Maryland. The experimental results and comparisons against\nstate-of-the-art methods demonstrate that the proposed method consistently\nachieves superior performance on all datasets. \n\n"}
{"id": "1807.08291", "contents": "Title: Correlation Net: Spatiotemporal multimodal deep learning for action\n  recognition Abstract: This paper describes a network that captures multimodal correlations over\narbitrary timestamps. The proposed scheme operates as a complementary, extended\nnetwork over a multimodal convolutional neural network (CNN). Spatial and\ntemporal streams are required for action recognition by a deep CNN, but\noverfitting reduction and fusing these two streams remain open problems. The\nexisting fusion approach averages the two streams. Here we propose a\ncorrelation network with a Shannon fusion for learning a pre-trained CNN. A\nLong-range video may consist of spatiotemporal correlations over arbitrary\ntimes, which can be captured by forming the correlation network from simple\nfully connected layers. This approach was found to complement the existing\nnetwork fusion methods. The importance of multimodal correlation is validated\nin comparison experiments on the UCF-101 and HMDB-51 datasets. The multimodal\ncorrelation enhanced the accuracy of the video recognition results. \n\n"}
{"id": "1807.08379", "contents": "Title: Towards Privacy-Preserving Visual Recognition via Adversarial Training:\n  A Pilot Study Abstract: This paper aims to improve privacy-preserving visual recognition, an\nincreasingly demanded feature in smart camera applications, by formulating a\nunique adversarial training framework. The proposed framework explicitly learns\na degradation transform for the original video inputs, in order to optimize the\ntrade-off between target task performance and the associated privacy budgets on\nthe degraded video. A notable challenge is that the privacy budget, often\ndefined and measured in task-driven contexts, cannot be reliably indicated\nusing any single model performance, because a strong protection of privacy has\nto sustain against any possible model that tries to hack privacy information.\nSuch an uncommon situation has motivated us to propose two strategies, i.e.,\nbudget model restarting and ensemble, to enhance the generalization of the\nlearned degradation on protecting privacy against unseen hacker models. Novel\ntraining strategies, evaluation protocols, and result visualization methods\nhave been designed accordingly. Two experiments on privacy-preserving action\nrecognition, with privacy budgets defined in various ways, manifest the\ncompelling effectiveness of the proposed framework in simultaneously\nmaintaining high target task (action recognition) performance while suppressing\nthe privacy breach risk. \n\n"}
{"id": "1807.09298", "contents": "Title: Ensemble of Multi-sized FCNs to Improve White Matter Lesion Segmentation Abstract: In this paper, we develop a two-stage neural network solution for the\nchallenging task of white-matter lesion segmentation. To cope with the vast\nvari- ability in lesion sizes, we sample brain MR scans with patches at three\ndiffer- ent dimensions and feed them into separate fully convolutional neural\nnetworks (FCNs). In the second stage, we process large and small lesion\nseparately, and use ensemble-nets to combine the segmentation results generated\nfrom the FCNs. A novel activation function is adopted in the ensemble-nets to\nimprove the segmen- tation accuracy measured by Dice Similarity Coefficient.\nExperiments on MICCAI 2017 White Matter Hyperintensities (WMH) Segmentation\nChallenge data demonstrate that our two-stage-multi-sized FCN approach, as well\nas the new activation function, are effective in capturing white-matter lesions\nin MR images. \n\n"}
{"id": "1807.09441", "contents": "Title: Two at Once: Enhancing Learning and Generalization Capacities via\n  IBN-Net Abstract: Convolutional neural networks (CNNs) have achieved great successes in many\ncomputer vision problems. Unlike existing works that designed CNN architectures\nto improve performance on a single task of a single domain and not\ngeneralizable, we present IBN-Net, a novel convolutional architecture, which\nremarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as\nwell as its generalization capacity on another domain (e.g. GTA5) without\nfinetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch\nNormalization (BN) as building blocks, and can be wrapped into many advanced\ndeep networks to improve their performances. This work has three key\ncontributions. (1) By delving into IN and BN, we disclose that IN learns\nfeatures that are invariant to appearance changes, such as colors, styles, and\nvirtuality/reality, while BN is essential for preserving content related\ninformation. (2) IBN-Net can be applied to many advanced deep architectures,\nsuch as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their\nperformance without increasing computational cost. (3) When applying the\ntrained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves\ncomparable improvements as domain adaptation methods, even without using data\nfrom the target domain. With IBN-Net, we won the 1st place on the WAD 2018\nChallenge Drivable Area track, with an mIoU of 86.18%. \n\n"}
{"id": "1807.09536", "contents": "Title: End-to-End Incremental Learning Abstract: Although deep learning approaches have stood out in recent years due to their\nstate-of-the-art results, they continue to suffer from catastrophic forgetting,\na dramatic decrease in overall performance when training with new classes added\nincrementally. This is due to current neural network architectures requiring\nthe entire dataset, consisting of all the samples from the old as well as the\nnew classes, to update the model -a requirement that becomes easily\nunsustainable as the number of classes grows. We address this issue with our\napproach to learn deep neural networks incrementally, using new data and only a\nsmall exemplar set corresponding to samples from the old classes. This is based\non a loss composed of a distillation measure to retain the knowledge acquired\nfrom the old classes, and a cross-entropy loss to learn the new classes. Our\nincremental training is achieved while keeping the entire framework end-to-end,\ni.e., learning the data representation and the classifier jointly, unlike\nrecent methods with no such guarantees. We evaluate our method extensively on\nthe CIFAR-100 and ImageNet (ILSVRC 2012) image classification datasets, and\nshow state-of-the-art performance. \n\n"}
{"id": "1807.09562", "contents": "Title: Change Detection between Multimodal Remote Sensing Data Using Siamese\n  CNN Abstract: Detecting topographic changes in the urban environment has always been an\nimportant task for urban planning and monitoring. In practice, remote sensing\ndata are often available in different modalities and at different time epochs.\nChange detection between multimodal data can be very challenging since the data\nshow different characteristics. Given 3D laser scanning point clouds and 2D\nimagery from different epochs, this paper presents a framework to detect\nbuilding and tree changes. First, the 2D and 3D data are transformed to image\npatches, respectively. A Siamese CNN is then employed to detect candidate\nchanges between the two epochs. Finally, the candidate patch-based changes are\ngrouped and verified as individual object changes. Experiments on the urban\ndata show that 86.4\\% of patch pairs can be correctly classified by the model. \n\n"}
{"id": "1807.09810", "contents": "Title: Coreset-Based Neural Network Compression Abstract: We propose a novel Convolutional Neural Network (CNN) compression algorithm\nbased on coreset representations of filters. We exploit the redundancies extant\nin the space of CNN weights and neuronal activations (across samples) in order\nto obtain compression. Our method requires no retraining, is easy to implement,\nand obtains state-of-the-art compression performance across a wide variety of\nCNN architectures. Coupled with quantization and Huffman coding, we create\nnetworks that provide AlexNet-like accuracy, with a memory footprint that is\n$832\\times$ smaller than the original AlexNet, while also introducing\nsignificant reductions in inference time as well. Additionally these compressed\nnetworks when fine-tuned, successfully generalize to other domains as well. \n\n"}
{"id": "1807.09834", "contents": "Title: Applying Domain Randomization to Synthetic Data for Object Category\n  Detection Abstract: Recent advances in deep learning-based object detection techniques have\nrevolutionized their applicability in several fields. However, since these\nmethods rely on unwieldy and large amounts of data, a common practice is to\ndownload models pre-trained on standard datasets and fine-tune them for\nspecific application domains with a small set of domain relevant images. In\nthis work, we show that using synthetic datasets that are not necessarily\nphoto-realistic can be a better alternative to simply fine-tune pre-trained\nnetworks. Specifically, our results show an impressive 25% improvement in the\nmAP metric over a fine-tuning baseline when only about 200 labelled images are\navailable to train. Finally, an ablation study of our results is presented to\ndelineate the individual contribution of different components in the\nrandomization pipeline. \n\n"}
{"id": "1807.09993", "contents": "Title: Divide and Grow: Capturing Huge Diversity in Crowd Images with\n  Incrementally Growing CNN Abstract: Automated counting of people in crowd images is a challenging task. The major\ndifficulty stems from the large diversity in the way people appear in crowds.\nIn fact, features available for crowd discrimination largely depend on the\ncrowd density to the extent that people are only seen as blobs in a highly\ndense scene. We tackle this problem with a growing CNN which can progressively\nincrease its capacity to account for the wide variability seen in crowd scenes.\nOur model starts from a base CNN density regressor, which is trained in\nequivalence on all types of crowd images. In order to adapt with the huge\ndiversity, we create two child regressors which are exact copies of the base\nCNN. A differential training procedure divides the dataset into two clusters\nand fine-tunes the child networks on their respective specialties.\nConsequently, without any hand-crafted criteria for forming specialties, the\nchild regressors become experts on certain types of crowds. The child networks\nare again split recursively, creating two experts at every division. This\nhierarchical training leads to a CNN tree, where the child regressors are more\nfine experts than any of their parents. The leaf nodes are taken as the final\nexperts and a classifier network is then trained to predict the correct\nspecialty for a given test image patch. The proposed model achieves higher\ncount accuracy on major crowd datasets. Further, we analyse the characteristics\nof specialties mined automatically by our method. \n\n"}
{"id": "1807.10073", "contents": "Title: Loosely-Coupled Semi-Direct Monocular SLAM Abstract: We propose a novel semi-direct approach for monocular simultaneous\nlocalization and mapping (SLAM) that combines the complementary strengths of\ndirect and feature-based methods. The proposed pipeline loosely couples direct\nodometry and feature-based SLAM to perform three levels of parallel\noptimizations: (1) photometric bundle adjustment (BA) that jointly optimizes\nthe local structure and motion, (2) geometric BA that refines keyframe poses\nand associated feature map points, and (3) pose graph optimization to achieve\nglobal map consistency in the presence of loop closures. This is achieved in\nreal-time by limiting the feature-based operations to marginalized keyframes\nfrom the direct odometry module. Exhaustive evaluation on two benchmark\ndatasets demonstrates that our system outperforms the state-of-the-art\nmonocular odometry and SLAM systems in terms of overall accuracy and\nrobustness. \n\n"}
{"id": "1807.10267", "contents": "Title: Generating 3D faces using Convolutional Mesh Autoencoders Abstract: Learned 3D representations of human faces are useful for computer vision\nproblems such as 3D face tracking and reconstruction from images, as well as\ngraphics applications such as character generation and animation. Traditional\nmodels learn a latent representation of a face using linear subspaces or\nhigher-order tensor generalizations. Due to this linearity, they can not\ncapture extreme deformations and non-linear expressions. To address this, we\nintroduce a versatile model that learns a non-linear representation of a face\nusing spectral convolutions on a mesh surface. We introduce mesh sampling\noperations that enable a hierarchical mesh representation that captures\nnon-linear variations in shape and expression at multiple scales within the\nmodel. In a variational setting, our model samples diverse realistic 3D faces\nfrom a multivariate Gaussian distribution. Our training data consists of 20,466\nmeshes of extreme expressions captured over 12 different subjects. Despite\nlimited training data, our trained model outperforms state-of-the-art face\nmodels with 50% lower reconstruction error, while using 75% fewer parameters.\nWe also show that, replacing the expression space of an existing\nstate-of-the-art face model with our autoencoder, achieves a lower\nreconstruction error. Our data, model and code are available at\nhttp://github.com/anuragranj/coma \n\n"}
{"id": "1807.10437", "contents": "Title: Connecting Gaze, Scene, and Attention: Generalized Attention Estimation\n  via Joint Modeling of Gaze and Scene Saliency Abstract: This paper addresses the challenging problem of estimating the general visual\nattention of people in images. Our proposed method is designed to work across\nmultiple naturalistic social scenarios and provides a full picture of the\nsubject's attention and gaze. In contrast, earlier works on gaze and attention\nestimation have focused on constrained problems in more specific contexts. In\nparticular, our model explicitly represents the gaze direction and handles\nout-of-frame gaze targets. We leverage three different datasets using a\nmulti-task learning approach. We evaluate our method on widely used benchmarks\nfor single-tasks such as gaze angle estimation and attention-within-an-image,\nas well as on the new challenging task of generalized visual attention\nprediction. In addition, we have created extended annotations for the MMDB and\nGazeFollow datasets which are used in our experiments, which we will publicly\nrelease. \n\n"}
{"id": "1807.10569", "contents": "Title: The Helmholtz Method: Using Perceptual Compression to Reduce Machine\n  Learning Complexity Abstract: This paper proposes a fundamental answer to a frequently asked question in\nmultimedia computing and machine learning: Do artifacts from perceptual\ncompression contribute to error in the machine learning process and if so, how\nmuch? Our approach to the problem is a reinterpretation of the Helmholtz Free\nEnergy formula from physics to explain the relationship between content and\nnoise when using sensors (such as cameras or microphones) to capture multimedia\ndata. The reinterpretation allows a bit-measurement of the noise contained in\nimages, audio, and video by combining a classifier with perceptual compression,\nsuch as JPEG or MP3. Our experiments on CIFAR-10 as well as Fraunhofer's\nIDMT-SMT-Audio-Effects dataset indicate that, at the right quality level,\nperceptual compression is actually not harmful but contributes to a significant\nreduction of complexity of the machine learning process. That is, our noise\nquantification method can be used to speed up the training of deep learning\nclassifiers significantly while maintaining, or sometimes even improving,\noverall classification accuracy. Moreover, our results provide insights into\nthe reasons for the success of deep learning. \n\n"}
{"id": "1807.10590", "contents": "Title: Harmonic Adversarial Attack Method Abstract: Adversarial attacks find perturbations that can fool models into\nmisclassifying images. Previous works had successes in generating\nnoisy/edge-rich adversarial perturbations, at the cost of degradation of image\nquality. Such perturbations, even when they are small in scale, are usually\neasily spottable by human vision. In contrast, we propose Harmonic Adversar-\nial Attack Methods (HAAM), that generates edge-free perturbations by using\nharmonic functions. The property of edge-free guarantees that the generated\nadversarial images can still preserve visual quality, even when perturbations\nare of large magnitudes. Experiments also show that adversaries generated by\nHAAM often have higher rates of success when transferring between models. In\naddition, we find harmonic perturbations can simulate natural phenomena like\nnatural lighting and shadows. It would then be possible to help find corner\ncases for given models, as a first step to improving them. \n\n"}
{"id": "1807.10936", "contents": "Title: Unsupervised Learning of a Hierarchical Spiking Neural Network for\n  Optical Flow Estimation: From Events to Global Motion Perception Abstract: The combination of spiking neural networks and event-based vision sensors\nholds the potential of highly efficient and high-bandwidth optical flow\nestimation. This paper presents the first hierarchical spiking architecture in\nwhich motion (direction and speed) selectivity emerges in an unsupervised\nfashion from the raw stimuli generated with an event-based camera. A novel\nadaptive neuron model and stable spike-timing-dependent plasticity formulation\nare at the core of this neural network governing its spike-based processing and\nlearning, respectively. After convergence, the neural architecture exhibits the\nmain properties of biological visual motion systems, namely feature extraction\nand local and global motion perception. Convolutional layers with input\nsynapses characterized by single and multiple transmission delays are employed\nfor feature and local motion perception, respectively; while global motion\nselectivity emerges in a final fully-connected layer. The proposed solution is\nvalidated using synthetic and real event sequences. Along with this paper, we\nprovide the cuSNN library, a framework that enables GPU-accelerated simulations\nof large-scale spiking neural networks. Source code and samples are available\nat https://github.com/tudelft/cuSNN. \n\n"}
{"id": "1807.10993", "contents": "Title: U-Finger: Multi-Scale Dilated Convolutional Network for Fingerprint\n  Image Denoising and Inpainting Abstract: This paper studies the challenging problem of fingerprint image denoising and\ninpainting. To tackle the challenge of suppressing complicated artifacts (blur,\nbrightness, contrast, elastic transformation, occlusion, scratch, resolution,\nrotation, and so on) while preserving fine textures, we develop a multi-scale\nconvolutional network, termed U- Finger. Based on the domain expertise, we show\nthat the usage of dilated convolutions as well as the removal of padding have\nimportant positive impacts on the final restoration performance, in addition to\nmulti-scale cascaded feature modules. Our model achieves the overall ranking of\nNo.2 in the ECCV 2018 Chalearn LAP Inpainting Competition Track 3 (Fingerprint\nDenoising and Inpainting). Among all participating teams, we obtain the MSE of\n0.0231 (rank 2), PSNR 16.9688 dB (rank 2), and SSIM 0.8093 (rank 3) on the\nhold-out testing set. \n\n"}
{"id": "1807.11078", "contents": "Title: Semi-supervised Transfer Learning for Image Rain Removal Abstract: Single image rain removal is a typical inverse problem in computer vision.\nThe deep learning technique has been verified to be effective for this task and\nachieved state-of-the-art performance. However, previous deep learning methods\nneed to pre-collect a large set of image pairs with/without synthesized rain\nfor training, which tends to make the neural network be biased toward learning\nthe specific patterns of the synthesized rain, while be less able to generalize\nto real test samples whose rain types differ from those in the training data.\nTo this issue, this paper firstly proposes a semi-supervised learning paradigm\ntoward this task. Different from traditional deep learning methods which only\nuse supervised image pairs with/without synthesized rain, we further put real\nrainy images, without need of their clean ones, into the network training\nprocess. This is realized by elaborately formulating the residual between an\ninput rainy image and its expected network output (clear image without rain) as\na specific parametrized rain streaks distribution. The network is therefore\ntrained to adapt real unsupervised diverse rain types through transferring from\nthe supervised synthesized rain, and thus both the short-of-training-sample and\nbias-to-supervised-sample issues can be evidently alleviated. Experiments on\nsynthetic and real data verify the superiority of our model compared to the\nstate-of-the-arts. \n\n"}
{"id": "1807.11089", "contents": "Title: Towards Automatic Speech Identification from Vocal Tract Shape Dynamics\n  in Real-time MRI Abstract: Vocal tract configurations play a vital role in generating distinguishable\nspeech sounds, by modulating the airflow and creating different resonant\ncavities in speech production. They contain abundant information that can be\nutilized to better understand the underlying speech production mechanism. As a\nstep towards automatic mapping of vocal tract shape geometry to acoustics, this\npaper employs effective video action recognition techniques, like Long-term\nRecurrent Convolutional Networks (LRCN) models, to identify different\nvowel-consonant-vowel (VCV) sequences from dynamic shaping of the vocal tract.\nSuch a model typically combines a CNN based deep hierarchical visual feature\nextractor with Recurrent Networks, that ideally makes the network\nspatio-temporally deep enough to learn the sequential dynamics of a short video\nclip for video classification tasks. We use a database consisting of 2D\nreal-time MRI of vocal tract shaping during VCV utterances by 17 speakers. The\ncomparative performances of this class of algorithms under various parameter\nsettings and for various classification tasks are discussed. Interestingly, the\nresults show a marked difference in the model performance in the context of\nspeech classification with respect to generic sequence or video classification\ntasks. \n\n"}
{"id": "1807.11245", "contents": "Title: Recurrently Exploring Class-wise Attention in A Hybrid Convolutional and\n  Bidirectional LSTM Network for Multi-label Aerial Image Classification Abstract: Aerial image classification is of great significance in remote sensing\ncommunity, and many researches have been conducted over the past few years.\nAmong these studies, most of them focus on categorizing an image into one\nsemantic label, while in the real world, an aerial image is often associated\nwith multiple labels, e.g., multiple object-level labels in our case. Besides,\na comprehensive picture of present objects in a given high resolution aerial\nimage can provide more in-depth understanding of the studied region. For these\nreasons, aerial image multi-label classification has been attracting increasing\nattention. However, one common limitation shared by existing methods in the\ncommunity is that the co-occurrence relationship of various classes, so called\nclass dependency, is underexplored and leads to an inconsiderate decision. In\nthis paper, we propose a novel end-to-end network, namely class-wise\nattention-based convolutional and bidirectional LSTM network (CA-Conv-BiLSTM),\nfor this task. The proposed network consists of three indispensable components:\n1) a feature extraction module, 2) a class attention learning layer, and 3) a\nbidirectional LSTM-based sub-network. Particularly, the feature extraction\nmodule is designed for extracting fine-grained semantic feature maps, while the\nclass attention learning layer aims at capturing discriminative class-specific\nfeatures. As the most important part, the bidirectional LSTM-based sub-network\nmodels the underlying class dependency in both directions and produce\nstructured multiple object labels. Experimental results on UCM multi-label\ndataset and DFC15 multi-label dataset validate the effectiveness of our model\nquantitatively and qualitatively. \n\n"}
{"id": "1807.11541", "contents": "Title: Markerless Visual Robot Programming by Demonstration Abstract: In this paper we present an approach for learning to imitate human behavior\non a semantic level by markerless visual observation. We analyze a set of\nspatial constraints on human pose data extracted using convolutional pose\nmachines and object informations extracted from 2D image sequences. A scene\nanalysis, based on an ontology of objects and affordances, is combined with\ncontinuous human pose estimation and spatial object relations. Using a set of\nconstraints we associate the observed human actions with a set of executable\nrobot commands. We demonstrate our approach in a kitchen task, where the robot\nlearns to prepare a meal. \n\n"}
{"id": "1807.11649", "contents": "Title: The Devil of Face Recognition is in the Noise Abstract: The growing scale of face recognition datasets empowers us to train strong\nconvolutional networks for face recognition. While a variety of architectures\nand loss functions have been devised, we still have a limited understanding of\nthe source and consequence of label noise inherent in existing datasets. We\nmake the following contributions: 1) We contribute cleaned subsets of popular\nface databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new\nlarge-scale noise-controlled IMDb-Face dataset. 2) With the original datasets\nand cleaned subsets, we profile and analyze label noise properties of MegaFace\nand MS-Celeb-1M. We show that a few orders more samples are needed to achieve\nthe same accuracy yielded by a clean subset. 3) We study the association\nbetween different types of noise, i.e., label flips and outliers, with the\naccuracy of face recognition models. 4) We investigate ways to improve data\ncleanliness, including a comprehensive user study on the influence of data\nlabeling strategies to annotation accuracy. The IMDb-Face dataset has been\nreleased on https://github.com/fwang91/IMDb-Face. \n\n"}
{"id": "1807.11783", "contents": "Title: Scale equivariance in CNNs with vector fields Abstract: We study the effect of injecting local scale equivariance into Convolutional\nNeural Networks. This is done by applying each convolutional filter at multiple\nscales. The output is a vector field encoding for the maximally activating\nscale and the scale itself, which is further processed by the following\nconvolutional layers. This allows all the intermediate representations to be\nlocally scale equivariant. We show that this improves the performance of the\nmodel by over $20\\%$ in the scale equivariant task of regressing the scaling\nfactor applied to randomly scaled MNIST digits. Furthermore, we find it also\nuseful for scale invariant tasks, such as the actual classification of randomly\nscaled digits. This highlights the usefulness of allowing for a compact\nrepresentation that can also learn relationships between different local scales\nby keeping internal scale equivariance. \n\n"}
{"id": "1807.11886", "contents": "Title: Deep Dual Pyramid Network for Barcode Segmentation using Barcode-30k\n  Database Abstract: Digital signs(such as barcode or QR code) are widely used in our daily life,\nand for many applications, we need to localize them on images. However,\ndifficult cases such as targets with small scales, half-occlusion, shape\ndeformation and large illumination changes cause challenges for conventional\nmethods. In this paper, we address this problem by producing a large-scale\ndataset and adopting a deep learning based semantic segmentation approach.\nSpecifically, a synthesizing method was proposed to generate well-annotated\nimages containing barcode and QR code labels, which contributes to largely\ndecrease the annotation time. Through the synthesis strategy, we introduce a\ndataset that contains 30000 images with Barcode and QR code - Barcode-30k.\nMoreover, we further propose a dual pyramid structure based segmentation\nnetwork - BarcodeNet, which is mainly formed with two novel modules, Prior\nPyramid Pooling Module(P3M) and Pyramid Refine Module(PRM). We validate the\neffectiveness of BarcodeNet on the proposed synthetic dataset, and it yields\nthe result of mIoU accuracy 95.36\\% on validation set. Additional segmentation\nresults of real images have shown that accurate segmentation performance is\nachieved. \n\n"}
{"id": "1808.00046", "contents": "Title: Lip-Reading Driven Deep Learning Approach for Speech Enhancement Abstract: This paper proposes a novel lip-reading driven deep learning framework for\nspeech enhancement. The proposed approach leverages the complementary strengths\nof both deep learning and analytical acoustic modelling (filtering based\napproach) as compared to recently published, comparatively simpler benchmark\napproaches that rely only on deep learning. The proposed audio-visual (AV)\nspeech enhancement framework operates at two levels. In the first level, a\nnovel deep learning-based lip-reading regression model is employed. In the\nsecond level, lip-reading approximated clean-audio features are exploited,\nusing an enhanced, visually-derived Wiener filter (EVWF), for the clean audio\npower spectrum estimation. Specifically, a stacked long-short-term memory\n(LSTM) based lip-reading regression model is designed for clean audio features\nestimation using only temporal visual features considering different number of\nprior visual frames. For clean speech spectrum estimation, a new\nfilterbank-domain EVWF is formulated, which exploits estimated speech features.\nThe proposed EVWF is compared with conventional Spectral Subtraction and\nLog-Minimum Mean-Square Error methods using both ideal AV mapping and LSTM\ndriven AV mapping. The potential of the proposed speech enhancement framework\nis evaluated under different dynamic real-world commercially-motivated\nscenarios (e.g. cafe, public transport, pedestrian area) at different SNR\nlevels (ranging from low to high SNRs) using benchmark Grid and ChiME3 corpora.\nFor objective testing, perceptual evaluation of speech quality is used to\nevaluate the quality of restored speech. For subjective testing, the standard\nmean-opinion-score method is used with inferential statistics. Comparative\nsimulation results demonstrate significant lip-reading and speech enhancement\nimprovement in terms of both speech quality and speech intelligibility. \n\n"}
{"id": "1808.00297", "contents": "Title: TraMNet - Transition Matrix Network for Efficient Action Tube Proposals Abstract: Current state-of-the-art methods solve spatiotemporal action localisation by\nextending 2D anchors to 3D-cuboid proposals on stacks of frames, to generate\nsets of temporally connected bounding boxes called \\textit{action micro-tubes}.\nHowever, they fail to consider that the underlying anchor proposal hypotheses\nshould also move (transition) from frame to frame, as the actor or the camera\ndoes. Assuming we evaluate $n$ 2D anchors in each frame, then the number of\npossible transitions from each 2D anchor to the next, for a sequence of $f$\nconsecutive frames, is in the order of $O(n^f)$, expensive even for small\nvalues of $f$. To avoid this problem, we introduce a Transition-Matrix-based\nNetwork (TraMNet) which relies on computing transition probabilities between\nanchor proposals while maximising their overlap with ground truth bounding\nboxes across frames, and enforcing sparsity via a transition threshold. As the\nresulting transition matrix is sparse and stochastic, this reduces the proposal\nhypothesis search space from $O(n^f)$ to the cardinality of the thresholded\nmatrix. At training time, transitions are specific to cell locations of the\nfeature maps, so that a sparse (efficient) transition matrix is used to train\nthe network. At test time, a denser transition matrix can be obtained either by\ndecreasing the threshold or by adding to it all the relative transitions\noriginating from any cell location, allowing the network to handle transitions\nin the test data that might not have been present in the training data, and\nmaking detection translation-invariant. Finally, we show that our network can\nhandle sparse annotations such as those available in the DALY dataset. We\nreport extensive experiments on the DALY, UCF101-24 and Transformed-UCF101-24\ndatasets to support our claims. \n\n"}
{"id": "1808.00327", "contents": "Title: Generative Adversarial Frontal View to Bird View Synthesis Abstract: Environment perception is an important task with great practical value and\nbird view is an essential part for creating panoramas of surrounding\nenvironment. Due to the large gap and severe deformation between the frontal\nview and bird view, generating a bird view image from a single frontal view is\nchallenging. To tackle this problem, we propose the BridgeGAN, i.e., a novel\ngenerative model for bird view synthesis. First, an intermediate view, i.e.,\nhomography view, is introduced to bridge the large gap. Next, conditioned on\nthe three views (frontal view, homography view and bird view) in our task, a\nmulti-GAN based model is proposed to learn the challenging cross-view\ntranslation. Extensive experiments conducted on a synthetic dataset have\ndemonstrated that the images generated by our model are much better than those\ngenerated by existing methods, with more consistent global appearance and\nsharper details. Ablation studies and discussions show its reliability and\nrobustness in some challenging cases. \n\n"}
{"id": "1808.00505", "contents": "Title: Wavelet Sparse Regularization for Manifold-Valued Data Abstract: In this paper, we consider the sparse regularization of manifold-valued data\nwith respect to an interpolatory wavelet/multiscale transform. We propose and\nstudy variational models for this task and provide results on their\nwell-posedness. We present algorithms for a numerical realization of these\nmodels in the manifold setup. Further, we provide experimental results to show\nthe potential of the proposed schemes for applications. \n\n"}
{"id": "1808.00736", "contents": "Title: Dynamic Adaptation on Non-Stationary Visual Domains Abstract: Domain adaptation aims to learn models on a supervised source domain that\nperform well on an unsupervised target. Prior work has examined domain\nadaptation in the context of stationary domain shifts, i.e. static data sets.\nHowever, with large-scale or dynamic data sources, data from a defined domain\nis not usually available all at once. For instance, in a streaming data\nscenario, dataset statistics effectively become a function of time. We\nintroduce a framework for adaptation over non-stationary distribution shifts\napplicable to large-scale and streaming data scenarios. The model is adapted\nsequentially over incoming unsupervised streaming data batches. This enables\nimprovements over several batches without the need for any additionally\nannotated data. To demonstrate the effectiveness of our proposed framework, we\nmodify associative domain adaptation to work well on source and target data\nbatches with unequal class distributions. We apply our method to several\nadaptation benchmark datasets for classification and show improved classifier\naccuracy not only for the currently adapted batch, but also when applied on\nfuture stream batches. Furthermore, we show the applicability of our\nassociative learning modifications to semantic segmentation, where we achieve\ncompetitive results. \n\n"}
{"id": "1808.01026", "contents": "Title: Prosodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device\n  Text-Independent Speaker Verification Abstract: In this paper a novel cross-device text-independent speaker verification\narchitecture is proposed. Majority of the state-of-the-art deep architectures\nthat are used for speaker verification tasks consider Mel-frequency cepstral\ncoefficients. In contrast, our proposed Siamese convolutional neural network\narchitecture uses Mel-frequency spectrogram coefficients to benefit from the\ndependency of the adjacent spectro-temporal features. Moreover, although\nspectro-temporal features have proved to be highly reliable in speaker\nverification models, they only represent some aspects of short-term acoustic\nlevel traits of the speaker's voice. However, the human voice consists of\nseveral linguistic levels such as acoustic, lexicon, prosody, and phonetics,\nthat can be utilized in speaker verification models. To compensate for these\ninherited shortcomings in spectro-temporal features, we propose to enhance the\nproposed Siamese convolutional neural network architecture by deploying a\nmultilayer perceptron network to incorporate the prosodic, jitter, and shimmer\nfeatures. The proposed end-to-end verification architecture performs feature\nextraction and verification simultaneously. This proposed architecture displays\nsignificant improvement over classical signal processing approaches and deep\nalgorithms for forensic cross-device speaker verification. \n\n"}
{"id": "1808.01097", "contents": "Title: CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images Abstract: We present a simple yet efficient approach capable of training deep neural\nnetworks on large-scale weakly-supervised web images, which are crawled raw\nfrom the Internet by using text queries, without any human annotation. We\ndevelop a principled learning strategy by leveraging curriculum learning, with\nthe goal of handling a massive amount of noisy labels and data imbalance\neffectively. We design a new learning curriculum by measuring the complexity of\ndata using its distribution density in a feature space, and rank the complexity\nin an unsupervised manner. This allows for an efficient implementation of\ncurriculum learning on large-scale web images, resulting in a high-performance\nCNN model, where the negative impact of noisy labels is reduced substantially.\nImportantly, we show by experiments that those images with highly noisy labels\ncan surprisingly improve the generalization capability of the model, by serving\nas a manner of regularization. Our approaches obtain state-of-the-art\nperformance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101.\nWith an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on\nthe WebVision challenge for 1000-category classification. This result was the\ntop performance by a wide margin, outperforming second place by a nearly 50%\nrelative error rate. Code and models are available at:\nhttps://github.com/MalongTech/CurriculumNet . \n\n"}
{"id": "1808.01562", "contents": "Title: Tracklet Association Tracker: An End-to-End Learning-based Association\n  Approach for Multi-Object Tracking Abstract: Traditional multiple object tracking methods divide the task into two parts:\naffinity learning and data association. The separation of the task requires to\ndefine a hand-crafted training goal in affinity learning stage and a\nhand-crafted cost function of data association stage, which prevents the\ntracking goals from learning directly from the feature. In this paper, we\npresent a new multiple object tracking (MOT) framework with data-driven\nassociation method, named as Tracklet Association Tracker (TAT). The framework\naims at gluing feature learning and data association into a unity by a bi-level\noptimization formulation so that the association results can be directly\nlearned from features. To boost the performance, we also adopt the popular\nhierarchical association and perform the necessary alignment and selection of\nraw detection responses. Our model trains over 20X faster than a similar\napproach, and achieves the state-of-the-art performance on both MOT2016 and\nMOT2017 benchmarks. \n\n"}
{"id": "1808.01944", "contents": "Title: V-FCNN: Volumetric Fully Convolution Neural Network For Automatic Atrial\n  Segmentation Abstract: Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder\nthat causes changes in the anatomy of the atria. A better characterization of\nthese changes is desirable for the definition of clinical biomarkers,\nfurthermore, thus there is a need for its fully automatic segmentation from\nclinical images. In this work, we present an architecture based on\n3D-convolution kernels, a Volumetric Fully Convolution Neural Network (V-FCNN),\nable to segment the entire volume in a one-shot, and consequently integrate the\nimplicit spatial redundancy present in high-resolution images. A loss function\nbased on the mixture of both Mean Square Error (MSE) and Dice Loss (DL) is\nused, in an attempt to combine the ability to capture the bulk shape as well as\nthe reduction of local errors products by over-segmentation. Results\ndemonstrate a reasonable performance in the middle region of the atria along\nwith the impact of the challenges of capturing the variability of the pulmonary\nveins or the identification of the valve plane that separates the atria to the\nventricle. A final dice of $92.5\\%$ in $54$ patients ($4752$ atria test slices\nin total) is shown. \n\n"}
{"id": "1808.01992", "contents": "Title: Simultaneous Edge Alignment and Learning Abstract: Edge detection is among the most fundamental vision problems for its role in\nperceptual grouping and its wide applications. Recent advances in\nrepresentation learning have led to considerable improvements in this area.\nMany state of the art edge detection models are learned with fully\nconvolutional networks (FCNs). However, FCN-based edge learning tends to be\nvulnerable to misaligned labels due to the delicate structure of edges. While\nsuch problem was considered in evaluation benchmarks, similar issue has not\nbeen explicitly addressed in general edge learning. In this paper, we show that\nlabel misalignment can cause considerably degraded edge learning quality, and\naddress this issue by proposing a simultaneous edge alignment and learning\nframework. To this end, we formulate a probabilistic model where edge alignment\nis treated as latent variable optimization, and is learned end-to-end during\nnetwork training. Experiments show several applications of this work, including\nimproved edge detection with state of the art performance, and automatic\nrefinement of noisy annotations. \n\n"}
{"id": "1808.02096", "contents": "Title: Semi-supervised Deep Generative Modelling of Incomplete Multi-Modality\n  Emotional Data Abstract: There are threefold challenges in emotion recognition. First, it is difficult\nto recognize human's emotional states only considering a single modality.\nSecond, it is expensive to manually annotate the emotional data. Third,\nemotional data often suffers from missing modalities due to unforeseeable\nsensor malfunction or configuration issues. In this paper, we address all these\nproblems under a novel multi-view deep generative framework. Specifically, we\npropose to model the statistical relationships of multi-modality emotional data\nusing multiple modality-specific generative networks with a shared latent\nspace. By imposing a Gaussian mixture assumption on the posterior approximation\nof the shared latent variables, our framework can learn the joint deep\nrepresentation from multiple modalities and evaluate the importance of each\nmodality simultaneously. To solve the labeled-data-scarcity problem, we extend\nour multi-view model to semi-supervised learning scenario by casting the\nsemi-supervised classification problem as a specialized missing data imputation\ntask. To address the missing-modality problem, we further extend our\nsemi-supervised multi-view model to deal with incomplete data, where a missing\nview is treated as a latent variable and integrated out during inference. This\nway, the proposed overall framework can utilize all available (both labeled and\nunlabeled, as well as both complete and incomplete) data to improve its\ngeneralization ability. The experiments conducted on two real multi-modal\nemotion datasets demonstrated the superiority of our framework. \n\n"}
{"id": "1808.02357", "contents": "Title: Acoustic Scene Classification: A Competition Review Abstract: In this paper we study the problem of acoustic scene classification, i.e.,\ncategorization of audio sequences into mutually exclusive classes based on\ntheir spectral content. We describe the methods and results discovered during a\ncompetition organized in the context of a graduate machine learning course;\nboth by the students and external participants. We identify the most suitable\nmethods and study the impact of each by performing an ablation study of the\nmixture of approaches. We also compare the results with a neural network\nbaseline, and show the improvement over that. Finally, we discuss the impact of\nusing a competition as a part of a university course, and justify its\nimportance in the curriculum based on student feedback. \n\n"}
{"id": "1808.04256", "contents": "Title: CT Super-resolution GAN Constrained by the Identical, Residual, and\n  Cycle Learning Ensemble(GAN-CIRCLE) Abstract: Computed tomography (CT) is widely used in screening, diagnosis, and\nimage-guided therapy for both clinical and research purposes. Since CT involves\nionizing radiation, an overarching thrust of related technical research is\ndevelopment of novel methods enabling ultrahigh quality imaging with fine\nstructural details while reducing the X-ray radiation. In this paper, we\npresent a semi-supervised deep learning approach to accurately recover\nhigh-resolution (HR) CT images from low-resolution (LR) counterparts.\nSpecifically, with the generative adversarial network (GAN) as the building\nblock, we enforce the cycle-consistency in terms of the Wasserstein distance to\nestablish a nonlinear end-to-end mapping from noisy LR input images to denoised\nand deblurred HR outputs. We also include the joint constraints in the loss\nfunction to facilitate structural preservation. In this deep imaging process,\nwe incorporate deep convolutional neural network (CNN), residual learning, and\nnetwork in network techniques for feature extraction and restoration. In\ncontrast to the current trend of increasing network depth and complexity to\nboost the CT imaging performance, which limit its real-world applications by\nimposing considerable computational and memory overheads, we apply a parallel\n$1\\times1$ CNN to compress the output of the hidden layer and optimize the\nnumber of layers and the number of filters for each convolutional layer.\nQuantitative and qualitative evaluations demonstrate that our proposed model is\naccurate, efficient and robust for super-resolution (SR) image restoration from\nnoisy LR input images. In particular, we validate our composite SR networks on\nthree large-scale CT datasets, and obtain promising results as compared to the\nother state-of-the-art methods. \n\n"}
{"id": "1808.04610", "contents": "Title: Looking Beyond a Clever Narrative: Visual Context and Attention are\n  Primary Drivers of Affect in Video Advertisements Abstract: Emotion evoked by an advertisement plays a key role in influencing brand\nrecall and eventual consumer choices. Automatic ad affect recognition has\nseveral useful applications. However, the use of content-based feature\nrepresentations does not give insights into how affect is modulated by aspects\nsuch as the ad scene setting, salient object attributes and their interactions.\nNeither do such approaches inform us on how humans prioritize visual\ninformation for ad understanding. Our work addresses these lacunae by\ndecomposing video content into detected objects, coarse scene structure, object\nstatistics and actively attended objects identified via eye-gaze. We measure\nthe importance of each of these information channels by systematically\nincorporating related information into ad affect prediction models. Contrary to\nthe popular notion that ad affect hinges on the narrative and the clever use of\nlinguistic and social cues, we find that actively attended objects and the\ncoarse scene structure better encode affective information as compared to\nindividual scene objects or conspicuous background elements. \n\n"}
{"id": "1808.04803", "contents": "Title: Hierarchical binary CNNs for landmark localization with limited\n  resources Abstract: Our goal is to design architectures that retain the groundbreaking\nperformance of Convolutional Neural Networks (CNNs) for landmark localization\nand at the same time are lightweight, compact and suitable for applications\nwith limited computational resources. To this end, we make the following\ncontributions: (a) we are the first to study the effect of neural network\nbinarization on localization tasks, namely human pose estimation and face\nalignment. We exhaustively evaluate various design choices, identify\nperformance bottlenecks, and more importantly propose multiple orthogonal ways\nto boost performance. (b) Based on our analysis, we propose a novel\nhierarchical, parallel and multi-scale residual architecture that yields large\nperformance improvement over the standard bottleneck block while having the\nsame number of parameters, thus bridging the gap between the original network\nand its binarized counterpart. (c) We perform a large number of ablation\nstudies that shed light on the properties and the performance of the proposed\nblock. (d) We present results for experiments on the most challenging datasets\nfor human pose estimation and face alignment, reporting in many cases\nstate-of-the-art performance. (e) We further provide additional results for the\nproblem of facial part segmentation. Code can be downloaded from\nhttps://www.adrianbulat.com/binary-cnn-landmark \n\n"}
{"id": "1808.05382", "contents": "Title: Typhoon track prediction using satellite images in a Generative\n  Adversarial Network Abstract: Tracks of typhoons are predicted using satellite images as input for a\nGenerative Adversarial Network (GAN). The satellite images have time gaps of 6\nhours and are marked with a red square at the location of the typhoon center.\nThe GAN uses images from the past to generate an image one time step ahead. The\ngenerated image shows the future location of the typhoon center, as well as the\nfuture cloud structures. The errors between predicted and real typhoon centers\nare measured quantitatively in kilometers. 42.4% of all typhoon center\npredictions have absolute errors of less than 80 km, 32.1% lie within a range\nof 80 - 120 km and the remaining 25.5% have accuracies above 120 km. The\nrelative error sets the above mentioned absolute error in relation to the\ndistance that has been traveled by a typhoon over the past 6 hours. High\nrelative errors are found in three types of situations, when a typhoon moves on\nthe open sea far away from land, when a typhoon changes its course suddenly and\nwhen a typhoon is about to hit the mainland. The cloud structure prediction is\nevaluated qualitatively. It is shown that the GAN is able to predict trends in\ncloud motion. In order to improve both, the typhoon center and cloud motion\nprediction, the present study suggests to add information about the sea surface\ntemperature, surface pressure and velocity fields to the input data. \n\n"}
{"id": "1808.05492", "contents": "Title: Metric Learning for Novelty and Anomaly Detection Abstract: When neural networks process images which do not resemble the distribution\nseen during training, so called out-of-distribution images, they often make\nwrong predictions, and do so too confidently. The capability to detect\nout-of-distribution images is therefore crucial for many real-world\napplications. We divide out-of-distribution detection between novelty detection\n---images of classes which are not in the training set but are related to\nthose---, and anomaly detection ---images with classes which are unrelated to\nthe training set. By related we mean they contain the same type of objects,\nlike digits in MNIST and SVHN. Most existing work has focused on anomaly\ndetection, and has addressed this problem considering networks trained with the\ncross-entropy loss. Differently from them, we propose to use metric learning\nwhich does not have the drawback of the softmax layer (inherent to\ncross-entropy methods), which forces the network to divide its prediction power\nover the learned classes. We perform extensive experiments and evaluate both\nnovelty and anomaly detection, even in a relevant application such as traffic\nsign recognition, obtaining comparable or better results than previous works. \n\n"}
{"id": "1808.05848", "contents": "Title: Performance Analysis and Robustification of Single-query 6-DoF Camera\n  Pose Estimation Abstract: We consider a single-query 6-DoF camera pose estimation with reference images\nand a point cloud, i.e. the problem of estimating the position and orientation\nof a camera by using reference images and a point cloud. In this work, we\nperform a systematic comparison of three state-of-the-art strategies for 6-DoF\ncamera pose estimation, i.e. feature-based, photometric-based and\nmutual-information-based approaches. The performance of the studied methods is\nevaluated on two standard datasets in terms of success rate, translation error\nand max orientation error. Building on the results analysis, we propose a\nhybrid approach that combines feature-based and mutual-information-based pose\nestimation methods since it provides complementary properties for pose\nestimation. Experiments show that (1) in cases with large environmental\nvariance, the hybrid approach outperforms feature-based and\nmutual-information-based approaches by an average of 25.1% and 5.8% in terms of\nsuccess rate, respectively; (2) in cases where query and reference images are\ncaptured at similar imaging conditions, the hybrid approach performs similarly\nas the feature-based approach, but outperforms both photometric-based and\nmutual-information-based approaches with a clear margin; (3) the feature-based\napproach is consistently more accurate than mutual-information-based and\nphotometric-based approaches when at least 4 consistent matching points are\nfound between the query and reference images. \n\n"}
{"id": "1808.05965", "contents": "Title: On Geometric Analysis of Affine Sparse Subspace Clustering Abstract: Sparse subspace clustering (SSC) is a state-of-the-art method for segmenting\na set of data points drawn from a union of subspaces into their respective\nsubspaces. It is now well understood that SSC produces subspace-preserving data\naffinity under broad geometric conditions but suffers from a connectivity\nissue. In this paper, we develop a novel geometric analysis for a variant of\nSSC, named affine SSC (ASSC), for the problem of clustering data from a union\nof affine subspaces. Our contributions include a new concept called affine\nindependence for capturing the arrangement of a collection of affine subspaces.\nUnder the affine independence assumption, we show that ASSC is guaranteed to\nproduce subspace-preserving affinity. Moreover, inspired by the phenomenon that\nthe $\\ell_1$ regularization no longer induces sparsity when the solution is\nnonnegative, we further show that subspace-preserving recovery can be achieved\nunder much weaker conditions for all data points other than the extreme points\nof samples from each subspace. In addition, we confirm a curious observation\nthat the affinity produced by ASSC may be subspace-dense---which could\nguarantee the subspace-preserving affinity of ASSC to produce correct\nclustering under rather weak conditions. We validate the theoretical findings\non carefully designed synthetic data and evaluate the performance of ASSC on\nseveral real data sets. \n\n"}
{"id": "1808.06671", "contents": "Title: Adversarial Sampling for Active Learning Abstract: This paper proposes asal, a new GAN based active learning method that\ngenerates high entropy samples. Instead of directly annotating the synthetic\nsamples, ASAL searches similar samples from the pool and includes them for\ntraining. Hence, the quality of new samples is high and annotations are\nreliable. To the best of our knowledge, ASAL is the first GAN based AL method\napplicable to multi-class problems that outperforms random sample selection.\nAnother benefit of ASAL is its small run-time complexity (sub-linear) compared\nto traditional uncertainty sampling (linear). We present a comprehensive set of\nexperiments on multiple traditional data sets and show that ASAL outperforms\nsimilar methods and clearly exceeds the established baseline (random sampling).\nIn the discussion section we analyze in which situations ASAL performs best and\nwhy it is sometimes hard to outperform random sample selection. \n\n"}
{"id": "1808.06739", "contents": "Title: Constrained-size Tensorflow Models for YouTube-8M Video Understanding\n  Challenge Abstract: This paper presents our 7th place solution to the second YouTube-8M video\nunderstanding competition which challenges participates to build a\nconstrained-size model to classify millions of YouTube videos into thousands of\nclasses. Our final model consists of four single models aggregated into one\ntensorflow graph. For each single model, we use the same network architecture\nas in the winning solution of the first YouTube-8M video understanding\ncompetition, namely Gated NetVLAD. We train the single models separately in\ntensorflow's default float32 precision, then replace weights with float16\nprecision and ensemble them in the evaluation and inference stages., achieving\n48.5% compression rate without loss of precision. Our best model achieved\n88.324% GAP on private leaderboard. The code is publicly available at\nhttps://github.com/boliu61/youtube-8m \n\n"}
{"id": "1808.08282", "contents": "Title: Controlling Over-generalization and its Effect on Adversarial Examples\n  Generation and Detection Abstract: Convolutional Neural Networks (CNNs) significantly improve the\nstate-of-the-art for many applications, especially in computer vision. However,\nCNNs still suffer from a tendency to confidently classify out-distribution\nsamples from unknown classes into pre-defined known classes. Further, they are\nalso vulnerable to adversarial examples. We are relating these two issues\nthrough the tendency of CNNs to over-generalize for areas of the input space\nnot covered well by the training set. We show that a CNN augmented with an\nextra output class can act as a simple yet effective end-to-end model for\ncontrolling over-generalization. As an appropriate training set for the extra\nclass, we introduce two resources that are computationally efficient to obtain:\na representative natural out-distribution set and interpolated in-distribution\nsamples. To help select a representative natural out-distribution set among\navailable ones, we propose a simple measurement to assess an out-distribution\nset's fitness. We also demonstrate that training such an augmented CNN with\nrepresentative out-distribution natural datasets and some interpolated samples\nallows it to better handle a wide range of unseen out-distribution samples and\nblack-box adversarial examples without training it on any adversaries. Finally,\nwe show that generation of white-box adversarial attacks using our proposed\naugmented CNN can become harder, as the attack algorithms have to get around\nthe rejection regions when generating actual adversaries. \n\n"}
{"id": "1808.08509", "contents": "Title: Efficient Single Image Super Resolution using Enhanced Learned Group\n  Convolutions Abstract: Convolutional Neural Networks (CNNs) have demonstrated great results for the\nsingle-image super-resolution (SISR) problem. Currently, most CNN algorithms\npromote deep and computationally expensive models to solve SISR. However, we\npropose a novel SISR method that uses relatively less number of computations.\nOn training, we get group convolutions that have unused connections removed. We\nhave refined this system specifically for the task at hand by removing\nunnecessary modules from original CondenseNet. Further, a reconstruction\nnetwork consisting of deconvolutional layers has been used in order to upscale\nto high resolution. All these steps significantly reduce the number of\ncomputations required at testing time. Along with this, bicubic upsampled input\nis added to the network output for easier learning. Our model is named\nSRCondenseNet. We evaluate the method using various benchmark datasets and show\nthat it performs favourably against the state-of-the-art methods in terms of\nboth accuracy and number of computations required. \n\n"}
{"id": "1808.09351", "contents": "Title: 3D-Aware Scene Manipulation via Inverse Graphics Abstract: We aim to obtain an interpretable, expressive, and disentangled scene\nrepresentation that contains comprehensive structural and textural information\nfor each object. Previous scene representations learned by neural networks are\noften uninterpretable, limited to a single object, or lacking 3D knowledge. In\nthis work, we propose 3D scene de-rendering networks (3D-SDN) to address the\nabove issues by integrating disentangled representations for semantics,\ngeometry, and appearance into a deep generative model. Our scene encoder\nperforms inverse graphics, translating a scene into a structured object-wise\nrepresentation. Our decoder has two components: a differentiable shape renderer\nand a neural texture generator. The disentanglement of semantics, geometry, and\nappearance supports 3D-aware scene manipulation, e.g., rotating and moving\nobjects freely while keeping the consistent shape and texture, and changing the\nobject appearance without affecting its shape. Experiments demonstrate that our\nediting scheme based on 3D-SDN is superior to its 2D counterpart. \n\n"}
{"id": "1808.09964", "contents": "Title: Semi-Metrification of the Dynamic Time Warping Distance Abstract: The dynamic time warping (dtw) distance fails to satisfy the triangle\ninequality and the identity of indiscernibles. As a consequence, the\ndtw-distance is not warping-invariant, which in turn results in peculiarities\nin data mining applications. This article converts the dtw-distance to a\nsemi-metric and shows that its canonical extension is warping-invariant.\nEmpirical results indicate that the nearest-neighbor classifier in the proposed\nsemi-metric space performs comparably to the same classifier in the standard\ndtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result\nsuggests to further explore the semi-metric space for data mining applications. \n\n"}
{"id": "1809.00101", "contents": "Title: Attentive Crowd Flow Machines Abstract: Traffic flow prediction is crucial for urban traffic management and public\nsafety. Its key challenges lie in how to adaptively integrate the various\nfactors that affect the flow changes. In this paper, we propose a unified\nneural network module to address this problem, called Attentive Crowd Flow\nMachine~(ACFM), which is able to infer the evolution of the crowd flow by\nlearning dynamic representations of temporally-varying data with an attention\nmechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units\nconnected with a convolutional layer for spatial weight prediction. The first\nLSTM takes the sequential flow density representation as input and generates a\nhidden state at each time-step for attention map inference, while the second\nLSTM aims at learning the effective spatial-temporal feature expression from\nattentionally weighted crowd flow features. Based on the ACFM, we further build\na deep architecture with the application to citywide crowd flow prediction,\nwhich naturally incorporates the sequential and periodic data as well as other\nexternal influences. Extensive experiments on two standard benchmarks (i.e.,\ncrowd flow in Beijing and New York City) show that the proposed method achieves\nsignificant improvements over the state-of-the-art methods. \n\n"}
{"id": "1809.00241", "contents": "Title: Activity Recognition on a Large Scale in Short Videos - Moments in Time\n  Dataset Abstract: Moments capture a huge part of our lives. Accurate recognition of these\nmoments is challenging due to the diverse and complex interpretation of the\nmoments. Action recognition refers to the act of classifying the desired\naction/activity present in a given video. In this work, we perform experiments\non Moments in Time dataset to recognize accurately activities occurring in 3\nsecond clips. We use state of the art techniques for visual, auditory and\nspatio temporal localization and develop method to accurately classify the\nactivity in the Moments in Time dataset. Our novel approach of using Visual\nBased Textual features and fusion techniques performs well providing an overall\n89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the\nBaseline TRN model. \n\n"}
{"id": "1809.00251", "contents": "Title: Car Monitoring System in Apartment Garages by Small Autonomous Car using\n  Deep Learning Abstract: Currently, there is an increase in the number of Peruvian families living in\napartments instead of houses for the lots of advantage; However, in some cases\nthere are troubles such as robberies of goods that are usually left at the\nparking lots or the entrance of strangers that use the tenants parking lots\n(this last trouble sometimes is related to kidnappings or robberies in building\napartments). Due to these problems, the use of a self-driving mini-car is\nproposed to implement a monitoring system of license plates in an underground\ngarage inside a building using a deep learning model with the aim of recording\nthe vehicles and identifying their owners if they were tenants or not. In\naddition, the small robot has its own location system using beacons that allow\nus to identify the position of the parking lot corresponding to each tenant of\nthe building while the mini-car is on its way. Finally, one of the objectives\nof this work is to build a low-cost mini-robot that would replace expensive\ncameras or work together in order to keep safe the goods of tenants. \n\n"}
{"id": "1809.00888", "contents": "Title: MesoNet: a Compact Facial Video Forgery Detection Network Abstract: This paper presents a method to automatically and efficiently detect face\ntampering in videos, and particularly focuses on two recent techniques used to\ngenerate hyper-realistic forged videos: Deepfake and Face2Face. Traditional\nimage forensics techniques are usually not well suited to videos due to the\ncompression that strongly degrades the data. Thus, this paper follows a deep\nlearning approach and presents two networks, both with a low number of layers\nto focus on the mesoscopic properties of images. We evaluate those fast\nnetworks on both an existing dataset and a dataset we have constituted from\nonline videos. The tests demonstrate a very successful detection rate with more\nthan 98% for Deepfake and 95% for Face2Face. \n\n"}
{"id": "1809.01000", "contents": "Title: Bayesian Outdoor Defect Detection Abstract: We introduce a Bayesian defect detector to facilitate the defect detection on\nthe motion blurred images on rough texture surfaces. To enhance the accuracy of\nBayesian detection on removing non-defect pixels, we develop a class of\nreflected non-local prior distributions, which is constructed by using the mode\nof a distribution to subtract its density. The reflected non-local priors\nforces the Bayesian detector to approach 0 at the non-defect locations. We\nconduct experiments studies to demonstrate the superior performance of the\nBayesian detector in eliminating the non-defect points. We implement the\nBayesian detector in the motion blurred drone images, in which the detector\nsuccessfully identifies the hail damages on the rough surface and substantially\nenhances the accuracy of the entire defect detection pipeline. \n\n"}
{"id": "1809.01337", "contents": "Title: Localizing Moments in Video with Temporal Language Abstract: Localizing moments in a longer video via natural language queries is a new,\nchallenging task at the intersection of language and video understanding.\nThough moment localization with natural language is similar to other language\nand vision tasks like natural language object retrieval in images, moment\nlocalization offers an interesting opportunity to model temporal dependencies\nand reasoning in text. We propose a new model that explicitly reasons about\ndifferent temporal segments in a video, and shows that temporal context is\nimportant for localizing phrases which include temporal language. To benchmark\nwhether our model, and other recent video localization models, can effectively\nreason about temporal language, we collect the novel TEMPOral reasoning in\nvideo and language (TEMPO) dataset. Our dataset consists of two parts: a\ndataset with real videos and template sentences (TEMPO - Template Language)\nwhich allows for controlled studies on temporal language, and a human language\ndataset which consists of temporal sentences annotated by humans (TEMPO - Human\nLanguage). \n\n"}
{"id": "1809.01438", "contents": "Title: How is Contrast Encoded in Deep Neural Networks? Abstract: Contrast is a crucial factor in visual information processing. It is desired\nfor a visual system - irrespective of being biological or artificial - to\n\"perceive\" the world robustly under large potential changes in illumination. In\nthis work, we studied the responses of deep neural networks (DNN) to identical\nimages at different levels of contrast. We analysed the activation of kernels\nin the convolutional layers of eight prominent networks with distinct\narchitectures (e.g. VGG and Inception). The results of our experiments indicate\nthat those networks with a higher tolerance to alteration of contrast have more\nthan one convolutional layer prior to the first max-pooling operator. It\nappears that the last convolutional layer before the first max-pooling acts as\na mitigator of contrast variation in input images. In our investigation,\ninterestingly, we observed many similarities between the mechanisms of these\nDNNs and biological visual systems. These comparisons allow us to understand\nmore profoundly the underlying mechanisms of a visual system that is grounded\non the basis of \"data-analysis\". \n\n"}
{"id": "1809.01567", "contents": "Title: Deep Depth from Defocus: how can defocus blur improve 3D estimation\n  using dense neural networks? Abstract: Depth estimation is of critical interest for scene understanding and accurate\n3D reconstruction. Most recent approaches in depth estimation with deep\nlearning exploit geometrical structures of standard sharp images to predict\ncorresponding depth maps. However, cameras can also produce images with defocus\nblur depending on the depth of the objects and camera settings. Hence, these\nfeatures may represent an important hint for learning to predict depth. In this\npaper, we propose a full system for single-image depth prediction in the wild\nusing depth-from-defocus and neural networks. We carry out thorough experiments\nto test deep convolutional networks on real and simulated defocused images\nusing a realistic model of blur variation with respect to depth. We also\ninvestigate the influence of blur on depth prediction observing model\nuncertainty with a Bayesian neural network approach. From these studies, we\nshow that out-of-focus blur greatly improves the depth-prediction network\nperformances. Furthermore, we transfer the ability learned on a synthetic,\nindoor dataset to real, indoor and outdoor images. For this purpose, we present\na new dataset containing real all-focus and defocused images from a Digital\nSingle-Lens Reflex (DSLR) camera, paired with ground truth depth maps obtained\nwith an active 3D sensor for indoor scenes. The proposed approach is\nsuccessfully validated on both this new dataset and standard ones as NYUv2 or\nDepth-in-the-Wild. Code and new datasets are available at\nhttps://github.com/marcelampc/d3net_depth_estimation \n\n"}
{"id": "1809.01610", "contents": "Title: Bimodal network architectures for automatic generation of image\n  annotation from text Abstract: Medical image analysis practitioners have embraced big data methodologies.\nThis has created a need for large annotated datasets. The source of big data is\ntypically large image collections and clinical reports recorded for these\nimages. In many cases, however, building algorithms aimed at segmentation and\ndetection of disease requires a training dataset with markings of the areas of\ninterest on the image that match with the described anomalies. This process of\nannotation is expensive and needs the involvement of clinicians. In this work\nwe propose two separate deep neural network architectures for automatic marking\nof a region of interest (ROI) on the image best representing a finding\nlocation, given a textual report or a set of keywords. One architecture\nconsists of LSTM and CNN components and is trained end to end with images,\nmatching text, and markings of ROIs for those images. The output layer\nestimates the coordinates of the vertices of a polygonal region. The second\narchitecture uses a network pre-trained on a large dataset of the same image\ntypes for learning feature representations of the findings of interest. We show\nthat for a variety of findings from chest X-ray images, both proposed\narchitectures learn to estimate the ROI, as validated by clinical annotations.\nThere is a clear advantage obtained from the architecture with pre-trained\nimaging network. The centroids of the ROIs marked by this network were on\naverage at a distance equivalent to 5.1% of the image width from the centroids\nof the ground truth ROIs. \n\n"}
{"id": "1809.02110", "contents": "Title: Panoptic Segmentation with a Joint Semantic and Instance Segmentation\n  Network Abstract: We present a single network method for panoptic segmentation. This method\ncombines the predictions from a jointly trained semantic and instance\nsegmentation network using heuristics. Joint training is the first step towards\nan end-to-end panoptic segmentation network and is faster and more memory\nefficient than training and predicting with two networks, as done in previous\nwork. The architecture consists of a ResNet-50 feature extractor shared by the\nsemantic segmentation and instance segmentation branch. For instance\nsegmentation, a Mask R-CNN type of architecture is used, while the semantic\nsegmentation branch is augmented with a Pyramid Pooling Module. Results for\nthis method are submitted to the COCO and Mapillary Joint Recognition Challenge\n2018. Our approach achieves a PQ score of 17.6 on the Mapillary Vistas\nvalidation set and 27.2 on the COCO test-dev set. \n\n"}
{"id": "1809.02156", "contents": "Title: Object Hallucination in Image Captioning Abstract: Despite continuously improving performance, contemporary image captioning\nmodels are prone to \"hallucinating\" objects that are not actually in a scene.\nOne problem is that standard metrics only measure similarity to ground truth\ncaptions and may not fully capture image relevance. In this work, we propose a\nnew image relevance metric to evaluate current models with veridical visual\nlabels and assess their rate of object hallucination. We analyze how captioning\nmodel architectures and learning objectives contribute to object hallucination,\nexplore when hallucination is likely due to image misclassification or language\npriors, and assess how well current sentence metrics capture object\nhallucination. We investigate these questions on the standard image captioning\nbenchmark, MSCOCO, using a diverse set of models. Our analysis yields several\ninteresting findings, including that models which score best on standard\nsentence metrics do not always have lower hallucination and that models which\nhallucinate more tend to make errors driven by language priors. \n\n"}
{"id": "1809.02587", "contents": "Title: Self-Supervised Generation of Spatial Audio for 360 Video Abstract: We introduce an approach to convert mono audio recorded by a 360 video camera\ninto spatial audio, a representation of the distribution of sound over the full\nviewing sphere. Spatial audio is an important component of immersive 360 video\nviewing, but spatial audio microphones are still rare in current 360 video\nproduction. Our system consists of end-to-end trainable neural networks that\nseparate individual sound sources and localize them on the viewing sphere,\nconditioned on multi-modal analysis of audio and 360 video frames. We introduce\nseveral datasets, including one filmed ourselves, and one collected in-the-wild\nfrom YouTube, consisting of 360 videos uploaded with spatial audio. During\ntraining, ground-truth spatial audio serves as self-supervision and a mixed\ndown mono track forms the input to our network. Using our approach, we show\nthat it is possible to infer the spatial location of sound sources based only\non 360 video and a mono audio track. \n\n"}
{"id": "1809.02983", "contents": "Title: Dual Attention Network for Scene Segmentation Abstract: In this paper, we address the scene segmentation task by capturing rich\ncontextual dependencies based on the selfattention mechanism. Unlike previous\nworks that capture contexts by multi-scale features fusion, we propose a Dual\nAttention Networks (DANet) to adaptively integrate local features with their\nglobal dependencies. Specifically, we append two types of attention modules on\ntop of traditional dilated FCN, which model the semantic interdependencies in\nspatial and channel dimensions respectively. The position attention module\nselectively aggregates the features at each position by a weighted sum of the\nfeatures at all positions. Similar features would be related to each other\nregardless of their distances. Meanwhile, the channel attention module\nselectively emphasizes interdependent channel maps by integrating associated\nfeatures among all channel maps. We sum the outputs of the two attention\nmodules to further improve feature representation which contributes to more\nprecise segmentation results. We achieve new state-of-the-art segmentation\nperformance on three challenging scene segmentation datasets, i.e., Cityscapes,\nPASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5%\non Cityscapes test set is achieved without using coarse data. We make the code\nand trained model publicly available at https://github.com/junfu1115/DANet \n\n"}
{"id": "1809.03316", "contents": "Title: Hierarchical Video Understanding Abstract: We introduce a hierarchical architecture for video understanding that\nexploits the structure of real world actions by capturing targets at different\nlevels of granularity. We design the model such that it first learns simpler\ncoarse-grained tasks, and then moves on to learn more fine-grained targets. The\nmodel is trained with a joint loss on different granularity levels. We\ndemonstrate empirical results on the recent release of Something-Something\ndataset, which provides a hierarchy of targets, namely coarse-grained action\ngroups, fine-grained action categories, and captions. Experiments suggest that\nmodels that exploit targets at different levels of granularity achieve better\nperformance on all levels. \n\n"}
{"id": "1809.04135", "contents": "Title: Simultaneous Localization and Layout Model Selection in Manhattan Worlds Abstract: In this paper, we will demonstrate how Manhattan structure can be exploited\nto transform the Simultaneous Localization and Mapping (SLAM) problem, which is\ntypically solved by a nonlinear optimization over feature positions, into a\nmodel selection problem solved by a convex optimization over higher order\nlayout structures, namely walls, floors, and ceilings. Furthermore, we show how\nour novel formulation leads to an optimization procedure that automatically\nperforms data association and loop closure and which ultimately produces the\nsimplest model of the environment that is consistent with the available\nmeasurements. We verify our method on real world data sets collected with\nvarious sensing modalities. \n\n"}
{"id": "1809.05825", "contents": "Title: Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN\n  Trained on Synthetic Data Abstract: The ability to segment unknown objects in depth images has potential to\nenhance robot skills in grasping and object tracking. Recent computer vision\nresearch has demonstrated that Mask R-CNN can be trained to segment specific\ncategories of objects in RGB images when massive hand-labeled datasets are\navailable. As generating these datasets is time consuming, we instead train\nwith synthetic depth images. Many robots now use depth sensors, and recent\nresults suggest training on synthetic depth data can transfer successfully to\nthe real world. We present a method for automated dataset generation and\nrapidly generate a synthetic training dataset of 50,000 depth images and\n320,000 object masks using simulated heaps of 3D CAD models. We train a variant\nof Mask R-CNN with domain randomization on the generated dataset to perform\ncategory-agnostic instance segmentation without any hand-labeled data and we\nevaluate the trained network, which we refer to as Synthetic Depth (SD) Mask\nR-CNN, on a set of real, high-resolution depth images of challenging,\ndensely-cluttered bins containing objects with highly-varied geometry. SD Mask\nR-CNN outperforms point cloud clustering baselines by an absolute 15% in\nAverage Precision and 20% in Average Recall on COCO benchmarks, and achieves\nperformance levels similar to a Mask R-CNN trained on a massive, hand-labeled\nRGB dataset and fine-tuned on real images from the experimental setup. We\ndeploy the model in an instance-specific grasping pipeline to demonstrate its\nusefulness in a robotics application. Code, the synthetic training dataset, and\nsupplementary material are available at https://bit.ly/2letCuE. \n\n"}
{"id": "1809.05828", "contents": "Title: Real-Time, Highly Accurate Robotic Grasp Detection using Fully\n  Convolutional Neural Networks with High-Resolution Images Abstract: Robotic grasp detection for novel objects is a challenging task, but for the\nlast few years, deep learning based approaches have achieved remarkable\nperformance improvements, up to 96.1% accuracy, with RGB-D data. In this paper,\nwe propose fully convolutional neural network (FCNN) based methods for robotic\ngrasp detection. Our methods also achieved state-of-the-art detection accuracy\n(up to 96.6%) with state-of- the-art real-time computation time for\nhigh-resolution images (6-20ms per 360x360 image) on Cornell dataset. Due to\nFCNN, our proposed method can be applied to images with any size for detecting\nmultigrasps on multiobjects. Proposed methods were evaluated using 4-axis robot\narm with small parallel gripper and RGB-D camera for grasping challenging\nsmall, novel objects. With accurate vision-robot coordinate calibration through\nour proposed learning-based, fully automatic approach, our proposed method\nyielded 90% success rate. \n\n"}
{"id": "1809.06036", "contents": "Title: Binocular Tone Mapping with Improved Overall Contrast and Local Details Abstract: Tone mapping is a commonly used technique that maps the set of colors in\nhigh-dynamic-range (HDR) images to another set of colors in low-dynamic-range\n(LDR) images, to fit the need for print-outs, LCD monitors and projectors.\nUnfortunately, during the compression of dynamic range, the overall contrast\nand local details generally cannot be preserved simultaneously. Recently, with\nthe increased use of stereoscopic devices, the notion of binocular tone mapping\nhas been proposed in the existing research study. However, the existing\nresearch lacks the binocular perception study and is unable to generate the\noptimal binocular pair that presents the most visual content. In this paper, we\npropose a novel perception-based binocular tone mapping method, that can\ngenerate an optimal binocular image pair (generating left and right images\nsimultaneously) from an HDR image that presents the most visual content by\ndesigning a binocular perception metric. Our method outperforms the existing\nmethod in terms of both visual and time performance. \n\n"}
{"id": "1809.06045", "contents": "Title: Building Prior Knowledge: A Markov Based Pedestrian Prediction Model\n  Using Urban Environmental Data Abstract: Autonomous Vehicles navigating in urban areas have a need to understand and\npredict future pedestrian behavior for safer navigation. This high level of\nsituational awareness requires observing pedestrian behavior and extrapolating\ntheir positions to know future positions. While some work has been done in this\nfield using Hidden Markov Models (HMMs), one of the few observed drawbacks of\nthe method is the need for informed priors for learning behavior. In this work,\nan extension to the Growing Hidden Markov Model (GHMM) method is proposed to\nsolve some of these drawbacks. This is achieved by building on existing work\nusing potential cost maps and the principle of Natural Vision. As a\nconsequence, the proposed model is able to predict pedestrian positions more\nprecisely over a longer horizon compared to the state of the art. The method is\ntested over \"legal\" and \"illegal\" behavior of pedestrians, having trained the\nmodel with sparse observations and partial trajectories. The method, with no\ntraining data, is compared against a trained state of the art model. It is\nobserved that the proposed method is robust even in new, previously unseen\nareas. \n\n"}
{"id": "1809.06357", "contents": "Title: Apple Flower Detection using Deep Convolutional Networks Abstract: To optimize fruit production, a portion of the flowers and fruitlets of apple\ntrees must be removed early in the growing season. The proportion to be removed\nis determined by the bloom intensity, i.e., the number of flowers present in\nthe orchard. Several automated computer vision systems have been proposed to\nestimate bloom intensity, but their overall performance is still far from\nsatisfactory even in relatively controlled environments. With the goal of\ndevising a technique for flower identification which is robust to clutter and\nto changes in illumination, this paper presents a method in which a pre-trained\nconvolutional neural network is fine-tuned to become specially sensitive to\nflowers. Experimental results on a challenging dataset demonstrate that our\nmethod significantly outperforms three approaches that represent the state of\nthe art in flower detection, with recall and precision rates higher than\n$90\\%$. Moreover, a performance assessment on three additional datasets\npreviously unseen by the network, which consist of different flower species and\nwere acquired under different conditions, reveals that the proposed method\nhighly surpasses baseline approaches in terms of generalization capability. \n\n"}
{"id": "1809.07082", "contents": "Title: Deep Learning Based Rib Centerline Extraction and Labeling Abstract: Automated extraction and labeling of rib centerlines is a typically needed\nprerequisite for more advanced assisted reading tools that help the radiologist\nto efficiently inspect all 24 ribs in a CT volume. In this paper, we combine a\ndeep learning-based rib detection with a dedicated centerline extraction\nalgorithm applied to the detection result for the purpose of fast, robust and\naccurate rib centerline extraction and labeling from CT volumes. More\nspecifically, we first apply a fully convolutional neural network (FCNN) to\ngenerate a probability map for detecting the first rib pair, the twelfth rib\npair, and the collection of all intermediate ribs. In a second stage, a newly\ndesigned centerline extraction algorithm is applied to this multi-label\nprobability map. Finally, the distinct detection of first and twelfth rib\nseparately, allows to derive individual rib labels by simple sorting and\ncounting the detected centerlines. We applied our method to CT volumes from 116\npatients which included a variety of different challenges and achieved a\ncenterline accuracy of 0.787 mm with respect to manual centerline annotations.\n  This article is a preprint version of: Lenga M., Klinder T., B\\\"urger C., von\nBerg J., Franz A., Lorenz C. (2019) Deep Learning Based Rib Centerline\nExtraction and Labeling. In: Vrtovec T., Yao J., Zheng G., Pozo J. (eds)\nComputational Methods and Clinical Applications in Musculoskeletal Imaging.\nMSKI 2018. Lecture Notes in Computer Science, vol 11404. Springer, Cham \n\n"}
{"id": "1809.07517", "contents": "Title: The 2018 PIRM Challenge on Perceptual Image Super-resolution Abstract: This paper reports on the 2018 PIRM challenge on perceptual super-resolution\n(SR), held in conjunction with the Perceptual Image Restoration and\nManipulation (PIRM) workshop at ECCV 2018. In contrast to previous SR\nchallenges, our evaluation methodology jointly quantifies accuracy and\nperceptual quality, therefore enabling perceptual-driven methods to compete\nalongside algorithms that target PSNR maximization. Twenty-one participating\nteams introduced algorithms which well-improved upon the existing\nstate-of-the-art methods in perceptual SR, as confirmed by a human opinion\nstudy. We also analyze popular image quality measures and draw conclusions\nregarding which of them correlates best with human opinion scores. We conclude\nwith an analysis of the current trends in perceptual SR, as reflected from the\nleading submissions. \n\n"}
{"id": "1809.10954", "contents": "Title: Deep Adaptive Learning for Writer Identification based on Single\n  Handwritten Word Images Abstract: There are two types of information in each handwritten word image: explicit\ninformation which can be easily read or derived directly, such as lexical\ncontent or word length, and implicit attributes such as the author's identity.\nWhether features learned by a neural network for one task can be used for\nanother task remains an open question. In this paper, we present a deep\nadaptive learning method for writer identification based on single-word images\nusing multi-task learning. An auxiliary task is added to the training process\nto enforce the emergence of reusable features. Our proposed method transfers\nthe benefits of the learned features of a convolutional neural network from an\nauxiliary task such as explicit content recognition to the main task of writer\nidentification in a single procedure. Specifically, we propose a new adaptive\nconvolutional layer to exploit the learned deep features. A multi-task neural\nnetwork with one or several adaptive convolutional layers is trained\nend-to-end, to exploit robust generic features for a specific main task, i.e.,\nwriter identification. Three auxiliary tasks, corresponding to three explicit\nattributes of handwritten word images (lexical content, word length and\ncharacter attributes), are evaluated. Experimental results on two benchmark\ndatasets show that the proposed deep adaptive learning method can improve the\nperformance of writer identification based on single-word images, compared to\nnon-adaptive and simple linear-adaptive approaches. \n\n"}
{"id": "1810.00091", "contents": "Title: Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized\n  Dropout Abstract: Recently convolutional neural networks (CNNs) achieve great accuracy in\nvisual recognition tasks. DenseNet becomes one of the most popular CNN models\ndue to its effectiveness in feature-reuse. However, like other CNN models,\nDenseNets also face overfitting problem if not severer. Existing dropout method\ncan be applied but not as effective due to the introduced nonlinear\nconnections. In particular, the property of feature-reuse in DenseNet will be\nimpeded, and the dropout effect will be weakened by the spatial correlation\ninside feature maps. To address these problems, we craft the design of a\nspecialized dropout method from three aspects, dropout location, dropout\ngranularity, and dropout probability. The insights attained here could\npotentially be applied as a general approach for boosting the accuracy of other\nCNN models with similar nonlinear connections. Experimental results show that\nDenseNets with our specialized dropout method yield better accuracy compared to\nvanilla DenseNet and state-of-the-art CNN models, and such accuracy boost\nincreases with the model depth. \n\n"}
{"id": "1810.00602", "contents": "Title: Privado: Practical and Secure DNN Inference with Enclaves Abstract: Cloud providers are extending support for trusted hardware primitives such as\nIntel SGX. Simultaneously, the field of deep learning is seeing enormous\ninnovation as well as an increase in adoption. In this paper, we ask a timely\nquestion: \"Can third-party cloud services use Intel SGX enclaves to provide\npractical, yet secure DNN Inference-as-a-service?\" We first demonstrate that\nDNN models executing inside enclaves are vulnerable to access pattern based\nattacks. We show that by simply observing access patterns, an attacker can\nclassify encrypted inputs with 97% and 71% attack accuracy for MNIST and\nCIFAR10 datasets on models trained to achieve 99% and 79% original accuracy\nrespectively. This motivates the need for PRIVADO, a system we have designed\nfor secure, easy-to-use, and performance efficient inference-as-a-service.\nPRIVADO is input-oblivious: it transforms any deep learning framework that is\nwritten in C/C++ to be free of input-dependent access patterns thus eliminating\nthe leakage. PRIVADO is fully-automated and has a low TCB: with zero developer\neffort, given an ONNX description of a model, it generates compact and\nenclave-compatible code which can be deployed on an SGX cloud platform. PRIVADO\nincurs low performance overhead: we use PRIVADO with Torch framework and show\nits overhead to be 17.18% on average on 11 different contemporary neural\nnetworks. \n\n"}
{"id": "1810.01069", "contents": "Title: Cloud Chaser: Real Time Deep Learning Computer Vision on Low Computing\n  Power Devices Abstract: Internet of Things(IoT) devices, mobile phones, and robotic systems are often\ndenied the power of deep learning algorithms due to their limited computing\npower. However, to provide time-critical services such as emergency response,\nhome assistance, surveillance, etc, these devices often need real-time analysis\nof their camera data. This paper strives to offer a viable approach to\nintegrate high-performance deep learning-based computer vision algorithms with\nlow-resource and low-power devices by leveraging the computing power of the\ncloud. By offloading the computation work to the cloud, no dedicated hardware\nis needed to enable deep neural networks on existing low computing power\ndevices. A Raspberry Pi based robot, Cloud Chaser, is built to demonstrate the\npower of using cloud computing to perform real-time vision tasks. Furthermore,\nto reduce latency and improve real-time performance, compression algorithms are\nproposed and evaluated for streaming real-time video frames to the cloud. \n\n"}
{"id": "1810.01365", "contents": "Title: On Self Modulation for Generative Adversarial Networks Abstract: Training Generative Adversarial Networks (GANs) is notoriously challenging.\nWe propose and study an architectural modification, self-modulation, which\nimproves GAN performance across different data sets, architectures, losses,\nregularizers, and hyperparameter settings. Intuitively, self-modulation allows\nthe intermediate feature maps of a generator to change as a function of the\ninput noise vector. While reminiscent of other conditioning techniques, it\nrequires no labeled data. In a large-scale empirical study we observe a\nrelative decrease of $5\\%-35\\%$ in FID. Furthermore, all else being equal,\nadding this modification to the generator leads to improved performance in\n$124/144$ ($86\\%$) of the studied settings. Self-modulation is a simple\narchitectural change that requires no additional parameter tuning, which\nsuggests that it can be applied readily to any GAN. \n\n"}
{"id": "1810.01621", "contents": "Title: Extreme Augmentation : Can deep learning based medical image\n  segmentation be trained using a single manually delineated scan? Abstract: Yes, it can. Data augmentation is perhaps the oldest preprocessing step in\ncomputer vision literature. Almost every computer vision model trained on\nimaging data uses some form of augmentation. In this paper, we use the\ninter-vertebral disk segmentation task alongside a deep residual U-Net as the\nlearning model, to explore the effectiveness of augmentation. In the extreme,\nwe observed that a model trained on patches extracted from just one scan, with\neach patch augmented 50 times; achieved a Dice score of 0.73 in a validation\nset of 40 cases. Qualitative evaluation indicated a clinically usable\nsegmentation algorithm, which appropriately segments regions of interest,\nalongside limited false positive specks. When the initial patches are extracted\nfrom nine scans the average Dice coefficient jumps to 0.86 and most of the\nfalse positives disappear. While this still falls short of state-of-the-art\ndeep learning based segmentation of discs reported in literature, qualitative\nexamination reveals that it does yield segmentation, which can be amended by\nexpert clinicians with minimal effort to generate additional data for training\nimproved deep models. Extreme augmentation of training data, should thus be\nconstrued as a strategy for training deep learning based algorithms, when very\nlittle manually annotated data is available to work with. Models trained with\nextreme augmentation can then be used to accelerate the generation of manually\nlabelled data. Hence, we show that extreme augmentation can be a valuable tool\nin addressing scaling up small imaging data sets to address medical image\nsegmentation tasks. \n\n"}
{"id": "1810.01829", "contents": "Title: Weighted Sigmoid Gate Unit for an Activation Function of Deep Neural\n  Network Abstract: An activation function has crucial role in a deep neural network.\n  A simple rectified linear unit (ReLU) are widely used for the activation\nfunction.\n  In this paper, a weighted sigmoid gate unit (WiG) is proposed as the\nactivation function.\n  The proposed WiG consists of a multiplication of inputs and the weighted\nsigmoid gate.\n  It is shown that the WiG includes the ReLU and same activation functions as a\nspecial case.\n  Many activation functions have been proposed to overcome the performance of\nthe ReLU.\n  In the literature, the performance is mainly evaluated with an object\nrecognition task.\n  The proposed WiG is evaluated with the object recognition task and the image\nrestoration task.\n  Then, the expeirmental comparisons demonstrate the proposed WiG overcomes the\nexisting activation functions including the ReLU. \n\n"}
{"id": "1810.02126", "contents": "Title: Learning Finer-class Networks for Universal Representations Abstract: Many real-world visual recognition use-cases can not directly benefit from\nstate-of-the-art CNN-based approaches because of the lack of many annotated\ndata. The usual approach to deal with this is to transfer a representation\npre-learned on a large annotated source-task onto a target-task of interest.\nThis raises the question of how well the original representation is\n\"universal\", that is to say directly adapted to many different target-tasks. To\nimprove such universality, the state-of-the-art consists in training networks\non a diversified source problem, that is modified either by adding generic or\nspecific categories to the initial set of categories. In this vein, we proposed\na method that exploits finer-classes than the most specific ones existing, for\nwhich no annotation is available. We rely on unsupervised learning and a\nbottom-up split and merge strategy. We show that our method learns more\nuniversal representations than state-of-the-art, leading to significantly\nbetter results on 10 target-tasks from multiple domains, using several network\narchitectures, either alone or combined with networks learned at a coarser\nsemantic level. \n\n"}
{"id": "1810.02338", "contents": "Title: Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language\n  Understanding Abstract: We marry two powerful ideas: deep representation learning for visual\nrecognition and language understanding, and symbolic program execution for\nreasoning. Our neural-symbolic visual question answering (NS-VQA) system first\nrecovers a structural scene representation from the image and a program trace\nfrom the question. It then executes the program on the scene representation to\nobtain an answer. Incorporating symbolic structure as prior knowledge offers\nthree unique advantages. First, executing programs on a symbolic space is more\nrobust to long program traces; our model can solve complex reasoning tasks\nbetter, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model\nis more data- and memory-efficient: it performs well after learning on a small\nnumber of training data; it can also encode an image into a compact\nrepresentation, requiring less storage than existing methods for offline\nquestion answering. Third, symbolic program execution offers full transparency\nto the reasoning process; we are thus able to interpret and diagnose each\nexecution step. \n\n"}
{"id": "1810.03307", "contents": "Title: Local Explanation Methods for Deep Neural Networks Lack Sensitivity to\n  Parameter Values Abstract: Explaining the output of a complicated machine learning model like a deep\nneural network (DNN) is a central challenge in machine learning. Several\nproposed local explanation methods address this issue by identifying what\ndimensions of a single input are most responsible for a DNN's output. The goal\nof this work is to assess the sensitivity of local explanations to DNN\nparameter values. Somewhat surprisingly, we find that DNNs with\nrandomly-initialized weights produce explanations that are both visually and\nquantitatively similar to those produced by DNNs with learned weights. Our\nconjecture is that this phenomenon occurs because these explanations are\ndominated by the lower level features of a DNN, and that a DNN's architecture\nprovides a strong prior which significantly affects the representations learned\nat these lower layers. NOTE: This work is now subsumed by our recent\nmanuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we\nexpand on findings and address concerns raised in Sundararajan et. al. (2018). \n\n"}
{"id": "1810.03654", "contents": "Title: Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo\n  Videos Abstract: Learning depth and optical flow via deep neural networks by watching videos\nhas made significant progress recently. In this paper, we jointly solve the two\ntasks by exploiting the underlying geometric rules within stereo videos.\nSpecifically, given two consecutive stereo image pairs from a video, we first\nestimate depth, camera ego-motion and optical flow from three neural networks.\nThen the whole scene is decomposed into moving foreground and static background\nby compar- ing the estimated optical flow and rigid flow derived from the depth\nand ego-motion. We propose a novel consistency loss to let the optical flow\nlearn from the more accurate rigid flow in static regions. We also design a\nrigid alignment module which helps refine ego-motion estimation by using the\nestimated depth and optical flow. Experiments on the KITTI dataset show that\nour results significantly outperform other state-of- the-art algorithms. Source\ncodes can be found at https: //github.com/baidu-research/UnDepthflow \n\n"}
{"id": "1810.03744", "contents": "Title: Neural Networks Models for Analyzing Magic: the Gathering Cards Abstract: Historically, games of all kinds have often been the subject of study in\nscientific works of Computer Science, including the field of machine learning.\nBy using machine learning techniques and applying them to a game with defined\nrules or a structured dataset, it's possible to learn and improve on the\nalready existing techniques and methods to tackle new challenges and solve\nproblems that are out of the ordinary. The already existing work on card games\ntends to focus on gameplay and card mechanics. This work aims to apply neural\nnetworks models, including Convolutional Neural Networks and Recurrent Neural\nNetworks, in order to analyze Magic: the Gathering cards, both in terms of card\ntext and illustrations; the card images and texts are used to train the\nnetworks in order to be able to classify them into multiple categories. The\nultimate goal was to develop a methodology that could generate card text\nmatching it to an input image, which was attained by relating the prediction\nvalues of the images and generated text across the different categories. \n\n"}
{"id": "1810.04017", "contents": "Title: Comparison of U-net-based Convolutional Neural Networks for Liver\n  Segmentation in CT Abstract: Various approaches for liver segmentation in CT have been proposed: Besides\nstatistical shape models, which played a major role in this research area,\nnovel approaches on the basis of convolutional neural networks have been\nintroduced recently. Using a set of 219 liver CT datasets with reference\nsegmentations from liver surgery planning, we evaluate the performance of\nseveral neural network classifiers based on 2D and 3D U-net architectures. An\ninteresting observation is that slice-wise approaches perform surprisingly\nwell, with mean and median Dice coefficients above 0.97, and may be preferable\nover 3D approaches given current hardware and software limitations. \n\n"}
{"id": "1810.04028", "contents": "Title: Hartley Spectral Pooling for Deep Learning Abstract: In most convolution neural networks (CNNs), downsampling hidden layers is\nadopted for increasing computation efficiency and the receptive field size.\nSuch operation is commonly so-called pooling. Maximation and averaging over\nsliding windows (max/average pooling), and plain downsampling in the form of\nstrided convolution are popular pooling methods. Since the pooling is a lossy\nprocedure, a motivation of our work is to design a new pooling approach for\nless lossy in the dimensionality reduction. Inspired by the Fourier spectral\npooling(FSP) proposed by Rippel et. al. [1], we present the Hartley transform\nbased spectral pooling method in CNNs. Compared with FSP, the proposed spectral\npooling avoids the use of complex arithmetic for frequency representation and\nreduces the computation. Spectral pooling preserves more structure features for\nnetwork's discriminability than max and average pooling. We empirically show\nthat Hartley spectral pooling gives rise to the convergence of training CNNs on\nMNIST and CIFAR-10 datasets. \n\n"}
{"id": "1810.04041", "contents": "Title: On Learning and Learned Data Representation by Capsule Networks Abstract: In this work, we investigate the following: 1) how the routing affects the\nCapsNet model fitting; 2) how the representation using capsules helps discover\nglobal structures in data distribution, and; 3) how the learned data\nrepresentation adapts and generalizes to new tasks. Our investigation yielded\nthe results some of which have been mentioned in the original paper of CapsNet,\nthey are: 1) the routing operation determines the certainty with which a layer\nof capsules pass information to the layer above and the appropriate level of\ncertainty is related to the model fitness; 2) in a designed experiment using\ndata with a known 2D structure, capsule representations enable a more\nmeaningful 2D manifold embedding than neurons do in a standard convolutional\nneural network (CNN), and; 3) compared with neurons of the standard CNN,\ncapsules of successive layers are less coupled and more adaptive to new data\ndistribution. \n\n"}
{"id": "1810.04261", "contents": "Title: A Tale of Three Probabilistic Families: Discriminative, Descriptive and\n  Generative Models Abstract: The pattern theory of Grenander is a mathematical framework where patterns\nare represented by probability models on random variables of algebraic\nstructures. In this paper, we review three families of probability models,\nnamely, the discriminative models, the descriptive models, and the generative\nmodels. A discriminative model is in the form of a classifier. It specifies the\nconditional probability of the class label given the input signal. A\ndescriptive model specifies the probability distribution of the signal, based\non an energy function defined on the signal. A generative model assumes that\nthe signal is generated by some latent variables via a transformation. We shall\nreview these models within a common framework and explore their connections. We\nshall also review the recent developments that take advantage of the high\napproximation capacities of deep neural networks. \n\n"}
{"id": "1810.04511", "contents": "Title: Interpretable Spatio-temporal Attention for Video Action Recognition Abstract: Inspired by the observation that humans are able to process videos\nefficiently by only paying attention where and when it is needed, we propose an\ninterpretable and easy plug-in spatial-temporal attention mechanism for video\naction recognition. For spatial attention, we learn a saliency mask to allow\nthe model to focus on the most salient parts of the feature maps. For temporal\nattention, we employ a convolutional LSTM based attention mechanism to identify\nthe most relevant frames from an input video. Further, we propose a set of\nregularizers to ensure that our attention mechanism attends to coherent regions\nin space and time. Our model not only improves video action recognition\naccuracy, but also localizes discriminative regions both spatially and\ntemporally, despite being trained in a weakly-supervised manner with only\nclassification labels (no bounding box labels or time frame temporal labels).\nWe evaluate our approach on several public video action recognition datasets\nwith ablation studies. Furthermore, we quantitatively and qualitatively\nevaluate our model's ability to localize discriminative regions spatially and\ncritical frames temporally. Experimental results demonstrate the efficacy of\nour approach, showing superior or comparable accuracy with the state-of-the-art\nmethods while increasing model interpretability. \n\n"}
{"id": "1810.05401", "contents": "Title: A Gentle Introduction to Deep Learning in Medical Image Processing Abstract: This paper tries to give a gentle introduction to deep learning in medical\nimage processing, proceeding from theoretical foundations to applications. We\nfirst discuss general reasons for the popularity of deep learning, including\nseveral major breakthroughs in computer science. Next, we start reviewing the\nfundamental basics of the perceptron and neural networks, along with some\nfundamental theory that is often omitted. Doing so allows us to understand the\nreasons for the rise of deep learning in many application domains. Obviously\nmedical image processing is one of these areas which has been largely affected\nby this rapid progress, in particular in image detection and recognition, image\nsegmentation, image registration, and computer-aided diagnosis. There are also\nrecent trends in physical simulation, modelling, and reconstruction that have\nled to astonishing results. Yet, some of these approaches neglect prior\nknowledge and hence bear the risk of producing implausible results. These\napparent weaknesses highlight current limitations of deep learning. However, we\nalso briefly discuss promising approaches that might be able to resolve these\nproblems in the future. \n\n"}
{"id": "1810.05874", "contents": "Title: Embedded deep learning in ophthalmology: Making ophthalmic imaging\n  smarter Abstract: Deep learning has recently gained high interest in ophthalmology, due to its\nability to detect clinically significant features for diagnosis and prognosis.\nDespite these significant advances, little is known about the ability of\nvarious deep learning systems to be embedded within ophthalmic imaging devices,\nallowing automated image acquisition. In this work, we will review the existing\nand future directions for \"active acquisition\" embedded deep learning, leading\nto as high quality images with little intervention by the human operator. In\nclinical practice, the improved image quality should translate into more robust\ndeep learning-based clinical diagnostics. Embedded deep learning will be\nenabled by the constantly improving hardware performance with low cost. We will\nbriefly review possible computation methods in larger clinical systems.\nBriefly, they can be included in a three-layer framework composed of edge, fog\nand cloud layers, the former being performed at a device-level. Improved edge\nlayer performance via \"active acquisition\" serves as an automatic data curation\noperator translating to better quality data in electronic health records\n(EHRs), as well as on the cloud layer, for improved deep learning-based\nclinical data mining. \n\n"}
{"id": "1810.05989", "contents": "Title: Lung Structures Enhancement in Chest Radiographs via CT based FCNN\n  Training Abstract: The abundance of overlapping anatomical structures appearing in chest\nradiographs can reduce the performance of lung pathology detection by automated\nalgorithms (CAD) as well as the human reader. In this paper, we present a deep\nlearning based image processing technique for enhancing the contrast of soft\nlung structures in chest radiographs using Fully Convolutional Neural Networks\n(FCNN). Two 2D FCNN architectures were trained to accomplish the task: The\nfirst performs 2D lung segmentation which is used for normalization of the lung\narea. The second FCNN is trained to extract lung structures. To create the\ntraining images, we employed Simulated X-Ray or Digitally Reconstructed\nRadiographs (DRR) derived from 516 scans belonging to the LIDC-IDRI dataset. By\nfirst segmenting the lungs in the CT domain, we are able to create a dataset of\n2D lung masks to be used for training the segmentation FCNN. For training the\nextraction FCNN, we create DRR images of only voxels belonging to the 3D lung\nsegmentation which we call \"Lung X-ray\" and use them as target images. Once the\nlung structures are extracted, the original image can be enhanced by fusing the\noriginal input x-ray and the synthesized \"Lung X-ray\". We show that our\nenhancement technique is applicable to real x-ray data, and display our results\non the recently released NIH Chest X-Ray-14 dataset. We see promising results\nwhen training a DenseNet-121 based architecture to work directly on the lung\nenhanced X-ray images. \n\n"}
{"id": "1810.06231", "contents": "Title: A Context-aware Capsule Network for Multi-label Classification Abstract: Recently proposed Capsule Network is a brain inspired architecture that\nbrings a new paradigm to deep learning by modelling input domain variations\nthrough vector based representations. Despite being a seminal contribution,\nCapsNet does not explicitly model structured relationships between the detected\nentities and among the capsule features for related inputs. Motivated by the\nworking of cortical network in human visual system, we seek to resolve CapsNet\nlimitations by proposing several intuitive modifications to the CapsNet\narchitecture. We introduce, (1) a novel routing weight initialization\ntechnique, (2) an improved CapsNet design that exploits semantic relationships\nbetween the primary capsule activations using a densely connected Conditional\nRandom Field and (3) a Cholesky transformation based correlation module to\nlearn a general priority scheme. Our proposed design allows CapsNet to scale\nbetter to more complex problems, such as the multi-label classification task,\nwhere semantically related categories co-exist with various interdependencies.\nWe present theoretical bases for our extensions and demonstrate significant\nimprovements on ADE20K scene dataset. \n\n"}
{"id": "1810.06951", "contents": "Title: Deep Metric Learning with Hierarchical Triplet Loss Abstract: We present a novel hierarchical triplet loss (HTL) capable of automatically\ncollecting informative training samples (triplets) via a defined hierarchical\ntree that encodes global context information. This allows us to cope with the\nmain limitation of random sampling in training a conventional triplet loss,\nwhich is a central issue for deep metric learning. Our main contributions are\ntwo-fold. (i) we construct a hierarchical class-level tree where neighboring\nclasses are merged recursively. The hierarchical structure naturally captures\nthe intrinsic data distribution over the whole database. (ii) we formulate the\nproblem of triplet collection by introducing a new violate margin, which is\ncomputed dynamically based on the designed hierarchical tree. This allows it to\nautomatically select meaningful hard samples with the guide of global context.\nIt encourages the model to learn more discriminative features from visual\nsimilar classes, leading to faster convergence and better performance. Our\nmethod is evaluated on the tasks of image retrieval and face recognition, where\nit outperforms the standard triplet loss substantially by 1%-18%. It achieves\nnew state-of-the-art performance on a number of benchmarks, with much fewer\nlearning iterations. \n\n"}
{"id": "1810.07599", "contents": "Title: Orthogonal Deep Features Decomposition for Age-Invariant Face\n  Recognition Abstract: As facial appearance is subject to significant intra-class variations caused\nby the aging process over time, age-invariant face recognition (AIFR) remains a\nmajor challenge in face recognition community. To reduce the intra-class\ndiscrepancy caused by the aging, in this paper we propose a novel approach\n(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep\nface features. Specifically, we decompose deep face features into two\northogonal components to represent age-related and identity-related features.\nAs a result, identity-related features that are robust to aging are then used\nfor AIFR. Besides, for complementing the existing cross-age datasets and\nadvancing the research in this field, we construct a brand-new large-scale\nCross-Age Face dataset (CAF). Extensive experiments conducted on the three\npublic domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have\nshown the effectiveness of the proposed approach and the value of the\nconstructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most\npopular general face recognition (GFR) dataset LFW additionally demonstrates\nthe comparable generalization performance on GFR. \n\n"}
{"id": "1810.07911", "contents": "Title: Domain Adaptation for Semantic Segmentation via Class-Balanced\n  Self-Training Abstract: Recent deep networks achieved state of the art performance on a variety of\nsemantic segmentation tasks. Despite such progress, these models often face\nchallenges in real world `wild tasks' where large difference between labeled\ntraining/source data and unseen test/target data exists. In particular, such\ndifference is often referred to as `domain gap', and could cause significantly\ndecreased performance which cannot be easily remedied by further increasing the\nrepresentation power. Unsupervised domain adaptation (UDA) seeks to overcome\nsuch problem without target domain labels. In this paper, we propose a novel\nUDA framework based on an iterative self-training procedure, where the problem\nis formulated as latent variable loss minimization, and can be solved by\nalternatively generating pseudo labels on target data and re-training the model\nwith these labels. On top of self-training, we also propose a novel\nclass-balanced self-training framework to avoid the gradual dominance of large\nclasses on pseudo-label generation, and introduce spatial priors to refine\ngenerated labels. Comprehensive experiments show that the proposed methods\nachieve state of the art semantic segmentation performance under multiple major\nUDA settings. \n\n"}
{"id": "1810.07961", "contents": "Title: LeukoNet: DCT-based CNN architecture for the classification of normal\n  versus Leukemic blasts in B-ALL Cancer Abstract: Acute lymphoblastic leukemia (ALL) constitutes approximately 25% of the\npediatric cancers. In general, the task of identifying immature leukemic blasts\nfrom normal cells under the microscope is challenging because morphologically\nthe images of the two cells appear similar. In this paper, we propose a deep\nlearning framework for classifying immature leukemic blasts and normal cells.\nThe proposed model combines the Discrete Cosine Transform (DCT) domain features\nextracted via CNN with the Optical Density (OD) space features to build a\nrobust classifier. Elaborate experiments have been conducted to validate the\nproposed LeukoNet classifier. \n\n"}
{"id": "1810.08126", "contents": "Title: KTAN: Knowledge Transfer Adversarial Network Abstract: To reduce the large computation and storage cost of a deep convolutional\nneural network, the knowledge distillation based methods have pioneered to\ntransfer the generalization ability of a large (teacher) deep network to a\nlight-weight (student) network. However, these methods mostly focus on\ntransferring the probability distribution of the softmax layer in a teacher\nnetwork and thus neglect the intermediate representations. In this paper, we\npropose a knowledge transfer adversarial network to better train a student\nnetwork. Our technique holistically considers both intermediate representations\nand probability distributions of a teacher network. To transfer the knowledge\nof intermediate representations, we set high-level teacher feature maps as a\ntarget, toward which the student feature maps are trained. Specifically, we\narrange a Teacher-to-Student layer for enabling our framework suitable for\nvarious student structures. The intermediate representation helps the student\nnetwork better understand the transferred generalization as compared to the\nprobability distribution only. Furthermore, we infuse an adversarial learning\nprocess by employing a discriminator network, which can fully exploit the\nspatial correlation of feature maps in training a student network. The\nexperimental results demonstrate that the proposed method can significantly\nimprove the performance of a student network on both image classification and\nobject detection tasks. \n\n"}
{"id": "1810.08326", "contents": "Title: Domain-Invariant Projection Learning for Zero-Shot Recognition Abstract: Zero-shot learning (ZSL) aims to recognize unseen object classes without any\ntraining samples, which can be regarded as a form of transfer learning from\nseen classes to unseen ones. This is made possible by learning a projection\nbetween a feature space and a semantic space (e.g. attribute space). Key to ZSL\nis thus to learn a projection function that is robust against the often large\ndomain gap between the seen and unseen classes. In this paper, we propose a\nnovel ZSL model termed domain-invariant projection learning (DIPL). Our model\nhas two novel components: (1) A domain-invariant feature self-reconstruction\ntask is introduced to the seen/unseen class data, resulting in a simple linear\nformulation that casts ZSL into a min-min optimization problem. Solving the\nproblem is non-trivial, and a novel iterative algorithm is formulated as the\nsolver, with rigorous theoretic algorithm analysis provided. (2) To further\nalign the two domains via the learned projection, shared semantic structure\namong seen and unseen classes is explored via forming superclasses in the\nsemantic space. Extensive experiments show that our model outperforms the\nstate-of-the-art alternatives by significant margins. \n\n"}
{"id": "1810.08437", "contents": "Title: Learning with privileged information via adversarial discriminative\n  modality distillation Abstract: Heterogeneous data modalities can provide complementary cues for several\ntasks, usually leading to more robust algorithms and better performance.\nHowever, while training data can be accurately collected to include a variety\nof sensory modalities, it is often the case that not all of them are available\nin real life (testing) scenarios, where a model has to be deployed. This raises\nthe challenge of how to extract information from multimodal data in the\ntraining stage, in a form that can be exploited at test time, considering\nlimitations such as noisy or missing modalities. This paper presents a new\napproach in this direction for RGB-D vision tasks, developed within the\nadversarial learning and privileged information frameworks. We consider the\npractical case of learning representations from depth and RGB videos, while\nrelying only on RGB data at test time. We propose a new approach to train a\nhallucination network that learns to distill depth information via adversarial\nlearning, resulting in a clean approach without several losses to balance or\nhyperparameters. We report state-of-the-art results on object classification on\nthe NYUD dataset and video action recognition on the largest multimodal dataset\navailable for this task, the NTU RGB+D, as well as on the Northwestern-UCLA. \n\n"}
{"id": "1810.08768", "contents": "Title: MEMC-Net: Motion Estimation and Motion Compensation Driven Neural\n  Network for Video Interpolation and Enhancement Abstract: Motion estimation (ME) and motion compensation (MC) have been widely used for\nclassical video frame interpolation systems over the past decades. Recently, a\nnumber of data-driven frame interpolation methods based on convolutional neural\nnetworks have been proposed. However, existing learning based methods typically\nestimate either flow or compensation kernels, thereby limiting performance on\nboth computational efficiency and interpolation accuracy. In this work, we\npropose a motion estimation and compensation driven neural network for video\nframe interpolation. A novel adaptive warping layer is developed to integrate\nboth optical flow and interpolation kernels to synthesize target frame pixels.\nThis layer is fully differentiable such that both the flow and kernel\nestimation networks can be optimized jointly. The proposed model benefits from\nthe advantages of motion estimation and compensation methods without using\nhand-crafted features. Compared to existing methods, our approach is\ncomputationally efficient and able to generate more visually appealing results.\nFurthermore, the proposed MEMC-Net can be seamlessly adapted to several video\nenhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed method\nperforms favorably against the state-of-the-art video frame interpolation and\nenhancement algorithms on a wide range of datasets. \n\n"}
{"id": "1810.11181", "contents": "Title: Neural Modular Control for Embodied Question Answering Abstract: We present a modular approach for learning policies for navigation over long\nplanning horizons from language input. Our hierarchical policy operates at\nmultiple timescales, where the higher-level master policy proposes subgoals to\nbe executed by specialized sub-policies. Our choice of subgoals is\ncompositional and semantic, i.e. they can be sequentially combined in arbitrary\norderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find\nkitchen', 'find refrigerator', etc.).\n  We use imitation learning to warm-start policies at each level of the\nhierarchy, dramatically increasing sample efficiency, followed by reinforcement\nlearning. Independent reinforcement learning at each level of hierarchy enables\nsub-policies to adapt to consequences of their actions and recover from errors.\nSubsequent joint hierarchical training enables the master policy to adapt to\nthe sub-policies.\n  On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al.,\n2018), requiring navigating diverse realistic indoor environments, our approach\noutperforms prior work by a significant margin, both in terms of navigation and\nquestion answering. \n\n"}
{"id": "1810.11189", "contents": "Title: Fine-grained Video Categorization with Redundancy Reduction Attention Abstract: For fine-grained categorization tasks, videos could serve as a better source\nthan static images as videos have a higher chance of containing discriminative\npatterns. Nevertheless, a video sequence could also contain a lot of redundant\nand irrelevant frames. How to locate critical information of interest is a\nchallenging task. In this paper, we propose a new network structure, known as\nRedundancy Reduction Attention (RRA), which learns to focus on multiple\ndiscriminative patterns by sup- pressing redundant feature channels.\nSpecifically, it firstly summarizes the video by weight-summing all feature\nvectors in the feature maps of selected frames with a spatio-temporal soft\nattention, and then predicts which channels to suppress or to enhance according\nto this summary with a learned non-linear transform. Suppression is achieved by\nmodulating the feature maps and threshing out weak activations. The updated\nfeature maps are then used in the next iteration. Finally, the video is\nclassified based on multiple summaries. The proposed method achieves out-\nstanding performances in multiple video classification datasets. Further- more,\nwe have collected two large-scale video datasets, YouTube-Birds and\nYouTube-Cars, for future researches on fine-grained video categorization. The\ndatasets are available at http://www.cs.umd.edu/~chenzhu/fgvc. \n\n"}
{"id": "1810.11408", "contents": "Title: Anytime Stereo Image Depth Estimation on Mobile Devices Abstract: Many applications of stereo depth estimation in robotics require the\ngeneration of accurate disparity maps in real time under significant\ncomputational constraints. Current state-of-the-art algorithms force a choice\nbetween either generating accurate mappings at a slow pace, or quickly\ngenerating inaccurate ones, and additionally these methods typically require\nfar too many parameters to be usable on power- or memory-constrained devices.\nMotivated by these shortcomings, we propose a novel approach for disparity\nprediction in the anytime setting. In contrast to prior work, our end-to-end\nlearned approach can trade off computation and accuracy at inference time.\nDepth estimation is performed in stages, during which the model can be queried\nat any time to output its current best estimate. Our final model can process\n1242$ \\times $375 resolution images within a range of 10-35 FPS on an NVIDIA\nJetson TX2 module with only marginal increases in error -- using two orders of\nmagnitude fewer parameters than the most competitive baseline. The source code\nis available at https://github.com/mileyan/AnyNet . \n\n"}
{"id": "1810.11780", "contents": "Title: Deep Affinity Network for Multiple Object Tracking Abstract: Multiple Object Tracking (MOT) plays an important role in solving many\nfundamental problems in video analysis in computer vision. Most MOT methods\nemploy two steps: Object Detection and Data Association. The first step detects\nobjects of interest in every frame of a video, and the second establishes\ncorrespondence between the detected objects in different frames to obtain their\ntracks. Object detection has made tremendous progress in the last few years due\nto deep learning. However, data association for tracking still relies on hand\ncrafted constraints such as appearance, motion, spatial proximity, grouping\netc. to compute affinities between the objects in different frames. In this\npaper, we harness the power of deep learning for data association in tracking\nby jointly modelling object appearances and their affinities between different\nframes in an end-to-end fashion. The proposed Deep Affinity Network (DAN)\nlearns compact; yet comprehensive features of pre-detected objects at several\nlevels of abstraction, and performs exhaustive pairing permutations of those\nfeatures in any two frames to infer object affinities. DAN also accounts for\nmultiple objects appearing and disappearing between video frames. We exploit\nthe resulting efficient affinity computations to associate objects in the\ncurrent frame deep into the previous frames for reliable on-line tracking. Our\ntechnique is evaluated on popular multiple object tracking challenges MOT15,\nMOT17 and UA-DETRAC. Comprehensive benchmarking under twelve evaluation metrics\ndemonstrates that our approach is among the best performing techniques on the\nleader board for these challenges. The open source implementation of our work\nis available at https://github.com/shijieS/SST.git. \n\n"}
{"id": "1810.11794", "contents": "Title: Cascaded Pyramid Mining Network for Weakly Supervised Temporal Action\n  Localization Abstract: Weakly supervised temporal action localization, which aims at temporally\nlocating action instances in untrimmed videos using only video-level class\nlabels during training, is an important yet challenging problem in video\nanalysis. Many current methods adopt the \"localization by classification\"\nframework: first do video classification, then locate temporal area\ncontributing to the results most. However, this framework fails to locate the\nentire action instances and gives little consideration to the local context. In\nthis paper, we present a novel architecture called Cascaded Pyramid Mining\nNetwork (CPMN) to address these issues using two effective modules. First, to\ndiscover the entire temporal interval of specific action, we design a two-stage\ncascaded module with proposed Online Adversarial Erasing (OAE) mechanism, where\nnew and complementary regions are mined through feeding the erased feature maps\nof discovered regions back to the system. Second, to exploit hierarchical\ncontextual information in videos and reduce missing detections, we design a\npyramid module which produces a scale-invariant attention map through combining\nthe feature maps from different levels. Final, we aggregate the results of two\nmodules to perform action localization via locating high score areas in\ntemporal Class Activation Sequence (CAS). Extensive experiments conducted on\nTHUMOS14 and ActivityNet-1.3 datasets demonstrate the effectiveness of our\nmethod. \n\n"}
{"id": "1810.12864", "contents": "Title: Image Restoration using Total Variation Regularized Deep Image Prior Abstract: In the past decade, sparsity-driven regularization has led to significant\nimprovements in image reconstruction. Traditional regularizers, such as total\nvariation (TV), rely on analytical models of sparsity. However, increasingly\nthe field is moving towards trainable models, inspired from deep learning. Deep\nimage prior (DIP) is a recent regularization framework that uses a\nconvolutional neural network (CNN) architecture without data-driven training.\nThis paper extends the DIP framework by combining it with the traditional TV\nregularization. We show that the inclusion of TV leads to considerable\nperformance gains when tested on several traditional restoration tasks such as\nimage denoising and deblurring. \n\n"}
{"id": "1810.13125", "contents": "Title: Compact Generalized Non-local Network Abstract: The non-local module is designed for capturing long-range spatio-temporal\ndependencies in images and videos. Although having shown excellent performance,\nit lacks the mechanism to model the interactions between positions across\nchannels, which are of vital importance in recognizing fine-grained objects and\nactions. To address this limitation, we generalize the non-local module and\ntake the correlations between the positions of any two channels into account.\nThis extension utilizes the compact representation for multiple kernel\nfunctions with Taylor expansion that makes the generalized non-local module in\na fast and low-complexity computation flow. Moreover, we implement our\ngeneralized non-local method within channel groups to ease the optimization.\nExperimental results illustrate the clear-cut improvements and practical\napplicability of the generalized non-local module on both fine-grained object\nrecognition and video classification. Code is available at:\nhttps://github.com/KaiyuYue/cgnl-network.pytorch. \n\n"}
{"id": "1810.13373", "contents": "Title: Analyzing biological and artificial neural networks: challenges with\n  opportunities for synergy? Abstract: Deep neural networks (DNNs) transform stimuli across multiple processing\nstages to produce representations that can be used to solve complex tasks, such\nas object recognition in images. However, a full understanding of how they\nachieve this remains elusive. The complexity of biological neural networks\nsubstantially exceeds the complexity of DNNs, making it even more challenging\nto understand the representations that they learn. Thus, both machine learning\nand computational neuroscience are faced with a shared challenge: how can we\nanalyze their representations in order to understand how they solve complex\ntasks?\n  We review how data-analysis concepts and techniques developed by\ncomputational neuroscientists can be useful for analyzing representations in\nDNNs, and in turn, how recently developed techniques for analysis of DNNs can\nbe useful for understanding representations in biological neural networks. We\nexplore opportunities for synergy between the two fields, such as the use of\nDNNs as in-silico model systems for neuroscience, and how this synergy can lead\nto new hypotheses about the operating principles of biological neural networks. \n\n"}
{"id": "1811.00174", "contents": "Title: Pixel Level Data Augmentation for Semantic Image Segmentation using\n  Generative Adversarial Networks Abstract: Semantic segmentation is one of the basic topics in computer vision, it aims\nto assign semantic labels to every pixel of an image. Unbalanced semantic label\ndistribution could have a negative influence on segmentation accuracy. In this\npaper, we investigate using data augmentation approach to balance the semantic\nlabel distribution in order to improve segmentation performance. We propose\nusing generative adversarial networks (GANs) to generate realistic images for\nimproving the performance of semantic segmentation networks. Experimental\nresults show that the proposed method can not only improve segmentation\nperformance on those classes with low accuracy, but also obtain 1.3% to 2.1%\nincrease in average segmentation accuracy. It shows that this augmentation\nmethod can boost accuracy and be easily applicable to any other segmentation\nmodels. \n\n"}
{"id": "1811.01343", "contents": "Title: Underwater Single Image Color Restoration Using Haze-Lines and a New\n  Quantitative Dataset Abstract: Underwater images suffer from color distortion and low contrast, because\nlight is attenuated while it propagates through water. Attenuation under water\nvaries with wavelength, unlike terrestrial images where attenuation is assumed\nto be spectrally uniform. The attenuation depends both on the water body and\nthe 3D structure of the scene, making color restoration difficult.\n  Unlike existing single underwater image enhancement techniques, our method\ntakes into account multiple spectral profiles of different water types. By\nestimating just two additional global parameters: the attenuation ratios of the\nblue-red and blue-green color channels, the problem is reduced to single image\ndehazing, where all color channels have the same attenuation coefficients.\nSince the water type is unknown, we evaluate different parameters out of an\nexisting library of water types. Each type leads to a different restored image\nand the best result is automatically chosen based on color distribution.\n  We collected a dataset of images taken in different locations with varying\nwater properties, showing color charts in the scenes. Moreover, to obtain\nground truth, the 3D structure of the scene was calculated based on stereo\nimaging. This dataset enables a quantitative evaluation of restoration\nalgorithms on natural images and shows the advantage of our method. \n\n"}
{"id": "1811.01474", "contents": "Title: Fast Face Image Synthesis with Minimal Training Abstract: We propose an algorithm to generate realistic face images of both real and\nsynthetic identities (people who do not exist) with different facial yaw, shape\nand resolution.The synthesized images can be used to augment datasets to train\nCNNs or as massive distractor sets for biometric verification experiments\nwithout any privacy concerns. Additionally, law enforcement can make use of\nthis technique to train forensic experts to recognize faces. Our method samples\nface components from a pool of multiple face images of real identities to\ngenerate the synthetic texture. Then, a real 3D head model compatible to the\ngenerated texture is used to render it under different facial yaw\ntransformations. We perform multiple quantitative experiments to assess the\neffectiveness of our synthesis procedure in CNN training and its potential use\nto generate distractor face images. Additionally, we compare our method with\npopular GAN models in terms of visual quality and execution time. \n\n"}
{"id": "1811.01476", "contents": "Title: Dynamic Representations Toward Efficient Inference on Deep Neural\n  Networks by Decision Gates Abstract: While deep neural networks extract rich features from the input data, the\ncurrent trade-off between depth and computational cost makes it difficult to\nadopt deep neural networks for many industrial applications, especially when\ncomputing power is limited. Here, we are inspired by the idea that, while\ndeeper embeddings are needed to discriminate difficult samples (i.e.,\nfine-grained discrimination), a large number of samples can be well\ndiscriminated via much shallower embeddings (i.e., coarse-grained\ndiscrimination). In this study, we introduce the simple yet effective concept\nof decision gates (d-gate), modules trained to decide whether a sample needs to\nbe projected into a deeper embedding or if an early prediction can be made at\nthe d-gate, thus enabling the computation of dynamic representations at\ndifferent depths. The proposed d-gate modules can be integrated with any deep\nneural network and reduces the average computational cost of the deep neural\nnetworks while maintaining modeling accuracy. The proposed d-gate framework is\nexamined via different network architectures and datasets, with experimental\nresults showing that leveraging the proposed d-gate modules led to a ~43%\nspeed-up and 44% FLOPs reduction on ResNet-101 and 55% speed-up and 39% FLOPs\nreduction on DenseNet-201 trained on the CIFAR10 dataset with only ~2% drop in\naccuracy. Furthermore, experiments where d-gate modules are integrated into\nResNet-101 trained on the ImageNet dataset demonstrate that it is possible to\nreduce the computational cost of the network by 1.5 GFLOPs without any drop in\nthe modeling accuracy. \n\n"}
{"id": "1811.02643", "contents": "Title: Distilling Critical Paths in Convolutional Neural Networks Abstract: Neural network compression and acceleration are widely demanded currently due\nto the resource constraints on most deployment targets. In this paper, through\nanalyzing the filter activation, gradients, and visualizing the filters'\nfunctionality in convolutional neural networks, we show that the filters in\nhigher layers learn extremely task-specific features, which are exclusive for\nonly a small subset of the overall tasks, or even a single class. Based on such\nfindings, we reveal the critical paths of information flow for different\nclasses. And by their intrinsic property of exclusiveness, we propose a\ncritical path distillation method, which can effectively customize the\nconvolutional neural networks to small ones with much smaller model size and\nless computation. \n\n"}
{"id": "1811.03305", "contents": "Title: BAR: Bayesian Activity Recognition using variational inference Abstract: Uncertainty estimation in deep neural networks is essential for designing\nreliable and robust AI systems. Applications such as video surveillance for\nidentifying suspicious activities are designed with deep neural networks\n(DNNs), but DNNs do not provide uncertainty estimates. Capturing reliable\nuncertainty estimates in safety and security critical applications will help to\nestablish trust in the AI system. Our contribution is to apply Bayesian deep\nlearning framework to visual activity recognition application and quantify\nmodel uncertainty along with principled confidence. We utilize the stochastic\nvariational inference technique while training the Bayesian DNNs to infer the\napproximate posterior distribution around model parameters and perform Monte\nCarlo sampling on the posterior of model parameters to obtain the predictive\ndistribution. We show that the Bayesian inference applied to DNNs provide\nreliable confidence measures for visual activity recognition task as compared\nto conventional DNNs. We also show that our method improves the visual activity\nrecognition precision-recall AUC by 6.2% compared to non-Bayesian baseline. We\nevaluate our models on Moments-In-Time (MiT) activity recognition dataset by\nselecting a subset of in- and out-of-distribution video samples. \n\n"}
{"id": "1811.03815", "contents": "Title: Neural Stain Normalization and Unsupervised Classification of Cell\n  Nuclei in Histopathological Breast Cancer Images Abstract: In this paper, we develop a complete pipeline for stain normalization,\nsegmentation, and classification of nuclei in hematoxylin and eosin (H&E)\nstained breast cancer histopathology images. In the first step, we use a\nCNN-based stain transfer technique to normalize the staining characteristics of\n(H&E) images. We then train a neural network to segment images of nuclei from\nthe H&E images. Finally, we train an Information Maximizing Generative\nAdversarial Network (InfoGAN) to learn visual representations of different\ntypes of nuclei and classify them in an entirely unsupervised manner. The\nresults show that our proposed CNN stain normalization yields improved visual\nsimilarity and cell segmentation performance compared to the conventional\nSVD-based stain normalization method. In the final step of our pipeline, we\ndemonstrate the ability to perform fully unsupervised clustering of various\nbreast histopathology cell types based on morphological and color attributes.\nIn addition, we quantitatively evaluate our neural network - based techniques\nagainst various quantitative metrics to validate the effectiveness of our\npipeline. \n\n"}
{"id": "1811.05014", "contents": "Title: NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features\n  for Large-scale Video Classification Abstract: This paper introduces a fast and efficient network architecture, NeXtVLAD, to\naggregate frame-level features into a compact feature vector for large-scale\nvideo classification. Briefly speaking, the basic idea is to decompose a\nhigh-dimensional feature into a group of relatively low-dimensional vectors\nwith attention before applying NetVLAD aggregation over time. This NeXtVLAD\napproach turns out to be both effective and parameter efficient in aggregating\ntemporal information. In the 2nd Youtube-8M video understanding challenge, a\nsingle NeXtVLAD model with less than 80M parameters achieves a GAP score of\n0.87846 in private leaderboard. A mixture of 3 NeXtVLAD models results in\n0.88722, which is ranked 3rd over 394 teams. The code is publicly available at\nhttps://github.com/linrongc/youtube-8m. \n\n"}
{"id": "1811.05304", "contents": "Title: Self-Supervised Learning of Depth and Camera Motion from 360{\\deg}\n  Videos Abstract: As 360{\\deg} cameras become prevalent in many autonomous systems (e.g.,\nself-driving cars and drones), efficient 360{\\deg} perception becomes more and\nmore important. We propose a novel self-supervised learning approach for\npredicting the omnidirectional depth and camera motion from a 360{\\deg} video.\nIn particular, starting from the SfMLearner, which is designed for cameras with\nnormal field-of-view, we introduce three key features to process 360{\\deg}\nimages efficiently. Firstly, we convert each image from equirectangular\nprojection to cubic projection in order to avoid image distortion. In each\nnetwork layer, we use Cube Padding (CP), which pads intermediate features from\nadjacent faces, to avoid image boundaries. Secondly, we propose a novel\n\"spherical\" photometric consistency constraint on the whole viewing sphere. In\nthis way, no pixel will be projected outside the image boundary which typically\nhappens in images with normal field-of-view. Finally, rather than naively\nestimating six independent camera motions (i.e., naively applying SfM-Learner\nto each face on a cube), we propose a novel camera pose consistency loss to\nensure the estimated camera motions reaching consensus. To train and evaluate\nour approach, we collect a new PanoSUNCG dataset containing a large amount of\n360{\\deg} videos with groundtruth depth and camera motion. Our approach\nachieves state-of-the-art depth prediction and camera motion estimation on\nPanoSUNCG with faster inference speed comparing to equirectangular. In\nreal-world indoor videos, our approach can also achieve qualitatively\nreasonable depth prediction by acquiring model pre-trained on PanoSUNCG. \n\n"}
{"id": "1811.05967", "contents": "Title: No-Frills Human-Object Interaction Detection: Factorization, Layout\n  Encodings, and Training Techniques Abstract: We show that for human-object interaction detection a relatively simple\nfactorized model with appearance and layout encodings constructed from\npre-trained object detectors outperforms more sophisticated approaches. Our\nmodel includes factors for detection scores, human and object appearance, and\ncoarse (box-pair configuration) and optionally fine-grained layout (human\npose). We also develop training techniques that improve learning efficiency by:\n(1) eliminating a train-inference mismatch; (2) rejecting easy negatives during\nmini-batch training; and (3) using a ratio of negatives to positives that is\ntwo orders of magnitude larger than existing approaches. We conduct a thorough\nablation study to understand the importance of different factors and training\ntechniques using the challenging HICO-Det dataset. \n\n"}
{"id": "1811.06458", "contents": "Title: Psychophysical evaluation of individual low-level feature influences on\n  visual attention Abstract: In this study we provide the analysis of eye movement behavior elicited by\nlow-level feature distinctiveness with a dataset of synthetically-generated\nimage patterns. Design of visual stimuli was inspired by the ones used in\nprevious psychophysical experiments, namely in free-viewing and visual\nsearching tasks, to provide a total of 15 types of stimuli, divided according\nto the task and feature to be analyzed. Our interest is to analyze the\ninfluences of low-level feature contrast between a salient region and the rest\nof distractors, providing fixation localization characteristics and reaction\ntime of landing inside the salient region. Eye-tracking data was collected from\n34 participants during the viewing of a 230 images dataset. Results show that\nsaliency is predominantly and distinctively influenced by: 1. feature type, 2.\nfeature contrast, 3. temporality of fixations, 4. task difficulty and 5. center\nbias. This experimentation proposes a new psychophysical basis for saliency\nmodel evaluation using synthetic images. \n\n"}
{"id": "1811.06488", "contents": "Title: Exploring the Deep Feature Space of a Cell Classification Neural Network Abstract: In this paper, we present contemporary techniques for visualising the feature\nspace of a deep learning image classification neural network. These techniques\nare viewed in the context of a feed-forward network trained to classify low\nresolution fluorescence images of white blood cells captured using optofluidic\nimaging. The model has two output classes corresponding to two different cell\ntypes, which are often difficult to distinguish by eye. This paper has two\nmajor sections. The first looks to develop the information space presented by\ndimension reduction techniques, such as t-SNE, used to embed high-dimensional\npre-softmax layer activations into a two-dimensional plane. The second section\nlooks at feature visualisation by optimisation to generate feature images\nrepresenting the learned features of the network. Using and developing these\ntechniques we visualise class separation and structures within the dataset at\nvarious depths using clustering algorithms and feature images; track the\ndevelopment of feature complexity as we ascend the network; and begin to\nextract the features the network has learnt by modulating single-channel\nfeature images with up-scaled neuron activation maps to distinguish their most\nsalient parts. \n\n"}
{"id": "1811.07246", "contents": "Title: PointConv: Deep Convolutional Networks on 3D Point Clouds Abstract: Unlike images which are represented in regular dense grids, 3D point clouds\nare irregular and unordered, hence applying convolution on them can be\ndifficult. In this paper, we extend the dynamic filter to a new convolution\noperation, named PointConv. PointConv can be applied on point clouds to build\ndeep convolutional networks. We treat convolution kernels as nonlinear\nfunctions of the local coordinates of 3D points comprised of weight and density\nfunctions. With respect to a given point, the weight functions are learned with\nmulti-layer perceptron networks and density functions through kernel density\nestimation. The most important contribution of this work is a novel\nreformulation proposed for efficiently computing the weight functions, which\nallowed us to dramatically scale up the network and significantly improve its\nperformance. The learned convolution kernel can be used to compute\ntranslation-invariant and permutation-invariant convolution on any point set in\nthe 3D space. Besides, PointConv can also be used as deconvolution operators to\npropagate features from a subsampled point cloud back to its original\nresolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep\nconvolutional neural networks built on PointConv are able to achieve\nstate-of-the-art on challenging semantic segmentation benchmarks on 3D point\nclouds. Besides, our experiments converting CIFAR-10 into a point cloud showed\nthat networks built on PointConv can match the performance of convolutional\nnetworks in 2D images of a similar structure. \n\n"}
{"id": "1811.07768", "contents": "Title: Handwriting Recognition of Historical Documents with few labeled data Abstract: Historical documents present many challenges for offline handwriting\nrecognition systems, among them, the segmentation and labeling steps. Carefully\nannotated textlines are needed to train an HTR system. In some scenarios,\ntranscripts are only available at the paragraph level with no text-line\ninformation. In this work, we demonstrate how to train an HTR system with few\nlabeled data. Specifically, we train a deep convolutional recurrent neural\nnetwork (CRNN) system on only 10% of manually labeled text-line data from a\ndataset and propose an incremental training procedure that covers the rest of\nthe data. Performance is further increased by augmenting the training set with\nspecially crafted multiscale data. We also propose a model-based normalization\nscheme which considers the variability in the writing scale at the recognition\nphase. We apply this approach to the publicly available READ dataset. Our\nsystem achieved the second best result during the ICDAR2017 competition. \n\n"}
{"id": "1811.07966", "contents": "Title: Mitigating Architectural Mismatch During the Evolutionary Synthesis of\n  Deep Neural Networks Abstract: Evolutionary deep intelligence has recently shown great promise for producing\nsmall, powerful deep neural network models via the organic synthesis of\nincreasingly efficient architectures over successive generations. Existing\nevolutionary synthesis processes, however, have allowed the mating of parent\nnetworks independent of architectural alignment, resulting in a mismatch of\nnetwork structures. We present a preliminary study into the effects of\narchitectural alignment during evolutionary synthesis using a gene tagging\nsystem. Surprisingly, the network architectures synthesized using the gene\ntagging approach resulted in slower decreases in performance accuracy and\nstorage size; however, the resultant networks were comparable in size and\nperformance accuracy to the non-gene tagging networks. Furthermore, we\nspeculate that there is a noticeable decrease in network variability for\nnetworks synthesized with gene tagging, indicating that enforcing a\nlike-with-like mating policy potentially restricts the exploration of the\nsearch space of possible network architectures. \n\n"}
{"id": "1811.08011", "contents": "Title: Explain to Fix: A Framework to Interpret and Correct DNN Object Detector\n  Predictions Abstract: Explaining predictions of deep neural networks (DNNs) is an important and\nnontrivial task. In this paper, we propose a practical approach to interpret\ndecisions made by a DNN object detector that has fidelity comparable to\nstate-of-the-art methods and sufficient computational efficiency to process\nlarge datasets. Our method relies on recent theory and approximates Shapley\nfeature importance values. We qualitatively and quantitatively show that the\nproposed explanation method can be used to find image features which cause\nfailures in DNN object detection. The developed software tool combined into the\n\"Explain to Fix\" (E2X) framework has a factor of 10 higher computational\nefficiency than prior methods and can be used for cluster processing using\ngraphics processing units (GPUs). Lastly, we propose a potential extension of\nthe E2X framework where the discovered missing features can be added into\ntraining dataset to overcome failures after model retraining. \n\n"}
{"id": "1811.08043", "contents": "Title: Recurrent Iterative Gating Networks for Semantic Segmentation Abstract: In this paper, we present an approach for Recurrent Iterative Gating called\nRIGNet. The core elements of RIGNet involve recurrent connections that control\nthe flow of information in neural networks in a top-down manner, and different\nvariants on the core structure are considered. The iterative nature of this\nmechanism allows for gating to spread in both spatial extent and feature space.\nThis is revealed to be a powerful mechanism with broad compatibility with\ncommon existing networks. Analysis shows how gating interacts with different\nnetwork characteristics, and we also show that more shallow networks with\ngating may be made to perform better than much deeper networks that do not\ninclude RIGNet modules. \n\n"}
{"id": "1811.08412", "contents": "Title: A Baseline for Multi-Label Image Classification Using An Ensemble of\n  Deep Convolutional Neural Networks Abstract: Recent studies on multi-label image classification have focused on designing\nmore complex architectures of deep neural networks such as the use of attention\nmechanisms and region proposal networks. Although performance gains have been\nreported, the backbone deep models of the proposed approaches and the\nevaluation metrics employed in different works vary, making it difficult to\ncompare each fairly. Moreover, due to the lack of properly investigated\nbaselines, the advantage introduced by the proposed techniques are often\nambiguous. To address these issues, we make a thorough investigation of the\nmainstream deep convolutional neural network architectures for multi-label\nimage classification and present a strong baseline. With the use of proper data\naugmentation techniques and model ensembles, the basic deep architectures can\nachieve better performance than many existing more complex ones on three\nbenchmark datasets, providing great insight for the future studies on\nmulti-label image classification. \n\n"}
{"id": "1811.08728", "contents": "Title: AttentionMask: Attentive, Efficient Object Proposal Generation Focusing\n  on Small Objects Abstract: We propose a novel approach for class-agnostic object proposal generation,\nwhich is efficient and especially well-suited to detect small objects.\nEfficiency is achieved by scale-specific objectness attention maps which focus\nthe processing on promising parts of the image and reduce the amount of sampled\nwindows strongly. This leads to a system, which is $33\\%$ faster than the\nstate-of-the-art and clearly outperforming state-of-the-art in terms of average\nrecall. Secondly, we add a module for detecting small objects, which are often\nmissed by recent models. We show that this module improves the average recall\nfor small objects by about $53\\%$. \n\n"}
{"id": "1811.09178", "contents": "Title: Object-oriented Targets for Visual Navigation using Rich Semantic\n  Representations Abstract: When searching for an object humans navigate through a scene using semantic\ninformation and spatial relationships. We look for an object using our\nknowledge of its attributes and relationships with other objects to infer the\nprobable location. In this paper, we propose to tackle the visual navigation\nproblem using rich semantic representations of the observed scene and\nobject-oriented targets to train an agent. We show that both allows the agent\nto generalize to new targets and unseen scene in a short amount of training\ntime. \n\n"}
{"id": "1811.09192", "contents": "Title: Self Paced Adversarial Training for Multimodal Few-shot Learning Abstract: State-of-the-art deep learning algorithms yield remarkable results in many\nvisual recognition tasks. However, they still fail to provide satisfactory\nresults in scarce data regimes. To a certain extent this lack of data can be\ncompensated by multimodal information. Missing information in one modality of a\nsingle data point (e.g. an image) can be made up for in another modality (e.g.\na textual description). Therefore, we design a few-shot learning task that is\nmultimodal during training (i.e. image and text) and single-modal during test\ntime (i.e. image). In this regard, we propose a self-paced class-discriminative\ngenerative adversarial network incorporating multimodality in the context of\nfew-shot learning. The proposed approach builds upon the idea of cross-modal\ndata generation in order to alleviate the data sparsity problem. We improve\nfew-shot learning accuracies on the finegrained CUB and Oxford-102 datasets. \n\n"}
{"id": "1811.09361", "contents": "Title: Pointwise Rotation-Invariant Network with Adaptive Sampling and 3D\n  Spherical Voxel Convolution Abstract: Point cloud analysis without pose priors is very challenging in real\napplications, as the orientations of point clouds are often unknown. In this\npaper, we propose a brand new point-set learning framework PRIN, namely,\nPointwise Rotation-Invariant Network, focusing on rotation-invariant feature\nextraction in point clouds analysis. We construct spherical signals by Density\nAware Adaptive Sampling to deal with distorted point distributions in spherical\nspace. In addition, we propose Spherical Voxel Convolution and Point\nRe-sampling to extract rotation-invariant features for each point. Our network\ncan be applied to tasks ranging from object classification, part segmentation,\nto 3D feature matching and label alignment. We show that, on the dataset with\nrandomly rotated point clouds, PRIN demonstrates better performance than\nstate-of-the-art methods without any data augmentation. We also provide\ntheoretical analysis for the rotation-invariance achieved by our methods. \n\n"}
{"id": "1811.09716", "contents": "Title: Robustness via curvature regularization, and vice versa Abstract: State-of-the-art classifiers have been shown to be largely vulnerable to\nadversarial perturbations. One of the most effective strategies to improve\nrobustness is adversarial training. In this paper, we investigate the effect of\nadversarial training on the geometry of the classification landscape and\ndecision boundaries. We show in particular that adversarial training leads to a\nsignificant decrease in the curvature of the loss surface with respect to\ninputs, leading to a drastically more \"linear\" behaviour of the network. Using\na locally quadratic approximation, we provide theoretical evidence on the\nexistence of a strong relation between large robustness and small curvature. To\nfurther show the importance of reduced curvature for improving the robustness,\nwe propose a new regularizer that directly minimizes curvature of the loss\nsurface, and leads to adversarial robustness that is on par with adversarial\ntraining. Besides being a more efficient and principled alternative to\nadversarial training, the proposed regularizer confirms our claims on the\nimportance of exhibiting quasi-linear behavior in the vicinity of data points\nin order to achieve robustness. \n\n"}
{"id": "1811.09885", "contents": "Title: Forward Stability of ResNet and Its Variants Abstract: The residual neural network (ResNet) is a popular deep network architecture\nwhich has the ability to obtain high-accuracy results on several image\nprocessing problems. In order to analyze the behavior and structure of ResNet,\nrecent work has been on establishing connections between ResNets and\ncontinuous-time optimal control problems. In this work, we show that the\npost-activation ResNet is related to an optimal control problem with\ndifferential inclusions, and provide continuous-time stability results for the\ndifferential inclusion associated with ResNet. Motivated by the stability\nconditions, we show that alterations of either the architecture or the\noptimization problem can generate variants of ResNet which improve the\ntheoretical stability bounds. In addition, we establish stability bounds for\nthe full (discrete) network associated with two variants of ResNet, in\nparticular, bounds on the growth of the features and a measure of the\nsensitivity of the features with respect to perturbations. These results also\nhelp to show the relationship between the depth, regularization, and stability\nof the feature space. Computational experiments on the proposed variants show\nthat the accuracy of ResNet is preserved and that the accuracy seems to be\nmonotone with respect to the depth and various corruptions. \n\n"}
{"id": "1811.10419", "contents": "Title: Multi-Task Generative Adversarial Network for Handling Imbalanced\n  Clinical Data Abstract: We propose a new generative adversarial architecture to mitigate imbalance\ndata problem for the task of medical image semantic segmentation where the\nmajority of pixels belong to a healthy region and few belong to lesion or\nnon-health region. A model trained with imbalanced data tends to bias towards\nhealthy data which is not desired in clinical applications. We design a new\nconditional GAN with two components: a generative model and a discriminative\nmodel to mitigate imbalanced data problem through selective weighted loss.\nWhile the generator is trained on sequential magnetic resonance images (MRI) to\nlearn semantic segmentation and disease classification, the discriminator\nclassifies whether a generated output is real or fake. The proposed\narchitecture achieved state-of-the-art results on ACDC-2017 for cardiac\nsegmentation and diseases classification. We have achieved competitive results\non BraTS-2017 for brain tumor segmentation and brain diseases classification. \n\n"}
{"id": "1811.10779", "contents": "Title: Tackling Early Sparse Gradients in Softmax Activation Using Leaky\n  Squared Euclidean Distance Abstract: Softmax activation is commonly used to output the probability distribution\nover categories based on certain distance metric. In scenarios like one-shot\nlearning, the distance metric is often chosen to be squared Euclidean distance\nbetween the query sample and the category prototype. This practice works well\nin most time. However, we find that choosing squared Euclidean distance may\ncause distance explosion leading gradients to be extremely sparse in the early\nstage of back propagation. We term this phenomena as the early sparse gradients\nproblem. Though it doesn't deteriorate the convergence of the model, it may set\nup a barrier to further model improvement. To tackle this problem, we propose\nto use leaky squared Euclidean distance to impose a restriction on distances.\nIn this way, we can avoid distance explosion and increase the magnitude of\ngradients. Extensive experiments are conducted on Omniglot and miniImageNet\ndatasets. We show that using leaky squared Euclidean distance can improve\none-shot classification accuracy on both datasets. \n\n"}
{"id": "1811.10870", "contents": "Title: Affinity Derivation and Graph Merge for Instance Segmentation Abstract: We present an instance segmentation scheme based on pixel affinity\ninformation, which is the relationship of two pixels belonging to a same\ninstance. In our scheme, we use two neural networks with similar structure. One\nis to predict pixel level semantic score and the other is designed to derive\npixel affinities.\n  Regarding pixels as the vertexes and affinities as edges, we then propose a\nsimple yet effective graph merge algorithm to cluster pixels into instances.\nExperimental results show that our scheme can generate fine-grained instance\nmask.\n  With Cityscapes training data, the proposed scheme achieves 27.3 AP on test\nset. \n\n"}
{"id": "1811.10872", "contents": "Title: Automatic Image Stylization Using Deep Fully Convolutional Networks Abstract: Color and tone stylization strives to enhance unique themes with artistic\ncolor and tone adjustments. It has a broad range of applications from\nprofessional image postprocessing to photo sharing over social networks.\nMainstream photo enhancement softwares provide users with predefined styles,\nwhich are often hand-crafted through a trial-and-error process. Such photo\nadjustment tools lack a semantic understanding of image contents and the\nresulting global color transform limits the range of artistic styles it can\nrepresent. On the other hand, stylistic enhancement needs to apply distinct\nadjustments to various semantic regions. Such an ability enables a broader\nrange of visual styles. In this paper, we propose a novel deep learning\narchitecture for automatic image stylization, which learns local enhancement\nstyles from image pairs. Our deep learning architecture is an end-to-end deep\nfully convolutional network performing semantics-aware feature extraction as\nwell as automatic image adjustment prediction. Image stylization can be\nefficiently accomplished with a single forward pass through our deep network.\nExperiments on existing datasets for image stylization demonstrate the\neffectiveness of our deep learning architecture. \n\n"}
{"id": "1811.11080", "contents": "Title: MobiFace: A Lightweight Deep Learning Face Recognition on Mobile Devices Abstract: Deep neural networks have been widely used in numerous computer vision\napplications, particularly in face recognition. However, deploying deep neural\nnetwork face recognition on mobile devices has recently become a trend but\nstill limited since most high-accuracy deep models are both time and GPU\nconsumption in the inference stage. Therefore, developing a lightweight deep\nneural network is one of the most practical solutions to deploy face\nrecognition on mobile devices. Such the lightweight deep neural network\nrequires efficient memory with small number of weights representation and low\ncost operators. In this paper, a novel deep neural network named MobiFace, a\nsimple but effective approach, is proposed for productively deploying face\nrecognition on mobile devices. The experimental results have shown that our\nlightweight MobiFace is able to achieve high performance with 99.73% on LFW\ndatabase and 91.3% on large-scale challenging Megaface database. It is also\neventually competitive against large-scale deep-networks face recognition while\nsignificant reducing computational time and memory consumption. \n\n"}
{"id": "1811.11155", "contents": "Title: FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained\n  Object Generation and Discovery Abstract: We propose FineGAN, a novel unsupervised GAN framework, which disentangles\nthe background, object shape, and object appearance to hierarchically generate\nimages of fine-grained object categories. To disentangle the factors without\nsupervision, our key idea is to use information theory to associate each factor\nto a latent code, and to condition the relationships between the codes in a\nspecific way to induce the desired hierarchy. Through extensive experiments, we\nshow that FineGAN achieves the desired disentanglement to generate realistic\nand diverse images belonging to fine-grained classes of birds, dogs, and cars.\nUsing FineGAN's automatically learned features, we also cluster real images as\na first attempt at solving the novel problem of unsupervised fine-grained\nobject category discovery. Our code/models/demo can be found at\nhttps://github.com/kkanshul/finegan \n\n"}
{"id": "1811.11165", "contents": "Title: Label-Noise Robust Generative Adversarial Networks Abstract: Generative adversarial networks (GANs) are a framework that learns a\ngenerative distribution through adversarial training. Recently, their\nclass-conditional extensions (e.g., conditional GAN (cGAN) and auxiliary\nclassifier GAN (AC-GAN)) have attracted much attention owing to their ability\nto learn the disentangled representations and to improve the training\nstability. However, their training requires the availability of large-scale\naccurate class-labeled data, which are often laborious or impractical to\ncollect in a real-world scenario. To remedy this, we propose a novel family of\nGANs called label-noise robust GANs (rGANs), which, by incorporating a noise\ntransition model, can learn a clean label conditional generative distribution\neven when training labels are noisy. In particular, we propose two variants:\nrAC-GAN, which is a bridging model between AC-GAN and the label-noise robust\nclassification model, and rcGAN, which is an extension of cGAN and solves this\nproblem with no reliance on any classifier. In addition to providing the\ntheoretical background, we demonstrate the effectiveness of our models through\nextensive experiments using diverse GAN configurations, various noise settings,\nand multiple evaluation metrics (in which we tested 402 conditions in total).\nOur code is available at https://github.com/takuhirok/rGAN/. \n\n"}
{"id": "1811.11205", "contents": "Title: You Look Twice: GaterNet for Dynamic Filter Selection in CNNs Abstract: The concept of conditional computation for deep nets has been proposed\npreviously to improve model performance by selectively using only parts of the\nmodel conditioned on the sample it is processing. In this paper, we investigate\ninput-dependent dynamic filter selection in deep convolutional neural networks\n(CNNs). The problem is interesting because the idea of forcing different parts\nof the model to learn from different types of samples may help us acquire\nbetter filters in CNNs, improve the model generalization performance and\npotentially increase the interpretability of model behavior. We propose a novel\nyet simple framework called GaterNet, which involves a backbone and a gater\nnetwork. The backbone network is a regular CNN that performs the major\ncomputation needed for making a prediction, while a global gater network is\nintroduced to generate binary gates for selectively activating filters in the\nbackbone network based on each input. Extensive experiments on CIFAR and\nImageNet datasets show that our models consistently outperform the original\nmodels with a large margin. On CIFAR-10, our model also improves upon\nstate-of-the-art results. \n\n"}
{"id": "1811.11387", "contents": "Title: Self-Supervised Spatiotemporal Feature Learning via Video Rotation\n  Prediction Abstract: The success of deep neural networks generally requires a vast amount of\ntraining data to be labeled, which is expensive and unfeasible in scale,\nespecially for video collections. To alleviate this problem, in this paper, we\npropose 3DRotNet: a fully self-supervised approach to learn spatiotemporal\nfeatures from unlabeled videos. A set of rotations are applied to all videos,\nand a pretext task is defined as prediction of these rotations. When\naccomplishing this task, 3DRotNet is actually trained to understand the\nsemantic concepts and motions in videos. In other words, it learns a\nspatiotemporal video representation, which can be transferred to improve video\nunderstanding tasks in small datasets. Our extensive experiments successfully\ndemonstrate the effectiveness of the proposed framework on action recognition,\nleading to significant improvements over the state-of-the-art self-supervised\nmethods. With the self-supervised pre-trained 3DRotNet from large datasets, the\nrecognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51\nrespectively, compared to the models trained from scratch. \n\n"}
{"id": "1811.11455", "contents": "Title: CrowdCam: Dynamic Region Segmentation Abstract: We consider the problem of segmenting dynamic regions in CrowdCam images,\nwhere a dynamic region is the projection of a moving 3D object on the image\nplane. Quite often, these regions are the most interesting parts of an image.\nCrowdCam images is a set of images of the same dynamic event, captured by a\ngroup of non-collaborating users. Almost every event of interest today is\ncaptured this way. This new type of images raises the need to develop new\nalgorithms tailored specifically for it. We propose a comprehensive solution to\nthe problem. Our solution combines cues that are based on geometry, appearance\nand proximity. First, geometric reasoning is used to produce rough score maps\nthat determine, for every pixel, how likely it is to be the projection of a\nstatic or dynamic scene point. These maps are noisy because CrowdCam images are\nusually few and far apart both in space and in time. Then, we use similarity in\nappearance space and proximity in the image plane to encourage neighboring\npixels to be labeled similarly as either static or dynamic. We collected a new,\nand challenging, data set to evaluate our algorithm. Results show that the\nsuccess score of our algorithm is nearly double that of the current state of\nthe art approach. \n\n"}
{"id": "1811.11683", "contents": "Title: Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding Abstract: We address the problem of phrase grounding by lear ing a multi-level common\nsemantic space shared by the textual and visual modalities. We exploit multiple\nlevels of feature maps of a Deep Convolutional Neural Network, as well as\ncontextualized word and sentence embeddings extracted from a character-based\nlanguage model. Following dedicated non-linear mappings for visual features at\neach level, word, and sentence embeddings, we obtain multiple instantiations of\nour common semantic space in which comparisons between any target text and the\nvisual content is performed with cosine similarity. We guide the model by a\nmulti-level multimodal attention mechanism which outputs attended visual\nfeatures at each level. The best level is chosen to be compared with text\ncontent for maximizing the pertinence scores of image-sentence pairs of the\nground truth. Experiments conducted on three publicly available datasets show\nsignificant performance gains (20%-60% relative) over the state-of-the-art in\nphrase localization and set a new performance record on those datasets. We\nprovide a detailed ablation study to show the contribution of each element of\nour approach and release our code on GitHub. \n\n"}
{"id": "1811.11968", "contents": "Title: ADCrowdNet: An Attention-injective Deformable Convolutional Network for\n  Crowd Understanding Abstract: We propose an attention-injective deformable convolutional network called\nADCrowdNet for crowd understanding that can address the accuracy degradation\nproblem of highly congested noisy scenes. ADCrowdNet contains two concatenated\nnetworks. An attention-aware network called Attention Map Generator (AMG) first\ndetects crowd regions in images and computes the congestion degree of these\nregions. Based on detected crowd regions and congestion priors, a multi-scale\ndeformable network called Density Map Estimator (DME) then generates\nhigh-quality density maps. With the attention-aware training scheme and\nmulti-scale deformable convolutional scheme, the proposed ADCrowdNet achieves\nthe capability of being more effective to capture the crowd features and more\nresistant to various noises. We have evaluated our method on four popular crowd\ncounting datasets (ShanghaiTech, UCF_CC_50, WorldEXPO'10, and UCSD) and an\nextra vehicle counting dataset TRANCOS, and our approach beats existing\nstate-of-the-art approaches on all of these datasets. \n\n"}
{"id": "1811.12016", "contents": "Title: 3D Shape Reconstruction from a Single 2D Image via 2D-3D\n  Self-Consistency Abstract: Aiming at inferring 3D shapes from 2D images, 3D shape reconstruction has\ndrawn huge attention from researchers in computer vision and deep learning\ncommunities. However, it is not practical to assume that 2D input images and\ntheir associated ground truth 3D shapes are always available during training.\nIn this paper, we propose a framework for semi-supervised 3D reconstruction.\nThis is realized by our introduced 2D-3D self-consistency, which aligns the\npredicted 3D models and the projected 2D foreground segmentation masks.\nMoreover, our model not only enables recovering 3D shapes with the\ncorresponding 2D masks, camera pose information can be jointly disentangled and\npredicted, even such supervision is never available during training. In the\nexperiments, we qualitatively and quantitatively demonstrate the effectiveness\nof our model, which performs favorably against state-of-the-art approaches in\neither supervised or semi-supervised settings. \n\n"}
{"id": "1811.12019", "contents": "Title: Large-Scale Distributed Second-Order Optimization Using\n  Kronecker-Factored Approximate Curvature for Deep Convolutional Neural\n  Networks Abstract: Large-scale distributed training of deep neural networks suffer from the\ngeneralization gap caused by the increase in the effective mini-batch size.\nPrevious approaches try to solve this problem by varying the learning rate and\nbatch size over epochs and layers, or some ad hoc modification of the batch\nnormalization. We propose an alternative approach using a second-order\noptimization method that shows similar generalization capability to first-order\nmethods, but converges faster and can handle larger mini-batches. To test our\nmethod on a benchmark where highly optimized first-order methods are available\nas references, we train ResNet-50 on ImageNet. We converged to 75% Top-1\nvalidation accuracy in 35 epochs for mini-batch sizes under 16,384, and\nachieved 75% even with a mini-batch size of 131,072, which took only 978\niterations. \n\n"}
{"id": "1811.12432", "contents": "Title: AdaFrame: Adaptive Frame Selection for Fast Video Recognition Abstract: We present AdaFrame, a framework that adaptively selects relevant frames on a\nper-input basis for fast video recognition. AdaFrame contains a Long Short-Term\nMemory network augmented with a global memory that provides context information\nfor searching which frames to use over time. Trained with policy gradient\nmethods, AdaFrame generates a prediction, determines which frame to observe\nnext, and computes the utility, i.e., expected future rewards, of seeing more\nframes at each time step. At testing time, AdaFrame exploits predicted\nutilities to achieve adaptive lookahead inference such that the overall\ncomputational costs are reduced without incurring a decrease in accuracy.\nExtensive experiments are conducted on two large-scale video benchmarks, FCVID\nand ActivityNet. AdaFrame matches the performance of using all frames with only\n8.21 and 8.65 frames on FCVID and ActivityNet, respectively. We further\nqualitatively demonstrate learned frame usage can indicate the difficulty of\nmaking classification decisions; easier samples need fewer frames while harder\nones require more, both at instance-level within the same class and at\nclass-level among different categories. \n\n"}
{"id": "1811.12596", "contents": "Title: Parsing R-CNN for Instance-Level Human Analysis Abstract: Instance-level human analysis is common in real-life scenarios and has\nmultiple manifestations, such as human part segmentation, dense pose\nestimation, human-object interactions, etc. Models need to distinguish\ndifferent human instances in the image panel and learn rich features to\nrepresent the details of each instance. In this paper, we present an end-to-end\npipeline for solving the instance-level human analysis, named Parsing R-CNN. It\nprocesses a set of human instances simultaneously through comprehensive\nconsidering the characteristics of region-based approach and the appearance of\na human, thus allowing representing the details of instances. Parsing R-CNN is\nvery flexible and efficient, which is applicable to many issues in human\ninstance analysis. Our approach outperforms all state-of-the-art methods on\nCIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and\nDensePose-COCO datasets. Based on the proposed Parsing R-CNN, we reach the 1st\nplace in the COCO 2018 Challenge DensePose Estimation task. Code and models are\npublic available. \n\n"}
{"id": "1812.00101", "contents": "Title: DVC: An End-to-end Deep Video Compression Framework Abstract: Conventional video compression approaches use the predictive coding\narchitecture and encode the corresponding motion information and residual\ninformation. In this paper, taking advantage of both classical architecture in\nthe conventional video compression method and the powerful non-linear\nrepresentation ability of neural networks, we propose the first end-to-end\nvideo compression deep model that jointly optimizes all the components for\nvideo compression. Specifically, learning based optical flow estimation is\nutilized to obtain the motion information and reconstruct the current frames.\nThen we employ two auto-encoder style neural networks to compress the\ncorresponding motion and residual information. All the modules are jointly\nlearned through a single loss function, in which they collaborate with each\nother by considering the trade-off between reducing the number of compression\nbits and improving quality of the decoded video. Experimental results show that\nthe proposed approach can outperform the widely used video coding standard\nH.264 in terms of PSNR and be even on par with the latest standard H.265 in\nterms of MS-SSIM. Code is released at https://github.com/GuoLusjtu/DVC. \n\n"}
{"id": "1812.00312", "contents": "Title: ECO: Egocentric Cognitive Mapping Abstract: We present a new method to localize a camera within a previously unseen\nenvironment perceived from an egocentric point of view. Although this is, in\ngeneral, an ill-posed problem, humans can effortlessly and efficiently\ndetermine their relative location and orientation and navigate into a\npreviously unseen environments, e.g., finding a specific item in a new grocery\nstore. To enable such a capability, we design a new egocentric representation,\nwhich we call ECO (Egocentric COgnitive map). ECO is biologically inspired, by\nthe cognitive map that allows human navigation, and it encodes the surrounding\nvisual semantics with respect to both distance and orientation. ECO possesses\nthree main properties: (1) reconfigurability: complex semantics and geometry is\ncaptured via the synthesis of atomic visual representations (e.g., image\npatch); (2) robustness: the visual semantics are registered in a geometrically\nconsistent way (e.g., aligning with respect to the gravity vector,\nfrontalizing, and rescaling to canonical depth), thus enabling us to learn\nmeaningful atomic representations; (3) adaptability: a domain adaptation\nframework is designed to generalize the learned representation without manual\ncalibration. As a proof-of-concept, we use ECO to localize a camera within\nreal-world scenes---various grocery stores---and demonstrate performance\nimprovements when compared to existing semantic localization approaches. \n\n"}
{"id": "1812.00408", "contents": "Title: A Dataset and Benchmark for Large-scale Multi-modal Face Anti-spoofing Abstract: Face anti-spoofing is essential to prevent face recognition systems from a\nsecurity breach. Much of the progresses have been made by the availability of\nface anti-spoofing benchmark datasets in recent years. However, existing face\nanti-spoofing benchmarks have limited number of subjects ($\\le\\negmedspace170$)\nand modalities ($\\leq\\negmedspace2$), which hinder the further development of\nthe academic community. To facilitate face anti-spoofing research, we introduce\na large-scale multi-modal dataset, namely CASIA-SURF, which is the largest\npublicly available dataset for face anti-spoofing in terms of both subjects and\nvisual modalities. Specifically, it consists of $1,000$ subjects with $21,000$\nvideos and each sample has $3$ modalities (i.e., RGB, Depth and IR). We also\nprovide a measurement set, evaluation protocol and training/validation/testing\nsubsets, developing a new benchmark for face anti-spoofing. Moreover, we\npresent a new multi-modal fusion method as baseline, which performs feature\nre-weighting to select the more informative channel features while suppressing\nthe less useful ones for each modal. Extensive experiments have been conducted\non the proposed dataset to verify its significance and generalization\ncapability. The dataset is available at\nhttps://sites.google.com/qq.com/chalearnfacespoofingattackdete \n\n"}
{"id": "1812.00442", "contents": "Title: Deep Cosine Metric Learning for Person Re-Identification Abstract: Metric learning aims to construct an embedding where two extracted features\ncorresponding to the same identity are likely to be closer than features from\ndifferent identities. This paper presents a method for learning such a feature\nspace where the cosine similarity is effectively optimized through a simple\nre-parametrization of the conventional softmax classification regime. At test\ntime, the final classification layer can be stripped from the network to\nfacilitate nearest neighbor queries on unseen individuals using the cosine\nsimilarity metric. This approach presents a simple alternative to direct metric\nlearning objectives such as siamese networks that have required sophisticated\npair or triplet sampling strategies in the past. The method is evaluated on two\nlarge-scale pedestrian re-identification datasets where competitive results are\nachieved overall. In particular, we achieve better generalization on the test\nset compared to a network trained with triplet loss. \n\n"}
{"id": "1812.00555", "contents": "Title: SUSAN: Segment Unannotated image Structure using Adversarial Network Abstract: Segmentation of magnetic resonance (MR) images is a fundamental step in many\nmedical imaging-based applications. The recent implementation of deep\nconvolutional neural networks (CNNs) in image processing has been shown to have\nsignificant impacts on medical image segmentation. Network training of\nsegmentation CNNs typically requires images and paired annotation data\nrepresenting pixel-wise tissue labels referred to as masks. However, the\nsupervised training of highly efficient CNNs with deeper structure and more\nnetwork parameters requires a large number of training images and paired tissue\nmasks. Thus, there is great need to develop a generalized CNN-based\nsegmentation method which would be applicable for a wide variety of MR image\ndatasets with different tissue contrasts. The purpose of this study was to\ndevelop and evaluate a generalized CNN-based method for fully-automated\nsegmentation of different MR image datasets using a single set of annotated\ntraining data. A technique called cycle-consistent generative adversarial\nnetwork (CycleGAN) is applied as the core of the proposed method to perform\nimage-to-image translation between MR image datasets with different tissue\ncontrasts. A joint segmentation network is incorporated into the adversarial\nnetwork to obtain additional segmentation functionality. The proposed method\nwas evaluated for segmenting bone and cartilage on two clinical knee MR image\ndatasets acquired at our institution using only a single set of annotated data\nfrom a publicly available knee MR image dataset. The new technique may further\nimprove the applicability and efficiency of CNN-based segmentation of medical\nimages while eliminating the need for large amounts of annotated training data. \n\n"}
{"id": "1812.00874", "contents": "Title: SUGAMAN: Describing Floor Plans for Visually Impaired by Annotation\n  Learning and Proximity based Grammar Abstract: In this paper, we propose SUGAMAN (Supervised and Unified framework using\nGrammar and Annotation Model for Access and Navigation). SUGAMAN is a Hindi\nword meaning \"easy passage from one place to another\". SUGAMAN synthesizes\ntextual description from a given floor plan image for the visually impaired. A\nvisually impaired person can navigate in an indoor environment using the\ntextual description generated by SUGAMAN. With the help of a text reader\nsoftware, the target user can understand the rooms within the building and\narrangement of furniture to navigate. SUGAMAN is the first framework for\ndescribing a floor plan and giving direction for obstacle-free movement within\na building. We learn $5$ classes of room categories from $1355$ room image\nsamples under a supervised learning paradigm. These learned annotations are fed\ninto a description synthesis framework to yield a holistic description of a\nfloor plan image. We demonstrate the performance of various supervised\nclassifiers on room learning. We also provide a comparative analysis of system\ngenerated and human written descriptions. SUGAMAN gives state of the art\nperformance on challenging, real-world floor plan images. This work can be\napplied to areas like understanding floor plans of historical monuments,\nstability analysis of buildings, and retrieval. \n\n"}
{"id": "1812.00893", "contents": "Title: Domain Alignment with Triplets Abstract: Deep domain adaptation methods can reduce the distribution discrepancy by\nlearning domain-invariant embedddings. However, these methods only focus on\naligning the whole data distributions, without considering the class-level\nrelations among source and target images. Thus, a target embeddings of a bird\nmight be aligned to source embeddings of an airplane. This semantic\nmisalignment can directly degrade the classifier performance on the target\ndataset. To alleviate this problem, we present a similarity constrained\nalignment (SCA) method for unsupervised domain adaptation. When aligning the\ndistributions in the embedding space, SCA enforces a similarity-preserving\nconstraint to maintain class-level relations among the source and target\nimages, i.e., if a source image and a target image are of the same class label,\ntheir corresponding embeddings are supposed to be aligned nearby, and vise\nversa. In the absence of target labels, we assign pseudo labels for target\nimages. Given labeled source images and pseudo-labeled target images, the\nsimilarity-preserving constraint can be implemented by minimizing the triplet\nloss. With the joint supervision of domain alignment loss and\nsimilarity-preserving constraint, we train a network to obtain domain-invariant\nembeddings with two critical characteristics, intra-class compactness and\ninter-class separability. Extensive experiments conducted on the two datasets\nwell demonstrate the effectiveness of SCA. \n\n"}
{"id": "1812.01157", "contents": "Title: Cross-Classification Clustering: An Efficient Multi-Object Tracking\n  Technique for 3-D Instance Segmentation in Connectomics Abstract: Pixel-accurate tracking of objects is a key element in many computer vision\napplications, often solved by iterated individual object tracking or instance\nsegmentation followed by object matching. Here we introduce\ncross-classification clustering (3C), a technique that simultaneously tracks\ncomplex, interrelated objects in an image stack. The key idea in\ncross-classification is to efficiently turn a clustering problem into a\nclassification problem by running a logarithmic number of independent\nclassifications per image, letting the cross-labeling of these classifications\nuniquely classify each pixel to the object labels. We apply the 3C mechanism to\nachieve state-of-the-art accuracy in connectomics -- the nanoscale mapping of\nneural tissue from electron microscopy volumes. Our reconstruction system\nincreases scalability by an order of magnitude over existing single-object\ntracking methods (such as flood-filling networks). This scalability is\nimportant for the deployment of connectomics pipelines, since currently the\nbest performing techniques require computing infrastructures that are beyond\nthe reach of most laboratories. Our algorithm may offer benefits in other\ndomains that require pixel-accurate tracking of multiple objects, such as\nsegmentation of videos and medical imagery. \n\n"}
{"id": "1812.01681", "contents": "Title: Deep Bayesian Self-Training Abstract: Supervised Deep Learning has been highly successful in recent years,\nachieving state-of-the-art results in most tasks. However, with the ongoing\nuptake of such methods in industrial applications, the requirement for large\namounts of annotated data is often a challenge. In most real world problems,\nmanual annotation is practically intractable due to time/labour constraints,\nthus the development of automated and adaptive data annotation systems is\nhighly sought after. In this paper, we propose both a (i) Deep Bayesian\nSelf-Training methodology for automatic data annotation, by leveraging\npredictive uncertainty estimates using variational inference and modern Neural\nNetwork architectures, as well as (ii) a practical adaptation procedure for\nhandling high label variability between different dataset distributions through\nclustering of Neural Network latent variable representations. An experimental\nstudy on both public and private datasets is presented illustrating the\nsuperior performance of the proposed approach over standard Self-Training\nbaselines, highlighting the importance of predictive uncertainty estimates in\nsafety-critical domains. \n\n"}
{"id": "1812.01711", "contents": "Title: A Graph-CNN for 3D Point Cloud Classification Abstract: Graph convolutional neural networks (Graph-CNNs) extend traditional CNNs to\nhandle data that is supported on a graph. Major challenges when working with\ndata on graphs are that the support set (the vertices of the graph) do not\ntypically have a natural ordering, and in general, the topology of the graph is\nnot regular (i.e., vertices do not all have the same number of neighbors).\nThus, Graph-CNNs have huge potential to deal with 3D point cloud data which has\nbeen obtained from sampling a manifold. In this paper, we develop a Graph-CNN\nfor classifying 3D point cloud data, called PointGCN. The architecture combines\nlocalized graph convolutions with two types of graph downsampling operations\n(also known as pooling). By the effective exploration of the point cloud local\nstructure using the Graph-CNN, the proposed architecture achieves competitive\nperformance on the 3D object classification benchmark ModelNet, and our\narchitecture is more stable than competing schemes. \n\n"}
{"id": "1812.01946", "contents": "Title: Automatic Generation of Dense Non-rigid Optical Flow Abstract: There hardly exists any large-scale datasets with dense optical flow of\nnon-rigid motion from real-world imagery as of today. The reason lies mainly in\nthe required setup to derive ground truth optical flows: a series of images\nwith known camera poses along its trajectory, and an accurate 3D model from a\ntextured scene. Human annotation is not only too tedious for large databases,\nit can simply hardly contribute to accurate optical flow. To circumvent the\nneed for manual annotation, we propose a framework to automatically generate\noptical flow from real-world videos. The method extracts and matches objects\nfrom video frames to compute initial constraints, and applies a deformation\nover the objects of interest to obtain dense optical flow fields. We propose\nseveral ways to augment the optical flow variations. Extensive experimental\nresults show that training on our automatically generated optical flow\noutperforms methods that are trained on rigid synthetic data using FlowNet-S,\nLiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow\ngeneration framework are released at https://github.com/lhoangan/arap_flow \n\n"}
{"id": "1812.02676", "contents": "Title: Theoretical Guarantees of Deep Embedding Losses Under Label Noise Abstract: Collecting labeled data to train deep neural networks is costly and even\nimpractical for many tasks. Thus, research effort has been focused in\nautomatically curated datasets or unsupervised and weakly supervised learning.\nThe common problem in these directions is learning with unreliable label\ninformation. In this paper, we address the tolerance of deep embedding learning\nlosses against label noise, i.e. when the observed labels are different from\nthe true labels. Specifically, we provide the sufficient conditions to achieve\ntheoretical guarantees for the 2 common loss functions: marginal loss and\ntriplet loss. From these theoretical results, we can estimate how sampling\nstrategies and initialization can affect the level of resistance against label\nnoise. The analysis also helps providing more effective guidelines in\nunsupervised and weakly supervised deep embedding learning. \n\n"}
{"id": "1812.02781", "contents": "Title: ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape Abstract: We present a deep learning method for end-to-end monocular 3D object\ndetection and metric shape retrieval. We propose a novel loss formulation by\nlifting 2D detection, orientation, and scale estimation into 3D space. Instead\nof optimizing these quantities separately, the 3D instantiation allows to\nproperly measure the metric misalignment of boxes. We experimentally show that\nour 10D lifting of sparse 2D Regions of Interests (RoIs) achieves great results\nboth for 6D pose and recovery of the textured metric geometry of instances.\nThis further enables 3D synthetic data augmentation via inpainting recovered\nmeshes directly onto the 2D scenes. We evaluate on KITTI3D against other strong\nmonocular methods and demonstrate that our approach doubles the AP on the 3D\npose metrics on the official test set, defining the new state of the art. \n\n"}
{"id": "1812.02822", "contents": "Title: Learning Implicit Fields for Generative Shape Modeling Abstract: We advocate the use of implicit fields for learning generative models of\nshapes and introduce an implicit field decoder, called IM-NET, for shape\ngeneration, aimed at improving the visual quality of the generated shapes. An\nimplicit field assigns a value to each point in 3D space, so that a shape can\nbe extracted as an iso-surface. IM-NET is trained to perform this assignment by\nmeans of a binary classifier. Specifically, it takes a point coordinate, along\nwith a feature vector encoding a shape, and outputs a value which indicates\nwhether the point is outside the shape or not. By replacing conventional\ndecoders by our implicit decoder for representation learning (via IM-AE) and\nshape generation (via IM-GAN), we demonstrate superior results for tasks such\nas generative shape modeling, interpolation, and single-view 3D reconstruction,\nparticularly in terms of visual quality. Code and supplementary material are\navailable at https://github.com/czq142857/implicit-decoder. \n\n"}
{"id": "1812.02891", "contents": "Title: Adversarial Defense of Image Classification Using a Variational\n  Auto-Encoder Abstract: Deep neural networks are known to be vulnerable to adversarial attacks. This\nexposes them to potential exploits in security-sensitive applications and\nhighlights their lack of robustness. This paper uses a variational auto-encoder\n(VAE) to defend against adversarial attacks for image classification tasks.\nThis VAE defense has a few nice properties: (1) it is quite flexible and its\nuse of randomness makes it harder to attack; (2) it can learn disentangled\nrepresentations that prevent blurry reconstruction; and (3) a patch-wise VAE\ndefense strategy is used that does not require retraining for different size\nimages. For moderate to severe attacks, this system outperforms or closely\nmatches the performance of JPEG compression, with the best quality parameter.\nIt also has more flexibility and potential for improvement via training. \n\n"}
{"id": "1812.03026", "contents": "Title: A High-Order Scheme for Image Segmentation via a modified Level-Set\n  method Abstract: In this paper we propose a high-order accurate scheme for image segmentation\nbased on the level-set method. In this approach, the curve evolution is\ndescribed as the 0-level set of a representation function but we modify the\nvelocity that drives the curve to the boundary of the object in order to obtain\na new velocity with additional properties that are extremely useful to develop\na more stable high-order approximation with a small additional cost. The\napproximation scheme proposed here is the first 2D version of an adaptive\n\"filtered\" scheme recently introduced and analyzed by the authors in 1D. This\napproach is interesting since the implementation of the filtered scheme is\nrather efficient and easy. The scheme combines two building blocks (a monotone\nscheme and a high-order scheme) via a filter function and smoothness indicators\nthat allow to detect the regularity of the approximate solution adapting the\nscheme in an automatic way. Some numerical tests on synthetic and real images\nconfirm the accuracy of the proposed method and the advantages given by the new\nvelocity. \n\n"}
{"id": "1812.03622", "contents": "Title: 3D Scene Parsing via Class-Wise Adaptation Abstract: We propose the method that uses only computer graphics datasets to parse the\nreal world 3D scenes. 3D scene parsing based on semantic segmentation is\nrequired to implement the categorical interaction in the virtual world.\nConvolutional Neural Networks (CNNs) have recently shown state-of-theart\nperformance on computer vision tasks including semantic segmentation. However,\ncollecting and annotating a huge amount of data are needed to train CNNs.\nEspecially in the case of semantic segmentation, annotating pixel by pixel\ntakes a significant amount of time and often makes mistakes. In contrast,\ncomputer graphics can generate a lot of accurate annotated data and easily\nscale up by changing camera positions, textures and lights. Despite these\nadvantages, models trained on computer graphics datasets cannot perform well on\nreal data, which is known as the domain shift. To address this issue, we first\npresent that depth modal and synthetic noise are effective to reduce the domain\nshift. Then, we develop the class-wise adaptation which obtains domain\ninvariant features of CNNs. To reduce the domain shift, we create computer\ngraphics rooms with a lot of props, and provide photo-realistic rendered\nimages.We also demonstrate the application which is combined semantic\nsegmentation with Simultaneous Localization and Mapping (SLAM). Our application\nperforms accurate 3D scene parsing in real-time on an actual room. \n\n"}
{"id": "1812.03626", "contents": "Title: EDF: Ensemble, Distill, and Fuse for Easy Video Labeling Abstract: We present a way to rapidly bootstrap object detection on unseen videos using\nminimal human annotations. We accomplish this by combining two complementary\nsources of knowledge (one generic and the other specific) using bounding box\nmerging and model distillation. The first (generic) knowledge source is\nobtained from ensembling pre-trained object detectors using a novel bounding\nbox merging and confidence reweighting scheme. We make the observation that\nmodel distillation with data augmentation can train a specialized detector that\noutperforms the noisy labels it was trained on, and train a Student Network on\nthe ensemble detections that obtains higher mAP than the ensemble itself. The\nsecond (specialized) knowledge source comes from training a detector (which we\ncall the Supervised Labeler) on a labeled subset of the video to generate\ndetections on the unlabeled portion. We demonstrate on two popular vehicular\ndatasets that these techniques work to emit bounding boxes for all vehicles in\nthe frame with higher mean average precision (mAP) than any of the reference\nnetworks used, and that the combination of ensembled and human-labeled data\nproduces object detections that outperform either alone. \n\n"}
{"id": "1812.04042", "contents": "Title: Supervised Deep Kriging for Single-Image Super-Resolution Abstract: We propose a novel single-image super-resolution approach based on the\ngeostatistical method of kriging. Kriging is a zero-bias minimum-variance\nestimator that performs spatial interpolation based on a weighted average of\nknown observations. Rather than solving for the kriging weights via the\ntraditional method of inverting covariance matrices, we propose a supervised\nform in which we learn a deep network to generate said weights. We combine the\nkriging weight generation and kriging process into a joint network that can be\nlearned end-to-end. Our network achieves competitive super-resolution results\nas other state-of-the-art methods. In addition, since the super-resolution\nprocess follows a known statistical framework, we are able to estimate bias and\nvariance, something which is rarely possible for other deep networks. \n\n"}
{"id": "1812.04955", "contents": "Title: Prior-Knowledge and Attention-based Meta-Learning for Few-Shot Learning Abstract: Recently, meta-learning has been shown as a promising way to solve few-shot\nlearning. In this paper, inspired by the human cognition process which utilizes\nboth prior-knowledge and vision attention in learning new knowledge, we present\na novel paradigm of meta-learning approach with three developments to introduce\nattention mechanism and prior-knowledge for meta-learning. In our approach,\nprior-knowledge is responsible for helping meta-learner expressing the input\ndata into high-level representation space, and attention mechanism enables\nmeta-learner focusing on key features of the data in the representation space.\nCompared with existing meta-learning approaches that pay little attention to\nprior-knowledge and vision attention, our approach alleviates the\nmeta-learner's few-shot cognition burden. Furthermore, a Task-Over-Fitting\n(TOF) problem, which indicates that the meta-learner has poor generalization on\ndifferent K-shot learning tasks, is discovered and we propose a Cross-Entropy\nacross Tasks (CET) metric to model and solve the TOF problem. Extensive\nexperiments demonstrate that we improve the meta-learner with state-of-the-art\nperformance on several few-shot learning benchmarks, and at the same time the\nTOF problem can also be released greatly. \n\n"}
{"id": "1812.05418", "contents": "Title: DLOW: Domain Flow for Adaptation and Generalization Abstract: In this work, we present a domain flow generation(DLOW) model to bridge two\ndifferent domains by generating a continuous sequence of intermediate domains\nflowing from one domain to the other. The benefits of our DLOW model are\ntwo-fold. First, it is able to transfer source images into different styles in\nthe intermediate domains. The transferred images smoothly bridge the gap\nbetween source and target domains, thus easing the domain adaptation task.\nSecond, when multiple target domains are provided for training, our DLOW model\nis also able to generate new styles of images that are unseen in the training\ndata. We implement our DLOW model based on CycleGAN. A domainness variable is\nintroduced to guide the model to generate the desired intermediate domain\nimages. In the inference phase, a flow of various styles of images can be\nobtained by varying the domainness variable. We demonstrate the effectiveness\nof our model for both cross-domain semantic segmentation and the style\ngeneralization tasks on benchmark datasets. Our implementation is available at\nhttps://github.com/ETHRuiGong/DLOW. \n\n"}
{"id": "1812.05802", "contents": "Title: Pyramid Network with Online Hard Example Mining for Accurate Left Atrium\n  Segmentation Abstract: Accurately segmenting left atrium in MR volume can benefit the ablation\nprocedure of atrial fibrillation. Traditional automated solutions often fail in\nrelieving experts from the labor-intensive manual labeling. In this paper, we\npropose a deep neural network based solution for automated left atrium\nsegmentation in gadolinium-enhanced MR volumes with promising performance. We\nfirstly argue that, for this volumetric segmentation task, networks in 2D\nfashion can present great superiorities in time efficiency and segmentation\naccuracy than networks with 3D fashion. Considering the highly varying shape of\natrium and the branchy structure of associated pulmonary veins, we propose to\nadopt a pyramid module to collect semantic cues in feature maps from multiple\nscales for fine-grained segmentation. Also, to promote our network in\nclassifying the hard examples, we propose an Online Hard Negative Example\nMining strategy to identify voxels in slices with low classification\ncertainties and penalize the wrong predictions on them. Finally, we devise a\ncompetitive training scheme to further boost the generalization ability of\nnetworks. Extensively verified on 20 testing volumes, our proposed framework\nachieves an average Dice of 92.83% in segmenting the left atria and pulmonary\nveins. \n\n"}
{"id": "1812.05831", "contents": "Title: Combining Deep and Depth: Deep Learning and Face Depth Maps for Driver\n  Attention Monitoring Abstract: Recently, deep learning approaches have achieved promising results in various\nfields of computer vision. In this paper, we investigate the combination of\ndeep learning based methods and depth maps as input images to tackle the\nproblem of driver attention monitoring. Moreover, we assume the concept of\nattention as Head Pose Estimation and Facial Landmark Detection tasks.\nDifferently from other proposals in the literature, the proposed systems are\nable to work directly and based only on raw depth data. All presented methods\nare trained and tested on two new public datasets, namely Pandora and\nMotorMark, achieving state-of-art results and running with real time\nperformance. \n\n"}
{"id": "1812.05914", "contents": "Title: End to End Video Segmentation for Driving : Lane Detection For\n  Autonomous Car Abstract: Safety and decline of road traffic accidents remain important issues of\nautonomous driving. Statistics show that unintended lane departure is a leading\ncause of worldwide motor vehicle collisions, making lane detection the most\npromising and challenge task for self-driving. Today, numerous groups are\ncombining deep learning techniques with computer vision problems to solve\nself-driving problems. In this paper, a Global Convolution Networks (GCN) model\nis used to address both classification and localization issues for semantic\nsegmentation of lane. We are using color-based segmentation is presented and\nthe usability of the model is evaluated. A residual-based boundary refinement\nand Adam optimization is also used to achieve state-of-art performance. As\nnormal cars could not afford GPUs on the car, and training session for a\nparticular road could be shared by several cars. We propose a framework to get\nit work in real world. We build a real time video transfer system to get video\nfrom the car, get the model trained in edge server (which is equipped with\nGPUs), and send the trained model back to the car. \n\n"}
{"id": "1812.06071", "contents": "Title: On Attention Modules for Audio-Visual Synchronization Abstract: With the development of media and networking technologies, multimedia\napplications ranging from feature presentation in a cinema setting to video on\ndemand to interactive video conferencing are in great demand. Good\nsynchronization between audio and video modalities is a key factor towards\ndefining the quality of a multimedia presentation. The audio and visual signals\nof a multimedia presentation are commonly managed by independent workflows -\nthey are often separately authored, processed, stored and even delivered to the\nplayback system. This opens up the possibility of temporal misalignment between\nthe two modalities - such a tendency is often more pronounced in the case of\nproduced content (such as movies).\n  To judge whether audio and video signals of a multimedia presentation are\nsynchronized, we as humans often pay close attention to discriminative\nspatio-temporal blocks of the video (e.g. synchronizing the lip movement with\nthe utterance of words, or the sound of a bouncing ball at the moment it hits\nthe ground). At the same time, we ignore large portions of the video in which\nno discriminative sounds exist (e.g. background music playing in a movie).\nInspired by this observation, we study leveraging attention modules for\nautomatically detecting audio-visual synchronization. We propose neural network\nbased attention modules, capable of weighting different portions\n(spatio-temporal blocks) of the video based on their respective discriminative\npower. Our experiments indicate that incorporating attention modules yields\nstate-of-the-art results for the audio-visual synchronization classification\nproblem. \n\n"}
{"id": "1812.06297", "contents": "Title: Hinted Networks Abstract: We present Hinted Networks: a collection of architectural transformations for\nimproving the accuracies of neural network models for regression tasks, through\nthe injection of a prior for the output prediction (i.e. a hint). We ground our\ninvestigations within the camera relocalization domain, and propose two\nvariants, namely the Hinted Embedding and Hinted Residual networks, both\napplied to the PoseNet base model for regressing camera pose from an image. Our\nevaluations show practical improvements in localization accuracy for standard\noutdoor and indoor localization datasets, without using additional information.\nWe further assess the range of accuracy gains within an aerial-view\nlocalization setup, simulated across vast areas at different times of the year. \n\n"}
{"id": "1812.06571", "contents": "Title: Latent Dirichlet Allocation in Generative Adversarial Networks Abstract: We study the problem of multimodal generative modelling of images based on\ngenerative adversarial networks (GANs). Despite the success of existing\nmethods, they often ignore the underlying structure of vision data or its\nmultimodal generation characteristics. To address this problem, we introduce\nthe Dirichlet prior for multimodal image generation, which leads to a new\nLatent Dirichlet Allocation based GAN (LDAGAN). In detail, for the generative\nprocess modelling, LDAGAN defines a generative mode for each sample,\ndetermining which generative sub-process it belongs to. For the adversarial\ntraining, LDAGAN derives a variational expectation-maximization (VEM) algorithm\nto estimate model parameters. Experimental results on real-world datasets have\ndemonstrated the outstanding performance of LDAGAN over other existing GANs. \n\n"}
{"id": "1812.06597", "contents": "Title: Learning Student Networks via Feature Embedding Abstract: Deep convolutional neural networks have been widely used in numerous\napplications, but their demanding storage and computational resource\nrequirements prevent their applications on mobile devices. Knowledge\ndistillation aims to optimize a portable student network by taking the\nknowledge from a well-trained heavy teacher network. Traditional\nteacher-student based methods used to rely on additional fully-connected layers\nto bridge intermediate layers of teacher and student networks, which brings in\na large number of auxiliary parameters. In contrast, this paper aims to\npropagate information from teacher to student without introducing new variables\nwhich need to be optimized. We regard the teacher-student paradigm from a new\nperspective of feature embedding. By introducing the locality preserving loss,\nthe student network is encouraged to generate the low-dimensional features\nwhich could inherit intrinsic properties of their corresponding\nhigh-dimensional features from teacher network. The resulting portable network\nthus can naturally maintain the performance as that of the teacher network.\nTheoretical analysis is provided to justify the lower computation complexity of\nthe proposed method. Experiments on benchmark datasets and well-trained\nnetworks suggest that the proposed algorithm is superior to state-of-the-art\nteacher-student learning methods in terms of computational and storage\ncomplexity. \n\n"}
{"id": "1812.06847", "contents": "Title: Convolutional herbal prescription building method from multi-scale\n  facial features Abstract: In Traditional Chinese Medicine (TCM), facial features are important basis\nfor diagnosis and treatment. A doctor of TCM can prescribe according to a\npatient's physical indicators such as face, tongue, voice, symptoms, pulse.\nPrevious works analyze and generate prescription according to symptoms.\nHowever, research work to mine the association between facial features and\nprescriptions has not been found for the time being. In this work, we try to\nuse deep learning methods to mine the relationship between the patient's face\nand herbal prescriptions (TCM prescriptions), and propose to construct\nconvolutional neural networks that generate TCM prescriptions according to the\npatient's face image. It is a novel and challenging job. In order to mine\nfeatures from different granularities of faces, we design a multi-scale\nconvolutional neural network based on three-grained face, which mines the\npatient's face information from the organs, local regions, and the entire face.\nOur experiments show that convolutional neural networks can learn relevant\ninformation from face to prescribe, and the multi-scale convolutional neural\nnetworks based on three-grained face perform better. \n\n"}
{"id": "1812.07104", "contents": "Title: Reading Industrial Inspection Sheets by Inferring Visual Relations Abstract: The traditional mode of recording faults in heavy factory equipment has been\nvia hand marked inspection sheets, wherein a machine engineer manually marks\nthe faulty machine regions on a paper outline of the machine. Over the years,\nmillions of such inspection sheets have been recorded and the data within these\nsheets has remained inaccessible. However, with industries going digital and\nwaking up to the potential value of fault data for machine health monitoring,\nthere is an increased impetus towards digitization of these hand marked\ninspection records. To target this digitization, we propose a novel visual\npipeline combining state of the art deep learning models, with domain knowledge\nand low level vision techniques, followed by inference of visual relationships.\nOur framework is robust to the presence of both static and non-static\nbackground in the document, variability in the machine template diagrams,\nunstructured shape of graphical objects to be identified and variability in the\nstrokes of handwritten text. The proposed pipeline incorporates a capsule and\nspatial transformer network based classifier for accurate text reading, and a\ncustomized CTPN network for text detection in addition to hybrid techniques for\narrow detection and dialogue cloud removal. We have tested our approach on a\nreal world dataset of 50 inspection sheets for large containers and boilers.\nThe results are visually appealing and the pipeline achieved an accuracy of\n87.1% for text detection and 94.6% for text reading. \n\n"}
{"id": "1812.07169", "contents": "Title: Explaining Neural Networks Semantically and Quantitatively Abstract: This paper presents a method to explain the knowledge encoded in a\nconvolutional neural network (CNN) quantitatively and semantically. The\nanalysis of the specific rationale of each prediction made by the CNN presents\na key issue of understanding neural networks, but it is also of significant\npractical values in certain applications. In this study, we propose to distill\nknowledge from the CNN into an explainable additive model, so that we can use\nthe explainable model to provide a quantitative explanation for the CNN\nprediction. We analyze the typical bias-interpreting problem of the explainable\nmodel and develop prior losses to guide the learning of the explainable\nadditive model. Experimental results have demonstrated the effectiveness of our\nmethod. \n\n"}
{"id": "1812.07174", "contents": "Title: SREdgeNet: Edge Enhanced Single Image Super Resolution using Dense Edge\n  Detection Network and Feature Merge Network Abstract: Deep learning based single image super-resolution (SR) methods have been\nrapidly evolved over the past few years and have yielded state-of-the-art\nperformances over conventional methods. Since these methods usually minimized\nl1 loss between the output SR image and the ground truth image, they yielded\nvery high peak signal-to-noise ratio (PSNR) that is inversely proportional to\nthese losses. Unfortunately, minimizing these losses inevitably lead to blurred\nedges due to averaging of plausible solutions. Recently, SRGAN was proposed to\navoid this average effect by minimizing perceptual losses instead of l1 loss\nand it yielded perceptually better SR images (or images with sharp edges) at\nthe price of lowering PSNR. In this paper, we propose SREdgeNet, edge enhanced\nsingle image SR network, that was inspired by conventional SR theories so that\naverage effect could be avoided not by changing the loss, but by changing the\nSR network property with the same l1 loss. Our SREdgeNet consists of 3\nsequential deep neural network modules: the first module is any\nstate-of-the-art SR network and we selected a variant of EDSR. The second\nmodule is any edge detection network taking the output of the first SR module\nas an input and we propose DenseEdgeNet for this module. Lastly, the third\nmodule is merging the outputs of the first and second modules to yield edge\nenhanced SR image and we propose MergeNet for this module. Qualitatively, our\nproposed method yielded images with sharp edges compared to other\nstate-of-the-art SR methods. Quantitatively, our SREdgeNet yielded\nstate-of-the-art performance in terms of structural similarity (SSIM) while\nmaintained comparable PSNR for x8 enlargement. \n\n"}
{"id": "1812.07444", "contents": "Title: FDSNet: Finger dorsal image spoof detection network using light field\n  camera Abstract: At present spoofing attacks via which biometric system is potentially\nvulnerable against a fake biometric characteristic, introduces a great\nchallenge to recognition performance. Despite the availability of a broad range\nof presentation attack detection (PAD) or liveness detection algorithms,\nfingerprint sensors are vulnerable to spoofing via fake fingers. In such\nsituations, finger dorsal images can be thought of as an alternative which can\nbe captured without much user cooperation and are more appropriate for outdoor\nsecurity applications. In this paper, we present a first feasibility study of\nspoofing attack scenarios on finger dorsal authentication system, which include\nfour types of presentation attacks such as printed paper, wrapped printed\npaper, scan and mobile. This study also presents a CNN based spoofing attack\ndetection method which employ state-of-the-art deep learning techniques along\nwith transfer learning mechanism. We have collected 196 finger dorsal real\nimages from 33 subjects, captured with a Lytro camera and also created a set of\n784 finger dorsal spoofing images. Extensive experimental results have been\nperformed that demonstrates the superiority of the proposed approach for\nvarious spoofing attacks. \n\n"}
{"id": "1812.08789", "contents": "Title: Steerable $e$PCA: Rotationally Invariant Exponential Family PCA Abstract: In photon-limited imaging, the pixel intensities are affected by photon count\nnoise. Many applications, such as 3-D reconstruction using correlation analysis\nin X-ray free electron laser (XFEL) single molecule imaging, require an\naccurate estimation of the covariance of the underlying 2-D clean images.\nAccurate estimation of the covariance from low-photon count images must take\ninto account that pixel intensities are Poisson distributed, hence the\nclassical sample covariance estimator is sub-optimal. Moreover, in single\nmolecule imaging, including in-plane rotated copies of all images could further\nimprove the accuracy of covariance estimation. In this paper we introduce an\nefficient and accurate algorithm for covariance matrix estimation of count\nnoise 2-D images, including their uniform planar rotations and possibly\nreflections. Our procedure, steerable $e$PCA, combines in a novel way two\nrecently introduced innovations. The first is a methodology for principal\ncomponent analysis (PCA) for Poisson distributions, and more generally,\nexponential family distributions, called $e$PCA. The second is steerable PCA, a\nfast and accurate procedure for including all planar rotations for PCA. The\nresulting principal components are invariant to the rotation and reflection of\nthe input images. We demonstrate the efficiency and accuracy of steerable\n$e$PCA in numerical experiments involving simulated XFEL datasets and rotated\nYale B face data. \n\n"}
{"id": "1812.09041", "contents": "Title: A Multi-task Neural Approach for Emotion Attribution, Classification and\n  Summarization Abstract: Emotional content is a crucial ingredient in user-generated videos. However,\nthe sparsity of emotional expressions in the videos poses an obstacle to visual\nemotion analysis. In this paper, we propose a new neural approach, Bi-stream\nEmotion Attribution-Classification Network (BEAC-Net), to solve three related\nemotion analysis tasks: emotion recognition, emotion attribution, and\nemotion-oriented summarization, in a single integrated framework. BEAC-Net has\ntwo major constituents, an attribution network and a classification network.\nThe attribution network extracts the main emotional segment that classification\nshould focus on in order to mitigate the sparsity issue. The classification\nnetwork utilizes both the extracted segment and the original video in a\nbi-stream architecture. We contribute a new dataset for the emotion attribution\ntask with human-annotated ground-truth labels for emotion segments. Experiments\non two video datasets demonstrate superior performance of the proposed\nframework and the complementary nature of the dual classification streams. \n\n"}
{"id": "1812.09280", "contents": "Title: Canonical Correlation Analysis for Misaligned Satellite Image Change\n  Detection Abstract: Canonical correlation analysis (CCA) is a statistical learning method that\nseeks to build view-independent latent representations from multi-view data.\nThis method has been successfully applied to several pattern analysis tasks\nsuch as image-to-text mapping and view-invariant object/action recognition.\nHowever, this success is highly dependent on the quality of data pairing (i.e.,\nalignments) and mispairing adversely affects the generalization ability of the\nlearned CCA representations. In this paper, we address the issue of alignment\nerrors using a new variant of canonical correlation analysis referred to as\nalignment-agnostic (AA) CCA. Starting from erroneously paired data taken from\ndifferent views, this CCA finds transformation matrices by optimizing a\nconstrained maximization problem that mixes a data correlation term with\ncontext regularization; the particular design of these two terms mitigates the\neffect of alignment errors when learning the CCA transformations. Experiments\nconducted on multi-view tasks, including multi-temporal satellite image change\ndetection, show that our AA CCA method is highly effective and resilient to\nmispairing errors. \n\n"}
{"id": "1812.09502", "contents": "Title: Disentangling Latent Space for VAE by Label Relevant/Irrelevant\n  Dimensions Abstract: VAE requires the standard Gaussian distribution as a prior in the latent\nspace. Since all codes tend to follow the same prior, it often suffers the\nso-called \"posterior collapse\". To avoid this, this paper introduces the class\nspecific distribution for the latent code. But different from CVAE, we present\na method for disentangling the latent space into the label relevant and\nirrelevant dimensions, $\\bm{\\mathrm{z}}_s$ and $\\bm{\\mathrm{z}}_u$, for a\nsingle input. We apply two separated encoders to map the input into\n$\\bm{\\mathrm{z}}_s$ and $\\bm{\\mathrm{z}}_u$ respectively, and then give the\nconcatenated code to the decoder to reconstruct the input. The label irrelevant\ncode $\\bm{\\mathrm{z}}_u$ represent the common characteristics of all inputs,\nhence they are constrained by the standard Gaussian, and their encoder is\ntrained in amortized variational inference way, like VAE. While\n$\\bm{\\mathrm{z}}_s$ is assumed to follow the Gaussian mixture distribution in\nwhich each component corresponds to a particular class. The parameters for the\nGaussian components in $\\bm{\\mathrm{z}}_s$ encoder are optimized by the label\nsupervision in a global stochastic way. In theory, we show that our method is\nactually equivalent to adding a KL divergence term on the joint distribution of\n$\\bm{\\mathrm{z}}_s$ and the class label $c$, and it can directly increase the\nmutual information between $\\bm{\\mathrm{z}}_s$ and the label $c$. Our model can\nalso be extended to GAN by adding a discriminator in the pixel domain so that\nit produces high quality and diverse images. \n\n"}
{"id": "1812.10016", "contents": "Title: A Unified Framework for Mutual Improvement of SLAM and Semantic\n  Segmentation Abstract: This paper presents a novel framework for simultaneously implementing\nlocalization and segmentation, which are two of the most important vision-based\ntasks for robotics. While the goals and techniques used for them were\nconsidered to be different previously, we show that by making use of the\nintermediate results of the two modules, their performance can be enhanced at\nthe same time. Our framework is able to handle both the instantaneous motion\nand long-term changes of instances in localization with the help of the\nsegmentation result, which also benefits from the refined 3D pose information.\nWe conduct experiments on various datasets, and prove that our framework works\neffectively on improving the precision and robustness of the two tasks and\noutperforms existing localization and segmentation algorithms. \n\n"}
{"id": "1812.10066", "contents": "Title: Selectivity or Invariance: Boundary-aware Salient Object Detection Abstract: Typically, a salient object detection (SOD) model faces opposite requirements\nin processing object interiors and boundaries. The features of interiors should\nbe invariant to strong appearance change so as to pop-out the salient object as\na whole, while the features of boundaries should be selective to slight\nappearance change to distinguish salient objects and background. To address\nthis selectivity-invariance dilemma, we propose a novel boundary-aware network\nwith successive dilation for image-based SOD. In this network, the feature\nselectivity at boundaries is enhanced by incorporating a boundary localization\nstream, while the feature invariance at interiors is guaranteed with a complex\ninterior perception stream. Moreover, a transition compensation stream is\nadopted to amend the probable failures in transitional regions between\ninteriors and boundaries. In particular, an integrated successive dilation\nmodule is proposed to enhance the feature invariance at interiors and\ntransitional regions. Extensive experiments on six datasets show that the\nproposed approach outperforms 16 state-of-the-art methods. \n\n"}
{"id": "1812.10071", "contents": "Title: Coupled Recurrent Network (CRN) Abstract: Many semantic video analysis tasks can benefit from multiple, heterogenous\nsignals. For example, in addition to the original RGB input sequences,\nsequences of optical flow are usually used to boost the performance of human\naction recognition in videos. To learn from these heterogenous input sources,\nexisting methods reply on two-stream architectural designs that contain\nindependent, parallel streams of Recurrent Neural Networks (RNNs). However,\ntwo-stream RNNs do not fully exploit the reciprocal information contained in\nthe multiple signals, let alone exploit it in a recurrent manner. To this end,\nwe propose in this paper a novel recurrent architecture, termed Coupled\nRecurrent Network (CRN), to deal with multiple input sources. In CRN, the\nparallel streams of RNNs are coupled together. Key design of CRN is a Recurrent\nInterpretation Block (RIB) that supports learning of reciprocal feature\nrepresentations from multiple signals in a recurrent manner. Different from\nRNNs which stack the training loss at each time step or the last time step, we\npropose an effective and efficient training strategy for CRN. Experiments show\nthe efficacy of the proposed CRN. In particular, we achieve the new state of\nthe art on the benchmark datasets of human action recognition and multi-person\npose estimation. \n\n"}
{"id": "1812.10240", "contents": "Title: Studying the Plasticity in Deep Convolutional Neural Networks using\n  Random Pruning Abstract: Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations.The key idea is to rank the filters based on a certain criterion\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\nfilters are pruned away the remainder of the network is fine tuned and is shown\nto give performance comparable to the original unpruned network. In this work,\nwe report experiments which suggest that the comparable performance of the\npruned network is not due to the specific criterion chosen but due to the\ninherent plasticity of deep neural networks which allows them to recover from\nthe loss of pruned filters once the rest of the filters are fine-tuned.\nSpecifically we show counter-intuitive results wherein by randomly pruning\n25-50% filters from deep CNNs we are able to obtain the same performance as\nobtained by using state-of-the-art pruning methods. We empirically validate our\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\nneeds to be tested on only a small set of classes at test time (say, only\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\nclass specific pruning and show that even here a random pruning strategy gives\nclose to state-of-the-art performance. Unlike existing approaches which mainly\nfocus on the task of image classification, in this work we also report results\non object detection and image segmentation. We show that using a simple random\npruning strategy we can achieve significant speed up in object detection (74%\nimprovement in fps) while retaining the same accuracy as that of the original\nFaster RCNN model. Similarly we show that the performance of a pruned\nSegmentation Network (SegNet) is actually very similar to that of the original\nunpruned SegNet. \n\n"}
{"id": "1812.10305", "contents": "Title: Spatial and Temporal Mutual Promotion for Video-based Person\n  Re-identification Abstract: Video-based person re-identification is a crucial task of matching video\nsequences of a person across multiple camera views. Generally, features\ndirectly extracted from a single frame suffer from occlusion, blur,\nillumination and posture changes. This leads to false activation or missing\nactivation in some regions, which corrupts the appearance and motion\nrepresentation. How to explore the abundant spatial-temporal information in\nvideo sequences is the key to solve this problem. To this end, we propose a\nRefining Recurrent Unit (RRU) that recovers the missing parts and suppresses\nnoisy parts of the current frame's features by referring historical frames.\nWith RRU, the quality of each frame's appearance representation is improved.\nThen we use the Spatial-Temporal clues Integration Module (STIM) to mine the\nspatial-temporal information from those upgraded features. Meanwhile, the\nmulti-level training objective is used to enhance the capability of RRU and\nSTIM. Through the cooperation of those modules, the spatial and temporal\nfeatures mutually promote each other and the final spatial-temporal feature\nrepresentation is more discriminative and robust. Extensive experiments are\nconducted on three challenging datasets, i.e., iLIDS-VID, PRID-2011 and MARS.\nThe experimental results demonstrate that our approach outperforms existing\nstate-of-the-art methods of video-based person re-identification on iLIDS-VID\nand MARS and achieves favorable results on PRID-2011. \n\n"}
{"id": "1812.10352", "contents": "Title: Learning Not to Learn: Training Deep Neural Networks with Biased Data Abstract: We propose a novel regularization algorithm to train deep neural networks, in\nwhich data at training time is severely biased. Since a neural network\nefficiently learns data distribution, a network is likely to learn the bias\ninformation to categorize input data. It leads to poor performance at test\ntime, if the bias is, in fact, irrelevant to the categorization. In this paper,\nwe formulate a regularization loss based on mutual information between feature\nembedding and bias. Based on the idea of minimizing this mutual information, we\npropose an iterative algorithm to unlearn the bias information. We employ an\nadditional network to predict the bias distribution and train the network\nadversarially against the feature embedding network. At the end of learning,\nthe bias prediction network is not able to predict the bias not because it is\npoorly trained, but because the feature embedding network successfully unlearns\nthe bias information. We also demonstrate quantitative and qualitative\nexperimental results which show that our algorithm effectively removes the bias\ninformation from feature embedding. \n\n"}
{"id": "1901.00055", "contents": "Title: Multiple Sclerosis Lesion Inpainting Using Non-Local Partial\n  Convolutions Abstract: Multiple sclerosis (MS) is an inflammatory demyelinating disease of the\ncentral nervous system (CNS) that results in focal injury to the grey and white\nmatter. The presence of white matter lesions biases morphometric analyses such\nas registration, individual longitudinal measurements and tissue segmentation\nfor brain volume measurements. Lesion-inpainting with intensities derived from\nsurrounding healthy tissue represents one approach to alleviate such problems.\nHowever, existing methods inpaint lesions based on texture information derived\nfrom local surrounding tissue, often leading to inconsistent inpainting and the\ngeneration of artifacts such as intensity discrepancy and blurriness. Based on\nthese observations, we propose non-local partial convolutions (NLPC) that\nintegrates a Unet-like network with the non-local module. The non-local module\nis exploited to capture long range dependencies between the lesion area and\nremaining normal-appearing brain regions. Then, the lesion area is filled by\nreferring to normal-appearing regions with more similar features. This method\ngenerates inpainted regions that appear more realistic and natural. Our\nquantitative experimental results also demonstrate superiority of this\ntechnique of existing state-of-the-art inpainting methods. \n\n"}
{"id": "1901.00121", "contents": "Title: FPGA-based Accelerators of Deep Learning Networks for Learning and\n  Classification: A Review Abstract: Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers. \n\n"}
{"id": "1901.00484", "contents": "Title: Action2Vec: A Crossmodal Embedding Approach to Action Learning Abstract: We describe a novel cross-modal embedding space for actions, named\nAction2Vec, which combines linguistic cues from class labels with\nspatio-temporal features derived from video clips. Our approach uses a\nhierarchical recurrent network to capture the temporal structure of video\nfeatures. We train our embedding using a joint loss that combines\nclassification accuracy with similarity to Word2Vec semantics. We evaluate\nAction2Vec by performing zero shot action recognition and obtain state of the\nart results on three standard datasets. In addition, we present two novel\nanalogy tests which quantify the extent to which our joint embedding captures\ndistributional semantics. This is the first joint embedding space to combine\nverbs and action videos, and the first to be thoroughly evaluated with respect\nto its distributional semantics. \n\n"}
{"id": "1901.00850", "contents": "Title: CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions Abstract: Referring object detection and referring image segmentation are important\ntasks that require joint understanding of visual information and natural\nlanguage. Yet there has been evidence that current benchmark datasets suffer\nfrom bias, and current state-of-the-art models cannot be easily evaluated on\ntheir intermediate reasoning process. To address these issues and complement\nsimilar efforts in visual question answering, we build CLEVR-Ref+, a synthetic\ndiagnostic dataset for referring expression comprehension. The precise\nlocations and attributes of the objects are readily available, and the\nreferring expressions are automatically associated with functional programs.\nThe synthetic nature allows control over dataset bias (through sampling\nstrategy), and the modular programs enable intermediate reasoning ground truth\nwithout human annotators.\n  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we\nalso propose IEP-Ref, a module network approach that significantly outperforms\nother models on our dataset. In particular, we present two interesting and\nimportant findings using IEP-Ref: (1) the module trained to transform feature\nmaps into segmentation masks can be attached to any intermediate module to\nreveal the entire reasoning process step-by-step; (2) even if all training data\nhas at least one object referred, IEP-Ref can correctly predict no-foreground\nwhen presented with false-premise referring expressions. To the best of our\nknowledge, this is the first direct and quantitative proof that neural modules\nbehave in the way they are intended. \n\n"}
{"id": "1901.01578", "contents": "Title: CC-Net: Image Complexity Guided Network Compression for Biomedical Image\n  Segmentation Abstract: Convolutional neural networks (CNNs) for biomedical image analysis are often\nof very large size, resulting in high memory requirement and high latency of\noperations. Searching for an acceptable compressed representation of the base\nCNN for a specific imaging application typically involves a series of\ntime-consuming training/validation experiments to achieve a good compromise\nbetween network size and accuracy. To address this challenge, we propose\nCC-Net, a new image complexity-guided CNN compression scheme for biomedical\nimage segmentation. Given a CNN model, CC-Net predicts the final accuracy of\nnetworks of different sizes based on the average image complexity computed from\nthe training data. It then selects a multiplicative factor for producing a\ndesired network with acceptable network accuracy and size. Experiments show\nthat CC-Net is effective for generating compressed segmentation networks,\nretaining up to 95% of the base network segmentation accuracy and utilizing\nonly 0.1% of trainable parameters of the full-sized networks in the best case. \n\n"}
{"id": "1901.01651", "contents": "Title: Tooth morphometry using quasi-conformal theory Abstract: Shape analysis is important in anthropology, bioarchaeology and forensic\nscience for interpreting useful information from human remains. In particular,\nteeth are morphologically stable and hence well-suited for shape analysis. In\nthis work, we propose a framework for tooth morphometry using quasi-conformal\ntheory. Landmark-matching Teichm\\\"uller maps are used for establishing a 1-1\ncorrespondence between tooth surfaces with prescribed anatomical landmarks.\nThen, a quasi-conformal statistical shape analysis model based on the\nTeichm\\\"uller mapping results is proposed for building a tooth classification\nscheme. We deploy our framework on a dataset of human premolars to analyze the\ntooth shape variation among genders and ancestries. Experimental results show\nthat our method achieves much higher classification accuracy with respect to\nboth gender and ancestry when compared to the existing methods. Furthermore,\nour model reveals the underlying tooth shape difference between different\ngenders and ancestries in terms of the local geometric distortion and\ncurvatures. \n\n"}
{"id": "1901.01928", "contents": "Title: DSConv: Efficient Convolution Operator Abstract: Quantization is a popular way of increasing the speed and lowering the memory\nusage of Convolution Neural Networks (CNNs). When labelled training data is\navailable, network weights and activations have successfully been quantized\ndown to 1-bit. The same cannot be said about the scenario when labelled\ntraining data is not available, e.g. when quantizing a pre-trained model, where\ncurrent approaches show, at best, no loss of accuracy at 8-bit quantizations.\nWe introduce DSConv, a flexible quantized convolution operator that replaces\nsingle-precision operations with their far less expensive integer counterparts,\nwhile maintaining the probability distributions over both the kernel weights\nand the outputs. We test our model as a plug-and-play replacement for standard\nconvolution on most popular neural network architectures, ResNet, DenseNet,\nGoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with\nless than 1% loss of accuracy, without retraining, using only 4-bit\nquantization. We also show how a distillation-based adaptation stage with\nunlabelled data can improve results even further. \n\n"}
{"id": "1901.02446", "contents": "Title: Panoptic Feature Pyramid Networks Abstract: The recently introduced panoptic segmentation task has renewed our\ncommunity's interest in unifying the tasks of instance segmentation (for thing\nclasses) and semantic segmentation (for stuff classes). However, current\nstate-of-the-art methods for this joint task use separate and dissimilar\nnetworks for instance and semantic segmentation, without performing any shared\ncomputation. In this work, we aim to unify these methods at the architectural\nlevel, designing a single network for both tasks. Our approach is to endow Mask\nR-CNN, a popular instance segmentation method, with a semantic segmentation\nbranch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly,\nthis simple baseline not only remains effective for instance segmentation, but\nalso yields a lightweight, top-performing method for semantic segmentation. In\nthis work, we perform a detailed study of this minimally extended version of\nMask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust\nand accurate baseline for both tasks. Given its effectiveness and conceptual\nsimplicity, we hope our method can serve as a strong baseline and aid future\nresearch in panoptic segmentation. \n\n"}
{"id": "1901.03035", "contents": "Title: Self-Monitoring Navigation Agent via Auxiliary Progress Estimation Abstract: The Vision-and-Language Navigation (VLN) task entails an agent following\nnavigational instruction in photo-realistic unknown environments. This\nchallenging task demands that the agent be aware of which instruction was\ncompleted, which instruction is needed next, which way to go, and its\nnavigation progress towards the goal. In this paper, we introduce a\nself-monitoring agent with two complementary components: (1) visual-textual\nco-grounding module to locate the instruction completed in the past, the\ninstruction required for the next action, and the next moving direction from\nsurrounding images and (2) progress monitor to ensure the grounded instruction\ncorrectly reflects the navigation progress. We test our self-monitoring agent\non a standard benchmark and analyze our proposed approach through a series of\nablation studies that elucidate the contributions of the primary components.\nUsing our proposed method, we set the new state of the art by a significant\nmargin (8% absolute increase in success rate on the unseen test set). Code is\navailable at https://github.com/chihyaoma/selfmonitoring-agent . \n\n"}
{"id": "1901.03796", "contents": "Title: Learning Pairwise Relationship for Multi-object Detection in Crowded\n  Scenes Abstract: As the post-processing step for object detection, non-maximum suppression\n(GreedyNMS) is widely used in most of the detectors for many years. It is\nefficient and accurate for sparse scenes, but suffers an inevitable trade-off\nbetween precision and recall in crowded scenes. To overcome this drawback, we\npropose a Pairwise-NMS to cure GreedyNMS. Specifically, a pairwise-relationship\nnetwork that is based on deep learning is learned to predict if two overlapping\nproposal boxes contain two objects or zero/one object, which can handle\nmultiple overlapping objects effectively. Through neatly coupling with\nGreedyNMS without losing efficiency, consistent improvements have been achieved\nin heavily occluded datasets including MOT15, TUD-Crossing and PETS. In\naddition, Pairwise-NMS can be integrated into any learning based detectors\n(Both of Faster-RCNN and DPM detectors are tested in this paper), thus building\na bridge between GreedyNMS and end-to-end learning detectors. \n\n"}
{"id": "1901.03861", "contents": "Title: HorizonNet: Learning Room Layout with 1D Representation and Pano Stretch\n  Data Augmentation Abstract: We present a new approach to the problem of estimating the 3D room layout\nfrom a single panoramic image. We represent room layout as three 1D vectors\nthat encode, at each image column, the boundary positions of floor-wall and\nceiling-wall, and the existence of wall-wall boundary. The proposed network,\nHorizonNet, trained for predicting 1D layout, outperforms previous\nstate-of-the-art approaches. The designed post-processing procedure for\nrecovering 3D room layouts from 1D predictions can automatically infer the room\nshape with low computation cost - it takes less than 20ms for a panorama image\nwhile prior works might need dozens of seconds. We also propose Pano Stretch\nData Augmentation, which can diversify panorama data and be applied to other\npanorama-related learning tasks. Due to the limited data available for\nnon-cuboid layout, we relabel 65 general layout from the current dataset for\nfinetuning. Our approach shows good performance on general layouts by\nqualitative results and cross-validation. \n\n"}
{"id": "1901.03892", "contents": "Title: SteganoGAN: High Capacity Image Steganography with GANs Abstract: Image steganography is a procedure for hiding messages inside pictures. While\nother techniques such as cryptography aim to prevent adversaries from reading\nthe secret message, steganography aims to hide the presence of the message\nitself. In this paper, we propose a novel technique for hiding arbitrary binary\ndata in images using generative adversarial networks which allow us to optimize\nthe perceptual quality of the images produced by our model. We show that our\napproach achieves state-of-the-art payloads of 4.4 bits per pixel, evades\ndetection by steganalysis tools, and is effective on images from multiple\ndatasets. To enable fair comparisons, we have released an open source library\nthat is available online at https://github.com/DAI-Lab/SteganoGAN. \n\n"}
{"id": "1901.04530", "contents": "Title: CrossNet: Latent Cross-Consistency for Unpaired Image Translation Abstract: Recent GAN-based architectures have been able to deliver impressive\nperformance on the general task of image-to-image translation. In particular,\nit was shown that a wide variety of image translation operators may be learned\nfrom two image sets, containing images from two different domains, without\nestablishing an explicit pairing between the images. This was made possible by\nintroducing clever regularizers to overcome the under-constrained nature of the\nunpaired translation problem. In this work, we introduce a novel architecture\nfor unpaired image translation, and explore several new regularizers enabled by\nit. Specifically, our architecture comprises a pair of GANs, as well as a pair\nof translators between their respective latent spaces. These cross-translators\nenable us to impose several regularizing constraints on the learnt image\ntranslation operator, collectively referred to as latent cross-consistency. Our\nresults show that our proposed architecture and latent cross-consistency\nconstraints are able to outperform the existing state-of-the-art on a variety\nof image translation tasks. \n\n"}
{"id": "1901.04584", "contents": "Title: Towards Personalized Management of Type B Aortic Dissection Using STENT:\n  a STandard cta database with annotation of the ENtire aorta and True-false\n  lumen Abstract: Type B Aortic Dissection(TBAD) is a rare aortic disease with a high 5-year\nmortality.Personalized and precise management of TBAD has been increasingly\ndesired in clinic which requires the geometric parameters of TBAD specific to\nthe patient be measured accurately.This remains to be a challenging task for\nvascular surgeons as manual measurement is highly subjective and imprecise. To\nsolve this problem,we introduce STENT-a STandard cta database with annotation\nof the ENtire aorta and True-false lumen. The database contains 274 CT\nangiography (CTA) scans from 274 unique TBAD patients and is split into a\ntraining set(254 cases including 210 preoperative and 44 postoperative scans )\nand a test set(20 cases).Based on STENT,we develop a series of methods\nincluding automated TBAD segmentation and automated measurement of TBAD\nparameters that facilitate personalized and precise management of the disease.\nIn this work, the database and the proposed methods are thoroughly introduced\nand evaluated and the results of our study shows the feasibility and\neffectiveness of our approach to easing the decision-making process for\nvascular surgeons during personalized TBAD management. \n\n"}
{"id": "1901.04604", "contents": "Title: Dual Generator Generative Adversarial Networks for Multi-Domain\n  Image-to-Image Translation Abstract: State-of-the-art methods for image-to-image translation with Generative\nAdversarial Networks (GANs) can learn a mapping from one domain to another\ndomain using unpaired image data. However, these methods require the training\nof one specific model for every pair of image domains, which limits the\nscalability in dealing with more than two image domains. In addition, the\ntraining stage of these methods has the common problem of model collapse that\ndegrades the quality of the generated images. To tackle these issues, we\npropose a Dual Generator Generative Adversarial Network (G$^2$GAN), which is a\nrobust and scalable approach allowing to perform unpaired image-to-image\ntranslation for multiple domains using only dual generators within a single\nmodel. Moreover, we explore different optimization losses for better training\nof G$^2$GAN, and thus make unpaired image-to-image translation with higher\nconsistency and better stability. Extensive experiments on six publicly\navailable datasets with different scenarios, i.e., architectural buildings,\nseasons, landscape and human faces, demonstrate that the proposed G$^2$GAN\nachieves superior model capacity and better generation performance comparing\nwith existing image-to-image translation GAN models. \n\n"}
{"id": "1901.04687", "contents": "Title: URNet : User-Resizable Residual Networks with Conditional Gating Module Abstract: Convolutional Neural Networks are widely used to process spatial scenes, but\ntheir computational cost is fixed and depends on the structure of the network\nused. There are methods to reduce the cost by compressing networks or varying\nits computational path dynamically according to the input image. However, since\na user can not control the size of the learned model, it is difficult to\nrespond dynamically if the amount of service requests suddenly increases. We\npropose User-Resizable Residual Networks (URNet), which allows users to adjust\nthe scale of the network as needed during evaluation. URNet includes\nConditional Gating Module (CGM) that determines the use of each residual block\naccording to the input image and the desired scale. CGM is trained in a\nsupervised manner using the newly proposed scale loss and its corresponding\ntraining methods. URNet can control the amount of computation according to\nuser's demand without degrading the accuracy significantly. It can also be used\nas a general compression method by fixing the scale size during training. In\nthe experiments on ImageNet, URNet based on ResNet-101 maintains the accuracy\nof the baseline even when resizing it to approximately 80% of the original\nnetwork, and demonstrates only about 1% accuracy degradation when using about\n65% of the computation. \n\n"}
{"id": "1901.04988", "contents": "Title: A Survey of FPGA Based Deep Learning Accelerators: Challenges and\n  Opportunities Abstract: With the rapid development of in-depth learning, neural network and deep\nlearning algorithms have been widely used in various fields, e.g., image, video\nand voice processing. However, the neural network model is getting larger and\nlarger, which is expressed in the calculation of model parameters. Although a\nwealth of existing efforts on GPU platforms currently used by researchers for\nimproving computing performance, dedicated hardware solutions are essential and\nemerging to provide advantages over pure software solutions. In this paper, we\nsystematically investigate the neural network accelerator based on FPGA.\nSpecifically, we respectively review the accelerators designed for specific\nproblems, specific algorithms, algorithm features, and general templates. We\nalso compared the design and implementation of the accelerator based on FPGA\nunder different devices and network models and compared it with the versions of\nCPU and GPU. Finally, we present to discuss the advantages and disadvantages of\naccelerators on FPGA platforms and to further explore the opportunities for\nfuture research. \n\n"}
{"id": "1901.05376", "contents": "Title: Joint Spatial and Layer Attention for Convolutional Networks Abstract: In this paper, we propose a novel approach that learns to sequentially attend\nto different Convolutional Neural Networks (CNN) layers (i.e., ``what'' feature\nabstraction to attend to) and different spatial locations of the selected\nfeature map (i.e., ``where'') to perform the task at hand. Specifically, at\neach Recurrent Neural Network (RNN) step, both a CNN layer and localized\nspatial region within it are selected for further processing. We demonstrate\nthe effectiveness of this approach on two computer vision tasks: (i)\nimage-based six degree of freedom camera pose regression and (ii) indoor scene\nclassification. Empirically, we show that combining the ``what'' and ``where''\naspects of attention improves network performance on both tasks. We evaluate\nour method on standard benchmarks for camera localization (Cambridge, 7-Scenes,\nand TUM-LSI) and for scene classification (MIT-67 Indoor Scenes). For camera\nlocalization our approach reduces the median error by 18.8\\% for position and\n8.2\\% for orientation (averaged over all scenes), and for scene classification\nit improves the mean accuracy by 3.4\\% over previous methods. \n\n"}
{"id": "1901.05553", "contents": "Title: Truly Generalizable Radiograph Segmentation with Conditional Domain\n  Adaptation Abstract: Digitization techniques for biomedical images yield different visual patterns\nin radiological exams. These differences may hamper the use of data-driven\napproaches for inference over these images, such as Deep Neural Networks.\nAnother noticeable difficulty in this field is the lack of labeled data, even\nthough in many cases there is an abundance of unlabeled data available.\nTherefore an important step in improving the generalization capabilities of\nthese methods is to perform Unsupervised and Semi-Supervised Domain Adaptation\nbetween different datasets of biomedical images. In order to tackle this\nproblem, in this work we propose an Unsupervised and Semi-Supervised Domain\nAdaptation method for segmentation of biomedical images using Generative\nAdversarial Networks for Unsupervised Image Translation. We merge these\nunsupervised networks with supervised deep semantic segmentation architectures\nin order to create a semi-supervised method capable of learning from both\nunlabeled and labeled data, whenever labeling is available. We compare our\nmethod using several domains, datasets, segmentation tasks and traditional\nbaselines, such as unsupervised distance-based methods and reusing pretrained\nmodels both with and without Fine-tuning. We perform both quantitative and\nqualitative analysis of the proposed method and baselines in the distinct\nscenarios considered in our experimental evaluation. The proposed method shows\nconsistently better results than the baselines in scarce labeled data\nscenarios, achieving Jaccard values greater than 0.9 and good segmentation\nquality in most tasks. Unsupervised Domain Adaptation results were observed to\nbe close to the Fully Supervised Domain Adaptation used in the traditional\nprocedure of Fine-tuning pretrained networks. \n\n"}
{"id": "1901.05590", "contents": "Title: Disentangling Video with Independent Prediction Abstract: We propose an unsupervised variational model for disentangling video into\nindependent factors, i.e. each factor's future can be predicted from its past\nwithout considering the others. We show that our approach often learns factors\nwhich are interpretable as objects in a scene. \n\n"}
{"id": "1901.05880", "contents": "Title: UltraCompression: Framework for High Density Compression of Ultrasound\n  Volumes using Physics Modeling Deep Neural Networks Abstract: Ultrasound image compression by preserving speckle-based key information is a\nchallenging task. In this paper, we introduce an ultrasound image compression\nframework with the ability to retain realism of speckle appearance despite\nachieving very high-density compression factors. The compressor employs a\ntissue segmentation method, transmitting segments along with transducer\nfrequency, number of samples and image size as essential information required\nfor decompression. The decompressor is based on a convolutional network trained\nto generate patho-realistic ultrasound images which convey essential\ninformation pertinent to tissue pathology visible in the images. We demonstrate\ngeneralizability of the building blocks using two variants to build the\ncompressor. We have evaluated the quality of decompressed images using\ndistortion losses as well as perception loss and compared it with other off the\nshelf solutions. The proposed method achieves a compression ratio of $725:1$\nwhile preserving the statistical distribution of speckles. This enables image\nsegmentation on decompressed images to achieve dice score of $0.89 \\pm 0.11$,\nwhich evidently is not so accurately achievable when images are compressed with\ncurrent standards like JPEG, JPEG 2000, WebP and BPG. We envision this frame\nwork to serve as a roadmap for speckle image compression standards. \n\n"}
{"id": "1901.06322", "contents": "Title: Learning Spatial Pyramid Attentive Pooling in Image Synthesis and\n  Image-to-Image Translation Abstract: Image synthesis and image-to-image translation are two important generative\nlearning tasks. Remarkable progress has been made by learning Generative\nAdversarial Networks (GANs)~\\cite{goodfellow2014generative} and\ncycle-consistent GANs (CycleGANs)~\\cite{zhu2017unpaired} respectively. This\npaper presents a method of learning Spatial Pyramid Attentive Pooling (SPAP)\nwhich is a novel architectural unit and can be easily integrated into both\ngenerators and discriminators in GANs and CycleGANs. The proposed SPAP\nintegrates Atrous spatial pyramid~\\cite{chen2018deeplab}, a proposed cascade\nattention mechanism and residual connections~\\cite{he2016deep}. It leverages\nthe advantages of the three components to facilitate effective end-to-end\ngenerative learning: (i) the capability of fusing multi-scale information by\nASPP; (ii) the capability of capturing relative importance between both spatial\nlocations (especially multi-scale context) or feature channels by attention;\n(iii) the capability of preserving information and enhancing optimization\nfeasibility by residual connections. Coarse-to-fine and fine-to-coarse SPAP are\nstudied and intriguing attention maps are observed in both tasks. In\nexperiments, the proposed SPAP is tested in GANs on the Celeba-HQ-128\ndataset~\\cite{karras2017progressive}, and tested in CycleGANs on the\nImage-to-Image translation datasets including the Cityscape\ndataset~\\cite{cordts2016cityscapes}, Facade and Aerial Maps\ndataset~\\cite{zhu2017unpaired}, both obtaining better performance. \n\n"}
{"id": "1901.07031", "contents": "Title: CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and\n  Expert Comparison Abstract: Large, labeled datasets have driven deep learning methods to achieve\nexpert-level performance on a variety of medical imaging tasks. We present\nCheXpert, a large dataset that contains 224,316 chest radiographs of 65,240\npatients. We design a labeler to automatically detect the presence of 14\nobservations in radiology reports, capturing uncertainties inherent in\nradiograph interpretation. We investigate different approaches to using the\nuncertainty labels for training convolutional neural networks that output the\nprobability of these observations given the available frontal and lateral\nradiographs. On a validation set of 200 chest radiographic studies which were\nmanually annotated by 3 board-certified radiologists, we find that different\nuncertainty approaches are useful for different pathologies. We then evaluate\nour best model on a test set composed of 500 chest radiographic studies\nannotated by a consensus of 5 board-certified radiologists, and compare the\nperformance of our model to that of 3 additional radiologists in the detection\nof 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the\nmodel ROC and PR curves lie above all 3 radiologist operating points. We\nrelease the dataset to the public as a standard benchmark to evaluate\nperformance of chest radiograph interpretation models.\n  The dataset is freely available at\nhttps://stanfordmlgroup.github.io/competitions/chexpert . \n\n"}
{"id": "1901.07172", "contents": "Title: Efficient Image Splicing Localization via Contrastive Feature Extraction Abstract: In this work, we propose a new data visualization and clustering technique\nfor discovering discriminative structures in high-dimensional data. This\ntechnique, referred to as cPCA++, utilizes the fact that the interesting\nfeatures of a \"target\" dataset may be obscured by high variance components\nduring traditional PCA. By analyzing what is referred to as a \"background\"\ndataset (i.e., one that exhibits the high variance principal components but not\nthe interesting structures), our technique is capable of efficiently\nhighlighting the structure that is unique to the \"target\" dataset. Similar to\nanother recently proposed algorithm called \"contrastive PCA\" (cPCA), the\nproposed cPCA++ method identifies important dataset specific patterns that are\nnot detected by traditional PCA in a wide variety of settings. However, the\nproposed cPCA++ method is significantly more efficient than cPCA, because it\ndoes not require the parameter sweep in the latter approach. We applied the\ncPCA++ method to the problem of image splicing localization. In this\napplication, we utilize authentic edges as the background dataset and the\nspliced edges as the target dataset. The proposed method is significantly more\nefficient than state-of-the-art methods, as the former does not require\niterative updates of filter weights via stochastic gradient descent and\nbackpropagation, nor the training of a classifier. Furthermore, the cPCA++\nmethod is shown to provide performance scores comparable to the\nstate-of-the-art Multi-task Fully Convolutional Network (MFCN). \n\n"}
{"id": "1901.07355", "contents": "Title: Optical Flow augmented Semantic Segmentation networks for Automated\n  Driving Abstract: Motion is a dominant cue in automated driving systems. Optical flow is\ntypically computed to detect moving objects and to estimate depth using\ntriangulation. In this paper, our motivation is to leverage the existing dense\noptical flow to improve the performance of semantic segmentation. To provide a\nsystematic study, we construct four different architectures which use RGB only,\nflow only, RGBF concatenated and two-stream RGB + flow. We evaluate these\nnetworks on two automotive datasets namely Virtual KITTI and Cityscapes using\nthe state-of-the-art flow estimator FlowNet v2. We also make use of the ground\ntruth optical flow in Virtual KITTI to serve as an ideal estimator and a\nstandard Farneback optical flow algorithm to study the effect of noise. Using\nthe flow ground truth in Virtual KITTI, two-stream architecture achieves the\nbest results with an improvement of 4% IoU. As expected, there is a large\nimprovement for moving objects like trucks, vans and cars with 38%, 28% and 6%\nincrease in IoU. FlowNet produces an improvement of 2.4% in average IoU with\nlarger improvement in the moving objects corresponding to 26%, 11% and 5% in\ntrucks, vans and cars. In Cityscapes, flow augmentation provided an improvement\nfor moving objects like motorcycle and train with an increase of 17% and 7% in\nIoU. \n\n"}
{"id": "1901.08933", "contents": "Title: Self-Supervised Generalisation with Meta Auxiliary Learning Abstract: Learning with auxiliary tasks can improve the ability of a primary task to\ngeneralise. However, this comes at the cost of manually labelling auxiliary\ndata. We propose a new method which automatically learns appropriate labels for\nan auxiliary task, such that any supervised learning task can be improved\nwithout requiring access to any further data. The approach is to train two\nneural networks: a label-generation network to predict the auxiliary labels,\nand a multi-task network to train the primary task alongside the auxiliary\ntask. The loss for the label-generation network incorporates the loss of the\nmulti-task network, and so this interaction between the two networks can be\nseen as a form of meta learning with a double gradient. We show that our\nproposed method, Meta AuXiliary Learning (MAXL), outperforms single-task\nlearning on 7 image datasets, without requiring any additional data. We also\nshow that MAXL outperforms several other baselines for generating auxiliary\nlabels, and is even competitive when compared with human-defined auxiliary\nlabels. The self-supervised nature of our method leads to a promising new\ndirection towards automated generalisation. Source code can be found at\nhttps://github.com/lorenmt/maxl. \n\n"}
{"id": "1901.08971", "contents": "Title: FaceForensics++: Learning to Detect Manipulated Facial Images Abstract: The rapid progress in synthetic image generation and manipulation has now\ncome to a point where it raises significant concerns for the implications\ntowards society. At best, this leads to a loss of trust in digital content, but\ncould potentially cause further harm by spreading false information or fake\nnews. This paper examines the realism of state-of-the-art image manipulations,\nand how difficult it is to detect them, either automatically or by humans. To\nstandardize the evaluation of detection methods, we propose an automated\nbenchmark for facial manipulation detection. In particular, the benchmark is\nbased on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent\nrepresentatives for facial manipulations at random compression level and size.\nThe benchmark is publicly available and contains a hidden test set as well as a\ndatabase of over 1.8 million manipulated images. This dataset is over an order\nof magnitude larger than comparable, publicly available, forgery datasets.\nBased on this data, we performed a thorough analysis of data-driven forgery\ndetectors. We show that the use of additional domainspecific knowledge improves\nforgery detection to unprecedented accuracy, even in the presence of strong\ncompression, and clearly outperforms human observers. \n\n"}
{"id": "1901.09321", "contents": "Title: Fixup Initialization: Residual Learning Without Normalization Abstract: Normalization layers are a staple in state-of-the-art deep neural network\narchitectures. They are widely believed to stabilize training, enable higher\nlearning rate, accelerate convergence and improve generalization, though the\nreason for their effectiveness is still an active research topic. In this work,\nwe challenge the commonly-held beliefs by showing that none of the perceived\nbenefits is unique to normalization. Specifically, we propose fixed-update\ninitialization (Fixup), an initialization motivated by solving the exploding\nand vanishing gradient problem at the beginning of training via properly\nrescaling a standard initialization. We find training residual networks with\nFixup to be as stable as training with normalization -- even for networks with\n10,000 layers. Furthermore, with proper regularization, Fixup enables residual\nnetworks without normalization to achieve state-of-the-art performance in image\nclassification and machine translation. \n\n"}
{"id": "1901.09878", "contents": "Title: CapsAttacks: Robust and Imperceptible Adversarial Attacks on Capsule\n  Networks Abstract: Capsule Networks preserve the hierarchical spatial relationships between\nobjects, and thereby bears a potential to surpass the performance of\ntraditional Convolutional Neural Networks (CNNs) in performing tasks like image\nclassification. A large body of work has explored adversarial examples for\nCNNs, but their effectiveness on Capsule Networks has not yet been well\nstudied. In our work, we perform an analysis to study the vulnerabilities in\nCapsule Networks to adversarial attacks. These perturbations, added to the test\ninputs, are small and imperceptible to humans, but can fool the network to\nmispredict. We propose a greedy algorithm to automatically generate targeted\nimperceptible adversarial examples in a black-box attack scenario. We show that\nthis kind of attacks, when applied to the German Traffic Sign Recognition\nBenchmark (GTSRB), mislead Capsule Networks. Moreover, we apply the same kind\nof adversarial attacks to a 5-layer CNN and a 9-layer CNN, and analyze the\noutcome, compared to the Capsule Networks to study differences in their\nbehavior. \n\n"}
{"id": "1901.10271", "contents": "Title: Combined tract segmentation and orientation mapping for bundle-specific\n  tractography Abstract: While the major white matter tracts are of great interest to numerous studies\nin neuroscience and medicine, their manual dissection in larger cohorts from\ndiffusion MRI tractograms is time-consuming, requires expert knowledge and is\nhard to reproduce. In previous work we presented tract orientation mapping\n(TOM) as a novel concept for bundle-specific tractography. It is based on a\nlearned mapping from the original fiber orientation distribution function (FOD)\npeaks to tract specific peaks, called tract orientation maps. Each tract\norientation map represents the voxel-wise principal orientation of one tract.\nHere, we present an extension of this approach that combines TOM with accurate\nsegmentations of the tract outline and its start and end region. We also\nintroduce a custom probabilistic tracking algorithm that samples from a\nGaussian distribution with fixed standard deviation centered on each peak thus\nenabling more complete trackings on the tract orientation maps than\ndeterministic tracking. These extensions enable the automatic creation of\nbundle-specific tractograms with previously unseen accuracy. We show for 72\ndifferent bundles on high quality, low quality and phantom data that our\napproach runs faster and produces more accurate bundle-specific tractograms\nthan 7 state of the art benchmark methods while avoiding cumbersome processing\nsteps like whole brain tractography, non-linear registration, clustering or\nmanual dissection. Moreover, we show on 17 datasets that our approach\ngeneralizes well to datasets acquired with different scanners and settings as\nwell as with pathologies. The code of our method is openly available at\nhttps://github.com/MIC-DKFZ/TractSeg. \n\n"}
{"id": "1901.10345", "contents": "Title: Quality Measures for Speaker Verification with Short Utterances Abstract: The performances of the automatic speaker verification (ASV) systems degrade\ndue to the reduction in the amount of speech used for enrollment and\nverification. Combining multiple systems based on different features and\nclassifiers considerably reduces speaker verification error rate with short\nutterances. This work attempts to incorporate supplementary information during\nthe system combination process. We use quality of the estimated model\nparameters as supplementary information. We introduce a class of novel quality\nmeasures formulated using the zero-order sufficient statistics used during the\ni-vector extraction process. We have used the proposed quality measures as side\ninformation for combining ASV systems based on Gaussian mixture model-universal\nbackground model (GMM-UBM) and i-vector. The proposed methods demonstrate\nconsiderable improvement in speaker recognition performance on NIST SRE\ncorpora, especially in short duration conditions. We have also observed\nimprovement over existing systems based on different duration-based quality\nmeasures. \n\n"}
{"id": "1901.10431", "contents": "Title: Influence of segmentation on deep iris recognition performance Abstract: Despite the rise of deep learning in numerous areas of computer vision and\nimage processing, iris recognition has not benefited considerably from these\ntrends so far. Most of the existing research on deep iris recognition is\nfocused on new models for generating discriminative and robust iris\nrepresentations and relies on methodologies akin to traditional iris\nrecognition pipelines. Hence, the proposed models do not approach iris\nrecognition in an end-to-end manner, but rather use standard heuristic iris\nsegmentation (and unwrapping) techniques to produce normalized inputs for the\ndeep learning models. However, because deep learning is able to model very\ncomplex data distributions and nonlinear data changes, an obvious question\narises. How important is the use of traditional segmentation methods in a deep\nlearning setting? To answer this question, we present in this paper an\nempirical analysis of the impact of iris segmentation on the performance of\ndeep learning models using a simple two stage pipeline consisting of a\nsegmentation and a recognition step. We evaluate how the accuracy of\nsegmentation influences recognition performance but also examine if\nsegmentation is needed at all. We use the CASIA Thousand and SBVPI datasets for\nthe experiments and report several interesting findings. \n\n"}
{"id": "1901.10713", "contents": "Title: A Mobile Robot Generating Video Summaries of Seniors' Indoor Activities Abstract: We develop a system which generates summaries from seniors' indoor-activity\nvideos captured by a social robot to help remote family members know their\nseniors' daily activities at home. Unlike the traditional video summarization\ndatasets, indoor videos captured from a moving robot poses additional\nchallenges, namely, (i) the video sequences are very long (ii) a significant\nnumber of video-frames contain no-subject or with subjects at ill-posed\nlocations and scales (iii) most of the well-posed frames contain highly\nredundant information. To address this problem, we propose to \\hl{exploit} pose\nestimation \\hl{for detecting} people in frames\\hl{. This guides the robot} to\nfollow the user and capture effective videos. We use person identification to\ndistinguish a target senior from other people. We \\hl{also make use of} action\nrecognition to analyze seniors' major activities at different moments, and\ndevelop a video summarization method to select diverse and representative\nkeyframes as summaries. \n\n"}
{"id": "1901.11379", "contents": "Title: TUNet: Incorporating segmentation maps to improve classification Abstract: Determining the localization of specific protein in human cells is important\nfor understanding cellular functions and biological processes of underlying\ndiseases. Among imaging techniques, high-throughput fluorescence microscopy\nimaging is an efficient biotechnology to stain the protein of interest in a\ncell. In this work, we present a novel classification model Twin U-Net (TUNet)\nfor processing and classifying the belonging of protein in the Atlas images.\nSeveral notable Deep Learning models including GoogleNet and Resnet have been\nemployed for comparison. Results have shown that our system obtaining\ncompetitive performance. \n\n"}
{"id": "1901.11382", "contents": "Title: Learning to Clean: A GAN Perspective Abstract: In the big data era, the impetus to digitize the vast reservoirs of data\ntrapped in unstructured scanned documents such as invoices, bank documents and\ncourier receipts has gained fresh momentum. The scanning process often results\nin the introduction of artifacts such as background noise, blur due to camera\nmotion, watermarkings, coffee stains, or faded text. These artifacts pose many\nreadability challenges to current text recognition algorithms and significantly\ndegrade their performance. Existing learning based denoising techniques require\na dataset comprising of noisy documents paired with cleaned versions. In such\nscenarios, a model can be trained to generate clean documents from noisy\nversions. However, very often in the real world such a paired dataset is not\navailable, and all we have for training our denoising model are unpaired sets\nof noisy and clean images. This paper explores the use of GANs to generate\ndenoised versions of the noisy documents. In particular, where paired\ninformation is available, we formulate the problem as an image-to-image\ntranslation task i.e, translating a document from noisy domain ( i.e.,\nbackground noise, blurred, faded, watermarked ) to a target clean document\nusing Generative Adversarial Networks (GAN). However, in the absence of paired\nimages for training, we employed CycleGAN which is known to learn a mapping\nbetween the distributions of the noisy images to the denoised images using\nunpaired data to achieve image-to-image translation for cleaning the noisy\ndocuments. We compare the performance of CycleGAN for document cleaning tasks\nusing unpaired images with a Conditional GAN trained on paired data from the\nsame dataset. Experiments were performed on a public document dataset on which\ndifferent types of noise were artificially induced, results demonstrate that\nCycleGAN learns a more robust mapping from the space of noisy to clean\ndocuments. \n\n"}
{"id": "cs/0409031", "contents": "Title: Field Geology with a Wearable Computer: First Results of the Cyborg\n  Astrobiologist System Abstract: We present results from the first geological field tests of the `Cyborg\nAstrobiologist', which is a wearable computer and video camcorder system that\nwe are using to test and train a computer-vision system towards having some of\nthe autonomous decision-making capabilities of a field-geologist. The Cyborg\nAstrobiologist platform has thus far been used for testing and development of\nthese algorithms and systems: robotic acquisition of quasi-mosaics of images,\nreal-time image segmentation, and real-time determination of interesting points\nin the image mosaics. The hardware and software systems function reliably, and\nthe computer-vision algorithms are adequate for the first field tests. In\naddition to the proof-of-concept aspect of these field tests, the main result\nof these field tests is the enumeration of those issues that we can improve in\nthe future, including: dealing with structural shadow and microtexture, and\nalso, controlling the camera's zoom lens in an intelligent manner. Nonetheless,\ndespite these and other technical inadequacies, this Cyborg Astrobiologist\nsystem, consisting of a camera-equipped wearable-computer and its\ncomputer-vision algorithms, has demonstrated its ability of finding genuinely\ninteresting points in real-time in the geological scenery, and then gathering\nmore information about these interest points in an automated manner. \n\n"}
{"id": "cs/0506089", "contents": "Title: Field geology with a wearable computer: 1st results of the Cyborg\n  Astrobiologist System Abstract: We present results from the first geological field tests of the `Cyborg\nAstrobiologist', which is a wearable computer and video camcorder system that\nwe are using to test and train a computer-vision system towards having some of\nthe autonomous decision-making capabilities of a field-geologist. The Cyborg\nAstrobiologist platform has thus far been used for testing and development of\nthese algorithms and systems: robotic acquisition of quasi-mosaics of images,\nreal-time image segmentation, and real-time determination of interesting points\nin the image mosaics. This work is more of a test of the whole system, rather\nthan of any one part of the system. However, beyond the concept of the system\nitself, the uncommon map (despite its simplicity) is the main innovative part\nof the system. The uncommon map helps to determine interest-points in a\ncontext-free manner. Overall, the hardware and software systems function\nreliably, and the computer-vision algorithms are adequate for the first field\ntests. In addition to the proof-of-concept aspect of these field tests, the\nmain result of these field tests is the enumeration of those issues that we can\nimprove in the future, including: dealing with structural shadow and\nmicrotexture, and also, controlling the camera's zoom lens in an intelligent\nmanner. Nonetheless, despite these and other technical inadequacies, this\nCyborg Astrobiologist system, consisting of a camera-equipped wearable-computer\nand its computer-vision algorithms, has demonstrated its ability of finding\ngenuinely interesting points in real-time in the geological scenery, and then\ngathering more information about these interest points in an automated manner.\nWe use these capabilities for autonomous guidance towards geological\npoints-of-interest. \n\n"}
{"id": "quant-ph/9802028", "contents": "Title: Analogue Quantum Computers for Data Analysis Abstract: Analogue computers use continuous properties of physical system for modeling.\nIn the paper is described possibility of modeling by analogue quantum computers\nfor some model of data analysis. It is analogue associative memory and a formal\nneural network. A particularity of the models is combination of continuous\ninternal processes with discrete set of output states. The modeling of the\nsystem by classical analogue computers was offered long times ago, but now it\nis not very effectively in comparison with modern digital computers. The\napplication of quantum analogue modelling looks quite possible for modern level\nof technology and it may be more effective than digital one, because number of\nelement may be about Avogadro number (N=6.0E23). \n\n"}
