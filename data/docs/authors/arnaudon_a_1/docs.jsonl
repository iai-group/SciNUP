{"id": "0704.1779", "contents": "Title: Reciprocal transformations and flat metrics on Hurwitz spaces Abstract: We consider hydrodynamic systems which possess a local Hamiltonian structure\nof Dubrovin-Novikov type. To such a system there are also associated an\ninfinite number of nonlocal Hamiltonian structures. We give necessary and\nsufficient conditions so that, after a nonlinear transformation of the\nindependent variables, the reciprocal system still possesses a local\nHamiltonian structure of Dubrovin-Novikov type. We show that, under our\nhypotheses, bi-hamiltonicity is preserved by the reciprocal transformation.\nFinally we apply such results to reciprocal systems of genus g Whitham-KdV\nmodulation equations. \n\n"}
{"id": "0705.0475", "contents": "Title: Production of trans-Neptunian binaries through chaos-assisted capture Abstract: The recent discovery of binary objects in the Kuiper-belt opens an invaluable\nwindow into past and present conditions in the trans-Neptunian part of the\nSolar System. For example, knowledge of how these objects formed can be used to\nimpose constraints on planetary formation theories. We have recently proposed a\nbinary-object formation model based on the notion of chaos-assisted capture.\nHere we present a more detailed analysis with calculations performed in the\nspatial (three-dimensional) three- and four-body Hill approximations. It is\nassumed that the potential binary partners are initially following heliocentric\nKeplerian orbits and that their relative motion becomes perturbed as these\nobjects undergo close encounters. First, the mass, velocity, and orbital\nelement distribu- tions which favour binary formation are identified in the\ncircular and elliptical Hill limits. We then consider intruder scattering in\nthe circular Hill four-body problem and find that the chaos-assisted capture\nmechanism is consistent with observed, apparently randomly distributed, binary\nmutual orbit inclinations. It also predicts asymmetric distributions of\nretrograde versus prograde orbits. The time-delay induced by chaos on particle\ntransport through the Hill sphere is analogous to the formation of a resonance\nin a chemical reaction. Implications for binary formation rates are considered\nand the 'fine-tuning' problem recently identified by Noll et al. (2007) is also\naddressed. \n\n"}
{"id": "0705.3294", "contents": "Title: Shear-Induced Chaos Abstract: Guided by a geometric understanding developed in earlier works of Wang and\nYoung, we carry out some numerical studies of shear-induced chaos. The settings\nconsidered include periodic kicking of limit cycles, random kicks at Poisson\ntimes, and continuous-time driving by white noise. The forcing of a\nquasi-periodic model describing two coupled oscillators is also investigated.\nIn all cases, positive Lyapunov exponents are found in suitable parameter\nranges when the forcing is suitably directed. \n\n"}
{"id": "0706.0077", "contents": "Title: A discrete time neural network model with spiking neurons. Rigorous\n  results on the spontaneous dynamics Abstract: We derive rigorous results describing the asymptotic dynamics of a discrete\ntime model of spiking neurons introduced in \\cite{BMS}. Using symbolic dynamic\ntechniques we show how the dynamics of membrane potential has a one to one\ncorrespondence with sequences of spikes patterns (``raster plots''). Moreover,\nthough the dynamics is generically periodic, it has a weak form of initial\nconditions sensitivity due to the presence of a sharp threshold in the model\ndefinition. As a consequence, the model exhibits a dynamical regime\nindistinguishable from chaos in numerical experiments. \n\n"}
{"id": "0706.2428", "contents": "Title: Multi-Hamiltonian structure for the finite defocusing Ablowitz-Ladik\n  equation Abstract: We study the Poisson structure associated to the defocusing Ablowitz-Ladik\nequation from a functional-analytical point of view, by reexpressing the\nPoisson bracket in terms of the associated Caratheodory function. Using this\nexpression, we are able to introduce a family of compatible Poisson brackets\nwhich form a multi-Hamiltonian structure for the Ablowitz-Ladik equation.\nFurthermore, we show using some of these new Poisson brackets that the\nGeronimus relations between orthogonal polynomials on the unit circle and those\non the interval define an algebraic and symplectic mapping between the\nAblowitz-Ladik and Toda hierarchies. \n\n"}
{"id": "0706.3810", "contents": "Title: The Camassa-Holm equation as a geodesic flow for the $H^1$\n  right-invariant metric Abstract: The fundamental role played by the Lie groups in mechanics, and especially by\nthe dual space of the Lie algebra of the group and the coadjoint action are\nillustrated through the Camassa-Holm equation (CH). In 1996 Misio{\\l}ek\nobserved that CH is a geodesic flow equation on the group of diffeomorphisms,\npreserving the $H^1$ metric. This example is analogous to the Euler equations\nin hydrodynamics, which describe geodesic flow for a right-invariant metric on\nthe infinite-dimensional group of diffeomorphisms preserving the volume element\nof the domain of fluid flow and to the Euler equations of rigid body whith a\nfixed point, describing geodesics for a left-invariant metric on SO(3). The\nmomentum map and an explicit parametrization of the Virasoro group, related to\nrecently obtained solutions for the CH equation are presented. \n\n"}
{"id": "0707.1084", "contents": "Title: Integrable discrete systems on R and related dispersionless systems Abstract: The general framework for integrable discrete systems on R in particular\ncontaining lattice soliton systems and their q-deformed analogues is presented.\nThe concept of regular grain structures on R, generated by discrete\none-parameter groups of diffeomorphisms, through which one can define algebras\nof shift operators is introduced. Two integrable hierarchies of discrete chains\ntogether with bi-Hamiltonian structures are constructed. Their continuous limit\nand the inverse problem based on the deformation quantization scheme are\nconsidered. \n\n"}
{"id": "0707.4088", "contents": "Title: An integrable discretization of the rational su(2) Gaudin model and\n  related systems Abstract: The first part of the present paper is devoted to a systematic construction\nof continuous-time finite-dimensional integrable systems arising from the\nrational su(2) Gaudin model through certain contraction procedures. In the\nsecond part, we derive an explicit integrable Poisson map discretizing a\nparticular Hamiltonian flow of the rational su(2) Gaudin model. Then, the\ncontraction procedures enable us to construct explicit integrable\ndiscretizations of the continuous systems derived in the first part of the\npaper. \n\n"}
{"id": "0709.4370", "contents": "Title: On Dynamics of Integrate-and-Fire Neural Networks with Conductance Based\n  Synapses Abstract: We present a mathematical analysis of a networks with Integrate-and-Fire\nneurons and adaptive conductances. Taking into account the realistic fact that\nthe spike time is only known within some \\textit{finite} precision, we propose\na model where spikes are effective at times multiple of a characteristic time\nscale $\\delta$, where $\\delta$ can be \\textit{arbitrary} small (in particular,\nwell beyond the numerical precision). We make a complete mathematical\ncharacterization of the model-dynamics and obtain the following results. The\nasymptotic dynamics is composed by finitely many stable periodic orbits, whose\nnumber and period can be arbitrary large and can diverge in a region of the\nsynaptic weights space, traditionally called the \"edge of chaos\", a notion\nmathematically well defined in the present paper. Furthermore, except at the\nedge of chaos, there is a one-to-one correspondence between the membrane\npotential trajectories and the raster plot. This shows that the neural code is\nentirely \"in the spikes\" in this case. As a key tool, we introduce an order\nparameter, easy to compute numerically, and closely related to a natural notion\nof entropy, providing a relevant characterization of the computational\ncapabilities of the network. This allows us to compare the computational\ncapabilities of leaky and Integrate-and-Fire models and conductance based\nmodels. The present study considers networks with constant input, and without\ntime-dependent plasticity, but the framework has been designed for both\nextensions. \n\n"}
{"id": "0710.3464", "contents": "Title: Mathematical remarks on transcritical bifurcation in Hamiltonian systems Abstract: This article is meant as a mathematical appendix or comment on [BT]. We first\nconsider the notion of transcritical bifurcations of fixed points of general\narea-preserving maps, and then adress some questions related to [BT] on\nbifurcation in Poincar\\'e maps of 2-dimensional Hamiltonian systems.\n  [BT] M. Brack and K. Tanaka, arXiv:0705.0753 \n\n"}
{"id": "0710.3466", "contents": "Title: Bifurcation of straight-line librations Abstract: We study a class of 2-dimensional Hamiltonian systems\n$H(x,y,p_x,p_y)=\\frac12(p_x^2+p_y^2) +V(x,y)$ in which the plane $x$=$p_x$=0 is\ninvariant under the Hamiltonian flow, so that straight-line librations along\nthe y axis exist, and we also consider perturbations $\\delta H=\\delta\\cdot\nF(x,y,p_x,p_y)$ that preserve these librations. We describe a procedure for the\nanalytical calculation of partial derivatives of the Poincar\\'e map. These\npartial derivatives can be used to predict the bifurcation behavior of the\nlibration, in particular to distinguish between transcritical and fork-like\nbifurcations, as was mathematically investigated in [1] and numerically studied\nin [2].\n  [1] K. J\\\"anich, arXiv.org/abs/0710.3464 [2] M. Brack and K. Tanaka,\narXiv:0705.0753 \n\n"}
{"id": "0711.2793", "contents": "Title: Long-Time Asymptotics for the Toda Lattice in the Soliton Region Abstract: We apply the method of nonlinear steepest descent to compute the long-time\nasymptotics of the Toda lattice for decaying initial data in the soliton\nregion. In addition, we point out how to reduce the problem in the remaining\nregion to the known case without solitons. \n\n"}
{"id": "0802.0184", "contents": "Title: Classical R-Operators and Integrable Generalizations of Thirring\n  Equations Abstract: We construct different integrable generalizations of the massive Thirring\nequations corresponding loop algebras $\\widetilde{\\mathfrak{g}}^{\\sigma}$ in\ndifferent gradings and associated ''triangular'' $R$-operators. We consider the\nmost interesting cases connected with the Coxeter automorphisms, second order\nautomorphisms and with ''Kostant-Adler-Symes'' $R$-operators. We recover a\nknown matrix generalization of the complex Thirring equations as a partial case\nof our construction. \n\n"}
{"id": "0802.4032", "contents": "Title: Global action-angle variables for the periodic Toda lattice Abstract: In this paper we construct global action-angle variables for the periodic\nToda lattice. \n\n"}
{"id": "0803.1439", "contents": "Title: R-matrix approach to integrable systems on time scales Abstract: A general unifying framework for integrable soliton-like systems on time\nscales is introduced. The $R$-matrix formalism is applied to the algebra of\n$\\delta$-differential operators in terms of which one can construct infinite\nhierarchy of commuting vector fields. The theory is illustrated by two\ninfinite-field integrable hierarchies on time scales which are difference\ncounterparts of KP and mKP. The difference counterparts of AKNS and Kaup-Broer\nsoliton systems are constructed as related finite-field restrictions. \n\n"}
{"id": "0804.4693", "contents": "Title: Long-Time Asymptotics of the Toda Lattice for Decaying Initial Data\n  Revisited Abstract: The purpose of this article is to give a streamlined and self-contained\ntreatment of the long-time asymptotics of the Toda lattice for decaying initial\ndata in the soliton and in the similarity region via the method of nonlinear\nsteepest descent. \n\n"}
{"id": "0805.3477", "contents": "Title: Boundaries of Siegel disks - numerical studies of their dynamics and\n  regularity Abstract: Siegel disks are domains around fixed points of holomorphic maps in which the\nmaps are locally linearizable (i.e., become a rotation under an appropriate\nchange of coordinates which is analytic in a neighborhood of the origin). The\ndynamical behavior of the iterates of the map on the boundary of the Siegel\ndisk exhibits strong scaling properties which have been intensively studied in\nthe physical and mathematical literature.\n  In the cases we study, the boundary of the Siegel disk is a Jordan curve\ncontaining a critical point of the map (we consider critical maps of different\norders), and there exists a natural parameterization which transforms the\ndynamics on the boundary into a rotation. We compute numerically this\nparameterization and use methods of harmonic analysis to compute the global\nHolder regularity of the parameterization for different maps and rotation\nnumbers.\n  We obtain that the regularity of the boundaries and the scaling exponents are\nuniversal numbers in the sense of renormalization theory (i.e., they do not\ndepend on the map when the map ranges in an open set), and only depend on the\norder of the critical point of the map in the boundary of the Siegel disk and\nthe tail of the continued function expansion of the rotation number. We also\ndiscuss some possible relations between the regularity of the parameterization\nof the boundaries and the corresponding scaling exponents. \n\n"}
{"id": "0807.1294", "contents": "Title: Central extensions of cotangent universal hierarchy: (2+1)-dimensional\n  bi-Hamiltonian systems Abstract: We introduce the cotangent universal hierarchy that extends the so-called\nuniversal hierarchy (as for the latter, see e.g. arXiv:nlin/0202008,\narXiv:nlin/0312043 and arXiv:nlin/0310036). Then we construct a\n(2+1)-dimensional double central extension of the cotangent universal hierarchy\nand show that this extension is bi-Hamiltonian. This yields, as a byproduct,\nthe central extension of the original universal hierarchy. \n\n"}
{"id": "0810.0280", "contents": "Title: Random matrices with external source and KP $\\tau$ functions Abstract: In this paper we prove that the partition function in the random matrix model\nwith external source is a KP $\\tau$ function. \n\n"}
{"id": "0812.2622", "contents": "Title: Multi-Cuts Solutions of Laplacian Growth Abstract: A new class of solutions to Laplacian growth with zero surface tension is\npresented and shown to contain all other known solutions as special or limiting\ncases. These solutions, which are time-dependent conformal maps with branch\ncuts inside the unit circle, are governed by a nonlinear integral equation and\ndescribe oil fjords with non-parallel walls in viscous fingering experiments in\nHele-Shaw cells. Integrals of motion for the multi-cut Laplacian growth\nsolutions in terms of singularities of the Schwarz function are found, and the\ndynamics of densities (jumps) on the cuts are derived. The subclass of these\nsolutions with linear Cauchy densities on the cuts of the Schwarz function is\nof particular interest, because in this case the integral equation for the\nconformal map becomes linear. These solutions can also be of physical\nimportance by representing oil/air interfaces, which form oil fjords with a\nconstant opening angle, in accordance with recent experiments in a Hele-shaw\ncell. \n\n"}
{"id": "0904.2620", "contents": "Title: Soliton Solutions of the KP Equation with V-Shape Initial Waves Abstract: We consider the initial value problems of the Kadomtsev-Petviashvili (KP)\nequation for symmetric V-shape initial waves consisting of two semi-infinite\nline solitons with the same amplitude. Numerical simulations show that the\nsolutions of the initial value problem approach asymptotically to certain exact\nsolutions of the KP equation found recently in [Chakravarty and Kodama, JPA, 41\n(2008) 275209]. We then use a chord diagram to explain the asymptotic result.\nWe also demonstrate a real experiment of shallow water wave which may represent\nthe solution discussed in this Letter. \n\n"}
{"id": "0906.1410", "contents": "Title: Level Set Structure of an Integrable Cellular Automaton Abstract: Based on a group theoretical setting a sort of discrete dynamical system is\nconstructed and applied to a combinatorial dynamical system defined on the set\nof certain Bethe ansatz related objects known as the rigged configurations.\nThis system is then used to study a one-dimensional periodic cellular automaton\nrelated to discrete Toda lattice. It is shown for the first time that the level\nset of this cellular automaton is decomposed into connected components and\nevery such component is a torus. \n\n"}
{"id": "0908.1800", "contents": "Title: The discrete potential Boussinesq equation and its multisoliton\n  solutions Abstract: An alternate form of discrete potential Boussinesq equation is proposed and\nits multisoliton solutions are constructed. An ultradiscrete potential\nBoussinesq equation is also obtained from the discrete potential Boussinesq\nequation using the ultradiscretization technique. The detail of the\nmultisoliton solutions is discussed by using the reduction technique. The\nlattice potential Boussinesq equation derived by Nijhoff et al. is also\ninvestigated by using the singularity confinement test. The relation between\nthe proposed alternate discrete potential Boussinesq equation and the lattice\npotential Boussinesq equation by Nijhoff et al. is clarified. \n\n"}
{"id": "0912.1914", "contents": "Title: Integrable discretizations of the short pulse equation Abstract: In the present paper, we propose integrable semi-discrete and full-discrete\nanalogues of the short pulse (SP) equation. The key of the construction is the\nbilinear forms and determinant structure of solutions of the SP equation. We\nalso give the determinant formulas of N-soliton solutions of the semi-discrete\nand full-discrete analogues of the SP equations, from which the multi-loop and\nmulti-breather solutions can be generated. In the continuous limit, the\nfull-discrete SP equation converges to the semi-discrete SP equation, then to\nthe continuous SP equation. Based on the semi-discrete SP equation, an\nintegrable numerical scheme, i.e., a self-adaptive moving mesh scheme, is\nproposed and used for the numerical computation of the short pulse equation. \n\n"}
{"id": "0912.2810", "contents": "Title: On Degenerate Planar Hopf Bifurcations Abstract: Our concern is the study of degenerate Hopf bifurcation of smooth planar\ndynamical systems near isolated singular points. To do so, we propose to split\nup the definition of degeneracy into two types. Degeneracy of first kind shall\nmeans that no limit cycle surrounding the steady state can emerge after or\nbefore the critical point, with the possible emergence of limit cycles\nsurrounding the point at infinity. Degeneracy of second kind shall means that\neither several limit cycles or semistable cycles as a limiting case, emerge\nsurrounding the steady state super or subcritically. In degenerate bifurcation\nof second kind we also show that the radius of the emerging cycle tends to zero\nwith an \"anomalous\" order as the bifurcation parameter tends to the critical\nvalue. Finally, we give a sufficient condition for degenerate bifurcations of\nsecond kind up to 6-jet-equivalence, and show some \"typical\" forms for\ndegenerate bifurcations. \n\n"}
{"id": "0912.4571", "contents": "Title: Fast Alternating Linearization Methods for Minimizing the Sum of Two\n  Convex Functions Abstract: We present in this paper first-order alternating linearization algorithms\nbased on an alternating direction augmented Lagrangian approach for minimizing\nthe sum of two convex functions. Our basic methods require at most\n$O(1/\\epsilon)$ iterations to obtain an $\\epsilon$-optimal solution, while our\naccelerated (i.e., fast) versions of them require at most\n$O(1/\\sqrt{\\epsilon})$ iterations, with little change in the computational\neffort required at each iteration. For both types of methods, we present one\nalgorithm that requires both functions to be smooth with Lipschitz continuous\ngradients and one algorithm that needs only one of the functions to be so.\nAlgorithms in this paper are Gauss-Seidel type methods, in contrast to the ones\nproposed by Goldfarb and Ma in [21] where the algorithms are Jacobi type\nmethods. Numerical results are reported to support our theoretical conclusions\nand demonstrate the practical potential of our algorithms. \n\n"}
{"id": "1004.1627", "contents": "Title: Bidifferential Calculus Approach to AKNS Hierarchies and Their Solutions Abstract: We express AKNS hierarchies, admitting reductions to matrix NLS and matrix\nmKdV hierarchies, in terms of a bidifferential graded algebra. Application of a\nuniversal result in this framework quickly generates an infinite family of\nexact solutions, including e.g. the matrix solitons in the focusing NLS case.\nExploiting a general Miura transformation, we recover the generalized\nHeisenberg magnet hierarchy and establish a corresponding solution formula for\nit. Simply by exchanging the roles of the two derivations of the bidifferential\ngraded algebra, we recover \"negative flows\", leading to an extension of the\nrespective hierarchy. In this way we also meet a matrix and vector version of\nthe short pulse equation and also the sine-Gordon equation. For these equations\ncorresponding solution formulas are also derived. In all these cases the\nsolutions are parametrized in terms of matrix data that have to satisfy a\ncertain Sylvester equation. \n\n"}
{"id": "1004.3854", "contents": "Title: Necessary conditions for classical super-integrability of a certain\n  family of potentials in constant curvature spaces Abstract: We formulate the necessary conditions for the maximal super-integrability of\na certain family of classical potentials defined in the constant curvature\ntwo-dimensional spaces. We give examples of homogeneous potentials of degree -2\non $E^2$ as well as their equivalents on $S^2$ and $H^2$ for which these\nnecessary conditions are also sufficient. We show explicit forms of the\nadditional first integrals which always can be chosen polynomial with respect\nto the momenta and which can be of an arbitrary high degree with respect to the\nmomenta. \n\n"}
{"id": "1005.2160", "contents": "Title: On complete integrability of the Mikhailov-Novikov-Wang system Abstract: We obtain compatible Hamiltonian and symplectic structure for a new\ntwo-component fifth-order integrable system recently found by Mikhailov,\nNovikov and Wang (arXiv:0712.1972), and show that this system possesses a\nhereditary recursion operator and infinitely many commuting symmetries and\nconservation laws, as well as infinitely many compatible Hamiltonian and\nsymplectic structures, and is therefore completely integrable. The system in\nquestion admits a reduction to the Kaup--Kupershmidt equation. \n\n"}
{"id": "1006.0301", "contents": "Title: Multi-Component NLS Models on Symmetric Spaces: Spectral Properties\n  versus Representations Theory Abstract: The algebraic structure and the spectral properties of a special class of\nmulti-component NLS equations, related to the symmetric spaces of {\\bf\nBD.I}-type are analyzed. The focus of the study is on the spectral theory of\nthe relevant Lax operators for different fundamental representations of the\nunderlying simple Lie algebra $\\mathfrak{g}$. Special attention is paid to the\nstructure of the dressing factors in spinor representation of the orthogonal\nsimple Lie algebras of ${\\bf B}_r\\simeq so(2r+1,{\\mathbb C})$ type. \n\n"}
{"id": "1007.0070", "contents": "Title: Monotonicity of the Lozi Family and the Zero Entropy Locus Abstract: Ishii and Sands showed the monotonicity of the Lozi family $\\mathcal L_{a,b}$\nin a $\\mathcal C^1$ neighborhood of a-axis in the $a$-$b$ parameter space. We\nshow the monotonicity of the entropy in the vertical direction around $a=2$ and\nin some other directions for $1<a\\leq2$. Also we give some rigorous and\nnumerical results for the parameters at which the Lozi family has zero entropy. \n\n"}
{"id": "1007.2607", "contents": "Title: B\\\"acklund Transformations for the Kirchhoff Top Abstract: We construct B\\\"acklund transformations (BTs) for the Kirchhoff top by taking\nadvantage of the common algebraic Poisson structure between this system and the\n$sl(2)$ trigonometric Gaudin model. Our BTs are integrable maps providing an\nexact time-discretization of the system, inasmuch as they preserve both its\nPoisson structure and its invariants. Moreover, in some special cases we are\nable to show that these maps can be explicitly integrated in terms of the\ninitial conditions and of the \"iteration time\" $n$. Encouraged by these partial\nresults we make the conjecture that the maps are interpolated by a specific\none-parameter family of hamiltonian flows, and present the corresponding\nsolution. We enclose a few pictures where the orbits of the continuous and of\nthe discrete flow are depicted. \n\n"}
{"id": "1008.4592", "contents": "Title: Exact Solutions of the Two-Dimensional Discrete Nonlinear Schr\\\"odinger\n  Equation with Saturable Nonlinearity Abstract: We show that the two-dimensional, nonlinear Schr\\\"odinger lattice with a\nsaturable nonlinearity admits periodic and pulse-like exact solutions. We\nestablish the general formalism for the stability considerations of these\nsolutions and give examples of stability diagrams. Finally, we show that the\neffective Peierls-Nabarro barrier for the pulse-like soliton solution is zero. \n\n"}
{"id": "1009.5374", "contents": "Title: Two-component {CH} system: Inverse Scattering, Peakons and Geometry Abstract: An inverse scattering transform method corresponding to a Riemann-Hilbert\nproblem is formulated for CH2, the two-component generalization of the\nCamassa-Holm (CH) equation. As an illustration of the method, the multi -\nsoliton solutions corresponding to the reflectionless potentials are\nconstructed in terms of the scattering data for CH2. \n\n"}
{"id": "1010.0880", "contents": "Title: Symmetry classification of third-order nonlinear evolution equations.\n  Part I: Semi-simple algebras Abstract: We give a complete point-symmetry classification of all third-order evolution\nequations of the form $u_t=F(t,x,u,u_x, u_{xx})u_{xxx}+G(t,x,u,u_x, u_{xx})$\nwhich admit semi-simple symmetry algebras and extensions of these semi-simple\nLie algebras by solvable Lie algebras. The methods we employ are extensions and\nrefinements of previous techniques which have been used in such\nclassifications. \n\n"}
{"id": "1010.5725", "contents": "Title: Integrable Origins of Higher Order Painleve Equations Abstract: Higher order Painleve equations invariant under extended affine Weyl groups\n$A^{(1)}_n$ are obtained through self-similarity limit of a class of\npseudo-differential Lax hierarchies with symmetry inherited from the underlying\ngeneralized Volterra lattice structure. \n\n"}
{"id": "1011.1418", "contents": "Title: Tracy-Widom GUE law and symplectic invariants Abstract: We establish the relation between two objects: an integrable system related\nto Painleve II equation, and the symplectic invariants of a certain plane curve\n\\Sigma_{TW} describing the average eigenvalue density of a random hermitian\nmatrix spectrum near a hard edge (a bound for its maximal eigenvalue). This\nexplains directly how the Tracy-Widow law F_{GUE}, governing the distribution\nof the maximal eigenvalue in hermitian random matrices, can also be recovered\nfrom symplectic invariants. \n\n"}
{"id": "1012.5245", "contents": "Title: $R$-matrices and Hamiltonian Structures for Certain Lax Equations Abstract: In this paper a list of $R$-matrices on a certain coupled Lie algebra is\nobtained. With one of these $R$-matrices, we construct infinitely many\nbi-Hamiltonian structures for each of the two-component BKP and the Toda\nlattice hierarchies. We also show that, when such two hierarchies are reduced\nto their subhierarchies, these bi-Hamiltonian structures are reduced\ncorrespondingly. \n\n"}
{"id": "1102.1942", "contents": "Title: Rational Bundles and Recursion Operators for Integrable Equations on\n  A.III-type Symmetric Spaces Abstract: We analyze and compare the methods of construction of the recursion operators\nfor a special class of integrable nonlinear differential equations related to\nA.III-type symmetric spaces in Cartan's classification and having additional\nreductions. \n\n"}
{"id": "1103.5934", "contents": "Title: On spatial and temporal multilevel dynamics and scaling effects in\n  epileptic seizures Abstract: Epileptic seizures are one of the most well-known dysfunctions of the nervous\nsystem. During a seizure, a highly synchronized behavior of neural activity is\nobserved that can cause symptoms ranging from mild sensual malfunctions to the\ncomplete loss of body control. In this paper, we aim to contribute towards a\nbetter understanding of the dynamical systems phenomena that cause seizures.\nBased on data analysis and modelling, seizure dynamics can be identified to\npossess multiple spatial scales and on each spatial scale also multiple time\nscales. At each scale, we reach several novel insights. On the smallest spatial\nscale we consider single model neurons and investigate early-warning signs of\nspiking. This introduces the theory of critical transitions to excitable\nsystems. For clusters of neurons (or neuronal regions) we use patient data and\nfind oscillatory behavior and new scaling laws near the seizure onset. These\nscalings lead to substantiate the conjecture obtained from mean-field models\nthat a Hopf bifurcation could be involved near seizure onset. On the largest\nspatial scale we introduce a measure based on phase-locking intervals and\nwavelets into seizure modelling. It is used to resolve synchronization between\ndifferent regions in the brain and identifies time-shifted scaling laws at\ndifferent wavelet scales. We also compare our wavelet-based multiscale approach\nwith maximum linear cross-correlation and mean-phase coherence measures. \n\n"}
{"id": "1104.1166", "contents": "Title: A higher-rank version of the Q3 equation Abstract: A lattice system is derived which amounts to a higher-rank analogue of the Q3\nequation, the latter being an integrable partial difference equation which has\nappeared in the ABS list of multidimensionally consistent quadrilateral lattice\nequations. By construction this new system incorporates various lattice\nequations of Boussinesq type that were discovered many years ago. A\ncorresponding Lax representation is derived. \n\n"}
{"id": "1105.2985", "contents": "Title: Symplectic Maps from Cluster Algebras Abstract: We consider nonlinear recurrences generated from the iteration of maps that\narise from cluster algebras. More precisely, starting from a skew-symmetric\ninteger matrix, or its corresponding quiver, one can define a set of mutation\noperations, as well as a set of associated cluster mutations that are applied\nto a set of affine coordinates (the cluster variables). Fordy and Marsh\nrecently provided a complete classification of all such quivers that have a\ncertain periodicity property under sequences of mutations. This periodicity\nimplies that a suitable sequence of cluster mutations is precisely equivalent\nto iteration of a nonlinear recurrence relation. Here we explain briefly how to\nintroduce a symplectic structure in this setting, which is preserved by a\ncorresponding birational map (possibly on a space of lower dimension). We give\nexamples of both integrable and non-integrable maps that arise from this\nconstruction. We use algebraic entropy as an approach to classifying integrable\ncases. The degrees of the iterates satisfy a tropical version of the map. \n\n"}
{"id": "1105.3128", "contents": "Title: Fractal Weyl law for open quantum chaotic maps Abstract: We study the semiclassical quantization of Poincar\\'e maps arising in\nscattering problems with fractal hyperbolic trapped sets. The main application\nis the proof of a fractal Weyl upper bound for the number of\nresonances/scattering poles in small domains near the real axis. This result\nencompasses the case of several convex (hard) obstacles satisfying a no-eclipse\ncondition. \n\n"}
{"id": "1105.3662", "contents": "Title: Nonlocal Generalized Models of Predator-Prey Systems Abstract: The method of generalized modeling has been applied successfully in many\ndifferent contexts, particularly in ecology and systems biology. It can be used\nto analyze the stability and bifurcations of steady-state solutions. Although\nmany dynamical systems in mathematical biology exhibit steady-state behaviour\none also wants to understand nonlocal dynamics beyond equilibrium points. In\nthis paper we analyze predator-prey dynamical systems and extend the method of\ngeneralized models to periodic solutions. First, we adapt the equilibrium\ngeneralized modeling approach and compute the unique Floquet multiplier of the\nperiodic solution which depends upon so-called generalized elasticity and scale\nfunctions. We prove that these functions also have to satisfy a flow on\nparameter (or moduli) space. Then we use Fourier analysis to provide computable\nconditions for stability and the moduli space flow. The final stability\nanalysis reduces to two discrete convolutions which can be interpreted to\nunderstand when the predator-prey system is stable and what factors enhance or\nprohibit stable oscillatory behaviour. Finally, we provide a sampling algorithm\nfor parameter space based on nonlinear optimization and the Fast Fourier\nTransform which enables us to gain a statistical understanding of the stability\nproperties of periodic predator-prey dynamics. \n\n"}
{"id": "1105.4404", "contents": "Title: Twist number and order properties of periodic orbits Abstract: A less studied numerical characteristic of periodic orbits of area preserving\ntwist maps of the annulus is the twist or torsion number, called initially the\namount of rotation. It measures the average rotation of tangent vectors under\nthe action of the derivative of the map along that orbit, and characterizes the\ndegree of complexity of the dynamics. The aim of this paper is to give new\ninsights into the definition and properties of the twist number, and to relate\nits range to the order properties of periodic orbits. We derive an algorithm to\ndeduce the exact value or a demi--unit interval containing the exact value of\nthe twist number. We prove that at a period-doubling bifurcation threshold of a\nmini-maximizing periodic orbit, the new born doubly periodic orbit has the\nabsolute twist number larger than the absolute twist of the original orbit\nafter bifurcation. We give examples of periodic orbits having large absolute\ntwist number, that are badly ordered, and illustrate how characterization of\nthese orbits only by their residue can lead to incorrect results. In connection\nto the study of the twist number of periodic orbits of standard--like maps we\nintroduce a new tool, called 1-cone function. We prove that the location of\nminima of this function with respect to the vertical symmetry lines of a\nstandard-like map encodes a valuable information on the symmetric periodic\norbits and their twist number. \n\n"}
{"id": "1105.6051", "contents": "Title: Gradient catastrophe and flutter in vortex filament dynamics Abstract: Gradient catastrophe and flutter instability in the motion of vortex filament\nwithin the localized induction approximation are analyzed. It is shown that the\norigin if this phenomenon is in the gradient catastrophe for the dispersionless\nDa Rios system which describes motion of filament with slow varying curvature\nand torsion. Geometrically this catastrophe manifests as a rapid oscillation of\na filament curve in a point that resembles the flutter of airfoils.\nAnalytically it is the elliptic umbilic singularity in the terminology of the\ncatastrophe theory. It is demonstrated that its double scaling regularization\nis governed by the Painlev\\'e-I equation. \n\n"}
{"id": "1107.0736", "contents": "Title: Tidal evolution of hierarchical and inclined systems Abstract: We investigate the dynamical evolution of hierarchical three-body systems\nunder the effect of tides, when the ratio of the orbital semi-major axes is\nsmall and the mutual inclination is relatively large (greater than 20 degrees).\nUsing the quadrupolar non-restricted approximation for the gravitational\ninteractions and the viscous linear model for tides, we derive the averaged\nequations of motion in a vectorial formalism which is suitable to model the\nlong-term evolution of a large variety of exoplanetary systems in very\neccentric and inclined orbits. In particular, it can be used to derive\nconstraints for stellar spin-orbit misalignment, capture in Cassini states,\ntidal-Kozai migration, or damping of the mutual inclination. Because our model\nis valid for the non-restricted problem, it can be used to study systems of\nidentical mass or for the outer restricted problem, such as the evolution of a\nplanet around a binary of stars. Here, we apply our model to three distinct\nsituations: 1) the HD80606 planetary system, for which we obtain the\nprobability density function distribution for the misalignment angle, with two\npronounced peaks of higher probability around 53 and 109 degrees; 2) the\nHD98800 binary system, for which we show that initial prograde orbits inside\nthe observed disc may become retrograde and vice-versa, only because of tidal\nmigration within the binary stars; 3) the HD11964 planetary system, for which\nwe show that tidal dissipation combined with gravitational perturbations may\nlead to a decrease in the mutual inclination, and a fast circularization of the\ninner orbit. \n\n"}
{"id": "1107.1148", "contents": "Title: Discrete Integrable Systems and Hodograph Transformations Arising from\n  Motions of Discrete Plane Curves Abstract: We consider integrable discretizations of some soliton equations associated\nwith the motions of plane curves: the Wadati-Konno-Ichikawa elastic beam\nequation, the complex Dym equation, and the short pulse equation. They are\nrelated to the modified KdV or the sine-Gordon equations by the hodograph\ntransformations. Based on the observation that the hodograph transformations\nare regarded as the Euler-Lagrange transformations of the curve motions, we\nconstruct the discrete analogues of the hodograph transformations, which yield\nintegrable discretizations of those soliton equations. \n\n"}
{"id": "1107.5673", "contents": "Title: Extreme value laws in dynamical systems under physical observables Abstract: Extreme value theory for chaotic dynamical systems is a rapidly expanding\narea of research. Given a system and a real function (observable) defined on\nits phase space, extreme value theory studies the limit probabilistic laws\nobeyed by large values attained by the observable along orbits of the system.\nBased on this theory, the so-called block maximum method is often used in\napplications for statistical prediction of large value occurrences. In this\nmethod, one performs inference for the parameters of the Generalised Extreme\nValue (GEV) distribution, using maxima over blocks of regularly sampled\nobservations along an orbit of the system. The observables studied so far in\nthe theory are expressed as functions of the distance with respect to a point,\nwhich is assumed to be a density point of the system's invariant measure.\nHowever, this is not the structure of the observables typically encountered in\nphysical applications, such as windspeed or vorticity in atmospheric models. In\nthis paper we consider extreme value limit laws for observables which are not\nfunctions of the distance from a density point of the dynamical system. In such\ncases, the limit laws are no longer determined by the functional form of the\nobservable and the dimension of the invariant measure: they also depend on the\nspecific geometry of the underlying attractor and of the observable's level\nsets. We present a collection of analytical and numerical results, starting\nwith a toral hyperbolic automorphism as a simple template to illustrate the\nmain ideas. We then formulate our main results for a uniformly hyperbolic\nsystem, the solenoid map. We also discuss non-uniformly hyperbolic examples of\nmaps (H\\'enon and Lozi maps) and of flows (the Lorenz63 and Lorenz84 models).\nOur purpose is to outline the main ideas and to highlight several serious\nproblems found in the numerical estimation of the limit laws. \n\n"}
{"id": "1109.4200", "contents": "Title: $C^2$-robust heterodimensional tangencies Abstract: In this paper, we give sufficient conditions for the existence of $C^{2}$\nrobust heterodimensional tangency, and present a nonempty open set in\n$\\Diff^2(M)$ with $\\dim(M)\\geq 3$ each element of which has a non-degenerate\nheterodimensional tangency on a $C^2$ robust heterodimensional cycle. \n\n"}
{"id": "1109.6214", "contents": "Title: Is the astronomical forcing a reliable and unique pacemaker for climate?\n  A conceptual model study Abstract: There is evidence that ice age cycles are paced by astronomical forcing,\nsuggesting some kind of synchronisation phenomenon. Here, we identify the type\nof such synchronisation and explore systematically its uniqueness and\nrobustness using a simple paleoclimate model akin to the van der Pol relaxation\noscillator and dynamical system theory. As the insolation is quite a complex\nquasiperiodic signal involving different frequencies, the traditional concepts\nused to define synchronisation to periodic forcing are no longer applicable.\nInstead, we explore a different concept of generalised synchronisation in terms\nof (coexisting) synchronised solutions for the forced system, their basins of\nattraction and instabilities. We propose a clustering technique to compute the\nnumber of synchronised solutions, each of which corresponds to a different\npaleoclimate history. In this way, we uncover multistable synchronisation\n(reminiscent of phase- or frequency-locking to individual periodic components\nof astronomical forcing) at low forcing strength, and monostable or unique\nsynchronisation at stronger forcing. In the multistable regime, different\ninitial conditions may lead to different paleoclimate histories. To study their\nrobustness, we analyse Lyapunov exponents that quantify the rate of convergence\ntowards each synchronised solution (local stability), and basins of attraction\nthat indicate critical levels of external perturbations (global stability). We\nfind that even though synchronised solutions are stable on a long term, there\nexist short episodes of desynchronisation where nearby climate trajectories\ndiverge temporarily (for about 50 kyr). (...) \n\n"}
{"id": "1110.0169", "contents": "Title: Robust artificial neural networks and outlier detection. Technical\n  report Abstract: Large outliers break down linear and nonlinear regression models. Robust\nregression methods allow one to filter out the outliers when building a model.\nBy replacing the traditional least squares criterion with the least trimmed\nsquares criterion, in which half of data is treated as potential outliers, one\ncan fit accurate regression models to strongly contaminated data.\nHigh-breakdown methods have become very well established in linear regression,\nbut have started being applied for non-linear regression only recently. In this\nwork, we examine the problem of fitting artificial neural networks to\ncontaminated data using least trimmed squares criterion. We introduce a\npenalized least trimmed squares criterion which prevents unnecessary removal of\nvalid data. Training of ANNs leads to a challenging non-smooth global\noptimization problem. We compare the efficiency of several derivative-free\noptimization methods in solving it, and show that our approach identifies the\noutliers correctly when ANNs are used for nonlinear regression. \n\n"}
{"id": "1110.0957", "contents": "Title: Dictionary Learning for Deblurring and Digital Zoom Abstract: This paper proposes a novel approach to image deblurring and digital zooming\nusing sparse local models of image appearance. These models, where small image\npatches are represented as linear combinations of a few elements drawn from\nsome large set (dictionary) of candidates, have proven well adapted to several\nimage restoration tasks. A key to their success has been to learn dictionaries\nadapted to the reconstruction of small image patches. In contrast, recent works\nhave proposed instead to learn dictionaries which are not only adapted to data\nreconstruction, but also tuned for a specific task. We introduce here such an\napproach to deblurring and digital zoom, using pairs of blurry/sharp (or\nlow-/high-resolution) images for training, as well as an effective stochastic\ngradient algorithm for solving the corresponding optimization task. Although\nthis learning problem is not convex, once the dictionaries have been learned,\nthe sharp/high-resolution image can be recovered via convex optimization at\ntest time. Experiments with synthetic and real data demonstrate the\neffectiveness of the proposed approach, leading to state-of-the-art performance\nfor non-blind image deblurring and digital zoom. \n\n"}
{"id": "1110.3247", "contents": "Title: Nonvanishing boundary condition for the mKdV hierarchy and the Gardner\n  equation Abstract: A Kac-Moody algebra construction for the integrable hierarchy containing the\nGardner equation is proposed. Solutions are systematically constructed\nemploying the dressing method and deformed vertex operators which takes into\naccount the nonvanishing boundary value problem for the mKdV hierarchy.\nExplicit examples are given and besides usual KdV like solitons, our solutions\ncontemplate the large amplitude table-top solitons, kinks, dark solitons,\nbreathers and wobbles. \n\n"}
{"id": "1111.5372", "contents": "Title: Dressing approach to the nonvanishing boundary value problem for the\n  AKNS hierarchy Abstract: We propose an approach to the nonvanishing boundary value problem for\nintegrable hierarchies based on the dressing method. Then we apply the method\nto the AKNS hierarchy. The solutions are found by introducing appropriate\nvertex operators that takes into account the boundary conditions. \n\n"}
{"id": "1201.0534", "contents": "Title: On Soliton Interactions for a Hierarchy of Generalized Heisenberg\n  Ferromagnetic Models on SU(3)/S(U(1) $\\times$ U(2)) Symmetric Space Abstract: We consider an integrable hierarchy of nonlinear evolution equations (NLEE)\nrelated to linear bundle Lax operator L. The Lax representation is Z2 \\times Z2\nreduced and is naturally associated with the symmetric space SU(3)/S(U(1)\n\\times U(2)). The simplest nontrivial equation in the hierarchy is a\ngeneralization of Heisenberg ferromagnetic model. We construct the N-soliton\nsolutions for an arbitrary member of the hierarchy by using the Zakharov-Shabat\ndressing method with an appropriately chosen dressing factor. Two types of\nsoliton solutions: quadruplet and doublet solitons are found. The one-soliton\nsolutions of NLEEs with even and odd dispersion laws have different properties.\nIn particular, the one-soliton solutions for NLEEs with even dispersion laws\nare not traveling waves; their velocities and their amplitudes are time\ndependent. Calculating the asymptotics of the N-soliton solutions for t\n\\rightarrow \\pm \\infty we analyze the interactions of quadruplet solitons. \n\n"}
{"id": "1201.2514", "contents": "Title: Analytical properties of horizontal visibility graphs in the Feigenbaum\n  scenario Abstract: Time series are proficiently converted into graphs via the horizontal\nvisibility (HV) algorithm, which prompts interest in its capability for\ncapturing the nature of different classes of series in a network context. We\nhave recently shown [1] that dynamical systems can be studied from a novel\nperspective via the use of this method. Specifically, the period-doubling and\nband-splitting attractor cascades that characterize unimodal maps transform\ninto families of graphs that turn out to be independent of map nonlinearity or\nother particulars. Here we provide an in depth description of the HV treatment\nof the Feigenbaum scenario, together with analytical derivations that relate to\nthe degree distributions, mean distances, clustering coefficients, etc.,\nassociated to the bifurcation cascades and their accumulation points. We\ndescribe how the resultant families of graphs can be framed into a\nrenormalization group scheme in which fixed-point graphs reveal their scaling\nproperties. These fixed points are then re-derived from an entropy optimization\nprocess defined for the graph sets, confirming a suggested connection between\nrenormalization group and entropy optimization. Finally, we provide analytical\nand numerical results for the graph entropy and show that it emulates the\nLyapunov exponent of the map independently of its sign. \n\n"}
{"id": "1201.5920", "contents": "Title: New derivation of soliton solutions to the AKNS$_2$ system via dressing\n  transformation methods Abstract: We consider certain boundary conditions supporting soliton solutions in the\ngeneralized non-linear Schr\\\"{o}dinger equation (AKNS$_r$)\\,($r=1,2$). Using\nthe dressing transformation (DT) method and the related tau functions we study\nthe AKNS$_{r}$ system for the vanishing, (constant) non-vanishing and the mixed\nboundary conditions, and their associated bright, dark, and bright-dark\nN-soliton solutions, respectively. Moreover, we introduce a modified DT related\nto the dressing group in order to consider the free field boundary condition\nand derive generalized N-dark-dark solitons. As a reduced submodel of the\nAKNS$_r$ system we study the properties of the focusing, defocusing and mixed\nfocusing-defocusing versions of the so-called coupled non-linear\nSchr\\\"{o}dinger equation ($r-$CNLS), which has recently been considered in many\nphysical applications. We have shown that two$-$dark$-$dark$-$soliton bound\nstates exist in the AKNS$_2$ system, and three$-$ and\nhigher$-$dark$-$dark$-$soliton bound states can not exist. The\nAKNS$_r$\\,($r\\geq 3$) extension is briefly discussed in this approach. The\nproperties and calculations of some matrix elements using level one vertex\noperators are outlined. \n\n"}
{"id": "1201.6478", "contents": "Title: Solutions to the non-autonomous ABS lattice equations: Casoratians and\n  bilinearization Abstract: In the paper non-autonomous H1, H2, H3$_\\delta$ and Q1$_\\delta$ equations in\nthe ABS list are bilinearized. Their solutions are derived in Casoratian form.\nWe also list out some Casoratian shift formulae which are used to verify\nCasoratian solutions. \n\n"}
{"id": "1202.3900", "contents": "Title: Rare events, exponential hitting times and extremal indices via spectral\n  perturbation Abstract: We discuss how an eigenvalue perturbation formula for transfer operators of\ndynamical systems is related to exponential hitting time distributions and\nextreme value theory for processes generated by chaotic dynamical systems. We\nalso list a number of piecewise expanding systems to which this general theory\napplies and discuss the prospects to apply this theory to some classes of\npiecewise hyperbolic systems. \n\n"}
{"id": "1204.0250", "contents": "Title: Exponential growth of norms in semigroups of linear automorphisms and\n  Hausdorff dimension of self-projective IFS Abstract: Given a finitely generated semigroup S of the (normed) set of linear maps of\na vector space V into itself, we find sufficient conditions for the exponential\ngrowth of the number N(k) of elements of the semigroup contained in the sphere\nof radius k as k->infinity. We relate the growth rate lim log N(k)/log k to the\nexponent of a zeta function naturally defined on the semigroup and, in case S\nis a semigroup of volume-preserving automorpisms, to the Hausdorff and box\ndimensions of the limit set of the induced semigroup of automorphisms on the\ncorresponding projective space. \n\n"}
{"id": "1204.0343", "contents": "Title: Comments on \"Prediction of Subharmonic Oscillation in Switching\n  Converters Under Different Control Strategies\" Abstract: A recent paper [1] (El Aroudi, 2012) misapplied a critical condition (Fang\nand Abed, 2001) to a well-known example. Even if the mistake is corrected, the\nresults in [1] are applicable only to buck converters and period-doubling\nbifurcation. Actually, these results are known in Fang's works a decade ago\nwhich have broader critical conditions applicable to other converters and\nbifurcations. The flaws in [1] are identified. \n\n"}
{"id": "1204.1815", "contents": "Title: Using Nyquist or Nyquist-Like Plot to Predict Three Typical\n  Instabilities in DC-DC Converters Abstract: By transforming an exact stability condition, a new Nyquist-like plot is\nproposed to predict occurrences of three typical instabilities in DC-DC\nconverters. The three instabilities are saddle-node bifurcation (coexistence of\nmultiple solutions), period-doubling bifurcation (subharmonic oscillation), and\nNeimark bifurcation (quasi-periodic oscillation). In a single plot, it\naccurately predicts whether an instability occurs and what type the instability\nis. The plot is equivalent to the Nyquist plot, and it is a useful design tool\nto avoid these instabilities. Nine examples are used to illustrate the accuracy\nof this new plot to predict instabilities in the buck or boost converter with\nfixed or variable switching frequency. \n\n"}
{"id": "1204.3990", "contents": "Title: Comments on \"Bifurcations in DC-DC Switching Converters: Review of\n  Methods and Applications\" Abstract: In a review paper [1] (El Aroudi, et al., 2005), two stability conditions for\nDC-DC converters are presented. However, these two conditions were published\nyears earlier at least in a journal paper [2] (Fang and Abed, 2001). In this\nnote, the similar texts of [1] and [2] are compared. \n\n"}
{"id": "1204.6730", "contents": "Title: Topological entropy and secondary folding Abstract: A convenient measure of a map or flow's chaotic action is the topological\nentropy. In many cases, the entropy has a homological origin: it is forced by\nthe topology of the space. For example, in simple toral maps, the topological\nentropy is exactly equal to the growth induced by the map on the fundamental\ngroup of the torus. However, in many situations the numerically-computed\ntopological entropy is greater than the bound implied by this action. We\nassociate this gap between the bound and the true entropy with 'secondary\nfolding': material lines undergo folding which is not homologically forced. We\nexamine this phenomenon both for physical rod-stirring devices and toral linked\ntwist maps, and show rigorously that for the latter secondary folds occur. \n\n"}
{"id": "1205.3665", "contents": "Title: On the joint distribution of the maximum and its position of the Airy2\n  process minus a parabola Abstract: The maximal point of the Airy2 process minus a parabola is believed to\ndescribe the scaling limit of the end-point of the directed polymer in a random\nmedium, which was proved to be true for a few specific cases. Recently two\ndifferent formulas for the joint distribution of the location and the height of\nthis maximal point were obtained, one by Moreno Flores, Quastel and Remenik,\nand the other by Schehr. The first formula is given in terms of the Airy\nfunction and an associated operator, and the second formula is expressed in\nterms of the Lax pair equations of the Painleve II equation. We give a direct\nproof that these two formulas are the same. \n\n"}
{"id": "1206.1217", "contents": "Title: From Yang-Baxter maps to integrable quad maps and recurrences Abstract: Starting from known solutions of the functional Yang-Baxter equations, we\nexhibit Miura type of transformations leading to various known integrable quad\nequations. We then construct, from the same list of Yang-Baxter maps, a series\nof non-autonomous solvable recurrences of order two. \n\n"}
{"id": "1207.0387", "contents": "Title: Backlund transformations and Hamiltonian flows Abstract: In this work we show that, under certain conditions, parametric Backlund\ntransformations (BTs) for a finite dimensional integrable system can be\ninterpreted as solutions to the equations of motion defined by an associated\nnon-autonomous Hamiltonian. The two systems share the same constants of motion.\nThis observation lead to the identification of the Hamiltonian interpolating\nthe iteration of the discrete map defined by the transformations, that indeed\nwill be a linear combination of the integrals appearing in the spectral curve\nof the Lax matrix. An application to the Toda periodic lattice is given. \n\n"}
{"id": "1207.6072", "contents": "Title: Discrete integrable systems and Poisson algebras from cluster maps Abstract: We consider nonlinear recurrences generated from cluster mutations applied to\nquivers that have the property of being cluster mutation-periodic with period\n1. Such quivers were completely classified by Fordy and Marsh, who\ncharacterised them in terms of the skew-symmetric matrix that defines the\nquiver. The associated nonlinear recurrences are equivalent to birational maps,\nand we explain how these maps can be endowed with an invariant Poisson bracket\nand/or presymplectic structure.\n  Upon applying the algebraic entropy test, we are led to a series of\nconjectures which imply that the entropy of the cluster maps can be determined\nfrom their tropical analogues, which leads to a sharp classification result.\nOnly four special families of these maps should have zero entropy. These\nfamilies are examined in detail, with many explicit examples given, and we show\nhow they lead to discrete dynamics that is integrable in the Liouville-Arnold\nsense. \n\n"}
{"id": "1208.0526", "contents": "Title: Optimization hardness as transient chaos in an analog approach to\n  constraint satisfaction Abstract: Boolean satisfiability [1] (k-SAT) is one of the most studied optimization\nproblems, as an efficient (that is, polynomial-time) solution to k-SAT (for\n$k\\geq 3$) implies efficient solutions to a large number of hard optimization\nproblems [2,3]. Here we propose a mapping of k-SAT into a deterministic\ncontinuous-time dynamical system with a unique correspondence between its\nattractors and the k-SAT solution clusters. We show that beyond a constraint\ndensity threshold, the analog trajectories become transiently chaotic [4-7],\nand the boundaries between the basins of attraction [8] of the solution\nclusters become fractal [7-9], signaling the appearance of optimization\nhardness [10]. Analytical arguments and simulations indicate that the system\nalways finds solutions for satisfiable formulae even in the frozen regimes of\nrandom 3-SAT [11] and of locked occupation problems [12] (considered among the\nhardest algorithmic benchmarks); a property partly due to the system's\nhyperbolic [4,13] character. The system finds solutions in polynomial\ncontinuous-time, however, at the expense of exponential fluctuations in its\nenergy function. \n\n"}
{"id": "1208.1707", "contents": "Title: Numerical investigation of the Bautin bifurcation in a delay\n  differential equation modeling leukemia Abstract: In a previous work we investigated the existence of Hopf degenerate\nbifurcation points for a differential delay equation modeling leukemia and we\nactually found Hopf points of codimension two for the considered problem. If\naround the parameters corresponding to such a point we vary two parameters (the\nconsidered problem has five parameters), then a Bautin bifurcation should\noccur. In this work we chose a Hopf point of codimension two for the considered\nproblem and perform numerical integration for parameters chosen in a\nneighborhood of the bifurcation point parameters. The results show that,\nindeed, we have a Bautin bifurcation in the chosen point. \n\n"}
{"id": "1208.3752", "contents": "Title: Solutions to the ABS lattice equations via generalized Cauchy matrix\n  approach Abstract: The usual Cauchy matrix approach starts from a known plain wave factor vector\n$r$ and known dressed Cauchy matrix $M$. In this paper we start from a matrix\nequation set with undetermined $r$ and $M$. From the starting equation set we\ncan build shift relations for some defined scalar functions and then derive\nlattice equations. The starting matrix equation set admits more choices for $r$\nand $M$ and in the paper we give explicit formulae for all possible $r$ and\n$M$. As applications, we get more solutions than usual multi-soliton solutions\nfor many lattice equations including the lattice potential KdV equation, the\nlattice potential modified KdV equation, the lattice Schwarzian KdV equation,\nNQC equation and some lattice equations in ABS list. \n\n"}
{"id": "1208.4540", "contents": "Title: Effects of degree-frequency correlations on network synchronization:\n  universality and full phase-locking Abstract: We introduce a model to study the effect of degree-frequency correlations on\nsynchronization in networks of coupled oscillators. Analyzing this model, we\nfind several remarkable characteristics. We find a stationary synchronized\nstate that is (i) universal, i.e., the degree of synchrony, as measured by a\nglobal order parameter, is independent of network topology, and (ii) fully\nphase-locked, i.e., all oscillators become simultaneously phase-locked despite\nhaving different natural frequencies. This state separates qualitatively\ndifferent behaviors for two other classes of correlations where, respectively,\nslow and fast oscillators can remain unsynchronized. We close by presenting\nanalysis of the dynamics under arbitrary degree-frequency correlations. \n\n"}
{"id": "1209.0223", "contents": "Title: The space of initial conditions and the property of an almost good\n  reduction in discrete Painleve II equations over finite fields Abstract: Discrete versions of the Painleve equations (dPII and qPII) over finite\nfields are studied. We first show that they are well defined by extending the\ndomain according to the theory of the space of initial conditions, taking the\ndPII equation as an example. Then we define them over the field of p-adic\nnumbers and see that they have a property that is similar to the good reduction\nof dynamical systems modulo a prime. This property is called 'almost good\nreduction'. We study the q-discrete analogue of the Painleve II equation in\nthis paper, following the method in our previous work (arXiv: 1206.4456), in\nwhich the discrete Painleve II equation has been treated. We can consider\nalmost good reduction as an arithmetic analogue of the singularity confinement\ntest. We can also obtain special solutions over finite fields from those\ndefined over fields of characteristic zero.\n  (v2: excluded the review of arXiv: 1206.4456) (v3: several typos are\ncorrected) (v4: final version to appear in J. Nonlin. Math. Phys.) \n\n"}
{"id": "1209.2287", "contents": "Title: Stability index for chaotically driven concave maps Abstract: We study skew product systems driven by a hyperbolic base map S (e.g. a baker\nmap or an Anosov surface diffeomorphism) and with simple concave fibre maps on\nan interval [0,a] like h(x)=g(\\theta) tanh(x) where g(\\theta) is a factor\ndriven by the base map. The fibre-wise attractor is the graph of an upper\nsemicontinuous function \\phi(\\theta). For many choices of the function g, \\phi\nhas a residual set of zeros but \\phi>0 almost everywhere w.r.t. the\nSinai-Ruelle-Bowen measure of S^(-1).\n  In such situations we evaluate the stability index of the global attractor of\nthe system, which is the subgraph of \\phi, at all regular points (\\theta,0) in\nterms of the local exponents \\Gamma(\\theta):=\\lim_{n\\to\\infty} 1/n log\ng_n(\\theta) and \\Lambda(\\theta):=\\lim_{n\\to\\infty} 1/n\\log|D_u S^{-n}(\\theta)|\nand of the positive zero s_* of a certain thermodynamic pressure function\nassociated with S^(-1) and g. (In queuing theory, an analogon of s_* is known\nas Loyne's exponent.)\n  The stability index was introduced by Podvigina and Ashwin in 2011 to\nquantify the local scaling of basins of attraction. \n\n"}
{"id": "1209.2999", "contents": "Title: Infinite-dimensional prolongation Lie algebras and multicomponent\n  Landau-Lifshitz systems associated with higher genus curves Abstract: The Wahlquist-Estabrook prolongation method constructs for some PDEs a Lie\nalgebra that is responsible for Lax pairs and Backlund transformations of\ncertain type. We present some general properties of Wahlquist-Estabrook\nalgebras for (1+1)-dimensional evolution PDEs and compute this algebra for the\nn-component Landau-Lifshitz system of Golubchik and Sokolov for any $n\\ge 3$.\n  We prove that the resulting algebra is isomorphic to the direct sum of a\n2-dimensional abelian Lie algebra and an infinite-dimensional Lie algebra L(n)\nof certain matrix-valued functions on an algebraic curve of genus\n$1+(n-3)2^{n-2}$. This curve was used by Golubchik, Sokolov, Skrypnyk, Holod in\nconstructions of Lax pairs. Also, we find a presentation for the algebra L(n)\nin terms of a finite number of generators and relations. These results help to\nobtain a partial answer to the problem of classification of multicomponent\nLandau-Lifshitz systems with respect to Backlund transformations.\n  Furthermore, we construct a family of integrable evolution PDEs that are\nconnected with the n-component Landau-Lifshitz system by Miura type\ntransformations parametrized by the above-mentioned curve. Some solutions of\nthese PDEs are described. \n\n"}
{"id": "1210.0281", "contents": "Title: KP solitons and Mach reflection in shallow water Abstract: This gives a survey of our recent studies on soliton solutions of the\nKadomtsev-Petviashvili equation with an emphasis on the Mach reflection problem\nin shallow water. \n\n"}
{"id": "1210.1935", "contents": "Title: Saddle-Node Bifurcation Associated with Parasitic Inductor Resistance in\n  Boost Converters Abstract: Saddle-node bifurcation occurs in a boost converter when parasitic inductor\nresistance is modeled. Closed-form critical conditions of the bifurcation are\nderived. If the parasitic inductor resistance is modeled, the saddle-node\nbifurcation occurs in the voltage mode control or in the current mode control\nwith the voltage loop closed, but not in the current mode control with the\nvoltage loop open. If the parasitic inductor resistance is not modeled, the\nsaddle-node bifurcation does not occur, and one may be misled by the wrong\ndynamics and the wrong steady-state solutions. The saddle-node bifurcation\nstill exists even in a boost converter with a popular type-III compensator.\nWhen the saddle-node bifurcation occurs, multiple steady-state solutions may\ncoexist. The converter may operate with a voltage jump from one solution to\nanother. Care should be taken in the compensator design to ensure that only the\ndesired solution is stabilized. In industry practice, the solution with a\nhigher duty cycle (and thus the saddle-node bifurcation) may be prevented by\nplacing a limitation on the maximum duty cycle. \n\n"}
{"id": "1210.2522", "contents": "Title: N-order bright and dark rogue waves in a Resonant erbium-doped Fibre\n  system Abstract: The rogue waves in a resonant erbium-doped fibre system governed by a coupled\nsystem of the nonlinear Schr\\\"odinger equation and the Maxwell-Bloch equation\n(NLS-MB equations) are given explicitly by a Taylor series expansion about the\nbreather solutions of the normalized slowly varying amplitude of the complex\nfield envelope $E$, polarization $p$ and population inversion $\\eta$. The\nn-order breather solutions of the three fields are constructed using Darboux\ntransformation (DT) by assuming periodic seed solutions. What is more, the\nn-order rogue waves are given by determinant forms with $n+3$ free parameters.\nFurthermore, the possible connection between our rouge waves and the generation\nof supercontinuum generation is discussed. \n\n"}
{"id": "1210.7362", "contents": "Title: Discrete Energy Minimization, beyond Submodularity: Applications and\n  Approximations Abstract: In this thesis I explore challenging discrete energy minimization problems\nthat arise mainly in the context of computer vision tasks. This work motivates\nthe use of such \"hard-to-optimize\" non-submodular functionals, and proposes\nmethods and algorithms to cope with the NP-hardness of their optimization.\nConsequently, this thesis revolves around two axes: applications and\napproximations. The applications axis motivates the use of such\n\"hard-to-optimize\" energies by introducing new tasks. As the energies become\nless constrained and structured one gains more expressive power for the\nobjective function achieving more accurate models. Results show how\nchallenging, hard-to-optimize, energies are more adequate for certain computer\nvision applications. To overcome the resulting challenging optimization tasks\nthe second axis of this thesis proposes approximation algorithms to cope with\nthe NP-hardness of the optimization. Experiments show that these new methods\nyield good results for representative challenging problems. \n\n"}
{"id": "1211.2534", "contents": "Title: On the squared eigenfunction symmetry of the Toda lattice hierarchy Abstract: The squared eigenfunction symmetry for the Toda lattice hierarchy is\nexplicitly constructed in the form of the Kronecker product of the vector\neigenfunction and the vector adjoint eigenfunction, which can be viewed as the\ngenerating function for the additional symmetries when the eigenfunction and\nthe adjoint eigenfunction are the wave function and the adjoint wave function\nrespectively. Then after the Fay-like identities and some important relations\nabout the wave functions are investigated, the action of the squared\neigenfunction related to the additional symmetry on the tau function is\nderived, which is equivalent to the Adler-Shiota-van Moerbeke (ASvM) formulas. \n\n"}
{"id": "1211.4725", "contents": "Title: Amplified Hopf bifurcations in feed-forward networks Abstract: In a previous paper, the authors developed a method for computing normal\nforms of dynamical systems with a coupled cell network structure. We now apply\nthis theory to one-parameter families of homogeneous feed-forward chains with\n2-dimensional cells. Our main result is that Hopf bifurcations in such families\ngenerically generate branches of periodic solutions with amplitudes growing\nlike $\\lambda^{1/2}$, $\\lambda^{1/6}$, $\\lambda^{1/18}$, etc. Such amplified\nHopf branches were previously found by others in a subclass of feed-forward\nnetworks with three cells, first under a normal form assumption and later by\nexplicit computations. We explain here how these bifurcations arise generically\nin a broader class of feed-forward chains of arbitrary length. \n\n"}
{"id": "1211.5756", "contents": "Title: The multi-dimensional Hamiltonian Structures in the Whitham method Abstract: In this paper we consider the averaging of local field-theoretic Poisson\nbrackets in the multi-dimensional case. As a result, we construct a local\nPoisson bracket for the regular Whitham system in the multidimensional\nsituation. The procedure is based on the procedure of averaging of local\nconservation laws and follows the Dubrovin - Novikov scheme of the bracket\naveraging suggested in one-dimensional case. However, the features of the phase\nspace of modulated parameters in higher dimensions lead to a different natural\nclass of the averaged brackets in comparison with the one-dimensional\nsituation. Here we suggest a direct procedure of construction of the bracket\nfor the Whitham system for $d > 1$ and discuss the conditions of applicability\nof the corresponding scheme. At the end, we discuss canonical forms of the\naveraged Poisson bracket in the multidimensional case. \n\n"}
{"id": "1212.1952", "contents": "Title: On Addition Formulae of KP, mKP and BKP Hierarchies Abstract: In this paper we study the addition formulae of the KP, the mKP and the BKP\nhierarchies. We prove that the total hierarchies are equivalent to the simplest\nequations of their addition formulae. In the case of the KP and the mKP\nhierarchies those results had previously been proved by Noumi, Takasaki and\nTakebe by way of wave functions. Here we give alternative and direct proofs for\nthe case of the KP and mKP hierarchies. Our method can equally be applied to\nthe BKP hierarchy. \n\n"}
{"id": "1212.3530", "contents": "Title: A Multi-Orientation Analysis Approach to Retinal Vessel Tracking Abstract: This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications. \n\n"}
{"id": "1301.3468", "contents": "Title: Boltzmann Machines and Denoising Autoencoders for Image Denoising Abstract: Image denoising based on a probabilistic model of local image patches has\nbeen employed by various researchers, and recently a deep (denoising)\nautoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as\na good model for this. In this paper, we propose that another popular family of\nmodels in the field of deep learning, called Boltzmann machines, can perform\nimage denoising as well as, or in certain cases of high level of noise, better\nthan denoising autoencoders. We empirically evaluate the two models on three\ndifferent sets of images with different types and levels of noise. Throughout\nthe experiments we also examine the effect of the depth of the models. The\nexperiments confirmed our claim and revealed that the performance can be\nimproved by adding more hidden layers, especially when the level of noise is\nhigh. \n\n"}
{"id": "1302.0637", "contents": "Title: Two-Dimensional Toda-Heisenberg Lattice Abstract: We consider a nonlinear model that is a combination of the anisotropic\ntwo-dimensional classical Heisenberg and Toda-like lattices. In the framework\nof the Hirota direct approach, we present the field equations of this model as\na bilinear system, which is closely related to the Ablowitz-Ladik hierarchy,\nand derive its N-soliton solutions. \n\n"}
{"id": "1302.3191", "contents": "Title: Whitney-Holder continuity of the SRB measure for transversal families of\n  smooth unimodal maps Abstract: We consider C^2 families t->f_t of C^4 nondegenerate unimodal maps. We study\nthe absolutely continuous invariant probability (SRB) measure m_t of f_t, as a\nfunction of t on the set of Collet-Eckmann (CE) parameters:\n  Upper bounds: Assuming existence of a transversal CE parameter, we find a\npositive measure set D of CE parameters, and, for each s in D, a subset D0 of D\nof polynomially recurrent parameters containing s as a Lebesgue density point,\nand constants C>1, G >4, so that, for every 1/2-Holder function A (of\n1/2-Holder norm |A|) and all t in D0,\n  |\\int A dm_t -\\int A dm_s| < C |A| |t-s|^{1/2} |log|t-s||^G\n  (If f_t(x)=tx(1-x), the set D contains almost all CE parameters.)\n  Lower bounds: Assuming existence of a transversal mixing Misiurewicz-Thurston\nparameter s, we find a set of CE parameters D' accumulating at s, a constant C\n>1, and an infinitely differentiable function B, so that for all t in D'\n  C |t-s|^{1/2} > |\\int B dm_t -\\int B dm_s| > |t-s|^{1/2}/C \n\n"}
{"id": "1302.7144", "contents": "Title: Multi-time Lagrangian 1-forms for families of B\\\"acklund\n  transformations. Toda-type systems Abstract: General Lagrangian theory of discrete one-dimensional integrable systems is\nillustrated by a detailed study of B\\\"acklund transformations for Toda-type\nsystems. Commutativity of B\\\"acklund transformations is shown to be equivalent\nto consistency of the system of discrete multi-time Euler-Lagrange equations.\nThe precise meaning of the commutativity in the periodic case, when all maps\nare double-valued, is established. It is shown that gluing of different\nbranches is governed by the so called superposition formulas. The closure\nrelation for the multi-time Lagrangian 1-form on solutions of the variational\nequations is proved for all Toda-type systems. Superposition formulas are\ninstrumental for this proof. The closure relation was previously shown to be\nequivalent to the spectrality property of B\\\"acklund transformations, i.e., to\nthe fact that the derivative of the Lagrangian with respect to the spectral\nparameter is a common integral of motion of the family of B\\\"acklund\ntransformations. We relate this integral of motion to the monodromy matrix of\nthe zero curvature representation which is derived directly from equations of\nmotion in an algorithmic way. This serves as a further evidence in favor of the\nidea that B\\\"acklund transformations serve as zero curvature representations\nfor themselves. \n\n"}
{"id": "1303.3575", "contents": "Title: Lie algebras responsible for zero-curvature representations of scalar\n  evolution equations Abstract: Zero-curvature representations (ZCRs) are one of the main tools in the theory\nof integrable PDEs. In particular, Lax pairs for (1+1)-dimensional PDEs can be\ninterpreted as ZCRs.\n  For any (1+1)-dimensional scalar evolution equation $E$, we define a family\nof Lie algebras $F(E)$ which are responsible for all ZCRs of $E$ in the\nfollowing sense. Representations of the algebras $F(E)$ classify all ZCRs of\nthe equation $E$ up to local gauge transformations. To achieve this, we find a\nnormal form for ZCRs with respect to the action of the group of local gauge\ntransformations.\n  As we show in other publications, using these algebras, one obtains some\nnecessary conditions for integrability of the considered PDEs (where\nintegrability is understood in the sense of soliton theory) and necessary\nconditions for existence of a B\\\"acklund transformation between two given\nequations. Examples of proving non-integrability and applications to obtaining\nnon-existence results for B\\\"acklund transformations are presented in other\npublications as well.\n  In our approach, ZCRs may depend on partial derivatives of arbitrary order,\nwhich may be higher than the order of the equation $E$. The algebras $F(E)$\ngeneralize Wahlquist-Estabrook prolongation algebras, which are responsible for\na much smaller class of ZCRs.\n  In this paper we describe general properties of $F(E)$ and present generators\nand relations for these algebras. In other publications we study the structure\nof $F(E)$ for equations of KdV, Krichever-Novikov, Kaup-Kupershmidt,\nSawada-Kotera types. Among the obtained algebras, one finds\ninfinite-dimensional Lie algebras of certain matrix-valued functions on\nrational and elliptic algebraic curves. \n\n"}
{"id": "1304.1408", "contents": "Title: Restoration of Images Corrupted by Impulse Noise and Mixed Gaussian\n  Impulse Noise using Blind Inpainting Abstract: This article studies the problem of image restoration of observed images\ncorrupted by impulse noise and mixed Gaussian impulse noise. Since the pixels\ndamaged by impulse noise contain no information about the true image, how to\nfind this set correctly is a very important problem. We propose two methods\nbased on blind inpainting and $\\ell_0$ minimization that can simultaneously\nfind the damaged pixels and restore the image. By iteratively restoring the\nimage and updating the set of damaged pixels, these methods have better\nperformance than other methods, as shown in the experiments. In addition, we\nprovide convergence analysis for these methods, these algorithms will converge\nto coordinatewise minimum points. In addition, they will converge to local\nminimum points (or with probability one) with some modifications in the\nalgorithms. \n\n"}
{"id": "1304.7164", "contents": "Title: High-order rogue waves for the Hirota equation Abstract: The Hirota equation is better than the nonlinear Schr\\\"{o}dinger equation\nwhen approximating deep ocean waves. In this paper, high-order rational\nsolutions for the Hirota equation are constructed based on the parameterized\nDarboux transformation. Several types of this kind of solutions are classified\nby their structures. \n\n"}
{"id": "1305.6636", "contents": "Title: Rational solitons of wave resonant interaction models Abstract: Integrable models of resonant interaction of two or more waves in 1+1\ndimensions are known to be of applicative interest in several areas. Here we\nconsider a system of three coupled wave equations which includes as special\ncases the vector Nonlinear Schroedinger equations and the equations describing\nthe resonant interaction of three waves. The Darboux-Dressing construction of\nsoliton solutions is applied under the condition that the solutions have\nrational, or mixed rational-exponential, dependence on coordinates. Our\nalgebraic construction relies on the use of nilpotent matrices and their Jordan\nform. We systematically search for all bounded rational (mixed\nrational-exponential) solutions and find, for the first time to our knowledge,\na broad family of such solutions of the three wave resonant interaction\nequations. \n\n"}
{"id": "1306.1645", "contents": "Title: A geometric realization of the periodic discrete Toda lattice and its\n  tropicalization Abstract: An explicit formula concerning curve intersections equivalent to the time\nevolution of the periodic discrete Toda lattice is presented. First, the time\nevolution is realized as a point addition on a hyperelliptic curve, which is\nthe spectral curve of the periodic discrete Toda lattice, then the point\naddition is translated into curve intersections. Next, it is shown that the\ncurves which appear in the curve intersections are explicitly given by using\nthe conserved quantities of the periodic discrete Toda lattice. Finally, the\nformulation is lifted to the framework of tropical geometry, and a tropical\ngeometric realization of the periodic box-ball system is constructed via\ntropical curve intersections. \n\n"}
{"id": "1306.1679", "contents": "Title: Clifford Fourier-Mellin transform with two real square roots of -1 in\n  Cl(p,q), p+q=2 Abstract: We describe a non-commutative generalization of the complex Fourier-Mellin\ntransform to Clifford algebra valued signal functions over the domain\n$\\R^{p,q}$ taking values in Cl(p,q), p+q=2.\n  Keywords: algebra, Fourier transforms; Logic, set theory, and algebra,\nFourier analysis, Integral transforms \n\n"}
{"id": "1306.2984", "contents": "Title: Toda lattice G-Strands Abstract: Hamilton's principle is used to extend for the Toda lattice ODEs to systems\nof PDEs called the Toda lattice strand equations (T-Strands). The T-Strands in\nthe $n$-particle Toda case comprise $4n-2$ quadratically nonlinear PDEs in one\nspace and one time variable. T-Strands form a symmetric hyperbolic Lie-Poisson\nHamiltonian system of quadratically nonlinear PDEs with constant characteristic\nvelocities. The travelling wave solutions for the two-particle T-Strand\nequations are solved geometrically, and their Lax pair is given to show how\nnonlinearity affects the solution. The three-particle T-Strands equations are\nalso derived from Hamilton's principle. For both the two-particle and\nthree-particle T-Strand PDEs the determining conditions for the existence of a\nquadratic zero-curvature relation (ZCR) exactly cancel the nonlinear terms in\nthe PDEs. Thus, the two-particle and three-particle T-Strand PDEs do not pass\nthe ZCR test for integrability. \n\n"}
{"id": "1306.3003", "contents": "Title: Non-parametric Power-law Data Clustering Abstract: It has always been a great challenge for clustering algorithms to\nautomatically determine the cluster numbers according to the distribution of\ndatasets. Several approaches have been proposed to address this issue,\nincluding the recent promising work which incorporate Bayesian Nonparametrics\ninto the $k$-means clustering procedure. This approach shows simplicity in\nimplementation and solidity in theory, while it also provides a feasible way to\ninference in large scale datasets. However, several problems remains unsolved\nin this pioneering work, including the power-law data applicability, mechanism\nto merge centers to avoid the over-fitting problem, clustering order problem,\ne.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely\n\\emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-Yor\nProcess, \\emph{pyp-means} treats clusters differently by dynamically and\nadaptively changing the threshold to guarantee the generation of power-law\nclustering results. Also, one center agglomeration procedure is integrated into\nthe implementation to be able to merge small but close clusters and then\nadaptively determine the cluster number. With more discussion on the clustering\norder, the convergence proof, complexity analysis and extension to spectral\nclustering, our approach is compared with traditional clustering algorithm and\nvariational inference methods. The advantages and properties of pyp-means are\nvalidated by experiments on both synthetic datasets and real world datasets. \n\n"}
{"id": "1306.3945", "contents": "Title: Semiclassical matrix elements for a chaotic propagator in the Scar\n  functions basis Abstract: A semiclassical approximation for the matrix elements of a quantum chaotic\npropagator in the scar function basis has been derived. The obtained expression\nis solely expressed in terms of canonical invariant objects. For our purpose,\nwe have used, the recently developed, semiclassical matrix elements of the\npropagator in coherent states, together with the linearization of the flux in\nthe neighborhood of a classically unstable periodic orbit of chaotic two\ndimensional systems. The expression here derived is successfully verified to be\nexact for a (linear) cat map, after the theory is adapted to a discrete phase\nspace appropriate to a quantized torus. \n\n"}
{"id": "1307.3229", "contents": "Title: Construction of Recurrent Fractal Interpolation Surfaces with Function\n  Scaling Factors and Estimation of Box-counting Dimension on Rectangular Grids Abstract: We consider a construction of recurrent fractal interpolation surfaces with\nfunction vertical scaling factors and estimation of their box-counting\ndimension. A recurrent fractal interpolation surface (RFIS) is an attractor of\na recurrent iterated function system (RIFS) which is a graph of bivariate\ninterpolation function. For any given data set on rectangular grids, we\nconstruct general recurrent iterated function systems with function vertical\nscaling factors and prove the existence of bivariate functions whose graph are\nattractors of the above constructed RIFSs. Finally, we estimate lower and upper\nbounds for the box-counting dimension of the constructed RFISs. \n\n"}
{"id": "1307.3368", "contents": "Title: Kato perturbation expansion in classical mechanics and an explicit\n  expression for a Deprit generator Abstract: This work explores the structure of Poincare-Lindstedt perturbation series in\nDeprit operator formalism and establishes its connection to Kato resolvent\nexpansion. A discussion of invariant definitions for averaging and integrating\nperturbation operators and their canonical identities reveals a regular pattern\nin a Deprit generator. The pattern was explained using Kato series and the\nrelation of perturbation operators to Laurent coefficients for the resolvent of\nLiouville operator.\n  This purely canonical approach systematizes the series and leads to the\nexplicit expression for the Deprit generator in any perturbation order: \\[G = -\n\\hat{\\mathsf S}_H H_i.\\] Here, $\\hat{\\mathsf S}_H$ is the partial\npseudo-inverse of the perturbed Liouville operator. Corresponding Kato series\nprovides a reasonably effective computational algorithm.\n  The canonical connection of perturbed and unperturbed averaging operators\nallows for a description of ambiguities in the generator and transformed\nHamiltonian, while Gustavson integrals turn out to be insensitive to\nnormalization style. Non-perturbative examples are used for illustration. \n\n"}
{"id": "1308.2824", "contents": "Title: Non-commutative rational Yang-Baxter maps Abstract: Starting from multidimensional consistency of non-commutative lattice\nmodified Gel'fand-Dikii systems we present the corresponding solutions of the\nfunctional (set-theoretic) Yang-Baxter equation, which are non-commutative\nversions of the maps arising from geometric crystals. Our approach works under\nadditional condition of centrality of certain products of non-commuting\nvariables. Then we apply such a restriction on the level of the Gel'fand-Dikii\nsystems what allows to obtain non-autonomous (but with central non-autonomous\nfactors) versions of the equations. In particular we recover known\nnon-commutative version of Hirota's lattice sine-Gordon equation, and we\npresent an integrable non-commutative and non-autonomous lattice modified\nBoussinesq equation. \n\n"}
{"id": "1308.4919", "contents": "Title: Transients in the Synchronization of Oscillator Arrays Abstract: The purpose of this note is threefold. First we state a few conjectures that\nallow us to rigorously derive a theory which is asymptotic in N (the number of\nagents) that describes transients in large arrays of (identical) linear damped\nharmonic oscillators in R with completely decentralized nearest neighbor\ninteraction. We then use the theory to establish that in a certain range of the\nparameters transients grow linearly in the number of agents (and faster outside\nthat range). Finally, in the regime where this linear growth occurs we give the\nconstant of proportionality as a function of the signal velocities (see [3]) in\neach of the two directions. As corollaries we show that symmetric interactions\nare far from optimal and that all these results independent of (reasonable)\nboundary conditions. \n\n"}
{"id": "1308.5891", "contents": "Title: Consistent Riccati Expansion and Solvability Abstract: A consistent Riccati expansion (CRE) is proposed for solving nonlinear\nsystems with the help of a Riccati equation. A system is defined to be CRE\nsolvable if it has a CRE. Various integrable systems are CRE solvable.\nFurthermore, it is also revealed that many CRE solvable systems, including the\nKorteweg de-Vries, Kadomtsev-Patviashvili, nonlinear Schr\\\"dinger and\nsine-Gordon equations, possess a common determining equation which describes\ninteractions between a soliton and a cnoidal wave. \n\n"}
{"id": "1308.6623", "contents": "Title: Exact solutions of multicomponent nonlinear Schr\\\"odinger equations\n  under general plane-wave boundary conditions Abstract: We construct exact soliton solutions of integrable multicomponent nonlinear\nSchr\\\"odinger (NLS) equations under general nonvanishing boundary conditions.\nDifferent components of the vector (or matrix) dependent variable can approach\nplane waves with different wavenumbers and frequencies at spatial infinity. We\napply B\\\"acklund-Darboux transformations to the cubic NLS equations with a\nself-focusing nonlinearity, a self-defocusing nonlinearity or a mixed\nfocusing-defocusing nonlinearity. Both bright-soliton solutions and\ndark-soliton solutions are obtained, depending on the signs of the nonlinear\nterms and the type of B\\\"acklund-Darboux transformation. The multicomponent\nsolitons generally possess internal degrees of freedom and provide highly\nnontrivial generalizations of the scalar NLS solitons. The main step in the\nconstruction of the multicomponent solitons is to compute the matrix\nexponential of a constant non-diagonal matrix arising from the Lax pair. With a\nsuitable re-parametrization of the non-diagonal matrix, the matrix exponential\ncan be computed explicitly in closed form for the most interesting cases such\nas the two-component vector NLS equation. In particular, we do not resort to\nCardano's formula in diagonalizing a $3 \\times 3$ matrix, so our expressions\nfor the multicomponent solitons are in some sense more explicit and useful than\nthose obtained in [Q-H. Park and H. J. Shin, Phys. Rev. E 61 (2000) 3093]. \n\n"}
{"id": "1309.2689", "contents": "Title: Construction of KP solitons from wave patterns Abstract: We often observe that waves on the surface of shallow water form complex\nweb-like patterns. They are examples of nonlinear waves, and these patterns are\ngenerated by nonlinear interactions among several obliquely propagating waves.\nIn this note, we discuss how to construct an exact soliton solution of the KP\nequation from such web-pattern of shallow water wave. This can be regarded as\nan \"inverse problem\" in the sense that by measuring certain metric data of the\nsolitary waves in the given pattern, it is possible to construct an exact KP\nsoliton solution which can describe the non-stationary dynamics of the pattern. \n\n"}
{"id": "1309.3756", "contents": "Title: Extended Krein-Adler theorem for the translationally shape invariant\n  potentials Abstract: Considering successive extensions of primary translationally shape invariant\npotentials, we enlarge the Krein-Adler theorem to mixed chains of state adding\nand state-deleting Darboux-B\\\"acklund transformations. It allows us to\nestablish novel bilinear Wronskian and determinantal identities for classical\northogonal polynomials. \n\n"}
{"id": "1309.7177", "contents": "Title: Topological analysis and Boolean functions. I. Methods and application\n  to classical systems Abstract: We aim to completely formalize the rough topological analysis of integrable\nHamiltonian systems admitting analytical solutions such that the initial phase\nvariables along with the time derivatives of the auxiliary variables are\nexpressed as rational functions (in fact, as polynomials) in some set of\nradicals depending on one variable each. We suggest a method to define the\nadmissible regions in the integral constants space, the segments of oscillation\nof the separated variables and the number of connected components of integral\nmanifolds and critical integral surfaces. This method is based on some\nalgorithms of processing the tables of some Boolean vector-functions and of\nreducing the matrices of linear Boolean vector-functions to some canonical\nform. From this point of view we consider here the topologically richest\nclassical problems of the rigid body dynamics. The article will be continued\nwith the investigation of some new integrable problems. \n\n"}
{"id": "1310.4249", "contents": "Title: Mapping the stereotyped behaviour of freely-moving fruit flies Abstract: Most animals possess the ability to actuate a vast diversity of movements,\nostensibly constrained only by morphology and physics. In practice, however, a\nfrequent assumption in behavioral science is that most of an animal's\nactivities can be described in terms of a small set of stereotyped motifs. Here\nwe introduce a method for mapping the behavioral space of organisms, relying\nonly upon the underlying structure of postural movement data to organize and\nclassify behaviors. We find that six different drosophilid species each perform\na mix of non-stereotyped actions and over one hundred hierarchically-organized,\nstereotyped behaviors. Moreover, we use this approach to compare these species'\nbehavioral spaces, systematically identifying subtle behavioral differences\nbetween closely-related species. \n\n"}
{"id": "1310.7443", "contents": "Title: On Convergent Finite Difference Schemes for Variational - PDE Based\n  Image Processing Abstract: We study an adaptive anisotropic Huber functional based image restoration\nscheme. By using a combination of L2-L1 regularization functions, an adaptive\nHuber functional based energy minimization model provides denoising with edge\npreservation in noisy digital images. We study a convergent finite difference\nscheme based on continuous piecewise linear functions and use a variable\nsplitting scheme, namely the Split Bregman, to obtain the discrete minimizer.\nExperimental results are given in image denoising and comparison with additive\noperator splitting, dual fixed point, and projected gradient schemes illustrate\nthat the best convergence rates are obtained for our algorithm. \n\n"}
{"id": "1311.1597", "contents": "Title: First integrals of ordinary difference equations beyond Lagrangian\n  methods Abstract: A new method for finding first integrals of discrete equations is presented.\nIt can be used for discrete equations which do not possess a variational\n(Lagrangian or Hamiltonian) formulation. The method is based on a newly\nestablished identity which links symmetries of the underlying discrete\nequations, solutions of the discrete adjoint equations and first integrals. The\nmethod is applied to invariant mappings and discretizations of a second order\nand a third order ODEs. In examples the set of independent first integrals\nmakes it possible to find the general solution of the discrete equations. The\nmethod is compared to a direct method of constructing first integrals. \n\n"}
{"id": "1312.1066", "contents": "Title: A new construction of the Drinfeld-Sokolov Hierarchies Abstract: The Drinfeld-Sokolov hierarchies are integrable hierarchies associated with\nevery affine Lie algebra. We present a new construction of such hierarchies,\nwhich only requires the computations of a formal Laurent series. \n\n"}
{"id": "1312.2482", "contents": "Title: Automatic recognition and tagging of topologically different regimes in\n  dynamical systems Abstract: Complex systems are commonly modeled using nonlinear dynamical systems. These\nmodels are often high-dimensional and chaotic. An important goal in studying\nphysical systems through the lens of mathematical models is to determine when\nthe system undergoes changes in qualitative behavior. A detailed description of\nthe dynamics can be difficult or impossible to obtain for high-dimensional and\nchaotic systems. Therefore, a more sensible goal is to recognize and mark\ntransitions of a system between qualitatively different regimes of behavior. In\npractice, one is interested in developing techniques for detection of such\ntransitions from sparse observations, possibly contaminated by noise. In this\npaper we develop a framework to accurately tag different regimes of complex\nsystems based on topological features. In particular, our framework works with\na high degree of success in picking out a cyclically orbiting regime from a\nstationary equilibrium regime in high-dimensional stochastic dynamical systems. \n\n"}
{"id": "1312.5479", "contents": "Title: Sparse similarity-preserving hashing Abstract: In recent years, a lot of attention has been devoted to efficient nearest\nneighbor search by means of similarity-preserving hashing. One of the plights\nof existing hashing techniques is the intrinsic trade-off between performance\nand computational complexity: while longer hash codes allow for lower false\npositive rates, it is very difficult to increase the embedding dimensionality\nwithout incurring in very high false negatives rates or prohibiting\ncomputational costs. In this paper, we propose a way to overcome this\nlimitation by enforcing the hash codes to be sparse. Sparse high-dimensional\ncodes enjoy from the low false positive rates typical of long hashes, while\nkeeping the false negative rates similar to those of a shorter dense hashing\nscheme with equal number of degrees of freedom. We use a tailored feed-forward\nneural network for the hashing function. Extensive experimental evaluation\ninvolving visual and multi-modal data shows the benefits of the proposed\nmethod. \n\n"}
{"id": "1312.6774", "contents": "Title: On the geometry of motions in one integrable problem of the rigid body\n  dynamics Abstract: Due to Poinsot's theorem, the motion of a rigid body about a fixed point is\nrepresented as rolling without slipping of the moving hodograph of the angular\nvelocity over the fixed one. If the moving hodograph is a closed curve,\nvisualization of motion is obtained by the method of P.V.Kharlamov. For an\narbitrary motion in an integrable problem with an axially symmetric force field\nthe moving hodograph densely fills some two-dimensional surface and the fixed\none fills a three-dimensional surface. In this paper, we consider the\nirreducible integrable case in which both hodographs are two-frequency curves.\nWe obtain the equations of bearing surfaces, illustrate the main types of the\nsurfaces. We propose a method of the so-called non-straight geometric\ninterpretation representing the motion of a body as a superposition of two\nperiodic motions. \n\n"}
{"id": "1312.6976", "contents": "Title: B\\\"acklund-Darboux Transformations and Discretizations of Super KdV\n  Equation Abstract: For a generalized super KdV equation, three Darboux transformations and the\ncorresponding B\\\"acklund transformations are constructed. The compatibility of\nthese Darboux transformations leads to three discrete systems and their Lax\nrepresentations. The reduction of one of the B\\\"acklund-Darboux transformations\nand the corresponding discrete system are considered for Kupershmidt's super\nKdV equation. When all the odd variables vanish, a nonlinear superposition\nformula is obtained for Levi's B\\\"acklund transformation for the KdV equation. \n\n"}
{"id": "1401.2325", "contents": "Title: Delay-induced patterns in a two-dimensional lattice of coupled\n  oscillators Abstract: We show how a variety of stable spatio-temporal periodic patterns can be\ncreated in 2D-lattices of coupled oscillators with non-homogeneous coupling\ndelays. A \"hybrid dispersion relation\" is introduced, which allows studying the\nstability of time-periodic patterns analytically in the limit of large delay.\nThe results are illustrated using the FitzHugh-Nagumo coupled neurons as well\nas coupled limit cycle (Stuart-Landau) oscillators. \n\n"}
{"id": "1401.4163", "contents": "Title: Least Squares Shadowing for Sensitivity Analysis of Turbulent Fluid\n  Flows Abstract: Computational methods for sensitivity analysis are invaluable tools for\naerodynamics research and engineering design. However, traditional sensitivity\nanalysis methods break down when applied to long-time averaged quantities in\nturbulent fluid flow fields, specifically those obtained using high-fidelity\nturbulence simulations. This is because of a number of dynamical properties of\nturbulent and chaotic fluid flows, most importantly high sensitivity of the\ninitial value problem, popularly known as the \"butterfly effect\".\n  The recently developed least squares shadowing (LSS) method avoids the issues\nencountered by traditional sensitivity analysis methods by approximating the\n\"shadow trajectory\" in phase space, avoiding the high sensitivity of the\ninitial value problem. The following paper discusses how the least squares\nproblem associated with LSS is solved. Two methods are presented and are\ndemonstrated on a simulation of homogeneous isotropic turbulence and the\nKuramoto-Sivashinsky (KS) equation, a 4th order chaotic partial differential\nequation. We find that while LSS computes fairly accurate gradients, faster,\nmore efficient linear solvers are needed to apply both LSS methods presented in\nthis paper to larger simulations. \n\n"}
{"id": "1401.5268", "contents": "Title: Adapting to a Changing Environment: Non-obvious Thresholds in\n  Multi-Scale Systems Abstract: Many natural and technological systems fail to adapt to changing external\nconditions and move to a different state if the conditions vary too fast. Such\n\"non-adiabatic\" processes are ubiquitous, but little understood. We identify\nthese processes with a new nonlinear phenomenon---an intricate threshold where\na forced system fails to adiabatically follow a changing stable state. In\nsystems with multiple time-scales such thresholds are generic, but non-obvious,\nmeaning they cannot be captured by traditional stability theory. Rather, the\nphenomenon can be analysed using concepts from modern singular perturbation\ntheory: folded singularities and canard trajectories, including composite\ncanards. Thus, non-obvious thresholds should explain the failure to adapt to a\nchanging environment in a wide range of multi-scale systems including: tipping\npoints in the climate system, regime shifts in ecosystems, excitability in\nnerve cells, adaptation failure in regulatory genes, and adiabatic switching in\ntechnology. \n\n"}
{"id": "1401.8126", "contents": "Title: Extrinsic Methods for Coding and Dictionary Learning on Grassmann\n  Manifolds Abstract: Sparsity-based representations have recently led to notable results in\nvarious visual recognition tasks. In a separate line of research, Riemannian\nmanifolds have been shown useful for dealing with features and models that do\nnot lie in Euclidean spaces. With the aim of building a bridge between the two\nrealms, we address the problem of sparse coding and dictionary learning over\nthe space of linear subspaces, which form Riemannian structures known as\nGrassmann manifolds. To this end, we propose to embed Grassmann manifolds into\nthe space of symmetric matrices by an isometric mapping. This in turn enables\nus to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we\npropose closed-form solutions for learning a Grassmann dictionary, atom by\natom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann\nsparse coding and dictionary learning algorithms through embedding into Hilbert\nspaces.\n  Experiments on several classification tasks (gender recognition, gesture\nclassification, scene analysis, face recognition, action recognition and\ndynamic texture classification) show that the proposed approaches achieve\nconsiderable improvements in discrimination accuracy, in comparison to\nstate-of-the-art methods such as kernelized Affine Hull Method and\ngraph-embedding Grassmann discriminant analysis. \n\n"}
{"id": "1402.0240", "contents": "Title: Graph Cuts with Interacting Edge Costs - Examples, Approximations, and\n  Algorithms Abstract: We study an extension of the classical graph cut problem, wherein we replace\nthe modular (sum of edge weights) cost function by a submodular set function\ndefined over graph edges. Special cases of this problem have appeared in\ndifferent applications in signal processing, machine learning, and computer\nvision. In this paper, we connect these applications via the generic\nformulation of \"cooperative graph cuts\", for which we study complexity,\nalgorithms, and connections to polymatroidal network flows. Finally, we compare\nthe proposed algorithms empirically. \n\n"}
{"id": "1403.3995", "contents": "Title: Folding Difference and Differential Systems into Higher Order Equations Abstract: A typical system of k difference (or differential) equations can be\ncompressed, or folded into a difference (or ordinary differential) equation of\norder k. Such foldings appear in control theory as the canonical forms of the\ncontrollability matrices. They are also used in the classification of systems\nof three nonlinear differential equations with chaotic flows by examining the\nresulting jerk functions. The solutions of the higher order equation yield one\nof the components of the system's k-dimensional orbits and the remaining\ncomponents are determined from a set of associated passive equations. The\nfolding algorithm uses a sequence of substitutions and inversions along with\nindex shifts (for difference equations) or higher derivatives (for differential\nequations). For systems of two difference or differential equations this\ncompression process is short and in some cases yields second-order equations\nthat are simpler than the original system. For all systems, the folding\nalgorithm yields detailed amount of information about the structure of the\nsystem and the interdependence of its variables. As with two equations, some\nspecial cases where the derived higher order equation is simpler to analyze\nthan the original system are considered. \n\n"}
{"id": "1403.6520", "contents": "Title: Roaming dynamics in Ketene isomerization Abstract: A reduced two dimensional model is used to study Ketene isomerization\nreaction. In light of recent results by Ulusoy \\textit{et al.} [J.\\ Phys.\\\nChem.\\ A {\\bf 117}, 7553 (2013)], the present work focuses on the\ngeneralization of the roaming mechanism to the Ketene isomerization reaction by\napplying our phase space approach previously used to elucidate the roaming\nphenomenon in ion-molecule reactions. Roaming is again found be associated with\nthe trapping of trajectories in a phase space region between two dividing\nsurfaces; trajectories are classified as reactive or nonreactive, and are\nfurther naturally classified as direct or non-direct (roaming). The latter\nlong-lived trajectories are trapped in the region of non-linear mechanical\nresonances, which in turn define alternative reaction pathways in phase space.\nIt is demonstrated that resonances associated with periodic orbits provide a\ndynamical explanation of the quantum mechanical resonances found in the\nisomerization rate constant calculations by Gezelter and Miller [J.\\ Chem.\\\nPhys.\\ {\\bf 103}, 7868-7876 (1995)]. Evidence of the trapping of trajectories\nby `sticky' resonant periodic orbits is provided by plotting Poincar\\'e\nsurfaces of section, and a gap time analysis is carried out in order to\ninvestigate the statistical assumption inherent in transition state theory for\nKetene isomerization. \n\n"}
{"id": "1404.3293", "contents": "Title: The low-density limit of the Lorentz gas: periodic, aperiodic and random Abstract: The Lorentz gas is one of the simplest, most widely used models to study the\ntransport properties of rarified gases in matter. It describes the dynamics of\na cloud of non-interacting point particles in an infinite array of fixed\nspherical scatterers. More than one hundred years after its conception, it is\nstill a major challenge to understand the nature of the kinetic transport\nequation that governs the macroscopic particle dynamics in the limit of low\nscatterer density (the Boltzmann-Grad limit). Lorentz suggested that this\nequation should be the linear Boltzmann equation. This was confirmed in three\ncelebrated papers by Gallavotti, Spohn, and Boldrighini, Bunimovich and Sinai,\nunder the assumption that the distribution of scatterers is sufficiently\ndisordered. In the case of strongly correlated scatterer configurations (such\nas crystals or quasicrystals), we now understand why the linear Boltzmann\nequation fails and what to substitute it with. A particularly striking feature\nof the periodic Lorentz gas is a heavy tail for the distribution of free path\nlengths, with a diverging second moment, and superdiffusive transport in the\nlimit of large times. \n\n"}
{"id": "1405.0866", "contents": "Title: A General Mechanism of Diffusion in Hamiltonian Systems: Qualitative\n  Results Abstract: We present a general mechanism to establish the existence of diffusing orbits\nin a large class of nearly integrable Hamiltonian systems. Our approach relies\non successive applications of the `outer dynamics' along homoclinic orbits to a\nnormally hyperbolic invariant manifold. The information on the outer dynamics\nis encoded by a geometrically defined map, referred to as the `scattering map'.\n  We find pseudo-orbits of the scattering map that keep advancing in some\nprivileged direction. Then we use the recurrence property of the `inner\ndynamics', restricted to the normally hyperbolic invariant manifold, to return\nto those pseudo-orbits. Finally, we apply topological methods to show the\nexistence of true orbits that follow the successive applications of the two\ndynamics.\n  This method differs, in several crucial aspects, from earlier works. Unlike\nthe well known `two-dynamics' approach, the method we present relies on the\nouter dynamics alone. There are virtually no assumptions on the inner dynamics,\nas its invariant objects (e.g., primary and secondary tori, lower dimensional\nhyperbolic tori and their stable/unstable manifolds, Aubry-Mather sets) are not\nused at all.\n  The method applies to unperturbed Hamiltonians of arbitrary degrees of\nfreedom that are not necessarily convex. In addition, this mechanism is easy to\nverify (analytically or numerically) in concrete examples, as well as to\nestablish diffusion in generic systems.\n  We include several applications, such as bridging large gaps in a priori\nunstable models in any dimension, and establishing diffusion in cases when the\ninner dynamics is a non-twist map. \n\n"}
{"id": "1405.0975", "contents": "Title: Measuring logarithmic corrections to normal diffusion in\n  infinite-horizon billiards Abstract: We perform numerical measurements of the moments of the position of a tracer\nparticle in a two-dimensional periodic billiard model (Lorentz gas) with\ninfinite corridors. This model is known to exhibit a weak form of\nsuper-diffusion, in the sense that there is a logarithmic correction to the\nlinear growth in time of the mean-squared displacement. We show numerically\nthat this expected asymptotic behavior is easily overwhelmed by the subleading\nlinear growth throughout the time-range accessible to numerical simulations. We\ncompare our simulations to the known analytical results for the variance of the\nanomalously-rescaled limiting normal distributions. \n\n"}
{"id": "1405.1835", "contents": "Title: Integrable Discrete Nonautonomous Quad-equations as B\\\"acklund\n  Auto-transformations for Known Volterra and Toda Type Semidiscrete Equations Abstract: We construct integrable discrete nonautonomous quad-equations as B\\\"acklund\nauto-transformations for known Volterra and Toda type semidiscrete equations,\nsome of which are also nonautonomous. Additional examples of this kind are\nfound by using transformations of discrete equations which are invertible on\ntheir solutions. In this way we obtain integrable examples of different types:\ndiscrete analogs of the sine-Gordon equation, the Liouville equation and the\ndressing chain of Shabat. For Liouville type equations we construct general\nsolutions, using a specific linearization. For sine-Gordon type equations we\nfind generalized symmetries, conservation laws and $L-A$ pairs. \n\n"}
{"id": "1405.2316", "contents": "Title: Better Feature Tracking Through Subspace Constraints Abstract: Feature tracking in video is a crucial task in computer vision. Usually, the\ntracking problem is handled one feature at a time, using a single-feature\ntracker like the Kanade-Lucas-Tomasi algorithm, or one of its derivatives.\nWhile this approach works quite well when dealing with high-quality video and\n\"strong\" features, it often falters when faced with dark and noisy video\ncontaining low-quality features. We present a framework for jointly tracking a\nset of features, which enables sharing information between the different\nfeatures in the scene. We show that our method can be employed to track\nfeatures for both rigid and nonrigid motions (possibly of few moving bodies)\neven when some features are occluded. Furthermore, it can be used to\nsignificantly improve tracking results in poorly-lit scenes (where there is a\nmix of good and bad features). Our approach does not require direct modeling of\nthe structure or the motion of the scene, and runs in real time on a single CPU\ncore. \n\n"}
{"id": "1405.2745", "contents": "Title: Quadrirational Yang-Baxter maps and the affine-E8 Painleve lattice Abstract: We establish that the quadrirational Yang-Baxter maps, considered on their\nsymmetry-complete lattice, give an un-normalized form of the Painleve systems\nassociated with affine-E8 symmetry. This is a unified representation bringing\nKdV-type and Painleve-type systems together outside of the usual paradigm of\nreductions. Our approach exploits the geometric characterisation of the\nPainleve equations and the formulation of both kinds of systems in terms of\nbirational groups. \n\n"}
{"id": "1405.3352", "contents": "Title: Newton-Type Iterative Solver for Multiple View $L2$ Triangulation Abstract: In this note, we show that the L2 optimal solutions to most real multiple\nview L2 triangulation problems can be efficiently obtained by two-stage\nNewton-like iterative methods, while the difficulty of such problems mainly\nlies in how to verify the L2 optimality. Such a working two-stage bundle\nadjustment approach features: first, the algorithm is initialized by symmedian\npoint triangulation, a multiple-view generalization of the mid-point method;\nsecond, a symbolic-numeric method is employed to compute derivatives\naccurately; third, globalizing strategy such as line search or trust region is\nsmoothly applied to the underlying iteration which assures algorithm robustness\nin general cases.\n  Numerical comparison with tfml method shows that the local minimizers\nobtained by the two-stage iterative bundle adjustment approach proposed here\nare also the L2 optimal solutions to all the calibrated data sets available\nonline by the Oxford visual geometry group. Extensive numerical experiments\nindicate the bundle adjustment approach solves more than 99% the real\ntriangulation problems optimally. An IEEE 754 double precision C++\nimplementation shows that it takes only about 0.205 second tocompute allthe\n4983 points in the Oxford dinosaur data setvia Gauss-Newton iteration hybrid\nwith a line search strategy on a computer with a 3.4GHz Intel i7 CPU. \n\n"}
{"id": "1406.0132", "contents": "Title: Seeing the Big Picture: Deep Embedding with Contextual Evidences Abstract: In the Bag-of-Words (BoW) model based image retrieval task, the precision of\nvisual matching plays a critical role in improving retrieval performance.\nConventionally, local cues of a keypoint are employed. However, such strategy\ndoes not consider the contextual evidences of a keypoint, a problem which would\nlead to the prevalence of false matches. To address this problem, this paper\ndefines \"true match\" as a pair of keypoints which are similar on three levels,\ni.e., local, regional, and global. Then, a principled probabilistic framework\nis established, which is capable of implicitly integrating discriminative cues\nfrom all these feature levels.\n  Specifically, the Convolutional Neural Network (CNN) is employed to extract\nfeatures from regional and global patches, leading to the so-called \"Deep\nEmbedding\" framework. CNN has been shown to produce excellent performance on a\ndozen computer vision tasks such as image classification and detection, but few\nworks have been done on BoW based image retrieval. In this paper, firstly we\nshow that proper pre-processing techniques are necessary for effective usage of\nCNN feature. Then, in the attempt to fit it into our model, a novel indexing\nstructure called \"Deep Indexing\" is introduced, which dramatically reduces\nmemory usage.\n  Extensive experiments on three benchmark datasets demonstrate that, the\nproposed Deep Embedding method greatly promotes the retrieval accuracy when CNN\nfeature is integrated. We show that our method is efficient in terms of both\nmemory and time cost, and compares favorably with the state-of-the-art methods. \n\n"}
{"id": "1406.2199", "contents": "Title: Two-Stream Convolutional Networks for Action Recognition in Videos Abstract: We investigate architectures of discriminatively trained deep Convolutional\nNetworks (ConvNets) for action recognition in video. The challenge is to\ncapture the complementary information on appearance from still frames and\nmotion between frames. We also aim to generalise the best performing\nhand-crafted features within a data-driven learning framework.\n  Our contribution is three-fold. First, we propose a two-stream ConvNet\narchitecture which incorporates spatial and temporal networks. Second, we\ndemonstrate that a ConvNet trained on multi-frame dense optical flow is able to\nachieve very good performance in spite of limited training data. Finally, we\nshow that multi-task learning, applied to two different action classification\ndatasets, can be used to increase the amount of training data and improve the\nperformance on both.\n  Our architecture is trained and evaluated on the standard video actions\nbenchmarks of UCF-101 and HMDB-51, where it is competitive with the state of\nthe art. It also exceeds by a large margin previous attempts to use deep nets\nfor video classification. \n\n"}
{"id": "1406.4465", "contents": "Title: Multi-stage Multi-task feature learning via adaptive threshold Abstract: Multi-task feature learning aims to identity the shared features among tasks\nto improve generalization. It has been shown that by minimizing non-convex\nlearning models, a better solution than the convex alternatives can be\nobtained. Therefore, a non-convex model based on the capped-$\\ell_{1},\\ell_{1}$\nregularization was proposed in \\cite{Gong2013}, and a corresponding efficient\nmulti-stage multi-task feature learning algorithm (MSMTFL) was presented.\nHowever, this algorithm harnesses a prescribed fixed threshold in the\ndefinition of the capped-$\\ell_{1},\\ell_{1}$ regularization and the lack of\nadaptivity might result in suboptimal performance. In this paper we propose to\nemploy an adaptive threshold in the capped-$\\ell_{1},\\ell_{1}$ regularized\nformulation, where the corresponding variant of MSMTFL will incorporate an\nadditional component to adaptively determine the threshold value. This variant\nis expected to achieve a better feature selection performance over the original\nMSMTFL algorithm. In particular, the embedded adaptive threshold component\ncomes from our previously proposed iterative support detection (ISD) method\n\\cite{Wang2010}. Empirical studies on both synthetic and real-world data sets\ndemonstrate the effectiveness of this new variant over the original MSMTFL. \n\n"}
{"id": "1406.5266", "contents": "Title: Web-Scale Training for Face Identification Abstract: Scaling machine learning methods to very large datasets has attracted\nconsiderable attention in recent years, thanks to easy access to ubiquitous\nsensing and data from the web. We study face recognition and show that three\ndistinct properties have surprising effects on the transferability of deep\nconvolutional networks (CNN): (1) The bottleneck of the network serves as an\nimportant transfer learning regularizer, and (2) in contrast to the common\nwisdom, performance saturation may exist in CNN's (as the number of training\nsamples grows); we propose a solution for alleviating this by replacing the\nnaive random subsampling of the training set with a bootstrapping process.\nMoreover, (3) we find a link between the representation norm and the ability to\ndiscriminate in a target domain, which sheds lights on how such networks\nrepresent faces. Based on these discoveries, we are able to improve face\nrecognition accuracy on the widely used LFW benchmark, both in the verification\n(1:1) and identification (1:N) protocols, and directly compare, for the first\ntime, with the state of the art Commercially-Off-The-Shelf system and show a\nsizable leap in performance. \n\n"}
{"id": "1406.5679", "contents": "Title: Deep Fragment Embeddings for Bidirectional Image Sentence Mapping Abstract: We introduce a model for bidirectional retrieval of images and sentences\nthrough a multi-modal embedding of visual and natural language data. Unlike\nprevious models that directly map images or sentences into a common embedding\nspace, our model works on a finer level and embeds fragments of images\n(objects) and fragments of sentences (typed dependency tree relations) into a\ncommon space. In addition to a ranking objective seen in previous work, this\nallows us to add a new fragment alignment objective that learns to directly\nassociate these fragments across modalities. Extensive experimental evaluation\nshows that reasoning on both the global level of images and sentences and the\nfiner level of their respective fragments significantly improves performance on\nimage-sentence retrieval tasks. Additionally, our model provides interpretable\npredictions since the inferred inter-modal fragment alignment is explicit. \n\n"}
{"id": "1407.0221", "contents": "Title: Imaging with Kantorovich-Rubinstein discrepancy Abstract: We propose the use of the Kantorovich-Rubinstein norm from optimal transport\nin imaging problems. In particular, we discuss a variational regularisation\nmodel endowed with a Kantorovich-Rubinstein discrepancy term and total\nvariation regularization in the context of image denoising and cartoon-texture\ndecomposition. We point out connections of this approach to several other\nrecently proposed methods such as total generalized variation and norms\ncapturing oscillating patterns. We also show that the respective optimization\nproblem can be turned into a convex-concave saddle point problem with simple\nconstraints and hence, can be solved by standard tools. Numerical examples\nexhibit interesting features and favourable performance for denoising and\ncartoon-texture decomposition. \n\n"}
{"id": "1407.5194", "contents": "Title: Darboux transformation and classification of solution for mixed coupled\n  nonlinear Schr\\\"odinger equations Abstract: We derive generalized nonlinear wave solution formula for mixed coupled\nnonlinear Sch\\\"odinger equations (mCNLSE) by performing the unified Darboux\ntransformation. We give the classification of the general soliton formula on\nthe nonzero background based on the dynamical behavior. Especially, the\nconditions for breather, dark soliton and rogue wave solution for mCNLSE are\ngiven in detail. Moreover, we analysis the interaction between dark-dark\nsoliton solution and breather solution. These results would be helpful for\nnonlinear localized wave excitations and applications in vector nonlinear\nsystems. \n\n"}
{"id": "1407.8070", "contents": "Title: Weak chimeras in minimal networks of coupled phase oscillators Abstract: We suggest a definition for a type of chimera state that appears in networks\nof indistinguishable phase oscillators. Defining a \"weak chimera\" as a type of\ninvariant set showing partial frequency synchronization, we show that this\nmeans they cannot appear in phase oscillator networks that are either globally\ncoupled or too small. We exhibit various networks of four, six and ten\nindistinguishable oscillators where weak chimeras exist with various dynamics\nand stabilities. We examine the role of Kuramoto-Sakaguchi coupling in giving\ndegenerate (neutrally stable) families of weak chimera states in these example\nnetworks. \n\n"}
{"id": "1408.0349", "contents": "Title: Machta-Zwanzig regime of anomalous diffusion in infinite-horizon\n  billiards Abstract: We study diffusion on a periodic billiard table with infinite horizon in the\nlimit of narrow corridors. An effective trapping mechanism emerges according to\nwhich the process can be modeled by a L\\'evy walk combining\nexponentially-distributed trapping times with free propagation along paths\nwhose precise probabilities we compute. This description yields an\napproximation of the mean squared displacement of infinite-horizon billiards in\nterms of two transport coefficients which generalizes to this anomalous regime\nthe Machta-Zwanzig approximation of normal diffusion in finite-horizon\nbilliards [Phys. Rev. Lett. 50, 1959 (1983)]. \n\n"}
{"id": "1409.3505", "contents": "Title: DeepID-Net: multi-stage and deformable deep convolutional neural\n  networks for object detection Abstract: In this paper, we propose multi-stage and deformable deep convolutional\nneural networks for object detection. This new deep learning object detection\ndiagram has innovations in multiple aspects. In the proposed new deep\narchitecture, a new deformation constrained pooling (def-pooling) layer models\nthe deformation of object parts with geometric constraint and penalty. With the\nproposed multi-stage training strategy, multiple classifiers are jointly\noptimized to process samples at different difficulty levels. A new pre-training\nstrategy is proposed to learn feature representations more suitable for the\nobject detection task and with good generalization capability. By changing the\nnet structures, training strategies, adding and removing some key components in\nthe detection pipeline, a set of models with large diversity are obtained,\nwhich significantly improves the effectiveness of modeling averaging. The\nproposed approach ranked \\#2 in ILSVRC 2014. It improves the mean averaged\nprecision obtained by RCNN, which is the state-of-the-art of object detection,\nfrom $31\\%$ to $45\\%$. Detailed component-wise analysis is also provided\nthrough extensive experimental evaluation. \n\n"}
{"id": "1409.7168", "contents": "Title: On the Lagrangian structure of Calogero's Goldfish model Abstract: The discrete-time rational Calogero's goldfish system is obtained from the\nAnsatz Lax pair. The discrete-time Lagrangians of the system possess the\ndiscrete-time 1-form structure as those in the discrete-time Calogero-Moser\nsystem and discrete-time Ruijsenaars-Schneider system. Performing two steps of\ncontinuum limits, we obtain Lagrangian hierarchy for the system. Expectingly,\nthe continuous-time Lagrange 1-form structure of the system holds. Furthermore,\nthe connection to the lattice KP systems is also established. \n\n"}
{"id": "1410.2914", "contents": "Title: Automorphic Lie Algebras with dihedral symmetry Abstract: The concept of Automorphic Lie Algebras arises in the context of reduction\ngroups introduced in the early 1980s in the field of integrable systems.\nAutomorphic Lie Algebras are obtained by imposing a discrete group symmetry on\na current algebra of Krichever-Novikov type. Past work shows remarkable\nuniformity between algebras associated to different reduction groups. For\nexample, if the base Lie algebra is $\\mathfrak{sl}_2(\\mathbb{C})$ and the poles\nof the Automorphic Lie Algebra are restricted to an exceptional orbit of the\nsymmetry group, changing the reduction group does not affect the Lie algebra\nstructure. In the present research we fix the reduction group to be the\ndihedral group and vary the orbit of poles as well as the group action on the\nbase Lie algebra. We find a uniform description of Automorphic Lie Algebras\nwith dihedral symmetry, valid for poles at exceptional and generic orbits. \n\n"}
{"id": "1410.7448", "contents": "Title: Synchronization of Heterogeneous Kuramoto Oscillators with Arbitrary\n  Topology Abstract: We study synchronization of coupled Kuramoto oscillators with heterogeneous\ninherent frequencies and general underlying connectivity. We provide conditions\non the coupling strength and the initial phases which guarantee the existence\nof a Positively Invariant Set (PIS) and lead to synchronization. Unlike\nprevious works that focus only on analytical bounds, here we introduce an\noptimization approach to provide a computational-analytical bound that can\nfurther exploit the particular features of each individual system such as\ntopology and frequency distribution. Examples are provided to illustrate our\nresults as well as the improvement over previous existing bounds. \n\n"}
{"id": "1411.1045", "contents": "Title: Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on\n  ImageNet Abstract: Recent results suggest that state-of-the-art saliency models perform far from\noptimal in predicting fixations. This lack in performance has been attributed\nto an inability to model the influence of high-level image features such as\nobjects. Recent seminal advances in applying deep neural networks to tasks like\nobject recognition suggests that they are able to capture this kind of\nstructure. However, the enormous amount of training data necessary to train\nthese networks makes them difficult to apply directly to saliency prediction.\nWe present a novel way of reusing existing neural networks that have been\npretrained on the task of object recognition in models of fixation prediction.\nUsing the well-known network of Krizhevsky et al. (2012), we come up with a new\nsaliency model that significantly outperforms all state-of-the-art models on\nthe MIT Saliency Benchmark. We show that the structure of this network allows\nnew insights in the psychophysics of fixation selection and potentially their\nneural implementation. To train our network, we build on recent work on the\nmodeling of saliency as point processes. \n\n"}
{"id": "1411.1830", "contents": "Title: Introduction to the R package TDA Abstract: We present a short tutorial and introduction to using the R package TDA,\nwhich provides some tools for Topological Data Analysis. In particular, it\nincludes implementations of functions that, given some data, provide\ntopological information about the underlying space, such as the distance\nfunction, the distance to a measure, the kNN density estimator, the kernel\ndensity estimator, and the kernel distance. The salient topological features of\nthe sublevel sets (or superlevel sets) of these functions can be quantified\nwith persistent homology. We provide an R interface for the efficient\nalgorithms of the C++ libraries GUDHI, Dionysus and PHAT, including a function\nfor the persistent homology of the Rips filtration, and one for the persistent\nhomology of sublevel sets (or superlevel sets) of arbitrary functions evaluated\nover a grid of points. The significance of the features in the resulting\npersistence diagrams can be analyzed with functions that implement recently\ndeveloped statistical methods. The R package TDA also includes the\nimplementation of an algorithm for density clustering, which allows us to\nidentify the spatial organization of the probability mass associated to a\ndensity function and visualize it by means of a dendrogram, the cluster tree. \n\n"}
{"id": "1411.2861", "contents": "Title: Computational Baby Learning Abstract: Intuitive observations show that a baby may inherently possess the capability\nof recognizing a new visual concept (e.g., chair, dog) by learning from only\nvery few positive instances taught by parent(s) or others, and this recognition\ncapability can be gradually further improved by exploring and/or interacting\nwith the real instances in the physical world. Inspired by these observations,\nwe propose a computational model for slightly-supervised object detection,\nbased on prior knowledge modelling, exemplar learning and learning with video\ncontexts. The prior knowledge is modeled with a pre-trained Convolutional\nNeural Network (CNN). When very few instances of a new concept are given, an\ninitial concept detector is built by exemplar learning over the deep features\nfrom the pre-trained CNN. Simulating the baby's interaction with physical\nworld, the well-designed tracking solution is then used to discover more\ndiverse instances from the massive online unlabeled videos. Once a positive\ninstance is detected/identified with high score in each video, more variable\ninstances possibly from different view-angles and/or different distances are\ntracked and accumulated. Then the concept detector can be fine-tuned based on\nthese new instances. This process can be repeated again and again till we\nobtain a very mature concept detector. Extensive experiments on Pascal\nVOC-07/10/12 object detection datasets well demonstrate the effectiveness of\nour framework. It can beat the state-of-the-art full-training based\nperformances by learning from very few samples for each object category, along\nwith about 20,000 unlabeled videos. \n\n"}
{"id": "1411.5066", "contents": "Title: Chaotic Disintegration of the Inner Solar System Abstract: On timescales that greatly exceed an orbital period, typical planetary orbits\nevolve in a stochastic yet stable fashion. On even longer timescales, however,\nplanetary orbits can spontaneously transition from bounded to unbound chaotic\nstates. Large-scale instabilities associated with such behavior appear to play\na dominant role in shaping the architectures of planetary systems, including\nour own. Here we show how such transitions are possible, focusing on the\nspecific case of the long-term evolution of Mercury. We develop a simple\nanalytical model for Mercury's dynamics and elucidate the origins of its short\nterm stochastic behavior as well as of its sudden progression to unbounded\nchaos. Our model allows us to estimate the timescale on which this transition\nis likely to be triggered, i.e. the dynamical lifetime of the Solar System as\nwe know it. The formulated theory is consistent with the results of numerical\nsimulations and is broadly applicable to extrasolar planetary systems dominated\nby secular interactions. These results constitute a significant advancement in\nour understanding of the processes responsible for sculpting of the dynamical\nstructures of generic planetary systems. \n\n"}
{"id": "1412.1628", "contents": "Title: Fisher Kernel for Deep Neural Activations Abstract: Compared to image representation based on low-level local descriptors, deep\nneural activations of Convolutional Neural Networks (CNNs) are richer in\nmid-level representation, but poorer in geometric invariance properties. In\nthis paper, we present a straightforward framework for better image\nrepresentation by combining the two approaches. To take advantages of both\nrepresentations, we propose an efficient method to extract a fair amount of\nmulti-scale dense local activations from a pre-trained CNN. We then aggregate\nthe activations by Fisher kernel framework, which has been modified with a\nsimple scale-wise normalization essential to make it suitable for CNN\nactivations. Replacing the direct use of a single activation vector with our\nrepresentation demonstrates significant performance improvements: +17.76 (Acc.)\non MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest that\nour proposal can be used as a primary image representation for better\nperformances in visual recognition tasks. \n\n"}
{"id": "1412.2306", "contents": "Title: Deep Visual-Semantic Alignments for Generating Image Descriptions Abstract: We present a model that generates natural language descriptions of images and\ntheir regions. Our approach leverages datasets of images and their sentence\ndescriptions to learn about the inter-modal correspondences between language\nand visual data. Our alignment model is based on a novel combination of\nConvolutional Neural Networks over image regions, bidirectional Recurrent\nNeural Networks over sentences, and a structured objective that aligns the two\nmodalities through a multimodal embedding. We then describe a Multimodal\nRecurrent Neural Network architecture that uses the inferred alignments to\nlearn to generate novel descriptions of image regions. We demonstrate that our\nalignment model produces state of the art results in retrieval experiments on\nFlickr8K, Flickr30K and MSCOCO datasets. We then show that the generated\ndescriptions significantly outperform retrieval baselines on both full images\nand on a new dataset of region-level annotations. \n\n"}
{"id": "1412.2604", "contents": "Title: Actions and Attributes from Wholes and Parts Abstract: We investigate the importance of parts for the tasks of action and attribute\nclassification. We develop a part-based approach by leveraging convolutional\nnetwork features inspired by recent advances in computer vision. Our part\ndetectors are a deep version of poselets and capture parts of the human body\nunder a distinct set of poses. For the tasks of action and attribute\nclassification, we train holistic convolutional neural networks and show that\nadding parts leads to top-performing results for both tasks. In addition, we\ndemonstrate the effectiveness of our approach when we replace an oracle person\ndetector, as is the default in the current evaluation protocol for both tasks,\nwith a state-of-the-art person detection system. \n\n"}
{"id": "1412.5686", "contents": "Title: The redemption of singularity confinement Abstract: We present a novel way to apply the singularity confinement property as a\ndiscrete integrability criterion. We shall use what we call a full\ndeautonomisation approach, which consists in treating the free parameters in\nthe mapping as functions of the independent variable, applied to a mapping\ncomplemented with terms that are absent in the original mapping but which do\nnot change the singularity structure. We shall show, on a host of examples\nincluding the well-known mapping of Hietarinta-Viallet, that our approach\noffers a way to compute the algebraic entropy for these mappings exactly,\nthereby allowing one to distinguish between the integrable and non-integrable\ncases even when both have confined singularities. \n\n"}
{"id": "1412.6607", "contents": "Title: Visual Scene Representations: Contrast, Scaling and Occlusion Abstract: We study the structure of representations, defined as approximations of\nminimal sufficient statistics that are maximal invariants to nuisance factors,\nfor visual data subject to scaling and occlusion of line-of-sight. We derive\nanalytical expressions for such representations and show that, under certain\nrestrictive assumptions, they are related to features commonly in use in the\ncomputer vision community. This link highlights the condition tacitly assumed\nby these descriptors, and also suggests ways to improve and generalize them.\nThis new interpretation draws connections to the classical theories of\nsampling, hypothesis testing and group invariance. \n\n"}
{"id": "1412.7725", "contents": "Title: Automatic Photo Adjustment Using Deep Neural Networks Abstract: Photo retouching enables photographers to invoke dramatic visual impressions\nby artistically enhancing their photos through stylistic color and tone\nadjustments. However, it is also a time-consuming and challenging task that\nrequires advanced skills beyond the abilities of casual photographers. Using an\nautomated algorithm is an appealing alternative to manual work but such an\nalgorithm faces many hurdles. Many photographic styles rely on subtle\nadjustments that depend on the image content and even its semantics. Further,\nthese adjustments are often spatially varying. Because of these\ncharacteristics, existing automatic algorithms are still limited and cover only\na subset of these challenges. Recently, deep machine learning has shown unique\nabilities to address hard problems that resisted machine algorithms for long.\nThis motivated us to explore the use of deep learning in the context of photo\nediting. In this paper, we explain how to formulate the automatic photo\nadjustment problem in a way suitable for this approach. We also introduce an\nimage descriptor that accounts for the local semantics of an image. Our\nexperiments demonstrate that our deep learning formulation applied using these\ndescriptors successfully capture sophisticated photographic styles. In\nparticular and unlike previous techniques, it can model local adjustments that\ndepend on the image semantics. We show on several examples that this yields\nresults that are qualitatively and quantitatively better than previous work. \n\n"}
{"id": "1412.8070", "contents": "Title: Functional correspondence by matrix completion Abstract: In this paper, we consider the problem of finding dense intrinsic\ncorrespondence between manifolds using the recently introduced functional\nframework. We pose the functional correspondence problem as matrix completion\nwith manifold geometric structure and inducing functional localization with the\n$L_1$ norm. We discuss efficient numerical procedures for the solution of our\nproblem. Our method compares favorably to the accuracy of state-of-the-art\ncorrespondence algorithms on non-rigid shape matching benchmarks, and is\nespecially advantageous in settings when only scarce data is available. \n\n"}
{"id": "1501.00680", "contents": "Title: A New Method for Signal and Image Analysis: The Square Wave Method Abstract: A brief review is provided of the use of the Square Wave Method (SWM) in the\nfield of signal and image analysis and it is specified how results thus\nobtained are expressed using the Square Wave Transform (SWT), in the frequency\ndomain. To illustrate the new approach introduced in this field, the results of\ntwo cases are analyzed: a) a sequence of samples (that is, measured values) of\nan electromyographic recording; and b) the classic image of Lenna. \n\n"}
{"id": "1501.04711", "contents": "Title: DeepHash: Getting Regularization, Depth and Fine-Tuning Right Abstract: This work focuses on representing very high-dimensional global image\ndescriptors using very compact 64-1024 bit binary hashes for instance\nretrieval. We propose DeepHash: a hashing scheme based on deep networks. Key to\nmaking DeepHash work at extremely low bitrates are three important\nconsiderations -- regularization, depth and fine-tuning -- each requiring\nsolutions specific to the hashing problem. In-depth evaluation shows that our\nscheme consistently outperforms state-of-the-art methods across all data sets\nfor both Fisher Vectors and Deep Convolutional Neural Network features, by up\nto 20 percent over other schemes. The retrieval performance with 256-bit hashes\nis close to that of the uncompressed floating point features -- a remarkable\n512 times compression. \n\n"}
{"id": "1501.06209", "contents": "Title: Parallel Magnetic Resonance Imaging Abstract: The main disadvantage of Magnetic Resonance Imaging (MRI) are its long scan\ntimes and, in consequence, its sensitivity to motion. Exploiting the\ncomplementary information from multiple receive coils, parallel imaging is able\nto recover images from under-sampled k-space data and to accelerate the\nmeasurement. Because parallel magnetic resonance imaging can be used to\naccelerate basically any imaging sequence it has many important applications.\nParallel imaging brought a fundamental shift in image reconstruction: Image\nreconstruction changed from a simple direct Fourier transform to the solution\nof an ill-conditioned inverse problem. This work gives an overview of image\nreconstruction from the perspective of inverse problems. After introducing\nbasic concepts such as regularization, discretization, and iterative\nreconstruction, advanced topics are discussed including algorithms for\nauto-calibration, the connection to approximation theory, and the combination\nwith compressed sensing. \n\n"}
{"id": "1502.00192", "contents": "Title: Pose and Shape Estimation with Discriminatively Learned Parts Abstract: We introduce a new approach for estimating the 3D pose and the 3D shape of an\nobject from a single image. Given a training set of view exemplars, we learn\nand select appearance-based discriminative parts which are mapped onto the 3D\nmodel from the training set through a facil- ity location optimization. The\ntraining set of 3D models is summarized into a sparse set of shapes from which\nwe can generalize by linear combination. Given a test picture, we detect\nhypotheses for each part. The main challenge is to select from these hypotheses\nand compute the 3D pose and shape coefficients at the same time. To achieve\nthis, we optimize a function that minimizes simultaneously the geometric\nreprojection error as well as the appearance matching of the parts. We apply\nthe alternating direction method of multipliers (ADMM) to minimize the\nresulting convex function. We evaluate our approach on the Fine Grained 3D Car\ndataset with superior performance in shape and pose errors. Our main and novel\ncontribution is the simultaneous solution for part localization, 3D pose and\nshape by maximizing both geometric and appearance compatibility. \n\n"}
{"id": "1502.01853", "contents": "Title: Generalized Inpainting Method for Hyperspectral Image Acquisition Abstract: A recently designed hyperspectral imaging device enables multiplexed\nacquisition of an entire data volume in a single snapshot thanks to\nmonolithically-integrated spectral filters. Such an agile imaging technique\ncomes at the cost of a reduced spatial resolution and the need for a\ndemosaicing procedure on its interleaved data. In this work, we address both\nissues and propose an approach inspired by recent developments in compressed\nsensing and analysis sparse models. We formulate our superresolution and\ndemosaicing task as a 3-D generalized inpainting problem. Interestingly, the\ntarget spatial resolution can be adjusted for mitigating the compression level\nof our sensing. The reconstruction procedure uses a fast greedy method called\nPseudo-inverse IHT. We also show on simulations that a random arrangement of\nthe spectral filters on the sensor is preferable to regular mosaic layout as it\nimproves the quality of the reconstruction. The efficiency of our technique is\ndemonstrated through numerical experiments on both synthetic and real data as\nacquired by the snapshot imager. \n\n"}
{"id": "1502.04275", "contents": "Title: segDeepM: Exploiting Segmentation and Context in Deep Neural Networks\n  for Object Detection Abstract: In this paper, we propose an approach that exploits object segmentation in\norder to improve the accuracy of object detection. We frame the problem as\ninference in a Markov Random Field, in which each detection hypothesis scores\nobject appearance as well as contextual information using Convolutional Neural\nNetworks, and allows the hypothesis to choose and score a segment out of a\nlarge pool of accurate object segmentation proposals. This enables the detector\nto incorporate additional evidence when it is available and thus results in\nmore accurate detections. Our experiments show an improvement of 4.1% in mAP\nover the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the current\nstate-of-the-art, demonstrating the power of our approach. \n\n"}
{"id": "1502.04681", "contents": "Title: Unsupervised Learning of Video Representations using LSTMs Abstract: We use multilayer Long Short Term Memory (LSTM) networks to learn\nrepresentations of video sequences. Our model uses an encoder LSTM to map an\ninput sequence into a fixed length representation. This representation is\ndecoded using single or multiple decoder LSTMs to perform different tasks, such\nas reconstructing the input sequence, or predicting the future sequence. We\nexperiment with two kinds of input sequences - patches of image pixels and\nhigh-level representations (\"percepts\") of video frames extracted using a\npretrained convolutional net. We explore different design choices such as\nwhether the decoder LSTMs should condition on the generated output. We analyze\nthe outputs of the model qualitatively to see how well the model can\nextrapolate the learned video representation into the future and into the past.\nWe try to visualize and interpret the learned features. We stress test the\nmodel by running it on longer time scales and on out-of-domain data. We further\nevaluate the representations by finetuning them for a supervised learning\nproblem - human action recognition on the UCF-101 and HMDB-51 datasets. We show\nthat the representations help improve classification accuracy, especially when\nthere are only a few training examples. Even models pretrained on unrelated\ndatasets (300 hours of YouTube videos) can help action recognition performance. \n\n"}
{"id": "1502.05082", "contents": "Title: What makes for effective detection proposals? Abstract: Current top performing object detectors employ detection proposals to guide\nthe search for objects, thereby avoiding exhaustive sliding window search\nacross images. Despite the popularity and widespread use of detection\nproposals, it is unclear which trade-offs are made when using them during\nobject detection. We provide an in-depth analysis of twelve proposal methods\nalong with four baselines regarding proposal repeatability, ground truth\nannotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM,\nR-CNN, and Fast R-CNN detection performance. Our analysis shows that for object\ndetection improving proposal localisation accuracy is as important as improving\nrecall. We introduce a novel metric, the average recall (AR), which rewards\nboth high recall and good localisation and correlates surprisingly well with\ndetection performance. Our findings show common strengths and weaknesses of\nexisting methods, and provide insights and metrics for selecting and tuning\nproposal methods. \n\n"}
{"id": "1502.06137", "contents": "Title: Synchronization of Heterogeneous Kuramoto Oscillators with Graphs of\n  Diameter Two Abstract: In this article we study synchronization of Kuramoto oscillators with\nheterogeneous frequencies, and where underlying topology is a graph of diameter\ntwo. When the coupling strengths between every two connected oscillators are\nthe same, we find an analytic condition that guarantees an existence of a\nPositively Invariant Set (PIS) and demonstrate that existence of a PIS suffices\nfor frequency synchronization. For graphs of diameter two, this synchronization\ncondition is significantly better than existing general conditions for an\narbitrary topology. If the coupling strengths can be different for different\npairs of connected oscillators, we formulate an optimization problem that finds\nsufficient for synchronization coupling strengths such that their sum is\nminimal. \n\n"}
{"id": "1502.07058", "contents": "Title: Evaluation of Deep Convolutional Nets for Document Image Classification\n  and Retrieval Abstract: This paper presents a new state-of-the-art for document image classification\nand retrieval, using features learned by deep convolutional neural networks\n(CNNs). In object and scene analysis, deep neural nets are capable of learning\na hierarchical chain of abstraction from pixel inputs to concise and\ndescriptive representations. The current work explores this capacity in the\nrealm of document analysis, and confirms that this representation strategy is\nsuperior to a variety of popular hand-crafted alternatives. Experiments also\nshow that (i) features extracted from CNNs are robust to compression, (ii) CNNs\ntrained on non-document images transfer well to document analysis tasks, and\n(iii) enforcing region-specific feature-learning is unnecessary given\nsufficient training data. This work also makes available a new labelled subset\nof the IIT-CDIP collection, containing 400,000 document images across 16\ncategories, useful for training new CNNs for document analysis. \n\n"}
{"id": "1503.01804", "contents": "Title: Frequency Domain TOF: Encoding Object Depth in Modulation Frequency Abstract: Time of flight cameras may emerge as the 3-D sensor of choice. Today, time of\nflight sensors use phase-based sampling, where the phase delay between emitted\nand received, high-frequency signals encodes distance. In this paper, we\npresent a new time of flight architecture that relies only on frequency---we\nrefer to this technique as frequency-domain time of flight (FD-TOF). Inspired\nby optical coherence tomography (OCT), FD-TOF excels when frequency bandwidth\nis high. With the increasing frequency of TOF sensors, new challenges to time\nof flight sensing continue to emerge. At high frequencies, FD-TOF offers\nseveral potential benefits over phase-based time of flight methods. \n\n"}
{"id": "1503.01805", "contents": "Title: Periodic and chaotic orbits of plane-confined micro-rotors in creeping\n  flows Abstract: We explore theoretically the complex dynamics and emergent behaviors of\nspinning spheres immersed in viscous fluid. The particles are coupled to\neach-other via the fluid in which they are suspended: each particle disturbs\nthe surrounding fluid with a rotlet field and that fluid flow affects the\nmotion of the other particles. We notice the emergence of intricate periodic or\nchaotic trajectories that depend on the rotors initial position and separation.\nThe point-rotor motions confined to a plane bear similarities the classic 2D\npoint-vortex dynamics. Our analyses highlight the complexity of the interaction\nbetween just a few rotors and suggest richer behavior in denser populations. We\ndiscuss how the model gives insight into more complex systems and suggest\npossible extensions for future theoretical studies. \n\n"}
{"id": "1503.02351", "contents": "Title: Fully Connected Deep Structured Networks Abstract: Convolutional neural networks with many layers have recently been shown to\nachieve excellent results on many high-level tasks such as image\nclassification, object detection and more recently also semantic segmentation.\nParticularly for semantic segmentation, a two-stage procedure is often\nemployed. Hereby, convolutional networks are trained to provide good local\npixel-wise features for the second step being traditionally a more global\ngraphical model. In this work we unify this two-stage process into a single\njoint training algorithm. We demonstrate our method on the semantic image\nsegmentation task and show encouraging results on the challenging PASCAL VOC\n2012 dataset. \n\n"}
{"id": "1503.03637", "contents": "Title: On Computing the Translations Norm in the Epipolar Graph Abstract: This paper deals with the problem of recovering the unknown norm of relative\ntranslations between cameras based on the knowledge of relative rotations and\ntranslation directions. We provide theoretical conditions for the solvability\nof such a problem, and we propose a two-stage method to solve it. First, a\ncycle basis for the epipolar graph is computed, then all the scaling factors\nare recovered simultaneously by solving a homogeneous linear system. We\ndemonstrate the accuracy of our solution by means of synthetic and real\nexperiments. \n\n"}
{"id": "1503.06465", "contents": "Title: Lifting Object Detection Datasets into 3D Abstract: While data has certainly taken the center stage in computer vision in recent\nyears, it can still be difficult to obtain in certain scenarios. In particular,\nacquiring ground truth 3D shapes of objects pictured in 2D images remains a\nchallenging feat and this has hampered progress in recognition-based object\nreconstruction from a single image. Here we propose to bypass previous\nsolutions such as 3D scanning or manual design, that scale poorly, and instead\npopulate object category detection datasets semi-automatically with dense,\nper-object 3D reconstructions, bootstrapped from:(i) class labels, (ii) ground\ntruth figure-ground segmentations and (iii) a small set of keypoint\nannotations. Our proposed algorithm first estimates camera viewpoint using\nrigid structure-from-motion and then reconstructs object shapes by optimizing\nover visual hull proposals guided by loose within-class shape similarity\nassumptions. The visual hull sampling process attempts to intersect an object's\nprojection cone with the cones of minimal subsets of other similar objects\namong those pictured from certain vantage points. We show that our method is\nable to produce convincing per-object 3D reconstructions and to accurately\nestimate cameras viewpoints on one of the most challenging existing\nobject-category detection datasets, PASCAL VOC. We hope that our results will\nre-stimulate interest on joint object recognition and 3D reconstruction from a\nsingle image. \n\n"}
{"id": "1503.09187", "contents": "Title: A curved Henon-Heiles system and its integrable perturbations Abstract: The constant curvature analogue on the two-dimensional sphere and the\nhyperbolic space of the integrable H\\'enon-Heiles Hamiltonian $\\mathcal{H}$\ngiven by $$ \\mathcal{H}=\\dfrac{1}{2}(p_{1}^{2}+p_{2}^{2})+ \\Omega\n\\left(q_{1}^{2}+ 4 q_{2}^{2}\\right) +\\alpha \\left(q_{1}^{2}q_{2}+2\nq_{2}^{3}\\right), $$ where $\\Omega$ and $\\alpha$ are real constants, is\nrevisited. The resulting integrable curved Hamiltonian, $\\mathcal{H}_\\kappa$,\ndepends on a parameter $\\kappa$ which is just the curvature of the underlying\nspace and allows one to recover $\\mathcal{H}$ under the smooth flat/Euclidean\nlimit $\\kappa\\to 0$. This system can be regarded as an integrable cubic\nperturbation of a specific curved $1:2$ anisotropic oscillator, which was\nalready known in the literature. The Ramani-Dorizzi-Grammaticos (RDG) series of\npotentials associated to $\\mathcal{H}_\\kappa$ is fully constructed, and\ncorresponds to the curved integrable analogues of homogeneous polynomial\nperturbations of $\\mathcal{H}$ that are separable in parabolic coordinates.\nIntegrable perturbations of $\\mathcal{H}_\\kappa$ are also fully presented, and\nthey can be regarded as the curved counterpart of integrable rational\nperturbations of the Euclidean Hamiltonian $\\mathcal{H}$. It will be explicitly\nshown that the latter perturbations can be understood as the \"negative index\"\ncounterpart of the curved RDG series of potentials. Furthermore, it is shown\nthat the integrability of the curved H\\'enon-Heiles Hamiltonian\n$\\mathcal{H}_\\kappa$ is preserved under the simultaneous addition of curved\nanalogues of \"positive\" and \"negative\" families of RDG potentials. \n\n"}
{"id": "1504.00051", "contents": "Title: Dynamical Evolution of Multi-Resonant Systems: the Case of GJ876 Abstract: The GJ876 system was among the earliest multi-planetary detections outside of\nthe Solar System, and has long been known to harbor a resonant pair of giant\nplanets. Subsequent characterization of the system revealed the presence of an\nadditional Neptune mass object on an external orbit, locked in a three body\nLaplace mean motion resonance with the previously known planets. While this\nsystem is currently the only known extrasolar example of a Laplace resonance,\nit differs from the Galilean satellites in that the orbital motion of the\nplanets is known to be chaotic. In this work, we present a simple perturbative\nmodel that illuminates the origins of stochasticity inherent to this system and\nderive analytic estimates of the Lyapunov time as well as the chaotic diffusion\ncoefficient. We then address the formation of the multi-resonant structure\nwithin a protoplanetary disk and show that modest turbulent forcing in addition\nto dissipative effects is required to reproduce the observed chaotic\nconfiguration. Accordingly, this work places important constraints on the\ntypical formation environments of planetary systems and informs the attributes\nof representative orbital architectures that arise from extended disk-driven\nevolution. \n\n"}
{"id": "1504.04531", "contents": "Title: Hyperspectral pansharpening: a review Abstract: Pansharpening aims at fusing a panchromatic image with a multispectral one,\nto generate an image with the high spatial resolution of the former and the\nhigh spectral resolution of the latter. In the last decade, many algorithms\nhave been presented in the literature for pansharpening using multispectral\ndata. With the increasing availability of hyperspectral systems, these methods\nare now being adapted to hyperspectral images. In this work, we compare new\npansharpening techniques designed for hyperspectral data with some of the state\nof the art methods for multispectral pansharpening, which have been adapted for\nhyperspectral data. Eleven methods from different classes (component\nsubstitution, multiresolution analysis, hybrid, Bayesian and matrix\nfactorization) are analyzed. These methods are applied to three datasets and\ntheir effectiveness and robustness are evaluated with widely used performance\nindicators. In addition, all the pansharpening techniques considered in this\npaper have been implemented in a MATLAB toolbox that is made available to the\ncommunity. \n\n"}
{"id": "1504.05809", "contents": "Title: LOAD: Local Orientation Adaptive Descriptor for Texture and Material\n  Classification Abstract: In this paper, we propose a novel local feature, called Local Orientation\nAdaptive Descriptor (LOAD), to capture regional texture in an image. In LOAD,\nwe proposed to define point description on an Adaptive Coordinate System (ACS),\nadopt a binary sequence descriptor to capture relationships between one point\nand its neighbors and use multi-scale strategy to enhance the discriminative\npower of the descriptor. The proposed LOAD enjoys not only discriminative power\nto capture the texture information, but also has strong robustness to\nillumination variation and image rotation. Extensive experiments on benchmark\ndata sets of texture classification and real-world material recognition show\nthat the proposed LOAD yields the state-of-the-art performance. It is worth to\nmention that we achieve a 65.4\\% classification accuracy-- which is, to the\nbest of our knowledge, the highest record by far --on Flickr Material Database\nby using a single feature. Moreover, by combining LOAD with the feature\nextracted by Convolutional Neural Networks (CNN), we obtain significantly\nbetter performance than both the LOAD and CNN. This result confirms that the\nLOAD is complementary to the learning-based features. \n\n"}
{"id": "1504.06589", "contents": "Title: Spectral gaps, additive energy, and a fractal uncertainty principle Abstract: We obtain an essential spectral gap for $n$-dimensional convex co-compact\nhyperbolic manifolds with the dimension $\\delta$ of the limit set close to\n$(n-1)/2$. The size of the gap is expressed using the additive energy of\nstereographic projections of the limit set. This additive energy can in turn be\nestimated in terms of the constants in Ahlfors-David regularity of the limit\nset. Our proofs use new microlocal methods, in particular a notion of a fractal\nuncertainty principle. \n\n"}
{"id": "1504.06728", "contents": "Title: Asymptotic spectral gap for open partially expanding maps Abstract: We consider a $\\mathbb{R}$-extension of one dimensional uniformly expanding\nopen dynamical systems and prove a new explicit estimate for the asymptotic\nspectral gap. To get these results, we use a new application of a \"global\nnormal form\" for the dynamical system, a \"semiclassical expression beyond the\nEhrenfest time\" that expresses the transfer operator at large time as a sum\nover rank one operators (each is associated to one orbit). In this paper we\nestablish the validity of the so-called \"diagonal approximation\" up to twice\nthe local Ehrenfest time. \n\n"}
{"id": "1504.06785", "contents": "Title: Complete Dictionary Recovery over the Sphere Abstract: We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb R^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to the theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals, and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms provide recovery guarantees when $\\mathbf\nX_0$ has only $O(n^{1-\\delta})$ nonzeros per column for any constant $\\delta\n\\in (0, 1)$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. To show this apparently hard\nproblem is tractable, we first provide a geometric characterization of the\nhigh-dimensional objective landscape, which shows that with high probability\nthere are no \"spurious\" local minima. This particular geometric structure\nallows us to design a Riemannian trust region algorithm over the sphere that\nprovably converges to one local minimizer with an arbitrary initialization,\ndespite the presence of saddle points. The geometric approach we develop here\nmay also shed light on other problems arising from nonconvex recovery of\nstructured signals. \n\n"}
{"id": "1504.07658", "contents": "Title: Bifurcation structure of two coupled FHN neurons with delay Abstract: This paper presents an investigation of the dynamics of two coupled\nnon-identical FitzHugh-Nagumo neurons with delayed synaptic connection. We\nconsider coupling strength and time delay as bifurcation parameters, and try to\nclassify all possible dynamics which is fairly rich. The neural system exhibits\na unique rest point or three ones for the different values of coupling strength\nby employing the pitchfork bifurcation of non-trivial rest point. The\nasymptotic stability and possible Hopf bifurcations of the trivial rest point\nare studied by analyzing the corresponding characteristic equation. Homoclinic,\nfold, and pitchfork bifurcations of limit cycles are found. The delay-dependent\nstability regions are illustrated in the parameter plane, through which the\ndouble-Hopf bifurcation points can be obtained from the intersection points of\ntwo branches of Hopf bifurcation. The dynamical behavior of the system may\nexhibit one, two, or three different periodic solutions due to pitchfork cycle\nand torus (Neimark-Sacker) bifurcations. In addition, Hopf, double-Hopf, and\ntorus bifurcations of the non trivial rest points are found. Bifurcation\ndiagrams are obtained numerically or analytically from the mathematical model\nand the parameter regions of different behaviors are clarified. \n\n"}
{"id": "1504.08289", "contents": "Title: Neural Activation Constellations: Unsupervised Part Model Discovery with\n  Convolutional Networks Abstract: Part models of object categories are essential for challenging recognition\ntasks, where differences in categories are subtle and only reflected in\nappearances of small parts of the object. We present an approach that is able\nto learn part models in a completely unsupervised manner, without part\nannotations and even without given bounding boxes during learning. The key idea\nis to find constellations of neural activation patterns computed using\nconvolutional neural networks. In our experiments, we outperform existing\napproaches for fine-grained recognition on the CUB200-2011, NA birds, Oxford\nPETS, and Oxford Flowers dataset in case no part or bounding box annotations\nare available and achieve state-of-the-art performance for the Stanford Dog\ndataset. We also show the benefits of neural constellation models as a data\naugmentation technique for fine-tuning. Furthermore, our paper unites the areas\nof generic and fine-grained classification, since our approach is suitable for\nboth scenarios. The source code of our method is available online at\nhttp://www.inf-cv.uni-jena.de/part_discovery \n\n"}
{"id": "1505.00315", "contents": "Title: Learning Temporal Embeddings for Complex Video Analysis Abstract: In this paper, we propose to learn temporal embeddings of video frames for\ncomplex video analysis. Large quantities of unlabeled video data can be easily\nobtained from the Internet. These videos possess the implicit weak label that\nthey are sequences of temporally and semantically coherent images. We leverage\nthis information to learn temporal embeddings for video frames by associating\nframes with the temporal context that they appear in. To do this, we propose a\nscheme for incorporating temporal context based on past and future frames in\nvideos, and compare this to other contextual representations. In addition, we\nshow how data augmentation using multi-resolution samples and hard negatives\nhelps to significantly improve the quality of the learned embeddings. We\nevaluate various design decisions for learning temporal embeddings, and show\nthat our embeddings can improve performance for multiple video tasks such as\nretrieval, classification, and temporal order recovery in unconstrained\nInternet video. \n\n"}
{"id": "1505.00670", "contents": "Title: Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database\n  for Automated Image Interpretation Abstract: Despite tremendous progress in computer vision, there has not been an attempt\nfor machine learning on very large-scale medical image databases. We present an\ninterleaved text/image deep learning system to extract and mine the semantic\ninteractions of radiology images and reports from a national research\nhospital's Picture Archiving and Communication System. With natural language\nprocessing, we mine a collection of representative ~216K two-dimensional key\nimages selected by clinicians for diagnostic reference, and match the images\nwith their descriptions in an automated manner. Our system interleaves between\nunsupervised learning and supervised learning on document- and sentence-level\ntext collections, to generate semantic labels and to predict them given an\nimage. Given an image of a patient scan, semantic topics in radiology levels\nare predicted, and associated key-words are generated. Also, a number of\nfrequent disease types are detected as present or absent, to provide more\nspecific interpretation of a patient scan. This shows the potential of\nlarge-scale learning and prediction in electronic patient records available in\nmost modern clinical institutions. \n\n"}
{"id": "1505.01440", "contents": "Title: Leaders do not look back, or do they? Abstract: We study the effect of adding to a directed chain of interconnected systems a\ndirected feedback from the last element in the chain to the first. The problem\nis closely related to the fundamental question of how a change in network\ntopology may influence the behavior of coupled systems. We begin the analysis\nby investigating a simple linear system. The matrix that specifies the system\ndynamics is the transpose of the network Laplacian matrix, which codes the\nconnectivity of the network. Our analysis shows that for any nonzero complex\neigenvalue $\\lambda$ of this matrix, the following inequality holds:\n$\\frac{|\\Im \\lambda |}{|\\Re \\lambda |} \\leq \\cot\\frac{\\pi}{n}$. This bound is\nsharp, as it becomes an equality for an eigenvalue of a simple directed cycle\nwith uniform interaction weights. The latter has the slowest decay of\noscillations among all other network configurations with the same number of\nstates. The result is generalized to directed rings and chains of identical\nnonlinear oscillators. For directed rings, a lower bound $\\sigma_c$ for the\nconnection strengths that guarantees asymptotic synchronization is found to\nfollow a similar pattern: $\\sigma_c=\\frac{1}{1-\\cos\\left( 2\\pi /n\\right)} $.\nNumerical analysis revealed that, depending on the network size $n$, multiple\ndynamic regimes co-exist in the state space of the system. In addition to the\nfully synchronous state a rotating wave solution occurs. The effect is observed\nin networks exceeding a certain critical size. The emergence of a rotating wave\nhighlights the importance of long chains and loops in networks of oscillators:\nthe larger the size of chains and loops, the more sensitive the network\ndynamics becomes to removal or addition of a single connection. \n\n"}
{"id": "1505.02000", "contents": "Title: Deep Learning for Medical Image Segmentation Abstract: This report provides an overview of the current state of the art deep\nlearning architectures and optimisation techniques, and uses the ADNI\nhippocampus MRI dataset as an example to compare the effectiveness and\nefficiency of different convolutional architectures on the task of patch-based\n3-dimensional hippocampal segmentation, which is important in the diagnosis of\nAlzheimer's Disease. We found that a slightly unconventional \"stacked 2D\"\napproach provides much better classification performance than simple 2D patches\nwithout requiring significantly more computational power. We also examined the\npopular \"tri-planar\" approach used in some recently published studies, and\nfound that it provides much better results than the 2D approaches, but also\nwith a moderate increase in computational power requirement. Finally, we\nevaluated a full 3D convolutional architecture, and found that it provides\nmarginally better results than the tri-planar approach, but at the cost of a\nvery significant increase in computational power requirement. \n\n"}
{"id": "1505.05192", "contents": "Title: Unsupervised Visual Representation Learning by Context Prediction Abstract: This work explores the use of spatial context as a source of free and\nplentiful supervisory signal for training a rich visual representation. Given\nonly a large, unlabeled image collection, we extract random pairs of patches\nfrom each image and train a convolutional neural net to predict the position of\nthe second patch relative to the first. We argue that doing well on this task\nrequires the model to learn to recognize objects and their parts. We\ndemonstrate that the feature representation learned using this within-image\ncontext indeed captures visual similarity across images. For example, this\nrepresentation allows us to perform unsupervised visual discovery of objects\nlike cats, people, and even birds from the Pascal VOC 2011 detection dataset.\nFurthermore, we show that the learned ConvNet can be used in the R-CNN\nframework and provides a significant boost over a randomly-initialized ConvNet,\nresulting in state-of-the-art performance among algorithms which use only\nPascal-provided training set annotations. \n\n"}
{"id": "1505.06973", "contents": "Title: Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts Abstract: Formulations of the Image Decomposition Problem as a Multicut Problem (MP)\nw.r.t. a superpixel graph have received considerable attention. In contrast,\ninstances of the MP w.r.t. a pixel grid graph have received little attention,\nfirstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are\nhard to solve in practice, and, secondly, due to the lack of long-range terms\nin the objective function of the MP. We propose a generalization of the MP with\nlong-range terms (LMP). We design and implement two efficient algorithms\n(primal feasible heuristics) for the MP and LMP which allow us to study\ninstances of both problems w.r.t. the pixel grid graphs of the images in the\nBSDS-500 benchmark. The decompositions we obtain do not differ significantly\nfrom the state of the art, suggesting that the LMP is a competitive formulation\nof the Image Decomposition Problem. To demonstrate the generality of the LMP,\nwe apply it also to the Mesh Decomposition Problem posed by the Princeton\nbenchmark, obtaining state-of-the-art decompositions. \n\n"}
{"id": "1505.07924", "contents": "Title: Integrable discretization of the vector/matrix nonlinear Schr\\\"odinger\n  equation and the associated Yang-Baxter map Abstract: The action of a B\\\"acklund-Darboux transformation on a spectral problem\nassociated with a known integrable system can define a new discrete spectral\nproblem. In this paper, we interpret a slightly generalized version of the\nbinary B\\\"acklund-Darboux (or Zakharov-Shabat dressing) transformation for the\nnonlinear Schr\\\"odinger (NLS) hierarchy as a discrete spectral problem, wherein\nthe two intermediate potentials appearing in the Darboux matrix are considered\nas a pair of new dependent variables. Then, we associate the discrete spectral\nproblem with a suitable isospectral time-evolution equation, which forms the\nLax-pair representation for a space-discrete NLS system. This formulation is\nvalid for the most general case where the two dependent variables take values\nin (rectangular) matrices. In contrast to the matrix generalization of the\nAblowitz-Ladik lattice, our discretization has a rational nonlinearity and\nadmits a Hermitian conjugation reduction between the two dependent variables.\nThus, a new proper space-discretization of the vector/matrix NLS equation is\nobtained; by changing the time part of the Lax pair, we also obtain an\nintegrable space-discretization of the vector/matrix modified KdV (mKdV)\nequation. Because B\\\"acklund-Darboux transformations are permutable, we can\nincrease the number of discrete independent variables in a multi-dimensionally\nconsistent way. By solving the consistency condition on the two-dimensional\nlattice, we obtain a Yang-Baxter map of the NLS type, which can be considered\nas a fully discrete analog of the principal chiral model for projection\nmatrices. \n\n"}
{"id": "1506.02362", "contents": "Title: Traveling Wave Solutions of Degenerate Coupled Multi-KdV Equations Abstract: Traveling wave solutions of degenerate coupled $\\ell$-KdV equations are\nstudied. Due to symmetry reduction these equations reduce to one ODE,\n$(f')^2=P_n(f)$ where $P_n(f)$ is a polynomial function of $f$ of degree\n$n=\\ell+2$, where $\\ell \\geq 3$ in this work. Here $\\ell$ is the number of\ncoupled fields. There is no known method to solve such ordinary differential\nequations when $\\ell \\geq 3$. For this purpose, we introduce two different type\nof methods to solve the reduced equation and apply these methods to degenerate\nthree-coupled KdV equation. One of the methods uses the Chebyshev's Theorem. In\nthis case we find several solutions some of which may correspond to solitary\nwaves. The second method is a kind of factorizing the polynomial $P_n(f)$ as a\nproduct of lower degree polynomials. Each part of this product is assumed to\nsatisfy different ODEs. \n\n"}
{"id": "1506.02484", "contents": "Title: Limitations of PLL simulation: hidden oscillations in MatLab and SPICE Abstract: Nonlinear analysis of the phase-locked loop (PLL) based circuits is a\nchallenging task, thus in modern engineering literature simplified mathematical\nmodels and simulation are widely used for their study. In this work the\nlimitations of numerical approach is discussed and it is shown that, e.g.\nhidden oscillations may not be found by simulation. Corresponding examples in\nSPICE and MatLab, which may lead to wrong conclusions concerning the\noperability of PLL-based circuits, are presented. \n\n"}
{"id": "1506.03099", "contents": "Title: Scheduled Sampling for Sequence Prediction with Recurrent Neural\n  Networks Abstract: Recurrent Neural Networks can be trained to produce sequences of tokens given\nsome input, as exemplified by recent results in machine translation and image\ncaptioning. The current approach to training them consists of maximizing the\nlikelihood of each token in the sequence given the current (recurrent) state\nand the previous token. At inference, the unknown previous token is then\nreplaced by a token generated by the model itself. This discrepancy between\ntraining and inference can yield errors that can accumulate quickly along the\ngenerated sequence. We propose a curriculum learning strategy to gently change\nthe training process from a fully guided scheme using the true previous token,\ntowards a less guided scheme which mostly uses the generated token instead.\nExperiments on several sequence prediction tasks show that this approach yields\nsignificant improvements. Moreover, it was used successfully in our winning\nentry to the MSCOCO image captioning challenge, 2015. \n\n"}
{"id": "1506.04130", "contents": "Title: CloudCV: Large Scale Distributed Computer Vision as a Cloud Service Abstract: We are witnessing a proliferation of massive visual data. Unfortunately\nscaling existing computer vision algorithms to large datasets leaves\nresearchers repeatedly solving the same algorithmic, logistical, and\ninfrastructural problems. Our goal is to democratize computer vision; one\nshould not have to be a computer vision, big data and distributed computing\nexpert to have access to state-of-the-art distributed computer vision\nalgorithms. We present CloudCV, a comprehensive system to provide access to\nstate-of-the-art distributed computer vision algorithms as a cloud service\nthrough a Web Interface and APIs. \n\n"}
{"id": "1506.04395", "contents": "Title: Reading Scene Text in Deep Convolutional Sequences Abstract: We develop a Deep-Text Recurrent Network (DTRN) that regards scene text\nreading as a sequence labelling problem. We leverage recent advances of deep\nconvolutional neural networks to generate an ordered high-level sequence from a\nwhole word image, avoiding the difficult character segmentation problem. Then a\ndeep recurrent model, building on long short-term memory (LSTM), is developed\nto robustly recognize the generated CNN sequences, departing from most existing\napproaches recognising each character independently. Our model has a number of\nappealing properties in comparison to existing scene text recognition methods:\n(i) It can recognise highly ambiguous words by leveraging meaningful context\ninformation, allowing it to work reliably without either pre- or\npost-processing; (ii) the deep CNN feature is robust to various image\ndistortions; (iii) it retains the explicit order information in word image,\nwhich is essential to discriminate word strings; (iv) the model does not depend\non pre-defined dictionary, and it can process unknown words and arbitrary\nstrings. Codes for the DTRN will be available. \n\n"}
{"id": "1506.06448", "contents": "Title: DeepOrgan: Multi-level Deep Convolutional Networks for Automated\n  Pancreas Segmentation Abstract: Automatic organ segmentation is an important yet challenging problem for\nmedical image analysis. The pancreas is an abdominal organ with very high\nanatomical variability. This inhibits previous segmentation methods from\nachieving high accuracies, especially compared to other organs such as the\nliver, heart or kidneys. In this paper, we present a probabilistic bottom-up\napproach for pancreas segmentation in abdominal computed tomography (CT) scans,\nusing multi-level deep convolutional networks (ConvNets). We propose and\nevaluate several variations of deep ConvNets in the context of hierarchical,\ncoarse-to-fine classification on image patches and regions, i.e. superpixels.\nWe first present a dense labeling of local image patches via\n$P{-}\\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regional\nConvNet ($R_1{-}\\mathrm{ConvNet}$) that samples a set of bounding boxes around\neach image superpixel at different scales of contexts in a \"zoom-out\" fashion.\nOur ConvNets learn to assign class probabilities for each superpixel region of\nbeing pancreas. Last, we study a stacked $R_2{-}\\mathrm{ConvNet}$ leveraging\nthe joint space of CT intensities and the $P{-}\\mathrm{ConvNet}$ dense\nprobability maps. Both 3D Gaussian smoothing and 2D conditional random fields\nare exploited as structured predictions for post-processing. We evaluate on CT\nimages of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity\nCoefficient of 83.6$\\pm$6.3% in training and 71.8$\\pm$10.7% in testing. \n\n"}
{"id": "1506.08229", "contents": "Title: Dimer with gain and loss: Integrability and $\\mathcal{PT}$-symmetry\n  restoration Abstract: A $\\mathcal{PT}$-symmetric nonlinear Schr\\\"odinger dimer is a two-site\ndiscrete nonlinear Schr\\\"odinger equation with one site losing and the other\none gaining energy at the same rate. In this paper, two four-parameter families\nof cubic $\\mathcal{PT}$-symmetric dimers are constructed as gain-loss\nextensions of their conservative, Hamiltonian, counterparts. We prove that all\nthese damped-driven equations define completely integrable Hamiltonian systems.\nThe second aim of our study is to identify nonlinearities that give rise to the\nspontaneous $\\mathcal{PT}$-symmetry restoration. When the symmetry of the\nunderlying linear dimer is broken and an unstable small perturbation starts to\ngrow, the nonlinear coupling of the required type diverts progressively large\namounts of energy from the gaining to the losing site. As a result, the\nexponential growth is saturated and all trajectories remain trapped in a finite\npart of the phase space regardless of the value of the gain-loss coefficient. \n\n"}
{"id": "1506.08316", "contents": "Title: Keypoint Encoding for Improved Feature Extraction from Compressed Video\n  at Low Bitrates Abstract: In many mobile visual analysis applications, compressed video is transmitted\nover a communication network and analyzed by a server. Typical processing steps\nperformed at the server include keypoint detection, descriptor calculation, and\nfeature matching. Video compression has been shown to have an adverse effect on\nfeature-matching performance. The negative impact of compression can be reduced\nby using the keypoints extracted from the uncompressed video to calculate\ndescriptors from the compressed video. Based on this observation, we propose to\nprovide these keypoints to the server as side information and to extract only\nthe descriptors from the compressed video. First, we introduce four different\nframe types for keypoint encoding to address different types of changes in\nvideo content. These frame types represent a new scene, the same scene, a\nslowly changing scene, or a rapidly moving scene and are determined by\ncomparing features between successive video frames. Then, we propose Intra,\nSkip and Inter modes of encoding the keypoints for different frame types. For\nexample, keypoints for new scenes are encoded using the Intra mode, and\nkeypoints for unchanged scenes are skipped. As a result, the bitrate of the\nside information related to keypoint encoding is significantly reduced.\nFinally, we present pairwise matching and image retrieval experiments conducted\nto evaluate the performance of the proposed approach using the Stanford mobile\naugmented reality dataset and 720p format videos. The results show that the\nproposed approach offers significantly improved feature matching and image\nretrieval performance at a given bitrate. \n\n"}
{"id": "1507.01537", "contents": "Title: Identifying Functional Thermodynamics in Autonomous Maxwellian Ratchets Abstract: We introduce a family of Maxwellian Demons for which correlations among\ninformation bearing degrees of freedom can be calculated exactly and in compact\nanalytical form. This allows one to precisely determine Demon functional\nthermodynamic operating regimes, when previous methods either misclassify or\nsimply fail due to approximations they invoke. This reveals that these Demons\nare more functional than previous candidates. They too behave either as\nengines, lifting a mass against gravity by extracting energy from a single heat\nreservoir, or as Landauer erasers, consuming external work to remove\ninformation from a sequence of binary symbols by decreasing their individual\nuncertainty. Going beyond these, our Demon exhibits a new functionality that\nerases bits not by simply decreasing individual-symbol uncertainty, but by\nincreasing inter-bit correlations (that is, by adding temporal order) while\nincreasing single-symbol uncertainty. In all cases, but especially in the new\nerasure regime, exactly accounting for informational correlations leads to\ntight bounds on Demon performance, expressed as a refined Second Law of\nThermodynamics that relies on the Kolmogorov-Sinai entropy for dynamical\nprocesses and not on changes purely in system configurational entropy, as\npreviously employed. We rigorously derive the refined Second Law under minimal\nassumptions and so it applies quite broadly---for Demons with and without\nmemory and input sequences that are correlated or not. We note that general\nMaxwellian Demons readily violate previously proposed, alternative such bounds,\nwhile the current bound still holds. \n\n"}
{"id": "1507.03468", "contents": "Title: Limitations of the classical phase-locked loop analysis Abstract: Nonlinear analysis of the classical phase-locked loop (PLL) is a challenging\ntask. In classical engineering literature simplified mathematical models and\nsimulation are widely used for its study. In this work the limitations of\nclassical engineering phase-locked loop analysis are demonstrated, e.g., hidden\noscillations, which can not be found by simulation, are discussed. It is shown\nthat the use of simplified dynamical models and the application of simulation\nmay lead to wrong conclusions concerning the operability of PLL-based circuits. \n\n"}
{"id": "1507.08754", "contents": "Title: Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural\n  Networks for Image Classification Abstract: This paper presents a new version of Dropout called Split Dropout (sDropout)\nand rotational convolution techniques to improve CNNs' performance on image\nclassification. The widely used standard Dropout has advantage of preventing\ndeep neural networks from overfitting by randomly dropping units during\ntraining. Our sDropout randomly splits the data into two subsets and keeps both\nrather than discards one subset. We also introduce two rotational convolution\ntechniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-pooling\nconvolution (FRPC) to boost CNNs' performance on the robustness for rotation\ntransformation. These two techniques encode rotation invariance into the\nnetwork without adding extra parameters. Experimental evaluations on\nImageNet2012 classification task demonstrate that sDropout not only enhances\nthe performance but also converges faster. Additionally, RPC and FRPC make CNNs\nmore robust for rotation transformations. Overall, FRPC together with sDropout\nbring $1.18\\%$ (model of Zeiler and Fergus~\\cite{zeiler2013visualizing},\n10-view, top-1) accuracy increase in ImageNet 2012 classification task compared\nto the original network. \n\n"}
{"id": "1508.00307", "contents": "Title: Local Color Contrastive Descriptor for Image Classification Abstract: Image representation and classification are two fundamental tasks towards\nmultimedia content retrieval and understanding. The idea that shape and texture\ninformation (e.g. edge or orientation) are the key features for visual\nrepresentation is ingrained and dominated in current multimedia and computer\nvision communities. A number of low-level features have been proposed by\ncomputing local gradients (e.g. SIFT, LBP and HOG), and have achieved great\nsuccesses on numerous multimedia applications. In this paper, we present a\nsimple yet efficient local descriptor for image classification, referred as\nLocal Color Contrastive Descriptor (LCCD), by leveraging the neural mechanisms\nof color contrast. The idea originates from the observation in neural science\nthat color and shape information are linked inextricably in visual cortical\nprocessing. The color contrast yields key information for visual color\nperception and provides strong linkage between color and shape. We propose a\nnovel contrastive mechanism to compute the color contrast in both spatial\nlocation and multiple channels. The color contrast is computed by measuring\n\\emph{f}-divergence between the color distributions of two regions. Our\ndescriptor enriches local image representation with both color and contrast\ninformation. We verified experimentally that it can compensate strongly for the\nshape based descriptor (e.g. SIFT), while keeping computationally simple.\nExtensive experimental results on image classification show that our descriptor\nimproves the performance of SIFT substantially by combinations, and achieves\nthe state-of-the-art performance on three challenging benchmark datasets. It\nimproves recent Deep Learning model (DeCAF) [1] largely from the accuracy of\n40.94% to 49.68% in the large scale SUN397 database. Codes for the LCCD will be\navailable. \n\n"}
{"id": "1508.02496", "contents": "Title: A Practical Guide to CNNs and Fisher Vectors for Image Instance\n  Retrieval Abstract: With deep learning becoming the dominant approach in computer vision, the use\nof representations extracted from Convolutional Neural Nets (CNNs) is quickly\ngaining ground on Fisher Vectors (FVs) as favoured state-of-the-art global\nimage descriptors for image instance retrieval. While the good performance of\nCNNs for image classification are unambiguously recognised, which of the two\nhas the upper hand in the image retrieval context is not entirely clear yet. In\nthis work, we propose a comprehensive study that systematically evaluates FVs\nand CNNs for image retrieval. The first part compares the performances of FVs\nand CNNs on multiple publicly available data sets. We investigate a number of\ndetails specific to each method. For FVs, we compare sparse descriptors based\non interest point detectors with dense single-scale and multi-scale variants.\nFor CNNs, we focus on understanding the impact of depth, architecture and\ntraining data on retrieval results. Our study shows that no descriptor is\nsystematically better than the other and that performance gains can usually be\nobtained by using both types together. The second part of the study focuses on\nthe impact of geometrical transformations such as rotations and scale changes.\nFVs based on interest point detectors are intrinsically resilient to such\ntransformations while CNNs do not have a built-in mechanism to ensure such\ninvariance. We show that performance of CNNs can quickly degrade in presence of\nrotations while they are far less affected by changes in scale. We then propose\na number of ways to incorporate the required invariances in the CNN pipeline.\nOverall, our work is intended as a reference guide offering practically useful\nand simply implementable guidelines to anyone looking for state-of-the-art\nglobal descriptors best suited to their specific image instance retrieval\nproblem. \n\n"}
{"id": "1508.02700", "contents": "Title: Linear response for intermittent maps Abstract: We consider the one parameter family $\\alpha \\mapsto T_\\alpha$ ($\\alpha \\in\n[0,1)$) of Pomeau-Manneville type interval maps $T_\\alpha(x)=x(1+2^\\alpha\nx^\\alpha)$ for $x \\in [0,1/2)$ and $T_\\alpha(x)=2x-1$ for $x \\in [1/2, 1]$,\nwith the associated absolutely continuous invariant probability measure\n$\\mu_\\alpha$. For $\\alpha \\in (0,1)$, Sarig and Gou\\\"ezel proved that the\nsystem mixes only polynomially with rate $n^{1-1/\\alpha}$ (in particular, there\nis no spectral gap). We show that for any $\\psi\\in L^q$, the map $\\alpha \\to\n\\int_0^1 \\psi\\, d\\mu_\\alpha$ is differentiable on $[0,1-1/q)$, and we give a\n(linear response) formula for the value of the derivative. This is the first\ntime that a linear response formula for the SRB measure is obtained in the\nsetting of slowly mixing dynamics. Our argument shows how cone techniques can\nbe used in this context. For $\\alpha \\ge 1/2$ we need the $n^{-1/\\alpha}$\ndecorrelation obtained by Gou\\\"ezel under additional conditions. \n\n"}
{"id": "1508.03191", "contents": "Title: Exponential Sensitivity and its Cost in Quantum Physics Abstract: State selective protocols, like entanglement purification, lead to an\nessentially non-linear quantum evolution, unusual in naturally occurring\nquantum processes. Sensitivity to initial states in quantum systems, stemming\nfrom such non-linear dynamics, is a promising perspective for applications.\nHere we demonstrate that chaotic behaviour is a rather generic feature in state\nselective protocols: exponential sensitivity can exist for all initial states\nin an experimentally realisable optical scheme. Moreover, any complex rational\npolynomial map, including the example of the Mandelbrot set, can be directly\nrealised. In state selective protocols, one needs an ensemble of initial\nstates, the size of which decreases with each iteration. We prove that\nexponential sensitivity to initial states in any quantum system have to be\nrelated to downsizing the initial ensemble also exponentially. Our results show\nthat magnifying initial differences of quantum states (a Schr\\\"odinger\nmicroscope) is possible, however, there is a strict bound on the number of\ncopies needed. \n\n"}
{"id": "1508.05636", "contents": "Title: B\\\"acklund transformations for the Camassa-Holm equation Abstract: The B\\\"acklund transformation (BT) for the Camassa-Holm (CH) equation is\npresented and discussed. Unlike the vast majority of BTs studied in the past,\nfor CH the transformation acts on both the dependent and (one of) the\nindependent variables. Superposition principles are given for the action of\ndouble BTs on the variables of the CH and the potential CH equations.\nApplications of the BT and its superposition principles are presented,\nspecifically the construction of travelling wave solutions, a new method to\nconstruct multi-soliton, multi-cuspon and soliton-cuspon solutions, and a\nderivation of generating functions for the local symmetries and conservation\nlaws of the CH hierarchy. \n\n"}
{"id": "1508.06840", "contents": "Title: A Modified Quadratic Lorenz attractor Abstract: This study introduces a modified quadratic Lorenz attractor. The properties\nof this new chaotic system are analysed and discussed in detail, by determining\nthe equilibria points, the eigenvalues of the Jacobian, and the Lyapunov\nexponents. The numerical simulations, the time series analysis, and the\nprojections to the $xy$-plane, $xz$-plane, and $yz$-plane are conducted to\nhighlight the chaotic behaviour. The multiplicative form of the new system is\nalso presented and the simulations are conducted using multiplicative\nRunge-Kutta methods. \n\n"}
{"id": "1508.07498", "contents": "Title: The Lyapunov dimension formula for the global attractor of the Lorenz\n  system Abstract: The exact Lyapunov dimension formula for the Lorenz system has been\nanalytically obtained first due to G.A.Leonov in 2002 under certain\nrestrictions on parameters, permitting classical values. He used the\nconstruction technique of special Lyapunov-type functions developed by him in\n1991 year. Later it was shown that the consideration of larger class of\nLyapunov-type functions permits proving the validity of this formula for all\nparameters of the system such that all the equilibria of the system are\nhyperbolically unstable. In the present work it is proved the validity of the\nformula for Lyapunov dimension for a wider variety of parameters values, which\ninclude all parameters satisfying the classical physical limitations. One of\nthe motivation of this work is the possibility of computing a chaotic attractor\nin the Lorenz system in the case of one unstable and two stable equilibria. \n\n"}
{"id": "1509.00296", "contents": "Title: Fast Randomized Singular Value Thresholding for Low-rank Optimization Abstract: Rank minimization can be converted into tractable surrogate problems, such as\nNuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems related\nto NNM, or WNNM, can be solved iteratively by applying a closed-form proximal\noperator, called Singular Value Thresholding (SVT), or Weighted SVT, but they\nsuffer from high computational cost of Singular Value Decomposition (SVD) at\neach iteration. We propose a fast and accurate approximation method for SVT,\nthat we call fast randomized SVT (FRSVT), with which we avoid direct\ncomputation of SVD. The key idea is to extract an approximate basis for the\nrange of the matrix from its compressed matrix. Given the basis, we compute\npartial singular values of the original matrix from the small factored matrix.\nIn addition, by developping a range propagation method, our method further\nspeeds up the extraction of approximate basis at each iteration. Our\ntheoretical analysis shows the relationship between the approximation bound of\nSVD and its effect to NNM via SVT. Along with the analysis, our empirical\nresults quantitatively and qualitatively show that our approximation rarely\nharms the convergence of the host algorithms. We assess the efficiency and\naccuracy of the proposed method on various computer vision problems, e.g.,\nsubspace clustering, weather artifact removal, and simultaneous multi-image\nalignment and rectification. \n\n"}
{"id": "1509.01696", "contents": "Title: Early-warning indicators for rate-induced tipping Abstract: A dynamical system is said to undergo rate-induced tipping when it fails to\ntrack its quasi-equilibrium state due to an above-critical-rate change of\nsystem parameters. We study a prototypical model for rate-induced tipping, the\nsaddle-node normal form subject to time-varying equilibrium drift and noise. We\nfind that both most commonly used early-warning indicators, increase in\nvariance and increase in autocorrelation, occur not when the equilibrium drift\nis fastest but with a delay. We explain this delay by demonstrating that the\nmost likely trajectory for tipping also crosses the tipping threshold with a\ndelay and therefore the tipping itself is delayed. We find solutions of the\nvariational problem determining the most likely tipping path using numerical\ncontinuation techniques. The result is a systematic study of the most likely\ntipping time in the plane of two parameters, distance from tipping threshold\nand noise intensity. \n\n"}
{"id": "1509.03453", "contents": "Title: A reliable order-statistics-based approximate nearest neighbor search\n  algorithm Abstract: We propose a new algorithm for fast approximate nearest neighbor search based\non the properties of ordered vectors. Data vectors are classified based on the\nindex and sign of their largest components, thereby partitioning the space in a\nnumber of cones centered in the origin. The query is itself classified, and the\nsearch starts from the selected cone and proceeds to neighboring ones. Overall,\nthe proposed algorithm corresponds to locality sensitive hashing in the space\nof directions, with hashing based on the order of components. Thanks to the\nstatistical features emerging through ordering, it deals very well with the\nchallenging case of unstructured data, and is a valuable building block for\nmore complex techniques dealing with structured data. Experiments on both\nsimulated and real-world data prove the proposed algorithm to provide a\nstate-of-the-art performance. \n\n"}
{"id": "1509.03877", "contents": "Title: Learning Contextual Dependencies with Convolutional Hierarchical\n  Recurrent Neural Networks Abstract: Existing deep convolutional neural networks (CNNs) have shown their great\nsuccess on image classification. CNNs mainly consist of convolutional and\npooling layers, both of which are performed on local image areas without\nconsidering the dependencies among different image regions. However, such\ndependencies are very important for generating explicit image representation.\nIn contrast, recurrent neural networks (RNNs) are well known for their ability\nof encoding contextual information among sequential data, and they only require\na limited number of network parameters. General RNNs can hardly be directly\napplied on non-sequential data. Thus, we proposed the hierarchical RNNs\n(HRNNs). In HRNNs, each RNN layer focuses on modeling spatial dependencies\namong image regions from the same scale but different locations. While the\ncross RNN scale connections target on modeling scale dependencies among regions\nfrom the same location but different scales. Specifically, we propose two\nrecurrent neural network models: 1) hierarchical simple recurrent network\n(HSRN), which is fast and has low computational cost; and 2) hierarchical\nlong-short term memory recurrent network (HLSTM), which performs better than\nHSRN with the price of more computational cost.\n  In this manuscript, we integrate CNNs with HRNNs, and develop end-to-end\nconvolutional hierarchical recurrent neural networks (C-HRNNs). C-HRNNs not\nonly make use of the representation power of CNNs, but also efficiently encodes\nspatial and scale dependencies among different image regions. On four of the\nmost challenging object/scene image classification benchmarks, our C-HRNNs\nachieve state-of-the-art results on Places 205, SUN 397, MIT indoor, and\ncompetitive results on ILSVRC 2012. \n\n"}
{"id": "1509.04581", "contents": "Title: Kernelized Deep Convolutional Neural Network for Describing Complex\n  Images Abstract: With the impressive capability to capture visual content, deep convolutional\nneural networks (CNN) have demon- strated promising performance in various\nvision-based ap- plications, such as classification, recognition, and objec- t\ndetection. However, due to the intrinsic structure design of CNN, for images\nwith complex content, it achieves lim- ited capability on invariance to\ntranslation, rotation, and re-sizing changes, which is strongly emphasized in\nthe s- cenario of content-based image retrieval. In this paper, to address this\nproblem, we proposed a new kernelized deep convolutional neural network. We\nfirst discuss our motiva- tion by an experimental study to demonstrate the\nsensitivi- ty of the global CNN feature to the basic geometric trans-\nformations. Then, we propose to represent visual content with approximate\ninvariance to the above geometric trans- formations from a kernelized\nperspective. We extract CNN features on the detected object-like patches and\naggregate these patch-level CNN features to form a vectorial repre- sentation\nwith the Fisher vector model. The effectiveness of our proposed algorithm is\ndemonstrated on image search application with three benchmark datasets. \n\n"}
{"id": "1509.06066", "contents": "Title: On Large-Scale Retrieval: Binary or n-ary Coding? Abstract: The growing amount of data available in modern-day datasets makes the need to\nefficiently search and retrieve information. To make large-scale search\nfeasible, Distance Estimation and Subset Indexing are the main approaches.\nAlthough binary coding has been popular for implementing both techniques, n-ary\ncoding (known as Product Quantization) is also very effective for Distance\nEstimation. However, their relative performance has not been studied for Subset\nIndexing. We investigate whether binary or n-ary coding works better under\ndifferent retrieval strategies. This leads to the design of a new n-ary coding\nmethod, \"Linear Subspace Quantization (LSQ)\" which, unlike other n-ary\nencoders, can be used as a similarity-preserving embedding. Experiments on\nimage retrieval show that when Distance Estimation is used, n-ary LSQ\noutperforms other methods. However, when Subset Indexing is applied,\ninterestingly, binary codings are more effective and binary LSQ achieves the\nbest accuracy. \n\n"}
{"id": "1509.06825", "contents": "Title: Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700\n  Robot Hours Abstract: Current learning-based robot grasping approaches exploit human-labeled\ndatasets for training the models. However, there are two problems with such a\nmethodology: (a) since each object can be grasped in multiple ways, manually\nlabeling grasp locations is not a trivial task; (b) human labeling is biased by\nsemantics. While there have been attempts to train robots using trial-and-error\nexperiments, the amount of data used in such experiments remains substantially\nlow and hence makes the learner prone to over-fitting. In this paper, we take\nthe leap of increasing the available training data to 40 times more than prior\nwork, leading to a dataset size of 50K data points collected over 700 hours of\nrobot grasping attempts. This allows us to train a Convolutional Neural Network\n(CNN) for the task of predicting grasp locations without severe overfitting. In\nour formulation, we recast the regression problem to an 18-way binary\nclassification over image patches. We also present a multi-stage learning\napproach where a CNN trained in one stage is used to collect hard negatives in\nsubsequent stages. Our experiments clearly show the benefit of using\nlarge-scale datasets (and multi-stage training) for the task of grasping. We\nalso compare to several baselines and show state-of-the-art performance on\ngeneralization to unseen objects for grasping. \n\n"}
{"id": "1509.06919", "contents": "Title: Un-reduction in field theory, with applications Abstract: The un-reduction procedure introduced previously in the context of Mechanics\nis extended to covariant Field Theory. The new covariant un-reduction procedure\nis applied to the problem of shape matching of images which depend on more than\none independent variable (for instance, time and an additional labelling\nparameter). Other possibilities are also explored: non-linear $\\sigma$-models\nand the hyperbolic flows of curves. \n\n"}
{"id": "1509.07277", "contents": "Title: Asymptotic stability of pseudo-simple heteroclinic cycles in R^4 Abstract: Robust heteroclinic cycles in equivariant dynamical systems in R^4 have been\na subject of intense scientific investigation because, unlike heteroclinic\ncycles in R^3, they can have an intricate geometric structure and complex\nasymptotic stability properties that are not yet completely understood. In a\nrecent work, we have compiled an exhaustive list of finite subgroups of O(4)\nadmitting the so-called simple heteroclinic cycles, and have identified a new\nclass which we have called pseudo-simple heteroclinic cycles. By contrast with\nsimple heteroclinic cycles, a pseudo-simple one has at least one equilibrium\nwith an unstable manifold which has dimension 2 due to a symmetry. Here, we\nanalyse the dynamics of nearby trajectories and asymptotic stability of\npseudo-simple heteroclinic cycles in R^4. \n\n"}
{"id": "1509.09018", "contents": "Title: Multispecies TASEP and the tetrahedron equation Abstract: We introduce a family of layer to layer transfer matrices in a\nthree-dimensional (3D) lattice model which can be viewed as partition functions\nof the $q$-oscillator valued six-vertex model on $m \\times n$ square lattice.\nBy invoking the tetrahedron equation we establish their commutativity and\nbilinear relations mixing various boundary conditions. At $q=0$ and $m=n$, they\nultimately yield a new proof of the steady state formula for the $n$-species\ntotally asymmetric simple exclusion process (TASEP) obtained recently by the\nauthors, revealing the 3D integrability in the matrix product construction. \n\n"}
{"id": "1510.01401", "contents": "Title: On the Existence of Epipolar Matrices Abstract: This paper considers the foundational question of the existence of a\nfundamental (resp. essential) matrix given $m$ point correspondences in two\nviews. We present a complete answer for the existence of fundamental matrices\nfor any value of $m$. Using examples we disprove the widely held beliefs that\nfundamental matrices always exist whenever $m \\leq 7$. At the same time, we\nprove that they exist unconditionally when $m \\leq 5$. Under a mild genericity\ncondition, we show that an essential matrix always exists when $m \\leq 4$. We\nalso characterize the six and seven point configurations in two views for which\nall matrices satisfying the epipolar constraint have rank at most one. \n\n"}
{"id": "1510.01527", "contents": "Title: Linearizability and fake Lax pair for a consistent around the cube\n  nonlinear non-autonomous quad-graph equation Abstract: We discuss the linearization of a non-autonomous nonlinear partial difference\nequation belonging to the Boll classification of quad-graph equations\nconsistent around the cube. We show that its Lax pair is fake. We present its\ngeneralized symmetries which turn out to be non-autonomous and depending on an\narbitrary function of the dependent variables defined in two lattice points.\nThese generalized symmetries are differential difference equations which, in\nsome case, admit peculiar B\\\"acklund transformations. \n\n"}
{"id": "1510.05822", "contents": "Title: Sequential Score Adaptation with Extreme Value Theory for Robust Railway\n  Track Inspection Abstract: Periodic inspections are necessary to keep railroad tracks in state of good\nrepair and prevent train accidents. Automatic track inspection using machine\nvision technology has become a very effective inspection tool. Because of its\nnon-contact nature, this technology can be deployed on virtually any railway\nvehicle to continuously survey the tracks and send exception reports to track\nmaintenance personnel. However, as appearance and imaging conditions vary,\nfalse alarm rates can dramatically change, making it difficult to select a good\noperating point. In this paper, we use extreme value theory (EVT) within a\nBayesian framework to optimally adjust the sensitivity of anomaly detectors. We\nshow that by approximating the lower tail of the probability density function\n(PDF) of the scores with an Exponential distribution (a special case of the\nGeneralized Pareto distribution), and using the Gamma conjugate prior learned\nfrom the training data, it is possible to reduce the variability in false alarm\nrate and improve the overall performance. This method has shown an increase in\nthe defect detection rate of rail fasteners in the presence of clutter (at PFA\n0.1%) from 95.40% to 99.26% on the 85-mile Northeast Corridor (NEC) 2012-2013\nconcrete tie dataset. \n\n"}
{"id": "1510.08583", "contents": "Title: Privacy Prediction of Images Shared on Social Media Sites Using Deep\n  Features Abstract: Online image sharing in social media sites such as Facebook, Flickr, and\nInstagram can lead to unwanted disclosure and privacy violations, when privacy\nsettings are used inappropriately. With the exponential increase in the number\nof images that are shared online every day, the development of effective and\nefficient prediction methods for image privacy settings are highly needed. The\nperformance of models critically depends on the choice of the feature\nrepresentation. In this paper, we present an approach to image privacy\nprediction that uses deep features and deep image tags as feature\nrepresentations. Specifically, we explore deep features at various neural\nnetwork layers and use the top layer (probability) as an auto-annotation\nmechanism. The results of our experiments show that models trained on the\nproposed deep features and deep image tags substantially outperform baselines\nsuch as those based on SIFT and GIST as well as those that use \"bag of tags\" as\nfeatures. \n\n"}
{"id": "1510.08822", "contents": "Title: Transformation and integrability of a generalized short pulse equation Abstract: By means of transformations to nonlinear Klein-Gordon equations, we show that\na generalized short pulse equation is integrable in two (and, most probably,\nonly two) distinct cases of its coefficients. The first case is the original\nshort pulse equation (SPE). The second case, which we call the single-cycle\npulse equation (SCPE), is a previously overlooked scalar reduction of a known\nintegrable system of coupled SPEs. We get the Lax pair and bi-Hamiltonian\nstructure for the SCPE and show that the smooth envelope soliton of the SCPE\ncan be as short as only one cycle of its carrier frequency. \n\n"}
{"id": "1510.09171", "contents": "Title: Accurate Vision-based Vehicle Localization using Satellite Imagery Abstract: We propose a method for accurately localizing ground vehicles with the aid of\nsatellite imagery. Our approach takes a ground image as input, and outputs the\nlocation from which it was taken on a georeferenced satellite image. We perform\nvisual localization by estimating the co-occurrence probabilities between the\nground and satellite images based on a ground-satellite feature dictionary. The\nmethod is able to estimate likelihoods over arbitrary locations without the\nneed for a dense ground image database. We present a ranking-loss based\nalgorithm that learns location-discriminative feature projection matrices that\nresult in further improvements in accuracy. We evaluate our method on the\nMalaga and KITTI public datasets and demonstrate significant improvements over\na baseline that performs exhaustive search. \n\n"}
{"id": "1511.03544", "contents": "Title: Multi-soliton, multi-breather and higher-order rogue wave solutions to\n  the complex short pulse equation Abstract: In the present paper, we are concerned with the general localized solutions\nfor the complex short pulse equation including soliton, breather and rogue wave\nsolutions. With the aid of a generalized Darboux transformation, we construct\nthe $N$-bright soliton solution in a compact determinant form, then the\n$N$-breather solution including the Akhmediev breather and a general higher\norder rogue wave solution. The first- and second-order rogue wave solutions are\ngiven explicitly and illustrated by graphs. The asymptotic analysis is\nperformed rigorously for both the $N$-soliton and the $N$-breather solutions.\nAll three forms of the localized solutions admit either smoothed-, cusped- or\nlooped-type ones for the CSP equation depending on the parameters. It is noted\nthat, due to the reciprocal (hodograph) transformation, the rogue wave solution\nto the CSP equation is different from the one to the nonlinear Schr\\\"odinger\n(NLS) equation, which could be a cusponed- or a looped one. \n\n"}
{"id": "1511.04397", "contents": "Title: Similarity-based Text Recognition by Deeply Supervised Siamese Network Abstract: In this paper, we propose a new text recognition model based on measuring the\nvisual similarity of text and predicting the content of unlabeled texts. First\na Siamese convolutional network is trained with deep supervision on a labeled\ntraining dataset. This network projects texts into a similarity manifold. The\nDeeply Supervised Siamese network learns visual similarity of texts. Then a\nK-nearest neighbor classifier is used to predict unlabeled text based on\nsimilarity distance to labeled texts. The performance of the model is evaluated\non three datasets of machine-print and hand-written text combined. We\ndemonstrate that the model reduces the cost of human estimation by $50\\%-85\\%$.\nThe error of the system is less than $0.5\\%$. The proposed model outperform\nconventional Siamese network by finding visually-similar barely-readable and\nreadable text, e.g. machine-printed, handwritten, due to deep supervision. The\nresults also demonstrate that the predicted labels are sometimes better than\nhuman labels e.g. spelling correction. \n\n"}
{"id": "1511.04491", "contents": "Title: Deeply-Recursive Convolutional Network for Image Super-Resolution Abstract: We propose an image super-resolution method (SR) using a deeply-recursive\nconvolutional network (DRCN). Our network has a very deep recursive layer (up\nto 16 recursions). Increasing recursion depth can improve performance without\nintroducing new parameters for additional convolutions. Albeit advantages,\nlearning a DRCN is very hard with a standard gradient descent method due to\nexploding/vanishing gradients. To ease the difficulty of training, we propose\ntwo extensions: recursive-supervision and skip-connection. Our method\noutperforms previous methods by a large margin. \n\n"}
{"id": "1511.05547", "contents": "Title: Return of Frustratingly Easy Domain Adaptation Abstract: Unlike human learning, machine learning often fails to handle changes between\ntraining (source) and test (target) input distributions. Such domain shifts,\ncommon in practical scenarios, severely damage the performance of conventional\nmachine learning methods. Supervised domain adaptation methods have been\nproposed for the case when the target data have labels, including some that\nperform very well despite being \"frustratingly easy\" to implement. However, in\npractice, the target domain is often unlabeled, requiring unsupervised\nadaptation. We propose a simple, effective, and efficient method for\nunsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL\nminimizes domain shift by aligning the second-order statistics of source and\ntarget distributions, without requiring any target labels. Even though it is\nextraordinarily simple--it can be implemented in four lines of Matlab\ncode--CORAL performs remarkably well in extensive evaluations on standard\nbenchmark datasets. \n\n"}
{"id": "1511.05906", "contents": "Title: A simple proof of the exactness of expanding maps of the interval with\n  an indifferent fixed point Abstract: Expanding maps with indifferent fixed points, a.k.a. intermittent maps, are\npopular models in nonlinear dynamics and infinite ergodic theory. We present a\nsimple proof of the exactness of a wide class of expanding maps of [0,1], with\ncountably many surjective branches and a strongly neutral fixed point in 0. \n\n"}
{"id": "1511.06681", "contents": "Title: Deep End2End Voxel2Voxel Prediction Abstract: Over the last few years deep learning methods have emerged as one of the most\nprominent approaches for video analysis. However, so far their most successful\napplications have been in the area of video classification and detection, i.e.,\nproblems involving the prediction of a single class label or a handful of\noutput variables per video. Furthermore, while deep networks are commonly\nrecognized as the best models to use in these domains, there is a widespread\nperception that in order to yield successful results they often require\ntime-consuming architecture search, manual tweaking of parameters and\ncomputationally intensive pre-processing or post-processing methods.\n  In this paper we challenge these views by presenting a deep 3D convolutional\narchitecture trained end to end to perform voxel-level prediction, i.e., to\noutput a variable at every voxel of the video. Most importantly, we show that\nthe same exact architecture can be used to achieve competitive results on three\nwidely different voxel-prediction tasks: video semantic segmentation, optical\nflow estimation, and video coloring. The three networks learned on these\nproblems are trained from raw video without any form of preprocessing and their\noutputs do not require post-processing to achieve outstanding performance.\nThus, they offer an efficient alternative to traditional and much more\ncomputationally expensive methods in these video domains. \n\n"}
{"id": "1511.06744", "contents": "Title: Training CNNs with Low-Rank Filters for Efficient Image Classification Abstract: We propose a new method for creating computationally efficient convolutional\nneural networks (CNNs) by using low-rank representations of convolutional\nfilters. Rather than approximating filters in previously-trained networks with\nmore efficient versions, we learn a set of small basis filters from scratch;\nduring training, the network learns to combine these basis filters into more\ncomplex filters that are discriminative for image classification. To train such\nnetworks, a novel weight initialization scheme is used. This allows effective\ninitialization of connection weights in convolutional layers composed of groups\nof differently-shaped filters. We validate our approach by applying it to\nseveral existing CNN architectures and training these networks from scratch\nusing the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or\nhigher accuracy than conventional CNNs with much less compute. Applying our\nmethod to an improved version of VGG-11 network using global max-pooling, we\nachieve comparable validation accuracy using 41% less compute and only 24% of\nthe original VGG-11 model parameters; another variant of our method gives a 1\npercentage point increase in accuracy over our improved VGG-11 model, giving a\ntop-5 center-crop validation accuracy of 89.7% while reducing computation by\n16% relative to the original VGG-11 model. Applying our method to the GoogLeNet\narchitecture for ILSVRC, we achieved comparable accuracy with 26% less compute\nand 41% fewer model parameters. Applying our method to a near state-of-the-art\nnetwork for CIFAR, we achieved comparable accuracy with 46% less compute and\n55% fewer parameters. \n\n"}
{"id": "1511.06789", "contents": "Title: The Unreasonable Effectiveness of Noisy Data for Fine-Grained\n  Recognition Abstract: Current approaches for fine-grained recognition do the following: First,\nrecruit experts to annotate a dataset of images, optionally also collecting\nmore structured data in the form of part annotations and bounding boxes.\nSecond, train a model utilizing this data. Toward the goal of solving\nfine-grained recognition, we introduce an alternative approach, leveraging\nfree, noisy data from the web and simple, generic methods of recognition. This\napproach has benefits in both performance and scalability. We demonstrate its\nefficacy on four fine-grained datasets, greatly exceeding existing state of the\nart without the manual collection of even a single label, and furthermore show\nfirst results at scaling to more than 10,000 fine-grained categories.\nQuantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on\nBirdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using\ntheir annotated training sets. We compare our approach to an active learning\napproach for expanding fine-grained datasets. \n\n"}
{"id": "1511.07404", "contents": "Title: Learning Visual Predictive Models of Physics for Playing Billiards Abstract: The ability to plan and execute goal specific actions in varied, unexpected\nsettings is a central requirement of intelligent agents. In this paper, we\nexplore how an agent can be equipped with an internal model of the dynamics of\nthe external world, and how it can use this model to plan novel actions by\nrunning multiple internal simulations (\"visual imagination\"). Our models\ndirectly process raw visual input, and use a novel object-centric prediction\nformulation based on visual glimpses centered on objects (fixations) to enforce\ntranslational invariance of the learned physical laws. The agent gathers\ntraining data through random interaction with a collection of different\nenvironments, and the resulting model can then be used to plan goal-directed\nactions in novel environments that the agent has not seen before. We\ndemonstrate that our agent can accurately plan actions for playing a simulated\nbilliards game, which requires pushing a ball into a target position or into\ncollision with another ball. \n\n"}
{"id": "1511.08919", "contents": "Title: Phase space barriers and dividing surfaces in the absence of critical\n  points of the potential energy: Application to roaming in ozone Abstract: We examine the phase space structures that govern reaction dynamics in the\nabsence of critical points on the potential energy surface. We show that in the\nvicinity of hyperbolic invariant tori it is possible to define phase space\ndividing surfaces that are analogous to the dividing surfaces governing\ntransition from reactants to products near a critical point of the potential\nenergy surface. We investigate the problem of capture of an atom by a diatomic\nmolecule and show that a normally hyperbolic invariant manifold exists at large\natom-diatom distances, away from any critical points on the potential. This\nnormally hyperbolic invariant manifold is the anchor for the construction of a\ndividing surface in phase space, which defines the outer or loose transition\nstate governing capture dynamics. We present an algorithm for sampling an\napproximate capture dividing surface, and apply our methods to the\nrecombination of the ozone molecule. We treat both 2 and 3 degree of freedom\nmodels with zero total angular momentum. We have located the normally\nhyperbolic invariant manifold from which the orbiting (outer) transition state\nis constructed. This forms the basis for our analysis of trajectories for ozone\nin general, but with particular emphasis on the roaming trajectories. \n\n"}
{"id": "1511.09430", "contents": "Title: Infinitely many nonlocal conservation laws for the $ABC$ equation with\n  $A+B+C\\neq 0$ Abstract: We construct an infinite hierarchy of nonlocal conservation laws for the\n$ABC$ equation $A u_t\\,u_{xy}+B u_x\\,u_{ty}+C u_y\\,u_{tx} = 0$, where $A,B,C$\nare constants and $A+B+C\\neq 0$, using a nonisospectral Lax pair. As a\nbyproduct, we present new coverings for the ABC equation. The method of proof\nof nontriviality of the conservation laws under study is quite general and can\nbe applied to many other integrable multidimensional systems. \n\n"}
{"id": "1512.00389", "contents": "Title: Accelerated graph-based nonlinear denoising filters Abstract: Denoising filters, such as bilateral, guided, and total variation filters,\napplied to images on general graphs may require repeated application if noise\nis not small enough. We formulate two acceleration techniques of the resulted\niterations: conjugate gradient method and Nesterov's acceleration. We\nnumerically show efficiency of the accelerated nonlinear filters for image\ndenoising and demonstrate 2-12 times speed-up, i.e., the acceleration\ntechniques reduce the number of iterations required to reach a given peak\nsignal-to-noise ratio (PSNR) by the above indicated factor of 2-12. \n\n"}
{"id": "1512.00395", "contents": "Title: A non autonomous generalization of the $Q_{\\text{V}}$ equation Abstract: In this paper we introduce a non autonomous generalization of the $\\QV$\nequation introduced by Viallet. All the equations of Boll's classification\nappear in it for special choices of the parameters. Using the algebraic entropy\ntest we infer that the equation should be integrable and with the aid of a\nformula introduced by Xenitidis we find its three point generalized symmetries. \n\n"}
{"id": "1512.01840", "contents": "Title: Multisoliton solutions of the vector nonlinear Schr\\\"odinger equation\n  (Kulish-Sklyanin model) and the vector mKdV equation Abstract: There exist two natural vector generalizations of the completely integrable\nnonlinear Schr\\\"odinger (NLS) equation in $1+1$ dimensions: the well-known\nManakov model and the lesser-known Kulish-Sklyanin model. In this paper, we\npropose a binary Darboux (or Zakharov-Shabat dressing) transformation that can\nbe directly applied to the Kulish-Sklyanin model. By deriving a simple closed\nexpression for iterations of the binary Darboux transformation, we obtain an\nexplicit formula for the $N$-soliton solution of the Kulish-Sklyanin model\nunder vanishing boundary conditions. Because the third-order symmetry of the\nvector NLS equation can be reduced to a vector generalization of the modified\nKdV (mKdV) equation, we can also obtain multisoliton (or multi-breather)\nsolutions of the vector mKdV equation in closed form. \n\n"}
{"id": "1512.01967", "contents": "Title: On Partial Differential and Difference Equations with Symmetries\n  Depending on Arbitrary Functions Abstract: In this note we present some ideas on when Lie symmetries, both point and\ngeneralized, can depend on arbitrary functions. We show on a few examples, both\nin partial differential and partial difference equations when this happens.\nMoreover we show that the infinitesimal generators of generalized symmetries\ndepending on arbitrary functions, both for continuous and discrete equations,\neffectively play the role of master symmetries. \n\n"}
{"id": "1512.02946", "contents": "Title: Tensor fields defined by Lax representations Abstract: In this paper, some properties of tensor fields constructed by the Lax\nrepresentation of chiral-type systems are investigated. \n\n"}
{"id": "1512.05986", "contents": "Title: Can Pretrained Neural Networks Detect Anatomy? Abstract: Convolutional neural networks demonstrated outstanding empirical results in\ncomputer vision and speech recognition tasks where labeled training data is\nabundant. In medical imaging, there is a huge variety of possible imaging\nmodalities and contrasts, where annotated data is usually very scarce. We\npresent two approaches to deal with this challenge. A network pretrained in a\ndifferent domain with abundant data is used as a feature extractor, while a\nsubsequent classifier is trained on a small target dataset; and a deep\narchitecture trained with heavy augmentation and equipped with sophisticated\nregularization methods. We test the approaches on a corpus of X-ray images to\ndesign an anatomy detection system. \n\n"}
{"id": "1512.06658", "contents": "Title: Deep Learning for Surface Material Classification Using Haptic And\n  Visual Information Abstract: When a user scratches a hand-held rigid tool across an object surface, an\nacceleration signal can be captured, which carries relevant information about\nthe surface. More importantly, such a haptic signal is complementary to the\nvisual appearance of the surface, which suggests the combination of both\nmodalities for the recognition of the surface material. In this paper, we\npresent a novel deep learning method dealing with the surface material\nclassification problem based on a Fully Convolutional Network (FCN), which\ntakes as input the aforementioned acceleration signal and a corresponding image\nof the surface texture. Compared to previous surface material classification\nsolutions, which rely on a careful design of hand-crafted domain-specific\nfeatures, our method automatically extracts discriminative features utilizing\nthe advanced deep learning methodologies. Experiments performed on the TUM\nsurface material database demonstrate that our method achieves state-of-the-art\nclassification accuracy robustly and efficiently. \n\n"}
{"id": "1512.06963", "contents": "Title: Multi-Instance Visual-Semantic Embedding Abstract: Visual-semantic embedding models have been recently proposed and shown to be\neffective for image classification and zero-shot learning, by mapping images\ninto a continuous semantic label space. Although several approaches have been\nproposed for single-label embedding tasks, handling images with multiple labels\n(which is a more general setting) still remains an open problem, mainly due to\nthe complex underlying corresponding relationship between image and its labels.\nIn this work, we present Multi-Instance visual-semantic Embedding model (MIE)\nfor embedding images associated with either single or multiple labels. Our\nmodel discovers and maps semantically-meaningful image subregions to their\ncorresponding labels. And we demonstrate the superiority of our method over the\nstate-of-the-art on two tasks, including multi-label image annotation and\nzero-shot learning. \n\n"}
{"id": "1512.07155", "contents": "Title: Do Less and Achieve More: Training CNNs for Action Recognition Utilizing\n  Action Images from the Web Abstract: Recently, attempts have been made to collect millions of videos to train CNN\nmodels for action recognition in videos. However, curating such large-scale\nvideo datasets requires immense human labor, and training CNNs on millions of\nvideos demands huge computational resources. In contrast, collecting action\nimages from the Web is much easier and training on images requires much less\ncomputation. In addition, labeled web images tend to contain discriminative\naction poses, which highlight discriminative portions of a video's temporal\nprogression. We explore the question of whether we can utilize web action\nimages to train better CNN models for action recognition in videos. We collect\n23.8K manually filtered images from the Web that depict the 101 actions in the\nUCF101 action video dataset. We show that by utilizing web action images along\nwith videos in training, significant performance boosts of CNN models can be\nachieved. We then investigate the scalability of the process by leveraging\ncrawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M\nvideo frames by 393K unfiltered images and get comparable performance. \n\n"}
{"id": "1512.09300", "contents": "Title: Autoencoding beyond pixels using a learned similarity metric Abstract: We present an autoencoder that leverages learned representations to better\nmeasure similarities in data space. By combining a variational autoencoder with\na generative adversarial network we can use learned feature representations in\nthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,\nwe replace element-wise errors with feature-wise errors to better capture the\ndata distribution while offering invariance towards e.g. translation. We apply\nour method to images of faces and show that it outperforms VAEs with\nelement-wise similarity measures in terms of visual fidelity. Moreover, we show\nthat the method learns an embedding in which high-level abstract visual\nfeatures (e.g. wearing glasses) can be modified using simple arithmetic. \n\n"}
{"id": "1601.00088", "contents": "Title: Understanding Symmetric Smoothing Filters: A Gaussian Mixture Model\n  Perspective Abstract: Many patch-based image denoising algorithms can be formulated as applying a\nsmoothing filter to the noisy image. Expressed as matrices, the smoothing\nfilters must be row normalized so that each row sums to unity. Surprisingly, if\nwe apply a column normalization before the row normalization, the performance\nof the smoothing filter can often be significantly improved. Prior works showed\nthat such performance gain is related to the Sinkhorn-Knopp balancing\nalgorithm, an iterative procedure that symmetrizes a row-stochastic matrix to a\ndoubly-stochastic matrix. However, a complete understanding of the performance\ngain phenomenon is still lacking.\n  In this paper, we study the performance gain phenomenon from a statistical\nlearning perspective. We show that Sinkhorn-Knopp is equivalent to an\nExpectation-Maximization (EM) algorithm of learning a Gaussian mixture model of\nthe image patches. By establishing the correspondence between the steps of\nSinkhorn-Knopp and the EM algorithm, we provide a geometrical interpretation of\nthe symmetrization process. This observation allows us to develop a new\ndenoising algorithm called Gaussian mixture model symmetric smoothing filter\n(GSF). GSF is an extension of the Sinkhorn-Knopp and is a generalization of the\noriginal smoothing filters. Despite its simple formulation, GSF outperforms\nmany existing smoothing filters and has a similar performance compared to\nseveral state-of-the-art denoising algorithms. \n\n"}
{"id": "1601.00400", "contents": "Title: Multi-task CNN Model for Attribute Prediction Abstract: This paper proposes a joint multi-task learning algorithm to better predict\nattributes in images using deep convolutional neural networks (CNN). We\nconsider learning binary semantic attributes through a multi-task CNN model,\nwhere each CNN will predict one binary attribute. The multi-task learning\nallows CNN models to simultaneously share visual knowledge among different\nattribute categories. Each CNN will generate attribute-specific feature\nrepresentations, and then we apply multi-task learning on the features to\npredict their attributes. In our multi-task framework, we propose a method to\ndecompose the overall model's parameters into a latent task matrix and\ncombination matrix. Furthermore, under-sampled classifiers can leverage shared\nstatistics from other classifiers to improve their performance. Natural\ngrouping of attributes is applied such that attributes in the same group are\nencouraged to share more knowledge. Meanwhile, attributes in different groups\nwill generally compete with each other, and consequently share less knowledge.\nWe show the effectiveness of our method on two popular attribute datasets. \n\n"}
{"id": "1601.00740", "contents": "Title: Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep\n  Learning Architecture Abstract: Advanced Driver Assistance Systems (ADAS) have made driving safer over the\nlast decade. They prepare vehicles for unsafe road conditions and alert drivers\nif they perform a dangerous maneuver. However, many accidents are unavoidable\nbecause by the time drivers are alerted, it is already too late. Anticipating\nmaneuvers beforehand can alert drivers before they perform the maneuver and\nalso give ADAS more time to avoid or prepare for the danger.\n  In this work we propose a vehicular sensor-rich platform and learning\nalgorithms for maneuver anticipation. For this purpose we equip a car with\ncameras, Global Positioning System (GPS), and a computing device to capture the\ndriving context from both inside and outside of the car. In order to anticipate\nmaneuvers, we propose a sensory-fusion deep learning architecture which jointly\nlearns to anticipate and fuse multiple sensory streams. Our architecture\nconsists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory\n(LSTM) units to capture long temporal dependencies. We propose a novel training\nprocedure which allows the network to predict the future given only a partial\ntemporal context. We introduce a diverse data set with 1180 miles of natural\nfreeway and city driving, and show that we can anticipate maneuvers 3.5 seconds\nbefore they occur in real-time with a precision and recall of 90.5\\% and 87.4\\%\nrespectively. \n\n"}
{"id": "1601.01929", "contents": "Title: On the N-wave Equations with PT-symmetry Abstract: We study extensions of N-wave systems with PT-symmetry. The types of\n(nonlocal) reductions leading to integrable equations invariant with respect to\nP- (spatial reflection) and T- (time reversal) symmetries is described. The\ncorresponding constraints on the fundamental analytic solutions and the\nscattering data are derived. Based on examples of 3-wave (related to the\nalgebra sl(3,C)) and 4-wave (related to the algebra so(5,C)) systems, the\nproperties of different types of 1- and 2-soliton solutions are discussed. It\nis shown that the PT symmetric 3-wave equations may have regular multi-soliton\nsolutions for some specific choices of their parameters. \n\n"}
{"id": "1601.03642", "contents": "Title: Creativity in Machine Learning Abstract: Recent machine learning techniques can be modified to produce creative\nresults. Those results did not exist before; it is not a trivial combination of\nthe data which was fed into the machine learning system. The obtained results\ncome in multiple forms: As images, as text and as audio.\n  This paper gives a high level overview of how they are created and gives some\nexamples. It is meant to be a summary of the current work and give people who\nare new to machine learning some starting points. \n\n"}
{"id": "1601.06909", "contents": "Title: Hidden and self-excited attractors in electromechanical systems with and\n  without equilibria Abstract: This paper studies hidden oscillations appearing in electromechanical systems\nwith and without equilibria. Three different systems with such effects are\nconsidered: translational oscillator-rotational actuator, drilling system\nactuated by a DC-motor and drilling system actuated by induction motor. We\ndemonstrate that all three systems experience hidden oscillations in sense of\nmathematical definition. But from physical point of view in certain cases it is\nquite easy to localize there oscillations. \n\n"}
{"id": "1601.07258", "contents": "Title: Fast Integral Image Estimation at 1% measurement rate Abstract: We propose a framework called ReFInE to directly obtain integral image\nestimates from a very small number of spatially multiplexed measurements of the\nscene without iterative reconstruction of any auxiliary image, and demonstrate\ntheir practical utility in visual object tracking. Specifically, we design\nmeasurement matrices which are tailored to facilitate extremely fast estimation\nof the integral image, by using a single-shot linear operation on the measured\nvector. Leveraging a prior model for the images, we formulate a nuclear norm\nminimization problem with second order conic constraints to jointly obtain the\nmeasurement matrix and the linear operator. Through qualitative and\nquantitative experiments, we show that high quality integral image estimates\ncan be obtained using our framework at very low measurement rates. Further, on\na standard dataset of 50 videos, we present object tracking results which are\ncomparable to the state-of-the-art methods, even at an extremely low\nmeasurement rate of 1%. \n\n"}
{"id": "1601.08188", "contents": "Title: Lipreading with Long Short-Term Memory Abstract: Lipreading, i.e. speech recognition from visual-only recordings of a\nspeaker's face, can be achieved with a processing pipeline based solely on\nneural networks, yielding significantly better accuracy than conventional\nmethods. Feed-forward and recurrent neural network layers (namely Long\nShort-Term Memory; LSTM) are stacked to form a single structure which is\ntrained by back-propagating error gradients through all the layers. The\nperformance of such a stacked network was experimentally evaluated and compared\nto a standard Support Vector Machine classifier using conventional computer\nvision features (Eigenlips and Histograms of Oriented Gradients). The\nevaluation was performed on data from 19 speakers of the publicly available\nGRID corpus. With 51 different words to classify, we report a best word\naccuracy on held-out evaluation speakers of 79.6% using the end-to-end neural\nnetwork-based solution (11.6% improvement over the best feature-based solution\nevaluated). \n\n"}
{"id": "1602.02540", "contents": "Title: Variations on chaos in physics: from unpredictability to universal laws Abstract: The tremendous popular success of Chaos Theory shares some common points with\nthe not less fortunate Relativity: they both rely on a misunderstanding.\nIndeed, ironically , the scientific meaning of these terms for mathematicians\nand physicists is quite opposite to the one most people have in mind and are\nattracted by. One may suspect that part of the psychological roots of this\nseductive appeal relies in the fact that with these ambiguous names, together\nwith some superficial clich{\\'e}s or slogans immediately related to them (\"the\nbutterfly effect\" or \"everything is relative\"), some have the more or less\nsecret hope to find matter that would undermine two pillars of science, namely\nits ability to predict and to bring out a universal objectivity. Here I propose\nto focus on Chaos Theory and illustrate on several examples how, very much like\nRelativity, it strengthens the position it seems to contend with at first\nsight: the failure of predictability can be overcome and leads to precise,\nstable and even more universal predictions. \n\n"}
{"id": "1602.04243", "contents": "Title: Response to: \"Limitations of the Method of Lagrangian Descriptors\"\n  [arXiv:1510.04838] Abstract: This Response is concerned with the recent Comment of Ruiz-Herrera,\n\"Limitations of the Method of Lagrangian Descriptors\" [arXiv:1510.04838],\ncriticising the method of Lagrangian Descriptors. In spite of the significant\nbody of literature asserting the contrary, Ruiz-Herrera claims that the method\nfails to reveal the presence of stable and unstable manifolds of hyperbolic\ntrajectories in incompressible systems and in almost all linear systems. He\nsupports this claim by considering the method of Lagrangian descriptors applied\nto three specific examples. However in this response we show that Ruiz-Herrera\ndoes not understand the proper application and interpretation of the method\nand, when correctly applied, the method beautifully and unambiguously detects\nthe stable and unstable manifolds of the hyperbolic trajectories in his\nexamples. \n\n"}
{"id": "1602.04348", "contents": "Title: Character Proposal Network for Robust Text Extraction Abstract: Maximally stable extremal regions (MSER), which is a popular method to\ngenerate character proposals/candidates, has shown superior performance in\nscene text detection. However, the pixel-level operation limits its capability\nfor handling some challenging cases (e.g., multiple connected characters,\nseparated parts of one character and non-uniform illumination). To better\ntackle these cases, we design a character proposal network (CPN) by taking\nadvantage of the high capacity and fast computing of fully convolutional\nnetwork (FCN). Specifically, the network simultaneously predicts characterness\nscores and refines the corresponding locations. The characterness scores can be\nused for proposal ranking to reject non-character proposals and the refining\nprocess aims to obtain the more accurate locations. Furthermore, considering\nthe situation that different characters have different aspect ratios, we\npropose a multi-template strategy, designing a refiner for each aspect ratio.\nThe extensive experiments indicate our method achieves recall rates of 93.88%,\n93.60% and 96.46% on ICDAR 2013, SVT and Chinese2k datasets respectively using\nless than 1000 proposals, demonstrating promising performance of our character\nproposal network. \n\n"}
{"id": "1602.05941", "contents": "Title: Multi-resolution Compressive Sensing Reconstruction Abstract: We consider the problem of reconstructing an image from compressive\nmeasurements using a multi-resolution grid. In this context, the reconstructed\nimage is divided into multiple regions, each one with a different resolution.\nThis problem arises in situations where the image to reconstruct contains a\ncertain region of interest (RoI) that is more important than the rest. Through\na theoretical analysis and simulation experiments we show that the\nmulti-resolution reconstruction provides a higher quality of the RoI compared\nto the traditional single-resolution approach. \n\n"}
{"id": "1602.07017", "contents": "Title: A survey of sparse representation: algorithms and applications Abstract: Sparse representation has attracted much attention from researchers in fields\nof signal processing, image processing, computer vision and pattern\nrecognition. Sparse representation also has a good reputation in both\ntheoretical research and practical applications. Many different algorithms have\nbeen proposed for sparse representation. The main purpose of this article is to\nprovide a comprehensive study and an updated review on sparse representation\nand to supply a guidance for researchers. The taxonomy of sparse representation\nmethods can be studied from various viewpoints. For example, in terms of\ndifferent norm minimizations used in sparsity constraints, the methods can be\nroughly categorized into five groups: sparse representation with $l_0$-norm\nminimization, sparse representation with $l_p$-norm (0$<$p$<$1) minimization,\nsparse representation with $l_1$-norm minimization and sparse representation\nwith $l_{2,1}$-norm minimization. In this paper, a comprehensive overview of\nsparse representation is provided. The available sparse representation\nalgorithms can also be empirically categorized into four groups: greedy\nstrategy approximation, constrained optimization, proximity algorithm-based\noptimization, and homotopy algorithm-based sparse representation. The\nrationales of different algorithms in each category are analyzed and a wide\nrange of sparse representation applications are summarized, which could\nsufficiently reveal the potential nature of the sparse representation theory.\nSpecifically, an experimentally comparative study of these sparse\nrepresentation algorithms was presented. The Matlab code used in this paper can\nbe available at: http://www.yongxu.org/lunwen.html. \n\n"}
{"id": "1602.07157", "contents": "Title: Multi-fold Darboux transformations of the extended bigraded Toda\n  hierarchy Abstract: With the extended logarithmic flow equations and some extended Vertex\noperators in generalized Hirota bilinear equations, extended bigraded Toda\nhierarchy(EBTH) was proved to govern the Gromov-Witten theory of orbiford\n$c_{NM}$ in literature. The generating function of these Gromov-Witten\ninvariants is one special solution of the EBTH. In this paper, the multi-fold\nDarboux transformations and their determinant representations of the EBTH are\ngiven with two different gauge transformation operators. The two Darboux\ntransformations in different directions are used to generate new solutions from\nknown solutions which include soliton solutions of $(N,N)$-EBTH, i.e. the EBTH\nwhen $N=M$. From the generation of new solutions, one can find the big\ndifference between the EBTH and the extended Toda hierarchy(ETH). Meanwhile we\nplotted the soliton graphs of the $(N,N)$-EBTH from which some approximation\nanalysis will be given. From the analysis on velocities of soliton solutions,\nthe difference between the extended flows and other flows are shown. The two\ndifferent Darboux transformations constructed by us might be useful in\nGromov-Witten theory of orbiford $c_{NM}$. \n\n"}
{"id": "1603.01976", "contents": "Title: Deep Contrast Learning for Salient Object Detection Abstract: Salient object detection has recently witnessed substantial progress due to\npowerful features extracted using deep convolutional neural networks (CNNs).\nHowever, existing CNN-based methods operate at the patch level instead of the\npixel level. Resulting saliency maps are typically blurry, especially near the\nboundary of salient objects. Furthermore, image patches are treated as\nindependent samples even when they are overlapping, giving rise to significant\nredundancy in computation and storage. In this CVPR 2016 paper, we propose an\nend-to-end deep contrast network to overcome the aforementioned limitations.\nOur deep network consists of two complementary components, a pixel-level fully\nconvolutional stream and a segment-wise spatial pooling stream. The first\nstream directly produces a saliency map with pixel-level accuracy from an input\nimage. The second stream extracts segment-wise features very efficiently, and\nbetter models saliency discontinuities along object boundaries. Finally, a\nfully connected CRF model can be optionally incorporated to improve spatial\ncoherence and contour localization in the fused result from these two streams.\nExperimental results demonstrate that our deep model significantly improves the\nstate of the art. \n\n"}
{"id": "1603.02814", "contents": "Title: Image Captioning and Visual Question Answering Based on Attributes and\n  External Knowledge Abstract: Much recent progress in Vision-to-Language problems has been achieved through\na combination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs). This approach does not explicitly represent high-level\nsemantic concepts, but rather seeks to progress directly from image features to\ntext. In this paper we first propose a method of incorporating high-level\nconcepts into the successful CNN-RNN approach, and show that it achieves a\nsignificant improvement on the state-of-the-art in both image captioning and\nvisual question answering. We further show that the same mechanism can be used\nto incorporate external knowledge, which is critically important for answering\nhigh level visual questions. Specifically, we design a visual question\nanswering model that combines an internal representation of the content of an\nimage with information extracted from a general knowledge base to answer a\nbroad range of image-based questions. It particularly allows questions to be\nasked about the contents of an image, even when the image itself does not\ncontain a complete answer. Our final model achieves the best reported results\non both image captioning and visual question answering on several benchmark\ndatasets. \n\n"}
{"id": "1603.03542", "contents": "Title: Completeness of the Bethe states for the rational, spin-1/2\n  Richardson--Gaudin system Abstract: Establishing the completeness of a Bethe Ansatz solution for an exactly\nsolved model is a perennial challenge, which is typically approached on a case\nby case basis. For the rational, spin-1/2 Richardson--Gaudin system it will be\nargued that, for generic values of the system's coupling parameters, the Bethe\nstates are complete. This method does not depend on knowledge of the\ndistribution of Bethe roots, such as a string hypothesis, and is generalisable\nto a wider class of systems. \n\n"}
{"id": "1603.04525", "contents": "Title: Pushing the Limits of Deep CNNs for Pedestrian Detection Abstract: Compared to other applications in computer vision, convolutional neural\nnetworks have under-performed on pedestrian detection. A breakthrough was made\nvery recently by using sophisticated deep CNN models, with a number of\nhand-crafted features, or explicit occlusion handling mechanism. In this work,\nwe show that by re-using the convolutional feature maps (CFMs) of a deep\nconvolutional neural network (DCNN) model as image features to train an\nensemble of boosted decision models, we are able to achieve the best reported\naccuracy without using specially designed learning algorithms. We empirically\nidentify and disclose important implementation details. We also show that pixel\nlabelling may be simply combined with a detector to boost the detection\nperformance. By adding complementary hand-crafted features such as optical\nflow, the DCNN based detector can be further improved. We set a new record on\nthe Caltech pedestrian dataset, lowering the log-average miss rate from\n$11.7\\%$ to $8.9\\%$, a relative improvement of $24\\%$. We also achieve a\ncomparable result to the state-of-the-art approaches on the KITTI dataset. \n\n"}
{"id": "1603.06182", "contents": "Title: Modelling Temporal Information Using Discrete Fourier Transform for\n  Video Classification Abstract: Recently, video classification attracts intensive research efforts. However,\nmost existing works are based on framelevel visual features, which might fail\nto model the temporal information, e.g. characteristics accumulated along time.\nIn order to capture video temporal information, we propose to analyse features\nin frequency domain transformed by discrete Fourier transform (DFT features).\nFrame-level features are firstly extract by a pre-trained deep convolutional\nneural network (CNN). Then, time domain features are transformed and\ninterpolated into DFT features. CNN and DFT features are further encoded by\nusing different pooling methods and fused for video classification. In this\nway, static image features extracted from a pre-trained deep CNN and temporal\ninformation represented by DFT features are jointly considered for video\nclassification. We test our method for video emotion classification and action\nrecognition. Experimental results demonstrate that combining DFT features can\neffectively capture temporal information and therefore improve the performance\nof both video emotion classification and action recognition. Our approach has\nachieved a state-of-the-art performance on the largest video emotion dataset\n(VideoEmotion-8 dataset) and competitive results on UCF-101. \n\n"}
{"id": "1603.08233", "contents": "Title: Evolution of active categorical image classification via saccadic eye\n  movement Abstract: Pattern recognition and classification is a central concern for modern\ninformation processing systems. In particular, one key challenge to image and\nvideo classification has been that the computational cost of image processing\nscales linearly with the number of pixels in the image or video. Here we\npresent an intelligent machine (the \"active categorical classifier,\" or ACC)\nthat is inspired by the saccadic movements of the eye, and is capable of\nclassifying images by selectively scanning only a portion of the image. We\nharness evolutionary computation to optimize the ACC on the MNIST hand-written\ndigit classification task, and provide a proof-of-concept that the ACC works on\nnoisy multi-class data. We further analyze the ACC and demonstrate its ability\nto classify images after viewing only a fraction of the pixels, and provide\ninsight on future research paths to further improve upon the ACC presented\nhere. \n\n"}
{"id": "1603.08637", "contents": "Title: Learning a Predictable and Generative Vector Representation for Objects Abstract: What is a good vector representation of an object? We believe that it should\nbe generative in 3D, in the sense that it can produce new 3D objects; as well\nas be predictable from 2D, in the sense that it can be perceived from 2D\nimages. We propose a novel architecture, called the TL-embedding network, to\nlearn an embedding space with these properties. The network consists of two\ncomponents: (a) an autoencoder that ensures the representation is generative;\nand (b) a convolutional network that ensures the representation is predictable.\nThis enables tackling a number of tasks including voxel prediction from 2D\nimages and 3D model retrieval. Extensive experimental analysis demonstrates the\nusefulness and versatility of this embedding. \n\n"}
{"id": "1603.08907", "contents": "Title: Cross-modal Supervision for Learning Active Speaker Detection in Video Abstract: In this paper, we show how to use audio to supervise the learning of active\nspeaker detection in video. Voice Activity Detection (VAD) guides the learning\nof the vision-based classifier in a weakly supervised manner. The classifier\nuses spatio-temporal features to encode upper body motion - facial expressions\nand gesticulations associated with speaking. We further improve a generic model\nfor active speaker detection by learning person specific models. Finally, we\ndemonstrate the online adaptation of generic models learnt on one dataset, to\npreviously unseen people in a new dataset, again using audio (VAD) for weak\nsupervision. The use of temporal continuity overcomes the lack of clean\ntraining data. We are the first to present an active speaker detection system\nthat learns on one audio-visual dataset and automatically adapts to speakers in\na new dataset. This work can be seen as an example of how the availability of\nmulti-modal data allows us to learn a model without the need for supervision,\nby transferring knowledge from one modality to another. \n\n"}
{"id": "1603.09056", "contents": "Title: Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks\n  with Symmetric Skip Connections Abstract: In this paper, we propose a very deep fully convolutional encoding-decoding\nframework for image restoration such as denoising and super-resolution. The\nnetwork is composed of multiple layers of convolution and de-convolution\noperators, learning end-to-end mappings from corrupted images to the original\nones. The convolutional layers act as the feature extractor, which capture the\nabstraction of image contents while eliminating noises/corruptions.\nDe-convolutional layers are then used to recover the image details. We propose\nto symmetrically link convolutional and de-convolutional layers with skip-layer\nconnections, with which the training converges much faster and attains a\nhigher-quality local optimum. First, The skip connections allow the signal to\nbe back-propagated to bottom layers directly, and thus tackles the problem of\ngradient vanishing, making training deep networks easier and achieving\nrestoration performance gains consequently. Second, these skip connections pass\nimage details from convolutional layers to de-convolutional layers, which is\nbeneficial in recovering the original image. Significantly, with the large\ncapacity, we can handle different levels of noises using a single model.\nExperimental results show that our network achieves better performance than all\npreviously reported state-of-the-art methods. \n\n"}
{"id": "1603.09690", "contents": "Title: Linear and fractional response for the SRB measure of smooth hyperbolic\n  attractors and discontinuous observables Abstract: We consider a smooth one-parameter family $t \\to f_t$ of diffeomorphisms with\ncompact transitive Axiom A attractors. Our first result (corrected) is that for\nany function $G$ in the Sobolev space $H^r_p$, with $p>1$ and $0<r<1/p$, the\nmap $R(t)$ sending $t$ to the average of $G$ with respect to the SRB measure of\n$f_t$ is $\\alpha$-H\\\"older continuous for all $\\alpha <r- |log \\mathcal\nJ|/(p|log \\nu_s|$) where $\\mathcal J\\le 1$ is the strongest volume contraction\nand $\\nu_s<1$ is the weakest contraction. This applies to\n$\\theta(x)=h(x)\\Theta(g(x)-a)$ (for all $\\alpha <1- |log \\mathcal J|/|log\n\\nu_s|$) for $h$ and $g$ smooth and $\\Theta$ the Heaviside function, if $a$ is\nnot a critical value of $g$. Our second result says that for any such function\nso that, in addition, the intersection of the set of points $x$ so that\n$g(x)=a$ with the support of $h$ is foliated by \"admissible stable leaves\" of\n$f_t$, the map $R(t)$ is differentiable. (We provide distributional linear\nresponse and fluctuation-dissipation formulas for the derivative.) Obtaining\nlinear response or fractional response for such observables is motivated by\nextreme-value theory. --- Second version, following the referee's comments: We\nexplain better the cone choices around (2.4). Appendix A contains information\non the Banach spaces. We added the paragraph containing (2.6) in the proof of\nTheorem 2.1. In the proof of Theorem 3.3, we do not need to introduce\nmollifiers. However, the new argument around (2.6) is not available here, so we\nmust replace the pair $(u-1, |s-1|)$ by $(u-2, |s-2|)$. This is why we now\nassume that $h$ is $C^3$ and that $g$ and the foliations are $C^4$. --- Third\nversion: We have added a corrigendum modifying the first result (Theorem 2.1). \n\n"}
{"id": "1604.01802", "contents": "Title: Learning to Track at 100 FPS with Deep Regression Networks Abstract: Machine learning techniques are often used in computer vision due to their\nability to leverage large amounts of training data to improve performance.\nUnfortunately, most generic object trackers are still trained from scratch\nonline and do not benefit from the large number of videos that are readily\navailable for offline training. We propose a method for offline training of\nneural networks that can track novel objects at test-time at 100 fps. Our\ntracker is significantly faster than previous methods that use neural networks\nfor tracking, which are typically very slow to run and not practical for\nreal-time applications. Our tracker uses a simple feed-forward network with no\nonline training required. The tracker learns a generic relationship between\nobject motion and appearance and can be used to track novel objects that do not\nappear in the training set. We test our network on a standard tracking\nbenchmark to demonstrate our tracker's state-of-the-art performance. Further,\nour performance improves as we add more videos to our offline training set. To\nthe best of our knowledge, our tracker is the first neural-network tracker that\nlearns to track generic objects at 100 fps. \n\n"}
{"id": "1604.01879", "contents": "Title: GIFT: A Real-time and Scalable 3D Shape Search Engine Abstract: Projective analysis is an important solution for 3D shape retrieval, since\nhuman visual perceptions of 3D shapes rely on various 2D observations from\ndifferent view points. Although multiple informative and discriminative views\nare utilized, most projection-based retrieval systems suffer from heavy\ncomputational cost, thus cannot satisfy the basic requirement of scalability\nfor search engines. In this paper, we present a real-time 3D shape search\nengine based on the projective images of 3D shapes. The real-time property of\nour search engine results from the following aspects: (1) efficient projection\nand view feature extraction using GPU acceleration; (2) the first inverted\nfile, referred as F-IF, is utilized to speed up the procedure of multi-view\nmatching; (3) the second inverted file (S-IF), which captures a local\ndistribution of 3D shapes in the feature manifold, is adopted for efficient\ncontext-based re-ranking. As a result, for each query the retrieval task can be\nfinished within one second despite the necessary cost of IO overhead. We name\nthe proposed 3D shape search engine, which combines GPU acceleration and\nInverted File Twice, as GIFT. Besides its high efficiency, GIFT also\noutperforms the state-of-the-art methods significantly in retrieval accuracy on\nvarious shape benchmarks and competitions. \n\n"}
{"id": "1604.02125", "contents": "Title: Resolving Language and Vision Ambiguities Together: Joint Segmentation &\n  Prepositional Attachment Resolution in Captioned Scenes Abstract: We present an approach to simultaneously perform semantic segmentation and\nprepositional phrase attachment resolution for captioned images. Some\nambiguities in language cannot be resolved without simultaneously reasoning\nabout an associated image. If we consider the sentence \"I shot an elephant in\nmy pajamas\", looking at language alone (and not using common sense), it is\nunclear if it is the person or the elephant wearing the pajamas or both. Our\napproach produces a diverse set of plausible hypotheses for both semantic\nsegmentation and prepositional phrase attachment resolution that are then\njointly reranked to select the most consistent pair. We show that our semantic\nsegmentation and prepositional phrase attachment resolution modules have\ncomplementary strengths, and that joint reasoning produces more accurate\nresults than any module operating in isolation. Multiple hypotheses are also\nshown to be crucial to improved multiple-module reasoning. Our vision and\nlanguage approach significantly outperforms the Stanford Parser (De Marneffe et\nal., 2006) by 17.91% (28.69% relative) and 12.83% (25.28% relative) in two\ndifferent experiments. We also make small improvements over DeepLab-CRF (Chen\net al., 2015). \n\n"}
{"id": "1604.02194", "contents": "Title: Dynamical indicators for the prediction of bursting phenomena in\n  high-dimensional systems Abstract: Drawing upon the bursting mechanism in slow-fast systems, we propose\nindicators for the prediction of such rare extreme events which do not require\na priori known slow and fast coordinates. The indicators are associated with\nfunctionals defined in terms of Optimally Time Dependent (OTD) modes. One such\nfunctional has the form of the largest eigenvalue of the symmetric part of the\nlinearized dynamics reduced to these modes. In contrast to other choices of\nsubspaces, the proposed modes are flow invariant and therefore a projection\nonto them is dynamically meaningful. We illustrate the application of these\nindicators on three examples: a prototype low-dimensional model, a body forced\nturbulent fluid flow, and a unidirectional model of nonlinear water waves. We\nuse Bayesian statistics to quantify the predictive power of the proposed\nindicators. \n\n"}
{"id": "1604.02275", "contents": "Title: Online Open World Recognition Abstract: As we enter into the big data age and an avalanche of images have become\nreadily available, recognition systems face the need to move from close, lab\nsettings where the number of classes and training data are fixed, to dynamic\nscenarios where the number of categories to be recognized grows continuously\nover time, as well as new data providing useful information to update the\nsystem. Recent attempts, like the open world recognition framework, tried to\ninject dynamics into the system by detecting new unknown classes and adding\nthem incrementally, while at the same time continuously updating the models for\nthe known classes. incrementally adding new classes and detecting instances\nfrom unknown classes, while at the same time continuously updating the models\nfor the known classes. In this paper we argue that to properly capture the\nintrinsic dynamic of open world recognition, it is necessary to add to these\naspects (a) the incremental learning of the underlying metric, (b) the\nincremental estimate of confidence thresholds for the unknown classes, and (c)\nthe use of local learning to precisely describe the space of classes. We extend\nthree existing metric learning algorithms towards these goals by using online\nmetric learning. Experimentally we validate our approach on two large-scale\ndatasets in different learning scenarios. For all these scenarios our proposed\nmethods outperform their non-online counterparts. We conclude that local and\nonline learning is important to capture the full dynamics of open world\nrecognition. \n\n"}
{"id": "1604.03249", "contents": "Title: Attributes as Semantic Units between Natural Language and Visual\n  Recognition Abstract: Impressive progress has been made in the fields of computer vision and\nnatural language processing. However, it remains a challenge to find the best\npoint of interaction for these very different modalities. In this chapter we\ndiscuss how attributes allow us to exchange information between the two\nmodalities and in this way lead to an interaction on a semantic level.\nSpecifically we discuss how attributes allow using knowledge mined from\nlanguage resources for recognizing novel visual categories, how we can generate\nsentence description about images and video, how we can ground natural language\nin visual content, and finally, how we can answer natural language questions\nabout images. \n\n"}
{"id": "1604.03426", "contents": "Title: Sweep Distortion Removal from THz Images via Blind Demodulation Abstract: Heavy sweep distortion induced by alignments and inter-reflections of layers\nof a sample is a major burden in recovering 2D and 3D information in time\nresolved spectral imaging. This problem cannot be addressed by conventional\ndenoising and signal processing techniques as it heavily depends on the physics\nof the acquisition. Here we propose and implement an algorithmic framework\nbased on low-rank matrix recovery and alternating minimization that exploits\nthe forward model for THz acquisition. The method allows recovering the\noriginal signal in spite of the presence of temporal-spatial distortions. We\naddress a blind-demodulation problem, where based on several observations of\nthe sample texture modulated by an undesired sweep pattern, the two classes of\nsignals are separated. The performance of the method is examined in both\nsynthetic and experimental data, and the successful reconstructions are\ndemonstrated. The proposed general scheme can be implemented to advance\ninspection and imaging applications in THz and other time-resolved sensing\nmodalities. \n\n"}
{"id": "1604.03880", "contents": "Title: Detangling People: Individuating Multiple Close People and Their Body\n  Parts via Region Assembly Abstract: Today's person detection methods work best when people are in common upright\nposes and appear reasonably well spaced out in the image. However, in many real\nimages, that's not what people do. People often appear quite close to each\nother, e.g., with limbs linked or heads touching, and their poses are often not\npedestrian-like. We propose an approach to detangle people in multi-person\nimages. We formulate the task as a region assembly problem. Starting from a\nlarge set of overlapping regions from body part semantic segmentation and\ngeneric object proposals, our optimization approach reassembles those pieces\ntogether into multiple person instances. It enforces that the composed body\npart regions of each person instance obey constraints on relative sizes, mutual\nspatial relationships, foreground coverage, and exclusive label assignments\nwhen overlapping. Since optimal region assembly is a challenging combinatorial\nproblem, we present a Lagrangian relaxation method to accelerate the lower\nbound estimation, thereby enabling a fast branch and bound solution for the\nglobal optimum. As output, our method produces a pixel-level map indicating\nboth 1) the body part labels (arm, leg, torso, and head), and 2) which parts\nbelong to which individual person. Our results on three challenging datasets\nshow our method is robust to clutter, occlusion, and complex poses. It\noutperforms a variety of competing methods, including existing detector CRF\nmethods and region CNN approaches. In addition, we demonstrate its impact on a\nproxemics recognition task, which demands a precise representation of \"whose\nbody part is where\" in crowded images. \n\n"}
{"id": "1604.03968", "contents": "Title: Visual Storytelling Abstract: We introduce the first dataset for sequential vision-to-language, and explore\nhow this data may be used for the task of visual storytelling. The first\nrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211\nsequences, aligned to both descriptive (caption) and story language. We\nestablish several strong baselines for the storytelling task, and motivate an\nautomatic metric to benchmark progress. Modelling concrete description as well\nas figurative and social language, as provided in this dataset and the\nstorytelling task, has the potential to move artificial intelligence from basic\nunderstandings of typical visual scenes towards more and more human-like\nunderstanding of grounded event structure and subjective expression. \n\n"}
{"id": "1604.04382", "contents": "Title: Precomputed Real-Time Texture Synthesis with Markovian Generative\n  Adversarial Networks Abstract: This paper proposes Markovian Generative Adversarial Networks (MGANs), a\nmethod for training generative neural networks for efficient texture synthesis.\nWhile deep neural network approaches have recently demonstrated remarkable\nresults in terms of synthesis quality, they still come at considerable\ncomputational costs (minutes of run-time for low-res images). Our paper\naddresses this efficiency issue. Instead of a numerical deconvolution in\nprevious work, we precompute a feed-forward, strided convolutional network that\ncaptures the feature statistics of Markovian patches and is able to directly\ngenerate outputs of arbitrary dimensions. Such network can directly decode\nbrown noise to realistic texture, or photos to artistic paintings. With\nadversarial training, we obtain quality comparable to recent neural texture\nsynthesis methods. As no optimization is required any longer at generation\ntime, our run-time performance (0.25M pixel images at 25Hz) surpasses previous\nneural texture synthesizers by a significant margin (at least 500 times\nfaster). We apply this idea to texture synthesis, style transfer, and video\nstylization. \n\n"}
{"id": "1604.04693", "contents": "Title: Subcategory-aware Convolutional Neural Networks for Object Proposals and\n  Detection Abstract: In CNN-based object detection methods, region proposal becomes a bottleneck\nwhen objects exhibit significant scale variation, occlusion or truncation. In\naddition, these methods mainly focus on 2D object detection and cannot estimate\ndetailed properties of objects. In this paper, we propose subcategory-aware\nCNNs for object detection. We introduce a novel region proposal network that\nuses subcategory information to guide the proposal generating process, and a\nnew detection network for joint detection and subcategory classification. By\nusing subcategories related to object pose, we achieve state-of-the-art\nperformance on both detection and pose estimation on commonly used benchmarks. \n\n"}
{"id": "1604.06832", "contents": "Title: Refining Architectures of Deep Convolutional Neural Networks Abstract: Deep Convolutional Neural Networks (CNNs) have recently evinced immense\nsuccess for various image recognition tasks. However, a question of paramount\nimportance is somewhat unanswered in deep learning research - is the selected\nCNN optimal for the dataset in terms of accuracy and model size? In this paper,\nwe intend to answer this question and introduce a novel strategy that alters\nthe architecture of a given CNN for a specified dataset, to potentially enhance\nthe original accuracy while possibly reducing the model size. We use two\noperations for architecture refinement, viz. stretching and symmetrical\nsplitting. Our procedure starts with a pre-trained CNN for a given dataset, and\noptimally decides the stretch and split factors across the network to refine\nthe architecture. We empirically demonstrate the necessity of the two\noperations. We evaluate our approach on two natural scenes attributes datasets,\nSUN Attributes and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, that\nare quite contrasting in their construction. We justify our choice of datasets,\nand show that they are interestingly distinct from each other, and together\npose a challenge to our architectural refinement algorithm. Our results\nsubstantiate the usefulness of the proposed method. \n\n"}
{"id": "1604.08220", "contents": "Title: Diving deeper into mentee networks Abstract: Modern computer vision is all about the possession of powerful image\nrepresentations. Deeper and deeper convolutional neural networks have been\nbuilt using larger and larger datasets and are made publicly available. A large\nswath of computer vision scientists use these pre-trained networks with varying\ndegrees of successes in various tasks. Even though there is tremendous success\nin copying these networks, the representational space is not learnt from the\ntarget dataset in a traditional manner. One of the reasons for opting to use a\npre-trained network over a network learnt from scratch is that small datasets\nprovide less supervision and require meticulous regularization, smaller and\ncareful tweaking of learning rates to even achieve stable learning without\nweight explosion. It is often the case that large deep networks are not\nportable, which necessitates the ability to learn mid-sized networks from\nscratch.\n  In this article, we dive deeper into training these mid-sized networks on\nsmall datasets from scratch by drawing additional supervision from a large\npre-trained network. Such learning also provides better generalization\naccuracies than networks trained with common regularization techniques such as\nl2, l1 and dropouts. We show that features learnt thus, are more general than\nthose learnt independently. We studied various characteristics of such networks\nand found some interesting behaviors. \n\n"}
{"id": "1604.08772", "contents": "Title: Towards Conceptual Compression Abstract: We introduce a simple recurrent variational auto-encoder architecture that\nsignificantly improves image modeling. The system represents the\nstate-of-the-art in latent variable models for both the ImageNet and Omniglot\ndatasets. We show that it naturally separates global conceptual information\nfrom lower level details, thus addressing one of the fundamentally desired\nproperties of unsupervised learning. Furthermore, the possibility of\nrestricting ourselves to storing only global information about an image allows\nus to achieve high quality 'conceptual compression'. \n\n"}
{"id": "1605.00031", "contents": "Title: Deep Convolutional Neural Networks on Cartoon Functions Abstract: Wiatowski and B\\\"olcskei, 2015, proved that deformation stability and\nvertical translation invariance of deep convolutional neural network-based\nfeature extractors are guaranteed by the network structure per se rather than\nthe specific convolution kernels and non-linearities. While the translation\ninvariance result applies to square-integrable functions, the deformation\nstability bound holds for band-limited functions only. Many signals of\npractical relevance (such as natural images) exhibit, however, sharp and curved\ndiscontinuities and are, hence, not band-limited. The main contribution of this\npaper is a deformation stability result that takes these structural properties\ninto account. Specifically, we establish deformation stability bounds for the\nclass of cartoon functions introduced by Donoho, 2001. \n\n"}
{"id": "1605.01710", "contents": "Title: Plug-and-Play ADMM for Image Restoration: Fixed Point Convergence and\n  Applications Abstract: Alternating direction method of multiplier (ADMM) is a widely used algorithm\nfor solving constrained optimization problems in image restoration. Among many\nuseful features, one critical feature of the ADMM algorithm is its modular\nstructure which allows one to plug in any off-the-shelf image denoising\nalgorithm for a subproblem in the ADMM algorithm. Because of the plug-in\nnature, this type of ADMM algorithms is coined the name \"Plug-and-Play ADMM\".\nPlug-and-Play ADMM has demonstrated promising empirical results in a number of\nrecent papers. However, it is unclear under what conditions and by using what\ndenoising algorithms would it guarantee convergence. Also, since Plug-and-Play\nADMM uses a specific way to split the variables, it is unclear if fast\nimplementation can be made for common Gaussian and Poissonian image restoration\nproblems.\n  In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixed\npoint convergence. We show that for any denoising algorithm satisfying an\nasymptotic criteria, called bounded denoisers, Plug-and-Play ADMM converges to\na fixed point under a continuation scheme. We also present fast implementations\nfor two image restoration problems on super-resolution and single-photon\nimaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in each\nproblem type, and demonstrate promising experimental results of the algorithm. \n\n"}
{"id": "1605.03428", "contents": "Title: Image-level Classification in Hyperspectral Images using Feature\n  Descriptors, with Application to Face Recognition Abstract: In this paper, we proposed a novel pipeline for image-level classification in\nthe hyperspectral images. By doing this, we show that the discriminative\nspectral information at image-level features lead to significantly improved\nperformance in a face recognition task. We also explored the potential of\ntraditional feature descriptors in the hyperspectral images. From our\nevaluations, we observe that SIFT features outperform the state-of-the-art\nhyperspectral face recognition methods, and also the other descriptors. With\nthe increasing deployment of hyperspectral sensors in a multitude of\napplications, we believe that our approach can effectively exploit the spectral\ninformation in hyperspectral images, thus beneficial to more accurate\nclassification. \n\n"}
{"id": "1605.04006", "contents": "Title: A Gaussian Mixture MRF for Model-Based Iterative Reconstruction with\n  Applications to Low-Dose X-ray CT Abstract: Markov random fields (MRFs) have been widely used as prior models in various\ninverse problems such as tomographic reconstruction. While MRFs provide a\nsimple and often effective way to model the spatial dependencies in images,\nthey suffer from the fact that parameter estimation is difficult. In practice,\nthis means that MRFs typically have very simple structure that cannot\ncompletely capture the subtle characteristics of complex images.\n  In this paper, we present a novel Gaussian mixture Markov random field model\n(GM-MRF) that can be used as a very expressive prior model for inverse problems\nsuch as denoising and reconstruction. The GM-MRF forms a global image model by\nmerging together individual Gaussian-mixture models (GMMs) for image patches.\nIn addition, we present a novel analytical framework for computing MAP\nestimates using the GM-MRF prior model through the construction of surrogate\nfunctions that result in a sequence of quadratic optimizations. We also\nintroduce a simple but effective method to adjust the GM-MRF so as to control\nthe sharpness in low- and high-contrast regions of the reconstruction\nseparately. We demonstrate the value of the model with experiments including\nimage denoising and low-dose CT reconstruction. \n\n"}
{"id": "1605.05396", "contents": "Title: Generative Adversarial Text to Image Synthesis Abstract: Automatic synthesis of realistic images from text would be interesting and\nuseful, but current AI systems are still far from this goal. However, in recent\nyears generic and powerful recurrent neural network architectures have been\ndeveloped to learn discriminative text feature representations. Meanwhile, deep\nconvolutional generative adversarial networks (GANs) have begun to generate\nhighly compelling images of specific categories, such as faces, album covers,\nand room interiors. In this work, we develop a novel deep architecture and GAN\nformulation to effectively bridge these advances in text and image model- ing,\ntranslating visual concepts from characters to pixels. We demonstrate the\ncapability of our model to generate plausible images of birds and flowers from\ndetailed text descriptions. \n\n"}
{"id": "1605.06010", "contents": "Title: Topological dynamics of Zadeh's extension on the space of upper\n  semi-continuous fuzzy sets Abstract: In this paper, some characterizations about transitivity, mildly mixing\nproperty, $\\mathbf{a}$-transitivity, equicontinuity, uniform rigidity and\nproximality of Zadeh's extensions restricted on some invariant closed subsets\nof the space of all upper semi-continuous fuzzy sets with the level-wise metric\nare obtained. In particular, it is proved that a dynamical system is weakly\nmixing (resp., mildly mixing, weakly mixing and $\\mathbf{a}$-transitive,\nequicontinuous, uniformly rigid) if and only if the Zadeh's extension is\ntransitive (resp., mildly mixing, $\\mathbf{a}$-transitive, equicontinuous,\nuniformly rigid). \n\n"}
{"id": "1605.06155", "contents": "Title: Inter-Battery Topic Representation Learning Abstract: In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach\nextends traditional topic models by learning a factorized latent variable\nrepresentation. The structured representation leads to a model that marries\nbenefits traditionally associated with a discriminative approach, such as\nfeature selection, with those of a generative model, such as principled\nregularization and ability to handle missing data. The factorization is\nprovided by representing data in terms of aligned pairs of observations as\ndifferent views. This provides means for selecting a representation that\nseparately models topics that exist in both views from the topics that are\nunique to a single view. This structured consolidation allows for efficient and\nrobust inference and provides a compact and efficient representation. Learning\nis performed in a Bayesian fashion by maximizing a rigorous bound on the\nlog-likelihood. Firstly, we illustrate the benefits of the model on a synthetic\ndataset,. The model is then evaluated in both uni- and multi-modality settings\non two different classification tasks with off-the-shelf convolutional neural\nnetwork (CNN) features which generate state-of-the-art results with extremely\ncompact representations. \n\n"}
{"id": "1605.06431", "contents": "Title: Residual Networks Behave Like Ensembles of Relatively Shallow Networks Abstract: In this work we propose a novel interpretation of residual networks showing\nthat they can be seen as a collection of many paths of differing length.\nMoreover, residual networks seem to enable very deep networks by leveraging\nonly the short paths during training. To support this observation, we rewrite\nresidual networks as an explicit collection of paths. Unlike traditional\nmodels, paths through residual networks vary in length. Further, a lesion study\nreveals that these paths show ensemble-like behavior in the sense that they do\nnot strongly depend on each other. Finally, and most surprising, most paths are\nshorter than one might expect, and only the short paths are needed during\ntraining, as longer paths do not contribute any gradient. For example, most of\nthe gradient in a residual network with 110 layers comes from paths that are\nonly 10-34 layers deep. Our results reveal one of the key characteristics that\nseem to enable the training of very deep networks: Residual networks avoid the\nvanishing gradient problem by introducing short paths which can carry gradient\nthroughout the extent of very deep networks. \n\n"}
{"id": "1605.06695", "contents": "Title: Fine-to-coarse Knowledge Transfer For Low-Res Image Classification Abstract: We address the difficult problem of distinguishing fine-grained object\ncategories in low resolution images. Wepropose a simple an effective deep\nlearning approach that transfers fine-grained knowledge gained from high\nresolution training data to the coarse low-resolution test scenario. Such\nfine-to-coarse knowledge transfer has many real world applications, such as\nidentifying objects in surveillance photos or satellite images where the image\nresolution at the test time is very low but plenty of high resolution photos of\nsimilar objects are available. Our extensive experiments on two standard\nbenchmark datasets containing fine-grained car models and bird species\ndemonstrate that our approach can effectively transfer fine-detail knowledge to\ncoarse-detail imagery. \n\n"}
{"id": "1605.08512", "contents": "Title: SNN: Stacked Neural Networks Abstract: It has been proven that transfer learning provides an easy way to achieve\nstate-of-the-art accuracies on several vision tasks by training a simple\nclassifier on top of features obtained from pre-trained neural networks. The\ngoal of this work is to generate better features for transfer learning from\nmultiple publicly available pre-trained neural networks. To this end, we\npropose a novel architecture called Stacked Neural Networks which leverages the\nfast training time of transfer learning while simultaneously being much more\naccurate. We show that using a stacked NN architecture can result in up to 8%\nimprovements in accuracy over state-of-the-art techniques using only one\npre-trained network for transfer learning. A second aim of this work is to make\nnetwork fine- tuning retain the generalizability of the base network to unseen\ntasks. To this end, we propose a new technique called \"joint fine-tuning\" that\nis able to give accuracies comparable to finetuning the same network\nindividually over two datasets. We also show that a jointly finetuned network\ngeneralizes better to unseen tasks when compared to a network finetuned over a\nsingle task. \n\n"}
{"id": "1606.01561", "contents": "Title: Shallow Networks for High-Accuracy Road Object-Detection Abstract: The ability to automatically detect other vehicles on the road is vital to\nthe safety of partially-autonomous and fully-autonomous vehicles. Most of the\nhigh-accuracy techniques for this task are based on R-CNN or one of its faster\nvariants. In the research community, much emphasis has been applied to using 3D\nvision or complex R-CNN variants to achieve higher accuracy. However, are there\nmore straightforward modifications that could deliver higher accuracy? Yes. We\nshow that increasing input image resolution (i.e. upsampling) offers up to 12\npercentage-points higher accuracy compared to an off-the-shelf baseline. We\nalso find situations where earlier/shallower layers of CNN provide higher\naccuracy than later/deeper layers. We further show that shallow models and\nupsampled images yield competitive accuracy. Our findings contrast with the\ncurrent trend towards deeper and larger models to achieve high accuracy in\ndomain specific detection tasks. \n\n"}
{"id": "1606.02894", "contents": "Title: A Comprehensive Analysis of Deep Learning Based Representation for Face\n  Recognition Abstract: Deep learning based approaches have been dominating the face recognition\nfield due to the significant performance improvement they have provided on the\nchallenging wild datasets. These approaches have been extensively tested on\nsuch unconstrained datasets, on the Labeled Faces in the Wild and YouTube\nFaces, to name a few. However, their capability to handle individual appearance\nvariations caused by factors such as head pose, illumination, occlusion, and\nmisalignment has not been thoroughly assessed till now. In this paper, we\npresent a comprehensive study to evaluate the performance of deep learning\nbased face representation under several conditions including the varying head\npose angles, upper and lower face occlusion, changing illumination of different\nstrengths, and misalignment due to erroneous facial feature localization. Two\nsuccessful and publicly available deep learning models, namely VGG-Face and\nLightened CNN have been utilized to extract face representations. The obtained\nresults show that although deep learning provides a powerful representation for\nface recognition, it can still benefit from preprocessing, for example, for\npose and illumination normalization in order to achieve better performance\nunder various conditions. Particularly, if these variations are not included in\nthe dataset used to train the deep learning model, the role of preprocessing\nbecomes more crucial. Experimental results also show that deep learning based\nrepresentation is robust to misalignment and can tolerate facial feature\nlocalization errors up to 10% of the interocular distance. \n\n"}
{"id": "1606.04450", "contents": "Title: Multiple Human Tracking in RGB-D Data: A Survey Abstract: Multiple human tracking (MHT) is a fundamental task in many computer vision\napplications. Appearance-based approaches, primarily formulated on RGB data,\nare constrained and affected by problems arising from occlusions and/or\nillumination variations. In recent years, the arrival of cheap RGB-Depth\n(RGB-D) devices has {led} to many new approaches to MHT, and many of these\nintegrate color and depth cues to improve each and every stage of the process.\nIn this survey, we present the common processing pipeline of these methods and\nreview their methodology based (a) on how they implement this pipeline and (b)\non what role depth plays within each stage of it. We identify and introduce\nexisting, publicly available, benchmark datasets and software resources that\nfuse color and depth data for MHT. Finally, we present a brief comparative\nevaluation of the performance of those works that have applied their methods to\nthese datasets. \n\n"}
{"id": "1606.05158", "contents": "Title: CLEAR: Covariant LEAst-square Re-fitting with applications to image\n  restoration Abstract: In this paper, we propose a new framework to remove parts of the systematic\nerrors affecting popular restoration algorithms, with a special focus for image\nprocessing tasks. Generalizing ideas that emerged for $\\ell_1$ regularization,\nwe develop an approach re-fitting the results of standard methods towards the\ninput data. Total variation regularizations and non-local means are special\ncases of interest. We identify important covariant information that should be\npreserved by the re-fitting method, and emphasize the importance of preserving\nthe Jacobian (w.r.t. the observed signal) of the original estimator. Then, we\nprovide an approach that has a \"twicing\" flavor and allows re-fitting the\nrestored signal by adding back a local affine transformation of the residual\nterm. We illustrate the benefits of our method on numerical simulations for\nimage restoration tasks. \n\n"}
{"id": "1606.05426", "contents": "Title: DecomposeMe: Simplifying ConvNets for End-to-End Learning Abstract: Deep learning and convolutional neural networks (ConvNets) have been\nsuccessfully applied to most relevant tasks in the computer vision community.\nHowever, these networks are computationally demanding and not suitable for\nembedded devices where memory and time consumption are relevant.\n  In this paper, we propose DecomposeMe, a simple but effective technique to\nlearn features using 1D convolutions. The proposed architecture enables both\nsimplicity and filter sharing leading to increased learning capacity. A\ncomprehensive set of large-scale experiments on ImageNet and Places2\ndemonstrates the ability of our method to improve performance while\nsignificantly reducing the number of parameters required. Notably, on Places2,\nwe obtain an improvement in relative top-1 classification accuracy of 7.7\\%\nwith an architecture that requires 92% fewer parameters compared to VGG-B. The\nproposed network is also demonstrated to generalize to other tasks by\nconverting existing networks. \n\n"}
{"id": "1606.05589", "contents": "Title: Human Attention in Visual Question Answering: Do Humans and Deep\n  Networks Look at the Same Regions? Abstract: We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans. \n\n"}
{"id": "1606.06308", "contents": "Title: Noise and dissipation in rigid body motion Abstract: Using the rigid body as an example, we illustrate some features of stochastic\ngeometric mechanics. These features include: i) a geometric variational\nmotivation for the noise structure involving Lie-Poisson brackets and momentum\nmaps, ii) stochastic coadjoint motion with double bracket dissipation, iii) the\nLie-Poisson Fokker-Planck description and its stationary solutions, iv) random\ndynamical systems, random attractors and SRB measures connected to statistical\nphysics. \n\n"}
{"id": "1606.06724", "contents": "Title: Tagger: Deep Unsupervised Perceptual Grouping Abstract: We present a framework for efficient perceptual inference that explicitly\nreasons about the segmentation of its inputs and features. Rather than being\ntrained for any specific segmentation, our framework learns the grouping\nprocess in an unsupervised manner or alongside any supervised task. By\nenriching the representations of a neural network, we enable it to group the\nrepresentations of different objects in an iterative manner. By allowing the\nsystem to amortize the iterative inference of the groupings, we achieve very\nfast convergence. In contrast to many other recently proposed methods for\naddressing multi-object scenes, our system does not assume the inputs to be\nimages and can therefore directly handle other modalities. For multi-digit\nclassification of very cluttered images that require texture segmentation, our\nmethod offers improved classification performance over convolutional networks\ndespite being fully connected. Furthermore, we observe that our system greatly\nimproves on the semi-supervised result of a baseline Ladder network on our\ndataset, indicating that segmentation can also improve sample efficiency. \n\n"}
{"id": "1606.07415", "contents": "Title: Find your Way by Observing the Sun and Other Semantic Cues Abstract: In this paper we present a robust, efficient and affordable approach to\nself-localization which does not require neither GPS nor knowledge about the\nappearance of the world. Towards this goal, we utilize freely available\ncartographic maps and derive a probabilistic model that exploits semantic cues\nin the form of sun direction, presence of an intersection, road type, speed\nlimit as well as the ego-car trajectory in order to produce very reliable\nlocalization results. Our experimental evaluation shows that our approach can\nlocalize much faster (in terms of driving time) with less computation and more\nrobustly than competing approaches, which ignore semantic information. \n\n"}
{"id": "1606.07575", "contents": "Title: Multipartite Ranking-Selection of Low-Dimensional Instances by\n  Supervised Projection to High-Dimensional Space Abstract: Pruning of redundant or irrelevant instances of data is a key to every\nsuccessful solution for pattern recognition. In this paper, we present a novel\nranking-selection framework for low-length but highly correlated instances.\nInstead of working in the low-dimensional instance space, we learn a supervised\nprojection to high-dimensional space spanned by the number of classes in the\ndataset under study. Imposing higher distinctions via exposing the notion of\nlabels to the instances, lets to deploy one versus all ranking for each\nindividual classes and selecting quality instances via adaptive thresholding of\nthe overall scores. To prove the efficiency of our paradigm, we employ it for\nthe purpose of texture understanding which is a hard recognition challenge due\nto high similarity of texture pixels and low dimensionality of their color\nfeatures. Our experiments show considerable improvements in recognition\nperformance over other local descriptors on several publicly available\ndatasets. \n\n"}
{"id": "1607.00654", "contents": "Title: The quest for the ultimate anisotropic Banach space Abstract: We present a new scale $U^{t,s}_p$ (with $s<-t<0$ and $1 \\le p <\\infty$) of\nanisotropic Banach spaces, defined via Paley-Littlewood, on which the transfer\noperator associated to a hyperbolic dynamical system has good spectral\nproperties. When $p=1$ and $t$ is an integer, the spaces are analogous to the\n\"geometric\" spaces considered by Gou\\\"ezel and Liverani. When $p>1$ and\n$-1+1/p<s<-t<0<t<1/p$, the spaces are somewhat analogous to the geometric\nspaces considered by Demers and Liverani. In addition, just like for the\n\"microlocal\" spaces defined by Baladi-Tsujii, the spaces $U^{t,s}_p$ are\namenable to the kneading approach of Milnor-Thurson to study dynamical\ndeterminants and zeta functions.\n  In v2, following referees' reports, typos have been corrected (in particular\n(39) and (43)). Section 4 now includes a formal statement (Theorem 4.1) about\nthe essential spectral radius if $d_s=1$ (its proof includes the content of\nSection 4.2 from v1). The Lasota-Yorke Lemma 4.2 (Lemma 4.1 in v1) includes the\nclaim that $\\cal M_b$ is compact.\n  Version v3 contains an additional text \"Corrections and complements\" showing\nthat s> t-(r-1) is needed in Section 4. \n\n"}
{"id": "1607.01577", "contents": "Title: CUNet: A Compact Unsupervised Network for Image Classification Abstract: In this paper, we propose a compact network called CUNet (compact\nunsupervised network) to counter the image classification challenge. Different\nfrom the traditional convolutional neural networks learning filters by the\ntime-consuming stochastic gradient descent, CUNet learns the filter bank from\ndiverse image patches with the simple K-means, which significantly avoids the\nrequirement of scarce labeled training images, reduces the training\nconsumption, and maintains the high discriminative ability. Besides, we propose\na new pooling method named weighted pooling considering the different weight\nvalues of adjacent neurons, which helps to improve the robustness to small\nimage distortions. In the output layer, CUNet integrates the feature maps\ngained in the last hidden layer, and straightforwardly computes histograms in\nnon-overlapped blocks. To reduce feature redundancy, we implement the\nmax-pooling operation on adjacent blocks to select the most competitive\nfeatures. Comprehensive experiments are conducted to demonstrate the\nstate-of-the-art classification performances with CUNet on CIFAR-10, STL-10,\nMNIST and Caltech101 benchmark datasets. \n\n"}
{"id": "1607.03961", "contents": "Title: Deleting and Testing Forbidden Patterns in Multi-Dimensional Arrays Abstract: Understanding the local behaviour of structured multi-dimensional data is a\nfundamental problem in various areas of computer science. As the amount of data\nis often huge, it is desirable to obtain sublinear time algorithms, and\nspecifically property testers, to understand local properties of the data.\n  We focus on the natural local problem of testing pattern freeness: given a\nlarge $d$-dimensional array $A$ and a fixed $d$-dimensional pattern $P$ over a\nfinite alphabet, we say that $A$ is $P$-free if it does not contain a copy of\nthe forbidden pattern $P$ as a consecutive subarray. The distance of $A$ to\n$P$-freeness is the fraction of entries of $A$ that need to be modified to make\nit $P$-free. For any $\\epsilon \\in [0,1]$ and any large enough pattern $P$ over\nany alphabet, other than a very small set of exceptional patterns, we design a\ntolerant tester that distinguishes between the case that the distance is at\nleast $\\epsilon$ and the case that it is at most $a_d \\epsilon$, with query\ncomplexity and running time $c_d \\epsilon^{-1}$, where $a_d < 1$ and $c_d$\ndepend only on $d$.\n  To analyze the testers we establish several combinatorial results, including\nthe following $d$-dimensional modification lemma, which might be of independent\ninterest: for any large enough pattern $P$ over any alphabet (excluding a small\nset of exceptional patterns for the binary case), and any array $A$ containing\na copy of $P$, one can delete this copy by modifying one of its locations\nwithout creating new $P$-copies in $A$.\n  Our results address an open question of Fischer and Newman, who asked whether\nthere exist efficient testers for properties related to tight substructures in\nmulti-dimensional structured data. They serve as a first step towards a general\nunderstanding of local properties of multi-dimensional arrays, as any such\nproperty can be characterized by a fixed family of forbidden patterns. \n\n"}
{"id": "1607.05074", "contents": "Title: Deep Active Contours Abstract: We propose a method for interactive boundary extraction which combines a\ndeep, patch-based representation with an active contour framework. We train a\nclass-specific convolutional neural network which predicts a vector pointing\nfrom the respective point on the evolving contour towards the closest point on\nthe boundary of the object of interest. These predictions form a vector field\nwhich is then used for evolving the contour by the Sobolev active contour\nframework proposed by Sundaramoorthi et al. The resulting interactive\nsegmentation method is very efficient in terms of required computational\nresources and can even be trained on comparatively small graphics cards. We\nevaluate the potential of the proposed method on both medical and non-medical\nchallenge data sets, such as the STACOM data set and the PASCAL VOC 2012 data\nset. \n\n"}
{"id": "1607.07220", "contents": "Title: Local- and Holistic- Structure Preserving Image Super Resolution via\n  Deep Joint Component Learning Abstract: Recently, machine learning based single image super resolution (SR)\napproaches focus on jointly learning representations for high-resolution (HR)\nand low-resolution (LR) image patch pairs to improve the quality of the\nsuper-resolved images. However, due to treat all image pixels equally without\nconsidering the salient structures, these approaches usually fail to produce\nvisual pleasant images with sharp edges and fine details. To address this\nissue, in this work we present a new novel SR approach, which replaces the main\nbuilding blocks of the classical interpolation pipeline by a flexible,\ncontent-adaptive deep neural networks. In particular, two well-designed\nstructure-aware components, respectively capturing local- and holistic- image\ncontents, are naturally incorporated into the fully-convolutional\nrepresentation learning to enhance the image sharpness and naturalness.\nExtensively evaluations on several standard benchmarks (e.g., Set5, Set14 and\nBSD200) demonstrate that our approach can achieve superior results, especially\non the image with salient structures, over many existing state-of-the-art SR\nmethods under both quantitative and qualitative measures. \n\n"}
{"id": "1607.07646", "contents": "Title: Emotion-Based Crowd Representation for Abnormality Detection Abstract: In crowd behavior understanding, a model of crowd behavior need to be trained\nusing the information extracted from video sequences. Since there is no\nground-truth available in crowd datasets except the crowd behavior labels, most\nof the methods proposed so far are just based on low-level visual features.\nHowever, there is a huge semantic gap between low-level motion/appearance\nfeatures and high-level concept of crowd behaviors. In this paper we propose an\nattribute-based strategy to alleviate this problem. While similar strategies\nhave been recently adopted for object and action recognition, as far as we\nknow, we are the first showing that the crowd emotions can be used as\nattributes for crowd behavior understanding. The main idea is to train a set of\nemotion-based classifiers, which can subsequently be used to represent the\ncrowd motion. For this purpose, we collect a big dataset of video clips and\nprovide them with both annotations of \"crowd behaviors\" and \"crowd emotions\".\nWe show the results of the proposed method on our dataset, which demonstrate\nthat the crowd emotions enable the construction of more descriptive models for\ncrowd behaviors. We aim at publishing the dataset with the article, to be used\nas a benchmark for the communities. \n\n"}
{"id": "1607.08481", "contents": "Title: A Nonlocal Denoising Algorithm for Manifold-Valued Images Using Second\n  Order Statistics Abstract: Nonlocal patch-based methods, in particular the Bayes' approach of Lebrun,\nBuades and Morel (2013), are considered as state-of-the-art methods for\ndenoising (color) images corrupted by white Gaussian noise of moderate\nvariance. This paper is the first attempt to generalize this technique to\nmanifold-valued images. Such images, for example images with phase or\ndirectional entries or with values in the manifold of symmetric positive\ndefinite matrices, are frequently encountered in real-world applications.\nGeneralizing the normal law to manifolds is not canonical and different\nattempts have been considered. Here we focus on a straightforward intrinsic\nmodel and discuss the relation to other approaches for specific manifolds. We\nreinterpret the Bayesian approach of Lebrun et al. (2013) in terms of minimum\nmean squared error estimation, which motivates our definition of a\ncorresponding estimator on the manifold. With this estimator at hand we present\na nonlocal patch-based method for the restoration of manifold-valued images.\nVarious proof of concept examples demonstrate the potential of the proposed\nalgorithm. \n\n"}
{"id": "1608.00797", "contents": "Title: CUHK & ETHZ & SIAT Submission to ActivityNet Challenge 2016 Abstract: This paper presents the method that underlies our submission to the untrimmed\nvideo classification task of ActivityNet Challenge 2016. We follow the basic\npipeline of temporal segment networks and further raise the performance via a\nnumber of other techniques. Specifically, we use the latest deep model\narchitecture, e.g., ResNet and Inception V3, and introduce new aggregation\nschemes (top-k and attention-weighted pooling). Additionally, we incorporate\nthe audio as a complementary channel, extracting relevant information via a CNN\napplied to the spectrograms. With these techniques, we derive an ensemble of\ndeep models, which, together, attains a high classification accuracy (mAP\n$93.23\\%$) on the testing set and secured the first place in the challenge. \n\n"}
{"id": "1608.00905", "contents": "Title: PicHunt: Social Media Image Retrieval for Improved Law Enforcement Abstract: First responders are increasingly using social media to identify and reduce\ncrime for well-being and safety of the society. Images shared on social media\nhurting religious, political, communal and other sentiments of people, often\ninstigate violence and create law & order situations in society. This results\nin the need for first responders to inspect the spread of such images and users\npropagating them on social media. In this paper, we present a comparison\nbetween different hand-crafted features and a Convolutional Neural Network\n(CNN) model to retrieve similar images, which outperforms state-of-art\nhand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time\nimage search system, robust to retrieve modified images that allows first\nresponders to analyze the current spread of images, sentiments floating and\ndetails of users propagating such content. The system also aids officials to\nsave time of manually analyzing the content by reducing the search space on an\naverage by 67%. \n\n"}
{"id": "1608.01431", "contents": "Title: An efficient iterative thresholding method for image segmentation Abstract: We proposed an efficient iterative thresholding method for multi-phase image\nsegmentation. The algorithm is based on minimizing piecewise constant\nMumford-Shah functional in which the contour length (or perimeter) is\napproximated by a non-local multi-phase energy. The minimization problem is\nsolved by an iterative method. Each iteration consists of computing simple\nconvolutions followed by a thresholding step. The algorithm is easy to\nimplement and has the optimal complexity $O(N \\log N)$ per iteration. We also\nshow that the iterative algorithm has the total energy decaying property. We\npresent some numerical results to show the efficiency of our method. \n\n"}
{"id": "1608.03914", "contents": "Title: When was that made? Abstract: In this paper, we explore deep learning methods for estimating when objects\nwere made. Automatic methods for this task could potentially be useful for\nhistorians, collectors, or any individual interested in estimating when their\nartifact was created. Direct applications include large-scale data organization\nor retrieval. Toward this goal, we utilize features from existing deep networks\nand also fine-tune new networks for temporal estimation. In addition, we create\ntwo new datasets of 67,771 dated clothing items from Flickr and museum\ncollections. Our method outperforms both a color-based baseline and previous\nstate of the art methods for temporal estimation. We also provide several\nanalyses of what our networks have learned, and demonstrate applications to\nidentifying temporal inspiration in fashion collections. \n\n"}
{"id": "1608.04546", "contents": "Title: Classical and Quantum Superintegrability of St\\\"ackel Systems Abstract: In this paper we discuss maximal superintegrability of both classical and\nquantum St\\\"ackel systems. We prove a sufficient condition for a flat or\nconstant curvature St\\\"ackel system to be maximally superintegrable. Further,\nwe prove a sufficient condition for a St\\\"ackel transform to preserve maximal\nsuperintegrability and we apply this condition to our class of St\\\"ackel\nsystems, which yields new maximally superintegrable systems as conformal\ndeformations of the original systems. Further, we demonstrate how to perform\nthe procedure of minimal quantization to considered systems in order to produce\nquantum superintegrable and quantum separable systems. \n\n"}
{"id": "1608.05203", "contents": "Title: Seeing with Humans: Gaze-Assisted Neural Image Captioning Abstract: Gaze reflects how humans process visual scenes and is therefore increasingly\nused in computer vision systems. Previous works demonstrated the potential of\ngaze for object-centric tasks, such as object localization and recognition, but\nit remains unclear if gaze can also be beneficial for scene-centric tasks, such\nas image captioning. We present a new perspective on gaze-assisted image\ncaptioning by studying the interplay between human gaze and the attention\nmechanism of deep neural networks. Using a public large-scale gaze dataset, we\nfirst assess the relationship between state-of-the-art object and scene\nrecognition models, bottom-up visual saliency, and human gaze. We then propose\na novel split attention model for image captioning. Our model integrates human\ngaze information into an attention-based long short-term memory architecture,\nand allows the algorithm to allocate attention selectively to both fixated and\nnon-fixated image regions. Through evaluation on the COCO/SALICON datasets we\nshow that our method improves image captioning performance and that gaze can\ncomplement machine attention for semantic scene understanding tasks. \n\n"}
{"id": "1608.05306", "contents": "Title: Chaos as an Intermittently Forced Linear System Abstract: Understanding the interplay of order and disorder in chaotic systems is a\ncentral challenge in modern quantitative science. We present a universal,\ndata-driven decomposition of chaos as an intermittently forced linear system.\nThis work combines Takens' delay embedding with modern Koopman operator theory\nand sparse regression to obtain linear representations of strongly nonlinear\ndynamics. The result is a decomposition of chaotic dynamics into a linear model\nin the leading delay coordinates with forcing by low energy delay coordinates;\nwe call this the Hankel alternative view of Koopman (HAVOK) analysis. This\nanalysis is applied to the canonical Lorenz system, as well as to real-world\nexamples such as the Earth's magnetic field reversal, and data from\nelectrocardiogram, electroencephalogram, and measles outbreaks. In each case,\nthe forcing statistics are non-Gaussian, with long tails corresponding to rare\nevents that trigger intermittent switching and bursting phenomena; this forcing\nis highly predictive, providing a clear signature that precedes these events.\nMoreover, the activity of the forcing signal demarcates large coherent regions\nof phase space where the dynamics are approximately linear from those that are\nstrongly nonlinear. \n\n"}
{"id": "1608.05930", "contents": "Title: FPGA Design for Pseudorandom Number Generator Based on Chaotic Iteration\n  used in Information Hiding Application Abstract: Lots of researches indicate that the inefficient generation of random numbers\nis a significant bottleneck for information communication applications.\nTherefore, Field Programmable Gate Array (FPGA) is developed to process a\nscalable fixed-point method for random streams generation. In our previous\nresearches, we have proposed a technique by applying some well-defined discrete\nchaotic iterations that satisfy the reputed Devaney's definition of chaos,\nnamely chaotic iterations (CI). We have formerly proven that the generator with\nCI can provide qualified chaotic random numbers. In this paper, this generator\nbased on chaotic iterations is optimally redesigned for FPGA device. By doing\nso, the generation rate can be largely improved. Analyses show that these\nhardware generators can also provide good statistical chaotic random bits and\ncan be cryptographically secure too. An application in the information hiding\nsecurity field is finally given as an illustrative example. \n\n"}
{"id": "1608.06884", "contents": "Title: Towards Bayesian Deep Learning: A Framework and Some Existing Methods Abstract: While perception tasks such as visual object recognition and text\nunderstanding play an important role in human intelligence, the subsequent\ntasks that involve inference, reasoning and planning require an even higher\nlevel of intelligence. The past few years have seen major advances in many\nperception tasks using deep learning models. For higher-level inference,\nhowever, probabilistic graphical models with their Bayesian nature are still\nmore powerful and flexible. To achieve integrated intelligence that involves\nboth perception and inference, it is naturally desirable to tightly integrate\ndeep learning and Bayesian models within a principled probabilistic framework,\nwhich we call Bayesian deep learning. In this unified framework, the perception\nof text or images using deep learning can boost the performance of higher-level\ninference and in return, the feedback from the inference process is able to\nenhance the perception of text or images. This paper proposes a general\nframework for Bayesian deep learning and reviews its recent applications on\nrecommender systems, topic models, and control. In this paper, we also discuss\nthe relationship and differences between Bayesian deep learning and other\nrelated topics like Bayesian treatment of neural networks. \n\n"}
{"id": "1608.07017", "contents": "Title: Ambient Sound Provides Supervision for Visual Learning Abstract: The sound of crashing waves, the roar of fast-moving cars -- sound conveys\nimportant information about the objects in our surroundings. In this work, we\nshow that ambient sounds can be used as a supervisory signal for learning\nvisual models. To demonstrate this, we train a convolutional neural network to\npredict a statistical summary of the sound associated with a video frame. We\nshow that, through this process, the network learns a representation that\nconveys information about objects and scenes. We evaluate this representation\non several recognition tasks, finding that its performance is comparable to\nthat of other state-of-the-art unsupervised learning methods. Finally, we show\nthrough visualizations that the network learns units that are selective to\nobjects that are often associated with characteristic sounds. \n\n"}
{"id": "1608.07639", "contents": "Title: Learning to generalize to new compositions in image understanding Abstract: Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure. \n\n"}
{"id": "1608.07916", "contents": "Title: Vehicle Detection from 3D Lidar Using Fully Convolutional Network Abstract: Convolutional network techniques have recently achieved great success in\nvision based detection tasks. This paper introduces the recent development of\nour research on transplanting the fully convolutional network technique to the\ndetection tasks on 3D range scan data. Specifically, the scenario is set as the\nvehicle detection task from the range data of Velodyne 64E lidar. We proposes\nto present the data in a 2D point map and use a single 2D end-to-end fully\nconvolutional network to predict the objectness confidence and the bounding\nboxes simultaneously. By carefully design the bounding box encoding, it is able\nto predict full 3D bounding boxes even using a 2D convolutional network.\nExperiments on the KITTI dataset shows the state-of-the-art performance of the\nproposed method. \n\n"}
{"id": "1609.00129", "contents": "Title: Grid Loss: Detecting Occluded Faces Abstract: Detection of partially occluded objects is a challenging computer vision\nproblem. Standard Convolutional Neural Network (CNN) detectors fail if parts of\nthe detection window are occluded, since not every sub-part of the window is\ndiscriminative on its own. To address this issue, we propose a novel loss layer\nfor CNNs, named grid loss, which minimizes the error rate on sub-blocks of a\nconvolution layer independently rather than over the whole feature map. This\nresults in parts being more discriminative on their own, enabling the detector\nto recover if the detection window is partially occluded. By mapping our loss\nlayer back to a regular fully connected layer, no additional computational cost\nis incurred at runtime compared to standard CNNs. We demonstrate our method for\nface detection on several public face detection benchmarks and show that our\nmethod outperforms regular CNNs, is suitable for realtime applications and\nachieves state-of-the-art performance. \n\n"}
{"id": "1609.00463", "contents": "Title: Stochastic Discrete Hamiltonian Variational Integrators Abstract: Variational integrators are derived for structure-preserving simulation of\nstochastic Hamiltonian systems with a certain type of multiplicative noise\narising in geometric mechanics. The derivation is based on a stochastic\ndiscrete Hamiltonian which approximates a type-II stochastic generating\nfunction for the stochastic flow of the Hamiltonian system. The generating\nfunction is obtained by introducing an appropriate stochastic action functional\nand its corresponding variational principle. Our approach permits to recast in\na unified framework a number of integrators previously studied in the\nliterature, and presents a general methodology to derive new\nstructure-preserving numerical schemes. The resulting integrators are\nsymplectic; they preserve integrals of motion related to Lie group symmetries;\nand they include stochastic symplectic Runge-Kutta methods as a special case.\nSeveral new low-stage stochastic symplectic methods of mean-square order 1.0\nderived using this approach are presented and tested numerically to demonstrate\ntheir superior long-time numerical stability and energy behavior compared to\nnonsymplectic methods. \n\n"}
{"id": "1609.04116", "contents": "Title: Joint Gender Classification and Age Estimation by Nearly Orthogonalizing\n  Their Semantic Spaces Abstract: In human face-based biometrics, gender classification and age estimation are\ntwo typical learning tasks. Although a variety of approaches have been proposed\nto handle them, just a few of them are solved jointly, even so, these joint\nmethods do not yet specifically concern the semantic difference between human\ngender and age, which is intuitively helpful for joint learning, consequently\nleaving us a room of further improving the performance. To this end, in this\nwork we firstly propose a general learning framework for jointly estimating\nhuman gender and age by specially attempting to formulate such semantic\nrelationships as a form of near-orthogonality regularization and then\nincorporate it into the objective of the joint learning framework. In order to\nevaluate the effectiveness of the proposed framework, we exemplify it by\nrespectively taking the widely used binary-class SVM for gender classification,\nand two threshold-based ordinal regression methods (i.e., the discriminant\nlearning for ordinal regression and support vector ordinal regression) for age\nestimation, and crucially coupling both through the proposed semantic\nformulation. Moreover, we develop its kernelized nonlinear counterpart by\nderiving a representer theorem for the joint learning strategy. Finally,\nthrough extensive experiments on three aging datasets FG-NET, Morph Album I and\nMorph Album II, we demonstrate the effectiveness and superiority of the\nproposed joint learning strategy. \n\n"}
{"id": "1609.04541", "contents": "Title: Matrix Product State for Higher-Order Tensor Compression and\n  Classification Abstract: This paper introduces matrix product state (MPS) decomposition as a new and\nsystematic method to compress multidimensional data represented by higher-order\ntensors. It solves two major bottlenecks in tensor compression: computation and\ncompression quality. Regardless of tensor order, MPS compresses tensors to\nmatrices of moderate dimension which can be used for classification. Mainly\nbased on a successive sequence of singular value decompositions (SVD), MPS is\nquite simple to implement and arrives at the global optimal matrix, bypassing\nlocal alternating optimization, which is not only computationally expensive but\ncannot yield the global solution. Benchmark results show that MPS can achieve\nbetter classification performance with favorable computation cost compared to\nother tensor compression methods. \n\n"}
{"id": "1609.06753", "contents": "Title: How should we evaluate supervised hashing? Abstract: Hashing produces compact representations for documents, to perform tasks like\nclassification or retrieval based on these short codes. When hashing is\nsupervised, the codes are trained using labels on the training data. This paper\nfirst shows that the evaluation protocols used in the literature for supervised\nhashing are not satisfactory: we show that a trivial solution that encodes the\noutput of a classifier significantly outperforms existing supervised or\nsemi-supervised methods, while using much shorter codes. We then propose two\nalternative protocols for supervised hashing: one based on retrieval on a\ndisjoint set of classes, and another based on transfer learning to new classes.\nWe provide two baseline methods for image-related tasks to assess the\nperformance of (semi-)supervised hashing: without coding and with unsupervised\ncodes. These baselines give a lower- and upper-bound on the performance of a\nsupervised hashing scheme. \n\n"}
{"id": "1609.07859", "contents": "Title: Visual Fashion-Product Search at SK Planet Abstract: We build a large-scale visual search system which finds similar product\nimages given a fashion item. Defining similarity among arbitrary\nfashion-products is still remains a challenging problem, even there is no exact\nground-truth. To resolve this problem, we define more than 90 fashion-related\nattributes, and combination of these attributes can represent thousands of\nunique fashion-styles. The fashion-attributes are one of the ingredients to\ndefine semantic similarity among fashion-product images. To build our system at\nscale, these fashion-attributes are again used to build an inverted indexing\nscheme. In addition to these fashion-attributes for semantic similarity, we\nextract colour and appearance features in a region-of-interest (ROI) of a\nfashion item for visual similarity. By sharing our approach, we expect active\ndiscussion on that how to apply current computer vision research into the\ne-commerce industry. \n\n"}
{"id": "1609.07916", "contents": "Title: Deep Structured Features for Semantic Segmentation Abstract: We propose a highly structured neural network architecture for semantic\nsegmentation with an extremely small model size, suitable for low-power\nembedded and mobile platforms. Specifically, our architecture combines i) a\nHaar wavelet-based tree-like convolutional neural network (CNN), ii) a random\nlayer realizing a radial basis function kernel approximation, and iii) a linear\nclassifier. While stages i) and ii) are completely pre-specified, only the\nlinear classifier is learned from data. We apply the proposed architecture to\noutdoor scene and aerial image semantic segmentation and show that the accuracy\nof our architecture is competitive with conventional pixel classification CNNs.\nFurthermore, we demonstrate that the proposed architecture is data efficient in\nthe sense of matching the accuracy of pixel classification CNNs when trained on\na much smaller data set. \n\n"}
{"id": "1609.08685", "contents": "Title: Understanding and Exploiting Object Interaction Landscapes Abstract: Interactions play a key role in understanding objects and scenes, for both\nvirtual and real world agents. We introduce a new general representation for\nproximal interactions among physical objects that is agnostic to the type of\nobjects or interaction involved. The representation is based on tracking\nparticles on one of the participating objects and then observing them with\nsensors appropriately placed in the interaction volume or on the interaction\nsurfaces. We show how to factorize these interaction descriptors and project\nthem into a particular participating object so as to obtain a new functional\ndescriptor for that object, its interaction landscape, capturing its observed\nuse in a spatio-temporal framework. Interaction landscapes are independent of\nthe particular interaction and capture subtle dynamic effects in how objects\nmove and behave when in functional use. Our method relates objects based on\ntheir function, establishes correspondences between shapes based on functional\nkey points and regions, and retrieves peer and partner objects with respect to\nan interaction. \n\n"}
{"id": "1609.08740", "contents": "Title: Scalable Discrete Supervised Hash Learning with Asymmetric Matrix\n  Factorization Abstract: Hashing method maps similar data to binary hashcodes with smaller hamming\ndistance, and it has received a broad attention due to its low storage cost and\nfast retrieval speed. However, the existing limitations make the present\nalgorithms difficult to deal with large-scale datasets: (1) discrete\nconstraints are involved in the learning of the hash function; (2) pairwise or\ntriplet similarity is adopted to generate efficient hashcodes, resulting both\ntime and space complexity are greater than O(n^2). To address these issues, we\npropose a novel discrete supervised hash learning framework which can be\nscalable to large-scale datasets. First, the discrete learning procedure is\ndecomposed into a binary classifier learning scheme and binary codes learning\nscheme, which makes the learning procedure more efficient. Second, we adopt the\nAsymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based\nBatch Coordinate Descent method, such that the time and space complexity is\nreduced to O(n). The proposed framework also provides a flexible paradigm to\nincorporate with arbitrary hash function, including deep neural networks and\nkernel methods. Experiments on large-scale datasets demonstrate that the\nproposed method is superior or comparable with state-of-the-art hashing\nalgorithms. \n\n"}
{"id": "1610.02947", "contents": "Title: End-to-end Concept Word Detection for Video Captioning, Retrieval, and\n  Question Answering Abstract: We propose a high-level concept word detector that can be integrated with any\nvideo-to-language models. It takes a video as input and generates a list of\nconcept words as useful semantic priors for language generation models. The\nproposed word detector has two important properties. First, it does not require\nany external knowledge sources for training. Second, the proposed word detector\nis trainable in an end-to-end manner jointly with any video-to-language models.\nTo maximize the values of detected words, we also develop a semantic attention\nmechanism that selectively focuses on the detected concept words and fuse them\nwith the word encoding and decoding in the language model. In order to\ndemonstrate that the proposed approach indeed improves the performance of\nmultiple video-to-language tasks, we participate in four tasks of LSMDC 2016.\nOur approach achieves the best accuracies in three of them, including\nfill-in-the-blank, multiple-choice test, and movie retrieval. We also attain\ncomparable performance for the other task, movie description. \n\n"}
{"id": "1610.04460", "contents": "Title: On the Existence of a Sample Mean in Dynamic Time Warping Spaces Abstract: The concept of sample mean in dynamic time warping (DTW) spaces has been\nsuccessfully applied to improve pattern recognition systems and generalize\ncentroid-based clustering algorithms. Its existence has neither been proved nor\nchallenged. This article presents sufficient conditions for existence of a\nsample mean in DTW spaces. The proposed result justifies prior work on\napproximate mean algorithms, sets the stage for constructing exact mean\nalgorithms, and is a first step towards a statistical theory of DTW spaces. \n\n"}
{"id": "1610.04631", "contents": "Title: A Harmonic Mean Linear Discriminant Analysis for Robust Image\n  Classification Abstract: Linear Discriminant Analysis (LDA) is a widely-used supervised dimensionality\nreduction method in computer vision and pattern recognition. In null space\nbased LDA (NLDA), a well-known LDA extension, between-class distance is\nmaximized in the null space of the within-class scatter matrix. However, there\nare some limitations in NLDA. Firstly, for many data sets, null space of\nwithin-class scatter matrix does not exist, thus NLDA is not applicable to\nthose datasets. Secondly, NLDA uses arithmetic mean of between-class distances\nand gives equal consideration to all between-class distances, which makes\nlarger between-class distances can dominate the result and thus limits the\nperformance of NLDA. In this paper, we propose a harmonic mean based Linear\nDiscriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image\nclassification, which minimizes the reciprocal of weighted harmonic mean of\npairwise between-class distance. More importantly, MCDA gives higher priority\nto maximize small between-class distances. MCDA can be extended to multi-label\ndimension reduction. Results on 7 single-label data sets and 4 multi-label data\nsets show that MCDA has consistently better performance than 10 other\nsingle-label approaches and 4 other multi-label approaches in terms of\nclassification accuracy, macro and micro average F1 score. \n\n"}
{"id": "1610.05036", "contents": "Title: Encoding the Local Connectivity Patterns of fMRI for Cognitive State\n  Classification Abstract: In this work, we propose a novel framework to encode the local connectivity\npatterns of brain, using Fisher Vectors (FV), Vector of Locally Aggregated\nDescriptors (VLAD) and Bag-of-Words (BoW) methods. We first obtain local\ndescriptors, called Mesh Arc Descriptors (MADs) from fMRI data, by forming\nlocal meshes around anatomical regions, and estimating their relationship\nwithin a neighborhood. Then, we extract a dictionary of relationships, called\n\\textit{brain connectivity dictionary} by fitting a generative Gaussian mixture\nmodel (GMM) to a set of MADs, and selecting the codewords at the mean of each\ncomponent of the mixture. Codewords represent the connectivity patterns among\nanatomical regions. We also encode MADs by VLAD and BoW methods using the\nk-Means clustering.\n  We classify the cognitive states of Human Connectome Project (HCP) task fMRI\ndataset, where we train support vector machines (SVM) by the encoded MADs.\nResults demonstrate that, FV encoding of MADs can be successfully employed for\nclassification of cognitive tasks, and outperform the VLAD and BoW\nrepresentations. Moreover, we identify the significant Gaussians in mixture\nmodels by computing energy of their corresponding FV parts, and analyze their\neffect on classification accuracy. Finally, we suggest a new method to\nvisualize the codewords of brain connectivity dictionary. \n\n"}
{"id": "1610.05586", "contents": "Title: Deep Identity-aware Transfer of Facial Attributes Abstract: This paper presents a Deep convolutional network model for Identity-Aware\nTransfer (DIAT) of facial attributes. Given the source input image and the\nreference attribute, DIAT aims to generate a facial image that owns the\nreference attribute as well as keeps the same or similar identity to the input\nimage. In general, our model consists of a mask network and an attribute\ntransform network which work in synergy to generate a photo-realistic facial\nimage with the reference attribute. Considering that the reference attribute\nmay be only related to some parts of the image, the mask network is introduced\nto avoid the incorrect editing on attribute irrelevant region. Then the\nestimated mask is adopted to combine the input and transformed image for\nproducing the transfer result. For joint training of transform network and mask\nnetwork, we incorporate the adversarial attribute loss, identity-aware adaptive\nperceptual loss, and VGG-FACE based identity loss. Furthermore, a denoising\nnetwork is presented to serve for perceptual regularization to suppress the\nartifacts in transfer result, while an attribute ratio regularization is\nintroduced to constrain the size of attribute relevant region. Our DIAT can\nprovide a unified solution for several representative facial attribute transfer\ntasks, e.g., expression transfer, accessory removal, age progression, and\ngender transfer, and can be extended for other face enhancement tasks such as\nface hallucination. The experimental results validate the effectiveness of the\nproposed method. Even for the identity-related attribute (e.g., gender), our\nDIAT can obtain visually impressive results by changing the attribute while\nretaining most identity-aware features. \n\n"}
{"id": "1610.06626", "contents": "Title: Chimera states in a network-organized public goods game with destructive\n  agents Abstract: We found that a network-organized metapopulation of cooperators, defectors\nand destructive agents playing the public goods game with mutations, can\ncollectively reach global synchronization or chimera states. Global\nsynchronization is accompanied by a collective periodic burst of cooperation,\nwhereas chimera states reflect the tendency of the networked metapopulation to\nbe fragmented in clusters of synchronous and incoherent bursts of cooperation.\nNumerical simulations have shown that the system's dynamics alternates between\nthese two steady states through a first order transition. Depending on the\nparameters determining the dynamical and topological properties, chimera states\nwith different numbers of coherent and incoherent clusters are observed. Our\nresults present the first systematic study of chimera states and their\ncharacterization in the context of evolutionary game theory. This provides a\nvaluable insight into the details of their occurrence, extending the relevance\nof such states to natural and social systems. \n\n"}
{"id": "1610.07008", "contents": "Title: Optimization on Submanifolds of Convolution Kernels in CNNs Abstract: Kernel normalization methods have been employed to improve robustness of\noptimization methods to reparametrization of convolution kernels, covariate\nshift, and to accelerate training of Convolutional Neural Networks (CNNs).\nHowever, our understanding of theoretical properties of these methods has\nlagged behind their success in applications. We develop a geometric framework\nto elucidate underlying mechanisms of a diverse range of kernel normalization\nmethods. Our framework enables us to expound and identify geometry of space of\nnormalized kernels. We analyze and delineate how state-of-the-art kernel\nnormalization methods affect the geometry of search spaces of the stochastic\ngradient descent (SGD) algorithms in CNNs. Following our theoretical results,\nwe propose a SGD algorithm with assurance of almost sure convergence of the\nmethods to a solution at single minimum of classification loss of CNNs.\nExperimental results show that the proposed method achieves state-of-the-art\nperformance for major image classification benchmarks with CNNs. \n\n"}
{"id": "1610.09609", "contents": "Title: Generalized Haar Filter based Deep Networks for Real-Time Object\n  Detection in Traffic Scene Abstract: Vision-based object detection is one of the fundamental functions in numerous\ntraffic scene applications such as self-driving vehicle systems and advance\ndriver assistance systems (ADAS). However, it is also a challenging task due to\nthe diversity of traffic scene and the storage, power and computing source\nlimitations of the platforms for traffic scene applications. This paper\npresents a generalized Haar filter based deep network which is suitable for the\nobject detection tasks in traffic scene. In this approach, we first decompose a\nobject detection task into several easier local regression tasks. Then, we\nhandle the local regression tasks by using several tiny deep networks which\nsimultaneously output the bounding boxes, categories and confidence scores of\ndetected objects. To reduce the consumption of storage and computing resources,\nthe weights of the deep networks are constrained to the form of generalized\nHaar filter in training phase. Additionally, we introduce the strategy of\nsparse windows generation to improve the efficiency of the algorithm. Finally,\nwe perform several experiments to validate the performance of our proposed\napproach. Experimental results demonstrate that the proposed approach is both\nefficient and effective in traffic scene compared with the state-of-the-art. \n\n"}
{"id": "1611.00471", "contents": "Title: Dual Attention Networks for Multimodal Reasoning and Matching Abstract: We propose Dual Attention Networks (DANs) which jointly leverage visual and\ntextual attention mechanisms to capture fine-grained interplay between vision\nand language. DANs attend to specific regions in images and words in text\nthrough multiple steps and gather essential information from both modalities.\nBased on this framework, we introduce two types of DANs for multimodal\nreasoning and matching, respectively. The reasoning model allows visual and\ntextual attentions to steer each other during collaborative inference, which is\nuseful for tasks such as Visual Question Answering (VQA). In addition, the\nmatching model exploits the two attention mechanisms to estimate the similarity\nbetween images and sentences by focusing on their shared semantics. Our\nextensive experiments validate the effectiveness of DANs in combining vision\nand language, achieving the state-of-the-art performance on public benchmarks\nfor VQA and image-text matching. \n\n"}
{"id": "1611.02730", "contents": "Title: Robust Cardiac Motion Estimation using Ultrafast Ultrasound Data: A\n  Low-Rank-Topology-Preserving Approach Abstract: Cardiac motion estimation is an important diagnostic tool to detect heart\ndiseases and it has been explored with modalities such as MRI and conventional\nultrasound (US) sequences. US cardiac motion estimation still presents\nchallenges because of the complex motion patterns and the presence of noise. In\nthis work, we propose a novel approach to estimate the cardiac motion using\nultrafast ultrasound data. -- Our solution is based on a variational\nformulation characterized by the L2-regularized class. The displacement is\nrepresented by a lattice of b-splines and we ensure robustness by applying a\nmaximum likelihood type estimator. While this is an important part of our\nsolution, the main highlight of this paper is to combine a low-rank data\nrepresentation with topology preservation. Low-rank data representation\n(achieved by finding the k-dominant singular values of a Casorati Matrix\narranged from the data sequence) speeds up the global solution and achieves\nnoise reduction. On the other hand, topology preservation (achieved by\nmonitoring the Jacobian determinant) allows to radically rule out distortions\nwhile carefully controlling the size of allowed expansions and contractions.\nOur variational approach is carried out on a realistic dataset as well as on a\nsimulated one. We demonstrate how our proposed variational solution deals with\ncomplex deformations through careful numerical experiments. While maintaining\nthe accuracy of the solution, the low-rank preprocessing is shown to speed up\nthe convergence of the variational problem. Beyond cardiac motion estimation,\nour approach is promising for the analysis of other organs that experience\nmotion. \n\n"}
{"id": "1611.02764", "contents": "Title: Inferring low-dimensional microstructure representations using\n  convolutional neural networks Abstract: We apply recent advances in machine learning and computer vision to a central\nproblem in materials informatics: The statistical representation of\nmicrostructural images. We use activations in a pre-trained convolutional\nneural network to provide a high-dimensional characterization of a set of\nsynthetic microstructural images. Next, we use manifold learning to obtain a\nlow-dimensional embedding of this statistical characterization. We show that\nthe low-dimensional embedding extracts the parameters used to generate the\nimages. According to a variety of metrics, the convolutional neural network\nmethod yields dramatically better embeddings than the analogous method derived\nfrom two-point correlations alone. \n\n"}
{"id": "1611.02879", "contents": "Title: Audio Visual Speech Recognition using Deep Recurrent Neural Networks Abstract: In this work, we propose a training algorithm for an audio-visual automatic\nspeech recognition (AV-ASR) system using deep recurrent neural network\n(RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal\nClassification (CTC) objective function. The frame labels obtained from the\nacoustic model are then used to perform a non-linear dimensionality reduction\nof the visual features using a deep bottleneck network. Audio and visual\nfeatures are fused and used to train a fusion RNN. The use of bottleneck\nfeatures for visual modality helps the model to converge properly during\ntraining. Our system is evaluated on GRID corpus. Our results show that\npresence of visual modality gives significant improvement in character error\nrate (CER) at various levels of noise even when the model is trained without\nnoisy data. We also provide a comparison of two fusion methods: feature fusion\nand decision fusion. \n\n"}
{"id": "1611.02886", "contents": "Title: Node-Adapt, Path-Adapt and Tree-Adapt:Model-Transfer Domain Adaptation\n  for Random Forest Abstract: Random Forest (RF) is a successful paradigm for learning classifiers due to\nits ability to learn from large feature spaces and seamlessly integrate\nmulti-class classification, as well as the achieved accuracy and processing\nefficiency. However, as many other classifiers, RF requires domain adaptation\n(DA) provided that there is a mismatch between the training (source) and\ntesting (target) domains which provokes classification degradation.\nConsequently, different RF-DA methods have been proposed, which not only\nrequire target-domain samples but revisiting the source-domain ones, too. As\nnovelty, we propose three inherently different methods (Node-Adapt, Path-Adapt\nand Tree-Adapt) that only require the learned source-domain RF and a relatively\nfew target-domain samples for DA, i.e. source-domain samples do not need to be\navailable. To assess the performance of our proposals we focus on image-based\nobject detection, using the pedestrian detection problem as challenging\nproof-of-concept. Moreover, we use the RF with expert nodes because it is a\ncompetitive patch-based pedestrian model. We test our Node-, Path- and\nTree-Adapt methods in standard benchmarks, showing that DA is largely achieved. \n\n"}
{"id": "1611.04076", "contents": "Title: Least Squares Generative Adversarial Networks Abstract: Unsupervised learning with generative adversarial networks (GANs) has proven\nhugely successful. Regular GANs hypothesize the discriminator as a classifier\nwith the sigmoid cross entropy loss function. However, we found that this loss\nfunction may lead to the vanishing gradients problem during the learning\nprocess. To overcome such a problem, we propose in this paper the Least Squares\nGenerative Adversarial Networks (LSGANs) which adopt the least squares loss\nfunction for the discriminator. We show that minimizing the objective function\nof LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two\nbenefits of LSGANs over regular GANs. First, LSGANs are able to generate higher\nquality images than regular GANs. Second, LSGANs perform more stable during the\nlearning process. We evaluate LSGANs on five scene datasets and the\nexperimental results show that the images generated by LSGANs are of better\nquality than the ones generated by regular GANs. We also conduct two comparison\nexperiments between LSGANs and regular GANs to illustrate the stability of\nLSGANs. \n\n"}
{"id": "1611.04835", "contents": "Title: Multilinear Low-Rank Tensors on Graphs & Applications Abstract: We propose a new framework for the analysis of low-rank tensors which lies at\nthe intersection of spectral graph theory and signal processing. As a first\nstep, we present a new graph based low-rank decomposition which approximates\nthe classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,\nbuilding on this novel decomposition we construct a general class of convex\noptimization problems for approximately solving low-rank tensor inverse\nproblems, such as tensor Robust PCA. The whole framework is named as\n'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis\nshows: 1) MLRTG stands on the notion of approximate stationarity of\nmulti-dimensional signals on graphs and 2) the approximation error depends on\nthe eigen gaps of the graphs. We demonstrate applications for a wide variety of\n4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance\nvideos and hyperspectral images. Generalization of the tensor concepts to\nnon-euclidean domain, orders of magnitude speed-up, low-memory requirement and\nsignificantly enhanced performance at low SNR are the key aspects of our\nframework. \n\n"}
{"id": "1611.04849", "contents": "Title: Deeply supervised salient object detection with short connections Abstract: Recent progress on saliency detection is substantial, benefiting mostly from\nthe explosive development of Convolutional Neural Networks (CNNs). Semantic\nsegmentation and saliency detection algorithms developed lately have been\nmostly based on Fully Convolutional Neural Networks (FCNs). There is still a\nlarge room for improvement over the generic FCN models that do not explicitly\ndeal with the scale-space problem. Holistically-Nested Edge Detector (HED)\nprovides a skip-layer structure with deep supervision for edge and boundary\ndetection, but the performance gain of HED on salience detection is not\nobvious. In this paper, we propose a new method for saliency detection by\nintroducing short connections to the skip-layer structures within the HED\narchitecture. Our framework provides rich multi-scale feature maps at each\nlayer, a property that is critically needed to perform segment detection. Our\nmethod produces state-of-the-art results on 5 widely tested salient object\ndetection benchmarks, with advantages in terms of efficiency (0.15 seconds per\nimage), effectiveness, and simplicity over the existing algorithms. \n\n"}
{"id": "1611.05128", "contents": "Title: Designing Energy-Efficient Convolutional Neural Networks using\n  Energy-Aware Pruning Abstract: Deep convolutional neural networks (CNNs) are indispensable to\nstate-of-the-art computer vision algorithms. However, they are still rarely\ndeployed on battery-powered mobile devices, such as smartphones and wearable\ngadgets, where vision algorithms can enable many revolutionary real-world\napplications. The key limiting factor is the high energy consumption of CNN\nprocessing due to its high computational complexity. While there are many\nprevious efforts that try to reduce the CNN model size or amount of\ncomputation, we find that they do not necessarily result in lower energy\nconsumption, and therefore do not serve as a good metric for energy cost\nestimation.\n  To close the gap between CNN design and energy consumption optimization, we\npropose an energy-aware pruning algorithm for CNNs that directly uses energy\nconsumption estimation of a CNN to guide the pruning process. The energy\nestimation methodology uses parameters extrapolated from actual hardware\nmeasurements that target realistic battery-powered system setups. The proposed\nlayer-by-layer pruning algorithm also prunes more aggressively than previously\nproposed pruning methods by minimizing the error in output feature maps instead\nof filter weights. For each layer, the weights are first pruned and then\nlocally fine-tuned with a closed-form least-square solution to quickly restore\nthe accuracy. After all layers are pruned, the entire network is further\nglobally fine-tuned using back-propagation. With the proposed pruning method,\nthe energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x,\nrespectively, with less than 1% top-5 accuracy loss. Finally, we show that\npruning the AlexNet with a reduced number of target classes can greatly\ndecrease the number of weights but the energy reduction is limited.\n  Energy modeling tool and energy-aware pruned models available at\nhttp://eyeriss.mit.edu/energy.html \n\n"}
{"id": "1611.05369", "contents": "Title: Fast On-Line Kernel Density Estimation for Active Object Localization Abstract: A major goal of computer vision is to enable computers to interpret visual\nsituations---abstract concepts (e.g., \"a person walking a dog,\" \"a crowd\nwaiting for a bus,\" \"a picnic\") whose image instantiations are linked more by\ntheir common spatial and semantic structure than by low-level visual\nsimilarity. In this paper, we propose a novel method for prior learning and\nactive object localization for this kind of knowledge-driven search in static\nimages. In our system, prior situation knowledge is captured by a set of\nflexible, kernel-based density estimations---a situation model---that represent\nthe expected spatial structure of the given situation. These estimations are\nefficiently updated by information gained as the system searches for relevant\nobjects, allowing the system to use context as it is discovered to narrow the\nsearch.\n  More specifically, at any given time in a run on a test image, our system\nuses image features plus contextual information it has discovered to identify a\nsmall subset of training images---an importance cluster---that is deemed most\nsimilar to the given test image, given the context. This subset is used to\ngenerate an updated situation model in an on-line fashion, using an efficient\nmultipole expansion technique.\n  As a proof of concept, we apply our algorithm to a highly varied and\nchallenging dataset consisting of instances of a \"dog-walking\" situation. Our\nresults support the hypothesis that dynamically-rendered, context-based\nprobability models can support efficient object localization in visual\nsituations. Moreover, our approach is general enough to be applied to diverse\nmachine learning paradigms requiring interpretable, probabilistic\nrepresentations generated from partially observed data. \n\n"}
{"id": "1611.05396", "contents": "Title: Dynamic Attention-controlled Cascaded Shape Regression Exploiting\n  Training Data Augmentation and Fuzzy-set Sample Weighting Abstract: We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic\nAttention-Controlled CSR (DAC-CSR), for robust facial landmark detection on\nunconstrained faces. Our DAC-CSR divides facial landmark detection into three\ncascaded sub-tasks: face bounding box refinement, general CSR and\nattention-controlled CSR. The first two stages refine initial face bounding\nboxes and output intermediate facial landmarks. Then, an online dynamic model\nselection method is used to choose appropriate domain-specific CSRs for further\nlandmark refinement. The key innovation of our DAC-CSR is the fault-tolerant\nmechanism, using fuzzy set sample weighting for attention-controlled\ndomain-specific model training. Moreover, we advocate data augmentation with a\nsimple but effective 2D profile face generator, and context-aware feature\nextraction for better facial feature representation. Experimental results\nobtained on challenging datasets demonstrate the merits of our DAC-CSR over the\nstate-of-the-art. \n\n"}
{"id": "1611.05588", "contents": "Title: Instance-aware Image and Sentence Matching with Selective Multimodal\n  LSTM Abstract: Effective image and sentence matching depends on how to well measure their\nglobal visual-semantic similarity. Based on the observation that such a global\nsimilarity arises from a complex aggregation of multiple local similarities\nbetween pairwise instances of image (objects) and sentence (words), we propose\na selective multimodal Long Short-Term Memory network (sm-LSTM) for\ninstance-aware image and sentence matching. The sm-LSTM includes a multimodal\ncontext-modulated attention scheme at each timestep that can selectively attend\nto a pair of instances of image and sentence, by predicting pairwise\ninstance-aware saliency maps for image and sentence. For selected pairwise\ninstances, their representations are obtained based on the predicted saliency\nmaps, and then compared to measure their local similarity. By similarly\nmeasuring multiple local similarities within a few timesteps, the sm-LSTM\nsequentially aggregates them with hidden states to obtain a final matching\nscore as the desired global similarity. Extensive experiments show that our\nmodel can well match image and sentence with complex content, and achieve the\nstate-of-the-art results on two public benchmark datasets. \n\n"}
{"id": "1611.05896", "contents": "Title: Answering Image Riddles using Vision and Reasoning through Probabilistic\n  Soft Logic Abstract: In this work, we explore a genre of puzzles (\"image riddles\") which involves\na set of images and a question. Answering these puzzles require both\ncapabilities involving visual detection (including object, activity\nrecognition) and, knowledge-based or commonsense reasoning. We compile a\ndataset of over 3k riddles where each riddle consists of 4 images and a\ngroundtruth answer. The annotations are validated using crowd-sourced\nevaluation. We also define an automatic evaluation metric to track future\nprogress. Our task bears similarity with the commonly known IQ tasks such as\nanalogy solving, sequence filling that are often used to test intelligence.\n  We develop a Probabilistic Reasoning-based approach that utilizes\nprobabilistic commonsense knowledge to answer these riddles with a reasonable\naccuracy. We demonstrate the results of our approach using both automatic and\nhuman evaluations. Our approach achieves some promising results for these\nriddles and provides a strong baseline for future attempts. We make the entire\ndataset and related materials publicly available to the community in\nImageRiddle Website (http://bit.ly/22f9Ala). \n\n"}
{"id": "1611.05947", "contents": "Title: Minimal Problems for the Calibrated Trifocal Variety Abstract: We determine the algebraic degree of minimal problems for the calibrated\ntrifocal variety in computer vision. We rely on numerical algebraic geometry\nand the homotopy continuation software Bertini. \n\n"}
{"id": "1611.06026", "contents": "Title: Cross Domain Knowledge Transfer for Person Re-identification Abstract: Person Re-Identification (re-id) is a challenging task in computer vision,\nespecially when there are limited training data from multiple camera views. In\nthis paper, we pro- pose a deep learning based person re-identification method\nby transferring knowledge of mid-level attribute features and high-level\nclassification features. Building on the idea that identity classification,\nattribute recognition and re- identification share the same mid-level semantic\nrepresentations, they can be trained sequentially by fine-tuning one based on\nanother. In our framework, we train identity classification and attribute\nrecognition tasks from deep Convolutional Neural Network (dCNN) to learn person\ninformation. The information can be transferred to the person re-id task and\nimproves its accuracy by a large margin. Further- more, a Long Short Term\nMemory(LSTM) based Recurrent Neural Network (RNN) component is extended by a\nspacial gate. This component is used in the re-id model to pay attention to\ncertain spacial parts in each recurrent unit. Experimental results show that\nour method achieves 78.3% of rank-1 recognition accuracy on the CUHK03\nbenchmark. \n\n"}
{"id": "1611.06779", "contents": "Title: TextBoxes: A Fast Text Detector with a Single Deep Neural Network Abstract: This paper presents an end-to-end trainable fast scene text detector, named\nTextBoxes, which detects scene text with both high accuracy and efficiency in a\nsingle network forward pass, involving no post-process except for a standard\nnon-maximum suppression. TextBoxes outperforms competing methods in terms of\ntext localization accuracy and is much faster, taking only 0.09s per image in a\nfast implementation. Furthermore, combined with a text recognizer, TextBoxes\nsignificantly outperforms state-of-the-art approaches on word spotting and\nend-to-end text recognition tasks. \n\n"}
{"id": "1611.07573", "contents": "Title: Relaxed Earth Mover's Distances for Chain- and Tree-connected Spaces and\n  their use as a Loss Function in Deep Learning Abstract: The Earth Mover's Distance (EMD) computes the optimal cost of transforming\none distribution into another, given a known transport metric between them. In\ndeep learning, the EMD loss allows us to embed information during training\nabout the output space structure like hierarchical or semantic relations. This\nhelps in achieving better output smoothness and generalization. However EMD is\ncomputationally expensive.Moreover, solving EMD optimization problems usually\nrequire complex techniques like lasso. These properties limit the applicability\nof EMD-based approaches in large scale machine learning.\n  We address in this work the difficulties facing incorporation of EMD-based\nloss in deep learning frameworks. Additionally, we provide insight and novel\nsolutions on how to integrate such loss function in training deep neural\nnetworks. Specifically, we make three main contributions: (i) we provide an\nin-depth analysis of the fastest state-of-the-art EMD algorithm (Sinkhorn\nDistance) and discuss its limitations in deep learning scenarios. (ii) we\nderive fast and numerically stable closed-form solutions for the EMD gradient\nin output spaces with chain- and tree- connectivity; and (iii) we propose a\nrelaxed form of the EMD gradient with equivalent computational complexity but\nfaster convergence rate. We support our claims with experiments on real\ndatasets. In a restricted data setting on the ImageNet dataset, we train a\nmodel to classify 1000 categories using 50K images, and demonstrate that our\nrelaxed EMD loss achieves better Top-1 accuracy than the cross entropy loss.\nOverall, we show that our relaxed EMD loss criterion is a powerful asset for\ndeep learning in the small data regime. \n\n"}
{"id": "1611.09502", "contents": "Title: Deep Quantization: Encoding Convolutional Activations with Deep\n  Generative Model Abstract: Deep convolutional neural networks (CNNs) have proven highly effective for\nvisual recognition, where learning a universal representation from activations\nof convolutional layer plays a fundamental problem. In this paper, we present\nFisher Vector encoding with Variational Auto-Encoder (FV-VAE), a novel deep\narchitecture that quantizes the local activations of convolutional layer in a\ndeep generative model, by training them in an end-to-end manner. To incorporate\nFV encoding strategy into deep generative models, we introduce Variational\nAuto-Encoder model, which steers a variational inference and learning in a\nneural network which can be straightforwardly optimized using standard\nstochastic gradient method. Different from the FV characterized by conventional\ngenerative models (e.g., Gaussian Mixture Model) which parsimoniously fit a\ndiscrete mixture model to data distribution, the proposed FV-VAE is more\nflexible to represent the natural property of data for better generalization.\nExtensive experiments are conducted on three public datasets, i.e., UCF101,\nActivityNet, and CUB-200-2011 in the context of video action recognition and\nfine-grained image classification, respectively. Superior results are reported\nwhen compared to state-of-the-art representations. Most remarkably, our\nproposed FV-VAE achieves to-date the best published accuracy of 94.2% on\nUCF101. \n\n"}
{"id": "1611.10176", "contents": "Title: Effective Quantization Methods for Recurrent Neural Networks Abstract: Reducing bit-widths of weights, activations, and gradients of a Neural\nNetwork can shrink its storage size and memory usage, and also allow for faster\ntraining and inference by exploiting bitwise operations. However, previous\nattempts for quantization of RNNs show considerable performance degradation\nwhen using low bit-width weights and activations. In this paper, we propose\nmethods to quantize the structure of gates and interlinks in LSTM and GRU\ncells. In addition, we propose balanced quantization methods for weights to\nfurther reduce performance degradation. Experiments on PTB and IMDB datasets\nconfirm effectiveness of our methods as performances of our models match or\nsurpass the previous state-of-the-art of quantized RNN. \n\n"}
{"id": "1612.00215", "contents": "Title: Learning to Generate Images of Outdoor Scenes from Attributes and\n  Semantic Layouts Abstract: Automatic image synthesis research has been rapidly growing with deep\nnetworks getting more and more expressive. In the last couple of years, we have\nobserved images of digits, indoor scenes, birds, chairs, etc. being\nautomatically generated. The expressive power of image generators have also\nbeen enhanced by introducing several forms of conditioning variables such as\nobject names, sentences, bounding box and key-point locations. In this work, we\npropose a novel deep conditional generative adversarial network architecture\nthat takes its strength from the semantic layout and scene attributes\nintegrated as conditioning variables. We show that our architecture is able to\ngenerate realistic outdoor scene images under different conditions, e.g.\nday-night, sunny-foggy, with clear object boundaries. \n\n"}
{"id": "1612.01991", "contents": "Title: Diverse Sampling for Self-Supervised Learning of Semantic Segmentation Abstract: We propose an approach for learning category-level semantic segmentation\npurely from image-level classification tags indicating presence of categories.\nIt exploits localization cues that emerge from training classification-tasked\nconvolutional networks, to drive a \"self-supervision\" process that\nautomatically labels a sparse, diverse training set of points likely to belong\nto classes of interest. Our approach has almost no hyperparameters, is modular,\nand allows for very fast training of segmentation in less than 3 minutes. It\nobtains competitive results on the VOC 2012 segmentation benchmark. More,\nsignificantly the modularity and fast training of our framework allows new\nclasses to efficiently added for inference. \n\n"}
{"id": "1612.02534", "contents": "Title: Contextual Visual Similarity Abstract: Measuring visual similarity is critical for image understanding. But what\nmakes two images similar? Most existing work on visual similarity assumes that\nimages are similar because they contain the same object instance or category.\nHowever, the reason why images are similar is much more complex. For example,\nfrom the perspective of category, a black dog image is similar to a white dog\nimage. However, in terms of color, a black dog image is more similar to a black\nhorse image than the white dog image. This example serves to illustrate that\nvisual similarity is ambiguous but can be made precise when given an explicit\ncontextual perspective. Based on this observation, we propose the concept of\ncontextual visual similarity. To be concrete, we examine the concept of\ncontextual visual similarity in the application domain of image search. Instead\nof providing only a single image for image similarity search (\\eg, Google image\nsearch), we require three images. Given a query image, a second positive image\nand a third negative image, dissimilar to the first two images, we define a\ncontextualized similarity search criteria. In particular, we learn feature\nweights over all the feature dimensions of each image such that the distance\nbetween the query image and the positive image is small and their distances to\nthe negative image are large after reweighting their features. The learned\nfeature weights encode the contextualized visual similarity specified by the\nuser and can be used for attribute specific image search. We also show the\nusefulness of our contextualized similarity weighting scheme for different\ntasks, such as answering visual analogy questions and unsupervised attribute\ndiscovery. \n\n"}
{"id": "1612.03667", "contents": "Title: Extended V-systems and almost-duality for extended affine Weyl orbit\n  spaces Abstract: Rational solutions of the Witten-Dijkgraaf-Verlinde-Verlinde (or WDVV)\nequations of associativity are given in terms a configurations of vectors which\nsatisfy certain algebraic conditions known as $\\bigvee$-conditions. The\nsimplest examples of such configuration are the root systems of finite Coxeter\ngroups. In this paper conditions are derived which ensure that an extended\nconfiguration - a configuration in a space one-dimension higher -satisfy these\n$\\bigvee$-conditions. Such a construction utilizes the notion of a small-orbit,\nas defined by Serganova. Symmetries of such resulting solutions to the\nWDVV-equations are studied; in particular, Legendre transformations. It is\nshown that these Legendre transformations map extended-rational solutions to\ntrigonometric solutions and, for certain values of the free data, one obtains a\ntransformation from extended $\\bigvee$-systems to the trigonometric almost dual\nsolutions corresponding to the classical extended affine Weyl groups. \n\n"}
{"id": "1612.03809", "contents": "Title: Generalizable Features From Unsupervised Learning Abstract: Humans learn a predictive model of the world and use this model to reason\nabout future events and the consequences of actions. In contrast to most\nmachine predictors, we exhibit an impressive ability to generalize to unseen\nscenarios and reason intelligently in these settings. One important aspect of\nthis ability is physical intuition(Lake et al., 2016). In this work, we explore\nthe potential of unsupervised learning to find features that promote better\ngeneralization to settings outside the supervised training distribution. Our\ntask is predicting the stability of towers of square blocks. We demonstrate\nthat an unsupervised model, trained to predict future frames of a video\nsequence of stable and unstable block configurations, can yield features that\nsupport extrapolating stability prediction to blocks configurations outside the\ntraining set distribution \n\n"}
{"id": "1612.04440", "contents": "Title: Disentangling Space and Time in Video with Hierarchical Variational\n  Auto-encoders Abstract: There are many forms of feature information present in video data. Principle\namong them are object identity information which is largely static across\nmultiple video frames, and object pose and style information which continuously\ntransforms from frame to frame. Most existing models confound these two types\nof representation by mapping them to a shared feature space. In this paper we\npropose a probabilistic approach for learning separable representations of\nobject identity and pose information using unsupervised video data. Our\napproach leverages a deep generative model with a factored prior distribution\nthat encodes properties of temporal invariances in the hidden feature set.\nLearning is achieved via variational inference. We present results of learning\nidentity and pose information on a dataset of moving characters as well as a\ndataset of rotating 3D objects. Our experimental results demonstrate our\nmodel's success in factoring its representation, and demonstrate that the model\nachieves improved performance in transfer learning tasks. \n\n"}
{"id": "1612.05207", "contents": "Title: Generalisation of the explicit expression for the Deprit generator to\n  Hamiltonians nonlinearly dependent on small parameter Abstract: This work explores a structure of the Deprit perturbation series and its\nconnection to a Kato resolvent expansion. It extends the formalism previously\ndeveloped for the Hamiltonians linearly dependent on perturbation parameter to\na nonlinear case. We construct a canonical intertwining of perturbed and\nunperturbed averaging operators. This leads to an explicit expression for the\ngenerator of the Lie-Deprit transform in any perturbation order. Using this\nexpression, we discuss a regular pattern in the series, non-uniqueness of the\ngenerator and normalised Hamiltonian, and the uniqueness of the Gustavson\nintegrals. Comparison of the corresponding computational algorithm with\nclassical perturbation methods demonstrates its competitiveness for\nHamiltonians with a limited number of perturbation terms. \n\n"}
{"id": "1612.05323", "contents": "Title: A Stochastic Large Deformation Model for Computational Anatomy Abstract: In the study of shapes of human organs using computational anatomy,\nvariations are found to arise from inter-subject anatomical differences,\ndisease-specific effects, and measurement noise. This paper introduces a\nstochastic model for incorporating random variations into the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) framework. By accounting for randomness in\na particular setup which is crafted to fit the geometrical properties of LDDMM,\nwe formulate the template estimation problem for landmarks with noise and give\ntwo methods for efficiently estimating the parameters of the noise fields from\na prescribed data set. One method directly approximates the time evolution of\nthe variance of each landmark by a finite set of differential equations, and\nthe other is based on an Expectation-Maximisation algorithm. In the second\nmethod, the evaluation of the data likelihood is achieved without registering\nthe landmarks, by applying bridge sampling using a stochastically perturbed\nversion of the large deformation gradient flow algorithm. The method and the\nestimation algorithms are experimentally validated on synthetic examples and\nshape data of human corpora callosa. \n\n"}
{"id": "1612.05332", "contents": "Title: Fast, Dense Feature SDM on an iPhone Abstract: In this paper, we present our method for enabling dense SDM to run at over 90\nFPS on a mobile device. Our contributions are two-fold. Drawing inspiration\nfrom the FFT, we propose a Sparse Compositional Regression (SCR) framework,\nwhich enables a significant speed up over classical dense regressors. Second,\nwe propose a binary approximation to SIFT features. Binary Approximated SIFT\n(BASIFT) features, which are a computationally efficient approximation to SIFT,\na commonly used feature with SDM. We demonstrate the performance of our\nalgorithm on an iPhone 7, and show that we achieve similar accuracy to SDM. \n\n"}
{"id": "1612.06176", "contents": "Title: An extended Perona-Malik model based on probabilistic models Abstract: The Perona-Malik model has been very successful at restoring images from\nnoisy input. In this paper, we reinterpret the Perona-Malik model in the\nlanguage of Gaussian scale mixtures and derive some extensions of the model.\nSpecifically, we show that the expectation-maximization (EM) algorithm applied\nto Gaussian scale mixtures leads to the lagged-diffusivity algorithm for\ncomputing stationary points of the Perona-Malik diffusion equations. Moreover,\nwe show how mean field approximations to these Gaussian scale mixtures lead to\na modification of the lagged-diffusivity algorithm that better captures the\nuncertainties in the restoration. Since this modification can be hard to\ncompute in practice we propose relaxations to the mean field objective to make\nthe algorithm computationally feasible. Our numerical experiments show that\nthis modified lagged-diffusivity algorithm often performs better at restoring\ntextured areas and fuzzy edges than the unmodified algorithm. As a second\napplication of the Gaussian scale mixture framework, we show how an efficient\nsampling procedure can be obtained for the probabilistic model, making the\ncomputation of the conditional mean and other expectations algorithmically\nfeasible. Again, the resulting algorithm has a strong resemblance to the\nlagged-diffusivity algorithm. Finally, we show that a probabilistic version of\nthe Mumford-Shah segementation model can be obtained in the same framework with\na discrete edge-prior. \n\n"}
{"id": "1612.07409", "contents": "Title: Lyapunov spectrum of separated flows and its dependence on numerical\n  discretization Abstract: We investigate the Lyapunov spectrum of separated flows and their dependence\non the numerical discretization. The chaotic flow around the NACA 0012 airfoil\nat low Reynolds number and large angle of attack is considered to that end, and\nt-, h- and p-refinement studies are performed to examine each effect\nseparately. Numerical results show that the time discretization has a small\nimpact on the dynamics of the system, whereas the spatial discretization can\ndramatically change them. In particular, the asymptotic Lyapunov spectrum for\ntime refinement is achieved for CFL numbers as large as\n$\\mathcal{O}(10^1-10^2)$, whereas the system continues to become more and more\nchaotic even for meshes that are much finer than the best practice for this\ntype of flows. \n\n"}
{"id": "1612.07528", "contents": "Title: Handwriting recognition using Cohort of LSTM and lexicon verification\n  with extremely large lexicon Abstract: State-of-the-art methods for handwriting recognition are based on Long Short\nTerm Memory (LSTM) recurrent neural networks (RNN), which now provides very\nimpressive character recognition performance. The character recognition is\ngenerally coupled with a lexicon driven decoding process which integrates\ndictionaries. Unfortunately these dictionaries are limited to hundred of\nthousands words for the best systems, which prevent from having a good language\ncoverage, and therefore limit the global recognition performance. In this\narticle, we propose an alternative to the lexicon driven decoding process based\non a lexicon verification process, coupled with an original cascade\narchitecture. The cascade is made of a large number of complementary networks\nextracted from a single training (called cohort), making the learning process\nvery light. The proposed method achieves new state-of-the art word recognition\nperformance on the Rimes and IAM databases. Dealing with gigantic lexicon of 3\nmillions words, the methods also demonstrates interesting performance with a\nfast decision stage. \n\n"}
{"id": "1612.07767", "contents": "Title: Adversarial Examples Detection in Deep Networks with Convolutional\n  Filter Statistics Abstract: Deep learning has greatly improved visual recognition in recent years.\nHowever, recent research has shown that there exist many adversarial examples\nthat can negatively impact the performance of such an architecture. This paper\nfocuses on detecting those adversarial examples by analyzing whether they come\nfrom the same distribution as the normal examples. Instead of directly training\na deep neural network to detect adversarials, a much simpler approach was\nproposed based on statistics on outputs from convolutional layers. A cascade\nclassifier was designed to efficiently detect adversarials. Furthermore,\ntrained from one particular adversarial generating mechanism, the resulting\nclassifier can successfully detect adversarials from a completely different\nmechanism as well. The resulting classifier is non-subdifferentiable, hence\ncreates a difficulty for adversaries to attack by using the gradient of the\nclassifier. After detecting adversarial examples, we show that many of them can\nbe recovered by simply performing a small average filter on the image. Those\nfindings should lead to more insights about the classification mechanisms in\ndeep convolutional neural networks. \n\n"}
{"id": "1701.01486", "contents": "Title: Motion Deblurring in the Wild Abstract: The task of image deblurring is a very ill-posed problem as both the image\nand the blur are unknown. Moreover, when pictures are taken in the wild, this\ntask becomes even more challenging due to the blur varying spatially and the\nocclusions between the object. Due to the complexity of the general image model\nwe propose a novel convolutional network architecture which directly generates\nthe sharp image.This network is built in three stages, and exploits the\nbenefits of pyramid schemes often used in blind deconvolution. One of the main\ndifficulties in training such a network is to design a suitable dataset. While\nuseful data can be obtained by synthetically blurring a collection of images,\nmore realistic data must be collected in the wild. To obtain such data we use a\nhigh frame rate video camera and keep one frame as the sharp image and frame\naverage as the corresponding blurred image. We show that this realistic dataset\nis key in achieving state-of-the-art performance and dealing with occlusions. \n\n"}
{"id": "1701.01619", "contents": "Title: Learning From Noisy Large-Scale Datasets With Minimal Supervision Abstract: We present an approach to effectively use millions of images with noisy\nannotations in conjunction with a small subset of cleanly-annotated images to\nlearn powerful image representations. One common approach to combine clean and\nnoisy data is to first pre-train a network using the large noisy dataset and\nthen fine-tune with the clean dataset. We show this approach does not fully\nleverage the information contained in the clean set. Thus, we demonstrate how\nto use the clean annotations to reduce the noise in the large dataset before\nfine-tuning the network using both the clean set and the full set with reduced\nnoise. The approach comprises a multi-task network that jointly learns to clean\nnoisy annotations and to accurately classify images. We evaluate our approach\non the recently released Open Images dataset, containing ~9 million images,\nmultiple annotations per image and over 6000 unique classes. For the small\nclean set of annotations we use a quarter of the validation set with ~40k\nimages. Our results demonstrate that the proposed approach clearly outperforms\ndirect fine-tuning across all major categories of classes in the Open Image\ndataset. Further, our approach is particularly effective for a large number of\nclasses with wide range of noise in annotations (20-80% false positive\nannotations). \n\n"}
{"id": "1701.01945", "contents": "Title: A Framework for Wasserstein-1-Type Metrics Abstract: We propose a unifying framework for generalising the Wasserstein-1 metric to\na discrepancy measure between nonnegative measures of different mass. This\ngeneralization inherits the convexity and computational efficiency from the\nWasserstein-1 metric, and it includes several previous approaches from the\nliterature as special cases. For various specific instances of the generalized\nWasserstein-1 metric we furthermore demonstrate their usefulness in\napplications by numerical experiments. \n\n"}
{"id": "1701.02343", "contents": "Title: Information Pursuit: A Bayesian Framework for Sequential Scene Parsing Abstract: Despite enormous progress in object detection and classification, the problem\nof incorporating expected contextual relationships among object instances into\nmodern recognition systems remains a key challenge. In this work we propose\nInformation Pursuit, a Bayesian framework for scene parsing that combines prior\nmodels for the geometry of the scene and the spatial arrangement of objects\ninstances with a data model for the output of high-level image classifiers\ntrained to answer specific questions about the scene. In the proposed\nframework, the scene interpretation is progressively refined as evidence\naccumulates from the answers to a sequence of questions. At each step, we\nchoose the question to maximize the mutual information between the new answer\nand the full interpretation given the current evidence obtained from previous\ninquiries. We also propose a method for learning the parameters of the model\nfrom synthesized, annotated scenes obtained by top-down sampling from an\neasy-to-learn generative scene model. Finally, we introduce a database of\nannotated indoor scenes of dining room tables, which we use to evaluate the\nproposed approach. \n\n"}
{"id": "1701.04210", "contents": "Title: Bandwidth limited object recognition in high resolution imagery Abstract: This paper proposes a novel method to optimize bandwidth usage for object\ndetection in critical communication scenarios. We develop two operating models\nof active information seeking. The first model identifies promising regions in\nlow resolution imagery and progressively requests higher resolution regions on\nwhich to perform recognition of higher semantic quality. The second model\nidentifies promising regions in low resolution imagery while simultaneously\npredicting the approximate location of the object of higher semantic quality.\nFrom this general framework, we develop a car recognition system via\nidentification of its license plate and evaluate the performance of both models\non a car dataset that we introduce. Results are compared with traditional JPEG\ncompression and demonstrate that our system saves up to one order of magnitude\nof bandwidth while sacrificing little in terms of recognition performance. \n\n"}
{"id": "1701.06123", "contents": "Title: Optimization on Product Submanifolds of Convolution Kernels Abstract: Recent advances in optimization methods used for training convolutional\nneural networks (CNNs) with kernels, which are normalized according to\nparticular constraints, have shown remarkable success. This work introduces an\napproach for training CNNs using ensembles of joint spaces of kernels\nconstructed using different constraints. For this purpose, we address a problem\nof optimization on ensembles of products of submanifolds (PEMs) of convolution\nkernels. To this end, we first propose three strategies to construct ensembles\nof PEMs in CNNs. Next, we expound their geometric properties (metric and\ncurvature properties) in CNNs. We make use of our theoretical results by\ndeveloping a geometry-aware SGD algorithm (G-SGD) for optimization on ensembles\nof PEMs to train CNNs. Moreover, we analyze convergence properties of G-SGD\nconsidering geometric properties of PEMs. In the experimental analyses, we\nemploy G-SGD to train CNNs on Cifar-10, Cifar-100 and Imagenet datasets. The\nresults show that geometric adaptive step size computation methods of G-SGD can\nimprove training loss and convergence properties of CNNs. Moreover, we observe\nthat classification performance of baseline CNNs can be boosted using G-SGD on\nensembles of PEMs identified by multiple constraints. \n\n"}
{"id": "1701.07451", "contents": "Title: Stability interchanges in a curved Sitnikov problem Abstract: We consider a curved Sitnikov problem, in which an infinitesimal particle\nmoves on a circle under the gravitational influence of two equal masses in\nKeplerian motion within a plane perpendicular to that circle. There are two\nequilibrium points, whose stability we are studying. We show that one of the\nequilibrium points undergoes stability interchanges as the semi-major axis of\nthe Keplerian ellipses approaches the diameter of that circle. To derive this\nresult, we first formulate and prove a general theorem on stability\ninterchanges, and then we apply it to our model. The motivation for our model\nresides with the $n$-body problem in spaces of constant curvature. \n\n"}
{"id": "1701.08869", "contents": "Title: 3D Shape Retrieval via Irrelevance Filtering and Similarity Ranking\n  (IF/SR) Abstract: A novel solution for the content-based 3D shape retrieval problem using an\nunsupervised clustering approach, which does not need any label information of\n3D shapes, is presented in this work. The proposed shape retrieval system\nconsists of two modules in cascade: the irrelevance filtering (IF) module and\nthe similarity ranking (SR) module. The IF module attempts to cluster gallery\nshapes that are similar to each other by examining global and local features\nsimultaneously. However, shapes that are close in the local feature space can\nbe distant in the global feature space, and vice versa. To resolve this issue,\nwe propose a joint cost function that strikes a balance between two distances.\nIrrelevant samples that are close in the local feature space but distant in the\nglobal feature space can be removed in this stage. The remaining gallery\nsamples are ranked in the SR module using the local feature. The superior\nperformance of the proposed IF/SR method is demonstrated by extensive\nexperiments conducted on the popular SHREC12 dataset. \n\n"}
{"id": "1701.08886", "contents": "Title: SenseGen: A Deep Learning Architecture for Synthetic Sensor Data\n  Generation Abstract: Our ability to synthesize sensory data that preserves specific statistical\nproperties of the real data has had tremendous implications on data privacy and\nbig data analytics. The synthetic data can be used as a substitute for\nselective real data segments,that are sensitive to the user, thus protecting\nprivacy and resulting in improved analytics.However, increasingly adversarial\nroles taken by data recipients such as mobile apps, or other cloud-based\nanalytics services, mandate that the synthetic data, in addition to preserving\nstatistical properties, should also be difficult to distinguish from the real\ndata. Typically, visual inspection has been used as a test to distinguish\nbetween datasets. But more recently, sophisticated classifier models\n(discriminators), corresponding to a set of events, have also been employed to\ndistinguish between synthesized and real data. The model operates on both\ndatasets and the respective event outputs are compared for consistency. In this\npaper, we take a step towards generating sensory data that can pass a deep\nlearning based discriminator model test, and make two specific contributions:\nfirst, we present a deep learning based architecture for synthesizing sensory\ndata. This architecture comprises of a generator model, which is a stack of\nmultiple Long-Short-Term-Memory (LSTM) networks and a Mixture Density Network.\nsecond, we use another LSTM network based discriminator model for\ndistinguishing between the true and the synthesized data. Using a dataset of\naccelerometer traces, collected using smartphones of users doing their daily\nactivities, we show that the deep learning based discriminator model can only\ndistinguish between the real and synthesized traces with an accuracy in the\nneighborhood of 50%. \n\n"}
{"id": "1701.08985", "contents": "Title: Deep Multitask Architecture for Integrated 2D and 3D Human Sensing Abstract: We propose a deep multitask architecture for \\emph{fully automatic 2d and 3d\nhuman sensing} (DMHS), including \\emph{recognition and reconstruction}, in\n\\emph{monocular images}. The system computes the figure-ground segmentation,\nsemantically identifies the human body parts at pixel level, and estimates the\n2d and 3d pose of the person. The model supports the joint training of all\ncomponents by means of multi-task losses where early processing stages\nrecursively feed into advanced ones for increasingly complex calculations,\naccuracy and robustness. The design allows us to tie a complete training\nprotocol, by taking advantage of multiple datasets that would otherwise\nrestrictively cover only some of the model components: complex 2d image data\nwith no body part labeling and without associated 3d ground truth, or complex\n3d data with limited 2d background variability. In detailed experiments based\non several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we\nevaluate the sub-structures of the model, the effect of various types of\ntraining data in the multitask loss, and demonstrate that state-of-the-art\nresults can be achieved at all processing levels. We also show that in the wild\nour monocular RGB architecture is perceptually competitive to a state-of-the\nart (commercial) Kinect system based on RGB-D data. \n\n"}
{"id": "1702.00615", "contents": "Title: A Fast and Compact Saliency Score Regression Network Based on Fully\n  Convolutional Network Abstract: Visual saliency detection aims at identifying the most visually distinctive\nparts in an image, and serves as a pre-processing step for a variety of\ncomputer vision and image processing tasks. To this end, the saliency detection\nprocedure must be as fast and compact as possible and optimally processes input\nimages in a real time manner. It is an essential application requirement for\nthe saliency detection task. However, contemporary detection methods often\nutilize some complicated procedures to pursue feeble improvements on the\ndetection precession, which always take hundreds of milliseconds and make them\nnot easy to be applied practically. In this paper, we tackle this problem by\nproposing a fast and compact saliency score regression network which employs\nfully convolutional network, a special deep convolutional neural network, to\nestimate the saliency of objects in images. It is an extremely simplified\nend-to-end deep neural network without any pre-processings and\npost-processings. When given an image, the network can directly predict a dense\nfull-resolution saliency map (image-to-image prediction). It works like a\ncompact pipeline which effectively simplifies the detection procedure. Our\nmethod is evaluated on six public datasets, and experimental results show that\nit can achieve comparable or better precision performance than the\nstate-of-the-art methods while get a significant improvement in detection speed\n(35 FPS, processing in real time). \n\n"}
{"id": "1702.00926", "contents": "Title: FCSS: Fully Convolutional Self-Similarity for Dense Semantic\n  Correspondence Abstract: We present a descriptor, called fully convolutional self-similarity (FCSS),\nfor dense semantic correspondence. To robustly match points among different\ninstances within the same object class, we formulate FCSS using local\nself-similarity (LSS) within a fully convolutional network. In contrast to\nexisting CNN-based descriptors, FCSS is inherently insensitive to intra-class\nappearance variations because of its LSS-based structure, while maintaining the\nprecise localization ability of deep neural networks. The sampling patterns of\nlocal structure and the self-similarity measure are jointly learned within the\nproposed network in an end-to-end and multi-scale manner. As training data for\nsemantic correspondence is rather limited, we propose to leverage object\ncandidate priors provided in existing image datasets and also correspondence\nconsistency between object pairs to enable weakly-supervised learning.\nExperiments demonstrate that FCSS outperforms conventional handcrafted\ndescriptors and CNN-based descriptors on various benchmarks. \n\n"}
{"id": "1702.01005", "contents": "Title: Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear\n  Subspace Learning Abstract: Principal Component Analysis (PCA) and Kernel Principal Component Analysis\n(KPCA) are fundamental methods in machine learning for dimensionality\nreduction. The former is a technique for finding this approximation in finite\ndimensions and the latter is often in an infinite dimensional Reproducing\nKernel Hilbert-space (RKHS). In this paper, we present a geometric framework\nfor computing the principal linear subspaces in both situations as well as for\nthe robust PCA case, that amounts to computing the intrinsic average on the\nspace of all subspaces: the Grassmann manifold. Points on this manifold are\ndefined as the subspaces spanned by $K$-tuples of observations. The intrinsic\nGrassmann average of these subspaces are shown to coincide with the principal\ncomponents of the observations when they are drawn from a Gaussian\ndistribution. We show similar results in the RKHS case and provide an efficient\nalgorithm for computing the projection onto the this average subspace. The\nresult is a method akin to KPCA which is substantially faster. Further, we\npresent a novel online version of the KPCA using our geometric framework.\nCompetitive performance of all our algorithms are demonstrated on a variety of\nreal and synthetic data sets. \n\n"}
{"id": "1702.01128", "contents": "Title: A dressing method for soliton solutions of the Camassa-Holm equation Abstract: The soliton solutions of the Camassa-Holm equation are derived by the\nimplementation of the dressing method. The form of the one and two soliton\nsolutions coincides with the form obtained by other methods. \n\n"}
{"id": "1702.02666", "contents": "Title: Response Formulae for $n$-point Correlations in Statistical Mechanical\n  Systems and Application to a Problem of Coarse Graining Abstract: Predicting the response of a system to perturbations is a key challenge in\nmathematical and natural sciences. Under suitable conditions on the nature of\nthe system, of the perturbation, and of the observables of interest, response\ntheories allow to construct operators describing the smooth change of the\ninvariant measure of the system of interest as a function of the small\nparameter controlling the intensity of the perturbation. In particular,\nresponse theories can be developed both for stochastic and chaotic\ndeterministic dynamical systems, where in the latter case stricter conditions\nimposing some degree of structural stability are required. In this paper we\nextend previous findings and derive general response formulae describing how\nn-point correlations are affected by perturbations to the vector flow. We also\nshow how to compute the response of the spectral properties of the system to\nperturbations. We then apply our results to the seemingly unrelated problem of\ncoarse graining in multiscale systems: we find explicit formulae describing the\nchange in the terms describing parameterisation of the neglected degrees of\nfreedom resulting from applying perturbations to the full system. All the terms\nenvisioned by the Mori-Zwanzig theory - the deterministic, stochastic, and\nnon-Markovian terms - are affected at 1st order in the perturbation. The\nobtained results provide a more comprehesive understanding of the response of\nstatistical mechanical systems to perturbations and contribute to the goal of\nconstructing accurate and robust parameterisations and are of potential\nrelevance for fields like molecular dynamics, condensed matter, and geophysical\nfluid dynamics. We envision possible applications of our general results to the\nstudy of the response of climate variability to anthropogenic and natural\nforcing and to the study of the equivalence of thermostatted statistical\nmechanical systems. \n\n"}
{"id": "1702.02680", "contents": "Title: Manifold Based Low-rank Regularization for Image Restoration and\n  Semi-supervised Learning Abstract: Low-rank structures play important role in recent advances of many problems\nin image science and data science. As a natural extension of low-rank\nstructures for data with nonlinear structures, the concept of the\nlow-dimensional manifold structure has been considered in many data processing\nproblems. Inspired by this concept, we consider a manifold based low-rank\nregularization as a linear approximation of manifold dimension. This\nregularization is less restricted than the global low-rank regularization, and\nthus enjoy more flexibility to handle data with nonlinear structures. As\napplications, we demonstrate the proposed regularization to classical inverse\nproblems in image sciences and data sciences including image inpainting, image\nsuper-resolution, X-ray computer tomography (CT) image reconstruction and\nsemi-supervised learning. We conduct intensive numerical experiments in several\nimage restoration problems and a semi-supervised learning problem of\nclassifying handwritten digits using the MINST data. Our numerical tests\ndemonstrate the effectiveness of the proposed methods and illustrate that the\nnew regularization methods produce outstanding results by comparing with many\nexisting methods. \n\n"}
{"id": "1702.02706", "contents": "Title: Semi-Supervised Deep Learning for Monocular Depth Map Prediction Abstract: Supervised deep learning often suffers from the lack of sufficient training\ndata. Specifically in the context of monocular depth map prediction, it is\nbarely possible to determine dense ground truth depth images in realistic\ndynamic outdoor environments. When using LiDAR sensors, for instance, noise is\npresent in the distance measurements, the calibration between sensors cannot be\nperfect, and the measurements are typically much sparser than the camera\nimages. In this paper, we propose a novel approach to depth map prediction from\nmonocular images that learns in a semi-supervised way. While we use sparse\nground-truth depth for supervised learning, we also enforce our deep network to\nproduce photoconsistent dense depth maps in a stereo setup using a direct image\nalignment loss. In experiments we demonstrate superior performance in depth map\nprediction from single images compared to the state-of-the-art methods. \n\n"}
{"id": "1702.03435", "contents": "Title: Distributed Mapping with Privacy and Communication Constraints:\n  Lightweight Algorithms and Object-based Models Abstract: We consider the following problem: a team of robots is deployed in an unknown\nenvironment and it has to collaboratively build a map of the area without a\nreliable infrastructure for communication. The backbone for modern mapping\ntechniques is pose graph optimization, which estimates the trajectory of the\nrobots, from which the map can be easily built. The first contribution of this\npaper is a set of distributed algorithms for pose graph optimization: rather\nthan sending all sensor data to a remote sensor fusion server, the robots\nexchange very partial and noisy information to reach an agreement on the pose\ngraph configuration. Our approach can be considered as a distributed\nimplementation of the two-stage approach of Carlone et al., where we use the\nSuccessive Over-Relaxation (SOR) and the Jacobi Over-Relaxation (JOR) as\nworkhorses to split the computation among the robots. As a second contribution,\nwe extend %and demonstrate the applicability of the proposed distributed\nalgorithms to work with object-based map models. The use of object-based models\navoids the exchange of raw sensor measurements (e.g., point clouds) further\nreducing the communication burden. Our third contribution is an extensive\nexperimental evaluation of the proposed techniques, including tests in\nrealistic Gazebo simulations and field experiments in a military test facility.\nAbundant experimental evidence suggests that one of the proposed algorithms\n(the Distributed Gauss-Seidel method or DGS) has excellent performance. The DGS\nrequires minimal information exchange, has an anytime flavor, scales well to\nlarge teams, is robust to noise, and is easy to implement. Our field tests show\nthat the combined use of our distributed algorithms and object-based models\nreduces the communication requirements by several orders of magnitude and\nenables distributed mapping with large teams of robots in real-world problems. \n\n"}
{"id": "1702.04663", "contents": "Title: Handwritten Arabic Numeral Recognition using Deep Learning Neural\n  Networks Abstract: Handwritten character recognition is an active area of research with\napplications in numerous fields. Past and recent works in this field have\nconcentrated on various languages. Arabic is one language where the scope of\nresearch is still widespread, with it being one of the most popular languages\nin the world and being syntactically different from other major languages. Das\net al. \\cite{DBLP:journals/corr/abs-1003-1891} has pioneered the research for\nhandwritten digit recognition in Arabic. In this paper, we propose a novel\nalgorithm based on deep learning neural networks using appropriate activation\nfunction and regularization layer, which shows significantly improved accuracy\ncompared to the existing Arabic numeral recognition methods. The proposed model\ngives 97.4 percent accuracy, which is the recorded highest accuracy of the\ndataset used in the experiment. We also propose a modification of the method\ndescribed in \\cite{DBLP:journals/corr/abs-1003-1891}, where our method scores\nidentical accuracy as that of \\cite{DBLP:journals/corr/abs-1003-1891}, with the\nvalue of 93.8 percent. \n\n"}
{"id": "1702.05448", "contents": "Title: Learning to Detect Human-Object Interactions Abstract: We study the problem of detecting human-object interactions (HOI) in static\nimages, defined as predicting a human and an object bounding box with an\ninteraction class label that connects them. HOI detection is a fundamental\nproblem in computer vision as it provides semantic information about the\ninteractions among the detected objects. We introduce HICO-DET, a new large\nbenchmark for HOI detection, by augmenting the current HICO classification\nbenchmark with instance annotations. To solve the task, we propose Human-Object\nRegion-based Convolutional Neural Networks (HO-RCNN). At the core of our\nHO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the\nspatial relations between two bounding boxes. Experiments on HICO-DET\ndemonstrate that our HO-RCNN, by exploiting human-object spatial relations\nthrough Interaction Patterns, significantly improves the performance of HOI\ndetection over baseline approaches. \n\n"}
{"id": "1702.05593", "contents": "Title: Uncovering the Edge of the Polar Vortex Abstract: The polar vortices play a crucial role in the formation of the ozone hole and\ncan cause severe weather anomalies. Their boundaries, known as the vortex\n`edges', are typically identified via methods that are either frame-dependent\nor return non-material structures, and hence are unsuitable for assessing\nmaterial transport barriers. Using two-dimensional velocity data on isentropic\nsurfaces in the northern hemisphere, we show that elliptic Lagrangian Coherent\nStructures (LCSs) identify the correct outermost material surface dividing the\ncoherent vortex core from the surrounding incoherent surf zone. Despite the\npurely kinematic construction of LCSs, we find a remarkable contrast in\ntemperature and ozone concentration across the identified vortex boundary. We\nalso show that potential vorticity-based methods, despite their simplicity,\nmisidentify the correct extent of the vortex edge. Finally, exploiting the\nshrinkage of the vortex at various isentropic levels, we observe a trend in the\nmagnitude of vertical motion inside the vortex which is consistent with\nprevious results. \n\n"}
{"id": "1702.05663", "contents": "Title: The Game Imitation: Deep Supervised Convolutional Networks for Quick\n  Video Game AI Abstract: We present a vision-only model for gaming AI which uses a late integration\ndeep convolutional network architecture trained in a purely supervised\nimitation learning context. Although state-of-the-art deep learning models for\nvideo game tasks generally rely on more complex methods such as deep-Q\nlearning, we show that a supervised model which requires substantially fewer\nresources and training time can already perform well at human reaction speeds\non the N64 classic game Super Smash Bros. We frame our learning task as a\n30-class classification problem, and our CNN model achieves 80% top-1 and 95%\ntop-3 validation accuracy. With slight test-time fine-tuning, our model is also\ncompetitive during live simulation with the highest-level AI built into the\ngame. We will further show evidence through network visualizations that the\nnetwork is successfully leveraging temporal information during inference to aid\nin decision making. Our work demonstrates that supervised CNN models can\nprovide good performance in challenging policy prediction tasks while being\nsignificantly simpler and more lightweight than alternatives. \n\n"}
{"id": "1702.07054", "contents": "Title: Learning Chained Deep Features and Classifiers for Cascade in Object\n  Detection Abstract: Cascade is a widely used approach that rejects obvious negative samples at\nearly stages for learning better classifier and faster inference. This paper\npresents chained cascade network (CC-Net). In this CC-Net, the cascaded\nclassifier at a stage is aided by the classification scores in previous stages.\nFeature chaining is further proposed so that the feature learning for the\ncurrent cascade stage uses the features in previous stages as the prior\ninformation. The chained ConvNet features and classifiers of multiple stages\nare jointly learned in an end-to-end network. In this way, features and\nclassifiers at latter stages handle more difficult samples with the help of\nfeatures and classifiers in previous stages. It yields consistent boost in\ndetection performance on benchmarks like PASCAL VOC 2007 and ImageNet. Combined\nwith better region proposal, CC-Net leads to state-of-the-art result of 81.1%\nmAP on PASCAL VOC 2007. \n\n"}
{"id": "1702.07424", "contents": "Title: Building Usage Profiles Using Deep Neural Nets Abstract: To improve software quality, one needs to build test scenarios resembling the\nusage of a software product in the field. This task is rendered challenging\nwhen a product's customer base is large and diverse. In this scenario, existing\nprofiling approaches, such as operational profiling, are difficult to apply. In\nthis work, we consider publicly available video tutorials of a product to\nprofile usage. Our goal is to construct an automatic approach to extract\ninformation about user actions from instructional videos. To achieve this goal,\nwe use a Deep Convolutional Neural Network (DCNN) to recognize user actions.\nOur pilot study shows that a DCNN trained to recognize user actions in video\ncan classify five different actions in a collection of 236 publicly available\nMicrosoft Word tutorial videos (published on YouTube). In our empirical\nevaluation we report a mean average precision of 94.42% across all actions.\nThis study demonstrates the efficacy of DCNN-based methods for extracting\nsoftware usage information from videos. Moreover, this approach may aid in\nother software engineering activities that require information about customer\nusage of a product. \n\n"}
{"id": "1702.08259", "contents": "Title: Adaptive Ensemble Prediction for Deep Neural Networks based on\n  Confidence Level Abstract: Ensembling multiple predictions is a widely used technique for improving the\naccuracy of various machine learning tasks. One obvious drawback of ensembling\nis its higher execution cost during inference. In this paper, we first describe\nour insights on the relationship between the probability of prediction and the\neffect of ensembling with current deep neural networks; ensembling does not\nhelp mispredictions for inputs predicted with a high probability even when\nthere is a non-negligible number of mispredicted inputs. This finding motivated\nus to develop a way to adaptively control the ensembling. If the prediction for\nan input reaches a high enough probability, i.e., the output from the softmax\nfunction, on the basis of the confidence level, we stop ensembling for this\ninput to avoid wasting computation power. We evaluated the adaptive ensembling\nby using various datasets and showed that it reduces the computation cost\nsignificantly while achieving accuracy similar to that of static ensembling\nusing a pre-defined number of local predictions. We also show that our\nstatistically rigorous confidence-level-based early-exit condition reduces the\nburden of task-dependent threshold tuning better compared with naive early exit\nbased on a pre-defined threshold in addition to yielding a better accuracy with\nthe same cost. \n\n"}
{"id": "1702.08434", "contents": "Title: Skin Lesion Classification Using Hybrid Deep Neural Networks Abstract: Skin cancer is one of the major types of cancers with an increasing incidence\nover the past decades. Accurately diagnosing skin lesions to discriminate\nbetween benign and malignant skin lesions is crucial to ensure appropriate\npatient treatment. While there are many computerised methods for skin lesion\nclassification, convolutional neural networks (CNNs) have been shown to be\nsuperior over classical methods. In this work, we propose a fully automatic\ncomputerised method for skin lesion classification which employs optimised deep\nfeatures from a number of well-established CNNs and from different abstraction\nlevels. We use three pre-trained deep models, namely AlexNet, VGG16 and\nResNet-18, as deep feature generators. The extracted features then are used to\ntrain support vector machine classifiers. In the final stage, the classifier\noutputs are fused to obtain a classification. Evaluated on the 150 validation\nimages from the ISIC 2017 classification challenge, the proposed method is\nshown to achieve very good classification performance, yielding an area under\nreceiver operating characteristic curve of 83.83% for melanoma classification\nand of 97.55% for seborrheic keratosis classification. \n\n"}
{"id": "1702.08740", "contents": "Title: Weakly- and Semi-Supervised Object Detection with\n  Expectation-Maximization Algorithm Abstract: Object detection when provided image-level labels instead of instance-level\nlabels (i.e., bounding boxes) during training is an important problem in\ncomputer vision, since large scale image datasets with instance-level labels\nare extremely costly to obtain. In this paper, we address this challenging\nproblem by developing an Expectation-Maximization (EM) based object detection\nmethod using deep convolutional neural networks (CNNs). Our method is\napplicable to both the weakly-supervised and semi-supervised settings.\nExtensive experiments on PASCAL VOC 2007 benchmark show that (1) in the weakly\nsupervised setting, our method provides significant detection performance\nimprovement over current state-of-the-art methods, (2) having access to a small\nnumber of strongly (instance-level) annotated images, our method can almost\nmatch the performace of the fully supervised Fast RCNN. We share our source\ncode at https://github.com/ZiangYan/EM-WSD. \n\n"}
{"id": "1703.00035", "contents": "Title: Context-Sensitive Super-Resolution for Fast Fetal Magnetic Resonance\n  Imaging Abstract: 3D Magnetic Resonance Imaging (MRI) is often a trade-off between fast but\nlow-resolution image acquisition and highly detailed but slow image\nacquisition. Fast imaging is required for targets that move to avoid motion\nartefacts. This is in particular difficult for fetal MRI. Spatially independent\nupsampling techniques, which are the state-of-the-art to address this problem,\nare error prone and disregard contextual information. In this paper we propose\na context-sensitive upsampling method based on a residual convolutional neural\nnetwork model that learns organ specific appearance and adopts semantically to\ninput data allowing for the generation of high resolution images with sharp\nedges and fine scale detail. By making contextual decisions about appearance\nand shape, present in different parts of an image, we gain a maximum of\nstructural detail at a similar contrast as provided by high-resolution data. We\nexperiment on $145$ fetal scans and show that our approach yields an increased\nPSNR of $1.25$ $dB$ when applied to under-sampled fetal data \\emph{cf.}\nbaseline upsampling. Furthermore, our method yields an increased PSNR of $1.73$\n$dB$ when utilizing under-sampled fetal data to perform brain volume\nreconstruction on motion corrupted captured data. \n\n"}
{"id": "1703.00792", "contents": "Title: Robust Spatial Filtering with Graph Convolutional Neural Networks Abstract: Convolutional Neural Networks (CNNs) have recently led to incredible\nbreakthroughs on a variety of pattern recognition problems. Banks of finite\nimpulse response filters are learned on a hierarchy of layers, each\ncontributing more abstract information than the previous layer. The simplicity\nand elegance of the convolutional filtering process makes them perfect for\nstructured problems such as image, video, or voice, where vertices are\nhomogeneous in the sense of number, location, and strength of neighbors. The\nvast majority of classification problems, for example in the pharmaceutical,\nhomeland security, and financial domains are unstructured. As these problems\nare formulated into unstructured graphs, the heterogeneity of these problems,\nsuch as number of vertices, number of connections per vertex, and edge\nstrength, cannot be tackled with standard convolutional techniques. We propose\na novel neural learning framework that is capable of handling both homogeneous\nand heterogeneous data, while retaining the benefits of traditional CNN\nsuccesses.\n  Recently, researchers have proposed variations of CNNs that can handle graph\ndata. In an effort to create learnable filter banks of graphs, these methods\neither induce constraints on the data or require preprocessing. As opposed to\nspectral methods, our framework, which we term Graph-CNNs, defines filters as\npolynomials of functions of the graph adjacency matrix. Graph-CNNs can handle\nboth heterogeneous and homogeneous graph data, including graphs having entirely\ndifferent vertex or edge sets. We perform experiments to validate the\napplicability of Graph-CNNs to a variety of structured and unstructured\nclassification problems and demonstrate state-of-the-art results on document\nand molecule classification problems. \n\n"}
{"id": "1703.00832", "contents": "Title: On the Reconstruction of Face Images from Deep Face Templates Abstract: State-of-the-art face recognition systems are based on deep (convolutional)\nneural networks. Therefore, it is imperative to determine to what extent face\ntemplates derived from deep networks can be inverted to obtain the original\nface image. In this paper, we study the vulnerabilities of a state-of-the-art\nface recognition system based on template reconstruction attack. We propose a\nneighborly de-convolutional neural network (\\textit{NbNet}) to reconstruct face\nimages from their deep templates. In our experiments, we assumed that no\nknowledge about the target subject and the deep network are available. To train\nthe \\textit{NbNet} reconstruction models, we augmented two benchmark face\ndatasets (VGG-Face and Multi-PIE) with a large collection of images synthesized\nusing a face generator. The proposed reconstruction was evaluated using type-I\n(comparing the reconstructed images against the original face images used to\ngenerate the deep template) and type-II (comparing the reconstructed images\nagainst a different face image of the same subject) attacks. Given the images\nreconstructed from \\textit{NbNets}, we show that for verification, we achieve\nTAR of 95.20\\% (58.05\\%) on LFW under type-I (type-II) attacks @ FAR of 0.1\\%.\nBesides, 96.58\\% (92.84\\%) of the images reconstruction from templates of\npartition \\textit{fa} (\\textit{fb}) can be identified from partition\n\\textit{fa} in color FERET. Our study demonstrates the need to secure deep\ntemplates in face recognition systems. \n\n"}
{"id": "1703.00868", "contents": "Title: Using Synthetic Data to Train Neural Networks is Model-Based Reasoning Abstract: We draw a formal connection between using synthetic training data to optimize\nneural network parameters and approximate, Bayesian, model-based reasoning. In\nparticular, training a neural network using synthetic data can be viewed as\nlearning a proposal distribution generator for approximate inference in the\nsynthetic-data generative model. We demonstrate this connection in a\nrecognition task where we develop a novel Captcha-breaking architecture and\ntrain it using synthetic data, demonstrating both state-of-the-art performance\nand a way of computing task-specific posterior uncertainty. Using a neural\nnetwork trained this way, we also demonstrate successful breaking of real-world\nCaptchas currently used by Facebook and Wikipedia. Reasoning from these\nempirical results and drawing connections with Bayesian modeling, we discuss\nthe robustness of synthetic data results and suggest important considerations\nfor ensuring good neural network generalization when training with synthetic\ndata. \n\n"}
{"id": "1703.01725", "contents": "Title: Cats and Captions vs. Creators and the Clock: Comparing Multimodal\n  Content to Context in Predicting Relative Popularity Abstract: The content of today's social media is becoming more and more rich,\nincreasingly mixing text, images, videos, and audio. It is an intriguing\nresearch question to model the interplay between these different modes in\nattracting user attention and engagement. But in order to pursue this study of\nmultimodal content, we must also account for context: timing effects, community\npreferences, and social factors (e.g., which authors are already popular) also\naffect the amount of feedback and reaction that social-media posts receive. In\nthis work, we separate out the influence of these non-content factors in\nseveral ways. First, we focus on ranking pairs of submissions posted to the\nsame community in quick succession, e.g., within 30 seconds, this framing\nencourages models to focus on time-agnostic and community-specific content\nfeatures. Within that setting, we determine the relative performance of author\nvs. content features. We find that victory usually belongs to \"cats and\ncaptions,\" as visual and textual features together tend to outperform\nidentity-based features. Moreover, our experiments show that when considered in\nisolation, simple unigram text features and deep neural network visual features\nyield the highest accuracy individually, and that the combination of the two\nmodalities generally leads to the best accuracies overall. \n\n"}
{"id": "1703.01976", "contents": "Title: Incorporating the Knowledge of Dermatologists to Convolutional Neural\n  Networks for the Diagnosis of Skin Lesions Abstract: This report describes our submission to the ISIC 2017 Challenge in Skin\nLesion Analysis Towards Melanoma Detection. We have participated in the Part 3:\nLesion Classification with a system for automatic diagnosis of nevus, melanoma\nand seborrheic keratosis. Our approach aims to incorporate the expert knowledge\nof dermatologists into the well known framework of Convolutional Neural\nNetworks (CNN), which have shown impressive performance in many visual\nrecognition tasks. In particular, we have designed several networks providing\nlesion area identification, lesion segmentation into structural patterns and\nfinal diagnosis of clinical cases. Furthermore, novel blocks for CNNs have been\ndesigned to integrate this information with the diagnosis processing pipeline. \n\n"}
{"id": "1703.02910", "contents": "Title: Deep Bayesian Active Learning with Image Data Abstract: Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task). \n\n"}
{"id": "1703.03940", "contents": "Title: A 3D Object Detection and Pose Estimation Pipeline Using RGB-D Images Abstract: 3D object detection and pose estimation has been studied extensively in\nrecent decades for its potential applications in robotics. However, there still\nremains challenges when we aim at detecting multiple objects while retaining\nlow false positive rate in cluttered environments. This paper proposes a robust\n3D object detection and pose estimation pipeline based on RGB-D images, which\ncan detect multiple objects simultaneously while reducing false positives.\nDetection begins with template matching and yields a set of template matches. A\nclustering algorithm then groups templates of similar spatial location and\nproduces multiple-object hypotheses. A scoring function evaluates the\nhypotheses using their associated templates and non-maximum suppression is\nadopted to remove duplicate results based on the scores. Finally, a combination\nof point cloud processing algorithms are used to compute objects' 3D poses.\nExisting object hypotheses are verified by computing the overlap between model\nand scene points. Experiments demonstrate that our approach provides\ncompetitive results comparable to the state-of-the-arts and can be applied to\nrobot random bin-picking. \n\n"}
{"id": "1703.04028", "contents": "Title: Contractions of Representations and Algebraic Families of Harish-Chandra\n  Modules Abstract: We examine from an algebraic point of view some families of unitary group\nrepresentations that arise in mathematical physics and are associated to\ncontraction families of Lie groups. The contraction families of groups relate\ndifferent real forms of a reductive group and are continuously parametrized,\nbut the unitary representations are defined over a parameter subspace that\nincludes both discrete and continuous parts. Both finite- and\ninfinite-dimensional representations can occur, even within the same family. We\nshall study the simplest nontrivial examples, and use the concepts of algebraic\nfamilies of Harish-Chandra pairs and Harish-Chandra modules, introduced in a\nprevious paper, together with the Jantzen filtration, to construct these\nfamilies of unitary representations algebraically. \n\n"}
{"id": "1703.04678", "contents": "Title: On a new avatar of the sine-Gordon equation Abstract: A chain of transformations is found which relates one new integrable case of\nthe generalized short pulse equation of Hone, Novikov and Wang\n[arXiv:1612.02481] with the sine-Gordon equation. \n\n"}
{"id": "1703.05289", "contents": "Title: A clever elimination strategy for efficient minimal solvers Abstract: We present a new insight into the systematic generation of minimal solvers in\ncomputer vision, which leads to smaller and faster solvers. Many minimal\nproblem formulations are coupled sets of linear and polynomial equations where\nimage measurements enter the linear equations only. We show that it is useful\nto solve such systems by first eliminating all the unknowns that do not appear\nin the linear equations and then extending solutions to the rest of unknowns.\nThis can be generalized to fully non-linear systems by linearization via\nlifting. We demonstrate that this approach leads to more efficient solvers in\nthree problems of partially calibrated relative camera pose computation with\nunknown focal length and/or radial distortion. Our approach also generates new\ninteresting constraints on the fundamental matrices of partially calibrated\ncameras, which were not known before. \n\n"}
{"id": "1703.07217", "contents": "Title: On Lie algebras responsible for zero-curvature representations of\n  multicomponent (1+1)-dimensional evolution PDEs Abstract: Zero-curvature representations (ZCRs) are one of the main tools in the theory\nof integrable $(1+1)$-dimensional PDEs. According to the preprint\narXiv:1212.2199, for any given $(1+1)$-dimensional evolution PDE one can define\na sequence of Lie algebras $F^p$, $p=0,1,2,3,\\dots$, such that representations\nof these algebras classify all ZCRs of the PDE up to local gauge equivalence.\nZCRs depending on derivatives of arbitrary finite order are allowed.\nFurthermore, these algebras provide necessary conditions for existence of\nBacklund transformations between two given PDEs. The algebras $F^p$ are defined\nin arXiv:1212.2199 in terms of generators and relations.\n  In the present paper, we describe some methods to study the structure of the\nalgebras $F^p$ for multicomponent $(1+1)$-dimensional evolution PDEs. Using\nthese methods, we compute the explicit structure (up to non-essential nilpotent\nideals) of the Lie algebras $F^p$ for the Landau-Lifshitz, nonlinear\nSchrodinger equations, and for the $n$-component Landau-Lifshitz system of\nGolubchik and Sokolov for any $n>3$. In particular, this means that for the\n$n$-component Landau-Lifshitz system we classify all ZCRs (depending on\nderivatives of arbitrary finite order), up to local gauge equivalence and up to\nkilling nilpotent ideals in the corresponding Lie algebras.\n  The presented methods to classify ZCRs can be applied also to other\n$(1+1)$-dimensional evolution PDEs. Furthermore, the obtained results can be\nused for proving non-existence of Backlund transformations between some PDEs,\nwhich will be described in forthcoming publications. \n\n"}
{"id": "1703.07655", "contents": "Title: ASP: Learning to Forget with Adaptive Synaptic Plasticity in Spiking\n  Neural Networks Abstract: A fundamental feature of learning in animals is the \"ability to forget\" that\nallows an organism to perceive, model and make decisions from disparate streams\nof information and adapt to changing environments. Against this backdrop, we\npresent a novel unsupervised learning mechanism ASP (Adaptive Synaptic\nPlasticity) for improved recognition with Spiking Neural Networks (SNNs) for\nreal time on-line learning in a dynamic environment. We incorporate an adaptive\nweight decay mechanism with the traditional Spike Timing Dependent Plasticity\n(STDP) learning to model adaptivity in SNNs. The leak rate of the synaptic\nweights is modulated based on the temporal correlation between the spiking\npatterns of the pre- and post-synaptic neurons. This mechanism helps in gradual\nforgetting of insignificant data while retaining significant, yet old,\ninformation. ASP, thus, maintains a balance between forgetting and immediate\nlearning to construct a stable-plastic self-adaptive SNN for continuously\nchanging inputs. We demonstrate that the proposed learning methodology\naddresses catastrophic forgetting while yielding significantly improved\naccuracy over the conventional STDP learning method for digit recognition\napplications. Additionally, we observe that the proposed learning model\nautomatically encodes selective attention towards relevant features in the\ninput data while eliminating the influence of background noise (or denoising)\nfurther improving the robustness of the ASP learning. \n\n"}
{"id": "1703.07910", "contents": "Title: Bidirectional-Convolutional LSTM Based Spectral-Spatial Feature Learning\n  for Hyperspectral Image Classification Abstract: This paper proposes a novel deep learning framework named\nbidirectional-convolutional long short term memory (Bi-CLSTM) network to\nautomatically learn the spectral-spatial feature from hyperspectral images\n(HSIs). In the network, the issue of spectral feature extraction is considered\nas a sequence learning problem, and a recurrent connection operator across the\nspectral domain is used to address it. Meanwhile, inspired from the widely used\nconvolutional neural network (CNN), a convolution operator across the spatial\ndomain is incorporated into the network to extract the spatial feature.\nBesides, to sufficiently capture the spectral information, a bidirectional\nrecurrent connection is proposed. In the classification phase, the learned\nfeatures are concatenated into a vector and fed to a softmax classifier via a\nfully-connected operator. To validate the effectiveness of the proposed\nBi-CLSTM framework, we compare it with several state-of-the-art methods,\nincluding the CNN framework, on three widely used HSIs. The obtained results\nshow that Bi-CLSTM can improve the classification performance as compared to\nother methods. \n\n"}
{"id": "1703.08001", "contents": "Title: Nonlinear Spectral Image Fusion Abstract: In this paper we demonstrate that the framework of nonlinear spectral\ndecompositions based on total variation (TV) regularization is very well suited\nfor image fusion as well as more general image manipulation tasks. The\nwell-localized and edge-preserving spectral TV decomposition allows to select\nfrequencies of a certain image to transfer particular features, such as\nwrinkles in a face, from one image to another. We illustrate the effectiveness\nof the proposed approach in several numerical experiments, including a\ncomparison to the competing techniques of Poisson image editing, linear\nosmosis, wavelet fusion and Laplacian pyramid fusion. We conclude that the\nproposed spectral TV image decomposition framework is a valuable tool for semi-\nand fully-automatic image editing and fusion. \n\n"}
{"id": "1703.08580", "contents": "Title: Deep Residual Learning for Instrument Segmentation in Robotic Surgery Abstract: Detection, tracking, and pose estimation of surgical instruments are crucial\ntasks for computer assistance during minimally invasive robotic surgery. In the\nmajority of cases, the first step is the automatic segmentation of surgical\ntools. Prior work has focused on binary segmentation, where the objective is to\nlabel every pixel in an image as tool or background. We improve upon previous\nwork in two major ways. First, we leverage recent techniques such as deep\nresidual learning and dilated convolutions to advance binary-segmentation\nperformance. Second, we extend the approach to multi-class segmentation, which\nlets us segment different parts of the tool, in addition to background. We\ndemonstrate the performance of this method on the MICCAI Endoscopic Vision\nChallenge Robotic Instruments dataset. \n\n"}
{"id": "1703.08893", "contents": "Title: Transductive Zero-Shot Learning with a Self-training dictionary approach Abstract: As an important and challenging problem in computer vision, zero-shot\nlearning (ZSL) aims at automatically recognizing the instances from unseen\nobject classes without training data. To address this problem, ZSL is usually\ncarried out in the following two aspects: 1) capturing the domain distribution\nconnections between seen classes data and unseen classes data; and 2) modeling\nthe semantic interactions between the image feature space and the label\nembedding space. Motivated by these observations, we propose a bidirectional\nmapping based semantic relationship modeling scheme that seeks for crossmodal\nknowledge transfer by simultaneously projecting the image features and label\nembeddings into a common latent space. Namely, we have a bidirectional\nconnection relationship that takes place from the image feature space to the\nlatent space as well as from the label embedding space to the latent space. To\ndeal with the domain shift problem, we further present a transductive learning\napproach that formulates the class prediction problem in an iterative refining\nprocess, where the object classification capacity is progressively reinforced\nthrough bootstrapping-based model updating over highly reliable instances.\nExperimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate\nthe effectiveness of the proposed approach against the state-of-the-art\napproaches. \n\n"}
{"id": "1703.09856", "contents": "Title: Automatic Detection of Knee Joints and Quantification of Knee\n  Osteoarthritis Severity using Convolutional Neural Networks Abstract: This paper introduces a new approach to automatically quantify the severity\nof knee OA using X-ray images. Automatically quantifying knee OA severity\ninvolves two steps: first, automatically localizing the knee joints; next,\nclassifying the localized knee joint images. We introduce a new approach to\nautomatically detect the knee joints using a fully convolutional neural network\n(FCN). We train convolutional neural networks (CNN) from scratch to\nautomatically quantify the knee OA severity optimizing a weighted ratio of two\nloss functions: categorical cross-entropy and mean-squared loss. This joint\ntraining further improves the overall quantification of knee OA severity, with\nthe added benefit of naturally producing simultaneous multi-class\nclassification and regression outputs. Two public datasets are used to evaluate\nour approach, the Osteoarthritis Initiative (OAI) and the Multicenter\nOsteoarthritis Study (MOST), with extremely promising results that outperform\nexisting approaches. \n\n"}
{"id": "1703.10106", "contents": "Title: Pose-conditioned Spatio-Temporal Attention for Human Action Recognition Abstract: We address human action recognition from multi-modal video data involving\narticulated pose and RGB frames and propose a two-stream approach. The pose\nstream is processed with a convolutional model taking as input a 3D tensor\nholding data from a sub-sequence. A specific joint ordering, which respects the\ntopology of the human body, ensures that different convolutional layers\ncorrespond to meaningful levels of abstraction. The raw RGB stream is handled\nby a spatio-temporal soft-attention mechanism conditioned on features from the\npose network. An LSTM network receives input from a set of image locations at\neach instant. A trainable glimpse sensor extracts features on a set of\npredefined locations specified by the pose stream, namely the 4 hands of the\ntwo people involved in the activity. Appearance features give important cues on\nhand motion and on objects held in each hand. We show that it is of high\ninterest to shift the attention to different hands at different time steps\ndepending on the activity itself. Finally a temporal attention mechanism learns\nhow to fuse LSTM features over time. We evaluate the method on 3 datasets.\nState-of-the-art results are achieved on the largest dataset for human activity\nrecognition, namely NTU-RGB+D, as well as on the SBU Kinect Interaction\ndataset. Performance close to state-of-the-art is achieved on the smaller MSR\nDaily Activity 3D dataset. \n\n"}
{"id": "1703.10114", "contents": "Title: Improved Lossy Image Compression with Priming and Spatially Adaptive Bit\n  Rates for Recurrent Networks Abstract: We propose a method for lossy image compression based on recurrent,\nconvolutional neural networks that outperforms BPG (4:2:0 ), WebP, JPEG2000,\nand JPEG as measured by MS-SSIM. We introduce three improvements over previous\nresearch that lead to this state-of-the-art result. First, we show that\ntraining with a pixel-wise loss weighted by SSIM increases reconstruction\nquality according to several metrics. Second, we modify the recurrent\narchitecture to improve spatial diffusion, which allows the network to more\neffectively capture and propagate image information through the network's\nhidden state. Finally, in addition to lossless entropy coding, we use a\nspatially adaptive bit allocation algorithm to more efficiently use the limited\nnumber of bits to encode visually complex image regions. We evaluate our method\non the Kodak and Tecnick image sets and compare against standard codecs as well\nrecently published methods based on deep neural networks. \n\n"}
{"id": "1703.10664", "contents": "Title: Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos Abstract: Deep learning has been demonstrated to achieve excellent results for image\nclassification and object detection. However, the impact of deep learning on\nvideo analysis (e.g. action detection and recognition) has been limited due to\ncomplexity of video data and lack of annotations. Previous convolutional neural\nnetworks (CNN) based video action detection approaches usually consist of two\nmajor steps: frame-level action proposal detection and association of proposals\nacross frames. Also, these methods employ two-stream CNN framework to handle\nspatial and temporal feature separately. In this paper, we propose an\nend-to-end deep network called Tube Convolutional Neural Network (T-CNN) for\naction detection in videos. The proposed architecture is a unified network that\nis able to recognize and localize action based on 3D convolution features. A\nvideo is first divided into equal length clips and for each clip a set of tube\nproposals are generated next based on 3D Convolutional Network (ConvNet)\nfeatures. Finally, the tube proposals of different clips are linked together\nemploying network flow and spatio-temporal action detection is performed using\nthese linked video proposals. Extensive experiments on several video datasets\ndemonstrate the superior performance of T-CNN for classifying and localizing\nactions in both trimmed and untrimmed videos compared to state-of-the-arts. \n\n"}
{"id": "1704.00509", "contents": "Title: Truncating Wide Networks using Binary Tree Architectures Abstract: Recent study shows that a wide deep network can obtain accuracy comparable to\na deeper but narrower network. Compared to narrower and deeper networks, wide\nnetworks employ relatively less number of layers and have various important\nbenefits, such that they have less running time on parallel computing devices,\nand they are less affected by gradient vanishing problems. However, the\nparameter size of a wide network can be very large due to use of large width of\neach layer in the network. In order to keep the benefits of wide networks\nmeanwhile improve the parameter size and accuracy trade-off of wide networks,\nwe propose a binary tree architecture to truncate architecture of wide networks\nby reducing the width of the networks. More precisely, in the proposed\narchitecture, the width is continuously reduced from lower layers to higher\nlayers in order to increase the expressive capacity of network with a less\nincrease on parameter size. Also, to ease the gradient vanishing problem,\nfeatures obtained at different layers are concatenated to form the output of\nour architecture. By employing the proposed architecture on a baseline wide\nnetwork, we can construct and train a new network with same depth but\nconsiderably less number of parameters. In our experimental analyses, we\nobserve that the proposed architecture enables us to obtain better parameter\nsize and accuracy trade-off compared to baseline networks using various\nbenchmark image classification datasets. The results show that our model can\ndecrease the classification error of baseline from 20.43% to 19.22% on\nCifar-100 using only 28% of parameters that baseline has. Code is available at\nhttps://github.com/ZhangVision/bitnet. \n\n"}
{"id": "1704.00524", "contents": "Title: Block-Matching Convolutional Neural Network for Image Denoising Abstract: There are two main streams in up-to-date image denoising algorithms:\nnon-local self similarity (NSS) prior based methods and convolutional neural\nnetwork (CNN) based methods. The NSS based methods are favorable on images with\nregular and repetitive patterns while the CNN based methods perform better on\nirregular structures. In this paper, we propose a block-matching convolutional\nneural network (BMCNN) method that combines NSS prior and CNN. Initially,\nsimilar local patches in the input image are integrated into a 3D block. In\norder to prevent the noise from messing up the block matching, we first apply\nan existing denoising algorithm on the noisy image. The denoised image is\nemployed as a pilot signal for the block matching, and then denoising function\nfor the block is learned by a CNN structure. Experimental results show that the\nproposed BMCNN algorithm achieves state-of-the-art performance. In detail,\nBMCNN can restore both repetitive and irregular structures. \n\n"}
{"id": "1704.01152", "contents": "Title: Pose2Instance: Harnessing Keypoints for Person Instance Segmentation Abstract: Human keypoints are a well-studied representation of people.We explore how to\nuse keypoint models to improve instance-level person segmentation. The main\nidea is to harness the notion of a distance transform of oracle provided\nkeypoints or estimated keypoint heatmaps as a prior for person instance\nsegmentation task within a deep neural network. For training and evaluation, we\nconsider all those images from COCO where both instance segmentation and human\nkeypoints annotations are available. We first show how oracle keypoints can\nboost the performance of existing human segmentation model during inference\nwithout any training. Next, we propose a framework to directly learn a deep\ninstance segmentation model conditioned on human pose. Experimental results\nshow that at various Intersection Over Union (IOU) thresholds, in a constrained\nenvironment with oracle keypoints, the instance segmentation accuracy achieves\n10% to 12% relative improvements over a strong baseline of oracle bounding\nboxes. In a more realistic environment, without the oracle keypoints, the\nproposed deep person instance segmentation model conditioned on human pose\nachieves 3.8% to 10.5% relative improvements comparing with its strongest\nbaseline of a deep network trained only for segmentation. \n\n"}
{"id": "1704.01262", "contents": "Title: Investigating Human Factors in Image Forgery Detection Abstract: In today's age of internet and social media, one can find an enormous volume\nof forged images on-line. These images have been used in the past to convey\nfalsified information and achieve harmful intentions. The spread and the effect\nof the social media only makes this problem more severe. While creating forged\nimages has become easier due to software advancements, there is no automated\nalgorithm which can reliably detect forgery.\n  Image forgery detection can be seen as a subset of image understanding\nproblem. Human performance is still the gold-standard for these type of\nproblems when compared to existing state-of-art automated algorithms. We\nconduct a subjective evaluation test with the aid of eye-tracker to investigate\ninto human factors associated with this problem. We compare the performance of\nan automated algorithm and humans for forgery detection problem. We also\ndevelop an algorithm which uses the data from the evaluation test to predict\nthe difficulty-level of an image (the difficulty-level of an image here denotes\nhow difficult it is for humans to detect forgery in an image. Terms such as\n\"Easy/difficult image\" will be used in the same context). The experimental\nresults presented in this paper should facilitate development of better\nalgorithms in the future. \n\n"}
{"id": "1704.02071", "contents": "Title: Convolutional Neural Pyramid for Image Processing Abstract: We propose a principled convolutional neural pyramid (CNP) framework for\ngeneral low-level vision and image processing tasks. It is based on the\nessential finding that many applications require large receptive fields for\nstructure understanding. But corresponding neural networks for regression\neither stack many layers or apply large kernels to achieve it, which is\ncomputationally very costly. Our pyramid structure can greatly enlarge the\nfield while not sacrificing computation efficiency. Extra benefit includes\nadaptive network depth and progressive upsampling for quasi-realtime testing on\nVGA-size input. Our method profits a broad set of applications, such as\ndepth/RGB image restoration, completion, noise/artifact removal, edge\nrefinement, image filtering, image enhancement and colorization. \n\n"}
{"id": "1704.02386", "contents": "Title: Pixelwise Instance Segmentation with a Dynamically Instantiated Network Abstract: Semantic segmentation and object detection research have recently achieved\nrapid progress. However, the former task has no notion of different instances\nof the same object, and the latter operates at a coarse, bounding-box level. We\npropose an Instance Segmentation system that produces a segmentation map where\neach pixel is assigned an object class and instance identity label. Most\napproaches adapt object detectors to produce segments instead of boxes. In\ncontrast, our method is based on an initial semantic segmentation module, which\nfeeds into an instance subnetwork. This subnetwork uses the initial\ncategory-level segmentation, along with cues from the output of an object\ndetector, within an end-to-end CRF to predict instances. This part of our model\nis dynamically instantiated to produce a variable number of instances per\nimage. Our end-to-end approach requires no post-processing and considers the\nimage holistically, instead of processing independent proposals. Therefore,\nunlike some related work, a pixel cannot belong to multiple instances.\nFurthermore, far more precise segmentations are achieved, as shown by our\nstate-of-the-art results (particularly at high IoU thresholds) on the Pascal\nVOC and Cityscapes datasets. \n\n"}
{"id": "1704.03549", "contents": "Title: Attention-based Extraction of Structured Information from Street View\n  Imagery Abstract: We present a neural network model - based on CNNs, RNNs and a novel attention\nmechanism - which achieves 84.2% accuracy on the challenging French Street Name\nSigns (FSNS) dataset, significantly outperforming the previous state of the art\n(Smith'16), which achieved 72.46%. Furthermore, our new method is much simpler\nand more general than the previous approach. To demonstrate the generality of\nour model, we show that it also performs well on an even more challenging\ndataset derived from Google Street View, in which the goal is to extract\nbusiness names from store fronts. Finally, we study the speed/accuracy tradeoff\nthat results from using CNN feature extractors of different depths.\nSurprisingly, we find that deeper is not always better (in terms of accuracy,\nas well as speed). Our resulting model is simple, accurate and fast, allowing\nit to be used at scale on a variety of challenging real-world text extraction\nproblems. \n\n"}
{"id": "1704.03593", "contents": "Title: Reformulating Level Sets as Deep Recurrent Neural Network Approach to\n  Semantic Segmentation Abstract: Variational Level Set (LS) has been a widely used method in medical\nsegmentation. However, it is limited when dealing with multi-instance objects\nin the real world. In addition, its segmentation results are quite sensitive to\ninitial settings and highly depend on the number of iterations. To address\nthese issues and boost the classic variational LS methods to a new level of the\nlearnable deep learning approaches, we propose a novel definition of contour\nevolution named Recurrent Level Set (RLS)} to employ Gated Recurrent Unit under\nthe energy minimization of a variational LS functional. The curve deformation\nprocess in RLS is formed as a hidden state evolution procedure and updated by\nminimizing an energy functional composed of fitting forces and contour length.\nBy sharing the convolutional features in a fully end-to-end trainable\nframework, we extend RLS to Contextual RLS (CRLS) to address semantic\nsegmentation in the wild. The experimental results have shown that our proposed\nRLS improves both computational time and segmentation accuracy against the\nclassic variations LS-based method, whereas the fully end-to-end system CRLS\nachieves competitive performance compared to the state-of-the-art semantic\nsegmentation approaches. \n\n"}
{"id": "1704.04116", "contents": "Title: A variational approach to probing extreme events in turbulent dynamical\n  systems Abstract: Extreme events are ubiquitous in a wide range of dynamical systems, including\nturbulent fluid flows, nonlinear waves, large scale networks and biological\nsystems. Here, we propose a variational framework for probing conditions that\ntrigger intermittent extreme events in high-dimensional nonlinear dynamical\nsystems. We seek the triggers as the probabilistically feasible solutions of an\nappropriately constrained optimization problem, where the function to be\nmaximized is a system observable exhibiting intermittent extreme bursts. The\nconstraints are imposed to ensure the physical admissibility of the optimal\nsolutions, i.e., significant probability for their occurrence under the natural\nflow of the dynamical system. We apply the method to a body-forced\nincompressible Navier--Stokes equation, known as the Kolmogorov flow. We find\nthat the intermittent bursts of the energy dissipation are independent of the\nexternal forcing and are instead caused by the spontaneous transfer of energy\nfrom large scales to the mean flow via nonlinear triad interactions. The global\nmaximizer of the corresponding variational problem identifies the responsible\ntriad, hence providing a precursor for the occurrence of extreme dissipation\nevents. Specifically, monitoring the energy transfers within this triad, allows\nus to develop a data-driven short-term predictor for the intermittent bursts of\nenergy dissipation. We assess the performance of this predictor through direct\nnumerical simulations. \n\n"}
{"id": "1704.04886", "contents": "Title: Multi-View Image Generation from a Single-View Abstract: This paper addresses a challenging problem -- how to generate multi-view\ncloth images from only a single view input. To generate realistic-looking\nimages with different views from the input, we propose a new image generation\nmodel termed VariGANs that combines the strengths of the variational inference\nand the Generative Adversarial Networks (GANs). Our proposed VariGANs model\ngenerates the target image in a coarse-to-fine manner instead of a single pass\nwhich suffers from severe artifacts. It first performs variational inference to\nmodel global appearance of the object (e.g., shape and color) and produce a\ncoarse image with a different view. Conditioned on the generated low resolution\nimages, it then proceeds to perform adversarial learning to fill details and\ngenerate images of consistent details with the input. Extensive experiments\nconducted on two clothing datasets, MVC and DeepFashion, have demonstrated that\nimages of a novel view generated by our model are more plausible than those\ngenerated by existing approaches, in terms of more consistent global appearance\nas well as richer and sharper details. \n\n"}
{"id": "1704.05051", "contents": "Title: Google's Cloud Vision API Is Not Robust To Noise Abstract: Google has recently introduced the Cloud Vision API for image analysis.\nAccording to the demonstration website, the API \"quickly classifies images into\nthousands of categories, detects individual objects and faces within images,\nand finds and reads printed words contained within images.\" It can be also used\nto \"detect different types of inappropriate content from adult to violent\ncontent.\"\n  In this paper, we evaluate the robustness of Google Cloud Vision API to input\nperturbation. In particular, we show that by adding sufficient noise to the\nimage, the API generates completely different outputs for the noisy image,\nwhile a human observer would perceive its original content. We show that the\nattack is consistently successful, by performing extensive experiments on\ndifferent image types, including natural images, images containing faces and\nimages with texts. For instance, using images from ImageNet dataset, we found\nthat adding an average of 14.25% impulse noise is enough to deceive the API.\nOur findings indicate the vulnerability of the API in adversarial environments.\nFor example, an adversary can bypass an image filtering system by adding noise\nto inappropriate images. We then show that when a noise filter is applied on\ninput images, the API generates mostly the same outputs for restored images as\nfor original images. This observation suggests that cloud vision API can\nreadily benefit from noise filtering, without the need for updating image\nanalysis algorithms. \n\n"}
{"id": "1704.05716", "contents": "Title: A Critical Comparison of Lagrangian Methods for Coherent Structure\n  Detection Abstract: We review and test twelve different approaches to the detection of\nfinite-time coherent material structures in two-dimensional, temporally\naperiodic flows. We consider both mathematical methods and diagnostic scalar\nfields, comparing their performance on three benchmark examples: the\nquasiperiodically forced Bickley jet, a two-dimensional turbulence simulation,\nand an observational wind velocity field from Jupiter's atmosphere. A close\ninspection of the results reveals that the various methods often produce very\ndifferent predictions for coherent structures, once they are evaluated beyond\nheuristic visual assessment. As we find by passive advection of the coherent\nset candidates, false positives and negatives can be produced even by some of\nthe mathematically justified methods due to the ineffectiveness of their\nunderlying coherence principles in certain flow configurations. We summarize\nthe inferred strengths and weaknesses of each method, and make general\nrecommendations for minimal self-consistency requirements that any Lagrangian\ncoherence detection technique should satisfy. \n\n"}
{"id": "1704.08045", "contents": "Title: The loss surface of deep and wide neural networks Abstract: While the optimization problem behind deep neural networks is highly\nnon-convex, it is frequently observed in practice that training deep networks\nseems possible without getting stuck in suboptimal points. It has been argued\nthat this is the case as all local minima are close to being globally optimal.\nWe show that this is (almost) true, in fact almost all local minima are\nglobally optimal, for a fully connected network with squared loss and analytic\nactivation function given that the number of hidden units of one layer of the\nnetwork is larger than the number of training points and the network structure\nfrom this layer on is pyramidal. \n\n"}
{"id": "1704.08328", "contents": "Title: Face Identification and Clustering Abstract: In this thesis, we study two problems based on clustering algorithms. In the\nfirst problem, we study the role of visual attributes using an agglomerative\nclustering algorithm to whittle down the search area where the number of\nclasses is high to improve the performance of clustering. We observe that as we\nadd more attributes, the clustering performance increases overall. In the\nsecond problem, we study the role of clustering in aggregating templates in a\n1:N open set protocol using multi-shot video as a probe. We observe that by\nincreasing the number of clusters, the performance increases with respect to\nthe baseline and reaches a peak, after which increasing the number of clusters\ncauses the performance to degrade. Experiments are conducted using recently\nintroduced unconstrained IARPA Janus IJB-A, CS2, and CS3 face recognition\ndatasets. \n\n"}
{"id": "1705.00534", "contents": "Title: Single image depth estimation by dilated deep residual convolutional\n  neural network and soft-weight-sum inference Abstract: This paper proposes a new residual convolutional neural network (CNN)\narchitecture for single image depth estimation. Compared with existing deep CNN\nbased methods, our method achieves much better results with fewer training\nexamples and model parameters. The advantages of our method come from the usage\nof dilated convolution, skip connection architecture and soft-weight-sum\ninference. Experimental evaluation on the NYU Depth V2 dataset shows that our\nmethod outperforms other state-of-the-art methods by a margin. \n\n"}
{"id": "1705.00754", "contents": "Title: Dense-Captioning Events in Videos Abstract: Most natural videos contain numerous events. For example, in a video of a\n\"man playing a piano\", the video might also contain \"another man dancing\" or \"a\ncrowd clapping\". We introduce the task of dense-captioning events, which\ninvolves both detecting and describing events in a video. We propose a new\nmodel that is able to identify all events in a single pass of the video while\nsimultaneously describing the detected events with natural language. Our model\nintroduces a variant of an existing proposal module that is designed to capture\nboth short as well as long events that span minutes. To capture the\ndependencies between the events in a video, our model introduces a new\ncaptioning module that uses contextual information from past and future events\nto jointly describe all events. We also introduce ActivityNet Captions, a\nlarge-scale benchmark for dense-captioning events. ActivityNet Captions\ncontains 20k videos amounting to 849 video hours with 100k total descriptions,\neach with it's unique start and end time. Finally, we report performances of\nour model for dense-captioning events, video retrieval and localization. \n\n"}
{"id": "1705.02050", "contents": "Title: Quantum torus algebras and B(C) type Toda systems Abstract: In this paper, we construct a new even constrained B(C) type Toda hierarchy\nand derive its B(C) type Block type additional symmetry. Also we generalize the\nB(C) type Toda hierarchy to the $N$-component B(C) type Toda hierarchy which is\nproved to have symmetries of a coupled $\\bigotimes^NQT_+ $ algebra ( $N$-folds\ndirect product of the positive half of the quantum torus algebra $QT$). \n\n"}
{"id": "1705.02101", "contents": "Title: TALL: Temporal Activity Localization via Language Query Abstract: This paper focuses on temporal localization of actions in untrimmed videos.\nExisting methods typically train classifiers for a pre-defined list of actions\nand apply them in a sliding window fashion. However, activities in the wild\nconsist of a wide combination of actors, actions and objects; it is difficult\nto design a proper activity list that meets users' needs. We propose to\nlocalize activities by natural language queries. Temporal Activity Localization\nvia Language (TALL) is challenging as it requires: (1) suitable design of text\nand video representations to allow cross-modal matching of actions and language\nqueries; (2) ability to locate actions accurately given features from sliding\nwindows of limited granularity. We propose a novel Cross-modal Temporal\nRegression Localizer (CTRL) to jointly model text query and video clips, output\nalignment scores and action boundary regression results for candidate clips.\nFor evaluation, we adopt TaCoS dataset, and build a new dataset for this task\non top of Charades by adding sentence temporal annotations, called\nCharades-STA. We also build complex sentence queries in Charades-STA for test.\nExperimental results show that CTRL outperforms previous methods significantly\non both datasets. \n\n"}
{"id": "1705.04819", "contents": "Title: On reductions of the discrete Kadomtsev--Petviashvili-type equations Abstract: The reduction by restricting the spectral parameters $k$ and $k'$ on a\ngeneric algebraic curve of degree $\\mathcal{N}$ is performed for the discrete\nAKP, BKP and CKP equations, respectively. A variety of two-dimensional discrete\nintegrable systems possessing a more general solution structure arise from the\nreduction, and in each case a unified formula for generic positive integer\n$\\mathcal{N}\\geq 2$ is given to express the corresponding reduced integrable\nlattice equations. The obtained extended two-dimensional lattice models give\nrise to many important integrable partial difference equations as special\ndegenerations. Some new integrable lattice models such as the discrete\nSawada--Kotera, Kaup--Kupershmidt and Hirota--Satsuma equations in extended\nform are given as examples within the framework. \n\n"}
{"id": "1705.04838", "contents": "Title: Revisiting IM2GPS in the Deep Learning Era Abstract: Image geolocalization, inferring the geographic location of an image, is a\nchallenging computer vision problem with many potential applications. The\nrecent state-of-the-art approach to this problem is a deep image classification\napproach in which the world is spatially divided into cells and a deep network\nis trained to predict the correct cell for a given image. We propose to combine\nthis approach with the original Im2GPS approach in which a query image is\nmatched against a database of geotagged images and the location is inferred\nfrom the retrieved set. We estimate the geographic location of a query image by\napplying kernel density estimation to the locations of its nearest neighbors in\nthe reference database. Interestingly, we find that the best features for our\nretrieval task are derived from networks trained with classification loss even\nthough we do not use a classification approach at test time. Training with\nclassification loss outperforms several deep feature learning methods (e.g.\nSiamese networks with contrastive of triplet loss) more typical for retrieval\napplications. Our simple approach achieves state-of-the-art geolocalization\naccuracy while also requiring significantly less training data. \n\n"}
{"id": "1705.07485", "contents": "Title: Shake-Shake regularization Abstract: The method introduced in this paper aims at helping deep learning\npractitioners faced with an overfit problem. The idea is to replace, in a\nmulti-branch network, the standard summation of parallel branches with a\nstochastic affine combination. Applied to 3-branch residual networks,\nshake-shake regularization improves on the best single shot published results\non CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%.\nExperiments on architectures without skip connections or Batch Normalization\nshow encouraging results and open the door to a large set of applications. Code\nis available at https://github.com/xgastaldi/shake-shake \n\n"}
{"id": "1705.07999", "contents": "Title: GP-Unet: Lesion Detection from Weak Labels with a 3D Regression Network Abstract: We propose a novel convolutional neural network for lesion detection from\nweak labels. Only a single, global label per image - the lesion count - is\nneeded for training. We train a regression network with a fully convolutional\narchitecture combined with a global pooling layer to aggregate the 3D output\ninto a scalar indicating the lesion count. When testing on unseen images, we\nfirst run the network to estimate the number of lesions. Then we remove the\nglobal pooling layer to compute localization maps of the size of the input\nimage. We evaluate the proposed network on the detection of enlarged\nperivascular spaces in the basal ganglia in MRI. Our method achieves a\nsensitivity of 62% with on average 1.5 false positives per image. Compared with\nfour other approaches based on intensity thresholding, saliency and class maps,\nour method has a 20% higher sensitivity. \n\n"}
{"id": "1705.08043", "contents": "Title: Direct measurement of superdiffusive and subdiffusive energy transport\n  in disordered granular chains Abstract: The study of energy transport properties in heterogeneous materials has\nattracted scientific interest for more than a century, and it continues to\noffer fundamental and rich questions. One of the unanswered challenges is to\nextend Anderson theory for uncorrelated and fully disordered lattices in\ncondensed-matter systems to physical settings in which additional effects\ncompete with disorder. Specifically, the effect of strong nonlinearity has been\nlargely unexplored experimentally, partly due to the paucity of testbeds that\ncan combine the effect of disorder and nonlinearity in a controllable manner.\nHere we present the first systematic experimental study of energy transport and\nlocalization properties in simultaneously disordered and nonlinear granular\ncrystals. We demonstrate experimentally that disorder and nonlinearity ---\nwhich are known from decades of studies to individually favor energy\nlocalization --- can in some sense \"cancel each other out\", resulting in the\ndestruction of wave localization. We also report that the combined effect of\ndisorder and nonlinearity can enable the manipulation of energy transport speed\nin granular crystals from subdiffusive to superdiffusive ranges. \n\n"}
{"id": "1705.08943", "contents": "Title: GridNet with automatic shape prior registration for automatic MRI\n  cardiac segmentation Abstract: In this paper, we propose a fully automatic MRI cardiac segmentation method\nbased on a novel deep convolutional neural network (CNN) designed for the 2017\nACDC MICCAI challenge. The novelty of our network comes with its embedded shape\nprior and its loss function tailored to the cardiac anatomy. Our model includes\na cardiac centerof-mass regression module which allows for an automatic shape\nprior registration. Also, since our method processes raw MR images without any\nmanual preprocessing and/or image cropping, our CNN learns both high-level\nfeatures (useful to distinguish the heart from other organs with a similar\nshape) and low-level features (useful to get accurate segmentation results).\nThose features are learned with a multi-resolution conv-deconv \"grid\"\narchitecture which can be seen as an extension of the U-Net. Experimental\nresults reveal that our method can segment the left and right ventricles as\nwell as the myocardium from a 3D MRI cardiac volume in 0.4 second with an\naverage Dice coefficient of 0.90 and an average Hausdorff distance of 10.4 mm. \n\n"}
{"id": "1705.09479", "contents": "Title: PL-SLAM: a Stereo SLAM System through the Combination of Points and Line\n  Segments Abstract: Traditional approaches to stereo visual SLAM rely on point features to\nestimate the camera trajectory and build a map of the environment. In\nlow-textured environments, though, it is often difficult to find a sufficient\nnumber of reliable point features and, as a consequence, the performance of\nsuch algorithms degrades. This paper proposes PL-SLAM, a stereo visual SLAM\nsystem that combines both points and line segments to work robustly in a wider\nvariety of scenarios, particularly in those where point features are scarce or\nnot well-distributed in the image. PL-SLAM leverages both points and segments\nat all the instances of the process: visual odometry, keyframe selection,\nbundle adjustment, etc. We contribute also with a loop closure procedure\nthrough a novel bag-of-words approach that exploits the combined descriptive\npower of the two kinds of features. Additionally, the resulting map is richer\nand more diverse in 3D elements, which can be exploited to infer valuable,\nhigh-level scene structures like planes, empty spaces, ground plane, etc. (not\naddressed in this work). Our proposal has been tested with several popular\ndatasets (such as KITTI and EuRoC), and is compared to state of the art methods\nlike ORB-SLAM, revealing a more robust performance in most of the experiments,\nwhile still running in real-time. An open source version of the PL-SLAM C++\ncode will be released for the benefit of the community. \n\n"}
{"id": "1705.10882", "contents": "Title: Morphological Error Detection in 3D Segmentations Abstract: Deep learning algorithms for connectomics rely upon localized classification,\nrather than overall morphology. This leads to a high incidence of erroneously\nmerged objects. Humans, by contrast, can easily detect such errors by acquiring\nintuition for the correct morphology of objects. Biological neurons have\ncomplicated and variable shapes, which are challenging to learn, and merge\nerrors take a multitude of different forms. We present an algorithm, MergeNet,\nthat shows 3D ConvNets can, in fact, detect merge errors from high-level\nneuronal morphology. MergeNet follows unsupervised training and operates across\ndatasets. We demonstrate the performance of MergeNet both on a variety of\nconnectomics data and on a dataset created from merged MNIST images. \n\n"}
{"id": "1706.00150", "contents": "Title: Shape and Positional Geometry of Multi-Object Configurations Abstract: In previous work, we introduced a method for modeling a configuration of\nobjects in 2D and 3D images using a mathematical \"medial/skeletal linking\nstructure.\" In this paper, we show how these structures allow us to capture\npositional properties of a multi-object configuration in addition to the shape\nproperties of the individual objects. In particular, we introduce numerical\ninvariants for positional properties which measure the closeness of neighboring\nobjects, including identifying the parts of the objects which are close, and\nthe \"relative significance\" of objects compared with the other objects in the\nconfiguration. Using these numerical measures, we introduce a hierarchical\nordering and relations between the individual objects, and quantitative\ncriteria for identifying subconfigurations. In addition, the invariants provide\na \"proximity matrix\" which yields a unique set of weightings measuring overall\nproximity of objects in the configuration. Furthermore, we show that these\ninvariants, which are volumetrically defined and involve external regions, may\nbe computed via integral formulas in terms of \"skeletal linking integrals\"\ndefined on the internal skeletal structures of the objects. \n\n"}
{"id": "1706.00159", "contents": "Title: Applied Koopman Operator Theory for Power Systems Technology Abstract: Koopman operator is a composition operator defined for a dynamical system\ndescribed by nonlinear differential or difference equation. Although the\noriginal system is nonlinear and evolves on a finite-dimensional state space,\nthe Koopman operator itself is linear but infinite-dimensional (evolves on a\nfunction space). This linear operator captures the full information of the\ndynamics described by the original nonlinear system. In particular, spectral\nproperties of the Koopman operator play a crucial role in analyzing the\noriginal system. In the first part of this paper, we review the so-called\nKoopman operator theory for nonlinear dynamical systems, with emphasis on modal\ndecomposition and computation that are direct to wide applications. Then, in\nthe second part, we present a series of applications of the Koopman operator\ntheory to power systems technology. The applications are established as\ndata-centric methods, namely, how to use massive quantities of data obtained\nnumerically and experimentally, through spectral analysis of the Koopman\noperator: coherency identification of swings in coupled synchronous generators,\nprecursor diagnostic of instabilities in the coupled swing dynamics, and\nstability assessment of power systems without any use of mathematical models.\nFuture problems of this research direction are identified in the last\nconcluding part of this paper. \n\n"}
{"id": "1706.01061", "contents": "Title: Face R-CNN Abstract: Faster R-CNN is one of the most representative and successful methods for\nobject detection, and has been becoming increasingly popular in various\nobjection detection applications. In this report, we propose a robust deep face\ndetection approach based on Faster R-CNN. In our approach, we exploit several\nnew techniques including new multi-task loss function design, online hard\nexample mining, and multi-scale training strategy to improve Faster R-CNN in\nmultiple aspects. The proposed approach is well suited for face detection, so\nwe call it Face R-CNN. Extensive experiments are conducted on two most popular\nand challenging face detection benchmarks, FDDB and WIDER FACE, to demonstrate\nthe superiority of the proposed approach over state-of-the-arts. \n\n"}
{"id": "1706.02003", "contents": "Title: Deep Convolutional Decision Jungle for Image Classification Abstract: We propose a novel method called deep convolutional decision jungle (CDJ) and\nits learning algorithm for image classification. The CDJ maintains the\nstructure of standard convolutional neural networks (CNNs), i.e. multiple\nlayers of multiple response maps fully connected. Each response map-or node-in\nboth the convolutional and fully-connected layers selectively respond to class\nlabels s.t. each data sample travels via a specific soft route of those\nactivated nodes. The proposed method CDJ automatically learns features, whereas\ndecision forests and jungles require pre-defined feature sets. Compared to\nCNNs, the method embeds the benefits of using data-dependent discriminative\nfunctions, which better handles multi-modal/heterogeneous data; further,the\nmethod offers more diverse sparse network responses, which in turn can be used\nfor cost-effective learning/classification. The network is learnt by combining\nconventional softmax and proposed entropy losses in each layer. The entropy\nloss,as used in decision tree growing, measures the purity of data activation\naccording to the class label distribution. The back-propagation rule for the\nproposed loss function is derived from stochastic gradient descent (SGD)\noptimization of CNNs. We show that our proposed method outperforms\nstate-of-the-art methods on three public image classification benchmarks and\none face verification dataset. We also demonstrate the use of auxiliary data\nlabels, when available, which helps our method to learn more discriminative\nrouting and representations and leads to improved classification. \n\n"}
{"id": "1706.02021", "contents": "Title: Network Sketching: Exploiting Binary Structure in Deep CNNs Abstract: Convolutional neural networks (CNNs) with deep architectures have\nsubstantially advanced the state-of-the-art in computer vision tasks. However,\ndeep networks are typically resource-intensive and thus difficult to be\ndeployed on mobile devices. Recently, CNNs with binary weights have shown\ncompelling efficiency to the community, whereas the accuracy of such models is\nusually unsatisfactory in practice. In this paper, we introduce network\nsketching as a novel technique of pursuing binary-weight CNNs, targeting at\nmore faithful inference and better trade-off for practical applications. Our\nbasic idea is to exploit binary structure directly in pre-trained filter banks\nand produce binary-weight models via tensor expansion. The whole process can be\ntreated as a coarse-to-fine model approximation, akin to the pencil drawing\nsteps of outlining and shading. To further speedup the generated models, namely\nthe sketches, we also propose an associative implementation of binary tensor\nconvolutions. Experimental results demonstrate that a proper sketch of AlexNet\n(or ResNet) outperforms the existing binary-weight models by large margins on\nthe ImageNet large scale classification task, while the committed memory for\nnetwork parameters only exceeds a little. \n\n"}
{"id": "1706.03319", "contents": "Title: Style Transfer for Anime Sketches with Enhanced Residual U-net and\n  Auxiliary Classifier GAN Abstract: Recently, with the revolutionary neural style transferring methods,\ncreditable paintings can be synthesized automatically from content images and\nstyle images. However, when it comes to the task of applying a painting's style\nto an anime sketch, these methods will just randomly colorize sketch lines as\noutputs and fail in the main task: specific style tranfer. In this paper, we\nintegrated residual U-net to apply the style to the gray-scale sketch with\nauxiliary classifier generative adversarial network (AC-GAN). The whole process\nis automatic and fast, and the results are creditable in the quality of art\nstyle as well as colorization. \n\n"}
{"id": "1706.04261", "contents": "Title: The \"something something\" video database for learning and evaluating\n  visual common sense Abstract: Neural networks trained on datasets such as ImageNet have led to major\nadvances in visual object classification. One obstacle that prevents networks\nfrom reasoning more deeply about complex scenes and situations, and from\nintegrating visual knowledge with natural language, like humans do, is their\nlack of common sense knowledge about the physical world. Videos, unlike still\nimages, contain a wealth of detailed information about the physical world.\nHowever, most labelled video datasets represent high-level concepts rather than\ndetailed physical aspects about actions and scenes. In this work, we describe\nour ongoing collection of the \"something-something\" database of video\nprediction tasks whose solutions require a common sense understanding of the\ndepicted situation. The database currently contains more than 100,000 videos\nacross 174 classes, which are defined as caption-templates. We also describe\nthe challenges in crowd-sourcing this data at scale. \n\n"}
{"id": "1706.04306", "contents": "Title: Photo-realistic Facial Texture Transfer Abstract: Style transfer methods have achieved significant success in recent years with\nthe use of convolutional neural networks. However, many of these methods\nconcentrate on artistic style transfer with few constraints on the output image\nappearance. We address the challenging problem of transferring face texture\nfrom a style face image to a content face image in a photorealistic manner\nwithout changing the identity of the original content image. Our framework for\nface texture transfer (FaceTex) augments the prior work of MRF-CNN with a novel\nfacial semantic regularization that incorporates a face prior regularization\nsmoothly suppressing the changes around facial meso-structures (e.g eyes, nose\nand mouth) and a facial structure loss function which implicitly preserves the\nfacial structure so that face texture can be transferred without changing the\noriginal identity. We demonstrate results on face images and compare our\napproach with recent state-of-the-art methods. Our results demonstrate superior\ntexture transfer because of the ability to maintain the identity of the\noriginal face image. \n\n"}
{"id": "1706.04372", "contents": "Title: Zoom-in-Net: Deep Mining Lesions for Diabetic Retinopathy Detection Abstract: We propose a convolution neural network based algorithm for simultaneously\ndiagnosing diabetic retinopathy and highlighting suspicious regions. Our\ncontributions are two folds: 1) a network termed Zoom-in-Net which mimics the\nzoom-in process of a clinician to examine the retinal images. Trained with only\nimage-level supervisions, Zoomin-Net can generate attention maps which\nhighlight suspicious regions, and predicts the disease level accurately based\non both the whole image and its high resolution suspicious patches. 2) Only\nfour bounding boxes generated from the automatically learned attention maps are\nenough to cover 80% of the lesions labeled by an experienced ophthalmologist,\nwhich shows good localization ability of the attention maps. By clustering\nfeatures at high response locations on the attention maps, we discover\nmeaningful clusters which contain potential lesions in diabetic retinopathy.\nExperiments show that our algorithm outperform the state-of-the-art methods on\ntwo datasets, EyePACS and Messidor. \n\n"}
{"id": "1706.04589", "contents": "Title: Learning without Prejudice: Avoiding Bias in Webly-Supervised Action\n  Recognition Abstract: Webly-supervised learning has recently emerged as an alternative paradigm to\ntraditional supervised learning based on large-scale datasets with manual\nannotations. The key idea is that models such as CNNs can be learned from the\nnoisy visual data available on the web. In this work we aim to exploit web data\nfor video understanding tasks such as action recognition and detection. One of\nthe main problems in webly-supervised learning is cleaning the noisy labeled\ndata from the web. The state-of-the-art paradigm relies on training a first\nclassifier on noisy data that is then used to clean the remaining dataset. Our\nkey insight is that this procedure biases the second classifier towards samples\nthat the first one understands. Here we train two independent CNNs, a RGB\nnetwork on web images and video frames and a second network using temporal\ninformation from optical flow. We show that training the networks independently\nis vastly superior to selecting the frames for the flow classifier by using our\nRGB network. Moreover, we show benefits in enriching the training set with\ndifferent data sources from heterogeneous public web databases. We demonstrate\nthat our framework outperforms all other webly-supervised methods on two public\nbenchmarks, UCF-101 and Thumos'14. \n\n"}
{"id": "1706.06759", "contents": "Title: Comicolorization: Semi-Automatic Manga Colorization Abstract: We developed \"Comicolorization\", a semi-automatic colorization system for\nmanga images. Given a monochrome manga and reference images as inputs, our\nsystem generates a plausible color version of the manga. This is the first work\nto address the colorization of an entire manga title (a set of manga pages).\nOur method colorizes a whole page (not a single panel) semi-automatically, with\nthe same color for the same character across multiple panels. To colorize the\ntarget character by the color from the reference image, we extract a color\nfeature from the reference and feed it to the colorization network to help the\ncolorization. Our approach employs adversarial loss to encourage the effect of\nthe color features. Optionally, our tool allows users to revise the\ncolorization result interactively. By feeding the color features to our deep\ncolorization network, we accomplish colorization of the entire manga using the\ndesired colors for each panel. \n\n"}
{"id": "1706.06969", "contents": "Title: Comparing deep neural networks against humans: object recognition when\n  the signal gets weaker Abstract: Human visual object recognition is typically rapid and seemingly effortless,\nas well as largely independent of viewpoint and object orientation. Until very\nrecently, animate visual systems were the only ones capable of this remarkable\ncomputational feat. This has changed with the rise of a class of computer\nvision algorithms called deep neural networks (DNNs) that achieve human-level\nclassification performance on object recognition tasks. Furthermore, a growing\nnumber of studies report similarities in the way DNNs and the human visual\nsystem process objects, suggesting that current DNNs may be good models of\nhuman visual object recognition. Yet there clearly exist important\narchitectural and processing differences between state-of-the-art DNNs and the\nprimate visual system. The potential behavioural consequences of these\ndifferences are not well understood. We aim to address this issue by comparing\nhuman and DNN generalisation abilities towards image degradations. We find the\nhuman visual system to be more robust to image manipulations like contrast\nreduction, additive noise or novel eidolon-distortions. In addition, we find\nprogressively diverging classification error-patterns between humans and DNNs\nwhen the signal gets weaker, indicating that there may still be marked\ndifferences in the way humans and current DNNs perform visual object\nrecognition. We envision that our findings as well as our carefully measured\nand freely available behavioural datasets provide a new useful benchmark for\nthe computer vision community to improve the robustness of DNNs and a\nmotivation for neuroscientists to search for mechanisms in the brain that could\nfacilitate this robustness. \n\n"}
{"id": "1706.07397", "contents": "Title: Fine-Grained Categorization via CNN-Based Automatic Extraction and\n  Integration of Object-Level and Part-Level Features Abstract: Fine-grained categorization can benefit from part-based features which reveal\nsubtle visual differences between object categories. Handcrafted features have\nbeen widely used for part detection and classification. Although a recent trend\nseeks to learn such features automatically using powerful deep learning models\nsuch as convolutional neural networks (CNN), their training and possibly also\ntesting require manually provided annotations which are costly to obtain. To\nrelax these requirements, we assume in this study a general problem setting in\nwhich the raw images are only provided with object-level class labels for model\ntraining with no other side information needed. Specifically, by extracting and\ninterpreting the hierarchical hidden layer features learned by a CNN, we\npropose an elaborate CNN-based system for fine-grained categorization. When\nevaluated on the Caltech-UCSD Birds-200-2011, FGVC-Aircraft, Cars and Stanford\ndogs datasets under the setting that only object-level class labels are used\nfor training and no other annotations are available for both training and\ntesting, our method achieves impressive performance that is superior or\ncomparable to the state of the art. Moreover, it sheds some light on ingenious\nuse of the hierarchical features learned by CNN which has wide applicability\nwell beyond the current fine-grained categorization task. \n\n"}
{"id": "1706.07680", "contents": "Title: Training Adversarial Discriminators for Cross-channel Abnormal Event\n  Detection in Crowds Abstract: Abnormal crowd behaviour detection attracts a large interest due to its\nimportance in video surveillance scenarios. However, the ambiguity and the lack\nof sufficient abnormal ground truth data makes end-to-end training of large\ndeep networks hard in this domain. In this paper we propose to use Generative\nAdversarial Nets (GANs), which are trained to generate only the normal\ndistribution of the data. During the adversarial GAN training, a discriminator\n(D) is used as a supervisor for the generator network (G) and vice versa. At\ntesting time we use D to solve our discriminative task (abnormality detection),\nwhere D has been trained without the need of manually-annotated abnormal data.\nMoreover, in order to prevent G learn a trivial identity function, we use a\ncross-channel approach, forcing G to transform raw-pixel data in motion\ninformation and vice versa. The quantitative results on standard benchmarks\nshow that our method outperforms previous state-of-the-art methods in both the\nframe-level and the pixel-level evaluation. \n\n"}
{"id": "1706.08474", "contents": "Title: Paying More Attention to Saliency: Image Captioning with Saliency and\n  Context Attention Abstract: Image captioning has been recently gaining a lot of attention thanks to the\nimpressive achievements shown by deep captioning architectures, which combine\nConvolutional Neural Networks to extract image representations, and Recurrent\nNeural Networks to generate the corresponding captions. At the same time, a\nsignificant research effort has been dedicated to the development of saliency\nprediction models, which can predict human eye fixations. Even though saliency\ninformation could be useful to condition an image captioning architecture, by\nproviding an indication of what is salient and what is not, research is still\nstruggling to incorporate these two techniques. In this work, we propose an\nimage captioning approach in which a generative recurrent neural network can\nfocus on different parts of the input image during the generation of the\ncaption, by exploiting the conditioning given by a saliency prediction model on\nwhich parts of the image are salient and which are contextual. We show, through\nextensive quantitative and qualitative experiments on large scale datasets,\nthat our model achieves superior performances with respect to captioning\nbaselines with and without saliency, and to different state of the art\napproaches combining saliency and captioning. \n\n"}
{"id": "1706.09858", "contents": "Title: What's Mine is Yours: Pretrained CNNs for Limited Training Sonar ATR Abstract: Finding mines in Sonar imagery is a significant problem with a great deal of\nrelevance for seafaring military and commercial endeavors. Unfortunately, the\nlack of enormous Sonar image data sets has prevented automatic target\nrecognition (ATR) algorithms from some of the same advances seen in other\ncomputer vision fields. Namely, the boom in convolutional neural nets (CNNs)\nwhich have been able to achieve incredible results - even surpassing human\nactors - has not been an easily feasible route for many practitioners of Sonar\nATR. We demonstrate the power of one avenue to incorporating CNNs into Sonar\nATR: transfer learning. We first show how well a straightforward, flexible CNN\nfeature-extraction strategy can be used to obtain impressive if not\nstate-of-the-art results. Secondly, we propose a way to utilize the powerful\ntransfer learning approach towards multiple instance target detection and\nidentification within a provided synthetic aperture Sonar data set. \n\n"}
{"id": "1707.00281", "contents": "Title: A Batch-Incremental Video Background Estimation Model using Weighted\n  Low-Rank Approximation of Matrices Abstract: Principal component pursuit (PCP) is a state-of-the-art approach for\nbackground estimation problems. Due to their higher computational cost, PCP\nalgorithms, such as robust principal component analysis (RPCA) and its\nvariants, are not feasible in processing high definition videos. To avoid the\ncurse of dimensionality in those algorithms, several methods have been proposed\nto solve the background estimation problem in an incremental manner. We propose\na batch-incremental background estimation model using a special weighted\nlow-rank approximation of matrices. Through experiments with real and synthetic\nvideo sequences, we demonstrate that our method is superior to the\nstate-of-the-art background estimation algorithms such as GRASTA, ReProCS,\nincPCP, and GFL. \n\n"}
{"id": "1707.00408", "contents": "Title: Pedestrian Alignment Network for Large-scale Person Re-identification Abstract: Person re-identification (person re-ID) is mostly viewed as an image\nretrieval problem. This task aims to search a query person in a large image\npool. In practice, person re-ID usually adopts automatic detectors to obtain\ncropped pedestrian images. However, this process suffers from two types of\ndetector errors: excessive background and part missing. Both errors deteriorate\nthe quality of pedestrian alignment and may compromise pedestrian matching due\nto the position and scale variances. To address the misalignment problem, we\npropose that alignment can be learned from an identification procedure. We\nintroduce the pedestrian alignment network (PAN) which allows discriminative\nembedding learning and pedestrian alignment without extra annotations. Our key\nobservation is that when the convolutional neural network (CNN) learns to\ndiscriminate between different identities, the learned feature maps usually\nexhibit strong activations on the human body rather than the background. The\nproposed network thus takes advantage of this attention mechanism to adaptively\nlocate and align pedestrians within a bounding box. Visual examples show that\npedestrians are better aligned with PAN. Experiments on three large-scale re-ID\ndatasets confirm that PAN improves the discriminative ability of the feature\nembeddings and yields competitive accuracy with the state-of-the-art methods. \n\n"}
{"id": "1707.01243", "contents": "Title: Exploration of object recognition from 3D point cloud Abstract: We present our latest experiment results of object recognition from 3D point\ncloud data collected through moving car. \n\n"}
{"id": "1707.01357", "contents": "Title: Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object\n  Rotation Abstract: Content-invariance in mapping codes learned by GAEs is a useful feature for\nvarious relation learning tasks. In this paper we show that the\ncontent-invariance of mapping codes for images of 2D and 3D rotated objects can\nbe substantially improved by extending the standard GAE loss (symmetric\nreconstruction error) with a regularization term that penalizes the symmetric\ncross-reconstruction error. This error term involves reconstruction of pairs\nwith mapping codes obtained from other pairs exhibiting similar\ntransformations. Although this would principally require knowledge of the\ntransformations exhibited by training pairs, our experiments show that a\nbootstrapping approach can sidestep this issue, and that the regularization\nterm can effectively be used in an unsupervised setting. \n\n"}
{"id": "1707.01737", "contents": "Title: q-Viscous Burgers' Equation: Dynamical Symmetry, Shock Solitons and\n  q-Semiclassical Expansion Abstract: We propose new type of $q$-diffusive heat equation with nonsymmetric\n$q$-extension of the diffusion term. Written in relative gradient variables\nthis system appears as the $q$- viscous Burgers' equation. Exact solutions of\nthis equation in polynomial form as generalized Kampe de Feriet polynomials,\ncorresponding dynamical symmetry and description in terms of Bell polynomials\nare derived. We found the generating function for these polynomials by\napplication of dynamical symmetry and the Zassenhaus formula. We have\nconstructed and analyzed shock solitons and their interactions with different\n$q$. We obtain modification of the soliton relative speeds depending on value\nof $q$.For $q< 1$ the soliton speed becomes bounded from above and as a result\nin addition to usual Burgers soliton process of fusion, we found a new\nphenomena, when soliton with higher amplitude but smaller velocity is fissing\nto two solitons. q-Semiclassical expansion of these equations are found in\nterms of Bernoulli polynomials in power of $\\ln q$. \n\n"}
{"id": "1707.01753", "contents": "Title: Weighted Low Rank Approximation for Background Estimation Problems Abstract: Classical principal component analysis (PCA) is not robust to the presence of\nsparse outliers in the data. The use of the $\\ell_1$ norm in the Robust PCA\n(RPCA) method successfully eliminates the weakness of PCA in separating the\nsparse outliers. In this paper, by sticking a simple weight to the Frobenius\nnorm, we propose a weighted low rank (WLR) method to avoid the often\ncomputationally expensive algorithms relying on the $\\ell_1$ norm. As a proof\nof concept, a background estimation model has been presented and compared with\ntwo $\\ell_1$ norm minimization algorithms. We illustrate that as long as a\nsimple weight matrix is inferred from the data, one can use the weighted\nFrobenius norm and achieve the same or better performance. \n\n"}
{"id": "1707.02711", "contents": "Title: Topology Reduction in Deep Convolutional Feature Extraction Networks Abstract: Deep convolutional neural networks (CNNs) used in practice employ potentially\nhundreds of layers and $10$,$000$s of nodes. Such network sizes entail\nsignificant computational complexity due to the large number of convolutions\nthat need to be carried out; in addition, a large number of parameters needs to\nbe learned and stored. Very deep and wide CNNs may therefore not be well suited\nto applications operating under severe resource constraints as is the case,\ne.g., in low-power embedded and mobile platforms. This paper aims at\nunderstanding the impact of CNN topology, specifically depth and width, on the\nnetwork's feature extraction capabilities. We address this question for the\nclass of scattering networks that employ either Weyl-Heisenberg filters or\nwavelets, the modulus non-linearity, and no pooling. The exponential feature\nmap energy decay results in Wiatowski et al., 2017, are generalized to\n$\\mathcal{O}(a^{-N})$, where an arbitrary decay factor $a>1$ can be realized\nthrough suitable choice of the Weyl-Heisenberg prototype function or the mother\nwavelet. We then show how networks of fixed (possibly small) depth $N$ can be\ndesigned to guarantee that $((1-\\varepsilon)\\cdot 100)\\%$ of the input signal's\nenergy are contained in the feature vector. Based on the notion of\noperationally significant nodes, we characterize, partly rigorously and partly\nheuristically, the topology-reducing effects of (effectively) band-limited\ninput signals, band-limited filters, and feature map symmetries. Finally, for\nnetworks based on Weyl-Heisenberg filters, we determine the prototype function\nbandwidth that minimizes---for fixed network depth $N$---the average number of\noperationally significant nodes per layer. \n\n"}
{"id": "1707.02749", "contents": "Title: Improving speaker turn embedding by crossmodal transfer learning from\n  face embedding Abstract: Learning speaker turn embeddings has shown considerable improvement in\nsituations where conventional speaker modeling approaches fail. However, this\nimprovement is relatively limited when compared to the gain observed in face\nembedding learning, which has been proven very successful for face verification\nand clustering tasks. Assuming that face and voices from the same identities\nshare some latent properties (like age, gender, ethnicity), we propose three\ntransfer learning approaches to leverage the knowledge from the face domain\n(learned from thousands of images and identities) for tasks in the speaker\ndomain. These approaches, namely target embedding transfer, relative distance\ntransfer, and clustering structure transfer, utilize the structure of the\nsource face embedding space at different granularities to regularize the target\nspeaker turn embedding space as optimizing terms. Our methods are evaluated on\ntwo public broadcast corpora and yield promising advances over competitive\nbaselines in verification and audio clustering tasks, especially when dealing\nwith short speaker utterances. The analysis of the results also gives insight\ninto characteristics of the embedding spaces and shows their potential\napplications. \n\n"}
{"id": "1707.03015", "contents": "Title: The structure of rationally factorized Lax type flows and their\n  analytical integrability Abstract: The work is devoted to constructing a wide class of differential-functional\ndynamical systems, whose rich algebraic structure makes their integrability\nanalytically effective. In particular, there is analyzed in detail the operator\nLax type equations for factorized seed elements, there is proved an important\ntheorem about their operator factorization and the related analytical solution\nscheme to the corresponding nonlinear differential-functional dynamical\nsystems. \n\n"}
{"id": "1707.03017", "contents": "Title: Learning Visual Reasoning Without Strong Priors Abstract: Achieving artificial visual reasoning - the ability to answer image-related\nquestions which require a multi-step, high-level process - is an important step\ntowards artificial general intelligence. This multi-modal task requires\nlearning a question-dependent, structured reasoning process over images from\nlanguage. Standard deep learning approaches tend to exploit biases in the data\nrather than learn this underlying structure, while leading methods learn to\nvisually reason successfully but are hand-crafted for reasoning. We show that a\ngeneral-purpose, Conditional Batch Normalization approach achieves\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\nerror rate. We outperform the next best end-to-end method (4.5%) and even\nmethods that use extra supervision (3.1%). We probe our model to shed light on\nhow it reasons, showing it has learned a question-dependent, multi-step\nprocess. Previous work has operated under the assumption that visual reasoning\ncalls for a specialized architecture, but we show that a general architecture\nwith proper conditioning can learn to visually reason effectively. \n\n"}
{"id": "1707.03374", "contents": "Title: Imitation from Observation: Learning to Imitate Behaviors from Raw Video\n  via Context Translation Abstract: Imitation learning is an effective approach for autonomous systems to acquire\ncontrol policies when an explicit reward function is unavailable, using\nsupervision provided as demonstrations from an expert, typically a human\noperator. However, standard imitation learning methods assume that the agent\nreceives examples of observation-action tuples that could be provided, for\ninstance, to a supervised learning algorithm. This stands in contrast to how\nhumans and animals imitate: we observe another person performing some behavior\nand then figure out which actions will realize that behavior, compensating for\nchanges in viewpoint, surroundings, object positions and types, and other\nfactors. We term this kind of imitation learning \"imitation-from-observation,\"\nand propose an imitation learning method based on video prediction with context\ntranslation and deep reinforcement learning. This lifts the assumption in\nimitation learning that the demonstration should consist of observations in the\nsame environment configuration, and enables a variety of interesting\napplications, including learning robotic skills that involve tool use simply by\nobserving videos of human tool use. Our experimental results show the\neffectiveness of our approach in learning a wide range of real-world robotic\ntasks modeled after common household chores from videos of a human\ndemonstrator, including sweeping, ladling almonds, pushing objects as well as a\nnumber of tasks in simulation. \n\n"}
{"id": "1707.03848", "contents": "Title: Reduced Electron Exposure for Energy-Dispersive Spectroscopy using\n  Dynamic Sampling Abstract: Analytical electron microscopy and spectroscopy of biological specimens,\npolymers, and other beam sensitive materials has been a challenging area due to\nirradiation damage. There is a pressing need to develop novel imaging and\nspectroscopic imaging methods that will minimize such sample damage as well as\nreduce the data acquisition time. The latter is useful for high-throughput\nanalysis of materials structure and chemistry. In this work, we present a novel\nmachine learning based method for dynamic sparse sampling of EDS data using a\nscanning electron microscope. Our method, based on the supervised learning\napproach for dynamic sampling algorithm and neural networks based\nclassification of EDS data, allows a dramatic reduction in the total sampling\nof up to 90%, while maintaining the fidelity of the reconstructed elemental\nmaps and spectroscopic data. We believe this approach will enable imaging and\nelemental mapping of materials that would otherwise be inaccessible to these\nanalysis techniques. \n\n"}
{"id": "1707.04991", "contents": "Title: Tracking as Online Decision-Making: Learning a Policy from Streaming\n  Videos with Reinforcement Learning Abstract: We formulate tracking as an online decision-making process, where a tracking\nagent must follow an object despite ambiguous image frames and a limited\ncomputational budget. Crucially, the agent must decide where to look in the\nupcoming frames, when to reinitialize because it believes the target has been\nlost, and when to update its appearance model for the tracked object. Such\ndecisions are typically made heuristically. Instead, we propose to learn an\noptimal decision-making policy by formulating tracking as a partially\nobservable decision-making process (POMDP). We learn policies with deep\nreinforcement learning algorithms that need supervision (a reward signal) only\nwhen the track has gone awry. We demonstrate that sparse rewards allow us to\nquickly train on massive datasets, several orders of magnitude more than past\nwork. Interestingly, by treating the data source of Internet videos as\nunlimited streams, we both learn and evaluate our trackers in a single, unified\ncomputational stream. \n\n"}
{"id": "1707.05659", "contents": "Title: The finite gap method and the analytic description of the exact rogue\n  wave recurrence in the periodic NLS Cauchy problem. 1 Abstract: The focusing NLS equation is the simplest universal model describing the\nmodulation instability (MI) of quasi monochromatic waves in weakly nonlinear\nmedia, considered the main physical mechanism for the appearance of rogue\n(anomalous) waves (RWs) in Nature. In this paper we study, using the finite gap\nmethod, the NLS Cauchy problem for periodic initial perturbations of the\nunstable background solution of NLS exciting just one of the unstable modes. We\ndistinguish two cases. In the case in which only the corresponding unstable gap\nis theoretically open, the solution describes an exact deterministic alternate\nrecurrence of linear and nonlinear stages of MI, and the nonlinear RW stages\nare described by the 1-breather Akhmediev solution, whose parameters, different\nat each RW appearance, are always given in terms of the initial data through\nelementary functions. If the number of unstable modes is >1, this uniform in t\ndynamics is sensibly affected by perturbations due to numerics and/or real\nexperiments, provoking O(1) corrections to the result. In the second case in\nwhich more than one unstable gap is open, a detailed investigation of all these\ngaps is necessary to get a uniform in $t$ dynamics, and this study is postponed\nto a subsequent paper. It is however possible to obtain the elementary\ndescription of the first nonlinear stage of MI, given again by the Akhmediev\n1-breather solution, and how perturbations due to numerics and/or real\nexperiments can affect this result. \n\n"}
{"id": "1707.06225", "contents": "Title: Waves along fractal coastlines: From fractal arithmetic to wave\n  equations Abstract: Beginning with addition and multiplication which are intrinsic to a Koch-type\ncurve, I formulate and solve a wave equation that describes wave propagation\nalong a fractal coastline. As opposed to the examples known from the literature\nI do not replace the fractal by the continuum in which it is embedded. This\nseems to be the first example of a truly intrinsic description of wave\npropagation along a fractal curve. \n\n"}
{"id": "1707.06342", "contents": "Title: ThiNet: A Filter Level Pruning Method for Deep Neural Network\n  Compression Abstract: We propose an efficient and unified framework, namely ThiNet, to\nsimultaneously accelerate and compress CNN models in both training and\ninference stages. We focus on the filter level pruning, i.e., the whole filter\nwould be discarded if it is less important. Our method does not change the\noriginal network structure, thus it can be perfectly supported by any\noff-the-shelf deep learning libraries. We formally establish filter pruning as\nan optimization problem, and reveal that we need to prune filters based on\nstatistics information computed from its next layer, not the current layer,\nwhich differentiates ThiNet from existing methods. Experimental results\ndemonstrate the effectiveness of this strategy, which has advanced the\nstate-of-the-art. We also show the performance of ThiNet on ILSVRC-12\nbenchmark. ThiNet achieves 3.31$\\times$ FLOPs reduction and 16.63$\\times$\ncompression on VGG-16, with only 0.52$\\%$ top-5 accuracy drop. Similar\nexperiments with ResNet-50 reveal that even for a compact network, ThiNet can\nalso reduce more than half of the parameters and FLOPs, at the cost of roughly\n1$\\%$ top-5 accuracy drop. Moreover, the original VGG-16 model can be further\npruned into a very small model with only 5.05MB model size, preserving AlexNet\nlevel accuracy but showing much stronger generalization ability. \n\n"}
{"id": "1707.06750", "contents": "Title: Temporal Convolution Based Action Proposal: Submission to ActivityNet\n  2017 Abstract: In this notebook paper, we describe our approach in the submission to the\ntemporal action proposal (task 3) and temporal action localization (task 4) of\nActivityNet Challenge hosted at CVPR 2017. Since the accuracy in action\nclassification task is already very high (nearly 90% in ActivityNet dataset),\nwe believe that the main bottleneck for temporal action localization is the\nquality of action proposals. Therefore, we mainly focus on the temporal action\nproposal task and propose a new proposal model based on temporal convolutional\nnetwork. Our approach achieves the state-of-the-art performances on both\ntemporal action proposal task and temporal action localization task. \n\n"}
{"id": "1707.07734", "contents": "Title: Liver lesion segmentation informed by joint liver segmentation Abstract: We propose a model for the joint segmentation of the liver and liver lesions\nin computed tomography (CT) volumes. We build the model from two fully\nconvolutional networks, connected in tandem and trained together end-to-end. We\nevaluate our approach on the 2017 MICCAI Liver Tumour Segmentation Challenge,\nattaining competitive liver and liver lesion detection and segmentation scores\nacross a wide range of metrics. Unlike other top performing methods, our model\noutput post-processing is trivial, we do not use data external to the\nchallenge, and we propose a simple single-stage model that is trained\nend-to-end. However, our method nearly matches the top lesion segmentation\nperformance and achieves the second highest precision for lesion detection\nwhile maintaining high recall. \n\n"}
{"id": "1707.08390", "contents": "Title: 3D Sketching using Multi-View Deep Volumetric Prediction Abstract: Sketch-based modeling strives to bring the ease and immediacy of drawing to\nthe 3D world. However, while drawings are easy for humans to create, they are\nvery challenging for computers to interpret due to their sparsity and\nambiguity. We propose a data-driven approach that tackles this challenge by\nlearning to reconstruct 3D shapes from one or more drawings. At the core of our\napproach is a deep convolutional neural network (CNN) that predicts occupancy\nof a voxel grid from a line drawing. This CNN provides us with an initial 3D\nreconstruction as soon as the user completes a single drawing of the desired\nshape. We complement this single-view network with an updater CNN that refines\nan existing prediction given a new drawing of the shape created from a novel\nviewpoint. A key advantage of our approach is that we can apply the updater\niteratively to fuse information from an arbitrary number of viewpoints, without\nrequiring explicit stroke correspondences between the drawings. We train both\nCNNs by rendering synthetic contour drawings from hand-modeled shape\ncollections as well as from procedurally-generated abstract shapes. Finally, we\nintegrate our CNNs in a minimal modeling interface that allows users to\nseamlessly draw an object, rotate it to see its 3D reconstruction, and refine\nit by re-drawing from another vantage point using the 3D reconstruction as\nguidance. The main strengths of our approach are its robustness to freehand\nbitmap drawings, its ability to adapt to different object categories, and the\ncontinuum it offers between single-view and multi-view sketch-based modeling. \n\n"}
{"id": "1707.09536", "contents": "Title: Integrability and linear stability of nonlinear waves Abstract: It is well known that the linear stability of solutions of partial\ndifferential equations which are integrable can be very efficiently\ninvestigated by means of spectral methods. We present here a direct\nconstruction of the eigenmodes of the linearized equation by using only their\nassociated Lax pair with no reference to spectral data and boundary conditions.\nThis local construction is given in the general $N \\times N$ matrix scheme so\nas to be applicable to a large class of integrable equations, including the\nmulticomponent nonlinear Schroedinger system and the multi-wave resonant\ninteraction system. The analytical and numerical computations involved in this\ngeneral approach are detailed as an example for $N = 3$ for the particular\nsystem of two coupled nonlinear Schroedinger equations in the defocusing,\nfocusing and mixed regimes. The instabilities of the continuous wave solutions\nare fully discussed in the entire parameter space of their amplitudes and wave\nnumbers. By defining and computing the spectrum in the complex plane of the\nspectral variable, the eigenfrequencies are explicitly expressed. According to\ntheir topological properties, the complete classification of these spectra in\nthe parameter space is presented and graphically displayed. The continuous wave\nsolutions are linearly unstable for a generic choice of the coupling constants. \n\n"}
{"id": "1707.09557", "contents": "Title: Improved Adversarial Systems for 3D Object Generation and Reconstruction Abstract: This paper describes a new approach for training generative adversarial\nnetworks (GAN) to understand the detailed 3D shape of objects. While GANs have\nbeen used in this domain previously, they are notoriously hard to train,\nespecially for the complex joint data distribution over 3D objects of many\ncategories and orientations. Our method extends previous work by employing the\nWasserstein distance normalized with gradient penalization as a training\nobjective. This enables improved generation from the joint object shape\ndistribution. Our system can also reconstruct 3D shape from 2D images and\nperform shape completion from occluded 2.5D range scans. We achieve notable\nquantitative improvements in comparison to existing baselines \n\n"}
{"id": "1707.09597", "contents": "Title: ScanNet: A Fast and Dense Scanning Framework for Metastatic Breast\n  Cancer Detection from Whole-Slide Images Abstract: Lymph node metastasis is one of the most significant diagnostic indicators in\nbreast cancer, which is traditionally observed under the microscope by\npathologists. In recent years, computerized histology diagnosis has become one\nof the most rapidly expanding fields in medical image computing, which\nalleviates pathologists' workload and reduces misdiagnosis rate. However,\nautomatic detection of lymph node metastases from whole slide images remains a\nchallenging problem, due to the large-scale data with enormous resolutions and\nexistence of hard mimics. In this paper, we propose a novel framework by\nleveraging fully convolutional networks for efficient inference to meet the\nspeed requirement for clinical practice, while reconstructing dense predictions\nunder different offsets for ensuring accurate detection on both micro- and\nmacro-metastases. Incorporating with the strategies of asynchronous sample\nprefetching and hard negative mining, the network can be effectively trained.\nExtensive experiments on the benchmark dataset of 2016 Camelyon Grand Challenge\ncorroborated the efficacy of our method. Compared with the state-of-the-art\nmethods, our method achieved superior performance with a faster speed on the\ntumor localization task and surpassed human performance on the WSI\nclassification task. \n\n"}
{"id": "1707.09873", "contents": "Title: Representation Learning on Large and Small Data Abstract: Deep learning owes its success to three key factors: scale of data, enhanced\nmodels to learn representations from data, and scale of computation. This book\nchapter presented the importance of the data-driven approach to learn good\nrepresentations from both big data and small data. In terms of big data, it has\nbeen widely accepted in the research community that the more data the better\nfor both representation and classification improvement. The question is then\nhow to learn representations from big data, and how to perform representation\nlearning when data is scarce. We addressed the first question by presenting CNN\nmodel enhancements in the aspects of representation, optimization, and\ngeneralization. To address the small data challenge, we showed transfer\nrepresentation learning to be effective. Transfer representation learning\ntransfers the learned representation from a source domain where abundant\ntraining data is available to a target domain where training data is scarce.\nTransfer representation learning gave the OM and melanoma diagnosis modules of\nour XPRIZE Tricorder device (which finished $2^{nd}$ out of $310$ competing\nteams) a significant boost in diagnosis accuracy. \n\n"}
{"id": "1708.00191", "contents": "Title: Extreme value theory for synchronization of coupled map lattices, Abstract: We show that the probability of appearance of synchronisation in chaotic\ncoupled map lattices is related to the distribution of the maximum of a certain\nobservable evaluated along almost all orbit. We show that such distribution\nbelongs to the family of extreme value laws, whose parameters, namely the\nextremal index, allow us to get a detailed description of the probability of\nsynchronisation. Theoretical results are supported by robust numerical\ncomputations that allow to go beyond the theoretical framework provided and are\npotentially applicable to physically relevant systems. \n\n"}
{"id": "1708.01022", "contents": "Title: When Kernel Methods meet Feature Learning: Log-Covariance Network for\n  Action Recognition from Skeletal Data Abstract: Human action recognition from skeletal data is a hot research topic and\nimportant in many open domain applications of computer vision, thanks to\nrecently introduced 3D sensors. In the literature, naive methods simply\ntransfer off-the-shelf techniques from video to the skeletal representation.\nHowever, the current state-of-the-art is contended between to different\nparadigms: kernel-based methods and feature learning with (recurrent) neural\nnetworks. Both approaches show strong performances, yet they exhibit heavy, but\ncomplementary, drawbacks. Motivated by this fact, our work aims at combining\ntogether the best of the two paradigms, by proposing an approach where a\nshallow network is fed with a covariance representation. Our intuition is that,\nas long as the dynamics is effectively modeled, there is no need for the\nclassification network to be deep nor recurrent in order to score favorably. We\nvalidate this hypothesis in a broad experimental analysis over 6 publicly\navailable datasets. \n\n"}
{"id": "1708.02300", "contents": "Title: Reinforced Video Captioning with Entailment Rewards Abstract: Sequence-to-sequence models have shown promising improvements on the temporal\ntask of video captioning, but they optimize word-level cross-entropy loss\nduring training. First, using policy gradient and mixed-loss methods for\nreinforcement learning, we directly optimize sentence-level task-based metrics\n(as rewards), achieving significant improvements over the baseline, based on\nboth automatic metrics and human evaluation on multiple datasets. Next, we\npropose a novel entailment-enhanced reward (CIDEnt) that corrects\nphrase-matching based metrics (such as CIDEr) to only allow for\nlogically-implied partial matches and avoid contradictions, achieving further\nsignificant improvements over the CIDEr-reward model. Overall, our\nCIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset. \n\n"}
{"id": "1708.02520", "contents": "Title: Hamiltonian structure of peakons as weak solutions for the modified\n  Camassa-Holm equation Abstract: The modified Camassa-Holm (mCH) equation is a bi-Hamiltonian system\npossessing $N$-peakon weak solutions, for all $N\\geq 1$, in the setting of an\nintegral formulation which is used in analysis for studying local\nwell-posedness, global existence, and wave breaking for non-peakon solutions.\nUnlike the original Camassa-Holm equation, the two Hamiltonians of the mCH\nequation do not reduce to conserved integrals (constants of motion) for\n$2$-peakon weak solutions. This perplexing situation is addressed here by\nfinding an explicit conserved integral for $N$-peakon weak solutions for all\n$N\\geq 2$. When $N$ is even, the conserved integral is shown to provide a\nHamiltonian structure with the use of a natural Poisson bracket that arises\nfrom reduction of one of the Hamiltonian structures of the mCH equation. But\nwhen $N$ is odd, the Hamiltonian equations of motion arising from the conserved\nintegral using this Poisson bracket are found to differ from the dynamical\nequations for the mCH $N$-peakon weak solutions. Moreover, the lack of\nconservation of the two Hamiltonians of the mCH equation when they are reduced\nto $2$-peakon weak solutions is shown to extend to $N$-peakon weak solutions\nfor all $N\\geq 2$. The connection between this loss of integrability structure\nand related work by Chang and Szmigielski on the Lax pair for the mCH equation\nis discussed. \n\n"}
{"id": "1708.02721", "contents": "Title: Deep Face Feature for Face Alignment Abstract: In this paper, we present a deep learning based image feature extraction\nmethod designed specifically for face images. To train the feature extraction\nmodel, we construct a large scale photo-realistic face image dataset with\nground-truth correspondence between multi-view face images, which are\nsynthesized from real photographs via an inverse rendering procedure. The deep\nface feature (DFF) is trained using correspondence between face images rendered\nfrom different views. Using the trained DFF model, we can extract a feature\nvector for each pixel of a face image, which distinguishes different facial\nregions and is shown to be more effective than general-purpose feature\ndescriptors for face-related tasks such as matching and alignment. Based on the\nDFF, we develop a robust face alignment method, which iteratively updates\nlandmarks, pose and 3D shape. Extensive experiments demonstrate that our method\ncan achieve state-of-the-art results for face alignment under highly\nunconstrained face images. \n\n"}
{"id": "1708.03383", "contents": "Title: Joint Multi-Person Pose Estimation and Semantic Part Segmentation Abstract: Human pose estimation and semantic part segmentation are two complementary\ntasks in computer vision. In this paper, we propose to solve the two tasks\njointly for natural multi-person images, in which the estimated pose provides\nobject-level shape prior to regularize part segments while the part-level\nsegments constrain the variation of pose locations. Specifically, we first\ntrain two fully convolutional neural networks (FCNs), namely Pose FCN and Part\nFCN, to provide initial estimation of pose joint potential and semantic part\npotential. Then, to refine pose joint location, the two types of potentials are\nfused with a fully-connected conditional random field (FCRF), where a novel\nsegment-joint smoothness term is used to encourage semantic and spatial\nconsistency between parts and joints. To refine part segments, the refined pose\nand the original part potential are integrated through a Part FCN, where the\nskeleton feature from pose serves as additional regularization cues for part\nsegments. Finally, to reduce the complexity of the FCRF, we induce human\ndetection boxes and infer the graph inside each box, making the inference forty\ntimes faster.\n  Since there's no dataset that contains both part segments and pose labels, we\nextend the PASCAL VOC part dataset with human pose joints and perform extensive\nexperiments to compare our method against several most recent strategies. We\nshow that on this dataset our algorithm surpasses competing methods by a large\nmargin in both tasks. \n\n"}
{"id": "1708.03615", "contents": "Title: Unsupervised Incremental Learning of Deep Descriptors From Video Streams Abstract: We present a novel unsupervised method for face identity learning from video\nsequences. The method exploits the ResNet deep network for face detection and\nVGGface fc7 face descriptors together with a smart learning mechanism that\nexploits the temporal coherence of visual data in video streams. We present a\nnovel feature matching solution based on Reverse Nearest Neighbour and a\nfeature forgetting strategy that supports incremental learning with memory size\ncontrol, while time progresses. It is shown that the proposed learning\nprocedure is asymptotically stable and can be effectively applied to relevant\napplications like multiple face tracking. \n\n"}
{"id": "1708.03725", "contents": "Title: Going Deeper with Semantics: Video Activity Interpretation using\n  Semantic Contextualization Abstract: A deeper understanding of video activities extends beyond recognition of\nunderlying concepts such as actions and objects: constructing deep semantic\nrepresentations requires reasoning about the semantic relationships among these\nconcepts, often beyond what is directly observed in the data. To this end, we\npropose an energy minimization framework that leverages large-scale commonsense\nknowledge bases, such as ConceptNet, to provide contextual cues to establish\nsemantic relationships among entities directly hypothesized from video signal.\nWe mathematically express this using the language of Grenander's canonical\npattern generator theory. We show that the use of prior encoded commonsense\nknowledge alleviate the need for large annotated training datasets and help\ntackle imbalance in training through prior knowledge. Using three different\npublicly available datasets - Charades, Microsoft Visual Description Corpus and\nBreakfast Actions datasets, we show that the proposed model can generate video\ninterpretations whose quality is better than those reported by state-of-the-art\napproaches, which have substantial training needs. Through extensive\nexperiments, we show that the use of commonsense knowledge from ConceptNet\nallows the proposed approach to handle various challenges such as training data\nimbalance, weak features, and complex semantic relationships and visual scenes. \n\n"}
{"id": "1708.03816", "contents": "Title: Mass Displacement Networks Abstract: Despite the large improvements in performance attained by using deep learning\nin computer vision, one can often further improve results with some additional\npost-processing that exploits the geometric nature of the underlying task. This\ncommonly involves displacing the posterior distribution of a CNN in a way that\nmakes it more appropriate for the task at hand, e.g. better aligned with local\nimage features, or more compact. In this work we integrate this geometric\npost-processing within a deep architecture, introducing a differentiable and\nprobabilistically sound counterpart to the common geometric voting technique\nused for evidence accumulation in vision. We refer to the resulting neural\nmodels as Mass Displacement Networks (MDNs), and apply them to human pose\nestimation in two distinct setups: (a) landmark localization, where we collapse\na distribution to a point, allowing for precise localization of body keypoints\nand (b) communication across body parts, where we transfer evidence from one\npart to the other, allowing for a globally consistent pose estimate. We\nevaluate on large-scale pose estimation benchmarks, such as MPII Human Pose and\nCOCO datasets, and report systematic improvements when compared to strong\nbaselines. \n\n"}
{"id": "1708.04169", "contents": "Title: Divide and Fuse: A Re-ranking Approach for Person Re-identification Abstract: As re-ranking is a necessary procedure to boost person re-identification\n(re-ID) performance on large-scale datasets, the diversity of feature becomes\ncrucial to person reID for its importance both on designing pedestrian\ndescriptions and re-ranking based on feature fusion. However, in many\ncircumstances, only one type of pedestrian feature is available. In this paper,\nwe propose a \"Divide and use\" re-ranking framework for person re-ID. It\nexploits the diversity from different parts of a high-dimensional feature\nvector for fusion-based re-ranking, while no other features are accessible.\nSpecifically, given an image, the extracted feature is divided into\nsub-features. Then the contextual information of each sub-feature is\niteratively encoded into a new feature. Finally, the new features from the same\nimage are fused into one vector for re-ranking. Experimental results on two\nperson re-ID benchmarks demonstrate the effectiveness of the proposed\nframework. Especially, our method outperforms the state-of-the-art on the\nMarket-1501 dataset. \n\n"}
{"id": "1708.04675", "contents": "Title: Learning Graph While Training: An Evolving Graph Convolutional Neural\n  Network Abstract: Convolution Neural Networks on Graphs are important generalization and\nextension of classical CNNs. While previous works generally assumed that the\ngraph structures of samples are regular with unified dimensions, in many\napplications, they are highly diverse or even not well defined. Under some\ncircumstances, e.g. chemical molecular data, clustering or coarsening for\nsimplifying the graphs is hard to be justified chemically. In this paper, we\npropose a more general and flexible graph convolution network (EGCN) fed by\nbatch of arbitrarily shaped data together with their evolving graph Laplacians\ntrained in supervised fashion. Extensive experiments have been conducted to\ndemonstrate the superior performance in terms of both the acceleration of\nparameter fitting and the significantly improved prediction accuracy on\nmultiple graph-structured datasets. \n\n"}
{"id": "1708.04692", "contents": "Title: GANs for Biological Image Synthesis Abstract: In this paper, we propose a novel application of Generative Adversarial\nNetworks (GAN) to the synthesis of cells imaged by fluorescence microscopy.\nCompared to natural images, cells tend to have a simpler and more geometric\nglobal structure that facilitates image generation. However, the correlation\nbetween the spatial pattern of different fluorescent proteins reflects\nimportant biological functions, and synthesized images have to capture these\nrelationships to be relevant for biological applications. We adapt GANs to the\ntask at hand and propose new models with casual dependencies between image\nchannels that can generate multi-channel images, which would be impossible to\nobtain experimentally. We evaluate our approach using two independent\ntechniques and compare it against sensible baselines. Finally, we demonstrate\nthat by interpolating across the latent space we can mimic the known changes in\nprotein localization that occur through time during the cell cycle, allowing us\nto predict temporal evolution from static images. \n\n"}
{"id": "1708.06734", "contents": "Title: Representation Learning by Learning to Count Abstract: We introduce a novel method for representation learning that uses an\nartificial supervision signal based on counting visual primitives. This\nsupervision signal is obtained from an equivariance relation, which does not\nrequire any manual annotation. We relate transformations of images to\ntransformations of the representations. More specifically, we look for the\nrepresentation that satisfies such relation rather than the transformations\nthat match a given representation. In this paper, we use two image\ntransformations in the context of counting: scaling and tiling. The first\ntransformation exploits the fact that the number of visual primitives should be\ninvariant to scale. The second transformation allows us to equate the total\nnumber of visual primitives in each tile to that in the whole image. These two\ntransformations are combined in one constraint and used to train a neural\nnetwork with a contrastive loss. The proposed task produces representations\nthat perform on par or exceed the state of the art in transfer learning\nbenchmarks. \n\n"}
{"id": "1708.07747", "contents": "Title: Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n  Algorithms Abstract: We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images\nof 70,000 fashion products from 10 categories, with 7,000 images per category.\nThe training set has 60,000 images and the test set has 10,000 images.\nFashion-MNIST is intended to serve as a direct drop-in replacement for the\noriginal MNIST dataset for benchmarking machine learning algorithms, as it\nshares the same image size, data format and the structure of training and\ntesting splits. The dataset is freely available at\nhttps://github.com/zalandoresearch/fashion-mnist \n\n"}
{"id": "1708.07755", "contents": "Title: Gait Recognition from Motion Capture Data Abstract: Gait recognition from motion capture data, as a pattern classification\ndiscipline, can be improved by the use of machine learning. This paper\ncontributes to the state-of-the-art with a statistical approach for extracting\nrobust gait features directly from raw data by a modification of Linear\nDiscriminant Analysis with Maximum Margin Criterion. Experiments on the CMU\nMoCap database show that the suggested method outperforms thirteen relevant\nmethods based on geometric features and a method to learn the features by a\ncombination of Principal Component Analysis and Linear Discriminant Analysis.\nThe methods are evaluated in terms of the distribution of biometric templates\nin respective feature spaces expressed in a number of class separability\ncoefficients and classification metrics. Results also indicate a high\nportability of learned features, that means, we can learn what aspects of walk\npeople generally differ in and extract those as general gait features.\nRecognizing people without needing group-specific features is convenient as\nparticular people might not always provide annotated learning data. As a\ncontribution to reproducible research, our evaluation framework and database\nhave been made publicly available. This research makes motion capture\ntechnology directly applicable for human recognition. \n\n"}
{"id": "1708.09066", "contents": "Title: Block-Simultaneous Direction Method of Multipliers: A proximal\n  primal-dual splitting algorithm for nonconvex problems with multiple\n  constraints Abstract: We introduce a generalization of the linearized Alternating Direction Method\nof Multipliers to optimize a real-valued function $f$ of multiple arguments\nwith potentially multiple constraints $g_\\circ$ on each of them. The function\n$f$ may be nonconvex as long as it is convex in every argument, while the\nconstraints $g_\\circ$ need to be convex but not smooth. If $f$ is smooth, the\nproposed Block-Simultaneous Direction Method of Multipliers (bSDMM) can be\ninterpreted as a proximal analog to inexact coordinate descent methods under\nconstraints. Unlike alternative approaches for joint solvers of\nmultiple-constraint problems, we do not require linear operators $L$ of a\nconstraint function $g(L\\ \\cdot)$ to be invertible or linked between each\nother. bSDMM is well-suited for a range of optimization problems, in particular\nfor data analysis, where $f$ is the likelihood function of a model and $L$\ncould be a transformation matrix describing e.g. finite differences or basis\ntransforms. We apply bSDMM to the Non-negative Matrix Factorization task of a\nhyperspectral unmixing problem and demonstrate convergence and effectiveness of\nmultiple constraints on both matrix factors. The algorithms are implemented in\npython and released as an open-source package. \n\n"}
{"id": "1709.00158", "contents": "Title: Reasoning with shapes: profiting cognitive susceptibilities to infer\n  linear mapping transformations between shapes Abstract: Visual information plays an indispensable role in our daily interactions with\nenvironment. Such information is manipulated for a wide range of purposes\nspanning from basic object and material perception to complex gesture\ninterpretations. There have been novel studies in cognitive science for\nin-depth understanding of visual information manipulation, which lead to answer\nquestions such as: how we infer 2D/3D motion from a sequence of 2D images? how\nwe understand a motion from a single image frame? how we see forest avoiding\ntrees?\n  Leveraging on congruence, linear mapping transformation determination between\na set of shapes facilitate motion perception. Present study methodizes recent\ndiscoveries of human cognitive ability for scene understanding. The proposed\nmethod processes images hierarchically, that is an iterative analysis of scene\nabstractions using a rapidly converging heuristic iterative method. The method\nhierarchically abstracts images; the abstractions are represented in polar\ncoordinate system, and any two consecutive abstractions have incremental level\nof details. The method then creates a graph of approximated linear mapping\ntransformations based on circular shift permutations of hierarchical\nabstractions. The graph is then traversed in best-first fashion to find best\nlinear mapping transformation. The accuracy of the proposed method is assessed\nusing normal, noisy, and deformed images. Additionally, the present study\ndeduces (i) the possibility of determining optimal mapping linear\ntransformations in logarithmic iterations with respect to the precision of\nresults, and (ii) computational cost is independent from the resolution of\ninput shapes. \n\n"}
{"id": "1709.00443", "contents": "Title: End-to-End Multi-View Lipreading Abstract: Non-frontal lip views contain useful information which can be used to enhance\nthe performance of frontal view lipreading. However, the vast majority of\nrecent lipreading works, including the deep learning approaches which\nsignificantly outperform traditional approaches, have focused on frontal mouth\nimages. As a consequence, research on joint learning of visual features and\nspeech classification from multiple views is limited. In this work, we present\nan end-to-end multi-view lipreading system based on Bidirectional Long-Short\nMemory (BLSTM) networks. To the best of our knowledge, this is the first model\nwhich simultaneously learns to extract features directly from the pixels and\nperforms visual speech classification from multiple views and also achieves\nstate-of-the-art performance. The model consists of multiple identical streams,\none for each view, which extract features directly from different poses of\nmouth images. The temporal dynamics in each stream/view are modelled by a BLSTM\nand the fusion of multiple streams/views takes place via another BLSTM. An\nabsolute average improvement of 3% and 3.8% over the frontal view performance\nis reported on the OuluVS2 database when the best two (frontal and profile) and\nthree views (frontal, profile, 45) are combined, respectively. The best\nthree-view model results in a 10.5% absolute improvement over the current\nmulti-view state-of-the-art performance on OuluVS2, without using external\ndatabases for training, achieving a maximum classification accuracy of 96.9%. \n\n"}
{"id": "1709.00483", "contents": "Title: Iteratively Linearized Reweighted Alternating Direction Method of\n  Multipliers for a Class of Nonconvex Problems Abstract: In this paper, we consider solving a class of nonconvex and nonsmooth\nproblems frequently appearing in signal processing and machine learning\nresearch. The traditional alternating direction method of multipliers\nencounters troubles in both mathematics and computations in solving the\nnonconvex and nonsmooth subproblem. In view of this, we propose a reweighted\nalternating direction method of multipliers. In this algorithm, all subproblems\nare convex and easy to solve. We also provide several guarantees for the\nconvergence and prove that the algorithm globally converges to a critical point\nof an auxiliary function with the help of the Kurdyka-{\\L}ojasiewicz property.\nSeveral numerical results are presented to demonstrate the efficiency of the\nproposed algorithm. \n\n"}
{"id": "1709.00513", "contents": "Title: Training Shallow and Thin Networks for Acceleration via Knowledge\n  Distillation with Conditional Adversarial Networks Abstract: There is an increasing interest on accelerating neural networks for real-time\napplications. We study the student-teacher strategy, in which a small and fast\nstudent network is trained with the auxiliary information learned from a large\nand accurate teacher network. We propose to use conditional adversarial\nnetworks to learn the loss function to transfer knowledge from teacher to\nstudent. The proposed method is particularly effective for relatively small\nstudent networks. Moreover, experimental results show the effect of network\nsize when the modern networks are used as student. We empirically study the\ntrade-off between inference time and classification accuracy, and provide\nsuggestions on choosing a proper student network. \n\n"}
{"id": "1709.01062", "contents": "Title: A hierarchical loss and its problems when classifying non-hierarchically Abstract: Failing to distinguish between a sheepdog and a skyscraper should be worse\nand penalized more than failing to distinguish between a sheepdog and a poodle;\nafter all, sheepdogs and poodles are both breeds of dogs. However, existing\nmetrics of failure (so-called \"loss\" or \"win\") used in textual or visual\nclassification/recognition via neural networks seldom leverage a-priori\ninformation, such as a sheepdog being more similar to a poodle than to a\nskyscraper. We define a metric that, inter alia, can penalize failure to\ndistinguish between a sheepdog and a skyscraper more than failure to\ndistinguish between a sheepdog and a poodle. Unlike previously employed\npossibilities, this metric is based on an ultrametric tree associated with any\ngiven tree organization into a semantically meaningful hierarchy of a\nclassifier's classes. An ultrametric tree is a tree with a so-called\nultrametric distance metric such that all leaves are at the same distance from\nthe root. Unfortunately, extensive numerical experiments indicate that the\nstandard practice of training neural networks via stochastic gradient descent\nwith random starting points often drives down the hierarchical loss nearly as\nmuch when minimizing the standard cross-entropy loss as when trying to minimize\nthe hierarchical loss directly. Thus, this hierarchical loss is unreliable as\nan objective for plain, randomly started stochastic gradient descent to\nminimize; the main value of the hierarchical loss may be merely as a meaningful\nmetric of success of a classifier. \n\n"}
{"id": "1709.01625", "contents": "Title: Exploring and Exploiting Diversity for Image Segmentation Abstract: Semantic image segmentation is an important computer vision task that is\ndifficult because it consists of both recognition and segmentation. The task is\noften cast as a structured output problem on an exponentially large\noutput-space, which is typically modeled by a discrete probabilistic model. The\nbest segmentation is found by inferring the Maximum a-Posteriori (MAP) solution\nover the output distribution defined by the model. Due to limitations in\noptimization, the model cannot be arbitrarily complex. This leads to a\ntrade-off: devise a more accurate model that incorporates rich high-order\ninteractions between image elements at the cost of inaccurate and possibly\nintractable optimization OR leverage a tractable model which produces less\naccurate MAP solutions but may contain high quality solutions as other modes of\nits output distribution.\n  This thesis investigates the latter and presents a two stage approach to\nsemantic segmentation. In the first stage a tractable segmentation model\noutputs a set of high probability segmentations from the underlying\ndistribution that are not just minor perturbations of each other. Critically\nthe output of this stage is a diverse set of plausible solutions and not just a\nsingle one. In the second stage, a discriminatively trained re-ranking model\nselects the best segmentation from this set. The re-ranking stage can use much\nmore complex features than what could be tractably used in the segmentation\nmodel, allowing a better exploration of the solution space than simply\nreturning the MAP solution. The formulation is agnostic to the underlying\nsegmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, which\nmakes it applicable to a wide range of models and inference methods. Evaluation\nof the approach on a number of semantic image segmentation benchmark datasets\nhighlight its superiority over inferring the MAP solution. \n\n"}
{"id": "1709.01872", "contents": "Title: Synthetic Medical Images from Dual Generative Adversarial Networks Abstract: Currently there is strong interest in data-driven approaches to medical image\nclassification. However, medical imaging data is scarce, expensive, and fraught\nwith legal concerns regarding patient privacy. Typical consent forms only allow\nfor patient data to be used in medical journals or education, meaning the\nmajority of medical data is inaccessible for general public research. We\npropose a novel, two-stage pipeline for generating synthetic medical images\nfrom a pair of generative adversarial networks, tested in practice on retinal\nfundi images. We develop a hierarchical generation process to divide the\ncomplex image generation task into two parts: geometry and photorealism. We\nhope researchers will use our pipeline to bring private medical data into the\npublic domain, sparking growth in imaging tasks that have previously relied on\nthe hand-tuning of models. We have begun this initiative through the\ndevelopment of SynthMed, an online repository for synthetic medical images. \n\n"}
{"id": "1709.03028", "contents": "Title: Convolutional Neural Networks: Ensemble Modeling, Fine-Tuning and\n  Unsupervised Semantic Localization for Intraoperative CLE Images Abstract: Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence\ntechnology undergoing assessment for applications in brain tumor surgery.\nDespite its promising potential, interpreting the unfamiliar gray tone images\nof fluorescent stains can be difficult. Many of the CLE images can be distorted\nby motion, extremely low or high fluorescence signal, or obscured by red blood\ncell accumulation, and these can be interpreted as nondiagnostic. However, just\none neat CLE image might suffice for intraoperative diagnosis of the tumor.\nWhile manual examination of thousands of nondiagnostic images during surgery\nwould be impractical, this creates an opportunity for a model to select\ndiagnostic images for the pathologists or surgeon's review. In this study, we\nsought to develop a deep learning model to automatically detect the diagnostic\nimages using a manually annotated dataset, and we employed a patient-based\nnested cross-validation approach to explore generalizability of the model. We\nexplored various training regimes: deep training, shallow fine-tuning, and deep\nfine-tuning. Further, we investigated the effect of ensemble modeling by\ncombining the top-5 single models crafted in the development phase. We\nlocalized histological features from diagnostic CLE images by visualization of\nshallow and deep neural activations. Our inter-rater experiment results\nconfirmed that our ensemble of deeply fine-tuned models achieved higher\nagreement with the ground truth than the other observers. With the speed and\nprecision of the proposed method (110 images/second; 85% on the gold standard\ntest subset), it has potential to be integrated into the operative workflow in\nthe brain tumor surgery. \n\n"}
{"id": "1709.03196", "contents": "Title: Deep multi-frame face super-resolution Abstract: Face verification and recognition problems have seen rapid progress in recent\nyears, however recognition from small size images remains a challenging task\nthat is inherently intertwined with the task of face super-resolution. Tackling\nthis problem using multiple frames is an attractive idea, yet requires solving\nthe alignment problem that is also challenging for low-resolution faces. Here\nwe present a holistic system for multi-frame recognition, alignment, and\nsuperresolution of faces. Our neural network architecture restores the central\nframe of each input sequence additionally taking into account a number of\nadjacent frames and making use of sub-pixel movements. We present our results\nusing the popular dataset for video face recognition (YouTube Faces). We show a\nnotable improvement of identification score compared to several baselines\nincluding the one based on single-image super-resolution. \n\n"}
{"id": "1709.03272", "contents": "Title: Fused Text Segmentation Networks for Multi-oriented Scene Text Detection Abstract: In this paper, we introduce a novel end-end framework for multi-oriented\nscene text detection from an instance-aware semantic segmentation perspective.\nWe present Fused Text Segmentation Networks, which combine multi-level features\nduring the feature extracting as text instance may rely on finer feature\nexpression compared to general objects. It detects and segments the text\ninstance jointly and simultaneously, leveraging merits from both semantic\nsegmentation task and region proposal based object detection task. Not\ninvolving any extra pipelines, our approach surpasses the current state of the\nart on multi-oriented scene text detection benchmarks: ICDAR2015 Incidental\nScene Text and MSRA-TD500 reaching Hmean 84.1% and 82.0% respectively. Morever,\nwe report a baseline on total-text containing curved text which suggests\neffectiveness of the proposed approach. \n\n"}
{"id": "1709.03395", "contents": "Title: Low-memory GEMM-based convolution algorithms for deep neural networks Abstract: Deep neural networks (DNNs) require very large amounts of computation both\nfor training and for inference when deployed in the field. A common approach to\nimplementing DNNs is to recast the most computationally expensive operations as\ngeneral matrix multiplication (GEMM). However, as we demonstrate in this paper,\nthere are a great many different ways to express DNN convolution operations\nusing GEMM. Although different approaches all perform the same number of\noperations, the size of temporary data structures differs significantly.\nConvolution of an input matrix with dimensions $C \\times H \\times W$, requires\n$O(K^2CHW)$ additional space using the classical im2col approach. More recently\nmemory-efficient approaches requiring just $O(KCHW)$ auxiliary space have been\nproposed.\n  We present two novel GEMM-based algorithms that require just $O(MHW)$ and\n$O(KW)$ additional space respectively, where $M$ is the number of channels in\nthe result of the convolution. These algorithms dramatically reduce the space\noverhead of DNN convolution, making it much more suitable for memory-limited\nembedded systems. Experimental evaluation shows that our low-memory algorithms\nare just as fast as the best patch-building approaches despite requiring just a\nfraction of the amount of additional memory. Our low-memory algorithms have\nexcellent data locality which gives them a further edge over patch-building\nalgorithms when multiple cores are used. As a result, our low memory algorithms\noften outperform the best patch-building algorithms using multiple threads. \n\n"}
{"id": "1709.03439", "contents": "Title: Why Do Deep Neural Networks Still Not Recognize These Images?: A\n  Qualitative Analysis on Failure Cases of ImageNet Classification Abstract: In a recent decade, ImageNet has become the most notable and powerful\nbenchmark database in computer vision and machine learning community. As\nImageNet has emerged as a representative benchmark for evaluating the\nperformance of novel deep learning models, its evaluation tends to include only\nquantitative measures such as error rate, rather than qualitative analysis.\nThus, there are few studies that analyze the failure cases of deep learning\nmodels in ImageNet, though there are numerous works analyzing the networks\nthemselves and visualizing them. In this abstract, we qualitatively analyze the\nfailure cases of ImageNet classification results from recent deep learning\nmodel, and categorize these cases according to the certain image patterns.\nThrough this failure analysis, we believe that it can be discovered what the\nfinal challenges are in ImageNet database, which the current deep learning\nmodel is still vulnerable to. \n\n"}
{"id": "1709.04577", "contents": "Title: DeepVoting: A Robust and Explainable Deep Network for Semantic Part\n  Detection under Partial Occlusion Abstract: In this paper, we study the task of detecting semantic parts of an object,\ne.g., a wheel of a car, under partial occlusion. We propose that all models\nshould be trained without seeing occlusions while being able to transfer the\nlearned knowledge to deal with occlusions. This setting alleviates the\ndifficulty in collecting an exponentially large dataset to cover occlusion\npatterns and is more essential. In this scenario, the proposal-based deep\nnetworks, like RCNN-series, often produce unsatisfactory results, because both\nthe proposal extraction and classification stages may be confused by the\nirrelevant occluders. To address this, [25] proposed a voting mechanism that\ncombines multiple local visual cues to detect semantic parts. The semantic\nparts can still be detected even though some visual cues are missing due to\nocclusions. However, this method is manually-designed, thus is hard to be\noptimized in an end-to-end manner.\n  In this paper, we present DeepVoting, which incorporates the robustness shown\nby [25] into a deep network, so that the whole pipeline can be jointly\noptimized. Specifically, it adds two layers after the intermediate features of\na deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the\nevidence of local visual cues, and the second layer performs a voting mechanism\nby utilizing the spatial relationship between visual cues and semantic parts.\nWe also propose an improved version DeepVoting+ by learning visual cues from\ncontext outside objects. In experiments, DeepVoting achieves significantly\nbetter performance than several baseline methods, including Faster-RCNN, for\nsemantic part detection under occlusion. In addition, DeepVoting enjoys\nexplainability as the detection results can be diagnosed via looking up the\nvoting cues. \n\n"}
{"id": "1709.05903", "contents": "Title: E$^2$BoWs: An End-to-End Bag-of-Words Model via Deep Convolutional\n  Neural Network Abstract: Traditional Bag-of-visual Words (BoWs) model is commonly generated with many\nsteps including local feature extraction, codebook generation, and feature\nquantization, etc. Those steps are relatively independent with each other and\nare hard to be jointly optimized. Moreover, the dependency on hand-crafted\nlocal feature makes BoWs model not effective in conveying high-level semantics.\nThese issues largely hinder the performance of BoWs model in large-scale image\napplications. To conquer these issues, we propose an End-to-End BoWs\n(E$^2$BoWs) model based on Deep Convolutional Neural Network (DCNN). Our model\ntakes an image as input, then identifies and separates the semantic objects in\nit, and finally outputs the visual words with high semantic discriminative\npower. Specifically, our model firstly generates Semantic Feature Maps (SFMs)\ncorresponding to different object categories through convolutional layers, then\nintroduces Bag-of-Words Layers (BoWL) to generate visual words for each\nindividual feature map. We also introduce a novel learning algorithm to\nreinforce the sparsity of the generated E$^2$BoWs model, which further ensures\nthe time and memory efficiency. We evaluate the proposed E$^2$BoWs model on\nseveral image search datasets including CIFAR-10, CIFAR-100, MIRFLICKR-25K and\nNUS-WIDE. Experimental results show that our method achieves promising accuracy\nand efficiency compared with recent deep learning based retrieval works. \n\n"}
{"id": "1709.06053", "contents": "Title: Coupled Ensembles of Neural Networks Abstract: We investigate in this paper the architecture of deep convolutional networks.\nBuilding on existing state of the art models, we propose a reconfiguration of\nthe model parameters into several parallel branches at the global network\nlevel, with each branch being a standalone CNN. We show that this arrangement\nis an efficient way to significantly reduce the number of parameters without\nlosing performance or to significantly improve the performance with the same\nlevel of performance. The use of branches brings an additional form of\nregularization. In addition to the split into parallel branches, we propose a\ntighter coupling of these branches by placing the \"fuse (averaging) layer\"\nbefore the Log-Likelihood and SoftMax layers during training. This gives\nanother significant performance improvement, the tighter coupling favouring the\nlearning of better representations, even at the level of the individual\nbranches. We refer to this branched architecture as \"coupled ensembles\". The\napproach is very generic and can be applied with almost any DCNN architecture.\nWith coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain\nerror rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and\nSVHN tasks. For the same budget, DenseNet-BC has error rate of 3.46%, 17.18%,\nand 1.8% respectively. With ensembles of coupled ensembles, of DenseNet-BC\nnetworks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and\n1.42% respectively on these tasks. \n\n"}
{"id": "1709.06531", "contents": "Title: Learning to Detect Violent Videos using Convolutional Long Short-Term\n  Memory Abstract: Developing a technique for the automatic analysis of surveillance videos in\norder to identify the presence of violence is of broad interest. In this work,\nwe propose a deep neural network for the purpose of recognizing violent videos.\nA convolutional neural network is used to extract frame level features from a\nvideo. The frame level features are then aggregated using a variant of the long\nshort term memory that uses convolutional gates. The convolutional neural\nnetwork along with the convolutional long short term memory is capable of\ncapturing localized spatio-temporal features which enables the analysis of\nlocal motion taking place in the video. We also propose to use adjacent frame\ndifferences as the input to the model thereby forcing it to encode the changes\noccurring in the video. The performance of the proposed feature extraction\npipeline is evaluated on three standard benchmark datasets in terms of\nrecognition accuracy. Comparison of the results obtained with the state of the\nart techniques revealed the promising capability of the proposed method in\nrecognizing violent videos. \n\n"}
{"id": "1709.07992", "contents": "Title: Visual Reference Resolution using Attention Memory for Visual Dialog Abstract: Visual dialog is a task of answering a series of inter-dependent questions\ngiven an input image, and often requires to resolve visual references among the\nquestions. This problem is different from visual question answering (VQA),\nwhich relies on spatial attention (a.k.a. visual grounding) estimated from an\nimage and question pair. We propose a novel attention mechanism that exploits\nvisual attentions in the past to resolve the current reference in the visual\ndialog scenario. The proposed model is equipped with an associative attention\nmemory storing a sequence of previous (attention, key) pairs. From this memory,\nthe model retrieves the previous attention, taking into account recency, which\nis most relevant for the current question, in order to resolve potentially\nambiguous references. The model then merges the retrieved attention with a\ntentative one to obtain the final attention for the current question;\nspecifically, we use dynamic parameter prediction to combine the two attentions\nconditioned on the question. Through extensive experiments on a new synthetic\nvisual dialog dataset, we show that our model significantly outperforms the\nstate-of-the-art (by ~16 % points) in situations, where visual reference\nresolution plays an important role. Moreover, the proposed model achieves\nsuperior performance (~ 2 % points improvement) in the Visual Dialog dataset,\ndespite having significantly fewer parameters than the baselines. \n\n"}
{"id": "1709.09075", "contents": "Title: Automated sub-cortical brain structure segmentation combining spatial\n  and deep convolutional features Abstract: Sub-cortical brain structure segmentation in Magnetic Resonance Images (MRI)\nhas attracted the interest of the research community for a long time because\nmorphological changes in these structures are related to different\nneurodegenerative disorders. However, manual segmentation of these structures\ncan be tedious and prone to variability, highlighting the need for robust\nautomated segmentation methods. In this paper, we present a novel convolutional\nneural network based approach for accurate segmentation of the sub-cortical\nbrain structures that combines both convolutional and prior spatial features\nfor improving the segmentation accuracy. In order to increase the accuracy of\nthe automated segmentation, we propose to train the network using a restricted\nsample selection to force the network to learn the most difficult parts of the\nstructures. We evaluate the accuracy of the proposed method on the public\nMICCAI 2012 challenge and IBSR 18 datasets, comparing it with different\navailable state-of-the-art methods and other recently proposed deep learning\napproaches. On the MICCAI 2012 dataset, our method shows an excellent\nperformance comparable to the best challenge participant strategy, while\nperforming significantly better than state-of-the-art techniques such as\nFreeSurfer and FIRST. On the IBSR 18 dataset, our method also exhibits a\nsignificant increase in the performance with respect to not only FreeSurfer and\nFIRST, but also comparable or better results than other recent deep learning\napproaches. Moreover, our experiments show that both the addition of the\nspatial priors and the restricted sampling strategy have a significant effect\non the accuracy of the proposed method. In order to encourage the\nreproducibility and the use of the proposed method, a public version of our\napproach is available to download for the neuroimaging community. \n\n"}
{"id": "1709.09215", "contents": "Title: Understanding Infographics through Textual and Visual Tag Prediction Abstract: We introduce the problem of visual hashtag discovery for infographics:\nextracting visual elements from an infographic that are diagnostic of its\ntopic. Given an infographic as input, our computational approach automatically\noutputs textual and visual elements predicted to be representative of the\ninfographic content. Concretely, from a curated dataset of 29K large\ninfographic images sampled across 26 categories and 391 tags, we present an\nautomated two step approach. First, we extract the text from an infographic and\nuse it to predict text tags indicative of the infographic content. And second,\nwe use these predicted text tags as a supervisory signal to localize the most\ndiagnostic visual elements from within the infographic i.e. visual hashtags. We\nreport performances on a categorization and multi-label tag prediction problem\nand compare our proposed visual hashtags to human annotations. \n\n"}
{"id": "1709.10282", "contents": "Title: Deep Competitive Pathway Networks Abstract: In the design of deep neural architectures, recent studies have demonstrated\nthe benefits of grouping subnetworks into a larger network. For examples, the\nInception architecture integrates multi-scale subnetworks and the residual\nnetwork can be regarded that a residual unit combines a residual subnetwork\nwith an identity shortcut. In this work, we embrace this observation and\npropose the Competitive Pathway Network (CoPaNet). The CoPaNet comprises a\nstack of competitive pathway units and each unit contains multiple parallel\nresidual-type subnetworks followed by a max operation for feature competition.\nThis mechanism enhances the model capability by learning a variety of features\nin subnetworks. The proposed strategy explicitly shows that the features\npropagate through pathways in various routing patterns, which is referred to as\npathway encoding of category information. Moreover, the cross-block shortcut\ncan be added to the CoPaNet to encourage feature reuse. We evaluated the\nproposed CoPaNet on four object recognition benchmarks: CIFAR-10, CIFAR-100,\nSVHN, and ImageNet. CoPaNet obtained the state-of-the-art or comparable results\nusing similar amounts of parameters. The code of CoPaNet is available at:\nhttps://github.com/JiaRenChang/CoPaNet. \n\n"}
{"id": "1709.10433", "contents": "Title: On the Capacity of Face Representation Abstract: In this paper we address the following question, given a face representation,\nhow many identities can it resolve? In other words, what is the capacity of the\nface representation? A scientific basis for estimating the capacity of a given\nface representation will not only benefit the evaluation and comparison of\ndifferent representation methods, but will also establish an upper bound on the\nscalability of an automatic face recognition system. We cast the face capacity\nproblem in terms of packing bounds on a low-dimensional manifold embedded\nwithin a deep representation space. By explicitly accounting for the manifold\nstructure of the representation as well two different sources of\nrepresentational noise: epistemic (model) uncertainty and aleatoric (data)\nvariability, our approach is able to estimate the capacity of a given face\nrepresentation. To demonstrate the efficacy of our approach, we estimate the\ncapacity of two deep neural network based face representations, namely\n128-dimensional FaceNet and 512-dimensional SphereFace. Numerical experiments\non unconstrained faces (IJB-C) provides a capacity upper bound of\n$2.7\\times10^4$ for FaceNet and $8.4\\times10^4$ for SphereFace representation\nat a false acceptance rate (FAR) of 1%. As expected, capacity reduces\ndrastically at lower FARs. The capacity at FAR of 0.1% and 0.001% is\n$2.2\\times10^3$ and $1.6\\times10^{1}$, respectively for FaceNet and\n$3.6\\times10^3$ and $6.0\\times10^0$, respectively for SphereFace. \n\n"}
{"id": "1709.10507", "contents": "Title: Vision-based deep execution monitoring Abstract: Execution monitor of high-level robot actions can be effectively improved by\nvisual monitoring the state of the world in terms of preconditions and\npostconditions that hold before and after the execution of an action.\nFurthermore a policy for searching where to look at, either for verifying the\nrelations that specify the pre and postconditions or to refocus in case of a\nfailure, can tremendously improve the robot execution in an uncharted\nenvironment. It is now possible to strongly rely on visual perception in order\nto make the assumption that the environment is observable, by the amazing\nresults of deep learning. In this work we present visual execution monitoring\nfor a robot executing tasks in an uncharted Lab environment. The execution\nmonitor interacts with the environment via a visual stream that uses two DCNN\nfor recognizing the objects the robot has to deal with and manipulate, and a\nnon-parametric Bayes estimation to discover the relations out of the DCNN\nfeatures. To recover from lack of focus and failures due to missed objects we\nresort to visual search policies via deep reinforcement learning. \n\n"}
{"id": "1710.01820", "contents": "Title: Energy-Based Spherical Sparse Coding Abstract: In this paper, we explore an efficient variant of convolutional sparse coding\nwith unit norm code vectors where reconstruction quality is evaluated using an\ninner product (cosine distance). To use these codes for discriminative\nclassification, we describe a model we term Energy-Based Spherical Sparse\nCoding (EB-SSC) in which the hypothesized class label introduces a learned\nlinear bias into the coding step. We evaluate and visualize performance of\nstacking this encoder to make a deep layered model for image classification. \n\n"}
{"id": "1710.02322", "contents": "Title: Human Pose Regression by Combining Indirect Part Detection and\n  Contextual Information Abstract: In this paper, we propose an end-to-end trainable regression approach for\nhuman pose estimation from still images. We use the proposed Soft-argmax\nfunction to convert feature maps directly to joint coordinates, resulting in a\nfully differentiable framework. Our method is able to learn heat maps\nrepresentations indirectly, without additional steps of artificial ground truth\ngeneration. Consequently, contextual information can be included to the pose\npredictions in a seamless way. We evaluated our method on two very challenging\ndatasets, the Leeds Sports Poses (LSP) and the MPII Human Pose datasets,\nreaching the best performance among all the existing regression methods and\ncomparable results to the state-of-the-art detection based approaches. \n\n"}
{"id": "1710.04076", "contents": "Title: Deep Semantic Abstractions of Everyday Human Activities: On Commonsense\n  Representations of Human Interactions Abstract: We propose a deep semantic characterization of space and motion categorically\nfrom the viewpoint of grounding embodied human-object interactions. Our key\nfocus is on an ontological model that would be adept to formalisation from the\nviewpoint of commonsense knowledge representation, relational learning, and\nqualitative reasoning about space and motion in cognitive robotics settings. We\ndemonstrate key aspects of the space & motion ontology and its formalization as\na representational framework in the backdrop of select examples from a dataset\nof everyday activities. Furthermore, focussing on human-object interaction data\nobtained from RGBD sensors, we also illustrate how declarative\n(spatio-temporal) reasoning in the (constraint) logic programming family may be\nperformed with the developed deep semantic abstractions. \n\n"}
{"id": "1710.04778", "contents": "Title: Retinal Fluid Segmentation and Detection in Optical Coherence Tomography\n  Images using Fully Convolutional Neural Network Abstract: As a non-invasive imaging modality, optical coherence tomography (OCT) can\nprovide micrometer-resolution 3D images of retinal structures. Therefore it is\ncommonly used in the diagnosis of retinal diseases associated with edema in and\nunder the retinal layers. In this paper, a new framework is proposed for the\ntask of fluid segmentation and detection in retinal OCT images. Based on the\nraw images and layers segmented by a graph-cut algorithm, a fully convolutional\nneural network was trained to recognize and label the fluid pixels. Random\nforest classification was performed on the segmented fluid regions to detect\nand reject the falsely labeled fluid regions. The leave-one-out cross\nvalidation experiments on the RETOUCH database show that our method performs\nwell in both segmentation (mean Dice: 0.7317) and detection (mean AUC: 0.985)\ntasks. \n\n"}
{"id": "1710.05179", "contents": "Title: Regularizing Deep Neural Networks by Noise: Its Interpretation and\n  Optimization Abstract: Overfitting is one of the most critical challenges in deep neural networks,\nand there are various types of regularization methods to improve generalization\nperformance. Injecting noises to hidden units during training, e.g., dropout,\nis known as a successful regularizer, but it is still not clear enough why such\ntraining techniques work well in practice and how we can maximize their benefit\nin the presence of two conflicting objectives---optimizing to true data\ndistribution and preventing overfitting by regularization. This paper addresses\nthe above issues by 1) interpreting that the conventional training methods with\nregularization by noise injection optimize the lower bound of the true\nobjective and 2) proposing a technique to achieve a tighter lower bound using\nmultiple noise samples per training example in a stochastic gradient descent\niteration. We demonstrate the effectiveness of our idea in several computer\nvision applications. \n\n"}
{"id": "1710.05539", "contents": "Title: Unfamiliar Aspects of B\\\"acklund Transformations and an Associated\n  Degasperis-Procesi Equation Abstract: We summarize the results of our recent work on B\\\"acklund transformations\n(BTs), particularly focusing on the relationship of BTs and infinitesimal\nsymmetries. We present a BT for an associated Degasperis-Procesi (aDP) equation\nand its superposition principle, and investigate the solutions generated by\napplication of this BT. Following our general methodology, we use the\nsuperposition principle of the BT to generate the infinitesimal symmetries of\nthe aDP equation. \n\n"}
{"id": "1710.05758", "contents": "Title: TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization Abstract: Recent research implies that training and inference of deep neural networks\n(DNN) can be computed with low precision numerical representations of the\ntraining/test data, weights and gradients without a general loss in accuracy.\nThe benefit of such compact representations is twofold: they allow a\nsignificant reduction of the communication bottleneck in distributed DNN\ntraining and faster neural network implementations on hardware accelerators\nlike FPGAs. Several quantization methods have been proposed to map the original\n32-bit floating point problem to low-bit representations. While most related\npublications validate the proposed approach on a single DNN topology, it\nappears to be evident, that the optimal choice of the quantization method and\nnumber of coding bits is topology dependent. To this end, there is no general\ntheory available, which would allow users to derive the optimal quantization\nduring the design of a DNN topology. In this paper, we present a quantization\ntool box for the TensorFlow framework. TensorQuant allows a transparent\nquantization simulation of existing DNN topologies during training and\ninference. TensorQuant supports generic quantization methods and allows\nexperimental evaluation of the impact of the quantization on single layers as\nwell as on the full topology. In a first series of experiments with\nTensorQuant, we show an analysis of fix-point quantizations of popular CNN\ntopologies. \n\n"}
{"id": "1710.07307", "contents": "Title: Interpretable Transformations with Encoder-Decoder Networks Abstract: Deep feature spaces have the capacity to encode complex transformations of\ntheir input data. However, understanding the relative feature-space\nrelationship between two transformed encoded images is difficult. For instance,\nwhat is the relative feature space relationship between two rotated images?\nWhat is decoded when we interpolate in feature space? Ideally, we want to\ndisentangle confounding factors, such as pose, appearance, and illumination,\nfrom object identity. Disentangling these is difficult because they interact in\nvery nonlinear ways. We propose a simple method to construct a deep feature\nspace, with explicitly disentangled representations of several known\ntransformations. A person or algorithm can then manipulate the disentangled\nrepresentation, for example, to re-render an image with explicit control over\nparameterized degrees of freedom. The feature space is constructed using a\ntransforming encoder-decoder network with a custom feature transform layer,\nacting on the hidden representations. We demonstrate the advantages of explicit\ndisentangling on a variety of datasets and transformations, and as an aid for\ntraditional tasks, such as classification. \n\n"}
{"id": "1710.07368", "contents": "Title: SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time\n  Road-Object Segmentation from 3D LiDAR Point Cloud Abstract: In this paper, we address semantic segmentation of road-objects from 3D LiDAR\npoint clouds. In particular, we wish to detect and categorize instances of\ninterest, such as cars, pedestrians and cyclists. We formulate this problem as\na point- wise classification problem, and propose an end-to-end pipeline called\nSqueezeSeg based on convolutional neural networks (CNN): the CNN takes a\ntransformed LiDAR point cloud as input and directly outputs a point-wise label\nmap, which is then refined by a conditional random field (CRF) implemented as a\nrecurrent layer. Instance-level labels are then obtained by conventional\nclustering algorithms. Our CNN model is trained on LiDAR point clouds from the\nKITTI dataset, and our point-wise segmentation labels are derived from 3D\nbounding boxes from KITTI. To obtain extra training data, we built a LiDAR\nsimulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize\nlarge amounts of realistic training data. Our experiments show that SqueezeSeg\nachieves high accuracy with astonishingly fast and stable runtime (8.7 ms per\nframe), highly desirable for autonomous driving applications. Furthermore,\nadditionally training on synthesized data boosts validation accuracy on\nreal-world data. Our source code and synthesized data will be open-sourced. \n\n"}
{"id": "1710.07991", "contents": "Title: Rethinking Convolutional Semantic Segmentation Learning Abstract: Deep convolutional semantic segmentation (DCSS) learning doesn't converge to\nan optimal local minimum with random parameters initializations; a pre-trained\nmodel on the same domain becomes necessary to achieve convergence.In this work,\nwe propose a joint cooperative end-to-end learning method for DCSS. It\naddresses many drawbacks with existing deep semantic segmentation learning; the\nproposed approach simultaneously learn both segmentation and classification;\ntaking away the essential need of the pre-trained model for learning\nconvergence. We present an improved inception based architecture with partial\nattention gating (PAG) over encoder information. The PAG also adds to achieve\nfaster convergence and better accuracy for segmentation task. We will show the\neffectiveness of this learning on a diabetic retinopathy classification and\nsegmentation dataset. \n\n"}
{"id": "1710.08177", "contents": "Title: Progressive Learning for Systematic Design of Large Neural Networks Abstract: We develop an algorithm for systematic design of a large artificial neural\nnetwork using a progression property. We find that some non-linear functions,\nsuch as the rectifier linear unit and its derivatives, hold the property. The\nsystematic design addresses the choice of network size and regularization of\nparameters. The number of nodes and layers in network increases in progression\nwith the objective of consistently reducing an appropriate cost. Each layer is\noptimized at a time, where appropriate parameters are learned using convex\noptimization. Regularization parameters for convex optimization do not need a\nsignificant manual effort for tuning. We also use random instances for some\nweight matrices, and that helps to reduce the number of parameters we learn.\nThe developed network is expected to show good generalization power due to\nappropriate regularization and use of random weights in the layers. This\nexpectation is verified by extensive experiments for classification and\nregression problems, using standard databases. \n\n"}
{"id": "1710.09008", "contents": "Title: The Shape of an Image: A Study of Mapper on Images Abstract: We study the topological construction called Mapper in the context of simply\nconnected domains, in particular on images. The Mapper construction can be\nconsidered as a generalization for contour, split, and joint trees on simply\nconnected domains. A contour tree on an image domain assumes the height\nfunction to be a piecewise linear Morse function. This is a rather restrictive\nclass of functions and does not allow us to explore the topology for most real\nworld images. The Mapper construction avoids this limitation by assuming only\ncontinuity on the height function allowing this construction to robustly deal\nwith a significant larger set of images. We provide a customized construction\nfor Mapper on images, give a fast algorithm to compute it, and show how to\nsimplify the Mapper structure in this case. Finally, we provide a simple\nprocedure that guarantees the equivalence of Mapper to contour, join, and split\ntrees on a simply connected domain. \n\n"}
{"id": "1710.10686", "contents": "Title: Regularization for Deep Learning: A Taxonomy Abstract: Regularization is one of the crucial ingredients of deep learning, yet the\nterm regularization has various definitions, and regularization methods are\noften studied separately from each other. In our work we present a systematic,\nunifying taxonomy to categorize existing methods. We distinguish methods that\naffect data, network architectures, error terms, regularization terms, and\noptimization procedures. We do not provide all details about the listed\nmethods; instead, we present an overview of how the methods can be sorted into\nmeaningful categories and sub-categories. This helps revealing links and\nfundamental similarities between them. Finally, we include practical\nrecommendations both for users and for developers of new regularization\nmethods. \n\n"}
{"id": "1710.10928", "contents": "Title: Optimization Landscape and Expressivity of Deep CNNs Abstract: We analyze the loss landscape and expressiveness of practical deep\nconvolutional neural networks (CNNs) with shared weights and max pooling\nlayers. We show that such CNNs produce linearly independent features at a\n\"wide\" layer which has more neurons than the number of training samples. This\ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide\nCNNs necessary and sufficient conditions for global minima with zero training\nerror. For the case where the wide layer is followed by a fully connected layer\nwe show that almost every critical point of the empirical loss is a global\nminimum with zero training error. Our analysis suggests that both depth and\nwidth are very important in deep learning. While depth brings more\nrepresentational power and allows the network to learn high level features,\nwidth smoothes the optimization landscape of the loss function in the sense\nthat a sufficiently wide network has a well-behaved loss surface with almost no\nbad local minima. \n\n"}
{"id": "1710.11063", "contents": "Title: Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks Abstract: Over the last decade, Convolutional Neural Network (CNN) models have been\nhighly successful in solving complex vision problems. However, these deep\nmodels are perceived as \"black box\" methods considering the lack of\nunderstanding of their internal functioning. There has been a significant\nrecent interest in developing explainable deep learning models, and this paper\nis an effort in this direction. Building on a recently proposed method called\nGrad-CAM, we propose a generalized method called Grad-CAM++ that can provide\nbetter visual explanations of CNN model predictions, in terms of better object\nlocalization as well as explaining occurrences of multiple object instances in\na single image, when compared to state-of-the-art. We provide a mathematical\nderivation for the proposed method, which uses a weighted combination of the\npositive partial derivatives of the last convolutional layer feature maps with\nrespect to a specific class score as weights to generate a visual explanation\nfor the corresponding class label. Our extensive experiments and evaluations,\nboth subjective and objective, on standard datasets showed that Grad-CAM++\nprovides promising human-interpretable visual explanations for a given CNN\narchitecture across multiple tasks including classification, image caption\ngeneration and 3D action recognition; as well as in new settings such as\nknowledge distillation. \n\n"}
{"id": "1711.00199", "contents": "Title: PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in\n  Cluttered Scenes Abstract: Estimating the 6D pose of known objects is important for robots to interact\nwith the real world. The problem is challenging due to the variety of objects\nas well as the complexity of a scene caused by clutter and occlusions between\nobjects. In this work, we introduce PoseCNN, a new Convolutional Neural Network\nfor 6D object pose estimation. PoseCNN estimates the 3D translation of an\nobject by localizing its center in the image and predicting its distance from\nthe camera. The 3D rotation of the object is estimated by regressing to a\nquaternion representation. We also introduce a novel loss function that enables\nPoseCNN to handle symmetric objects. In addition, we contribute a large scale\nvideo dataset for 6D object pose estimation named the YCB-Video dataset. Our\ndataset provides accurate 6D poses of 21 objects from the YCB dataset observed\nin 92 videos with 133,827 frames. We conduct extensive experiments on our\nYCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is\nhighly robust to occlusions, can handle symmetric objects, and provide accurate\npose estimation using only color images as input. When using depth data to\nfurther refine the poses, our approach achieves state-of-the-art results on the\nchallenging OccludedLINEMOD dataset. Our code and dataset are available at\nhttps://rse-lab.cs.washington.edu/projects/posecnn/. \n\n"}
{"id": "1711.00207", "contents": "Title: Learning deep features for source color laser printer identification\n  based on cascaded learning Abstract: Color laser printers have fast printing speed and high resolution, and\nforgeries using color laser printers can cause significant harm to society. A\nsource printer identification technique can be employed as a countermeasure to\nthose forgeries. This paper presents a color laser printer identification\nmethod based on cascaded learning of deep neural networks. The refiner network\nis trained by adversarial training to refine the synthetic dataset for halftone\ncolor decomposition. The halftone color decomposing ConvNet is trained with the\nrefined dataset, and the trained knowledge is transferred to the printer\nidentifying ConvNet to enhance the accuracy. The robustness about rotation and\nscaling is considered in training process, which is not considered in existing\nmethods. Experiments are performed on eight color laser printers, and the\nperformance is compared with several existing methods. The experimental results\nclearly show that the proposed method outperforms existing source color laser\nprinter identification methods. \n\n"}
{"id": "1711.00253", "contents": "Title: Adversarial Learning of Structure-Aware Fully Convolutional Networks for\n  Landmark Localization Abstract: Landmark/pose estimation in single monocular images have received much effort\nin computer vision due to its important applications. It remains a challenging\ntask when input images severe occlusions caused by, e.g., adverse camera views.\nUnder such circumstances, biologically implausible pose predictions may be\nproduced. In contrast, human vision is able to predict poses by exploiting\ngeometric constraints of landmark point inter-connectivity. To address the\nproblem, by incorporating priors about the structure of pose components, we\npropose a novel structure-aware fully convolutional network to implicitly take\nsuch priors into account during training of the deep network. Explicit learning\nof such constraints is typically challenging. Instead, inspired by how human\nidentifies implausible poses, we design discriminators to distinguish the real\nposes from the fake ones (such as biologically implausible ones). If the pose\ngenerator G generates results that the discriminator fails to distinguish from\nreal ones, the network successfully learns the priors. Training of the network\nfollows the strategy of conditional Generative Adversarial Networks (GANs). The\neffectiveness of the proposed network is evaluated on three pose-related tasks:\n2D single human pose estimation, 2D facial landmark estimation and 3D single\nhuman pose estimation. The proposed approach significantly outperforms the\nstate-of-the-art methods and almost always generates plausible pose\npredictions, demonstrating the usefulness of implicit learning of structures\nusing GANs. \n\n"}
{"id": "1711.00888", "contents": "Title: Set-to-Set Hashing with Applications in Visual Recognition Abstract: Visual data, such as an image or a sequence of video frames, is often\nnaturally represented as a point set. In this paper, we consider the\nfundamental problem of finding a nearest set from a collection of sets, to a\nquery set. This problem has obvious applications in large-scale visual\nretrieval and recognition, and also in applied fields beyond computer vision.\nOne challenge stands out in solving the problem---set representation and\nmeasure of similarity. Particularly, the query set and the sets in dataset\ncollection can have varying cardinalities. The training collection is large\nenough such that linear scan is impractical. We propose a simple representation\nscheme that encodes both statistical and structural information of the sets.\nThe derived representations are integrated in a kernel framework for flexible\nsimilarity measurement. For the query set process, we adopt a learning-to-hash\npipeline that turns the kernel representations into hash bits based on simple\nlearners, using multiple kernel learning. Experiments on two visual retrieval\ndatasets show unambiguously that our set-to-set hashing framework outperforms\nprior methods that do not take the set-to-set search setting. \n\n"}
{"id": "1711.01714", "contents": "Title: End-to-End Video Classification with Knowledge Graphs Abstract: Video understanding has attracted much research attention especially since\nthe recent availability of large-scale video benchmarks. In this paper, we\naddress the problem of multi-label video classification. We first observe that\nthere exists a significant knowledge gap between how machines and humans learn.\nThat is, while current machine learning approaches including deep neural\nnetworks largely focus on the representations of the given data, humans often\nlook beyond the data at hand and leverage external knowledge to make better\ndecisions. Towards narrowing the gap, we propose to incorporate external\nknowledge graphs into video classification. In particular, we unify traditional\n\"knowledgeless\" machine learning models and knowledge graphs in a novel\nend-to-end framework. The framework is flexible to work with most existing\nvideo classification algorithms including state-of-the-art deep models.\nFinally, we conduct extensive experiments on the largest public video dataset\nYouTube-8M. The results are promising across the board, improving mean average\nprecision by up to 2.9%. \n\n"}
{"id": "1711.01889", "contents": "Title: Radical analysis network for zero-shot learning in printed Chinese\n  character recognition Abstract: Chinese characters have a huge set of character categories, more than 20,000\nand the number is still increasing as more and more novel characters continue\nbeing created. However, the enormous characters can be decomposed into a\ncompact set of about 500 fundamental and structural radicals. This paper\nintroduces a novel radical analysis network (RAN) to recognize printed Chinese\ncharacters by identifying radicals and analyzing two-dimensional spatial\nstructures among them. The proposed RAN first extracts visual features from\ninput by employing convolutional neural networks as an encoder. Then a decoder\nbased on recurrent neural networks is employed, aiming at generating captions\nof Chinese characters by detecting radicals and two-dimensional structures\nthrough a spatial attention mechanism. The manner of treating a Chinese\ncharacter as a composition of radicals rather than a single character class\nlargely reduces the size of vocabulary and enables RAN to possess the ability\nof recognizing unseen Chinese character classes, namely zero-shot learning. \n\n"}
{"id": "1711.02856", "contents": "Title: Transductive Zero-Shot Hashing via Coarse-to-Fine Similarity Mining Abstract: Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes\nwithout training data, which is an important and challenging problem. Most\nexisting ZSH approaches exploit transfer learning via an intermediate shared\nsemantic representations between the seen/source classes and novel/target\nclasses. However, due to having disjoint, the hash functions learned from the\nsource dataset are biased when applied directly to the target classes. In this\npaper, we study the transductive ZSH, i.e., we have unlabeled data for novel\nclasses. We put forward a simple yet efficient joint learning approach via\ncoarse-to-fine similarity mining which transfers knowledges from source data to\ntarget data. It mainly consists of two building blocks in the proposed deep\narchitecture: 1) a shared two-streams network, which the first stream operates\non the source data and the second stream operates on the unlabeled data, to\nlearn the effective common image representations, and 2) a coarse-to-fine\nmodule, which begins with finding the most representative images from target\nclasses and then further detect similarities among these images, to transfer\nthe similarities of the source data to the target data in a greedy fashion.\nExtensive evaluation results on several benchmark datasets demonstrate that the\nproposed hashing method achieves significant improvement over the\nstate-of-the-art methods. \n\n"}
{"id": "1711.04178", "contents": "Title: CUR Decompositions, Similarity Matrices, and Subspace Clustering Abstract: A general framework for solving the subspace clustering problem using the CUR\ndecomposition is presented. The CUR decomposition provides a natural way to\nconstruct similarity matrices for data that come from a union of unknown\nsubspaces $\\mathscr{U}=\\underset{i=1}{\\overset{M}\\bigcup}S_i$. The similarity\nmatrices thus constructed give the exact clustering in the noise-free case.\nAdditionally, this decomposition gives rise to many distinct similarity\nmatrices from a given set of data, which allow enough flexibility to perform\naccurate clustering of noisy data. We also show that two known methods for\nsubspace clustering can be derived from the CUR decomposition. An algorithm\nbased on the theoretical construction of similarity matrices is presented, and\nexperiments on synthetic and real data are presented to test the method.\n  Additionally, an adaptation of our CUR based similarity matrices is utilized\nto provide a heuristic algorithm for subspace clustering; this algorithm yields\nthe best overall performance to date for clustering the Hopkins155 motion\nsegmentation dataset. \n\n"}
{"id": "1711.05769", "contents": "Title: PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning Abstract: This paper presents a method for adding multiple tasks to a single deep\nneural network while avoiding catastrophic forgetting. Inspired by network\npruning techniques, we exploit redundancies in large deep networks to free up\nparameters that can then be employed to learn new tasks. By performing\niterative pruning and network re-training, we are able to sequentially \"pack\"\nmultiple tasks into a single network while ensuring minimal drop in performance\nand minimal storage overhead. Unlike prior work that uses proxy losses to\nmaintain accuracy on older tasks, we always optimize for the task at hand. We\nperform extensive experiments on a variety of network architectures and\nlarge-scale datasets, and observe much better robustness against catastrophic\nforgetting than prior work. In particular, we are able to add three\nfine-grained classification tasks to a single ImageNet-trained VGG-16 network\nand achieve accuracies close to those of separately trained networks for each\ntask. Code available at https://github.com/arunmallya/packnet \n\n"}
{"id": "1711.05918", "contents": "Title: Priming Neural Networks Abstract: Visual priming is known to affect the human visual system to allow detection\nof scene elements, even those that may have been near unnoticeable before, such\nas the presence of camouflaged animals. This process has been shown to be an\neffect of top-down signaling in the visual system triggered by the said cue. In\nthis paper, we propose a mechanism to mimic the process of priming in the\ncontext of object detection and segmentation. We view priming as having a\nmodulatory, cue dependent effect on layers of features within a network. Our\nresults show how such a process can be complementary to, and at times more\neffective than simple post-processing applied to the output of the network,\nnotably so in cases where the object is hard to detect such as in severe noise.\nMoreover, we find the effects of priming are sometimes stronger when early\nvisual layers are affected. Overall, our experiments confirm that top-down\nsignals can go a long way in improving object detection and segmentation. \n\n"}
{"id": "1711.05998", "contents": "Title: Minimizing Supervision for Free-space Segmentation Abstract: Identifying \"free-space,\" or safely driveable regions in the scene ahead, is\na fundamental task for autonomous navigation. While this task can be addressed\nusing semantic segmentation, the manual labor involved in creating pixelwise\nannotations to train the segmentation model is very costly. Although weakly\nsupervised segmentation addresses this issue, most methods are not designed for\nfree-space. In this paper, we observe that homogeneous texture and location are\ntwo key characteristics of free-space, and develop a novel, practical framework\nfor free-space segmentation with minimal human supervision. Our experiments\nshow that our framework performs better than other weakly supervised methods\nwhile using less supervision. Our work demonstrates the potential for\nperforming free-space segmentation without tedious and costly manual\nannotation, which will be important for adapting autonomous driving systems to\ndifferent types of vehicles and environments \n\n"}
{"id": "1711.06011", "contents": "Title: DIMAL: Deep Isometric Manifold Learning Using Sparse Geodesic Sampling Abstract: This paper explores a fully unsupervised deep learning approach for computing\ndistance-preserving maps that generate low-dimensional embeddings for a certain\nclass of manifolds. We use the Siamese configuration to train a neural network\nto solve the problem of least squares multidimensional scaling for generating\nmaps that approximately preserve geodesic distances. By training with only a\nfew landmarks, we show a significantly improved local and nonlocal\ngeneralization of the isometric mapping as compared to analogous non-parametric\ncounterparts. Importantly, the combination of a deep-learning framework with a\nmultidimensional scaling objective enables a numerical analysis of network\narchitectures to aid in understanding their representation power. This provides\na geometric perspective to the generalizability of deep learning. \n\n"}
{"id": "1711.06368", "contents": "Title: Mobile Video Object Detection with Temporally-Aware Feature Maps Abstract: This paper introduces an online model for object detection in videos designed\nto run in real-time on low-powered mobile and embedded devices. Our approach\ncombines fast single-image object detection with convolutional long short term\nmemory (LSTM) layers to create an interweaved recurrent-convolutional\narchitecture. Additionally, we propose an efficient Bottleneck-LSTM layer that\nsignificantly reduces computational cost compared to regular LSTMs. Our network\nachieves temporal awareness by using Bottleneck-LSTMs to refine and propagate\nfeature maps across frames. This approach is substantially faster than existing\ndetection methods in video, outperforming the fastest single-frame models in\nmodel size and computational cost while attaining accuracy comparable to much\nmore expensive single-frame models on the Imagenet VID 2015 dataset. Our model\nreaches a real-time inference speed of up to 15 FPS on a mobile CPU. \n\n"}
{"id": "1711.06373", "contents": "Title: Thoracic Disease Identification and Localization with Limited\n  Supervision Abstract: Accurate identification and localization of abnormalities from radiology\nimages play an integral part in clinical diagnosis and treatment planning.\nBuilding a highly accurate prediction model for these tasks usually requires a\nlarge number of images manually annotated with labels and finding sites of\nabnormalities. In reality, however, such annotated data are expensive to\nacquire, especially the ones with location annotations. We need methods that\ncan work well with only a small amount of location annotations. To address this\nchallenge, we present a unified approach that simultaneously performs disease\nidentification and localization through the same underlying model for all\nimages. We demonstrate that our approach can effectively leverage both class\ninformation as well as limited location annotation, and significantly\noutperforms the comparative reference baseline in both classification and\nlocalization tasks. \n\n"}
{"id": "1711.07068", "contents": "Title: Diverse and Accurate Image Description Using a Variational Auto-Encoder\n  with an Additive Gaussian Encoding Space Abstract: This paper explores image caption generation using conditional variational\nauto-encoders (CVAEs). Standard CVAEs with a fixed Gaussian prior yield\ndescriptions with too little variability. Instead, we propose two models that\nexplicitly structure the latent space around $K$ components corresponding to\ndifferent types of image content, and combine components to create priors for\nimages that contain multiple types of content simultaneously (e.g., several\nkinds of objects). Our first model uses a Gaussian Mixture model (GMM) prior,\nwhile the second one defines a novel Additive Gaussian (AG) prior that linearly\ncombines component means. We show that both models produce captions that are\nmore diverse and more accurate than a strong LSTM baseline or a \"vanilla\" CVAE\nwith a fixed Gaussian prior, with AG-CVAE showing particular promise. \n\n"}
{"id": "1711.07231", "contents": "Title: Stochastic metamorphosis with template uncertainties Abstract: In this paper, we investigate two stochastic perturbations of the\nmetamorphosis equations of image analysis, in the geometrical context of the\nEuler-Poincar\\'e theory. In the metamorphosis of images, the Lie group of\ndiffeomorphisms deforms a template image that is undergoing its own internal\ndynamics as it deforms. This type of deformation allows more freedom for image\nmatching and has analogies with complex fluids when the template properties are\nregarded as order parameters (coset spaces of broken symmetries). The first\nstochastic perturbation we consider corresponds to uncertainty due to random\nerrors in the reconstruction of the deformation map from its vector field. We\nalso consider a second stochastic perturbation, which compounds the uncertainty\nin of the deformation map with the uncertainty in the reconstruction of the\ntemplate position from its velocity field. We apply this general geometric\ntheory to several classical examples, including landmarks, images, and closed\ncurves, and we discuss its use for functional data analysis. \n\n"}
{"id": "1711.07767", "contents": "Title: Receptive Field Block Net for Accurate and Fast Object Detection Abstract: Current top-performing object detectors depend on deep CNN backbones, such as\nResNet-101 and Inception, benefiting from their powerful feature\nrepresentations but suffering from high computational costs. Conversely, some\nlightweight model based detectors fulfil real time processing, while their\naccuracies are often criticized. In this paper, we explore an alternative to\nbuild a fast and accurate detector by strengthening lightweight features using\na hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs)\nin human visual systems, we propose a novel RF Block (RFB) module, which takes\nthe relationship between the size and eccentricity of RFs into account, to\nenhance the feature discriminability and robustness. We further assemble RFB to\nthe top of SSD, constructing the RFB Net detector. To evaluate its\neffectiveness, experiments are conducted on two major benchmarks and the\nresults show that RFB Net is able to reach the performance of advanced very\ndeep detectors while keeping the real-time speed. Code is available at\nhttps://github.com/ruinmessi/RFBNet. \n\n"}
{"id": "1711.08364", "contents": "Title: ForestHash: Semantic Hashing With Shallow Random Forests and Tiny\n  Convolutional Networks Abstract: Hash codes are efficient data representations for coping with the ever\ngrowing amounts of data. In this paper, we introduce a random forest semantic\nhashing scheme that embeds tiny convolutional neural networks (CNN) into\nshallow random forests, with near-optimal information-theoretic code\naggregation among trees. We start with a simple hashing scheme, where random\ntrees in a forest act as hashing functions by setting `1' for the visited tree\nleaf, and `0' for the rest. We show that traditional random forests fail to\ngenerate hashes that preserve the underlying similarity between the trees,\nrendering the random forests approach to hashing challenging. To address this,\nwe propose to first randomly group arriving classes at each tree split node\ninto two groups, obtaining a significantly simplified two-class classification\nproblem, which can be handled using a light-weight CNN weak learner. Such\nrandom class grouping scheme enables code uniqueness by enforcing each class to\nshare its code with different classes in different trees. A non-conventional\nlow-rank loss is further adopted for the CNN weak learners to encourage code\nconsistency by minimizing intra-class variations and maximizing inter-class\ndistance for the two random class groups. Finally, we introduce an\ninformation-theoretic approach for aggregating codes of individual trees into a\nsingle hash code, producing a near-optimal unique hash for each class. The\nproposed approach significantly outperforms state-of-the-art hashing methods\nfor image retrieval tasks on large-scale public datasets, while performing at\nthe level of other state-of-the-art image classification techniques while\nutilizing a more compact and efficient scalable representation. This work\nproposes a principled and robust procedure to train and deploy in parallel an\nensemble of light-weight CNNs, instead of simply going deeper. \n\n"}
{"id": "1711.08419", "contents": "Title: Nonlocal reductions of the Ablowitz-Ladik equation Abstract: The purpose of the present paper is to develop the inverse scattering\ntransform for the nonlocal semi-discrete nonlinear Schrodinger equation (known\nas Ablowitz-Ladik equation) with PT-symmetry. This includes: the eigenfunctions\n(Jost solutions) of the associated Lax pair, the scattering data and the\nfundamental analytic solutions. In addition, the paper studies the spectral\nproperties of the associated discrete Lax operator. Based on the formulated\n(additive) Riemann-Hilbert problem, the 1- and 2-soliton solutions for the\nnonlocal Ablowitz-Ladik equation are derived. Finally, the completeness\nrelation for the associated Jost solutions is proved. Based on this, the\nexpansion formula over the complete set of Jost solutions is derived. This will\nallow one to interpret the inverse scattering transform as a generalised\nFourier transform. \n\n"}
{"id": "1711.08496", "contents": "Title: Temporal Relational Reasoning in Videos Abstract: Temporal relational reasoning, the ability to link meaningful transformations\nof objects or entities over time, is a fundamental property of intelligent\nspecies. In this paper, we introduce an effective and interpretable network\nmodule, the Temporal Relation Network (TRN), designed to learn and reason about\ntemporal dependencies between video frames at multiple time scales. We evaluate\nTRN-equipped networks on activity recognition tasks using three recent video\ndatasets - Something-Something, Jester, and Charades - which fundamentally\ndepend on temporal relational reasoning. Our results demonstrate that the\nproposed TRN gives convolutional neural networks a remarkable capacity to\ndiscover temporal relations in videos. Through only sparsely sampled video\nframes, TRN-equipped networks can accurately predict human-object interactions\nin the Something-Something dataset and identify various human gestures on the\nJester dataset with very competitive performance. TRN-equipped networks also\noutperform two-stream networks and 3D convolution networks in recognizing daily\nactivities in the Charades dataset. Further analyses show that the models learn\nintuitive and interpretable visual common sense knowledge in videos. \n\n"}
{"id": "1711.08608", "contents": "Title: Unsupervised End-to-end Learning for Deformable Medical Image\n  Registration Abstract: We propose a registration algorithm for 2D CT/MRI medical images with a new\nunsupervised end-to-end strategy using convolutional neural networks. The\ncontributions of our algorithm are threefold: (1) We transplant traditional\nimage registration algorithms to an end-to-end convolutional neural network\nframework, while maintaining the unsupervised nature of image registration\nproblems. The image-to-image integrated framework can simultaneously learn both\nimage features and transformation matrix for registration. (2) Training with\nadditional data without any label can further improve the registration\nperformance by approximately 10 %. (3) The registration speed is 100x faster\nthan traditional methods. The proposed network is easy to implement and can be\ntrained efficiently. Experiments demonstrate that our system achieves\nstate-of-the-art results on 2D brain registration and achieves comparable\nresults on 2D liver registration. It can be extended to register other organs\nbeyond liver and brain such as kidney, lung, and heart. \n\n"}
{"id": "1711.09219", "contents": "Title: Stacked Kernel Network Abstract: Kernel methods are powerful tools to capture nonlinear patterns behind data.\nThey implicitly learn high (even infinite) dimensional nonlinear features in\nthe Reproducing Kernel Hilbert Space (RKHS) while making the computation\ntractable by leveraging the kernel trick. Classic kernel methods learn a single\nlayer of nonlinear features, whose representational power may be limited.\nMotivated by recent success of deep neural networks (DNNs) that learn\nmulti-layer hierarchical representations, we propose a Stacked Kernel Network\n(SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves\nseveral layers of nonlinear transformations (from a linear space to a RKHS) and\nlinear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN\nis composed of multiple layers of hidden units, but each parameterized by a\nRKHS function rather than a finite-dimensional vector. We propose three ways to\nrepresent the RKHS functions in SKN: (1)nonparametric representation,\n(2)parametric representation and (3)random Fourier feature representation.\nFurthermore, we expand SKN into CNN architecture called Stacked Kernel\nConvolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear\nfeatures by convolutional operation with each filter also parameterized by a\nRKHS function rather than a finite-dimensional matrix in CNN, which is suitable\nfor image inputs. Experiments on various datasets demonstrate the effectiveness\nof SKN and SKCN, which outperform the competitive methods. \n\n"}
{"id": "1711.09482", "contents": "Title: An Introduction to Deep Visual Explanation Abstract: The practical impact of deep learning on complex supervised learning problems\nhas been significant, so much so that almost every Artificial Intelligence\nproblem, or at least a portion thereof, has been somehow recast as a deep\nlearning problem. The applications appeal is significant, but this appeal is\nincreasingly challenged by what some call the challenge of explainability, or\nmore generally the more traditional challenge of debuggability: if the outcomes\nof a deep learning process produce unexpected results (e.g., less than expected\nperformance of a classifier), then there is little available in the way of\ntheories or tools to help investigate the potential causes of such unexpected\nbehavior, especially when this behavior could impact people's lives. We\ndescribe a preliminary framework to help address this issue, which we call\n\"deep visual explanation\" (DVE). \"Deep,\" because it is the development and\nperformance of deep neural network models that we want to understand. \"Visual,\"\nbecause we believe that the most rapid insight into a complex multi-dimensional\nmodel is provided by appropriate visualization techniques, and \"Explanation,\"\nbecause in the spectrum from instrumentation by inserting print statements to\nthe abductive inference of explanatory hypotheses, we believe that the key to\nunderstanding deep learning relies on the identification and exposure of\nhypotheses about the performance behavior of a learned deep model. In the\nexposition of our preliminary framework, we use relatively straightforward\nimage classification examples and a variety of choices on initial configuration\nof a deep model building scenario. By careful but not complicated\ninstrumentation, we expose classification outcomes of deep models using\nvisualization, and also show initial results for one potential application of\ninterpretability. \n\n"}
{"id": "1711.10583", "contents": "Title: Classical and Quantum Super-integrability: From Lissajous Figures to\n  Exact Solvability Abstract: The first part of this paper explains what super-integrability is and how it\ndiffers in the classical and quantum cases. This is illustrated with an\nelementary example of the resonant harmonic oscillator.\n  For Hamiltonians in \"natural form\", the kinetic energy has geometric origins\nand, in the flat and constant curvature cases, the large isometry group plays a\nvital role. We explain how to use the corresponding first integrals to build\nseparable and super-integrable systems. We also show how to use the\nautomorphisms of the symmetry algebra to help build the Poisson relations of\nthe corresponding non-Abelian Poisson algebra.\n  Finally, we take both the classical and quantum Zernike system, recently\ndiscussed by Pogosyan, et al, and show how the algebraic structure of its\nsuper-integrability can be understood in this framework. \n\n"}
{"id": "1711.11317", "contents": "Title: Unsupervised Learning for Cell-level Visual Representation in\n  Histopathology Images with Generative Adversarial Networks Abstract: The visual attributes of cells, such as the nuclear morphology and chromatin\nopenness, are critical for histopathology image analysis. By learning\ncell-level visual representation, we can obtain a rich mix of features that are\nhighly reusable for various tasks, such as cell-level classification, nuclei\nsegmentation, and cell counting. In this paper, we propose a unified generative\nadversarial networks architecture with a new formulation of loss to perform\nrobust cell-level visual representation learning in an unsupervised setting.\nOur model is not only label-free and easily trained but also capable of\ncell-level unsupervised classification with interpretable visualization, which\nachieves promising results in the unsupervised classification of bone marrow\ncellular components. Based on the proposed cell-level visual representation\nlearning, we further develop a pipeline that exploits the varieties of cellular\nelements to perform histopathology image classification, the advantages of\nwhich are demonstrated on bone marrow datasets. \n\n"}
{"id": "1711.11326", "contents": "Title: High Dynamic Range Imaging Technology Abstract: In this lecture note, we describe high dynamic range (HDR) imaging systems;\nsuch systems are able to represent luminances of much larger brightness and,\ntypically, also a larger range of colors than conventional standard dynamic\nrange (SDR) imaging systems. The larger luminance range greatly improve the\noverall quality of visual content, making it appears much more realistic and\nappealing to observers. HDR is one of the key technologies of the future\nimaging pipeline, which will change the way the digital visual content is\nrepresented and manipulated today. \n\n"}
{"id": "1712.00436", "contents": "Title: Unsupervised Learning for Color Constancy Abstract: Most digital camera pipelines use color constancy methods to reduce the\ninfluence of illumination and camera sensor on the colors of scene objects. The\nhighest accuracy of color correction is obtained with learning-based color\nconstancy methods, but they require a significant amount of calibrated training\nimages with known ground-truth illumination. Such calibration is time\nconsuming, preferably done for each sensor individually, and therefore a major\nbottleneck in acquiring high color constancy accuracy. Statistics-based methods\ndo not require calibrated training images, but they are less accurate. In this\npaper an unsupervised learning-based method is proposed that learns its\nparameter values after approximating the unknown ground-truth illumination of\nthe training images, thus avoiding calibration. In terms of accuracy the\nproposed method outperforms all statistics-based and many learning-based\nmethods. An extension of the method is also proposed, which learns the needed\nparameters from non-calibrated images taken with one sensor and which can then\nbe successfully applied to images taken with another sensor. This effectively\nenables inter-camera unsupervised learning for color constancy. Additionally, a\nnew high quality color constancy benchmark dataset with 1707 calibrated images\nis created, used for testing, and made publicly available. The results are\npresented and discussed. The source code and the dataset are available at\nhttp://www.fer.unizg.hr/ipg/resources/color_constancy/. \n\n"}
{"id": "1712.00489", "contents": "Title: Visual Features for Context-Aware Speech Recognition Abstract: Automatic transcriptions of consumer-generated multi-media content such as\n\"Youtube\" videos still exhibit high word error rates. Such data typically\noccupies a very broad domain, has been recorded in challenging conditions, with\ncheap hardware and a focus on the visual modality, and may have been\npost-processed or edited. In this paper, we extend our earlier work on adapting\nthe acoustic model of a DNN-based speech recognition system to an RNN language\nmodel and show how both can be adapted to the objects and scenes that can be\nautomatically detected in the video. We are working on a corpus of \"how-to\"\nvideos from the web, and the idea is that an object that can be seen (\"car\"),\nor a scene that is being detected (\"kitchen\") can be used to condition both\nmodels on the \"context\" of the recording, thereby reducing perplexity and\nimproving transcription. We achieve good improvements in both cases and compare\nand analyze the respective reductions in word error rate. We expect that our\nresults can be used for any type of speech processing in which \"context\"\ninformation is available, for example in robotics, man-machine interaction, or\nwhen indexing large audio-visual archives, and should ultimately help to bring\ntogether the \"video-to-text\" and \"speech-to-text\" communities. \n\n"}
{"id": "1712.01238", "contents": "Title: Learning by Asking Questions Abstract: We introduce an interactive learning framework for the development and\ntesting of intelligent visual systems, called learning-by-asking (LBA). We\nexplore LBA in context of the Visual Question Answering (VQA) task. LBA differs\nfrom standard VQA training in that most questions are not observed during\ntraining time, and the learner must ask questions it wants answers to. Thus,\nLBA more closely mimics natural learning and has the potential to be more\ndata-efficient than the traditional VQA setting. We present a model that\nperforms LBA on the CLEVR dataset, and show that it automatically discovers an\neasy-to-hard curriculum when learning interactively from an oracle. Our LBA\ngenerated data consistently matches or outperforms the CLEVR train data and is\nmore sample efficient. We also show that our model asks questions that\ngeneralize to state-of-the-art VQA models and to novel test time distributions. \n\n"}
{"id": "1712.02170", "contents": "Title: Detecting Curve Text in the Wild: New Dataset and New Solution Abstract: Scene text detection has been made great progress in recent years. The\ndetection manners are evolving from axis-aligned rectangle to rotated rectangle\nand further to quadrangle. However, current datasets contain very little curve\ntext, which can be widely observed in scene images such as signboard, product\nname and so on. To raise the concerns of reading curve text in the wild, in\nthis paper, we construct a curve text dataset named CTW1500, which includes\nover 10k text annotations in 1,500 images (1000 for training and 500 for\ntesting). Based on this dataset, we pioneering propose a polygon based curve\ntext detector (CTD) which can directly detect curve text without empirical\ncombination. Moreover, by seamlessly integrating the recurrent transverse and\nlongitudinal offset connection (TLOC), the proposed method can be end-to-end\ntrainable to learn the inherent connection among the position offsets. This\nallows the CTD to explore context information instead of predicting points\nindependently, resulting in more smooth and accurate detection. We also propose\ntwo simple but effective post-processing methods named non-polygon suppress\n(NPS) and polygonal non-maximum suppression (PNMS) to further improve the\ndetection accuracy. Furthermore, the proposed approach in this paper is\ndesigned in an universal manner, which can also be trained with rectangular or\nquadrilateral bounding boxes without extra efforts. Experimental results on\nCTW-1500 demonstrate our method with only a light backbone can outperform\nstate-of-the-art methods with a large margin. By evaluating only in the curve\nor non-curve subset, the CTD + TLOC can still achieve the best results. Code is\navailable at https://github.com/Yuliang-Liu/Curve-Text-Detector. \n\n"}
{"id": "1712.02976", "contents": "Title: Defense against Adversarial Attacks Using High-Level Representation\n  Guided Denoiser Abstract: Neural networks are vulnerable to adversarial examples, which poses a threat\nto their application in security sensitive systems. We propose high-level\nrepresentation guided denoiser (HGD) as a defense for image classification.\nStandard denoiser suffers from the error amplification effect, in which small\nresidual adversarial noise is progressively amplified and leads to wrong\nclassifications. HGD overcomes this problem by using a loss function defined as\nthe difference between the target model's outputs activated by the clean image\nand denoised image. Compared with ensemble adversarial training which is the\nstate-of-the-art defending method on large images, HGD has three advantages.\nFirst, with HGD as a defense, the target model is more robust to either\nwhite-box or black-box adversarial attacks. Second, HGD can be trained on a\nsmall subset of the images and generalizes well to other images and unseen\nclasses. Third, HGD can be transferred to defend models other than the one\nguiding it. In NIPS competition on defense against adversarial attacks, our HGD\nsolution won the first place and outperformed other models by a large margin. \n\n"}
{"id": "1712.03812", "contents": "Title: Error Correction for Dense Semantic Image Labeling Abstract: Pixelwise semantic image labeling is an important, yet challenging, task with\nmany applications. Typical approaches to tackle this problem involve either the\ntraining of deep networks on vast amounts of images to directly infer the\nlabels or the use of probabilistic graphical models to jointly model the\ndependencies of the input (i.e. images) and output (i.e. labels). Yet, the\nformer approaches do not capture the structure of the output labels, which is\ncrucial for the performance of dense labeling, and the latter rely on carefully\nhand-designed priors that require costly parameter tuning via optimization\ntechniques, which in turn leads to long inference times. To alleviate these\nrestrictions, we explore how to arrive at dense semantic pixel labels given\nboth the input image and an initial estimate of the output labels. We propose a\nparallel architecture that: 1) exploits the context information through a\nLabelPropagation network to propagate correct labels from nearby pixels to\nimprove the object boundaries, 2) uses a LabelReplacement network to directly\nreplace possibly erroneous, initial labels with new ones, and 3) combines the\ndifferent intermediate results via a Fusion network to obtain the final\nper-pixel label. We experimentally validate our approach on two different\ndatasets for the semantic segmentation and face parsing tasks respectively,\nwhere we show improvements over the state-of-the-art. We also provide both a\nquantitative and qualitative analysis of the generated results. \n\n"}
{"id": "1712.04046", "contents": "Title: Character-Based Handwritten Text Transcription with Attention Networks Abstract: The paper approaches the task of handwritten text recognition (HTR) with\nattentional encoder-decoder networks trained on sequences of characters, rather\nthan words. We experiment on lines of text from popular handwriting datasets\nand compare different activation functions for the attention mechanism used for\naligning image pixels and target characters. We find that softmax attention\nfocuses heavily on individual characters, while sigmoid attention focuses on\nmultiple characters at each step of the decoding. When the sequence alignment\nis one-to-one, softmax attention is able to learn a more precise alignment at\neach step of the decoding, whereas the alignment generated by sigmoid attention\nis much less precise. When a linear function is used to obtain attention\nweights, the model predicts a character by looking at the entire sequence of\ncharacters and performs poorly because it lacks a precise alignment between the\nsource and target. Future research may explore HTR in natural scene images,\nsince the model is capable of transcribing handwritten text without the need\nfor producing segmentations or bounding boxes of text in images. \n\n"}
{"id": "1712.04119", "contents": "Title: 200x Low-dose PET Reconstruction using Deep Learning Abstract: Positron emission tomography (PET) is widely used in various clinical\napplications, including cancer diagnosis, heart disease and neuro disorders.\nThe use of radioactive tracer in PET imaging raises concerns due to the risk of\nradiation exposure. To minimize this potential risk in PET imaging, efforts\nhave been made to reduce the amount of radio-tracer usage. However, lowing dose\nresults in low Signal-to-Noise-Ratio (SNR) and loss of information, both of\nwhich will heavily affect clinical diagnosis. Besides, the ill-conditioning of\nlow-dose PET image reconstruction makes it a difficult problem for iterative\nreconstruction algorithms. Previous methods proposed are typically complicated\nand slow, yet still cannot yield satisfactory results at significantly low\ndose. Here, we propose a deep learning method to resolve this issue with an\nencoder-decoder residual deep network with concatenate skip connections.\nExperiments shows the proposed method can reconstruct low-dose PET image to a\nstandard-dose quality with only two-hundredth dose. Different cost functions\nfor training model are explored. Multi-slice input strategy is introduced to\nprovide the network with more structural information and make it more robust to\nnoise. Evaluation on ultra-low-dose clinical data shows that the proposed\nmethod can achieve better result than the state-of-the-art methods and\nreconstruct images with comparable quality using only 0.5% of the original\nregular dose. \n\n"}
{"id": "1712.04919", "contents": "Title: Multidimensional Data Tensor Sensing for RF Tomographic Imaging Abstract: Radio-frequency (RF) tomographic imaging is a promising technique for\ninferring multi-dimensional physical space by processing RF signals traversed\nacross a region of interest. However, conventional RF tomography schemes are\ngenerally based on vector compressed sensing, which ignores the geometric\nstructures of the target spaces and leads to low recovery precision. The\nrecently proposed transform-based tensor model is more appropriate for sensory\ndata processing, as it helps exploit the geometric structures of the\nthree-dimensional target and improve the recovery precision. In this paper, we\npropose a novel tensor sensing approach that achieves highly accurate\nestimation for real-world three-dimensional spaces. First, we use the\ntransform-based tensor model to formulate a tensor sensing problem, and propose\na fast alternating minimization algorithm called Alt-Min. Secondly, we drive an\nalgorithm which is optimized to reduce memory and computation requirements.\nFinally, we present evaluation of our Alt-Min approach using IKEA 3D data and\ndemonstrate significant improvement in recovery error and convergence speed\ncompared to prior tensor-based compressed sensing. \n\n"}
{"id": "1712.05839", "contents": "Title: Mapping the world population one building at a time Abstract: High resolution datasets of population density which accurately map\nsparsely-distributed human populations do not exist at a global scale.\nTypically, population data is obtained using censuses and statistical modeling.\nMore recently, methods using remotely-sensed data have emerged, capable of\neffectively identifying urbanized areas. Obtaining high accuracy in estimation\nof population distribution in rural areas remains a very challenging task due\nto the simultaneous requirements of sufficient sensitivity and resolution to\ndetect very sparse populations through remote sensing as well as reliable\nperformance at a global scale. Here, we present a computer vision method based\non machine learning to create population maps from satellite imagery at a\nglobal scale, with a spatial sensitivity corresponding to individual buildings\nand suitable for global deployment. By combining this settlement data with\ncensus data, we create population maps with ~30 meter resolution for 18\ncountries. We validate our method, and find that the building identification\nhas an average precision and recall of 0.95 and 0.91, respectively and that the\npopulation estimates have a standard error of a factor ~2 or less. Based on our\ndata, we analyze 29 percent of the world population, and show that 99 percent\nlives within 36 km of the nearest urban cluster. The resulting high-resolution\npopulation datasets have applications in infrastructure planning, vaccination\ncampaign planning, disaster response efforts and risk analysis such as high\naccuracy flood risk analysis. \n\n"}
{"id": "1712.06302", "contents": "Title: Visual Explanation by Interpretation: Improving Visual Feedback\n  Capabilities of Deep Neural Networks Abstract: Interpretation and explanation of deep models is critical towards wide\nadoption of systems that rely on them. In this paper, we propose a novel scheme\nfor both interpretation as well as explanation in which, given a pretrained\nmodel, we automatically identify internal features relevant for the set of\nclasses considered by the model, without relying on additional annotations. We\ninterpret the model through average visualizations of this reduced set of\nfeatures. Then, at test time, we explain the network prediction by accompanying\nthe predicted class label with supporting visualizations derived from the\nidentified features. In addition, we propose a method to address the artifacts\nintroduced by stridded operations in deconvNet-based visualizations. Moreover,\nwe introduce an8Flower, a dataset specifically designed for objective\nquantitative evaluation of methods for visual explanation.Experiments on the\nMNIST,ILSVRC12,Fashion144k and an8Flower datasets show that our method produces\ndetailed explanations with good coverage of relevant features of the classes of\ninterest \n\n"}
{"id": "1712.06742", "contents": "Title: PixelBNN: Augmenting the PixelCNN with batch normalization and the\n  presentation of a fast architecture for retinal vessel segmentation Abstract: Analysis of retinal fundus images is essential for eye-care physicians in the\ndiagnosis, care and treatment of patients. Accurate fundus and/or retinal\nvessel maps give rise to longitudinal studies able to utilize multimedia image\nregistration and disease/condition status measurements, as well as applications\nin surgery preparation and biometrics. The segmentation of retinal morphology\nhas numerous applications in assessing ophthalmologic and cardiovascular\ndisease pathologies. The early detection of many such conditions is often the\nmost effective method for reducing patient risk. Computer aided segmentation of\nthe vasculature has proven to be a challenge, mainly due to inconsistencies\nsuch as noise and variations in hue and brightness that can greatly reduce the\nquality of fundus images. This paper presents PixelBNN, a highly efficient deep\nmethod for automating the segmentation of fundus morphologies. The model was\ntrained, tested and cross tested on the DRIVE, STARE and CHASE\\_DB1 retinal\nvessel segmentation datasets. Performance was evaluated using G-mean, Mathews\nCorrelation Coefficient and F1-score. The network was 8.5 times faster than the\ncurrent state-of-the-art at test time and performed comparatively well,\nconsidering a 5 to 19 times reduction in information from resizing images\nduring preprocessing. \n\n"}
{"id": "1712.08062", "contents": "Title: Note on Attacking Object Detectors with Adversarial Stickers Abstract: Deep learning has proven to be a powerful tool for computer vision and has\nseen widespread adoption for numerous tasks. However, deep learning algorithms\nare known to be vulnerable to adversarial examples. These adversarial inputs\nare created such that, when provided to a deep learning algorithm, they are\nvery likely to be mislabeled. This can be problematic when deep learning is\nused to assist in safety critical decisions. Recent research has shown that\nclassifiers can be attacked by physical adversarial examples under various\nphysical conditions. Given the fact that state-of-the-art objection detection\nalgorithms are harder to be fooled by the same set of adversarial examples,\nhere we show that these detectors can also be attacked by physical adversarial\nexamples. In this note, we briefly show both static and dynamic test results.\nWe design an algorithm that produces physical adversarial inputs, which can\nfool the YOLO object detector and can also attack Faster-RCNN with relatively\nhigh success rate based on transferability. Furthermore, our algorithm can\ncompress the size of the adversarial inputs to stickers that, when attached to\nthe targeted object, result in the detector either mislabeling or not detecting\nthe object a high percentage of the time. This note provides a small set of\nresults. Our upcoming paper will contain a thorough evaluation on other object\ndetectors, and will present the algorithm. \n\n"}
{"id": "1712.08087", "contents": "Title: Learning Intelligent Dialogs for Bounding Box Annotation Abstract: We introduce Intelligent Annotation Dialogs for bounding box annotation. We\ntrain an agent to automatically choose a sequence of actions for a human\nannotator to produce a bounding box in a minimal amount of time. Specifically,\nwe consider two actions: box verification, where the annotator verifies a box\ngenerated by an object detector, and manual box drawing. We explore two kinds\nof agents, one based on predicting the probability that a box will be\npositively verified, and the other based on reinforcement learning. We\ndemonstrate that (1) our agents are able to learn efficient annotation\nstrategies in several scenarios, automatically adapting to the image\ndifficulty, the desired quality of the boxes, and the detector strength; (2) in\nall scenarios the resulting annotation dialogs speed up annotation compared to\nmanual box drawing alone and box verification alone, while also outperforming\nany fixed combination of verification and drawing in most scenarios; (3) in a\nrealistic scenario where the detector is iteratively re-trained, our agents\nevolve a series of strategies that reflect the shifting trade-off between\nverification and drawing as the detector grows stronger. \n\n"}
{"id": "1712.08364", "contents": "Title: Differential geometry and stochastic dynamics with deep learning\n  numerics Abstract: In this paper, we demonstrate how deterministic and stochastic dynamics on\nmanifolds, as well as differential geometric constructions can be implemented\nconcisely and efficiently using modern computational frameworks that mix\nsymbolic expressions with efficient numerical computations. In particular, we\nuse the symbolic expression and automatic differentiation features of the\npython library Theano, originally developed for high-performance computations\nin deep learning. We show how various aspects of differential geometry and Lie\ngroup theory, connections, metrics, curvature, left/right invariance, geodesics\nand parallel transport can be formulated with Theano using the automatic\ncomputation of derivatives of any order. We will also show how symbolic\nstochastic integrators and concepts from non-linear statistics can be\nformulated and optimized with only a few lines of code. We will then give\nexplicit examples on low-dimensional classical manifolds for visualization and\ndemonstrate how this approach allows both a concise implementation and\nefficient scaling to high dimensional problems. \n\n"}
{"id": "1712.08364", "contents": "Title: Differential geometry and stochastic dynamics with deep learning\n  numerics Abstract: In this paper, we demonstrate how deterministic and stochastic dynamics on\nmanifolds, as well as differential geometric constructions can be implemented\nconcisely and efficiently using modern computational frameworks that mix\nsymbolic expressions with efficient numerical computations. In particular, we\nuse the symbolic expression and automatic differentiation features of the\npython library Theano, originally developed for high-performance computations\nin deep learning. We show how various aspects of differential geometry and Lie\ngroup theory, connections, metrics, curvature, left/right invariance, geodesics\nand parallel transport can be formulated with Theano using the automatic\ncomputation of derivatives of any order. We will also show how symbolic\nstochastic integrators and concepts from non-linear statistics can be\nformulated and optimized with only a few lines of code. We will then give\nexplicit examples on low-dimensional classical manifolds for visualization and\ndemonstrate how this approach allows both a concise implementation and\nefficient scaling to high dimensional problems. \n\n"}
{"id": "1712.08521", "contents": "Title: An Incremental Self-Organizing Architecture for Sensorimotor Learning\n  and Prediction Abstract: During visuomotor tasks, robots must compensate for temporal delays inherent\nin their sensorimotor processing systems. Delay compensation becomes crucial in\na dynamic environment where the visual input is constantly changing, e.g.,\nduring the interacting with a human demonstrator. For this purpose, the robot\nmust be equipped with a prediction mechanism for using the acquired perceptual\nexperience to estimate possible future motor commands. In this paper, we\npresent a novel neural network architecture that learns prototypical visuomotor\nrepresentations and provides reliable predictions on the basis of the visual\ninput. These predictions are used to compensate for the delayed motor behavior\nin an online manner. We investigate the performance of our method with a set of\nexperiments comprising a humanoid robot that has to learn and generate visually\nperceived arm motion trajectories. We evaluate the accuracy in terms of mean\nprediction error and analyze the response of the network to novel movement\ndemonstrations. Additionally, we report experiments with incomplete data\nsequences, showing the robustness of the proposed architecture in the case of a\nnoisy and faulty visual sensor. \n\n"}
{"id": "1712.08718", "contents": "Title: Families of exact solutions of a new extended (2+1)-dimensional\n  Boussinesq equation Abstract: A new variant of the $(2+1)$-dimensional [$(2+1)d$] Boussinesq equation was\nrecently introduced by J. Y. Zhu, arxiv:1704.02779v2, 2017; see eq. (3). First,\nwe derive in this paper the one-soliton solutions of both bright and dark types\nfor the extended $(2+1)d$ Boussinesq equation by using the traveling wave\nmethod. Second, $N$-soliton, breather, and rational solutions are obtained by\nusing the Hirota bilinear method and the long wave limit. Nonsingular rational\nsolutions of two types were obtained analytically, namely: (i) rogue-wave\nsolutions having the form of W-shaped lines waves and (ii) lump-type solutions.\nTwo generic types of semi-rational solutions were also put forward. The\nobtained semi-rational solutions are as follows: (iii) a hybrid of a\nfirst-order lump and a bright one-soliton solution and (iv) a hybrid of a\nfirst-order lump and a first-order breather. \n\n"}
{"id": "1712.08969", "contents": "Title: Mean Field Residual Networks: On the Edge of Chaos Abstract: We study randomly initialized residual networks using mean field theory and\nthe theory of difference equations. Classical feedforward neural networks, such\nas those with tanh activations, exhibit exponential behavior on the average\nwhen propagating inputs forward or gradients backward. The exponential forward\ndynamics causes rapid collapsing of the input space geometry, while the\nexponential backward dynamics causes drastic vanishing or exploding gradients.\nWe show, in contrast, that by adding skip connections, the network will,\ndepending on the nonlinearity, adopt subexponential forward and backward\ndynamics, and in many cases in fact polynomial. The exponents of these\npolynomials are obtained through analytic methods and proved and verified\nempirically to be correct. In terms of the \"edge of chaos\" hypothesis, these\nsubexponential and polynomial laws allow residual networks to \"hover over the\nboundary between stability and chaos,\" thus preserving the geometry of the\ninput space and the gradient information flow. In our experiments, for each\nactivation function we study here, we initialize residual networks with\ndifferent hyperparameters and train them on MNIST. Remarkably, our\ninitialization time theory can accurately predict test time performance of\nthese networks, by tracking either the expected amount of gradient explosion or\nthe expected squared distance between the images of two input vectors.\nImportantly, we show, theoretically as well as empirically, that common\ninitializations such as the Xavier or the He schemes are not optimal for\nresidual networks, because the optimal initialization variances depend on the\ndepth. Finally, we have made mathematical contributions by deriving several new\nidentities for the kernels of powers of ReLU functions by relating them to the\nzeroth Bessel function of the second kind. \n\n"}
{"id": "1712.09382", "contents": "Title: Audio to Body Dynamics Abstract: We present a method that gets as input an audio of violin or piano playing,\nand outputs a video of skeleton predictions which are further used to animate\nan avatar. The key idea is to create an animation of an avatar that moves their\nhands similarly to how a pianist or violinist would do, just from audio. Aiming\nfor a fully detailed correct arms and fingers motion is a goal, however, it's\nnot clear if body movement can be predicted from music at all. In this paper,\nwe present the first result that shows that natural body dynamics can be\npredicted at all. We built an LSTM network that is trained on violin and piano\nrecital videos uploaded to the Internet. The predicted points are applied onto\na rigged avatar to create the animation. \n\n"}
{"id": "1801.01967", "contents": "Title: Visual Text Correction Abstract: Videos, images, and sentences are mediums that can express the same\nsemantics. One can imagine a picture by reading a sentence or can describe a\nscene with some words. However, even small changes in a sentence can cause a\nsignificant semantic inconsistency with the corresponding video/image. For\nexample, by changing the verb of a sentence, the meaning may drastically\nchange. There have been many efforts to encode a video/sentence and decode it\nas a sentence/video. In this research, we study a new scenario in which both\nthe sentence and the video are given, but the sentence is inaccurate. A\nsemantic inconsistency between the sentence and the video or between the words\nof a sentence can result in an inaccurate description. This paper introduces a\nnew problem, called Visual Text Correction (VTC), i.e., finding and replacing\nan inaccurate word in the textual description of a video. We propose a deep\nnetwork that can simultaneously detect an inaccuracy in a sentence, and fix it\nby replacing the inaccurate word(s). Our method leverages the semantic\ninterdependence of videos and words, as well as the short-term and long-term\nrelations of the words in a sentence. In our formulation, part of a visual\nfeature vector for every single word is dynamically selected through a gating\nprocess. Furthermore, to train and evaluate our model, we propose an approach\nto automatically construct a large dataset for VTC problem. Our experiments and\nperformance analysis demonstrates that the proposed method provides very good\nresults and also highlights the general challenges in solving the VTC problem.\nTo the best of our knowledge, this work is the first of its kind for the Visual\nText Correction task. \n\n"}
{"id": "1801.02612", "contents": "Title: Spatially Transformed Adversarial Examples Abstract: Recent studies show that widely used deep neural networks (DNNs) are\nvulnerable to carefully crafted adversarial examples. Many advanced algorithms\nhave been proposed to generate adversarial examples by leveraging the\n$\\mathcal{L}_p$ distance for penalizing perturbations. Researchers have\nexplored different defense methods to defend against such adversarial attacks.\nWhile the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual\nquality remains an active research area, in this paper we will instead focus on\na different type of perturbation, namely spatial transformation, as opposed to\nmanipulating the pixel values directly as in prior works. Perturbations\ngenerated through spatial transformation could result in large $\\mathcal{L}_p$\ndistance measures, but our extensive experiments show that such spatially\ntransformed adversarial examples are perceptually realistic and more difficult\nto defend against with existing defense systems. This potentially provides a\nnew direction in adversarial example generation and the design of corresponding\ndefenses. We visualize the spatial transformation based perturbation for\ndifferent examples and show that our technique can produce realistic\nadversarial examples with smooth image deformation. Finally, we visualize the\nattention of deep networks with different types of adversarial examples to\nbetter understand how these examples are interpreted. \n\n"}
{"id": "1801.02642", "contents": "Title: Boundary Optimizing Network (BON) Abstract: Despite all the success that deep neural networks have seen in classifying\ncertain datasets, the challenge of finding optimal solutions that generalize\nstill remains. In this paper, we propose the Boundary Optimizing Network (BON),\na new approach to generalization for deep neural networks when used for\nsupervised learning. Given a classification network, we propose to use a\ncollaborative generative network that produces new synthetic data points in the\nform of perturbations of original data points. In this way, we create a data\nsupport around each original data point which prevents decision boundaries from\npassing too close to the original data points, i.e. prevents overfitting. We\nshow that BON improves convergence on CIFAR-10 using the state-of-the-art\nDensenet. We do however observe that the generative network suffers from\ncatastrophic forgetting during training, and we therefore propose to use a\nvariation of Memory Aware Synapses to optimize the generative network (called\nBON++). On the Iris dataset, we visualize the effect of BON++ when the\ngenerator does not suffer from catastrophic forgetting and conclude that the\napproach has the potential to create better boundaries in a higher dimensional\nspace. \n\n"}
{"id": "1801.02730", "contents": "Title: Data Augmentation for Brain-Computer Interfaces: Analysis on\n  Event-Related Potentials Data Abstract: On image data, data augmentation is becoming less relevant due to the large\namount of available training data and regularization techniques. Common\napproaches are moving windows (cropping), scaling, affine distortions, random\nnoise, and elastic deformations. For electroencephalographic data, the lack of\nsufficient training data is still a major issue. We suggest and evaluate\ndifferent approaches to generate augmented data using temporal and\nspatial/rotational distortions. Our results on the perception of rare stimuli\n(P300 data) and movement prediction (MRCP data) show that these approaches are\nfeasible and can significantly increase the performance of signal processing\nchains for brain-computer interfaces by 1% to 6%. \n\n"}
{"id": "1801.03399", "contents": "Title: Deep Supervision with Intermediate Concepts Abstract: Recent data-driven approaches to scene interpretation predominantly pose\ninference as an end-to-end black-box mapping, commonly performed by a\nConvolutional Neural Network (CNN). However, decades of work on perceptual\norganization in both human and machine vision suggests that there are often\nintermediate representations that are intrinsic to an inference task, and which\nprovide essential structure to improve generalization. In this work, we explore\nan approach for injecting prior domain structure into neural network training\nby supervising hidden layers of a CNN with intermediate concepts that normally\nare not observed in practice. We formulate a probabilistic framework which\nformalizes these notions and predicts improved generalization via this deep\nsupervision method. One advantage of this approach is that we are able to train\nonly from synthetic CAD renderings of cluttered scenes, where concept values\ncan be extracted, but apply the results to real images. Our implementation\nachieves the state-of-the-art performance of 2D/3D keypoint localization and\nimage classification on real image benchmarks, including KITTI, PASCAL VOC,\nPASCAL3D+, IKEA, and CIFAR100. We provide additional evidence that our approach\noutperforms alternative forms of supervision, such as multi-task networks. \n\n"}
{"id": "1801.03431", "contents": "Title: Inferring a Third Spatial Dimension from 2D Histological Images Abstract: Histological images are obtained by transmitting light through a tissue\nspecimen that has been stained in order to produce contrast. This process\nresults in 2D images of the specimen that has a three-dimensional structure. In\nthis paper, we propose a method to infer how the stains are distributed in the\ndirection perpendicular to the surface of the slide for a given 2D image in\norder to obtain a 3D representation of the tissue. This inference is achieved\nby decomposition of the staining concentration maps under constraints that\nensure realistic decomposition and reconstruction of the original 2D images.\nOur study shows that it is possible to generate realistic 3D images making this\nmethod a potential tool for data augmentation when training deep learning\nmodels. \n\n"}
{"id": "1801.05574", "contents": "Title: Brenier approach for optimal transportation between a quasi-discrete\n  measure and a discrete measure Abstract: Correctly estimating the discrepancy between two data distributions has\nalways been an important task in Machine Learning. Recently, Cuturi proposed\nthe Sinkhorn distance which makes use of an approximate Optimal Transport cost\nbetween two distributions as a distance to describe distribution discrepancy.\nAlthough it has been successfully adopted in various machine learning\napplications (e.g. in Natural Language Processing and Computer Vision) since\nthen, the Sinkhorn distance also suffers from two unnegligible limitations. The\nfirst one is that the Sinkhorn distance only gives an approximation of the real\nWasserstein distance, the second one is the `divide by zero' problem which\noften occurs during matrix scaling when setting the entropy regularization\ncoefficient to a small value. In this paper, we introduce a new Brenier\napproach for calculating a more accurate Wasserstein distance between two\ndiscrete distributions, this approach successfully avoids the two limitations\nshown above for Sinkhorn distance and gives an alternative way for estimating\ndistribution discrepancy. \n\n"}
{"id": "1801.05746", "contents": "Title: TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image\n  Segmentation Abstract: Pixel-wise image segmentation is demanding task in computer vision. Classical\nU-Net architectures composed of encoders and decoders are very popular for\nsegmentation of medical images, satellite images etc. Typically, neural network\ninitialized with weights from a network pre-trained on a large data set like\nImageNet shows better performance than those trained from scratch on a small\ndataset. In some practical applications, particularly in medicine and traffic\nsafety, the accuracy of the models is of utmost importance. In this paper, we\ndemonstrate how the U-Net type architecture can be improved by the use of the\npre-trained encoder. Our code and corresponding pre-trained weights are\npublicly available at https://github.com/ternaus/TernausNet. We compare three\nweight initialization schemes: LeCun uniform, the encoder with weights from\nVGG11 and full network trained on the Carvana dataset. This network\narchitecture was a part of the winning solution (1st out of 735) in the Kaggle:\nCarvana Image Masking Challenge. \n\n"}
{"id": "1801.06434", "contents": "Title: EffNet: An Efficient Structure for Convolutional Neural Networks Abstract: With the ever increasing application of Convolutional Neural Networks to\ncustomer products the need emerges for models to efficiently run on embedded,\nmobile hardware. Slimmer models have therefore become a hot research topic with\nvarious approaches which vary from binary networks to revised convolution\nlayers. We offer our contribution to the latter and propose a novel convolution\nblock which significantly reduces the computational burden while surpassing the\ncurrent state-of-the-art. Our model, dubbed EffNet, is optimised for models\nwhich are slim to begin with and is created to tackle issues in existing models\nsuch as MobileNet and ShuffleNet. \n\n"}
{"id": "1801.06593", "contents": "Title: A Foreground Inference Network for Video Surveillance Using Multi-View\n  Receptive Field Abstract: Foreground (FG) pixel labelling plays a vital role in video surveillance.\nRecent engineering solutions have attempted to exploit the efficacy of deep\nlearning (DL) models initially targeted for image classification to deal with\nFG pixel labelling. One major drawback of such strategy is the lacking\ndelineation of visual objects when training samples are limited. To grapple\nwith this issue, we introduce a multi-view receptive field fully convolutional\nneural network (MV-FCN) that harness recent seminal ideas, such as, fully\nconvolutional structure, inception modules, and residual networking. Therefrom,\nwe implement a system in an encoder-decoder fashion that subsumes a core and\ntwo complementary feature flow paths. The model exploits inception modules at\nearly and late stages with three different sizes of receptive fields to capture\ninvariance at various scales. The features learned in the encoding phase are\nfused with appropriate feature maps in the decoding phase through residual\nconnections for achieving enhanced spatial representation. These multi-view\nreceptive fields and residual feature connections are expected to yield highly\ngeneralized features for an accurate pixel-wise FG region identification. It\nis, then, trained with database specific exemplary segmentations to predict\ndesired FG objects.\n  The comparative experimental results on eleven benchmark datasets validate\nthat the proposed model achieves very competitive performance with the prior-\nand state-of-the-art algorithms. We also report that how well a transfer\nlearning approach can be useful to enhance the performance of our proposed\nMV-FCN. \n\n"}
{"id": "1801.06620", "contents": "Title: A high-performance analog Max-SAT solver and its application to Ramsey\n  numbers Abstract: We introduce a continuous-time analog solver for MaxSAT, a quintessential\nclass of NP-hard discrete optimization problems, where the task is to find a\ntruth assignment for a set of Boolean variables satisfying the maximum number\nof given logical constraints. We show that the scaling of an invariant of the\nsolver's dynamics, the escape rate, as function of the number of unsatisfied\nclauses can predict the global optimum value, often well before reaching the\ncorresponding state. We demonstrate the performance of the solver on hard\nMaxSAT competition problems. We then consider the two-color Ramsey number\n$R(m,m)$ problem, translate it to SAT, and apply our algorithm to the still\nunknown $R(5,5)$. We find edge colorings without monochromatic 5-cliques for\ncomplete graphs up to 42 vertices, while on 43 vertices we find colorings with\nonly two monochromatic 5-cliques, the best coloring found so far, supporting\nthe conjecture that $R(5,5) = 43$. \n\n"}
{"id": "1801.07198", "contents": "Title: Three Dimensional Fluorescence Microscopy Image Synthesis and\n  Segmentation Abstract: Advances in fluorescence microscopy enable acquisition of 3D image volumes\nwith better image quality and deeper penetration into tissue. Segmentation is a\nrequired step to characterize and analyze biological structures in the images\nand recent 3D segmentation using deep learning has achieved promising results.\nOne issue is that deep learning techniques require a large set of groundtruth\ndata which is impractical to annotate manually for large 3D microscopy volumes.\nThis paper describes a 3D deep learning nuclei segmentation method using\nsynthetic 3D volumes for training. A set of synthetic volumes and the\ncorresponding groundtruth are generated using spatially constrained\ncycle-consistent adversarial networks. Segmentation results demonstrate that\nour proposed method is capable of segmenting nuclei successfully for various\ndata sets. \n\n"}
{"id": "1801.08577", "contents": "Title: Effective Building Block Design for Deep Convolutional Neural Networks\n  using Search Abstract: Deep learning has shown promising results on many machine learning tasks but\nDL models are often complex networks with large number of neurons and layers,\nand recently, complex layer structures known as building blocks. Finding the\nbest deep model requires a combination of finding both the right architecture\nand the correct set of parameters appropriate for that architecture. In\naddition, this complexity (in terms of layer types, number of neurons, and\nnumber of layers) also present problems with generalization since larger\nnetworks are easier to overfit to the data. In this paper, we propose a search\nframework for finding effective architectural building blocks for convolutional\nneural networks (CNN). Our approach is much faster at finding models that are\nclose to state-of-the-art in performance. In addition, the models discovered by\nour approach are also smaller than models discovered by similar techniques. We\nachieve these twin advantages by designing our search space in such a way that\nit searches over a reduced set of state-of-the-art building blocks for CNNs\nincluding residual block, inception block, inception-residual block, ResNeXt\nblock and many others. We apply this technique to generate models for multiple\nimage datasets and show that these models achieve performance comparable to\nstate-of-the-art (and even surpassing the state-of-the-art in one case). We\nalso show that learned models are transferable between datasets. \n\n"}
{"id": "1801.09097", "contents": "Title: Towards an Understanding of Neural Networks in Natural-Image Spaces Abstract: Two major uncertainties, dataset bias and adversarial examples, prevail in\nstate-of-the-art AI algorithms with deep neural networks. In this paper, we\npresent an intuitive explanation for these issues as well as an interpretation\nof the performance of deep networks in a natural-image space. The explanation\nconsists of two parts: the philosophy of neural networks and a hypothetical\nmodel of natural-image spaces. Following the explanation, we 1) demonstrate\nthat the values of training samples differ, 2) provide incremental boost to the\naccuracy of a CIFAR-10 classifier by introducing an additional \"random-noise\"\ncategory during training, 3) alleviate over-fitting thereby enhancing the\nrobustness against adversarial examples by detecting and excluding illusive\ntraining samples that are consistently misclassified. Our overall contribution\nis therefore twofold. First, while most existing algorithms treat data equally\nand have a strong appetite for more data, we demonstrate in contrast that an\nindividual datum can sometimes have disproportionate and counterproductive\ninfluence and that it is not always better to train neural networks with more\ndata. Next, we consider more thoughtful strategies by taking into account the\ngeometric and topological properties of natural-image spaces to which deep\nnetworks are applied. \n\n"}
{"id": "1801.09335", "contents": "Title: Stochastic Downsampling for Cost-Adjustable Inference and Improved\n  Regularization in Convolutional Networks Abstract: It is desirable to train convolutional networks (CNNs) to run more\nefficiently during inference. In many cases however, the computational budget\nthat the system has for inference cannot be known beforehand during training,\nor the inference budget is dependent on the changing real-time resource\navailability. Thus, it is inadequate to train just inference-efficient CNNs,\nwhose inference costs are not adjustable and cannot adapt to varied inference\nbudgets. We propose a novel approach for cost-adjustable inference in CNNs -\nStochastic Downsampling Point (SDPoint). During training, SDPoint applies\nfeature map downsampling to a random point in the layer hierarchy, with a\nrandom downsampling ratio. The different stochastic downsampling configurations\nknown as SDPoint instances (of the same model) have computational costs\ndifferent from each other, while being trained to minimize the same prediction\nloss. Sharing network parameters across different instances provides\nsignificant regularization boost. During inference, one may handpick a SDPoint\ninstance that best fits the inference budget. The effectiveness of SDPoint, as\nboth a cost-adjustable inference approach and a regularizer, is validated\nthrough extensive experiments on image classification. \n\n"}
{"id": "1801.09377", "contents": "Title: On the validity of linear response theory in high-dimensional\n  deterministic dynamical systems Abstract: This theoretical work considers the following conundrum: linear response\ntheory is successfully used by scientists in numerous fields, but\nmathematicians have shown that typical low-dimensional dynamical systems\nviolate the theory's assumptions. Here we provide a proof of concept for the\nvalidity of linear response theory in high-dimensional deterministic systems\nfor large-scale observables. We introduce an exemplary model in which\nobservables of resolved degrees of freedom are weakly coupled to a large,\ninhomogeneous collection of unresolved chaotic degrees of freedom. By employing\nstatistical limit laws we give conditions under which such systems obey linear\nresponse theory even if all the degrees of freedom individually violate linear\nresponse. We corroborate our result with numerical simulations. \n\n"}
{"id": "1801.09679", "contents": "Title: The Lyapunov dimension, convergency and entropy for a dynamical model of\n  Chua memristor circuit Abstract: For the study of chaotic dynamics and dimension of attractors the concepts of\nthe Lyapunov exponents was found useful and became widely spread. Such\ncharacteristics of chaotic behavior, as the Lyapunov dimension and the entropy\nrate, can be estimated via the Lyapunov exponents. In this work an analytical\napproach to the study of the Lyapunov dimension, convergency and entropy for a\ndynamical model of Chua memristor circuit is demonstrated. \n\n"}
{"id": "1802.00397", "contents": "Title: Global-local mixing for the Boole map Abstract: In the context of 'infinite-volume mixing' we prove global-local mixing for\nthe Boole map, a.k.a. Boole transformation, which is the prototype of a\nnon-uniformly expanding map with two neutral fixed points. Global-local mixing\namounts to the decorrelation of all pairs of global and local observables. In\nterms of the equilibrium properties of the system it means that the evolution\nof every absolutely continuous probability measure converges, in a certain\nprecise sense, to an averaging functional over the entire space. \n\n"}
{"id": "1802.01268", "contents": "Title: ASMCNN: An Efficient Brain Extraction Using Active Shape Model and\n  Convolutional Neural Networks Abstract: Brain extraction (skull stripping) is a challenging problem in neuroimaging.\nIt is due to the variability in conditions from data acquisition or\nabnormalities in images, making brain morphology and intensity characteristics\nchangeable and complicated. In this paper, we propose an algorithm for skull\nstripping in Magnetic Resonance Imaging (MRI) scans, namely ASMCNN, by\ncombining the Active Shape Model (ASM) and Convolutional Neural Network (CNN)\nfor taking full of their advantages to achieve remarkable results. Instead of\nworking with 3D structures, we process 2D image sequences in the sagittal\nplane. First, we divide images into different groups such that, in each group,\nshapes and structures of brain boundaries have similar appearances. Second, a\nmodified version of ASM is used to detect brain boundaries by utilizing prior\nknowledge of each group. Finally, CNN and post-processing methods, including\nConditional Random Field (CRF), Gaussian processes, and several special rules\nare applied to refine the segmentation contours. Experimental results show that\nour proposed method outperforms current state-of-the-art algorithms by a\nsignificant margin in all experiments. \n\n"}
{"id": "1802.01421", "contents": "Title: First-order Adversarial Vulnerability of Neural Networks and Input\n  Dimension Abstract: Over the past few years, neural networks were proven vulnerable to\nadversarial images: targeted but imperceptible image perturbations lead to\ndrastically different predictions. We show that adversarial vulnerability\nincreases with the gradients of the training objective when viewed as a\nfunction of the inputs. Surprisingly, vulnerability does not depend on network\ntopology: for many standard network architectures, we prove that at\ninitialization, the $\\ell_1$-norm of these gradients grows as the square root\nof the input dimension, leaving the networks increasingly vulnerable with\ngrowing image size. We empirically show that this dimension dependence persists\nafter either usual or robust training, but gets attenuated with higher\nregularization. \n\n"}
{"id": "1802.01722", "contents": "Title: Compressive Light Field Reconstructions using Deep Learning Abstract: Light field imaging is limited in its computational processing demands of\nhigh sampling for both spatial and angular dimensions. Single-shot light field\ncameras sacrifice spatial resolution to sample angular viewpoints, typically by\nmultiplexing incoming rays onto a 2D sensor array. While this resolution can be\nrecovered using compressive sensing, these iterative solutions are slow in\nprocessing a light field. We present a deep learning approach using a new, two\nbranch network architecture, consisting jointly of an autoencoder and a 4D CNN,\nto recover a high resolution 4D light field from a single coded 2D image. This\nnetwork decreases reconstruction time significantly while achieving average\nPSNR values of 26-32 dB on a variety of light fields. In particular,\nreconstruction time is decreased from 35 minutes to 6.7 minutes as compared to\nthe dictionary method for equivalent visual quality. These reconstructions are\nperformed at small sampling/compression ratios as low as 8%, allowing for\ncheaper coded light field cameras. We test our network reconstructions on\nsynthetic light fields, simulated coded measurements of real light fields\ncaptured from a Lytro Illum camera, and real coded images from a custom CMOS\ndiffractive light field camera. The combination of compressive light field\ncapture with deep learning allows the potential for real-time light field video\nacquisition systems in the future. \n\n"}
{"id": "1802.01894", "contents": "Title: The steerable graph Laplacian and its application to filtering image\n  data-sets Abstract: In recent years, improvements in various image acquisition techniques gave\nrise to the need for adaptive processing methods, aimed particularly for large\ndatasets corrupted by noise and deformations. In this work, we consider\ndatasets of images sampled from a low-dimensional manifold (i.e. an\nimage-valued manifold), where the images can assume arbitrary planar rotations.\nTo derive an adaptive and rotation-invariant framework for processing such\ndatasets, we introduce a graph Laplacian (GL)-like operator over the dataset,\ntermed ${\\textit{steerable graph Laplacian}}$. Essentially, the steerable GL\nextends the standard GL by accounting for all (infinitely-many) planar\nrotations of all images. As it turns out, similarly to the standard GL, a\nproperly normalized steerable GL converges to the Laplace-Beltrami operator on\nthe low-dimensional manifold. However, the steerable GL admits an improved\nconvergence rate compared to the GL, where the improved convergence behaves as\nif the intrinsic dimension of the underlying manifold is lower by one.\nMoreover, it is shown that the steerable GL admits eigenfunctions of the form\nof Fourier modes (along the orbits of the images' rotations) multiplied by\neigenvectors of certain matrices, which can be computed efficiently by the FFT.\nFor image datasets corrupted by noise, we employ a subset of these\neigenfunctions to \"filter\" the dataset via a Fourier-like filtering scheme,\nessentially using all images and their rotations simultaneously. We demonstrate\nour filtering framework by de-noising simulated single-particle cryo-EM image\ndatasets. \n\n"}
{"id": "1802.02147", "contents": "Title: DeepTravel: a Neural Network Based Travel Time Estimation Model with\n  Auxiliary Supervision Abstract: Estimating the travel time of a path is of great importance to smart urban\nmobility. Existing approaches are either based on estimating the time cost of\neach road segment which are not able to capture many cross-segment complex\nfactors, or designed heuristically in a non-learning-based way which fail to\nutilize the existing abundant temporal labels of the data, i.e., the time stamp\nof each trajectory point. In this paper, we leverage on new development of deep\nneural networks and propose a novel auxiliary supervision model, namely\nDeepTravel, that can automatically and effectively extract different features,\nas well as make full use of the temporal labels of the trajectory data. We have\nconducted comprehensive experiments on real datasets to demonstrate the\nout-performance of DeepTravel over existing approaches. \n\n"}
{"id": "1802.02181", "contents": "Title: Applications of a Graph Theoretic Based Clustering Framework in Computer\n  Vision and Pattern Recognition Abstract: Recently, several clustering algorithms have been used to solve variety of\nproblems from different discipline. This dissertation aims to address different\nchallenging tasks in computer vision and pattern recognition by casting the\nproblems as a clustering problem. We proposed novel approaches to solve\nmulti-target tracking, visual geo-localization and outlier detection problems\nusing a unified underlining clustering framework, i.e., dominant set clustering\nand its extensions, and presented a superior result over several\nstate-of-the-art approaches. \n\n"}
{"id": "1802.02290", "contents": "Title: Spectral Image Visualization Using Generative Adversarial Networks Abstract: Spectral images captured by satellites and radio-telescopes are analyzed to\nobtain information about geological compositions distributions, distant asters\nas well as undersea terrain. Spectral images usually contain tens to hundreds\nof continuous narrow spectral bands and are widely used in various fields. But\nthe vast majority of those image signals are beyond the visible range, which\ncalls for special visualization technique. The visualizations of spectral\nimages shall convey as much information as possible from the original signal\nand facilitate image interpretation. However, most of the existing visualizatio\nmethods display spectral images in false colors, which contradict with human's\nexperience and expectation. In this paper, we present a novel visualization\ngenerative adversarial network (GAN) to display spectral images in natural\ncolors. To achieve our goal, we propose a loss function which consists of an\nadversarial loss and a structure loss. The adversarial loss pushes our solution\nto the natural image distribution using a discriminator network that is trained\nto differentiate between false-color images and natural-color images. We also\nuse a cycle loss as the structure constraint to guarantee structure\nconsistency. Experimental results show that our method is able to generate\nstructure-preserved and natural-looking visualizations. \n\n"}
{"id": "1802.02899", "contents": "Title: From Selective Deep Convolutional Features to Compact Binary\n  Representations for Image Retrieval Abstract: In the large-scale image retrieval task, the two most important requirements\nare the discriminability of image representations and the efficiency in\ncomputation and storage of representations. Regarding the former requirement,\nConvolutional Neural Network (CNN) is proven to be a very powerful tool to\nextract highly discriminative local descriptors for effective image search.\nAdditionally, in order to further improve the discriminative power of the\ndescriptors, recent works adopt fine-tuned strategies. In this paper, taking a\ndifferent approach, we propose a novel, computationally efficient, and\ncompetitive framework. Specifically, we firstly propose various strategies to\ncompute masks, namely SIFT-mask, SUM-mask, and MAX-mask, to select a\nrepresentative subset of local convolutional features and eliminate redundant\nfeatures. Our in-depth analyses demonstrate that proposed masking schemes are\neffective to address the burstiness drawback and improve retrieval accuracy.\nSecondly, we propose to employ recent embedding and aggregating methods which\ncan significantly boost the feature discriminability. Regarding the computation\nand storage efficiency, we include a hashing module to produce very compact\nbinary image representations. Extensive experiments on six image retrieval\nbenchmarks demonstrate that our proposed framework achieves the\nstate-of-the-art retrieval performances. \n\n"}
{"id": "1802.04977", "contents": "Title: Paraphrasing Complex Network: Network Compression via Factor Transfer Abstract: Many researchers have sought ways of model compression to reduce the size of\na deep neural network (DNN) with minimal performance degradation in order to\nuse DNNs in embedded systems. Among the model compression methods, a method\ncalled knowledge transfer is to train a student network with a stronger teacher\nnetwork. In this paper, we propose a novel knowledge transfer method which uses\nconvolutional operations to paraphrase teacher's knowledge and to translate it\nfor the student. This is done by two convolutional modules, which are called a\nparaphraser and a translator. The paraphraser is trained in an unsupervised\nmanner to extract the teacher factors which are defined as paraphrased\ninformation of the teacher network. The translator located at the student\nnetwork extracts the student factors and helps to translate the teacher factors\nby mimicking them. We observed that our student network trained with the\nproposed factor transfer method outperforms the ones trained with conventional\nknowledge transfer methods. \n\n"}
{"id": "1802.05232", "contents": "Title: Simple heteroclinic networks in ${\\mathbb R}^4$ Abstract: We classify simple heteroclinic networks for a $\\Gamma$-equivariant system in\n${\\mathbb R}^4$ with finite $\\Gamma \\subset {\\rm O}(4)$, proceeding as follows:\nwe define a graph associated with a given $\\Gamma \\subset {\\rm O}(n)$ and\nidentify all so-called simple graphs associated with subgroups of ${\\rm O}(4)$.\nThen, knowing the graph associated with a given $\\Gamma$, we determine the\ntypes of heteroclinic networks that the group admits. Our study is restricted\nto networks that are maximal in the sense that they have the highest possible\nnumber of connections -- any non-maximal network can then be derived by\ndeleting one or more connections. Finally, for networks of type A, i.e.,\nadmitted by $\\Gamma \\subset {\\rm SO}(4)$, we give necessary and sufficient\nconditions for fragmentary and essential asymptotic stability. (For other\nsimple heteroclinic networks the conditions for stability are known.) The\nresults are illustrated by a numerical example of a simple heteroclinic network\nthat involves two subcycles that can be essentially asymptotically stable\nsimultaneously. \n\n"}
{"id": "1802.05385", "contents": "Title: Fooling OCR Systems with Adversarial Text Images Abstract: We demonstrate that state-of-the-art optical character recognition (OCR)\nbased on deep learning is vulnerable to adversarial images. Minor modifications\nto images of printed text, which do not change the meaning of the text to a\nhuman reader, cause the OCR system to \"recognize\" a different text where\ncertain words chosen by the adversary are replaced by their semantic opposites.\nThis completely changes the meaning of the output produced by the OCR system\nand by the NLP applications that use OCR for preprocessing their inputs. \n\n"}
{"id": "1802.05751", "contents": "Title: Image Transformer Abstract: Image generation has been successfully cast as an autoregressive sequence\ngeneration or transformation problem. Recent work has shown that self-attention\nis an effective way of modeling textual sequences. In this work, we generalize\na recently proposed model architecture based on self-attention, the\nTransformer, to a sequence modeling formulation of image generation with a\ntractable likelihood. By restricting the self-attention mechanism to attend to\nlocal neighborhoods we significantly increase the size of images the model can\nprocess in practice, despite maintaining significantly larger receptive fields\nper layer than typical convolutional neural networks. While conceptually\nsimple, our generative models significantly outperform the current state of the\nart in image generation on ImageNet, improving the best published negative\nlog-likelihood on ImageNet from 3.83 to 3.77. We also present results on image\nsuper-resolution with a large magnification ratio, applying an encoder-decoder\nconfiguration of our architecture. In a human evaluation study, we find that\nimages generated by our super-resolution model fool human observers three times\nmore often than the previous state of the art. \n\n"}
{"id": "1802.06897", "contents": "Title: Machine Learning Methods for Data Association in Multi-Object Tracking Abstract: Data association is a key step within the multi-object tracking pipeline that\nis notoriously challenging due to its combinatorial nature. A popular and\ngeneral way to formulate data association is as the NP-hard multidimensional\nassignment problem (MDAP). Over the last few years, data-driven approaches to\nassignment have become increasingly prevalent as these techniques have started\nto mature. We focus this survey solely on learning algorithms for the\nassignment step of multi-object tracking, and we attempt to unify various\nmethods by highlighting their connections to linear assignment as well as to\nthe MDAP. First, we review probabilistic and end-to-end optimization approaches\nto data association, followed by methods that learn association affinities from\ndata. We then compare the performance of the methods presented in this survey,\nand conclude by discussing future research directions. \n\n"}
{"id": "1802.07072", "contents": "Title: Composite Optimization by Nonconvex Majorization-Minimization Abstract: The minimization of a nonconvex composite function can model a variety of\nimaging tasks. A popular class of algorithms for solving such problems are\nmajorization-minimization techniques which iteratively approximate the\ncomposite nonconvex function by a majorizing function that is easy to minimize.\nMost techniques, e.g. gradient descent, utilize convex majorizers in order to\nguarantee that the majorizer is easy to minimize. In our work we consider a\nnatural class of nonconvex majorizers for these functions, and show that these\nmajorizers are still sufficient for a globally convergent optimization scheme.\nNumerical results illustrate that by applying this scheme, one can often obtain\nsuperior local optima compared to previous majorization-minimization methods,\nwhen the nonconvex majorizers are solved to global optimality. Finally, we\nillustrate the behavior of our algorithm for depth super-resolution from raw\ntime-of-flight data. \n\n"}
{"id": "1802.07584", "contents": "Title: DeepASL: Enabling Ubiquitous and Non-Intrusive Word and Sentence-Level\n  Sign Language Translation Abstract: There is an undeniable communication barrier between deaf people and people\nwith normal hearing ability. Although innovations in sign language translation\ntechnology aim to tear down this communication barrier, the majority of\nexisting sign language translation systems are either intrusive or constrained\nby resolution or ambient lighting conditions. Moreover, these existing systems\ncan only perform single-sign ASL translation rather than sentence-level\ntranslation, making them much less useful in daily-life communication\nscenarios. In this work, we fill this critical gap by presenting DeepASL, a\ntransformative deep learning-based sign language translation technology that\nenables ubiquitous and non-intrusive American Sign Language (ASL) translation\nat both word and sentence levels. DeepASL uses infrared light as its sensing\nmechanism to non-intrusively capture the ASL signs. It incorporates a novel\nhierarchical bidirectional deep recurrent neural network (HB-RNN) and a\nprobabilistic framework based on Connectionist Temporal Classification (CTC)\nfor word-level and sentence-level ASL translation respectively. To evaluate its\nperformance, we have collected 7,306 samples from 11 participants, covering 56\ncommonly used ASL words and 100 ASL sentences. DeepASL achieves an average\n94.5% word-level translation accuracy and an average 8.2% word error rate on\ntranslating unseen ASL sentences. Given its promising performance, we believe\nDeepASL represents a significant step towards breaking the communication\nbarrier between deaf people and hearing majority, and thus has the significant\npotential to fundamentally change deaf people's lives. \n\n"}
{"id": "1802.07623", "contents": "Title: Explanations based on the Missing: Towards Contrastive Explanations with\n  Pertinent Negatives Abstract: In this paper we propose a novel method that provides contrastive\nexplanations justifying the classification of an input by a black box\nclassifier such as a deep neural network. Given an input we find what should be\n%necessarily and minimally and sufficiently present (viz. important object\npixels in an image) to justify its classification and analogously what should\nbe minimally and necessarily \\emph{absent} (viz. certain background pixels). We\nargue that such explanations are natural for humans and are used commonly in\ndomains such as health care and criminology. What is minimally but critically\n\\emph{absent} is an important part of an explanation, which to the best of our\nknowledge, has not been explicitly identified by current explanation methods\nthat explain predictions of neural networks. We validate our approach on three\nreal datasets obtained from diverse domains; namely, a handwritten digits\ndataset MNIST, a large procurement fraud dataset and a brain activity strength\ndataset. In all three cases, we witness the power of our approach in generating\nprecise explanations that are also easy for human experts to understand and\nevaluate. \n\n"}
{"id": "1802.08241", "contents": "Title: Hessian-based Analysis of Large Batch Training and Robustness to\n  Adversaries Abstract: Large batch size training of Neural Networks has been shown to incur accuracy\nloss when trained with the current methods. The exact underlying reasons for\nthis are still not completely understood. Here, we study large batch size\ntraining through the lens of the Hessian operator and robust optimization. In\nparticular, we perform a Hessian based study to analyze exactly how the\nlandscape of the loss function changes when training with large batch size. We\ncompute the true Hessian spectrum, without approximation, by back-propagating\nthe second derivative. Extensive experiments on multiple networks show that\nsaddle-points are not the cause for generalization gap of large batch size\ntraining, and the results consistently show that large batch converges to\npoints with noticeably higher Hessian spectrum. Furthermore, we show that\nrobust training allows one to favor flat areas, as points with large Hessian\nspectrum show poor robustness to adversarial perturbation. We further study\nthis relationship, and provide empirical and theoretical proof that the inner\nloop for robust training is a saddle-free optimization problem \\textit{almost\neverywhere}. We present detailed experiments with five different network\narchitectures, including a residual network, tested on MNIST, CIFAR-10, and\nCIFAR-100 datasets. We have open sourced our method which can be accessed at\n[1]. \n\n"}
{"id": "1802.09766", "contents": "Title: Learning Representations for Neural Network-Based Classification Using\n  the Information Bottleneck Principle Abstract: In this theory paper, we investigate training deep neural networks (DNNs) for\nclassification via minimizing the information bottleneck (IB) functional. We\nshow that the resulting optimization problem suffers from two severe issues:\nFirst, for deterministic DNNs, either the IB functional is infinite for almost\nall values of network parameters, making the optimization problem ill-posed, or\nit is piecewise constant, hence not admitting gradient-based optimization\nmethods. Second, the invariance of the IB functional under bijections prevents\nit from capturing properties of the learned representation that are desirable\nfor classification, such as robustness and simplicity. We argue that these\nissues are partly resolved for stochastic DNNs, DNNs that include a (hard or\nsoft) decision rule, or by replacing the IB functional with related, but more\nwell-behaved cost functions. We conclude that recent successes reported about\ntraining DNNs using the IB framework must be attributed to such solutions. As a\nside effect, our results indicate limitations of the IB framework for the\nanalysis of DNNs. We also note that rather than trying to repair the inherent\nproblems in the IB functional, a better approach may be to design regularizers\non latent representation enforcing the desired properties directly. \n\n"}
{"id": "1802.09900", "contents": "Title: Query-Free Attacks on Industry-Grade Face Recognition Systems under\n  Resource Constraints Abstract: To launch black-box attacks against a Deep Neural Network (DNN) based Face\nRecognition (FR) system, one needs to build \\textit{substitute} models to\nsimulate the target model, so the adversarial examples discovered from\nsubstitute models could also mislead the target model. Such\n\\textit{transferability} is achieved in recent studies through querying the\ntarget model to obtain data for training the substitute models. A real-world\ntarget, likes the FR system of law enforcement, however, is less accessible to\nthe adversary. To attack such a system, a substitute model with similar quality\nas the target model is needed to identify their common defects. This is hard\nsince the adversary often does not have the enough resources to train such a\npowerful model (hundreds of millions of images and rooms of GPUs are needed to\ntrain a commercial FR system).\n  We found in our research, however, that a resource-constrained adversary\ncould still effectively approximate the target model's capability to recognize\n\\textit{specific} individuals, by training \\textit{biased} substitute models on\nadditional images of those victims whose identities the attacker want to cover\nor impersonate. This is made possible by a new property we discovered, called\n\\textit{Nearly Local Linearity} (NLL), which models the observation that an\nideal DNN model produces the image representations (embeddings) whose distances\namong themselves truthfully describe the human perception of the differences\namong the input images. By simulating this property around the victim's images,\nwe significantly improve the transferability of black-box impersonation attacks\nby nearly 50\\%. Particularly, we successfully attacked a commercial system\ntrained over 20 million images, using 4 million images and 1/5 of the training\ntime but achieving 62\\% transferability in an impersonation attack and 89\\% in\na dodging attack. \n\n"}
{"id": "1802.09972", "contents": "Title: Fusion of Multispectral Data Through Illumination-aware Deep Neural\n  Networks for Pedestrian Detection Abstract: Multispectral pedestrian detection has received extensive attention in recent\nyears as a promising solution to facilitate robust human target detection for\naround-the-clock applications (e.g. security surveillance and autonomous\ndriving). In this paper, we demonstrate illumination information encoded in\nmultispectral images can be utilized to significantly boost performance of\npedestrian detection. A novel illumination-aware weighting mechanism is present\nto accurately depict illumination condition of a scene. Such illumination\ninformation is incorporated into two-stream deep convolutional neural networks\nto learn multispectral human-related features under different illumination\nconditions (daytime and nighttime). Moreover, we utilized illumination\ninformation together with multispectral data to generate more accurate semantic\nsegmentation which are used to boost pedestrian detection accuracy. Putting all\nof the pieces together, we present a powerful framework for multispectral\npedestrian detection based on multi-task learning of illumination-aware\npedestrian detection and semantic segmentation. Our proposed method is trained\nend-to-end using a well-designed multi-task loss function and outperforms\nstate-of-the-art approaches on KAIST multispectral pedestrian dataset. \n\n"}
{"id": "1802.10591", "contents": "Title: Stereoscopic Neural Style Transfer Abstract: This paper presents the first attempt at stereoscopic neural style transfer,\nwhich responds to the emerging demand for 3D movies or AR/VR. We start with a\ncareful examination of applying existing monocular style transfer methods to\nleft and right views of stereoscopic images separately. This reveals that the\noriginal disparity consistency cannot be well preserved in the final\nstylization results, which causes 3D fatigue to the viewers. To address this\nissue, we incorporate a new disparity loss into the widely adopted style loss\nfunction by enforcing the bidirectional disparity constraint in non-occluded\nregions. For a practical real-time solution, we propose the first feed-forward\nnetwork by jointly training a stylization sub-network and a disparity\nsub-network, and integrate them in a feature level middle domain. Our disparity\nsub-network is also the first end-to-end network for simultaneous bidirectional\ndisparity and occlusion mask estimation. Finally, our network is effectively\nextended to stereoscopic videos, by considering both temporal coherence and\ndisparity consistency. We will show that the proposed method clearly\noutperforms the baseline algorithms both quantitatively and qualitatively. \n\n"}
{"id": "1803.00638", "contents": "Title: Fast and accurate computation of orthogonal moments for texture analysis Abstract: In this work we describe a fast and stable algorithm for the computation of\nthe orthogonal moments of an image. Indeed, orthogonal moments are\ncharacterized by a high discriminative power, but some of their possible\nformulations are characterized by a large computational complexity, which\nlimits their real-time application. This paper describes in detail an approach\nbased on recurrence relations, and proposes an optimized Matlab implementation\nof the corresponding computational procedure, aiming to solve the above\nlimitations and put at the community's disposal an efficient and easy to use\nsoftware. In our experiments we evaluate the effectiveness of the recurrence\nformulation, as well as its performance for the reconstruction task, in\ncomparison to the closed form representation, often used in the literature. The\nresults show a sensible reduction in the computational complexity, together\nwith a greater accuracy in reconstruction. In order to assess and compare the\naccuracy of the computed moments in texture analysis, we perform classification\nexperiments on six well-known databases of texture images. Again, the\nrecurrence formulation performs better in classification than the closed form\nrepresentation. More importantly, if computed from the GLCM of the image using\nthe proposed stable procedure, the orthogonal moments outperform in some\nsituations some of the most diffused state-of-the-art descriptors for texture\nclassification. \n\n"}
{"id": "1803.00726", "contents": "Title: Pentagram maps and refactorization in Poisson-Lie groups Abstract: The pentagram map was introduced by R. Schwartz in 1992 and is now one of the\nmost renowned discrete integrable systems. In the present paper we prove that\nthis map, as well as all its known integrable multidimensional generalizations,\ncan be seen as refactorization-type mappings in the Poisson-Lie group of\npseudo-difference operators. This brings the pentagram map into the rich\nframework of Poisson-Lie groups, both describing new structures and simplifying\nand revealing the origin of its known properties. In particular, for\nmultidimensional pentagram maps the Poisson-Lie group setting provides new Lax\nforms with a spectral parameter and, more importantly, invariant Poisson\nstructures in all dimensions, the existence of which has been an open problem\nsince the introduction of those maps. Furthermore, for the classical pentagram\nmap our approach naturally yields its combinatorial description in terms of\nweighted directed networks and cluster algebras. \n\n"}
{"id": "1803.02623", "contents": "Title: TRLG: Fragile blind quad watermarking for image tamper detection and\n  recovery by providing compact digests with quality optimized using LWT and GA Abstract: In this paper, an efficient fragile blind quad watermarking scheme for image\ntamper detection and recovery based on lifting wavelet transform and genetic\nalgorithm is proposed. TRLG generates four compact digests with super quality\nbased on lifting wavelet transform and halftoning technique by distinguishing\nthe types of image blocks. In other words, for each 2*2 non-overlap blocks,\nfour chances for recovering destroyed blocks are considered. A special\nparameter estimation technique based on genetic algorithm is performed to\nimprove and optimize the quality of digests and watermarked image. Furthermore,\nCCS map is used to determine the mapping block for embedding information,\nencrypting and confusing the embedded information. In order to improve the\nrecovery rate, Mirror-aside and Partner-block are proposed. The experiments\nthat have been conducted to evaluate the performance of TRLG proved the\nsuperiority in terms of quality of the watermarked and recovered image, tamper\nlocalization and security compared with state-of-the-art methods. The results\nindicate that the PSNR and SSIM of the watermarked image are about 46 dB and\napproximately one, respectively. Also, the mean of PSNR and SSIM of several\nrecovered images which has been destroyed about 90% is reached to 24 dB and\n0.86, respectively. \n\n"}
{"id": "1803.04022", "contents": "Title: Deep Dictionary Learning: A PARametric NETwork Approach Abstract: Deep dictionary learning seeks multiple dictionaries at different image\nscales to capture complementary coherent characteristics. We propose a method\nfor learning a hierarchy of synthesis dictionaries with an image classification\ngoal. The dictionaries and classification parameters are trained by a\nclassification objective, and the sparse features are extracted by reducing a\nreconstruction loss in each layer. The reconstruction objectives in some sense\nregularize the classification problem and inject source signal information in\nthe extracted features. The performance of the proposed hierarchical method\nincreases by adding more layers, which consequently makes this model easier to\ntune and adapt. The proposed algorithm furthermore, shows remarkably lower\nfooling rate in presence of adversarial perturbation. The validation of the\nproposed approach is based on its classification performance using four\nbenchmark datasets and is compared to a CNN of similar size. \n\n"}
{"id": "1803.04351", "contents": "Title: idtracker.ai: Tracking all individuals in large collectives of unmarked\n  animals Abstract: Our understanding of collective animal behavior is limited by our ability to\ntrack each of the individuals. We describe an algorithm and software,\nidtracker.ai, that extracts from video all trajectories with correct identities\nat a high accuracy for collectives of up to 100 individuals. It uses two deep\nnetworks, one detecting when animals touch or cross and another one for animal\nidentification, trained adaptively to conditions and difficulty of the video. \n\n"}
{"id": "1803.04667", "contents": "Title: Dynamic Vision Sensors for Human Activity Recognition Abstract: Unlike conventional cameras which capture video at a fixed frame rate,\nDynamic Vision Sensors (DVS) record only changes in pixel intensity values. The\noutput of DVS is simply a stream of discrete ON/OFF events based on the\npolarity of change in its pixel values. DVS has many attractive features such\nas low power consumption, high temporal resolution, high dynamic range and\nfewer storage requirements. All these make DVS a very promising camera for\npotential applications in wearable platforms where power consumption is a major\nconcern.\n  In this paper, we explore the feasibility of using DVS for Human Activity\nRecognition (HAR). We propose to use the various slices (such as $x-y$, $x-t$,\nand $y-t$) of the DVS video as a feature map for HAR and denote them as Motion\nMaps. We show that fusing motion maps with Motion Boundary Histogram (MBH) give\ngood performance on the benchmark DVS dataset as well as on a real DVS gesture\ndataset collected by us. Interestingly, the performance of DVS is comparable to\nthat of conventional videos although DVS captures only sparse motion\ninformation. \n\n"}
{"id": "1803.05863", "contents": "Title: Learned Neural Iterative Decoding for Lossy Image Compression Systems Abstract: For lossy image compression systems, we develop an algorithm, iterative\nrefinement, to improve the decoder's reconstruction compared to standard\ndecoding techniques. Specifically, we propose a recurrent neural network\napproach for nonlinear, iterative decoding. Our decoder, which works with any\nencoder, employs self-connected memory units that make use of causal and\nnon-causal spatial context information to progressively reduce reconstruction\nerror over a fixed number of steps. We experiment with variants of our\nestimator and find that iterative refinement consistently creates lower\ndistortion images of higher perceptual quality compared to other approaches.\nSpecifically, on the Kodak Lossless True Color Image Suite, we observe as much\nas a 0.871 decibel (dB) gain over JPEG, a 1.095 dB gain over JPEG 2000, and a\n0.971 dB gain over a competitive neural model. \n\n"}
{"id": "1803.05873", "contents": "Title: Deep Structure Inference Network for Facial Action Unit Recognition Abstract: Facial expressions are combinations of basic components called Action Units\n(AU). Recognizing AUs is key for developing general facial expression analysis.\nIn recent years, most efforts in automatic AU recognition have been dedicated\nto learning combinations of local features and to exploiting correlations\nbetween Action Units. In this paper, we propose a deep neural architecture that\ntackles both problems by combining learned local and global features in its\ninitial stages and replicating a message passing algorithm between classes\nsimilar to a graphical model inference approach in later stages. We show that\nby training the model end-to-end with increased supervision we improve\nstate-of-the-art by 5.3% and 8.2% performance on BP4D and DISFA datasets,\nrespectively. \n\n"}
{"id": "1803.05984", "contents": "Title: Deep Co-Training for Semi-Supervised Image Recognition Abstract: In this paper, we study the problem of semi-supervised image recognition,\nwhich is to learn classifiers using both labeled and unlabeled images. We\npresent Deep Co-Training, a deep learning based method inspired by the\nCo-Training framework. The original Co-Training learns two classifiers on two\nviews which are data from different sources that describe the same instances.\nTo extend this concept to deep learning, Deep Co-Training trains multiple deep\nneural networks to be the different views and exploits adversarial examples to\nencourage view difference, in order to prevent the networks from collapsing\ninto each other. As a result, the co-trained networks provide different and\ncomplementary information about the data, which is necessary for the\nCo-Training framework to achieve good results. We test our method on SVHN,\nCIFAR-10/100 and ImageNet datasets, and our method outperforms the previous\nstate-of-the-art methods by a large margin. \n\n"}
{"id": "1803.06372", "contents": "Title: Stochastic basins of attraction and generalized committor functions Abstract: We generalize the concept of basin of attraction of a stable state in order\nto facilitate the analysis of dynamical systems with noise and to assess\nstability properties of metastable states and long transients. To this end we\nexamine the notions of mean sojourn times and absorption probabilities for\nMarkov chains and study their relation to the basins of attraction. Our\napproach is applicable to a large variety of problems since in most cases the\ntransfer operator associated to a dynamical system can be approximated by a\nMarkov chain. \n\n"}
{"id": "1803.06541", "contents": "Title: Adaptive strategy for superpixel-based region-growing image segmentation Abstract: This work presents a region-growing image segmentation approach based on\nsuperpixel decomposition. From an initial contour-constrained over-segmentation\nof the input image, the image segmentation is achieved by iteratively merging\nsimilar superpixels into regions. This approach raises two key issues: (1) how\nto compute the similarity between superpixels in order to perform accurate\nmerging and (2) in which order those superpixels must be merged together. In\nthis perspective, we firstly introduce a robust adaptive multi-scale superpixel\nsimilarity in which region comparisons are made both at content and common\nborder level. Secondly, we propose a global merging strategy to efficiently\nguide the region merging process. Such strategy uses an adpative merging\ncriterion to ensure that best region aggregations are given highest priorities.\nThis allows to reach a final segmentation into consistent regions with strong\nboundary adherence. We perform experiments on the BSDS500 image dataset to\nhighlight to which extent our method compares favorably against other\nwell-known image segmentation algorithms. The obtained results demonstrate the\npromising potential of the proposed approach. \n\n"}
{"id": "1803.06629", "contents": "Title: Cross-modality image synthesis from unpaired data using CycleGAN:\n  Effects of gradient consistency loss and training data size Abstract: CT is commonly used in orthopedic procedures. MRI is used along with CT to\nidentify muscle structures and diagnose osteonecrosis due to its superior soft\ntissue contrast. However, MRI has poor contrast for bone structures. Clearly,\nit would be helpful if a corresponding CT were available, as bone boundaries\nare more clearly seen and CT has standardized (i.e., Hounsfield) units.\nTherefore, we aim at MR-to-CT synthesis. The CycleGAN was successfully applied\nto unpaired CT and MR images of the head, these images do not have as much\nvariation of intensity pairs as do images in the pelvic region due to the\npresence of joints and muscles. In this paper, we extended the CycleGAN\napproach by adding the gradient consistency loss to improve the accuracy at the\nboundaries. We conducted two experiments. To evaluate image synthesis, we\ninvestigated dependency of image synthesis accuracy on 1) the number of\ntraining data and 2) the gradient consistency loss. To demonstrate the\napplicability of our method, we also investigated a segmentation accuracy on\nsynthesized images. \n\n"}
{"id": "1803.06798", "contents": "Title: Attention-GAN for Object Transfiguration in Wild Images Abstract: This paper studies the object transfiguration problem in wild images. The\ngenerative network in classical GANs for object transfiguration often\nundertakes a dual responsibility: to detect the objects of interests and to\nconvert the object from source domain to target domain. In contrast, we\ndecompose the generative network into two separat networks, each of which is\nonly dedicated to one particular sub-task. The attention network predicts\nspatial attention maps of images, and the transformation network focuses on\ntranslating objects. Attention maps produced by attention network are\nencouraged to be sparse, so that major attention can be paid to objects of\ninterests. No matter before or after object transfiguration, attention maps\nshould remain constant. In addition, learning attention network can receive\nmore instructions, given the available segmentation annotations of images.\nExperimental results demonstrate the necessity of investigating attention in\nobject transfiguration, and that the proposed algorithm can learn accurate\nattention to improve quality of generated images. \n\n"}
{"id": "1803.07716", "contents": "Title: Generative Adversarial Talking Head: Bringing Portraits to Life with a\n  Weakly Supervised Neural Network Abstract: This paper presents Generative Adversarial Talking Head (GATH), a novel deep\ngenerative neural network that enables fully automatic facial expression\nsynthesis of an arbitrary portrait with continuous action unit (AU)\ncoefficients. Specifically, our model directly manipulates image pixels to make\nthe unseen subject in the still photo express various emotions controlled by\nvalues of facial AU coefficients, while maintaining her personal\ncharacteristics, such as facial geometry, skin color and hair style, as well as\nthe original surrounding background. In contrast to prior work, GATH is purely\ndata-driven and it requires neither a statistical face model nor image\nprocessing tricks to enact facial deformations. Additionally, our model is\ntrained from unpaired data, where the input image, with its auxiliary identity\nlabel taken from abundance of still photos in the wild, and the target frame\nare from different persons. In order to effectively learn such model, we\npropose a novel weakly supervised adversarial learning framework that consists\nof a generator, a discriminator, a classifier and an action unit estimator. Our\nwork gives rise to template-and-target-free expression editing, where still\nfaces can be effortlessly animated with arbitrary AU coefficients provided by\nthe user. \n\n"}
{"id": "1803.07728", "contents": "Title: Unsupervised Representation Learning by Predicting Image Rotations Abstract: Over the last years, deep convolutional neural networks (ConvNets) have\ntransformed the field of computer vision thanks to their unparalleled capacity\nto learn high level semantic image features. However, in order to successfully\nlearn those features, they usually require massive amounts of manually labeled\ndata, which is both expensive and impractical to scale. Therefore, unsupervised\nsemantic feature learning, i.e., learning without requiring manual annotation\neffort, is of crucial importance in order to successfully harvest the vast\namount of visual data that are available today. In our work we propose to learn\nimage features by training ConvNets to recognize the 2d rotation that is\napplied to the image that it gets as input. We demonstrate both qualitatively\nand quantitatively that this apparently simple task actually provides a very\npowerful supervisory signal for semantic feature learning. We exhaustively\nevaluate our method in various unsupervised feature learning benchmarks and we\nexhibit in all of them state-of-the-art performance. Specifically, our results\non those benchmarks demonstrate dramatic improvements w.r.t. prior\nstate-of-the-art approaches in unsupervised representation learning and thus\nsignificantly close the gap with supervised feature learning. For instance, in\nPASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model\nachieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is\nonly 2.4 points lower from the supervised case. We get similarly striking\nresults when we transfer our unsupervised learned features on various other\ntasks, such as ImageNet classification, PASCAL classification, PASCAL\nsegmentation, and CIFAR-10 classification. The code and models of our paper\nwill be published on: https://github.com/gidariss/FeatureLearningRotNet . \n\n"}
{"id": "1803.07801", "contents": "Title: Domain Adaptation for Ear Recognition Using Deep Convolutional Neural\n  Networks Abstract: In this paper, we have extensively investigated the unconstrained ear\nrecognition problem. We have first shown the importance of domain adaptation,\nwhen deep convolutional neural network models are used for ear recognition. To\nenable domain adaptation, we have collected a new ear dataset using the\nMulti-PIE face dataset, which we named as Multi-PIE ear dataset. To improve the\nperformance further, we have combined different deep convolutional neural\nnetwork models. We have analyzed in depth the effect of ear image quality, for\nexample illumination and aspect ratio, on the classification performance.\nFinally, we have addressed the problem of dataset bias in the ear recognition\nfield. Experiments on the UERC dataset have shown that domain adaptation leads\nto a significant performance improvement. For example, when VGG-16 model is\nused and the domain adaptation is applied, an absolute increase of around 10\\%\nhas been achieved. Combining different deep convolutional neural network models\nhas further improved the accuracy by 4\\%. It has also been observed that image\nquality has an influence on the results. In the experiments that we have\nconducted to examine the dataset bias, given an ear image, we were able to\nclassify the dataset that it has come from with 99.71\\% accuracy, which\nindicates a strong bias among the ear recognition datasets. \n\n"}
{"id": "1803.07913", "contents": "Title: HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object\n  Classification Abstract: Event-based cameras have recently drawn the attention of the Computer Vision\ncommunity thanks to their advantages in terms of high temporal resolution, low\npower consumption and high dynamic range, compared to traditional frame-based\ncameras. These properties make event-based cameras an ideal choice for\nautonomous vehicles, robot navigation or UAV vision, among others. However, the\naccuracy of event-based object classification algorithms, which is of crucial\nimportance for any reliable system working in real-world conditions, is still\nfar behind their frame-based counterparts. Two main reasons for this\nperformance gap are: 1. The lack of effective low-level representations and\narchitectures for event-based object classification and 2. The absence of large\nreal-world event-based datasets. In this paper we address both problems. First,\nwe introduce a novel event-based feature representation together with a new\nmachine learning architecture. Compared to previous approaches, we use local\nmemory units to efficiently leverage past temporal information and build a\nrobust event-based representation. Second, we release the first large\nreal-world event-based dataset for object classification. We compare our method\nto the state-of-the-art with extensive experiments, showing better\nclassification performance and real-time computation. \n\n"}
{"id": "1803.08887", "contents": "Title: Dist-GAN: An Improved GAN using Distance Constraints Abstract: We introduce effective training algorithms for Generative Adversarial\nNetworks (GAN) to alleviate mode collapse and gradient vanishing. In our\nsystem, we constrain the generator by an Autoencoder (AE). We propose a\nformulation to consider the reconstructed samples from AE as \"real\" samples for\nthe discriminator. This couples the convergence of the AE with that of the\ndiscriminator, effectively slowing down the convergence of discriminator and\nreducing gradient vanishing. Importantly, we propose two novel distance\nconstraints to improve the generator. First, we propose a latent-data distance\nconstraint to enforce compatibility between the latent sample distances and the\ncorresponding data sample distances. We use this constraint to explicitly\nprevent the generator from mode collapse. Second, we propose a\ndiscriminator-score distance constraint to align the distribution of the\ngenerated samples with that of the real samples through the discriminator\nscore. We use this constraint to guide the generator to synthesize samples that\nresemble the real ones. Our proposed GAN using these distance constraints,\nnamely Dist-GAN, can achieve better results than state-of-the-art methods\nacross benchmark datasets: synthetic, MNIST, MNIST-1K, CelebA, CIFAR-10 and\nSTL-10 datasets. Our code is published here (https://github.com/tntrung/gan)\nfor research. \n\n"}
{"id": "1803.09318", "contents": "Title: Data-driven Discovery of Closure Models Abstract: Derivation of reduced order representations of dynamical systems requires the\nmodeling of the truncated dynamics on the retained dynamics. In its most\ngeneral form, this so-called closure model has to account for memory effects.\nIn this work, we present a framework of operator inference to extract the\ngoverning dynamics of closure from data in a compact, non-Markovian form. We\nemploy sparse polynomial regression and artificial neural networks to extract\nthe underlying operator. For a special class of non-linear systems,\nobservability of the closure in terms of the resolved dynamics is analyzed and\ntheoretical results are presented on the compactness of the memory. The\nproposed framework is evaluated on examples consisting of linear to nonlinear\nsystems with and without chaotic dynamics, with an emphasis on predictive\nperformance on unseen data. \n\n"}
{"id": "1803.09860", "contents": "Title: Three Birds One Stone: A General Architecture for Salient Object\n  Segmentation, Edge Detection and Skeleton Extraction Abstract: In this paper, we aim at solving pixel-wise binary problems, including\nsalient object segmentation, skeleton extraction, and edge detection, by\nintroducing a unified architecture. Previous works have proposed tailored\nmethods for solving each of the three tasks independently. Here, we show that\nthese tasks share some similarities that can be exploited for developing a\nunified framework. In particular, we introduce a horizontal cascade, each\ncomponent of which is densely connected to the outputs of previous component.\nStringing these components together allows us to effectively exploit features\nacross different levels hierarchically to effectively address the multiple\npixel-wise binary regression tasks. To assess the performance of our proposed\nnetwork on these tasks, we carry out exhaustive evaluations on multiple\nrepresentative datasets. Although these tasks are inherently very different, we\nshow that our unified approach performs very well on all of them and works far\nbetter than current single-purpose state-of-the-art methods. All the code in\nthis paper will be publicly available. \n\n"}
{"id": "1803.09932", "contents": "Title: Image Semantic Transformation: Faster, Lighter and Stronger Abstract: We propose Image-Semantic-Transformation-Reconstruction-Circle(ISTRC) model,\na novel and powerful method using facenet's Euclidean latent space to\nunderstand the images. As the name suggests, ISTRC construct the circle, able\nto perfectly reconstruct images. One powerful Euclidean latent space embedded\nin ISTRC is FaceNet's last layer with the power of distinguishing and\nunderstanding images. Our model will reconstruct the images and manipulate\nEuclidean latent vectors to achieve semantic transformations and semantic\nimages arthimetic calculations. In this paper, we show that ISTRC performs 10\nhigh-level semantic transformations like \"Male and female\",\"add smile\",\"open\nmouth\", \"deduct beard or add mustache\", \"bigger/smaller nose\", \"make older and\nyounger\", \"bigger lips\", \"bigger eyes\", \"bigger/smaller mouths\" and \"more\nattractive\". It just takes 3 hours(GTX 1080) to train the models of 10 semantic\ntransformations. \n\n"}
{"id": "1803.10567", "contents": "Title: Image Generation and Translation with Disentangled Representations Abstract: Generative models have made significant progress in the tasks of modeling\ncomplex data distributions such as natural images. The introduction of\nGenerative Adversarial Networks (GANs) and auto-encoders lead to the\npossibility of training on big data sets in an unsupervised manner. However,\nfor many generative models it is not possible to specify what kind of image\nshould be generated and it is not possible to translate existing images into\nnew images of similar domains. Furthermore, models that can perform\nimage-to-image translation often need distinct models for each domain, making\nit hard to scale these systems to multiple domain image-to-image translation.\nWe introduce a model that can do both, controllable image generation and\nimage-to-image translation between multiple domains. We split our image\nrepresentation into two parts encoding unstructured and structured information\nrespectively. The latter is designed in a disentangled manner, so that\ndifferent parts encode different image characteristics. We train an encoder to\nencode images into these representations and use a small amount of labeled data\nto specify what kind of information should be encoded in the disentangled part.\nA generator is trained to generate images from these representations using the\ncharacteristics provided by the disentangled part of the representation.\nThrough this we can control what kind of images the generator generates,\ntranslate images between different domains, and even learn unknown\ndata-generating factors while only using one single model. \n\n"}
{"id": "1803.11111", "contents": "Title: Bag of Recurrence Patterns Representation for Time-Series Classification Abstract: Time-Series Classification (TSC) has attracted a lot of attention in pattern\nrecognition, because wide range of applications from different domains such as\nfinance and health informatics deal with time-series signals. Bag of Features\n(BoF) model has achieved a great success in TSC task by summarizing signals\naccording to the frequencies of \"feature words\" of a data-learned dictionary.\nThis paper proposes embedding the Recurrence Plots (RP), a visualization\ntechnique for analysis of dynamic systems, in the BoF model for TSC. While the\ntraditional BoF approach extracts features from 1D signal segments, this paper\nuses the RP to transform time-series into 2D texture images and then applies\nthe BoF on them. Image representation of time-series enables us to explore\ndifferent visual descriptors that are not available for 1D signals and to\ntreats TSC task as a texture recognition problem. Experimental results on the\nUCI time-series classification archive demonstrates a significant accuracy\nboost by the proposed Bag of Recurrence patterns (BoR), compared not only to\nthe existing BoF models, but also to the state-of-the art algorithms. \n\n"}
{"id": "1803.11147", "contents": "Title: Learning Kinematic Descriptions using SPARE: Simulated and Physical\n  ARticulated Extendable dataset Abstract: Next generation robots will need to understand intricate and articulated\nobjects as they cooperate in human environments. To do so, these robots will\nneed to move beyond their current abilities--- working with relatively simple\nobjects in a task-indifferent manner--- toward more sophisticated abilities\nthat dynamically estimate the properties of complex, articulated objects. To\nthat end, we make two compelling contributions toward general articulated\n(physical) object understanding in this paper. First, we introduce a new\ndataset, SPARE: Simulated and Physical ARticulated Extendable dataset. SPARE is\nan extendable open-source dataset providing equivalent simulated and physical\ninstances of articulated objects (kinematic chains), providing the greater\nresearch community with a training and evaluation tool for methods generating\nkinematic descriptions of articulated objects. To the best of our knowledge,\nthis is the first joint visual and physical (3D-printable) dataset for the\nVision community. Second, we present a deep neural network that can predit the\nnumber of links and the length of the links of an articulated object. These new\nideas outperform classical approaches to understanding kinematic chains, such\ntracking-based methods, which fail in the case of occlusion and do not leverage\nmultiple views when available. \n\n"}
{"id": "1803.11186", "contents": "Title: Two can play this Game: Visual Dialog with Discriminative Question\n  Generation and Answering Abstract: Human conversation is a complex mechanism with subtle nuances. It is hence an\nambitious goal to develop artificial intelligence agents that can participate\nfluently in a conversation. While we are still far from achieving this goal,\nrecent progress in visual question answering, image captioning, and visual\nquestion generation shows that dialog systems may be realizable in the not too\ndistant future. To this end, a novel dataset was introduced recently and\nencouraging results were demonstrated, particularly for question answering. In\nthis paper, we demonstrate a simple symmetric discriminative baseline, that can\nbe applied to both predicting an answer as well as predicting a question. We\nshow that this method performs on par with the state of the art, even memory\nnet based methods. In addition, for the first time on the visual dialog\ndataset, we assess the performance of a system asking questions, and\ndemonstrate how visual dialog can be generated from discriminative question\ngeneration and question answering. \n\n"}
{"id": "1803.11303", "contents": "Title: Pancreas Segmentation in CT and MRI Images via Domain Specific Network\n  Designing and Recurrent Neural Contextual Learning Abstract: Automatic pancreas segmentation in radiology images, eg., computed tomography\n(CT) and magnetic resonance imaging (MRI), is frequently required by\ncomputer-aided screening, diagnosis, and quantitative assessment. Yet pancreas\nis a challenging abdominal organ to segment due to the high inter-patient\nanatomical variability in both shape and volume metrics. Recently,\nconvolutional neural networks (CNNs) have demonstrated promising performance on\naccurate segmentation of pancreas. However, the CNN-based method often suffers\nfrom segmentation discontinuity for reasons such as noisy image quality and\nblurry pancreatic boundary. From this point, we propose to introduce recurrent\nneural networks (RNNs) to address the problem of spatial non-smoothness of\ninter-slice pancreas segmentation across adjacent image slices. To inference\ninitial segmentation, we first train a 2D CNN sub-network, where we modify its\nnetwork architecture with deep-supervision and multi-scale feature map\naggregation so that it can be trained from scratch with small-sized training\ndata and presents superior performance than transferred models. Thereafter, the\nsuccessive CNN outputs are processed by another RNN sub-network, which refines\nthe consistency of segmented shapes. More specifically, the RNN sub-network\nconsists convolutional long short-term memory (CLSTM) units in both top-down\nand bottom-up directions, which regularizes the segmentation of an image by\nintegrating predictions of its neighboring slices. We train the stacked CNN-RNN\nmodel end-to-end and perform quantitative evaluations on both CT and MRI\nimages. \n\n"}
{"id": "1803.11361", "contents": "Title: DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer Abstract: We present a novel Dynamic Differentiable Reasoning (DDR) framework for\njointly learning branching programs and the functions composing them; this\nresolves a significant nondifferentiability inhibiting recent dynamic\narchitectures. We apply our framework to two settings in two highly compact and\ndata efficient architectures: DDRprog for CLEVR Visual Question Answering and\nDDRstack for reverse Polish notation expression evaluation. DDRprog uses a\nrecurrent controller to jointly predict and execute modular neural programs\nthat directly correspond to the underlying question logic; it explicitly forks\nsubprocesses to handle logical branching. By effectively leveraging additional\nstructural supervision, we achieve a large improvement over previous approaches\nin subtask consistency and a small improvement in overall accuracy. We further\ndemonstrate the benefits of structural supervision in the RPN setting: the\ninclusion of a stack assumption in DDRstack allows our approach to generalize\nto long expressions where an LSTM fails the task. \n\n"}
{"id": "1804.00060", "contents": "Title: Class Subset Selection for Transfer Learning using Submodularity Abstract: In recent years, it is common practice to extract fully-connected layer (fc)\nfeatures that were learned while performing image classification on a source\ndataset, such as ImageNet, and apply them generally to a wide range of other\ntasks. The general usefulness of some large training datasets for transfer\nlearning is not yet well understood, and raises a number of questions. For\nexample, in the context of transfer learning, what is the role of a specific\nclass in the source dataset, and how is the transferability of fc features\naffected when they are trained using various subsets of the set of all classes\nin the source dataset? In this paper, we address the question of how to select\nan optimal subset of the set of classes, subject to a budget constraint, that\nwill more likely generate good features for other tasks. To accomplish this, we\nuse a submodular set function to model the accuracy achievable on a new task\nwhen the features have been learned on a given subset of classes of the source\ndataset. An optimal subset is identified as the set that maximizes this\nsubmodular function. The maximization can be accomplished using an efficient\ngreedy algorithm that comes with guarantees on the optimality of the solution.\nWe empirically validate our submodular model by successfully identifying\nsubsets of classes that produce good features for new tasks. \n\n"}
{"id": "1804.00389", "contents": "Title: Low-Latency Video Semantic Segmentation Abstract: Recent years have seen remarkable progress in semantic segmentation. Yet, it\nremains a challenging task to apply segmentation techniques to video-based\napplications. Specifically, the high throughput of video streams, the sheer\ncost of running fully convolutional networks, together with the low-latency\nrequirements in many real-world applications, e.g. autonomous driving, present\na significant challenge to the design of the video segmentation framework. To\ntackle this combined challenge, we develop a framework for video semantic\nsegmentation, which incorporates two novel components: (1) a feature\npropagation module that adaptively fuses features over time via spatially\nvariant convolution, thus reducing the cost of per-frame computation; and (2)\nan adaptive scheduler that dynamically allocate computation based on accuracy\nprediction. Both components work together to ensure low latency while\nmaintaining high segmentation quality. On both Cityscapes and CamVid, the\nproposed framework obtained competitive performance compared to the state of\nthe art, while substantially reducing the latency, from 360 ms to 119 ms. \n\n"}
{"id": "1804.00410", "contents": "Title: SyncGAN: Synchronize the Latent Space of Cross-modal Generative\n  Adversarial Networks Abstract: Generative adversarial network (GAN) has achieved impressive success on\ncross-domain generation, but it faces difficulty in cross-modal generation due\nto the lack of a common distribution between heterogeneous data. Most existing\nmethods of conditional based cross-modal GANs adopt the strategy of\none-directional transfer and have achieved preliminary success on text-to-image\ntransfer. Instead of learning the transfer between different modalities, we aim\nto learn a synchronous latent space representing the cross-modal common\nconcept. A novel network component named synchronizer is proposed in this work\nto judge whether the paired data is synchronous/corresponding or not, which can\nconstrain the latent space of generators in the GANs. Our GAN model, named as\nSyncGAN, can successfully generate synchronous data (e.g., a pair of image and\nsound) from identical random noise. For transforming data from one modality to\nanother, we recover the latent code by inverting the mappings of a generator\nand use it to generate data of different modality. In addition, the proposed\nmodel can achieve semi-supervised learning, which makes our model more flexible\nfor practical applications. \n\n"}
{"id": "1804.00501", "contents": "Title: Multilayer Complex Network Descriptors for Color-Texture\n  Characterization Abstract: A new method based on complex networks is proposed for color-texture\nanalysis. The proposal consists on modeling the image as a multilayer complex\nnetwork where each color channel is a layer, and each pixel (in each color\nchannel) is represented as a network vertex. The network dynamic evolution is\naccessed using a set of modeling parameters (radii and thresholds), and new\ncharacterization techniques are introduced to capt information regarding within\nand between color channel spatial interaction. An automatic and adaptive\napproach for threshold selection is also proposed. We conduct classification\nexperiments on 5 well-known datasets: Vistex, Usptex, Outex13, CURet and MBT.\nResults among various literature methods are compared, including deep\nconvolutional neural networks with pre-trained architectures. The proposed\nmethod presented the highest overall performance over the 5 datasets, with 97.7\nof mean accuracy against 97.0 achieved by the ResNet convolutional neural\nnetwork with 50 layers. \n\n"}
{"id": "1804.01203", "contents": "Title: An integration of fast alignment and maximum-likelihood methods for\n  electron subtomogram averaging and classification Abstract: Motivation: Cellular Electron CryoTomography (CECT) is an emerging 3D imaging\ntechnique that visualizes subcellular organization of single cells at\nsubmolecular resolution and in near-native state. CECT captures large numbers\nof macromolecular complexes of highly diverse structures and abundances.\nHowever, the structural complexity and imaging limits complicate the systematic\nde novo structural recovery and recognition of these macromolecular complexes.\nEfficient and accurate reference-free subtomogram averaging and classification\nrepresent the most critical tasks for such analysis. Existing subtomogram\nalignment based methods are prone to the missing wedge effects and low\nsignal-to-noise ratio (SNR). Moreover, existing maximum-likelihood based\nmethods rely on integration operations, which are in principle computationally\ninfeasible for accurate calculation.\n  Results: Built on existing works, we propose an integrated method, Fast\nAlignment Maximum Likelihood method (FAML), which uses fast subtomogram\nalignment to sample sub-optimal rigid transformations. The transformations are\nthen used to approximate integrals for maximum-likelihood update of subtomogram\naverages through expectation-maximization algorithm. Our tests on simulated and\nexperimental subtomograms showed that, compared to our previously developed\nfast alignment method (FA), FAML is significantly more robust to noise and\nmissing wedge effects with moderate increases of computation cost.Besides, FAML\nperforms well with significantly fewer input subtomograms when the FA method\nfails. Therefore, FAML can serve as a key component for improved construction\nof initial structural models from macromolecules captured by CECT. \n\n"}
{"id": "1804.01307", "contents": "Title: Btrfly Net: Vertebrae Labelling with Energy-based Adversarial Learning\n  of Local Spine Prior Abstract: Robust localisation and identification of vertebrae is essential for\nautomated spine analysis. The contribution of this work to the task is\ntwo-fold: (1) Inspired by the human expert, we hypothesise that a sagittal and\ncoronal reformation of the spine contain sufficient information for labelling\nthe vertebrae. Thereby, we propose a butterfly-shaped network architecture\n(termed Btrfly Net) that efficiently combines the information across\nreformations. (2) Underpinning the Btrfly net, we present an energy-based\nadversarial training regime that encodes local spine structure as an anatomical\nprior into the network, thereby enabling it to achieve state-of-art performance\nin all standard metrics on a benchmark dataset of 302 scans without any\npost-processing during inference. \n\n"}
{"id": "1804.01793", "contents": "Title: End-to-End Saliency Mapping via Probability Distribution Prediction Abstract: Most saliency estimation methods aim to explicitly model low-level\nconspicuity cues such as edges or blobs and may additionally incorporate\ntop-down cues using face or text detection. Data-driven methods for training\nsaliency models using eye-fixation data are increasingly popular, particularly\nwith the introduction of large-scale datasets and deep architectures. However,\ncurrent methods in this latter paradigm use loss functions designed for\nclassification or regression tasks whereas saliency estimation is evaluated on\ntopographical maps. In this work, we introduce a new saliency map model which\nformulates a map as a generalized Bernoulli distribution. We then train a deep\narchitecture to predict such maps using novel loss functions which pair the\nsoftmax activation function with measures designed to compute distances between\nprobability distributions. We show in extensive experiments the effectiveness\nof such loss functions over standard ones on four public benchmark datasets,\nand demonstrate improved performance over state-of-the-art saliency methods. \n\n"}
{"id": "1804.02391", "contents": "Title: Learn To Pay Attention Abstract: We propose an end-to-end-trainable attention module for convolutional neural\nnetwork (CNN) architectures built for image classification. The module takes as\ninput the 2D feature vector maps which form the intermediate representations of\nthe input image at different stages in the CNN pipeline, and outputs a 2D\nmatrix of scores for each map. Standard CNN architectures are modified through\nthe incorporation of this module, and trained under the constraint that a\nconvex combination of the intermediate 2D feature vectors, as parameterised by\nthe score matrices, must \\textit{alone} be used for classification.\nIncentivised to amplify the relevant and suppress the irrelevant or misleading,\nthe scores thus assume the role of attention values. Our experimental\nobservations provide clear evidence to this effect: the learned attention maps\nneatly highlight the regions of interest while suppressing background clutter.\nConsequently, the proposed function is able to bootstrap standard CNN\narchitectures for the task of image classification, demonstrating superior\ngeneralisation over 6 unseen benchmark datasets. When binarised, our attention\nmaps outperform other CNN-based attention maps, traditional saliency maps, and\ntop object proposals for weakly supervised segmentation as demonstrated on the\nObject Discovery dataset. We also demonstrate improved robustness against the\nfast gradient sign method of adversarial attack. \n\n"}
{"id": "1804.02541", "contents": "Title: Statistical transformer networks: learning shape and appearance models\n  via self supervision Abstract: We generalise Spatial Transformer Networks (STN) by replacing the parametric\ntransformation of a fixed, regular sampling grid with a deformable, statistical\nshape model which is itself learnt. We call this a Statistical Transformer\nNetwork (StaTN). By training a network containing a StaTN end-to-end for a\nparticular task, the network learns the optimal nonrigid alignment of the input\ndata for the task. Moreover, the statistical shape model is learnt with no\ndirect supervision (such as landmarks) and can be reused for other tasks.\nBesides training for a specific task, we also show that a StaTN can learn a\nshape model using generic loss functions. This includes a loss inspired by the\nminimum description length principle in which an appearance model is also\nlearnt from scratch. In this configuration, our model learns an active\nappearance model and a means to fit the model from scratch with no supervision\nat all, even identity labels. \n\n"}
{"id": "1804.02595", "contents": "Title: Training Multi-organ Segmentation Networks with Sample Selection by\n  Relaxed Upper Confident Bound Abstract: Deep convolutional neural networks (CNNs), especially fully convolutional\nnetworks, have been widely applied to automatic medical image segmentation\nproblems, e.g., multi-organ segmentation. Existing CNN-based segmentation\nmethods mainly focus on looking for increasingly powerful network\narchitectures, but pay less attention to data sampling strategies for training\nnetworks more effectively. In this paper, we present a simple but effective\nsample selection method for training multi-organ segmentation networks. Sample\nselection exhibits an exploitation-exploration strategy, i.e., exploiting hard\nsamples and exploring less frequently visited samples. Based on the fact that\nvery hard samples might have annotation errors, we propose a new sample\nselection policy, named Relaxed Upper Confident Bound (RUCB). Compared with\nother sample selection policies, e.g., Upper Confident Bound (UCB), it exploits\na range of hard samples rather than being stuck with a small set of very hard\nones, which mitigates the influence of annotation errors during training. We\napply this new sample selection policy to training a multi-organ segmentation\nnetwork on a dataset containing 120 abdominal CT scans and show that it boosts\nsegmentation performance significantly. \n\n"}
{"id": "1804.03619", "contents": "Title: Looking to Listen at the Cocktail Party: A Speaker-Independent\n  Audio-Visual Model for Speech Separation Abstract: We present a joint audio-visual model for isolating a single speech signal\nfrom a mixture of sounds such as other speakers and background noise. Solving\nthis task using only audio as input is extremely challenging and does not\nprovide an association of the separated speech signals with speakers in the\nvideo. In this paper, we present a deep network-based model that incorporates\nboth visual and auditory signals to solve this task. The visual features are\nused to \"focus\" the audio on desired speakers in a scene and to improve the\nspeech separation quality. To train our joint audio-visual model, we introduce\nAVSpeech, a new dataset comprised of thousands of hours of video segments from\nthe Web. We demonstrate the applicability of our method to classic speech\nseparation tasks, as well as real-world scenarios involving heated interviews,\nnoisy bars, and screaming children, only requiring the user to specify the face\nof the person in the video whose speech they want to isolate. Our method shows\nclear advantage over state-of-the-art audio-only speech separation in cases of\nmixed speech. In addition, our model, which is speaker-independent (trained\nonce, applicable to any speaker), produces better results than recent\naudio-visual speech separation methods that are speaker-dependent (require\ntraining a separate model for each speaker of interest). \n\n"}
{"id": "1804.03821", "contents": "Title: ExFuse: Enhancing Feature Fusion for Semantic Segmentation Abstract: Modern semantic segmentation frameworks usually combine low-level and\nhigh-level features from pre-trained backbone convolutional models to boost\nperformance. In this paper, we first point out that a simple fusion of\nlow-level and high-level features could be less effective because of the gap in\nsemantic levels and spatial resolution. We find that introducing semantic\ninformation into low-level features and high-resolution details into high-level\nfeatures is more effective for the later fusion. Based on this observation, we\npropose a new framework, named ExFuse, to bridge the gap between low-level and\nhigh-level features thus significantly improve the segmentation quality by\n4.0\\% in total. Furthermore, we evaluate our approach on the challenging PASCAL\nVOC 2012 segmentation benchmark and achieve 87.9\\% mean IoU, which outperforms\nthe previous state-of-the-art results. \n\n"}
{"id": "1804.04512", "contents": "Title: DLL: A Blazing Fast Deep Neural Network Library Abstract: Deep Learning Library (DLL) is a new library for machine learning with deep\nneural networks that focuses on speed. It supports feed-forward neural networks\nsuch as fully-connected Artificial Neural Networks (ANNs) and Convolutional\nNeural Networks (CNNs). It also has very comprehensive support for Restricted\nBoltzmann Machines (RBMs) and Convolutional RBMs. Our main motivation for this\nwork was to propose and evaluate novel software engineering strategies with\npotential to accelerate runtime for training and inference. Such strategies are\nmostly independent of the underlying deep learning algorithms. On three\ndifferent datasets and for four different neural network models, we compared\nDLL to five popular deep learning frameworks. Experimentally, it is shown that\nthe proposed framework is systematically and significantly faster on CPU and\nGPU. In terms of classification performance, similar accuracies as the other\nframeworks are reported. \n\n"}
{"id": "1804.04600", "contents": "Title: Personalized Classifier for Food Image Recognition Abstract: Currently, food image recognition tasks are evaluated against fixed datasets.\nHowever, in real-world conditions, there are cases in which the number of\nsamples in each class continues to increase and samples from novel classes\nappear. In particular, dynamic datasets in which each individual user creates\nsamples and continues the updating process often have content that varies\nconsiderably between different users, and the number of samples per person is\nvery limited. A single classifier common to all users cannot handle such\ndynamic data. Bridging the gap between the laboratory environment and the real\nworld has not yet been accomplished on a large scale. Personalizing a\nclassifier incrementally for each user is a promising way to do this. In this\npaper, we address the personalization problem, which involves adapting to the\nuser's domain incrementally using a very limited number of samples. We propose\na simple yet effective personalization framework which is a combination of the\nnearest class mean classifier and the 1-nearest neighbor classifier based on\ndeep features. To conduct realistic experiments, we made use of a new dataset\nof daily food images collected by a food-logging application. Experimental\nresults show that our proposed method significantly outperforms existing\nmethods. \n\n"}
{"id": "1804.04876", "contents": "Title: Group Anomaly Detection using Deep Generative Models Abstract: Unlike conventional anomaly detection research that focuses on point\nanomalies, our goal is to detect anomalous collections of individual data\npoints. In particular, we perform group anomaly detection (GAD) with an\nemphasis on irregular group distributions (e.g. irregular mixtures of image\npixels). GAD is an important task in detecting unusual and anomalous phenomena\nin real-world applications such as high energy particle physics, social media,\nand medical imaging. In this paper, we take a generative approach by proposing\ndeep generative models: Adversarial autoencoder (AAE) and variational\nautoencoder (VAE) for group anomaly detection. Both AAE and VAE detect group\nanomalies using point-wise input data where group memberships are known a\npriori. We conduct extensive experiments to evaluate our models on real-world\ndatasets. The empirical results demonstrate that our approach is effective and\nrobust in detecting group anomalies. \n\n"}
{"id": "1804.05113", "contents": "Title: Multilevel Language and Vision Integration for Text-to-Clip Retrieval Abstract: We address the problem of text-based activity retrieval in video. Given a\nsentence describing an activity, our task is to retrieve matching clips from an\nuntrimmed video. To capture the inherent structures present in both text and\nvideo, we introduce a multilevel model that integrates vision and language\nfeatures earlier and more tightly than prior work. First, we inject text\nfeatures early on when generating clip proposals, to help eliminate unlikely\nclips and thus speed up processing and boost performance. Second, to learn a\nfine-grained similarity metric for retrieval, we use visual features to\nmodulate the processing of query sentences at the word level in a recurrent\nneural network. A multi-task loss is also employed by adding query\nre-generation as an auxiliary task. Our approach significantly outperforms\nprior work on two challenging benchmarks: Charades-STA and ActivityNet\nCaptions. \n\n"}
{"id": "1804.05164", "contents": "Title: Road Segmentation Using CNN with GRU Abstract: This paper presents an accurate and fast algorithm for road segmentation\nusing convolutional neural network (CNN) and gated recurrent units (GRU). For\nautonomous vehicles, road segmentation is a fundamental task that can provide\nthe drivable area for path planning. The existing deep neural network based\nsegmentation algorithms usually take a very deep encoder-decoder structure to\nfuse pixels, which requires heavy computations, large memory and long\nprocessing time. Hereby, a CNN-GRU network model is proposed and trained to\nperform road segmentation using data captured by the front camera of a vehicle.\nGRU network obtains a long spatial sequence with lower computational\ncomplexity, comparing to traditional encoder-decoder architecture. The proposed\nroad detector is evaluated on the KITTI road benchmark and achieves high\naccuracy for road segmentation at real-time processing speed. \n\n"}
{"id": "1804.05359", "contents": "Title: Pointwise convergence of Birkhoff averages for global observables Abstract: It is well-known that a strict analogue of the Birkhoff Ergodic Theorem in\ninfinite ergodic theory is trivial; it states that for any\ninfinite-measure-preserving ergodic system the Birkhoff average of every\nintegrable function is almost everywhere zero. Nor does a different rescaling\nof the Birkhoff sum that leads to a non-degenerate pointwise limit exist. In\nthis paper we give a version of Birkhoff's theorem for conservative, ergodic,\ninfinite-measure-preserving dynamical systems where instead of integrable\nfunctions we use certain elements of $L^\\infty$, which we generically call\nglobal observables. Our main theorem applies to general systems but requires an\nhypothesis of \"approximate partial averaging\" on the observables. The idea\nbehind the result, however, applies to more general situations, as we show with\nan example. Finally, by means of counterexamples and numerical simulations, we\ndiscuss the question of finding the optimal class of observables for which a\nBirkhoff theorem holds for infinite-measure-preserving systems. \n\n"}
{"id": "1804.06055", "contents": "Title: Co-occurrence Feature Learning from Skeleton Data for Action Recognition\n  and Detection with Hierarchical Aggregation Abstract: Skeleton-based human action recognition has recently drawn increasing\nattentions with the availability of large-scale skeleton datasets. The most\ncrucial factors for this task lie in two aspects: the intra-frame\nrepresentation for joint co-occurrences and the inter-frame representation for\nskeletons' temporal evolutions. In this paper we propose an end-to-end\nconvolutional co-occurrence feature learning framework. The co-occurrence\nfeatures are learned with a hierarchical methodology, in which different levels\nof contextual information are aggregated gradually. Firstly point-level\ninformation of each joint is encoded independently. Then they are assembled\ninto semantic representation in both spatial and temporal domains.\nSpecifically, we introduce a global spatial aggregation scheme, which is able\nto learn superior joint co-occurrence features over local aggregation. Besides,\nraw skeleton coordinates as well as their temporal difference are integrated\nwith a two-stream paradigm. Experiments show that our approach consistently\noutperforms other state-of-the-arts on action recognition and detection\nbenchmarks like NTU RGB+D, SBU Kinect Interaction and PKU-MMD. \n\n"}
{"id": "1804.06642", "contents": "Title: Superframes, A Temporal Video Segmentation Abstract: The goal of video segmentation is to turn video data into a set of concrete\nmotion clusters that can be easily interpreted as building blocks of the video.\nThere are some works on similar topics like detecting scene cuts in a video,\nbut there is few specific research on clustering video data into the desired\nnumber of compact segments. It would be more intuitive, and more efficient, to\nwork with perceptually meaningful entity obtained from a low-level grouping\nprocess which we call it superframe. This paper presents a new simple and\nefficient technique to detect superframes of similar content patterns in\nvideos. We calculate the similarity of content-motion to obtain the strength of\nchange between consecutive frames. With the help of existing optical flow\ntechnique using deep models, the proposed method is able to perform more\naccurate motion estimation efficiently. We also propose two criteria for\nmeasuring and comparing the performance of different algorithms on various\ndatabases. Experimental results on the videos from benchmark databases have\ndemonstrated the effectiveness of the proposed method. \n\n"}
{"id": "1804.06786", "contents": "Title: Quantifying the visual concreteness of words and topics in multimodal\n  datasets Abstract: Multimodal machine learning algorithms aim to learn visual-textual\ncorrespondences. Previous work suggests that concepts with concrete visual\nmanifestations may be easier to learn than concepts with abstract ones. We give\nan algorithm for automatically computing the visual concreteness of words and\ntopics within multimodal datasets. We apply the approach in four settings,\nranging from image captions to images/text scraped from historical books. In\naddition to enabling explorations of concepts in multimodal datasets, our\nconcreteness scores predict the capacity of machine learning algorithms to\nlearn textual/visual relationships. We find that 1) concrete concepts are\nindeed easier to learn; 2) the large number of algorithms we consider have\nsimilar failure cases; 3) the precise positive relationship between\nconcreteness and performance varies between datasets. We conclude with\nrecommendations for using concreteness scores to facilitate future multimodal\nresearch. \n\n"}
{"id": "1804.06870", "contents": "Title: Object Ordering with Bidirectional Matchings for Visual Reasoning Abstract: Visual reasoning with compositional natural language instructions, e.g.,\nbased on the newly-released Cornell Natural Language Visual Reasoning (NLVR)\ndataset, is a challenging task, where the model needs to have the ability to\ncreate an accurate mapping between the diverse phrases and the several objects\nplaced in complex arrangements in the image. Further, this mapping needs to be\nprocessed to answer the question in the statement given the ordering and\nrelationship of the objects across three similar images. In this paper, we\npropose a novel end-to-end neural model for the NLVR task, where we first use\njoint bidirectional attention to build a two-way conditioning between the\nvisual information and the language phrases. Next, we use an RL-based pointer\nnetwork to sort and process the varying number of unordered objects (so as to\nmatch the order of the statement phrases) in each of the three images and then\npool over the three decisions. Our model achieves strong improvements (of 4-6%\nabsolute) over the state-of-the-art on both the structured representation and\nraw image versions of the dataset. \n\n"}
{"id": "1804.06904", "contents": "Title: First Integrals from Conformal Symmetries: Darboux-Koenigs Metrics and\n  Beyond Abstract: On spaces of constant curvature, the geodesic equations automatically have\nhigher order integrals, which are just built out of first order integrals,\ncorresponding to the abundance of Killing vectors. This is no longer true for\ngeneral conformally flat spaces, but in this case there is a large algebra of\nconformal symmetries. In this paper we use these conformal symmetries to build\nhigher order integrals for the geodesic equations. We use this approach to give\na new derivation of the Darboux-Koenigs metrics, which have only one Killing\nvector, but two quadratic integrals. We also consider the case of possessing\none Killing vector and two cubic integrals.\n  The approach allows the quantum analogue to be constructed in a simpler\nmanner. \n\n"}
{"id": "1804.07492", "contents": "Title: Graph-based Hypothesis Generation for Parallax-tolerant Image Stitching Abstract: The seam-driven approach has been proven fairly effective for\nparallax-tolerant image stitching, whose strategy is to search for an invisible\nseam from finite representative hypotheses of local alignment. In this paper,\nwe propose a graph-based hypothesis generation and a seam-guided local\nalignment for improving the effectiveness and the efficiency of the seam-driven\napproach. The experiment demonstrates the significant reduction of number of\nhypotheses and the improved quality of naturalness of final stitching results,\ncomparing to the state-of-the-art method SEAGULL. \n\n"}
{"id": "1804.08071", "contents": "Title: Decoupled Networks Abstract: Inner product-based convolution has been a central component of convolutional\nneural networks (CNNs) and the key to learning visual representations. Inspired\nby the observation that CNN-learned features are naturally decoupled with the\nnorm of features corresponding to the intra-class variation and the angle\ncorresponding to the semantic difference, we propose a generic decoupled\nlearning framework which models the intra-class variation and semantic\ndifference independently. Specifically, we first reparametrize the inner\nproduct to a decoupled form and then generalize it to the decoupled convolution\noperator which serves as the building block of our decoupled networks. We\npresent several effective instances of the decoupled convolution operator. Each\ndecoupled operator is well motivated and has an intuitive geometric\ninterpretation. Based on these decoupled operators, we further propose to\ndirectly learn the operator from data. Extensive experiments show that such\ndecoupled reparameterization renders significant performance gain with easier\nconvergence and stronger robustness. \n\n"}
{"id": "1804.08651", "contents": "Title: Rendition: Reclaiming what a black box takes away Abstract: The premise of our work is deceptively familiar: A black box $f(\\cdot)$ has\naltered an image $\\mathbf{x} \\rightarrow f(\\mathbf{x})$. Recover the image\n$\\mathbf{x}$. This black box might be any number of simple or complicated\nthings: a linear or non-linear filter, some app on your phone, etc. The latter\nis a good canonical example for the problem we address: Given only \"the app\"\nand an image produced by the app, find the image that was fed to the app. You\ncan run the given image (or any other image) through the app as many times as\nyou like, but you can not look inside the (code for the) app to see how it\nworks. At first blush, the problem sounds a lot like a standard inverse\nproblem, but it is not in the following sense: While we have access to the\nblack box $f(\\cdot)$ and can run any image through it and observe the output,\nwe do not know how the block box alters the image. Therefore we have no\nexplicit form or model of $f(\\cdot)$. Nor are we necessarily interested in the\ninternal workings of the black box. We are simply happy to reverse its effect\non a particular image, to whatever extent possible. This is what we call the\n\"rendition\" (rather than restoration) problem, as it does not fit the mold of\nan inverse problem (blind or otherwise). We describe general conditions under\nwhich rendition is possible, and provide a remarkably simple algorithm that\nworks for both contractive and expansive black box operators. The principal and\nnovel take-away message from our work is this surprising fact: One simple\nalgorithm can reliably undo a wide class of (not too violent) image\ndistortions.\n  A higher quality pdf of this paper is available at http://www.milanfar.org \n\n"}
{"id": "1804.08758", "contents": "Title: Switchable Temporal Propagation Network Abstract: Videos contain highly redundant information between frames. Such redundancy\nhas been extensively studied in video compression and encoding, but is less\nexplored for more advanced video processing. In this paper, we propose a\nlearnable unified framework for propagating a variety of visual properties of\nvideo images, including but not limited to color, high dynamic range (HDR), and\nsegmentation information, where the properties are available for only a few\nkey-frames. Our approach is based on a temporal propagation network (TPN),\nwhich models the transition-related affinity between a pair of frames in a\npurely data-driven manner. We theoretically prove two essential factors for\nTPN: (a) by regularizing the global transformation matrix as orthogonal, the\n\"style energy\" of the property can be well preserved during propagation; (b)\nsuch regularization can be achieved by the proposed switchable TPN with\nbi-directional training on pairs of frames. We apply the switchable TPN to\nthree tasks: colorizing a gray-scale video based on a few color key-frames,\ngenerating an HDR video from a low dynamic range (LDR) video and a few HDR\nframes, and propagating a segmentation mask from the first frame in videos.\nExperimental results show that our approach is significantly more accurate and\nefficient than the state-of-the-art methods. \n\n"}
{"id": "1804.10123", "contents": "Title: IamNN: Iterative and Adaptive Mobile Neural Network for Efficient Image\n  Classification Abstract: Deep residual networks (ResNets) made a recent breakthrough in deep learning.\nThe core idea of ResNets is to have shortcut connections between layers that\nallow the network to be much deeper while still being easy to optimize avoiding\nvanishing gradients. These shortcut connections have interesting side-effects\nthat make ResNets behave differently from other typical network architectures.\nIn this work we use these properties to design a network based on a ResNet but\nwith parameter sharing and with adaptive computation time. The resulting\nnetwork is much smaller than the original network and can adapt the\ncomputational cost to the complexity of the input image. \n\n"}
{"id": "1804.10224", "contents": "Title: Integrability of a discrete Yajima-Oikawa system Abstract: A space discretization of an integrable long wave-short wave interaction\nmodel, called the Yajima-Oikawa system, was proposed in the recent paper\narXiv:1509.06996 using the Hirota bilinear method (see also\nhttps://link.aps.org/doi/10.1103/PhysRevE.91.062902). In this paper, we propose\na Lax-pair representation for the discrete Yajima-Oikawa system as well as its\nmulticomponent generalization also considered in arXiv:1509.06996 and prove\nthat it has an infinite number of conservation laws. We also derive the next\nhigher flow of the discrete Yajima-Oikawa hierarchy, which generalizes a\nmodified version of the Volterra lattice. Relations to two integrable discrete\nnonlinear Schr\\\"odinger hierarchies, the Ablowitz-Ladik hierarchy and the\nKonopelchenko-Chudnovsky hierarchy, are clarified. \n\n"}
{"id": "1804.10337", "contents": "Title: Latent Fingerprint Recognition: Role of Texture Template Abstract: We propose a texture template approach, consisting of a set of virtual\nminutiae, to improve the overall latent fingerprint recognition accuracy. To\ncompensate for the lack of sufficient number of minutiae in poor quality latent\nprints, we generate a set of virtual minutiae. However, due to a large number\nof these regularly placed virtual minutiae, texture based template matching has\na large computational requirement compared to matching true minutiae templates.\nTo improve both the accuracy and efficiency of the texture template matching,\nwe investigate: i) both original and enhanced fingerprint patches for training\nconvolutional neural networks (ConvNets) to improve the distinctiveness of\ndescriptors associated with each virtual minutiae, ii) smaller patches around\nvirtual minutiae and a fast ConvNet architecture to speed up descriptor\nextraction, iii) reduce the descriptor length, iv) a modified hierarchical\ngraph matching strategy to improve the matching speed, and v) extraction of\nmultiple texture templates to boost the performance. Experiments on NIST SD27\nlatent database show that the above strategies can improve the matching speed\nfrom 11 ms (24 threads) per comparison (between a latent and a reference print)\nto only 7.7 ms (single thread) per comparison while improving the rank-1\naccuracy by 8.9% against 10K gallery. \n\n"}
{"id": "1804.10481", "contents": "Title: Interactive Medical Image Segmentation via Point-Based Interaction and\n  Sequential Patch Learning Abstract: Due to low tissue contrast, irregular object appearance, and unpredictable\nlocation variation, segmenting the objects from different medical imaging\nmodalities (e.g., CT, MR) is considered as an important yet challenging task.\nIn this paper, we present a novel method for interactive medical image\nsegmentation with the following merits. (1) Our design is fundamentally\ndifferent from previous pure patch-based and image-based segmentation methods.\nWe observe that during delineation, the physician repeatedly check the\ninside-outside intensity changing to determine the boundary, which indicates\nthat comparison in an inside-outside manner is extremely important. Thus, we\ninnovatively model our segmentation task as learning the representation of the\nbi-directional sequential patches, starting from (or ending in) the given\ncentral point of the object. This can be realized by our proposed ConvRNN\nnetwork embedded with a gated memory propagation unit. (2) Unlike previous\ninteractive methods (requiring bounding box or seed points), we only ask the\nphysician to merely click on the rough central point of the object before\nsegmentation, which could simultaneously enhance the performance and reduce the\nsegmentation time. (3) We utilize our method in a multi-level framework for\nbetter performance. We systematically evaluate our method in three different\nsegmentation tasks including CT kidney tumor, MR prostate, and PROMISE12\nchallenge, showing promising results compared with state-of-the-art methods.\nThe code is available here:\n\\href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}. \n\n"}
{"id": "1804.10774", "contents": "Title: Complex dynamics, hidden attractors and continuous approximation of a\n  fractional-order hyperchaotic PWC system Abstract: In this paper, a continuous approximation to studying a class of PWC systems\nof fractionalorder is presented. Some known results of set-valued analysis and\ndifferential inclusions are utilized. The example of a hyperchaotic PWC system\nof fractional order is analyzed. It is found that without equilibria, the\nsystem has hidden attractors. \n\n"}
{"id": "1804.10916", "contents": "Title: Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical\n  Image Segmentations with Adversarial Loss Abstract: Convolutional networks (ConvNets) have achieved great successes in various\nchallenging vision tasks. However, the performance of ConvNets would degrade\nwhen encountering the domain shift. The domain adaptation is more significant\nwhile challenging in the field of biomedical image analysis, where\ncross-modality data have largely different distributions. Given that annotating\nthe medical data is especially expensive, the supervised transfer learning\napproaches are not quite optimal. In this paper, we propose an unsupervised\ndomain adaptation framework with adversarial learning for cross-modality\nbiomedical image segmentations. Specifically, our model is based on a dilated\nfully convolutional network for pixel-wise prediction. Moreover, we build a\nplug-and-play domain adaptation module (DAM) to map the target input to\nfeatures which are aligned with source domain feature space. A domain critic\nmodule (DCM) is set up for discriminating the feature space of both domains. We\noptimize the DAM and DCM via an adversarial loss without using any target\ndomain label. Our proposed method is validated by adapting a ConvNet trained\nwith MRI images to unpaired CT data for cardiac structures segmentations, and\nachieved very promising results. \n\n"}
{"id": "1804.11013", "contents": "Title: Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval Abstract: In this paper, we propose a novel deep generative approach to cross-modal\nretrieval to learn hash functions in the absence of paired training samples\nthrough the cycle consistency loss. Our proposed approach employs adversarial\ntraining scheme to lean a couple of hash functions enabling translation between\nmodalities while assuming the underlying semantic relationship. To induce the\nhash codes with semantics to the input-output pair, cycle consistency loss is\nfurther proposed upon the adversarial training to strengthen the correlations\nbetween inputs and corresponding outputs. Our approach is generative to learn\nhash functions such that the learned hash codes can maximally correlate each\ninput-output correspondence, meanwhile can also regenerate the inputs so as to\nminimize the information loss. The learning to hash embedding is thus performed\nto jointly optimize the parameters of the hash functions across modalities as\nwell as the associated generative models. Extensive experiments on a variety of\nlarge-scale cross-modal data sets demonstrate that our proposed method achieves\nbetter retrieval results than the state-of-the-arts. \n\n"}
{"id": "1805.00328", "contents": "Title: 3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object\n  Deformations Abstract: The ability to interact and understand the environment is a fundamental\nprerequisite for a wide range of applications from robotics to augmented\nreality. In particular, predicting how deformable objects will react to applied\nforces in real time is a significant challenge. This is further confounded by\nthe fact that shape information about encountered objects in the real world is\noften impaired by occlusions, noise and missing regions e.g. a robot\nmanipulating an object will only be able to observe a partial view of the\nentire solid. In this work we present a framework, 3D-PhysNet, which is able to\npredict how a three-dimensional solid will deform under an applied force using\nintuitive physics modelling. In particular, we propose a new method to encode\nthe physical properties of the material and the applied force, enabling\ngeneralisation over materials. The key is to combine deep variational\nautoencoders with adversarial training, conditioned on the applied force and\nthe material properties. We further propose a cascaded architecture that takes\na single 2.5D depth view of the object and predicts its deformation. Training\ndata is provided by a physics simulator. The network is fast enough to be used\nin real-time applications from partial views. Experimental results show the\nviability and the generalisation properties of the proposed architecture. \n\n"}
{"id": "1805.00334", "contents": "Title: Deep learning approach to Fourier ptychographic microscopy Abstract: Convolutional neural networks (CNNs) have gained tremendous success in\nsolving complex inverse problems. The aim of this work is to develop a novel\nCNN framework to reconstruct video sequence of dynamic live cells captured\nusing a computational microscopy technique, Fourier ptychographic microscopy\n(FPM). The unique feature of the FPM is its capability to reconstruct images\nwith both wide field-of-view (FOV) and high resolution, i.e. a large\nspace-bandwidth-product (SBP), by taking a series of low resolution intensity\nimages. For live cell imaging, a single FPM frame contains thousands of cell\nsamples with different morphological features. Our idea is to fully exploit the\nstatistical information provided by this large spatial ensemble so as to make\npredictions in a sequential measurement, without using any additional temporal\ndataset. Specifically, we show that it is possible to reconstruct high-SBP\ndynamic cell videos by a CNN trained only on the first FPM dataset captured at\nthe beginning of a time-series experiment. Our CNN approach reconstructs a\n12800X10800 pixels phase image using only ~25 seconds, a 50X speedup compared\nto the model-based FPM algorithm. In addition, the CNN further reduces the\nrequired number of images in each time frame by ~6X. Overall, this\nsignificantly improves the imaging throughput by reducing both the acquisition\nand computational times. The proposed CNN is based on the conditional\ngenerative adversarial network (cGAN) framework. Additionally, we also exploit\ntransfer learning so that our pre-trained CNN can be further optimized to image\nother cell types. Our technique demonstrates a promising deep learning approach\nto continuously monitor large live-cell populations over an extended time and\ngather useful spatial and temporal information with sub-cellular resolution. \n\n"}
{"id": "1805.00655", "contents": "Title: Convolutional Sequence to Sequence Model for Human Dynamics Abstract: Human motion modeling is a classic problem in computer vision and graphics.\nChallenges in modeling human motion include high dimensional prediction as well\nas extremely complicated dynamics.We present a novel approach to human motion\nmodeling based on convolutional neural networks (CNN). The hierarchical\nstructure of CNN makes it capable of capturing both spatial and temporal\ncorrelations effectively. In our proposed approach,a convolutional long-term\nencoder is used to encode the whole given motion sequence into a long-term\nhidden variable, which is used with a decoder to predict the remainder of the\nsequence. The decoder itself also has an encoder-decoder structure, in which\nthe short-term encoder encodes a shorter sequence to a short-term hidden\nvariable, and the spatial decoder maps the long and short-term hidden variable\nto motion predictions. By using such a model, we are able to capture both\ninvariant and dynamic information of human motion, which results in more\naccurate predictions. Experiments show that our algorithm outperforms the\nstate-of-the-art methods on the Human3.6M and CMU Motion Capture datasets. Our\ncode is available at the project website. \n\n"}
{"id": "1805.01195", "contents": "Title: BirdNet: a 3D Object Detection Framework from LiDAR information Abstract: Understanding driving situations regardless the conditions of the traffic\nscene is a cornerstone on the path towards autonomous vehicles; however,\ndespite common sensor setups already include complementary devices such as\nLiDAR or radar, most of the research on perception systems has traditionally\nfocused on computer vision. We present a LiDAR-based 3D object detection\npipeline entailing three stages. First, laser information is projected into a\nnovel cell encoding for bird's eye view projection. Later, both object location\non the plane and its heading are estimated through a convolutional neural\nnetwork originally designed for image processing. Finally, 3D oriented\ndetections are computed in a post-processing phase. Experiments on KITTI\ndataset show that the proposed framework achieves state-of-the-art results\namong comparable methods. Further tests with different LiDAR sensors in real\nscenarios assess the multi-device capabilities of the approach. \n\n"}
{"id": "1805.01220", "contents": "Title: Semantic segmentation of mFISH images using convolutional networks Abstract: Multicolor in situ hybridization (mFISH) is a karyotyping technique used to\ndetect major chromosomal alterations using fluorescent probes and imaging\ntechniques. Manual interpretation of mFISH images is a time consuming step that\ncan be automated using machine learning; in previous works, pixel or patch wise\nclassification was employed, overlooking spatial information which can help\nidentify chromosomes. In this work, we propose a fully convolutional semantic\nsegmentation network for the interpretation of mFISH images, which uses both\nspatial and spectral information to classify each pixel in an end-to-end\nfashion. The semantic segmentation network developed was tested on samples\nextracted from a public dataset using cross validation. Despite having no\nlabeling information of the image it was tested on our algorithm yielded an\naverage correct classification ratio (CCR) of 87.41%. Previously, this level of\naccuracy was only achieved with state of the art algorithms when classifying\npixels from the same image in which the classifier has been trained. These\nresults provide evidence that fully convolutional semantic segmentation\nnetworks may be employed in the computer aided diagnosis of genetic diseases\nwith improved performance over the current methods of image analysis. \n\n"}
{"id": "1805.01536", "contents": "Title: Diffusion on middle-$\\xi$ Cantor sets Abstract: In this paper, we study $C^{\\zeta}$-calculus on generalized Cantor sets,\nwhich have self-similar properties and fractional dimensions that exceed their\ntopological dimensions. Functions with fractal support are not differentiable\nor integrable in terms of standard calculus, so we must involve local\nfractional derivatives. We have generalized the $C^{\\zeta}$-calculus on the\ngeneralized Cantor sets known as middle-$\\xi$ Cantor sets. We have suggested a\ncalculus on the middle-$\\xi$ Cantor sets for different values of $\\xi$ with\n$0<\\xi<1$. Differential equations on the middle-$\\xi$ Cantor sets have been\nsolved, and we have presented the results using illustrative examples. The\nconditions for super-, normal, and sub-diffusion on fractal sets are given. \n\n"}
{"id": "1805.02336", "contents": "Title: Sharp Attention Network via Adaptive Sampling for Person\n  Re-identification Abstract: In this paper, we present novel sharp attention networks by adaptively\nsampling feature maps from convolutional neural networks (CNNs) for person\nre-identification (re-ID) problem. Due to the introduction of sampling-based\nattention models, the proposed approach can adaptively generate sharper\nattention-aware feature masks. This greatly differs from the gating-based\nattention mechanism that relies soft gating functions to select the relevant\nfeatures for person re-ID. In contrast, the proposed sampling-based attention\nmechanism allows us to effectively trim irrelevant features by enforcing the\nresultant feature masks to focus on the most discriminative features. It can\nproduce sharper attentions that are more assertive in localizing subtle\nfeatures relevant to re-identifying people across cameras. For this purpose, a\ndifferentiable Gumbel-Softmax sampler is employed to approximate the Bernoulli\nsampling to train the sharp attention networks. Extensive experimental\nevaluations demonstrate the superiority of this new sharp attention model for\nperson re-ID over the other state-of-the-art methods on three challenging\nbenchmarks including CUHK03, Market-1501, and DukeMTMC-reID. \n\n"}
{"id": "1805.02397", "contents": "Title: A Review on Facial Micro-Expressions Analysis: Datasets, Features and\n  Metrics Abstract: Facial micro-expressions are very brief, spontaneous facial expressions that\nappear on the face of humans when they either deliberately or unconsciously\nconceal an emotion. Micro-expression has shorter duration than\nmacro-expression, which makes it more challenging for human and machine. Over\nthe past ten years, automatic micro-expressions recognition has attracted\nincreasing attention from researchers in psychology, computer science,\nsecurity, neuroscience and other related disciplines. The aim of this paper is\nto provide the insights of automatic micro-expressions and recommendations for\nfuture research. There has been a lot of datasets released over the last decade\nthat facilitated the rapid growth in this field. However, comparison across\ndifferent datasets is difficult due to the inconsistency in experiment\nprotocol, features used and evaluation methods. To address these issues, we\nreview the datasets, features and the performance metrics deployed in the\nliterature. Relevant challenges such as the spatial temporal settings during\ndata collection, emotional classes versus objective classes in data labelling,\nface regions in data analysis, standardisation of metrics and the requirements\nfor real-world implementation are discussed. We conclude by proposing some\npromising future directions to advancing micro-expressions research. \n\n"}
{"id": "1805.04969", "contents": "Title: Learning Temporal Strategic Relationships using Generative Adversarial\n  Imitation Learning Abstract: This paper presents a novel framework for automatic learning of complex\nstrategies in human decision making. The task that we are interested in is to\nbetter facilitate long term planning for complex, multi-step events. We observe\ntemporal relationships at the subtask level of expert demonstrations, and\ndetermine the different strategies employed in order to successfully complete a\ntask. To capture the relationship between the subtasks and the overall goal, we\nutilise two external memory modules, one for capturing dependencies within a\nsingle expert demonstration, such as the sequential relationship among\ndifferent sub tasks, and a global memory module for modelling task level\ncharacteristics such as best practice employed by different humans based on\ntheir domain expertise. Furthermore, we demonstrate how the hidden state\nrepresentation of the memory can be used as a reward signal to smooth the state\ntransitions, eradicating subtle changes. We evaluate the effectiveness of the\nproposed model for an autonomous highway driving application, where we\ndemonstrate its capability to learn different expert policies and outperform\nstate-of-the-art methods. The scope in industrial applications extends to any\nrobotics and automation application which requires learning from complex\ndemonstrations containing series of subtasks. \n\n"}
{"id": "1805.06223", "contents": "Title: Adversarial Training for Patient-Independent Feature Learning with IVOCT\n  Data for Plaque Classification Abstract: Deep learning methods have shown impressive results for a variety of medical\nproblems over the last few years. However, datasets tend to be small due to\ntime-consuming annotation. As datasets with different patients are often very\nheterogeneous generalization to new patients can be difficult. This is\ncomplicated further if large differences in image acquisition can occur, which\nis common during intravascular optical coherence tomography for coronary plaque\nimaging. We address this problem with an adversarial training strategy where we\nforce a part of a deep neural network to learn features that are independent of\npatient- or acquisitionspecific characteristics. We compare our regularization\nmethod to typical data augmentation strategies and show that our approach\nimproves performance for a small medical dataset. \n\n"}
{"id": "1805.06306", "contents": "Title: Fully Associative Patch-based 1-to-N Matcher for Face Recognition Abstract: This paper focuses on improving face recognition performance by a patch-based\n1-to-N signature matcher that learns correlations between different facial\npatches. A Fully Associative Patch-based Signature Matcher (FAPSM) is proposed\nso that the local matching identity of each patch contributes to the global\nmatching identities of all the patches. The proposed matcher consists of three\nsteps. First, based on the signature, the local matching identity and the\ncorresponding matching score of each patch are computed. Then, a fully\nassociative weight matrix is learned to obtain the global matching identities\nand scores of all the patches. At last, the l1-regularized weighting is applied\nto combine the global matching identity of each patch and obtain a final\nmatching identity. The proposed matcher has been integrated with the UR2D\nsystem for evaluation. The experimental results indicate that the proposed\nmatcher achieves better performance than the current UR2D system. The Rank-1\naccuracy is improved significantly by 3% and 0.55% on the UHDB31 dataset and\nthe IJB-A dataset, respectively. \n\n"}
{"id": "1805.06374", "contents": "Title: Fast Retinomorphic Event Stream for Video Recognition and Reinforcement\n  Learning Abstract: Good temporal representations are crucial for video understanding, and the\nstate-of-the-art video recognition framework is based on two-stream networks.\nIn such framework, besides the regular ConvNets responsible for RGB frame\ninputs, a second network is introduced to handle the temporal representation,\nusually the optical flow (OF). However, OF or other task-oriented flow is\ncomputationally costly, and is thus typically pre-computed. Critically, this\nprevents the two-stream approach from being applied to reinforcement learning\n(RL) applications such as video game playing, where the next state depends on\ncurrent state and action choices. Inspired by the early vision systems of\nmammals and insects, we propose a fast event-driven representation (EDR) that\nmodels several major properties of early retinal circuits: (1) logarithmic\ninput response, (2) multi-timescale temporal smoothing to filter noise, and (3)\nbipolar (ON/OFF) pathways for primitive event detection[12]. Trading off the\ndirectional information for fast speed (> 9000 fps), EDR en-ables fast\nreal-time inference/learning in video applications that require interaction\nbetween an agent and the world such as game-playing, virtual robotics, and\ndomain adaptation. In this vein, we use EDR to demonstrate performance\nimprovements over state-of-the-art reinforcement learning algorithms for Atari\ngames, something that has not been possible with pre-computed OF. Moreover,\nwith UCF-101 video action recognition experiments, we show that EDR performs\nnear state-of-the-art in accuracy while achieving a 1,500x speedup in input\nrepresentation processing, as compared to optical flow. \n\n"}
{"id": "1805.06447", "contents": "Title: Resisting Large Data Variations via Introspective Transformation Network Abstract: Training deep networks that generalize to a wide range of variations in test\ndata is essential to building accurate and robust image classifiers. One\nstandard strategy is to apply data augmentation to synthetically enlarge the\ntraining set. However, data augmentation is essentially a brute-force method\nwhich generates uniform samples from some pre-defined set of transformations.\nIn this paper, we propose a principled approach to train networks with\nsignificantly improved resistance to large variations between training and\ntesting data. This is achieved by embedding a learnable transformation module\ninto the introspective network, which is a convolutional neural network (CNN)\nclassifier empowered with generative capabilities. Our approach alternates\nbetween synthesizing pseudo-negative samples and transformed positive examples\nbased on the current model, and optimizing model predictions on these\nsynthesized samples. Experimental results verify that our approach\nsignificantly improves the ability of deep networks to resist large variations\nbetween training and testing data and achieves classification accuracy\nimprovements on several benchmark datasets, including MNIST, affNIST, SVHN,\nCIFAR-10 and miniImageNet. \n\n"}
{"id": "1805.06660", "contents": "Title: Single Shot Active Learning using Pseudo Annotators Abstract: Standard myopic active learning assumes that human annotations are always\nobtainable whenever new samples are selected. This, however, is unrealistic in\nmany real-world applications where human experts are not readily available at\nall times. In this paper, we consider the single shot setting: all the required\nsamples should be chosen in a single shot and no human annotation can be\nexploited during the selection process. We propose a new method, Active\nLearning through Random Labeling (ALRL), which substitutes single human\nannotator for multiple, what we will refer to as, pseudo annotators. These\npseudo annotators always provide uniform and random labels whenever new\nunlabeled samples are queried. This random labeling enables standard active\nlearning algorithms to also exhibit the exploratory behavior needed for single\nshot active learning. The exploratory behavior is further enhanced by selecting\nthe most representative sample via minimizing nearest neighbor distance between\nunlabeled samples and queried samples. Experiments on real-world datasets\ndemonstrate that the proposed method outperforms several state-of-the-art\napproaches. \n\n"}
{"id": "1805.06958", "contents": "Title: Generalizing multistain immunohistochemistry tissue segmentation using\n  one-shot color deconvolution deep neural networks Abstract: A key challenge in cancer immunotherapy biomarker research is quantification\nof pattern changes in microscopic whole slide images of tumor biopsies.\nDifferent cell types tend to migrate into various tissue compartments and form\nvariable distribution patterns. Drug development requires correlative analysis\nof various biomarkers in and between the tissue compartments. To enable that,\ntissue slides are manually annotated by expert pathologists. Manual annotation\nof tissue slides is a labor intensive, tedious and error-prone task. Automation\nof this annotation process can improve accuracy and consistency while reducing\nworkload and cost in a way that will positively influence drug development\nefforts. In this paper we present a novel one-shot color deconvolution deep\nlearning method to automatically segment and annotate digitized slide images\nwith multiple stainings into compartments of tumor, healthy tissue, and\nnecrosis. We address the task in the context of drug development where multiple\nstains, tissue and tumor types exist and look into solutions for\ngeneralizations over these image populations. \n\n"}
{"id": "1805.07588", "contents": "Title: Robust Optimization over Multiple Domains Abstract: In this work, we study the problem of learning a single model for multiple\ndomains. Unlike the conventional machine learning scenario where each domain\ncan have the corresponding model, multiple domains (i.e., applications/users)\nmay share the same machine learning model due to maintenance loads in cloud\ncomputing services. For example, a digit-recognition model should be applicable\nto hand-written digits, house numbers, car plates, etc. Therefore, an ideal\nmodel for cloud computing has to perform well at each applicable domain. To\naddress this new challenge from cloud computing, we develop a framework of\nrobust optimization over multiple domains. In lieu of minimizing the empirical\nrisk, we aim to learn a model optimized to the adversarial distribution over\nmultiple domains. Hence, we propose to learn the model and the adversarial\ndistribution simultaneously with the stochastic algorithm for efficiency.\nTheoretically, we analyze the convergence rate for convex and non-convex\nmodels. To our best knowledge, we first study the convergence rate of learning\na robust non-convex model with a practical algorithm. Furthermore, we\ndemonstrate that the robustness of the framework and the convergence rate can\nbe further enhanced by appropriate regularizers over the adversarial\ndistribution. The empirical study on real-world fine-grained visual\ncategorization and digits recognition tasks verifies the effectiveness and\nefficiency of the proposed framework. \n\n"}
{"id": "1805.07621", "contents": "Title: CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule\n  Subspaces Abstract: In this paper, we formalize the idea behind capsule nets of using a capsule\nvector rather than a neuron activation to predict the label of samples. To this\nend, we propose to learn a group of capsule subspaces onto which an input\nfeature vector is projected. Then the lengths of resultant capsules are used to\nscore the probability of belonging to different classes. We train such a\nCapsule Projection Network (CapProNet) by learning an orthogonal projection\nmatrix for each capsule subspace, and show that each capsule subspace is\nupdated until it contains input feature vectors corresponding to the associated\nclass. We will also show that the capsule projection can be viewed as\nnormalizing the multiple columns of the weight matrix simultaneously to form an\northogonal basis, which makes it more effective in incorporating novel\ncomponents of input features to update capsule representations. In other words,\nthe capsule projection can be viewed as a multi-dimensional weight\nnormalization in capsule subspaces, where the conventional weight normalization\nis simply a special case of the capsule projection onto 1D lines. Only a small\nnegligible computing overhead is incurred to train the network in\nlow-dimensional capsule subspaces or through an alternative hyper-power\niteration to estimate the normalization matrix. Experiment results on image\ndatasets show the presented model can greatly improve the performance of the\nstate-of-the-art ResNet backbones by $10-20\\%$ and that of the Densenet by\n$5-7\\%$ respectively at the same level of computing and memory expenses. The\nCapProNet establishes the competitive state-of-the-art performance for the\nfamily of capsule nets by significantly reducing test errors on the benchmark\ndatasets. \n\n"}
{"id": "1805.07854", "contents": "Title: Integrable semi-discretization of the massive Thirring system in\n  laboratory coordinates Abstract: Several integrable semi-discretizations are known in the literature for the\nmassive Thirring system in characteristic coordinates. We present for the first\ntime an integrable semi-discretization of the massive Thirring system in\nlaboratory coordinates. Our approach relies on the relation between the\ncontinuous massive Thirring system and the Ablowitz-Ladik lattice. The Backlund\ntransformation for solutions to the Ablowitz-Ladik lattice and the time\nevolution of the massive Thirring system in laboratory coordinates are combined\ntogether in the derivation of the Lax system for the integrable\nsemi-discretization of the massive Thirring system. \n\n"}
{"id": "1805.07857", "contents": "Title: Parallel Transport Convolution: A New Tool for Convolutional Neural\n  Networks on Manifolds Abstract: Convolution has been playing a prominent role in various applications in\nscience and engineering for many years. It is the most important operation in\nconvolutional neural networks. There has been a recent growth of interests of\nresearch in generalizing convolutions on curved domains such as manifolds and\ngraphs. However, existing approaches cannot preserve all the desirable\nproperties of Euclidean convolutions, namely compactly supported filters,\ndirectionality, transferability across different manifolds. In this paper we\ndevelop a new generalization of the convolution operation, referred to as\nparallel transport convolution (PTC), on Riemannian manifolds and their\ndiscrete counterparts. PTC is designed based on the parallel transportation\nwhich is able to translate information along a manifold and to intrinsically\npreserve directionality. PTC allows for the construction of compactly supported\nfilters and is also robust to manifold deformations. This enables us to preform\nwavelet-like operations and to define deep convolutional neural networks on\ncurved domains. \n\n"}
{"id": "1805.08695", "contents": "Title: SqueezeJet: High-level Synthesis Accelerator Design for Deep\n  Convolutional Neural Networks Abstract: Deep convolutional neural networks have dominated the pattern recognition\nscene by providing much more accurate solutions in computer vision problems\nsuch as object recognition and object detection. Most of these solutions come\nat a huge computational cost, requiring billions of multiply-accumulate\noperations and, thus, making their use quite challenging in real-time\napplications that run on embedded mobile (resource-power constrained) hardware.\nThis work presents the architecture, the high-level synthesis design, and the\nimplementation of SqueezeJet, an FPGA accelerator for the inference phase of\nthe SqueezeNet DCNN architecture, which is designed specifically for use in\nembedded systems. Results show that SqueezeJet can achieve 15.16 times speed-up\ncompared to the software implementation of SqueezeNet running on an embedded\nmobile processor with less than 1% drop in top-5 accuracy. \n\n"}
{"id": "1805.08960", "contents": "Title: ICADx: Interpretable computer aided diagnosis of breast masses Abstract: In this study, a novel computer aided diagnosis (CADx) framework is devised\nto investigate interpretability for classifying breast masses. Recently, a deep\nlearning technology has been successfully applied to medical image analysis\nincluding CADx. Existing deep learning based CADx approaches, however, have a\nlimitation in explaining the diagnostic decision. In real clinical practice,\nclinical decisions could be made with reasonable explanation. So current deep\nlearning approaches in CADx are limited in real world deployment. In this\npaper, we investigate interpretability in CADx with the proposed interpretable\nCADx (ICADx) framework. The proposed framework is devised with a generative\nadversarial network, which consists of interpretable diagnosis network and\nsynthetic lesion generative network to learn the relationship between\nmalignancy and a standardized description (BI-RADS). The lesion generative\nnetwork and the interpretable diagnosis network compete in an adversarial\nlearning so that the two networks are improved. The effectiveness of the\nproposed method was validated on public mammogram database. Experimental\nresults showed that the proposed ICADx framework could provide the\ninterpretability of mass as well as mass classification. It was mainly\nattributed to the fact that the proposed method was effectively trained to find\nthe relationship between malignancy and interpretations via the adversarial\nlearning. These results imply that the proposed ICADx framework could be a\npromising approach to develop the CADx system. \n\n"}
{"id": "1805.09298", "contents": "Title: Learning towards Minimum Hyperspherical Energy Abstract: Neural networks are a powerful class of nonlinear functions that can be\ntrained end-to-end on various applications. While the over-parametrization\nnature in many neural networks renders the ability to fit complex functions and\nthe strong representation power to handle challenging tasks, it also leads to\nhighly correlated neurons that can hurt the generalization ability and incur\nunnecessary computation cost. As a result, how to regularize the network to\navoid undesired representation redundancy becomes an important issue. To this\nend, we draw inspiration from a well-known problem in physics -- Thomson\nproblem, where one seeks to find a state that distributes N electrons on a unit\nsphere as evenly as possible with minimum potential energy. In light of this\nintuition, we reduce the redundancy regularization problem to generic energy\nminimization, and propose a minimum hyperspherical energy (MHE) objective as\ngeneric regularization for neural networks. We also propose a few novel\nvariants of MHE, and provide some insights from a theoretical point of view.\nFinally, we apply neural networks with MHE regularization to several\nchallenging tasks. Extensive experiments demonstrate the effectiveness of our\nintuition, by showing the superior performance with MHE regularization. \n\n"}
{"id": "1805.09711", "contents": "Title: Matrix Boussinesq solitons and their tropical limit Abstract: We study soliton solutions of matrix \"good\" Boussinesq equations, generated\nvia a binary Darboux transformation. Essential features of these solutions are\nrevealed via their \"tropical limit\", as exploited in previous work about the KP\nequation. This limit associates a point particle interaction picture with a\nsoliton (wave) solution. \n\n"}
{"id": "1805.10344", "contents": "Title: Pathology Segmentation using Distributional Differences to Images of\n  Healthy Origin Abstract: Fully supervised segmentation methods require a large training cohort of\nalready segmented images, providing information at the pixel level of each\nimage. We present a method to automatically segment and model pathologies in\nmedical images, trained solely on data labelled on the image level as either\nhealthy or containing a visual defect. We base our method on CycleGAN, an\nimage-to-image translation technique, to translate images between the domains\nof healthy and pathological images. We extend the core idea with two key\ncontributions. Implementing the generators as residual generators allows us to\nexplicitly model the segmentation of the pathology. Realizing the translation\nfrom the healthy to the pathological domain using a variational autoencoder\nallows us to specify one representation of the pathology, as this\ntransformation is otherwise not unique. Our model hence not only allows us to\ncreate pixelwise semantic segmentations, it is also able to create inpaintings\nfor the segmentations to render the pathological image healthy. Furthermore, we\ncan draw new unseen pathology samples from this model based on the distribution\nin the data. We show quantitatively, that our method is able to segment\npathologies with a surprising accuracy being only slightly inferior to a\nstate-of-the-art fully supervised method, although the latter has per-pixel\nrather than per-image training information. Moreover, we show qualitative\nresults of both the segmentations and inpaintings. Our findings motivate\nfurther research into weakly-supervised segmentation using image level\nannotations, allowing for faster and cheaper acquisition of training data\nwithout a large sacrifice in segmentation accuracy. \n\n"}
{"id": "1805.10577", "contents": "Title: Data-driven reduced modelling of turbulent Rayleigh-Benard convection\n  using DMD-enhanced Fluctuation-Dissipation Theorem Abstract: A data-driven, model-free framework is introduced for calculating\nReduced-Order Models (ROMs) capable of accurately predicting time-mean\nresponses to external forcings, or forcings needed for specified responses,\ne.g., for control, in fully turbulent flows. The framework is based on using\nthe Fluctuation-Dissipation Theorem (FDT) in the space of a limited number of\nmodes obtained from Dynamic Mode Decomposition (DMD). Using the DMD modes as\nthe basis functions, rather than the commonly used Proper Orthogonal\nDecomposition (POD) modes, resolves a previously identified problem in applying\nFDT to high-dimensional, non-normal turbulent flows. Employing this\nDMD-enhanced FDT method (FDT$_\\mathrm{DMD}$), a 1D linear ROM with horizontally\naveraged temperature as state vector, is calculated for a 3D Rayleigh-B\\'enard\nconvection system at the Rayleigh number of $10^6$ using data obtained from\nDirect Numerical Simulation (DNS). The calculated ROM performs well in various\ntests for this turbulent flow, suggesting FDT$_\\mathrm{DMD}$ as a promising\nmethod for developing ROMs for high-dimensional, turbulent systems. \n\n"}
{"id": "1805.10726", "contents": "Title: A Neurobiological Evaluation Metric for Neural Network Model Search Abstract: Neuroscience theory posits that the brain's visual system coarsely identifies\nbroad object categories via neural activation patterns, with similar objects\nproducing similar neural responses. Artificial neural networks also have\ninternal activation behavior in response to stimuli. We hypothesize that\nnetworks exhibiting brain-like activation behavior will demonstrate brain-like\ncharacteristics, e.g., stronger generalization capabilities. In this paper we\nintroduce a human-model similarity (HMS) metric, which quantifies the\nsimilarity of human fMRI and network activation behavior. To calculate HMS,\nrepresentational dissimilarity matrices (RDMs) are created as abstractions of\nactivation behavior, measured by the correlations of activations to stimulus\npairs. HMS is then the correlation between the fMRI RDM and the neural network\nRDM across all stimulus pairs. We test the metric on unsupervised predictive\ncoding networks, which specifically model visual perception, and assess the\nmetric for statistical significance over a large range of hyperparameters. Our\nexperiments show that networks with increased human-model similarity are\ncorrelated with better performance on two computer vision tasks: next frame\nprediction and object matching accuracy. Further, HMS identifies networks with\nhigh performance on both tasks. An unexpected secondary finding is that the\nmetric can be employed during training as an early-stopping mechanism. \n\n"}
{"id": "1805.11026", "contents": "Title: Geometric theory of flexible and expandable tubes conveying fluid:\n  equations, solutions and shock waves Abstract: We present a theory for the three-dimensional evolution of tubes with\nexpandable walls conveying fluid. Our theory can accommodate arbitrary\ndeformations of the tube, arbitrary elasticity of the walls, and both\ncompressible and incompressible flows inside the tube. We also present the\ntheory of propagation of shock waves in such tubes and derive the conservation\nlaws and Rankine-Hugoniot conditions in arbitrary spatial configuration of the\ntubes, and compute several examples of particular solutions. The theory is\nderived from a variational treatment of Cosserat rod theory extended to\nincorporate expandable walls and moving flow inside the tube. The results\npresented here are useful for biological flows and industrial applications\ninvolving high speed motion of gas in flexible tubes. \n\n"}
{"id": "1805.11718", "contents": "Title: Random mesh projectors for inverse problems Abstract: We propose a new learning-based approach to solve ill-posed inverse problems\nin imaging. We address the case where ground truth training samples are rare\nand the problem is severely ill-posed - both because of the underlying physics\nand because we can only get few measurements. This setting is common in\ngeophysical imaging and remote sensing. We show that in this case the common\napproach to directly learn the mapping from the measured data to the\nreconstruction becomes unstable. Instead, we propose to first learn an ensemble\nof simpler mappings from the data to projections of the unknown image into\nrandom piecewise-constant subspaces. We then combine the projections to form a\nfinal reconstruction by solving a deconvolution-like problem. We show\nexperimentally that the proposed method is more robust to measurement noise and\ncorruptions not seen during training than a directly learned inverse. \n\n"}
{"id": "1805.11746", "contents": "Title: Semantic Road Layout Understanding by Generative Adversarial Inpainting Abstract: Autonomous driving is becoming a reality, yet vehicles still need to rely on\ncomplex sensor fusion to understand the scene they act in. The ability to\ndiscern static environment and dynamic entities provides a comprehension of the\nroad layout that poses constraints to the reasoning process about moving\nobjects. We pursue this through a GAN-based semantic segmentation inpainting\nmodel to remove all dynamic objects from the scene and focus on understanding\nits static components such as streets, sidewalks and buildings. We evaluate\nthis task on the Cityscapes dataset and on a novel synthetically generated\ndataset obtained with the CARLA simulator and specifically designed to\nquantitatively evaluate semantic segmentation inpaintings. We compare our\nmethods with a variety of baselines working both in the RGB and segmentation\ndomains. \n\n"}
{"id": "1805.12254", "contents": "Title: Multi-level 3D CNN for Learning Multi-scale Spatial Features Abstract: 3D object recognition accuracy can be improved by learning the multi-scale\nspatial features from 3D spatial geometric representations of objects such as\npoint clouds, 3D models, surfaces, and RGB-D data. Current deep learning\napproaches learn such features either using structured data representations\n(voxel grids and octrees) or from unstructured representations (graphs and\npoint clouds). Learning features from such structured representations is\nlimited by the restriction on resolution and tree depth while unstructured\nrepresentations creates a challenge due to non-uniformity among data samples.\nIn this paper, we propose an end-to-end multi-level learning approach on a\nmulti-level voxel grid to overcome these drawbacks. To demonstrate the utility\nof the proposed multi-level learning, we use a multi-level voxel representation\nof 3D objects to perform object recognition. The multi-level voxel\nrepresentation consists of a coarse voxel grid that contains volumetric\ninformation of the 3D object. In addition, each voxel in the coarse grid that\ncontains a portion of the object boundary is subdivided into multiple\nfine-level voxel grids. The performance of our multi-level learning algorithm\nfor object recognition is comparable to dense voxel representations while using\nsignificantly lower memory. \n\n"}
{"id": "1805.12358", "contents": "Title: Light Field Denoising via Anisotropic Parallax Analysis in a CNN\n  Framework Abstract: Light field (LF) cameras provide perspective information of scenes by taking\ndirectional measurements of the focusing light rays. The raw outputs are\nusually dark with additive camera noise, which impedes subsequent processing\nand applications. We propose a novel LF denoising framework based on\nanisotropic parallax analysis (APA). Two convolutional neural networks are\njointly designed for the task: first, the structural parallax synthesis network\npredicts the parallax details for the entire LF based on a set of anisotropic\nparallax features. These novel features can efficiently capture the high\nfrequency perspective components of a LF from noisy observations. Second, the\nview-dependent detail compensation network restores non-Lambertian variation to\neach LF view by involving view-specific spatial energies. Extensive experiments\nshow that the proposed APA LF denoiser provides a much better denoising\nperformance than state-of-the-art methods in terms of visual quality and in\npreservation of parallax details. \n\n"}
{"id": "1805.12371", "contents": "Title: Lip Reading Using Convolutional Auto Encoders as Feature Extractor Abstract: Visual recognition of speech using the lip movement is called Lip-reading.\nRecent developments in this nascent field uses different neural networks as\nfeature extractors which serve as input to a model which can map the temporal\nrelationship and classify. Though end to end sentence level Lip-reading is the\ncurrent trend, we proposed a new model which employs word level classification\nand breaks the set benchmarks for standard datasets. In our model we use\nconvolutional autoencoders as feature extractors which are then fed to a Long\nshort-term memory model. We tested our proposed model on BBC's LRW dataset,\nMIRACL-VC1 and GRID dataset. Achieving a classification accuracy of 98% on\nMIRACL-VC1 as compared to 93.4% of the set benchmark (Rekik et al., 2014). On\nBBC's LRW the proposed model performed better than the baseline model of\nconvolutional neural networks and Long short-term memory model (Garg et al.,\n2016). Showing the features learned by the models we clearly indicate how the\nproposed model works better than the baseline model. The same model can also be\nextended for end to end sentence level classification. \n\n"}
{"id": "1806.00153", "contents": "Title: k-Space Deep Learning for Reference-free EPI Ghost Correction Abstract: Nyquist ghost artifacts in EPI are originated from phase mismatch between the\neven and odd echoes. However, conventional correction methods using reference\nscans often produce erroneous results especially in high-field MRI due to the\nnon-linear and time-varying local magnetic field changes. Recently, it was\nshown that the problem of ghost correction can be reformulated as k-space\ninterpolation problem that can be solved using structured low-rank Hankel\nmatrix approaches. Another recent work showed that data driven Hankel matrix\ndecomposition can be reformulated to exhibit similar structures as deep\nconvolutional neural network. By synergistically combining these findings, we\npropose a k-space deep learning approach that immediately corrects the phase\nmismatch without a reference scan in both accelerated and non-accelerated EPI\nacquisitions. To take advantage of the even and odd-phase directional\nredundancy, the k-space data is divided into two channels configured with even\nand odd phase encodings. The redundancies between coils are also exploited by\nstacking the multi-coil k-space data into additional input channels. Then, our\nk-space ghost correction network is trained to learn the interpolation kernel\nto estimate the missing virtual k-space data. For the accelerated EPI data, the\nsame neural network is trained to directly estimate the interpolation kernels\nfor missing k-space data from both ghost and subsampling. Reconstruction\nresults using 3T and 7T in-vivo data showed that the proposed method\noutperformed the image quality compared to the existing methods, and the\ncomputing time is much faster.The proposed k-space deep learning for EPI ghost\ncorrection is highly robust and fast, and can be combined with acceleration, so\nthat it can be used as a promising correction tool for high-field MRI without\nchanging the current acquisition protocol. \n\n"}
{"id": "1806.00806", "contents": "Title: k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR\n  Angiography Abstract: Time-resolved angiography with interleaved stochastic trajectories (TWIST)\nhas been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve\nhighly accelerated acquisitions, TWIST combines the periphery of the k-space\ndata from several adjacent frames to reconstruct one temporal frame. However,\nthis view-sharing scheme limits the true temporal resolution of TWIST.\nMoreover, the k-space sampling patterns have been specially designed for a\nspecific generalized autocalibrating partial parallel acquisition (GRAPPA)\nfactor so that it is not possible to reduce the number of view-sharing once the\nk-data is acquired. To address these issues, this paper proposes a novel\nk-space deep learning approach for parallel MRI. In particular, we have\ndesigned our neural network so that accurate k-space interpolations are\nperformed simultaneously for multiple coils by exploiting the redundancies\nalong the coils and images. Reconstruction results using in vivo TWIST data set\nconfirm that the proposed method can immediately generate high-quality\nreconstruction results with various choices of view- sharing, allowing us to\nexploit the trade-off between spatial and temporal resolution in time-resolved\nMR angiography. \n\n"}
{"id": "1806.01013", "contents": "Title: Synthetic data generation for end-to-end thermal infrared tracking Abstract: The usage of both off-the-shelf and end-to-end trained deep networks have\nsignificantly improved performance of visual tracking on RGB videos. However,\nthe lack of large labeled datasets hampers the usage of convolutional neural\nnetworks for tracking in thermal infrared (TIR) images. Therefore, most state\nof the art methods on tracking for TIR data are still based on handcrafted\nfeatures. To address this problem, we propose to use image-to-image translation\nmodels. These models allow us to translate the abundantly available labeled RGB\ndata to synthetic TIR data. We explore both the usage of paired and unpaired\nimage translation models for this purpose. These methods provide us with a\nlarge labeled dataset of synthetic TIR sequences, on which we can train\nend-to-end optimal features for tracking. To the best of our knowledge we are\nthe first to train end-to-end features for TIR tracking. We perform extensive\nexperiments on VOT-TIR2017 dataset. We show that a network trained on a large\ndataset of synthetic TIR data obtains better performance than one trained on\nthe available real TIR data. Combining both data sources leads to further\nimprovement. In addition, when we combine the network with motion features we\noutperform the state of the art with a relative gain of over 10%, clearly\nshowing the efficiency of using synthetic data to train end-to-end TIR\ntrackers. \n\n"}
{"id": "1806.01069", "contents": "Title: Deep Multi-Structural Shape Analysis: Application to Neuroanatomy Abstract: We propose a deep neural network for supervised learning on neuroanatomical\nshapes. The network directly operates on raw point clouds without the need for\nmesh processing or the identification of point correspondences, as spatial\ntransformer networks map the data to a canonical space. Instead of relying on\nhand-crafted shape descriptors, an optimal representation is learned in the\nend-to-end training stage of the network. The proposed network consists of\nmultiple branches, so that features for multiple structures are learned\nsimultaneously. We demonstrate the performance of our method on two\napplications: (i) the prediction of Alzheimer's disease and mild cognitive\nimpairment and (ii) the regression of the brain age. Finally, we visualize the\nimportant parts of the anatomy for the prediction by adapting the occlusion\nmethod to point clouds. \n\n"}
{"id": "1806.01320", "contents": "Title: Cube Padding for Weakly-Supervised Saliency Prediction in 360{\\deg}\n  Videos Abstract: Automatic saliency prediction in 360{\\deg} videos is critical for viewpoint\nguidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal\nnetwork which is (1) weakly-supervised trained and (2) tailor-made for\n360{\\deg} viewing sphere. Note that most existing methods are less scalable\nsince they rely on annotated saliency map for training. Most importantly, they\nconvert 360{\\deg} sphere to 2D images (e.g., a single equirectangular image or\nmultiple separate Normal Field-of-View (NFoV) images) which introduces\ndistortion and image boundaries. In contrast, we propose a simple and effective\nCube Padding (CP) technique as follows. Firstly, we render the 360{\\deg} view\non six faces of a cube using perspective projection. Thus, it introduces very\nlittle distortion. Then, we concatenate all six faces while utilizing the\nconnectivity between faces on the cube for image padding (i.e., Cube Padding)\nin convolution, pooling, convolutional LSTM layers. In this way, CP introduces\nno image boundary while being applicable to almost all Convolutional Neural\nNetwork (CNN) structures. To evaluate our method, we propose Wild-360, a new\n360{\\deg} video saliency dataset, containing challenging videos with saliency\nheatmap annotations. In experiments, our method outperforms baseline methods in\nboth speed and quality. \n\n"}
{"id": "1806.01911", "contents": "Title: Adversarial Scene Editing: Automatic Object Removal from Weak\n  Supervision Abstract: While great progress has been made recently in automatic image manipulation,\nit has been limited to object centric images like faces or structured scene\ndatasets. In this work, we take a step towards general scene-level image\nediting by developing an automatic interaction-free object removal model. Our\nmodel learns to find and remove objects from general scene images using\nimage-level labels and unpaired data in a generative adversarial network (GAN)\nframework. We achieve this with two key contributions: a two-stage editor\narchitecture consisting of a mask generator and image in-painter that\nco-operate to remove objects, and a novel GAN based prior for the mask\ngenerator that allows us to flexibly incorporate knowledge about object shapes.\nWe experimentally show on two datasets that our method effectively removes a\nwide variety of objects using weak supervision only \n\n"}
{"id": "1806.02318", "contents": "Title: Adaptive feature recombination and recalibration for semantic\n  segmentation: application to brain tumor segmentation in MRI Abstract: Convolutional neural networks (CNNs) have been successfully used for brain\ntumor segmentation, specifically, fully convolutional networks (FCNs). FCNs can\nsegment a set of voxels at once, having a direct spatial correspondence between\nunits in feature maps (FMs) at a given location and the corresponding\nclassified voxels. In convolutional layers, FMs are merged to create new FMs,\nso, channel combination is crucial. However, not all FMs have the same\nrelevance for a given class. Recently, in classification problems,\nSqueeze-and-Excitation (SE) blocks have been proposed to re-calibrate FMs as a\nwhole, and suppress the less informative ones. However, this is not optimal in\nFCN due to the spatial correspondence between units and voxels. In this\narticle, we propose feature recombination through linear expansion and\ncompression to create more complex features for semantic segmentation.\nAdditionally, we propose a segmentation SE (SegSE) block for feature\nrecalibration that collects contextual information, while maintaining the\nspatial meaning. Finally, we evaluate the proposed methods in brain tumor\nsegmentation, using publicly available data. \n\n"}
{"id": "1806.02658", "contents": "Title: Super-Resolution using Convolutional Neural Networks without Any\n  Checkerboard Artifacts Abstract: It is well-known that a number of excellent super-resolution (SR) methods\nusing convolutional neural networks (CNNs) generate checkerboard artifacts. A\ncondition to avoid the checkerboard artifacts is proposed in this paper. So\nfar, checkerboard artifacts have been mainly studied for linear multirate\nsystems, but the condition to avoid checkerboard artifacts can not be applied\nto CNNs due to the non-linearity of CNNs. We extend the avoiding condition for\nCNNs, and apply the proposed structure to some typical SR methods to confirm\nthe effectiveness of the new scheme. Experiment results demonstrate that the\nproposed structure can perfectly avoid to generate checkerboard artifacts under\ntwo loss conditions: mean square error and perceptual loss, while keeping\nexcellent properties that the SR methods have. \n\n"}
{"id": "1806.02918", "contents": "Title: Color Sails: Discrete-Continuous Palettes for Deep Color Exploration Abstract: We present color sails, a discrete-continuous color gamut representation that\nextends the color gradient analogy to three dimensions and allows interactive\ncontrol of the color blending behavior. Our representation models a wide\nvariety of color distributions in a compact manner, and lends itself to\napplications such as color exploration for graphic design, illustration and\nsimilar fields. We propose a Neural Network that can fit a color sail to any\nimage. Then, the user can adjust color sail parameters to change the base\ncolors, their blending behavior and the number of colors, exploring a wide\nrange of options for the original design. In addition, we propose a Deep\nLearning model that learns to automatically segment an image into\ncolor-compatible alpha masks, each equipped with its own color sail. This\nallows targeted color exploration by either editing their corresponding color\nsails or using standard software packages. Our model is trained on a custom\ndiverse dataset of art and design. We provide both quantitative evaluations,\nand a user study, demonstrating the effectiveness of color sail interaction.\nInteractive demos are available at www.colorsails.com. \n\n"}
{"id": "1806.03724", "contents": "Title: Learning Answer Embeddings for Visual Question Answering Abstract: We propose a novel probabilistic model for visual question answering (Visual\nQA). The key idea is to infer two sets of embeddings: one for the image and the\nquestion jointly and the other for the answers. The learning objective is to\nlearn the best parameterization of those embeddings such that the correct\nanswer has higher likelihood among all possible answers. In contrast to several\nexisting approaches of treating Visual QA as multi-way classification, the\nproposed approach takes the semantic relationships (as characterized by the\nembeddings) among answers into consideration, instead of viewing them as\nindependent ordinal numbers. Thus, the learned embedded function can be used to\nembed unseen answers (in the training dataset). These properties make the\napproach particularly appealing for transfer learning for open-ended Visual QA,\nwhere the source dataset on which the model is learned has limited overlapping\nwith the target dataset in the space of answers. We have also developed\nlarge-scale optimization techniques for applying the model to datasets with a\nlarge number of answers, where the challenge is to properly normalize the\nproposed probabilistic models. We validate our approach on several Visual QA\ndatasets and investigate its utility for transferring models across datasets.\nThe empirical results have shown that the approach performs well not only on\nin-domain learning but also on transfer learning. \n\n"}
{"id": "1806.04374", "contents": "Title: Fast Rotational Sparse Coding Abstract: We propose an algorithm for rotational sparse coding along with an efficient\nimplementation using steerability. Sparse coding (also called dictionary\nlearning) is an important technique in image processing, useful in inverse\nproblems, compression, and analysis; however, the usual formulation fails to\ncapture an important aspect of the structure of images: images are formed from\nbuilding blocks, e.g., edges, lines, or points, that appear at different\nlocations, orientations, and scales. The sparse coding problem can be\nreformulated to explicitly account for these transforms, at the cost of\nincreased computation. In this work, we propose an algorithm for a rotational\nversion of sparse coding that is based on K-SVD with additional rotation\noperations. We then propose a method to accelerate these rotations by learning\nthe dictionary in a steerable basis. Our experiments on patch coding and\ntexture classification demonstrate that the proposed algorithm is fast enough\nfor practical use and compares favorably to standard sparse coding. \n\n"}
{"id": "1806.04413", "contents": "Title: Enhancing clinical MRI Perfusion maps with data-driven maps of\n  complementary nature for lesion outcome prediction Abstract: Stroke is the second most common cause of death in developed countries, where\nrapid clinical intervention can have a major impact on a patient's life. To\nperform the revascularization procedure, the decision making of physicians\nconsiders its risks and benefits based on multi-modal MRI and clinical\nexperience. Therefore, automatic prediction of the ischemic stroke lesion\noutcome has the potential to assist the physician towards a better stroke\nassessment and information about tissue outcome. Typically, automatic methods\nconsider the information of the standard kinetic models of diffusion and\nperfusion MRI (e.g. Tmax, TTP, MTT, rCBF, rCBV) to perform lesion outcome\nprediction. In this work, we propose a deep learning method to fuse this\ninformation with an automated data selection of the raw 4D PWI image\ninformation, followed by a data-driven deep-learning modeling of the underlying\nblood flow hemodynamics. We demonstrate the ability of the proposed approach to\nimprove prediction of tissue at risk before therapy, as compared to only using\nthe standard clinical perfusion maps, hence suggesting on the potential\nbenefits of the proposed data-driven raw perfusion data modelling approach. \n\n"}
{"id": "1806.05024", "contents": "Title: Self-Supervised Feature Learning by Learning to Spot Artifacts Abstract: We introduce a novel self-supervised learning method based on adversarial\ntraining. Our objective is to train a discriminator network to distinguish real\nimages from images with synthetic artifacts, and then to extract features from\nits intermediate layers that can be transferred to other data domains and\ntasks. To generate images with artifacts, we pre-train a high-capacity\nautoencoder and then we use a damage and repair strategy: First, we freeze the\nautoencoder and damage the output of the encoder by randomly dropping its\nentries. Second, we augment the decoder with a repair network, and train it in\nan adversarial manner against the discriminator. The repair network helps\ngenerate more realistic images by inpainting the dropped feature entries. To\nmake the discriminator focus on the artifacts, we also make it predict what\nentries in the feature were dropped. We demonstrate experimentally that\nfeatures learned by creating and spotting artifacts achieve state of the art\nperformance in several benchmarks. \n\n"}
{"id": "1806.05083", "contents": "Title: Multiple Instance Learning for Heterogeneous Images: Training a CNN for\n  Histopathology Abstract: Multiple instance (MI) learning with a convolutional neural network enables\nend-to-end training in the presence of weak image-level labels. We propose a\nnew method for aggregating predictions from smaller regions of the image into\nan image-level classification by using the quantile function. The quantile\nfunction provides a more complete description of the heterogeneity within each\nimage, improving image-level classification. We also adapt image augmentation\nto the MI framework by randomly selecting cropped regions on which to apply MI\naggregation during each epoch of training. This provides a mechanism to study\nthe importance of MI learning. We validate our method on five different\nclassification tasks for breast tumor histology and provide a visualization\nmethod for interpreting local image classifications that could lead to future\ninsights into tumor heterogeneity. \n\n"}
{"id": "1806.05228", "contents": "Title: 3D-CODED : 3D Correspondences by Deep Deformation Abstract: We present a new deep learning approach for matching deformable shapes by\nintroducing {\\it Shape Deformation Networks} which jointly encode 3D shapes and\ncorrespondences. This is achieved by factoring the surface representation into\n(i) a template, that parameterizes the surface, and (ii) a learnt global\nfeature vector that parameterizes the transformation of the template into the\ninput surface. By predicting this feature for a new shape, we implicitly\npredict correspondences between this shape and the template. We show that these\ncorrespondences can be improved by an additional step which improves the shape\nfeature by minimizing the Chamfer distance between the input and transformed\ntemplate. We demonstrate that our simple approach improves on state-of-the-art\nresults on the difficult FAUST-inter challenge, with an average correspondence\nerror of 2.88cm. We show, on the TOSCA dataset, that our method is robust to\nmany types of perturbations, and generalizes to non-human shapes. This\nrobustness allows it to perform well on real unclean, meshes from the the SCAPE\ndataset. \n\n"}
{"id": "1806.05473", "contents": "Title: Efficient Active Learning for Image Classification and Segmentation\n  using a Sample Selection and Conditional Generative Adversarial Network Abstract: Training robust deep learning (DL) systems for medical image classification\nor segmentation is challenging due to limited images covering different disease\ntypes and severity. We propose an active learning (AL) framework to select most\ninformative samples and add to the training data. We use conditional generative\nadversarial networks (cGANs) to generate realistic chest xray images with\ndifferent disease characteristics by conditioning its generation on a real\nimage sample. Informative samples to add to the training set are identified\nusing a Bayesian neural network. Experiments show our proposed AL framework is\nable to achieve state of the art performance by using about 35% of the full\ndataset, thus saving significant time and effort over conventional methods. \n\n"}
{"id": "1806.05512", "contents": "Title: NetScore: Towards Universal Metrics for Large-scale Performance Analysis\n  of Deep Neural Networks for Practical On-Device Edge Usage Abstract: Much of the focus in the design of deep neural networks has been on improving\naccuracy, leading to more powerful yet highly complex network architectures\nthat are difficult to deploy in practical scenarios, particularly on edge\ndevices such as mobile and other consumer devices given their high\ncomputational and memory requirements. As a result, there has been a recent\ninterest in the design of quantitative metrics for evaluating deep neural\nnetworks that accounts for more than just model accuracy as the sole indicator\nof network performance. In this study, we continue the conversation towards\nuniversal metrics for evaluating the performance of deep neural networks for\npractical on-device edge usage. In particular, we propose a new balanced metric\ncalled NetScore, which is designed specifically to provide a quantitative\nassessment of the balance between accuracy, computational complexity, and\nnetwork architecture complexity of a deep neural network, which is important\nfor on-device edge operation. In what is one of the largest comparative\nanalysis between deep neural networks in literature, the NetScore metric, the\ntop-1 accuracy metric, and the popular information density metric were compared\nacross a diverse set of 60 different deep convolutional neural networks for\nimage classification on the ImageNet Large Scale Visual Recognition Challenge\n(ILSVRC 2012) dataset. The evaluation results across these three metrics for\nthis diverse set of networks are presented in this study to act as a reference\nguide for practitioners in the field. The proposed NetScore metric, along with\nthe other tested metrics, are by no means perfect, but the hope is to push the\nconversation towards better universal metrics for evaluating deep neural\nnetworks for use in practical on-device edge scenarios to help guide\npractitioners in model design for such scenarios. \n\n"}
{"id": "1806.05660", "contents": "Title: Interactive Classification for Deep Learning Interpretation Abstract: We present an interactive system enabling users to manipulate images to\nexplore the robustness and sensitivity of deep learning image classifiers.\nUsing modern web technologies to run in-browser inference, users can remove\nimage features using inpainting algorithms and obtain new classifications in\nreal time, which allows them to ask a variety of \"what if\" questions by\nexperimentally modifying images and seeing how the model reacts. Our system\nallows users to compare and contrast what image regions humans and machine\nlearning models use for classification, revealing a wide range of surprising\nresults ranging from spectacular failures (e.g., a \"water bottle\" image becomes\na \"concert\" when removing a person) to impressive resilience (e.g., a \"baseball\nplayer\" image remains correctly classified even without a glove or base). We\ndemonstrate our system at The 2018 Conference on Computer Vision and Pattern\nRecognition (CVPR) for the audience to try it live. Our system is open-sourced\nat https://github.com/poloclub/interactive-classification. A video demo is\navailable at https://youtu.be/llub5GcOF6w. \n\n"}
{"id": "1806.05789", "contents": "Title: Image classification and retrieval with random depthwise signed\n  convolutional neural networks Abstract: We propose a random convolutional neural network to generate a feature space\nin which we study image classification and retrieval performance. Put briefly\nwe apply random convolutional blocks followed by global average pooling to\ngenerate a new feature, and we repeat this k times to produce a k-dimensional\nfeature space. This can be interpreted as partitioning the space of image\npatches with random hyperplanes which we formalize as a random depthwise\nconvolutional neural network. In the network's final layer we perform image\nclassification and retrieval with the linear support vector machine and\nk-nearest neighbor classifiers and study other empirical properties. We show\nthat the ratio of image pixel distribution similarity across classes to within\nclasses is higher in our network's final layer compared to the input space.\nWhen we apply the linear support vector machine for image classification we see\nthat the accuracy is higher than if we were to train just the final layer of\nVGG16, ResNet18, and DenseNet40 with random weights. In the same setting we\ncompare it to an unsupervised feature learning method and find our accuracy to\nbe comparable on CIFAR10 but higher on CIFAR100 and STL10. We see that the\naccuracy is not far behind that of trained networks, particularly in the top-k\nsetting. For example the top-2 accuracy of our network is near 90% on both\nCIFAR10 and a 10-class mini ImageNet, and 85% on STL10. We find that k-nearest\nneighbor gives a comparable precision on the Corel Princeton Image Similarity\nBenchmark than if we were to use the final layer of trained networks. As with\nother networks we find that our network fails to a black box attack even though\nwe lack a gradient and use the sign activation. We highlight sensitivity of our\nnetwork to background as a potential pitfall and an advantage. Overall our work\npushes the boundary of what can be achieved with random weights. \n\n"}
{"id": "1806.06193", "contents": "Title: Large Scale Fine-Grained Categorization and Domain-Specific Transfer\n  Learning Abstract: Transferring the knowledge learned from large scale datasets (e.g., ImageNet)\nvia fine-tuning offers an effective solution for domain-specific fine-grained\nvisual categorization (FGVC) tasks (e.g., recognizing bird species or car make\nand model). In such scenarios, data annotation often calls for specialized\ndomain knowledge and thus is difficult to scale. In this work, we first tackle\na problem in large scale FGVC. Our method won first place in iNaturalist 2017\nlarge scale species classification challenge. Central to the success of our\napproach is a training scheme that uses higher image resolution and deals with\nthe long-tailed distribution of training data. Next, we study transfer learning\nvia fine-tuning from large scale datasets to small scale, domain-specific FGVC\ndatasets. We propose a measure to estimate domain similarity via Earth Mover's\nDistance and demonstrate that transfer learning benefits from pre-training on a\nsource domain that is similar to the target domain by this measure. Our\nproposed transfer learning outperforms ImageNet pre-training and obtains\nstate-of-the-art results on multiple commonly used FGVC datasets. \n\n"}
{"id": "1806.07497", "contents": "Title: Fully Automatic Myocardial Segmentation of Contrast Echocardiography\n  Sequence Using Random Forests Guided by Shape Model Abstract: Myocardial contrast echocardiography (MCE) is an imaging technique that\nassesses left ventricle function and myocardial perfusion for the detection of\ncoronary artery diseases. Automatic MCE perfusion quantification is challenging\nand requires accurate segmentation of the myocardium from noisy and\ntime-varying images. Random forests (RF) have been successfully applied to many\nmedical image segmentation tasks. However, the pixel-wise RF classifier ignores\ncontextual relationships between label outputs of individual pixels. RF which\nonly utilizes local appearance features is also susceptible to data suffering\nfrom large intensity variations. In this paper, we demonstrate how to overcome\nthe above limitations of classic RF by presenting a fully automatic\nsegmentation pipeline for myocardial segmentation in full-cycle 2D MCE data.\nSpecifically, a statistical shape model is used to provide shape prior\ninformation that guide the RF segmentation in two ways. First, a novel shape\nmodel (SM) feature is incorporated into the RF framework to generate a more\naccurate RF probability map. Second, the shape model is fitted to the RF\nprobability map to refine and constrain the final segmentation to plausible\nmyocardial shapes. We further improve the performance by introducing a bounding\nbox detection algorithm as a preprocessing step in the segmentation pipeline.\nOur approach on 2D image is further extended to 2D+t sequence which ensures\ntemporal consistency in the resultant sequence segmentations. When evaluated on\nclinical MCE data, our proposed method achieves notable improvement in\nsegmentation accuracy and outperforms other state-of-the-art methods including\nthe classic RF and its variants, active shape model and image registration. \n\n"}
{"id": "1806.07754", "contents": "Title: Spatio-Temporal Channel Correlation Networks for Action Classification Abstract: The work in this paper is driven by the question if spatio-temporal\ncorrelations are enough for 3D convolutional neural networks (CNN)? Most of the\ntraditional 3D networks use local spatio-temporal features. We introduce a new\nblock that models correlations between channels of a 3D CNN with respect to\ntemporal and spatial features. This new block can be added as a residual unit\nto different parts of 3D CNNs. We name our novel block 'Spatio-Temporal Channel\nCorrelation' (STC). By embedding this block to the current state-of-the-art\narchitectures such as ResNext and ResNet, we improved the performance by 2-3\\%\non Kinetics dataset. Our experiments show that adding STC blocks to current\nstate-of-the-art architectures outperforms the state-of-the-art methods on the\nHMDB51, UCF101 and Kinetics datasets. The other issue in training 3D CNNs is\nabout training them from scratch with a huge labeled dataset to get a\nreasonable performance. So the knowledge learned in 2D CNNs is completely\nignored. Another contribution in this work is a simple and effective technique\nto transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D\nCNN for a stable weight initialization. This allows us to significantly reduce\nthe number of training samples for 3D CNNs. Thus, by fine-tuning this network,\nwe beat the performance of generic and recent methods in 3D CNNs, which were\ntrained on large video datasets, e.g. Sports-1M, and fine-tuned on the target\ndatasets, e.g. HMDB51/UCF101. \n\n"}
{"id": "1806.08174", "contents": "Title: Crowd disagreement about medical images is informative Abstract: Classifiers for medical image analysis are often trained with a single\nconsensus label, based on combining labels given by experts or crowds. However,\ndisagreement between annotators may be informative, and thus removing it may\nnot be the best strategy. As a proof of concept, we predict whether a skin\nlesion from the ISIC 2017 dataset is a melanoma or not, based on crowd\nannotations of visual characteristics of that lesion. We compare using the mean\nannotations, illustrating consensus, to standard deviations and other\ndistribution moments, illustrating disagreement. We show that the mean\nannotations perform best, but that the disagreement measures are still\ninformative. We also make the crowd annotations used in this paper available at\n\\url{https://figshare.com/s/5cbbce14647b66286544}. \n\n"}
{"id": "1806.08279", "contents": "Title: Don't only Feel Read: Using Scene text to understand advertisements Abstract: We propose a framework for automated classification of Advertisement Images,\nusing not just Visual features but also Textual cues extracted from embedded\ntext. Our approach takes inspiration from the assumption that Ad images contain\nmeaningful textual content, that can provide discriminative semantic\ninterpretetion, and can thus aid in classifcation tasks. To this end, we\ndevelop a framework using off-the-shelf components, and demonstrate the\neffectiveness of Textual cues in semantic Classfication tasks. \n\n"}
{"id": "1806.08906", "contents": "Title: Privacy-Protective-GAN for Face De-identification Abstract: Face de-identification has become increasingly important as the image sources\nare explosively growing and easily accessible. The advance of new face\nrecognition techniques also arises people's concern regarding the privacy\nleakage. The mainstream pipelines of face de-identification are mostly based on\nthe k-same framework, which bears critiques of low effectiveness and poor\nvisual quality. In this paper, we propose a new framework called\nPrivacy-Protective-GAN (PP-GAN) that adapts GAN with novel verificator and\nregulator modules specially designed for the face de-identification problem to\nensure generating de-identified output with retained structure similarity\naccording to a single input. We evaluate the proposed approach in terms of\nprivacy protection, utility preservation, and structure similarity. Our\napproach not only outperforms existing face de-identification techniques but\nalso provides a practical framework of adapting GAN with priors of domain\nknowledge. \n\n"}
{"id": "1806.09283", "contents": "Title: RAM: A Region-Aware Deep Model for Vehicle Re-Identification Abstract: Previous works on vehicle Re-ID mainly focus on extracting global features\nand learning distance metrics. Because some vehicles commonly share same model\nand maker, it is hard to distinguish them based on their global appearances.\nCompared with the global appearance, local regions such as decorations and\ninspection stickers attached to the windshield, may be more distinctive for\nvehicle Re-ID. To embed the detailed visual cues in those local regions, we\npropose a Region-Aware deep Model (RAM). Specifically, in addition to\nextracting global features, RAM also extracts features from a series of local\nregions. As each local region conveys more distinctive visual cues, RAM\nencourages the deep model to learn discriminative features. We also introduce a\nnovel learning algorithm to jointly use vehicle IDs, types/models, and colors\nto train the RAM. This strategy fuses more cues for training and results in\nmore discriminative global and regional features. We evaluate our methods on\ntwo large-scale vehicle Re-ID datasets, i.e., VeRi and VehicleID. Experimental\nresults show our methods achieve promising performance in comparison with\nrecent works. \n\n"}
{"id": "1806.09613", "contents": "Title: Attention-based Few-Shot Person Re-identification Using Meta Learning Abstract: In this paper, we investigate the challenging task of person\nre-identification from a new perspective and propose an end-to-end\nattention-based architecture for few-shot re-identification through\nmeta-learning. The motivation for this task lies in the fact that humans, can\nusually identify another person after just seeing that given person a few times\n(or even once) by attending to their memory. On the other hand, the unique\nnature of the person re-identification problem, i.e., only few examples exist\nper identity and new identities always appearing during testing, calls for a\nfew shot learning architecture with the capacity of handling new identities.\nHence, we frame the problem within a meta-learning setting, where a neural\nnetwork based meta-learner is trained to optimize a learner i.e., an\nattention-based matching function. Another challenge of the person\nre-identification problem is the small inter-class difference between different\nidentities and large intra-class difference of the same identity. In order to\nincrease the discriminative power of the model, we propose a new\nattention-based feature encoding scheme that takes into account the critical\nintra-view and cross-view relationship of images. We refer to the proposed\nAttention-based Re-identification Metalearning model as ARM. Extensive\nevaluations demonstrate the advantages of the ARM as compared to the\nstate-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501\ndatasets. \n\n"}
{"id": "1806.09766", "contents": "Title: Simultaneous Segmentation and Classification of Bone Surfaces from\n  Ultrasound Using a Multi-feature Guided CNN Abstract: Various imaging artifacts, low signal-to-noise ratio, and bone surfaces\nappearing several millimeters in thickness have hindered the success of\nultrasound (US) guided computer assisted orthopedic surgery procedures. In this\nwork, a multi-feature guided convolutional neural network (CNN) architecture is\nproposed for simultaneous enhancement, segmentation, and classification of bone\nsurfaces from US data. The proposed CNN consists of two main parts: a\npre-enhancing net, that takes the concatenation of B-mode US scan and three\nfiltered image features for the enhancement of bone surfaces, and a modified\nU-net with a classification layer. The proposed method was validated on 650 in\nvivo US scans collected using two US machines, by scanning knee, femur, distal\nradius and tibia bones. Validation, against expert annotation, achieved\nstatistically significant improvements in segmentation of bone surfaces\ncompared to state-of-the-art. \n\n"}
{"id": "1806.10359", "contents": "Title: Context Proposals for Saliency Detection Abstract: One of the fundamental properties of a salient object region is its contrast\nwith the immediate context. The problem is that numerous object regions exist\nwhich potentially can all be salient. One way to prevent an exhaustive search\nover all object regions is by using object proposal algorithms. These return a\nlimited set of regions which are most likely to contain an object. Several\nsaliency estimation methods have used object proposals. However, they focus on\nthe saliency of the proposal only, and the importance of its immediate context\nhas not been evaluated.\n  In this paper, we aim to improve salient object detection. Therefore, we\nextend object proposal methods with context proposals, which allow to\nincorporate the immediate context in the saliency computation. We propose\nseveral saliency features which are computed from the context proposals. In the\nexperiments, we evaluate five object proposal methods for the task of saliency\nsegmentation, and find that Multiscale Combinatorial Grouping outperforms the\nothers. Furthermore, experiments show that the proposed context features\nimprove performance, and that our method matches results on the FT datasets and\nobtains competitive results on three other datasets (PASCAL-S, MSRA-B and\nECSSD). \n\n"}
{"id": "1806.11226", "contents": "Title: A Multimodal Recommender System for Large-scale Assortment Generation in\n  E-commerce Abstract: E-commerce platforms surface interesting products largely through product\nrecommendations that capture users' styles and aesthetic preferences. Curating\nrecommendations as a complete complementary set, or assortment, is critical for\na successful e-commerce experience, especially for product categories such as\nfurniture, where items are selected together with the overall theme, style or\nambiance of a space in mind. In this paper, we propose two visually-aware\nrecommender systems that can automatically curate an assortment of living room\nfurniture around a couple of pre-selected seed pieces for the room. The first\nsystem aims to maximize the visual-based style compatibility of the entire\nselection by making use of transfer learning and topic modeling. The second\nsystem extends the first by incorporating text data and applying polylingual\ntopic modeling to infer style over both modalities. We review the production\npipeline for surfacing these visually-aware recommender systems and compare\nthem through offline validations and large-scale online A/B tests on Overstock.\nOur experimental results show that complimentary style is best discovered over\nproduct sets when both visual and textual data are incorporated. \n\n"}
{"id": "1807.00537", "contents": "Title: SphereReID: Deep Hypersphere Manifold Embedding for Person\n  Re-Identification Abstract: Many current successful Person Re-Identification(ReID) methods train a model\nwith the softmax loss function to classify images of different persons and\nobtain the feature vectors at the same time. However, the underlying feature\nembedding space is ignored. In this paper, we use a modified softmax function,\ntermed Sphere Softmax, to solve the classification problem and learn a\nhypersphere manifold embedding simultaneously. A balanced sampling strategy is\nalso introduced. Finally, we propose a convolutional neural network called\nSphereReID adopting Sphere Softmax and training a single model end-to-end with\na new warming-up learning rate schedule on four challenging datasets including\nMarket-1501, DukeMTMC-reID, CHHK-03, and CUHK-SYSU. Experimental results\ndemonstrate that this single model outperforms the state-of-the-art methods on\nall four datasets without fine-tuning or re-ranking. For example, it achieves\n94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on\nDukeMTMC-reID. The code and trained weights of our model will be released. \n\n"}
{"id": "1807.00751", "contents": "Title: Understanding the Effectiveness of Lipschitz-Continuity in Generative\n  Adversarial Nets Abstract: In this paper, we investigate the underlying factor that leads to failure and\nsuccess in the training of GANs. We study the property of the optimal\ndiscriminative function and show that in many GANs, the gradient from the\noptimal discriminative function is not reliable, which turns out to be the\nfundamental cause of failure in training of GANs. We further demonstrate that a\nwell-defined distance metric does not necessarily guarantee the convergence of\nGANs. Finally, we prove in this paper that Lipschitz-continuity condition is a\ngeneral solution to make the gradient of the optimal discriminative function\nreliable, and characterized the necessary condition where Lipschitz-continuity\nensures the convergence, which leads to a broad family of valid GAN objectives\nunder Lipschitz-continuity condition, where Wasserstein distance is one special\ncase. We experiment with several new objectives, which are sound according to\nour theorems, and we found that, compared with Wasserstein distance, the\noutputs of the discriminator with new objectives are more stable and the final\nqualities of generated samples are also consistently higher than those produced\nby Wasserstein distance. \n\n"}
{"id": "1807.01462", "contents": "Title: Video Frame Interpolation by Plug-and-Play Deep Locally Linear Embedding Abstract: We propose a generative framework which takes on the video frame\ninterpolation problem. Our framework, which we call Deep Locally Linear\nEmbedding (DeepLLE), is powered by a deep convolutional neural network (CNN)\nwhile it can be used instantly like conventional models. DeepLLE fits an\nauto-encoding CNN to a set of several consecutive frames and embeds a linearity\nconstraint on the latent codes so that new frames can be generated by\ninterpolating new latent codes. Different from the current deep learning\nparadigm which requires training on large datasets, DeepLLE works in a\nplug-and-play and unsupervised manner, and is able to generate an arbitrary\nnumber of frames. Thorough experiments demonstrate that without bells and\nwhistles, our method is highly competitive among current state-of-the-art\nmodels. \n\n"}
{"id": "1807.01493", "contents": "Title: Uncorrelated Feature Encoding for Faster Image Style Transfer Abstract: Recent fast style transfer methods use a pre-trained convolutional neural\nnetwork as a feature encoder and a perceptual loss network. Although the\npre-trained network is used to generate responses of receptive fields effective\nfor representing style and content of image, it is not optimized for image\nstyle transfer but rather for image classification. Furthermore, it also\nrequires a time-consuming and correlation-considering feature alignment process\nfor image style transfer because of its inter-channel correlation. In this\npaper, we propose an end-to-end learning method which optimizes an\nencoder/decoder network for the purpose of style transfer as well as relieves\nthe feature alignment complexity from considering inter-channel correlation. We\nused uncorrelation loss, i.e., the total correlation coefficient between the\nresponses of different encoder channels, with style and content losses for\ntraining style transfer network. This makes the encoder network to be trained\nto generate inter-channel uncorrelated features and to be optimized for the\ntask of image style transfer which maintained the quality of image style only\nwith a light-weighted and correlation-unaware feature alignment process.\nMoreover, our method drastically reduced redundant channels of the encoded\nfeature and this resulted in the efficient size of structure of network and\nfaster forward processing speed. Our method can also be applied to cascade\nnetwork scheme for multiple scaled style transferring and allows user-control\nof style strength by using a content-style trade-off parameter. \n\n"}
{"id": "1807.01788", "contents": "Title: MITOS-RCNN: A Novel Approach to Mitotic Figure Detection in Breast\n  Cancer Histopathology Images using Region Based Convolutional Neural Networks Abstract: Studies estimate that there will be 266,120 new cases of invasive breast\ncancer and 40,920 breast cancer induced deaths in the year of 2018 alone.\nDespite the pervasiveness of this affliction, the current process to obtain an\naccurate breast cancer prognosis is tedious and time consuming, requiring a\ntrained pathologist to manually examine histopathological images in order to\nidentify the features that characterize various cancer severity levels. We\npropose MITOS-RCNN: a novel region based convolutional neural network (RCNN)\ngeared for small object detection to accurately grade one of the three factors\nthat characterize tumor belligerence described by the Nottingham Grading\nSystem: mitotic count. Other computational approaches to mitotic figure\ncounting and detection do not demonstrate ample recall or precision to be\nclinically viable. Our models outperformed all previous participants in the\nICPR 2012 challenge, the AMIDA 2013 challenge and the MITOS-ATYPIA-14 challenge\nalong with recently published works. Our model achieved an F-measure score of\n0.955, a 6.11% improvement in accuracy from the most accurate of the previously\nproposed models. \n\n"}
{"id": "1807.01989", "contents": "Title: Revisiting Perspective Information for Efficient Crowd Counting Abstract: Crowd counting is the task of estimating people numbers in crowd images.\nModern crowd counting methods employ deep neural networks to estimate crowd\ncounts via crowd density regressions. A major challenge of this task lies in\nthe perspective distortion, which results in drastic person scale change in an\nimage. Density regression on the small person area is in general very hard. In\nthis work, we propose a perspective-aware convolutional neural network (PACNN)\nfor efficient crowd counting, which integrates the perspective information into\ndensity regression to provide additional knowledge of the person scale change\nin an image. Ground truth perspective maps are firstly generated for training;\nPACNN is then specifically designed to predict multi-scale perspective maps,\nand encode them as perspective-aware weighting layers in the network to\nadaptively combine the outputs of multi-scale density maps. The weights are\nlearned at every pixel of the maps such that the final density combination is\nrobust to the perspective distortion. We conduct extensive experiments on the\nShanghaiTech, WorldExpo'10, UCF_CC_50, and UCSD datasets, and demonstrate the\neffectiveness and efficiency of PACNN over the state-of-the-art. \n\n"}
{"id": "1807.03148", "contents": "Title: Deep Spatio-Temporal Random Fields for Efficient Video Segmentation Abstract: In this work we introduce a time- and memory-efficient method for structured\nprediction that couples neuron decisions across both space at time. We show\nthat we are able to perform exact and efficient inference on a densely\nconnected spatio-temporal graph by capitalizing on recent advances on deep\nGaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)\nefficient, (b) has a unique global minimum, and (c) can be trained end-to-end\nalongside contemporary deep networks for video understanding. We experiment\nwith multiple connectivity patterns in the temporal domain, and present\nempirical improvements over strong baselines on the tasks of both semantic and\ninstance segmentation of videos. \n\n"}
{"id": "1807.03514", "contents": "Title: Topic-Guided Attention for Image Captioning Abstract: Attention mechanisms have attracted considerable interest in image captioning\nbecause of its powerful performance. Existing attention-based models use\nfeedback information from the caption generator as guidance to determine which\nof the image features should be attended to. A common defect of these attention\ngeneration methods is that they lack a higher-level guiding information from\nthe image itself, which sets a limit on selecting the most informative image\nfeatures. Therefore, in this paper, we propose a novel attention mechanism,\ncalled topic-guided attention, which integrates image topics in the attention\nmodel as a guiding information to help select the most important image\nfeatures. Moreover, we extract image features and image topics with separate\nnetworks, which can be fine-tuned jointly in an end-to-end manner during\ntraining. The experimental results on the benchmark Microsoft COCO dataset show\nthat our method yields state-of-art performance on various quantitative\nmetrics. \n\n"}
{"id": "1807.04409", "contents": "Title: Sem-GAN: Semantically-Consistent Image-to-Image Translation Abstract: Unpaired image-to-image translation is the problem of mapping an image in the\nsource domain to one in the target domain, without requiring corresponding\nimage pairs. To ensure the translated images are realistically plausible,\nrecent works, such as Cycle-GAN, demands this mapping to be invertible. While,\nthis requirement demonstrates promising results when the domains are unimodal,\nits performance is unpredictable in a multi-modal scenario such as in an image\nsegmentation task. This is because, invertibility does not necessarily enforce\nsemantic correctness. To this end, we present a semantically-consistent GAN\nframework, dubbed Sem-GAN, in which the semantics are defined by the class\nidentities of image segments in the source domain as produced by a semantic\nsegmentation algorithm. Our proposed framework includes consistency constraints\non the translation task that, together with the GAN loss and the\ncycle-constraints, enforces that the images when translated will inherit the\nappearances of the target domain, while (approximately) maintaining their\nidentities from the source domain. We present experiments on several\nimage-to-image translation tasks and demonstrate that Sem-GAN improves the\nquality of the translated images significantly, sometimes by more than 20% on\nthe FCN score. Further, we show that semantic segmentation models, trained with\nsynthetic images translated via Sem-GAN, leads to significantly better\nsegmentation results than other variants. \n\n"}
{"id": "1807.04445", "contents": "Title: Adding Attentiveness to the Neurons in Recurrent Neural Networks Abstract: Recurrent neural networks (RNNs) are capable of modeling the temporal\ndynamics of complex sequential information. However, the structures of existing\nRNN neurons mainly focus on controlling the contributions of current and\nhistorical information but do not explore the different importance levels of\ndifferent elements in an input vector of a time slot. We propose adding a\nsimple yet effective Element-wiseAttention Gate (EleAttG) to an RNN block\n(e.g., all RNN neurons in a network layer) that empowers the RNN neurons to\nhave the attentiveness capability. For an RNN block, an EleAttG is added to\nadaptively modulate the input by assigning different levels of importance,\ni.e., attention, to each element/dimension of the input. We refer to an RNN\nblock equipped with an EleAttG as an EleAtt-RNN block. Specifically, the\nmodulation of the input is content adaptive and is performed at fine\ngranularity, being element-wise rather than input-wise. The proposed EleAttG,\nas an additional fundamental unit, is general and can be applied to any RNN\nstructures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated\nRecurrent Unit (GRU). We demonstrate the effectiveness of the proposed\nEleAtt-RNN by applying it to the action recognition tasks on both 3D human\nskeleton data and RGB videos. Experiments show that adding attentiveness\nthrough EleAttGs to RNN blocks significantly boosts the power of RNNs. \n\n"}
{"id": "1807.04812", "contents": "Title: Latent Transformations for Object View Points Synthesis Abstract: We propose a fully-convolutional conditional generative model, the latent\ntransformation neural network (LTNN), capable of view synthesis using a\nlight-weight neural network suited for real-time applications. In contrast to\nexisting conditional generative models which incorporate conditioning\ninformation via concatenation, we introduce a dedicated network component, the\nconditional transformation unit (CTU), designed to learn the latent space\ntransformations corresponding to specified target views. In addition, a\nconsistency loss term is defined to guide the network toward learning the\ndesired latent space mappings, a task-divided decoder is constructed to refine\nthe quality of generated views, and an adaptive discriminator is introduced to\nimprove the adversarial training process. The generality of the proposed\nmethodology is demonstrated on a collection of three diverse tasks: multi-view\nreconstruction on real hand depth images, view synthesis of real and synthetic\nfaces, and the rotation of rigid objects. The proposed model is shown to exceed\nstate-of-the-art results in each category while simultaneously achieving a\nreduction in the computational demand required for inference by 30% on average. \n\n"}
{"id": "1807.05284", "contents": "Title: Survey on Deep Learning Techniques for Person Re-Identification Task Abstract: Intelligent video-surveillance is currently an active research field in\ncomputer vision and machine learning techniques. It provides useful tools for\nsurveillance operators and forensic video investigators. Person\nre-identification (PReID) is one among these tools. It consists of recognizing\nwhether an individual has already been observed over a camera in a network or\nnot. This tool can also be employed in various possible applications such as\noff-line retrieval of all the video-sequences showing an individual of interest\nwhose image is given a query, and online pedestrian tracking over multiple\ncamera views. To this aim, many techniques have been proposed to increase the\nperformance of PReID. Among the systems, many researchers utilized deep neural\nnetworks (DNNs) because of their better performance and fast execution at test\ntime. Our objective is to provide for future researchers the work being done on\nPReID to date. Therefore, we summarized state-of-the-art DNN models being used\nfor this task. A brief description of each model along with their evaluation on\na set of benchmark datasets is given. Finally, a detailed comparison is\nprovided among these models followed by some limitations that can work as\nguidelines for future research. \n\n"}
{"id": "1807.05959", "contents": "Title: A Multimodal Approach to Predict Social Media Popularity Abstract: Multiple modalities represent different aspects by which information is\nconveyed by a data source. Modern day social media platforms are one of the\nprimary sources of multimodal data, where users use different modes of\nexpression by posting textual as well as multimedia content such as images and\nvideos for sharing information. Multimodal information embedded in such posts\ncould be useful in predicting their popularity. To the best of our knowledge,\nno such multimodal dataset exists for the prediction of social media photos. In\nthis work, we propose a multimodal dataset consisiting of content, context, and\nsocial information for popularity prediction. Specifically, we augment the\nSMPT1 dataset for social media prediction in ACM Multimedia grand challenge\n2017 with image content, titles, descriptions, and tags. Next, in this paper,\nwe propose a multimodal approach which exploits visual features (i.e., content\ninformation), textual features (i.e., contextual information), and social\nfeatures (e.g., average views and group counts) to predict popularity of social\nmedia photos in terms of view counts. Experimental results confirm that despite\nour multimodal approach uses the half of the training dataset from SMP-T1, it\nachieves comparable performance with that of state-of-the-art. \n\n"}
{"id": "1807.06009", "contents": "Title: ActiveStereoNet: End-to-End Self-Supervised Learning for Active Stereo\n  Systems Abstract: In this paper we present ActiveStereoNet, the first deep learning solution\nfor active stereo systems. Due to the lack of ground truth, our method is fully\nself-supervised, yet it produces precise depth with a subpixel precision of\n$1/30th$ of a pixel; it does not suffer from the common over-smoothing issues;\nit preserves the edges; and it explicitly handles occlusions. We introduce a\nnovel reconstruction loss that is more robust to noise and texture-less\npatches, and is invariant to illumination changes. The proposed loss is\noptimized using a window-based cost aggregation with an adaptive support weight\nscheme. This cost aggregation is edge-preserving and smooths the loss function,\nwhich is key to allow the network to reach compelling results. Finally we show\nhow the task of predicting invalid regions, such as occlusions, can be trained\nend-to-end without ground-truth. This component is crucial to reduce blur and\nparticularly improves predictions along depth discontinuities. Extensive\nquantitatively and qualitatively evaluations on real and synthetic data\ndemonstrate state of the art results in many challenging scenes. \n\n"}
{"id": "1807.06271", "contents": "Title: Real-time on-board obstacle avoidance for UAVs based on embedded stereo\n  vision Abstract: In order to improve usability and safety, modern unmanned aerial vehicles\n(UAVs) are equipped with sensors to monitor the environment, such as\nlaser-scanners and cameras. One important aspect in this monitoring process is\nto detect obstacles in the flight path in order to avoid collisions. Since a\nlarge number of consumer UAVs suffer from tight weight and power constraints,\nour work focuses on obstacle avoidance based on a lightweight stereo camera\nsetup. We use disparity maps, which are computed from the camera images, to\nlocate obstacles and to automatically steer the UAV around them. For disparity\nmap computation we optimize the well-known semi-global matching (SGM) approach\nfor the deployment on an embedded FPGA. The disparity maps are then converted\ninto simpler representations, the so called U-/V-Maps, which are used for\nobstacle detection. Obstacle avoidance is based on a reactive approach which\nfinds the shortest path around the obstacles as soon as they have a critical\ndistance to the UAV. One of the fundamental goals of our work was the reduction\nof development costs by closing the gap between application development and\nhardware optimization. Hence, we aimed at using high-level synthesis (HLS) for\nporting our algorithms, which are written in C/C++, to the embedded FPGA. We\nevaluated our implementation of the disparity estimation on the KITTI Stereo\n2015 benchmark. The integrity of the overall realtime reactive obstacle\navoidance algorithm has been evaluated by using Hardware-in-the-Loop testing in\nconjunction with two flight simulators. \n\n"}
{"id": "1807.06699", "contents": "Title: Adaptive Neural Trees Abstract: Deep neural networks and decision trees operate on largely separate\nparadigms; typically, the former performs representation learning with\npre-specified architectures, while the latter is characterised by learning\nhierarchies over pre-specified features with data-driven architectures. We\nunite the two via adaptive neural trees (ANTs) that incorporates representation\nlearning into edges, routing functions and leaf nodes of a decision tree, along\nwith a backpropagation-based training algorithm that adaptively grows the\narchitecture from primitive modules (e.g., convolutional layers). We\ndemonstrate that, whilst achieving competitive performance on classification\nand regression datasets, ANTs benefit from (i) lightweight inference via\nconditional computation, (ii) hierarchical separation of features useful to the\ntask e.g. learning meaningful class associations, such as separating natural\nvs. man-made objects, and (iii) a mechanism to adapt the architecture to the\nsize and complexity of the training dataset. \n\n"}
{"id": "1807.07203", "contents": "Title: Few-Shot Adaptation for Multimedia Semantic Indexing Abstract: We propose a few-shot adaptation framework, which bridges zero-shot learning\nand supervised many-shot learning, for semantic indexing of image and video\ndata. Few-shot adaptation provides robust parameter estimation with few\ntraining examples, by optimizing the parameters of zero-shot learning and\nsupervised many-shot learning simultaneously. In this method, first we build a\nzero-shot detector, and then update it by using the few examples. Our\nexperiments show the effectiveness of the proposed framework on three datasets:\nTRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, we\nshow that our method outperforms recent few-shot learning methods. On the\nTRECVID 2014 dataset, we achieve 15.19% and 35.98% in Mean Average Precision\nunder the zero-shot condition and the supervised condition, respectively. To\nthe best of our knowledge, these are the best results on this dataset. \n\n"}
{"id": "1807.07258", "contents": "Title: Visual Domain Adaptation with Manifold Embedded Distribution Alignment Abstract: Visual domain adaptation aims to learn robust classifiers for the target\ndomain by leveraging knowledge from a source domain. Existing methods either\nattempt to align the cross-domain distributions, or perform manifold subspace\nlearning. However, there are two significant challenges: (1) degenerated\nfeature transformation, which means that distribution alignment is often\nperformed in the original feature space, where feature distortions are hard to\novercome. On the other hand, subspace learning is not sufficient to reduce the\ndistribution divergence. (2) unevaluated distribution alignment, which means\nthat existing distribution alignment methods only align the marginal and\nconditional distributions with equal importance, while they fail to evaluate\nthe different importance of these two distributions in real applications. In\nthis paper, we propose a Manifold Embedded Distribution Alignment (MEDA)\napproach to address these challenges. MEDA learns a domain-invariant classifier\nin Grassmann manifold with structural risk minimization, while performing\ndynamic distribution alignment to quantitatively account for the relative\nimportance of marginal and conditional distributions. To the best of our\nknowledge, MEDA is the first attempt to perform dynamic distribution alignment\nfor manifold domain adaptation. Extensive experiments demonstrate that MEDA\nshows significant improvements in classification accuracy compared to\nstate-of-the-art traditional and deep methods. \n\n"}
{"id": "1807.07432", "contents": "Title: Signal Alignment for Humanoid Skeletons via the Globally Optimal\n  Reparameterization Algorithm Abstract: The general ability to analyze and classify the 3D kinematics of the human\nform is an essential step in the development of socially adept humanoid robots.\nA variety of different types of signals can be used by machines to represent\nand characterize actions such as RGB videos, infrared maps, and optical flow.\nIn particular, skeleton sequences provide a natural 3D kinematic description of\nhuman motions and can be acquired in real time using RGB+D cameras. Moreover,\nskeleton sequences are generalizable to characterize the motions of both humans\nand humanoid robots. The Globally Optimal Reparameterization Algorithm (GORA)\nis a novel, recently proposed algorithm for signal alignment in which signals\nare reparameterized to a globally optimal universal standard timescale (UST).\nHere, we introduce a variant of GORA for humanoid action recognition with\nskeleton sequences, which we call GORA-S. We briefly review the algorithm's\nmathematical foundations and contextualize them in the problem of action\nrecognition with skeleton sequences. Subsequently, we introduce GORA-S and\ndiscuss parameters and numerical techniques for its effective implementation.\nWe then compare its performance with that of the DTW and FastDTW algorithms, in\nterms of computational efficiency and accuracy in matching skeletons. Our\nresults show that GORA-S attains a complexity that is significantly less than\nthat of any tested DTW method. In addition, it displays a favorable balance\nbetween speed and accuracy that remains invariant under changes in skeleton\nsampling frequency, lending it a degree of versatility that could make it\nwell-suited for a variety of action recognition tasks. \n\n"}
{"id": "1807.08259", "contents": "Title: Deep Discriminative Model for Video Classification Abstract: This paper presents a new deep learning approach for video-based scene\nclassification. We design a Heterogeneous Deep Discriminative Model (HDDM)\nwhose parameters are initialized by performing an unsupervised pre-training in\na layer-wise fashion using Gaussian Restricted Boltzmann Machines (GRBM). In\norder to avoid the redundancy of adjacent frames, we extract spatiotemporal\nvariation patterns within frames and represent them sparsely using Sparse Cubic\nSymmetrical Pattern (SCSP). Then, a pre-initialized HDDM is separately trained\nusing the videos of each class to learn class-specific models. According to the\nminimum reconstruction error from the learnt class-specific models, a weighted\nvoting strategy is employed for the classification. The performance of the\nproposed method is extensively evaluated on two action recognition datasets;\nUCF101 and Hollywood II, and three dynamic texture and dynamic scene datasets;\nDynTex, YUPENN, and Maryland. The experimental results and comparisons against\nstate-of-the-art methods demonstrate that the proposed method consistently\nachieves superior performance on all datasets. \n\n"}
{"id": "1807.08333", "contents": "Title: AutoLoc: Weakly-supervised Temporal Action Localization Abstract: Temporal Action Localization (TAL) in untrimmed video is important for many\napplications. But it is very expensive to annotate the segment-level ground\ntruth (action class and temporal boundary). This raises the interest of\naddressing TAL with weak supervision, namely only video-level annotations are\navailable during training). However, the state-of-the-art weakly-supervised TAL\nmethods only focus on generating good Class Activation Sequence (CAS) over time\nbut conduct simple thresholding on CAS to localize actions. In this paper, we\nfirst develop a novel weakly-supervised TAL framework called AutoLoc to\ndirectly predict the temporal boundary of each action instance. We propose a\nnovel Outer-Inner-Contrastive (OIC) loss to automatically discover the needed\nsegment-level supervision for training such a boundary predictor. Our method\nachieves dramatically improved performance: under the IoU threshold 0.5, our\nmethod improves mAP on THUMOS'14 from 13.7% to 21.2% and mAP on ActivityNet\nfrom 7.4% to 27.3%. It is also very encouraging to see that our\nweakly-supervised method achieves comparable results with some fully-supervised\nmethods. \n\n"}
{"id": "1807.08931", "contents": "Title: CReaM: Condensed Real-time Models for Depth Prediction using\n  Convolutional Neural Networks Abstract: Since the resurgence of CNNs the robotic vision community has developed a\nrange of algorithms that perform classification, semantic segmentation and\nstructure prediction (depths, normals, surface curvature) using neural\nnetworks. While some of these models achieve state-of-the art results and super\nhuman level performance, deploying these models in a time critical robotic\nenvironment remains an ongoing challenge. Real-time frameworks are of paramount\nimportance to build a robotic society where humans and robots integrate\nseamlessly. To this end, we present a novel real-time structure prediction\nframework that predicts depth at 30fps on an NVIDIA-TX2. At the time of\nwriting, this is the first piece of work to showcase such a capability on a\nmobile platform. We also demonstrate with extensive experiments that neural\nnetworks with very large model capacities can be leveraged in order to train\naccurate condensed model architectures in a \"from teacher to student\" style\nknowledge transfer. \n\n"}
{"id": "1807.09372", "contents": "Title: A Synchronized Stereo and Plenoptic Visual Odometry Dataset Abstract: We present a new dataset to evaluate monocular, stereo, and plenoptic camera\nbased visual odometry algorithms. The dataset comprises a set of synchronized\nimage sequences recorded by a micro lens array (MLA) based plenoptic camera and\na stereo camera system. For this, the stereo cameras and the plenoptic camera\nwere assembled on a common hand-held platform. All sequences are recorded in a\nvery large loop, where beginning and end show the same scene. Therefore, the\ntracking accuracy of a visual odometry algorithm can be measured from the drift\nbetween beginning and end of the sequence. For both, the plenoptic camera and\nthe stereo system, we supply full intrinsic camera models, as well as\nvignetting data. The dataset consists of 11 sequences which were recorded in\nchallenging indoor and outdoor scenarios. We present, by way of example, the\nresults achieved by state-of-the-art algorithms. \n\n"}
{"id": "1807.09413", "contents": "Title: 3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud\n  Registration Abstract: In this paper, we propose the 3DFeat-Net which learns both 3D feature\ndetector and descriptor for point cloud matching using weak supervision. Unlike\nmany existing works, we do not require manual annotation of matching point\nclusters. Instead, we leverage on alignment and attention mechanisms to learn\nfeature correspondences from GPS/INS tagged 3D point clouds without explicitly\nspecifying them. We create training and benchmark outdoor Lidar datasets, and\nexperiments show that 3DFeat-Net obtains state-of-the-art performance on these\ngravity-aligned datasets. \n\n"}
{"id": "1807.09828", "contents": "Title: ADVIO: An authentic dataset for visual-inertial odometry Abstract: The lack of realistic and open benchmarking datasets for pedestrian\nvisual-inertial odometry has made it hard to pinpoint differences in published\nmethods. Existing datasets either lack a full six degree-of-freedom\nground-truth or are limited to small spaces with optical tracking systems. We\ntake advantage of advances in pure inertial navigation, and develop a set of\nversatile and challenging real-world computer vision benchmark sets for\nvisual-inertial odometry. For this purpose, we have built a test rig equipped\nwith an iPhone, a Google Pixel Android phone, and a Google Tango device. We\nprovide a wide range of raw sensor data that is accessible on almost any\nmodern-day smartphone together with a high-quality ground-truth track. We also\ncompare resulting visual-inertial tracks from Google Tango, ARCore, and Apple\nARKit with two recent methods published in academic forums. The data sets cover\nboth indoor and outdoor cases, with stairs, escalators, elevators, office\nenvironments, a shopping mall, and metro station. \n\n"}
{"id": "1807.09986", "contents": "Title: Recurrent Fusion Network for Image Captioning Abstract: Recently, much advance has been made in image captioning, and an\nencoder-decoder framework has been adopted by all the state-of-the-art models.\nUnder this framework, an input image is encoded by a convolutional neural\nnetwork (CNN) and then translated into natural language with a recurrent neural\nnetwork (RNN). The existing models counting on this framework merely employ one\nkind of CNNs, e.g., ResNet or Inception-X, which describe image contents from\nonly one specific view point. Thus, the semantic meaning of an input image\ncannot be comprehensively understood, which restricts the performance of\ncaptioning. In this paper, in order to exploit the complementary information\nfrom multiple encoders, we propose a novel Recurrent Fusion Network (RFNet) for\ntackling image captioning. The fusion process in our model can exploit the\ninteractions among the outputs of the image encoders and then generate new\ncompact yet informative representations for the decoder. Experiments on the\nMSCOCO dataset demonstrate the effectiveness of our proposed RFNet, which sets\na new state-of-the-art for image captioning. \n\n"}
{"id": "1807.10002", "contents": "Title: Deep Pictorial Gaze Estimation Abstract: Estimating human gaze from natural eye images only is a challenging task.\nGaze direction can be defined by the pupil- and the eyeball center where the\nlatter is unobservable in 2D images. Hence, achieving highly accurate gaze\nestimates is an ill-posed problem. In this paper, we introduce a novel deep\nneural network architecture specifically designed for the task of gaze\nestimation from single eye input. Instead of directly regressing two angles for\nthe pitch and yaw of the eyeball, we regress to an intermediate pictorial\nrepresentation which in turn simplifies the task of 3D gaze direction\nestimation. Our quantitative and qualitative results show that our approach\nachieves higher accuracies than the state-of-the-art and is robust to variation\nin gaze, head pose and image quality. \n\n"}
{"id": "1807.10487", "contents": "Title: Pull Message Passing for Nonparametric Belief Propagation Abstract: We present a \"pull\" approach to approximate products of Gaussian mixtures\nwithin message updates for Nonparametric Belief Propagation (NBP) inference.\nExisting NBP methods often represent messages between continuous-valued latent\nvariables as Gaussian mixture models. To avoid computational intractability in\nloopy graphs, NBP necessitates an approximation of the product of such\nmixtures. Sampling-based product approximations have shown effectiveness for\nNBP inference. However, such approximations used within the traditional \"push\"\nmessage update procedures quickly become computationally prohibitive for\nmulti-modal distributions over high-dimensional variables. In contrast, we\npropose a \"pull\" method, as the Pull Message Passing for Nonparametric Belief\npropagation (PMPNBP) algorithm, and demonstrate its viability for efficient\ninference. We report results using an experiment from an existing NBP method,\nPAMPAS, for inferring the pose of an articulated structure in clutter. Results\nfrom this illustrative problem found PMPNBP has a greater ability to\nefficiently scale the number of components in its mixtures and, consequently,\nimprove inference accuracy. \n\n"}
{"id": "1807.10760", "contents": "Title: Deep nested level sets: Fully automated segmentation of cardiac MR\n  images in patients with pulmonary hypertension Abstract: In this paper we introduce a novel and accurate optimisation method for\nsegmentation of cardiac MR (CMR) images in patients with pulmonary hypertension\n(PH). The proposed method explicitly takes into account the image features\nlearned from a deep neural network. To this end, we estimate simultaneous\nprobability maps over region and edge locations in CMR images using a fully\nconvolutional network. Due to the distinct morphology of the heart in patients\nwith PH, these probability maps can then be incorporated in a single nested\nlevel set optimisation framework to achieve multi-region segmentation with high\nefficiency. The proposed method uses an automatic way for level set\ninitialisation and thus the whole optimisation is fully automated. We\ndemonstrate that the proposed deep nested level set (DNLS) method outperforms\nexisting state-of-the-art methods for CMR segmentation in PH patients. \n\n"}
{"id": "1807.11147", "contents": "Title: Occluded Joints Recovery in 3D Human Pose Estimation based on Distance\n  Matrix Abstract: Albeit the recent progress in single image 3D human pose estimation due to\nthe convolutional neural network, it is still challenging to handle real\nscenarios such as highly occluded scenes. In this paper, we propose to address\nthe problem of single image 3D human pose estimation with occluded measurements\nby exploiting the Euclidean distance matrix (EDM). Specifically, we present two\napproaches based on EDM, which could effectively handle occluded joints in 2D\nimages. The first approach is based on 2D-to-2D distance matrix regression\nachieved by a simple CNN architecture. The second approach is based on sparse\ncoding along with a learned over-complete dictionary. Experiments on the\nHuman3.6M dataset show the excellent performance of these two approaches in\nrecovering occluded observations and demonstrate the improvements in accuracy\nfor 3D human pose estimation with occluded joints. \n\n"}
{"id": "1807.11176", "contents": "Title: Human Motion Analysis with Deep Metric Learning Abstract: Effectively measuring the similarity between two human motions is necessary\nfor several computer vision tasks such as gait analysis, person identi-\nfication and action retrieval. Nevertheless, we believe that traditional\napproaches such as L2 distance or Dynamic Time Warping based on hand-crafted\nlocal pose metrics fail to appropriately capture the semantic relationship\nacross motions and, as such, are not suitable for being employed as metrics\nwithin these tasks. This work addresses this limitation by means of a\ntriplet-based deep metric learning specifically tailored to deal with human\nmotion data, in particular with the prob- lem of varying input size and\ncomputationally expensive hard negative mining due to motion pair alignment.\nSpecifically, we propose (1) a novel metric learn- ing objective based on a\ntriplet architecture and Maximum Mean Discrepancy; as well as, (2) a novel deep\narchitecture based on attentive recurrent neural networks. One benefit of our\nobjective function is that it enforces a better separation within the learned\nembedding space of the different motion categories by means of the associated\ndistribution moments. At the same time, our attentive recurrent neural network\nallows processing varying input sizes to a fixed size of embedding while\nlearning to focus on those motion parts that are semantically distinctive. Our\nex- periments on two different datasets demonstrate significant improvements\nover conventional human motion metrics. \n\n"}
{"id": "1807.11195", "contents": "Title: Multi-Fiber Networks for Video Recognition Abstract: In this paper, we aim to reduce the computational cost of spatio-temporal\ndeep neural networks, making them run as fast as their 2D counterparts while\npreserving state-of-the-art accuracy on video recognition benchmarks. To this\nend, we present the novel Multi-Fiber architecture that slices a complex neural\nnetwork into an ensemble of lightweight networks or fibers that run through the\nnetwork. To facilitate information flow between fibers we further incorporate\nmultiplexer modules and end up with an architecture that reduces the\ncomputational cost of 3D networks by an order of magnitude, while increasing\nrecognition performance at the same time. Extensive experimental results show\nthat our multi-fiber architecture significantly boosts the efficiency of\nexisting convolution networks for both image and video recognition tasks,\nachieving state-of-the-art performance on UCF-101, HMDB-51 and Kinetics\ndatasets. Our proposed model requires over 9x and 13x less computations than\nthe I3D and R(2+1)D models, respectively, yet providing higher accuracy. \n\n"}
{"id": "1807.11936", "contents": "Title: Gender Privacy: An Ensemble of Semi Adversarial Networks for Confounding\n  Arbitrary Gender Classifiers Abstract: Recent research has proposed the use of Semi Adversarial Networks (SAN) for\nimparting privacy to face images. SANs are convolutional autoencoders that\nperturb face images such that the perturbed images cannot be reliably used by\nan attribute classifier (e.g., a gender classifier) but can still be used by a\nface matcher for matching purposes. However, the generalizability of SANs\nacross multiple arbitrary gender classifiers has not been demonstrated in the\nliterature. In this work, we tackle the generalization issue by designing an\nensemble SAN model that generates a diverse set of perturbed outputs for a\ngiven input face image. This is accomplished by enforcing diversity among the\nindividual models in the ensemble through the use of different data\naugmentation techniques. The goal is to ensure that at least one of the\nperturbed output faces will confound an arbitrary, previously unseen gender\nclassifier. Extensive experiments using different unseen gender classifiers and\nface matchers are performed to demonstrate the efficacy of the proposed\nparadigm in imparting gender privacy to face images. \n\n"}
{"id": "1808.00313", "contents": "Title: A Network Structure to Explicitly Reduce Confusion Errors in Semantic\n  Segmentation Abstract: Confusing classes that are ubiquitous in real world often degrade performance\nfor many vision related applications like object detection, classification, and\nsegmentation. The confusion errors are not only caused by similar visual\npatterns but also amplified by various factors during the training of our\ndesigned models, such as reduced feature resolution in the encoding process or\nimbalanced data distributions. A large amount of deep learning based network\nstructures has been proposed in recent years to deal with these individual\nfactors and improve network performance. However, to our knowledge, no existing\nwork in semantic image segmentation is designed to tackle confusion errors\nexplicitly. In this paper, we present a novel and general network structure\nthat reduces confusion errors in more direct manner and apply the network for\nsemantic segmentation. There are two major contributions in our network\nstructure: 1) We ensemble subnets with heterogeneous output spaces based on the\ndiscriminative confusing groups. The training for each subnet can distinguish\nconfusing classes within the group without affecting unrelated classes outside\nthe group. 2) We propose an improved cross-entropy loss function that maximizes\nthe probability assigned to the correct class and penalizes the probabilities\nassigned to the confusing classes at the same time. Our network structure is a\ngeneral structure and can be easily adapted to any other networks to further\nreduce confusion errors. Without any changes in the feature encoder and\npost-processing steps, our experiments demonstrate consistent and significant\nimprovements on different baseline models on Cityscapes and PASCAL VOC datasets\n(e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38). \n\n"}
{"id": "1808.00793", "contents": "Title: Weakly Supervised Localisation for Fetal Ultrasound Images Abstract: This paper addresses the task of detecting and localising fetal anatomical\nregions in 2D ultrasound images, where only image-level labels are present at\ntraining, i.e. without any localisation or segmentation information. We examine\nthe use of convolutional neural network architectures coupled with soft\nproposal layers. The resulting network simultaneously performs anatomical\nregion detection (classification) and localisation tasks. We generate a\nproposal map describing the attention of the network for a particular class.\nThe network is trained on 85,500 2D fetal Ultrasound images and their\nassociated labels. Labels correspond to six anatomical regions: head, spine,\nthorax, abdomen, limbs, and placenta. Detection achieves an average accuracy of\n90\\% on individual regions, and show that the proposal maps correlate well with\nrelevant anatomical structures. This work presents itself as a powerful and\nessential step towards subsequent tasks such as fetal position and pose\nestimation, organ-specific segmentation, or image-guided navigation. Code and\nadditional material is available at https://ntoussaint.github.io/fetalnav \n\n"}
{"id": "1808.01597", "contents": "Title: Pixel-level Semantics Guided Image Colorization Abstract: While many image colorization algorithms have recently shown the capability\nof producing plausible color versions from gray-scale photographs, they still\nsuffer from the problems of context confusion and edge color bleeding. To\naddress context confusion, we propose to incorporate the pixel-level object\nsemantics to guide the image colorization. The rationale is that human beings\nperceive and distinguish colors based on the object's semantic categories. We\npropose a hierarchical neural network with two branches. One branch learns what\nthe object is while the other branch learns the object's colors. The network\njointly optimizes a semantic segmentation loss and a colorization loss. To\nattack edge color bleeding we generate more continuous color maps with sharp\nedges by adopting a joint bilateral upsamping layer at inference. Our network\nis trained on PASCAL VOC2012 and COCO-stuff with semantic segmentation labels\nand it produces more realistic and finer results compared to the colorization\nstate-of-the-art. \n\n"}
{"id": "1808.01623", "contents": "Title: Multi-Scale Supervised Network for Human Pose Estimation Abstract: Human pose estimation is an important topic in computer vision with many\napplications including gesture and activity recognition. However, pose\nestimation from image is challenging due to appearance variations, occlusions,\nclutter background, and complex activities. To alleviate these problems, we\ndevelop a robust pose estimation method based on the recent deep conv-deconv\nmodules with two improvements: (1) multi-scale supervision of body keypoints,\nand (2) a global regression to improve structural consistency of keypoints. We\nrefine keypoint detection heatmaps using layer-wise multi-scale supervision to\nbetter capture local contexts. Pose inference via keypoint association is\noptimized globally using a regression network at the end. Our method can\neffectively disambiguate keypoint matches in close proximity including the\nmismatch of left-right body parts, and better infer occluded parts.\nExperimental results show that our method achieves competitive performance\namong state-of-the-art methods on the MPII and FLIC datasets. \n\n"}
{"id": "1808.01634", "contents": "Title: Self-Attention Recurrent Network for Saliency Detection Abstract: Feature maps in deep neural network generally contain different semantics.\nExisting methods often omit their characteristics that may lead to sub-optimal\nresults. In this paper, we propose a novel end-to-end deep saliency network\nwhich could effectively utilize multi-scale feature maps according to their\ncharacteristics. Shallow layers often contain more local information, and deep\nlayers have advantages in global semantics. Therefore, the network generates\nelaborate saliency maps by enhancing local and global information of feature\nmaps in different layers. On one hand, local information of shallow layers is\nenhanced by a recurrent structure which shared convolution kernel at different\ntime steps. On the other hand, global information of deep layers is utilized by\na self-attention module, which generates different attention weights for\nsalient objects and backgrounds thus achieve better performance. Experimental\nresults on four widely used datasets demonstrate that our method has advantages\nin performance over existing algorithms. \n\n"}
{"id": "1808.01676", "contents": "Title: A Multi-task Framework for Skin Lesion Detection and Segmentation Abstract: Early detection and segmentation of skin lesions is crucial for timely\ndiagnosis and treatment, necessary to improve the survival rate of patients.\nHowever, manual delineation is time consuming and subject to intra- and\ninter-observer variations among dermatologists. This underlines the need for an\naccurate and automatic approach to skin lesion segmentation. To tackle this\nissue, we propose a multi-task convolutional neural network (CNN) based, joint\ndetection and segmentation framework, designed to initially localize the lesion\nand subsequently, segment it. A `Faster region-based convolutional neural\nnetwork' (Faster-RCNN) which comprises a region proposal network (RPN), is used\nto generate bounding boxes/region proposals, for lesion localization in each\nimage. The proposed regions are subsequently refined using a softmax classifier\nand a bounding-box regressor. The refined bounding boxes are finally cropped\nand segmented using `SkinNet', a modified version of U-Net. We trained and\nevaluated the performance of our network, using the ISBI 2017 challenge and the\nPH2 datasets, and compared it with the state-of-the-art, using the official\ntest data released as part of the challenge for the former. Our approach\noutperformed others in terms of Dice coefficients ($>0.93$), Jaccard index\n($>0.88$), accuracy ($>0.96$) and sensitivity ($>0.95$), across five-fold cross\nvalidation experiments. \n\n"}
{"id": "1808.01911", "contents": "Title: Where-and-When to Look: Deep Siamese Attention Networks for Video-based\n  Person Re-identification Abstract: Video-based person re-identification (re-id) is a central application in\nsurveillance systems with significant concern in security. Matching persons\nacross disjoint camera views in their video fragments is inherently challenging\ndue to the large visual variations and uncontrolled frame rates. There are two\nsteps crucial to person re-id, namely discriminative feature learning and\nmetric learning. However, existing approaches consider the two steps\nindependently, and they do not make full use of the temporal and spatial\ninformation in videos. In this paper, we propose a Siamese attention\narchitecture that jointly learns spatiotemporal video representations and their\nsimilarity metrics. The network extracts local convolutional features from\nregions of each frame, and enhance their discriminative capability by focusing\non distinct regions when measuring the similarity with another pedestrian\nvideo. The attention mechanism is embedded into spatial gated recurrent units\nto selectively propagate relevant features and memorize their spatial\ndependencies through the network. The model essentially learns which parts\n(\\emph{where}) from which frames (\\emph{when}) are relevant and distinctive for\nmatching persons and attaches higher importance therein. The proposed Siamese\nmodel is end-to-end trainable to jointly learn comparable hidden\nrepresentations for paired pedestrian videos and their similarity value.\nExtensive experiments on three benchmark datasets show the effectiveness of\neach component of the proposed deep network while outperforming\nstate-of-the-art methods. \n\n"}
{"id": "1808.01944", "contents": "Title: V-FCNN: Volumetric Fully Convolution Neural Network For Automatic Atrial\n  Segmentation Abstract: Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder\nthat causes changes in the anatomy of the atria. A better characterization of\nthese changes is desirable for the definition of clinical biomarkers,\nfurthermore, thus there is a need for its fully automatic segmentation from\nclinical images. In this work, we present an architecture based on\n3D-convolution kernels, a Volumetric Fully Convolution Neural Network (V-FCNN),\nable to segment the entire volume in a one-shot, and consequently integrate the\nimplicit spatial redundancy present in high-resolution images. A loss function\nbased on the mixture of both Mean Square Error (MSE) and Dice Loss (DL) is\nused, in an attempt to combine the ability to capture the bulk shape as well as\nthe reduction of local errors products by over-segmentation. Results\ndemonstrate a reasonable performance in the middle region of the atria along\nwith the impact of the challenges of capturing the variability of the pulmonary\nveins or the identification of the valve plane that separates the atria to the\nventricle. A final dice of $92.5\\%$ in $54$ patients ($4752$ atria test slices\nin total) is shown. \n\n"}
{"id": "1808.02310", "contents": "Title: Effects of periodic forcing on a Paleoclimate delay model Abstract: We present a study of a delay differential equation (DDE) model for the\nMid-Pleistocene Transition (MPT). We investigate the behavior of the model when\nsubjected to periodic forcing. The unforced model has a bistable region\nconsisting of a stable equilibrium along with a large amplitude stable periodic\norbit. We study how forcing affects solutions in this region. Forcing based on\nastronomical data causes a sudden transition in time and under increase of the\nforcing amplitude, moving the model response from a non-MPT regime to an MPT\nregime. Similar transition behavior is found for periodic forcing. A\nbifurcation analysis shows that the transition is not due to a bifurcation but\ninstead to a shifting basin of attraction. While determining the basin boundary\nwe demonstrate how one can accurately compute the intersection of a stable\nmanifold of a saddle with a slow manifold in a DDE by embedding the algorithm\nfor planar maps proposed by England et al. (SIADS 2004(3)) into the\nequation-free framework by Kevrekidis et al. (Rev. Phys. Chem. 2009 (60)). \n\n"}
{"id": "1808.03766", "contents": "Title: The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary Abstract: The 3rd annual installment of the ActivityNet Large- Scale Activity\nRecognition Challenge, held as a full-day workshop in CVPR 2018, focused on the\nrecognition of daily life, high-level, goal-oriented activities from\nuser-generated videos as those found in internet video portals. The 2018\nchallenge hosted six diverse tasks which aimed to push the limits of semantic\nvisual understanding of videos as well as bridge visual content with human\ncaptions. Three out of the six tasks were based on the ActivityNet dataset,\nwhich was introduced in CVPR 2015 and organized hierarchically in a semantic\ntaxonomy. These tasks focused on tracing evidence of activities in time in the\nform of proposals, class labels, and captions. In this installment of the\nchallenge, we hosted three guest tasks to enrich the understanding of visual\ninformation in videos. The guest tasks focused on complementary aspects of the\nactivity recognition problem at large scale and involved three challenging and\nrecently compiled datasets: the Kinetics-600 dataset from Google DeepMind, the\nAVA dataset from Berkeley and Google, and the Moments in Time dataset from MIT\nand IBM Research. \n\n"}
{"id": "1808.03959", "contents": "Title: Open-World Stereo Video Matching with Deep RNN Abstract: Deep Learning based stereo matching methods have shown great successes and\nachieved top scores across different benchmarks. However, like most data-driven\nmethods, existing deep stereo matching networks suffer from some well-known\ndrawbacks such as requiring large amount of labeled training data, and that\ntheir performances are fundamentally limited by the generalization ability. In\nthis paper, we propose a novel Recurrent Neural Network (RNN) that takes a\ncontinuous (possibly previously unseen) stereo video as input, and directly\npredicts a depth-map at each frame without a pre-training process, and without\nthe need of ground-truth depth-maps as supervision. Thanks to the recurrent\nnature (provided by two convolutional-LSTM blocks), our network is able to\nmemorize and learn from its past experiences, and modify its inner parameters\n(network weights) to adapt to previously unseen or unfamiliar environments.\nThis suggests a remarkable generalization ability of the net, making it\napplicable in an {\\em open world} setting. Our method works robustly with\nchanges in scene content, image statistics, and lighting and season conditions\n{\\em etc}. By extensive experiments, we demonstrate that the proposed method\nseamlessly adapts between different scenarios. Equally important, in terms of\nthe stereo matching accuracy, it outperforms state-of-the-art deep stereo\napproaches on standard benchmark datasets such as KITTI and Middlebury stereo. \n\n"}
{"id": "1808.04303", "contents": "Title: Rank-1 Convolutional Neural Network Abstract: In this paper, we propose a convolutional neural network(CNN) with 3-D rank-1\nfilters which are composed by the outer product of 1-D filters. After being\ntrained, the 3-D rank-1 filters can be decomposed into 1-D filters in the test\ntime for fast inference. The reason that we train 3-D rank-1 filters in the\ntraining stage instead of consecutive 1-D filters is that a better gradient\nflow can be obtained with this setting, which makes the training possible even\nin the case where the network with consecutive 1-D filters cannot be trained.\nThe 3-D rank-1 filters are updated by both the gradient flow and the outer\nproduct of the 1-D filters in every epoch, where the gradient flow tries to\nobtain a solution which minimizes the loss function, while the outer product\noperation tries to make the parameters of the filter to live on a rank-1\nsub-space. Furthermore, we show that the convolution with the rank-1 filters\nresults in low rank outputs, constraining the final output of the CNN also to\nlive on a low dimensional subspace. \n\n"}
{"id": "1808.04456", "contents": "Title: Multimodal Deep Neural Networks using Both Engineered and Learned\n  Representations for Biodegradability Prediction Abstract: Deep learning algorithms excel at extracting patterns from raw data, and with\nlarge datasets, they have been very successful in computer vision and natural\nlanguage applications. However, in other domains, large datasets on which to\nlearn representations from may not exist. In this work, we develop a novel\nmultimodal CNN-MLP neural network architecture that utilizes both\ndomain-specific feature engineering as well as learned representations from raw\ndata. We illustrate the effectiveness of such network designs in the chemical\nsciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP\nnetwork is more accurate than either standalone network designs, and achieves\nan error classification rate of 0.125 that is 27% lower than the current\nstate-of-the-art. Thus, our work indicates that combining traditional feature\nengineering with representation learning can be effective, particularly in\nsituations where labeled data is limited. \n\n"}
{"id": "1808.04848", "contents": "Title: URSA: A Neural Network for Unordered Point Clouds Using Constellations Abstract: This paper describes a neural network layer, named Ursa, that uses a\nconstellation of points to learn classification information from point cloud\ndata. Unlike other machine learning classification problems where the task is\nto classify an individual high-dimensional observation, in a point-cloud\nclassification problem the goal is to classify a set of d-dimensional\nobservations. Because a point cloud is a set, there is no ordering to the\ncollection of points in a point-cloud classification problem. Thus, the\nchallenge of classifying point clouds inputs is in building a classifier which\nis agnostic to the ordering of the observations, yet preserves the\nd-dimensional information of each point in the set. This research presents\nUrsa, a new layer type for an artificial neural network which achieves these\ntwo properties. Similar to new methods for this task, this architecture works\ndirectly on d-dimensional points rather than first converting the points to a\nd-dimensional volume. The Ursa layer is followed by a series of dense layers to\nclassify 2D and 3D objects from point clouds. Experiments on ModelNet40 and\nMNIST data show classification results comparable with current methods, while\nreducing the training parameters by over 50 percent. \n\n"}
{"id": "1808.05331", "contents": "Title: On the Convergence of Learning-based Iterative Methods for Nonconvex\n  Inverse Problems Abstract: Numerous tasks at the core of statistics, learning and vision areas are\nspecific cases of ill-posed inverse problems. Recently, learning-based (e.g.,\ndeep) iterative methods have been empirically shown to be useful for these\nproblems. Nevertheless, integrating learnable structures into iterations is\nstill a laborious process, which can only be guided by intuitions or empirical\ninsights. Moreover, there is a lack of rigorous analysis about the convergence\nbehaviors of these reimplemented iterations, and thus the significance of such\nmethods is a little bit vague. This paper moves beyond these limits and\nproposes Flexible Iterative Modularization Algorithm (FIMA), a generic and\nprovable paradigm for nonconvex inverse problems. Our theoretical analysis\nreveals that FIMA allows us to generate globally convergent trajectories for\nlearning-based iterative methods. Meanwhile, the devised scheduling policies on\nflexible modules should also be beneficial for classical numerical methods in\nthe nonconvex scenario. Extensive experiments on real applications verify the\nsuperiority of FIMA. \n\n"}
{"id": "1808.05517", "contents": "Title: Network Decoupling: From Regular to Depthwise Separable Convolutions Abstract: Depthwise separable convolution has shown great efficiency in network design,\nbut requires time-consuming training procedure with full training-set\navailable. This paper first analyzes the mathematical relationship between\nregular convolutions and depthwise separable convolutions, and proves that the\nformer one could be approximated with the latter one in closed form. We show\ndepthwise separable convolutions are principal components of regular\nconvolutions. And then we propose network decoupling (ND), a training-free\nmethod to accelerate convolutional neural networks (CNNs) by transferring\npre-trained CNN models into the MobileNet-like depthwise separable convolution\nstructure, with a promising speedup yet negligible accuracy loss. We further\nverify through experiments that the proposed method is orthogonal to other\ntraining-free methods like channel decomposition, spatial decomposition, etc.\nCombining the proposed method with them will bring even larger CNN speedup. For\ninstance, ND itself achieves about 2X speedup for the widely used VGG16, and\ncombined with other methods, it reaches 3.7X speedup with graceful accuracy\ndegradation. We demonstrate that ND is widely applicable to classification\nnetworks like ResNet, and object detection network like SSD300. \n\n"}
{"id": "1808.05560", "contents": "Title: R$^3$-Net: A Deep Network for Multi-oriented Vehicle Detection in Aerial\n  Images and Videos Abstract: Vehicle detection is a significant and challenging task in aerial remote\nsensing applications. Most existing methods detect vehicles with regular\nrectangle boxes and fail to offer the orientation of vehicles. However, the\norientation information is crucial for several practical applications, such as\nthe trajectory and motion estimation of vehicles. In this paper, we propose a\nnovel deep network, called rotatable region-based residual network (R$^3$-Net),\nto detect multi-oriented vehicles in aerial images and videos. More specially,\nR$^3$-Net is utilized to generate rotatable rectangular target boxes in a half\ncoordinate system. First, we use a rotatable region proposal network (R-RPN) to\ngenerate rotatable region of interests (R-RoIs) from feature maps produced by a\ndeep convolutional neural network. Here, a proposed batch averaging rotatable\nanchor (BAR anchor) strategy is applied to initialize the shape of vehicle\ncandidates. Next, we propose a rotatable detection network (R-DN) for the final\nclassification and regression of the R-RoIs. In R-DN, a novel rotatable\nposition sensitive pooling (R-PS pooling) is designed to keep the position and\norientation information simultaneously while downsampling the feature maps of\nR-RoIs. In our model, R-RPN and R-DN can be trained jointly. We test our\nnetwork on two open vehicle detection image datasets, namely DLR 3K Munich\nDataset and VEDAI Dataset, demonstrating the high precision and robustness of\nour method. In addition, further experiments on aerial videos show the good\ngeneralization capability of the proposed method and its potential for vehicle\ntracking in aerial videos. The demo video is available at\nhttps://youtu.be/xCYD-tYudN0. \n\n"}
{"id": "1808.06753", "contents": "Title: Estimating Metric Poses of Dynamic Objects Using Monocular\n  Visual-Inertial Fusion Abstract: A monocular 3D object tracking system generally has only up-to-scale pose\nestimation results without any prior knowledge of the tracked object. In this\npaper, we propose a novel idea to recover the metric scale of an arbitrary\ndynamic object by optimizing the trajectory of the objects in the world frame,\nwithout motion assumptions. By introducing an additional constraint in the time\ndomain, our monocular visual-inertial tracking system can obtain continuous six\ndegree of freedom (6-DoF) pose estimation without scale ambiguity. Our method\nrequires neither fixed multi-camera nor depth sensor settings for scale\nobservability, instead, the IMU inside the monocular sensing suite provides\nscale information for both camera itself and the tracked object. We build the\nproposed system on top of our monocular visual-inertial system (VINS) to obtain\naccurate state estimation of the monocular camera in the world frame. The whole\nsystem consists of a 2D object tracker, an object region-based visual bundle\nadjustment (BA), VINS and a correlation analysis-based metric scale estimator.\nExperimental comparisons with ground truth demonstrate the tracking accuracy of\nour 3D tracking performance while a mobile augmented reality (AR) demo shows\nthe feasibility of potential applications. \n\n"}
{"id": "1808.07258", "contents": "Title: Escaping from Collapsing Modes in a Constrained Space Abstract: Generative adversarial networks (GANs) often suffer from unpredictable\nmode-collapsing during training. We study the issue of mode collapse of\nBoundary Equilibrium Generative Adversarial Network (BEGAN), which is one of\nthe state-of-the-art generative models. Despite its potential of generating\nhigh-quality images, we find that BEGAN tends to collapse at some modes after a\nperiod of training. We propose a new model, called \\emph{BEGAN with a\nConstrained Space} (BEGAN-CS), which includes a latent-space constraint in the\nloss function. We show that BEGAN-CS can significantly improve training\nstability and suppress mode collapse without either increasing the model\ncomplexity or degrading the image quality. Further, we visualize the\ndistribution of latent vectors to elucidate the effect of latent-space\nconstraint. The experimental results show that our method has additional\nadvantages of being able to train on small datasets and to generate images\nsimilar to a given real image yet with variations of designated attributes\non-the-fly. \n\n"}
{"id": "1808.07272", "contents": "Title: Deep Adaptive Temporal Pooling for Activity Recognition Abstract: Deep neural networks have recently achieved competitive accuracy for human\nactivity recognition. However, there is room for improvement, especially in\nmodeling long-term temporal importance and determining the activity relevance\nof different temporal segments in a video. To address this problem, we propose\na learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP).\nDATP applies a self-attention mechanism to adaptively pool the classification\nscores of different video segments. Specifically, using frame-level features,\nDATP regresses importance of different temporal segments and generates weights\nfor them. Remarkably, DATP is trained using only the video-level label. There\nis no need of additional supervision except video-level activity class label.\nWe conduct extensive experiments to investigate various input features and\ndifferent weight models. Experimental results show that DATP can learn to\nassign large weights to key video segments. More importantly, DATP can improve\ntraining of frame-level feature extractor. This is because relevant temporal\nsegments are assigned large weights during back-propagation. Overall, we\nachieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets. \n\n"}
{"id": "1808.07535", "contents": "Title: Learning Hierarchical Semantic Image Manipulation through Structured\n  Representations Abstract: Understanding, reasoning, and manipulating semantic concepts of images have\nbeen a fundamental research problem for decades. Previous work mainly focused\non direct manipulation on natural image manifold through color strokes,\nkey-points, textures, and holes-to-fill. In this work, we present a novel\nhierarchical framework for semantic image manipulation. Key to our hierarchical\nframework is that we employ a structured semantic layout as our intermediate\nrepresentation for manipulation. Initialized with coarse-level bounding boxes,\nour structure generator first creates pixel-wise semantic layout capturing the\nobject shape, object-object interactions, and object-scene relations. Then our\nimage generator fills in the pixel-level textures guided by the semantic\nlayout. Such framework allows a user to manipulate images at object-level by\nadding, removing, and moving one bounding box at a time. Experimental\nevaluations demonstrate the advantages of the hierarchical manipulation\nframework over existing image generation and context hole-filing models, both\nqualitatively and quantitatively. Benefits of the hierarchical framework are\nfurther demonstrated in applications such as semantic object manipulation,\ninteractive image editing, and data-driven image manipulation. \n\n"}
{"id": "1808.07659", "contents": "Title: PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for\n  3D Shape Recognition Abstract: 3D object recognition has attracted wide research attention in the field of\nmultimedia and computer vision. With the recent proliferation of deep learning,\nvarious deep models with different representations have achieved the\nstate-of-the-art performance. Among them, point cloud and multi-view based 3D\nshape representations are promising recently, and their corresponding deep\nmodels have shown significant performance on 3D shape recognition. However,\nthere is little effort concentrating point cloud data and multi-view data for\n3D shape representation, which is, in our consideration, beneficial and\ncompensated to each other. In this paper, we propose the Point-View Network\n(PVNet), the first framework integrating both the point cloud and the\nmulti-view data towards joint 3D shape recognition. More specifically, an\nembedding attention fusion scheme is proposed that could employ high-level\nfeatures from the multi-view data to model the intrinsic correlation and\ndiscriminability of different structure features from the point cloud data. In\nparticular, the discriminative descriptions are quantified and leveraged as the\nsoft attention mask to further refine the structure feature of the 3D shape. We\nhave evaluated the proposed method on the ModelNet40 dataset for 3D shape\nclassification and retrieval tasks. Experimental results and comparisons with\nstate-of-the-art methods demonstrate that our framework can achieve superior\nperformance. \n\n"}
{"id": "1808.08282", "contents": "Title: Controlling Over-generalization and its Effect on Adversarial Examples\n  Generation and Detection Abstract: Convolutional Neural Networks (CNNs) significantly improve the\nstate-of-the-art for many applications, especially in computer vision. However,\nCNNs still suffer from a tendency to confidently classify out-distribution\nsamples from unknown classes into pre-defined known classes. Further, they are\nalso vulnerable to adversarial examples. We are relating these two issues\nthrough the tendency of CNNs to over-generalize for areas of the input space\nnot covered well by the training set. We show that a CNN augmented with an\nextra output class can act as a simple yet effective end-to-end model for\ncontrolling over-generalization. As an appropriate training set for the extra\nclass, we introduce two resources that are computationally efficient to obtain:\na representative natural out-distribution set and interpolated in-distribution\nsamples. To help select a representative natural out-distribution set among\navailable ones, we propose a simple measurement to assess an out-distribution\nset's fitness. We also demonstrate that training such an augmented CNN with\nrepresentative out-distribution natural datasets and some interpolated samples\nallows it to better handle a wide range of unseen out-distribution samples and\nblack-box adversarial examples without training it on any adversaries. Finally,\nwe show that generation of white-box adversarial attacks using our proposed\naugmented CNN can become harder, as the attack algorithms have to get around\nthe rejection regions when generating actual adversaries. \n\n"}
{"id": "1808.08378", "contents": "Title: Fusion++: Volumetric Object-Level SLAM Abstract: We propose an online object-level SLAM system which builds a persistent and\naccurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera\nbrowses a cluttered indoor scene, Mask-RCNN instance segmentations are used to\ninitialise compact per-object Truncated Signed Distance Function (TSDF)\nreconstructions with object size-dependent resolutions and a novel 3D\nforeground mask. Reconstructed objects are stored in an optimisable 6DoF pose\ngraph which is our only persistent map representation. Objects are\nincrementally refined via depth fusion, and are used for tracking,\nrelocalisation and loop closure detection. Loop closures cause adjustments in\nthe relative pose estimates of object instances, but no intra-object warping.\nEach object also carries semantic information which is refined over time and an\nexistence probability to account for spurious instance predictions. We\ndemonstrate our approach on a hand-held RGB-D sequence from a cluttered office\nscene with a large number and variety of object instances, highlighting how the\nsystem closes loops and makes good use of existing objects on repeated loops.\nWe quantitatively evaluate the trajectory error of our system against a\nbaseline approach on the RGB-D SLAM benchmark, and qualitatively compare\nreconstruction quality of discovered objects on the YCB video dataset.\nPerformance evaluation shows our approach is highly memory efficient and runs\nonline at 4-8Hz (excluding relocalisation) despite not being optimised at the\nsoftware level. \n\n"}
{"id": "1808.08802", "contents": "Title: Discriminative Representation Combinations for Accurate Face Spoofing\n  Detection Abstract: Three discriminative representations for face presentation attack detection\nare introduced in this paper. Firstly we design a descriptor called spatial\npyramid coding micro-texture (SPMT) feature to characterize local appearance\ninformation. Secondly we utilize the SSD, which is a deep learning framework\nfor detection, to excavate context cues and conduct end-to-end face\npresentation attack detection. Finally we design a descriptor called template\nface matched binocular depth (TFBD) feature to characterize stereo structures\nof real and fake faces. For accurate presentation attack detection, we also\ndesign two kinds of representation combinations. Firstly, we propose a\ndecision-level cascade strategy to combine SPMT with SSD. Secondly, we use a\nsimple score fusion strategy to combine face structure cues (TFBD) with local\nmicro-texture features (SPMT). To demonstrate the effectiveness of our design,\nwe evaluate the representation combination of SPMT and SSD on three public\ndatasets, which outperforms all other state-of-the-art methods. In addition, we\nevaluate the representation combination of SPMT and TFBD on our dataset and\nexcellent performance is also achieved. \n\n"}
{"id": "1808.08885", "contents": "Title: Improved Breast Mass Segmentation in Mammograms with Conditional\n  Residual U-net Abstract: We explore the use of deep learning for breast mass segmentation in\nmammograms. By integrating the merits of residual learning and probabilistic\ngraphical modelling with standard U-Net, we propose a new deep network,\nConditional Residual U-Net (CRU-Net), to improve the U-Net segmentation\nperformance. Benefiting from the advantage of probabilistic graphical modelling\nin the pixel-level labelling, and the structure insights of a deep residual\nnetwork in the feature extraction, the CRU-Net provides excellent mass\nsegmentation performance. Evaluations based on INbreast and DDSM-BCRP datasets\ndemonstrate that the CRU-Net achieves the best mass segmentation performance\ncompared to the state-of-art methodologies. Moreover, neither tedious\npre-processing nor post-processing techniques are not required in our\nalgorithm. \n\n"}
{"id": "1808.08993", "contents": "Title: Open Set Chinese Character Recognition using Multi-typed Attributes Abstract: Recognition of Off-line Chinese characters is still a challenging problem,\nespecially in historical documents, not only in the number of classes extremely\nlarge in comparison to contemporary image retrieval methods, but also new\nunseen classes can be expected under open learning conditions (even for CNN).\nChinese character recognition with zero or a few training samples is a\ndifficult problem and has not been studied yet. In this paper, we propose a new\nChinese character recognition method by multi-type attributes, which are based\non pronunciation, structure and radicals of Chinese characters, applied to\ncharacter recognition in historical books. This intermediate attribute code has\na strong advantage over the common `one-hot' class representation because it\nallows for understanding complex and unseen patterns symbolically using\nattributes. First, each character is represented by four groups of attribute\ntypes to cover a wide range of character possibilities: Pinyin label, layout\nstructure, number of strokes, three different input methods such as Cangjie,\nZhengma and Wubi, as well as a four-corner encoding method. A convolutional\nneural network (CNN) is trained to learn these attributes. Subsequently,\ncharacters can be easily recognized by these attributes using a distance metric\nand a complete lexicon that is encoded in attribute space. We evaluate the\nproposed method on two open data sets: printed Chinese character recognition\nfor zero-shot learning, historical characters for few-shot learning and a\nclosed set: handwritten Chinese characters. Experimental results show a good\ngeneral classification of seen classes but also a very promising generalization\nability to unseen characters. \n\n"}
{"id": "1808.09829", "contents": "Title: MACNet: Multi-scale Atrous Convolution Networks for Food Places\n  Classification in Egocentric Photo-streams Abstract: First-person (wearable) camera continually captures unscripted interactions\nof the camera user with objects, people, and scenes reflecting his personal and\nrelational tendencies. One of the preferences of people is their interaction\nwith food events. The regulation of food intake and its duration has a great\nimportance to protect against diseases. Consequently, this work aims to develop\na smart model that is able to determine the recurrences of a person on food\nplaces during a day. This model is based on a deep end-to-end model for\nautomatic food places recognition by analyzing egocentric photo-streams. In\nthis paper, we apply multi-scale Atrous convolution networks to extract the key\nfeatures related to food places of the input images. The proposed model is\nevaluated on an in-house private dataset called \"EgoFoodPlaces\". Experimental\nresults shows promising results of food places classification recognition in\negocentric photo-streams. \n\n"}
{"id": "1809.00716", "contents": "Title: InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes\n  Dataset Abstract: Datasets have gained an enormous amount of popularity in the computer vision\ncommunity, from training and evaluation of Deep Learning-based methods to\nbenchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt,\nsynthetic imagery bears a vast potential due to scalability in terms of amounts\nof data obtainable without tedious manual ground truth annotations or\nmeasurements. Here, we present a dataset with the aim of providing a higher\ndegree of photo-realism, larger scale, more variability as well as serving a\nwider range of purposes compared to existing datasets. Our dataset leverages\nthe availability of millions of professional interior designs and millions of\nproduction-level furniture and object assets -- all coming with fine geometric\ndetails and high-resolution texture. We render high-resolution and high\nframe-rate video sequences following realistic trajectories while supporting\nvarious camera types as well as providing inertial measurements. Together with\nthe release of the dataset, we will make executable program of our interactive\nsimulator software as well as our renderer available at\nhttps://interiornetdataset.github.io. To showcase the usability and uniqueness\nof our dataset, we show benchmarking results of both sparse and dense SLAM\nalgorithms. \n\n"}
{"id": "1809.01564", "contents": "Title: Traffic Density Estimation using a Convolutional Neural Network Abstract: The goal of this project is to introduce and present a machine learning\napplication that aims to improve the quality of life of people in Singapore. In\nparticular, we investigate the use of machine learning solutions to tackle the\nproblem of traffic congestion in Singapore. In layman's terms, we seek to make\nSingapore (or any other city) a smoother place. To accomplish this aim, we\npresent an end-to-end system comprising of 1. A traffic density estimation\nalgorithm at traffic lights/junctions and 2. a suitable traffic signal control\nalgorithms that make use of the density information for better traffic control.\nTraffic density estimation can be obtained from traffic junction images using\nvarious machine learning techniques (combined with CV tools). After research\ninto various advanced machine learning methods, we decided on convolutional\nneural networks (CNNs). We conducted experiments on our algorithms, using the\npublicly available traffic camera dataset published by the Land Transport\nAuthority (LTA) to demonstrate the feasibility of this approach. With these\ntraffic density estimates, different traffic algorithms can be applied to\nminimize congestion at traffic junctions in general. \n\n"}
{"id": "1809.01816", "contents": "Title: Visual Coreference Resolution in Visual Dialog using Neural Module\n  Networks Abstract: Visual dialog entails answering a series of questions grounded in an image,\nusing dialog history as context. In addition to the challenges found in visual\nquestion answering (VQA), which can be seen as one-round dialog, visual dialog\nencompasses several more. We focus on one such problem called visual\ncoreference resolution that involves determining which words, typically noun\nphrases and pronouns, co-refer to the same entity/object instance in an image.\nThis is crucial, especially for pronouns (e.g., `it'), as the dialog agent must\nfirst link it to a previous coreference (e.g., `boat'), and only then can rely\non the visual grounding of the coreference `boat' to reason about the pronoun\n`it'. Prior work (in visual dialog) models visual coreference resolution either\n(a) implicitly via a memory network over history, or (b) at a coarse level for\nthe entire question; and not explicitly at a phrase level of granularity. In\nthis work, we propose a neural module network architecture for visual dialog by\nintroducing two novel modules - Refer and Exclude - that perform explicit,\ngrounded, coreference resolution at a finer word level. We demonstrate the\neffectiveness of our model on MNIST Dialog, a visually simple yet\ncoreference-wise complex dataset, by achieving near perfect accuracy, and on\nVisDial, a large and challenging visual dialog dataset on real images, where\nour model outperforms other approaches, and is more interpretable, grounded,\nand consistent qualitatively. \n\n"}
{"id": "1809.01943", "contents": "Title: Cascaded Mutual Modulation for Visual Reasoning Abstract: Visual reasoning is a special visual question answering problem that is\nmulti-step and compositional by nature, and also requires intensive text-vision\ninteractions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end\nvisual reasoning model. CMM includes a multi-step comprehension process for\nboth question and image. In each step, we use a Feature-wise Linear Modulation\n(FiLM) technique to enable textual/visual pipeline to mutually control each\nother. Experiments show that CMM significantly outperforms most related models,\nand reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR,\ncollected from both synthetic and natural languages. Ablation studies confirm\nthat both our multistep framework and our visual-guided language modulation are\ncritical to the task. Our code is available at\nhttps://github.com/FlamingHorizon/CMM-VR. \n\n"}
{"id": "1809.01995", "contents": "Title: Dense Pose Transfer Abstract: In this work we integrate ideas from surface-based modeling with neural\nsynthesis: we propose a combination of surface-based pose estimation and deep\ngenerative models that allows us to perform accurate pose transfer, i.e.\nsynthesize a new image of a person based on a single image of that person and\nthe image of a pose donor. We use a dense pose estimation system that maps\npixels from both images to a common surface-based coordinate system, allowing\nthe two images to be brought in correspondence with each other. We inpaint and\nrefine the source image intensities in the surface coordinate system, prior to\nwarping them onto the target pose. These predictions are fused with those of a\nconvolutional predictive module through a neural synthesis module allowing for\ntraining the whole pipeline jointly end-to-end, optimizing a combination of\nadversarial and perceptual losses. We show that dense pose estimation is a\nsubstantially more powerful conditioning input than landmark-, or mask-based\nalternatives, and report systematic improvements over state of the art\ngenerators on DeepFashion and MVC datasets. \n\n"}
{"id": "1809.02791", "contents": "Title: Adversarial Learning for Image Forensics Deep Matching with Atrous\n  Convolution Abstract: Constrained image splicing detection and localization (CISDL) is a newly\nproposed challenging task for image forensics, which investigates two input\nsuspected images and identifies whether one image has suspected regions pasted\nfrom the other. In this paper, we propose a novel adversarial learning\nframework to train the deep matching network for CISDL. Our framework mainly\nconsists of three building blocks: 1) the deep matching network based on atrous\nconvolution (DMAC) aims to generate two high-quality candidate masks which\nindicate the suspected regions of the two input images, 2) the detection\nnetwork is designed to rectify inconsistencies between the two corresponding\ncandidate masks, 3) the discriminative network drives the DMAC network to\nproduce masks that are hard to distinguish from ground-truth ones. In DMAC,\natrous convolution is adopted to extract features with rich spatial\ninformation, the correlation layer based on the skip architecture is proposed\nto capture hierarchical features, and atrous spatial pyramid pooling is\nconstructed to localize tampered regions at multiple scales. The detection\nnetwork and the discriminative network act as the losses with auxiliary\nparameters to supervise the training of DMAC in an adversarial way. Extensive\nexperiments, conducted on 21 generated testing sets and two public datasets,\ndemonstrate the effectiveness of the proposed framework and the superior\nperformance of DMAC. \n\n"}
{"id": "1809.03669", "contents": "Title: Temporal-Spatial Mapping for Action Recognition Abstract: Deep learning models have enjoyed great success for image related computer\nvision tasks like image classification and object detection. For video related\ntasks like human action recognition, however, the advancements are not as\nsignificant yet. The main challenge is the lack of effective and efficient\nmodels in modeling the rich temporal spatial information in a video. We\nintroduce a simple yet effective operation, termed Temporal-Spatial Mapping\n(TSM), for capturing the temporal evolution of the frames by jointly analyzing\nall the frames of a video. We propose a video level 2D feature representation\nby transforming the convolutional features of all frames to a 2D feature map,\nreferred to as VideoMap. With each row being the vectorized feature\nrepresentation of a frame, the temporal-spatial features are compactly\nrepresented, while the temporal dynamic evolution is also well embedded. Based\non the VideoMap representation, we further propose a temporal attention model\nwithin a shallow convolutional neural network to efficiently exploit the\ntemporal-spatial dynamics. The experiment results show that the proposed scheme\nachieves the state-of-the-art performance, with 4.2% accuracy gain over\nTemporal Segment Network (TSN), a competing baseline method, on the challenging\nhuman action benchmark dataset HMDB51. \n\n"}
{"id": "1809.04693", "contents": "Title: An Online Plug-and-Play Algorithm for Regularized Image Reconstruction Abstract: Plug-and-play priors (PnP) is a powerful framework for regularizing imaging\ninverse problems by using advanced denoisers within an iterative algorithm.\nRecent experimental evidence suggests that PnP algorithms achieve\nstate-of-the-art performance in a range of imaging applications. In this paper,\nwe introduce a new online PnP algorithm based on the iterative\nshrinkage/thresholding algorithm (ISTA). The proposed algorithm uses only a\nsubset of measurements at every iteration, which makes it scalable to very\nlarge datasets. We present a new theoretical convergence analysis, for both\nbatch and online variants of PnP-ISTA, for denoisers that do not necessarily\ncorrespond to proximal operators. We also present simulations illustrating the\napplicability of the algorithm to image reconstruction in diffraction\ntomography. The results in this paper have the potential to expand the\napplicability of the PnP framework to very large and redundant datasets. \n\n"}
{"id": "1809.05786", "contents": "Title: GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation\n  with Generative Adversarial Networks Abstract: In the last decade, supervised deep learning approaches have been extensively\nemployed in visual odometry (VO) applications, which is not feasible in\nenvironments where labelled data is not abundant. On the other hand,\nunsupervised deep learning approaches for localization and mapping in unknown\nenvironments from unlabelled data have received comparatively less attention in\nVO research. In this study, we propose a generative unsupervised learning\nframework that predicts 6-DoF pose camera motion and monocular depth map of the\nscene from unlabelled RGB image sequences, using deep convolutional Generative\nAdversarial Networks (GANs). We create a supervisory signal by warping view\nsequences and assigning the re-projection minimization to the objective loss\nfunction that is adopted in multi-view pose estimation and single-view depth\ngeneration network. Detailed quantitative and qualitative evaluations of the\nproposed framework on the KITTI and Cityscapes datasets show that the proposed\nmethod outperforms both existing traditional and unsupervised deep VO methods\nproviding better results for both pose estimation and depth recovery. \n\n"}
{"id": "1809.05825", "contents": "Title: Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN\n  Trained on Synthetic Data Abstract: The ability to segment unknown objects in depth images has potential to\nenhance robot skills in grasping and object tracking. Recent computer vision\nresearch has demonstrated that Mask R-CNN can be trained to segment specific\ncategories of objects in RGB images when massive hand-labeled datasets are\navailable. As generating these datasets is time consuming, we instead train\nwith synthetic depth images. Many robots now use depth sensors, and recent\nresults suggest training on synthetic depth data can transfer successfully to\nthe real world. We present a method for automated dataset generation and\nrapidly generate a synthetic training dataset of 50,000 depth images and\n320,000 object masks using simulated heaps of 3D CAD models. We train a variant\nof Mask R-CNN with domain randomization on the generated dataset to perform\ncategory-agnostic instance segmentation without any hand-labeled data and we\nevaluate the trained network, which we refer to as Synthetic Depth (SD) Mask\nR-CNN, on a set of real, high-resolution depth images of challenging,\ndensely-cluttered bins containing objects with highly-varied geometry. SD Mask\nR-CNN outperforms point cloud clustering baselines by an absolute 15% in\nAverage Precision and 20% in Average Recall on COCO benchmarks, and achieves\nperformance levels similar to a Mask R-CNN trained on a massive, hand-labeled\nRGB dataset and fine-tuned on real images from the experimental setup. We\ndeploy the model in an instance-specific grasping pipeline to demonstrate its\nusefulness in a robotics application. Code, the synthetic training dataset, and\nsupplementary material are available at https://bit.ly/2letCuE. \n\n"}
{"id": "1809.05989", "contents": "Title: FermiNets: Learning generative machines to generate efficient neural\n  networks via generative synthesis Abstract: The tremendous potential exhibited by deep learning is often offset by\narchitectural and computational complexity, making widespread deployment a\nchallenge for edge scenarios such as mobile and other consumer devices. To\ntackle this challenge, we explore the following idea: Can we learn generative\nmachines to automatically generate deep neural networks with efficient network\narchitectures? In this study, we introduce the idea of generative synthesis,\nwhich is premised on the intricate interplay between a generator-inquisitor\npair that work in tandem to garner insights and learn to generate highly\nefficient deep neural networks that best satisfies operational requirements.\nWhat is most interesting is that, once a generator has been learned through\ngenerative synthesis, it can be used to generate not just one but a large\nvariety of different, unique highly efficient deep neural networks that satisfy\noperational requirements. Experimental results for image classification,\nsemantic segmentation, and object detection tasks illustrate the efficacy of\ngenerative synthesis in producing generators that automatically generate highly\nefficient deep neural networks (which we nickname FermiNets) with higher model\nefficiency and lower computational costs (reaching >10x more efficient and\nfewer multiply-accumulate operations than several tested state-of-the-art\nnetworks), as well as higher energy efficiency (reaching >4x improvements in\nimage inferences per joule consumed on a Nvidia Tegra X2 mobile processor). As\nsuch, generative synthesis can be a powerful, generalized approach for\naccelerating and improving the building of deep neural networks for on-device\nedge scenarios. \n\n"}
{"id": "1809.06367", "contents": "Title: Scattering Networks for Hybrid Representation Learning Abstract: Scattering networks are a class of designed Convolutional Neural Networks\n(CNNs) with fixed weights. We argue they can serve as generic representations\nfor modelling images. In particular, by working in scattering space, we achieve\ncompetitive results both for supervised and unsupervised learning tasks, while\nmaking progress towards constructing more interpretable CNNs. For supervised\nlearning, we demonstrate that the early layers of CNNs do not necessarily need\nto be learned, and can be replaced with a scattering network instead. Indeed,\nusing hybrid architectures, we achieve the best results with predefined\nrepresentations to-date, while being competitive with end-to-end learned CNNs.\nSpecifically, even applying a shallow cascade of small-windowed scattering\ncoefficients followed by 1$\\times$1-convolutions results in AlexNet accuracy on\nthe ILSVRC2012 classification task. Moreover, by combining scattering networks\nwith deep residual networks, we achieve a single-crop top-5 error of 11.4% on\nILSVRC2012. Also, we show they can yield excellent performance in the small\nsample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. For\nunsupervised learning, scattering coefficients can be a competitive\nrepresentation that permits image recovery. We use this fact to train hybrid\nGANs to generate images. Finally, we empirically analyze several properties\nrelated to stability and reconstruction of images from scattering coefficients. \n\n"}
{"id": "1809.07041", "contents": "Title: Exploring Visual Relationship for Image Captioning Abstract: It is always well believed that modeling relationships between objects would\nbe helpful for representing and eventually describing an image. Nevertheless,\nthere has not been evidence in support of the idea on image description\ngeneration. In this paper, we introduce a new design to explore the connections\nbetween objects for image captioning under the umbrella of attention-based\nencoder-decoder framework. Specifically, we present Graph Convolutional\nNetworks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that\nnovelly integrates both semantic and spatial object relationships into image\nencoder. Technically, we build graphs over the detected objects in an image\nbased on their spatial and semantic connections. The representations of each\nregion proposed on objects are then refined by leveraging graph structure\nthrough GCN. With the learnt region-level features, our GCN-LSTM capitalizes on\nLSTM-based captioning framework with attention mechanism for sentence\ngeneration. Extensive experiments are conducted on COCO image captioning\ndataset, and superior results are reported when comparing to state-of-the-art\napproaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1%\nto 128.7% on COCO testing set. \n\n"}
{"id": "1809.08495", "contents": "Title: SqueezeSegV2: Improved Model Structure and Unsupervised Domain\n  Adaptation for Road-Object Segmentation from a LiDAR Point Cloud Abstract: Earlier work demonstrates the promise of deep-learning-based approaches for\npoint cloud segmentation; however, these approaches need to be improved to be\npractically useful. To this end, we introduce a new model SqueezeSegV2 that is\nmore robust to dropout noise in LiDAR point clouds. With improved model\nstructure, training loss, batch normalization and additional input channel,\nSqueezeSegV2 achieves significant accuracy improvement when trained on real\ndata. Training models for point cloud segmentation requires large amounts of\nlabeled point-cloud data, which is expensive to obtain. To sidestep the cost of\ncollection and annotation, simulators such as GTA-V can be used to create\nunlimited amounts of labeled, synthetic data. However, due to domain shift,\nmodels trained on synthetic data often do not generalize well to the real\nworld. We address this problem with a domain-adaptation training pipeline\nconsisting of three major components: 1) learned intensity rendering, 2)\ngeodesic correlation alignment, and 3) progressive domain calibration. When\ntrained on real data, our new model exhibits segmentation accuracy improvements\nof 6.0-8.6% over the original SqueezeSeg. When training our new model on\nsynthetic data using the proposed domain adaptation pipeline, we nearly double\ntest accuracy on real-world data, from 29.0% to 57.4%. Our source code and\nsynthetic dataset will be open-sourced. \n\n"}
{"id": "1809.08573", "contents": "Title: Learning for Video Super-Resolution through HR Optical Flow Estimation Abstract: Video super-resolution (SR) aims to generate a sequence of high-resolution\n(HR) frames with plausible and temporally consistent details from their\nlow-resolution (LR) counterparts. The generation of accurate correspondence\nplays a significant role in video SR. It is demonstrated by traditional video\nSR methods that simultaneous SR of both images and optical flows can provide\naccurate correspondences and better SR results. However, LR optical flows are\nused in existing deep learning based methods for correspondence generation. In\nthis paper, we propose an end-to-end trainable video SR framework to\nsuper-resolve both images and optical flows. Specifically, we first propose an\noptical flow reconstruction network (OFRnet) to infer HR optical flows in a\ncoarse-to-fine manner. Then, motion compensation is performed according to the\nHR optical flows. Finally, compensated LR inputs are fed to a super-resolution\nnetwork (SRnet) to generate the SR results. Extensive experiments demonstrate\nthat HR optical flows provide more accurate correspondences than their LR\ncounterparts and improve both accuracy and consistency performance. Comparative\nresults on the Vid4 and DAVIS-10 datasets show that our framework achieves the\nstate-of-the-art performance. \n\n"}
{"id": "1809.09767", "contents": "Title: Night-to-Day Image Translation for Retrieval-based Localization Abstract: Visual localization is a key step in many robotics pipelines, allowing the\nrobot to (approximately) determine its position and orientation in the world.\nAn efficient and scalable approach to visual localization is to use image\nretrieval techniques. These approaches identify the image most similar to a\nquery photo in a database of geo-tagged images and approximate the query's pose\nvia the pose of the retrieved database image. However, image retrieval across\ndrastically different illumination conditions, e.g. day and night, is still a\nproblem with unsatisfactory results, even in this age of powerful neural\nmodels. This is due to a lack of a suitably diverse dataset with true\ncorrespondences to perform end-to-end learning. A recent class of neural models\nallows for realistic translation of images among visual domains with relatively\nlittle training data and, most importantly, without ground-truth pairings. In\nthis paper, we explore the task of accurately localizing images captured from\ntwo traversals of the same area in both day and night. We propose ToDayGAN - a\nmodified image-translation model to alter nighttime driving images to a more\nuseful daytime representation. We then compare the daytime and translated night\nimages to obtain a pose estimate for the night image using the known 6-DOF\nposition of the closest day image. Our approach improves localization\nperformance by over 250% compared the current state-of-the-art, in the context\nof standard metrics in multiple categories. \n\n"}
{"id": "1809.09924", "contents": "Title: Hierarchy-based Image Embeddings for Semantic Image Retrieval Abstract: Deep neural networks trained for classification have been found to learn\npowerful image representations, which are also often used for other tasks such\nas comparing images w.r.t. their visual similarity. However, visual similarity\ndoes not imply semantic similarity. In order to learn semantically\ndiscriminative features, we propose to map images onto class embeddings whose\npair-wise dot products correspond to a measure of semantic similarity between\nclasses. Such an embedding does not only improve image retrieval results, but\ncould also facilitate integrating semantics for other tasks, e.g., novelty\ndetection or few-shot learning. We introduce a deterministic algorithm for\ncomputing the class centroids directly based on prior world-knowledge encoded\nin a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds,\nand ImageNet show that our learned semantic image embeddings improve the\nsemantic consistency of image retrieval results by a large margin. \n\n"}
{"id": "1809.10548", "contents": "Title: Real-time 3D Pose Estimation with a Monocular Camera Using Deep Learning\n  and Object Priors On an Autonomous Racecar Abstract: We propose a complete pipeline that allows object detection and\nsimultaneously estimate the pose of these multiple object instances using just\na single image. A novel \"keypoint regression\" scheme with a cross-ratio term is\nintroduced that exploits prior information about the object's shape and size to\nregress and find specific feature points. Further, a priori 3D information\nabout the object is used to match 2D-3D correspondences and accurately estimate\nobject positions up to a distance of 15m. A detailed discussion of the results\nand an in-depth analysis of the pipeline is presented. The pipeline runs\nefficiently on a low-powered Jetson TX2 and is deployed as part of the\nperception pipeline on a real-time autonomous vehicle cruising at a top speed\nof 54 km/hr. \n\n"}
{"id": "1809.10636", "contents": "Title: Conditional WaveGAN Abstract: Generative models are successfully used for image synthesis in the recent\nyears. But when it comes to other modalities like audio, text etc little\nprogress has been made. Recent works focus on generating audio from a\ngenerative model in an unsupervised setting. We explore the possibility of\nusing generative models conditioned on class labels. Concatenation based\nconditioning and conditional scaling were explored in this work with various\nhyper-parameter tuning methods. In this paper we introduce Conditional WaveGANs\n(cWaveGAN). Find our implementation at https://github.com/acheketa/cwavegan \n\n"}
{"id": "1809.10954", "contents": "Title: Deep Adaptive Learning for Writer Identification based on Single\n  Handwritten Word Images Abstract: There are two types of information in each handwritten word image: explicit\ninformation which can be easily read or derived directly, such as lexical\ncontent or word length, and implicit attributes such as the author's identity.\nWhether features learned by a neural network for one task can be used for\nanother task remains an open question. In this paper, we present a deep\nadaptive learning method for writer identification based on single-word images\nusing multi-task learning. An auxiliary task is added to the training process\nto enforce the emergence of reusable features. Our proposed method transfers\nthe benefits of the learned features of a convolutional neural network from an\nauxiliary task such as explicit content recognition to the main task of writer\nidentification in a single procedure. Specifically, we propose a new adaptive\nconvolutional layer to exploit the learned deep features. A multi-task neural\nnetwork with one or several adaptive convolutional layers is trained\nend-to-end, to exploit robust generic features for a specific main task, i.e.,\nwriter identification. Three auxiliary tasks, corresponding to three explicit\nattributes of handwritten word images (lexical content, word length and\ncharacter attributes), are evaluated. Experimental results on two benchmark\ndatasets show that the proposed deep adaptive learning method can improve the\nperformance of writer identification based on single-word images, compared to\nnon-adaptive and simple linear-adaptive approaches. \n\n"}
{"id": "1809.10966", "contents": "Title: Domain Generalization with Domain-Specific Aggregation Modules Abstract: Visual recognition systems are meant to work in the real world. For this to\nhappen, they must work robustly in any visual domain, and not only on the data\nused during training. Within this context, a very realistic scenario deals with\ndomain generalization, i.e. the ability to build visual recognition algorithms\nable to work robustly in several visual domains, without having access to any\ninformation about target data statistic. This paper contributes to this\nresearch thread, proposing a deep architecture that maintains separated the\ninformation about the available source domains data while at the same time\nleveraging over generic perceptual information. We achieve this by introducing\ndomain-specific aggregation modules that through an aggregation layer strategy\nare able to merge generic and specific information in an effective manner.\nExperiments on two different benchmark databases show the power of our\napproach, reaching the new state of the art in domain generalization. \n\n"}
{"id": "1810.00599", "contents": "Title: Unsupervised Trajectory Segmentation and Promoting of Multi-Modal\n  Surgical Demonstrations Abstract: To improve the efficiency of surgical trajectory segmentation for robot\nlearning in robot-assisted minimally invasive surgery, this paper presents a\nfast unsupervised method using video and kinematic data, followed by a\npromoting procedure to address the over-segmentation issue. Unsupervised deep\nlearning network, stacking convolutional auto-encoder, is employed to extract\nmore discriminative features from videos in an effective way. To further\nimprove the accuracy of segmentation, on one hand, wavelet transform is used to\nfilter out the noises existed in the features from video and kinematic data. On\nthe other hand, the segmentation result is promoted by identifying the adjacent\nsegments with no state transition based on the predefined similarity\nmeasurements. Extensive experiments on a public dataset JIGSAWS show that our\nmethod achieves much higher accuracy of segmentation than state-of-the-art\nmethods in the shorter time. \n\n"}
{"id": "1810.00912", "contents": "Title: Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition Abstract: In an open-world setting, it is inevitable that an intelligent agent (e.g., a\nrobot) will encounter visual objects, attributes or relationships it does not\nrecognize. In this work, we develop an agent empowered with visual curiosity,\ni.e. the ability to ask questions to an Oracle (e.g., human) about the contents\nin images (e.g., What is the object on the left side of the red cube?) and\nbuild visual recognition model based on the answers received (e.g., Cylinder).\nIn order to do this, the agent must (1) understand what it recognizes and what\nit does not, (2) formulate a valid, unambiguous and informative language query\n(a question) to ask the Oracle, (3) derive the parameters of visual classifiers\nfrom the Oracle response and (4) leverage the updated visual classifiers to ask\nmore clarified questions. Specifically, we propose a novel framework and\nformulate the learning of visual curiosity as a reinforcement learning problem.\nIn this framework, all components of our agent, visual recognition module (to\nsee), question generation policy (to ask), answer digestion module (to\nunderstand) and graph memory module (to memorize), are learned entirely\nend-to-end to maximize the reward derived from the scene graph obtained by the\nagent as a consequence of the dialog with the Oracle. Importantly, the question\ngeneration policy is disentangled from the visual recognition system and\nspecifics of the environment. Consequently, we demonstrate a sort of double\ngeneralization. Our question generation policy generalizes to new environments\nand a new pair of eyes, i.e., new visual system. Trained on a synthetic\ndataset, our results show that our agent learns new visual concepts\nsignificantly faster than several heuristic baselines, even when tested on\nsynthetic environments with novel objects, as well as in a realistic\nenvironment. \n\n"}
{"id": "1810.01638", "contents": "Title: Optimization Algorithm Inspired Deep Neural Network Structure Design Abstract: Deep neural networks have been one of the dominant machine learning\napproaches in recent years. Several new network structures are proposed and\nhave better performance than the traditional feedforward neural network\nstructure. Representative ones include the skip connection structure in ResNet\nand the dense connection structure in DenseNet. However, it still lacks a\nunified guidance for the neural network structure design. In this paper, we\npropose the hypothesis that the neural network structure design can be inspired\nby optimization algorithms and a faster optimization algorithm may lead to a\nbetter neural network structure. Specifically, we prove that the propagation in\nthe feedforward neural network with the same linear transformation in different\nlayers is equivalent to minimizing some function using the gradient descent\nalgorithm. Based on this observation, we replace the gradient descent algorithm\nwith the heavy ball algorithm and Nesterov's accelerated gradient descent\nalgorithm, which are faster and inspire us to design new and better network\nstructures. ResNet and DenseNet can be considered as two special cases of our\nframework. Numerical experiments on CIFAR-10, CIFAR-100 and ImageNet verify the\nadvantage of our optimization algorithm inspired structures over ResNet and\nDenseNet. \n\n"}
{"id": "1810.02426", "contents": "Title: Relative Saliency and Ranking: Models, Metrics, Data, and Benchmarks Abstract: Salient object detection is a problem that has been considered in detail and\n\\textcolor{black}{many solutions have been proposed}. In this paper, we argue\nthat work to date has addressed a problem that is relatively ill-posed.\nSpecifically, there is not universal agreement about what constitutes a salient\nobject when multiple observers are queried. This implies that some objects are\nmore likely to be judged salient than others, and implies a relative rank\nexists on salient objects. Initially, we present a novel deep learning solution\nbased on a hierarchical representation of relative saliency and stage-wise\nrefinement. Further to this, we present data, analysis and baseline benchmark\nresults towards addressing the problem of salient object ranking. Methods for\nderiving suitable ranked salient object instances are presented, along with\nmetrics suitable to measuring algorithm performance. In addition, we show how a\nderived dataset can be successively refined to provide cleaned results that\ncorrelate well with pristine ground truth in its characteristics and value for\ntraining and testing models. Finally, we provide a comparison among prevailing\nalgorithms that address salient object ranking or detection to establish\ninitial baselines providing a basis for comparison with future efforts\naddressing this problem. \\textcolor{black}{The source code and data are\npublicly available via our project page:}\n\\textrm{\\href{https://ryersonvisionlab.github.io/cocosalrank.html}{ryersonvisionlab.github.io/cocosalrank}} \n\n"}
{"id": "1810.03505", "contents": "Title: CINIC-10 is not ImageNet or CIFAR-10 Abstract: In this brief technical report we introduce the CINIC-10 dataset as a plug-in\nextended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with\nimages selected and downsampled from the ImageNet database. We present the\napproach to compiling the dataset, illustrate the example images for different\nclasses, give pixel distributions for each part of the repository, and give\nsome standard benchmarks for well known models. Details for download, usage,\nand compilation can be found in the associated github repository. \n\n"}
{"id": "1810.03851", "contents": "Title: Deep Attentive Tracking via Reciprocative Learning Abstract: Visual attention, derived from cognitive neuroscience, facilitates human\nperception on the most pertinent subset of the sensory data. Recently,\nsignificant efforts have been made to exploit attention schemes to advance\ncomputer vision systems. For visual tracking, it is often challenging to track\ntarget objects undergoing large appearance changes. Attention maps facilitate\nvisual tracking by selectively paying attention to temporal robust features.\nExisting tracking-by-detection approaches mainly use additional attention\nmodules to generate feature weights as the classifiers are not equipped with\nsuch mechanisms. In this paper, we propose a reciprocative learning algorithm\nto exploit visual attention for training deep classifiers. The proposed\nalgorithm consists of feed-forward and backward operations to generate\nattention maps, which serve as regularization terms coupled with the original\nclassification loss function for training. The deep classifier learns to attend\nto the regions of target objects robust to appearance changes. Extensive\nexperiments on large-scale benchmark datasets show that the proposed attentive\ntracking method performs favorably against the state-of-the-art approaches. \n\n"}
{"id": "1810.03966", "contents": "Title: Adaptive Image Stream Classification via Convolutional Neural Network\n  with Intrinsic Similarity Metrics Abstract: When performing data classification over a stream of continuously occurring\ninstances, a key challenge is to develop an open-world classifier that\nanticipates instances from an unknown class. Studies addressing this problem,\ntypically called novel class detection, have considered classification methods\nthat reactively adapt to such changes along the stream. Importantly, they rely\non the property of cohesion and separation among instances in feature space.\nInstances belonging to the same class are assumed to be closer to each other\n(cohesion) than those belonging to different classes (separation).\nUnfortunately, this assumption may not have large support when dealing with\nhigh dimensional data such as images. In this paper, we address this key\nchallenge by proposing a semisupervised multi-task learning framework called\nCSIM which aims to intrinsically search for a latent space suitable for\ndetecting labels of instances from both known and unknown classes.\nParticularly, we utilize a convolution neural network layer that aids in the\nlearning of a latent feature space suitable for novel class detection. We\nempirically measure the performance of CSIM over multiple realworld image\ndatasets and demonstrate its superiority by comparing its performance with\nexisting semi-supervised methods. \n\n"}
{"id": "1810.04093", "contents": "Title: Geometry meets semantics for semi-supervised monocular depth estimation Abstract: Depth estimation from a single image represents a very exciting challenge in\ncomputer vision. While other image-based depth sensing techniques leverage on\nthe geometry between different viewpoints (e.g., stereo or structure from\nmotion), the lack of these cues within a single image renders ill-posed the\nmonocular depth estimation task. For inference, state-of-the-art\nencoder-decoder architectures for monocular depth estimation rely on effective\nfeature representations learned at training time. For unsupervised training of\nthese models, geometry has been effectively exploited by suitable images\nwarping losses computed from views acquired by a stereo rig or a moving camera.\nIn this paper, we make a further step forward showing that learning semantic\ninformation from images enables to improve effectively monocular depth\nestimation as well. In particular, by leveraging on semantically labeled images\ntogether with unsupervised signals gained by geometry through an image warping\nloss, we propose a deep learning approach aimed at joint semantic segmentation\nand depth estimation. Our overall learning framework is semi-supervised, as we\ndeploy groundtruth data only in the semantic domain. At training time, our\nnetwork learns a common feature representation for both tasks and a novel\ncross-task loss function is proposed. The experimental findings show how,\njointly tackling depth prediction and semantic segmentation, allows to improve\ndepth estimation accuracy. In particular, on the KITTI dataset our network\noutperforms state-of-the-art methods for monocular depth estimation. \n\n"}
{"id": "1810.04250", "contents": "Title: Bird Species Classification using Transfer Learning with Multistage\n  Training Abstract: Bird species classification has received more and more attention in the field\nof computer vision, for its promising applications in biology and environmental\nstudies. Recognizing bird species is difficult due to the challenges of\ndiscriminative region localization and fine-grained feature learning. In this\npaper, we have introduced a Transfer learning based method with multistage\ntraining. We have used both Pre-Trained Mask-RCNN and an ensemble model\nconsisting of Inception Nets (InceptionV3 & InceptionResNetV2 ) to get\nlocalization and species of the bird from the images respectively. Our final\nmodel achieves an F1 score of 0.5567 or 55.67 % on the dataset provided in CVIP\n2018 Challenge. \n\n"}
{"id": "1810.04452", "contents": "Title: AI Learns to Recognize Bengali Handwritten Digits: Bengali.AI Computer\n  Vision Challenge 2018 Abstract: Solving problems with Artificial intelligence in a competitive manner has\nlong been absent in Bangladesh and Bengali-speaking community. On the other\nhand, there has not been a well structured database for Bengali Handwritten\ndigits for mass public use. To bring out the best minds working in machine\nlearning and use their expertise to create a model which can easily recognize\nBengali Handwritten digits, we organized Bengali.AI Computer Vision\nChallenge.The challenge saw both local and international teams participating\nwith unprecedented efforts. \n\n"}
{"id": "1810.06169", "contents": "Title: Traffic Signs in the Wild: Highlights from the IEEE Video and Image\n  Processing Cup 2017 Student Competition [SP Competitions] Abstract: Robust and reliable traffic sign detection is necessary to bring autonomous\nvehicles onto our roads. State-of-the-art algorithms successfully perform\ntraffic sign detection over existing databases that mostly lack severe\nchallenging conditions. VIP Cup 2017 competition focused on detecting such\ntraffic signs under challenging conditions. To facilitate such task and\ncompetition, we introduced a video dataset denoted as CURE-TSD that includes a\nvariety of challenging conditions. The goal of this challenge was to implement\ntraffic sign detection algorithms that can robustly perform under such\nchallenging conditions. In this article, we share an overview of the VIP Cup\n2017 experience including competition setup, teams, technical approaches,\nparticipation statistics, and competition experience through finalist teams\nmembers' and organizers' eyes. \n\n"}
{"id": "1810.07097", "contents": "Title: Salient Object Detection in Video using Deep Non-Local Neural Networks Abstract: Detection of salient objects in image and video is of great importance in\nmany computer vision applications. In spite of the fact that the state of the\nart in saliency detection for still images has been changed substantially over\nthe last few years, there have been few improvements in video saliency\ndetection. This paper investigates the use of recently introduced non-local\nneural networks in video salient object detection. Non-local neural networks\nare applied to capture global dependencies and hence determine the salient\nobjects. The effect of non-local operations is studied separately on static and\ndynamic saliency detection in order to exploit both appearance and motion\nfeatures. A novel deep non-local neural network architecture is introduced for\nvideo salient object detection and tested on two well-known datasets DAVIS and\nFBMS. The experimental results show that the proposed algorithm outperforms\nstate-of-the-art video saliency detection methods. \n\n"}
{"id": "1810.07599", "contents": "Title: Orthogonal Deep Features Decomposition for Age-Invariant Face\n  Recognition Abstract: As facial appearance is subject to significant intra-class variations caused\nby the aging process over time, age-invariant face recognition (AIFR) remains a\nmajor challenge in face recognition community. To reduce the intra-class\ndiscrepancy caused by the aging, in this paper we propose a novel approach\n(namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep\nface features. Specifically, we decompose deep face features into two\northogonal components to represent age-related and identity-related features.\nAs a result, identity-related features that are robust to aging are then used\nfor AIFR. Besides, for complementing the existing cross-age datasets and\nadvancing the research in this field, we construct a brand-new large-scale\nCross-Age Face dataset (CAF). Extensive experiments conducted on the three\npublic domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have\nshown the effectiveness of the proposed approach and the value of the\nconstructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most\npopular general face recognition (GFR) dataset LFW additionally demonstrates\nthe comparable generalization performance on GFR. \n\n"}
{"id": "1810.08126", "contents": "Title: KTAN: Knowledge Transfer Adversarial Network Abstract: To reduce the large computation and storage cost of a deep convolutional\nneural network, the knowledge distillation based methods have pioneered to\ntransfer the generalization ability of a large (teacher) deep network to a\nlight-weight (student) network. However, these methods mostly focus on\ntransferring the probability distribution of the softmax layer in a teacher\nnetwork and thus neglect the intermediate representations. In this paper, we\npropose a knowledge transfer adversarial network to better train a student\nnetwork. Our technique holistically considers both intermediate representations\nand probability distributions of a teacher network. To transfer the knowledge\nof intermediate representations, we set high-level teacher feature maps as a\ntarget, toward which the student feature maps are trained. Specifically, we\narrange a Teacher-to-Student layer for enabling our framework suitable for\nvarious student structures. The intermediate representation helps the student\nnetwork better understand the transferred generalization as compared to the\nprobability distribution only. Furthermore, we infuse an adversarial learning\nprocess by employing a discriminator network, which can fully exploit the\nspatial correlation of feature maps in training a student network. The\nexperimental results demonstrate that the proposed method can significantly\nimprove the performance of a student network on both image classification and\nobject detection tasks. \n\n"}
{"id": "1810.08565", "contents": "Title: Deep Person Re-identification for Probabilistic Data Association in\n  Multiple Pedestrian Tracking Abstract: We present a data association method for vision-based multiple pedestrian\ntracking, using deep convolutional features to distinguish between different\npeople based on their appearances. These re-identification (re-ID) features are\nlearned such that they are invariant to transformations such as rotation,\ntranslation, and changes in the background, allowing consistent identification\nof a pedestrian moving through a scene. We incorporate re-ID features into a\ngeneral data association likelihood model for multiple person tracking,\nexperimentally validate this model by using it to perform tracking in two\nevaluation video sequences, and examine the performance improvements gained as\ncompared to several baseline approaches. Our results demonstrate that using\ndeep person re-ID for data association greatly improves tracking robustness to\nchallenges such as occlusions and path crossings. \n\n"}
{"id": "1810.08770", "contents": "Title: Sequential Context Encoding for Duplicate Removal Abstract: Duplicate removal is a critical step to accomplish a reasonable amount of\npredictions in prevalent proposal-based object detection frameworks. Albeit\nsimple and effective, most previous algorithms utilize a greedy process without\nmaking sufficient use of properties of input data. In this work, we design a\nnew two-stage framework to effectively select the appropriate proposal\ncandidate for each object. The first stage suppresses most of easy negative\nobject proposals, while the second stage selects true positives in the reduced\nproposal set. These two stages share the same network structure, \\ie, an\nencoder and a decoder formed as recurrent neural networks (RNN) with global\nattention and context gate. The encoder scans proposal candidates in a\nsequential manner to capture the global context information, which is then fed\nto the decoder to extract optimal proposals. In our extensive experiments, the\nproposed method outperforms other alternatives by a large margin. \n\n"}
{"id": "1810.09102", "contents": "Title: Can We Gain More from Orthogonality Regularizations in Training Deep\n  CNNs? Abstract: This paper seeks to answer the question: as the (near-) orthogonality of\nweights is found to be a favorable property for training deep convolutional\nneural networks, how can we enforce it in more effective and easy-to-use ways?\nWe develop novel orthogonality regularizations on training deep CNNs, utilizing\nvarious advanced analytical tools such as mutual coherence and restricted\nisometry property. These plug-and-play regularizations can be conveniently\nincorporated into training almost any CNN without extra hassle. We then\nbenchmark their effects on state-of-the-art models: ResNet, WideResNet, and\nResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100,\nSVHN and ImageNet. We observe consistent performance gains after applying those\nproposed regularizations, in terms of both the final accuracies achieved, and\nfaster and more stable convergences. We have made our codes and pre-trained\nmodels publicly available:\nhttps://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality. \n\n"}
{"id": "1810.09381", "contents": "Title: Unsupervised Learning of Shape and Pose with Differentiable Point Clouds Abstract: We address the problem of learning accurate 3D shape and camera pose from a\ncollection of unlabeled category-specific images. We train a convolutional\nnetwork to predict both the shape and the pose from a single image by\nminimizing the reprojection error: given several views of an object, the\nprojections of the predicted shapes to the predicted camera poses should match\nthe provided views. To deal with pose ambiguity, we introduce an ensemble of\npose predictors which we then distill to a single \"student\" model. To allow for\nefficient learning of high-fidelity shapes, we represent the shapes by point\nclouds and devise a formulation allowing for differentiable projection of\nthese. Our experiments show that the distilled ensemble of pose predictors\nlearns to estimate the pose accurately, while the point cloud representation\nallows to predict detailed shape models. The supplementary video can be found\nat https://www.youtube.com/watch?v=LuIGovKeo60 \n\n"}
{"id": "1810.09660", "contents": "Title: Large scale visual place recognition with sub-linear storage growth Abstract: Robotic and animal mapping systems share many of the same objectives and\nchallenges, but differ in one key aspect: where much of the research in robotic\nmapping has focused on solving the data association problem, the grid cell\nneurons underlying maps in the mammalian brain appear to intentionally break\ndata association by encoding many locations with a single grid cell neuron. One\npotential benefit of this intentional aliasing is both sub-linear map storage\nand computational requirements growth with environment size, which we\ndemonstrated in a previous proof-of-concept study that detected and encoded\nmutually complementary co-prime pattern frequencies in the visual map data. In\nthis research, we solve several of the key theoretical and practical\nlimitations of that prototype model and achieve significantly better sub-linear\nstorage growth, a factor reduction in storage requirements per map location,\nscalability to large datasets on standard compute equipment and improved\nrobustness to environments with visually challenging appearance change. These\nimprovements are achieved through several innovations including a flexible\nuser-driven choice mechanism for the periodic patterns underlying the new\nencoding method, a parallelized chunking technique that splits the map into\nsub-sections processed in parallel and a novel feature selection approach that\nselects only the image information most relevant to the encoded temporal\npatterns. We evaluate our techniques on two large benchmark datasets with the\ncomparison to the previous state-of-the-art system, as well as providing a\ndetailed analysis of system performance with respect to parameters such as\nrequired precision performance and the number of cyclic patterns encoded. \n\n"}
{"id": "1810.10293", "contents": "Title: Coarse-to-fine volumetric segmentation of teeth in Cone-Beam CT Abstract: We consider the problem of localizing and segmenting individual teeth inside\n3D Cone-Beam Computed Tomography (CBCT) images. To handle large image sizes we\napproach this task with a coarse-to-fine framework, where the whole volume is\nfirst analyzed as a 33-class semantic segmentation (adults have up to 32 teeth)\nin coarse resolution, followed by binary semantic segmentation of the cropped\nregion of interest in original resolution. To improve the performance of the\nchallenging 33-class segmentation, we first train the Coarse step model on a\nlarge weakly labeled dataset, then fine-tune it on a smaller precisely labeled\ndataset. The Fine step model is trained with precise labels only. Experiments\nusing our in-house dataset show significant improvement for both\nweakly-supervised pretraining and for the addition of the Fine step.\nEmpirically, this framework yields precise teeth masks with low localization\nerrors sufficient for many real-world applications. \n\n"}
{"id": "1810.11184", "contents": "Title: Integrable discrete autonomous quad-equations admitting, as generalized\n  symmetries, known five-point differential-difference equations Abstract: In this paper we construct the autonomous quad-equations which admit as\nsymmetries the five-point differential-difference equations belonging to known\nlists found by Garifullin, Yamilov and Levi. The obtained equations are\nclassified up to autonomous point transformations and some simple\nnon-autonomous transformations. We discuss our results in the framework of the\nknown literature. There are among them a few new examples of both sine-Gordon\nand Liouville type equations. \n\n"}
{"id": "1810.11641", "contents": "Title: Cross-Modal Distillation for RGB-Depth Person Re-Identification Abstract: Person re-identification is a key challenge for surveillance across multiple\nsensors. Prompted by the advent of powerful deep learning models for visual\nrecognition, and inexpensive RGB-D cameras and sensor-rich mobile robotic\nplatforms, e.g. self-driving vehicles, we investigate the relatively unexplored\nproblem of cross-modal re-identification of persons between RGB (color) and\ndepth images. The considerable divergence in data distributions across\ndifferent sensor modalities introduces additional challenges to the typical\ndifficulties like distinct viewpoints, occlusions, and pose and illumination\nvariation. While some work has investigated re-identification across RGB and\ninfrared, we take inspiration from successes in transfer learning from RGB to\ndepth in object detection tasks. Our main contribution is a novel method for\ncross-modal distillation for robust person re-identification, which learns a\nshared feature representation space of person's appearance in both RGB and\ndepth images. In addition, we propose a cross-modal attention mechanism where\nthe gating signal from one modality can dynamically activate the most\ndiscriminant CNN filters of the other modality. The proposed distillation\nmethod is compared to conventional and deep learning approaches proposed for\nother cross-domain re-identification tasks. Results obtained on the public BIWI\nand RobotPKU datasets indicate that the proposed method can significantly\noutperform the state-of-the-art approaches by up to 16.1% in mean Average\nPrecision (mAP), demonstrating the benefit of the distillation paradigm. The\nexperimental results also indicate that using cross-modal attention allows to\nimprove recognition accuracy considerably with respect to the proposed\ndistillation method and relevant state-of-the-art approaches. \n\n"}
{"id": "1810.12186", "contents": "Title: DeepSphere: Efficient spherical Convolutional Neural Network with\n  HEALPix sampling for cosmological applications Abstract: Convolutional Neural Networks (CNNs) are a cornerstone of the Deep Learning\ntoolbox and have led to many breakthroughs in Artificial Intelligence. These\nnetworks have mostly been developed for regular Euclidean domains such as those\nsupporting images, audio, or video. Because of their success, CNN-based methods\nare becoming increasingly popular in Cosmology. Cosmological data often comes\nas spherical maps, which make the use of the traditional CNNs more complicated.\nThe commonly used pixelization scheme for spherical maps is the Hierarchical\nEqual Area isoLatitude Pixelisation (HEALPix). We present a spherical CNN for\nanalysis of full and partial HEALPix maps, which we call DeepSphere. The\nspherical CNN is constructed by representing the sphere as a graph. Graphs are\nversatile data structures that can act as a discrete representation of a\ncontinuous manifold. Using the graph-based representation, we define many of\nthe standard CNN operations, such as convolution and pooling. With filters\nrestricted to being radial, our convolutions are equivariant to rotation on the\nsphere, and DeepSphere can be made invariant or equivariant to rotation. This\nway, DeepSphere is a special case of a graph CNN, tailored to the HEALPix\nsampling of the sphere. This approach is computationally more efficient than\nusing spherical harmonics to perform convolutions. We demonstrate the method on\na classification problem of weak lensing mass maps from two cosmological models\nand compare the performance of the CNN with that of two baseline classifiers.\nThe results show that the performance of DeepSphere is always superior or equal\nto both of these baselines. For high noise levels and for data covering only a\nsmaller fraction of the sphere, DeepSphere achieves typically 10% better\nclassification accuracy than those baselines. Finally, we show how learned\nfilters can be visualized to introspect the neural network. \n\n"}
{"id": "1810.12575", "contents": "Title: Neural Nearest Neighbors Networks Abstract: Non-local methods exploiting the self-similarity of natural signals have been\nwell studied, for example in image analysis and restoration. Existing\napproaches, however, rely on k-nearest neighbors (KNN) matching in a fixed\nfeature space. The main hurdle in optimizing this feature space w.r.t.\napplication performance is the non-differentiability of the KNN selection rule.\nTo overcome this, we propose a continuous deterministic relaxation of KNN\nselection that maintains differentiability w.r.t. pairwise distances, but\nretains the original KNN as the limit of a temperature parameter approaching\nzero. To exploit our relaxation, we propose the neural nearest neighbors block\n(N3 block), a novel non-local processing layer that leverages the principle of\nself-similarity and can be used as building block in modern neural network\narchitectures. We show its effectiveness for the set reasoning task of\ncorrespondence classification as well as for image restoration, including image\ndenoising and single image super-resolution, where we outperform strong\nconvolutional neural network (CNN) baselines and recent non-local models that\nrely on KNN selection in hand-chosen features spaces. \n\n"}
{"id": "1810.12813", "contents": "Title: Contextual Hourglass Network for Semantic Segmentation of High\n  Resolution Aerial Imagery Abstract: Semantic segmentation for aerial imagery is a challenging and important\nproblem in remotely sensed imagery analysis. In recent years, with the success\nof deep learning, various convolutional neural network (CNN) based models have\nbeen developed. However, due to the varying sizes of the objects and imbalanced\nclass labels, it can be challenging to obtain accurate pixel-wise semantic\nsegmentation results. To address those challenges, we develop a novel semantic\nsegmentation method and call it Contextual Hourglass Network. In our method, in\norder to improve the robustness of the prediction, we design a new contextual\nhourglass module which incorporates attention mechanism on processed\nlow-resolution featuremaps to exploit the contextual semantics. We further\nexploit the stacked encoder-decoder structure by connecting multiple contextual\nhourglass modules from end to end. This architecture can effectively extract\nrich multi-scale features and add more feedback loops for better learning\ncontextual semantics through intermediate supervision. To demonstrate the\nefficacy of our semantic segmentation method, we test it on Potsdam and\nVaihingen datasets. Through the comparisons to other baseline methods, our\nmethod yields the best results on overall performance. \n\n"}
{"id": "1810.13368", "contents": "Title: Generalised Darboux-Koenigs Metrics and 3-Dimensional Superintegrable\n  Systems Abstract: The Darboux-Koenigs metrics in 2D are an important class of conformally flat,\nnon-constant curvature metrics with a single Killing vector and a pair of\nquadratic Killing tensors. In [arXiv:1804.06904] it was shown how to derive\nthese by using the conformal symmetries of the 2D Euclidean metric. In this\npaper we consider the conformal symmetries of the 3D Euclidean metric and\nsimilarly derive a large family of conformally flat metrics possessing between\n1 and 3 Killing vectors (and therefore not constant curvature), together with a\nnumber of quadratic Killing tensors. We refer to these as generalised\nDarboux-Koenigs metrics. We thus construct multi-parameter families of\nsuper-integrable systems in 3 degrees of freedom. Restricting the parameters\nincreases the isometry algebra, which enables us to fully determine the Poisson\nalgebra of first integrals. This larger algebra of isometries is then used to\nreduce from 3 to 2 degrees of freedom, obtaining Darboux-Koenigs kinetic\nenergies with potential functions, which are specific cases of the known\nsuper-integrable potentials. \n\n"}
{"id": "1811.00116", "contents": "Title: Face Recognition: From Traditional to Deep Learning Methods Abstract: Starting in the seventies, face recognition has become one of the most\nresearched topics in computer vision and biometrics. Traditional methods based\non hand-crafted features and traditional machine learning techniques have\nrecently been superseded by deep neural networks trained with very large\ndatasets. In this paper we provide a comprehensive and up-to-date literature\nreview of popular face recognition methods including both traditional\n(geometry-based, holistic, feature-based and hybrid methods) and deep learning\nmethods. \n\n"}
{"id": "1811.00228", "contents": "Title: A sequential guiding network with attention for image captioning Abstract: The recent advances of deep learning in both computer vision (CV) and natural\nlanguage processing (NLP) provide us a new way of understanding semantics, by\nwhich we can deal with more challenging tasks such as automatic description\ngeneration from natural images. In this challenge, the encoder-decoder\nframework has achieved promising performance when a convolutional neural\nnetwork (CNN) is used as image encoder and a recurrent neural network (RNN) as\ndecoder. In this paper, we introduce a sequential guiding network that guides\nthe decoder during word generation. The new model is an extension of the\nencoder-decoder framework with attention that has an additional guiding long\nshort-term memory (LSTM) and can be trained in an end-to-end manner by using\nimage/descriptions pairs. We validate our approach by conducting extensive\nexperiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model\nachieves significant improvement comparing to the other state-of-the-art deep\nlearning models. \n\n"}
{"id": "1811.00367", "contents": "Title: Bi-GANs-ST for Perceptual Image Super-resolution Abstract: Image quality measurement is a critical problem for image super-resolution\n(SR) algorithms. Usually, they are evaluated by some well-known objective\nmetrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results\nin accordance with the perception of human being. Recently, a more reasonable\nperception measurement has been proposed in [1], which is also adopted by the\nPIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a\nhigh-quality SR result which balances between the two indices, i.e., the\nperception index and root-mean-square error (RMSE). To do so, we design a new\ndeep SR framework, dubbed Bi-GANs-ST, by integrating two complementary\ngenerative adversarial networks (GAN) branches. One is memory residual SRGAN\n(MR-SRGAN), which emphasizes on improving the objective performance, such as\nreducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which\nobtains the result that favors better subjective perception via a two-stage\nadversarial training mechanism. Then, to produce final result with excellent\nperception scores and RMSE, we use soft-thresholding method to merge the\nresults generated by the two GANs. Our method performs well on the perceptual\nimage super-resolution task of the PIRM 2018 challenge. Experimental results on\nfive benchmarks show that our proposal achieves highly competent performance\ncompared with other state-of-the-art methods. \n\n"}
{"id": "1811.02658", "contents": "Title: When Not to Classify: Detection of Reverse Engineering Attacks on DNN\n  Image Classifiers Abstract: This paper addresses detection of a reverse engineering (RE) attack targeting\na deep neural network (DNN) image classifier; by querying, RE's aim is to\ndiscover the classifier's decision rule. RE can enable test-time evasion\nattacks, which require knowledge of the classifier. Recently, we proposed a\nquite effective approach (ADA) to detect test-time evasion attacks. In this\npaper, we extend ADA to detect RE attacks (ADA-RE). We demonstrate our method\nis successful in detecting \"stealthy\" RE attacks before they learn enough to\nlaunch effective test-time evasion attacks. \n\n"}
{"id": "1811.02942", "contents": "Title: Multi-branch Convolutional Neural Network for Multiple Sclerosis Lesion\n  Segmentation Abstract: In this paper, we present an automated approach for segmenting multiple\nsclerosis (MS) lesions from multi-modal brain magnetic resonance images. Our\nmethod is based on a deep end-to-end 2D convolutional neural network (CNN) for\nslice-based segmentation of 3D volumetric data. The proposed CNN includes a\nmulti-branch downsampling path, which enables the network to encode information\nfrom multiple modalities separately. Multi-scale feature fusion blocks are\nproposed to combine feature maps from different modalities at different stages\nof the network. Then, multi-scale feature upsampling blocks are introduced to\nupsize combined feature maps to leverage information from lesion shape and\nlocation. We trained and tested the proposed model using orthogonal plane\norientations of each 3D modality to exploit the contextual information in all\ndirections. The proposed pipeline is evaluated on two different datasets: a\nprivate dataset including 37 MS patients and a publicly available dataset known\nas the ISBI 2015 longitudinal MS lesion segmentation challenge dataset,\nconsisting of 14 MS patients. Considering the ISBI challenge, at the time of\nsubmission, our method was amongst the top performing solutions. On the private\ndataset, using the same array of performance metrics as in the ISBI challenge,\nthe proposed approach shows high improvements in MS lesion segmentation\ncompared with other publicly available tools. \n\n"}
{"id": "1811.03621", "contents": "Title: Satyam: Democratizing Groundtruth for Machine Vision Abstract: The democratization of machine learning (ML) has led to ML-based machine\nvision systems for autonomous driving, traffic monitoring, and video\nsurveillance. However, true democratization cannot be achieved without greatly\nsimplifying the process of collecting groundtruth for training and testing\nthese systems. This groundtruth collection is necessary to ensure good\nperformance under varying conditions. In this paper, we present the design and\nevaluation of Satyam, a first-of-its-kind system that enables a layperson to\nlaunch groundtruth collection tasks for machine vision with minimal effort.\nSatyam leverages a crowdtasking platform, Amazon Mechanical Turk, and automates\nseveral challenging aspects of groundtruth collection: creating and launching\nof custom web-UI tasks for obtaining the desired groundtruth, controlling\nresult quality in the face of spammers and untrained workers, adapting prices\nto match task complexity, filtering spammers and workers with poor performance,\nand processing worker payments. We validate Satyam using several popular\nbenchmark vision datasets, and demonstrate that groundtruth obtained by Satyam\nis comparable to that obtained from trained experts and provides matching ML\nperformance when used for training. \n\n"}
{"id": "1811.04323", "contents": "Title: Fully Convolutional Network with Multi-Step Reinforcement Learning for\n  Image Processing Abstract: This paper tackles a new problem setting: reinforcement learning with\npixel-wise rewards (pixelRL) for image processing. After the introduction of\nthe deep Q-network, deep RL has been achieving great success. However, the\napplications of deep RL for image processing are still limited. Therefore, we\nextend deep RL to pixelRL for various image processing applications. In\npixelRL, each pixel has an agent, and the agent changes the pixel value by\ntaking an action. We also propose an effective learning method for pixelRL that\nsignificantly improves the performance by considering not only the future\nstates of the own pixel but also those of the neighbor pixels. The proposed\nmethod can be applied to some image processing tasks that require pixel-wise\nmanipulations, where deep RL has never been applied. We apply the proposed\nmethod to three image processing tasks: image denoising, image restoration, and\nlocal color enhancement. Our experimental results demonstrate that the proposed\nmethod achieves comparable or better performance, compared with the\nstate-of-the-art methods based on supervised learning. \n\n"}
{"id": "1811.04535", "contents": "Title: Road Damage Detection And Classification In Smartphone Captured Images\n  Using Mask R-CNN Abstract: This paper summarizes the design, experiments and results of our solution to\nthe Road Damage Detection and Classification Challenge held as part of the 2018\nIEEE International Conference On Big Data Cup. Automatic detection and\nclassification of damage in roads is an essential problem for multiple\napplications like maintenance and autonomous driving. We demonstrate that\nconvolutional neural net based instance detection and classfication approaches\ncan be used to solve this problem. In particular we show that Mask-RCNN, one of\nthe state-of-the-art algorithms for object detection, localization and instance\nsegmentation of natural images, can be used to perform this task in a fast\nmanner with effective results. We achieve a mean F1 score of 0.528 at an IoU of\n50% on the task of detection and classification of different types of damages\nin real-world road images acquired using a smartphone camera and our average\ninference time for each image is 0.105 seconds on an NVIDIA GeForce 1080Ti\ngraphic card. The code and saved models for our approach can be found here :\nhttps://github.com/sshkhr/BigDataCup18 Submission \n\n"}
{"id": "1811.05013", "contents": "Title: Blindfold Baselines for Embodied QA Abstract: We explore blindfold (question-only) baselines for Embodied Question\nAnswering. The EmbodiedQA task requires an agent to answer a question by\nintelligently navigating in a simulated environment, gathering necessary visual\ninformation only through first-person vision before finally answering.\nConsequently, a blindfold baseline which ignores the environment and visual\ninformation is a degenerate solution, yet we show through our experiments on\nthe EQAv1 dataset that a simple question-only baseline achieves\nstate-of-the-art results on the EmbodiedQA task in all cases except when the\nagent is spawned extremely close to the object. \n\n"}
{"id": "1811.05027", "contents": "Title: Deep Neural Network Augmentation: Generating Faces for Affect Analysis Abstract: This paper presents a novel approach for synthesizing facial affect; either\nin terms of the six basic expressions (i.e., anger, disgust, fear, joy, sadness\nand surprise), or in terms of valence (i.e., how positive or negative is an\nemotion) and arousal (i.e., power of the emotion activation). The proposed\napproach accepts the following inputs: i) a neutral 2D image of a person; ii) a\nbasic facial expression or a pair of valence-arousal (VA) emotional state\ndescriptors to be generated, or a path of affect in the 2D VA Space to be\ngenerated as an image sequence. In order to synthesize affect in terms of VA,\nfor this person, $600,000$ frames from the 4DFAB database were annotated. The\naffect synthesis is implemented by fitting a 3D Morphable Model on the neutral\nimage, then deforming the reconstructed face and adding the inputted affect,\nand blending the new face with the given affect into the original image.\nQualitative experiments illustrate the generation of realistic images, when the\nneutral image is sampled from thirteen well known lab-controlled or in-the-wild\ndatabases, including Aff-Wild, AffectNet, RAF-DB; comparisons with Generative\nAdversarial Networks (GANs) show the higher quality achieved by the proposed\napproach. Then, quantitative experiments are conducted, in which the\nsynthesized images are used for data augmentation in training Deep Neural\nNetworks to perform affect recognition over all databases; greatly improved\nperformances are achieved when compared with state-of-the-art methods, as well\nas with GAN-based data augmentation, in all cases. \n\n"}
{"id": "1811.05760", "contents": "Title: A Multimodal Approach towards Emotion Recognition of Music using Audio\n  and Lyrical Content Abstract: We propose MoodNet - A Deep Convolutional Neural Network based architecture\nto effectively predict the emotion associated with a piece of music given its\naudio and lyrical content.We evaluate different architectures consisting of\nvarying number of two-dimensional convolutional and subsampling layers,followed\nby dense layers.We use Mel-Spectrograms to represent the audio content and word\nembeddings-specifically 100 dimensional word vectors, to represent the textual\ncontent represented by the lyrics.We feed input data from both modalities to\nour MoodNet architecture.The output from both the modalities are then fused as\na fully connected layer and softmax classfier is used to predict the category\nof emotion.Using F1-score as our metric,our results show excellent performance\nof MoodNet over the two datasets we experimented on-The MIREX Multimodal\ndataset and the Million Song Dataset.Our experiments reflect the hypothesis\nthat more complex models perform better with more training data.We also observe\nthat lyrics outperform audio as a better expressed modality and conclude that\ncombining and using features from multiple modalities for prediction tasks\nresult in superior performance in comparison to using a single modality as\ninput. \n\n"}
{"id": "1811.06981", "contents": "Title: Learned Video Compression Abstract: We present a new algorithm for video coding, learned end-to-end for the\nlow-latency mode. In this setting, our approach outperforms all existing video\ncodecs across nearly the entire bitrate range. To our knowledge, this is the\nfirst ML-based method to do so.\n  We evaluate our approach on standard video compression test sets of varying\nresolutions, and benchmark against all mainstream commercial codecs, in the\nlow-latency mode. On standard-definition videos, relative to our algorithm,\nHEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On\nhigh-definition 1080p videos, H.265 and VP9 typically produce codes up to 20%\nlarger, and H.264 up to 35% larger. Furthermore, our approach does not suffer\nfrom blocking artifacts and pixelation, and thus produces videos that are more\nvisually pleasing.\n  We propose two main contributions. The first is a novel architecture for\nvideo compression, which (1) generalizes motion estimation to perform any\nlearned compensation beyond simple translations, (2) rather than strictly\nrelying on previously transmitted reference frames, maintains a state of\narbitrary information learned by the model, and (3) enables jointly compressing\nall transmitted signals (such as optical flow and residual).\n  Secondly, we present a framework for ML-based spatial rate control: namely, a\nmechanism for assigning variable bitrates across space for each frame. This is\na critical component for video coding, which to our knowledge had not been\ndeveloped within a machine learning setting. \n\n"}
{"id": "1811.07122", "contents": "Title: Dynamics Motivated by Sierpinski Fractals Abstract: Fatou-Julia iteration (FJI) is an effective instrument to construct fractals.\nFamous Julia and Mandelbrot sets are strong confirmations of this. In the\npresent study, we use the paradigm of FJI to construct and map Sierpinski\nfractals. The fractals can be mapped by developing a mapping iteration on the\nbasis of FJI. Because of the close link between mappings, differential\nequations and dynamical systems, one can introduce dynamics for a fractal\nthrough differential equations such that it becomes points of the solution\ntrajectory. Thus, in this paper, we consider two types of dynamics motivated by\nthe Sierpinski fractals. The first one is the dynamics of FJI itself, and the\nsecond one is the dynamics of a fractal mapping iteration which can be\nperformed through differential equations. The characterization of fractals as\ntrajectory points of the dynamics can help to enhance and widen the scope of\ntheir applications in physics and engineering. \n\n"}
{"id": "1811.07793", "contents": "Title: DeepIR: A Deep Semantics Driven Framework for Image Retargeting Abstract: We present \\emph{Deep Image Retargeting} (\\emph{DeepIR}), a coarse-to-fine\nframework for content-aware image retargeting. Our framework first constructs\nthe semantic structure of input image with a deep convolutional neural network.\nThen a uniform re-sampling that suits for semantic structure preserving is\ndevised to resize feature maps to target aspect ratio at each feature layer.\nThe final retargeting result is generated by coarse-to-fine nearest neighbor\nfield search and step-by-step nearest neighbor field fusion. We empirically\ndemonstrate the effectiveness of our model with both qualitative and\nquantitative results on widely used RetargetMe dataset. \n\n"}
{"id": "1811.08230", "contents": "Title: Event-based High Dynamic Range Image and Very High Frame Rate Video\n  Generation using Conditional Generative Adversarial Networks Abstract: Event cameras have a lot of advantages over traditional cameras, such as low\nlatency, high temporal resolution, and high dynamic range. However, since the\noutputs of event cameras are the sequences of asynchronous events overtime\nrather than actual intensity images, existing algorithms could not be directly\napplied. Therefore, it is demanding to generate intensity images from events\nfor other tasks. In this paper, we unlock the potential of event camera-based\nconditional generative adversarial networks to create images/videos from an\nadjustable portion of the event data stream. The stacks of space-time\ncoordinates of events are used as inputs and the network is trained to\nreproduce images based on the spatio-temporal intensity changes. The usefulness\nof event cameras to generate high dynamic range(HDR) images even in extreme\nillumination conditions and also non blurred images under rapid motion is also\nshown.In addition, the possibility of generating very high frame rate videos is\ndemonstrated, theoretically up to 1 million frames per second (FPS) since the\ntemporal resolution of event cameras are about 1{\\mu}s. Proposed methods are\nevaluated by comparing the results with the intensity images captured on the\nsame pixel grid-line of events using online available real datasets and\nsynthetic datasets produced by the event camera simulator. \n\n"}
{"id": "1811.08412", "contents": "Title: A Baseline for Multi-Label Image Classification Using An Ensemble of\n  Deep Convolutional Neural Networks Abstract: Recent studies on multi-label image classification have focused on designing\nmore complex architectures of deep neural networks such as the use of attention\nmechanisms and region proposal networks. Although performance gains have been\nreported, the backbone deep models of the proposed approaches and the\nevaluation metrics employed in different works vary, making it difficult to\ncompare each fairly. Moreover, due to the lack of properly investigated\nbaselines, the advantage introduced by the proposed techniques are often\nambiguous. To address these issues, we make a thorough investigation of the\nmainstream deep convolutional neural network architectures for multi-label\nimage classification and present a strong baseline. With the use of proper data\naugmentation techniques and model ensembles, the basic deep architectures can\nachieve better performance than many existing more complex ones on three\nbenchmark datasets, providing great insight for the future studies on\nmulti-label image classification. \n\n"}
{"id": "1811.08594", "contents": "Title: Learning to Attend Relevant Regions in Videos from Eye Fixations Abstract: Attentively important regions in video frames account for a majority part of\nthe semantics in each frame. This information is helpful in many applications\nnot only for entertainment (such as auto generating commentary and tourist\nguide) but also for robotic control which holds a larascope supported for\nlaparoscopic surgery. However, it is not always straightforward to define and\nlocate such semantic regions in videos. In this work, we attempt to address the\nproblem of attending relevant regions in videos by leveraging the eye fixations\nlabels with a RNN-based visual attention model. Our experimental results\nsuggest that this approach holds a good potential to learn to attend semantic\nregions in videos while its performance also heavily relies on the quality of\neye fixations labels. \n\n"}
{"id": "1811.08632", "contents": "Title: A Deep Tree-Structured Fusion Model for Single Image Deraining Abstract: We propose a simple yet effective deep tree-structured fusion model based on\nfeature aggregation for the deraining problem. We argue that by effectively\naggregating features, a relatively simple network can still handle tough image\nderaining problems well. First, to capture the spatial structure of rain we use\ndilated convolutions as our basic network block. We then design a\ntree-structured fusion architecture which is deployed within each block\n(spatial information) and across all blocks (content information). Our method\nis based on the assumption that adjacent features contain redundant\ninformation. This redundancy obstructs generation of new representations and\ncan be reduced by hierarchically fusing adjacent features. Thus, the proposed\nmodel is more compact and can effectively use spatial and content information.\nExperiments on synthetic and real-world datasets show that our network achieves\nbetter deraining results with fewer parameters. \n\n"}
{"id": "1811.09134", "contents": "Title: IEGAN: Multi-purpose Perceptual Quality Image Enhancement Using\n  Generative Adversarial Network Abstract: Despite the breakthroughs in quality of image enhancement, an end-to-end\nsolution for simultaneous recovery of the finer texture details and sharpness\nfor degraded images with low resolution is still unsolved. Some existing\napproaches focus on minimizing the pixel-wise reconstruction error which\nresults in a high peak signal-to-noise ratio. The enhanced images fail to\nprovide high-frequency details and are perceptually unsatisfying, i.e., they\nfail to match the quality expected in a photo-realistic image. In this paper,\nwe present Image Enhancement Generative Adversarial Network (IEGAN), a\nversatile framework capable of inferring photo-realistic natural images for\nboth artifact removal and super-resolution simultaneously. Moreover, we propose\na new loss function consisting of a combination of reconstruction loss, feature\nloss and an edge loss counterpart. The feature loss helps to push the output\nimage to the natural image manifold and the edge loss preserves the sharpness\nof the output image. The reconstruction loss provides low-level semantic\ninformation to the generator regarding the quality of the generated images\ncompared to the original. Our approach has been experimentally proven to\nrecover photo-realistic textures from heavily compressed low-resolution images\non public benchmarks and our proposed high-resolution World100 dataset. \n\n"}
{"id": "1811.09310", "contents": "Title: Parametric Noise Injection: Trainable Randomness to Improve Deep Neural\n  Network Robustness against Adversarial Attack Abstract: Recent development in the field of Deep Learning have exposed the underlying\nvulnerability of Deep Neural Network (DNN) against adversarial examples. In\nimage classification, an adversarial example is a carefully modified image that\nis visually imperceptible to the original image but can cause DNN model to\nmisclassify it. Training the network with Gaussian noise is an effective\ntechnique to perform model regularization, thus improving model robustness\nagainst input variation. Inspired by this classical method, we explore to\nutilize the regularization characteristic of noise injection to improve DNN's\nrobustness against adversarial attack. In this work, we propose\nParametric-Noise-Injection (PNI) which involves trainable Gaussian noise\ninjection at each layer on either activation or weights through solving the\nmin-max optimization problem, embedded with adversarial training. These\nparameters are trained explicitly to achieve improved robustness. To the best\nof our knowledge, this is the first work that uses trainable noise injection to\nimprove network robustness against adversarial attacks, rather than manually\nconfiguring the injected noise level through cross-validation. The extensive\nresults show that our proposed PNI technique effectively improves the\nrobustness against a variety of powerful white-box and black-box attacks such\nas PGD, C & W, FGSM, transferable attack and ZOO attack. Last but not the\nleast, PNI method improves both clean- and perturbed-data accuracy in\ncomparison to the state-of-the-art defense methods, which outperforms current\nunbroken PGD defense by 1.1 % and 6.8 % on clean test data and perturbed test\ndata respectively using Resnet-20 architecture. \n\n"}
{"id": "1811.09426", "contents": "Title: Joint Neural Architecture Search and Quantization Abstract: Designing neural architectures is a fundamental step in deep learning\napplications. As a partner technique, model compression on neural networks has\nbeen widely investigated to gear the needs that the deep learning algorithms\ncould be run with the limited computation resources on mobile devices.\nCurrently, both the tasks of architecture design and model compression require\nexpertise tricks and tedious trials. In this paper, we integrate these two\ntasks into one unified framework, which enables the joint architecture search\nwith quantization (compression) policies for neural networks. This method is\nnamed JASQ. Here our goal is to automatically find a compact neural network\nmodel with high performance that is suitable for mobile devices. Technically, a\nmulti-objective evolutionary search algorithm is introduced to search the\nmodels under the balance between model size and performance accuracy. In\nexperiments, we find that our approach outperforms the methods that search only\nfor architectures or only for quantization policies. 1) Specifically, given\nexisting networks, our approach can provide them with learning-based\nquantization policies, and outperforms their 2 bits, 4 bits, 8 bits, and 16\nbits counterparts. It can yield higher accuracies than the float models, for\nexample, over 1.02% higher accuracy on MobileNet-v1. 2) What is more, under the\nbalance between model size and performance accuracy, two models are obtained\nwith joint search of architectures and quantization policies: a high-accuracy\nmodel and a small model, JASQNet and JASQNet-Small that achieves 2.97% error\nrate with 0.9 MB on CIFAR-10. \n\n"}
{"id": "1811.09655", "contents": "Title: Unsupervised brain lesion segmentation from MRI using a convolutional\n  autoencoder Abstract: Lesions that appear hyperintense in both Fluid Attenuated Inversion Recovery\n(FLAIR) and T2-weighted magnetic resonance images (MRIs) of the human brain are\ncommon in the brains of the elderly population and may be caused by ischemia or\ndemyelination. Lesions are biomarkers for various neurodegenerative diseases,\nmaking accurate quantification of them important for both disease diagnosis and\nprogression. Automatic lesion detection using supervised learning requires\nmanually annotated images, which can often be impractical to acquire.\nUnsupervised lesion detection, on the other hand, does not require any manual\ndelineation; however, these methods can be challenging to construct due to the\nvariability in lesion load, placement of lesions, and voxel intensities. Here\nwe present a novel approach to address this problem using a convolutional\nautoencoder, which learns to segment brain lesions as well as the white matter,\ngray matter, and cerebrospinal fluid by reconstructing FLAIR images as conical\ncombinations of softmax layer outputs generated from the corresponding T1, T2,\nand FLAIR images. Some of the advantages of this model are that it accurately\nlearns to segment lesions regardless of lesion load, and it can be used to\nquickly and robustly segment new images that were not in the training set.\nComparisons with state-of-the-art segmentation methods evaluated on ground\ntruth manual labels indicate that the proposed method works well for generating\naccurate lesion segmentations without the need for manual annotations. \n\n"}
{"id": "1811.09763", "contents": "Title: Mean Local Group Average Precision (mLGAP): A New Performance Metric for\n  Hashing-based Retrieval Abstract: The research on hashing techniques for visual data is gaining increased\nattention in recent years due to the need for compact representations\nsupporting efficient search/retrieval in large-scale databases such as online\nimages. Among many possibilities, Mean Average Precision(mAP) has emerged as\nthe dominant performance metric for hashing-based retrieval. One glaring\nshortcoming of mAP is its inability in balancing retrieval accuracy and\nutilization of hash codes: pushing a system to attain higher mAP will\ninevitably lead to poorer utilization of the hash codes. Poor utilization of\nthe hash codes hinders good retrieval because of increased collision of samples\nin the hash space. This means that a model giving a higher mAP values does not\nnecessarily do a better job in retrieval. In this paper, we introduce a new\nmetric named Mean Local Group Average Precision (mLGAP) for better evaluation\nof the performance of hashing-based retrieval. The new metric provides a\nretrieval performance measure that also reconciles the utilization of hash\ncodes, leading to a more practically meaningful performance metric than\nconventional ones like mAP. To this end, we start by mathematical analysis of\nthe deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and\nshow why it is more appropriate for hashing-based retrieval. Experiments on\nimage retrieval are used to demonstrate the effectiveness of the proposed\nmetric. \n\n"}
{"id": "1811.09862", "contents": "Title: On Periodic Functions as Regularizers for Quantization of Neural\n  Networks Abstract: Deep learning models have been successfully used in computer vision and many\nother fields. We propose an unorthodox algorithm for performing quantization of\nthe model parameters. In contrast with popular quantization schemes based on\nthresholds, we use a novel technique based on periodic functions, such as\ncontinuous trigonometric sine or cosine as well as non-continuous hat\nfunctions. We apply these functions component-wise and add the sum over the\nmodel parameters as a regularizer to the model loss during training. The\nfrequency and amplitude hyper-parameters of these functions can be adjusted\nduring training. The regularization pushes the weights into discrete points\nthat can be encoded as integers. We show that using this technique the\nresulting quantized models exhibit the same accuracy as the original ones on\nCIFAR-10 and ImageNet datasets. \n\n"}
{"id": "1811.09897", "contents": "Title: Conditional Recurrent Flow: Conditional Generation of Longitudinal\n  Samples with Applications to Neuroimaging Abstract: Generative models using neural network have opened a door to large-scale\nstudies for various application domains, especially for studies that suffer\nfrom lack of real samples to obtain statistically robust inference. Typically,\nthese generative models would train on existing data to learn the underlying\ndistribution of the measurements (e.g., images) in latent spaces conditioned on\ncovariates (e.g., image labels), and generate independent samples that are\nidentically distributed in the latent space. Such models may work for\ncross-sectional studies, however, they are not suitable to generate data for\nlongitudinal studies that focus on \"progressive\" behavior in a sequence of\ndata. In practice, this is a quite common case in various neuroimaging studies\nwhose goal is to characterize a trajectory of pathologies of a specific disease\neven from early stages. This may be too ambitious especially when the sample\nsize is small (e.g., up to a few hundreds). Motivated from the setup above, we\nseek to develop a conditional generative model for longitudinal data generation\nby designing an invertable neural network. Inspired by recurrent nature of\nlongitudinal data, we propose a novel neural network that incorporates\nrecurrent subnetwork and context gating to include smooth transition in a\nsequence of generated data. Our model is validated on a video sequence dataset\nand a longitudinal AD dataset with various experimental settings for\nqualitative and quantitative evaluations of the generated samples. The results\nwith the AD dataset captures AD specific group differences with sufficiently\ngenerated longitudinal samples that are consistent with existing literature,\nwhich implies a great potential to be applicable to other disease studies. \n\n"}
{"id": "1811.09986", "contents": "Title: Learning Conditional Random Fields with Augmented Observations for\n  Partially Observed Action Recognition Abstract: This paper aims at recognizing partially observed human actions in videos.\nAction videos acquired in uncontrolled environments often contain corrupt\nframes, which make actions partially observed. Furthermore, these frames can\nlast for arbitrary lengths of time and appear irregularly. They are\ninconsistent with training data and degrade the performance of pre-trained\naction recognition systems. We present an approach to address this issue. For\neach training and testing actions, we divide it into segments and explore the\nmutual dependency between temporal segments. This property states that the\nsimilarity of two actions at one segment often implies their similarity at\nanother. We augment each segment with extra alternatives retrieved from\ntraining data. The augmentation algorithm is designed in a way where a few\nalternatives are good enough to replace the original segment where corrupt\nframes occur. Our approach is developed upon hidden conditional random fields\nand leverages the flexibility of hidden variables for uncertainty handling. It\nturns out that our approach integrates corrupt segment detection and\nalternative selection into the process of prediction, and can recognize\npartially observed actions more accurately. It is evaluated on both fully\nobserved actions and partially observed ones with either synthetic or real\ncorrupt frames. The experimental results manifest its general applicability and\nsuperior performance, especially when corrupt frames are present in the action\nvideos. \n\n"}
{"id": "1811.11459", "contents": "Title: Coordinate-based Texture Inpainting for Pose-Guided Image Generation Abstract: We present a new deep learning approach to pose-guided resynthesis of human\nphotographs. At the heart of the new approach is the estimation of the complete\nbody surface texture based on a single photograph. Since the input photograph\nalways observes only a part of the surface, we suggest a new inpainting method\nthat completes the texture of the human body. Rather than working directly with\ncolors of texture elements, the inpainting network estimates an appropriate\nsource location in the input image for each element of the body surface. This\ncorrespondence field between the input image and the texture is then further\nwarped into the target image coordinate frame based on the desired pose,\neffectively establishing the correspondence between the source and the target\nview even when the pose change is drastic. The final convolutional network then\nuses the established correspondence and all other available information to\nsynthesize the output image. A fully-convolutional architecture with deformable\nskip connections guided by the estimated correspondence field is used. We show\nstate-of-the-art result for pose-guided image synthesis. Additionally, we\ndemonstrate the performance of our system for garment transfer and pose-guided\nface resynthesis. \n\n"}
{"id": "1811.11606", "contents": "Title: Escaping Plato's Cave: 3D Shape From Adversarial Rendering Abstract: We introduce PlatonicGAN to discover the 3D structure of an object class from\nan unstructured collection of 2D images, i.e., where no relation between photos\nis known, except that they are showing instances of the same category. The key\nidea is to train a deep neural network to generate 3D shapes which, when\nrendered to images, are indistinguishable from ground truth images (for a\ndiscriminator) under various camera poses. Discriminating 2D images instead of\n3D shapes allows tapping into unstructured 2D photo collections instead of\nrelying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish\nconstraints between 2D image observation and their 3D interpretation, we\nsuggest a family of rendering layers that are effectively differentiable. This\nfamily includes visual hull, absorption-only (akin to x-ray), and\nemission-absorption. We can successfully reconstruct 3D shapes from\nunstructured 2D images and extensively evaluate PlatonicGAN on a range of\nsynthetic and real data sets achieving consistent improvements over baseline\nmethods. We further show that PlatonicGAN can be combined with 3D supervision\nto improve on and in some cases even surpass the quality of 3D-supervised\nmethods. \n\n"}
{"id": "1811.11849", "contents": "Title: Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on\n  Crowd Videos Abstract: Group-level emotion recognition (ER) is a growing research area as the\ndemands for assessing crowds of all sizes are becoming an interest in both the\nsecurity arena as well as social media. This work extends the earlier ER\ninvestigations, which focused on either group-level ER on single images or\nwithin a video, by fully investigating group-level expression recognition on\ncrowd videos. In this paper, we propose an effective deep feature level fusion\nmechanism to model the spatial-temporal information in the crowd videos. In our\napproach, the fusing process is performed on the deep feature domain by a\ngenerative probabilistic model, Non-Volume Preserving Fusion (NVPF), that\nmodels spatial information relationships. Furthermore, we extend our proposed\nspatial NVPF approach to the spatial-temporal NVPF approach to learn the\ntemporal information between frames. To demonstrate the robustness and\neffectiveness of each component in the proposed approach, three experiments\nwere conducted: (i) evaluation on AffectNet database to benchmark the proposed\nEmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to\nbenchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)\nexamine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos\n(GECV) dataset composed of 627 videos collected from publicly available\nsources. GECV dataset is a collection of videos containing crowds of people.\nEach video is labeled with emotion categories at three levels: individual\nfaces, group of people, and the entire video frame. \n\n"}
{"id": "1811.11971", "contents": "Title: Simple stopping criteria for information theoretic feature selection Abstract: Feature selection aims to select the smallest feature subset that yields the\nminimum generalization error. In the rich literature in feature selection,\ninformation theory-based approaches seek a subset of features such that the\nmutual information between the selected features and the class labels is\nmaximized. Despite the simplicity of this objective, there still remain several\nopen problems in optimization. These include, for example, the automatic\ndetermination of the optimal subset size (i.e., the number of features) or a\nstopping criterion if the greedy searching strategy is adopted. In this paper,\nwe suggest two stopping criteria by just monitoring the conditional mutual\ninformation (CMI) among groups of variables. Using the recently developed\nmultivariate matrix-based Renyi's \\alpha-entropy functional, which can be\ndirectly estimated from data samples, we showed that the CMI among groups of\nvariables can be easily computed without any decomposition or approximation,\nhence making our criteria easy to implement and seamlessly integrated into any\nexisting information theoretic feature selection methods with a greedy search\nstrategy. \n\n"}
{"id": "1811.12152", "contents": "Title: Efficient Coarse-to-Fine Non-Local Module for the Detection of Small\n  Objects Abstract: An image is not just a collection of objects, but rather a graph where each\nobject is related to other objects through spatial and semantic relations.\nUsing relational reasoning modules, such as the non-local module\n\\cite{wang2017non}, can therefore improve object detection. Current schemes\napply such dedicated modules either to a specific layer of the bottom-up\nstream, or between already-detected objects. We show that the relational\nprocess can be better modeled in a coarse-to-fine manner and present a novel\nframework, applying a non-local module sequentially to increasing resolution\nfeature maps along the top-down stream. In this way, information can naturally\npassed from larger objects to smaller related ones. Applying the module to fine\nfeature maps further allows the information to pass between the small objects\nthemselves, exploiting repetitions of instances of the same class. In practice,\ndue to the expensive memory utilization of the non-local module, it is\ninfeasible to apply the module as currently used to high-resolution feature\nmaps. We redesigned the non local module, improved it in terms of memory and\nnumber of operations, allowing it to be placed anywhere along the network. We\nfurther incorporated relative spatial information into the module, in a manner\nthat can be incorporated into our efficient implementation. We show the\neffectiveness of our scheme by improving the results of detecting small objects\non COCO by 1-2 AP points over Faster and Mask RCNN and by 1 AP over using\nnon-local module on the bottom-up stream. \n\n"}
{"id": "1811.12238", "contents": "Title: Perceiving Physical Equation by Observing Visual Scenarios Abstract: Inferring universal laws of the environment is an important ability of human\nintelligence as well as a symbol of general AI. In this paper, we take a step\ntoward this goal such that we introduce a new challenging problem of inferring\ninvariant physical equation from visual scenarios. For instance, teaching a\nmachine to automatically derive the gravitational acceleration formula by\nwatching a free-falling object. To tackle this challenge, we present a novel\npipeline comprised of an Observer Engine and a Physicist Engine by respectively\nimitating the actions of an observer and a physicist in the real world.\nGenerally, the Observer Engine watches the visual scenarios and then extracting\nthe physical properties of objects. The Physicist Engine analyses these data\nand then summarizing the inherent laws of object dynamics. Specifically, the\nlearned laws are expressed by mathematical equations such that they are more\ninterpretable than the results given by common probabilistic models.\nExperiments on synthetic videos have shown that our pipeline is able to\ndiscover physical equations on various physical worlds with different visual\nappearances. \n\n"}
{"id": "1811.12463", "contents": "Title: Fast and Flexible Indoor Scene Synthesis via Deep Convolutional\n  Generative Models Abstract: We present a new, fast and flexible pipeline for indoor scene synthesis that\nis based on deep convolutional generative models. Our method operates on a\ntop-down image-based representation, and inserts objects iteratively into the\nscene by predicting their category, location, orientation and size with\nseparate neural network modules. Our pipeline naturally supports automatic\ncompletion of partial scenes, as well as synthesis of complete scenes. Our\nmethod is significantly faster than the previous image-based method and\ngenerates result that outperforms it and other state-of-the-art deep generative\nscene models in terms of faithfulness to training data and perceived visual\nquality. \n\n"}
{"id": "1811.12751", "contents": "Title: Domain-Invariant Adversarial Learning for Unsupervised Domain Adaption Abstract: Unsupervised domain adaption aims to learn a powerful classifier for the\ntarget domain given a labeled source data set and an unlabeled target data set.\nTo alleviate the effect of `domain shift', the major challenge in domain\nadaptation, studies have attempted to align the distributions of the two\ndomains. Recent research has suggested that generative adversarial network\n(GAN) has the capability of implicitly capturing data distribution. In this\npaper, we thus propose a simple but effective model for unsupervised domain\nadaption leveraging adversarial learning. The same encoder is shared between\nthe source and target domains which is expected to extract domain-invariant\nrepresentations with the help of an adversarial discriminator. With the labeled\nsource data, we introduce the center loss to increase the discriminative power\nof feature learned. We further align the conditional distribution of the two\ndomains to enforce the discrimination of the features in the target domain.\nUnlike previous studies where the source features are extracted with a fixed\npre-trained encoder, our method jointly learns feature representations of two\ndomains. Moreover, by sharing the encoder, the model does not need to know the\nsource of images during testing and hence is more widely applicable. We\nevaluate the proposed method on several unsupervised domain adaption benchmarks\nand achieve superior or comparable performance to state-of-the-art results. \n\n"}
{"id": "1811.12817", "contents": "Title: Practical Full Resolution Learned Lossless Image Compression Abstract: We propose the first practical learned lossless image compression system,\nL3C, and show that it outperforms the popular engineered codecs, PNG, WebP and\nJPEG 2000. At the core of our method is a fully parallelizable hierarchical\nprobabilistic model for adaptive entropy coding which is optimized end-to-end\nfor the compression task. In contrast to recent autoregressive discrete\nprobabilistic models such as PixelCNN, our method i) models the image\ndistribution jointly with learned auxiliary representations instead of\nexclusively modeling the image distribution in RGB space, and ii) only requires\nthree forward-passes to predict all pixel probabilities instead of one for each\npixel. As a result, L3C obtains over two orders of magnitude speedups when\nsampling compared to the fastest PixelCNN variant (Multiscale-PixelCNN).\nFurthermore, we find that learning the auxiliary representation is crucial and\noutperforms predefined auxiliary representations such as an RGB pyramid\nsignificantly. \n\n"}
{"id": "1812.00037", "contents": "Title: Adversarial Defense by Stratified Convolutional Sparse Coding Abstract: We propose an adversarial defense method that achieves state-of-the-art\nperformance among attack-agnostic adversarial defense methods while also\nmaintaining robustness to input resolution, scale of adversarial perturbation,\nand scale of dataset size. Based on convolutional sparse coding, we construct a\nstratified low-dimensional quasi-natural image space that faithfully\napproximates the natural image space while also removing adversarial\nperturbations. We introduce a novel Sparse Transformation Layer (STL) in\nbetween the input image and the first layer of the neural network to\nefficiently project images into our quasi-natural image space. Our experiments\nshow state-of-the-art performance of our method compared to other\nattack-agnostic adversarial defense methods in various adversarial settings. \n\n"}
{"id": "1812.00231", "contents": "Title: InGAN: Capturing and Remapping the \"DNA\" of a Natural Image Abstract: Generative Adversarial Networks (GANs) typically learn a distribution of\nimages in a large image dataset, and are then able to generate new images from\nthis distribution. However, each natural image has its own internal statistics,\ncaptured by its unique distribution of patches. In this paper we propose an\n\"Internal GAN\" (InGAN) - an image-specific GAN - which trains on a single input\nimage and learns its internal distribution of patches. It is then able to\nsynthesize a plethora of new natural images of significantly different sizes,\nshapes and aspect-ratios - all with the same internal patch-distribution (same\n\"DNA\") as the input image. In particular, despite large changes in global\nsize/shape of the image, all elements inside the image maintain their local\nsize/shape. InGAN is fully unsupervised, requiring no additional data other\nthan the input image itself. Once trained on the input image, it can remap the\ninput to any size or shape in a single feedforward pass, while preserving the\nsame internal patch distribution. InGAN provides a unified framework for a\nvariety of tasks, bridging the gap between textures and natural images. \n\n"}
{"id": "1812.00253", "contents": "Title: A Deep Learning Approach for Multi-View Engagement Estimation of\n  Children in a Child-Robot Joint Attention task Abstract: In this work we tackle the problem of child engagement estimation while\nchildren freely interact with a robot in their room. We propose a deep-based\nmulti-view solution that takes advantage of recent developments in human pose\ndetection. We extract the child's pose from different RGB-D cameras placed\nelegantly in the room, fuse the results and feed them to a deep neural network\ntrained for classifying engagement levels. The deep network contains a\nrecurrent layer, in order to exploit the rich temporal information contained in\nthe pose data. The resulting method outperforms a number of baseline\nclassifiers, and provides a promising tool for better automatic understanding\nof a child's attitude, interest and attention while cooperating with a robot.\nThe goal is to integrate this model in next generation social robots as an\nattention monitoring tool during various CRI tasks both for Typically Developed\n(TD) children and children affected by autism (ASD). \n\n"}
{"id": "1812.00324", "contents": "Title: CrowdPose: Efficient Crowded Scenes Pose Estimation and A New Benchmark Abstract: Multi-person pose estimation is fundamental to many computer vision tasks and\nhas made significant progress in recent years. However, few previous methods\nexplored the problem of pose estimation in crowded scenes while it remains\nchallenging and inevitable in many scenarios. Moreover, current benchmarks\ncannot provide an appropriate evaluation for such cases. In this paper, we\npropose a novel and efficient method to tackle the problem of pose estimation\nin the crowd and a new dataset to better evaluate algorithms. Our model\nconsists of two key components: joint-candidate single person pose estimation\n(SPPE) and global maximum joints association. With multi-peak prediction for\neach joint and global association using graph model, our method is robust to\ninevitable interference in crowded scenes and very efficient in inference. The\nproposed method surpasses the state-of-the-art methods on CrowdPose dataset by\n5.2 mAP and results on MSCOCO dataset demonstrate the generalization ability of\nour method. Source code and dataset will be made publicly available. \n\n"}
{"id": "1812.00422", "contents": "Title: A multi-task deep learning model for the classification of Age-related\n  Macular Degeneration Abstract: Age-related Macular Degeneration (AMD) is a leading cause of blindness.\nAlthough the Age-Related Eye Disease Study group previously developed a 9-step\nAMD severity scale for manual classification of AMD severity from color fundus\nimages, manual grading of images is time-consuming and expensive. Built on our\nprevious work DeepSeeNet, we developed a novel deep learning model for\nautomated classification of images into the 9-step scale. Instead of predicting\nthe 9-step score directly, our approach simulates the reading center grading\nprocess. It first detects four AMD characteristics (drusen area, geographic\natrophy, increased pigment, and depigmentation), then combines these to derive\nthe overall 9-step score. Importantly, we applied multi-task learning\ntechniques, which allowed us to train classification of the four\ncharacteristics in parallel, share representation, and prevent overfitting.\nEvaluation on two image datasets showed that the accuracy of the model exceeded\nthe current state-of-the-art model by > 10%. \n\n"}
{"id": "1812.00481", "contents": "Title: Neural Rejuvenation: Improving Deep Network Training by Enhancing\n  Computational Resource Utilization Abstract: In this paper, we study the problem of improving computational resource\nutilization of neural networks. Deep neural networks are usually\nover-parameterized for their tasks in order to achieve good performances, thus\nare likely to have underutilized computational resources. This observation\nmotivates a lot of research topics, e.g. network pruning, architecture search,\netc. As models with higher computational costs (e.g. more parameters or more\ncomputations) usually have better performances, we study the problem of\nimproving the resource utilization of neural networks so that their potentials\ncan be further realized. To this end, we propose a novel optimization method\nnamed Neural Rejuvenation. As its name suggests, our method detects dead\nneurons and computes resource utilization in real time, rejuvenates dead\nneurons by resource reallocation and reinitialization, and trains them with new\ntraining schemes. By simply replacing standard optimizers with Neural\nRejuvenation, we are able to improve the performances of neural networks by a\nvery large margin while using similar training efforts and maintaining their\noriginal resource usages. \n\n"}
{"id": "1812.00491", "contents": "Title: Adversarial Domain Randomization Abstract: Domain Randomization (DR) is known to require a significant amount of\ntraining data for good performance. We argue that this is due to DR's strategy\nof random data generation using a uniform distribution over simulation\nparameters, as a result, DR often generates samples which are uninformative for\nthe learner. In this work, we theoretically analyze DR using ideas from\nmulti-source domain adaptation. Based on our findings, we propose Adversarial\nDomain Randomization (ADR) as an efficient variant of DR which generates\nadversarial samples with respect to the learner during training. We implement\nADR as a policy whose action space is the quantized simulation parameter space.\nAt each iteration, the policy's action generates labeled data and the reward is\nset as negative of learner's loss on this data. As a result, we observe ADR\nfrequently generates novel samples for the learner like truncated and occluded\nobjects for object detection and confusing classes for image classification. We\nperform evaluations on datasets like CLEVR, Syn2Real, and VIRAT for various\ntasks where we demonstrate that ADR outperforms DR by generating fewer data\nsamples. \n\n"}
{"id": "1812.00862", "contents": "Title: Iterative Potts minimization for the recovery of signals with\n  discontinuities from indirect measurements -- the multivariate case Abstract: Signals and images with discontinuities appear in many problems in such\ndiverse areas as biology, medicine, mechanics, and electrical engineering. The\nconcrete data are often discrete, indirect and noisy measurements of some\nquantities describing the signal under consideration. A frequent task is to\nfind the segments of the signal or image which corresponds to finding the\ndiscontinuities or jumps in the data. Methods based on minimizing the piecewise\nconstant Mumford-Shah functional -- whose discretized version is known as Potts\nfunctional -- are advantageous in this scenario, in particular, in connection\nwith segmentation. However, due to their non-convexity, minimization of such\nfunctionals is challenging. In this paper we propose a new iterative\nminimization strategy for the multivariate Potts functional dealing with\nindirect, noisy measurements. We provide a convergence analysis and underpin\nour findings with numerical experiments. \n\n"}
{"id": "1812.01180", "contents": "Title: Deep Generative Modeling of LiDAR Data Abstract: Building models capable of generating structured output is a key challenge\nfor AI and robotics. While generative models have been explored on many types\nof data, little work has been done on synthesizing lidar scans, which play a\nkey role in robot mapping and localization. In this work, we show that one can\nadapt deep generative models for this task by unravelling lidar scans into a 2D\npoint map. Our approach can generate high quality samples, while simultaneously\nlearning a meaningful latent representation of the data. We demonstrate\nsignificant improvements against state-of-the-art point cloud generation\nmethods. Furthermore, we propose a novel data representation that augments the\n2D signal with absolute positional information. We show that this helps\nrobustness to noisy and imputed input; the learned model can recover the\nunderlying lidar scan from seemingly uninformative data \n\n"}
{"id": "1812.01261", "contents": "Title: Conditional Video Generation Using Action-Appearance Captions Abstract: The field of automatic video generation has received a boost thanks to the\nrecent Generative Adversarial Networks (GANs). However, most existing methods\ncannot control the contents of the generated video using a text caption, losing\ntheir usefulness to a large extent. This particularly affects human videos due\nto their great variety of actions and appearances. This paper presents\nConditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method\nfrom action-appearance captions. We propose a novel way of generating video by\nencoding a caption (e.g., \"a man in blue jeans is playing golf\") in a two-stage\ngeneration pipeline. Our CFT-GAN uses such caption to generate an optical flow\n(action) and a texture (appearance) for each frame. As a result, the output\nvideo reflects the content specified in the caption in a plausible way.\nMoreover, to train our method, we constructed a new dataset for human video\ngeneration with captions. We evaluated the proposed method qualitatively and\nquantitatively via an ablation study and a user study. The results demonstrate\nthat CFT-GAN is able to successfully generate videos containing the action and\nappearances indicated in the captions. \n\n"}
{"id": "1812.01393", "contents": "Title: TextField: Learning A Deep Direction Field for Irregular Scene Text\n  Detection Abstract: Scene text detection is an important step of scene text reading system. The\nmain challenges lie on significantly varied sizes and aspect ratios, arbitrary\norientations and shapes. Driven by recent progress in deep learning, impressive\nperformances have been achieved for multi-oriented text detection. Yet, the\nperformance drops dramatically in detecting curved texts due to the limited\ntext representation (e.g., horizontal bounding boxes, rotated rectangles, or\nquadrilaterals). It is of great interest to detect curved texts, which are\nactually very common in natural scenes. In this paper, we present a novel text\ndetector named TextField for detecting irregular scene texts. Specifically, we\nlearn a direction field pointing away from the nearest text boundary to each\ntext point. This direction field is represented by an image of two-dimensional\nvectors and learned via a fully convolutional neural network. It encodes both\nbinary text mask and direction information used to separate adjacent text\ninstances, which is challenging for classical segmentation-based approaches.\nBased on the learned direction field, we apply a simple yet effective\nmorphological-based post-processing to achieve the final detection.\nExperimental results show that the proposed TextField outperforms the\nstate-of-the-art methods by a large margin (28% and 8%) on two curved text\ndatasets: Total-Text and CTW1500, respectively, and also achieves very\ncompetitive performance on multi-oriented datasets: ICDAR 2015 and MSRA-TD500.\nFurthermore, TextField is robust in generalizing to unseen datasets. The code\nis available at https://github.com/YukangWang/TextField. \n\n"}
{"id": "1812.01659", "contents": "Title: Learning to Sample Abstract: Processing large point clouds is a challenging task. Therefore, the data is\noften sampled to a size that can be processed more easily. The question is how\nto sample the data? A popular sampling technique is Farthest Point Sampling\n(FPS). However, FPS is agnostic to a downstream application (classification,\nretrieval, etc.). The underlying assumption seems to be that minimizing the\nfarthest point distance, as done by FPS, is a good proxy to other objective\nfunctions.\n  We show that it is better to learn how to sample. To do that, we propose a\ndeep network to simplify 3D point clouds. The network, termed S-NET, takes a\npoint cloud and produces a smaller point cloud that is optimized for a\nparticular task. The simplified point cloud is not guaranteed to be a subset of\nthe original point cloud. Therefore, we match it to a subset of the original\npoints in a post-processing step. We contrast our approach with FPS by\nexperimenting on two standard data sets and show significantly better results\nfor a variety of applications. Our code is publicly available at:\nhttps://github.com/orendv/learning_to_sample \n\n"}
{"id": "1812.02496", "contents": "Title: Prediction of final infarct volume from native CT perfusion and\n  treatment parameters using deep learning Abstract: CT Perfusion (CTP) imaging has gained importance in the diagnosis of acute\nstroke. Conventional perfusion analysis performs a deconvolution of the\nmeasurements and thresholds the perfusion parameters to determine the tissue\nstatus. We pursue a data-driven and deconvolution-free approach, where a deep\nneural network learns to predict the final infarct volume directly from the\nnative CTP images and metadata such as the time parameters and treatment. This\nwould allow clinicians to simulate various treatments and gain insight into\npredicted tissue status over time. We demonstrate on a multicenter dataset that\nour approach is able to predict the final infarct and effectively uses the\nmetadata. An ablation study shows that using the native CTP measurements\ninstead of the deconvolved measurements improves the prediction. \n\n"}
{"id": "1812.02510", "contents": "Title: ForensicTransfer: Weakly-supervised Domain Adaptation for Forgery\n  Detection Abstract: Distinguishing manipulated from real images is becoming increasingly\ndifficult as new sophisticated image forgery approaches come out by the day.\nNaive classification approaches based on Convolutional Neural Networks (CNNs)\nshow excellent performance in detecting image manipulations when they are\ntrained on a specific forgery method. However, on examples from unseen\nmanipulation approaches, their performance drops significantly. To address this\nlimitation in transferability, we introduce Forensic-Transfer (FT). We devise a\nlearning-based forensic detector which adapts well to new domains, i.e., novel\nmanipulation methods and can handle scenarios where only a handful of fake\nexamples are available during training. To this end, we learn a forensic\nembedding based on a novel autoencoder-based architecture that can be used to\ndistinguish between real and fake imagery. The learned embedding acts as a form\nof anomaly detector; namely, an image manipulated from an unseen method will be\ndetected as fake provided it maps sufficiently far away from the cluster of\nreal images. Comparing to prior works, FT shows significant improvements in\ntransferability, which we demonstrate in a series of experiments on\ncutting-edge benchmarks. For instance, on unseen examples, we achieve up to 85%\nin terms of accuracy, and with only a handful of seen examples, our performance\nalready reaches around 95%. \n\n"}
{"id": "1812.02640", "contents": "Title: Pathological Evidence Exploration in Deep Retinal Image Diagnosis Abstract: Though deep learning has shown successful performance in classifying the\nlabel and severity stage of certain disease, most of them give few evidence on\nhow to make prediction. Here, we propose to exploit the interpretability of\ndeep learning application in medical diagnosis. Inspired by Koch's Postulates,\na well-known strategy in medical research to identify the property of pathogen,\nwe define a pathological descriptor that can be extracted from the activated\nneurons of a diabetic retinopathy detector. To visualize the symptom and\nfeature encoded in this descriptor, we propose a GAN based method to synthesize\npathological retinal image given the descriptor and a binary vessel\nsegmentation. Besides, with this descriptor, we can arbitrarily manipulate the\nposition and quantity of lesions. As verified by a panel of 5 licensed\nophthalmologists, our synthesized images carry the symptoms that are directly\nrelated to diabetic retinopathy diagnosis. The panel survey also shows that our\ngenerated images is both qualitatively and quantitatively superior to existing\nmethods. \n\n"}
{"id": "1812.02831", "contents": "Title: Neural Image Decompression: Learning to Render Better Image Previews Abstract: A rapidly increasing portion of Internet traffic is dominated by requests\nfrom mobile devices with limited- and metered-bandwidth constraints. To satisfy\nthese requests, it has become standard practice for websites to transmit small\nand extremely compressed image previews as part of the initial page-load\nprocess. Recent work, based on an adaptive triangulation of the target image,\nhas shown the ability to generate thumbnails of full images at extreme\ncompression rates: 200 bytes or less with impressive gains (in terms of PSNR\nand SSIM) over both JPEG and WebP standards. However, qualitative assessments\nand preservation of semantic content can be less favorable. We present a novel\nmethod to significantly improve the reconstruction quality of the original\nimage with no changes to the encoded information. Our neural-based decoding not\nonly achieves higher PSNR and SSIM scores than the original methods, but also\nyields a substantial increase in semantic-level content preservation. In\naddition, by keeping the same encoding stream, our solution is completely\ninter-operable with the original decoder. The end result is suitable for a\nrange of small-device deployments, as it involves only a single forward-pass\nthrough a small, scalable network. \n\n"}
{"id": "1812.02937", "contents": "Title: Optimizing speed/accuracy trade-off for person re-identification via\n  knowledge distillation Abstract: Finding a person across a camera network plays an important role in video\nsurveillance. For a real-world person re-identification application, in order\nto guarantee an optimal time response, it is crucial to find the balance\nbetween accuracy and speed. We analyse this trade-off, comparing a classical\nmethod, that comprises hand-crafted feature description and metric learning, in\nparticular, LOMO and XQDA, to deep learning based techniques, using image\nclassification networks, ResNet and MobileNets. Additionally, we propose and\nanalyse network distillation as a learning strategy to reduce the computational\ncost of the deep learning approach at test time. We evaluate both methods on\nthe Market-1501 and DukeMTMC-reID large-scale datasets, showing that\ndistillation helps reducing the computational cost at inference time while even\nincreasing the accuracy performance. \n\n"}
{"id": "1812.03050", "contents": "Title: Graph Cut Segmentation Methods Revisited with a Quantum Algorithm Abstract: The design and performance of computer vision algorithms are greatly\ninfluenced by the hardware on which they are implemented. CPUs, multi-core\nCPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas to\nbe realized. This is notably the case with GPUs, which has significantly\nchanged the landscape of computer vision research through deep learning. As the\nend of Moores law approaches, researchers and hardware manufacturers are\nexploring alternative hardware computing paradigms. Quantum computers are a\nvery promising alternative and offer polynomial or even exponential speed-ups\nover conventional computing for some problems. This paper presents a novel\napproach to image segmentation that uses new quantum computing hardware.\nSegmentation is formulated as a graph cut problem that can be mapped to the\nquantum approximate optimization algorithm (QAOA). This algorithm can be\nimplemented on current and near-term quantum computers. Encouraging results are\npresented on artificial and medical imaging data. This represents an important,\npractical step towards leveraging quantum computers for computer vision. \n\n"}
{"id": "1812.03115", "contents": "Title: Kernel Transformer Networks for Compact Spherical Convolution Abstract: Ideally, 360{\\deg} imagery could inherit the deep convolutional neural\nnetworks (CNNs) already trained with great success on perspective projection\nimages. However, existing methods to transfer CNNs from perspective to\nspherical images introduce significant computational costs and/or degradations\nin accuracy. In this work, we present the Kernel Transformer Network (KTN).\nKTNs efficiently transfer convolution kernels from perspective images to the\nequirectangular projection of 360{\\deg} images. Given a source CNN for\nperspective images as input, the KTN produces a function parameterized by a\npolar angle and kernel as output. Given a novel 360{\\deg} image, that function\nin turn can compute convolutions for arbitrary layers and kernels as would the\nsource CNN on the corresponding tangent plane projections. Distinct from all\nexisting methods, KTNs allow model transfer: the same model can be applied to\ndifferent source CNNs with the same base architecture. This enables application\nto multiple recognition tasks without re-training the KTN. Validating our\napproach with multiple source CNNs and datasets, we show that KTNs improve the\nstate of the art for spherical convolution. KTNs successfully preserve the\nsource CNN's accuracy, while offering transferability, scalability to typical\nimage resolutions, and, in many cases, a substantially lower memory footprint. \n\n"}
{"id": "1812.03405", "contents": "Title: AutoGAN: Robust Classifier Against Adversarial Attacks Abstract: Classifiers fail to classify correctly input images that have been\npurposefully and imperceptibly perturbed to cause misclassification. This\nsusceptability has been shown to be consistent across classifiers, regardless\nof their type, architecture or parameters. Common defenses against adversarial\nattacks modify the classifer boundary by training on additional adversarial\nexamples created in various ways. In this paper, we introduce AutoGAN, which\ncounters adversarial attacks by enhancing the lower-dimensional manifold\ndefined by the training data and by projecting perturbed data points onto it.\nAutoGAN mitigates the need for knowing the attack type and magnitude as well as\nthe need for having adversarial samples of the attack. Our approach uses a\nGenerative Adversarial Network (GAN) with an autoencoder generator and a\ndiscriminator that also serves as a classifier. We test AutoGAN against\nadversarial samples generated with state-of-the-art Fast Gradient Sign Method\n(FGSM) as well as samples generated with random Gaussian noise, both using the\nMNIST dataset. For different magnitudes of perturbation in training and\ntesting, AutoGAN can surpass the accuracy of FGSM method by up to 25\\% points\non samples perturbed using FGSM. Without an augmented training dataset, AutoGAN\nachieves an accuracy of 89\\% compared to 1\\% achieved by FGSM method on FGSM\ntesting adversarial samples. \n\n"}
{"id": "1812.03559", "contents": "Title: Deep Spectral Reflectance and Illuminant Estimation from\n  Self-Interreflections Abstract: In this work, we propose a CNN-based approach to estimate the spectral\nreflectance of a surface and the spectral power distribution of the light from\na single RGB image of a V-shaped surface. Interreflections happening in a\nconcave surface lead to gradients of RGB values over its area. These gradients\ncarry a lot of information concerning the physical properties of the surface\nand the illuminant. Our network is trained with only simulated data constructed\nusing a physics-based interreflection model. Coupling interreflection effects\nwith deep learning helps to retrieve the spectral reflectance under an unknown\nlight and to estimate the spectral power distribution of this light as well. In\naddition, it is more robust to the presence of image noise than the classical\napproaches. Our results show that the proposed approach outperforms the state\nof the art learning-based approaches on simulated data. In addition, it gives\nbetter results on real data compared to other interreflection-based approaches. \n\n"}
{"id": "1812.04072", "contents": "Title: PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image Abstract: This paper proposes a deep neural architecture, PlaneRCNN, that detects and\nreconstructs piecewise planar surfaces from a single RGB image. PlaneRCNN\nemploys a variant of Mask R-CNN to detect planes with their plane parameters\nand segmentation masks. PlaneRCNN then jointly refines all the segmentation\nmasks with a novel loss enforcing the consistency with a nearby view during\ntraining. The paper also presents a new benchmark with more fine-grained plane\nsegmentations in the ground-truth, in which, PlaneRCNN outperforms existing\nstate-of-the-art methods with significant margins in the plane detection,\nsegmentation, and reconstruction metrics. PlaneRCNN makes an important step\ntowards robust plane extraction, which would have an immediate impact on a wide\nrange of applications including Robotics, Augmented Reality, and Virtual\nReality. \n\n"}
{"id": "1812.04914", "contents": "Title: CFUN: Combining Faster R-CNN and U-net Network for Efficient Whole Heart\n  Segmentation Abstract: In this paper, we propose a novel heart segmentation pipeline Combining\nFaster R-CNN and U-net Network (CFUN). Due to Faster R-CNN's precise\nlocalization ability and U-net's powerful segmentation ability, CFUN needs only\none-step detection and segmentation inference to get the whole heart\nsegmentation result, obtaining good results with significantly reduced\ncomputational cost. Besides, CFUN adopts a new loss function based on edge\ninformation named 3D Edge-loss as an auxiliary loss to accelerate the\nconvergence of training and improve the segmentation results. Extensive\nexperiments on the public dataset show that CFUN exhibits competitive\nsegmentation performance in a sharply reduced inference time. Our source code\nand the model are publicly available at https://github.com/Wuziyi616/CFUN. \n\n"}
{"id": "1812.05637", "contents": "Title: Dynamic Graph Modules for Modeling Object-Object Interactions in\n  Activity Recognition Abstract: Video action recognition, a critical problem in video understanding, has been\ngaining increasing attention. To identify actions induced by complex\nobject-object interactions, we need to consider not only spatial relations\namong objects in a single frame, but also temporal relations among different or\nthe same objects across multiple frames. However, existing approaches that\nmodel video representations and non-local features are either incapable of\nexplicitly modeling relations at the object-object level or unable to handle\nstreaming videos. In this paper, we propose a novel dynamic hidden graph module\nto model complex object-object interactions in videos, of which two\ninstantiations are considered: a visual graph that captures appearance/motion\nchanges among objects and a location graph that captures relative\nspatiotemporal position changes among objects. Additionally, the proposed graph\nmodule allows us to process streaming videos, setting it apart from existing\nmethods. Experimental results on benchmark datasets, Something-Something and\nActivityNet, show the competitive performance of our method. \n\n"}
{"id": "1812.05785", "contents": "Title: Deep Active Learning for Video-based Person Re-identification Abstract: It is prohibitively expensive to annotate a large-scale video-based person\nre-identification (re-ID) dataset, which makes fully supervised methods\ninapplicable to real-world deployment. How to maximally reduce the annotation\ncost while retaining the re-ID performance becomes an interesting problem. In\nthis paper, we address this problem by integrating an active learning scheme\ninto a deep learning framework. Noticing that the truly matched tracklet-pairs,\nalso denoted as true positives (TP), are the most informative samples for our\nre-ID model, we propose a sampling criterion to choose the most TP-likely\ntracklet-pairs for annotation. A view-aware sampling strategy considering\nview-specific biases is designed to facilitate candidate selection, followed by\nan adaptive resampling step to leave out the selected candidates that are\nunnecessary to annotate. Our method learns the re-ID model and updates the\nannotation set iteratively. The re-ID model is supervised by the tracklets'\npesudo labels that are initialized by treating each tracklet as a distinct\nclass. With the gained annotations of the actively selected candidates, the\ntracklets' pesudo labels are updated by label merging and further used to\nre-train our re-ID model. While being simple, the proposed method demonstrates\nits effectiveness on three video-based person re-ID datasets. Experimental\nresults show that less than 3\\% pairwise annotations are needed for our method\nto reach comparable performance with the fully-supervised setting. \n\n"}
{"id": "1812.05869", "contents": "Title: The Coherent Point Drift for Clustered Point Sets Abstract: The problem of non-rigid point set registration is a key problem for many\ncomputer vision tasks. In many cases the nature of the data or capabilities of\nthe point detection algorithms can give us some prior information on point sets\ndistribution. In non-rigid case this information is able to drastically improve\nregistration results by limiting number of possible solutions. In this paper we\nexplore use of prior information about point sets clustering, such information\ncan be obtained with preliminary segmentation. We extend existing probabilistic\nframework for fitting two level Gaussian mixture model and derive closed form\nsolution for maximization step of the EM algorithm. This enables us to improve\nmethod accuracy with almost no performance loss. We evaluate our approach and\ncompare the Cluster Coherent Point Drift with other existing non-rigid point\nset registration methods and show it's advantages for digital medicine tasks,\nespecially for heart template model personalization using patient's medical\ndata. \n\n"}
{"id": "1812.06384", "contents": "Title: TET-GAN: Text Effects Transfer via Stylization and Destylization Abstract: Text effects transfer technology automatically makes the text dramatically\nmore impressive. However, previous style transfer methods either study the\nmodel for general style, which cannot handle the highly-structured text effects\nalong the glyph, or require manual design of subtle matching criteria for text\neffects. In this paper, we focus on the use of the powerful representation\nabilities of deep neural features for text effects transfer. For this purpose,\nwe propose a novel Texture Effects Transfer GAN (TET-GAN), which consists of a\nstylization subnetwork and a destylization subnetwork. The key idea is to train\nour network to accomplish both the objective of style transfer and style\nremoval, so that it can learn to disentangle and recombine the content and\nstyle features of text effects images. To support the training of our network,\nwe propose a new text effects dataset with as much as 64 professionally\ndesigned styles on 837 characters. We show that the disentangled feature\nrepresentations enable us to transfer or remove all these styles on arbitrary\nglyphs using one network. Furthermore, the flexible network design empowers\nTET-GAN to efficiently extend to a new text style via one-shot learning where\nonly one example is required. We demonstrate the superiority of the proposed\nmethod in generating high-quality stylized text over the state-of-the-art\nmethods. \n\n"}
{"id": "1812.06417", "contents": "Title: Visual Dialogue without Vision or Dialogue Abstract: We characterise some of the quirks and shortcomings in the exploration of\nVisual Dialogue - a sequential question-answering task where the questions and\ncorresponding answers are related through given visual stimuli. To do so, we\ndevelop an embarrassingly simple method based on Canonical Correlation Analysis\n(CCA) that, on the standard dataset, achieves near state-of-the-art performance\non mean rank (MR). In direct contrast to current complex and over-parametrised\narchitectures that are both compute and time intensive, our method ignores the\nvisual stimuli, ignores the sequencing of dialogue, does not need gradients,\nuses off-the-shelf feature extractors, has at least an order of magnitude fewer\nparameters, and learns in practically no time. We argue that these results are\nindicative of issues in current approaches to Visual Dialogue and conduct\nanalyses to highlight implicit dataset biases and effects of over-constrained\nevaluation metrics. Our code is publicly available. \n\n"}
{"id": "1812.06571", "contents": "Title: Latent Dirichlet Allocation in Generative Adversarial Networks Abstract: We study the problem of multimodal generative modelling of images based on\ngenerative adversarial networks (GANs). Despite the success of existing\nmethods, they often ignore the underlying structure of vision data or its\nmultimodal generation characteristics. To address this problem, we introduce\nthe Dirichlet prior for multimodal image generation, which leads to a new\nLatent Dirichlet Allocation based GAN (LDAGAN). In detail, for the generative\nprocess modelling, LDAGAN defines a generative mode for each sample,\ndetermining which generative sub-process it belongs to. For the adversarial\ntraining, LDAGAN derives a variational expectation-maximization (VEM) algorithm\nto estimate model parameters. Experimental results on real-world datasets have\ndemonstrated the outstanding performance of LDAGAN over other existing GANs. \n\n"}
{"id": "1812.06597", "contents": "Title: Learning Student Networks via Feature Embedding Abstract: Deep convolutional neural networks have been widely used in numerous\napplications, but their demanding storage and computational resource\nrequirements prevent their applications on mobile devices. Knowledge\ndistillation aims to optimize a portable student network by taking the\nknowledge from a well-trained heavy teacher network. Traditional\nteacher-student based methods used to rely on additional fully-connected layers\nto bridge intermediate layers of teacher and student networks, which brings in\na large number of auxiliary parameters. In contrast, this paper aims to\npropagate information from teacher to student without introducing new variables\nwhich need to be optimized. We regard the teacher-student paradigm from a new\nperspective of feature embedding. By introducing the locality preserving loss,\nthe student network is encouraged to generate the low-dimensional features\nwhich could inherit intrinsic properties of their corresponding\nhigh-dimensional features from teacher network. The resulting portable network\nthus can naturally maintain the performance as that of the teacher network.\nTheoretical analysis is provided to justify the lower computation complexity of\nthe proposed method. Experiments on benchmark datasets and well-trained\nnetworks suggest that the proposed algorithm is superior to state-of-the-art\nteacher-student learning methods in terms of computational and storage\ncomplexity. \n\n"}
{"id": "1812.06869", "contents": "Title: BriarPatches: Pixel-Space Interventions for Inducing Demographic Parity Abstract: We introduce the BriarPatch, a pixel-space intervention that obscures\nsensitive attributes from representations encoded in pre-trained classifiers.\nThe patches encourage internal model representations not to encode sensitive\ninformation, which has the effect of pushing downstream predictors towards\nexhibiting demographic parity with respect to the sensitive information. The\nnet result is that these BriarPatches provide an intervention mechanism\navailable at user level, and complements prior research on fair representations\nthat were previously only applicable by model developers and ML experts. \n\n"}
{"id": "1812.07003", "contents": "Title: 3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans Abstract: We introduce 3D-SIS, a novel neural network architecture for 3D semantic\ninstance segmentation in commodity RGB-D scans. The core idea of our method is\nto jointly learn from both geometric and color signal, thus enabling accurate\ninstance predictions. Rather than operate solely on 2D frames, we observe that\nmost computer vision applications have multi-view RGB-D input available, which\nwe leverage to construct an approach for 3D instance segmentation that\neffectively fuses together these multi-modal inputs. Our network leverages\nhigh-resolution RGB input by associating 2D images with the volumetric grid\nbased on the pose alignment of the 3D reconstruction. For each image, we first\nextract 2D features for each pixel with a series of 2D convolutions; we then\nbackproject the resulting feature vector to the associated voxel in the 3D\ngrid. This combination of 2D and 3D feature learning allows significantly\nhigher accuracy object detection and instance segmentation than\nstate-of-the-art alternatives. We show results on both synthetic and real-world\npublic benchmarks, achieving an improvement in mAP of over 13 on real-world\ndata. \n\n"}
{"id": "1812.07032", "contents": "Title: Boundary loss for highly unbalanced segmentation Abstract: Widely used loss functions for CNN segmentation, e.g., Dice or cross-entropy,\nare based on integrals over the segmentation regions. Unfortunately, for highly\nunbalanced segmentations, such regional summations have values that differ by\nseveral orders of magnitude across classes, which affects training performance\nand stability. We propose a boundary loss, which takes the form of a distance\nmetric on the space of contours, not regions. This can mitigate the\ndifficulties of highly unbalanced problems because it uses integrals over the\ninterface between regions instead of unbalanced integrals over the regions.\nFurthermore, a boundary loss complements regional information. Inspired by\ngraph-based optimization techniques for computing active-contour flows, we\nexpress a non-symmetric $L_2$ distance on the space of contours as a regional\nintegral, which avoids completely local differential computations involving\ncontour points. This yields a boundary loss expressed with the regional softmax\nprobability outputs of the network, which can be easily combined with standard\nregional losses and implemented with any existing deep network architecture for\nN-D segmentation. We report comprehensive evaluations and comparisons on\ndifferent unbalanced problems, showing that our boundary loss can yield\nsignificant increases in performances while improving training stability. Our\ncode is publicly available: https://github.com/LIVIAETS/surface-loss . \n\n"}
{"id": "1812.07368", "contents": "Title: Handcrafted and Deep Trackers: Recent Visual Object Tracking Approaches\n  and Trends Abstract: In recent years visual object tracking has become a very active research\narea. An increasing number of tracking algorithms are being proposed each year.\nIt is because tracking has wide applications in various real world problems\nsuch as human-computer interaction, autonomous vehicles, robotics, surveillance\nand security just to name a few. In the current study, we review latest trends\nand advances in the tracking area and evaluate the robustness of different\ntrackers based on the feature extraction methods. The first part of this work\ncomprises a comprehensive survey of the recently proposed trackers. We broadly\ncategorize trackers into Correlation Filter based Trackers (CFTs) and Non-CFTs.\nEach category is further classified into various types based on the\narchitecture and the tracking mechanism. In the second part, we experimentally\nevaluated 24 recent trackers for robustness, and compared handcrafted and deep\nfeature based trackers. We observe that trackers using deep features performed\nbetter, though in some cases a fusion of both increased performance\nsignificantly. In order to overcome the drawbacks of the existing benchmarks, a\nnew benchmark Object Tracking and Temple Color (OTTC) has also been proposed\nand used in the evaluation of different algorithms. We analyze the performance\nof trackers over eleven different challenges in OTTC, and three other\nbenchmarks. Our study concludes that Discriminative Correlation Filter (DCF)\nbased trackers perform better than the others. Our study also reveals that\ninclusion of different types of regularizations over DCF often results in\nboosted tracking performance. Finally, we sum up our study by pointing out some\ninsights and indicating future trends in visual object tracking field. \n\n"}
{"id": "1812.09922", "contents": "Title: Dynamic Runtime Feature Map Pruning Abstract: High bandwidth requirements are an obstacle for accelerating the training and\ninference of deep neural networks. Most previous research focuses on reducing\nthe size of kernel maps for inference. We analyze parameter sparsity of six\npopular convolutional neural networks - AlexNet, MobileNet, ResNet-50,\nSqueezeNet, TinyNet, and VGG16. Of the networks considered, those using ReLU\n(AlexNet, SqueezeNet, VGG16) contain a high percentage of 0-valued parameters\nand can be statically pruned. Networks with Non-ReLU activation functions in\nsome cases may not contain any 0-valued parameters (ResNet-50, TinyNet). We\nalso investigate runtime feature map usage and find that input feature maps\ncomprise the majority of bandwidth requirements when depth-wise convolution and\npoint-wise convolutions used. We introduce dynamic runtime pruning of feature\nmaps and show that 10% of dynamic feature map execution can be removed without\nloss of accuracy. We then extend dynamic pruning to allow for values within an\nepsilon of zero and show a further 5% reduction of feature map loading with a\n1% loss of accuracy in top-1. \n\n"}
{"id": "1812.10085", "contents": "Title: A Data-driven Adversarial Examples Recognition Framework via Adversarial\n  Feature Genome Abstract: Adversarial examples pose many security threats to convolutional neural\nnetworks (CNNs). Most defense algorithms prevent these threats by finding\ndifferences between the original images and adversarial examples. However, the\nfound differences do not contain features about the classes, so these defense\nalgorithms can only detect adversarial examples without recovering the correct\nlabels. In this regard, we propose the Adversarial Feature Genome (AFG), a\nnovel type of data that contains both the differences and features about\nclasses. This method is inspired by an observed phenomenon, namely the\nAdversarial Feature Separability (AFS), where the difference between the\nfeature maps of the original images and adversarial examples becomes larger\nwith deeper layers. On top of that, we further develop an adversarial example\nrecognition framework that detects adversarial examples and can recover the\ncorrect labels. In the experiments, the detection and classification of\nadversarial examples by AFGs has an accuracy of more than 90.01\\% in various\nattack scenarios. To the best of our knowledge, our method is the first method\nthat focuses on both attack detecting and recovering. AFG gives a new\ndata-driven perspective to improve the robustness of CNNs. The source code is\navailable at https://github.com/GeoX-Lab/Adv_Fea_Genome. \n\n"}
{"id": "1812.10240", "contents": "Title: Studying the Plasticity in Deep Convolutional Neural Networks using\n  Random Pruning Abstract: Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations.The key idea is to rank the filters based on a certain criterion\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\nfilters are pruned away the remainder of the network is fine tuned and is shown\nto give performance comparable to the original unpruned network. In this work,\nwe report experiments which suggest that the comparable performance of the\npruned network is not due to the specific criterion chosen but due to the\ninherent plasticity of deep neural networks which allows them to recover from\nthe loss of pruned filters once the rest of the filters are fine-tuned.\nSpecifically we show counter-intuitive results wherein by randomly pruning\n25-50% filters from deep CNNs we are able to obtain the same performance as\nobtained by using state-of-the-art pruning methods. We empirically validate our\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\nneeds to be tested on only a small set of classes at test time (say, only\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\nclass specific pruning and show that even here a random pruning strategy gives\nclose to state-of-the-art performance. Unlike existing approaches which mainly\nfocus on the task of image classification, in this work we also report results\non object detection and image segmentation. We show that using a simple random\npruning strategy we can achieve significant speed up in object detection (74%\nimprovement in fps) while retaining the same accuracy as that of the original\nFaster RCNN model. Similarly we show that the performance of a pruned\nSegmentation Network (SegNet) is actually very similar to that of the original\nunpruned SegNet. \n\n"}
{"id": "1812.11771", "contents": "Title: Predicting Group Cohesiveness in Images Abstract: The cohesiveness of a group is an essential indicator of the emotional state,\nstructure and success of a group of people. We study the factors that influence\nthe perception of group-level cohesion and propose methods for estimating the\nhuman-perceived cohesion on the group cohesiveness scale. In order to identify\nthe visual cues (attributes) for cohesion, we conducted a user survey. Image\nanalysis is performed at a group-level via a multi-task convolutional neural\nnetwork. For analyzing the contribution of facial expressions of the group\nmembers for predicting the Group Cohesion Score (GCS), a capsule network is\nexplored. We add GCS to the Group Affect database and propose the `GAF-Cohesion\ndatabase'. The proposed model performs well on the database and is able to\nachieve near human-level performance in predicting a group's cohesion score. It\nis interesting to note that group cohesion as an attribute, when jointly\ntrained for group-level emotion prediction, helps in increasing the performance\nfor the later task. This suggests that group-level emotion and cohesion are\ncorrelated. \n\n"}
{"id": "1812.11806", "contents": "Title: An introduction to domain adaptation and transfer learning Abstract: In machine learning, if the training data is an unbiased sample of an\nunderlying distribution, then the learned classification function will make\naccurate predictions for new samples. However, if the training data is not an\nunbiased sample, then there will be differences between how the training data\nis distributed and how the test data is distributed. Standard classifiers\ncannot cope with changes in data distributions between training and test\nphases, and will not perform well. Domain adaptation and transfer learning are\nsub-fields within machine learning that are concerned with accounting for these\ntypes of changes. Here, we present an introduction to these fields, guided by\nthe question: when and how can a classifier generalize from a source to a\ntarget domain? We will start with a brief introduction into risk minimization,\nand how transfer learning and domain adaptation expand upon this framework.\nFollowing that, we discuss three special cases of data set shift, namely prior,\ncovariate and concept shift. For more complex domain shifts, there are a wide\nvariety of approaches. These are categorized into: importance-weighting,\nsubspace mapping, domain-invariant spaces, feature augmentation, minimax\nestimators and robust algorithms. A number of points will arise, which we will\ndiscuss in the last section. We conclude with the remark that many open\nquestions will have to be addressed before transfer learners and\ndomain-adaptive classifiers become practical. \n\n"}
{"id": "1812.11842", "contents": "Title: Do GANs leave artificial fingerprints? Abstract: In the last few years, generative adversarial networks (GAN) have shown\ntremendous potential for a number of applications in computer vision and\nrelated fields. With the current pace of progress, it is a sure bet they will\nsoon be able to generate high-quality images and videos, virtually\nindistinguishable from real ones. Unfortunately, realistic GAN-generated images\npose serious threats to security, to begin with a possible flood of fake\nmultimedia, and multimedia forensic countermeasures are in urgent need. In this\nwork, we show that each GAN leaves its specific fingerprint in the images it\ngenerates, just like real-world cameras mark acquired images with traces of\ntheir photo-response non-uniformity pattern. Source identification experiments\nwith several popular GANs show such fingerprints to represent a precious asset\nfor forensic analyses. \n\n"}
{"id": "1901.00039", "contents": "Title: Mask-aware networks for crowd counting Abstract: Crowd counting problem aims to count the number of objects within an image or\na frame in the videos and is usually solved by estimating the density map\ngenerated from the object location annotations. The values in the density map,\nby nature, take two possible states: zero indicating no object around, a\nnon-zero value indicating the existence of objects and the value denoting the\nlocal object density. In contrast to traditional methods which do not\ndifferentiate the density prediction of these two states, we propose to use a\ndedicated network branch to predict the object/non-object mask and then combine\nits prediction with the input image to produce the density map. Our rationale\nis that the mask prediction could be better modeled as a binary segmentation\nproblem and the difficulty of estimating the density could be reduced if the\nmask is known. A key to the proposed scheme is the strategy of incorporating\nthe mask prediction into the density map estimator. To this end, we study five\npossible solutions, and via analysis and experimental validation we identify\nthe most effective one. Through extensive experiments on five public datasets,\nwe demonstrate the superior performance of the proposed approach over the\nbaselines and show that our network could achieve the state-of-the-art\nperformance. \n\n"}
{"id": "1901.00148", "contents": "Title: Rethinking on Multi-Stage Networks for Human Pose Estimation Abstract: Existing pose estimation approaches fall into two categories: single-stage\nand multi-stage methods. While multi-stage methods are seemingly more suited\nfor the task, their performance in current practice is not as good as\nsingle-stage methods. This work studies this issue. We argue that the current\nmulti-stage methods' unsatisfactory performance comes from the insufficiency in\nvarious design choices. We propose several improvements, including the\nsingle-stage module design, cross stage feature aggregation, and coarse-to-fine\nsupervision. The resulting method establishes the new state-of-the-art on both\nMS COCO and MPII Human Pose dataset, justifying the effectiveness of a\nmulti-stage architecture. The source code is publicly available for further\nresearch. \n\n"}
{"id": "1901.00248", "contents": "Title: A Survey on Multi-output Learning Abstract: Multi-output learning aims to simultaneously predict multiple outputs given\nan input. It is an important learning problem due to the pressing need for\nsophisticated decision making in real-world applications. Inspired by big data,\nthe 4Vs characteristics of multi-output imposes a set of challenges to\nmulti-output learning, in terms of the volume, velocity, variety and veracity\nof the outputs. Increasing number of works in the literature have been devoted\nto the study of multi-output learning and the development of novel approaches\nfor addressing the challenges encountered. However, it lacks a comprehensive\noverview on different types of challenges of multi-output learning brought by\nthe characteristics of the multiple outputs and the techniques proposed to\novercome the challenges. This paper thus attempts to fill in this gap to\nprovide a comprehensive review on this area. We first introduce different\nstages of the life cycle of the output labels. Then we present the paradigm on\nmulti-output learning, including its myriads of output structures, definitions\nof its different sub-problems, model evaluation metrics and popular data\nrepositories used in the study. Subsequently, we review a number of\nstate-of-the-art multi-output learning methods, which are categorized based on\nthe challenges. \n\n"}
{"id": "1901.00616", "contents": "Title: Volumetric Convolution: Automatic Representation Learning in Unit Ball Abstract: Convolution is an efficient technique to obtain abstract feature\nrepresentations using hierarchical layers in deep networks. Although performing\nconvolution in Euclidean geometries is fairly straightforward, its extension to\nother topological spaces---such as a sphere ($\\mathbb{S}^2$) or a unit ball\n($\\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel\n`\\emph{volumetric convolution}' operation that can effectively convolve\narbitrary functions in $\\mathbb{B}^3$. We develop a theoretical framework for\n\\emph{volumetric convolution} based on Zernike polynomials and efficiently\nimplement it as a differentiable and an easily pluggable layer for deep\nnetworks. Furthermore, our formulation leads to derivation of a novel formula\nto measure the symmetry of a function in $\\mathbb{B}^3$ around an arbitrary\naxis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of\nproposed volumetric convolution operation on a possible use-case i.e., 3D\nobject recognition task. \n\n"}
{"id": "1901.00799", "contents": "Title: Network Measures of Mixing Abstract: Transport and mixing processes in fluid flows can be studied directly from\nLagrangian trajectory data, such as obtained from particle tracking\nexperiments. Recent work in this context highlights the application of\ngraph-based approaches, where trajectories serve as nodes and some similarity\nor distance measure between them is employed to build a (possibly weighted)\nnetwork, which is then analyzed using spectral methods. Here, we consider the\nsimplest case of an unweighted, undirected network and analytically relate\nlocal network measures such as node degree or clustering coefficient to flow\nstructures. In particular, we use these local measures to divide the family of\ntrajectories into groups of similar dynamical behavior via manifold learning\nmethods. \n\n"}
{"id": "1901.01641", "contents": "Title: Blind Motion Deblurring with Cycle Generative Adversarial Networks Abstract: Blind motion deblurring is one of the most basic and challenging problems in\nimage processing and computer vision. It aims to recover a sharp image from its\nblurred version knowing nothing about the blur process. Many existing methods\nuse Maximum A Posteriori (MAP) or Expectation Maximization (EM) frameworks to\ndeal with this kind of problems, but they cannot handle well the figh frequency\nfeatures of natural images. Most recently, deep neural networks have been\nemerging as a powerful tool for image deblurring. In this paper, we prove that\nencoder-decoder architecture gives better results for image deblurring tasks.\nIn addition, we propose a novel end-to-end learning model which refines\ngenerative adversarial network by many novel training strategies so as to\ntackle the problem of deblurring. Experimental results show that our model can\ncapture high frequency features well, and the results on benchmark dataset show\nthat proposed model achieves the competitive performance. \n\n"}
{"id": "1901.01805", "contents": "Title: Fusing Body Posture with Facial Expressions for Joint Recognition of\n  Affect in Child-Robot Interaction Abstract: In this paper we address the problem of multi-cue affect recognition in\nchallenging scenarios such as child-robot interaction. Towards this goal we\npropose a method for automatic recognition of affect that leverages body\nexpressions alongside facial ones, as opposed to traditional methods that\ntypically focus only on the latter. Our deep-learning based method uses\nhierarchical multi-label annotations and multi-stage losses, can be trained\nboth jointly and separately, and offers us computational models for both\nindividual modalities, as well as for the whole body emotion. We evaluate our\nmethod on a challenging child-robot interaction database of emotional\nexpressions collected by us, as well as on the GEMEP public database of acted\nemotions by adults, and show that the proposed method achieves significantly\nbetter results than facial-only expression baselines. \n\n"}
{"id": "1901.01892", "contents": "Title: Scale-Aware Trident Networks for Object Detection Abstract: Scale variation is one of the key challenges in object detection. In this\nwork, we first present a controlled experiment to investigate the effect of\nreceptive fields for scale variation in object detection. Based on the findings\nfrom the exploration experiments, we propose a novel Trident Network\n(TridentNet) aiming to generate scale-specific feature maps with a uniform\nrepresentational power. We construct a parallel multi-branch architecture in\nwhich each branch shares the same transformation parameters but with different\nreceptive fields. Then, we adopt a scale-aware training scheme to specialize\neach branch by sampling object instances of proper scales for training. As a\nbonus, a fast approximation version of TridentNet could achieve significant\nimprovements without any additional parameters and computational cost compared\nwith the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101\nbackbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are\navailable at https://git.io/fj5vR. \n\n"}
{"id": "1901.02388", "contents": "Title: A focusing and defocusing semi-discrete complex short pulse equation and\n  its varioius soliton solutions Abstract: In this paper, we are concerned with a a semi-discrete complex short pulse\n(CSP) equation of both focusing and defocusing types, which can be viewed as an\nanalogue to the Ablowitz-Ladik (AL) lattice in the ultra-short pulse regime. By\nusing a generalized Darboux transformation method, various solutions to this\nnewly integrable semi-discrete equation are studied with both zero and nonzero\nboundary conditions. To be specific, for the focusing CSP equation, the\nmulti-bright solution (zero boundary condition), multi-breather and high-order\nrogue wave solutions (nonzero boudanry conditions) are derived, while for the\ndefocusing CSP equation with nonzero boundary condition, the multi-dark soliton\nsolution is constructed. We further show that, in the continuous limit, all the\nsolutions obtained converge to the ones for its original CSP equation (see\nPhysica D, 327 13-29 and Phys. Rev. E 93 052227) \n\n"}
{"id": "1901.02596", "contents": "Title: MSR: Multi-Scale Shape Regression for Scene Text Detection Abstract: State-of-the-art scene text detection techniques predict quadrilateral boxes\nthat are prone to localization errors while dealing with straight or curved\ntext lines of different orientations and lengths in scenes. This paper presents\na novel multi-scale shape regression network (MSR) that is capable of locating\ntext lines of different lengths, shapes and curvatures in scenes. The proposed\nMSR detects scene texts by predicting dense text boundary points that\ninherently capture the location and shape of text lines accurately and are also\nmore tolerant to the variation of text line length as compared with the state\nof the arts using proposals or segmentation. Additionally, the multi-scale\nnetwork extracts and fuses features at different scales which demonstrates\nsuperb tolerance to the text scale variation. Extensive experiments over\nseveral public datasets show that the proposed MSR obtains superior detection\nperformance for both curved and straight text lines of different lengths and\norientations. \n\n"}
{"id": "1901.03037", "contents": "Title: Image Transformation can make Neural Networks more robust against\n  Adversarial Examples Abstract: Neural networks are being applied in many tasks related to IoT with\nencouraging results. For example, neural networks can precisely detect human,\nobjects and animal via surveillance camera for security purpose. However,\nneural networks have been recently found vulnerable to well-designed input\nsamples that called adversarial examples. Such issue causes neural networks to\nmisclassify adversarial examples that are imperceptible to humans. We found\ngiving a rotation to an adversarial example image can defeat the effect of\nadversarial examples. Using MNIST number images as the original images, we\nfirst generated adversarial examples to neural network recognizer, which was\ncompletely fooled by the forged examples. Then we rotated the adversarial image\nand gave them to the recognizer to find the recognizer to regain the correct\nrecognition. Thus, we empirically confirmed rotation to images can protect\npattern recognizer based on neural networks from adversarial example attacks. \n\n"}
{"id": "1901.03353", "contents": "Title: RetinaMask: Learning to predict masks improves state-of-the-art\n  single-shot detection for free Abstract: Recently two-stage detectors have surged ahead of single-shot detectors in\nthe accuracy-vs-speed trade-off. Nevertheless single-shot detectors are\nimmensely popular in embedded vision applications. This paper brings\nsingle-shot detectors up to the same level as current two-stage techniques. We\ndo this by improving training for the state-of-the-art single-shot detector,\nRetinaNet, in three ways: integrating instance mask prediction for the first\ntime, making the loss function adaptive and more stable, and including\nadditional hard examples in training. We call the resulting augmented network\nRetinaMask. The detection component of RetinaMask has the same computational\ncost as the original RetinaNet, but is more accurate. COCO test-dev results are\nup to 41.4 mAP for RetinaMask-101 vs 39.1mAP for RetinaNet-101, while the\nruntime is the same during evaluation. Adding Group Normalization increases the\nperformance of RetinaMask-101 to 41.7 mAP. Code is\nat:https://github.com/chengyangfu/retinamask \n\n"}
{"id": "1901.03546", "contents": "Title: Retrieving Similar E-Commerce Images Using Deep Learning Abstract: In this paper, we propose a deep convolutional neural network for learning\nthe embeddings of images in order to capture the notion of visual similarity.\nWe present a deep siamese architecture that when trained on positive and\nnegative pairs of images learn an embedding that accurately approximates the\nranking of images in order of visual similarity notion. We also implement a\nnovel loss calculation method using an angular loss metrics based on the\nproblems requirement. The final embedding of the image is combined\nrepresentation of the lower and top-level embeddings. We used fractional\ndistance matrix to calculate the distance between the learned embeddings in\nn-dimensional space. In the end, we compare our architecture with other\nexisting deep architecture and go on to demonstrate the superiority of our\nsolution in terms of image retrieval by testing the architecture on four\ndatasets. We also show how our suggested network is better than the other\ntraditional deep CNNs used for capturing fine-grained image similarities by\nlearning an optimum embedding. \n\n"}
{"id": "1901.03814", "contents": "Title: Boundary-Aware Network for Fast and High-Accuracy Portrait Segmentation Abstract: Compared with other semantic segmentation tasks, portrait segmentation\nrequires both higher precision and faster inference speed. However, this\nproblem has not been well studied in previous works. In this paper, we propose\na lightweight network architecture, called Boundary-Aware Network (BANet) which\nselectively extracts detail information in boundary area to make high-quality\nsegmentation output with real-time( >25FPS) speed. In addition, we design a new\nloss function called refine loss which supervises the network with image level\ngradient information. Our model is able to produce finer segmentation results\nwhich has richer details than annotations. \n\n"}
{"id": "1901.05633", "contents": "Title: Deep Transfer Across Domains for Face Anti-spoofing Abstract: A practical face recognition system demands not only high recognition\nperformance, but also the capability of detecting spoofing attacks. While\nemerging approaches of face anti-spoofing have been proposed in recent years,\nmost of them do not generalize well to new database. The generalization ability\nof face anti-spoofing needs to be significantly improved before they can be\nadopted by practical application systems. The main reason for the poor\ngeneralization of current approaches is the variety of materials among the\nspoofing devices. As the attacks are produced by putting a spoofing display\n(e.t., paper, electronic screen, forged mask) in front of a camera, the variety\nof spoofing materials can make the spoofing attacks quite different.\nFurthermore, the background/lighting condition of a new environment can make\nboth the real accesses and spoofing attacks different. Another reason for the\npoor generalization is that limited labeled data is available for training in\nface anti-spoofing. In this paper, we focus on improving the generalization\nability across different kinds of datasets. We propose a CNN framework using\nsparsely labeled data from the target domain to learn features that are\ninvariant across domains for face anti-spoofing. Experiments on public-domain\nface spoofing databases show that the proposed method significantly improve the\ncross-dataset testing performance only with a small number of labeled samples\nfrom the target domain. \n\n"}
{"id": "1901.05808", "contents": "Title: AuxNet: Auxiliary tasks enhanced Semantic Segmentation for Automated\n  Driving Abstract: Decision making in automated driving is highly specific to the environment\nand thus semantic segmentation plays a key role in recognizing the objects in\nthe environment around the car. Pixel level classification once considered a\nchallenging task which is now becoming mature to be productized in a car.\nHowever, semantic annotation is time consuming and quite expensive. Synthetic\ndatasets with domain adaptation techniques have been used to alleviate the lack\nof large annotated datasets. In this work, we explore an alternate approach of\nleveraging the annotations of other tasks to improve semantic segmentation.\nRecently, multi-task learning became a popular paradigm in automated driving\nwhich demonstrates joint learning of multiple tasks improves overall\nperformance of each tasks. Motivated by this, we use auxiliary tasks like depth\nestimation to improve the performance of semantic segmentation task. We propose\nadaptive task loss weighting techniques to address scale issues in multi-task\nloss functions which become more crucial in auxiliary tasks. We experimented on\nautomotive datasets including SYNTHIA and KITTI and obtained 3% and 5%\nimprovement in accuracy respectively. \n\n"}
{"id": "1901.06322", "contents": "Title: Learning Spatial Pyramid Attentive Pooling in Image Synthesis and\n  Image-to-Image Translation Abstract: Image synthesis and image-to-image translation are two important generative\nlearning tasks. Remarkable progress has been made by learning Generative\nAdversarial Networks (GANs)~\\cite{goodfellow2014generative} and\ncycle-consistent GANs (CycleGANs)~\\cite{zhu2017unpaired} respectively. This\npaper presents a method of learning Spatial Pyramid Attentive Pooling (SPAP)\nwhich is a novel architectural unit and can be easily integrated into both\ngenerators and discriminators in GANs and CycleGANs. The proposed SPAP\nintegrates Atrous spatial pyramid~\\cite{chen2018deeplab}, a proposed cascade\nattention mechanism and residual connections~\\cite{he2016deep}. It leverages\nthe advantages of the three components to facilitate effective end-to-end\ngenerative learning: (i) the capability of fusing multi-scale information by\nASPP; (ii) the capability of capturing relative importance between both spatial\nlocations (especially multi-scale context) or feature channels by attention;\n(iii) the capability of preserving information and enhancing optimization\nfeasibility by residual connections. Coarse-to-fine and fine-to-coarse SPAP are\nstudied and intriguing attention maps are observed in both tasks. In\nexperiments, the proposed SPAP is tested in GANs on the Celeba-HQ-128\ndataset~\\cite{karras2017progressive}, and tested in CycleGANs on the\nImage-to-Image translation datasets including the Cityscape\ndataset~\\cite{cordts2016cityscapes}, Facade and Aerial Maps\ndataset~\\cite{zhu2017unpaired}, both obtaining better performance. \n\n"}
{"id": "1901.06722", "contents": "Title: Fitting 3D Shapes from Partial and Noisy Point Clouds with Evolutionary\n  Computing Abstract: Point clouds obtained from photogrammetry are noisy and incomplete models of\nreality. We propose an evolutionary optimization methodology that is able to\napproximate the underlying object geometry on such point clouds. This approach\nassumes a priori knowledge on the 3D structure modeled and enables the\nidentification of a collection of primitive shapes approximating the scene.\nBuilt-in mechanisms that enforce high shape diversity and adaptive population\nsize make this method suitable to modeling both simple and complex scenes. We\nfocus here on the case of cylinder approximations and we describe, test, and\ncompare a set of mutation operators designed for optimal exploration of their\nsearch space. We assess the robustness and limitations of this algorithm\nthrough a series of synthetic examples, and we finally demonstrate its general\napplicability on two real-life cases in vegetation and industrial settings. \n\n"}
{"id": "1901.07702", "contents": "Title: Exploring Uncertainty in Conditional Multi-Modal Retrieval Systems Abstract: We cast visual retrieval as a regression problem by posing triplet loss as a\nregression loss. This enables epistemic uncertainty estimation using dropout as\na Bayesian approximation framework in retrieval. Accordingly, Monte Carlo (MC)\nsampling is leveraged to boost retrieval performance. Our approach is evaluated\non two applications: person re-identification and autonomous car driving.\nComparable state-of-the-art results are achieved on multiple datasets for the\nformer application.\n  We leverage the Honda driving dataset (HDD) for autonomous car driving\napplication. It provides multiple modalities and similarity notions for\nego-motion action understanding. Hence, we present a multi-modal conditional\nretrieval network. It disentangles embeddings into separate representations to\nencode different similarities. This form of joint learning eliminates the need\nto train multiple independent networks without any performance degradation.\nQuantitative evaluation highlights our approach competence, achieving 6%\nimprovement in a highly uncertain environment. \n\n"}
{"id": "1901.07821", "contents": "Title: Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff Abstract: Lossy compression algorithms are typically designed and analyzed through the\nlens of Shannon's rate-distortion theory, where the goal is to achieve the\nlowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate.\nHowever, in recent years, it has become increasingly accepted that \"low\ndistortion\" is not a synonym for \"high perceptual quality\", and in fact\noptimization of one often comes at the expense of the other. In light of this\nunderstanding, it is natural to seek for a generalization of rate-distortion\ntheory which takes perceptual quality into account. In this paper, we adopt the\nmathematical definition of perceptual quality recently proposed by Blau &\nMichaeli (2018), and use it to study the three-way tradeoff between rate,\ndistortion, and perception. We show that restricting the perceptual quality to\nbe high, generally leads to an elevation of the rate-distortion curve, thus\nnecessitating a sacrifice in either rate or distortion. We prove several\nfundamental properties of this triple-tradeoff, calculate it in closed form for\na Bernoulli source, and illustrate it visually on a toy MNIST example. \n\n"}
{"id": "1901.08101", "contents": "Title: Domain Translation with Conditional GANs: from Depth to RGB Face-to-Face Abstract: Can faces acquired by low-cost depth sensors be useful to catch some\ncharacteristic details of the face? Typically the answer is no. However, new\ndeep architectures can generate RGB images from data acquired in a different\nmodality, such as depth data. In this paper, we propose a new\n\\textit{Deterministic Conditional GAN}, trained on annotated RGB-D face\ndatasets, effective for a face-to-face translation from depth to RGB. Although\nthe network cannot reconstruct the exact somatic features for unknown\nindividual faces, it is capable to reconstruct plausible faces; their\nappearance is accurate enough to be used in many pattern recognition tasks. In\nfact, we test the network capability to hallucinate with some\n\\textit{Perceptual Probes}, as for instance face aspect classification or\nlandmark detection. Depth face can be used in spite of the correspondent RGB\nimages, that often are not available due to difficult luminance conditions.\nExperimental results are very promising and are as far as better than\npreviously proposed approaches: this domain translation can constitute a new\nway to exploit depth data in new future applications. \n\n"}
{"id": "1901.08339", "contents": "Title: Semi-Supervised Semantic Matching Abstract: Convolutional neural networks (CNNs) have been successfully applied to solve\nthe problem of correspondence estimation between semantically related images.\nDue to non-availability of large training datasets, existing methods resort to\nself-supervised or unsupervised training paradigm. In this paper we propose a\nsemi-supervised learning framework that imposes cyclic consistency constraint\non unlabeled image pairs. Together with the supervised loss the proposed model\nachieves state-of-the-art on a benchmark semantic matching dataset. \n\n"}
{"id": "1901.08341", "contents": "Title: Semantic Matching by Weakly Supervised 2D Point Set Registration Abstract: In this paper we address the problem of establishing correspondences between\ndifferent instances of the same object. The problem is posed as finding the\ngeometric transformation that aligns a given image pair. We use a convolutional\nneural network (CNN) to directly regress the parameters of the transformation\nmodel. The alignment problem is defined in the setting where an unordered set\nof semantic key-points per image are available, but, without the correspondence\ninformation. To this end we propose a novel loss function based on cyclic\nconsistency that solves this 2D point set registration problem by inferring the\noptimal geometric transformation model parameters. We train and test our\napproach on a standard benchmark dataset Proposal-Flow\n(PF-PASCAL)\\cite{proposal_flow}. The proposed approach achieves\nstate-of-the-art results demonstrating the effectiveness of the method. In\naddition, we show our approach further benefits from additional training\nsamples in PF-PASCAL generated by using category level information. \n\n"}
{"id": "1901.08616", "contents": "Title: Boosting Standard Classification Architectures Through a Ranking\n  Regularizer Abstract: We employ triplet loss as a feature embedding regularizer to boost\nclassification performance. Standard architectures, like ResNet and Inception,\nare extended to support both losses with minimal hyper-parameter tuning. This\npromotes generality while fine-tuning pretrained networks. Triplet loss is a\npowerful surrogate for recently proposed embedding regularizers. Yet, it is\navoided due to large batch-size requirement and high computational cost.\nThrough our experiments, we re-assess these assumptions.\n  During inference, our network supports both classification and embedding\ntasks without any computational overhead. Quantitative evaluation highlights a\nsteady improvement on five fine-grained recognition datasets. Further\nevaluation on an imbalanced video dataset achieves significant improvement.\nTriplet loss brings feature embedding characteristics like nearest neighbor to\nclassification models. Code available at \\url{http://bit.ly/2LNYEqL}. \n\n"}
{"id": "1901.08765", "contents": "Title: Exact parity and time reversal symmetry invariant and symmetry breaking\n  solutions for a nonlocal KP system Abstract: A nonlocal Alice-Bob Kadomtsev-Petviashivili (ABKP) system with\nshifted-parities ($\\hat{P}_s^x$ and $\\hat{P}_s^y$ parities with shifts for the\nspace variables $x$ and $y$) and delayed time reversal ($\\hat{T}_d$, time\nreversal with a delay) symmetries is investigated. Some types of\n$\\hat{P}_s^y\\hat{P}_s^x\\hat{T}_d$ invariant solutions including multiple\nsoliton solutions, Painlev\\'e reductions and soliton and $p$-wave interaction\nsolutions are obtained via $\\hat{P}_s^y\\hat{P}_s^x\\hat{T}_d$ symmetry and the\nsolutions of the usual local KP equation. Some special\n$\\hat{P}_s^y\\hat{P}_s^x\\hat{T}_d$ symmetry breaking multi-soliton solutions and\ncnoidal wave solutions are found from the $\\hat{P}_s^y\\hat{P}_s^x\\hat{T}_d$\nsymmetry reduction of a coupled local KP system. \n\n"}
{"id": "1901.08787", "contents": "Title: Multiple Hypothesis Tracking Algorithm for Multi-Target Multi-Camera\n  Tracking with Disjoint Views Abstract: In this study, a multiple hypothesis tracking (MHT) algorithm for\nmulti-target multi-camera tracking (MCT) with disjoint views is proposed. Our\nmethod forms track-hypothesis trees, and each branch of them represents a\nmulti-camera track of a target that may move within a camera as well as move\nacross cameras. Furthermore, multi-target tracking within a camera is performed\nsimultaneously with the tree formation by manipulating a status of each track\nhypothesis. Each status represents three different stages of a multi-camera\ntrack: tracking, searching, and end-of-track. The tracking status means targets\nare tracked by a single camera tracker. In the searching status, the\ndisappeared targets are examined if they reappear in other cameras. The\nend-of-track status does the target exited the camera network due to its\nlengthy invisibility. These three status assists MHT to form the\ntrack-hypothesis trees for multi-camera tracking. Furthermore, they present a\ngating technique for eliminating of unlikely observation-to-track association.\nIn the experiments, they evaluate the proposed method using two datasets,\nDukeMTMC and NLPR-MCT, which demonstrates that the proposed method outperforms\nthe state-of-the-art method in terms of improvement of the accuracy. In\naddition, they show that the proposed method can operate in real-time and\nonline. \n\n"}
{"id": "1901.09124", "contents": "Title: DeepSZ: A Novel Framework to Compress Deep Neural Networks by Using\n  Error-Bounded Lossy Compression Abstract: DNNs have been quickly and broadly exploited to improve the data analysis\nquality in many complex science and engineering applications. Today's DNNs are\nbecoming deeper and wider because of increasing demand on the analysis quality\nand more and more complex applications to resolve. The wide and deep DNNs,\nhowever, require large amounts of resources, significantly restricting their\nutilization on resource-constrained systems. Although some network\nsimplification methods have been proposed to address this issue, they suffer\nfrom either low compression ratios or high compression errors, which may\nintroduce a costly retraining process for the target accuracy. In this paper,\nwe propose DeepSZ: an accuracy-loss bounded neural network compression\nframework, which involves four key steps: network pruning, error bound\nassessment, optimization for error bound configuration, and compressed model\ngeneration, featuring a high compression ratio and low encoding time. The\ncontribution is three-fold. (1) We develop an adaptive approach to select the\nfeasible error bounds for each layer. (2) We build a model to estimate the\noverall loss of accuracy based on the accuracy degradation caused by individual\ndecompressed layers. (3) We develop an efficient optimization algorithm to\ndetermine the best-fit configuration of error bounds in order to maximize the\ncompression ratio under the user-set accuracy constraint. Experiments show that\nDeepSZ can compress AlexNet and VGG-16 on the ImageNet by a compression ratio\nof 46X and 116X, respectively, and compress LeNet-300-100 and LeNet-5 on the\nMNIST by a compression ratio of 57X and 56X, respectively, with only up to 0.3%\nloss of accuracy. Compared with other state-of-the-art methods, DeepSZ can\nimprove the compression ratio by up to 1.43X, the DNN encoding performance by\nup to 4.0X (with four Nvidia Tesla V100 GPUs), and the decoding performance by\nup to 6.2X. \n\n"}
{"id": "1901.09244", "contents": "Title: DistInit: Learning Video Representations Without a Single Labeled Video Abstract: Video recognition models have progressed significantly over the past few\nyears, evolving from shallow classifiers trained on hand-crafted features to\ndeep spatiotemporal networks. However, labeled video data required to train\nsuch models have not been able to keep up with the ever-increasing depth and\nsophistication of these networks. In this work, we propose an alternative\napproach to learning video representations that require no semantically labeled\nvideos and instead leverages the years of effort in collecting and labeling\nlarge and clean still-image datasets. We do so by using state-of-the-art models\npre-trained on image datasets as \"teachers\" to train video models in a\ndistillation framework. We demonstrate that our method learns truly\nspatiotemporal features, despite being trained only using supervision from\nstill-image networks. Moreover, it learns good representations across different\ninput modalities, using completely uncurated raw video data sources and with\ndifferent 2D teacher models. Our method obtains strong transfer performance,\noutperforming standard techniques for bootstrapping video architectures with\nimage-based models by 16%. We believe that our approach opens up new approaches\nfor learning spatiotemporal representations from unlabeled video data. \n\n"}
{"id": "1901.09447", "contents": "Title: Open Source Face Recognition Performance Evaluation Package Abstract: Biometrics-related research has been accelerated significantly by deep\nlearning technology. However, there are limited open-source resources to help\nresearchers evaluate their deep learning-based biometrics algorithms\nefficiently, especially for the face recognition tasks. In this work, we design\nand implement a light-weight, maintainable, scalable, generalizable, and\nextendable face recognition evaluation toolbox named FaRE that supports both\nonline and offline evaluation to provide feedback to algorithm development and\naccelerate biometrics-related research. FaRE consists of a set of evaluation\nmetric functions and provides various APIs for commonly-used face recognition\ndatasets including LFW, CFP, UHDB31, and IJB-series datasets, which can be\neasily extended to include other customized datasets. The package and the\npre-trained baseline models will be released for public academic research use\nafter obtaining university approval. \n\n"}
{"id": "1901.10100", "contents": "Title: Discovering Underlying Person Structure Pattern with Relative Local\n  Distance for Person Re-identification Abstract: Modeling the underlying person structure for person re-identification (re-ID)\nis difficult due to diverse deformable poses, changeable camera views and\nimperfect person detectors. How to exploit underlying person structure\ninformation without extra annotations to improve the performance of person\nre-ID remains largely unexplored. To address this problem, we propose a novel\nRelative Local Distance (RLD) method that integrates a relative local distance\nconstraint into convolutional neural networks (CNNs) in an end-to-end way. It\nis the first time that the relative local constraint is proposed to guide the\nglobal feature representation learning. Specially, a relative local distance\nmatrix is computed by using feature maps and then regarded as a regularizer to\nguide CNNs to learn a structure-aware feature representation. With the\ndiscovered underlying person structure, the RLD method builds a bridge between\nthe global and local feature representation and thus improves the capacity of\nfeature representation for person re-ID. Furthermore, RLD also significantly\naccelerates deep network training compared with conventional methods. The\nexperimental results show the effectiveness of RLD on the CUHK03, Market-1501,\nand DukeMTMC-reID datasets. Code is available at\n\\url{https://github.com/Wanggcong/RLD_codes}. \n\n"}
{"id": "1901.10503", "contents": "Title: Time-Space tradeoff in deep learning models for crop classification on\n  satellite multi-spectral image time series Abstract: In this article, we investigate several structured deep learning models for\ncrop type classification on multi-spectral time series. In particular, our aim\nis to assess the respective importance of spatial and temporal structures in\nsuch data. With this objective, we consider several designs of convolutional,\nrecurrent, and hybrid neural networks, and assess their performance on a large\ndataset of freely available Sentinel-2 imagery. We find that the\nbest-performing approaches are hybrid configurations for which most of the\nparameters (up to 90%) are allocated to modeling the temporal structure of the\ndata. Our results thus constitute a set of guidelines for the design of bespoke\ndeep learning models for crop type classification. \n\n"}
{"id": "astro-ph/0408271", "contents": "Title: Capture and escape in the elliptic restricted three-body problem Abstract: Several families of irregular moons orbit the giant planets. These moons are\nthought to have been captured into planetocentric orbits after straying into a\nregion in which the planet's gravitation dominates solar perturbations (the\nHill sphere). This mechanism requires a source of dissipation, such as\ngas-drag, in order to make capture permanent. However, capture by gas-drag\nrequires that particles remain inside the Hill sphere long enough for\ndissipation to be effective. Recently we have proposed that in the circular\nrestricted three-body problem particles may become caught up in `sticky'\nchaotic layers which tends to prolong their sojourn within the planet's Hill\nsphere thereby assisting capture. Here we show that this mechanism survives\nperturbations due to the ellipticity of the planet's orbit. However, Monte\nCarlo simulations indicate that the planet's ability to capture moons decreases\nwith increasing orbital eccentricity. At the actual Jupiter's orbital\neccentricity, this effects in approximately an order of magnitude lower capture\nprobability than estimated in the circular model. Eccentricities of planetary\norbits in the Solar System are moderate but this is not necessarily the case\nfor extrasolar planets which typically have rather eccentric orbits. Therefore,\nour findings suggest that these extrasolar planets are unlikely to have\nsubstantial populations of irregular moons. \n\n"}
{"id": "astro-ph/0504060", "contents": "Title: Formation of Kuiper-belt binaries through multiple chaotic scattering\n  encounters with low-mass intruders Abstract: The discovery that many trans-neptunian objects exist in pairs, or binaries,\nis proving invaluable for shedding light on the formation, evolution and\nstructure of the outer Solar system. Based on recent systematic searches it has\nbeen estimated that up to 10% of Kuiper-belt objects might be binaries.\nHowever, all examples discovered to-date are unusual, as compared to near-Earth\nand main-belt asteroid binaries, for their mass ratios of order unity and their\nlarge, eccentric orbits. In this article we propose a common dynamical origin\nfor these compositional and orbital properties based on four-body simulations\nin the Hill approximation. Our calculations suggest that binaries are produced\nthrough the following chain of events: initially, long-lived quasi-bound\nbinaries form by two bodies getting entangled in thin layers of dynamical chaos\nproduced by solar tides within the Hill sphere. Next, energy transfer through\ngravitational scattering with a low-mass intruder nudges the binary into a\nnearby non-chaotic, stable zone of phase space. Finally, the binary hardens\n(loses energy) through a series of relatively gentle gravitational scattering\nencounters with further intruders. This produces binary orbits that are well\nfitted by Kepler ellipses. Dynamically, the overall process is strongly favored\nif the original quasi-bound binary contains comparable masses. We propose a\nsimplified model of chaotic scattering to explain these results. Our findings\nsuggest that the observed preference for roughly equal mass ratio binaries is\nprobably a real effect; that is, it is not primarily due to an observational\nbias for widely separated, comparably bright objects. Nevertheless, we predict\nthat a sizeable population of very unequal mass Kuiper-belt binaries is likely\nawaiting discovery. \n\n"}
{"id": "astro-ph/0603092", "contents": "Title: Gauge Freedom in Orbital Mechanics Abstract: In orbital and attitude dynamics the coordinates and the Euler angles are\nexpressed as functions of the time and six constants called elements. Under\ndisturbance, the constants are endowed with time dependence. The Lagrange\nconstraint is then imposed to guarantee that the functional dependence of the\nperturbed velocity on the time and constants stays the same as in the\nundisturbed case. Constants obeying this condition are called osculating\nelements. The constants chosen to be canonical are called Delaunay elements, in\nthe orbital case, or Andoyer elements, in the spin case. (As some Andoyer\nelements are time dependent even in the free-spin case, the role of constants\nis played by their initial values.) The Andoyer and Delaunay sets of elements\nshare a feature not readily apparent: in certain cases the standard equations\nrender them non-osculating. In orbital mechanics, elements furnished by the\nstandard planetary equations are non-osculating when perturbations depend on\nvelocities. To preserve osculation, the equations must be amended with extra\nterms that are not parts of the disturbing function. In the case of Delaunay\nparameterisation, these terms destroy canonicity. So under velocity-dependent\ndisturbances, osculation and canonicity are incompatible. (Efroimsky and\nGoldreich 2003, 2004) Similarly, the Andoyer elements turn out to be\nnon-osculating under angular-velocity-dependent perturbation. Amendment of only\nthe Hamiltonian makes the equations render nonosculating elements. To make them\nosculating, more terms must enter the equations (and the equations will no\nlonger be canonical). In practical calculations, is often convenient to\ndeliberately deviate from osculation by substituting the Lagrange constraint\nwith a condition that gives birth to a family of nonosculating elements. \n\n"}
{"id": "chao-dyn/9906001", "contents": "Title: Synchronization of chaotic systems driven by identical noise Abstract: An analysis of transition from chaotic to nonchaotic behavior and\nsynchronization in an ensemble of systems driven by identical random forces is\npresented. The synchronization phenomenon is investigated in the ensemble of\nparticles moving with friction in the time-dependent potential and driven by\nthe identical noise. The threshold values of the parameters for transition from\nchaotic to nonchaotic behavior are obtained and dependencies of the Lyapunov\nexponents and power spectral density of the current of the ensemble of\nparticles on the nonlinearity of the systems and intensity of the driven force\nare analyzed. \n\n"}
{"id": "cond-mat/0011396", "contents": "Title: Fractal Structure of the Harper Map Phase Diagram from Topological\n  Hierarchical Classification Abstract: It is suggested a topological hierarchical classification of the infinite\nmany Localized phases figuring in the phase diagram of the Harper equation for\nanisotropy parameter $\\epsilon$ versus Energy $E$ with irrational magnetic flux\n$\\omega$. It is also proposed a rule that explain the fractal structure of the\nphase diagram. Among many other applications, this system is equivalent to the\nSemi-classical problem of Bloch electrons in a uniform magnetic field, the\nAzbel-Hofstadter model, where the discrete magnetic translations operators\nconstitute the quantum algebra $U_q(sl_2)$ with $q^2=e^{i2\\pi\\omega}$. The\nmagnetic flux is taken to be the golden mean $\\omega^*=(\\sqrt{5}-1)/2$ and is\nobtained by successive rational approximants $\\omega_m=F_{m-1}/F_m$ with $F_m$\ngiven by the Fibonacci sequence $F_m$.[OUTP-00-08S, \\texttt{cond-mat/0011396}] \n\n"}
{"id": "cond-mat/0110180", "contents": "Title: Beyond the Fokker-Planck equation: Pathwise control of noisy bistable\n  systems Abstract: We introduce a new method, allowing to describe slowly time-dependent\nLangevin equations through the behaviour of individual paths. This approach\nyields considerably more information than the computation of the probability\ndensity. The main idea is to show that for sufficiently small noise intensity\nand slow time dependence, the vast majority of paths remain in small space-time\nsets, typically in the neighbourhood of potential wells. The size of these sets\noften has a power-law dependence on the small parameters, with universal\nexponents. The overall probability of exceptional paths is exponentially small,\nwith an exponent also showing power-law behaviour. The results cover time spans\nup to the maximal Kramers time of the system. We apply our method to three\nphenomena characteristic for bistable systems: stochastic resonance, dynamical\nhysteresis and bifurcation delay, where it yields precise bounds on transition\nprobabilities, and the distribution of hysteresis areas and first-exit times.\nWe also discuss the effect of coloured noise. \n\n"}
{"id": "cond-mat/0511248", "contents": "Title: Direct evaluation of large-deviation functions Abstract: We introduce a numerical procedure to evaluate directly the probabilities of\nlarge deviations of physical quantities, such as current or density, that are\nlocal in time. The large-deviation functions are given in terms of the typical\nproperties of a modified dynamics, and since they no longer involve rare\nevents, can be evaluated efficiently and over a wider ranges of values. We\nillustrate the method with the current fluctuations of the Totally Asymmetric\nExclusion Process and with the work distribution of a driven Lorentz gas. \n\n"}
{"id": "cs/0312044", "contents": "Title: Clustering by compression Abstract: We present a new method for clustering based on compression. The method\ndoesn't use subject-specific features or background knowledge, and works as\nfollows: First, we determine a universal similarity distance, the normalized\ncompression distance or NCD, computed from the lengths of compressed data files\n(singly and in pairwise concatenation). Second, we apply a hierarchical\nclustering method. The NCD is universal in that it is not restricted to a\nspecific application area, and works across application area boundaries. A\ntheoretical precursor, the normalized information distance, co-developed by one\nof the authors, is provably optimal but uses the non-computable notion of\nKolmogorov complexity. We propose precise notions of similarity metric, normal\ncompressor, and show that the NCD based on a normal compressor is a similarity\nmetric that approximates universality. To extract a hierarchy of clusters from\nthe distance matrix, we determine a dendrogram (binary tree) by a new quartet\nmethod and a fast heuristic to implement it. The method is implemented and\navailable as public software, and is robust under choice of different\ncompressors. To substantiate our claims of universality and robustness, we\nreport evidence of successful application in areas as diverse as genomics,\nvirology, languages, literature, music, handwritten digits, astronomy, and\ncombinations of objects from completely different domains, using statistical,\ndictionary, and block sorting compressors. In genomics we presented new\nevidence for major questions in Mammalian evolution, based on\nwhole-mitochondrial genomic analysis: the Eutherian orders and the Marsupionta\nhypothesis against the Theria hypothesis. \n\n"}
{"id": "gr-qc/0702076", "contents": "Title: Choreographic solution to the general relativistic three-body problem Abstract: We revisit the three-body problem in the framework of general relativity. The\nNewtonian N-body problem admits choreographic solutions, where a solution is\ncalled choreographic if every massive particles move periodically in a single\nclosed orbit. One is a stable figure-eight orbit for a three-body system, which\nwas found first by Moore (1993) and re-discovered with its existence proof by\nChenciner and Montgomery (2000). In general relativity, however, the periastron\nshift prohibits a binary system from orbiting in a single closed curve.\nTherefore, it is unclear whether general relativistic effects admit a\nchoreographic solution such as the figure eight. We carefully examine general\nrelativistic corrections to initial conditions so that an orbit for a\nthree-body system can be closed and a figure eight. This solution is still\nchoreographic. This illustration suggests that the general relativistic N-body\nproblem also may admit a certain class of choreographic solutions. \n\n"}
{"id": "math/0102006", "contents": "Title: Continued fractions, modular symbols, and non-commutative geometry Abstract: Using techniques introduced by D. Mayer, we prove an extension of the\nclassical Gauss-Kuzmin theorem about the distribution of continued fractions,\nwhich in particular allows one to take into account some congruence properties\nof successive convergents. This result has an application to the Mixmaster\nUniverse model in general relativity. We then study some averages involving\nmodular symbols and show that Dirichlet series related to modular forms of\nweight 2 can be obtained by integrating certain functions on real axis defined\nin terms of continued fractions. We argue that the quotient\n$PGL(2,\\bold{Z})\\setminus\\bold{P}^1(\\bold{R})$ should be considered as\nnon-commutative modular curve, and show that the modular complex can be seen as\na sequence of $K_0$-groups of the related crossed-product $C^*$-algebras.\n  This paper is an expanded version of the previous \"On the distribution of\ncontinued fractions and modular symbols\". The main new features are Section 4\non non-commutative geometry and the modular complex and Section 1.2.2 on the\nMixmaster Universe. \n\n"}
{"id": "math/0105011", "contents": "Title: Asymptotic description of nonlinear resonance Abstract: We study a hard regime of stimulation of two-frequency oscillations in the\nmain resonance equation with a fast oscillating external force: $ \\ve i \\psi' +\n|\\psi|^2\\psi = \\exp\\big(it^2/ (2\\ve)\\big), 0<\\ve\\ll1$. This phenomenon is\ncaused by resonance between an eigenmode and the external force. The asymptotic\nsolution before, inside and after the resonance layer is studied in detail and\nmatched. \n\n"}
{"id": "math/0311209", "contents": "Title: Dissipation time and decay of correlations Abstract: We consider the effect of noise on the dynamics generated by\nvolume-preserving maps on a d-dimensional torus. The quantity we use to measure\nthe irreversibility of the dynamics is the dissipation time. We focus on the\nasymptotic behaviour of this time in the limit of small noise. We derive\nuniversal lower and upper bounds for the dissipation time in terms of various\nproperties of the map and its associated propagators: spectral properties,\nlocal expansivity, and global mixing properties. We show that the dissipation\nis slow for a general class of non-weakly-mixing maps; on the opposite, it is\nfast for a large class of exponentially mixing systems which include uniformly\nexpanding maps and Anosov diffeomorphisms. \n\n"}
{"id": "math/0501151", "contents": "Title: Symmetries and reversing symmetries of polynomial automorphisms of the\n  plane Abstract: The polynomial automorphisms of the affine plane over a field K form a group\nwhich has the structure of an amalgamated free product. This well-known\nalgebraic structure can be used to determine some key results about the\nsymmetry and reversing symmetry groups of a given polynomial automorphism. \n\n"}
{"id": "math/0611468", "contents": "Title: Stability islands in domains of separatrix crossings in slow-fast\n  Hamiltonian systems Abstract: We consider a 2 d.o.f. Hamiltonian system with one degree of freedom\ncorresponding to fast motion and the other corresponding to slow motion. The\nratio of the time derivatives of slow and fast variables is of order $0<\\eps\n\\ll 1$. At frozen values of the slow variables there is a separatrix on the\nphase plane of the fast variables and there is a region in the phase space (the\ndomain of separatrix crossings) where the projections of phase points onto the\nplane of the fast variables repeatedly cross the separatrix in the process of\nevolution of the slow variables. Under a certain symmetry condition we prove\nexistence of many (of order $1/\\eps$) stable periodic trajectories in the\ndomain of the separatrix crossings. Each of these trajectories is surrounded by\na stability island whose measure is estimated from below by a value of order\n$\\eps$. Thus, the total measure of the stability islands is estimated from\nbelow by a value independent of $\\eps$. The proof is based on an analysis of\nthe asymptotic formulas for the corresponding Poincar\\'e map. \n\n"}
{"id": "nlin/0009019", "contents": "Title: An Additional Gibbs' State for the Cubic Schrodinger Equation on the\n  Circle Abstract: An invariant Gibbs' state for the nonlinear Schrodinger equation on the\ncircle was constructed by Bourgain, and McKean, out of the basic Hamiltonian\nusing a trigonometric cut-off. The cubic nonlinear Schrodinger equation is a\ncompletely integrable system having an infinite number of additional integrals\nof motion. In this paper we construct the second invariant Gibbs' state from\none of these additional integrals for the cubic NLS on the circle. This\nadditional Gibbs' state is singular with respect to the Gibbs' state previously\nconstructed from the basic Hamiltonian. Our approach employs the Ablowitz-Ladik\nsystem, a completely integrable discretization of the cubic Schrodinger\nequation. \n\n"}
{"id": "nlin/0112045", "contents": "Title: q-Painlev\\'e systems arising from q-KP hierarchy Abstract: A system of q-Painlev\\'e type equations with multi-time variables t_1,...,t_M\nis obtained as a similarity reduction of the N-reduced q-KP hierarchy. This\nsystem has affine Weyl group symmetry of type A^{(1)}_{M-1} \\times\nA^{(1)}_{N-1}. Its rational solutions are constructed in terms of q-Schur\nfunctions. \n\n"}
{"id": "nlin/0212045", "contents": "Title: Pseudo-potentials, nonlocal symmetries and integrability of some shallow\n  water equations Abstract: Zero curvature formulations, pseudo-potentials, modified versions, ``Miura\ntransformations'', and nonlocal symmetries of the Korteweg--de Vries, Camassa--\nHolm and Hunter--Saxton equations are investigated from an unified point of\nview: these three equations belong to a two--parameters family of equations\n``describing pseudo-spherical surfaces'', and therefore their basic\nintegrability properties can be studied by geometrical means. \n\n"}
{"id": "nlin/0310010", "contents": "Title: From dispersionless to soliton systems via Weyl-Moyal like deformations Abstract: The formalism of quantization deformation is reviewed and the Weyl-Moyal like\ndeformation is applied to systematic construction of the field and lattice\nintegrable soliton systems from Poisson algebras of dispersionless systems. \n\n"}
{"id": "nlin/0404020", "contents": "Title: Symmetry, Equivalence and Integrable Classes of Abel Equations Abstract: We suggest an approach for description of integrable cases of the Abel\nequations. It is based on increasing of the order of equations up to the second\none and using equivalence transformations for the corresponding second-order\nordinary differential equations. The problem of linearizability of the\nequations under consideration is discussed. \n\n"}
{"id": "nlin/0407025", "contents": "Title: On the Treves theorem for the AKNS equation Abstract: According to a theorem of Treves, the conserved functionals of the AKNS\nequation vanish on all pairs of formal Laurent series of a specified form, both\nof them with a pole of the first order. We propose a new and very simple proof\nfor this statement, based on the theory of B\\\"acklund transformations; using\nthe same method, we prove that the AKNS conserved functionals vanish on other\npairs of Laurent series. The spirit is the same of our previous paper on the\nTreves theorem for the KdV, with some non trivial technical differences. \n\n"}
{"id": "nlin/0409027", "contents": "Title: Nonlinear Lattice Dynamics of Bose-Einstein Condensates Abstract: The Fermi-Pasta-Ulam (FPU) model, which was proposed 50 years ago to examine\nthermalization in non-metallic solids and develop ``experimental'' techniques\nfor studying nonlinear problems, continues to yield a wealth of results in the\ntheory and applications of nonlinear Hamiltonian systems with many degrees of\nfreedom. Inspired by the studies of this seminal model, solitary-wave dynamics\nin lattice dynamical systems have proven vitally important in a diverse range\nof physical problems--including energy relaxation in solids, denaturation of\nthe DNA double strand, self-trapping of light in arrays of optical waveguides,\nand Bose-Einstein condensates (BECs) in optical lattices. BECS, in particular,\ndue to their widely ranging and easily manipulated dynamical apparatuses--with\none to three spatial dimensions, positive-to-negative tuning of the\nnonlinearity, one to multiple components, and numerous experimentally\naccessible external trapping potentials--provide one of the most fertile\ngrounds for the analysis of solitary waves and their interactions. In this\npaper, we review recent research on BECs in the presence of deep periodic\npotentials, which can be reduced to nonlinear chains in appropriate\ncircumstances. These reductions, in turn, exhibit many of the remarkable\nnonlinear structures (including solitons, intrinsic localized modes, and\nvortices) that lie at the heart of the nonlinear science research seeded by the\nFPU paradigm. \n\n"}
{"id": "nlin/0410016", "contents": "Title: Algebraic extensions of Gaudin models Abstract: We perform a In\\\"on\\\"u--Wigner contraction on Gaudin models, showing how the\nintegrability property is preserved by this algebraic procedure. Starting from\nGaudin models we obtain new integrable chains, that we call Lagrange chains,\nassociated to the same linear $r$-matrix structure. We give a general\nconstruction involving rational, trigonometric and elliptic solutions of the\nclassical Yang-Baxter equation. Two particular examples are explicitly\nconsidered: the rational Lagrange chain and the trigonometric one. In both\ncases local variables of the models are the generators of the direct sum of $N$\n$\\mathfrak{e}(3)$ interacting tops. \n\n"}
{"id": "nlin/0410062", "contents": "Title: A method to discern complexity in two-dimensional patterns generated by\n  coupled map lattices Abstract: Complex patterns generated by the time evolution of a one-dimensional\ndigitalized coupled map lattice are quantitatively analyzed. A method for\ndiscerning complexity among the different patterns is implemented. The\nquantitative results indicate two zones in parameter space where the dynamics\nshows the most complex patterns. These zones are located on the two edges of an\nabsorbent region where the system displays spatio-temporal intermittency. \n\n"}
{"id": "nlin/0411044", "contents": "Title: Box-ball system with reflecting end Abstract: A soliton cellular automaton on a one dimensional semi-infinite lattice with\na reflecting end is presented. It extends a box-ball system on an infinite\nlattice associated with the crystal base of U_q(sl_n). A commuting family of\ntime evolutions are obtained by adapting the K matrices and the double row\nconstruction of transfer matrices in solvable lattice models to a crystal\nsetting. Factorized scattering among the left and the right moving solitons and\nthe boundary reflection rule are determined. \n\n"}
{"id": "nlin/0412015", "contents": "Title: Some symmetry classifications of hyperbolic vector evolution equations Abstract: Motivated by recent work on integrable flows of curves and 1+1 dimensional\nsigma models, several O(N)-invariant classes of hyperbolic equations $u_{tx}\n=f(u,u_t,u_x)$ for an $N$-component vector $u(t,x)$ are considered. In each\nclass we find all scaling-homogeneous equations admitting a higher symmetry of\nleast possible scaling weight. Sigma model interpretations of these equations\nare presented. \n\n"}
{"id": "nlin/0412017", "contents": "Title: B\\\"acklund transformations for the rational Lagrange chain Abstract: We consider a long--range homogeneous chain where the local variables are the\ngenerators of the direct sum of $N$ $\\mathfrak{e}(3)$ interacting Lagrange\ntops. We call this classical integrable model rational ``Lagrange chain''\nshowing how one can obtain it starting from $\\mathfrak{su}(2)$ rational Gaudin\nmodels. Moreover we construct one- and two--point integrable maps (B\\\"acklund\ntransformations). \n\n"}
{"id": "nlin/0505018", "contents": "Title: Gel'fand-Zakharevich Systems and Algebraic Integrability: the Volterra\n  Lattice Revisited Abstract: In this paper we will discuss some features of the bihamiltonian method for\nsolving the Hamilton-Jacobi (H-J) equations by Separation of Variables, and\nmake contact with the theory of Algebraic Complete Integrability and,\nspecifically, with the Veselov--Novikov notion of algebro-geometric (AG)\nPoisson brackets. The \"bihamiltonian\" method for separating the Hamilton-Jacobi\nequations is based on the notion of pencil of Poisson brackets and on the\nGel'fand-Zakharevich (GZ) approach to integrable systems. We will herewith show\nhow, quite naturally, GZ systems may give rise to AG Poisson brackets, together\nwith specific recipes to solve the H-J equations. We will then show how this\nsetting works by framing results by Veselov and Penskoi about the algebraic\nintegrability of the Volterra lattice within the bihamiltonian setting for\nSeparation of Variables. \n\n"}
{"id": "nlin/0505059", "contents": "Title: On a Camassa-Holm type equation with two dependent variables Abstract: We consider a generalization of the Camassa Holm (CH) equation with two\ndependent variables, called CH2, introduced by Liu and Zhang. We briefly\nprovide an alternative derivation of it based on the theory of Hamiltonian\nstructures on (the dual of) a Lie Algebra. The Lie Algebra here involved is the\nsame algebra underlying the NLS hierarchy. We study the structural properties\nof the CH2 hierarchy within the bihamiltonian theory of integrable PDEs, and\nprovide its Lax representation. Then we explicitly discuss how to construct\nclasses of solutions, both of peakon and of algebro-geometrical type. We\nfinally sketch the construction of a class of singular solutions, defined by\nsetting to zero one of the two dependent variables. \n\n"}
{"id": "nlin/0506027", "contents": "Title: Characteristic Algebras of Fully Discrete Hyperbolic Type Equations Abstract: The notion of the characteristic Lie algebra of the discrete hyperbolic type\nequation is introduced. An effective algorithm to compute the algebra for the\nequation given is suggested. Examples and further applications are discussed. \n\n"}
{"id": "nlin/0511026", "contents": "Title: Metastability and dispersive shock waves in Fermi-Pasta-Ulam system Abstract: We show the relevance of the dispersive analogue of the shock waves in the\nFPU dynamics. In particular we give strict numerical evidences that metastable\nstates emerging from low frequency initial excitations, are indeed constituted\nby dispersive shock waves travelling through the chain. Relevant\ncharacteristics of the metastable states, such as their frequency extension and\ntheir time scale of formation, are correctly obtained within this framework,\nusing the underlying continuum model, the KdV equation. \n\n"}
{"id": "nlin/0601007", "contents": "Title: Lax Integrable Supersymmetric Hierarchies on Extented Phase Spaces Abstract: We obtain via B\\\"acklund transformation the Hamiltonian representation for a\nLax type nonlinear dynamical system hierarchy on a dual space to the Lie\nalgebra of super-integral-differential operators of one anticommuting variable,\nextended by evolutions of the corresponding spectral problem eigenfunctions and\nadjoint eigenfunctions, as well as for the hierarchies of their additional\nsymmetries. The relation of these hierarchies with the integrable by Lax\n(2|1+1)-dimensional supersymmetric Davey-Stewartson system is investigated. \n\n"}
{"id": "nlin/0601019", "contents": "Title: Solitary wave solutions of the short pulse equation Abstract: An exact nonsingular solitary wave solution of the Schafer-Wayne short pulse\nequation is derived from the breather solution of the sine-Gordon equation by\nmeans of a transformation between these two integrable equations. \n\n"}
{"id": "nlin/0603018", "contents": "Title: Functional representations of integrable hierarchies Abstract: We consider a general framework for integrable hierarchies in Lax form and\nderive certain universal equations from which `functional representations' of\nparticular hierarchies (like KP, discrete KP, mKP, AKNS), i.e. formulations in\nterms of functional equations, are systematically and quite easily obtained.\nThe formalism genuinely applies to hierarchies where the dependent variables\nlive in a noncommutative (typically matrix) algebra. The obtained functional\nrepresentations can be understood as `noncommutative' analogs of `Fay\nidentities' for the KP hierarchy. \n\n"}
{"id": "nlin/0607057", "contents": "Title: Reciprocal transformations for Stackel-related Liouville integrable\n  systems Abstract: We consider the St\\\"ackel transform, also known as the coupling-constant\nmetamorphosis, which under certain conditions turns a Hamiltonian dynamical\nsystem into another such system and preserves the Liouville integrability. We\nshow that the corresponding transformation for the equations of motion is\nnothing but the reciprocal transformation of a special form and we investigate\nthe properties of this transformation. This result is further applied for the\nstudy of the $k$-hole deformations of the Benenti systems or more general seed\nsystems. \n\n"}
{"id": "nlin/0608038", "contents": "Title: On the Linearization of Second-Order Differential and Difference\n  Equations Abstract: This article complements recent results of the papers [J. Math. Phys. 41\n(2000), 480; 45 (2004), 336] on the symmetry classification of second-order\nordinary difference equations and meshes, as well as the Lagrangian formalism\nand Noether-type integration technique. It turned out that there exist\nnonlinear superposition principles for solutions of special second-order\nordinary difference equations which possess Lie group symmetries. This\nsuperposition springs from the linearization of second-order ordinary\ndifference equations by means of non-point transformations which act\nsimultaneously on equations and meshes. These transformations become some sort\nof contact transformations in the continuous limit. \n\n"}
{"id": "nlin/0610011", "contents": "Title: Painleve Analysis and Similarity Reductions for the Magma Equation Abstract: In this paper, we examine a generalized magma equation for rational values of\ntwo parameters, $m$ and $n$. Firstly, the similarity reductions are found using\nthe Lie group method of infinitesimal transformations. The Painlev\\'e ODE test\nis then applied to the travelling wave reduction, and the pairs of $m$ and $n$\nwhich pass the test are identified. These particular pairs are further\nsubjected to the ODE test on their other symmetry reductions. Only two cases\nremain which pass the ODE test for all such symmetry reductions and these are\ncompletely integrable. The case when $m=0$, $n=-1$ is related to the\nHirota-Satsuma equation and for $m=\\tfrac12$, $n=-\\tfrac12$, it is a real,\ngeneralized, pumped Maxwell-Bloch equation. \n\n"}
{"id": "nlin/0610031", "contents": "Title: Integrable Models of Interaction of Matter with Radiation Abstract: The simplified models of interaction of charged matter with resonance modes\nof radiation generalizing the well-known Jaynes-Cummings and Dicke models are\nconsidered. It is found that these new models are integrable for arbitrary\nnumbers of dipole sources and resonance modes of the radiation field. The\nproblem of explicit diagonalisation of corresponding Hamiltonians is discussed. \n\n"}
{"id": "nlin/0610045", "contents": "Title: Burgers and KP hierarchies: A functional representation approach Abstract: From a 'discrete' functional zero curvature equation, functional\nrepresentations of (matrix) Burgers and potential KP (pKP) hierarchies (and\nothers), as well as corresponding Backlund transformations, can be obtained in\na surprisingly simple way. With their help we show that any solution of a\nBurgers hierarchy is also a solution of the pKP hierarchy. Moreover, the pKP\nhierarchy can be expressed in the form of an inhomogeneous Burgers hierarchy.\nIn particular, this leads to an extension of the Cole-Hopf transformation to\nthe pKP hierarchy. Furthermore, these hierarchies are solved by the solutions\nof certain functional Riccati equations. \n\n"}
{"id": "nlin/0611046", "contents": "Title: Combinatorial Bethe ansatz and ultradiscrete Riemann theta function with\n  rational characteristics Abstract: The U_q(\\hat{sl}_2) vertex model at q=0 with periodic boundary condition is\nan integrable cellular automaton in one-dimension. By the combinatorial Bethe\nansatz, the initial value problem is solved for arbitrary states in terms of an\nultradiscrete analogue of the Riemann theta function with rational\ncharacteristics. \n\n"}
{"id": "nlin/0611055", "contents": "Title: Algebro-Geometric Finite-Gap Solutions of the Ablowitz-Ladik Hierarchy Abstract: We provide a detailed derivation of all complex-valued algebro-geometric\nfinite-band solutions of the Ablowitz-Ladik hierarchy. In addition, we survey a\nrecursive construction of the Ablowitz-Ladik hierarchy and its zero-curvature\nand Lax formalism. \n\n"}
{"id": "nlin/0701003", "contents": "Title: Restricted Flows and the Soliton Equation with Self-Consistent Sources Abstract: The KdV equation is used as an example to illustrate the relation between the\nrestricted flows and the soliton equation with self-consistent sources.\nInspired by the results on the Backlund transformation for the restricted flows\n(by V.B. Kuznetsov et al.), we constructed two types of Darboux transformations\nfor the KdV equation with self-consistent sources (KdVES). These Darboux\ntransformations are used to get some explicit solutions of the KdVES, which\ninclude soliton, rational, positon, and negaton solutions. \n\n"}
{"id": "nlin/0701004", "contents": "Title: On the Darboux-Nijenhuis Variables for the Open Toda Lattice Abstract: We discuss two known constructions proposed by Moser and by Sklyanin of the\nDarboux-Nijenhuis coordinates for the open Toda lattice. \n\n"}
{"id": "nlin/0701010", "contents": "Title: Weakly nonassociative algebras, Riccati and KP hierarchies Abstract: It has recently been observed that certain nonassociative algebras (called\n\"weakly nonassociative\", WNA) determine, via a universal hierarchy of ordinary\ndifferential equations, solutions of the KP hierarchy with dependent variable\nin an associative subalgebra (the middle nucleus). We recall central results\nand consider a class of WNA algebras for which the hierarchy of ODEs reduces to\na matrix Riccati hierarchy, which can be easily solved. The resulting solutions\nof a matrix KP hierarchy then determine (under a rank 1 condition) solutions of\nthe scalar KP hierarchy. We extend these results to the discrete KP hierarchy.\nMoreover, we build a bridge from the WNA framework to the Gelfand-Dickey\nformulation of the KP hierarchy. \n\n"}
{"id": "nlin/0701015", "contents": "Title: Exact solvability of superintegrable Benenti systems Abstract: We establish quantum and classical exact solvability for two large classes of\nmaximally superintegrable Benenti systems in $n$ dimensions with arbitrarily\nlarge $n$. Namely, we solve the Hamilton--Jacobi and Schr\\\"odinger equations\nfor the systems in question. The results obtained are illustrated for a model\nwith the cubic potential. \n\n"}
{"id": "nlin/0702051", "contents": "Title: Two-component Analogue of Two-dimensional Long Wave-Short Wave Resonance\n  Interaction Equations: A Derivation and Solutions Abstract: The two-component analogue of two-dimensional long wave-short wave resonance\ninteraction equations is derived in a physical setting. Wronskian solutions of\nthe integrable two-component analogue of two-dimensional long wave-short wave\nresonance interaction equations are presented. \n\n"}
{"id": "nlin/0702058", "contents": "Title: The Ablowitz-Ladik Hierarchy Revisited Abstract: We provide a detailed recursive construction of the Ablowitz-Ladik (AL)\nhierarchy and its zero-curvature formalism. The two-coefficient AL hierarchy\nunder investigation can be considered a complexified version of the discrete\nnonlinear Schr\\\"odinger equation and its hierarchy of nonlinear evolution\nequations.\n  Specifically, we discuss in detail the stationary Ablowitz-Ladik formalism in\nconnection with the underlying hyperelliptic curve and the stationary\nBaker-Akhiezer function and separately the corresponding time-dependent\nAblowitz-Ladik formalism. \n\n"}
{"id": "physics/9801019", "contents": "Title: Momentum Maps and Classical Relativistic Fields. Part I: Covariant Field\n  Theory Abstract: This is the first paper of a five part work in which we study the Lagrangian\nand Hamiltonian structure of classical field theories with constraints. Our\ngoal is to explore some of the connections between initial value constraints\nand gauge transformations in such theories (either relativistic or not). To do\nthis, in the course of these four papers, we develop and use a number of tools\nfrom symplectic and multisymplectic geometry. Of central importance in our\nanalysis is the notion of the ``energy-momentum map'' associated to the gauge\ngroup of a given classical field theory. We hope to demonstrate that many\ndifferent and apparently unrelated facets of field theories can be thereby tied\ntogether and understood in an essentially new way.\n  In Part I we develop some of the basic theory of classical fields from a\nspacetime covariant viewpoint. We begin with a study of the covariant\nLagrangian and Hamiltonian formalisms, on jet bundles and multisymplectic\nmanifolds, respectively. Then we discuss symmetries, conservation laws, and\nNoether's theorem in terms of ``covariant momentum maps.'' \n\n"}
