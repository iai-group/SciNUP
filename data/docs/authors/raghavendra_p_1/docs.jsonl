{"id": "0704.0229", "contents": "Title: Geometric Complexity Theory VI: the flip via saturated and positive\n  integer programming in representation theory and algebraic geometry Abstract: This article belongs to a series on geometric complexity theory (GCT), an\napproach to the P vs. NP and related problems through algebraic geometry and\nrepresentation theory. The basic principle behind this approach is called the\nflip. In essence, it reduces the negative hypothesis in complexity theory (the\nlower bound problems), such as the P vs. NP problem in characteristic zero, to\nthe positive hypothesis in complexity theory (the upper bound problems):\nspecifically, to showing that the problems of deciding nonvanishing of the\nfundamental structural constants in representation theory and algebraic\ngeometry, such as the well known plethysm constants--or rather certain relaxed\nforms of these decision probelms--belong to the complexity class P. In this\narticle, we suggest a plan for implementing the flip, i.e., for showing that\nthese relaxed decision problems belong to P. This is based on the reduction of\nthe preceding complexity-theoretic positive hypotheses to mathematical\npositivity hypotheses: specifically, to showing that there exist positive\nformulae--i.e. formulae with nonnegative coefficients--for the structural\nconstants under consideration and certain functions associated with them. These\nturn out be intimately related to the similar positivity properties of the\nKazhdan-Lusztig polynomials and the multiplicative structural constants of the\ncanonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum\ngroups. The known proofs of these positivity properties depend on the Riemann\nhypothesis over finite fields and the related results. Thus the reduction here,\nin conjunction with the flip, in essence, says that the validity of the P vs.\nNP conjecture in characteristic zero is intimately linked to the Riemann\nhypothesis over finite fields and related problems. \n\n"}
{"id": "0704.1269", "contents": "Title: Phase Transitions in the Coloring of Random Graphs Abstract: We consider the problem of coloring the vertices of a large sparse random\ngraph with a given number of colors so that no adjacent vertices have the same\ncolor. Using the cavity method, we present a detailed and systematic analytical\nstudy of the space of proper colorings (solutions).\n  We show that for a fixed number of colors and as the average vertex degree\n(number of constraints) increases, the set of solutions undergoes several phase\ntransitions similar to those observed in the mean field theory of glasses.\nFirst, at the clustering transition, the entropically dominant part of the\nphase space decomposes into an exponential number of pure states so that beyond\nthis transition a uniform sampling of solutions becomes hard. Afterward, the\nspace of solutions condenses over a finite number of the largest states and\nconsequently the total entropy of solutions becomes smaller than the annealed\none. Another transition takes place when in all the entropically dominant\nstates a finite fraction of nodes freezes so that each of these nodes is\nallowed a single color in all the solutions inside the state. Eventually, above\nthe coloring threshold, no more solutions are available. We compute all the\ncritical connectivities for Erdos-Renyi and regular random graphs and determine\ntheir asymptotic values for large number of colors.\n  Finally, we discuss the algorithmic consequences of our findings. We argue\nthat the onset of computational hardness is not associated with the clustering\ntransition and we suggest instead that the freezing transition might be the\nrelevant phenomenon. We also discuss the performance of a simple local Walk-COL\nalgorithm and of the belief propagation algorithm in the light of our results. \n\n"}
{"id": "0704.1694", "contents": "Title: Locally Decodable Codes From Nice Subsets of Finite Fields and Prime\n  Factors of Mersenne Numbers Abstract: A k-query Locally Decodable Code (LDC) encodes an n-bit message x as an N-bit\ncodeword C(x), such that one can probabilistically recover any bit x_i of the\nmessage by querying only k bits of the codeword C(x), even after some constant\nfraction of codeword bits has been corrupted. The major goal of LDC related\nresearch is to establish the optimal trade-off between length and query\ncomplexity of such codes.\n  Recently [Y] introduced a novel technique for constructing locally decodable\ncodes and vastly improved the upper bounds for code length. The technique is\nbased on Mersenne primes. In this paper we extend the work of [Y] and argue\nthat further progress via these methods is tied to progress on an old number\ntheory question regarding the size of the largest prime factors of Mersenne\nnumbers.\n  Specifically, we show that every Mersenne number m=2^t-1 that has a prime\nfactor p>m^\\gamma yields a family of k(\\gamma)-query locally decodable codes of\nlength Exp(n^{1/t}). Conversely, if for some fixed k and all \\epsilon > 0 one\ncan use the technique of [Y] to obtain a family of k-query LDCs of length\nExp(n^\\epsilon); then infinitely many Mersenne numbers have prime factors arger\nthan known currently. \n\n"}
{"id": "0705.2876", "contents": "Title: An online algorithm for generating fractal hash chains applied to\n  digital chains of custody Abstract: This paper gives an online algorithm for generating Jakobsson's fractal hash\nchains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm\nfor preimage traversal since his algorithm assumes the entire hash chain is\nprecomputed and a particular list of Ceiling(log n) hash elements or pebbles\nare saved. Our online algorithm for hash chain traversal incrementally\ngenerates a hash chain of n hash elements without knowledge of n before it\nstarts. For any n, our algorithm stores only the Ceiling(log n) pebbles which\nare precisely the inputs for Jakobsson's amortized hash chain preimage\ntraversal algorithm. This compact representation is useful to generate,\ntraverse, and store a number of large digital hash chains on a small and\nconstrained device. We also give an application using both Jakobsson's and our\nnew algorithm applied to digital chains of custody for validating dynamically\nchanging forensics data. \n\n"}
{"id": "0706.1918", "contents": "Title: Affine Buildings and Tropical Convexity Abstract: The notion of convexity in tropical geometry is closely related to notions of\nconvexity in the theory of affine buildings. We explore this relationship from\na combinatorial and computational perspective. Our results include a convex\nhull algorithm for the Bruhat--Tits building of SL$_d(K)$ and techniques for\ncomputing with apartments and membranes. While the original inspiration was the\nwork of Dress and Terhalle in phylogenetics, and of Faltings, Kapranov, Keel\nand Tevelev in algebraic geometry, our tropical algorithms will also be\napplicable to problems in other fields of mathematics. \n\n"}
{"id": "0708.1909", "contents": "Title: Lower Bounds for the Complexity of the Voronoi Diagram of Polygonal\n  Curves under the Discrete Frechet Distance Abstract: We give lower bounds for the combinatorial complexity of the Voronoi diagram\nof polygonal curves under the discrete Frechet distance. We show that the\nVoronoi diagram of n curves in R^d with k vertices each, has complexity\nOmega(n^{dk}) for dimension d=1,2 and Omega(n^{d(k-1)+2}) for d>2. \n\n"}
{"id": "0709.1341", "contents": "Title: Coincidence rotations of the root lattice $A_4$ Abstract: The coincidence site lattices of the root lattice $A_4$ are considered, and\nthe statistics of the corresponding coincidence rotations according to their\nindices is expressed in terms of a Dirichlet series generating function. This\nis possible via an embedding of $A_4$ into the icosian ring with its rich\narithmetic structure, which recently (arXiv:math.MG/0702448) led to the\nclassification of the similar sublattices of $A_4$. \n\n"}
{"id": "0709.4273", "contents": "Title: Set Matrices and The Path/Cycle Problem Abstract: Presentation of set matrices and demonstration of their efficiency as a tool\nusing the path/cycle problem. \n\n"}
{"id": "0710.2665", "contents": "Title: Lower bounds on the coefficients of Ehrhart polynomials Abstract: We present lower bounds for the coefficients of Ehrhart polynomials of convex\nlattice polytopes in terms of their volume. Concerning the coefficients of the\nEhrhart series of a lattice polytope we show that Hibi's lower bound is not\ntrue for lattice polytopes without interior lattice points. The counterexample\nis based on a formula of the Ehrhart series of the join of two lattice\npolytope. We also present a formula for calculating the Ehrhart series of\nintegral dilates of a polytope. \n\n"}
{"id": "0712.4027", "contents": "Title: Accurate and Efficient Expression Evaluation and Linear Algebra Abstract: We survey and unify recent results on the existence of accurate algorithms\nfor evaluating multivariate polynomials, and more generally for accurate\nnumerical linear algebra with structured matrices. By \"accurate\" we mean that\nthe computed answer has relative error less than 1, i.e., has some correct\nleading digits. We also address efficiency, by which we mean algorithms that\nrun in polynomial time in the size of the input. Our results will depend\nstrongly on the model of arithmetic: Most of our results will use the so-called\nTraditional Model (TM). We give a set of necessary and sufficient conditions to\ndecide whether a high accuracy algorithm exists in the TM, and describe\nprogress toward a decision procedure that will take any problem and provide\neither a high accuracy algorithm or a proof that none exists. When no accurate\nalgorithm exists in the TM, it is natural to extend the set of available\naccurate operations by a library of additional operations, such as $x+y+z$, dot\nproducts, or indeed any enumerable set which could then be used to build\nfurther accurate algorithms. We show how our accurate algorithms and decision\nprocedure for finding them extend to this case. Finally, we address other\nmodels of arithmetic, and the relationship between (im)possibility in the TM\nand (in)efficient algorithms operating on numbers represented as bit strings. \n\n"}
{"id": "0712.4213", "contents": "Title: Exact Quantum Algorithms for the Leader Election Problem Abstract: This paper gives the first separation of quantum and classical pure (i.e.,\nnon-cryptographic) computing abilities with no restriction on the amount of\navailable computing resources, by considering the exact solvability of a\ncelebrated unsolvable problem in classical distributed computing, the ``leader\nelection problem'' on anonymous networks. The goal of the leader election\nproblem is to elect a unique leader from among distributed parties. The paper\nconsiders this problem for anonymous networks, in which each party has the same\nidentifier. It is well-known that no classical algorithm can solve exactly\n(i.e., in bounded time without error) the leader election problem in anonymous\nnetworks, even if it is given the number of parties. This paper gives two\nquantum algorithms that, given the number of parties, can exactly solve the\nproblem for any network topology in polynomial rounds and polynomial\ncommunication/time complexity with respect to the number of parties, when the\nparties are connected by quantum communication links. \n\n"}
{"id": "0802.2612", "contents": "Title: On Subgraph Isomorphism Abstract: Article explicitly expresses Subgraph Isomorphism by a polynomial size\nasymmetric linear system. \n\n"}
{"id": "0803.0731", "contents": "Title: Complexity Analysis of Reed-Solomon Decoding over GF(2^m) Without Using\n  Syndromes Abstract: For the majority of the applications of Reed-Solomon (RS) codes, hard\ndecision decoding is based on syndromes. Recently, there has been renewed\ninterest in decoding RS codes without using syndromes. In this paper, we\ninvestigate the complexity of syndromeless decoding for RS codes, and compare\nit to that of syndrome-based decoding. Aiming to provide guidelines to\npractical applications, our complexity analysis differs in several aspects from\nexisting asymptotic complexity analysis, which is typically based on\nmultiplicative fast Fourier transform (FFT) techniques and is usually in big O\nnotation. First, we focus on RS codes over characteristic-2 fields, over which\nsome multiplicative FFT techniques are not applicable. Secondly, due to\nmoderate block lengths of RS codes in practice, our analysis is complete since\nall terms in the complexities are accounted for. Finally, in addition to fast\nimplementation using additive FFT techniques, we also consider direct\nimplementation, which is still relevant for RS codes with moderate lengths.\nComparing the complexities of both syndromeless and syndrome-based decoding\nalgorithms based on direct and fast implementations, we show that syndromeless\ndecoding algorithms have higher complexities than syndrome-based ones for high\nrate RS codes regardless of the implementation. Both errors-only and\nerrors-and-erasures decoding are considered in this paper. We also derive\ntighter bounds on the complexities of fast polynomial multiplications based on\nCantor's approach and the fast extended Euclidean algorithm. \n\n"}
{"id": "0803.2325", "contents": "Title: When is a symmetric pin-jointed framework isostatic? Abstract: Maxwell's rule from 1864 gives a necessary condition for a framework to be\nisostatic in 2D or in 3D. Given a framework with point group symmetry, group\nrepresentation theory is exploited to provide further necessary conditions.\nThis paper shows how, for an isostatic framework, these conditions imply very\nsimply stated restrictions on the numbers of those structural components that\nare unshifted by the symmetry operations of the framework. In particular, it\nturns out that an isostatic framework in 2D can belong to one of only six point\ngroups. Some conjectures and initial results are presented that would give\nsufficient conditions (in both 2D and 3D) for a framework that is realized\ngenerically for a given symmetry group to be an isostatic framework. \n\n"}
{"id": "0804.0362", "contents": "Title: Exhaustive enumeration unveils clustering and freezing in random 3-SAT Abstract: We study geometrical properties of the complete set of solutions of the\nrandom 3-satisfiability problem. We show that even for moderate system sizes\nthe number of clusters corresponds surprisingly well with the theoretic\nasymptotic prediction. We locate the freezing transition in the space of\nsolutions which has been conjectured to be relevant in explaining the onset of\ncomputational hardness in random constraint satisfaction problems. \n\n"}
{"id": "0804.0657", "contents": "Title: The discrete square peg problem Abstract: The square peg problem asks whether every Jordan curve in the plane has four\npoints which form a square. The problem has been resolved (positively) for\nvarious classes of curves, but remains open in full generality. We present two\nnew direct proofs for the case of piecewise linear curves. \n\n"}
{"id": "0804.2288", "contents": "Title: Parimutuel Betting on Permutations Abstract: We focus on a permutation betting market under parimutuel call auction model\nwhere traders bet on the final ranking of n candidates. We present a\nProportional Betting mechanism for this market. Our mechanism allows the\ntraders to bet on any subset of the n x n 'candidate-rank' pairs, and rewards\nthem proportionally to the number of pairs that appear in the final outcome. We\nshow that market organizer's decision problem for this mechanism can be\nformulated as a convex program of polynomial size. More importantly, the\nformulation yields a set of n x n unique marginal prices that are sufficient to\nprice the bets in this mechanism, and are computable in polynomial-time. The\nmarginal prices reflect the traders' beliefs about the marginal distributions\nover outcomes. We also propose techniques to compute the joint distribution\nover n! permutations from these marginal distributions. We show that using a\nmaximum entropy criterion, we can obtain a concise parametric form (with only n\nx n parameters) for the joint distribution which is defined over an\nexponentially large state space. We then present an approximation algorithm for\ncomputing the parameters of this distribution. In fact, the algorithm addresses\nthe generic problem of finding the maximum entropy distribution over\npermutations that has a given mean, and may be of independent interest. \n\n"}
{"id": "0804.2699", "contents": "Title: A Critique of a Polynomial-time SAT Solver Devised by Sergey Gubin Abstract: This paper refutes the validity of the polynomial-time algorithm for solving\nsatisfiability proposed by Sergey Gubin. Gubin introduces the algorithm using\n3-SAT and eventually expands it to accept a broad range of forms of the Boolean\nsatisfiability problem. Because 3-SAT is NP-complete, the algorithm would have\nimplied P = NP, had it been correct. Additionally, this paper refutes the\ncorrectness of his polynomial-time reduction of SAT to 2-SAT. \n\n"}
{"id": "0804.4666", "contents": "Title: Combining geometry and combinatorics: A unified approach to sparse\n  signal recovery Abstract: There are two main algorithmic approaches to sparse signal recovery:\ngeometric and combinatorial. The geometric approach starts with a geometric\nconstraint on the measurement matrix and then uses linear programming to decode\ninformation about the signal from its measurements. The combinatorial approach\nconstructs the measurement matrix and a combinatorial decoding algorithm to\nmatch. We present a unified approach to these two classes of sparse signal\nrecovery algorithms.\n  The unifying elements are the adjacency matrices of high-quality unbalanced\nexpanders. We generalize the notion of Restricted Isometry Property (RIP),\ncrucial to compressed sensing results for signal recovery, from the Euclidean\nnorm to the l_p norm for p about 1, and then show that unbalanced expanders are\nessentially equivalent to RIP-p matrices.\n  From known deterministic constructions for such matrices, we obtain new\ndeterministic measurement matrix constructions and algorithms for signal\nrecovery which, compared to previous deterministic algorithms, are superior in\neither the number of measurements or in noise tolerance. \n\n"}
{"id": "0806.2274", "contents": "Title: Exposing Multi-Relational Networks to Single-Relational Network Analysis\n  Algorithms Abstract: Many, if not most network analysis algorithms have been designed specifically\nfor single-relational networks; that is, networks in which all edges are of the\nsame type. For example, edges may either represent \"friendship,\" \"kinship,\" or\n\"collaboration,\" but not all of them together. In contrast, a multi-relational\nnetwork is a network with a heterogeneous set of edge labels which can\nrepresent relationships of various types in a single data structure. While\nmulti-relational networks are more expressive in terms of the variety of\nrelationships they can capture, there is a need for a general framework for\ntransferring the many single-relational network analysis algorithms to the\nmulti-relational domain. It is not sufficient to execute a single-relational\nnetwork analysis algorithm on a multi-relational network by simply ignoring\nedge labels. This article presents an algebra for mapping multi-relational\nnetworks to single-relational networks, thereby exposing them to\nsingle-relational network analysis algorithms. \n\n"}
{"id": "0806.4790", "contents": "Title: AMS Without 4-Wise Independence on Product Domains Abstract: In their seminal work, Alon, Matias, and Szegedy introduced several sketching\ntechniques, including showing that 4-wise independence is sufficient to obtain\ngood approximations of the second frequency moment. In this work, we show that\ntheir sketching technique can be extended to product domains $[n]^k$ by using\nthe product of 4-wise independent functions on $[n]$. Our work extends that of\nIndyk and McGregor, who showed the result for $k = 2$. Their primary motivation\nwas the problem of identifying correlations in data streams. In their model, a\nstream of pairs $(i,j) \\in [n]^2$ arrive, giving a joint distribution $(X,Y)$,\nand they find approximation algorithms for how close the joint distribution is\nto the product of the marginal distributions under various metrics, which\nnaturally corresponds to how close $X$ and $Y$ are to being independent. By\nusing our technique, we obtain a new result for the problem of approximating\nthe $\\ell_2$ distance between the joint distribution and the product of the\nmarginal distributions for $k$-ary vectors, instead of just pairs, in a single\npass. Our analysis gives a randomized algorithm that is a $(1 \\pm \\epsilon)$\napproximation (with probability $1-\\delta$) that requires space logarithmic in\n$n$ and $m$ and proportional to $3^k$. \n\n"}
{"id": "0808.1549", "contents": "Title: The Peculiar Phase Structure of Random Graph Bisection Abstract: The mincut graph bisection problem involves partitioning the n vertices of a\ngraph into disjoint subsets, each containing exactly n/2 vertices, while\nminimizing the number of \"cut\" edges with an endpoint in each subset. When\nconsidered over sparse random graphs, the phase structure of the graph\nbisection problem displays certain familiar properties, but also some\nsurprises. It is known that when the mean degree is below the critical value of\n2 log 2, the cutsize is zero with high probability. We study how the minimum\ncutsize increases with mean degree above this critical threshold, finding a new\nanalytical upper bound that improves considerably upon previous bounds.\nCombined with recent results on expander graphs, our bound suggests the unusual\nscenario that random graph bisection is replica symmetric up to and beyond the\ncritical threshold, with a replica symmetry breaking transition possibly taking\nplace above the threshold. An intriguing algorithmic consequence is that\nalthough the problem is NP-hard, we can find near-optimal cutsizes (whose ratio\nto the optimal value approaches 1 asymptotically) in polynomial time for\ntypical instances near the phase transition. \n\n"}
{"id": "0808.1762", "contents": "Title: Communication Complexities of XOR functions Abstract: We call $F:\\{0, 1\\}^n\\times \\{0, 1\\}^n\\to\\{0, 1\\}$ a symmetric XOR function\nif for a function $S:\\{0, 1, ..., n\\}\\to\\{0, 1\\}$, $F(x, y)=S(|x\\oplus y|)$,\nfor any $x, y\\in\\{0, 1\\}^n$, where $|x\\oplus y|$ is the Hamming weight of the\nbit-wise XOR of $x$ and $y$.\n  We show that for any such function, (a) the deterministic communication\ncomplexity is always $\\Theta(n)$ except for four simple functions that have a\nconstant complexity, and (b) up to a polylog factor, the error-bounded\nrandomized and quantum communication complexities are $\\Theta(r_0+r_1)$, where\n$r_0$ and $r_1$ are the minimum integers such that $r_0, r_1\\leq n/2$ and\n$S(k)=S(k+2)$ for all $k\\in[r_0, n-r_1)$. \n\n"}
{"id": "0809.4332", "contents": "Title: From one solution of a 3-satisfiability formula to a solution cluster:\n  Frozen variables and entropy Abstract: A solution to a 3-satisfiability (3-SAT) formula can be expanded into a\ncluster, all other solutions of which are reachable from this one through a\nsequence of single-spin flips. Some variables in the solution cluster are\nfrozen to the same spin values by one of two different mechanisms: frozen-core\nformation and long-range frustrations. While frozen cores are identified by a\nlocal whitening algorithm, long-range frustrations are very difficult to trace,\nand they make an entropic belief-propagation (BP) algorithm fail to converge.\nFor BP to reach a fixed point the spin values of a tiny fraction of variables\n(chosen according to the whitening algorithm) are externally fixed during the\niteration. From the calculated entropy values, we infer that, for a large\nrandom 3-SAT formula with constraint density close to the satisfiability\nthreshold, the solutions obtained by the survey-propagation or the walksat\nalgorithm belong neither to the most dominating clusters of the formula nor to\nthe most abundant clusters. This work indicates that a single solution cluster\nof a random 3-SAT formula may have further community structures. \n\n"}
{"id": "0810.4840", "contents": "Title: The Pursuit of Uniqueness: Extending Valiant-Vazirani Theorem to the\n  Probabilistic and Quantum Settings Abstract: Valiant-Vazirani showed in 1985 [VV85] that solving NP with the promise that\n\"yes\" instances have only one witness is powerful enough to solve the entire NP\nclass (under randomized reductions).\n  We are interested in extending this result to the quantum setting. We prove\nextensions to the classes Merlin-Arthur MA and Quantum-Classical-Merlin-Arthur\nQCMA. Our results have implications for the complexity of approximating the\nground state energy of a quantum local Hamiltonian with a unique ground state\nand an inverse polynomial spectral gap. We show that the estimation (to within\npolynomial accuracy) of the ground state energy of poly-gapped 1-D local\nHamiltonians is QCMA-hard [AN02], under randomized reductions. This is in stark\ncontrast to the case of constant gapped 1-D Hamiltonians, which is in NP\n[Has07]. Moreover, it shows that unless QCMA can be reduced to NP by randomized\nreductions, there is no classical description of the ground state of every\npoly-gapped local Hamiltonian that allows efficient calculation of expectation\nvalues.\n  Finally, we discuss a few of the obstacles to the establishment of an\nanalogous result to the class Quantum-Merlin-Arthur (QMA). In particular, we\nshow that random projections fail to provide a polynomial gap between two\nwitnesses. \n\n"}
{"id": "0810.4946", "contents": "Title: FPT Algorithms and Kernels for the Directed $k$-Leaf Problem Abstract: A subgraph $T$ of a digraph $D$ is an {\\em out-branching} if $T$ is an\noriented spanning tree with only one vertex of in-degree zero (called the {\\em\nroot}). The vertices of $T$ of out-degree zero are {\\em leaves}. In the {\\sc\nDirected $k$-Leaf} Problem, we are given a digraph $D$ and an integral\nparameter $k$, and we are to decide whether $D$ has an out-branching with at\nleast $k$ leaves. Recently, Kneis et al. (2008) obtained an algorithm for the\nproblem of running time $4^{k}\\cdot n^{O(1)}$. We describe a new algorithm for\nthe problem of running time $3.72^{k}\\cdot n^{O(1)}$. In {\\sc Rooted Directed\n$k$-Leaf} Problem, apart from $D$ and $k$, we are given a vertex $r$ of $D$ and\nwe are to decide whether $D$ has an out-branching rooted at $r$ with at least\n$k$ leaves. Very recently, Fernau et al. (2008) found an $O(k^3)$-size kernel\nfor {\\sc Rooted Directed $k$-Leaf}. In this paper, we obtain an $O(k)$ kernel\nfor {\\sc Rooted Directed $k$-Leaf} restricted to acyclic digraphs. \n\n"}
{"id": "0901.0373", "contents": "Title: Highly Undecidable Problems For Infinite Computations Abstract: We show that many classical decision problems about 1-counter\nomega-languages, context free omega-languages, or infinitary rational\nrelations, are $\\Pi_2^1$-complete, hence located at the second level of the\nanalytical hierarchy, and \"highly undecidable\". In particular, the universality\nproblem, the inclusion problem, the equivalence problem, the determinizability\nproblem, the complementability problem, and the unambiguity problem are all\n$\\Pi_2^1$-complete for context-free omega-languages or for infinitary rational\nrelations. Topological and arithmetical properties of 1-counter\nomega-languages, context free omega-languages, or infinitary rational\nrelations, are also highly undecidable. These very surprising results provide\nthe first examples of highly undecidable problems about the behaviour of very\nsimple finite machines like 1-counter automata or 2-tape automata. \n\n"}
{"id": "0901.3284", "contents": "Title: Simplices and spectra of graphs, continued Abstract: In this note we show that the n-2-dimensional volumes of codimension 2 faces\nof an n-dimensional simplex are algebraically independent quantities of the\nvolumes of its edge-lengths. The proof involves computation of the eigenvalues\nof Kneser graphs. We also construct families of non-congurent simplices not\ndetermined by their codimension-2 areas. \n\n"}
{"id": "0901.3348", "contents": "Title: Nuclear norm minimization for the planted clique and biclique problems Abstract: We consider the problems of finding a maximum clique in a graph and finding a\nmaximum-edge biclique in a bipartite graph. Both problems are NP-hard. We write\nboth problems as matrix-rank minimization and then relax them using the nuclear\nnorm. This technique, which may be regarded as a generalization of compressive\nsensing, has recently been shown to be an effective way to solve rank\noptimization problems. In the special cases that the input graph has a planted\nclique or biclique (i.e., a single large clique or biclique plus diversionary\nedges), our algorithm successfully provides an exact solution to the original\ninstance. For each problem, we provide two analyses of when our algorithm\nsucceeds. In the first analysis, the diversionary edges are placed by an\nadversary. In the second, they are placed at random. In the case of random\nedges for the planted clique problem, we obtain the same bound as Alon,\nKrivelevich and Sudakov as well as Feige and Krauthgamer, but we use different\ntechniques. \n\n"}
{"id": "0902.0828", "contents": "Title: Finding Exact Minimal Polynomial by Approximations Abstract: We present a new algorithm for reconstructing an exact algebraic number from\nits approximate value using an improved parameterized integer relation\nconstruction method. Our result is consistent with the existence of error\ncontrolling on obtaining an exact rational number from its approximation. The\nalgorithm is applicable for finding exact minimal polynomial by its approximate\nroot. This also enables us to provide an efficient method of converting the\nrational approximation representation to the minimal polynomial representation,\nand devise a simple algorithm to factor multivariate polynomials with rational\ncoefficients.\n  Compared with other methods, this method has the numerical computation\nadvantage of high efficiency. The experimental results show that the method is\nmore efficient than \\emph{identify} in \\emph{Maple} 11 for obtaining an exact\nalgebraic number from its approximation. In this paper, we completely implement\nhow to obtain exact results by numerical approximate computations. \n\n"}
{"id": "0902.2537", "contents": "Title: Communication-optimal Parallel and Sequential Cholesky Decomposition Abstract: Numerical algorithms have two kinds of costs: arithmetic and communication,\nby which we mean either moving data between levels of a memory hierarchy (in\nthe sequential case) or over a network connecting processors (in the parallel\ncase). Communication costs often dominate arithmetic costs, so it is of\ninterest to design algorithms minimizing communication. In this paper we first\nextend known lower bounds on the communication cost (both for bandwidth and for\nlatency) of conventional (O(n^3)) matrix multiplication to Cholesky\nfactorization, which is used for solving dense symmetric positive definite\nlinear systems. Second, we compare the costs of various Cholesky decomposition\nimplementations to these lower bounds and identify the algorithms and data\nstructures that attain them. In the sequential case, we consider both the\ntwo-level and hierarchical memory models. Combined with prior results in [13,\n14, 15], this gives a set of communication-optimal algorithms for O(n^3)\nimplementations of the three basic factorizations of dense linear algebra: LU\nwith pivoting, QR and Cholesky. But it goes beyond this prior work on\nsequential LU by optimizing communication for any number of levels of memory\nhierarchy. \n\n"}
{"id": "0904.1630", "contents": "Title: Self-Assembly of a Statistically Self-Similar Fractal Abstract: We demonstrate existence of a tile assembly system that self-assembles the\nstatistically self-similar Sierpinski Triangle in the Winfree-Rothemund Tile\nAssembly Model. This appears to be the first paper that considers self-assembly\nof a random fractal, instead of a deterministic fractal or a finite, bounded\nshape. Our technical contributions include a way to remember, and use,\nunboundedly-long prefixes of an infinite coding sequence at each stage of\nfractal construction; a tile assembly mechanism for nested recursion; and a\ndefinition of \"almost-everywhere local determinism,\" to describe a tileset\nwhose assembly is locally determined, conditional upon a zeta-dimension zero\nset of (infinitely many) \"input\" tiles. This last is similar to the definition\nof randomized computation for Turing machines, in which an algorithm is\ndeterministic relative to an oracle sequence of coin flips that provides advice\nbut does not itself compute. Keywords: tile self-assembly, statistically\nself-similar Sierpinski Triangle. \n\n"}
{"id": "0904.3251", "contents": "Title: On evaluation of permanents Abstract: We study the time and space complexity of matrix permanents over rings and\nsemirings. \n\n"}
{"id": "0905.2485", "contents": "Title: Minimizing Communication in Linear Algebra Abstract: In 1981 Hong and Kung proved a lower bound on the amount of communication\nneeded to perform dense, matrix-multiplication using the conventional $O(n^3)$\nalgorithm, where the input matrices were too large to fit in the small, fast\nmemory. In 2004 Irony, Toledo and Tiskin gave a new proof of this result and\nextended it to the parallel case. In both cases the lower bound may be\nexpressed as $\\Omega$(#arithmetic operations / $\\sqrt{M}$), where M is the size\nof the fast memory (or local memory in the parallel case). Here we generalize\nthese results to a much wider variety of algorithms, including LU\nfactorization, Cholesky factorization, $LDL^T$ factorization, QR factorization,\nalgorithms for eigenvalues and singular values, i.e., essentially all direct\nmethods of linear algebra. The proof works for dense or sparse matrices, and\nfor sequential or parallel algorithms. In addition to lower bounds on the\namount of data moved (bandwidth) we get lower bounds on the number of messages\nrequired to move it (latency). We illustrate how to extend our lower bound\ntechnique to compositions of linear algebra operations (like computing powers\nof a matrix), to decide whether it is enough to call a sequence of simpler\noptimal algorithms (like matrix multiplication) to minimize communication, or\nif we can do better. We give examples of both. We also show how to extend our\nlower bounds to certain graph theoretic problems.\n  We point out recently designed algorithms for dense LU, Cholesky, QR,\neigenvalue and the SVD problems that attain these lower bounds; implementations\nof LU and QR show large speedups over conventional linear algebra algorithms in\nstandard libraries like LAPACK and ScaLAPACK. Many open problems remain. \n\n"}
{"id": "0906.2262", "contents": "Title: Analogues of the central point theorem for families with\n  $d$-intersection property in $\\mathbb R^d$ Abstract: In this paper we consider families of compact convex sets in $\\mathbb R^d$\nsuch that any subfamily of size at most $d$ has a nonempty intersection. We\nprove some analogues of the central point theorem and Tverberg's theorem for\nsuch families. \n\n"}
{"id": "0906.2671", "contents": "Title: Oscillations and Random Perturbations of a FitzHugh-Nagumo System Abstract: We consider a stochastic perturbation of a FitzHugh-Nagumo system. We show\nthat it is possible to generate oscillations for values of parameters which do\nnot allow oscillations for the deterministic system. We also study the\nappearance of a new equilibrium point and new bifurcation parameters due to the\nnoisy component. \n\n"}
{"id": "0907.1297", "contents": "Title: Bounds on the quantum satisfiability threshold Abstract: Quantum k-SAT is the problem of deciding whether there is a n-qubit state\nwhich is perpendicular to a set of vectors, each of which lies in the Hilbert\nspace of k qubits. Equivalently, the problem is to decide whether a particular\ntype of local Hamiltonian has a ground state with zero energy. We consider\nrandom quantum k-SAT formulas with n variables and m = \\alpha n clauses, and\nask at what value of \\alpha these formulas cease to be satisfiable. We show\nthat the threshold for random quantum 3-SAT is at most 3.594. For comparison,\nconvincing arguments from statistical physics suggest that the classical 3-SAT\nthreshold is \\alpha \\approx 4.267. For larger k, we show that the quantum\nthreshold is a constant factor smaller than the classical one. Our bounds work\nby determining the generic rank of the satisfying subspace for certain gadgets,\nand then using the technique of differential equations to analyze various\nalgorithms that partition the hypergraph into a collection of these gadgets.\nOur use of differential equation to establish upper bounds on a satisfiability\nthreshold appears to be novel, and our techniques may apply to various\nclassical problems as well. \n\n"}
{"id": "0907.3583", "contents": "Title: Web of Lossy Adapters for Interface Interoperability: An Algorithm and\n  NP-completeness of Minimization Abstract: By using different interface adapters for different methods, it is possible\nto construct a maximally covering web of interface adapters which incurs\nminimum loss during interface adaptation. We introduce a polynomial-time\nalgorithm that can achieve this. However, we also show that minimizing the\nnumber of adapters included in a maximally covering web of interface adapters\nis an NP-complete problem. \n\n"}
{"id": "0907.4283", "contents": "Title: Domination Problems in Nowhere-Dense Classes of Graphs Abstract: We investigate the parameterized complexity of generalisations and variations\nof the dominating set problem on classes of graphs that are nowhere dense. In\nparticular, we show that the distance-d dominating-set problem, also known as\nthe (k,d)-centres problem, is fixed-parameter tractable on any class that is\nnowhere dense and closed under induced subgraphs. This generalises known\nresults about the dominating set problem on H-minor free classes, classes with\nlocally excluded minors and classes of graphs of bounded expansion. A key\nfeature of our proof is that it is based simply on the fact that these graph\nclasses are uniformly quasi-wide, and does not rely on a structural\ndecomposition. Our result also establishes that the distance-d dominating-set\nproblem is FPT on classes of bounded expansion, answering a question of Ne{\\v\ns}et{\\v{r}}il and Ossona de Mendez. \n\n"}
{"id": "0908.2940", "contents": "Title: A Strong Direct Product Theorem for Disjointness Abstract: A strong direct product theorem states that if we want to compute $k$\nindependent instances of a function, using less than $k$ times the resources\nneeded for one instance, then the overall success probability will be\nexponentially small in $k$. We establish such a theorem for the randomized\ncommunication complexity of the Disjointness problem, i.e., with communication\n$const\\cdot kn$ the success probability of solving $k$ instances of size $n$\ncan only be exponentially small in $k$. We show that this bound even holds for\n$AM$ communication protocols with limited ambiguity. This also implies a new\nlower bound for Disjointness in a restricted 3-player NOF protocol, and optimal\ncommunication-space tradeoffs for Boolean matrix product. Our main result\nfollows from a solution to the dual of a linear programming problem, whose\nfeasibility comes from a so-called Intersection Sampling Lemma that generalizes\na result by Razborov. \n\n"}
{"id": "0908.4177", "contents": "Title: Prodsimplicial-Neighborly Polytopes Abstract: Simultaneously generalizing both neighborly and neighborly cubical polytopes,\nwe introduce PSN polytopes: their k-skeleton is combinatorially equivalent to\nthat of a product of r simplices. We construct PSN polytopes by three different\nmethods, the most versatile of which is an extension of Sanyal and Ziegler's\n\"projecting deformed products\" construction to products of arbitrary simple\npolytopes. For general r and k, the lowest dimension we achieve is 2k+r+1.\nUsing topological obstructions similar to those introduced by Sanyal to bound\nthe number of vertices of Minkowski sums, we show that this dimension is\nminimal if we additionally require that the PSN polytope is obtained as a\nprojection of a polytope that is combinatorially equivalent to the product of r\nsimplices, when the dimensions of these simplices are all large compared to k. \n\n"}
{"id": "0908.4499", "contents": "Title: Monadic second-order model-checking on decomposable matroids Abstract: A notion of branch-width, which generalizes the one known for graphs, can be\ndefined for matroids. We first give a proof of the polynomial time\nmodel-checking of monadic second-order formulas on representable matroids of\nbounded branch-width, by reduction to monadic second-order formulas on trees.\nThis proof is much simpler than the one previously known. We also provide a\nlink between our logical approach and a grammar that allows to build matroids\nof bounded branch-width. Finally, we introduce a new class of non-necessarily\nrepresentable matroids, described by a grammar and on which monadic\nsecond-order formulas can be checked in linear time. \n\n"}
{"id": "0909.2893", "contents": "Title: New Classes of Counterexamples to Hendrickson's Global Rigidity\n  Conjecture Abstract: We examine the generic local and global rigidity of various graphs in R^d.\nBruce Hendrickson showed that some necessary conditions for generic global\nrigidity are (d+1)-connectedness and generic redundant rigidity and\nhypothesized that they were sufficient in all dimensions. We analyze two\nclasses of graphs that satisfy Hendrickson's conditions for generic global\nrigidity, yet fail to be generically globally rigid. We find a large family of\nbipartite graphs for d > 3, and we define a construction that generates\ninfinitely many graphs in R^5. Finally, we state some conjectures for further\nexploration. \n\n"}
{"id": "0909.4766", "contents": "Title: Quantum Adiabatic Algorithms, Small Gaps, and Different Paths Abstract: We construct a set of instances of 3SAT which are not solved efficiently\nusing the simplest quantum adiabatic algorithm. These instances are obtained by\npicking random clauses all consistent with two disparate planted solutions and\nthen penalizing one of them with a single additional clause. We argue that by\nrandomly modifying the beginning Hamiltonian, one obtains (with substantial\nprobability) an adiabatic path that removes this difficulty. This suggests that\nthe quantum adiabatic algorithm should in general be run on each instance with\nmany different random paths leading to the problem Hamiltonian. We do not know\nwhether this trick will help for a random instance of 3SAT (as opposed to an\ninstance from the particular set we consider), especially if the instance has\nan exponential number of disparate assignments that violate few clauses. We use\na continuous imaginary time Quantum Monte Carlo algorithm in a novel way to\nnumerically investigate the ground state as well as the first excited state of\nour system. Our arguments are supplemented by Quantum Monte Carlo data from\nsimulations with up to 150 spins. \n\n"}
{"id": "0909.4969", "contents": "Title: MACH: Fast Randomized Tensor Decompositions Abstract: Tensors naturally model many real world processes which generate multi-aspect\ndata. Such processes appear in many different research disciplines, e.g,\nchemometrics, computer vision, psychometrics and neuroimaging analysis. Tensor\ndecompositions such as the Tucker decomposition are used to analyze\nmulti-aspect data and extract latent factors, which capture the multilinear\ndata structure. Such decompositions are powerful mining tools, for extracting\npatterns from large data volumes. However, most frequently used algorithms for\nsuch decompositions involve the computationally expensive Singular Value\nDecomposition.\n  In this paper we propose MACH, a new sampling algorithm to compute such\ndecompositions. Our method is of significant practical value for tensor\nstreams, such as environmental monitoring systems, IP traffic matrices over\ntime, where large amounts of data are accumulated and the analysis is\ncomputationally intensive but also in \"post-mortem\" data analysis cases where\nthe tensor does not fit in the available memory. We provide the theoretical\nanalysis of our proposed method, and verify its efficacy in monitoring system\napplications. \n\n"}
{"id": "0910.0443", "contents": "Title: Stackelberg Pricing is Hard to Approximate within $2-\\epsilon$ Abstract: Stackelberg Pricing Games is a two-level combinatorial pricing problem\nstudied in the Economics, Operation Research, and Computer Science communities.\nIn this paper, we consider the decade-old shortest path version of this problem\nwhich is the first and most studied problem in this family.\n  The game is played on a graph (representing a network) consisting of {\\em\nfixed cost} edges and {\\em pricable} or {\\em variable cost} edges. The fixed\ncost edges already have some fixed price (representing the competitor's\nprices). Our task is to choose prices for the variable cost edges. After that,\na client will buy the cheapest path from a node $s$ to a node $t$, using any\ncombination of fixed cost and variable cost edges. The goal is to maximize the\nrevenue on variable cost edges.\n  In this paper, we show that the problem is hard to approximate within\n$2-\\epsilon$, improving the previous \\APX-hardness result by Joret [to appear\nin {\\em Networks}]. Our technique combines the existing ideas with a new\ninsight into the price structure and its relation to the hardness of the\ninstances. \n\n"}
{"id": "0910.2041", "contents": "Title: Towards a Calculus for Non-Linear Spectral Gaps [Extended Abstract] Abstract: Given a finite regular graph G=(V,E) and a metric space (X,d_X), let\n$gamma_+(G,X) denote the smallest constant $\\gamma_+>0$ such that for all\nf,g:V\\to X we have:\n  \\frac{1}{|V|^2}\\sum_{x,y\\in V} d_X(f(x),g(y))^2\\le \\frac{\\gamma_+}{|E|}\n\\sum_{xy\\in E} d_X(f(x),g(y))^2.\n  In the special case X=R this quantity coincides with the reciprocal of the\nabsolute spectral gap of $G$, but for other geometries the parameter\n\\gamma_+(G,X), which we still think of as measuring the non-linear spectral gap\nof G with respect to X (even though there is no actual spectrum present here),\ncan behave very differently.\n  Non-linear spectral gaps arise often in the theory of metric embeddings, and\nin the present paper we systematically study the theory of non-linear spectral\ngaps, partially in order to obtain a combinatorial construction of\nsuper-expander -- a family of bounded-degree graphs G_i=(V_i,E_i), with\n\\lim_{i\\to \\infty} |V_i|=\\infty, which do not admit a coarse embedding into any\nuniformly convex normed space. In addition, the bi-Lipschitz distortion of G_i\nin any uniformly convex Banach space is \\Omega(\\log |V_i|), which is the worst\npossible behavior due to Bourgain's embedding theorem. Such remarkable graph\nfamilies were previously known to exist due to a tour de force algebraic\nconstruction of Lafforgue. Our construction is different and combinatorial,\nrelying on the zigzag product of Reingold-Vadhan-Wigderson. \n\n"}
{"id": "0910.2370", "contents": "Title: On the hardness of the noncommutative determinant Abstract: In this paper we study the computational complexity of computing the\nnoncommutative determinant. We first consider the arithmetic circuit complexity\nof computing the noncommutative determinant polynomial. Then, more generally,\nwe also examine the complexity of computing the determinant (as a function)\nover noncommutative domains. Our hardness results are summarized below:\n  1. We show that if the noncommutative determinant polynomial has small\nnoncommutative arithmetic circuits then so does the noncommutative permanent.\nConsequently, the commutative permanent polynomial has small commutative\narithmetic circuits. 2. For any field F we show that computing the n X n\npermanent over F is polynomial-time reducible to computing the 2n X 2n\n(noncommutative) determinant whose entries are O(n^2) X O(n^2) matrices over\nthe field F. 3. We also derive as a consequence that computing the n X n\npermanent over nonnegative rationals is polynomial-time reducible to computing\nthe noncommutative determinant over Clifford algebras of n^{O(1)} dimension.\n  Our techniques are elementary and use primarily the notion of the Hadamard\nProduct of noncommutative polynomials. \n\n"}
{"id": "0910.4103", "contents": "Title: On the growth of cocompact hyperbolic Coxeter groups Abstract: For an arbitrary cocompact hyperbolic Coxeter group G with finite generator\nset S and complete growth function P(x)/Q(x), we provide a recursion formula\nfor the coefficients of the denominator polynomial Q(x) which allows to\ndetermine recursively the Taylor coefficients and the pole behavior of the\ngrowth function of G in terms of its Coxeter subgroup structure. We illustrate\nthis in the easy case of compact right-angled hyperbolic n-polytopes. Finally,\nwe provide detailed insight into the case of Coxeter groups with at most 6\ngenerators, acting cocompactly on hyperbolic 4-space, by considering the three\ncombinatorially different families discovered and classified by Lanner,\nKaplinskaya and Esselmann, respectively. \n\n"}
{"id": "0911.3195", "contents": "Title: Efficient Distributed Random Walks with Applications Abstract: We focus on the problem of performing random walks efficiently in a\ndistributed network. Given bandwidth constraints, the goal is to minimize the\nnumber of rounds required to obtain a random walk sample. We first present a\nfast sublinear time distributed algorithm for performing random walks whose\ntime complexity is sublinear in the length of the walk. Our algorithm performs\na random walk of length $\\ell$ in $\\tilde{O}(\\sqrt{\\ell D})$ rounds (with high\nprobability) on an undirected network, where $D$ is the diameter of the\nnetwork. This improves over the previous best algorithm that ran in\n$\\tilde{O}(\\ell^{2/3}D^{1/3})$ rounds (Das Sarma et al., PODC 2009). We further\nextend our algorithms to efficiently perform $k$ independent random walks in\n$\\tilde{O}(\\sqrt{k\\ell D} + k)$ rounds. We then show that there is a\nfundamental difficulty in improving the dependence on $\\ell$ any further by\nproving a lower bound of $\\Omega(\\sqrt{\\frac{\\ell}{\\log \\ell}} + D)$ under a\ngeneral model of distributed random walk algorithms. Our random walk algorithms\nare useful in speeding up distributed algorithms for a variety of applications\nthat use random walks as a subroutine. We present two main applications. First,\nwe give a fast distributed algorithm for computing a random spanning tree (RST)\nin an arbitrary (undirected) network which runs in $\\tilde{O}(\\sqrt{m}D)$\nrounds (with high probability; here $m$ is the number of edges). Our second\napplication is a fast decentralized algorithm for estimating mixing time and\nrelated parameters of the underlying network. Our algorithm is fully\ndecentralized and can serve as a building block in the design of\ntopologically-aware networks. \n\n"}
{"id": "0912.0850", "contents": "Title: Grammar-Based Compression in a Streaming Model Abstract: We show that, given a string $s$ of length $n$, with constant memory and\nlogarithmic passes over a constant number of streams we can build a\ncontext-free grammar that generates $s$ and only $s$ and whose size is within\nan $\\Oh{\\min (g \\log g, \\sqrt{n \\log g})}$-factor of the minimum $g$. This\nstands in contrast to our previous result that, with polylogarithmic memory and\npolylogarithmic passes over a single stream, we cannot build such a grammar\nwhose size is within any polynomial of $g$. \n\n"}
{"id": "1001.0018", "contents": "Title: Nonadaptive quantum query complexity Abstract: We study the power of nonadaptive quantum query algorithms, which are\nalgorithms whose queries to the input do not depend on the result of previous\nqueries. First, we show that any bounded-error nonadaptive quantum query\nalgorithm that computes some total boolean function depending on n variables\nmust make Omega(n) queries to the input in total. Second, we show that, if\nthere exists a quantum algorithm that uses k nonadaptive oracle queries to\nlearn which one of a set of m boolean functions it has been given, there exists\na nonadaptive classical algorithm using O(k log m) queries to solve the same\nproblem. Thus, in the nonadaptive setting, quantum algorithms can achieve at\nmost a very limited speed-up over classical query algorithms. \n\n"}
{"id": "1001.1139", "contents": "Title: Spatial search in a honeycomb network Abstract: The spatial search problem consists in minimizing the number of steps\nrequired to find a given site in a network, under the restriction that only\noracle queries or translations to neighboring sites are allowed. In this paper,\na quantum algorithm for the spatial search problem on a honeycomb lattice with\n$N$ sites and torus-like boundary conditions. The search algorithm is based on\na modified quantum walk on a hexagonal lattice and the general framework\nproposed by Ambainis, Kempe and Rivosh is used to show that the time complexity\nof this quantum search algorithm is $O(\\sqrt{N \\log N})$. \n\n"}
{"id": "1001.2331", "contents": "Title: Information Theoretic Bounds for Low-Rank Matrix Completion Abstract: This paper studies the low-rank matrix completion problem from an information\ntheoretic perspective. The completion problem is rephrased as a communication\nproblem of an (uncoded) low-rank matrix source over an erasure channel. The\npaper then uses achievability and converse arguments to present order-wise\noptimal bounds for the completion problem. \n\n"}
{"id": "1001.2767", "contents": "Title: Universally Optimal Privacy Mechanisms for Minimax Agents Abstract: A scheme that publishes aggregate information about sensitive data must\nresolve the trade-off between utility to information consumers and privacy of\nthe database participants. Differential privacy is a well-established\ndefinition of privacy--this is a universal guarantee against all attackers,\nwhatever their side-information or intent. In this paper, we present a\nuniversal treatment of utility based on the standard minimax rule from decision\ntheory (in contrast to the utility model in, which is Bayesian). In our model,\ninformation consumers are minimax (risk-averse) agents, each possessing some\nside-information about the query, and each endowed with a loss-function which\nmodels their tolerance to inaccuracies. Further, information consumers are\nrational in the sense that they actively combine information from the mechanism\nwith their side-information in a way that minimizes their loss. Under this\nassumption of rational behavior, we show that for every fixed count query, a\ncertain geometric mechanism is universally optimal for all minimax information\nconsumers. Additionally, our solution makes it possible to release query\nresults at multiple levels of privacy in a collusion-resistant manner. \n\n"}
{"id": "1001.3528", "contents": "Title: Rigidity of quasicrystallic and Z^\\gamma-circle patterns Abstract: The uniqueness of the orthogonal Z^\\gamma-circle patterns as studied by\nBobenko and Agafonov is shown, given the combinatorics and some boundary\nconditions. Furthermore we study (infinite) rhombic embeddings in the plane\nwhich are quasicrystallic, that is they have only finitely many different edge\ndirections. Bicoloring the vertices of the rhombi and adding circles with\ncenters at vertices of one of the colors and radius equal to the edge length\nleads to isoradial quasicrystallic circle patterns. We prove for a large class\nof such circle patterns which cover the whole plane that they are uniquely\ndetermined up to affine transformations by the combinatorics and the\nintersection angles. Combining these two results, we obtain the rigidity of\nlarge classes of quasicrystallic Z^\\gamma-circle patterns. \n\n"}
{"id": "1002.0739", "contents": "Title: Gradual sub-lattice reduction and a new complexity for factoring\n  polynomials Abstract: We present a lattice algorithm specifically designed for some classical\napplications of lattice reduction. The applications are for lattice bases with\na generalized knapsack-type structure, where the target vectors are boundably\nshort. For such applications, the complexity of the algorithm improves\ntraditional lattice reduction by replacing some dependence on the bit-length of\nthe input vectors by some dependence on the bound for the output vectors. If\nthe bit-length of the target vectors is unrelated to the bit-length of the\ninput, then our algorithm is only linear in the bit-length of the input\nentries, which is an improvement over the quadratic complexity floating-point\nLLL algorithms. To illustrate the usefulness of this algorithm we show that a\ndirect application to factoring univariate polynomials over the integers leads\nto the first complexity bound improvement since 1984. A second application is\nalgebraic number reconstruction, where a new complexity bound is obtained as\nwell. \n\n"}
{"id": "1002.3864", "contents": "Title: Limits of Approximation Algorithms: PCPs and Unique Games (DIMACS\n  Tutorial Lecture Notes) Abstract: These are the lecture notes for the DIMACS Tutorial \"Limits of Approximation\nAlgorithms: PCPs and Unique Games\" held at the DIMACS Center, CoRE Building,\nRutgers University on 20-21 July, 2009. This tutorial was jointly sponsored by\nthe DIMACS Special Focus on Hardness of Approximation, the DIMACS Special Focus\non Algorithmic Foundations of the Internet, and the Center for Computational\nIntractability with support from the National Security Agency and the National\nScience Foundation.\n  The speakers at the tutorial were Matthew Andrews, Sanjeev Arora, Moses\nCharikar, Prahladh Harsha, Subhash Khot, Dana Moshkovitz and Lisa Zhang. The\nsribes were Ashkan Aazami, Dev Desai, Igor Gorodezky, Geetha Jagannathan,\nAlexander S. Kulikov, Darakhshan J. Mir, Alantha Newman, Aleksandar Nikolov,\nDavid Pritchard and Gwen Spencer. \n\n"}
{"id": "1002.4464", "contents": "Title: Deterministic Sample Sort For GPUs Abstract: We present and evaluate GPU Bucket Sort, a parallel deterministic sample sort\nalgorithm for many-core GPUs. Our method is considerably faster than Thrust\nMerge (Satish et.al., Proc. IPDPS 2009), the best comparison-based sorting\nalgorithm for GPUs, and it is as fast as the new randomized sample sort for\nGPUs by Leischner et.al. (to appear in Proc. IPDPS 2010). Our deterministic\nsample sort has the advantage that bucket sizes are guaranteed and therefore\nits running time does not have the input data dependent fluctuations that can\noccur for randomized sample sort. \n\n"}
{"id": "1002.4577", "contents": "Title: Bounded Rationality, Strategy Simplification, and Equilibrium Abstract: It is frequently suggested that predictions made by game theory could be\nimproved by considering computational restrictions when modeling agents. Under\nthe supposition that players in a game may desire to balance maximization of\npayoff with minimization of strategy complexity, Rubinstein and co-authors\nstudied forms of Nash equilibrium where strategies are maximally simplified in\nthat no strategy can be further simplified without sacrificing payoff. Inspired\nby this line of work, we introduce a notion of equilibrium whereby strategies\nare also maximally simplified, but with respect to a simplification procedure\nthat is more careful in that a player will not simplify if the simplification\nincents other players to deviate. We study such equilibria in two-player\nmachine games in which players choose finite automata that succinctly represent\nstrategies for repeated games; in this context, we present techniques for\nestablishing that an outcome is at equilibrium and present results on the\nstructure of equilibria. \n\n"}
{"id": "1004.0351", "contents": "Title: An Oblivious Spanning Tree for Buy-at-Bulk Network Design Problems Abstract: We consider the problem of constructing a single spanning tree for the\nsingle-source buy-at-bulk network design problem for doubling-dimension graphs.\nWe compute a spanning tree to route a set of demands (or data) along a graph to\nor from a designated root node. The demands could be aggregated at (or\nsymmetrically distributed to) intermediate nodes where the fusion-cost is\nspecified by a non-negative concave function $f$. We describe a novel approach\nfor developing an oblivious spanning tree in the sense that it is independent\nof the number of data sources (or demands) and cost function at intermediate\nnodes. To our knowledge, this is the first paper to propose a single spanning\ntree solution to this problem (as opposed to multiple overlay trees). There has\nbeen no prior work where the tree is oblivious to both the fusion cost function\nand the set of sources (demands). We present a deterministic, polynomial-time\nalgorithm for constructing a spanning tree in low doubling graphs that\nguarantees $\\log^{3}D\\cdot\\log n$-approximation over the optimal cost, where\n$D$ is the diameter of the graph and $n$ the total number of nodes. With\nconstant fusion-cost function our spanning tree gives a $O(\\log^3\nD)$-approximation for every Steiner tree to the root. \n\n"}
{"id": "1004.1986", "contents": "Title: Wedderburn rank reduction and Krylov subspace method for tensor\n  approximation. Part 1: Tucker case Abstract: New algorithms are proposed for the Tucker approximation of a 3-tensor, that\naccess it using only the tensor-by-vector-by-vector multiplication subroutine.\nIn the matrix case, Krylov methods are methods of choice to approximate the\ndominant column and row subspaces of a sparse or structured matrix given\nthrough the matrix-by-vector multiplication subroutine. Using the Wedderburn\nrank reduction formula, we propose an algorithm of matrix approximation that\ncomputes Krylov subspaces and allows generalization to the tensor case. Several\nvariants of proposed tensor algorithms differ by pivoting strategies, overall\ncost and quality of approximation. By convincing numerical experiments we show\nthat the proposed methods are faster and more accurate than the minimal Krylov\nrecursion, proposed recently by Elden and Savas. \n\n"}
{"id": "1005.2642", "contents": "Title: Pebbles and Branching Programs for Tree Evaluation Abstract: We introduce the Tree Evaluation Problem, show that it is in logDCFL (and\nhence in P), and study its branching program complexity in the hope of\neventually proving a superlogarithmic space lower bound. The input to the\nproblem is a rooted, balanced d-ary tree of height h, whose internal nodes are\nlabeled with d-ary functions on [k] = {1,...,k}, and whose leaves are labeled\nwith elements of [k]. Each node obtains a value in [k] equal to its d-ary\nfunction applied to the values of its d children. The output is the value of\nthe root. We show that the standard black pebbling algorithm applied to the\nbinary tree of height h yields a deterministic k-way branching program with\nTheta(k^h) states solving this problem, and we prove that this upper bound is\ntight for h=2 and h=3. We introduce a simple semantic restriction called\n\"thrifty\" on k-way branching programs solving tree evaluation problems and show\nthat the same state bound of Theta(k^h) is tight (up to a constant factor) for\nall h >= 2 for deterministic thrifty programs. We introduce fractional pebbling\nfor trees and show that this yields nondeterministic thrifty programs with\nTheta(k^{h/2+1}) states solving the Boolean problem \"determine whether the root\nhas value 1\". We prove that this bound is tight for h=2,3,4, and tight for\nunrestricted nondeterministic k-way branching programs for h=2,3. \n\n"}
{"id": "1005.3730", "contents": "Title: Obtaining the Quantum Fourier Transform from the Classical FFT with QR\n  Decomposition Abstract: We present the detailed process of converting the classical Fourier Transform\nalgorithm into the quantum one by using QR decomposition. This provides an\nexample of a technique for building quantum algorithms using classical ones.\nThe Quantum Fourier Transform is one of the most important quantum subroutines\nknown at present, used in most algorithms that have exponential speed up\ncompared to the classical ones. We briefly review Fast Fourier Transform and\nthen make explicit all the steps that led to the quantum formulation of the\nalgorithm, generalizing Coppersmith's work. \n\n"}
{"id": "1005.5520", "contents": "Title: The potential to improve the choice: list conflict-free coloring for\n  geometric hypergraphs Abstract: Given a geometric hypergraph (or a range-space) $H=(V,\\cal E)$, a coloring of\nits vertices is said to be conflict-free if for every hyperedge $S \\in \\cal E$\nthere is at least one vertex in $S$ whose color is distinct from the colors of\nall other vertices in $S$. The study of this notion is motivated by frequency\nassignment problems in wireless networks. We study the list-coloring (or\nchoice) version of this notion. In this version, each vertex is associated with\na set of (admissible) colors and it is allowed to be colored only with colors\nfrom its set. List coloring arises naturally in the context of wireless\nnetworks.\n  Our main result is a list coloring algorithm based on a new potential method.\nThe algorithm produces a stronger unique-maximum coloring, in which colors are\npositive integers and the maximum color in every hyperedge occurs uniquely. As\na corollary, we provide asymptotically sharp bounds on the size of the lists\nrequired to assure the existence of such unique-maximum colorings for many\ngeometric hypergraphs (e.g., discs or pseudo-discs in the plane or points with\nrespect to discs). Moreover, we provide an algorithm, such that, given a family\nof lists with the appropriate sizes, computes such a coloring from these lists. \n\n"}
{"id": "1006.3046", "contents": "Title: Identifying Shapes Using Self-Assembly (extended abstract) Abstract: In this paper, we introduce the following problem in the theory of\nalgorithmic self-assembly: given an input shape as the seed of a tile-based\nself-assembly system, design a finite tile set that can, in some sense,\nuniquely identify whether or not the given input shape--drawn from a very\ngeneral class of shapes--matches a particular target shape. We first study the\ncomplexity of correctly identifying squares. Then we investigate the complexity\nassociated with the identification of a considerably more general class of\nnon-square, hole-free shapes. \n\n"}
{"id": "1006.3651", "contents": "Title: Quantum algorithms for formula evaluation Abstract: We survey the recent sequence of algorithms for evaluating Boolean formulas\nconsisting of NAND gates. \n\n"}
{"id": "1006.4396", "contents": "Title: Faster Algorithms for Feedback Arc Set Tournament, Kemeny Rank\n  Aggregation and Betweenness Tournament Abstract: We study fixed parameter algorithms for three problems: Kemeny rank\naggregation, feedback arc set tournament, and betweenness tournament. For\nKemeny rank aggregation we give an algorithm with runtime O*(2^O(sqrt{OPT})),\nwhere n is the number of candidates, OPT is the cost of the optimal ranking,\nand O* hides polynomial factors. This is a dramatic improvement on the\npreviously best known runtime of O*(2^O(OPT)). For feedback arc set tournament\nwe give an algorithm with runtime O*(2^O(sqrt{OPT})), an improvement on the\npreviously best known O*(OPT^O(sqrt{OPT})) (Alon, Lokshtanov and Saurabh 2009).\nFor betweenness tournament we give an algorithm with runtime\nO*(2^O(sqrt{OPT/n})), where n is the number of vertices and OPT is the optimal\ncost. This improves on the previously known O*(OPT^O(OPT^{1/3}))$ (Saurabh\n2009), especially when OPT is small. Unusually we can solve instances with OPT\nas large as n (log n)^2 in polynomial time! \n\n"}
{"id": "1006.5574", "contents": "Title: Notes on lattice points of zonotopes and lattice-face polytopes Abstract: Minkowski's second theorem on successive minima gives an upper bound on the\nvolume of a convex body in terms of its successive minima. We study the problem\nto generalize Minkowski's bound by replacing the volume by the lattice point\nenumerator of a convex body. In this context we are interested in bounds on the\ncoefficients of Ehrhart polynomials of lattice polytopes via the successive\nminima. Our results for lattice zonotopes and lattice-face polytopes imply, in\nparticular, that for 0-symmetric lattice-face polytopes and lattice\nparallelepipeds the volume can be replaced by the lattice point enumerator. \n\n"}
{"id": "1007.1800", "contents": "Title: Multimode Control Attacks on Elections Abstract: In 1992, Bartholdi, Tovey, and Trick opened the study of control attacks on\nelections---attempts to improve the election outcome by such actions as\nadding/deleting candidates or voters. That work has led to many results on how\nalgorithms can be used to find attacks on elections and how\ncomplexity-theoretic hardness results can be used as shields against attacks.\nHowever, all the work in this line has assumed that the attacker employs just a\nsingle type of attack. In this paper, we model and study the case in which the\nattacker launches a multipronged (i.e., multimode) attack. We do so to more\nrealistically capture the richness of real-life settings. For example, an\nattacker might simultaneously try to suppress some voters, attract new voters\ninto the election, and introduce a spoiler candidate. Our model provides a\nunified framework for such varied attacks, and by constructing polynomial-time\nmultiprong attack algorithms we prove that for various election systems even\nsuch concerted, flexible attacks can be perfectly planned in deterministic\npolynomial time. \n\n"}
{"id": "1007.5032", "contents": "Title: Approximation Algorithms for Secondary Spectrum Auctions Abstract: We study combinatorial auctions for the secondary spectrum market. In this\nmarket, short-term licenses shall be given to wireless nodes for communication\nin their local neighborhood. In contrast to the primary market, channels can be\nassigned to multiple bidders, provided that the corresponding devices are well\nseparated such that the interference is sufficiently low. Interference\nconflicts are described in terms of a conflict graph in which the nodes\nrepresent the bidders and the edges represent conflicts such that the feasible\nallocations for a channel correspond to the independent sets in the conflict\ngraph.\n  In this paper, we suggest a novel LP formulation for combinatorial auctions\nwith conflict graph using a non-standard graph parameter, the so-called\ninductive independence number. Taking into account this parameter enables us to\nbypass the well-known lower bound of \\Omega(n^{1-\\epsilon}) on the\napproximability of independent set in general graphs with n nodes (bidders). We\nachieve significantly better approximation results by showing that interference\nconstraints for wireless networks yield conflict graphs with bounded inductive\nindependence number.\n  Our framework covers various established models of wireless communication,\ne.g., the protocol or the physical model. For the protocol model, we achieve an\nO(\\sqrt{k})-approximation, where k is the number of available channels. For the\nmore realistic physical model, we achieve an O(\\sqrt{k} \\log^2 n) approximation\nbased on edge-weighted conflict graphs. Combining our approach with the the\nLP-based framework of Lavi and Swamy, we obtain incentive compatible mechanisms\nfor general bidders with arbitrary valuations on bundles of channels specified\nin terms of demand oracles. \n\n"}
{"id": "1008.2422", "contents": "Title: The Quantum Query Complexity of AC0 Abstract: We show that any quantum algorithm deciding whether an input function $f$\nfrom $[n]$ to $[n]$ is 2-to-1 or almost 2-to-1 requires $\\Theta(n)$ queries to\n$f$. The same lower bound holds for determining whether or not a function $f$\nfrom $[2n-2]$ to $[n]$ is surjective. These results yield a nearly linear\n$\\Omega(n/\\log n)$ lower bound on the quantum query complexity of $\\cl{AC}^0$.\nThe best previous lower bound known for any $\\cl{AC^0}$ function was the\n$\\Omega ((n/\\log n)^{2/3})$ bound given by Aaronson and Shi's $\\Omega(n^{2/3})$\nlower bound for the element distinctness problem. \n\n"}
{"id": "1008.2814", "contents": "Title: Convex optimization for the planted k-disjoint-clique problem Abstract: We consider the k-disjoint-clique problem. The input is an undirected graph G\nin which the nodes represent data items, and edges indicate a similarity\nbetween the corresponding items. The problem is to find within the graph k\ndisjoint cliques that cover the maximum number of nodes of G. This problem may\nbe understood as a general way to pose the classical `clustering' problem. In\nclustering, one is given data items and a distance function, and one wishes to\npartition the data into disjoint clusters of data items, such that the items in\neach cluster are close to each other. Our formulation additionally allows\n`noise' nodes to be present in the input data that are not part of any of the\ncliques. The k-disjoint-clique problem is NP-hard, but we show that a convex\nrelaxation can solve it in polynomial time for input instances constructed in a\ncertain way. The input instances for which our algorithm finds the optimal\nsolution consist of k disjoint large cliques (called `planted cliques') that\nare then obscured by noise edges and noise nodes inserted either at random or\nby an adversary. \n\n"}
{"id": "1008.4071", "contents": "Title: Hybrid tractability of soft constraint problems Abstract: The constraint satisfaction problem (CSP) is a central generic problem in\ncomputer science and artificial intelligence: it provides a common framework\nfor many theoretical problems as well as for many real-life applications. Soft\nconstraint problems are a generalisation of the CSP which allow the user to\nmodel optimisation problems. Considerable effort has been made in identifying\nproperties which ensure tractability in such problems. In this work, we\ninitiate the study of hybrid tractability of soft constraint problems; that is,\nproperties which guarantee tractability of the given soft constraint problem,\nbut which do not depend only on the underlying structure of the instance (such\nas being tree-structured) or only on the types of soft constraints in the\ninstance (such as submodularity). We present several novel hybrid classes of\nsoft constraint problems, which include a machine scheduling problem,\nconstraint problems of arbitrary arities with no overlapping nogoods, and the\nSoftAllDiff constraint with arbitrary unary soft constraints. An important tool\nin our investigation will be the notion of forbidden substructures. \n\n"}
{"id": "1008.4563", "contents": "Title: Shortest paths between shortest paths and independent sets Abstract: We study problems of reconfiguration of shortest paths in graphs. We prove\nthat the shortest reconfiguration sequence can be exponential in the size of\nthe graph and that it is NP-hard to compute the shortest reconfiguration\nsequence even when we know that the sequence has polynomial length. Moreover,\nwe also study reconfiguration of independent sets in three different models and\nanalyze relationships between these models, observing that shortest path\nreconfiguration is a special case of independent set reconfiguration in perfect\ngraphs, under any of the three models. Finally, we give polynomial results for\nrestricted classes of graphs (even-hole-free and $P_4$-free graphs). \n\n"}
{"id": "1009.1381", "contents": "Title: A Branch-and-Reduce Algorithm for Finding a Minimum Independent\n  Dominating Set Abstract: An independent dominating set D of a graph G = (V,E) is a subset of vertices\nsuch that every vertex in V \\ D has at least one neighbor in D and D is an\nindependent set, i.e. no two vertices of D are adjacent in G. Finding a minimum\nindependent dominating set in a graph is an NP-hard problem. Whereas it is hard\nto cope with this problem using parameterized and approximation algorithms,\nthere is a simple exact O(1.4423^n)-time algorithm solving the problem by\nenumerating all maximal independent sets. In this paper we improve the latter\nresult, providing the first non trivial algorithm computing a minimum\nindependent dominating set of a graph in time O(1.3569^n). Furthermore, we give\na lower bound of \\Omega(1.3247^n) on the worst-case running time of this\nalgorithm, showing that the running time analysis is almost tight. \n\n"}
{"id": "1009.2242", "contents": "Title: Minimal-memory realization of pearl-necklace encoders of general quantum\n  convolutional codes Abstract: Quantum convolutional codes, like their classical counterparts, promise to\noffer higher error correction performance than block codes of equivalent\nencoding complexity, and are expected to find important applications in\nreliable quantum communication where a continuous stream of qubits is\ntransmitted. Grassl and Roetteler devised an algorithm to encode a quantum\nconvolutional code with a \"pearl-necklace encoder.\" Despite their theoretical\nsignificance as a neat way of representing quantum convolutional codes, they\nare not well-suited to practical realization. In fact, there is no\nstraightforward way to implement any given pearl-necklace structure. This paper\ncloses the gap between theoretical representation and practical implementation.\nIn our previous work, we presented an efficient algorithm for finding a\nminimal-memory realization of a pearl-necklace encoder for\nCalderbank-Shor-Steane (CSS) convolutional codes. This work extends our\nprevious work and presents an algorithm for turning a pearl-necklace encoder\nfor a general (non-CSS) quantum convolutional code into a realizable quantum\nconvolutional encoder. We show that a minimal-memory realization depends on the\ncommutativity relations between the gate strings in the pearl-necklace encoder.\nWe find a realization by means of a weighted graph which details the\nnon-commutative paths through the pearl-necklace. The weight of the longest\npath in this graph is equal to the minimal amount of memory needed to implement\nthe encoder. The algorithm has a polynomial-time complexity in the number of\ngate strings in the pearl-necklace encoder. \n\n"}
{"id": "1010.3091", "contents": "Title: Near-Optimal Bayesian Active Learning with Noisy Observations Abstract: We tackle the fundamental problem of Bayesian active learning with noise,\nwhere we need to adaptively select from a number of expensive tests in order to\nidentify an unknown hypothesis sampled from a known prior distribution. In the\ncase of noise-free observations, a greedy algorithm called generalized binary\nsearch (GBS) is known to perform near-optimally. We show that if the\nobservations are noisy, perhaps surprisingly, GBS can perform very poorly. We\ndevelop EC2, a novel, greedy active learning algorithm and prove that it is\ncompetitive with the optimal policy, thus obtaining the first competitiveness\nguarantees for Bayesian active learning with noisy observations. Our bounds\nrely on a recently discovered diminishing returns property called adaptive\nsubmodularity, generalizing the classical notion of submodular set functions to\nadaptive policies. Our results hold even if the tests have non-uniform cost and\ntheir noise is correlated. We also propose EffECXtive, a particularly fast\napproximation of EC2, and evaluate it on a Bayesian experimental design problem\ninvolving human subjects, intended to tease apart competing economic theories\nof how people make decisions under uncertainty. \n\n"}
{"id": "1010.4458", "contents": "Title: Variable time amplitude amplification and a faster quantum algorithm for\n  solving systems of linear equations Abstract: We present two new quantum algorithms. Our first algorithm is a\ngeneralization of amplitude amplification to the case when parts of the quantum\nalgorithm that is being amplified stop at different times.\n  Our second algorithm uses the first algorithm to improve the running time of\nHarrow et al. algorithm for solving systems of linear equations from O(kappa^2\nlog N) to O(kappa log^3 kappa log N) where \\kappa is the condition number of\nthe system of equations. \n\n"}
{"id": "1010.5016", "contents": "Title: A Unified Framework for Testing Linear-Invariant Properties Abstract: The study of the interplay between the testability of properties of Boolean\nfunctions and the invariances acting on their domain which preserve the\nproperty was initiated by Kaufman and Sudan (STOC 2008). Invariance with\nrespect to F_2-linear transformations is arguably the most common symmetry\nexhibited by natural properties of Boolean functions on the hypercube. Hence,\nan important goal in Property Testing is to describe necessary and sufficient\nconditions for the testability of linear-invariant properties. This direction\nwas explicitly proposed for investigation in a recent survey of Sudan.\n  We obtain the following results:\n  1. We show that every linear-invariant property that can be characterized by\nforbidding induced solutions to a (possibly infinite) set of linear equations\ncan be tested with one-sided error.\n  2. We show that every linear-invariant property that can be tested with\none-sided error can be characterized by forbidding induced solutions to a\n(possibly infinite) set of systems of linear equations.\n  We conjecture that our result from item (1) can be extended to cover systems\nof linear equations. We further show that the validity of this conjecture would\nhave the following implications:\n  1. It would imply that every linear-invariant property that is closed under\nrestrictions to linear subspaces is testable with one-sided error. Such a\nresult would unify several previous results on testing Boolean functions, such\nas the testability of low-degree polynomials and of Fourier dimensionality.\n  2. It would imply that a linear-invariant property P is testable with\none-sided error if and only if P is closed under restrictions to linear\nsubspaces, thus resolving Sudan's problem. \n\n"}
{"id": "1010.5440", "contents": "Title: Finite motions from periodic frameworks with added symmetry Abstract: Recent work from authors across disciplines has made substantial\ncontributions to counting rules (Maxwell type theorems) which predict when an\ninfinite periodic structure would be rigid or flexible while preserving the\nperiodic pattern, as an engineering type framework, or equivalently, as an\nidealized molecular framework. Other work has shown that for finite frameworks,\nintroducing symmetry modifies the previous general counts, and under some\ncircumstances this symmetrized Maxwell type count can predict added finite\nflexibility in the structure.\n  In this paper we combine these approaches to present new Maxwell type counts\nfor the columns and rows of a modified orbit matrix for structures that have\nboth a periodic structure and additional symmetry within the periodic cells. In\na number of cases, this count for the combined group of symmetry operations\ndemonstrates there is added finite flexibility in what would have been rigid\nwhen realized without the symmetry. Given that many crystal structures have\nthese added symmetries, and that their flexibility may be key to their physical\nand chemical properties, we present a summary of the results as a way to\ngenerate further developments of both a practical and theoretic interest. \n\n"}
{"id": "1011.0468", "contents": "Title: Efficient Triangle Counting in Large Graphs via Degree-based Vertex\n  Partitioning Abstract: The number of triangles is a computationally expensive graph statistic which\nis frequently used in complex network analysis (e.g., transitivity ratio), in\nvarious random graph models (e.g., exponential random graph model) and in\nimportant real world applications such as spam detection, uncovering of the\nhidden thematic structure of the Web and link recommendation. Counting\ntriangles in graphs with millions and billions of edges requires algorithms\nwhich run fast, use small amount of space, provide accurate estimates of the\nnumber of triangles and preferably are parallelizable.\n  In this paper we present an efficient triangle counting algorithm which can\nbe adapted to the semistreaming model. The key idea of our algorithm is to\ncombine the sampling algorithm of Tsourakakis et al. and the partitioning of\nthe set of vertices into a high degree and a low degree subset respectively as\nin the Alon, Yuster and Zwick work treating each set appropriately. We obtain a\nrunning time $O \\left(m + \\frac{m^{3/2} \\Delta \\log{n}}{t \\epsilon^2} \\right)$\nand an $\\epsilon$ approximation (multiplicative error), where $n$ is the number\nof vertices, $m$ the number of edges and $\\Delta$ the maximum number of\ntriangles an edge is contained.\n  Furthermore, we show how this algorithm can be adapted to the semistreaming\nmodel with space usage $O\\left(m^{1/2}\\log{n} + \\frac{m^{3/2} \\Delta \\log{n}}{t\n\\epsilon^2} \\right)$ and a constant number of passes (three) over the graph\nstream. We apply our methods in various networks with several millions of edges\nand we obtain excellent results. Finally, we propose a random projection based\nmethod for triangle counting and provide a sufficient condition to obtain an\nestimate with low variance. \n\n"}
{"id": "1011.3493", "contents": "Title: Program Size and Temperature in Self-Assembly Abstract: Winfree's abstract Tile Assembly Model (aTAM) is a model of molecular\nself-assembly of DNA complexes known as tiles, which float freely in solution\nand attach one at a time to a growing \"seed\" assembly based on specific binding\nsites on their four sides. We show that there is a polynomial-time algorithm\nthat, given an n x n square, finds the minimal tile system (i.e., the system\nwith the smallest number of distinct tile types) that uniquely self-assembles\nthe square, answering an open question of Adleman, Cheng, Goel, Huang, Kempe,\nMoisset de Espanes, and Rothemund (\"Combinatorial Optimization Problems in\nSelf-Assembly\", STOC 2002). Our investigation leading to this algorithm reveals\nother positive and negative results about the relationship between the size of\na tile system and its \"temperature\" (the binding strength threshold required\nfor a tile to attach). \n\n"}
{"id": "1011.3843", "contents": "Title: Magnetic Towers of Hanoi and their Optimal Solutions Abstract: The Magnetic Tower of Hanoi puzzle - a modified \"base 3\" version of the\nclassical Tower of Hanoi puzzle as described in earlier papers, is actually a\nsmall set of independent sister-puzzles, depending on the \"pre-coloring\"\ncombination of the tower's posts. Starting with Red facing up on a Source post,\nworking through an Intermediate - colored or Neutral post, and ending Blue\nfacing up on a Destination post, we identify the different pre-coloring\ncombinations in (S,I,D) order. The Tower's pre-coloring combinations are\n{[(R,B,B) / (R,R,B)] ; [(R,B,N) / (N,R,B)] ; [(N,B,N) / (N,R,N)] ; [R,N,B] ;\n[(R,N,N) / (N,N,B)] ; [N,N,N]}. In this paper we investigate these\nsister-puzzles, identify the algorithm that optimally solves each pre-colored\npuzzle, and prove its Optimality. As it turns out, five of the six algorithms,\nchallenging on their own, are part of the algorithm solving the \"natural\", Free\nMagnetic Tower of Hanoi puzzle [N,N,N]. We start by showing that the N-disk\nColored Tower [(R,B,B) / (R,R,B)] is solved by (3^N - 1)/2 moves. Defining\n\"Algorithm Duration\" as the ratio of number of algorithm-moves solving the\npuzzle to the number of algorithm-moves solving the Colored Tower, we find the\nDuration-Limits for all sister-puzzles. In the order of the list above they are\n{[1] ; [10/11] ; [10/11] ; [8/11] ; [7/11] ; [20/33]}. Thus, the Duration-Limit\nof the Optimal Algorithm solving the Free Magnetic Tower of Hanoi puzzle is\n20/33 or 606 0/00. On the road to optimally solve this colorful Magnetic\npuzzle, we hit other \"forward-moving\" puzzle-solving algorithms. Overall we\nlooked at 10 pairs of integer sequences. Of the twenty integer sequences, five\nare listed in the On-line Encyclopedia of Integer Sequences, the other fifteen\n- not yet. The large set of different solutions is a clear indication to the\nfreedom-of-wondering that makes this Magnetic Tower of Hanoi puzzle so\ncolorful. \n\n"}
{"id": "1011.6021", "contents": "Title: Border basis detection is NP-complete Abstract: Border basis detection (BBD) is described as follows: given a set of\ngenerators of an ideal, decide whether that set of generators is a border basis\nof the ideal with respect to some order ideal. The motivation for this problem\ncomes from a similar problem related to Gr\\\"obner bases termed as Gr\\\"obner\nbasis detection (GBD) which was proposed by Gritzmann and Sturmfels (1993). GBD\nwas shown to be NP-hard by Sturmfels and Wiegelmann (1996). In this paper, we\ninvestigate the computational complexity of BBD and show that it is\nNP-complete. \n\n"}
{"id": "1011.6397", "contents": "Title: Almost Optimal Explicit Johnson-Lindenstrauss Transformations Abstract: The Johnson-Lindenstrauss lemma is a fundamental result in probability with\nseveral applications in the design and analysis of algorithms in high\ndimensional geometry. Most known constructions of linear embeddings that\nsatisfy the Johnson-Lindenstrauss property involve randomness. We address the\nquestion of explicitly constructing such embedding families and provide a\nconstruction with an almost optimal use of randomness: we use\nO(log(n/delta)log(log(n/delta)/epsilon)) random bits for embedding n dimensions\nto O(log(1/delta)/epsilon^2) dimensions with error probability at most delta,\nand distortion at most epsilon.\n  In particular, for delta = 1/poly(n) and fixed epsilon, we use O(log n loglog\nn) random bits. Previous constructions required at least O(log^2 n) random bits\nto get polynomially small error. \n\n"}
{"id": "1012.0729", "contents": "Title: Agnostic Learning of Monomials by Halfspaces is Hard Abstract: We prove the following strong hardness result for learning: Given a\ndistribution of labeled examples from the hypercube such that there exists a\nmonomial consistent with $(1-\\eps)$ of the examples, it is NP-hard to find a\nhalfspace that is correct on $(1/2+\\eps)$ of the examples, for arbitrary\nconstants $\\eps > 0$. In learning theory terms, weak agnostic learning of\nmonomials is hard, even if one is allowed to output a hypothesis from the much\nbigger concept class of halfspaces. This hardness result subsumes a long line\nof previous results, including two recent hardness results for the proper\nlearning of monomials and halfspaces. As an immediate corollary of our result\nwe show that weak agnostic learning of decision lists is NP-hard.\n  Our techniques are quite different from previous hardness proofs for\nlearning. We define distributions on positive and negative examples for\nmonomials whose first few moments match. We use the invariance principle to\nargue that regular halfspaces (all of whose coefficients have small absolute\nvalue relative to the total $\\ell_2$ norm) cannot distinguish between\ndistributions whose first few moments match. For highly non-regular subspaces,\nwe use a structural lemma from recent work on fooling halfspaces to argue that\nthey are ``junta-like'' and one can zero out all but the top few coefficients\nwithout affecting the performance of the halfspace. The top few coefficients\nform the natural list decoding of a halfspace in the context of dictatorship\ntests/Label Cover reductions.\n  We note that unlike previous invariance principle based proofs which are only\nknown to give Unique-Games hardness, we are able to reduce from a version of\nLabel Cover problem that is known to be NP-hard. This has inspired follow-up\nwork on bypassing the Unique Games conjecture in some optimal geometric\ninapproximability results. \n\n"}
{"id": "1012.1886", "contents": "Title: Sublinear Time, Measurement-Optimal, Sparse Recovery For All Abstract: An approximate sparse recovery system in ell_1 norm formally consists of\nparameters N, k, epsilon an m-by-N measurement matrix, Phi, and a decoding\nalgorithm, D. Given a vector, x, where x_k denotes the optimal k-term\napproximation to x, the system approximates x by hat_x = D(Phi.x), which must\nsatisfy\n  ||hat_x - x||_1 <= (1+epsilon)||x - x_k||_1.\n  Among the goals in designing such systems are minimizing m and the runtime of\nD. We consider the \"forall\" model, in which a single matrix Phi is used for all\nsignals x.\n  All previous algorithms that use the optimal number m=O(k log(N/k)) of\nmeasurements require superlinear time Omega(N log(N/k)). In this paper, we give\nthe first algorithm for this problem that uses the optimum number of\nmeasurements (up to a constant factor) and runs in sublinear time o(N) when\nk=o(N), assuming access to a data structure requiring space and preprocessing\nO(N). \n\n"}
{"id": "1012.3018", "contents": "Title: On the size of data structures used in symbolic model checking Abstract: Temporal Logic Model Checking is a verification method in which we describe a\nsystem, the model, and then we verify whether some properties, expressed in a\ntemporal logic formula, hold in the system. It has many industrial\napplications. In order to improve performance, some tools allow preprocessing\nof the model, verifying on-line a set of properties reusing the same compiled\nmodel; we prove that the complexity of the Model Checking problem, without any\npreprocessing or preprocessing the model or the formula in a polynomial data\nstructure, is the same. As a result preprocessing does not always exponentially\nimprove performance.\n  Symbolic Model Checking algorithms work by manipulating sets of states, and\nthese sets are often represented by BDDs. It has been observed that the size of\nBDDs may grow exponentially as the model and formula increase in size. As a\nside result, we formally prove that a superpolynomial increase of the size of\nthese BDDs is unavoidable in the worst case. While this exponential growth has\nbeen empirically observed, to the best of our knowledge it has never been\nproved so far in general terms. This result not only holds for all types of\nBDDs regardless of the variable ordering, but also for more powerful data\nstructures, such as BEDs, RBCs, MTBDDs, and ADDs. \n\n"}
{"id": "1012.4404", "contents": "Title: Multicolored Dynamos on Toroidal Meshes Abstract: Detecting on a graph the presence of the minimum number of nodes (target set)\nthat will be able to \"activate\" a prescribed number of vertices in the graph is\ncalled the target set selection problem (TSS) proposed by Kempe, Kleinberg, and\nTardos. In TSS's settings, nodes have two possible states (active or\nnon-active) and the threshold triggering the activation of a node is given by\nthe number of its active neighbors. Dealing with fault tolerance in a majority\nbased system the two possible states are used to denote faulty or non-faulty\nnodes, and the threshold is given by the state of the majority of neighbors.\nHere, the major effort was in determining the distribution of initial faults\nleading the entire system to a faulty behavior. Such an activation pattern,\nalso known as dynamic monopoly (or shortly dynamo), was introduced by Peleg in\n1996. In this paper we extend the TSS problem's settings by representing nodes'\nstates with a \"multicolored\" set. The extended version of the problem can be\ndescribed as follows: let G be a simple connected graph where every node is\nassigned a color from a finite ordered set C = {1, . . ., k} of colors. At each\nlocal time step, each node can recolor itself, depending on the local\nconfigurations, with the color held by the majority of its neighbors. Given G,\nwe study the initial distributions of colors leading the system to a k\nmonochromatic configuration in toroidal meshes, focusing on the minimum number\nof initial k-colored nodes. We find upper and lower bounds to the size of a\ndynamo, and then special classes of dynamos, outlined by means of a new\napproach based on recoloring patterns, are characterized. \n\n"}
{"id": "1101.2170", "contents": "Title: The Complexity of Finding Multiple Solutions to Betweenness and Quartet\n  Compatibility Abstract: We show that two important problems that have applications in computational\nbiology are ASP-complete, which implies that, given a solution to a problem, it\nis NP-complete to decide if another solution exists. We show first that a\nvariation of Betweenness, which is the underlying problem of questions related\nto radiation hybrid mapping, is ASP-complete. Subsequently, we use that result\nto show that Quartet Compatibility, a fundamental problem in phylogenetics that\nasks whether a set of quartets can be represented by a parent tree, is also\nASP-complete. The latter result shows that Steel's \\sc Quartet Challenge, which\nasks whether a solution to Quartet Compatibility is unique, is coNP-complete. \n\n"}
{"id": "1101.2245", "contents": "Title: Invertible Bloom Lookup Tables Abstract: We present a version of the Bloom filter data structure that supports not\nonly the insertion, deletion, and lookup of key-value pairs, but also allows a\ncomplete listing of its contents with high probability, as long the number of\nkey-value pairs is below a designed threshold. Our structure allows the number\nof key-value pairs to greatly exceed this threshold during normal operation.\nExceeding the threshold simply temporarily prevents content listing and reduces\nthe probability of a successful lookup. If later entries are deleted to return\nthe structure below the threshold, everything again functions appropriately. We\nalso show that simple variations of our structure are robust to certain\nstandard errors, such as the deletion of a key without a corresponding\ninsertion or the insertion of two distinct values for a key. The properties of\nour structure make it suitable for several applications, including database and\nnetworking applications that we highlight. \n\n"}
{"id": "1101.2705", "contents": "Title: Lower bound for deterministic semantic-incremental branching programs\n  solving GEN Abstract: We answer a problem posed in (G\\'al, Kouck\\'y, McKenzie 2008) regarding a\nrestricted model of small-space computation, tailored for solving the GEN\nproblem. They define two variants of \"incremental branching programs\", the\nsyntactic variant defined by a restriction on the graph-theoretic paths in the\nprogram, and the more-general semantic variant in which the same restriction is\nenforced only on the consistent paths - those that are followed by at least one\ninput. They show that exponential size is required for the syntactic variant,\nbut leave open the problem of superpolynomial lower bounds for the semantic\nvariant. Here we give an exponential lower bound for the semantic variant by\ngeneralizing lower bound arguments, from earlier work, for a similar restricted\nmodel tailored for solving a special case of GEN called Tree Evaluation. \n\n"}
{"id": "1101.2973", "contents": "Title: Maximizing Non-monotone Submodular Set Functions Subject to Different\n  Constraints: Combined Algorithms Abstract: We study the problem of maximizing constrained non-monotone submodular\nfunctions and provide approximation algorithms that improve existing algorithms\nin terms of either the approximation factor or simplicity. Our algorithms\ncombine existing local search and greedy based algorithms. Different\nconstraints that we study are exact cardinality and multiple knapsack\nconstraints. For the multiple-knapsack constraints we achieve a\n$(0.25-2\\epsilon)$-factor algorithm.\n  We also show, as our main contribution, how to use the continuous greedy\nprocess for non-monotone functions and, as a result, obtain a $0.13$-factor\napproximation algorithm for maximization over any solvable down-monotone\npolytope. The continuous greedy process has been previously used for maximizing\nsmooth monotone submodular function over a down-monotone polytope\n\\cite{CCPV08}. This implies a 0.13-approximation for several discrete problems,\nsuch as maximizing a non-negative submodular function subject to a matroid\nconstraint and/or multiple knapsack constraints. \n\n"}
{"id": "1101.3182", "contents": "Title: Multi-Stage Improved Route Planning Approach: theoretical foundations Abstract: A new approach to the static route planning problem, based on a multi-staging\nconcept and a \\emph{scope} notion, is presented. The main goal (besides implied\nefficiency of planning) of our approach is to address---with a solid\ntheoretical foundation---the following two practically motivated aspects: a\n\\emph{route comfort} and a very \\emph{limited storage} space of a small\nnavigation device, which both do not seem to be among the chief objectives of\nmany other studies. We show how our novel idea can tackle both these seemingly\nunrelated aspects at once, and may also contribute to other established route\nplanning approaches with which ours can be naturally combined. We provide a\ntheoretical proof that our approach efficiently computes exact optimal routes\nwithin this concept, as well as we demonstrate with experimental results on\npublicly available road networks of the US the good practical performance of\nthe solution. \n\n"}
{"id": "1101.3682", "contents": "Title: Diversification improves interpolation Abstract: We consider the problem of interpolating an unknown multivariate polynomial\nwith coefficients taken from a finite field or as numerical approximations of\ncomplex numbers. Building on the recent work of Garg and Schost, we improve on\nthe best-known algorithm for interpolation over large finite fields by\npresenting a Las Vegas randomized algorithm that uses fewer black box\nevaluations. Using related techniques, we also address numerical interpolation\nof sparse polynomials with complex coefficients, and provide the first provably\nstable algorithm (in the sense of relative error) for this problem, at the cost\nof modestly more evaluations. A key new technique is a randomization which\nmakes all coefficients of the unknown polynomial distinguishable, producing\nwhat we call a diverse polynomial. Another departure from most previous\napproaches is that our algorithms do not rely on root finding as a subroutine.\nWe show how these improvements affect the practical performance with trial\nimplementations. \n\n"}
{"id": "1102.0666", "contents": "Title: Probabilistic and quantum finite automata with postselection Abstract: We prove that endowing a real-time probabilistic or quantum computer with the\nability of postselection increases its computational power. For this purpose,\nwe provide a new model of finite automata with postselection, and compare it\nwith the model of L\\={a}ce et al. We examine the related language classes, and\nalso establish separations between the classical and quantum versions, and\nbetween the zero-error vs. bounded-error modes of recognition in this model. \n\n"}
{"id": "1102.3635", "contents": "Title: Rapid mixing of subset Glauber dynamics on graphs of bounded tree-width Abstract: Motivated by the `subgraphs world' view of the ferromagnetic Ising model, we\ndevelop a general approach to studying mixing times of Glauber dynamics based\non subset expansion expressions for a class of graph polynomials. With a\ncanonical paths argument, we demonstrate that the chains defined within this\nframework mix rapidly upon graphs of bounded tree-width. This extends known\nresults on rapid mixing for the Tutte polynomial, the adjacency-rank\n($R_2$-)polynomial and the interlace polynomial. \n\n"}
{"id": "1102.3766", "contents": "Title: Derandomizing HSSW Algorithm for 3-SAT Abstract: We present a (full) derandomization of HSSW algorithm for 3-SAT, proposed by\nHofmeister, Sch\\\"oning, Schuler, and Watanabe in [STACS'02]. Thereby, we obtain\nan O(1.3303^n)-time deterministic algorithm for 3-SAT, which is currently\nfastest. \n\n"}
{"id": "1102.5538", "contents": "Title: Pseudo-random graphs and bit probe schemes with one-sided error Abstract: We study probabilistic bit-probe schemes for the membership problem. Given a\nset A of at most n elements from the universe of size m we organize such a\nstructure that queries of type \"Is x in A?\" can be answered very quickly.\nH.Buhrman, P.B.Miltersen, J.Radhakrishnan, and S.Venkatesh proposed a bit-probe\nscheme based on expanders. Their scheme needs space of $O(n\\log m)$ bits, and\nrequires to read only one randomly chosen bit from the memory to answer a\nquery. The answer is correct with high probability with two-sided errors. In\nthis paper we show that for the same problem there exists a bit-probe scheme\nwith one-sided error that needs space of $O(n\\log^2 m+\\poly(\\log m))$ bits. The\ndifference with the model of Buhrman, Miltersen, Radhakrishnan, and Venkatesh\nis that we consider a bit-probe scheme with an auxiliary word. This means that\nin our scheme the memory is split into two parts of different size: the main\nstorage of $O(n\\log^2 m)$ bits and a short word of $\\log^{O(1)}m$ bits that is\npre-computed once for the stored set A and `cached'. To answer a query \"Is x in\nA?\" we allow to read the whole cached word and only one bit from the main\nstorage. For some reasonable values of parameters our space bound is better\nthan what can be achieved by any scheme without cached data. \n\n"}
{"id": "1103.0040", "contents": "Title: From Convex Optimization to Randomized Mechanisms: Toward Optimal\n  Combinatorial Auctions Abstract: We design an expected polynomial-time, truthful-in-expectation,\n(1-1/e)-approximation mechanism for welfare maximization in a fundamental class\nof combinatorial auctions. Our results apply to bidders with valuations that\nare m matroid rank sums (MRS), which encompass most concrete examples of\nsubmodular functions studied in this context, including coverage functions,\nmatroid weighted-rank functions, and convex combinations thereof. Our\napproximation factor is the best possible, even for known and explicitly given\ncoverage valuations, assuming P != NP. Ours is the first\ntruthful-in-expectation and polynomial-time mechanism to achieve a\nconstant-factor approximation for an NP-hard welfare maximization problem in\ncombinatorial auctions with heterogeneous goods and restricted valuations.\n  Our mechanism is an instantiation of a new framework for designing\napproximation mechanisms based on randomized rounding algorithms. A typical\nsuch algorithm first optimizes over a fractional relaxation of the original\nproblem, and then randomly rounds the fractional solution to an integral one.\nWith rare exceptions, such algorithms cannot be converted into truthful\nmechanisms. The high-level idea of our mechanism design framework is to\noptimize directly over the (random) output of the rounding algorithm, rather\nthan over the input to the rounding algorithm. This approach leads to\ntruthful-in-expectation mechanisms, and these mechanisms can be implemented\nefficiently when the corresponding objective function is concave. For bidders\nwith MRS valuations, we give a novel randomized rounding algorithm that leads\nto both a concave objective function and a (1-1/e)-approximation of the optimal\nwelfare. \n\n"}
{"id": "1103.0534", "contents": "Title: Solving connectivity problems parameterized by treewidth in single\n  exponential time Abstract: For the vast majority of local graph problems standard dynamic programming\ntechniques give c^tw V^O(1) algorithms, where tw is the treewidth of the input\ngraph. On the other hand, for problems with a global requirement (usually\nconnectivity) the best-known algorithms were naive dynamic programming schemes\nrunning in tw^O(tw) V^O(1) time.\n  We breach this gap by introducing a technique we dubbed Cut&Count that allows\nto produce c^tw V^O(1) Monte Carlo algorithms for most connectivity-type\nproblems, including Hamiltonian Path, Feedback Vertex Set and Connected\nDominating Set, consequently answering the question raised by Lokshtanov, Marx\nand Saurabh [SODA'11] in a surprising way. We also show that (under reasonable\ncomplexity assumptions) the gap cannot be breached for some problems for which\nCut&Count does not work, like CYCLE PACKING.\n  The constant c we obtain is in all cases small (at most 4 for undirected\nproblems and at most 6 for directed ones), and in several cases we are able to\nshow that improving those constants would cause the Strong Exponential Time\nHypothesis to fail.\n  Our results have numerous consequences in various fields, like FPT\nalgorithms, exact and approximate algorithms on planar and H-minor-free graphs\nand algorithms on graphs of bounded degree. In all these fields we are able to\nimprove the best-known results for some problems. \n\n"}
{"id": "1103.0853", "contents": "Title: Generalized Satisfiability for the Description Logic ALC Abstract: The standard reasoning problem, concept satisfiability, in the basic\ndescription logic ALC is PSPACE-complete, and it is EXPTIME-complete in the\npresence of unrestricted axioms. Several fragments of ALC, notably logics in\nthe FL, EL, and DL-Lite families, have an easier satisfiability problem;\nsometimes it is even tractable. We classify the complexity of the standard\nsatisfiability problems for all possible Boolean and quantifier fragments of\nALC in the presence of general axioms. \n\n"}
{"id": "1103.3656", "contents": "Title: Classical and Quantum Annealing in the Median of Three Satisfiability Abstract: We determine the classical and quantum complexities of a specific ensemble of\nthree-satisfiability problems with a unique satisfying assignment for up to\nN=100 and N=80 variables, respectively. In the classical limit we employ\ngeneralized ensemble techniques and measure the time that a Markovian Monte\nCarlo process spends in searching classical ground states. In the quantum limit\nwe determine the maximum finite correlation length along a quantum adiabatic\ntrajectory determined by the linear sweep of the adiabatic control parameter in\nthe Hamiltonian composed of the problem Hamiltonian and the constant transverse\nfield Hamiltonian. In the median of our ensemble both complexities diverge\nexponentially with the number of variables. Hence, standard, conventional\nadiabatic quantum computation fails to reduce the computational complexity to\npolynomial. Moreover, the growth-rate constant in the quantum limit is 3.8\ntimes as large as the one in the classical limit, making classical fluctuations\nmore beneficial than quantum fluctuations in ground-state searches. \n\n"}
{"id": "1103.6161", "contents": "Title: The Grothendieck constant is strictly smaller than Krivine's bound Abstract: We prove that $K_G<\\frac{\\pi}{2\\log(1+\\sqrt{2})}$, where $K_G$ is the\nGrothendieck constant. \n\n"}
{"id": "1104.0882", "contents": "Title: Xheal: Localized Self-healing using Expanders Abstract: We consider the problem of self-healing in reconfigurable networks (e.g.\npeer-to-peer and wireless mesh networks) that are under repeated attack by an\nomniscient adversary and propose a fully distributed algorithm, Xheal that\nmaintains good expansion and spectral properties of the network, also keeping\nthe network connected. Moreover, Xheal does this while allowing only low\nstretch and degree increase per node. Thus, the algorithm heals global\nproperties while only doing local changes and using only local information.\n  Our work improves over the self-healing algorithms 'Forgiving tree'[PODC\n2008] and 'Forgiving graph'[PODC 2009] (using a similar model) in that we are\nable to give guarantees on degree and stretch, while at the same time\npreserving the expansion and spectral properties of the network. These repairs\npreserve the invariants in the following sense. At any point in the algorithm,\nthe expansion of the graph will be either `better' than the expansion of the\ngraph formed by considering only the adversarial insertions (not the\nadversarial deletions) or the expansion will be, at least, a constant. Also,\nthe stretch i.e. the distance between any pair of nodes in the healed graph is\nno more than a $O(\\log n)$ factor. Similarly, at any point, a node $v$ whose\ndegree would have been $d$ in the graph with adversarial insertions only, will\nhave degree at most $O(\\kappa d)$ in the actual graph, for a small parameter\n$\\kappa$. We also provide bounds on the second smallest eigenvalue of the\nLaplacian which captures key properties such as mixing time, conductance,\ncongestion in routing etc. Our distributed data structure has low amortized\nlatency and bandwidth requirements. \n\n"}
{"id": "1104.2074", "contents": "Title: New Hardness Results in Rainbow Connectivity Abstract: A path in an edge colored graph is said to be a rainbow path if no two edges\non the path have the same color. An edge colored graph is (strongly) rainbow\nconnected if there exists a (geodesic) rainbow path between every pair of\nvertices. The (strong) rainbow connectivity of a graph $G$, denoted by\n($src(G)$, respectively) $rc(G)$ is the smallest number of colors required to\nedge color the graph such that the graph is (strong) rainbow connected. It is\nknown that for \\emph{even} $k$ to decide whether the rainbow connectivity of a\ngraph is at most $k$ or not is NP-hard. It was conjectured that for all $k$, to\ndecide whether $rc(G) \\leq k$ is NP-hard. In this paper we prove this\nconjecture. We also show that it is NP-hard to decide whether $src(G) \\leq k$\nor not even when $G$ is a bipartite graph. \n\n"}
{"id": "1104.2809", "contents": "Title: Self-Assembly with Geometric Tiles Abstract: In this work we propose a generalization of Winfree's abstract Tile Assembly\nModel (aTAM) in which tile types are assigned rigid shapes, or geometries,\nalong each tile face. We examine the number of distinct tile types needed to\nassemble shapes within this model, the temperature required for efficient\nassembly, and the problem of designing compact geometric faces to meet given\ncompatibility specifications. Our results show a dramatic decrease in the\nnumber of tile types needed to assemble $n \\times n$ squares to\n$\\Theta(\\sqrt{\\log n})$ at temperature 1 for the most simple model which meets\na lower bound from Kolmogorov complexity, and $O(\\log\\log n)$ in a model in\nwhich tile aggregates must move together through obstacle free paths within the\nplane. This stands in contrast to the $\\Theta(\\log n / \\log\\log n)$ tile types\nat temperature 2 needed in the basic aTAM. We also provide a general method for\nsimulating a large and computationally universal class of temperature 2 aTAM\nsystems with geometric tiles at temperature 1. Finally, we consider the problem\nof computing a set of compact geometric faces for a tile system to implement a\ngiven set of compatibility specifications. We show a number of bounds on the\ncomplexity of geometry size needed for various classes of compatibility\nspecifications, many of which we directly apply to our tile assembly results to\nachieve non-trivial reductions in geometry size. \n\n"}
{"id": "1104.4618", "contents": "Title: Minimum cell connection and separation in line segment arrangements Abstract: We study the complexity of the following cell connection and separation\nproblems in segment arrangements. Given a set of straight-line segments in the\nplane and two points $a$ and $b$ in different cells of the induced arrangement:\n  (i) compute the minimum number of segments one needs to remove so that there\nis a path connecting $a$ to $b$ that does not intersect any of the remaining\nsegments; (ii) compute the minimum number of segments one needs to remove so\nthat the arrangement induced by the remaining segments has a single cell; (iii)\ncompute the minimum number of segments one needs to retain so that any path\nconnecting $a$ to $b$ intersects some of the retained segments.\n  We show that problems (i) and (ii) are NP-hard and discuss some special,\ntractable cases. Most notably, we provide a linear-time algorithm for a variant\nof problem (i) where the path connecting $a$ to $b$ must stay inside a given\npolygon $P$ with a constant number of holes, the segments are contained in $P$,\nand the endpoints of the segments are on the boundary of $P$. For problem (iii)\nwe provide a cubic-time algorithm. \n\n"}
{"id": "1104.4746", "contents": "Title: Lasserre Hierarchy, Higher Eigenvalues, and Approximation Schemes for\n  Quadratic Integer Programming with PSD Objectives Abstract: We present an approximation scheme for optimizing certain Quadratic Integer\nProgramming problems with positive semidefinite objective functions and global\nlinear constraints. This framework includes well known graph problems such as\nMinimum graph bisection, Edge expansion, Uniform sparsest cut, and Small Set\nexpansion, as well as the Unique Games problem. These problems are notorious\nfor the existence of huge gaps between the known algorithmic results and\nNP-hardness results. Our algorithm is based on rounding semidefinite programs\nfrom the Lasserre hierarchy, and the analysis uses bounds for low-rank\napproximations of a matrix in Frobenius norm using columns of the matrix.\n  For all the above graph problems, we give an algorithm running in time\n$n^{O(r/\\epsilon^2)}$ with approximation ratio\n$\\frac{1+\\epsilon}{\\min\\{1,\\lambda_r\\}}$, where $\\lambda_r$ is the $r$'th\nsmallest eigenvalue of the normalized graph Laplacian $\\mathcal{L}$. In the\ncase of graph bisection and small set expansion, the number of vertices in the\ncut is within lower-order terms of the stipulated bound. Our results imply\n$(1+O(\\epsilon))$ factor approximation in time $n^{O(r^\\ast/\\epsilon^2)}$ where\n$r^\\ast$ is the number of eigenvalues of $\\mathcal{L}$ smaller than\n$1-\\epsilon$.\n  For Unique Games, we give a factor $(1+\\frac{2+\\epsilon}{\\lambda_r})$\napproximation for minimizing the number of unsatisfied constraints in\n$n^{O(r/\\epsilon)}$ time. This improves an earlier bound for solving Unique\nGames on expanders, and also shows that Lasserre SDPs are powerful enough to\nsolve well-known integrality gap instances for the basic SDP.\n  We also give an algorithm for independent sets in graphs that performs well\nwhen the Laplacian does not have too many eigenvalues bigger than $1+o(1)$. \n\n"}
{"id": "1104.5168", "contents": "Title: Neighborliness of the symmetric moment curve Abstract: We consider the convex hull B_k of the symmetric moment curve U(t)=(cos t,\nsin t, cos 3t, sin 3t, ..., cos (2k-1)t, sin (2k-1)t) in R^{2k}, where t ranges\nover the unit circle S= R/2pi Z. The curve U(t) is locally neighborly: as long\nas t_1, ..., t_k lie in an open arc of S of a certain length phi_k>0, the\nconvex hull of the points U(t_1), ..., U(t_k) is a face of B_k. We characterize\nthe maximum possible length phi_k, proving, in particular, that phi_k > pi/2\nfor all k and that the limit of phi_k is pi/2 as k grows. This allows us to\nconstruct centrally symmetric polytopes with a record number of faces. \n\n"}
{"id": "1104.5226", "contents": "Title: Parallelism and Time in Hierarchical Self-Assembly Abstract: We study the role that parallelism plays in time complexity of Winfree's\nabstract Tile Assembly Model (aTAM), a model of molecular algorithmic\nself-assembly. In the \"hierarchical\" aTAM, two assemblies, both consisting of\nmultiple tiles, are allowed to aggregate together, whereas in the \"seeded\"\naTAM, tiles attach one at a time to a growing assembly. Adleman, Cheng, Goel,\nand Huang (\"Running Time and Program Size for Self-Assembled Squares\", STOC\n2001) showed how to assemble an n x n square in O(n) time in the seeded aTAM\nusing O(log n / log log n) unique tile types, where both of these parameters\nare optimal. They asked whether the hierarchical aTAM could allow a tile system\nto use the ability to form large assemblies in parallel before they attach to\nbreak the Omega(n) lower bound for assembly time. We show that there is a tile\nsystem with the optimal O(log n / log log n) tile types that assembles an n x n\nsquare using O(log^2 n) parallel \"stages\", which is close to the optimal\nOmega(log n) stages, forming the final n x n square from four n/2 x n/2\nsquares, which are themselves recursively formed from n/4 x n/4 squares, etc.\nHowever, despite this nearly maximal parallelism, the system requires\nsuperlinear time to assemble the square. We extend the definition of *partial\norder tile systems* studied by Adleman et al. in a natural way to hierarchical\nassembly and show that no hierarchical partial order tile system can build any\nshape with diameter N in less than time Omega(N), demonstrating that in this\ncase the hierarchical model affords no speedup whatsoever over the seeded\nmodel. We strengthen the Omega(N) time lower bound for deterministic seeded\nsystems of Adleman et al. to nondeterministic seeded systems. Finally, we show\nthat for infinitely many n, a tile system can assemble an n x n' rectangle,\nwith n > n', in time O(n^{4/5} log n), breaking the linear-time lower bound. \n\n"}
{"id": "1106.0449", "contents": "Title: Centrally symmetric polytopes with many faces Abstract: We present explicit constructions of centrally symmetric polytopes with many\nfaces: first, we construct a d-dimensional centrally symmetric polytope P with\nabout (1.316)^d vertices such that every pair of non-antipodal vertices of P\nspans an edge of P, second, for an integer k>1, we construct a d-dimensional\ncentrally symmetric polytope P of an arbitrarily high dimension d and with an\narbitrarily large number N of vertices such that for some 0 < delta_k < 1 at\nleast (1-delta_k^d) {N choose k} k-subsets of the set of vertices span faces of\nP, and third, for an integer k>1 and a>0, we construct a centrally symmetric\npolytope Q with an arbitrary large number N of vertices and of dimension\nd=k^{1+o(1)} such that least (1 - k^{-a}){N choose k} k-subsets of the set of\nvertices span faces of Q. \n\n"}
{"id": "1106.2122", "contents": "Title: Parameterized complexity results for 1-safe Petri nets Abstract: We associate a graph with a 1-safe Petri net and study the parameterized\ncomplexity of various problems with parameters derived from the graph. With\ntreewidth as the parameter, we give W[1]-hardness results for many problems\nabout 1-safe Petri nets. As a corollary, this proves a conjecture of Downey et.\nal. about the hardness of some graph pebbling problems. We consider the\nparameter benefit depth (that is known to be helpful in getting better\nalgorithms for general Petri nets) and again give W[1]-hardness results for\nvarious problems on 1-safe Petri nets. We also consider the stronger parameter\nvertex cover number. Combining the well known automata-theoretic method and a\npowerful fixed parameter tractability (FPT) result about Integer Linear\nProgramming, we give a FPT algorithm for model checking Monadic Second Order\n(MSO) formulas on 1-safe Petri nets, with parameters vertex cover number and\nthe size of the formula. \n\n"}
{"id": "1106.4141", "contents": "Title: Kernel Bounds for Path and Cycle Problems Abstract: Connectivity problems like k-Path and k-Disjoint Paths relate to many\nimportant milestones in parameterized complexity, namely the Graph Minors\nProject, color coding, and the recent development of techniques for obtaining\nkernelization lower bounds. This work explores the existence of polynomial\nkernels for various path and cycle problems, by considering nonstandard\nparameterizations. We show polynomial kernels when the parameters are a given\nvertex cover, a modulator to a cluster graph, or a (promised) max leaf number.\nWe obtain lower bounds via cross-composition, e.g., for Hamiltonian Cycle and\nrelated problems when parameterized by a modulator to an outerplanar graph. \n\n"}
{"id": "1106.6336", "contents": "Title: External-Memory Network Analysis Algorithms for Naturally Sparse Graphs Abstract: In this paper, we present a number of network-analysis algorithms in the\nexternal-memory model. We focus on methods for large naturally sparse graphs,\nthat is, n-vertex graphs that have O(n) edges and are structured so that this\nsparsity property holds for any subgraph of such a graph. We give efficient\nexternal-memory algorithms for the following problems for such graphs: -\nFinding an approximate d-degeneracy ordering; - Finding a cycle of length\nexactly c; - Enumerating all maximal cliques. Such problems are of interest,\nfor example, in the analysis of social networks, where they are used to study\nnetwork cohesion. \n\n"}
{"id": "1107.0789", "contents": "Title: Distributed Matrix Completion and Robust Factorization Abstract: If learning methods are to scale to the massive sizes of modern datasets, it\nis essential for the field of machine learning to embrace parallel and\ndistributed computing. Inspired by the recent development of matrix\nfactorization methods with rich theory but poor computational complexity and by\nthe relative ease of mapping matrices onto distributed architectures, we\nintroduce a scalable divide-and-conquer framework for noisy matrix\nfactorization. We present a thorough theoretical analysis of this framework in\nwhich we characterize the statistical errors introduced by the \"divide\" step\nand control their magnitude in the \"conquer\" step, so that the overall\nalgorithm enjoys high-probability estimation guarantees comparable to those of\nits base algorithm. We also present experiments in collaborative filtering and\nvideo background modeling that demonstrate the near-linear to superlinear\nspeed-ups attainable with this approach. \n\n"}
{"id": "1107.2722", "contents": "Title: On the Feasibility of Maintenance Algorithms in Dynamic Graphs Abstract: Near ubiquitous mobile computing has led to intense interest in dynamic graph\ntheory. This provides a new and challenging setting for algorithmics and\ncomplexity theory. For any graph-based problem, the rapid evolution of a\n(possibly disconnected) graph over time naturally leads to the important\ncomplexity question: is it better to calculate a new solution from scratch or\nto adapt the known solution on the prior graph to quickly provide a solution of\nguaranteed quality for the changed graph?\n  In this paper, we demonstrate that the former is the best approach in some\ncases, but that there are cases where the latter is feasible. We prove that,\nunder certain conditions, hard problems cannot even be approximated in any\nreasonable complexity bound --- i.e., even with a large amount of time, having\na solution to a very similar graph does not help in computing a solution to the\ncurrent graph. To achieve this, we formalize the idea as a maintenance\nalgorithm. Using r-Regular Subgraph as the primary example we show that\nW[1]-hardness for the parameterized approximation problem implies the\nnon-existence of a maintenance algorithm for the given approximation ratio.\nConversely we show that Vertex Cover, which is fixed-parameter tractable, has a\n2-approximate maintenance algorithm. The implications of NP-hardness and\nNPO-hardness are also explored. \n\n"}
{"id": "1107.5789", "contents": "Title: Collapsibility of CAT(0) spaces Abstract: Collapsibility is a combinatorial strengthening of contractibility. We relate\nthis property to metric geometry by proving the collapsibility of any complex\nthat is CAT(0) with a metric for which all vertex stars are convex. This\nstrengthens and generalizes a result by Crowley. Further consequences of our\nwork are:\n  (1) All CAT(0) cube complexes are collapsible.\n  (2) Any triangulated manifold admits a CAT(0) metric if and only if it admits\ncollapsible triangulations.\n  (3) All contractible d-manifolds ($d \\ne 4$) admit collapsible CAT(0)\ntriangulations. This discretizes a classical result by Ancel--Guilbault. \n\n"}
{"id": "1108.0065", "contents": "Title: Approximating the Permanent with Fractional Belief Propagation Abstract: We discuss schemes for exact and approximate computations of permanents, and\ncompare them with each other. Specifically, we analyze the Belief Propagation\n(BP) approach and its Fractional Belief Propagation (FBP) generalization for\ncomputing the permanent of a non-negative matrix. Known bounds and conjectures\nare verified in experiments, and some new theoretical relations, bounds and\nconjectures are proposed. The Fractional Free Energy (FFE) functional is\nparameterized by a scalar parameter $\\gamma\\in[-1;1]$, where $\\gamma=-1$\ncorresponds to the BP limit and $\\gamma=1$ corresponds to the exclusion\nprinciple (but ignoring perfect matching constraints) Mean-Field (MF) limit.\nFFE shows monotonicity and continuity with respect to $\\gamma$. For every\nnon-negative matrix, we define its special value $\\gamma_*\\in[-1;0]$ to be the\n$\\gamma$ for which the minimum of the $\\gamma$-parameterized FFE functional is\nequal to the permanent of the matrix, where the lower and upper bounds of the\n$\\gamma$-interval corresponds to respective bounds for the permanent. Our\nexperimental analysis suggests that the distribution of $\\gamma_*$ varies for\ndifferent ensembles but $\\gamma_*$ always lies within the $[-1;-1/2]$ interval.\nMoreover, for all ensembles considered the behavior of $\\gamma_*$ is highly\ndistinctive, offering an emprirical practical guidance for estimating\npermanents of non-negative matrices via the FFE approach. \n\n"}
{"id": "1109.1674", "contents": "Title: A Linear-Optical Proof that the Permanent is #P-Hard Abstract: One of the crown jewels of complexity theory is Valiant's 1979 theorem that\ncomputing the permanent of an n*n matrix is #P-hard. Here we show that, by\nusing the model of linear-optical quantum computing---and in particular, a\nuniversality theorem due to Knill, Laflamme, and Milburn---one can give a\ndifferent and arguably more intuitive proof of this theorem. \n\n"}
{"id": "1109.5544", "contents": "Title: Many non-equivalent realizations of the associahedron Abstract: Hohlweg and Lange (2007) and Santos (2004, unpublished) have found two\ndifferent ways of constructing exponential families of realizations of the\nn-dimensional associahedron with normal vectors in {0,1,-1}^n, generalizing the\nconstructions of Loday (2004) and Chapoton-Fomin-Zelevinsky (2002). We classify\nthe associahedra obtained by these constructions modulo linear equivalence of\ntheir normal fans and show, in particular, that the only realization that can\nbe obtained with both methods is the Chapoton-Fomin-Zelevinsky (2002)\nassociahedron.\n  For the Hohlweg-Lange associahedra our classification is a priori coarser\nthan the classification up to isometry of normal fans, by\nBergeron-Hohlweg-Lange-Thomas (2009). However, both yield the same classes. As\na consequence, we get that two Hohlweg-Lange associahedra have linearly\nequivalent normal fans if and only if they are isometric.\n  The Santos construction, which produces an even larger family of\nassociahedra, appears here in print for the first time. Apart of describing it\nin detail we relate it with the c-cluster complexes and the denominator fans in\ncluster algebras of type A.\n  A third classical construction of the associahedron, as the secondary\npolytope of a convex n-gon (Gelfand-Kapranov-Zelevinsky, 1990), is shown to\nnever produce a normal fan linearly equivalent to any of the other two\nconstructions. \n\n"}
{"id": "1109.5579", "contents": "Title: Strong convergence of partial match queries in random quadtrees Abstract: We prove that the rescaled costs of partial match queries in a random\ntwo-dimensional quadtree converge almost surely towards a random limit which is\nidentified as the terminal value of a martingale. Our approach shares many\nsimilarities with the theory of self-similar fragmentations. \n\n"}
{"id": "1109.5615", "contents": "Title: A Regularity Measure for Context Free Grammars Abstract: Parikh's theorem states that every Context Free Language (CFL) has the same\nParikh image as that of a regular language. A finite state automaton accepting\nsuch a regular language is called a Parikh-equivalent automaton. In the worst\ncase, the number of states in any non-deterministic Parikh-equivalent automaton\nis exponentially large in the size of the Context Free Grammar (CFG). We\nassociate a regularity width d with a CFG that measures the closeness of the\nCFL with regular languages. The degree m of a CFG is one less than the maximum\nnumber of variable occurrences in the right hand side of any production. Given\na CFG with n variables, we construct a Parikh-equivalent non-deterministic\nautomaton whose number of states is upper bounded by a polynomial in $n\n(d^{2d(m+1)}), the degree of the polynomial being a small fixed constant. Our\nprocedure is constructive and runs in time polynomial in the size of the\nautomaton. In the terminology of parameterized complexity, we prove that\nconstructing a Parikh-equivalent automaton for a given CFG is Fixed Parameter\nTractable (FPT) when the degree m and regularity width d are parameters. We\nalso give an example from program verification domain where the degree and\nregularity are small compared to the size of the grammar. \n\n"}
{"id": "1109.5981", "contents": "Title: LSRN: A Parallel Iterative Solver for Strongly Over- or Under-Determined\n  Systems Abstract: We describe a parallel iterative least squares solver named \\texttt{LSRN}\nthat is based on random normal projection. \\texttt{LSRN} computes the\nmin-length solution to $\\min_{x \\in \\mathbb{R}^n} \\|A x - b\\|_2$, where $A \\in\n\\mathbb{R}^{m \\times n}$ with $m \\gg n$ or $m \\ll n$, and where $A$ may be\nrank-deficient. Tikhonov regularization may also be included. Since $A$ is only\ninvolved in matrix-matrix and matrix-vector multiplications, it can be a dense\nor sparse matrix or a linear operator, and \\texttt{LSRN} automatically speeds\nup when $A$ is sparse or a fast linear operator. The preconditioning phase\nconsists of a random normal projection, which is embarrassingly parallel, and a\nsingular value decomposition of size $\\lceil \\gamma \\min(m,n) \\rceil \\times\n\\min(m,n)$, where $\\gamma$ is moderately larger than 1, e.g., $\\gamma = 2$. We\nprove that the preconditioned system is well-conditioned, with a strong\nconcentration result on the extreme singular values, and hence that the number\nof iterations is fully predictable when we apply LSQR or the Chebyshev\nsemi-iterative method. As we demonstrate, the Chebyshev method is particularly\nefficient for solving large problems on clusters with high communication cost.\nNumerical results demonstrate that on a shared-memory machine, \\texttt{LSRN}\noutperforms LAPACK's DGELSD on large dense problems, and MATLAB's backslash\n(SuiteSparseQR) on sparse problems. Further experiments demonstrate that\n\\texttt{LSRN} scales well on an Amazon Elastic Compute Cloud cluster. \n\n"}
{"id": "1109.6178", "contents": "Title: Space-efficient Local Computation Algorithms Abstract: Recently Rubinfeld et al. (ICS 2011, pp. 223--238) proposed a new model of\nsublinear algorithms called \\emph{local computation algorithms}. In this model,\na computation problem $F$ may have more than one legal solution and each of\nthem consists of many bits. The local computation algorithm for $F$ should\nanswer in an online fashion, for any index $i$, the $i^{\\mathrm{th}}$ bit of\nsome legal solution of $F$. Further, all the answers given by the algorithm\nshould be consistent with at least one solution of $F$.\n  In this work, we continue the study of local computation algorithms. In\nparticular, we develop a technique which under certain conditions can be\napplied to construct local computation algorithms that run not only in\npolylogarithmic time but also in polylogarithmic \\emph{space}. Moreover, these\nlocal computation algorithms are easily parallelizable and can answer all\nparallel queries consistently. Our main technical tools are pseudorandom\nnumbers with bounded independence and the theory of branching processes. \n\n"}
{"id": "1110.1715", "contents": "Title: Determining All Universal Tilers Abstract: A universal tiler is a convex polyhedron whose every cross-section tiles the\nplane. In this paper, we introduce a certain slight-rotating operation for\ncross-sections of pentahedra. Based on a selected initial cross-section and by\napplying the slight-rotating operation suitably, we prove that a convex\npolyhedron is a universal tiler if and only if it is a tetrahedron or a\ntriangular prism. \n\n"}
{"id": "1110.4493", "contents": "Title: Improved Grammar-Based Compressed Indexes Abstract: We introduce the first grammar-compressed representation of a sequence that\nsupports searches in time that depends only logarithmically on the size of the\ngrammar. Given a text $T[1..u]$ that is represented by a (context-free) grammar\nof $n$ (terminal and nonterminal) symbols and size $N$ (measured as the sum of\nthe lengths of the right hands of the rules), a basic grammar-based\nrepresentation of $T$ takes $N\\lg n$ bits of space. Our representation requires\n$2N\\lg n + N\\lg u + \\epsilon\\, n\\lg n + o(N\\lg n)$ bits of space, for any\n$0<\\epsilon \\le 1$. It can find the positions of the $occ$ occurrences of a\npattern of length $m$ in $T$ in $O((m^2/\\epsilon)\\lg (\\frac{\\lg u}{\\lg n})\n+occ\\lg n)$ time, and extract any substring of length $\\ell$ of $T$ in time\n$O(\\ell+h\\lg(N/h))$, where $h$ is the height of the grammar tree. \n\n"}
{"id": "1111.1750", "contents": "Title: Near Linear-Work Parallel SDD Solvers, Low-Diameter Decomposition, and\n  Low-Stretch Subgraphs Abstract: We present the design and analysis of a near linear-work parallel algorithm\nfor solving symmetric diagonally dominant (SDD) linear systems. On input of a\nSDD $n$-by-$n$ matrix $A$ with $m$ non-zero entries and a vector $b$, our\nalgorithm computes a vector $\\tilde{x}$ such that $\\norm[A]{\\tilde{x} - A^+b}\n\\leq \\vareps \\cdot \\norm[A]{A^+b}$ in $O(m\\log^{O(1)}{n}\\log{\\frac1\\epsilon})$\nwork and $O(m^{1/3+\\theta}\\log \\frac1\\epsilon)$ depth for any fixed $\\theta >\n0$.\n  The algorithm relies on a parallel algorithm for generating low-stretch\nspanning trees or spanning subgraphs. To this end, we first develop a parallel\ndecomposition algorithm that in polylogarithmic depth and $\\otilde(|E|)$ work,\npartitions a graph into components with polylogarithmic diameter such that only\na small fraction of the original edges are between the components. This can be\nused to generate low-stretch spanning trees with average stretch\n$O(n^{\\alpha})$ in $O(n^{1+\\alpha})$ work and $O(n^{\\alpha})$ depth.\nAlternatively, it can be used to generate spanning subgraphs with\npolylogarithmic average stretch in $\\otilde(|E|)$ work and polylogarithmic\ndepth. We apply this subgraph construction to derive a parallel linear system\nsolver. By using this solver in known applications, our results imply improved\nparallel randomized algorithms for several problems, including single-source\nshortest paths, maximum flow, minimum-cost flow, and approximate maximum flow. \n\n"}
{"id": "1111.2943", "contents": "Title: Crystal Frameworks, Matrix-valued Functions and Rigidity Operators Abstract: An introduction and survey is given of some recent work on the infinitesimal\ndynamics of \\textit{crystal frameworks}, that is, of translationally periodic\ndiscrete bond-node structures in $\\mathbb{R}^d$, for $ d=2,3,...$. We discuss\nthe rigidity matrix, a fundamental object from finite bar-joint framework\ntheory, rigidity operators, matrix-function representations and low energy\nphonons. These phonons in material crystals, such as quartz and zeolites, are\nknown as rigid unit modes, or RUMs, and are associated with the relative\nmotions of rigid units, such as ~SiO$_4$ tetrahedra in the tetrahedral\npolyhedral bond-node model for quartz. We also introduce semi-infinite crystal\nframeworks, bi-crystal frameworks and associated multi-variable Toeplitz\noperators. \n\n"}
{"id": "1111.5442", "contents": "Title: Improved Lower Bounds for the Shortest Superstring and Related Problems Abstract: We study the approximation hardness of the Shortest Superstring, the Maximal\nCompression and the Maximum Asymmetric Traveling Salesperson (MAX-ATSP)\nproblem. We introduce a new reduction method that produces strongly restricted\ninstances of the Shortest Superstring problem, in which the maximal orbit size\nis eight (with no character appearing more than eight times) and all given\nstrings having length four. Based on this reduction method, we are able to\nimprove the best up to now known approximation lower bound for the Shortest\nSuperstring problem and the Maximal Compression problem by an order of\nmagnitude. The results imply also an improved approximation lower bound for the\nMAX-ATSP problem. \n\n"}
{"id": "1111.6616", "contents": "Title: Constraint Satisfaction Tractability from Semi-lattice Operations on\n  Infinite Sets Abstract: A famous result by Jeavons, Cohen, and Gyssens shows that every constraint\nsatisfaction problem (CSP) where the constraints are preserved by a\nsemi-lattice operation can be solved in polynomial time. This is one of the\nbasic facts for the so-called universal-algebraic approach to a systematic\ntheory of tractability and hardness in finite domain constraint satisfaction.\n  Not surprisingly, the theorem of Jeavons et al. fails for arbitrary infinite\ndomain CSPs. Many CSPs of practical interest, though, and in particular those\nCSPs that are motivated by qualitative reasoning calculi from Artificial\nIntelligence, can be formulated with constraint languages that are rather\nwell-behaved from a model-theoretic point of view. In particular, the\nautomorphism group of these constraint languages tends to be large in the sense\nthat the number of orbits of n-subsets of the automorphism group is bounded by\nsome function in n.\n  In this paper we present a generalization of the theorem by Jeavons et al. to\ninfinite domain CSPs where the number of orbits of n-subsets grows\nsub-exponentially in n, and prove that preservation under a semi-lattice\noperation for such CSPs implies polynomial-time tractability. Unlike the result\nof Jeavons et al., this includes many CSPs that cannot be solved by Datalog. \n\n"}
{"id": "1112.1538", "contents": "Title: Jungles, bundles, and fixed parameter tractability Abstract: We give a fixed-parameter tractable (FPT) approximation algorithm computing\nthe path-width of a tournament, and more generally, of a semi-complete digraph.\nBased on this result, we prove that topological containment and rooted\nimmersion problems are FPT on semi-complete digraphs. \n\n"}
{"id": "1112.3337", "contents": "Title: Search by quantum walks on two-dimensional grid without amplitude\n  amplification Abstract: We study search by quantum walk on a finite two dimensional grid. The\nalgorithm of Ambainis, Kempe, Rivosh (quant-ph/0402107) takes O(\\sqrt{N log N})\nsteps and finds a marked location with probability O(1/log N) for grid of size\n\\sqrt{N} * \\sqrt{N}. This probability is small, thus amplitude amplification is\nneeded to achieve \\Theta(1) success probability. The amplitude amplification\nadds an additional O(\\sqrt{log N}) factor to the number of steps, making it\nO(\\sqrt{N} log N).\n  In this paper, we show that despite a small probability to find a marked\nlocation, the probability to be within an O(\\sqrt{N}) neighbourhood (at an\nO(\\sqrt[4]{N}) distance) of the marked location is \\Theta(1). This allows to\nskip amplitude amplification step and leads to an O(\\sqrt{log N}) speed-up.\n  We describe the results of numerical experiments supporting this idea, and we\nprove this fact analytically. \n\n"}
{"id": "1201.0127", "contents": "Title: Faster Subset Selection for Matrices and Applications Abstract: We study subset selection for matrices defined as follows: given a matrix\n$\\matX \\in \\R^{n \\times m}$ ($m > n$) and an oversampling parameter $k$ ($n \\le\nk \\le m$), select a subset of $k$ columns from $\\matX$ such that the\npseudo-inverse of the subsampled matrix has as smallest norm as possible. In\nthis work, we focus on the Frobenius and the spectral matrix norms. We describe\nseveral novel (deterministic and randomized) approximation algorithms for this\nproblem with approximation bounds that are optimal up to constant factors.\nAdditionally, we show that the combinatorial problem of finding a low-stretch\nspanning tree in an undirected graph corresponds to subset selection, and\ndiscuss various implications of this reduction. \n\n"}
{"id": "1201.0330", "contents": "Title: Testing Low Complexity Affine-Invariant Properties Abstract: Invariance with respect to linear or affine transformations of the domain is\narguably the most common symmetry exhibited by natural algebraic properties. In\nthis work, we show that any low complexity affine-invariant property of\nmultivariate functions over finite fields is testable with a constant number of\nqueries. This immediately reproves, for instance, that the Reed-Muller code\nover F_p of degree d < p is testable, with an argument that uses no detailed\nalgebraic information about polynomials except that low degree is preserved by\ncomposition with affine maps.\n  The complexity of an affine-invariant property P refers to the maximum\ncomplexity, as defined by Green and Tao (Ann. Math. 2008), of the sets of\nlinear forms used to characterize P. A more precise statement of our main\nresult is that for any fixed prime p >=2 and fixed integer R >= 2, any\naffine-invariant property P of functions f: F_p^n -> [R] is testable, assuming\nthe complexity of the property is less than p. Our proof involves developing\nanalogs of graph-theoretic techniques in an algebraic setting, using tools from\nhigher-order Fourier analysis. \n\n"}
{"id": "1201.2892", "contents": "Title: Algebraic Relaxations and Hardness Results in Polynomial Optimization\n  and Lyapunov Analysis Abstract: This thesis settles a number of questions related to computational complexity\nand algebraic, semidefinite programming based relaxations in optimization and\ncontrol. \n\n"}
{"id": "1201.3184", "contents": "Title: Partial Degree Bounded Edge Packing Problem Abstract: In [1], whether a target binary string s can be represented from a boolean\nformula with operands chosen from a set of binary strings W was studied. In\nthis paper, we first examine selecting a maximum subset X from W, so that for\nany string t in X, t is not representable by X\\{t}. We rephrase this problem as\ngraph, and surprisingly find it give rise to a broad model of edge packing\nproblem, which itself falls into the model of forbidden subgraph problem.\nSpecifically, given a graph G(V;E) and a constant c, the problem asks to choose\nas many as edges to form a subgraph G'. So that in G', for each edge, at least\none of its endpoints has degree no more than c. We call such G' partial c\ndegree bounded. When c = 1, it turns out to be the complement of dominating\nset. We present several results about hardness, approximation for the general\ngraph and efficient exact algorithm on trees. This edge packing problem model\nalso has a direct interpretation in resource allocation. There are n types of\nresources and m jobs. Each job needs two types of resources. A job can be\naccomplished if either one of its necessary resources is shared by no more than\nc other jobs. The problem then asks to nish as many jobs as possible. We\nbelieve this partial degree bounded graph problem merits more attention. \n\n"}
{"id": "1201.4245", "contents": "Title: Descent of affine buildings - I. Large minimal angles Abstract: In this two-part paper we prove an existence result for affine buildings\narising from exceptional algebraic reductive groups. Combined with earlier\nresults on classical groups, this gives a complete and positive answer to the\nconjecture concerning the existence of affine buildings arising from such\ngroups defined over a (skew) field with a complete valuation, as proposed by\nJacques Tits.\n  This first part lays the foundations for our approach and deals with the\n`large minimal angle' case. \n\n"}
{"id": "1201.4344", "contents": "Title: On the intrinsic complexity of elimination problems in effective\n  Algebraic Geometry Abstract: The representation of polynomials by arithmetic circuits evaluating them is\nan alternative data structure which allowed considerable progress in polynomial\nequation solving in the last fifteen years. We present a circuit based\ncomputation model which captures all known symbolic elimination algorithms in\neffective algebraic geometry and show the intrinsically exponential complexity\ncharacter of elimination in this complexity model. \n\n"}
{"id": "1202.0313", "contents": "Title: The Complexity of Computing the Sign of the Tutte Polynomial Abstract: We study the complexity of computing the sign of the Tutte polynomial of a\ngraph. As there are only three possible outcomes (positive, negative, and\nzero), this seems at first sight more like a decision problem than a counting\nproblem. Surprisingly, however, there are large regions of the parameter space\nfor which computing the sign of the Tutte polynomial is actually #P-hard. As a\ntrivial consequence, approximating the polynomial is also #P-hard in this case.\nThus, approximately evaluating the Tutte polynomial in these regions is as hard\nas exactly counting the satisfying assignments to a CNF Boolean formula. For\nmost other points in the parameter space, we show that computing the sign of\nthe polynomial is in FP, whereas approximating the polynomial can be done in\npolynomial time with an NP oracle. As a special case, we completely resolve the\ncomplexity of computing the sign of the chromatic polynomial - this is easily\ncomputable at q=2 and when q is less than or equal to 32/27, and is NP-hard to\ncompute for all other values of the parameter q. \n\n"}
{"id": "1202.0535", "contents": "Title: List decoding subspace codes from insertions and deletions Abstract: We present a construction of subspace codes along with an efficient algorithm\nfor list decoding from both insertions and deletions, handling an\ninformation-theoretically maximum fraction of these with polynomially small\nrate. Our construction is based on a variant of the folded Reed-Solomon codes\nin the world of linearized polynomials, and the algorithm is inspired by the\nrecent linear-algebraic approach to list decoding. Ours is the first list\ndecoding algorithm for subspace codes that can handle deletions; even one\ndeletion can totally distort the structure of the basis of a subspace and is\nthus challenging to handle. When there are only insertions, we also present\nresults for list decoding subspace codes that are the linearized analog of\nReed-Solomon codes (proposed previously, and closely related to the Gabidulin\ncodes for rank-metric), obtaining some improvements over similar results in\nprevious work. \n\n"}
{"id": "1202.0922", "contents": "Title: Low-distortion Inference of Latent Similarities from a Multiplex Social\n  Network Abstract: Much of social network analysis is - implicitly or explicitly - predicated on\nthe assumption that individuals tend to be more similar to their friends than\nto strangers. Thus, an observed social network provides a noisy signal about\nthe latent underlying \"social space:\" the way in which individuals are similar\nor dissimilar. Many research questions frequently addressed via social network\nanalysis are in reality questions about this social space, raising the question\nof inverting the process: Given a social network, how accurately can we\nreconstruct the social structure of similarities and dissimilarities?\n  We begin to address this problem formally. Observed social networks are\nusually multiplex, in the sense that they reflect (dis)similarities in several\ndifferent \"categories,\" such as geographical proximity, kinship, or similarity\nof professions/hobbies. We assume that each such category is characterized by a\nlatent metric capturing (dis)similarities in this category. Each category gives\nrise to a separate social network: a random graph parameterized by this metric.\nFor a concrete model, we consider Kleinberg's small world model and some\nvariations thereof. The observed social network is the unlabeled union of these\ngraphs, i.e., the presence or absence of edges can be observed, but not their\norigins. Our main result is an algorithm which reconstructs each metric with\nprovably low distortion. \n\n"}
{"id": "1202.3367", "contents": "Title: Faster Approximate Multicommodity Flow Using Quadratically Coupled Flows Abstract: The maximum multicommodity flow problem is a natural generalization of the\nmaximum flow problem to route multiple distinct flows. Obtaining a $1-\\epsilon$\napproximation to the multicommodity flow problem on graphs is a well-studied\nproblem. In this paper we present an adaptation of recent advances in\nsingle-commodity flow algorithms to this problem. As the underlying linear\nsystems in the electrical problems of multicommodity flow problems are no\nlonger Laplacians, our approach is tailored to generate specialized systems\nwhich can be preconditioned and solved efficiently using Laplacians. Given an\nundirected graph with m edges and k commodities, we give algorithms that find\n$1-\\epsilon$ approximate solutions to the maximum concurrent flow problem and\nthe maximum weighted multicommodity flow problem in time\n$\\tilde{O}(m^{4/3}\\poly(k,\\epsilon^{-1}))$. \n\n"}
{"id": "1202.4301", "contents": "Title: Algebraic Independence in Positive Characteristic -- A p-Adic Calculus Abstract: A set of multivariate polynomials, over a field of zero or large\ncharacteristic, can be tested for algebraic independence by the well-known\nJacobian criterion. For fields of other characteristic p>0, there is no\nanalogous characterization known. In this paper we give the first such\ncriterion. Essentially, it boils down to a non-degeneracy condition on a lift\nof the Jacobian polynomial over (an unramified extension of) the ring of p-adic\nintegers.\n  Our proof builds on the de Rham-Witt complex, which was invented by Illusie\n(1979) for crystalline cohomology computations, and we deduce a natural\ngeneralization of the Jacobian. This new avatar we call the Witt-Jacobian. In\nessence, we show how to faithfully differentiate polynomials over F_p (i.e.\nsomehow avoid dx^p/dx=0) and thus capture algebraic independence.\n  We apply the new criterion to put the problem of testing algebraic\nindependence in the complexity class NP^#P (previously best was PSPACE). Also,\nwe give a modest application to the problem of identity testing in algebraic\ncomplexity theory. \n\n"}
{"id": "1202.4504", "contents": "Title: A Constant Factor Approximation Algorithm for Reordering Buffer\n  Management Abstract: In the reordering buffer management problem (RBM) a sequence of $n$ colored\nitems enters a buffer with limited capacity $k$. When the buffer is full, one\nitem is removed to the output sequence, making room for the next input item.\nThis step is repeated until the input sequence is exhausted and the buffer is\nempty. The objective is to find a sequence of removals that minimizes the total\nnumber of color changes in the output sequence. The problem formalizes numerous\napplications in computer and production systems, and is known to be NP-hard.\n  We give the first constant factor approximation guarantee for RBM. Our\nalgorithm is based on an intricate \"rounding\" of the solution to an LP\nrelaxation for RBM, so it also establishes a constant upper bound on the\nintegrality gap of this relaxation. Our results improve upon the best previous\nbound of $O(\\sqrt{\\log k})$ of Adamaszek et al. (STOC 2011) that used different\nmethods and gave an online algorithm. Our constant factor approximation beats\nthe super-constant lower bounds on the competitive ratio given by Adamaszek et\nal. This is the first demonstration of an offline algorithm for RBM that is\nprovably better than any online algorithm. \n\n"}
{"id": "1202.4665", "contents": "Title: Algorithms and Almost Tight Results for 3-Colorability of Small Diameter\n  Graphs Abstract: In spite of the extensive studies of the 3-coloring problem with respect to\nseveral basic parameters, the complexity status of the 3-coloring problem on\ngraphs with small diameter, i.e. with diameter 2 or 3, has been a longstanding\nand challenging open question. For graphs with diameter 2 we provide the first\nsubexponential algorithm with complexity $2^{O(\\sqrt{n\\log n})}$, which is\nasymptotically the same as the currently best known time complexity for the\ngraph isomorphism (GI) problem. Moreover, we prove that the graph isomorphism\nproblem on 3-colorable graphs with diameter 2 is GI-complete. Furthermore we\npresent a subclass of graphs with diameter 2 that admits a polynomial algorithm\nfor 3-coloring. For graphs with diameter 3 we establish the complexity of\n3-coloring by proving that for every $\\varepsilon \\in [0,1)$, 3-coloring is\nNP-complete on triangle-free graphs of diameter 3 and radius 2 with $n$\nvertices and minimum degree $\\delta=\\Theta(n^{\\varepsilon})$. Moreover,\nassuming ETH, we provide three different amplifications of our hardness results\nto obtain for every $\\varepsilon \\in [0,1)$ subexponential lower bounds for the\ncomplexity of 3-coloring on triangle-free graphs with diameter 3 and minimum\ndegree $\\delta=\\Theta(n^{\\varepsilon})$. Finally, we provide a 3-coloring\nalgorithm with running time\n$2^{O(\\min\\{\\delta\\Delta,\\frac{n}{\\delta}\\log\\delta\\})}$ for graphs with\ndiameter 3, where $\\delta$ (resp. $\\Delta $) is the minimum (resp. maximum)\ndegree of the input graph. To the best of our knowledge, this algorithm is the\nfirst subexponential algorithm for graphs with $\\delta=\\omega(1)$ and for\ngraphs with $\\delta=O(1)$ and $\\Delta=o(n)$. Due to the above lower bounds of\nthe complexity of 3-coloring, the running time of this algorithm is\nasymptotically almost tight when the minimum degree if the input graph is\n$\\delta=\\Theta(n^{\\varepsilon})$, where $\\varepsilon \\in [1/2,1)$. \n\n"}
{"id": "1202.5715", "contents": "Title: Computing L1 Shortest Paths among Polygonal Obstacles in the Plane Abstract: Given a point $s$ and a set of $h$ pairwise disjoint polygonal obstacles of\ntotally $n$ vertices in the plane, we present a new algorithm for building an\n$L_1$ shortest path map of size O(n) in $O(T)$ time and O(n) space such that\nfor any query point $t$, the length of the $L_1$ shortest obstacle-avoiding\npath from $s$ to $t$ can be reported in $O(\\log n)$ time and the actual\nshortest path can be found in additional time proportional to the number of\nedges of the path, where $T$ is the time for triangulating the free space. It\nis currently known that $T=O(n+h\\log^{1+\\epsilon}h)$ for an arbitrarily small\nconstant $\\epsilon>0$. If the triangulation can be done optimally (i.e.,\n$T=O(n+h\\log h)$), then our algorithm is optimal. Previously, the best\nalgorithm computes such an $L_1$ shortest path map in $O(n\\log n)$ time and\nO(n) space. Our techniques can be extended to obtain improved results for other\nrelated problems, e.g., computing the $L_1$ geodesic Voronoi diagram for a set\nof point sites in a polygonal domain, finding shortest paths with fixed\norientations, finding approximate Euclidean shortest paths, etc. \n\n"}
{"id": "1202.6101", "contents": "Title: Maximum Inner-Product Search using Tree Data-structures Abstract: The problem of {\\em efficiently} finding the best match for a query in a\ngiven set with respect to the Euclidean distance or the cosine similarity has\nbeen extensively studied in literature. However, a closely related problem of\nefficiently finding the best match with respect to the inner product has never\nbeen explored in the general setting to the best of our knowledge. In this\npaper we consider this general problem and contrast it with the existing\nbest-match algorithms. First, we propose a general branch-and-bound algorithm\nusing a tree data structure. Subsequently, we present a dual-tree algorithm for\nthe case where there are multiple queries. Finally we present a new data\nstructure for increasing the efficiency of the dual-tree algorithm. These\nbranch-and-bound algorithms involve novel bounds suited for the purpose of\nbest-matching with inner products. We evaluate our proposed algorithms on a\nvariety of data sets from various applications, and exhibit up to five orders\nof magnitude improvement in query time over the naive search technique. \n\n"}
{"id": "1203.1226", "contents": "Title: Dynamic Packet Scheduling in Wireless Networks Abstract: We consider protocols that serve communication requests arising over time in\na wireless network that is subject to interference. Unlike previous approaches,\nwe take the geometry of the network and power control into account, both\nallowing to increase the network's performance significantly. We introduce a\nstochastic and an adversarial model to bound the packet injection. Although\ntaken as the primary motivation, this approach is not only suitable for models\nbased on the signal-to-interference-plus-noise ratio (SINR). It also covers\nvirtually all other common interference models, for example the multiple-access\nchannel, the radio-network model, the protocol model, and distance-2 matching.\nPacket-routing networks allowing each edge or each node to transmit or receive\none packet at a time can be modeled as well.\n  Starting from algorithms for the respective scheduling problem with static\ntransmission requests, we build distributed stable protocols. This is more\ninvolved than in previous, similar approaches because the algorithms we\nconsider do not necessarily scale linearly when scaling the input instance. We\ncan guarantee a throughput that is as large as the one of the original static\nalgorithm. In particular, for SINR models the competitive ratios of the\nprotocol in comparison to optimal ones in the respective model are between\nconstant and O(log^2 m) for a network of size m. \n\n"}
{"id": "1203.2801", "contents": "Title: On Exact Algorithms for Permutation CSP Abstract: In the Permutation Constraint Satisfaction Problem (Permutation CSP) we are\ngiven a set of variables $V$ and a set of constraints C, in which constraints\nare tuples of elements of V. The goal is to find a total ordering of the\nvariables, $\\pi\\ : V \\rightarrow [1,...,|V|]$, which satisfies as many\nconstraints as possible. A constraint $(v_1,v_2,...,v_k)$ is satisfied by an\nordering $\\pi$ when $\\pi(v_1)<\\pi(v_2)<...<\\pi(v_k)$. An instance has arity $k$\nif all the constraints involve at most $k$ elements.\n  This problem expresses a variety of permutation problems including {\\sc\nFeedback Arc Set} and {\\sc Betweenness} problems. A naive algorithm, listing\nall the $n!$ permutations, requires $2^{O(n\\log{n})}$ time. Interestingly, {\\sc\nPermutation CSP} for arity 2 or 3 can be solved by Held-Karp type algorithms in\ntime $O^*(2^n)$, but no algorithm is known for arity at least 4 with running\ntime significantly better than $2^{O(n\\log{n})}$. In this paper we resolve the\ngap by showing that {\\sc Arity 4 Permutation CSP} cannot be solved in time\n$2^{o(n\\log{n})}$ unless ETH fails. \n\n"}
{"id": "1203.4903", "contents": "Title: Distance Queries from Sampled Data: Accurate and Efficient Abstract: Distance queries are a basic tool in data analysis. They are used for\ndetection and localization of change for the purpose of anomaly detection,\nmonitoring, or planning. Distance queries are particularly useful when data\nsets such as measurements, snapshots of a system, content, traffic matrices,\nand activity logs are collected repeatedly.\n  Random sampling, which can be efficiently performed over streamed or\ndistributed data, is an important tool for scalable data analysis. The sample\nconstitutes an extremely flexible summary, which naturally supports domain\nqueries and scalable estimation of statistics, which can be specified after the\nsample is generated. The effectiveness of a sample as a summary, however,\nhinges on the estimators we have.\n  We derive novel estimators for estimating $L_p$ distance from sampled data.\nOur estimators apply with the most common weighted sampling schemes: Poisson\nProbability Proportional to Size (PPS) and its fixed sample size variants. They\nalso apply when the samples of different data sets are independent or\ncoordinated. Our estimators are admissible (Pareto optimal in terms of\nvariance) and have compelling properties.\n  We study the performance of our Manhattan and Euclidean distance ($p=1,2$)\nestimators on diverse datasets, demonstrating scalability and accuracy even\nwhen a small fraction of the data is sampled. Our work, for the first time,\nfacilitates effective distance estimation over sampled data. \n\n"}
{"id": "1203.5453", "contents": "Title: Optimal Private Halfspace Counting via Discrepancy Abstract: A range counting problem is specified by a set $P$ of size $|P| = n$ of\npoints in $\\mathbb{R}^d$, an integer weight $x_p$ associated to each point $p\n\\in P$, and a range space ${\\cal R} \\subseteq 2^{P}$. Given a query range $R\n\\in {\\cal R}$, the target output is $R(\\vec{x}) = \\sum_{p \\in R}{x_p}$. Range\ncounting for different range spaces is a central problem in Computational\nGeometry.\n  We study $(\\epsilon, \\delta)$-differentially private algorithms for range\ncounting. Our main results are for the range space given by hyperplanes, that\nis, the halfspace counting problem. We present an $(\\epsilon,\n\\delta)$-differentially private algorithm for halfspace counting in $d$\ndimensions which achieves $O(n^{1-1/d})$ average squared error. This contrasts\nwith the $\\Omega(n)$ lower bound established by the classical result of Dinur\nand Nissim [PODS 2003] for arbitrary subset counting queries. We also show a\nmatching lower bound on average squared error for any $(\\epsilon,\n\\delta)$-differentially private algorithm for halfspace counting. Both bounds\nare obtained using discrepancy theory. For the lower bound, we use a modified\ndiscrepancy measure and bound approximation of $(\\epsilon,\n\\delta)$-differentially private algorithms for range counting queries in terms\nof this discrepancy. We also relate the modified discrepancy measure to\nclassical combinatorial discrepancy, which allows us to exploit known\ndiscrepancy lower bounds. This approach also yields a lower bound of\n$\\Omega((\\log n)^{d-1})$ for $(\\epsilon, \\delta)$-differentially private\northogonal range counting in $d$ dimensions, the first known superconstant\nlower bound for this problem. For the upper bound, we use an approach inspired\nby partial coloring methods for proving discrepancy upper bounds, and obtain\n$(\\epsilon, \\delta)$-differentially private algorithms for range counting with\npolynomially bounded shatter function range spaces. \n\n"}
{"id": "1203.6623", "contents": "Title: One brick at a time: a survey of inductive constructions in rigidity\n  theory Abstract: We present a survey of results concerning the use of inductive constructions\nto study the rigidity of frameworks. By inductive constructions we mean simple\ngraph moves which can be shown to preserve the rigidity of the corresponding\nframework. We describe a number of cases in which characterisations of rigidity\nwere proved by inductive constructions. That is, by identifying recursive\noperations that preserved rigidity and proving that these operations were\nsufficient to generate all such frameworks. We also outline the use of\ninductive constructions in some recent areas of particularly active interest,\nnamely symmetric and periodic frameworks, frameworks on surfaces, and body-bar\nframeworks. We summarize the key outstanding open problems related to\ninductions. \n\n"}
{"id": "1203.6867", "contents": "Title: Explicit constructions of centrally symmetric k-neighborly polytopes and\n  large strictly antipodal sets Abstract: We present explicit constructions of centrally symmetric 2-neighborly\nd-dimensional polytopes with about 3^{d/2} = (1.73)^d vertices and of centrally\nsymmetric k-neighborly d-polytopes with about 2^{c_k d} vertices where c_k=3/20\nk^2 2^k. Using this result, we construct for a fixed k > 1 and arbitrarily\nlarge d and N, a centrally symmetric d-polytope with N vertices that has at\nleast (1-k^2 (gamma_k)^d) binom(N, k) faces of dimension k-1, where\ngamma_2=1/\\sqrt{3} = 0.58 and gamma_k = 2^{-3/{20k^2 2^k}} for k > 2. Another\napplication is a construction of a set of 3^{d/2 -1}-1 points in R^d every two\nof which are strictly antipodal as well as a construction of an n-point set\n(for an arbitrarily large n) in R^d with many pairs of strictly antipodal\npoints. The two latter results significantly improve the previous bounds by\nTalata, and Makai and Martini, respectively. \n\n"}
{"id": "1204.1939", "contents": "Title: Random walks which prefer unvisited edges. Exploring high girth even\n  degree expanders in linear time Abstract: We consider a modified random walk which uses unvisited edges whenever\npossible, and makes a simple random walk otherwise. We call such a walk an\nedge-process. We assume there is a rule A, which tells the walk which unvisited\nedge to use whenever there is a choice. In the simplest case, A is a uniform\nrandom choice over unvisited edges incident with the current walk position.\nHowever we do not exclude arbitrary choices of rule A. For example, the rule\ncould be determined on-line by an adversary, or could vary from vertex to\nvertex.\n  For even degree expander graphs, of bounded maximum degree, we have the\nfollowing result. Let G be an n vertex even degree expander graph, for which\nevery vertex is in at least one vertex induced cycle of length L. Any\nedge-process on G has cover time (n+ (n log n)/L). This result is independent\nof the rule A used to select the order of the unvisited edges, which can be\nchosen on-line by an adversary.\n  As an example, With high probability, random r-regular graphs, (r at least 4,\neven), are expanders for which L = Omega(log n). Thus, for almost all such\ngraphs, the vertex cover time of the edge-process is Theta(n). This improves\nthe vertex cover time of such graphs by a factor of log n, compared to the\nOmega(n log n) cover time of any weighted random walk. \n\n"}
{"id": "1204.2124", "contents": "Title: Finding vertex-surjective graph homomorphisms Abstract: The Surjective Homomorphism problem is to test whether a given graph G called\nthe guest graph allows a vertex-surjective homomorphism to some other given\ngraph H called the host graph. The bijective and injective homomorphism\nproblems can be formulated in terms of spanning subgraphs and subgraphs, and as\nsuch their computational complexity has been extensively studied. What about\nthe surjective variant? Because this problem is NP-complete in general, we\nrestrict the guest and the host graph to belong to graph classes G and H,\nrespectively. We determine to what extent a certain choice of G and H\ninfluences its computational complexity. We observe that the problem is\npolynomial-time solvable if H is the class of paths, whereas it is NP-complete\nif G is the class of paths. Moreover, we show that the problem is even\nNP-complete on many other elementary graph classes, namely linear forests,\nunions of complete graphs, cographs, proper interval graphs, split graphs and\ntrees of pathwidth at most 2. In contrast, we prove that the problem is\nfixed-parameter tractable in k if G is the class of trees and H is the class of\ntrees with at most k leaves, or if G and H are equal to the class of graphs\nwith vertex cover number at most k. \n\n"}
{"id": "1204.2202", "contents": "Title: Clique in 3-track interval graphs is APX-hard Abstract: Butman, Hermelin, Lewenstein, and Rawitz proved that Clique in t-interval\ngraphs is NP-hard for t >= 3. We strengthen this result to show that Clique in\n3-track interval graphs is APX-hard. \n\n"}
{"id": "1204.4047", "contents": "Title: Access Graphs Results for LRU versus FIFO under Relative Worst Order\n  Analysis Abstract: Access graphs, which have been used previously in connection with competitive\nanalysis to model locality of reference in paging, are considered in connection\nwith relative worst order analysis. In this model, FWF is shown to be strictly\nworse than both LRU and FIFO on any access graph. LRU is shown to be strictly\nbetter than FIFO on paths and cycles, but they are incomparable on some\nfamilies of graphs which grow with the length of the sequences. \n\n"}
{"id": "1204.4547", "contents": "Title: Minkowski Decomposition of Associahedra and Related Combinatorics Abstract: Realisations of associahedra with linearly non-isomorphic normal fans can be\nobtained by alteration of the right-hand sides of the facet-defining\ninequalities from a classical permutahedron. These polytopes can be expressed\nas Minkowski sums and differences of dilated faces of a standard simplex as\ndescribed by Ardila, Benedetti & Doker (2010). The coefficients $y_I$ of such a\nMinkowski decomposition can be computed by M\\\"obius inversion if tight\nright-hand sides $z_I$ are known not just for the facet-defining inequalities\nof the associahedron but also for all inequalities of the permutahedron that\nare redundant for the associahedron.\n  We show for certain families of these associahedra: (a) how to compute tight\nvalues $z_I$ for the redundant inequalities from the values $z_I$ for the\nfacet-defining inequalities; (b) the computation of the values $y_I$ of Ardila,\nBenedetti & Doker can be significantly simplified and at most four values\n$z_{a(I)}$, $z_{b(I)}$, $z_{c(I)}$ and $z_{d(I)}$ are needed to compute $y_I$;\n(c) the four indices $a(I)$, $b(I)$, $c(I)$ and $d(I)$ are determined by the\ngeometry of the normal fan of the associahedron and are described\ncombinatorially; (d) a combinatorial interpretation of the values $y_I$ using a\nlabeled $n$-gon. This last result is inspired from similar interpretations for\nvertex coordinates originally described originally by J.-L. Loday and\nwell-known interpretations for the $z_I$-values of facet-defining inequalities. \n\n"}
{"id": "1204.5952", "contents": "Title: Clustering by hypergraphs and dimensionality of cluster systems Abstract: In the present paper we discuss the clustering procedure in the case where\ninstead of a single metric we have a family of metrics. In this case we can\nobtain a partially ordered graph of clusters which is not necessarily a tree.\nWe discuss a structure of a hypergraph above this graph. We propose two\ndefinitions of dimension for hyperedges of this hypergraph and show that for\nthe multidimensional p-adic case both dimensions are reduced to the number of\np-adic parameters.\n  We discuss the application of the hypergraph clustering procedure to the\nconstruction of phylogenetic graphs in biology. In this case the dimension of a\nhyperedge will describe the number of sources of genetic diversity. \n\n"}
{"id": "1204.6233", "contents": "Title: Strong Backdoors to Bounded Treewidth SAT Abstract: There are various approaches to exploiting \"hidden structure\" in instances of\nhard combinatorial problems to allow faster algorithms than for general\nunstructured or random instances. For SAT and its counting version #SAT, hidden\nstructure has been exploited in terms of decomposability and strong backdoor\nsets. Decomposability can be considered in terms of the treewidth of a graph\nthat is associated with the given CNF formula, for instance by considering\nclauses and variables as vertices of the graph, and making a variable adjacent\nwith all the clauses it appears in. On the other hand, a strong backdoor set of\na CNF formula is a set of variables such that each possible partial assignment\nto this set moves the formula into a fixed class for which (#)SAT can be solved\nin polynomial time.\n  In this paper we combine the two above approaches. In particular, we study\nthe algorithmic question of finding a small strong backdoor set into the class\nW_t of CNF formulas whose associated graphs have treewidth at most t. The main\nresults are positive:\n  (1) There is a cubic-time algorithm that, given a CNF formula F and two\nconstants k,t\\ge 0, either finds a strong W_t-backdoor set of size at most 2^k,\nor concludes that F has no strong W_t-backdoor set of size at most k.\n  (2) There is a cubic-time algorithm that, given a CNF formula F, computes the\nnumber of satisfying assignments of F or concludes that sb_t(F)>k, for any pair\nof constants k,t\\ge 0. Here, sb_t(F) denotes the size of a smallest strong\nW_t-backdoor set of F.\n  The significance of our results lies in the fact that they allow us to\nexploit algorithmically a hidden structure in formulas that is not accessible\nby any one of the two approaches (decomposability, backdoors) alone. Already a\nbackdoor size 1 on top of treewidth 1 (i.e., sb_1(F)=1) entails formulas of\narbitrarily large treewidth and arbitrarily large cycle cutsets. \n\n"}
{"id": "1205.0144", "contents": "Title: Everywhere-Sparse Spanners via Dense Subgraphs Abstract: The significant progress in constructing graph spanners that are sparse\n(small number of edges) or light (low total weight) has skipped spanners that\nare everywhere-sparse (small maximum degree). This disparity is in line with\nother network design problems, where the maximum-degree objective has been a\nnotorious technical challenge. Our main result is for the Lowest Degree\n2-Spanner (LD2S) problem, where the goal is to compute a 2-spanner of an input\ngraph so as to minimize the maximum degree. We design a polynomial-time\nalgorithm achieving approximation factor $\\tilde O(\\Delta^{3-2\\sqrt{2}})\n\\approx \\tilde O(\\Delta^{0.172})$, where $\\Delta$ is the maximum degree of the\ninput graph. The previous $\\tilde O(\\Delta^{1/4})$ -approximation was proved\nnearly two decades ago by Kortsarz and Peleg [SODA 1994, SICOMP 1998].\n  Our main conceptual contribution is to establish a formal connection between\nLD2S and a variant of the Densest k-Subgraph (DkS) problem. Specifically, we\ndesign for both problems strong relaxations based on the Sherali-Adams linear\nprogramming (LP) hierarchy, and show that \"faithful\" randomized rounding of the\nDkS-variant can be used to round LD2S solutions. Our notion of faithfulness\nintuitively means that all vertices and edges are chosen with probability\nproportional to their LP value, but the precise formulation is more subtle.\n  Unfortunately, the best algorithms known for DkS use the Lov\\'asz-Schrijver\nLP hierarchy in a non-faithful way [Bhaskara, Charikar, Chlamtac, Feige, and\nVijayaraghavan, STOC 2010]. Our main technical contribution is to overcome this\nshortcoming, while still matching the gap that arises in random graphs by\nplanting a subgraph with same log-density. \n\n"}
{"id": "1205.1183", "contents": "Title: On the Complexity of Trial and Error Abstract: Motivated by certain applications from physics, biochemistry, economics, and\ncomputer science, in which the objects under investigation are not accessible\nbecause of various limitations, we propose a trial-and-error model to examine\nalgorithmic issues in such situations. Given a search problem with a hidden\ninput, we are asked to find a valid solution, to find which we can propose\ncandidate solutions (trials), and use observed violations (errors), to prepare\nfuture proposals. In accordance with our motivating applications, we consider\nthe fairly broad class of constraint satisfaction problems, and assume that\nerrors are signaled by a verification oracle in the format of the index of a\nviolated constraint (with the content of the constraint still hidden).\n  Our discoveries are summarized as follows. On one hand, despite the seemingly\nvery little information provided by the verification oracle, efficient\nalgorithms do exist for a number of important problems. For the Nash, Core,\nStable Matching, and SAT problems, the unknown-input versions are as hard as\nthe corresponding known-input versions, up to a factor of polynomial. We\nfurther give almost tight bounds on the latter two problems' trial\ncomplexities. On the other hand, there are problems whose complexities are\nsubstantially increased in the unknown-input model. In particular, no\ntime-efficient algorithms exist (under standard hardness assumptions) for Graph\nIsomorphism and Group Isomorphism problems. The tools used to achieve these\nresults include order theory, strong ellipsoid method, and some non-standard\nreductions.\n  Our model investigates the value of information, and our results demonstrate\nthat the lack of input information can introduce various levels of extra\ndifficulty. The model exhibits intimate connections with (and we hope can also\nserve as a useful supplement to) certain existing learning and complexity\ntheories. \n\n"}
{"id": "1205.4484", "contents": "Title: Hypercontractivity, Sum-of-Squares Proofs, and their Applications Abstract: We study the computational complexity of approximating the 2->q norm of\nlinear operators (defined as ||A||_{2->q} = sup_v ||Av||_q/||v||_2), as well as\nconnections between this question and issues arising in quantum information\ntheory and the study of Khot's Unique Games Conjecture (UGC). We show the\nfollowing:\n  1. For any constant even integer q>=4, a graph $G$ is a \"small-set expander\"\nif and only if the projector into the span of the top eigenvectors of G's\nadjacency matrix has bounded 2->q norm. As a corollary, a good approximation to\nthe 2->q norm will refute the Small-Set Expansion Conjecture--a close variant\nof the UGC. We also show that such a good approximation can be obtained in\nexp(n^(2/q)) time, thus obtaining a different proof of the known subexponential\nalgorithm for Small Set Expansion.\n  2. Constant rounds of the \"Sum of Squares\" semidefinite programing hierarchy\ncertify an upper bound on the 2->4 norm of the projector to low-degree\npolynomials over the Boolean cube, as well certify the unsatisfiability of the\n\"noisy cube\" and \"short code\" based instances of Unique Games considered by\nprior works. This improves on the previous upper bound of exp(poly log n)\nrounds (for the \"short code\"), as well as separates the \"Sum of\nSquares\"/\"Lasserre\" hierarchy from weaker hierarchies that were known to\nrequire omega(1) rounds.\n  3. We show reductions between computing the 2->4 norm and computing the\ninjective tensor norm of a tensor, a problem with connections to quantum\ninformation theory. Three corollaries are: (i) the 2->4 norm is NP-hard to\napproximate to precision inverse-polynomial in the dimension, (ii) the 2->4\nnorm does not have a good approximation (in the sense above) unless 3-SAT can\nbe solved in time exp(sqrt(n) polylog(n)), and (iii) known algorithms for the\nquantum separability problem imply a non-trivial additive approximation for the\n2->4 norm. \n\n"}
{"id": "1205.4605", "contents": "Title: No Sublogarithmic-time Approximation Scheme for Bipartite Vertex Cover Abstract: K\\\"onig's theorem states that on bipartite graphs the size of a maximum\nmatching equals the size of a minimum vertex cover. It is known from prior work\nthat for every \\epsilon > 0 there exists a constant-time distributed algorithm\nthat finds a (1+\\epsilon)-approximation of a maximum matching on 2-coloured\ngraphs of bounded degree. In this work, we show---somewhat surprisingly---that\nno sublogarithmic-time approximation scheme exists for the dual problem: there\nis a constant \\delta > 0 so that no randomised distributed algorithm with\nrunning time o(\\log n) can find a (1+\\delta)-approximation of a minimum vertex\ncover on 2-coloured graphs of maximum degree 3. In fact, a simple application\nof the Linial--Saks (1993) decomposition demonstrates that this lower bound is\ntight.\n  Our lower-bound construction is simple and, to some extent, independent of\nprevious techniques. Along the way we prove that a certain cut minimisation\nproblem, which might be of independent interest, is hard to approximate locally\non expander graphs. \n\n"}
{"id": "1206.0594", "contents": "Title: Simple and Deterministic Matrix Sketching Abstract: We adapt a well known streaming algorithm for approximating item frequencies\nto the matrix sketching setting. The algorithm receives the rows of a large\nmatrix $A \\in \\R^{n \\times m}$ one after the other in a streaming fashion. It\nmaintains a sketch matrix $B \\in \\R^ {1/\\eps \\times m}$ such that for any unit\nvector $x$ [\\|Ax\\|^2 \\ge \\|Bx\\|^2 \\ge \\|Ax\\|^2 - \\eps \\|A\\|_{f}^2 \\.] Sketch\nupdates per row in $A$ require $O(m/\\eps^2)$ operations in the worst case. A\nslight modification of the algorithm allows for an amortized update time of\n$O(m/\\eps)$ operations per row. The presented algorithm stands out in that it\nis: deterministic, simple to implement, and elementary to prove. It also\nexperimentally produces more accurate sketches than widely used approaches\nwhile still being computationally competitive. \n\n"}
{"id": "1206.1623", "contents": "Title: Proximal Newton-type methods for minimizing composite functions Abstract: We generalize Newton-type methods for minimizing smooth functions to handle a\nsum of two convex functions: a smooth function and a nonsmooth function with a\nsimple proximal mapping. We show that the resulting proximal Newton-type\nmethods inherit the desirable convergence behavior of Newton-type methods for\nminimizing smooth functions, even when search directions are computed\ninexactly. Many popular methods tailored to problems arising in bioinformatics,\nsignal processing, and statistical learning are special cases of proximal\nNewton-type methods, and our analysis yields new convergence results for some\nof these methods. \n\n"}
{"id": "1206.3628", "contents": "Title: An Infinite Class of Sparse-Yao Spanners Abstract: We show that, for any integer k > 5, the Sparse-Yao graph YY_{6k} (also known\nas Yao-Yao) is a spanner with stretch factor 11.67. The stretch factor drops\ndown to 4.75 for k > 7. \n\n"}
{"id": "1206.3993", "contents": "Title: Thrifty approximations of convex bodies by polytopes Abstract: Given a convex body C in R^d containing the origin in its interior and a real\nnumber tau > 1 we seek to construct a polytope P in C with as few vertices as\npossible such that C in tau P. Our construction is nearly optimal for a wide\nrange of d and tau. In particular, we prove that if C=-C then for any\n1>epsilon>0 and tau=1+epsilon one can choose P having roughly epsilon^{-d/2}\nvertices and for tau=sqrt{epsilon d} one can choose P having roughly\nd^{1/epsilon} vertices. Similarly, we prove that if C in R^d is a convex body\nsuch that -C in mu C for some mu > 1 then one can choose P having roughly\n((mu+1)/(tau-1))^{d/2} vertices provided (tau-1)/(mu+1) << 1. \n\n"}
{"id": "1206.4912", "contents": "Title: Preprocessing Subgraph and Minor Problems: When Does a Small Vertex\n  Cover Help? Abstract: We prove a number of results around kernelization of problems parameterized\nby the size of a given vertex cover of the input graph. We provide three sets\nof simple general conditions characterizing problems admitting kernels of\npolynomial size. Our characterizations not only give generic explanations for\nthe existence of many known polynomial kernels for problems like q-Coloring,\nOdd Cycle Transversal, Chordal Deletion, Eta Transversal, or Long Path,\nparameterized by the size of a vertex cover, but also imply new polynomial\nkernels for problems like F-Minor-Free Deletion, which is to delete at most k\nvertices to obtain a graph with no minor from a fixed finite set F.\n  While our characterization captures many interesting problems, the\nkernelization complexity landscape of parameterizations by vertex cover is much\nmore involved. We demonstrate this by several results about induced subgraph\nand minor containment testing, which we find surprising. While it was known\nthat testing for an induced complete subgraph has no polynomial kernel unless\nNP is in coNP/poly, we show that the problem of testing if a graph contains a\ncomplete graph on t vertices as a minor admits a polynomial kernel. On the\nother hand, it was known that testing for a path on t vertices as a minor\nadmits a polynomial kernel, but we show that testing for containment of an\ninduced path on t vertices is unlikely to admit a polynomial kernel. \n\n"}
{"id": "1206.6528", "contents": "Title: Adversary Lower Bound for the k-sum Problem Abstract: We prove a tight quantum query lower bound $\\Omega(n^{k/(k+1)})$ for the\nproblem of deciding whether there exist $k$ numbers among $n$ that sum up to a\nprescribed number, provided that the alphabet size is sufficiently large.\n  This is an extended and simplified version of an earlier preprint of one of\nthe authors arXiv:1204.5074. \n\n"}
{"id": "1207.2229", "contents": "Title: A robust Khintchine inequality, and algorithms for computing optimal\n  constants in Fourier analysis and high-dimensional geometry Abstract: This paper makes two contributions towards determining some well-studied\noptimal constants in Fourier analysis \\newa{of Boolean functions} and\nhigh-dimensional geometry.\n  \\begin{enumerate}\n  \\item It has been known since 1994 \\cite{GL:94} that every linear threshold\nfunction has squared Fourier mass at least 1/2 on its degree-0 and degree-1\ncoefficients. Denote the minimum such Fourier mass by $\\w^{\\leq 1}[\\ltf]$,\nwhere the minimum is taken over all $n$-variable linear threshold functions and\nall $n \\ge 0$. Benjamini, Kalai and Schramm \\cite{BKS:99} have conjectured that\nthe true value of $\\w^{\\leq 1}[\\ltf]$ is $2/\\pi$. We make progress on this\nconjecture by proving that $\\w^{\\leq 1}[\\ltf] \\geq 1/2 + c$ for some absolute\nconstant $c>0$. The key ingredient in our proof is a \"robust\" version of the\nwell-known Khintchine inequality in functional analysis, which we believe may\nbe of independent interest.\n  \\item We give an algorithm with the following property: given any $\\eta > 0$,\nthe algorithm runs in time $2^{\\poly(1/\\eta)}$ and determines the value of\n$\\w^{\\leq 1}[\\ltf]$ up to an additive error of $\\pm\\eta$. We give a similar\n$2^{{\\poly(1/\\eta)}}$-time algorithm to determine \\emph{Tomaszewski's constant}\nto within an additive error of $\\pm \\eta$; this is the minimum (over all\norigin-centered hyperplanes $H$) fraction of points in $\\{-1,1\\}^n$ that lie\nwithin Euclidean distance 1 of $H$. Tomaszewski's constant is conjectured to be\n1/2; lower bounds on it have been given by Holzman and Kleitman \\cite{HK92} and\nindependently by Ben-Tal, Nemirovski and Roos \\cite{BNR02}.\n  Our algorithms combine tools from anti-concentration of sums of independent\nrandom variables, Fourier analysis, and Hermite analysis of linear threshold\nfunctions.\n  \\end{enumerate} \n\n"}
{"id": "1207.2598", "contents": "Title: Hitting Sets Online and Unique-Max Coloring Abstract: We consider the problem of hitting sets online. The hypergraph (i.e.,\nrange-space consisting of points and ranges) is known in advance, and the\nranges to be stabbed are input one-by-one in an online fashion. The online\nalgorithm must stab each range upon arrival. An online algorithm may add points\nto the hitting set but may not remove already chosen points. The goal is to use\nthe smallest number of points. The best known competitive ratio for hitting\nsets online by Alon et al. \\cite{alon2009online} is $O(\\log n \\cdot \\log m)$\nfor general hypergraphs, where $n$ and $m$ denote the number of points and the\nnumber of ranges, respectively. We consider hypergraphs in which the union of\ntwo intersecting ranges is also a range. Our main result for such hypergraphs\nis as follows. The competitive ratio of the online hitting set problem is at\nmost the unique-max number and at least this number minus one. \n\n"}
{"id": "1207.5211", "contents": "Title: Can Quantum Communication Speed Up Distributed Computation? Abstract: The focus of this paper is on {\\em quantum distributed} computation, where we\ninvestigate whether quantum communication can help in {\\em speeding up}\ndistributed network algorithms. Our main result is that for certain fundamental\nnetwork problems such as minimum spanning tree, minimum cut, and shortest\npaths, quantum communication {\\em does not} help in substantially speeding up\ndistributed algorithms for these problems compared to the classical setting.\n  In order to obtain this result, we extend the technique of Das Sarma et al.\n[SICOMP 2012] to obtain a uniform approach to prove non-trivial lower bounds\nfor quantum distributed algorithms for several graph optimization (both exact\nand approximate versions) as well as verification problems, some of which are\nnew even in the classical setting, e.g. tight randomized lower bounds for\nHamiltonian cycle and spanning tree verification, answering an open problem of\nDas Sarma et al., and a lower bound in terms of the weight aspect ratio,\nmatching the upper bounds of Elkin [STOC 2004]. Our approach introduces the\n{\\em Server model} and {\\em Quantum Simulation Theorem} which together provide\na connection between distributed algorithms and communication complexity. The\nServer model is the standard two-party communication complexity model augmented\nwith additional power; yet, most of the hardness in the two-party model is\ncarried over to this new model. The Quantum Simulation Theorem carries this\nhardness further to quantum distributed computing. Our techniques, except the\nproof of the hardness in the Server model, require very little knowledge in\nquantum computing, and this can help overcoming a usual impediment in proving\nbounds on quantum distributed algorithms. \n\n"}
{"id": "1207.6199", "contents": "Title: Achieving Approximate Soft Clustering in Data Streams Abstract: In recent years, data streaming has gained prominence due to advances in\ntechnologies that enable many applications to generate continuous flows of\ndata. This increases the need to develop algorithms that are able to\nefficiently process data streams. Additionally, real-time requirements and\nevolving nature of data streams make stream mining problems, including\nclustering, challenging research problems.\n  In this paper, we propose a one-pass streaming soft clustering (membership of\na point in a cluster is described by a distribution) algorithm which\napproximates the \"soft\" version of the k-means objective function. Soft\nclustering has applications in various aspects of databases and machine\nlearning including density estimation and learning mixture models. We first\nachieve a simple pseudo-approximation in terms of the \"hard\" k-means algorithm,\nwhere the algorithm is allowed to output more than $k$ centers. We convert this\nbatch algorithm to a streaming one (using an extension of the k-means++\nalgorithm recently proposed) in the \"cash register\" model. We also extend this\nalgorithm when the clustering is done over a moving window in the data stream. \n\n"}
{"id": "1207.6528", "contents": "Title: Fast matrix multiplication using coherent configurations Abstract: We introduce a relaxation of the notion of tensor rank, called s-rank, and\nshow that upper bounds on the s-rank of the matrix multiplication tensor imply\nupper bounds on the ordinary rank. In particular, if the \"s-rank exponent of\nmatrix multiplication\" equals 2, then omega = 2. This connection between the\ns-rank exponent and the ordinary exponent enables us to significantly\ngeneralize the group-theoretic approach of Cohn and Umans, from group algebras\nto general algebras. Embedding matrix multiplication into general algebra\nmultiplication yields bounds on s-rank (not ordinary rank) and, prior to this\npaper, that had been a barrier to working with general algebras.\n  We identify adjacency algebras of coherent configurations as a promising\nfamily of algebras in the generalized framework. Coherent configurations are\ncombinatorial objects that generalize groups and group actions; adjacency\nalgebras are the analogue of group algebras and retain many of their important\nfeatures. As with groups, coherent configurations support matrix multiplication\nwhen a natural combinatorial condition is satisfied, involving triangles of\npoints in their underlying geometry.\n  Finally, we prove a closure property involving symmetric powers of adjacency\nalgebras, which enables us to prove nontrivial bounds on omega using\ncommutative coherent configurations and suggests that commutative coherent\nconfigurations may be sufficient to prove omega = 2. Altogether, our results\nshow that bounds on omega can be established by embedding large matrix\nmultiplication instances into small commutative coherent configurations, while\navoiding the representation-theoretic complications that were present in the\ngroup-theoretic approach. \n\n"}
{"id": "1208.0539", "contents": "Title: Locally decodable codes and the failure of cotype for projective tensor\n  products Abstract: It is shown that for every $p\\in (1,\\infty)$ there exists a Banach space $X$\nof finite cotype such that the projective tensor product $\\ell_p\\tp X$ fails to\nhave finite cotype. More generally, if $p_1,p_2,p_3\\in (1,\\infty)$ satisfy\n$\\frac{1}{p_1}+\\frac{1}{p_2}+\\frac{1}{p_3}\\le 1$ then\n$\\ell_{p_1}\\tp\\ell_{p_2}\\tp\\ell_{p_3}$ does not have finite cotype. This is a\nproved via a connection to the theory of locally decodable codes. \n\n"}
{"id": "1208.2755", "contents": "Title: Two-Way Finite Automata: Old and Recent Results Abstract: The notion of two-way automata was introduced at the very beginning of\nautomata theory. In 1959, Rabin and Scott and, independently, Shepherdson,\nproved that these models, both in the deterministic and in the nondeterministic\nversions, have the same power of one-way automata, namely, they characterize\nthe class of regular languages.\n  In 1978, Sakoda and Sipser posed the question of the cost, in the number of\nthe states, of the simulation of one-way and two-way nondeterministic automata\nby two-way deterministic automata. They conjectured that these costs are\nexponential. In spite of all attempts to solve it, this question is still open.\n  In the last ten years the problem of Sakoda and Sipser was widely\nreconsidered and many new results related to it have been obtained. In this\nwork we discuss some of them. In particular, we focus on the restriction to the\nunary case and on the connections with open questions in space complexity. \n\n"}
{"id": "1208.6589", "contents": "Title: Efficient Computation of the Permanent of Block Factorizable Matrices Abstract: We present an efficient algorithm for computing the permanent for matrices of\nsize N that can written as a product of L block diagonal matrices with blocks\nof size at most 2. For fixed L, the time and space resources scale linearly in\nN, with a prefactor that scales exponentially in L. This class of matrices\ncontains banded matrices with banded inverse. We show that such a factorization\ninto a product of block diagonal matrices gives rise to a circuit acting on a\nHilbert space with a tensor product structure and that the permanent is equal\nto the transition amplitude of this circuit and a product basis state. In this\ncorrespondence, a block diagonal matrix gives rise to one layer of the circuit,\nwhere each block to a gate acting either on a single tensor component or on two\nadjacent tensor components. This observation allows us to adopt matrix product\nstates, a computational method from condensed matter physics and quantum\ninformation theory used to simulate quantum systems, to evaluate the transition\namplitude. \n\n"}
{"id": "1210.3231", "contents": "Title: Hyperbolicity and stable polynomials in combinatorics and probability Abstract: This was the basis of two lectures in the Current Developments in Mathematics\nconference in 2011. These lectures survey the theory of hyperbolic and stable\npolynomials, from their origins in the theory of linear PDE's to their present\nuses in combinatorics and probability theory. \n\n"}
{"id": "1210.3277", "contents": "Title: Shortest, Fastest, and Foremost Broadcast in Dynamic Networks Abstract: Highly dynamic networks rarely offer end-to-end connectivity at a given time.\nYet, connectivity in these networks can be established over time and space,\nbased on temporal analogues of multi-hop paths (also called {\\em journeys}).\nAttempting to optimize the selection of the journeys in these networks\nnaturally leads to the study of three cases: shortest (minimum hop), fastest\n(minimum duration), and foremost (earliest arrival) journeys. Efficient\ncentralized algorithms exists to compute all cases, when the full knowledge of\nthe network evolution is given.\n  In this paper, we study the {\\em distributed} counterparts of these problems,\ni.e. shortest, fastest, and foremost broadcast with termination detection\n(TDB), with minimal knowledge on the topology.\n  We show that the feasibility of each of these problems requires distinct\nfeatures on the evolution, through identifying three classes of dynamic graphs\nwherein the problems become gradually feasible: graphs in which the\nre-appearance of edges is {\\em recurrent} (class R), {\\em bounded-recurrent}\n(B), or {\\em periodic} (P), together with specific knowledge that are\nrespectively $n$ (the number of nodes), $\\Delta$ (a bound on the recurrence\ntime), and $p$ (the period). In these classes it is not required that all pairs\nof nodes get in contact -- only that the overall {\\em footprint} of the graph\nis connected over time.\n  Our results, together with the strict inclusion between $P$, $B$, and $R$,\nimplies a feasibility order among the three variants of the problem, i.e.\nTDB[foremost] requires weaker assumptions on the topology dynamics than\nTDB[shortest], which itself requires less than TDB[fastest]. Reversely, these\ndifferences in feasibility imply that the computational powers of $R_n$,\n$B_\\Delta$, and $P_p$ also form a strict hierarchy. \n\n"}
{"id": "1210.3978", "contents": "Title: Fixed-Parameter Tractability of Workflow Satisfiability in the Presence\n  of Seniority Constraints Abstract: The workflow satisfiability problem is concerned with determining whether it\nis possible to find an allocation of authorized users to the steps in a\nworkflow in such a way that all constraints are satisfied. The problem is\nNP-hard in general, but is known to be fixed-parameter tractable for certain\nclasses of constraints. The known results on fixed-parameter tractability rely\non the symmetry (in some sense) of the constraints. In this paper, we provide\nthe first results that establish fixed-parameter tractability of the\nsatisfiability problem when the constraints are asymmetric. In particular, we\nintroduce the notion of seniority constraints, in which the execution of steps\nis determined, in part, by the relative seniority of the users that perform\nthem. Our results require new techniques, which make use of tree decompositions\nof the graph of the binary relation defining the constraint. Finally, we\nestablish a lower bound for the hardness of the workflow satisfiability\nproblem. \n\n"}
{"id": "1210.4081", "contents": "Title: Getting Feasible Variable Estimates From Infeasible Ones: MRF Local\n  Polytope Study Abstract: This paper proposes a method for construction of approximate feasible primal\nsolutions from dual ones for large-scale optimization problems possessing\ncertain separability properties. Whereas infeasible primal estimates can\ntypically be produced from (sub-)gradients of the dual function, it is often\nnot easy to project them to the primal feasible set, since the projection\nitself has a complexity comparable to the complexity of the initial problem. We\npropose an alternative efficient method to obtain feasibility and show that its\nproperties influencing the convergence to the optimum are similar to the\nproperties of the Euclidean projection. We apply our method to the local\npolytope relaxation of inference problems for Markov Random Fields and\ndemonstrate its superiority over existing methods. \n\n"}
{"id": "1210.5048", "contents": "Title: Convergence of SDP hierarchies for polynomial optimization on the\n  hypersphere Abstract: We show how to bound the accuracy of a family of semi-definite programming\nrelaxations for the problem of polynomial optimization on the hypersphere. Our\nmethod is inspired by a set of results from quantum information known as\nquantum de Finetti theorems. In particular, we prove a de Finetti theorem for a\nspecial class of real symmetric matrices to establish the existence of\napproximate representing measures for moment matrix relaxations. \n\n"}
{"id": "1210.6287", "contents": "Title: Fast Exact Max-Kernel Search Abstract: The wide applicability of kernels makes the problem of max-kernel search\nubiquitous and more general than the usual similarity search in metric spaces.\nWe focus on solving this problem efficiently. We begin by characterizing the\ninherent hardness of the max-kernel search problem with a novel notion of\ndirectional concentration. Following that, we present a method to use an $O(n\n\\log n)$ algorithm to index any set of objects (points in $\\Real^\\dims$ or\nabstract objects) directly in the Hilbert space without any explicit feature\nrepresentations of the objects in this space. We present the first provably\n$O(\\log n)$ algorithm for exact max-kernel search using this index. Empirical\nresults for a variety of data sets as well as abstract objects demonstrate up\nto 4 orders of magnitude speedup in some cases. Extensions for approximate\nmax-kernel search are also presented. \n\n"}
{"id": "1210.6367", "contents": "Title: Quantum de Finetti Theorems under Local Measurements with Applications Abstract: Quantum de Finetti theorems are a useful tool in the study of correlations in\nquantum multipartite states. In this paper we prove two new quantum de Finetti\ntheorems, both showing that under tests formed by local measurements one can\nget a much improved error dependence on the dimension of the subsystems. We\nalso obtain similar results for non-signaling probability distributions. We\ngive the following applications of the results:\n  We prove the optimality of the Chen-Drucker protocol for 3-SAT, under the\nexponential time hypothesis.\n  We show that the maximum winning probability of free games can be estimated\nin polynomial time by linear programming. We also show that 3-SAT with m\nvariables can be reduced to obtaining a constant error approximation of the\nmaximum winning probability under entangled strategies of O(m^{1/2})-player\none-round non-local games, in which the players communicate O(m^{1/2}) bits all\ntogether.\n  We show that the optimization of certain polynomials over the hypersphere can\nbe performed in quasipolynomial time in the number of variables n by\nconsidering O(log(n)) rounds of the Sum-of-Squares (Parrilo/Lasserre) hierarchy\nof semidefinite programs. As an application to entanglement theory, we find a\nquasipolynomial-time algorithm for deciding multipartite separability.\n  We consider a result due to Aaronson -- showing that given an unknown n qubit\nstate one can perform tomography that works well for most observables by\nmeasuring only O(n) independent and identically distributed (i.i.d.) copies of\nthe state -- and relax the assumption of having i.i.d copies of the state to\nmerely the ability to select subsystems at random from a quantum multipartite\nstate.\n  The proofs of the new quantum de Finetti theorems are based on information\ntheory, in particular on the chain rule of mutual information. \n\n"}
{"id": "1210.6951", "contents": "Title: Higher dimensional distortion of random complexes Abstract: Using the random complexes of Linial and Meshulam, we exhibit a large family\nof simplicial complexes for which, whenever affinely embedded into Euclidean\nspace, the filling areas of simplicial cycles is greatly distorted. This\nphenomenon can be regarded as a higher order analogue of the metric distortion\nof embeddings of random graphs. \n\n"}
{"id": "1210.7605", "contents": "Title: List-coloring embedded graphs Abstract: For any fixed surface Sigma of genus g, we give an algorithm to decide\nwhether a graph G of girth at least five embedded in Sigma is colorable from an\nassignment of lists of size three in time O(|V(G)|). Furthermore, we can allow\na subgraph (of any size) with at most s components to be precolored, at the\nexpense of increasing the time complexity of the algorithm to\nO(|V(G)|^{K(g+s)+1}) for some absolute constant K; in both cases, the\nmultiplicative constant hidden in the O-notation depends on g and s. This also\nenables us to find such a coloring when it exists. The idea of the algorithm\ncan be applied to other similar problems, e.g., 5-list-coloring of graphs on\nsurfaces. \n\n"}
{"id": "1211.0616", "contents": "Title: The complexity of learning halfspaces using generalized linear methods Abstract: Many popular learning algorithms (E.g. Regression, Fourier-Transform based\nalgorithms, Kernel SVM and Kernel ridge regression) operate by reducing the\nproblem to a convex optimization problem over a vector space of functions.\nThese methods offer the currently best approach to several central problems\nsuch as learning half spaces and learning DNF's. In addition they are widely\nused in numerous application domains. Despite their importance, there are still\nvery few proof techniques to show limits on the power of these algorithms.\n  We study the performance of this approach in the problem of (agnostically and\nimproperly) learning halfspaces with margin $\\gamma$. Let $\\mathcal{D}$ be a\ndistribution over labeled examples. The $\\gamma$-margin error of a hyperplane\n$h$ is the probability of an example to fall on the wrong side of $h$ or at a\ndistance $\\le\\gamma$ from it. The $\\gamma$-margin error of the best $h$ is\ndenoted $\\mathrm{Err}_\\gamma(\\mathcal{D})$. An $\\alpha(\\gamma)$-approximation\nalgorithm receives $\\gamma,\\epsilon$ as input and, using i.i.d. samples of\n$\\mathcal{D}$, outputs a classifier with error rate $\\le\n\\alpha(\\gamma)\\mathrm{Err}_\\gamma(\\mathcal{D}) + \\epsilon$. Such an algorithm\nis efficient if it uses $\\mathrm{poly}(\\frac{1}{\\gamma},\\frac{1}{\\epsilon})$\nsamples and runs in time polynomial in the sample size.\n  The best approximation ratio achievable by an efficient algorithm is\n$O\\left(\\frac{1/\\gamma}{\\sqrt{\\log(1/\\gamma)}}\\right)$ and is achieved using an\nalgorithm from the above class. Our main result shows that the approximation\nratio of every efficient algorithm from this family must be $\\ge\n\\Omega\\left(\\frac{1/\\gamma}{\\mathrm{poly}\\left(\\log\\left(1/\\gamma\\right)\\right)}\\right)$,\nessentially matching the best known upper bound. \n\n"}
{"id": "1211.1149", "contents": "Title: Stochastic Combinatorial Optimization via Poisson Approximation Abstract: We study several stochastic combinatorial problems, including the expected\nutility maximization problem, the stochastic knapsack problem and the\nstochastic bin packing problem. A common technical challenge in these problems\nis to optimize some function of the sum of a set of random variables. The\ndifficulty is mainly due to the fact that the probability distribution of the\nsum is the convolution of a set of distributions, which is not an easy\nobjective function to work with. To tackle this difficulty, we introduce the\nPoisson approximation technique. The technique is based on the Poisson\napproximation theorem discovered by Le Cam, which enables us to approximate the\ndistribution of the sum of a set of random variables using a compound Poisson\ndistribution.\n  We first study the expected utility maximization problem introduced recently\n[Li and Despande, FOCS11]. For monotone and Lipschitz utility functions, we\nobtain an additive PTAS if there is a multidimensional PTAS for the\nmulti-objective version of the problem, strictly generalizing the previous\nresult.\n  For the stochastic bin packing problem (introduced in [Kleinberg, Rabani and\nTardos, STOC97]), we show there is a polynomial time algorithm which uses at\nmost the optimal number of bins, if we relax the size of each bin and the\noverflow probability by eps.\n  For stochastic knapsack, we show a 1+eps-approximation using eps extra\ncapacity, even when the size and reward of each item may be correlated and\ncancelations of items are allowed. This generalizes the previous work [Balghat,\nGoel and Khanna, SODA11] for the case without correlation and cancelation. Our\nalgorithm is also simpler. We also present a factor 2+eps approximation\nalgorithm for stochastic knapsack with cancelations. the current known\napproximation factor of 8 [Gupta, Krishnaswamy, Molinaro and Ravi, FOCS11]. \n\n"}
{"id": "1211.2944", "contents": "Title: On the optimality of the ideal right-angled 24-cell Abstract: We prove that among four-dimensional ideal right-angled hyperbolic polytopes\nthe 24-cell is of minimal volume and of minimal facet number. As a corollary, a\ndimension bound for ideal right-angled hyperbolic polytopes is obtained. \n\n"}
{"id": "1211.5350", "contents": "Title: Note on the Greedy Parsing Optimality for Dictionary-Based Text\n  Compression Abstract: Dynamic dictionary-based compression schemes are the most daily used data\ncompression schemes since they appeared in the foundational papers of Ziv and\nLempel in 1977, commonly referred to as LZ77. Their work is the base of\nDeflate, gZip, WinZip, 7Zip and many others compression software. All of those\ncompression schemes use variants of the greedy approach to parse the text into\ndictionary phrases. Greedy parsing optimality was proved by Cohn et al. (1996)\nfor fixed length code and unbounded dictionaries. The optimality of the greedy\nparsing was never proved for bounded size dictionary which actually all of\nthose schemes require. We define the suffix-closed property for dynamic\ndictionaries and we show that any LZ77-based dictionary, including the bounded\nvariants, satisfy this property. Under this condition we prove the optimality\nof the greedy parsing as a variant of the proof by Cohn et al. \n\n"}
{"id": "1211.5933", "contents": "Title: Interval Deletion is Fixed-Parameter Tractable Abstract: We study the minimum \\emph{interval deletion} problem, which asks for the\nremoval of a set of at most $k$ vertices to make a graph of $n$ vertices into\nan interval graph. We present a parameterized algorithm of runtime $10^k \\cdot\nn^{O(1)}$ for this problem, that is, we show the problem is fixed-parameter\ntractable. \n\n"}
{"id": "1211.6656", "contents": "Title: Subexponential and FPT-time Inapproximability of Independent Set and\n  Related Problems Abstract: Fixed-parameter algorithms, approximation algorithms and moderately\nexponential algorithms are three major approaches to algorithms design. While\neach of them being very active in its own, there is an increasing attention to\nthe connection between different approaches. In particular, whether Maximum\nIndependent Set would be better approximable once endowed with\nsubexponential-time or FPT-time is a central question. In this paper, we\npresent a strong link between the linear PCP conjecture and the\ninapproximability, thus partially answering this question. \n\n"}
{"id": "1212.0649", "contents": "Title: Optimal packings of congruent circles on a square flat torus Abstract: We consider packings of congruent circles on a square flat torus, i.e.,\nperiodic (w.r.t. a square lattice) planar circle packings, with the maximal\ncircle radius. This problem is interesting due to a practical reason - the\nproblem of \"super resolution of images.\" We have found optimal arrangements for\nN=6, 7 and 8 circles. Surprisingly, for the case N=7 there are three different\noptimal arrangements. Our proof is based on a computer enumeration of toroidal\nirreducible contact graphs. \n\n"}
{"id": "1212.0752", "contents": "Title: Parameters of Two-Prover-One-Round Game and The Hardness of Connectivity\n  Problems Abstract: Optimizing parameters of Two-Prover-One-Round Game (2P1R) is an important\ntask in PCPs literature as it would imply a smaller PCP with the same or\nstronger soundness. While this is a basic question in PCPs community, the\nconnection between the parameters of PCPs and hardness of approximations is\nsometime obscure to approximation algorithm community. In this paper, we\ninvestigate the connection between the parameters of 2P1R and the hardness of\napproximating the class of so-called connectivity problems, which includes as\nsubclasses the survivable network design and (multi)cut problems. Based on\nrecent development on 2P1R by Chan (ECCC 2011) and several techniques in PCPs\nliterature, we improve hardness results of some connectivity problems that are\nin the form $k^\\sigma$, for some (very) small constant $\\sigma>0$, to hardness\nresults of the form $k^c$ for some explicit constant $c$, where $k$ is a\nconnectivity parameter. In addition, we show how to convert these hardness into\nhardness results of the form $D^{c'}$, where $D$ is the number of demand pairs\n(or the number of terminals).\n  Thus, we give improved hardness results of k^{1/2-\\epsilon} and\nk^{1/10-\\epsilon} for the root $k$-connectivity problem on directed and\nundirected graphs, k^{1/6-\\epsilon} for the vertex-connectivity survivable\nnetwork design problem on undirected graphs, and k^{1/6-\\epsilon} for the\nvertex-connectivity $k$-route cut problem on undirected graphs. \n\n"}
{"id": "1212.2284", "contents": "Title: The Complexity of Planar Boolean #CSP with Complex Weights Abstract: We prove a complexity dichotomy theorem for symmetric complex-weighted\nBoolean #CSP when the constraint graph of the input must be planar. The\nproblems that are #P-hard over general graphs but tractable over planar graphs\nare precisely those with a holographic reduction to matchgates. This\ngeneralizes a theorem of Cai, Lu, and Xia for the case of real weights. We also\nobtain a dichotomy theorem for a symmetric arity 4 signature with complex\nweights in the planar Holant framework, which we use in the proof of our #CSP\ndichotomy. In particular, we reduce the problem of evaluating the Tutte\npolynomial of a planar graph at the point (3,3) to counting the number of\nEulerian orientations over planar 4-regular graphs to show the latter is\n#P-hard. This strengthens a theorem by Huang and Lu to the planar setting. Our\nproof techniques combine new ideas with refinements and extensions of existing\ntechniques. These include planar pairings, the recursive unary construction,\nthe anti-gadget technique, and pinning in the Hadamard basis. \n\n"}
{"id": "1212.3849", "contents": "Title: Every locally characterized affine-invariant property is testable Abstract: Let F = F_p for any fixed prime p >= 2. An affine-invariant property is a\nproperty of functions on F^n that is closed under taking affine transformations\nof the domain. We prove that all affine-invariant property having local\ncharacterizations are testable. In fact, we show a proximity-oblivious test for\nany such property P, meaning that there is a test that, given an input function\nf, makes a constant number of queries to f, always accepts if f satisfies P,\nand rejects with positive probability if the distance between f and P is\nnonzero. More generally, we show that any affine-invariant property that is\nclosed under taking restrictions to subspaces and has bounded complexity is\ntestable.\n  We also prove that any property that can be described as the property of\ndecomposing into a known structure of low-degree polynomials is locally\ncharacterized and is, hence, testable. For example, whether a function is a\nproduct of two degree-d polynomials, whether a function splits into a product\nof d linear polynomials, and whether a function has low rank are all examples\nof degree-structural properties and are therefore locally characterized.\n  Our results depend on a new Gowers inverse theorem by Tao and Ziegler for low\ncharacteristic fields that decomposes any polynomial with large Gowers norm\ninto a function of low-degree non-classical polynomials. We establish a new\nequidistribution result for high rank non-classical polynomials that drives the\nproofs of both the testability results and the local characterization of\ndegree-structural properties. \n\n"}
{"id": "1212.4372", "contents": "Title: Sliding Windows with Limited Storage Abstract: We consider time-space tradeoffs for exactly computing frequency moments and\norder statistics over sliding windows. Given an input of length 2n-1, the task\nis to output the function of each window of length n, giving n outputs in\ntotal. Computations over sliding windows are related to direct sum problems\nexcept that inputs to instances almost completely overlap.\n  We show an average case and randomized time-space tradeoff lower bound of TS\nin Omega(n^2) for multi-way branching programs, and hence standard RAM and\nword-RAM models, to compute the number of distinct elements, F_0, in sliding\nwindows over alphabet [n]. The same lower bound holds for computing the\nlow-order bit of F_0 and computing any frequency moment F_k for k not equal to\n1. We complement this lower bound with a TS in \\tilde O(n^2) deterministic RAM\nalgorithm for exactly computing F_k in sliding windows.\n  We show time-space separations between the complexity of sliding-window\nelement distinctness and that of sliding-window $F_0\\bmod 2$ computation. In\nparticular for alphabet [n] there is a very simple errorless sliding-window\nalgorithm for element distinctness that runs in O(n) time on average and uses\nO(log{n}) space.\n  We show that any algorithm for a single element distinctness instance can be\nextended to an algorithm for the sliding-window version of element distinctness\nwith at most a polylogarithmic increase in the time-space product.\n  Finally, we show that the sliding-window computation of order statistics such\nas the maximum and minimum can be computed with only a logarithmic increase in\ntime, but that a TS in Omega(n^2) lower bound holds for sliding-window\ncomputation of order statistics such as the median, a nearly linear increase in\ntime when space is small. \n\n"}
{"id": "1212.6925", "contents": "Title: Superlinear lower bounds for multipass graph processing Abstract: We prove $n^{1+\\Omega(1/p)}/p^{O(1)}$ lower bounds for the space complexity\nof $p$-pass streaming algorithms solving the following problems on $n$-vertex\ngraphs:\n  * testing if an undirected graph has a perfect matching (this implies lower\nbounds for computing a maximum matching or even just the maximum matching\nsize),\n  * testing if two specific vertices are at distance at most $2(p+1)$ in an\nundirected graph,\n  * testing if there is a directed path from $s$ to $t$ for two specific\nvertices $s$ and $t$ in a directed graph.\n  Prior to our result, it was known that these problems require $\\Omega(n^2)$\nspace in one pass, but no $n^{1+\\Omega(1)}$ lower bound was known for any $p\\ge\n2$.\n  These streaming results follow from a communication complexity lower bound\nfor a communication game in which the players hold two graphs on the same set\nof vertices. The task of the players is to find out whether the sets of\nvertices at distance exactly $p+1$ from a specific vertex intersect. The game\nrequires a significant amount of communication only if the players are forced\nto speak in a specific difficult order. This is reminiscent of lower bounds for\ncommunication problems such as indexing and pointer chasing. Among other\nthings, our line of attack requires proving an information cost lower bound for\na decision version of the classic pointer chasing problem and a direct sum type\ntheorem for the disjunction of several instances of this problem. \n\n"}
{"id": "1301.0820", "contents": "Title: Moment-Matching Polynomials Abstract: We give a new framework for proving the existence of low-degree, polynomial\napproximators for Boolean functions with respect to broad classes of\nnon-product distributions. Our proofs use techniques related to the classical\nmoment problem and deviate significantly from known Fourier-based methods,\nwhich require the underlying distribution to have some product structure.\n  Our main application is the first polynomial-time algorithm for agnostically\nlearning any function of a constant number of halfspaces with respect to any\nlog-concave distribution (for any constant accuracy parameter). This result was\nnot known even for the case of learning the intersection of two halfspaces\nwithout noise. Additionally, we show that in the \"smoothed-analysis\" setting,\nthe above results hold with respect to distributions that have sub-exponential\ntails, a property satisfied by many natural and well-studied distributions in\nmachine learning.\n  Given that our algorithms can be implemented using Support Vector Machines\n(SVMs) with a polynomial kernel, these results give a rigorous theoretical\nexplanation as to why many kernel methods work so well in practice. \n\n"}
{"id": "1301.1547", "contents": "Title: Short lists with short programs in short time Abstract: Given a machine $U$, a $c$-short program for $x$ is a string $p$ such that\n$U(p)=x$ and the length of $p$ is bounded by $c$ + (the length of a shortest\nprogram for $x$). We show that for any standard Turing machine, it is possible\nto compute in polynomial time on input $x$ a list of polynomial size guaranteed\nto contain a O$(\\log |x|)$-short program for $x$. We also show that there\nexists a computable function that maps every $x$ to a list of size $|x|^2$\ncontaining a O$(1)$-short program for $x$. This is essentially optimal because\nwe prove that for each such function there is a $c$ and infinitely many $x$ for\nwhich the list has size at least $c|x|^2$. Finally we show that for some\nstandard machines, computable functions generating lists with $0$-short\nprograms, must have infinitely often list sizes proportional to $2^{|x|}$. \n\n"}
{"id": "1301.3780", "contents": "Title: Bounds on the Size of Sound Monotone Switching Networks Accepting\n  Permutation Sets of Directed Trees Abstract: In this paper, we prove almost tight bounds on the size of sound monotone\nswitching networks accepting permutations sets of directed trees. This roughly\ncorresponds to proving almost tight bounds bounds on the monotone memory\nefficiency of the directed ST-connectivity problem for the special case in\nwhich the input graph is guaranteed to have no path from s to t or be\nisomorphic to a specific directed tree. \n\n"}
{"id": "1301.5584", "contents": "Title: Improved Cheeger's Inequality: Analysis of Spectral Partitioning\n  Algorithms through Higher Order Spectral Gap Abstract: Let \\phi(G) be the minimum conductance of an undirected graph G, and let\n0=\\lambda_1 <= \\lambda_2 <=... <= \\lambda_n <= 2 be the eigenvalues of the\nnormalized Laplacian matrix of G. We prove that for any graph G and any k >= 2,\n  \\phi(G) = O(k) \\lambda_2 / \\sqrt{\\lambda_k}, and this performance guarantee\nis achieved by the spectral partitioning algorithm. This improves Cheeger's\ninequality, and the bound is optimal up to a constant factor for any k. Our\nresult shows that the spectral partitioning algorithm is a constant factor\napproximation algorithm for finding a sparse cut if \\lambda_k$ is a constant\nfor some constant k. This provides some theoretical justification to its\nempirical performance in image segmentation and clustering problems. We extend\nthe analysis to other graph partitioning problems, including multi-way\npartition, balanced separator, and maximum cut. \n\n"}
{"id": "1302.0892", "contents": "Title: Search using queries on indistinguishable items Abstract: We investigate the problem of determining a set S of k indistinguishable\nintegers in the range [1,n]. The algorithm is allowed to query an integer $q\\in\n[1,n]$, and receive a response comparing this integer to an integer randomly\nchosen from S. The algorithm has no control over which element of S the query q\nis compared to. We show tight bounds for this problem. In particular, we show\nthat in the natural regime where $k\\le n$, the optimal number of queries to\nattain $n^{-\\Omega(1)}$ error probability is $\\Theta(k^3 \\log n)$. In the\nregime where $k>n$, the optimal number of queries is $\\Theta(n^2 k \\log n)$.\n  Our main technical tools include the use of information theory to derive the\nlower bounds, and the application of noisy binary search in the spirit of\nFeige, Raghavan, Peleg, and Upfal (1994). In particular, our lower bound\ntechnique is likely to be applicable in other situations that involve search\nunder uncertainty. \n\n"}
{"id": "1302.1232", "contents": "Title: When are the most informative components for inference also the\n  principal components? Abstract: Which components of the singular value decomposition of a signal-plus-noise\ndata matrix are most informative for the inferential task of detecting or\nestimating an embedded low-rank signal matrix? Principal component analysis\nascribes greater importance to the components that capture the greatest\nvariation, i.e., the singular vectors associated with the largest singular\nvalues. This choice is often justified by invoking the Eckart-Young theorem\neven though that work addresses the problem of how to best represent a\nsignal-plus-noise matrix using a low-rank approximation and not how to\nbest_infer_ the underlying low-rank signal component.\n  Here we take a first-principles approach in which we start with a\nsignal-plus-noise data matrix and show how the spectrum of the noise-only\ncomponent governs whether the principal or the middle components of the\nsingular value decomposition of the data matrix will be the informative\ncomponents for inference. Simply put, if the noise spectrum is supported on a\nconnected interval, in a sense we make precise, then the use of the principal\ncomponents is justified. When the noise spectrum is supported on multiple\nintervals, then the middle components might be more informative than the\nprincipal components.\n  The end result is a proper justification of the use of principal components\nin the setting where the noise matrix is i.i.d. Gaussian and the identification\nof scenarios, generically involving heterogeneous noise models such as mixtures\nof Gaussians, where the middle components might be more informative than the\nprincipal components so that they may be exploited to extract additional\nprocessing gain. Our results show how the blind use of principal components can\nlead to suboptimal or even faulty inference because of phase transitions that\nseparate a regime where the principal components are informative from a regime\nwhere they are uninformative. \n\n"}
{"id": "1302.1329", "contents": "Title: Injective hulls of odd cycles Abstract: The injective hulls of odd cycles are described explicitly. \n\n"}
{"id": "1302.2787", "contents": "Title: Acquaintance Time of a Graph Abstract: We define the following parameter of connected graphs. For a given graph $G$\nwe place one agent in each vertex of $G$. Every pair of agents sharing a common\nedge is declared to be acquainted. In each round we choose some matching of $G$\n(not necessarily a maximal matching), and for each edge in the matching the\nagents on this edge swap places. After the swap, again, every pair of agents\nsharing a common edge become acquainted, and the process continues. We define\nthe \\emph{acquaintance time} of a graph $G$, denoted by $AC(G)$, to be the\nminimal number of rounds required until every two agents are acquainted.\n  We first study the acquaintance time for some natural families of graphs\nincluding the path, expanders, the binary tree, and the complete bipartite\ngraph. We also show that for all positive integers $n$ and $k \\leq n^{1.5}$\nthere exists an $n$-vertex graph $G$ such that $AC(G) =\\Theta(k)$. We also\nprove that for all $n$-vertex connected graphs $G$ we have $AC(G) =\nO\\left(\\frac{n^2}{\\log(n)/\\log\\log(n)}\\right)$, improving the $O(n^2)$ trivial\nupper bound achieved by sequentially letting each agent perform depth-first\nsearch along a spanning tree of $G$.\n  Studying the computational complexity of this problem, we prove that for any\nconstant $t \\geq 1$ the problem of deciding that a given graph $G$ has $AC(G)\n\\leq t$ or $AC(G) \\geq 2t$ is $\\mathcal{NP}$-complete. That is, $AC(G)$ is\n$\\mathcal{NP}$-hard to approximate within multiplicative factor of 2, as well\nas within any additive constant factor.\n  On the algorithmic side, we give a deterministic algorithm that given a graph\n$G$ with $AC(G)=1$ finds a ${\\lceil n/c\\rceil}$-rounds strategy for\nacquaintance in time $n^{c+O(1)}$. We also design a randomized polynomial time\nalgorithm that given a graph $G$ with $AC(G)=1$ finds with high probability an\n$O(\\log(n))$-rounds strategy for acquaintance. \n\n"}
{"id": "1302.3496", "contents": "Title: On Polynomial Kernels for Integer Linear Programs: Covering, Packing and\n  Feasibility Abstract: We study the existence of polynomial kernels for the problem of deciding\nfeasibility of integer linear programs (ILPs), and for finding good solutions\nfor covering and packing ILPs. Our main results are as follows: First, we show\nthat the ILP Feasibility problem admits no polynomial kernelization when\nparameterized by both the number of variables and the number of constraints,\nunless NP \\subseteq coNP/poly. This extends to the restricted cases of bounded\nvariable degree and bounded number of variables per constraint, and to covering\nand packing ILPs. Second, we give a polynomial kernelization for the Cover ILP\nproblem, asking for a solution to Ax >= b with c^Tx <= k, parameterized by k,\nwhen A is row-sparse; this generalizes a known polynomial kernelization for the\nspecial case with 0/1-variables and coefficients (d-Hitting Set). \n\n"}
{"id": "1302.4587", "contents": "Title: Efficient Parallel and External Matching Abstract: We show that a simple algorithm for computing a matching on a graph runs in a\nlogarithmic number of phases incurring work linear in the input size. The\nalgorithm can be adapted to provide efficient algorithms in several models of\ncomputation, such as PRAM, External Memory, MapReduce and distributed memory\nmodels. Our CREW PRAM algorithm is the first O(log^2 n) time, linear work\nalgorithm. Our experimental results indicate the algorithm's high speed and\nefficiency combined with good solution quality. \n\n"}
{"id": "1302.5382", "contents": "Title: Reversible Logic Synthesis by Quantum Rotation Gates Abstract: A rotation-based synthesis framework for reversible logic is proposed. We\ndevelop a canonical representation based on binary decision diagrams and\nintroduce operators to manipulate the developed representation model.\nFurthermore, a recursive functional bi-decomposition approach is proposed to\nautomatically synthesize a given function. While Boolean reversible logic is\nparticularly addressed, our framework constructs intermediate quantum states\nthat may be in superposition, hence we combine techniques from reversible\nBoolean logic and quantum computation. The proposed approach results in\nquadratic gate count for multiple-control Toffoli gates without ancillae,\nlinear depth for quantum carry-ripple adder, and quasilinear size for quantum\nmultiplexer. \n\n"}
{"id": "1302.5843", "contents": "Title: Ising formulations of many NP problems Abstract: We provide Ising formulations for many NP-complete and NP-hard problems,\nincluding all of Karp's 21 NP-complete problems. This collects and extends\nmappings to the Ising model from partitioning, covering and satisfiability. In\neach case, the required number of spins is at most cubic in the size of the\nproblem. This work may be useful in designing adiabatic quantum optimization\nalgorithms. \n\n"}
{"id": "1303.2981", "contents": "Title: On the Complexity of the Orbit Problem Abstract: We consider higher-dimensional versions of Kannan and Lipton's Orbit\nProblem---determining whether a target vector space V may be reached from a\nstarting point x under repeated applications of a linear transformation A.\nAnswering two questions posed by Kannan and Lipton in the 1980s, we show that\nwhen V has dimension one, this problem is solvable in polynomial time, and when\nV has dimension two or three, the problem is in NP^{RP}. \n\n"}
{"id": "1303.5865", "contents": "Title: Arithmetic of triangles Abstract: The set of segments, each of the next is n times bigger than the first one is\na simple geometric interpretation of the set $\\mathbb{N}$ of natural numbers.\nIn this paper we investigate the opposite situation. We give algebraic\nstructure to the set of similar triangles with parallel sides. We present\ngeometric construction of adding of triangles and use it to dissection of\ntriangles into 15 triangles of different sides. \n\n"}
{"id": "1303.6872", "contents": "Title: Order-Preserving Suffix Trees and Their Algorithmic Applications Abstract: Recently Kubica et al. (Inf. Process. Let., 2013) and Kim et al. (submitted\nto Theor. Comp. Sci.) introduced order-preserving pattern matching. In this\nproblem we are looking for consecutive substrings of the text that have the\nsame \"shape\" as a given pattern. These results include a linear-time\norder-preserving pattern matching algorithm for polynomially-bounded alphabet\nand an extension of this result to pattern matching with multiple patterns. We\nmake one step forward in the analysis and give an\n$O(\\frac{n\\log{n}}{\\log\\log{n}})$ time randomized algorithm constructing suffix\ntrees in the order-preserving setting. We show a number of applications of\norder-preserving suffix trees to identify patterns and repetitions in time\nseries. \n\n"}
{"id": "1304.0371", "contents": "Title: On the Structure of Boolean Functions with Small Spectral Norm Abstract: In this paper we prove results regarding Boolean functions with small\nspectral norm (the spectral norm of f is\n$\\|\\hat{f}\\|_1=\\sum_{\\alpha}|\\hat{f}(\\alpha)|$). Specifically, we prove the\nfollowing results for functions $f:\\{0,1\\}^n \\to \\{0,1\\}$ with\n$\\|\\hat{f}\\|_1=A$.\n  1. There is a subspace $V$ of co-dimension at most $A^2$ such that $f|_V$ is\nconstant.\n  2. f can be computed by a parity decision tree of size $2^{A^2}n^{2A}$. (a\nparity decision tree is a decision tree whose nodes are labeled with arbitrary\nlinear functions.)\n  3. If in addition f has at most s nonzero Fourier coefficients, then f can be\ncomputed by a parity decision tree of depth $A^2 \\log s$.\n  4. For every $0<\\epsilon$ there is a parity decision tree of depth $O(A^2 +\n\\log(1/\\epsilon))$ and size $2^{O(A^2)} \\cdot\n\\min\\{1/\\epsilon^2,O(\\log(1/\\epsilon))^{2A}\\}$ that \\epsilon-approximates f.\nFurthermore, this tree can be learned, with probability $1-\\delta$, using\n$\\poly(n,\\exp(A^2),1/\\epsilon,\\log(1/\\delta))$ membership queries.\n  All the results above also hold (with a slight change in parameters) to\nfunctions $f:Z_p^n\\to \\{0,1\\}$. \n\n"}
{"id": "1304.0419", "contents": "Title: Top-K Product Design Based on Collaborative Tagging Data Abstract: The widespread use and popularity of collaborative content sites (e.g., IMDB,\nAmazon, Yelp, etc.) has created rich resources for users to consult in order to\nmake purchasing decisions on various products such as movies, e-commerce\nproducts, restaurants, etc. Products with desirable tags (e.g., modern,\nreliable, etc.) have higher chances of being selected by prospective customers.\nThis creates an opportunity for product designers to design better products\nthat are likely to attract desirable tags when published. In this paper, we\ninvestigate how to mine collaborative tagging data to decide the attribute\nvalues of new products and to return the top-k products that are likely to\nattract the maximum number of desirable tags when published. Given a training\nset of existing products with their features and user-submitted tags, we first\nbuild a Naive Bayes Classifier for each tag. We show that the problem of is\nNP-complete even if simple Naive Bayes Classifiers are used for tag prediction.\nWe present a suite of algorithms for solving this problem: (a) an exact two\ntier algorithm(based on top-k querying techniques), which performs much better\nthan the naive brute-force algorithm and works well for moderate problem\ninstances, and (b) a set of approximation algorithms for larger problem\ninstances: a novel polynomial-time approximation algorithm with provable error\nbound and a practical hill-climbing heuristic. We conduct detailed experiments\non synthetic and real data crawled from the web to evaluate the efficiency and\nquality of our proposed algorithms, as well as show how product designers can\nbenefit by leveraging collaborative tagging information. \n\n"}
{"id": "1304.0552", "contents": "Title: Performance of the Metropolis algorithm on a disordered tree: The\n  Einstein relation Abstract: Consider a $d$-ary rooted tree ($d\\geq3$) where each edge $e$ is assigned an\ni.i.d. (bounded) random variable $X(e)$ of negative mean. Assign to each vertex\n$v$ the sum $S(v)$ of $X(e)$ over all edges connecting $v$ to the root, and\nassume that the maximum $S_n^*$ of $S(v)$ over all vertices $v$ at distance $n$\nfrom the root tends to infinity (necessarily, linearly) as $n$ tends to\ninfinity. We analyze the Metropolis algorithm on the tree and show that under\nthese assumptions there always exists a temperature $1/\\beta$ of the algorithm\nso that it achieves a linear (positive) growth rate in linear time. This\nconfirms a conjecture of Aldous [Algorithmica 22 (1998) 388-412]. The proof is\nobtained by establishing an Einstein relation for the Metropolis algorithm on\nthe tree. \n\n"}
{"id": "1304.0730", "contents": "Title: Representation, Approximation and Learning of Submodular Functions Using\n  Low-rank Decision Trees Abstract: We study the complexity of approximate representation and learning of\nsubmodular functions over the uniform distribution on the Boolean hypercube\n$\\{0,1\\}^n$. Our main result is the following structural theorem: any\nsubmodular function is $\\epsilon$-close in $\\ell_2$ to a real-valued decision\ntree (DT) of depth $O(1/\\epsilon^2)$. This immediately implies that any\nsubmodular function is $\\epsilon$-close to a function of at most\n$2^{O(1/\\epsilon^2)}$ variables and has a spectral $\\ell_1$ norm of\n$2^{O(1/\\epsilon^2)}$. It also implies the closest previous result that states\nthat submodular functions can be approximated by polynomials of degree\n$O(1/\\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved by\nconstructing an approximation of a submodular function by a DT of rank\n$4/\\epsilon^2$ and a proof that any rank-$r$ DT can be $\\epsilon$-approximated\nby a DT of depth $\\frac{5}{2}(r+\\log(1/\\epsilon))$.\n  We show that these structural results can be exploited to give an\nattribute-efficient PAC learning algorithm for submodular functions running in\ntime $\\tilde{O}(n^2) \\cdot 2^{O(1/\\epsilon^{4})}$. The best previous algorithm\nfor the problem requires $n^{O(1/\\epsilon^{2})}$ time and examples (Cheraghchi\net al., 2012) but works also in the agnostic setting. In addition, we give\nimproved learning algorithms for a number of related settings.\n  We also prove that our PAC and agnostic learning algorithms are essentially\noptimal via two lower bounds: (1) an information-theoretic lower bound of\n$2^{\\Omega(1/\\epsilon^{2/3})}$ on the complexity of learning monotone\nsubmodular functions in any reasonable model; (2) computational lower bound of\n$n^{\\Omega(1/\\epsilon^{2/3})}$ based on a reduction to learning of sparse\nparities with noise, widely-believed to be intractable. These are the first\nlower bounds for learning of submodular functions over the uniform\ndistribution. \n\n"}
{"id": "1304.1467", "contents": "Title: Dimension Independent Matrix Square using MapReduce Abstract: We compute the singular values of an $m \\times n$ sparse matrix $A$ in a\ndistributed setting, without communication dependence on $m$, which is useful\nfor very large $m$. In particular, we give a simple nonadaptive sampling scheme\nwhere the singular values of $A$ are estimated within relative error with\nconstant probability. Our proven bounds focus on the MapReduce framework, which\nhas become the de facto tool for handling such large matrices that cannot be\nstored or even streamed through a single machine.\n  On the way, we give a general method to compute $A^TA$. We preserve singular\nvalues of $A^TA$ with $\\epsilon$ relative error with shuffle size\n$O(n^2/\\epsilon^2)$ and reduce-key complexity $O(n/\\epsilon^2)$. We further\nshow that if only specific entries of $A^TA$ are required and $A$ has\nnonnegative entries, then we can reduce the shuffle size to $O(n \\log(n) / s)$\nand reduce-key complexity to $O(\\log(n)/s)$, where $s$ is the minimum cosine\nsimilarity for the entries being estimated. All of our bounds are independent\nof $m$, the larger dimension. We provide open-source implementations in Spark\nand Scalding, along with experiments in an industrial setting. \n\n"}
{"id": "1304.2079", "contents": "Title: Learning Coverage Functions and Private Release of Marginals Abstract: We study the problem of approximating and learning coverage functions. A\nfunction $c: 2^{[n]} \\rightarrow \\mathbf{R}^{+}$ is a coverage function, if\nthere exists a universe $U$ with non-negative weights $w(u)$ for each $u \\in U$\nand subsets $A_1, A_2, \\ldots, A_n$ of $U$ such that $c(S) = \\sum_{u \\in\n\\cup_{i \\in S} A_i} w(u)$. Alternatively, coverage functions can be described\nas non-negative linear combinations of monotone disjunctions. They are a\nnatural subclass of submodular functions and arise in a number of applications.\n  We give an algorithm that for any $\\gamma,\\delta>0$, given random and uniform\nexamples of an unknown coverage function $c$, finds a function $h$ that\napproximates $c$ within factor $1+\\gamma$ on all but $\\delta$-fraction of the\npoints in time $poly(n,1/\\gamma,1/\\delta)$. This is the first fully-polynomial\nalgorithm for learning an interesting class of functions in the demanding PMAC\nmodel of Balcan and Harvey (2011). Our algorithms are based on several new\nstructural properties of coverage functions. Using the results in (Feldman and\nKothari, 2014), we also show that coverage functions are learnable agnostically\nwith excess $\\ell_1$-error $\\epsilon$ over all product and symmetric\ndistributions in time $n^{\\log(1/\\epsilon)}$. In contrast, we show that,\nwithout assumptions on the distribution, learning coverage functions is at\nleast as hard as learning polynomial-size disjoint DNF formulas, a class of\nfunctions for which the best known algorithm runs in time\n$2^{\\tilde{O}(n^{1/3})}$ (Klivans and Servedio, 2004).\n  As an application of our learning results, we give simple\ndifferentially-private algorithms for releasing monotone conjunction counting\nqueries with low average error. In particular, for any $k \\leq n$, we obtain\nprivate release of $k$-way marginals with average error $\\bar{\\alpha}$ in time\n$n^{O(\\log(1/\\bar{\\alpha}))}$. \n\n"}
{"id": "1304.3812", "contents": "Title: Time-Optimal Interactive Proofs for Circuit Evaluation Abstract: Recently, researchers have been working toward the development of practical\ngeneral-purpose protocols for verifiable computation. These protocols enable a\ncomputationally weak verifier to offload computations to a powerful but\nuntrusted prover, while providing the verifier with a guarantee that the prover\nperformed the computations correctly. Despite substantial progress, existing\nimplementations are not yet practical. The main bottleneck is typically the\nextra effort required by the prover to return an answer with a guarantee of\ncorrectness, compared to returning an answer with no guarantee.\n  We describe a refinement of a powerful interactive proof protocol originally\ndue to Goldwasser, Kalai, and Rothblum. Cormode, Mitzenmacher, and Thaler show\nhow to implement the prover in this protocol in time O(S log S), where S is the\nsize of an arithmetic circuit computing the function of interest. Our\nrefinements apply to circuits whose wiring pattern is sufficiently \"regular\";\nfor these circuits, we bring the runtime of the prover down to O(S). That is,\nour prover can evaluate the circuit with a guarantee of correctness, with only\na constant-factor blowup in work compared to evaluating the circuit with no\nguarantee.\n  We argue that our refinements capture a large class of circuits, and prove\nsome theorems formalizing this. Experimentally, our refinements yield a 200x\nspeedup for the prover over the implementation of Cormode et al., and our\nprover is less than 10x slower than a C++ program that simply evaluates the\ncircuit. Along the way, we describe a special-purpose protocol for matrix\nmultiplication that is of interest in its own right.\n  Our final contribution is a protocol targeted at general data parallel\ncomputation. Compared to prior work, this protocol can more efficiently verify\ncomplicated computations as long as that computation is applied independently\nto many pieces of data. \n\n"}
{"id": "1304.3816", "contents": "Title: Annotations for Sparse Data Streams Abstract: Motivated by cloud computing, a number of recent works have studied annotated\ndata streams and variants thereof. In this setting, a computationally weak\nverifier (cloud user), lacking the resources to store and manipulate his\nmassive input locally, accesses a powerful but untrusted prover (cloud\nservice). The verifier must work within the restrictive data streaming\nparadigm. The prover, who can annotate the data stream as it is read, must not\njust supply the answer but also convince the verifier of its correctness.\nIdeally, both the amount of annotation and the space used by the verifier\nshould be sublinear in the relevant input size parameters.\n  A rich theory of such algorithms -- which we call schemes -- has emerged.\nPrior work has shown how to leverage the prover's power to efficiently solve\nproblems that have no non-trivial standard data stream algorithms. However,\nwhile optimal schemes are now known for several basic problems, such optimality\nholds only for streams whose length is commensurate with the size of the data\nuniverse. In contrast, many real-world datasets are relatively sparse,\nincluding graphs that contain only O(n^2) edges, and IP traffic streams that\ncontain much fewer than the total number of possible IP addresses, 2^128 in\nIPv6.\n  We design the first schemes that allow both the annotation and the space\nusage to be sublinear in the total number of stream updates rather than the\nsize of the data universe. We solve significant problems, including variations\nof INDEX, SET-DISJOINTNESS, and FREQUENCY-MOMENTS, plus several natural\nproblems on graphs. On the other hand, we give a new lower bound that, for the\nfirst time, rules out smooth tradeoffs between annotation and space usage for a\nspecific problem. Our technique brings out new nuances in Merlin-Arthur\ncommunication complexity models, and provides a separation between online\nversions of the MA and AMA models. \n\n"}
{"id": "1304.4321", "contents": "Title: Polar Codes: Speed of polarization and polynomial gap to capacity Abstract: We prove that, for all binary-input symmetric memoryless channels, polar\ncodes enable reliable communication at rates within $\\epsilon > 0$ of the\nShannon capacity with a block length, construction complexity, and decoding\ncomplexity all bounded by a {\\em polynomial} in $1/\\epsilon$. Polar coding\ngives the {\\em first known explicit construction} with rigorous proofs of all\nthese properties; previous constructions were not known to achieve capacity\nwith less than $\\exp(1/\\epsilon)$ decoding complexity except for erasure\nchannels.\n  We establish the capacity-achieving property of polar codes via a direct\nanalysis of the underlying martingale of conditional entropies, without relying\non the martingale convergence theorem. This step gives rough polarization\n(noise levels $\\approx \\epsilon$ for the \"good\" channels), which can then be\nadequately amplified by tracking the decay of the channel Bhattacharyya\nparameters. Our effective bounds imply that polar codes can have block length\n(and encoding/decoding complexity) bounded by a polynomial in $1/\\epsilon$. The\ngenerator matrix of such polar codes can be constructed in polynomial time by\nalgorithmically computing an adequate approximation of the polarization\nprocess. \n\n"}
{"id": "1304.5429", "contents": "Title: A note on the complexity of comparing succinctly represented integers,\n  with an application to maximum probability parsing Abstract: The following two decision problems capture the complexity of comparing\nintegers or rationals that are succinctly represented in\nproduct-of-exponentials notation, or equivalently, via arithmetic circuits\nusing only multiplication and division gates, and integer inputs:\n  Input instance: four lists of positive integers: a_1, ...., a_n ; b_1,....,\nb_n ; c_1,....,c_m ; d_1, ...., d_m ; where each of the integers is represented\nin binary.\n  Problem 1 (equality testing): Decide whether a_1^{b_1} a_2^{b_2} ....\na_n^{b_n} = c_1^{d_1} c_2^{d_2} .... c_m^{d_m} .\n  Problem 2 (inequality testing): Decide whether a_1^{b_1} a_2^{b_2} ...\na_n^{b_n} >= c_1^{d_1} c_2^{d_2} .... c_m^{d_m} .\n  Problem 1 is easily decidable in polynomial time using a simple iterative\nalgorithm. Problem 2 is much harder. We observe that the complexity of Problem\n2 is intimately connected to deep conjectures and results in number theory. In\nparticular, if a refined form of the ABC conjecture formulated by Baker in 1998\nholds, or if the older Lang-Waldschmidt conjecture (formulated in 1978) on\nlinear forms in logarithms holds, then Problem 2 is decidable in P-time (in the\nstandard Turing model of computation). Moreover, it follows from the best\navailable quantitative bounds on linear forms in logarithms, e.g., by Baker and\nW\\\"{u}stholz (1993) or Matveev (2000), that if m and n are fixed universal\nconstants then Problem 2 is decidable in P-time (without relying on any\nconjectures). This latter fact was observed earlier by Shub (1993).\n  We describe one application: P-time maximum probability parsing for arbitrary\nstochastic context-free grammars (where \\epsilon-rules are allowed). \n\n"}
{"id": "1304.5719", "contents": "Title: Synchronous Counting and Computational Algorithm Design Abstract: Consider a complete communication network on $n$ nodes, each of which is a\nstate machine. In synchronous 2-counting, the nodes receive a common clock\npulse and they have to agree on which pulses are \"odd\" and which are \"even\". We\nrequire that the solution is self-stabilising (reaching the correct operation\nfrom any initial state) and it tolerates $f$ Byzantine failures (nodes that\nsend arbitrary misinformation). Prior algorithms are expensive to implement in\nhardware: they require a source of random bits or a large number of states.\n  This work consists of two parts. In the first part, we use computational\ntechniques (often known as synthesis) to construct very compact deterministic\nalgorithms for the first non-trivial case of $f = 1$. While no algorithm exists\nfor $n < 4$, we show that as few as 3 states per node are sufficient for all\nvalues $n \\ge 4$. Moreover, the problem cannot be solved with only 2 states per\nnode for $n = 4$, but there is a 2-state solution for all values $n \\ge 6$.\n  In the second part, we develop and compare two different approaches for\nsynthesising synchronous counting algorithms. Both approaches are based on\ncasting the synthesis problem as a propositional satisfiability (SAT) problem\nand employing modern SAT-solvers. The difference lies in how to solve the SAT\nproblem: either in a direct fashion, or incrementally within a counter-example\nguided abstraction refinement loop. Empirical results suggest that the former\ntechnique is more efficient if we want to synthesise time-optimal algorithms,\nwhile the latter technique discovers non-optimal algorithms more quickly. \n\n"}
{"id": "1304.5773", "contents": "Title: Graph isomorphism and adiabatic quantum computing Abstract: In the Graph Isomorphism problem two N-vertex graphs G and G' are given and\nthe task is to determine whether there exists a permutation of the vertices of\nG that preserves adjacency and transforms G into G'. If yes, then G and G' are\nsaid to be isomorphic; otherwise they are non-isomorphic. The GI problem is an\nimportant problem in computer science and is thought to be of comparable\ndifficulty to integer factorization. In this paper we present a quantum\nalgorithm that solves arbitrary instances of GI and can also determine all\nautomorphisms of a given graph. We show how the GI problem can be converted to\na combinatorial optimization problem that can be solved using adiabatic quantum\nevolution. We numerically simulate the algorithm's quantum dynamics and show\nthat it correctly: (i) distinguishes non-isomorphic graphs; (ii) recognizes\nisomorphic graphs; and (iii) finds all automorphisms of a given graph G. We\nthen discuss the GI quantum algorithm's experimental implementation, and close\nby showing how it can be leveraged to give a quantum algorithm that solves\narbitrary instances of the NP-Complete Sub-Graph Isomorphism problem. \n\n"}
{"id": "1304.5808", "contents": "Title: Improved Complexity Results on $k$-Coloring $P_t$-Free Graphs Abstract: A graph is $H$-free if it does not contain an induced subgraph isomorphic to\n$H$. We denote by $P_k$ and $C_k$ the path and the cycle on $k$ vertices,\nrespectively. In this paper, we prove that 4-COLORING is NP-complete for\n$P_7$-free graphs, and that 5-COLORING is NP-complete for $P_6$-free graphs.\nThese two results improve all previous results on $k$-coloring $P_t$-free\ngraphs, and almost complete the classification of complexity of $k$-COLORING\n$P_t$-free graphs for $k\\ge 4$ and $t\\ge 1$, leaving as the only missing case\n4-COLORING $P_6$-free graphs. We expect that 4-COLORING is polynomial time\nsolvable for $P_6$-free graphs; in support of this, we describe a polynomial\ntime algorithm for 4-COLORING $P_6$-free graphs which are also $P$-free, where\n$P$ is the graph obtained from $C_4$ by adding a new vertex and making it\nadjacent to exactly one vertex on the $C_4$. \n\n"}
{"id": "1304.6321", "contents": "Title: A O(c^k n) 5-Approximation Algorithm for Treewidth Abstract: We give an algorithm that for an input n-vertex graph G and integer k>0, in\ntime 2^[O(k)]n either outputs that the treewidth of G is larger than k, or\ngives a tree decomposition of G of width at most 5k+4. This is the first\nalgorithm providing a constant factor approximation for treewidth which runs in\ntime single-exponential in k and linear in n. Treewidth based computations are\nsubroutines of numerous algorithms. Our algorithm can be used to speed up many\nsuch algorithms to work in time which is single-exponential in the treewidth\nand linear in the input size. \n\n"}
{"id": "1304.7235", "contents": "Title: Finding Short Paths on Polytopes by the Shadow Vertex Algorithm Abstract: We show that the shadow vertex algorithm can be used to compute a short path\nbetween a given pair of vertices of a polytope P = {x : Ax \\leq b} along the\nedges of P, where A \\in R^{m \\times n} is a real-valued matrix. Both, the\nlength of the path and the running time of the algorithm, are polynomial in m,\nn, and a parameter 1/delta that is a measure for the flatness of the vertices\nof P. For integer matrices A \\in Z^{m \\times n} we show a connection between\ndelta and the largest absolute value Delta of any sub-determinant of A,\nyielding a bound of O(Delta^4 m n^4) for the length of the computed path. This\nbound is expressed in the same parameter Delta as the recent non-constructive\nbound of O(Delta^2 n^4 \\log (n Delta)) by Bonifas et al.\n  For the special case of totally unimodular matrices, the length of the\ncomputed path simplifies to O(m n^4), which significantly improves the\npreviously best known constructive bound of O(m^{16} n^3 \\log^3(mn)) by Dyer\nand Frieze. \n\n"}
{"id": "1305.0087", "contents": "Title: Quantile Regression for Large-scale Applications Abstract: Quantile regression is a method to estimate the quantiles of the conditional\ndistribution of a response variable, and as such it permits a much more\naccurate portrayal of the relationship between the response variable and\nobserved covariates than methods such as Least-squares or Least Absolute\nDeviations regression. It can be expressed as a linear program, and, with\nappropriate preprocessing, interior-point methods can be used to find a\nsolution for moderately large problems. Dealing with very large problems,\n\\emph(e.g.), involving data up to and beyond the terabyte regime, remains a\nchallenge. Here, we present a randomized algorithm that runs in nearly linear\ntime in the size of the input and that, with constant probability, computes a\n$(1+\\epsilon)$ approximate solution to an arbitrary quantile regression\nproblem. As a key step, our algorithm computes a low-distortion\nsubspace-preserving embedding with respect to the loss function of quantile\nregression. Our empirical evaluation illustrates that our algorithm is\ncompetitive with the best previous work on small to medium-sized problems, and\nthat in addition it can be implemented in MapReduce-like environments and\napplied to terabyte-sized problems. \n\n"}
{"id": "1305.1021", "contents": "Title: Parameterized Quantum Query Complexity of Graph Collision Abstract: We present three new quantum algorithms in the quantum query model for\n\\textsc{graph-collision} problem: \\begin{itemize} \\item an algorithm based on\ntree decomposition that uses $O\\left(\\sqrt{n}t^{\\sfrac{1}{6}}\\right)$ queries\nwhere $t$ is the treewidth of the graph; \\item an algorithm constructed on a\nspan program that improves a result by Gavinsky and Ito. The algorithm uses\n$O(\\sqrt{n}+\\sqrt{\\alpha^{**}})$ queries, where $\\alpha^{**}(G)$ is a graph\nparameter defined by \\[\\alpha^{**}(G):=\\min_{VC\\text{-- vertex cover\nof}G}{\\max_{\\substack{I\\subseteq VC\\\\I\\text{-- independent set}}}{\\sum_{v\\in\nI}{\\deg{v}}}};\\] \\item an algorithm for a subclass of circulant graphs that\nuses $O(\\sqrt{n})$ queries. \\end{itemize} We also present an example of a\npossibly difficult graph $G$ for which all the known graphs fail to solve graph\ncollision in $O(\\sqrt{n} \\log^c n)$ queries. \n\n"}
{"id": "1305.1327", "contents": "Title: Classical and Quantum Algorithms for Testing Equivalence of Group\n  Extensions Abstract: While efficient algorithms are known for solving many important problems\nrelated to groups, no efficient algorithm is known for determining whether two\narbitrary groups are isomorphic. The particular case of 2-nilpotent groups, a\nspecial type of central extension, is widely believed to contain the essential\nhard cases. However, looking specifically at central extensions, the natural\nformulation of being \"the same\" is not isomorphism but rather \"equivalence,\"\nwhich requires an isomorphism to preserves the structure of the extension. In\nthis paper, we show that equivalence of central extensions can be computed\nefficiently on a classical computer when the groups are small enough to be\ngiven by their multiplication tables. However, in the model of black box\ngroups, which allows the groups to be much larger, we show that equivalence can\nbe computed efficiently on a quantum computer but not a classical one (under\ncommon complexity assumptions). Our quantum algorithm demonstrates a new\napplication of the hidden subgroup problem for general abelian groups. \n\n"}
{"id": "1305.2796", "contents": "Title: Tiling simply connected regions with rectangles Abstract: In [BNRR], it was shown that tiling of general regions with two rectangles is\nNP-complete, except for a few trivial special cases. In a different direction,\nR\\'emila showed that for simply connected regions by two rectangles, the\ntileability can be solved in quadratic time (in the area). We prove that there\nis a finite set of at most 10^6 rectangles for which the tileability problem of\nsimply connected regions is NP-complete, closing the gap between positive and\nnegative results in the field. We also prove that counting such rectangular\ntilings is #P-complete, a first result of this kind. \n\n"}
{"id": "1305.3688", "contents": "Title: The Thinnest Path Problem Abstract: We formulate and study the thinnest path problem in wireless ad hoc networks.\nThe objective is to find a path from a source to its destination that results\nin the minimum number of nodes overhearing the message by a judicious choice of\nrelaying nodes and their corresponding transmission power. We adopt a directed\nhypergraph model of the problem and establish the NP-completeness of the\nproblem in 2-D networks. We then develop two polynomial-time approximation\nalgorithms that offer $\\sqrt{\\frac{n}{2}}$ and $\\frac{n}{2\\sqrt{n-1}}$\napproximation ratios for general directed hypergraphs (which can model\nnon-isomorphic signal propagation in space) and constant approximation ratios\nfor ring hypergraphs (which result from isomorphic signal propagation). We also\nconsider the thinnest path problem in 1-D networks and 1-D networks embedded in\n2-D field of eavesdroppers with arbitrary unknown locations (the so-called\n1.5-D networks). We propose a linear-complexity algorithm based on nested\nbackward induction that obtains the optimal solution to both 1-D and 1.5-D\nnetworks. This algorithm does not require the knowledge of eavesdropper\nlocations and achieves the best performance offered by any algorithm that\nassumes complete location information of the eavesdroppers. \n\n"}
{"id": "1305.4454", "contents": "Title: Entanglement in the Grover's Search Algorithm Abstract: Quantum Algorithms have long captured the imagination of computer scientists\nand physicists primarily because of the speed up achieved by them over their\nclassical counterparts using principles of quantum mechanics. Entanglement is\nbelieved to be the primary phenomena behind this speed up. However their\nprecise role in quantum algorithms is yet unclear. In this article, we explore\nthe nature of entanglement in the Grover's search algorithm. This algorithm\nenables searching of elements from an unstructured database quadratically\nfaster than the best known classical algorithm. Geometric measure of\nentanglement has been used to quantify and analyse entanglement across\niterations of the algorithm. We reveal how the entanglement varies with\nincrease in the number of qubits and also with the number of marked or solution\nstates. Numerically, it is seen that the behaviour of the maximum value of\nentanglement is monotonous with the number of qubits. Also, for a given value\nof the number of qubits, a change in the marked states alters the amount of\nentanglement. The amount of entanglement in the final state of the algorithm\nhas been shown to depend solely on the nature of the marked states. Explicit\nanalytical expressions are given showing the variation of entanglement with the\nnumber of iterations and the global maximum value of entanglement attained\nacross all iterations of the algorithm. \n\n"}
{"id": "1305.6285", "contents": "Title: Around the Petty theorem on equilateral sets Abstract: The main goal of this paper is to provide an alternative proof of the\nfollowing theorem of Petty: in the normed space of dimension at least three,\nevery 3-element equilateral set can be extended to a 4-element equilateral set.\nOur approach is based on the result of Kramer and N\\'emeth about inscribing a\nsimplex into a convex body. To prove the theorem of Petty, we shall also\nestablish that for every 3 points in the normed plane, forming an equilateral\nset of the common distance $p$, there exists a fourth point, which is\nequidistant to the given points with the distance not larger than $p$. We will\nalso improve the example given by Petty and obtain the existence of a smooth\nand strictly convex norm in $\\mathbb{R}^n$, which contain a maximal 4-element\nequilateral set. This shows that the theorem of Petty cannot be generalized to\nhigher dimensions, even for smooth and strictly convex norms. \n\n"}
{"id": "1305.6376", "contents": "Title: Fractional Pebbling Game Lower Bounds Abstract: Fractional pebbling is a generalization of black-white pebbling introduced\nrecently. In this reasearch paper we solve an open problem by proving a tight\nlower bound on the pebble weight required to fractionally pebble a balanced\nd-ary tree of height h. This bound has close ties with branching programs and\nthe separation of P from NL. \n\n"}
{"id": "1306.1114", "contents": "Title: On the complexity of Boolean matrix ranks Abstract: We construct a reduction which proves that the fooling set number and the\ndeterminantal rank of a Boolean matrix are NP-hard to compute. \n\n"}
{"id": "1306.2187", "contents": "Title: Metric Dimension for Gabriel Unit Disk Graphs is NP-Complete Abstract: We show that finding a minimal number of landmark nodes for a unique virtual\naddressing by hop-distances in wireless ad-hoc sensor networks is NP-complete\neven if the networks are unit disk graphs that contain only Gabriel edges. This\nproblem is equivalent to Metric Dimension for Gabriel unit disk graphs. The\nGabriel edges of a unit disc graph induce a planar O(\\sqrt{n}) distance and an\noptimal energy spanner. This is one of the most interesting restrictions of\nMetric Dimension in the context of wireless multi-hop networks. \n\n"}
{"id": "1306.2217", "contents": "Title: Multi-parameter complexity analysis for constrained size graph problems:\n  using greediness for parameterization Abstract: We study the parameterized complexity of a broad class of problems called\n\"local graph partitioning problems\" that includes the classical fixed\ncardinality problems as max k-vertex cover, k-densest subgraph, etc. By\ndeveloping a technique \"greediness-for-parameterization\", we obtain fixed\nparameter algorithms with respect to a pair of parameters k, the size of the\nsolution (but not its value) and \\Delta, the maximum degree of the input graph.\nIn particular, greediness-for-parameterization improves asymptotic running\ntimes for these problems upon random separation (that is a special case of\ncolor coding) and is more intuitive and simple. Then, we show how these results\ncan be easily extended for getting standard-parameterization results (i.e.,\nwith parameter the value of the optimal solution) for a well known local graph\npartitioning problem. \n\n"}
{"id": "1306.2515", "contents": "Title: Apollonian Ball Packings and Stacked Polytopes Abstract: We investigate in this paper the relation between Apollonian $d$-ball\npackings and stacked $(d+1)$-polytopes for dimension $d\\ge 3$. For $d=3$, the\nrelation is fully described: we prove that the $1$-skeleton of a stacked\n$4$-polytope is the tangency graph of an Apollonian $3$-ball packing if and\nonly if no six $4$-cliques share a $3$-clique. For higher dimension, we have\nsome partial results. \n\n"}
{"id": "1306.6710", "contents": "Title: The two-handed tile assembly model is not intrinsically universal Abstract: The well-studied Two-Handed Tile Assembly Model (2HAM) is a model of tile\nassembly in which pairs of large assemblies can bind, or self-assemble,\ntogether. In order to bind, two assemblies must have matching glues that can\nsimultaneously touch each other, and stick together with strength that is at\nleast the temperature $\\tau$, where $\\tau$ is some fixed positive integer. We\nask whether the 2HAM is intrinsically universal, in other words we ask: is\nthere a single universal 2HAM tile set $U$ which can be used to simulate any\ninstance of the model? Our main result is a negative answer to this question.\nWe show that for all $\\tau' < \\tau$, each temperature-$\\tau'$ 2HAM tile system\ndoes not simulate at least one temperature-$\\tau$ 2HAM tile system. This\nimpossibility result proves that the 2HAM is not intrinsically universal, in\nstark contrast to the simpler (single-tile addition only) abstract Tile\nAssembly Model which is intrinsically universal (\"The tile assembly model is\nintrinsically universal\", FOCS 2012). However, on the positive side, we prove\nthat, for every fixed temperature $\\tau \\geq 2$, temperature-$\\tau$ 2HAM tile\nsystems are indeed intrinsically universal: in other words, for each $\\tau$\nthere is a single universal 2HAM tile set $U$ that, when appropriately\ninitialized, is capable of simulating the behavior of any temperature-$\\tau$\n2HAM tile system. As a corollary of these results we find an infinite set of\ninfinite hierarchies of 2HAM systems with strictly increasing simulation power\nwithin each hierarchy. Finally, we show that for each $\\tau$, there is a\ntemperature-$\\tau$ 2HAM system that simultaneously simulates all\ntemperature-$\\tau$ 2HAM systems. \n\n"}
{"id": "1307.2747", "contents": "Title: Impossibility of Local State Transformation via Hypercontractivity Abstract: Local state transformation is the problem of transforming an arbitrary number\nof copies of a bipartite resource state to a bipartite target state under local\noperations. That is, given two bipartite states, is it possible to transform an\narbitrary number of copies of one of them to one copy of the other state under\nlocal operations only? This problem is a hard one in general since we assume\nthat the number of copies of the resource state is arbitrarily large. In this\npaper we prove some bounds on this problem using the hypercontractivity\nproperties of some super-operators corresponding to bipartite states. We\nmeasure hypercontractivity in terms of both the usual super-operator norms as\nwell as completely bounded norms. \n\n"}
{"id": "1307.2808", "contents": "Title: A Constant Factor Approximation Algorithm for Fault-Tolerant k-Median Abstract: In this paper, we consider the fault-tolerant $k$-median problem and give the\n\\emph{first} constant factor approximation algorithm for it. In the\nfault-tolerant generalization of classical $k$-median problem, each client $j$\nneeds to be assigned to at least $r_j \\ge 1$ distinct open facilities. The\nservice cost of $j$ is the sum of its distances to the $r_j$ facilities, and\nthe $k$-median constraint restricts the number of open facilities to at most\n$k$. Previously, a constant factor was known only for the special case when all\n$r_j$s are the same, and a logarithmic approximation ratio for the general\ncase. In addition, we present the first polynomial time algorithm for the\nfault-tolerant $k$-median problem on a path or a HST by showing that the\ncorresponding LP always has an integral optimal solution.\n  We also consider the fault-tolerant facility location problem, where the\nservice cost of $j$ can be a weighted sum of its distance to the $r_j$\nfacilities. We give a simple constant factor approximation algorithm,\ngeneralizing several previous results which only work for nonincreasing weight\nvectors. \n\n"}
{"id": "1307.4927", "contents": "Title: Linear-Time FPT Algorithms via Network Flow Abstract: In the area of parameterized complexity, to cope with NP-Hard problems, we\nintroduce a parameter k besides the input size n, and we aim to design\nalgorithms (called FPT algorithms) that run in O(f(k)n^d) time for some\nfunction f(k) and constant d. Though FPT algorithms have been successfully\ndesigned for many problems, typically they are not sufficiently fast because of\nhuge f(k) and d. In this paper, we give FPT algorithms with small f(k) and d\nfor many important problems including Odd Cycle Transversal and Almost 2-SAT.\nMore specifically, we can choose f(k) as a single exponential (4^k) and d as\none, that is, linear in the input size. To the best of our knowledge, our\nalgorithms achieve linear time complexity for the first time for these\nproblems. To obtain our algorithms for these problems, we consider a large\nclass of integer programs, called BIP2. Then we show that, in linear time, we\ncan reduce BIP2 to Vertex Cover Above LP preserving the parameter k, and we can\ncompute an optimal LP solution for Vertex Cover Above LP using network flow.\nThen, we perform an exhaustive search by fixing half-integral values in the\noptimal LP solution for Vertex Cover Above LP. A bottleneck here is that we\nneed to recompute an LP optimal solution after branching. To address this\nissue, we exploit network flow to update the optimal LP solution in linear\ntime. \n\n"}
{"id": "1307.6738", "contents": "Title: Efficient quantum protocols for XOR functions Abstract: We show that for any Boolean function f on {0,1}^n, the bounded-error quantum\ncommunication complexity of XOR functions $f\\circ \\oplus$ satisfies that\n$Q_\\epsilon(f\\circ \\oplus) = O(2^d (\\log\\|\\hat f\\|_{1,\\epsilon} + \\log\n\\frac{n}{\\epsilon}) \\log(1/\\epsilon))$, where d is the F2-degree of f, and\n$\\|\\hat f\\|_{1,\\epsilon} = \\min_{g:\\|f-g\\|_\\infty \\leq \\epsilon} \\|\\hat f\\|_1$.\nThis implies that the previous lower bound $Q_\\epsilon(f\\circ \\oplus) =\n\\Omega(\\log\\|\\hat f\\|_{1,\\epsilon})$ by Lee and Shraibman \\cite{LS09} is tight\nfor f with low F2-degree. The result also confirms the quantum version of the\nLog-rank Conjecture for low-degree XOR functions. In addition, we show that the\nexact quantum communication complexity satisfies $Q_E(f) = O(2^d \\log \\|\\hat\nf\\|_0)$, where $\\|\\hat f\\|_0$ is the number of nonzero Fourier coefficients of\nf. This matches the previous lower bound $Q_E(f(x,y)) = \\Omega(\\log rank(M_f))$\nby Buhrman and de Wolf \\cite{BdW01} for low-degree XOR functions. \n\n"}
{"id": "1307.7811", "contents": "Title: A Novel Combinatorial Method for Estimating Transcript Expression with\n  RNA-Seq: Bounding the Number of Paths Abstract: RNA-Seq technology offers new high-throughput ways for transcript\nidentification and quantification based on short reads, and has recently\nattracted great interest. The problem is usually modeled by a weighted splicing\ngraph whose nodes stand for exons and whose edges stand for split alignments to\nthe exons. The task consists of finding a number of paths, together with their\nexpression levels, which optimally explain the coverages of the graph under\nvarious fitness functions, such least sum of squares. In (Tomescu et al.\nRECOMB-seq 2013) we showed that under general fitness functions, if we allow a\npolynomially bounded number of paths in an optimal solution, this problem can\nbe solved in polynomial time by a reduction to a min-cost flow program. In this\npaper we further refine this problem by asking for a bounded number k of paths\nthat optimally explain the splicing graph. This problem becomes NP-hard in the\nstrong sense, but we give a fast combinatorial algorithm based on dynamic\nprogramming for it. In order to obtain a practical tool, we implement three\noptimizations and heuristics, which achieve better performance on real data,\nand similar or better performance on simulated data, than state-of-the-art\ntools Cufflinks, IsoLasso and SLIDE. Our tool, called Traph, is available at\nhttp://www.cs.helsinki.fi/gsa/traph/ \n\n"}
{"id": "1308.0068", "contents": "Title: Communication lower bounds and optimal algorithms for programs that\n  reference arrays -- Part 1 Abstract: The movement of data (communication) between levels of a memory hierarchy, or\nbetween parallel processors on a network, can greatly dominate the cost of\ncomputation, so algorithms that minimize communication are of interest.\nMotivated by this, attainable lower bounds for the amount of communication\nrequired by algorithms were established by several groups for a variety of\nalgorithms, including matrix computations. Prior work of\nBallard-Demmel-Holtz-Schwartz relied on a geometric inequality of Loomis and\nWhitney for this purpose. In this paper the general theory of discrete\nmultilinear Holder-Brascamp-Lieb (HBL) inequalities is used to establish\ncommunication lower bounds for a much wider class of algorithms. In some cases,\nalgorithms are presented which attain these lower bounds.\n  Several contributions are made to the theory of HBL inequalities proper. The\noptimal constant in such an inequality for torsion-free Abelian groups is shown\nto equal one whenever it is finite. Bennett-Carbery-Christ-Tao had\ncharacterized the tuples of exponents for which such an inequality is valid as\nthe convex polyhedron defined by a certain finite list of inequalities. The\nproblem of constructing an algorithm to decide whether a given inequality is on\nthis list, is shown to be equivalent to Hilbert's Tenth Problem over the\nrationals, which remains open. Nonetheless, an algorithm which computes the\npolyhedron itself is constructed. \n\n"}
{"id": "1308.0177", "contents": "Title: Distinct distances on algebraic curves in the plane Abstract: Let $P$ be a set of $n$ points in the real plane contained in an algebraic\ncurve $C$ of degree $d$. We prove that the number of distinct distances\ndetermined by $P$ is at least $c_d n^{4/3}$, unless $C$ contains a line or a\ncircle. We also prove the lower bound $c_d' \\min(m^{2/3}n^{2/3}, m^2, n^2)$ for\nthe number of distinct distances between $m$ points on one irreducible plane\nalgebraic curve and $n$ points on another, unless the two curves are parallel\nlines, orthogonal lines, or concentric circles. This generalizes a result on\ndistances between lines of Sharir, Sheffer, and Solymosi in arXiv:1302.3081. \n\n"}
{"id": "1308.0833", "contents": "Title: Data Structures in Classical and Quantum Computing Abstract: This survey summarizes several results about quantum computing related to\n(mostly static) data structures. First, we describe classical data structures\nfor the set membership and the predecessor search problems: Perfect Hash tables\nfor set membership by Fredman, Koml\\'{o}s and Szemer\\'{e}di and a data\nstructure by Beame and Fich for predecessor search. We also prove results about\ntheir space complexity (how many bits are required) and time complexity (how\nmany bits have to be read to answer a query). After that, we turn our attention\nto classical data structures with quantum access. In the quantum access model,\ndata is stored in classical bits, but they can be accessed in a quantum way: We\nmay read several bits in superposition for unit cost. We give proofs for lower\nbounds in this setting that show that the classical data structures from the\nfirst section are, in some sense, asymptotically optimal - even in the quantum\nmodel. In fact, these proofs are simpler and give stronger results than\nprevious proofs for the classical model of computation. The lower bound for set\nmembership was proved by Radhakrishnan, Sen and Venkatesh and the result for\nthe predecessor problem by Sen and Venkatesh. Finally, we examine fully quantum\ndata structures. Instead of encoding the data in classical bits, we now encode\nit in qubits. We allow any unitary operation or measurement in order to answer\nqueries. We describe one data structure by de Wolf for the set membership\nproblem and also a general framework using fully quantum data structures in\nquantum walks by Jeffery, Kothari and Magniez. \n\n"}
{"id": "1308.2409", "contents": "Title: On the Parameterized Complexity of Reconfiguration Problems Abstract: We present the first results on the parameterized complexity of\nreconfiguration problems, where a reconfiguration version of an optimization\nproblem $Q$ takes as input two feasible solutions $S$ and $T$ and determines if\nthere is a sequence of {\\em reconfiguration steps} that can be applied to\ntransform $S$ into $T$ such that each step results in a feasible solution to\n$Q$. For most of the results in this paper, $S$ and $T$ are subsets of vertices\nof a given graph and a reconfiguration step adds or deletes a vertex. Our study\nis motivated by recent results establishing that for most NP-hard problems, the\nclassical complexity of reconfiguration is PSPACE-complete. We address the\nquestion for several important graph properties under two natural\nparameterizations: $k$, the size of the solutions, and $\\ell$, the length of\nthe sequence of steps. Our first general result is an algorithmic paradigm, the\n{\\em reconfiguration kernel}, used to obtain fixed-parameter algorithms for the\nreconfiguration versions of {\\sc Vertex Cover} and, more generally, {\\sc\nBounded Hitting Set} and {\\sc Feedback Vertex Set}, all parameterized by $k$.\nIn contrast, we show that reconfiguring {\\sc Unbounded Hitting Set} is\n$W[2]$-hard when parameterized by $k+\\ell$. We also demonstrate the\n$W[1]$-hardness of the reconfiguration versions of a large class of\nmaximization problems parameterized by $k+\\ell$, and of their corresponding\ndeletion problems parameterized by $\\ell$; in doing so, we show that there\nexist problems in FPT when parameterized by $k$, but whose reconfiguration\nversions are $W[1]$-hard when parameterized by $k+\\ell$. \n\n"}
{"id": "1308.2617", "contents": "Title: Independent Set, Induced Matching, and Pricing: Connections and Tight\n  (Subexponential Time) Approximation Hardnesses Abstract: We present a series of almost settled inapproximability results for three\nfundamental problems. The first in our series is the subexponential-time\ninapproximability of the maximum independent set problem, a question studied in\nthe area of parameterized complexity. The second is the hardness of\napproximating the maximum induced matching problem on bounded-degree bipartite\ngraphs. The last in our series is the tight hardness of approximating the\nk-hypergraph pricing problem, a fundamental problem arising from the area of\nalgorithmic game theory. In particular, assuming the Exponential Time\nHypothesis, our two main results are:\n  - For any r larger than some constant, any r-approximation algorithm for the\nmaximum independent set problem must run in at least\n2^{n^{1-\\epsilon}/r^{1+\\epsilon}} time. This nearly matches the upper bound of\n2^{n/r} (Cygan et al., 2008). It also improves some hardness results in the\ndomain of parameterized complexity (e.g., Escoffier et al., 2012 and Chitnis et\nal., 2013)\n  - For any k larger than some constant, there is no polynomial time min\n(k^{1-\\epsilon}, n^{1/2-\\epsilon})-approximation algorithm for the k-hypergraph\npricing problem, where n is the number of vertices in an input graph. This\nalmost matches the upper bound of min (O(k), \\tilde O(\\sqrt{n})) (by Balcan and\nBlum, 2007 and an algorithm in this paper).\n  We note an interesting fact that, in contrast to n^{1/2-\\epsilon} hardness\nfor polynomial-time algorithms, the k-hypergraph pricing problem admits\nn^{\\delta} approximation for any \\delta >0 in quasi-polynomial time. This puts\nthis problem in a rare approximability class in which approximability\nthresholds can be improved significantly by allowing algorithms to run in\nquasi-polynomial time. \n\n"}
{"id": "1308.3336", "contents": "Title: The Power of Dynamic Distance Oracles: Efficient Dynamic Algorithms for\n  the Steiner Tree Abstract: In this paper we study the Steiner tree problem over a dynamic set of\nterminals. We consider the model where we are given an $n$-vertex graph\n$G=(V,E,w)$ with positive real edge weights, and our goal is to maintain a tree\nwhich is a good approximation of the minimum Steiner tree spanning a terminal\nset $S \\subseteq V$, which changes over time. The changes applied to the\nterminal set are either terminal additions (incremental scenario), terminal\nremovals (decremental scenario), or both (fully dynamic scenario). Our task\nhere is twofold. We want to support updates in sublinear $o(n)$ time, and keep\nthe approximation factor of the algorithm as small as possible. We show that we\ncan maintain a $(6+\\varepsilon)$-approximate Steiner tree of a general graph in\n$\\tilde{O}(\\sqrt{n} \\log D)$ time per terminal addition or removal. Here, $D$\ndenotes the stretch of the metric induced by $G$. For planar graphs we achieve\nthe same running time and the approximation ratio of $(2+\\varepsilon)$.\nMoreover, we show faster algorithms for incremental and decremental scenarios.\nFinally, we show that if we allow higher approximation ratio, even more\nefficient algorithms are possible. In particular we show a polylogarithmic time\n$(4+\\varepsilon)$-approximate algorithm for planar graphs.\n  One of the main building blocks of our algorithms are dynamic distance\noracles for vertex-labeled graphs, which are of independent interest. We also\nimprove and use the online algorithms for the Steiner tree problem. \n\n"}
{"id": "1308.3613", "contents": "Title: Polynomial kernels collapse the W-hierarchy Abstract: We prove that, for many parameterized problems in the class FPT, the\nexistence of polynomial kernels implies the collapse of the W-hierarchy (i.e.,\nW[P] = FPT). The collapsing results are also extended to assumed exponential\nkernels for problems in the class FPT. In particular, we establish a close\nrelationship between polynomial (and exponential) kernelizability and the\nexistence of sub-exponential time algorithms for a spectrum of circuit\nsatisfiability problems in FPT. To the best of our knowledge, this is the first\nwork that connects hardness for polynomial kernelizability of FPT problems to\nparameterized intractability. Our work also offers some new insights into the\nclass FPT. \n\n"}
{"id": "1308.6007", "contents": "Title: Tree Codes and a Conjecture on Exponential Sums Abstract: We propose a new conjecture on some exponential sums. These particular sums\nhave not apparently been considered in the literature. Subject to the\nconjecture we obtain the first effective construction of asymptotically good\ntree codes. The available numerical evidence is consistent with the conjecture\nand is sufficient to certify codes for significant-length communications. \n\n"}
{"id": "1309.0302", "contents": "Title: Unmixing Incoherent Structures of Big Data by Randomized or Greedy\n  Decomposition Abstract: Learning big data by matrix decomposition always suffers from expensive\ncomputation, mixing of complicated structures and noise. In this paper, we\nstudy more adaptive models and efficient algorithms that decompose a data\nmatrix as the sum of semantic components with incoherent structures. We firstly\nintroduce \"GO decomposition (GoDec)\", an alternating projection method\nestimating the low-rank part $L$ and the sparse part $S$ from data matrix\n$X=L+S+G$ corrupted by noise $G$. Two acceleration strategies are proposed to\nobtain scalable unmixing algorithm on big data: 1) Bilateral random projection\n(BRP) is developed to speed up the update of $L$ in GoDec by a closed-form\nbuilt from left and right random projections of $X-S$ in lower dimensions; 2)\nGreedy bilateral (GreB) paradigm updates the left and right factors of $L$ in a\nmutually adaptive and greedy incremental manner, and achieve significant\nimprovement in both time and sample complexities. Then we proposes three\nnontrivial variants of GoDec that generalizes GoDec to more general data type\nand whose fast algorithms can be derived from the two strategies...... \n\n"}
{"id": "1309.1147", "contents": "Title: Curves in R^d intersecting every hyperplane at most d+1 times Abstract: By a curve in R^d we mean a continuous map gamma:I -> R^d, where I is a\nclosed interval. We call a curve gamma in R^d at most k crossing if it\nintersects every hyperplane at most k times (counted with multiplicity). The at\nmost d crossing curves in R^d are often called convex curves and they form an\nimportant class; a primary example is the moment curve\n{(t,t^2,...,t^d):t\\in[0,1]}. They are also closely related to Chebyshev\nsystems, which is a notion of considerable importance, e.g., in approximation\ntheory. We prove that for every d there is M=M(d) such that every at most d+1\ncrossing curve in R^d can be subdivided into at most M convex curves. As a\nconsequence, based on the work of Elias, Roldan, Safernova, and the second\nauthor, we obtain an essentially tight lower bound for a geometric Ramsey-type\nproblem in R^d concerning order-type homogeneous sequences of points,\ninvestigated in several previous papers. \n\n"}
{"id": "1309.1559", "contents": "Title: Large induced subgraphs via triangulations and CMSO Abstract: We obtain an algorithmic meta-theorem for the following optimization problem.\nLet \\phi\\ be a Counting Monadic Second Order Logic (CMSO) formula and t be an\ninteger. For a given graph G, the task is to maximize |X| subject to the\nfollowing: there is a set of vertices F of G, containing X, such that the\nsubgraph G[F] induced by F is of treewidth at most t, and structure (G[F],X)\nmodels \\phi.\n  Some special cases of this optimization problem are the following generic\nexamples. Each of these cases contains various problems as a special subcase:\n  1) \"Maximum induced subgraph with at most l copies of cycles of length 0\nmodulo m\", where for fixed nonnegative integers m and l, the task is to find a\nmaximum induced subgraph of a given graph with at most l vertex-disjoint cycles\nof length 0 modulo m.\n  2) \"Minimum \\Gamma-deletion\", where for a fixed finite set of graphs \\Gamma\\\ncontaining a planar graph, the task is to find a maximum induced subgraph of a\ngiven graph containing no graph from \\Gamma\\ as a minor.\n  3) \"Independent \\Pi-packing\", where for a fixed finite set of connected\ngraphs \\Pi, the task is to find an induced subgraph G[F] of a given graph G\nwith the maximum number of connected components, such that each connected\ncomponent of G[F] is isomorphic to some graph from \\Pi.\n  We give an algorithm solving the optimization problem on an n-vertex graph G\nin time O(#pmc n^{t+4} f(t,\\phi)), where #pmc is the number of all potential\nmaximal cliques in G and f is a function depending of t and \\phi\\ only. We also\nshow how a similar running time can be obtained for the weighted version of the\nproblem. Pipelined with known bounds on the number of potential maximal\ncliques, we deduce that our optimization problem can be solved in time\nO(1.7347^n) for arbitrary graphs, and in polynomial time for graph classes with\npolynomial number of minimal separators. \n\n"}
{"id": "1309.1779", "contents": "Title: Fractal dimension versus process complexity Abstract: Complexity measures are designed to capture complex behavior and quantify\n*how* complex, according to that measure, that particular behavior is. It can\nbe expected that different complexity measures from possibly entirely different\nfields are related to each other in a non-trivial fashion. Here we study small\nTuring machines (TMs) with two symbols, and two and three states. For any\nparticular such machine $\\tau$ and any particular input $x$ we consider what we\ncall the 'space-time' diagram which is the collection of consecutive tape\nconfigurations of the computation $\\tau(x)$. In our setting, we define fractal\ndimension of a Turing machine as the limiting fractal dimension of the\ncorresponding space-time diagram. It turns out that there is a very strong\nrelation between the fractal dimension of a Turing machine of the\nabove-specified type and its runtime complexity. In particular, a TM with three\nstates and two colors runs in at most linear time iff its dimension is 2, and\nits dimension is 1 iff it runs in super-polynomial time and it uses polynomial\nspace. If a TM runs in time $O(x^n)$ we have empirically verified that the\ncorresponding dimension is $(n+1)/n$, a result that we can only partially\nprove. We find the results presented here remarkable because they relate two\ncompletely different complexity measures: the geometrical fractal dimension on\nthe one side versus the time complexity of a computation on the other side. \n\n"}
{"id": "1309.5206", "contents": "Title: New Algorithms for Solving Tropical Linear Systems Abstract: The problem of solving tropical linear systems, a natural problem of tropical\nmathematics, has already proven to be very interesting from the algorithmic\npoint of view: it is known to be in $NP\\cap coNP$ but no polynomial time\nalgorithm is known, although counterexamples for existing pseudopolynomial\nalgorithms are (and have to be) very complex.\n  In this work, we continue the study of algorithms for solving tropical linear\nsystems. First, we present a new reformulation of Grigoriev's algorithm that\nbrings it closer to the algorithm of Akian, Gaubert, and Guterman; this lets us\nformulate a whole family of new algorithms, and we present algorithms from this\nfamily for which no known superpolynomial counterexamples work. Second, we\npresent a family of algorithms for solving overdetermined tropical systems. We\nshow that for weakly overdetermined systems, there are polynomial algorithms in\nthis family. We also present a concrete algorithm from this family that can\nsolve a tropical linear system defined by an $m\\times n$ matrix with maximal\nelement $M$ in time $\\Theta\\left({m \\choose n} \\mathrm{poly}\\left(m, n, \\log\nM\\right)\\right)$, and this time matches the complexity of the best of\npreviously known algorithms for feasibility testing. \n\n"}
{"id": "1309.6069", "contents": "Title: Beyond the Shannon's Bound Abstract: Let $G=(V,E)$ be a multigraph of maximum degree $\\Delta$. The edges of $G$\ncan be colored with at most $\\frac{3}{2}\\Delta$ colors by Shannon's theorem. We\nstudy lower bounds on the size of subgraphs of $G$ that can be colored with\n$\\Delta$ colors.\n  Shannon's Theorem gives a bound of\n$\\frac{\\Delta}{\\lfloor\\frac{3}{2}\\Delta\\rfloor}|E|$. However, for $\\Delta=3$,\nKami\\'{n}ski and Kowalik [SWAT'10] showed that there is a 3-edge-colorable\nsubgraph of size at least $\\frac{7}{9}|E|$, unless $G$ has a connected\ncomponent isomorphic to $K_3+e$ (a $K_3$ with an arbitrary edge doubled). Here\nwe extend this line of research by showing that $G$ has a $\\Delta$-edge\ncolorable subgraph with at least\n$\\frac{\\Delta}{\\lfloor\\frac{3}{2}\\Delta\\rfloor-1}|E|$ edges, unless $\\Delta$ is\neven and $G$ contains $\\frac{\\Delta}{2}K_3$ or $\\Delta$ is odd and $G$ contains\n$\\frac{\\Delta-1}{2}K_3+e$. Moreover, the subgraph and its coloring can be found\nin polynomial time.\n  Our results have applications in approximation algorithms for the Maximum\n$k$-Edge-Colorable Subgraph problem, where given a graph $G$ (without any bound\non its maximum degree or other restrictions) one has to find a\n$k$-edge-colorable subgraph with maximum number of edges. In particular, for\nevery even $k \\ge 4$ we obtain a $\\frac{2k+2}{3k+2}$-approximation and for\nevery odd $k\\ge 5$ we get a $\\frac{2k+1}{3k}$-approximation. When $4\\le k \\le\n13$ this improves over earlier algorithms due to Feige et al. [APPROX'02] \n\n"}
{"id": "1309.6116", "contents": "Title: Optimal parallel quantum query algorithms Abstract: We study the complexity of quantum query algorithms that make p queries in\nparallel in each timestep. This model is in part motivated by the fact that\ndecoherence times of qubits are typically small, so it makes sense to\nparallelize quantum algorithms as much as possible. We show tight bounds for a\nnumber of problems, specifically Theta((n/p)^{2/3}) p-parallel queries for\nelement distinctness and Theta((n/p)^{k/(k+1)} for k-sum. Our upper bounds are\nobtained by parallelized quantum walk algorithms, and our lower bounds are\nbased on a relatively small modification of the adversary lower bound method,\ncombined with recent results of Belovs et al. on learning graphs. We also prove\nsome general bounds, in particular that quantum and classical p-parallel\ncomplexity are polynomially related for all total functions f when p is small\ncompared to f's block sensitivity. \n\n"}
{"id": "1309.7713", "contents": "Title: Span-program-based quantum algorithm for tree detection Abstract: Span program is a linear-algebraic model of computation originally proposed\nfor studying the complexity theory. Recently, it has become a useful tool for\ndesigning quantum algorithms. In this paper, we present a time-efficient\nspan-program-based quantum algorithm for the following problem. Let $T$ be an\narbitrary tree. Given query access to the adjacency matrix of a graph $G$ with\n$n$ vertices, we need to determine whether $G$ contains $T$ as a subgraph, or\n$G$ does not contain $T$ as a minor, under the promise that one of these cases\nholds. We call this problem the subgraph/not-a-minor problem for $T$. We show\nthat this problem can be solved by a bounded-error quantum algorithm with\n$O(n)$ query complexity and $\\tilde{O}(n)$ time complexity. The query\ncomplexity is optimal, and the time complexity is tight up to polylog factors. \n\n"}
{"id": "1310.0398", "contents": "Title: On the optimality of approximation schemes for the classical scheduling\n  problem Abstract: We consider the classical scheduling problem on parallel identical machines\nto minimize the makespan, and achieve the following results under the\nExponential Time Hypothesis (ETH)\n  1. The scheduling problem on a constant number $m$ of identical machines,\nwhich is denoted as $Pm||C_{max}$, is known to admit a fully polynomial time\napproximation scheme (FPTAS) of running time $O(n) + (1/\\epsilon)^{O(m)}$\n(indeed, the algorithm works for an even more general problem where machines\nare unrelated). We prove this algorithm is essentially the best possible in the\nsense that a $(1/\\epsilon)^{O(m^{1-\\delta})}+n^{O(1)}$ time FPTAS for any\n$\\delta>0$ implies that ETH fails.\n  2. The scheduling problem on an arbitrary number of identical machines, which\nis denoted as $P||C_{max}$, is known to admit a polynomial time approximation\nscheme (PTAS) of running time $2^{O(1/\\epsilon^2\\log^3(1/\\epsilon))}+n^{O(1)}$.\nWe prove this algorithm is nearly optimal in the sense that a\n$2^{O((1/\\epsilon)^{1-\\delta})}+n^{O(1)}$ time PTAS for any $\\delta>0$ implies\nthat ETH fails, leaving a small room for improvement.\n  To obtain these results we will provide two new reductions from 3SAT, one for\n$Pm||C_{max}$ and another for $P||C_{max}$. Indeed, the new reductions explore\nthe structure of scheduling problems and can also lead to other interesting\nresults. For example, using the framework of our reduction for $P||C_{max}$,\nChen et al. (arXiv:1306.3727) is able to prove the APX-hardness of the\nscheduling problem in which the matrix of job processing times\n$P=(p_{ij})_{m\\times n}$ is of rank 3, solving the open problem mentioned by\nBhaskara et al. (SODA 2013). \n\n"}
{"id": "1310.1493", "contents": "Title: Gap Amplification for Small-Set Expansion via Random Walks Abstract: In this work, we achieve gap amplification for the Small-Set Expansion\nproblem. Specifically, we show that an instance of the Small-Set Expansion\nProblem with completeness $\\epsilon$ and soundness $\\frac{1}{2}$ is at least as\ndifficult as Small-Set Expansion with completeness $\\epsilon$ and soundness\n$f(\\epsilon)$, for any function $f(\\epsilon)$ which grows faster than\n$\\sqrt{\\epsilon}$. We achieve this amplification via random walks -- our gadget\nis the graph with adjacency matrix corresponding to a random walk on the\noriginal graph. An interesting feature of our reduction is that unlike gap\namplification via parallel repetition, the size of the instances (number of\nvertices) produced by the reduction remains the same. \n\n"}
{"id": "1310.1860", "contents": "Title: The rigidity of infinite graphs Abstract: A rigidity theory is developed for the Euclidean and non-Euclidean placements\nof countably infinite simple graphs in R^d with respect to the classical l^p\nnorms, for d>1 and 1<p<\\infty. Generalisations are obtained for the Laman and\nHenneberg combinatorial characterisations of generic infinitesimal rigidity for\nfinite graphs in the Euclidean plane. Also Tay's multi-graph characterisation\nof the rigidity of generic finite body-bar frameworks in d-dimensional\nEuclidean space is generalised to the non-Euclidean l^p norms and to countably\ninfinite graphs. For all dimensions and norms it is shown that a generically\nrigid countable simple graph is the direct limit of an inclusion tower of\nfinite graphs for which the inclusions satisfy a relative rigidity property.\nFor d>2 a countable graph which is rigid for generic placements in R^d may fail\nthe stronger property of sequential rigidity, while for d=2 the equivalence\nwith sequential rigidity is obtained from the generalised Laman\ncharacterisations. Applications are given to the flexibility of non-Euclidean\nconvex polyhedra and to the infinitesimal and continuous rigidity of compact\ninfinitely-faceted simplicial polytopes. \n\n"}
{"id": "1310.3673", "contents": "Title: Evaluation of DNF Formulas Abstract: Stochastic Boolean Function Evaluation (SBFE) is the problem of determining\nthe value of a given Boolean function $f$ on an unknown input $x$, when each\nbit of $x_i$ of $x$ can only be determined by paying a given associated cost\n$c_i$. Further, $x$ is drawn from a given product distribution: for each $x_i$,\n$Prob[x_i=1] = p_i$, and the bits are independent. The goal is to minimize the\nexpected cost of evaluation. Stochastic Boolean Function Evaluation (SBFE) is\nthe problem of determining the value of a given Boolean function $f$ on an\nunknown input $x$, when each bit of $x_i$ of $x$ can only be determined by\npaying a given associated cost $c_i$. Further, $x$ is drawn from a given\nproduct distribution: for each $x_i$, $Prob[x_i=1] = p_i$, and the bits are\nindependent. The goal is to minimize the expected cost of evaluation. In this\npaper, we study the complexity of the SBFE problem for classes of DNF formulas.\nWe consider both exact and approximate versions of the problem for subclasses\nof DNF, for arbitrary costs and product distributions, and for unit costs\nand/or the uniform distribution. \n\n"}
{"id": "1310.4141", "contents": "Title: Topological Additive Numbering of Directed Acyclic Graphs Abstract: We propose to study a problem that arises naturally from both Topological\nNumbering of Directed Acyclic Graphs, and Additive Coloring (also known as\nLucky Labeling). Let $D$ be a digraph and $f$ a labeling of its vertices with\npositive integers; denote by $S(v)$ the sum of labels over all neighbors of\neach vertex $v$. The labeling $f$ is called \\emph{topological additive\nnumbering} if $S(u) < S(v)$ for each arc $(u,v)$ of the digraph. The problem\nasks to find the minimum number $k$ for which $D$ has a topological additive\nnumbering with labels belonging to $\\{ 1, \\ldots, k \\}$, denoted by\n$\\eta_t(D)$.\n  We characterize when a digraph has topological additive numberings, give a\nlower bound for $\\eta_t(D)$, and provide an integer programming formulation for\nour problem, characterizing when its coefficient matrix is totally unimodular.\nWe also present some families for which $\\eta_t(D)$ can be computed in\npolynomial time. Finally, we prove that this problem is \\np-Hard even when its\ninput is restricted to planar bipartite digraphs. \n\n"}
{"id": "1311.0293", "contents": "Title: Pebbling Arguments for Tree Evaluation Abstract: The Tree Evaluation Problem was introduced by Cook et al. in 2010 as a\ncandidate for separating P from L and NL. The most general space lower bounds\nknown for the Tree Evaluation Problem require a semantic restriction on the\nbranching programs and use a connection to well-known pebble games to generate\na bottleneck argument. These bounds are met by corresponding upper bounds\ngenerated by natural implementations of optimal pebbling algorithms. In this\npaper we extend these ideas to a variety of restricted families of both\ndeterministic and non-deterministic branching programs, proving tight lower\nbounds under these restricted models. We also survey and unify known lower\nbounds in our \"pebbling argument\" framework. \n\n"}
{"id": "1311.0484", "contents": "Title: Deterministic Parameterized Algorithms for Matching and Packing Problems Abstract: We present three deterministic parameterized algorithms for well-studied\npacking and matching problems, namely, Weighted q-Dimensional p-Matching\n((q,p)-WDM) and Weighted q-Set p-Packing ((q,p)-WSP). More specifically, we\npresent an O*(2.85043^{(q-1)p}) time deterministic algorithm for (q,p)-WDM, an\nO*(8.04143^p) time deterministic algorithm for the unweighted version of\n(3,p)-WDM, and an O*((0.56201\\cdot 2.85043^q)^p) time deterministic algorithm\nfor (q,p)-WSP. Our algorithms significantly improve the previously best known\nO* running times in solving (q,p)-WDM and (q,p)-WSP, and the previously best\nknown deterministic O* running times in solving the unweighted versions of\nthese problems. Moreover, we present kernels of size O(e^qq(p-1)^q) for\n(q,p)-WDM and (q,p)-WSP, improving the previously best known kernels of size\nO(q!q(p-1)^q) for these problems. \n\n"}
{"id": "1311.1098", "contents": "Title: Mirror Prox Algorithm for Multi-Term Composite Minimization and\n  Semi-Separable Problems Abstract: In the paper, we develop a composite version of Mirror Prox algorithm for\nsolving convex-concave saddle point problems and monotone variational\ninequalities of special structure, allowing to cover saddle point/variational\nanalogies of what is usually called \"composite minimization\" (minimizing a sum\nof an easy-to-handle nonsmooth and a general-type smooth convex functions \"as\nif\" there were no nonsmooth component at all). We demonstrate that the\ncomposite Mirror Prox inherits the favourable (and unimprovable already in the\nlarge-scale bilinear saddle point case) $O(1/\\epsilon)$ efficiency estimate of\nits prototype. We demonstrate that the proposed approach can be naturally\napplied to Lasso-type problems with several penalizing terms (e.g. acting\ntogether $\\ell_1$ and nuclear norm regularization) and to problems of the\nstructure considered in the alternating directions methods, implying in both\ncases methods with the $O(\\epsilon^{-1})$ complexity bounds. \n\n"}
{"id": "1311.1616", "contents": "Title: Hardness Amplification and the Approximate Degree of Constant-Depth\n  Circuits Abstract: We establish a generic form of hardness amplification for the approximability\nof constant-depth Boolean circuits by polynomials. Specifically, we show that\nif a Boolean circuit cannot be pointwise approximated by low-degree polynomials\nto within constant error in a certain one-sided sense, then an OR of disjoint\ncopies of that circuit cannot be pointwise approximated even with very high\nerror. As our main application, we show that for every sequence of degrees\n$d(n)$, there is an explicit depth-three circuit $F: \\{-1,1\\}^n \\to \\{-1,1\\}$\nof polynomial-size such that any degree-$d$ polynomial cannot pointwise\napproximate $F$ to error better than\n$1-\\exp\\left(-\\tilde{\\Omega}(nd^{-3/2})\\right)$. As a consequence of our main\nresult, we obtain an $\\exp\\left(-\\tilde{\\Omega}(n^{2/5})\\right)$ upper bound on\nthe the discrepancy of a function in AC$^0$, and an\n$\\exp\\left(\\tilde{\\Omega}(n^{2/5})\\right)$ lower bound on the threshold weight\nof AC$^0$, improving over the previous best results of\n$\\exp\\left(-\\Omega(n^{1/3})\\right)$ and $\\exp\\left(\\Omega(n^{1/3})\\right)$\nrespectively.\n  Our techniques also yield a new lower bound of\n$\\Omega\\left(n^{1/2}/\\log^{(d-2)/2}(n)\\right)$ on the approximate degree of the\nAND-OR tree of depth $d$, which is tight up to polylogarithmic factors for any\nconstant $d$, as well as new bounds for read-once DNF formulas. In turn, these\nresults imply new lower bounds on the communication and circuit complexity of\nthese classes, and demonstrate strong limitations on existing PAC learning\nalgorithms. \n\n"}
{"id": "1311.2369", "contents": "Title: The matching polytope has exponential extension complexity Abstract: A popular method in combinatorial optimization is to express polytopes P,\nwhich may potentially have exponentially many facets, as solutions of linear\nprograms that use few extra variables to reduce the number of constraints down\nto a polynomial. After two decades of standstill, recent years have brought\namazing progress in showing lower bounds for the so called extension\ncomplexity, which for a polytope P denotes the smallest number of inequalities\nnecessary to describe a higher dimensional polytope Q that can be linearly\nprojected on P.\n  However, the central question in this field remained wide open: can the\nperfect matching polytope be written as an LP with polynomially many\nconstraints?\n  We answer this question negatively. In fact, the extension complexity of the\nperfect matching polytope in a complete n-node graph is 2^Omega(n). By a known\nreduction this also improves the lower bound on the extension complexity for\nthe TSP polytope from 2^Omega(n^1/2) to 2^Omega(n). \n\n"}
{"id": "1311.3054", "contents": "Title: Losing Weight by Gaining Edges Abstract: We present a new way to encode weighted sums into unweighted pairwise\nconstraints, obtaining the following results.\n  - Define the k-SUM problem to be: given n integers in [-n^2k, n^2k] are there\nk which sum to zero? (It is well known that the same problem over arbitrary\nintegers is equivalent to the above definition, by linear-time randomized\nreductions.) We prove that this definition of k-SUM remains W[1]-hard, and is\nin fact W[1]-complete: k-SUM can be reduced to f(k) * n^o(1) instances of\nk-Clique.\n  - The maximum node-weighted k-Clique and node-weighted k-dominating set\nproblems can be reduced to n^o(1) instances of the unweighted k-Clique and\nk-dominating set problems, respectively. This implies a strong equivalence\nbetween the time complexities of the node weighted problems and the unweighted\nproblems: any polynomial improvement on one would imply an improvement for the\nother.\n  - A triangle of weight 0 in a node weighted graph with m edges can be\ndeterministically found in m^1.41 time. \n\n"}
{"id": "1311.4115", "contents": "Title: A Proof Of The Block Model Threshold Conjecture Abstract: We study a random graph model named the \"block model\" in statistics and the\n\"planted partition model\" in theoretical computer science. In its simplest\nform, this is a random graph with two equal-sized clusters, with a\nbetween-class edge probability of $q$ and a within-class edge probability of\n$p$.\n  A striking conjecture of Decelle, Krzkala, Moore and Zdeborov\\'a based on\ndeep, non-rigorous ideas from statistical physics, gave a precise prediction\nfor the algorithmic threshold of clustering in the sparse planted partition\nmodel. In particular, if $p = a/n$ and $q = b/n$, $s=(a-b)/2$ and $p=(a+b)/2$\nthen Decelle et al.\\ conjectured that it is possible to efficiently cluster in\na way correlated with the true partition if $s^2 > p$ and impossible if $s^2 <\np$. By comparison, the best-known rigorous result is that of Coja-Oghlan, who\nshowed that clustering is possible if $s^2 > C p \\ln p$ for some sufficiently\nlarge $C$.\n  In a previous work, we proved that indeed it is information theoretically\nimpossible to to cluster if $s^2 < p$ and furthermore it is information\ntheoretically impossible to even estimate the model parameters from the graph\nwhen $s^2 < p$. Here we complete the proof of the conjecture by providing an\nefficient algorithm for clustering in a way that is correlated with the true\npartition when $s^2 > p$. A different independent proof of the same result was\nrecently obtained by Laurent Massoulie. \n\n"}
{"id": "1311.4759", "contents": "Title: Approximation Algorithms for Hard Capacitated $k$-facility Location\n  Problems Abstract: We study the capacitated $k$-facility location problem, in which we are given\na set of clients with demands, a set of facilities with capacities and a\nconstant number $k$. It costs $f_i$ to open facility $i$, and $c_{ij}$ for\nfacility $i$ to serve one unit of demand from client $j$. The objective is to\nopen at most $k$ facilities serving all the demands and satisfying the capacity\nconstraints while minimizing the sum of service and opening costs.\n  In this paper, we give the first fully polynomial time approximation scheme\n(FPTAS) for the single-sink (single-client) capacitated $k$-facility location\nproblem. Then, we show that the capacitated $k$-facility location problem with\nuniform capacities is solvable in polynomial time if the number of clients is\nfixed by reducing it to a collection of transportation problems. Third, we\nanalyze the structure of extreme point solutions, and examine the efficiency of\nthis structure in designing approximation algorithms for capacitated\n$k$-facility location problems. Finally, we extend our results to obtain an\nimproved approximation algorithm for the capacitated facility location problem\nwith uniform opening cost. \n\n"}
{"id": "1311.4768", "contents": "Title: Editing to a Graph of Given Degrees Abstract: We consider the Editing to a Graph of Given Degrees problem that asks for a\ngraph G, non-negative integers d,k and a function \\delta:V(G)->{1,...,d},\nwhether it is possible to obtain a graph G' from G such that the degree of v is\n\\delta(v) for any vertex v by at most k vertex or edge deletions or edge\nadditions. We construct an FPT-algorithm for Editing to a Graph of Given\nDegrees parameterized by d+k. We complement this result by showing that the\nproblem has no polynomial kernel unless NP\\subseteq coNP/poly. \n\n"}
{"id": "1311.5481", "contents": "Title: Bi-Criteria and Approximation Algorithms for Restricted Matchings Abstract: In this work we study approximation algorithms for the \\textit{Bounded Color\nMatching} problem (a.k.a. Restricted Matching problem) which is defined as\nfollows: given a graph in which each edge $e$ has a color $c_e$ and a profit\n$p_e \\in \\mathbb{Q}^+$, we want to compute a maximum (cardinality or profit)\nmatching in which no more than $w_j \\in \\mathbb{Z}^+$ edges of color $c_j$ are\npresent. This kind of problems, beside the theoretical interest on its own\nright, emerges in multi-fiber optical networking systems, where we interpret\neach unique wavelength that can travel through the fiber as a color class and\nwe would like to establish communication between pairs of systems. We study\napproximation and bi-criteria algorithms for this problem which are based on\nlinear programming techniques and, in particular, on polyhedral\ncharacterizations of the natural linear formulation of the problem. In our\nsetting, we allow violations of the bounds $w_j$ and we model our problem as a\nbi-criteria problem: we have two objectives to optimize namely (a) to maximize\nthe profit (maximum matching) while (b) minimizing the violation of the color\nbounds. We prove how we can \"beat\" the integrality gap of the natural linear\nprogramming formulation of the problem by allowing only a slight violation of\nthe color bounds. In particular, our main result is \\textit{constant}\napproximation bounds for both criteria of the corresponding bi-criteria\noptimization problem. \n\n"}
{"id": "1311.6209", "contents": "Title: Distributed Algorithms for Large-Scale Graphs Abstract: Motivated by the increasing need for fast processing of large-scale graphs,\nwe study a number of fundamental graph problems in a message-passing model for\ndistributed computing, called $k$-machine model, where we have $k$ machines\nthat jointly perform computations on $n$-node graphs. The graph is assumed to\nbe partitioned in a balanced fashion among the $k$ machines, a common\nimplementation in many real-world systems. Communication is point-to-point via\nbandwidth-constrained links, and the goal is to minimize the round complexity,\ni.e., the number of communication rounds required to finish a computation.\n  We present a generic methodology that allows to obtain efficient algorithms\nin the $k$-machine model using distributed algorithms for the classical CONGEST\nmodel of distributed computing. Using this methodology, we obtain algorithms\nfor various fundamental graph problems such as connectivity, minimum spanning\ntrees, shortest paths, maximal independent sets, and finding subgraphs, showing\nthat many of these problems can be solved in $\\tilde{O}(n/k)$ rounds; this\nshows that one can achieve speedup nearly linear in $k$.\n  To complement our upper bounds, we present lower bounds on the round\ncomplexity that quantify the fundamental limitations of solving graph problems\ndistributively. We first show a lower bound of $\\Omega(n/k)$ rounds for\ncomputing a spanning tree of the input graph. This result implies the same\nbound for other fundamental problems such as computing a minimum spanning tree,\nbreadth-first tree, or shortest paths tree. We also show a $\\tilde\n\\Omega(n/k^2)$ lower bound for connectivity, spanning tree verification and\nother related problems. The latter lower bounds follow from the development and\napplication of novel results in a random-partition variant of the classical\ncommunication complexity model. \n\n"}
{"id": "1311.7178", "contents": "Title: Efficient deterministic approximate counting for low-degree polynomial\n  threshold functions Abstract: We give a deterministic algorithm for approximately counting satisfying\nassignments of a degree-$d$ polynomial threshold function (PTF). Given a\ndegree-$d$ input polynomial $p(x_1,\\dots,x_n)$ over $R^n$ and a parameter\n$\\epsilon> 0$, our algorithm approximates $\\Pr_{x \\sim \\{-1,1\\}^n}[p(x) \\geq\n0]$ to within an additive $\\pm \\epsilon$ in time $O_{d,\\epsilon}(1)\\cdot\n\\mathop{poly}(n^d)$. (Any sort of efficient multiplicative approximation is\nimpossible even for randomized algorithms assuming $NP\\not=RP$.) Note that the\nrunning time of our algorithm (as a function of $n^d$, the number of\ncoefficients of a degree-$d$ PTF) is a \\emph{fixed} polynomial. The fastest\nprevious algorithm for this problem (due to Kane), based on constructions of\nunconditional pseudorandom generators for degree-$d$ PTFs, runs in time\n$n^{O_{d,c}(1) \\cdot \\epsilon^{-c}}$ for all $c > 0$.\n  The key novel contributions of this work are: A new multivariate central\nlimit theorem, proved using tools from Malliavin calculus and Stein's Method.\nThis new CLT shows that any collection of Gaussian polynomials with small\neigenvalues must have a joint distribution which is very close to a\nmultidimensional Gaussian distribution. A new decomposition of low-degree\nmultilinear polynomials over Gaussian inputs. Roughly speaking we show that (up\nto some small error) any such polynomial can be decomposed into a bounded\nnumber of multilinear polynomials all of which have extremely small\neigenvalues. We use these new ingredients to give a deterministic algorithm for\na Gaussian-space version of the approximate counting problem, and then employ\nstandard techniques for working with low-degree PTFs (invariance principles and\nregularity lemmas) to reduce the original approximate counting problem over the\nBoolean hypercube to the Gaussian version. \n\n"}
{"id": "1311.7523", "contents": "Title: The word problem for free adequate semigroups Abstract: We study the complexity of computation in finitely generated free left, right\nand two-sided adequate semigroups and monoids. We present polynomial time\n(quadratic in the RAM model of computation) algorithms to solve the word\nproblem and compute normal forms in each of these, and hence also to test\nwhether any given identity holds in the classes of left, right and/or two-sided\nadequate semigroups. \n\n"}
{"id": "1312.1480", "contents": "Title: Symmetry-forced rigidity of frameworks on surfaces Abstract: A fundamental theorem of Laman characterises when a bar-joint framework\nrealised generically in the Euclidean plane admits a non-trivial continuous\ndeformation of its vertices. This has recently been extended in two ways.\nFirstly to frameworks that are symmetric with respect to some point group but\nare otherwise generic, and secondly to frameworks in Euclidean 3-space that are\nconstrained to lie on 2-dimensional algebraic varieties.\n  We combine these two settings and consider the rigidity of symmetric\nframeworks realised on such surfaces. By extending the orbit matrix techniques\nof [32, 12], we prove necessary conditions for a framework to be\nsymmetry-forced rigid (i.e., to have no non-trivial symmetry-preserving motion)\nfor any group and any surface. In the cases when the surface is a sphere, a\ncylinder or a cone we use Henneberg-type inductive constructions on\ngroup-labeled quotient graphs to prove that these conditions are also\nsufficient for a number of symmetry groups, including rotation, reflection,\ninversion and dihedral symmetry. For the remaining groups - as well as for\nother types of surfaces - we provide some observations and conjectures. \n\n"}
{"id": "1312.2141", "contents": "Title: Dynamic Complexity of Planar 3-connected Graph Isomorphism Abstract: Dynamic Complexity (as introduced by Patnaik and Immerman) tries to express\nhow hard it is to update the solution to a problem when the input is changed\nslightly. It considers the changes required to some stored data structure\n(possibly a massive database) as small quantities of data (or a tuple) are\ninserted or deleted from the database (or a structure over some vocabulary).\nThe main difference from previous notions of dynamic complexity is that instead\nof treating the update quantitatively by finding the the time/space trade-offs,\nit tries to consider the update qualitatively, by finding the complexity class\nin which the update can be expressed (or made). In this setting, DynFO, or\nDynamic First-Order, is one of the smallest and the most natural complexity\nclass (since SQL queries can be expressed in First-Order Logic), and contains\nthose problems whose solutions (or the stored data structure from which the\nsolution can be found) can be updated in First-Order Logic when the data\nstructure undergoes small changes.\n  Etessami considered the problem of isomorphism in the dynamic setting, and\nshowed that Tree Isomorphism can be decided in DynFO. In this work, we show\nthat isomorphism of Planar 3-connected graphs can be decided in DynFO+ (which\nis DynFO with some polynomial precomputation). We maintain a canonical\ndescription of 3-connected Planar graphs by maintaining a database which is\naccessed and modified by First-Order queries when edges are added to or deleted\nfrom the graph. We specifically exploit the ideas of Breadth-First Search and\nCanonical Breadth-First Search to prove the results. We also introduce a novel\nmethod for canonizing a 3-connected planar graph in First-Order Logic from\nCanonical Breadth-First Search Trees. \n\n"}
{"id": "1312.2367", "contents": "Title: High Dimensional Expanders and Property Testing Abstract: We show that the high dimensional expansion property as defined by Gromov,\nLinial and Meshulam, for simplicial complexes is a form of testability. Namely,\na simplicial complex is a high dimensional expander iff a suitable property is\ntestable. Using this connection, we derive several testability results. \n\n"}
{"id": "1312.2889", "contents": "Title: The role of planarity in connectivity problems parameterized by\n  treewidth Abstract: For some years it was believed that for \"connectivity\" problems such as\nHamiltonian Cycle, algorithms running in time 2^{O(tw)}n^{O(1)} -called\nsingle-exponential- existed only on planar and other sparse graph classes,\nwhere tw stands for the treewidth of the n-vertex input graph. This was\nrecently disproved by Cygan et al. [FOCS 2011], Bodlaender et al. [ICALP 2013],\nand Fomin et al. [SODA 2014], who provided single-exponential algorithms on\ngeneral graphs for essentially all connectivity problems that were known to be\nsolvable in single-exponential time on sparse graphs. In this article we\nfurther investigate the role of planarity in connectivity problems\nparameterized by treewidth, and convey that several problems can indeed be\ndistinguished according to their behavior on planar graphs. Known results from\nthe literature imply that there exist problems, like Cycle Packing, that cannot\nbe solved in time 2^{o(tw logtw)}n^{O(1)} on general graphs but that can be\nsolved in time 2^{O(tw)}n^{O(1)} when restricted to planar graphs. Our main\ncontribution is to show that there exist problems that can be solved in time\n2^{O(tw logtw)}n^{O(1)} on general graphs but that cannot be solved in time\n2^{o(tw logtw)}n^{O(1)} even when restricted to planar graphs. Furthermore, we\nprove that Planar Cycle Packing and Planar Disjoint Paths cannot be solved in\ntime 2^{o(tw)}n^{O(1)}. The mentioned negative results hold unless the ETH\nfails. We feel that our results constitute a first step in a subject that can\nbe further exploited. \n\n"}
{"id": "1312.4628", "contents": "Title: Counting Triangulations and other Crossing-free Structures via Onion\n  Layers Abstract: Let $P$ be a set of $n$ points in the plane. A crossing-free structure on $P$\nis a plane graph with vertex set $P$. Examples of crossing-free structures\ninclude triangulations of $P$, spanning cycles of $P$, also known as\npolygonalizations of $P$, among others. In this paper we develop a general\ntechnique for computing the number of crossing-free structures of an input set\n$P$. We apply the technique to obtain algorithms for computing the number of\ntriangulations, matchings, and spanning cycles of $P$. The running time of our\nalgorithms is upper bounded by $n^{O(k)}$, where $k$ is the number of onion\nlayers of $P$. In particular, for $k = O(1)$ our algorithms run in polynomial\ntime. In addition, we show that our algorithm for counting triangulations is\nnever slower than $O^{*}(3.1414^{n})$, even when $k = \\Theta(n)$. Given that\nthere are several well-studied configurations of points with at least\n$\\Omega(3.464^{n})$ triangulations, and some even with $\\Omega(8^{n})$\ntriangulations, our algorithm asymptotically outperforms any enumeration\nalgorithm for such instances. In fact, it is widely believed that any set of\n$n$ points must have at least $\\Omega(3.464^{n})$ triangulations. If this is\ntrue, then our algorithm is strictly sub-linear in the number of triangulations\ncounted. We also show that our techniques are general enough to solve the\n\"Restricted-Triangulation-Counting-Problem\", which we prove to be $W[2]$-hard\nin the parameter $k$. This implies a \"no free lunch\" result: In order to be\nfixed-parameter tractable, our general algorithm must rely on additional\nproperties that are specific to the considered class of structures. \n\n"}
{"id": "1312.5972", "contents": "Title: A Comprehensive Analysis of Polyhedral Lift-and-Project Methods Abstract: We consider lift-and-project methods for combinatorial optimization problems\nand focus mostly on those lift-and-project methods which generate polyhedral\nrelaxations of the convex hull of integer solutions. We introduce many new\nvariants of Sherali--Adams and Bienstock--Zuckerberg operators. These new\noperators fill the spectrum of polyhedral lift-and-project operators in a way\nwhich makes all of them more transparent, easier to relate to each other, and\neasier to analyze. We provide new techniques to analyze the worst-case\nperformances as well as relative strengths of these operators in a unified way.\nIn particular, using the new techniques and a result of Mathieu and Sinclair\nfrom 2009, we prove that the polyhedral Bienstock--Zuckerberg operator requires\nat least $\\sqrt{2n}- \\frac{3}{2}$ iterations to compute the matching polytope\nof the $(2n+1)$-clique. We further prove that the operator requires\napproximately $\\frac{n}{2}$ iterations to reach the stable set polytope of the\n$n$-clique, if we start with the fractional stable set polytope. Lastly, we\nshow that some of the worst-case instances for the positive semidefinite\nLov\\'asz--Schrijver lift-and-project operator are also bad instances for the\nstrongest variants of the Sherali--Adams operator with positive semidefinite\nstrengthenings, and discuss some consequences for integrality gaps of convex\nrelaxations. \n\n"}
{"id": "1312.6550", "contents": "Title: Bi-Factor Approximation Algorithms for Hard Capacitated $k$-Median\n  Problems Abstract: The $k$-Facility Location problem is a generalization of the classical\nproblems $k$-Median and Facility Location. The goal is to select a subset of at\nmost $k$ facilities that minimizes the total cost of opened facilities and\nestablished connections between clients and opened facilities. We consider the\nhard-capacitated version of the problem, where a single facility may only serve\na limited number of clients and creating multiple copies of a facility is not\nallowed. We construct approximation algorithms slightly violating the\ncapacities based on rounding a fractional solution to the standard LP.\n  It is well known that the standard LP (even in the case of uniform capacities\nand opening costs) has unbounded integrality gap if we only allow violating\ncapacities by a factor smaller than $2$, or if we only allow violating the\nnumber of facilities by a factor smaller than $2$. In this paper, we present\nthe first constant-factor approximation algorithms for the hard-capacitated\nvariants of the problem. For uniform capacities, we obtain a\n$(2+\\varepsilon)$-capacity violating algorithm with approximation ratio\n$O(1/\\varepsilon^2)$; our result has not yet been improved. Then, for\nnon-uniform capacities, we consider the case of $k$-Median, which is equivalent\nto $k$-Facility Location with uniform opening cost of the facilities. Here, we\nobtain a $(3+\\varepsilon)$-capacity violating algorithm with approximation\nratio $O(1/\\varepsilon)$. \n\n"}
{"id": "1312.6838", "contents": "Title: Greedy Column Subset Selection for Large-scale Data Sets Abstract: In today's information systems, the availability of massive amounts of data\nnecessitates the development of fast and accurate algorithms to summarize these\ndata and represent them in a succinct format. One crucial problem in big data\nanalytics is the selection of representative instances from large and\nmassively-distributed data, which is formally known as the Column Subset\nSelection (CSS) problem. The solution to this problem enables data analysts to\nunderstand the insights of the data and explore its hidden structure. The\nselected instances can also be used for data preprocessing tasks such as\nlearning a low-dimensional embedding of the data points or computing a low-rank\napproximation of the corresponding matrix. This paper presents a fast and\naccurate greedy algorithm for large-scale column subset selection. The\nalgorithm minimizes an objective function which measures the reconstruction\nerror of the data matrix based on the subset of selected columns. The paper\nfirst presents a centralized greedy algorithm for column subset selection which\ndepends on a novel recursive formula for calculating the reconstruction error\nof the data matrix. The paper then presents a MapReduce algorithm which selects\na few representative columns from a matrix whose columns are massively\ndistributed across several commodity machines. The algorithm first learns a\nconcise representation of all columns using random projection, and it then\nsolves a generalized column subset selection problem at each machine in which a\nsubset of columns are selected from the sub-matrix on that machine such that\nthe reconstruction error of the concise representation is minimized. The paper\ndemonstrates the effectiveness and efficiency of the proposed algorithm through\nan empirical evaluation on benchmark data sets. \n\n"}
{"id": "1312.7468", "contents": "Title: Tree-width and Logspace: Determinants and Counting Euler Tours Abstract: Motivated by the recent result of [EJT10] showing that MSO properties are\nLogspace computable on graphs of bounded tree-width, we consider the complexity\nof computing the determinant of the adjacency matrix of a bounded tree-width\ngraph and prove that it is L-complete. It is important to notice that the\ndeterminant is neither an MSO-property nor counts the number of solutions of an\nMSO-predicate. We extend this technique to count the number of spanning\narborescences and directed Euler tours in bounded tree-width digraphs, and\nfurther to counting the number of spanning trees and the number of Euler tours\nin undirected graphs, all in L. Notice that undirected Euler tours are not\nknown to be MSO-expressible and the corresponding counting problem is in fact\n#P-hard for general graphs. Counting undirected Euler tours in bounded\ntree-width graphs was not known to be polynomial time computable till very\nrecently Chebolu et al [CCM13] gave a polynomial time algorithm for this\nproblem (concurrently and independently of this work). Finally, we also show\nsome linear algebraic extensions of the determinant algorithm to show how to\ncompute the charcteristic polynomial and trace of the powers of a bounded\ntree-width graph in L. \n\n"}
{"id": "1401.1336", "contents": "Title: Finite and infinitesimal rigidity with polyhedral norms Abstract: We characterise finite and infinitesimal rigidity for bar-joint frameworks in\nR^d with respect to polyhedral norms (i.e. norms with closed unit ball P a\nconvex d-dimensional polytope). Infinitesimal and continuous rigidity are shown\nto be equivalent for finite frameworks in R^d which are well-positioned with\nrespect to P. An edge-labelling determined by the facets of the unit ball and\nplacement of the framework is used to characterise infinitesimal rigidity in\nR^d in terms of monochrome spanning trees. An analogue of Laman's theorem is\nobtained for all polyhedral norms on R^2. \n\n"}
{"id": "1401.1450", "contents": "Title: A Recursive Algorithmic Approach to the Finding of Permutations for the\n  Combination of Any Two Sets Abstract: In this paper I present a conjecture for a recursive algorithm that finds\neach permutation of combining two sets of objects (AKA the Shuffle Product).\nThis algorithm provides an efficient way to navigate this problem, as each\natomic operation yields a permutation of the union. The permutations of the\nunion of the two sets are represented as binary integers which are then\nmanipulated mathematically to find the next permutation. The routes taken to\nfind each of the permutations then form a series of associations or adjacencies\nwhich can be represented in a tree graph which appears to possess some\nproperties of a fractal.\n  This algorithm was discovered while attempting to identify every possible\nend-state of a Tic-Tac-Toe (Naughts and Crosses) board. It was found to be a\nviable and efficient solution to the problem, and now---in its more generalized\nstate---it is my belief that it may find applications among a wide range of\ntheoretical and applied sciences.\n  I hypothesize that, due to the fractal-like nature of the tree it traverses,\nthis algorithm sheds light on a more generic principle of combinatorics and as\nsuch could be further generalized to perhaps be applied to the union of any\nnumber of sets. \n\n"}
{"id": "1401.1919", "contents": "Title: Temporal Graph Traversals: Definitions, Algorithms, and Applications Abstract: A temporal graph is a graph in which connections between vertices are active\nat specific times, and such temporal information leads to completely new\npatterns and knowledge that are not present in a non-temporal graph. In this\npaper, we study traversal problems in a temporal graph. Graph traversals, such\nas DFS and BFS, are basic operations for processing and studying a graph. While\nboth DFS and BFS are well-known simple concepts, it is non-trivial to adopt the\nsame notions from a non-temporal graph to a temporal graph. We analyze the\ndifficulties of defining temporal graph traversals and propose new definitions\nof DFS and BFS for a temporal graph. We investigate the properties of temporal\nDFS and BFS, and propose efficient algorithms with optimal complexity. In\nparticular, we also study important applications of temporal DFS and BFS. We\nverify the efficiency and importance of our graph traversal algorithms in real\nworld temporal graphs. \n\n"}
{"id": "1401.2532", "contents": "Title: Parameterized Complexity of Edge Interdiction Problems Abstract: We study the parameterized complexity of interdiction problems in graphs. For\nan optimization problem on graphs, one can formulate an interdiction problem as\na game consisting of two players, namely, an interdictor and an evader, who\ncompete on an objective with opposing interests. In edge interdiction problems,\nevery edge of the input graph has an interdiction cost associated with it and\nthe interdictor interdicts the graph by modifying the edges in the graph, and\nthe number of such modifications is constrained by the interdictor's budget.\nThe evader then solves the given optimization problem on the modified graph.\nThe action of the interdictor must impede the evader as much as possible. We\nfocus on edge interdiction problems related to minimum spanning tree, maximum\nmatching and shortest paths. These problems arise in different real world\nscenarios. We derive several fixed-parameter tractability and W[1]-hardness\nresults for these interdiction problems with respect to various parameters.\nNext, we show close relation between interdiction problems and partial cover\nproblems on bipartite graphs where the goal is not to cover all elements but to\nminimize/maximize the number of covered elements with specific number of sets.\nHereby, we investigate the parameterized complexity of several partial cover\nproblems on bipartite graphs. \n\n"}
{"id": "1401.3638", "contents": "Title: On Lattice-Free Orbit Polytopes Abstract: Given a permutation group acting on coordinates of $\\mathbb{R}^n$, we\nconsider lattice-free polytopes that are the convex hull of an orbit of one\nintegral vector. The vertices of such polytopes are called \\emph{core points}\nand they play a key role in a recent approach to exploit symmetry in integer\nconvex optimization problems. Here, naturally the question arises, for which\ngroups the number of core points is finite up to translations by vectors fixed\nby the group. In this paper we consider transitive permutation groups and prove\nthis type of finiteness for the $2$-homogeneous ones. We provide tools for\npractical computations of core points and obtain a complete list of\nrepresentatives for all $2$-homogeneous groups up to degree twelve. For\ntransitive groups that are not $2$-homogeneous we conjecture that there exist\ninfinitely many core points up to translations by the all-ones-vector. We prove\nour conjecture for two large classes of groups: For imprimitive groups and\ngroups that have an irrational invariant subspace. \n\n"}
{"id": "1401.3916", "contents": "Title: Quantum Hamiltonian Complexity Abstract: Constraint satisfaction problems are a central pillar of modern computational\ncomplexity theory. This survey provides an introduction to the rapidly growing\nfield of Quantum Hamiltonian Complexity, which includes the study of quantum\nconstraint satisfaction problems. Over the past decade and a half, this field\nhas witnessed fundamental breakthroughs, ranging from the establishment of a\n\"Quantum Cook-Levin Theorem\" to deep insights into the structure of 1D\nlow-temperature quantum systems via so-called area laws. Our aim here is to\nprovide a computer science-oriented introduction to the subject in order to\nhelp bridge the language barrier between computer scientists and physicists in\nthe field. As such, we include the following in this survey: (1) The\nmotivations and history of the field, (2) a glossary of condensed matter\nphysics terms explained in computer-science friendly language, (3) overviews of\ncentral ideas from condensed matter physics, such as indistinguishable\nparticles, mean field theory, tensor networks, and area laws, and (4) brief\nexpositions of selected computer science-based results in the area. For\nexample, as part of the latter, we provide a novel information theoretic\npresentation of Bravyi's polynomial time algorithm for Quantum 2-SAT. \n\n"}
{"id": "1401.4689", "contents": "Title: On Keller's conjecture in dimension seven Abstract: A cube tiling of $\\mathbb{R}^d$ is a family of pairwise disjoint cubes\n$[0,1)^d+T=\\{[0,1)^d+t:t\\in T\\}$ such that $\\bigcup_{t\\in\nT}([0,1)^d+t)=\\mathbb{R}^d$. Two cubes $[0,1)^d+t$, $[0,1)^d+s$ are called a\ntwin pair if $|t_j-s_j|=1$ for some $j\\in [d]=\\{1,\\ldots, d\\}$ and $t_i=s_i$\nfor every $i\\in [d]\\setminus \\{j\\}$. In $1930$, Keller conjectured that in\nevery cube tiling of $\\mathbb{R}^d$ there is a twin pair. Keller's conjecture\nis true for dimensions $d\\leq 6$ and false for all dimensions $d\\geq 8$. For\n$d=7$ the conjecture is still open. Let $x\\in \\mathbb{R}^d$, $i\\in [d]$, and\nlet $L(T,x,i)$ be the set of all $i$th coordinates $t_i$ of vectors $t\\in T$\nsuch that $([0,1)^d+t)\\cap ([0,1]^d+x)\\neq \\emptyset$ and $t_i\\leq x_i$. It is\nknown that if $|L(T,x,i)|\\leq 2$ for some $x\\in \\mathbb{R}^7$ and every $i\\in\n[7]$ or $|L(T,x,i)|\\geq 6$ for some $x\\in \\mathbb{R}^7$ and $i\\in [7]$, then\nKeller's conjecture is true for $d=7$. In the present paper we show that it is\nalso true for $d=7$ if $|L(T,x,i)|=5$ for some $x\\in \\mathbb{R}^7$ and $i\\in\n[7]$. Thus, if there is a counterexample to Keller's conjecture in dimension\nseven, then $|L(T,x,i)|\\in \\{3,4\\}$ for some $x\\in \\mathbb{R}^7$ and $i\\in\n[7]$. \n\n"}
{"id": "1401.4720", "contents": "Title: Computing low-degree factors of lacunary polynomials: a Newton-Puiseux\n  approach Abstract: We present a new algorithm for the computation of the irreducible factors of\ndegree at most $d$, with multiplicity, of multivariate lacunary polynomials\nover fields of characteristic zero. The algorithm reduces this computation to\nthe computation of irreducible factors of degree at most $d$ of univariate\nlacunary polynomials and to the factorization of low-degree multivariate\npolynomials. The reduction runs in time polynomial in the size of the input\npolynomial and in $d$. As a result, we obtain a new polynomial-time algorithm\nfor the computation of low-degree factors, with multiplicity, of multivariate\nlacunary polynomials over number fields, but our method also gives partial\nresults for other fields, such as the fields of $p$-adic numbers or for\nabsolute or approximate factorization for instance.\n  The core of our reduction uses the Newton polygon of the input polynomial,\nand its validity is based on the Newton-Puiseux expansion of roots of bivariate\npolynomials. In particular, we bound the valuation of $f(X,\\phi)$ where $f$ is\na lacunary polynomial and $\\phi$ a Puiseux series whose vanishing polynomial\nhas low degree. \n\n"}
{"id": "1401.5383", "contents": "Title: On the representation of de Bruijn graphs Abstract: The de Bruijn graph plays an important role in bioinformatics, especially in\nthe context of de novo assembly. However, the representation of the de Bruijn\ngraph in memory is a computational bottleneck for many assemblers. Recent\npapers proposed a navigational data structure approach in order to improve\nmemory usage. We prove several theoretical space lower bounds to show the\nlimitation of these types of approaches. We further design and implement a\ngeneral data structure (DBGFM) and demonstrate its use on a human whole-genome\ndataset, achieving space usage of 1.5 GB and a 46% improvement over previous\napproaches. As part of DBGFM, we develop the notion of frequency-based\nminimizers and show how it can be used to enumerate all maximal simple paths of\nthe de Bruijn graph using only 43 MB of memory. Finally, we demonstrate that\nour approach can be integrated into an existing assembler by modifying the\nABySS software to use DBGFM. \n\n"}
{"id": "1401.6686", "contents": "Title: Perturbed Message Passing for Constraint Satisfaction Problems Abstract: We introduce an efficient message passing scheme for solving Constraint\nSatisfaction Problems (CSPs), which uses stochastic perturbation of Belief\nPropagation (BP) and Survey Propagation (SP) messages to bypass decimation and\ndirectly produce a single satisfying assignment. Our first CSP solver, called\nPerturbed Blief Propagation, smoothly interpolates two well-known inference\nprocedures; it starts as BP and ends as a Gibbs sampler, which produces a\nsingle sample from the set of solutions. Moreover we apply a similar\nperturbation scheme to SP to produce another CSP solver, Perturbed Survey\nPropagation. Experimental results on random and real-world CSPs show that\nPerturbed BP is often more successful and at the same time tens to hundreds of\ntimes more efficient than standard BP guided decimation. Perturbed BP also\ncompares favorably with state-of-the-art SP-guided decimation, which has a\ncomputational complexity that generally scales exponentially worse than our\nmethod (wrt the cardinality of variable domains and constraints). Furthermore,\nour experiments with random satisfiability and coloring problems demonstrate\nthat Perturbed SP can outperform SP-guided decimation, making it the best\nincomplete random CSP-solver in difficult regimes. \n\n"}
{"id": "1402.0039", "contents": "Title: Linking Rigid Bodies Symmetrically Abstract: The mathematical theory of rigidity of body-bar and body-hinge frameworks\nprovides a useful tool for analyzing the rigidity and flexibility of many\narticulated structures appearing in engineering, robotics and biochemistry. In\nthis paper we develop a symmetric extension of this theory which permits a\nrigidity analysis of body-bar and body-hinge structures with point group\nsymmetries. The infinitesimal rigidity of body-bar frameworks can naturally be\nformulated in the language of the exterior (or Grassmann) algebra. Using this\nalgebraic formulation, we derive symmetry-adapted rigidity matrices to analyze\nthe infinitesimal rigidity of body-bar frameworks with Abelian point group\nsymmetries in an arbitrary dimension. In particular, from the patterns of these\nnew matrices, we derive combinatorial characterizations of infinitesimally\nrigid body-bar frameworks which are generic with respect to a point group of\nthe form $\\mathbb{Z}/2\\mathbb{Z}\\times \\dots \\times \\mathbb{Z}/2\\mathbb{Z}$.\nOur characterizations are given in terms of packings of bases of signed-graphic\nmatroids on quotient graphs. Finally, we also extend our methods and results to\nbody-hinge frameworks with Abelian point group symmetries in an arbitrary\ndimension. As special cases of these results, we obtain combinatorial\ncharacterizations of infinitesimally rigid body-hinge frameworks with\n$\\mathcal{C}_2$ or $\\mathcal{D}_2$ symmetry - the most common symmetry groups\nfound in proteins. \n\n"}
{"id": "1402.0532", "contents": "Title: Approximate Computation of DFT without Performing Any Multiplications:\n  Applications to Radar Signal Processing Abstract: In many practical problems it is not necessary to compute the DFT in a\nperfect manner including some radar problems. In this article a new\nmultiplication free algorithm for approximate computation of the DFT is\nintroduced. All multiplications $(a\\times b)$ in DFT are replaced by an\noperator which computes $sign(a\\times b)(|a|+|b|)$. The new transform is\nespecially useful when the signal processing algorithm requires correlations.\nAmbiguity function in radar signal processing requires high number of\nmultiplications to compute the correlations. This new additive operator is used\nto decrease the number of multiplications. Simulation examples involving\npassive radars are presented. \n\n"}
{"id": "1402.2136", "contents": "Title: Hybridization Number on Three Rooted Binary Trees is EPT Abstract: Phylogenetic networks are leaf-labelled directed acyclic graphs that are used\nto describe non-treelike evolutionary histories and are thus a generalization\nof phylogenetic trees. The hybridization number of a phylogenetic network is\nthe sum of all indegrees minus the number of nodes plus one. The Hybridization\nNumber problem takes as input a collection of phylogenetic trees and asks to\nconstruct a phylogenetic network that contains an embedding of each of the\ninput trees and has a smallest possible hybridization number. We present an\nalgorithm for the Hybridization Number problem on three binary trees on $n$\nleaves, which runs in time $O(c^k poly(n))$, with $k$ the hybridization number\nof an optimal network and $c$ a constant. For two trees, an algorithm with\nrunning time $O(3.18^k n)$ was proposed before whereas an algorithm with\nrunning time $O(c^k poly(n))$ had prior to this article remained elusive for\nmore than two trees. The algorithm for two trees uses the close connection to\nacyclic agreement forests to achieve a linear exponent in the running time,\nwhile previous algorithms for more than two trees (explicitly or implicitly)\nrelied on a brute force search through all possible underlying network\ntopologies, leading to running times that are not $O(c^k poly(n))$ for any $c$.\nThe connection to acyclic agreement forests is much weaker for more than two\ntrees, so even given the right agreement forest, reconstructing the network\nposes major challenges. We prove novel structural results that allow us to\nreconstruct a network without having to guess the underlying topology. Our\ntechniques generalize to more than three input trees with the exception of one\nkey lemma that maps nodes in the network to tree nodes and, thus, minimizes the\namount of guessing involved in constructing the network. The main open problem\ntherefore is to establish a similar mapping for more than three trees. \n\n"}
{"id": "1402.2371", "contents": "Title: On Maximum, Typical and Generic Ranks Abstract: We show that for several notions of rank including tensor rank, Waring rank,\nand generalized rank with respect to a projective variety, the maximum value of\nrank is at most twice the generic rank. We show that over the real numbers, the\nmaximum value of the real rank is at most twice the smallest typical rank,\nwhich is equal to the (complex) generic rank. \n\n"}
{"id": "1402.2801", "contents": "Title: An Anti-Folk Theorem for Large Repeated Games with Imperfect Monitoring Abstract: We study infinitely repeated games in settings of imperfect monitoring. We\nfirst prove a family of theorems that show that when the signals observed by\nthe players satisfy a condition known as $(\\epsilon, \\gamma)$-differential\nprivacy, that the folk theorem has little bite: for values of $\\epsilon$ and\n$\\gamma$ sufficiently small, for a fixed discount factor, any equilibrium of\nthe repeated game involve players playing approximate equilibria of the stage\ngame in every period. Next, we argue that in large games ($n$ player games in\nwhich unilateral deviations by single players have only a small impact on the\nutility of other players), many monitoring settings naturally lead to signals\nthat satisfy $(\\epsilon,\\gamma)$-differential privacy, for $\\epsilon$ and\n$\\gamma$ tending to zero as the number of players $n$ grows large. We conclude\nthat in such settings, the set of equilibria of the repeated game collapse to\nthe set of equilibria of the stage game. \n\n"}
{"id": "1402.2843", "contents": "Title: Sparsification and subexponential approximation Abstract: Instance sparsification is well-known in the world of exact computation since\nit is very closely linked to the Exponential Time Hypothesis. In this paper, we\nextend the concept of sparsification in order to capture subexponential time\napproximation. We develop a new tool for inapproximability, called\napproximation preserving sparsification and use it in order to get strong\ninapproximability results in subexponential time for several fundamental\noptimization problems as Max Independent Set, Min Dominating Set, Min Feedback\nVertex Set, and Min Set Cover. \n\n"}
{"id": "1402.3364", "contents": "Title: Metric tree-like structures in real-life networks: an empirical study Abstract: Based on solid theoretical foundations, we present strong evidences that a\nnumber of real-life networks, taken from different domains like Internet\nmeasurements, biological data, web graphs, social and collaboration networks,\nexhibit tree-like structures from a metric point of view. We investigate few\ngraph parameters, namely, the tree-distortion and the tree-stretch, the\ntree-length and the tree-breadth, the Gromov's hyperbolicity, the\ncluster-diameter and the cluster-radius in a layering partition of a graph,\nwhich capture and quantify this phenomenon of being metrically close to a tree.\nBy bringing all those parameters together, we not only provide efficient means\nfor detecting such metric tree-like structures in large-scale networks but also\nshow how such structures can be used, for example, to efficiently and compactly\nencode approximate distance and almost shortest path information and to fast\nand accurately estimate diameters and radii of those networks. Estimating the\ndiameter and the radius of a graph or distances between its arbitrary vertices\nare fundamental primitives in many data and graph mining algorithms. \n\n"}
{"id": "1402.4346", "contents": "Title: The Complexity of Ferromagnetic Two-spin Systems with External Fields Abstract: We study the approximability of computing the partition function for\nferromagnetic two-state spin systems. The remarkable algorithm by Jerrum and\nSinclair showed that there is a fully polynomial-time randomized approximation\nscheme (FPRAS) for the special ferromagnetic Ising model with any given uniform\nexternal field. Later, Goldberg and Jerrum proved that it is #BIS-hard for\nIsing model if we allow inconsistent external fields on different nodes. In\ncontrast to these two results, we prove that for any ferromagnetic two-state\nspin systems except the Ising model, there exists a threshold for external\nfields beyond which the problem is #BIS-hard, even if the external field is\nuniform. \n\n"}
{"id": "1402.4376", "contents": "Title: On Coloring Resilient Graphs Abstract: We introduce a new notion of resilience for constraint satisfaction problems,\nwith the goal of more precisely determining the boundary between NP-hardness\nand the existence of efficient algorithms for resilient instances. In\nparticular, we study $r$-resiliently $k$-colorable graphs, which are those\n$k$-colorable graphs that remain $k$-colorable even after the addition of any\n$r$ new edges. We prove lower bounds on the NP-hardness of coloring resiliently\ncolorable graphs, and provide an algorithm that colors sufficiently resilient\ngraphs. We also analyze the corresponding notion of resilience for $k$-SAT.\nThis notion of resilience suggests an array of open questions for graph\ncoloring and other combinatorial problems. \n\n"}
{"id": "1402.5687", "contents": "Title: Monoidal computer II: Normal complexity by string diagrams Abstract: In Monoidal Computer I, we introduced a categorical model of computation\nwhere the formal reasoning about computability was supported by the simple and\npopular diagrammatic language of string diagrams. In the present paper, we\nrefine and extend that model of computation to support a formal complexity\ntheory as well. This formalization brings to the foreground the concept of\nnormal complexity measures, which allow decompositions akin to Kleene's normal\nform. Such measures turn out to be just those where evaluating the complexity\nof a program does not require substantially more resources than evaluating the\nprogram itself. The usual time and space complexity are thus normal measures,\nwhereas the average and the randomized complexity measures are not. While the\nmeasures that are not normal provide important design time information about\nalgorithms, and for theoretical analyses, normal measures can also be used at\nrun time, as practical tools of computation, e.g. to set the bounds for\nhypothesis testing, inductive inference and algorithmic learning. \n\n"}
{"id": "1402.6970", "contents": "Title: The P versus NP Problem in Quantum Physics Abstract: Motivated by the fact that information is encoded and processed by physical\nsystems, the P versus NP problem is examined in terms of physical processes. In\nparticular, we consider P as a class of deterministic, and NP as\nnondeterministic, polynomial-time physical processes. Based on these\nidentifications, we review a self-reference physical process in quantum theory,\nwhich belongs to NP but cannot be contained in P. \n\n"}
{"id": "1402.7359", "contents": "Title: Quantum Inference on Bayesian Networks Abstract: Performing exact inference on Bayesian networks is known to be #P-hard.\nTypically approximate inference techniques are used instead to sample from the\ndistribution on query variables given the values $e$ of evidence variables.\nClassically, a single unbiased sample is obtained from a Bayesian network on\n$n$ variables with at most $m$ parents per node in time\n$\\mathcal{O}(nmP(e)^{-1})$, depending critically on $P(e)$, the probability the\nevidence might occur in the first place. By implementing a quantum version of\nrejection sampling, we obtain a square-root speedup, taking\n$\\mathcal{O}(n2^mP(e)^{-\\frac12})$ time per sample. We exploit the Bayesian\nnetwork's graph structure to efficiently construct a quantum state, a q-sample,\nrepresenting the intended classical distribution, and also to efficiently apply\namplitude amplification, the source of our speedup. Thus, our speedup is\nnotable as it is unrelativized -- we count primitive operations and require no\nblackbox oracle queries. \n\n"}
{"id": "1403.2125", "contents": "Title: Two-orbit convex polytopes and tilings Abstract: We classify the convex polytopes whose symmetry groups have two orbits on the\nflags. These exist only in two or three dimensions, and the only ones whose\ncombinatorial automorphism group is also two-orbit are the cuboctahedron, the\nicosidodecahedron, and their duals. The combinatorially regular two-orbit\nconvex polytopes are certain 2n-gons for each n > 1. We also classify the\nface-to-face tilings of Euclidean space by convex polytopes whose symmetry\ngroups have two flag orbits. There are finitely many families, tiling one, two,\nor three dimensions. The only such tilings which are also combinatorially\ntwo-orbit are the trihexagonal plane tiling, the rhombille plane tiling, the\ntetrahedral-octahedral honeycomb, and the rhombic dodecahedral honeycomb. \n\n"}
{"id": "1403.3841", "contents": "Title: Doubles and Negatives are Positive (in Self-Assembly) Abstract: In the abstract Tile Assembly Model (aTAM), the phenomenon of cooperation\noccurs when the attachment of a new tile to a growing assembly requires it to\nbind to more than one tile already in the assembly. Often referred to as\n``temperature-2'' systems, those which employ cooperation are known to be quite\npowerful (i.e. they are computationally universal and can build an enormous\nvariety of shapes and structures). Conversely, aTAM systems which do not\nenforce cooperative behavior, a.k.a. ``temperature-1'' systems, are conjectured\nto be relatively very weak, likely to be unable to perform complex computations\nor algorithmically direct the process of self-assembly. Nonetheless, a variety\nof models based on slight modifications to the aTAM have been developed in\nwhich temperature-1 systems are in fact capable of Turing universal computation\nthrough a restricted notion of cooperation. Despite that power, though, several\nof those models have previously been proven to be unable to perform or simulate\nthe stronger form of cooperation exhibited by temperature-2 aTAM systems.\n  In this paper, we first prove that another model in which temperature-1\nsystems are computationally universal, namely the restricted glue TAM (rgTAM)\nin which tiles are allowed to have edges which exhibit repulsive forces, is\nalso unable to simulate the strongly cooperative behavior of the temperature-2\naTAM. We then show that by combining the properties of two such models, the\nDupled Tile Assembly Model (DTAM) and the rgTAM into the DrgTAM, we derive a\nmodel which is actually more powerful at temperature-1 than the aTAM at\ntemperature-2. Specifically, the DrgTAM, at temperature-1, can simulate any\naTAM system of any temperature, and it also contains systems which cannot be\nsimulated by any system in the aTAM. \n\n"}
{"id": "1403.5830", "contents": "Title: Bejeweled, Candy Crush and other Match-Three Games are (NP-)Hard Abstract: The twentieth century has seen the rise of a new type of video games targeted\nat a mass audience of \"casual\" gamers. Many of these games require the player\nto swap items in order to form matches of three and are collectively known as\n\\emph{tile-matching match-three games}. Among these, the most influential one\nis arguably \\emph{Bejeweled} in which the matched items (gems) pop and the\nabove gems fall in their place. Bejeweled has been ported to many different\nplatforms and influenced an incredible number of similar games. Very recently\none of them, named \\emph{Candy Crush Saga} enjoyed a huge popularity and\nquickly went viral on social networks. We generalize this kind of games by only\nparameterizing the size of the board, while all the other elements (such as the\nrules or the number of gems) remain unchanged. Then, we prove that answering\nmany natural questions regarding such games is actually \\NP-Hard. These\nquestions include determining if the player can reach a certain score, play for\na certain number of turns, and others. We also\n\\href{http://candycrush.isnphard.com}{provide} a playable web-based\nimplementation of our reduction. \n\n"}
{"id": "1403.8144", "contents": "Title: Coding for Random Projections and Approximate Near Neighbor Search Abstract: This technical note compares two coding (quantization) schemes for random\nprojections in the context of sub-linear time approximate near neighbor search.\nThe first scheme is based on uniform quantization while the second scheme\nutilizes a uniform quantization plus a uniformly random offset (which has been\npopular in practice). The prior work compared the two schemes in the context of\nsimilarity estimation and training linear classifiers, with the conclusion that\nthe step of random offset is not necessary and may hurt the performance\n(depending on the similarity level). The task of near neighbor search is\nrelated to similarity estimation with importance distinctions and requires own\nstudy. In this paper, we demonstrate that in the context of near neighbor\nsearch, the step of random offset is not needed either and may hurt the\nperformance (sometimes significantly so, depending on the similarity and other\nparameters). \n\n"}
{"id": "1404.1689", "contents": "Title: Potential of quantum finite automata with exact acceptance Abstract: The potential of the exact quantum information processing is an interesting,\nimportant and intriguing issue. For examples, it has been believed that quantum\ntools can provide significant, that is larger than polynomial, advantages in\nthe case of exact quantum computation only, or mainly, for problems with very\nspecial structures. We will show that this is not the case.\n  In this paper the potential of quantum finite automata producing outcomes not\nonly with a (high) probability, but with certainty (so called exactly) is\nexplored in the context of their uses for solving promise problems and with\nrespect to the size of automata. It is shown that for solving particular\nclasses $\\{A^n\\}_{n=1}^{\\infty}$ of promise problems, even those without some\nvery special structure, that succinctness of the exact quantum finite automata\nunder consideration, with respect to the number of (basis) states, can be very\nsmall (and constant) though it grows proportional to $n$ in the case\ndeterministic finite automata (DFAs) of the same power are used. This is here\ndemonstrated also for the case that the component languages of the promise\nproblems solvable by DFAs are non-regular. The method used can be applied in\nfinding more exact quantum finite automata or quantum algorithms for other\npromise problems. \n\n"}
{"id": "1404.3801", "contents": "Title: Shortest reconfiguration paths in the solution space of Boolean formulas Abstract: Given a Boolean formula and a satisfying assignment, a flip is an operation\nthat changes the value of a variable in the assignment so that the resulting\nassignment remains satisfying. We study the problem of computing the shortest\nsequence of flips (if one exists) that transforms a given satisfying assignment\n$s$ to another satisfying assignment $t$ of a Boolean formula. Earlier work\ncharacterized the complexity of finding any (not necessarily the shortest)\nsequence of flips from one satisfying assignment to another using Schaefer's\nframework for classification of Boolean formulas. We build on it to provide a\ntrichotomy for the complexity of finding the shortest sequence of flips and\nshow that it is either in P, NP-complete, or PSPACE-complete.\n  Our result adds to the small set of complexity results known for shortest\nreconfiguration sequence problems by providing an example where the shortest\nsequence can be found in polynomial time even though its length is not equal to\nthe symmetric difference of the values of the variables in $s$ and $t$. This is\nin contrast to all reconfiguration problems studied so far, where polynomial\ntime algorithms for computing the shortest path were known only for cases where\nthe path modified the symmetric difference only. \n\n"}
{"id": "1404.3820", "contents": "Title: Circuit complexity, proof complexity, and polynomial identity testing Abstract: We introduce a new algebraic proof system, which has tight connections to\n(algebraic) circuit complexity. In particular, we show that any\nsuper-polynomial lower bound on any Boolean tautology in our proof system\nimplies that the permanent does not have polynomial-size algebraic circuits\n(VNP is not equal to VP). As a corollary to the proof, we also show that\nsuper-polynomial lower bounds on the number of lines in Polynomial Calculus\nproofs (as opposed to the usual measure of number of monomials) imply the\nPermanent versus Determinant Conjecture. Note that, prior to our work, there\nwas no proof system for which lower bounds on an arbitrary tautology implied\nany computational lower bound.\n  Our proof system helps clarify the relationships between previous algebraic\nproof systems, and begins to shed light on why proof complexity lower bounds\nfor various proof systems have been so much harder than lower bounds on the\ncorresponding circuit classes. In doing so, we highlight the importance of\npolynomial identity testing (PIT) for understanding proof complexity.\n  More specifically, we introduce certain propositional axioms satisfied by any\nBoolean circuit computing PIT. We use these PIT axioms to shed light on\nAC^0[p]-Frege lower bounds, which have been open for nearly 30 years, with no\nsatisfactory explanation as to their apparent difficulty. We show that either:\na) Proving super-polynomial lower bounds on AC^0[p]-Frege implies VNP does not\nhave polynomial-size circuits of depth d - a notoriously open question for d at\nleast 4 - thus explaining the difficulty of lower bounds on AC^0[p]-Frege, or\nb) AC^0[p]-Frege cannot efficiently prove the depth d PIT axioms, and hence we\nhave a lower bound on AC^0[p]-Frege.\n  Using the algebraic structure of our proof system, we propose a novel way to\nextend techniques from algebraic circuit complexity to prove lower bounds in\nproof complexity. \n\n"}
{"id": "1404.3875", "contents": "Title: Boltzmann samplers for random generation of lambda terms Abstract: Randomly generating structured objects is important in testing and optimizing\nfunctional programs, whereas generating random $'l$-terms is more specifically\nneeded for testing and optimizing compilers. For that a tool called QuickCheck\nhas been proposed, but in this tool the control of the random generation is\nleft to the programmer. Ten years ago, a method called Boltzmann samplers has\nbeen proposed to generate combinatorial structures. In this paper, we show how\nBoltzmann samplers can be developed to generate lambda-terms, but also other\ndata structures like trees. These samplers rely on a critical value which\nparameters the main random selector and which is exhibited here with\nexplanations on how it is computed. Haskell programs are proposed to show how\nsamplers are actually implemented. \n\n"}
{"id": "1405.0093", "contents": "Title: Parameterized Streaming Algorithms for Vertex Cover Abstract: As graphs continue to grow in size, we seek ways to effectively process such\ndata at scale. The model of streaming graph processing, in which a compact\nsummary is maintained as each edge insertion/deletion is observed, is an\nattractive one. However, few results are known for optimization problems over\nsuch dynamic graph streams.\n  In this paper, we introduce a new approach to handling graph streams, by\ninstead seeking solutions for the parameterized versions of these problems\nwhere we are given a parameter $k$ and the objective is to decide whether there\nis a solution bounded by $k$. By combining kernelization techniques with\nrandomized sketch structures, we obtain the first streaming algorithms for the\nparameterized versions of the Vertex Cover problem. We consider the following\nthree models for a graph stream on $n$ nodes:\n  1. The insertion-only model where the edges can only be added.\n  2. The dynamic model where edges can be both inserted and deleted.\n  3. The \\emph{promised} dynamic model where we are guaranteed that at each\ntimestamp there is a solution of size at most $k$.\n  In each of these three models we are able to design parameterized streaming\nalgorithms for the Vertex Cover problem. We are also able to show matching\nlower bound for the space complexity of our algorithms.\n  (Due to the arXiv limit of 1920 characters for abstract field, please see the\nabstract in the paper for detailed description of our results) \n\n"}
{"id": "1405.0527", "contents": "Title: Parallel computation using active self-assembly Abstract: We study the computational complexity of the recently proposed nubot model of\nmolecular-scale self-assembly. The model generalises asynchronous cellular\nautomata to have non-local movement where large assemblies of molecules can be\npushed and pulled around, analogous to millions of molecular motors in animal\nmuscle effecting the rapid movement of macroscale arms and legs. We show that\nthe nubot model is capable of simulating Boolean circuits of polylogarithmic\ndepth and polynomial size, in only polylogarithmic expected time. In\ncomputational complexity terms, we show that any problem from the complexity\nclass NC is solvable in polylogarithmic expected time and polynomial workspace\nusing nubots.\n  Along the way, we give fast parallel nubot algorithms for a number of\nproblems including line growth, sorting, Boolean matrix multiplication and\nspace-bounded Turing machine simulation, all using a constant number of nubot\nstates (monomer types). Circuit depth is a well-studied notion of parallel\ntime, and our result implies that the nubot model is a highly parallel model of\ncomputation in a formal sense. Asynchronous cellular automata are not capable\nof this parallelism, and our result shows that adding a rigid-body movement\nprimitive to such a model, to get the nubot model, drastically increases\nparallel processing abilities. \n\n"}
{"id": "1405.4917", "contents": "Title: An Algebraic Hardness Criterion for Surjective Constraint Satisfaction Abstract: The constraint satisfaction problem (CSP) on a relational structure B is to\ndecide, given a set of constraints on variables where the relations come from\nB, whether or not there is a assignment to the variables satisfying all of the\nconstraints; the surjective CSP is the variant where one decides the existence\nof a surjective satisfying assignment onto the universe of B. We present an\nalgebraic condition on the polymorphism clone of B and prove that it is\nsufficient for the hardness of the surjective CSP on a finite structure B, in\nthe sense that this problem admits a reduction from a certain fixed-structure\nCSP. To our knowledge, this is the first result that allows one to use\nalgebraic information from a relational structure B to infer information on the\ncomplexity hardness of surjective constraint satisfaction on B. A corollary of\nour result is that, on any finite non-trivial structure having only essentially\nunary polymorphisms, surjective constraint satisfaction is NP-complete. \n\n"}
{"id": "1405.4993", "contents": "Title: On extensions of Minkowski's theorem on successive minima Abstract: Minkowski's 2nd theorem in the Geometry of Numbers provides optimal upper and\nlower bounds for the volume of a $o$-symmetric convex body in terms of its\nsuccessive minima. In this paper we study extensions of this theorem from two\ndifferent points of view: either relaxing the symmetry condition, assuming that\nthe centroid of the set lies at the origin, or replacing the volume functional\nby the surface area. \n\n"}
{"id": "1405.5754", "contents": "Title: Twenty-Five Comparators is Optimal when Sorting Nine Inputs (and\n  Twenty-Nine for Ten) Abstract: This paper describes a computer-assisted non-existence proof of nine-input\nsorting networks consisting of 24 comparators, hence showing that the\n25-comparator sorting network found by Floyd in 1964 is optimal. As a\ncorollary, we obtain that the 29-comparator network found by Waksman in 1969 is\noptimal when sorting ten inputs.\n  This closes the two smallest open instances of the optimal size sorting\nnetwork problem, which have been open since the results of Floyd and Knuth from\n1966 proving optimality for sorting networks of up to eight inputs.\n  The proof involves a combination of two methodologies: one based on\nexploiting the abundance of symmetries in sorting networks, and the other,\nbased on an encoding of the problem to that of satisfiability of propositional\nlogic. We illustrate that, while each of these can single handed solve smaller\ninstances of the problem, it is their combination which leads to an efficient\nsolution for nine inputs. \n\n"}
{"id": "1405.6791", "contents": "Title: Agnostic Learning of Disjunctions on Symmetric Distributions Abstract: We consider the problem of approximating and learning disjunctions (or\nequivalently, conjunctions) on symmetric distributions over $\\{0,1\\}^n$.\nSymmetric distributions are distributions whose PDF is invariant under any\npermutation of the variables. We give a simple proof that for every symmetric\ndistribution $\\mathcal{D}$, there exists a set of $n^{O(\\log{(1/\\epsilon)})}$\nfunctions $\\mathcal{S}$, such that for every disjunction $c$, there is function\n$p$, expressible as a linear combination of functions in $\\mathcal{S}$, such\nthat $p$ $\\epsilon$-approximates $c$ in $\\ell_1$ distance on $\\mathcal{D}$ or\n$\\mathbf{E}_{x \\sim \\mathcal{D}}[ |c(x)-p(x)|] \\leq \\epsilon$. This directly\ngives an agnostic learning algorithm for disjunctions on symmetric\ndistributions that runs in time $n^{O( \\log{(1/\\epsilon)})}$. The best known\nprevious bound is $n^{O(1/\\epsilon^4)}$ and follows from approximation of the\nmore general class of halfspaces (Wimmer, 2010). We also show that there exists\na symmetric distribution $\\mathcal{D}$, such that the minimum degree of a\npolynomial that $1/3$-approximates the disjunction of all $n$ variables is\n$\\ell_1$ distance on $\\mathcal{D}$ is $\\Omega( \\sqrt{n})$. Therefore the\nlearning result above cannot be achieved via $\\ell_1$-regression with a\npolynomial basis used in most other agnostic learning algorithms.\n  Our technique also gives a simple proof that for any product distribution\n$\\mathcal{D}$ and every disjunction $c$, there exists a polynomial $p$ of\ndegree $O(\\log{(1/\\epsilon)})$ such that $p$ $\\epsilon$-approximates $c$ in\n$\\ell_1$ distance on $\\mathcal{D}$. This was first proved by Blais et al.\n(2008) via a more involved argument. \n\n"}
{"id": "1405.6874", "contents": "Title: Disk-based genome sequencing data compression Abstract: Motivation: High-coverage sequencing data have significant, yet hard to\nexploit, redundancy. Most FASTQ compressors cannot efficiently compress the DNA\nstream of large datasets, since the redundancy between overlapping reads cannot\nbe easily captured in the (relatively small) main memory. More interesting\nsolutions for this problem are disk-based~(Yanovsky, 2011; Cox et al., 2012),\nwhere the better of these two, from Cox~{\\it et al.}~(2012), is based on the\nBurrows--Wheeler transform (BWT) and achieves 0.518 bits per base for a 134.0\nGb human genome sequencing collection with almost 45-fold coverage.\n  Results: We propose ORCOM (Overlapping Reads COmpression with Minimizers), a\ncompression algorithm dedicated to sequencing reads (DNA only). Our method\nmakes use of a conceptually simple and easily parallelizable idea of\nminimizers, to obtain 0.317 bits per base as the compression ratio, allowing to\nfit the 134.0 Gb dataset into only 5.31 GB of space.\n  Availability: http://sun.aei.polsl.pl/orcom under a free license. \n\n"}
{"id": "1405.7112", "contents": "Title: Optimal query complexity for estimating the trace of a matrix Abstract: Given an implicit $n\\times n$ matrix $A$ with oracle access $x^TA x$ for any\n$x\\in \\mathbb{R}^n$, we study the query complexity of randomized algorithms for\nestimating the trace of the matrix. This problem has many applications in\nquantum physics, machine learning, and pattern matching. Two metrics are\ncommonly used for evaluating the estimators: i) variance; ii) a high\nprobability multiplicative-approximation guarantee. Almost all the known\nestimators are of the form $\\frac{1}{k}\\sum_{i=1}^k x_i^T A x_i$ for $x_i\\in\n\\mathbb{R}^n$ being i.i.d. for some special distribution.\n  Our main results are summarized as follows. We give an exact characterization\nof the minimum variance unbiased estimator in the broad class of linear\nnonadaptive estimators (which subsumes all the existing known estimators). We\nalso consider the query complexity lower bounds for any (possibly nonlinear and\nadaptive) estimators: (1) We show that any estimator requires\n$\\Omega(1/\\epsilon)$ queries to have a guarantee of variance at most\n$\\epsilon$. (2) We show that any estimator requires\n$\\Omega(\\frac{1}{\\epsilon^2}\\log \\frac{1}{\\delta})$ queries to achieve a\n$(1\\pm\\epsilon)$-multiplicative approximation guarantee with probability at\nleast $1 - \\delta$. Both above lower bounds are asymptotically tight.\n  As a corollary, we also resolve a conjecture in the seminal work of Avron and\nToledo (Journal of the ACM 2011) regarding the sample complexity of the\nGaussian Estimator. \n\n"}
{"id": "1405.7300", "contents": "Title: Radio Network Lower Bounds Made Easy Abstract: Theoreticians have studied distributed algorithms in the radio network model\nfor close to three decades. A significant fraction of this work focuses on\nlower bounds for basic communication problems such as wake-up (symmetry\nbreaking among an unknown set of nodes) and broadcast (message dissemination\nthrough an unknown network topology). In this paper, we introduce a new\ntechnique for proving this type of bound, based on reduction from a\nprobabilistic hitting game, that simplifies and strengthens much of this\nexisting work. In more detail, in this single paper we prove new expected time\nand high probability lower bounds for wake-up and global broadcast in single\nand multichannel versions of the radio network model both with and without\ncollision detection. In doing so, we are able to reproduce results that\npreviously spanned a half-dozen papers published over a period of twenty-five\nyears. In addition to simplifying these existing results, our technique, in\nmany places, also improves the state of the art: of the eight bounds we prove,\nfour strictly strengthen the best known previous result (in terms of time\ncomplexity and/or generality of the algorithm class for which it holds), and\nthree provide the first known non-trivial bound for the case in question. The\nfact that the same technique can easily generate this diverse collection of\nlower bounds indicates a surprising unity underlying communication tasks in the\nradio network model---revealing that deep down, below the specifics of the\nproblem definition and model assumptions, communication in this setting reduces\nto finding efficient strategies for a simple game. \n\n"}
{"id": "1406.0576", "contents": "Title: Welfare and Revenue Guarantees for Competitive Bundling Equilibrium Abstract: We study equilibria of markets with $m$ heterogeneous indivisible goods and\n$n$ consumers with combinatorial preferences. It is well known that a\ncompetitive equilibrium is not guaranteed to exist when valuations are not\ngross substitutes. Given the widespread use of bundling in real-life markets,\nwe study its role as a stabilizing and coordinating device by considering the\nnotion of \\emph{competitive bundling equilibrium}: a competitive equilibrium\nover the market induced by partitioning the goods for sale into fixed bundles.\nCompared to other equilibrium concepts involving bundles, this notion has the\nadvantage of simulatneous succinctness ($O(m)$ prices) and market clearance.\n  Our first set of results concern welfare guarantees. We show that in markets\nwhere consumers care only about the number of goods they receive (known as\nmulti-unit or homogeneous markets), even in the presence of complementarities,\nthere always exists a competitive bundling equilibrium that guarantees a\nlogarithmic fraction of the optimal welfare, and this guarantee is tight. We\nalso establish non-trivial welfare guarantees for general markets, two-consumer\nmarkets, and markets where the consumer valuations are additive up to a fixed\nbudget (budget-additive).\n  Our second set of results concern revenue guarantees. Motivated by the fact\nthat the revenue extracted in a standard competitive equilibrium may be zero\n(even with simple unit-demand consumers), we show that for natural subclasses\nof gross substitutes valuations, there always exists a competitive bundling\nequilibrium that extracts a logarithmic fraction of the optimal welfare, and\nthis guarantee is tight. The notion of competitive bundling equilibrium can\nthus be useful even in markets which possess a standard competitive\nequilibrium. \n\n"}
{"id": "1406.3671", "contents": "Title: Max-min Fair Rate Allocation and Routing in Energy Harvesting Networks:\n  Algorithmic Analysis Abstract: This paper considers max-min fair rate allocation and routing in energy\nharvesting networks where fairness is required among both the nodes and the\ntime slots. Unlike most previous work on fairness, we focus on multihop\ntopologies and consider different routing methods. We assume a predictable\nenergy profile and focus on the design of efficient and optimal algorithms that\ncan serve as benchmarks for distributed and approximate algorithms. We first\ndevelop an algorithm that obtains a max-min fair rate assignment for any given\n(time-variable or time-invariable) unsplittable routing or a routing tree. For\ntime-invariable unsplittable routing, we also develop an algorithm that finds\nroutes that maximize the minimum rate assigned to any node in any slot. For\nfractional routing, we study the joint routing and rate assignment problem. We\ndevelop an algorithm for the time-invariable case with constant rates. We show\nthat the time-variable case is at least as hard as the 2-commodity feasible\nflow problem and design an FPTAS to combat the high running time. Finally, we\nshow that finding an unsplittable routing or a routing tree that provides\nlexicographically maximum rate assignment (i.e., that is the best in the\nmax-min fairness terms) is NP-hard, even for a time horizon of a single slot.\nOur analysis provides insights into the problem structure and can be applied to\nother related fairness problems. \n\n"}
{"id": "1406.5279", "contents": "Title: Tensor network non-zero testing Abstract: Tensor networks are a central tool in condensed matter physics. In this\npaper, we study the task of tensor network non-zero testing (TNZ): Given a\ntensor network T, does T represent a non-zero vector? We show that TNZ is not\nin the Polynomial-Time Hierarchy unless the hierarchy collapses. We next show\n(among other results) that the special cases of TNZ on non-negative and\ninjective tensor networks are in NP. Using this, we make a simple observation:\nThe commuting variant of the MA-complete stoquastic k-SAT problem on\nD-dimensional qudits is in NP for logarithmic k and constant D. This reveals\nthe first class of quantum Hamiltonians whose commuting variant is known to be\nin NP for all (1) logarithmic k, (2) constant D, and (3) for arbitrary\ninteraction graphs. \n\n"}
{"id": "1406.5336", "contents": "Title: On the Complexity of Trial and Error for Constraint Satisfaction\n  Problems Abstract: In 2013 Bei, Chen and Zhang introduced a trial and error model of computing,\nand applied to some constraint satisfaction problems. In this model the input\nis hidden by an oracle which, for a candidate assignment, reveals some\ninformation about a violated constraint if the assignment is not satisfying. In\nthis paper we initiate a {\\em systematic} study of constraint satisfaction\nproblems in the trial and error model. To achieve this, we first adopt a formal\nframework for CSPs, and based on this framework we define several types of\nrevealing oracles. Our main contribution is to develop a \\emph{transfer\ntheorem} for each type of the revealing oracle, under a broad class of\nparameters. To any hidden CSP with a specific type of revealing oracle, the\ntransfer theorem associates another, potentially harder CSP in the normal\nsetting, such that their complexities are polynomial time equivalent. This in\nprinciple transfers the study of a large class of hidden CSPs, possibly with a\npromise on the instances, to the study of CSPs in the normal setting. We then\napply the transfer theorems to get polynomial-time algorithms or hardness\nresults for hidden CSPs, including satisfaction problems, monotone graph\nproperties, isomorphism problems, and the exact version of the Unique Games\nproblem. Most of the proofs of these results are short and straightforward,\nwhich exhibits the power of the transfer theorems. \n\n"}
{"id": "1406.5667", "contents": "Title: Correlation Clustering with Noisy Partial Information Abstract: In this paper, we propose and study a semi-random model for the Correlation\nClustering problem on arbitrary graphs G. We give two approximation algorithms\nfor Correlation Clustering instances from this model. The first algorithm finds\na solution of value $(1+ \\delta) optcost + O_{\\delta}(n\\log^3 n)$ with high\nprobability, where $optcost$ is the value of the optimal solution (for every\n$\\delta > 0$). The second algorithm finds the ground truth clustering with an\narbitrarily small classification error $\\eta$ (under some additional\nassumptions on the instance). \n\n"}
{"id": "1406.6889", "contents": "Title: Noncooperative algorithms in self-assembly Abstract: We show the first non-trivial positive algorithmic results (i.e. programs\nwhose output is larger than their size), in a model of self-assembly that has\nso far resisted many attempts of formal analysis or programming: the planar\nnon-cooperative variant of Winfree's abstract Tile Assembly Model.\n  This model has been the center of several open problems and conjectures in\nthe last fifteen years, and the first fully general results on its\ncomputational power were only proven recently (SODA 2014). These results, as\nwell as ours, exemplify the intricate connections between computation and\ngeometry that can occur in self-assembly.\n  In this model, tiles can stick to an existing assembly as soon as one of\ntheir sides matches the existing assembly. This feature contrasts with the\ngeneral cooperative model, where it can be required that tiles match on\n\\emph{several} of their sides in order to bind.\n  In order to describe our algorithms, we also introduce a generalization of\nregular expressions called Baggins expressions. Finally, we compare this model\nto other automata-theoretic models. \n\n"}
{"id": "1407.0085", "contents": "Title: Improved Quantum Algorithm for Triangle Finding via Combinatorial\n  Arguments Abstract: In this paper we present a quantum algorithm solving the triangle finding\nproblem in unweighted graphs with query complexity $\\tilde O(n^{5/4})$, where\n$n$ denotes the number of vertices in the graph. This improves the previous\nupper bound $O(n^{9/7})=O(n^{1.285...})$ recently obtained by Lee, Magniez and\nSantha. Our result shows, for the first time, that in the quantum query\ncomplexity setting unweighted triangle finding is easier than its edge-weighted\nversion, since for finding an edge-weighted triangle Belovs and Rosmanis proved\nthat any quantum algorithm requires $\\Omega(n^{9/7}/\\sqrt{\\log n})$ queries.\nOur result also illustrates some limitations of the non-adaptive learning graph\napproach used to obtain the previous $O(n^{9/7})$ upper bound since, even over\nunweighted graphs, any quantum algorithm for triangle finding obtained using\nthis approach requires $\\Omega(n^{9/7}/\\sqrt{\\log n})$ queries as well. To\nbypass the obstacles characterized by these lower bounds, our quantum algorithm\nuses combinatorial ideas exploiting the graph-theoretic properties of triangle\nfinding, which cannot be used when considering edge-weighted graphs or the\nnon-adaptive learning graph approach. \n\n"}
{"id": "1407.0375", "contents": "Title: Mathematical and Algorithmic Analysis of Network and Biological Data Abstract: This dissertation contributes to mathematical and algorithmic problems that\narise in the analysis of network and biological data. \n\n"}
{"id": "1407.1571", "contents": "Title: Private Multiplicative Weights Beyond Linear Queries Abstract: A wide variety of fundamental data analyses in machine learning, such as\nlinear and logistic regression, require minimizing a convex function defined by\nthe data. Since the data may contain sensitive information about individuals,\nand these analyses can leak that sensitive information, it is important to be\nable to solve convex minimization in a privacy-preserving way.\n  A series of recent results show how to accurately solve a single convex\nminimization problem in a differentially private manner. However, the same data\nis often analyzed repeatedly, and little is known about solving multiple convex\nminimization problems with differential privacy. For simpler data analyses,\nsuch as linear queries, there are remarkable differentially private algorithms\nsuch as the private multiplicative weights mechanism (Hardt and Rothblum, FOCS\n2010) that accurately answer exponentially many distinct queries. In this work,\nwe extend these results to the case of convex minimization and show how to give\naccurate and differentially private solutions to *exponentially many* convex\nminimization problems on a sensitive dataset. \n\n"}
{"id": "1407.1870", "contents": "Title: Spectral norm of random tensors Abstract: We show that the spectral norm of a random $n_1\\times n_2\\times \\cdots \\times\nn_K$ tensor (or higher-order array) scales as\n$O\\left(\\sqrt{(\\sum_{k=1}^{K}n_k)\\log(K)}\\right)$ under some sub-Gaussian\nassumption on the entries. The proof is based on a covering number argument.\nSince the spectral norm is dual to the tensor nuclear norm (the tightest convex\nrelaxation of the set of rank one tensors), the bound implies that the convex\nrelaxation yields sample complexity that is linear in (the sum of) the number\nof dimensions, which is much smaller than other recently proposed convex\nrelaxations of tensor rank that use unfolding. \n\n"}
{"id": "1407.1925", "contents": "Title: Using Optimization to Solve Positive LPs Faster in Parallel Abstract: Positive linear programs (LP), also known as packing and covering linear\nprograms, are an important class of problems that bridges computer science,\noperations research, and optimization. Despite the consistent efforts on this\nproblem, all known nearly-linear-time algorithms require\n$\\tilde{O}(\\varepsilon^{-4})$ iterations to converge to $1\\pm \\varepsilon$\napproximate solutions. This $\\varepsilon^{-4}$ dependence has not been improved\nsince 1993, and limits the performance of parallel implementations for such\nalgorithms. Moreover, previous algorithms and their analyses rely on update\nsteps and convergence arguments that are combinatorial in nature and do not\nseem to arise naturally from an optimization viewpoint.\n  In this paper, we leverage new insights from optimization theory to construct\na novel algorithm that breaks the longstanding $\\varepsilon^{-4}$ barrier. Our\nalgorithm has a simple analysis and a clear motivation. Our work introduces a\nnumber of novel techniques, such as the combined application of gradient\ndescent and mirror descent, and a truncated, smoothed version of the standard\nmultiplicative weight update, which may be of independent interest. \n\n"}
{"id": "1407.2178", "contents": "Title: Restricted Isometry Property for General p-Norms Abstract: The Restricted Isometry Property (RIP) is a fundamental property of a matrix\nwhich enables sparse recovery. Informally, an $m \\times n$ matrix satisfies RIP\nof order $k$ for the $\\ell_p$ norm, if $\\|Ax\\|_p \\approx \\|x\\|_p$ for every\nvector $x$ with at most $k$ non-zero coordinates.\n  For every $1 \\leq p < \\infty$ we obtain almost tight bounds on the minimum\nnumber of rows $m$ necessary for the RIP property to hold. Prior to this work,\nonly the cases $p = 1$, $1 + 1 / \\log k$, and $2$ were studied. Interestingly,\nour results show that the case $p = 2$ is a \"singularity\" point: the optimal\nnumber of rows $m$ is $\\widetilde{\\Theta}(k^{p})$ for all $p\\in\n[1,\\infty)\\setminus \\{2\\}$, as opposed to $\\widetilde{\\Theta}(k)$ for $k=2$.\n  We also obtain almost tight bounds for the column sparsity of RIP matrices\nand discuss implications of our results for the Stable Sparse Recovery problem. \n\n"}
{"id": "1407.3263", "contents": "Title: LP-Based Algorithms for Capacitated Facility Location Abstract: Linear programming has played a key role in the study of algorithms for\ncombinatorial optimization problems. In the field of approximation algorithms,\nthis is well illustrated by the uncapacitated facility location problem. A\nvariety of algorithmic methodologies, such as LP-rounding and primal-dual\nmethod, have been applied to and evolved from algorithms for this problem.\nUnfortunately, this collection of powerful algorithmic techniques had not yet\nbeen applicable to the more general capacitated facility location problem. In\nfact, all of the known algorithms with good performance guarantees were based\non a single technique, local search, and no linear programming relaxation was\nknown to efficiently approximate the problem.\n  In this paper, we present a linear programming relaxation with constant\nintegrality gap for capacitated facility location. We demonstrate that the\nfundamental theories of multi-commodity flows and matchings provide key\ninsights that lead to the strong relaxation. Our algorithmic proof of\nintegrality gap is obtained by finally accessing the rich toolbox of LP-based\nmethodologies: we present a constant factor approximation algorithm based on\nLP-rounding. \n\n"}
{"id": "1407.4094", "contents": "Title: Ignorance is Almost Bliss: Near-Optimal Stochastic Matching With Few\n  Queries Abstract: The stochastic matching problem deals with finding a maximum matching in a\ngraph whose edges are unknown but can be accessed via queries. This is a\nspecial case of stochastic $k$-set packing, where the problem is to find a\nmaximum packing of sets, each of which exists with some probability. In this\npaper, we provide edge and set query algorithms for these two problems,\nrespectively, that provably achieve some fraction of the omniscient optimal\nsolution.\n  Our main theoretical result for the stochastic matching (i.e., $2$-set\npacking) problem is the design of an \\emph{adaptive} algorithm that queries\nonly a constant number of edges per vertex and achieves a $(1-\\epsilon)$\nfraction of the omniscient optimal solution, for an arbitrarily small\n$\\epsilon>0$. Moreover, this adaptive algorithm performs the queries in only a\nconstant number of rounds. We complement this result with a \\emph{non-adaptive}\n(i.e., one round of queries) algorithm that achieves a $(0.5 - \\epsilon)$\nfraction of the omniscient optimum. We also extend both our results to\nstochastic $k$-set packing by designing an adaptive algorithm that achieves a\n$(\\frac{2}{k} - \\epsilon)$ fraction of the omniscient optimal solution, again\nwith only $O(1)$ queries per element. This guarantee is close to the best known\npolynomial-time approximation ratio of $\\frac{3}{k+1} -\\epsilon$ for the\n\\emph{deterministic} $k$-set packing problem [Furer and Yu, 2013]\n  We empirically explore the application of (adaptations of) these algorithms\nto the kidney exchange problem, where patients with end-stage renal failure\nswap willing but incompatible donors. We show on both generated data and on\nreal data from the first 169 match runs of the UNOS nationwide kidney exchange\nthat even a very small number of non-adaptive edge queries per vertex results\nin large gains in expected successful matches. \n\n"}
{"id": "1407.6178", "contents": "Title: Computing the $2$-blocks of directed graphs Abstract: Let $G$ be a directed graph. A \\textit{$2$-directed block} in $G$ is a\nmaximal vertex set $C^{2d}\\subseteq V$ with $|C^{2d}|\\geq 2$ such that for each\npair of distinct vertices $x,y \\in C^{2d}$, there exist two vertex-disjoint\npaths from $x$ to $y$ and two vertex-disjoint paths from $y$ to $x$ in $G$. In\ncontrast to the $2$-vertex-connected components of $G$, the subgraphs induced\nby the $2$-directed blocks may consist of few or no edges. In this paper we\npresent two algorithms for computing the $2$-directed blocks of $G$ in\n$O(\\min\\lbrace m,(t_{sap}+t_{sb})n\\rbrace n)$ time, where $t_{sap}$ is the\nnumber of the strong articulation points of $G$ and $t_{sb}$ is the number of\nthe strong bridges of $G$. Furthermore, we study two related concepts: the\n$2$-strong blocks and the $2$-edge blocks of $G$. We give two algorithms for\ncomputing the $2$-strong blocks of $G$ in $O( \\min \\lbrace m,t_{sap} n\\rbrace\nn)$ time and we show that the $2$-edge blocks of $G$ can be computed in $O(\\min\n\\lbrace m, t_{sb} n \\rbrace n)$ time. In this paper we also study some\noptimization problems related to the strong articulation points and the\n$2$-blocks of a directed graph. Given a strongly connected graph $G=(V,E)$,\nfind a minimum cardinality set $E^{*}\\subseteq E$ such that $G^{*}=(V,E^{*})$\nis strongly connected and the strong articulation points of $G$ coincide with\nthe strong articulation points of $G^{*}$. This problem is called minimum\nstrongly connected spanning subgraph with the same strong articulation points.\nWe show that there is a linear time $17/3$ approximation algorithm for this\nNP-hard problem. We also consider the problem of finding a minimum strongly\nconnected spanning subgraph with the same $2$-blocks in a strongly connected\ngraph $G$. We present approximation algorithms for three versions of this\nproblem, depending on the type of $2$-blocks. \n\n"}
{"id": "1407.6958", "contents": "Title: Chip-firing games on Eulerian digraphs and NP-hardness of computing the\n  rank of a divisor on a graph Abstract: Baker and Norine introduced a graph-theoretic analogue of the Riemann-Roch\ntheory. A central notion in this theory is the rank of a divisor. In this paper\nwe prove that computing the rank of a divisor on a graph is NP-hard.\n  The determination of the rank of a divisor can be translated to a question\nabout a chip-firing game on the same underlying graph. We prove the NP-hardness\nof this question by relating chip-firing on directed and undirected graphs. \n\n"}
{"id": "1407.7763", "contents": "Title: Social choice, computational complexity, Gaussian geometry, and Boolean\n  functions Abstract: We describe a web of connections between the following topics: the\nmathematical theory of voting and social choice; the computational complexity\nof the Maximum Cut problem; the Gaussian Isoperimetric Inequality and Borell's\ngeneralization thereof; the Hypercontractive Inequality of Bonami; and, the\nanalysis of Boolean functions. A major theme is the technique of reducing\ninequalities about Gaussian functions to inequalities about Boolean functions f\n: {-1,1}^n -> {-1,1}, and then using induction on n to further reduce to\ninequalities about functions f : {-1,1} -> {-1,1}. We especially highlight De,\nMossel, and Neeman's recent use of this technique to prove the Majority Is\nStablest Theorem and Borell's Isoperimetric Inequality simultaneously. \n\n"}
{"id": "1407.7887", "contents": "Title: Going for Speed: Sublinear Algorithms for Dense r-CSPs Abstract: We give new sublinear and parallel algorithms for the extensively studied\nproblem of approximating n-variable r-CSPs (constraint satisfaction problems\nwith constraints of arity r up to an additive error. The running time of our\nalgorithms is O(n/\\epsilon^2) + 2^O(1/\\epsilon^2) for Boolean r-CSPs and O(k^4\nn / \\epsilon^2) + 2^O(log k / \\epsilon^2) for r-CSPs with constraints on\nvariables over an alphabet of size k. For any constant k this gives optimal\ndependence on n in the running time unconditionally, while the exponent in the\ndependence on 1/\\epsilon is polynomially close to the lower bound under the\nexponential-time hypothesis, which is 2^\\Omega(\\epsilon^(-1/2)).\n  For Max-Cut this gives an exponential improvement in dependence on 1/\\epsilon\ncompared to the sublinear algorithms of Goldreich, Goldwasser and Ron (JACM'98)\nand a linear speedup in n compared to the algorithms of Mathieu and Schudy\n(SODA'08). For the maximization version of k-Correlation Clustering problem our\nrunning time is O(k^4 n / \\epsilon^2) + k^O(1/\\epsilon^2), improving the\npreviously best n k^{O(1/\\epsilon^3 log k/\\epsilon) by Guruswami and Giotis\n(SODA'06). \n\n"}
{"id": "1408.0751", "contents": "Title: Spectral Approaches to Nearest Neighbor Search Abstract: We study spectral algorithms for the high-dimensional Nearest Neighbor Search\nproblem (NNS). In particular, we consider a semi-random setting where a dataset\n$P$ in $\\mathbb{R}^d$ is chosen arbitrarily from an unknown subspace of low\ndimension $k\\ll d$, and then perturbed by fully $d$-dimensional Gaussian noise.\nWe design spectral NNS algorithms whose query time depends polynomially on $d$\nand $\\log n$ (where $n=|P|$) for large ranges of $k$, $d$ and $n$. Our\nalgorithms use a repeated computation of the top PCA vector/subspace, and are\neffective even when the random-noise magnitude is {\\em much larger} than the\ninterpoint distances in $P$. Our motivation is that in practice, a number of\nspectral NNS algorithms outperform the random-projection methods that seem\notherwise theoretically optimal on worst case datasets. In this paper we aim to\nprovide theoretical justification for this disparity. \n\n"}
{"id": "1408.1000", "contents": "Title: Estimating Renyi Entropy of Discrete Distributions Abstract: It was recently shown that estimating the Shannon entropy $H({\\rm p})$ of a\ndiscrete $k$-symbol distribution ${\\rm p}$ requires $\\Theta(k/\\log k)$ samples,\na number that grows near-linearly in the support size. In many applications\n$H({\\rm p})$ can be replaced by the more general R\\'enyi entropy of order\n$\\alpha$, $H_\\alpha({\\rm p})$. We determine the number of samples needed to\nestimate $H_\\alpha({\\rm p})$ for all $\\alpha$, showing that $\\alpha < 1$\nrequires a super-linear, roughly $k^{1/\\alpha}$ samples, noninteger $\\alpha>1$\nrequires a near-linear $k$ samples, but, perhaps surprisingly, integer\n$\\alpha>1$ requires only $\\Theta(k^{1-1/\\alpha})$ samples. Furthermore,\ndeveloping on a recently established connection between polynomial\napproximation and estimation of additive functions of the form $\\sum_{x} f({\\rm\np}_x)$, we reduce the sample complexity for noninteger values of $\\alpha$ by a\nfactor of $\\log k$ compared to the empirical estimator. The estimators\nachieving these bounds are simple and run in time linear in the number of\nsamples. Our lower bounds provide explicit constructions of distributions with\ndifferent R\\'enyi entropies that are hard to distinguish. \n\n"}
{"id": "1408.1577", "contents": "Title: Towards More Practical Linear Programming-based Techniques for\n  Algorithmic Mechanism Design Abstract: R. Lavy and C. Swamy (FOCS 2005, J. ACM 2011) introduced a general method for\nobtaining truthful-in-expectation mechanisms from linear programming based\napproximation algorithms. Due to the use of the Ellipsoid method, a direct\nimplementation of the method is unlikely to be efficient in practice. We\npropose to use the much simpler and usually faster multiplicative weights\nupdate method instead. The simplification comes at the cost of slightly weaker\napproximation and truthfulness guarantees. \n\n"}
{"id": "1408.3522", "contents": "Title: The Ihara Zeta function for infinite graphs Abstract: We put forward the concept of measure graphs. These are (possibly\nuncountable) graphs equipped with an action of a groupoid and a measure\ninvariant under this action. Examples include finite graphs, periodic graphs,\ngraphings and percolation graphs. Making use of Connes' non-commutative\nintegration theory we construct a Zeta function and present a determinant\nformula for it. We further introduce a notion of weak convergence of measure\ngraphs and show that our construction is compatible with it. The approximation\nof the Ihara Zeta function via the normalized version on finite graphs in the\nsense of Benjamini-Schramm follows as a special case. Our framework not only\nunifies corresponding earlier results occurring in the literature. It likewise\nprovides extensions to rich new classes of objects such as percolation graphs. \n\n"}
{"id": "1408.3590", "contents": "Title: Complexity of Nondeterministic Graph Parameter Testing Abstract: We study the sample complexity of nondeterministically testable graph\nparameters and improve existing bounds on it by several orders of magnitude.\nThe technique used would be also of independent interest. We also discuss the\nspecial case of weak nondeterministic testing for uniform hypergraphs of\narbitrary order. \n\n"}
{"id": "1408.4830", "contents": "Title: Measure Partitions Using Hyperplanes with Fixed Directions Abstract: We study nested partitions of $R^d$ obtained by successive cuts using\nhyperplanes with fixed directions. We establish the number of measures that can\nbe split evenly simultaneously by taking a partition of this kind and then\ndistributing the parts among $k$ sets. This generalises classical necklace\nsplitting results and their more recent high-dimensional versions. With similar\nmethods we show that in the plane, for any $t$ measures there is a path formed\nonly by horizontal and vertical segments using at most $t-1$ turns that splits\nthem by half simultaneously, and optimal mass-partitioning results for\nchessboard-colourings of $R^d$ using hyperplanes with fixed directions. \n\n"}
{"id": "1409.0035", "contents": "Title: Computing Classic Closeness Centrality, at Scale Abstract: Closeness centrality, first considered by Bavelas (1948), is an importance\nmeasure of a node in a network which is based on the distances from the node to\nall other nodes. The classic definition, proposed by Bavelas (1950), Beauchamp\n(1965), and Sabidussi (1966), is (the inverse of) the average distance to all\nother nodes.\n  We propose the first highly scalable (near linear-time processing and linear\nspace overhead) algorithm for estimating, within a small relative error, the\nclassic closeness centralities of all nodes in the graph. Our algorithm applies\nto undirected graphs, as well as for centrality computed with respect to\nround-trip distances in directed graphs.\n  For directed graphs, we also propose an efficient algorithm that approximates\ngeneralizations of classic closeness centrality to outbound and inbound\ncentralities. Although it does not provide worst-case theoretical approximation\nguarantees, it is designed to perform well on real networks.\n  We perform extensive experiments on large networks, demonstrating high\nscalability and accuracy. \n\n"}
{"id": "1409.0451", "contents": "Title: Computational complexity of solving polynomial differential equations\n  over unbounded domains Abstract: In this paper we investigate the computational complexity of solving ordinary\ndifferential equations (ODEs) $y^{\\prime}=p(y)$ over \\emph{unbounded time\ndomains}, where $p$ is a vector of polynomials. Contrarily to the bounded\n(compact) time case, this problem has not been well-studied, apparently due to\nthe \"intuition\" that it can always be reduced to the bounded case by using\nrescaling techniques. However, as we show in this paper, rescaling techniques\ndo not seem to provide meaningful insights on the complexity of this problem,\nsince the use of such techniques introduces a dependence on parameters which\nare hard to compute.\n  We present algorithms which numerically solve these ODEs over unbounded time\ndomains. These algorithms have guaranteed accuracy, i.e. given some arbitrarily\nlarge time $t$ and error bound $\\varepsilon$ as input, they will output a value\n$\\tilde{y}$ which satisfies $\\|y(t)-\\tilde{y}\\|\\leq\\varepsilon$. We analyze the\ncomplexity of these algorithms and show that they compute $\\tilde{y}$ in time\npolynomial in several quantities including the time $t$, the accuracy of the\noutput $\\varepsilon$ and the length of the curve $y$ from $0$ to $t$, assuming\nit exists until time $t$. We consider both algebraic complexity and bit\ncomplexity. \n\n"}
{"id": "1409.2042", "contents": "Title: Recommendation Subgraphs for Web Discovery Abstract: Recommendations are central to the utility of many websites including\nYouTube, Quora as well as popular e-commerce stores. Such sites typically\ncontain a set of recommendations on every product page that enables visitors to\neasily navigate the website. Choosing an appropriate set of recommendations at\neach page is one of the key features of backend engines that have been deployed\nat several e-commerce sites.\n  Specifically at BloomReach, an engine consisting of several independent\ncomponents analyzes and optimizes its clients' websites. This paper focuses on\nthe structure optimizer component which improves the website navigation\nexperience that enables the discovery of novel content.\n  We begin by formalizing the concept of recommendations used for discovery. We\nformulate this as a natural graph optimization problem which in its simplest\ncase, reduces to a bipartite matching problem. In practice, solving these\nmatching problems requires superlinear time and is not scalable. Also,\nimplementing simple algorithms is critical in practice because they are\nsignificantly easier to maintain in production. This motivated us to analyze\nthree methods for solving the problem in increasing order of sophistication: a\nsampling algorithm, a greedy algorithm and a more involved partitioning based\nalgorithm.\n  We first theoretically analyze the performance of these three methods on\nrandom graph models characterizing when each method will yield a solution of\nsufficient quality and the parameter ranges when more sophistication is needed.\nWe complement this by providing an empirical analysis of these algorithms on\nsimulated and real-world production data. Our results confirm that it is not\nalways necessary to implement complicated algorithms in the real-world and that\nvery good practical results can be obtained by using heuristics that are backed\nby the confidence of concrete theoretical guarantees. \n\n"}
{"id": "1409.2138", "contents": "Title: Streaming Lower Bounds for Approximating MAX-CUT Abstract: We consider the problem of estimating the value of max cut in a graph in the\nstreaming model of computation. At one extreme, there is a trivial\n$2$-approximation for this problem that uses only $O(\\log n)$ space, namely,\ncount the number of edges and output half of this value as the estimate for max\ncut value. On the other extreme, if one allows $\\tilde{O}(n)$ space, then a\nnear-optimal solution to the max cut value can be obtained by storing an\n$\\tilde{O}(n)$-size sparsifier that essentially preserves the max cut. An\nintriguing question is if poly-logarithmic space suffices to obtain a\nnon-trivial approximation to the max-cut value (that is, beating the factor\n$2$). It was recently shown that the problem of estimating the size of a\nmaximum matching in a graph admits a non-trivial approximation in\npoly-logarithmic space.\n  Our main result is that any streaming algorithm that breaks the\n$2$-approximation barrier requires $\\tilde{\\Omega}(\\sqrt{n})$ space even if the\nedges of the input graph are presented in random order. Our result is obtained\nby exhibiting a distribution over graphs which are either bipartite or\n$\\frac{1}{2}$-far from being bipartite, and establishing that\n$\\tilde{\\Omega}(\\sqrt{n})$ space is necessary to differentiate between these\ntwo cases. Thus as a direct corollary we obtain that $\\tilde{\\Omega}(\\sqrt{n})$\nspace is also necessary to test if a graph is bipartite or $\\frac{1}{2}$-far\nfrom being bipartite.\n  We also show that for any $\\epsilon > 0$, any streaming algorithm that\nobtains a $(1 + \\epsilon)$-approximation to the max cut value when edges arrive\nin adversarial order requires $n^{1 - O(\\epsilon)}$ space, implying that\n$\\Omega(n)$ space is necessary to obtain an arbitrarily good approximation to\nthe max cut value. \n\n"}
{"id": "1409.3892", "contents": "Title: Weakly modular graphs and nonpositive curvature Abstract: This article investigates structural, geometrical, and topological\ncharacterizations and properties of weakly modular graphs and of cell complexes\nderived from them. The unifying themes of our investigation are various\n`nonpositive curvature' and `local-to-global' properties and characterizations\nof weakly modular graphs and their subclasses. Weakly modular graphs have been\nintroduced as a far-reaching common generalization of median graphs (and more\ngenerally, of modular and orientable modular graphs), Helly graphs, bridged\ngraphs, and dual polar graphs occurring under different disguises in several\nseemingly-unrelated fields of mathematics: Metric graph theory, Geometric group\ntheory, Incidence geometries and buildings, Theoretical computer science and\ncombinatorial optimization. We give a local-to-global characterization of\nweakly modular graphs and their subclasses in terms of simple connectedness of\nassociated triangle-square complexes and specific local combinatorial\nconditions. In particular, we revisit characterizations of dual polar graphs by\nCameron and by Brouwer-Cohen. We also show that (disk-)Helly graphs are\nprecisely the clique-Helly graphs with simply connected clique complexes. With\n$l_1$-embeddable weakly modular and sweakly modular graphs we associate\nhigh-dimensional cell complexes, having several strong topological and\ngeometrical properties (contractibility and the CAT(0) property). Their cells\nhave a specific structure: they are basis polyhedra of even\n$\\triangle$-matroids in the first case and orthoscheme complexes of gated dual\npolar subgraphs in the second case. We resolve some open problems concerning\nsubclasses of weakly modular graphs: we prove a Brady-McCammond conjecture\nabout CAT(0) metric on the orthoscheme complexes of modular lattices; we answer\nChastand's question about prime graphs for pre-median graphs. \n\n"}
{"id": "1409.5641", "contents": "Title: Lempel-Ziv Factorization May Be Harder Than Computing All Runs Abstract: The complexity of computing the Lempel-Ziv factorization and the set of all\nruns (= maximal repetitions) is studied in the decision tree model of\ncomputation over ordered alphabet. It is known that both these problems can be\nsolved by RAM algorithms in $O(n\\log\\sigma)$ time, where $n$ is the length of\nthe input string and $\\sigma$ is the number of distinct letters in it. We prove\nan $\\Omega(n\\log\\sigma)$ lower bound on the number of comparisons required to\nconstruct the Lempel-Ziv factorization and thereby conclude that a popular\ntechnique of computation of runs using the Lempel-Ziv factorization cannot\nachieve an $o(n\\log\\sigma)$ time bound. In contrast with this, we exhibit an\n$O(n)$ decision tree algorithm finding all runs in a string. Therefore, in the\ndecision tree model the runs problem is easier than the Lempel-Ziv\nfactorization. Thus we support the conjecture that there is a linear RAM\nalgorithm finding all runs. \n\n"}
{"id": "1409.6241", "contents": "Title: Approximating Betweenness Centrality in Large Evolving Networks Abstract: Betweenness centrality ranks the importance of nodes by their participation\nin all shortest paths of the network. Therefore computing exact betweenness\nvalues is impractical in large networks. For static networks, approximation\nbased on randomly sampled paths has been shown to be significantly faster in\npractice. However, for dynamic networks, no approximation algorithm for\nbetweenness centrality is known that improves on static recomputation. We\naddress this deficit by proposing two incremental approximation algorithms (for\nweighted and unweighted connected graphs) which provide a provable guarantee on\nthe absolute approximation error. Processing batches of edge insertions, our\nalgorithms yield significant speedups up to a factor of $10^4$ compared to\nrestarting the approximation. This is enabled by investing memory to store and\nefficiently update shortest paths. As a building block, we also propose an\nasymptotically faster algorithm for updating the SSSP problem in unweighted\ngraphs. Our experimental study shows that our algorithms are the first to make\nin-memory computation of a betweenness ranking practical for million-edge\nsemi-dynamic networks. Moreover, our results show that the accuracy is even\nbetter than the theoretical guarantees in terms of absolutes errors and the\nrank of nodes is well preserved, in particular for those with high betweenness. \n\n"}
{"id": "1409.6277", "contents": "Title: 2-Vertex Connectivity in Directed Graphs Abstract: We complement our study of 2-connectivity in directed graphs, by considering\nthe computation of the following 2-vertex-connectivity relations: We say that\ntwo vertices v and w are 2-vertex-connected if there are two internally\nvertex-disjoint paths from v to w and two internally vertex-disjoint paths from\nw to v. We also say that v and w are vertex-resilient if the removal of any\nvertex different from v and w leaves v and w in the same strongly connected\ncomponent. We show how to compute the above relations in linear time so that we\ncan report in constant time if two vertices are 2-vertex-connected or if they\nare vertex-resilient. We also show how to compute in linear time a sparse\ncertificate for these relations, i.e., a subgraph of the input graph that has\nO(n) edges and maintains the same 2-vertex-connectivity and vertex-resilience\nrelations as the input graph, where n is the number of vertices. \n\n"}
{"id": "1409.6792", "contents": "Title: Commuting Quantum Circuits with Few Outputs are Unlikely to be\n  Classically Simulatable Abstract: We study the classical simulatability of commuting quantum circuits with n\ninput qubits and O(log n) output qubits, where a quantum circuit is classically\nsimulatable if its output probability distribution can be sampled up to an\nexponentially small additive error in classical polynomial time. First, we show\nthat there exists a commuting quantum circuit that is not classically\nsimulatable unless the polynomial hierarchy collapses to the third level. This\nis the first formal evidence that a commuting quantum circuit is not\nclassically simulatable even when the number of output qubits is exponentially\nsmall. Then, we consider a generalized version of the circuit and clarify the\ncondition under which it is classically simulatable. Lastly, we apply the\nargument for the above evidence to Clifford circuits in a similar setting and\nprovide evidence that such a circuit augmented by a depth-1 non-Clifford layer\nis not classically simulatable. These results reveal subtle differences between\nquantum and classical computation. \n\n"}
{"id": "1409.7771", "contents": "Title: Global Information Sharing under Network Dynamics Abstract: We study how to spread $k$ tokens of information to every node on an $n$-node\ndynamic network, the edges of which are changing at each round. This basic {\\em\ngossip problem} can be completed in $O(n + k)$ rounds in any static network,\nand determining its complexity in dynamic networks is central to understanding\nthe algorithmic limits and capabilities of various dynamic network models. Our\nfocus is on token-forwarding algorithms, which do not manipulate tokens in any\nway other than storing, copying and forwarding them.\n  We first consider the {\\em strongly adaptive} adversary model where in each\nround, each node first chooses a token to broadcast to all its neighbors\n(without knowing who they are), and then an adversary chooses an arbitrary\nconnected communication network for that round with the knowledge of the tokens\nchosen by each node. We show that $\\Omega(nk/\\log n + n)$ rounds are needed for\nany randomized (centralized or distributed) token-forwarding algorithm to\ndisseminate the $k$ tokens, thus resolving an open problem raised\nin~\\cite{kuhn+lo:dynamic}. The bound applies to a wide class of initial token\ndistributions, including those in which each token is held by exactly one node\nand {\\em well-mixed} ones in which each node has each token independently with\na constant probability.\n  We also show several upper bounds in varying models. \n\n"}
{"id": "1410.0562", "contents": "Title: A massively parallel algorithm for constructing the BWT of large string\n  sets Abstract: We present a new scalable, lightweight algorithm to incrementally construct\nthe BWT and FM-index of large string sets such as those produced by Next\nGeneration Sequencing. The algorithm is designed for massive parallelism and\ncan effectively exploit the combination of low capacity high bandwidth memory\nand slower external system memory typical of GPU accelerated systems.\nParticularly, for a string set of n characters from an alphabet with \\sigma\nsymbols, it uses a constant amount of high-bandwidth memory and at most 3n\nlog(\\sigma) bits of system memory. Given that deep memory hierarchies are\nbecoming a pervasive trait of high performance computing architectures, we\nbelieve this to be a relevant feature. The implementation can handle reads of\narbitrary length and is up to 2 and respectively 6.5 times faster than\nstate-of-the-art for short and long genomic reads \n\n"}
{"id": "1410.2595", "contents": "Title: Spatial mixing and the connective constant: Optimal bounds Abstract: We study the problem of deterministic approximate counting of matchings and\nindependent sets in graphs of bounded connective constant. More generally, we\nconsider the problem of evaluating the partition functions of the monomer-dimer\nmodel (which is defined as a weighted sum over all matchings where each\nmatching is given a weight $\\gamma^{|V| - 2 |M|}$ in terms of a fixed parameter\ngamma called the monomer activity) and the hard core model (which is defined as\na weighted sum over all independent sets where an independent set I is given a\nweight $\\lambda^{|I|}$ in terms of a fixed parameter lambda called the vertex\nactivity). The connective constant is a natural measure of the average degree\nof a graph which has been studied extensively in combinatorics and mathematical\nphysics, and can be bounded by a constant even for certain unbounded degree\ngraphs such as those sampled from the sparse Erd\\H{o}s-R\\'enyi model $G(n,\nd/n)$.\n  Our main technical contribution is to prove the best possible rates of decay\nof correlations in the natural probability distributions induced by both the\nhard core model and the monomer-dimer model in graphs with a given bound on the\nconnective constant. These results on decay of correlations are obtained using\na new framework based on the so-called message approach that has been\nextensively used recently to prove such results for bounded degree graphs. We\nthen use these optimal decay of correlations results to obtain FPTASs for the\ntwo problems on graphs of bounded connective constant.\n  Our techniques also allow us to improve upon known bounds for decay of\ncorrelations for the hard core model on various regular lattices, including\nthose obtained by Restrepo, Shin, Vigoda and Tetali (2011) for the special case\nof Z^2 using sophisticated numerically intensive methods tailored to that\nspecial case. \n\n"}
{"id": "1410.2652", "contents": "Title: More Natural Models of Electoral Control by Partition Abstract: \"Control\" studies attempts to set the outcome of elections through the\naddition, deletion, or partition of voters or candidates. The set of benchmark\ncontrol types was largely set in the seminal 1992 paper by Bartholdi, Tovey,\nand Trick that introduced control, and there now is a large literature studying\nhow many of the benchmark types various election systems are vulnerable to,\ni.e., have polynomial-time attack algorithms for.\n  However, although the longstanding benchmark models of addition and deletion\nmodel relatively well the real-world settings that inspire them, the\nlongstanding benchmark models of partition model settings that are arguably\nquite distant from those they seek to capture.\n  In this paper, we introduce--and for some important cases analyze the\ncomplexity of--new partition models that seek to better capture many real-world\npartition settings. In particular, in many partition settings one wants the two\nparts of the partition to be of (almost) equal size, or is partitioning into\nmore than two parts, or has groups of actors who must be placed in the same\npart of the partition. Our hope is that having these new partition types will\nallow studies of control attacks to include such models that more realistically\ncapture many settings. \n\n"}
{"id": "1410.5410", "contents": "Title: Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner\n  Product Search (MIPS) Abstract: Recently it was shown that the problem of Maximum Inner Product Search (MIPS)\nis efficient and it admits provably sub-linear hashing algorithms. Asymmetric\ntransformations before hashing were the key in solving MIPS which was otherwise\nhard. In the prior work, the authors use asymmetric transformations which\nconvert the problem of approximate MIPS into the problem of approximate near\nneighbor search which can be efficiently solved using hashing. In this work, we\nprovide a different transformation which converts the problem of approximate\nMIPS into the problem of approximate cosine similarity search which can be\nefficiently solved using signed random projections. Theoretical analysis show\nthat the new scheme is significantly better than the original scheme for MIPS.\nExperimental evaluations strongly support the theoretical findings. \n\n"}
{"id": "1410.7251", "contents": "Title: A metric characterisation of repulsive tilings Abstract: A tiling of $\\mathbb{R}^d$ is repulsive if no $r$-patch can repeat\narbitrarily close to itself, relative to $r$. This is a characteristic property\nof aperiodic order, for a non repulsive tiling has arbitrarily large local\nperiodic patterns.\n  We consider an aperiodic, repetitive tiling $T$ of $\\mathbb{R}^d$, with\nfinite local complexity. From a spectral triple built on the discrete hull\n$\\Xi$ of $T$, and its Connes distance, we derive two metrics $d_{sup}$ and\n$d_{inf}$ on $\\Xi$. We show that $T$ is repulsive if and only if $d_{sup}$ and\n$d_{inf}$ are Lipschitz equivalent. This generalises previous works for\nsubshifts by J. Kellendonk, D. Lenz, and the author. \n\n"}
{"id": "1410.8816", "contents": "Title: Affine reductions for LPs and SDPs Abstract: We define a reduction mechanism for LP and SDP formulations that degrades\napproximation factors in a controlled fashion. Our reduction mechanism is a\nminor restriction of classical reductions establishing inapproximability in the\ncontext of PCP theorems. As a consequence we establish strong linear\nprogramming inapproximability (for LPs with a polynomial number of constraints)\nfor many problems. In particular we obtain a $3/2-\\varepsilon$\ninapproximability for VertexCover answering an open question in\n[arXiv:1309.0563] and we answer a weak version of our sparse graph conjecture\nposed in [arXiv:1311.4001] showing an inapproximability factor of\n$1/2+\\varepsilon$ for bounded degree IndependentSet. In the case of SDPs, we\nobtain inapproximability results for these problems relative to the\nSDP-inapproximability of MaxCUT. Moreover, using our reduction framework we are\nable to reproduce various results for CSPs from [arXiv:1309.0563] via simple\nreductions from Max-2-XOR. \n\n"}
{"id": "1411.0871", "contents": "Title: An exact characterization of tractable demand patterns for maximum\n  disjoint path problems Abstract: We study the following general disjoint paths problem: given a supply graph\n$G$, a set $T\\subseteq V(G)$ of terminals, a demand graph $H$ on the vertices\n$T$, and an integer $k$, the task is to find a set of $k$ pairwise\nvertex-disjoint valid paths, where we say that a path of the supply graph $G$\nis valid if its endpoints are in $T$ and adjacent in the demand graph $H$. For\na class $\\mathcal{H}$ of graphs, we denote by $\\mathcal{H}$-Maximum Disjoint\nPaths the restriction of this problem when the demand graph $H$ is assumed to\nbe a member of $\\mathcal{H}$. We study the fixed-parameter tractability of this\nfamily of problems, parameterized by $k$. Our main result is a complete\ncharacterization of the fixed-parameter tractable cases of\n$\\mathcal{H}$-Maximum Disjoint Paths for every hereditary class $\\mathcal{H}$\nof graphs: it turns out that complexity depends on the existence of large\ninduced matchings and large induced skew bicliques in the demand graph $H$ (a\nskew biclique is a bipartite graph on vertices $a_1$, $\\dots$, $a_n$, $b_1$,\n$\\dots$, $b_n$ with $a_i$ and $b_j$ being adjacent if and only if $i\\le j$).\nSpecifically, we prove the following classification for every hereditary class\n$\\mathcal{H}$.\n  1. If $\\mathcal{H}$ does not contain every matching and does not contain\nevery skew biclique, then $\\mathcal{H}$-Maximum Disjoint Paths is FPT.\n  2. If $\\mathcal{H}$ does not contain every matching, but contains every skew\nbiclique, then $\\mathcal{H}$-Maximum Disjoint Paths is W[1]-hard, admits an FPT\napproximation, and the valid paths satisfy an analog of the Erd\\H{o}s-P\\'osa\nproperty.\n  3. If $\\mathcal{H}$ contains every matching, then $\\mathcal{H}$-Maximum\nDisjoint Paths is W[1]-hard and the valid paths do not satisfy the analog of\nthe Erd\\H{o}s-P\\'osa property. \n\n"}
{"id": "1411.1782", "contents": "Title: Combinatorially two-orbit convex polytopes Abstract: Any convex polytope whose combinatorial automorphism group has two orbits on\nthe flags is isomorphic to one whose group of Euclidean symmetries has two\norbits on the flags (equivalently, to one whose automorphism group and symmetry\ngroup coincide.) Hence, a combinatorially two-orbit convex polytope is\nisomorphic to one of a known finite list, all of which are 3-dimensional: the\ncuboctahedron, icosidodecahedron, rhombic dodecahedron, or rhombic\ntriacontahedron. The same is true of combinatorially two-orbit normal\nface-to-face tilings by convex polytopes. \n\n"}
{"id": "1411.1977", "contents": "Title: Towards an Isomorphism Dichotomy for Hereditary Graph Classes Abstract: In this paper we resolve the complexity of the isomorphism problem on all but\nfinitely many of the graph classes characterized by two forbidden induced\nsubgraphs. To this end we develop new techniques applicable for the structural\nand algorithmic analysis of graphs. First, we develop a methodology to show\nisomorphism completeness of the isomorphism problem on graph classes by\nproviding a general framework unifying various reduction techniques. Second, we\ngeneralize the concept of the modular decomposition to colored graphs, allowing\nfor non-standard decompositions. We show that, given a suitable decomposition\nfunctor, the graph isomorphism problem reduces to checking isomorphism of\ncolored prime graphs. Third, we extend the techniques of bounded color valence\nand hypergraph isomorphism on hypergraphs of bounded color size as follows. We\nsay a colored graph has generalized color valence at most k if, after removing\nall vertices in color classes of size at most k, for each color class C every\nvertex has at most k neighbors in C or at most k non-neighbors in C. We show\nthat isomorphism of graphs of bounded generalized color valence can be solved\nin polynomial time. \n\n"}
{"id": "1411.2785", "contents": "Title: Faster Compressed Quadtrees Abstract: Real-world point sets tend to be clustered, so using a machine word for each\npoint is wasteful. In this paper we first show how a compact representation of\nquadtrees using $\\Oh{1}$ bits per node can break this bound on clustered point\nsets, while offering efficient range searches. We then describe a new compact\nquadtree representation based on heavy path decompositions, which supports\nqueries faster than previous compact structures. We present experimental\nevidence showing that our structure is competitive in practice. \n\n"}
{"id": "1411.3164", "contents": "Title: A linear time algorithm for the orbit problem over cyclic groups Abstract: The orbit problem is at the heart of symmetry reduction methods for model\nchecking concurrent systems. It asks whether two given configurations in a\nconcurrent system (represented as finite strings over some finite alphabet) are\nin the same orbit with respect to a given finite permutation group (represented\nby their generators) acting on this set of configurations by permuting indices.\nIt is known that the problem is in general as hard as the graph isomorphism\nproblem, whose precise complexity (whether it is solvable in polynomial-time)\nis a long-standing open problem. In this paper, we consider the restriction of\nthe orbit problem when the permutation group is cyclic (i.e. generated by a\nsingle permutation), an important restriction of the problem. It is known that\nthis subproblem is solvable in polynomial-time. Our main result is a\nlinear-time algorithm for this subproblem. \n\n"}
{"id": "1411.4417", "contents": "Title: Barycenters of points that are constrained to a polytope skeleton Abstract: We give a short and simple proof of a recent result of Dobbins that any point\nin an $nd$-polytope is the barycenter of $n$ points in the $d$-skeleton. This\nnew proof builds on the constraint method that we recently introduced to prove\nTverberg-type results. \n\n"}
{"id": "1411.5729", "contents": "Title: Forrelation: A Problem that Optimally Separates Quantum from Classical\n  Computing Abstract: We achieve essentially the largest possible separation between quantum and\nclassical query complexities. We do so using a property-testing problem called\nForrelation, where one needs to decide whether one Boolean function is highly\ncorrelated with the Fourier transform of a second function. This problem can be\nsolved using 1 quantum query, yet we show that any randomized algorithm needs\n~sqrt(N)/log(N) queries (improving an ~N^{1/4} lower bound of Aaronson).\nConversely, we show that this 1 versus ~sqrt(N) separation is optimal: indeed,\nany t-query quantum algorithm whatsoever can be simulated by an\nO(N^{1-1/2t})-query randomized algorithm. Thus, resolving an open question of\nBuhrman et al. from 2002, there is no partial Boolean function whose quantum\nquery complexity is constant and whose randomized query complexity is linear.\nWe conjecture that a natural generalization of Forrelation achieves the optimal\nt versus ~N^{1-1/2t} separation for all t. As a bonus, we show that this\ngeneralization is BQP-complete. This yields what's arguably the simplest\nBQP-complete problem yet known, and gives a second sense in which Forrelation\n\"captures the maximum power of quantum computation.\" \n\n"}
{"id": "1411.6317", "contents": "Title: Lower bounds on the size of semidefinite programming relaxations Abstract: We introduce a method for proving lower bounds on the efficacy of\nsemidefinite programming (SDP) relaxations for combinatorial problems. In\nparticular, we show that the cut, TSP, and stable set polytopes on $n$-vertex\ngraphs are not the linear image of the feasible region of any SDP (i.e., any\nspectrahedron) of dimension less than $2^{n^c}$, for some constant $c > 0$.\nThis result yields the first super-polynomial lower bounds on the semidefinite\nextension complexity of any explicit family of polytopes.\n  Our results follow from a general technique for proving lower bounds on the\npositive semidefinite rank of a matrix. To this end, we establish a close\nconnection between arbitrary SDPs and those arising from the sum-of-squares SDP\nhierarchy. For approximating maximum constraint satisfaction problems, we prove\nthat SDPs of polynomial-size are equivalent in power to those arising from\ndegree-$O(1)$ sum-of-squares relaxations. This result implies, for instance,\nthat no family of polynomial-size SDP relaxations can achieve better than a\n7/8-approximation for MAX-3-SAT. \n\n"}
{"id": "1411.6317", "contents": "Title: Lower bounds on the size of semidefinite programming relaxations Abstract: We introduce a method for proving lower bounds on the efficacy of\nsemidefinite programming (SDP) relaxations for combinatorial problems. In\nparticular, we show that the cut, TSP, and stable set polytopes on $n$-vertex\ngraphs are not the linear image of the feasible region of any SDP (i.e., any\nspectrahedron) of dimension less than $2^{n^c}$, for some constant $c > 0$.\nThis result yields the first super-polynomial lower bounds on the semidefinite\nextension complexity of any explicit family of polytopes.\n  Our results follow from a general technique for proving lower bounds on the\npositive semidefinite rank of a matrix. To this end, we establish a close\nconnection between arbitrary SDPs and those arising from the sum-of-squares SDP\nhierarchy. For approximating maximum constraint satisfaction problems, we prove\nthat SDPs of polynomial-size are equivalent in power to those arising from\ndegree-$O(1)$ sum-of-squares relaxations. This result implies, for instance,\nthat no family of polynomial-size SDP relaxations can achieve better than a\n7/8-approximation for MAX-3-SAT. \n\n"}
{"id": "1411.6614", "contents": "Title: Two-dimensional local Hamiltonian problem with area laws is QMA-complete Abstract: We show that the two-dimensional (2D) local Hamiltonian problem with the\nconstraint that the ground state obeys area laws is QMA-complete. We also prove\nsimilar results in 2D translation-invariant systems and for the 3D Heisenberg\nand Hubbard models with local magnetic fields. Consequently, unless MA = QMA,\nnot all ground states of 2D local Hamiltonians with area laws have efficient\nclassical representations that support efficient computation of local\nexpectation values. In the future, even if area laws are proved for ground\nstates of 2D gapped systems, the computational complexity of these systems\nremains unclear. \n\n"}
{"id": "1411.6712", "contents": "Title: The square root rank of the correlation polytope is exponential Abstract: The square root rank of a nonnegative matrix $A$ is the minimum rank of a\nmatrix $B$ such that $A=B \\circ B$, where $\\circ$ denotes entrywise product. We\nshow that the square root rank of the slack matrix of the correlation polytope\nis exponential. Our main technique is a way to lower bound the rank of certain\nmatrices under arbitrary sign changes of the entries using properties of the\nroots of polynomials in number fields. The square root rank is an upper bound\non the positive semidefinite rank of a matrix, and corresponds the special case\nwhere all matrices in the factorization are rank-one. \n\n"}
{"id": "1411.7492", "contents": "Title: Subexponential Size Hitting Sets for Bounded Depth Multilinear Formulas Abstract: In this paper we give subexponential size hitting sets for bounded depth\nmultilinear arithmetic formulas. Using the known relation between black-box PIT\nand lower bounds we obtain lower bounds for these models.\n  For depth-3 multilinear formulas, of size $\\exp(n^\\delta)$, we give a hitting\nset of size $\\exp(\\tilde{O}(n^{2/3 + 2\\delta/3}))$. This implies a lower bound\nof $\\exp(\\tilde{\\Omega}(n^{1/2}))$ for depth-3 multilinear formulas, for some\nexplicit polynomial.\n  For depth-4 multilinear formulas, of size $\\exp(n^\\delta)$, we give a hitting\nset of size $\\exp(\\tilde{O}(n^{2/3 + 4\\delta/3}))$. This implies a lower bound\nof $\\exp(\\tilde{\\Omega}(n^{1/4}))$ for depth-4 multilinear formulas, for some\nexplicit polynomial.\n  A regular formula consists of alternating layers of $+,\\times$ gates, where\nall gates at layer $i$ have the same fan-in. We give a hitting set of size\n(roughly) $\\exp\\left(n^{1- \\delta} \\right)$, for regular depth-$d$ multilinear\nformulas of size $\\exp(n^\\delta)$, where $\\delta = O(\\frac{1}{\\sqrt{5}^d})$.\nThis result implies a lower bound of roughly\n$\\exp(\\tilde{\\Omega}(n^{\\frac{1}{\\sqrt{5}^d}}))$ for such formulas.\n  We note that better lower bounds are known for these models, but also that\nnone of these bounds was achieved via construction of a hitting set. Moreover,\nno lower bound that implies such PIT results, even in the white-box model, is\ncurrently known.\n  Our results are combinatorial in nature and rely on reducing the underlying\nformula, first to a depth-4 formula, and then to a read-once algebraic\nbranching program (from depth-3 formulas we go straight to read-once algebraic\nbranching programs). \n\n"}
{"id": "1412.0348", "contents": "Title: Edit Distance Cannot Be Computed in Strongly Subquadratic Time (unless\n  SETH is false) Abstract: The edit distance (a.k.a. the Levenshtein distance) between two strings is\ndefined as the minimum number of insertions, deletions or substitutions of\nsymbols needed to transform one string into another. The problem of computing\nthe edit distance between two strings is a classical computational task, with a\nwell-known algorithm based on dynamic programming. Unfortunately, all known\nalgorithms for this problem run in nearly quadratic time.\n  In this paper we provide evidence that the near-quadratic running time bounds\nknown for the problem of computing edit distance might be tight. Specifically,\nwe show that, if the edit distance can be computed in time $O(n^{2-\\delta})$\nfor some constant $\\delta>0$, then the satisfiability of conjunctive normal\nform formulas with $N$ variables and $M$ clauses can be solved in time\n$M^{O(1)} 2^{(1-\\epsilon)N}$ for a constant $\\epsilon>0$. The latter result\nwould violate the Strong Exponential Time Hypothesis, which postulates that\nsuch algorithms do not exist. \n\n"}
{"id": "1412.0784", "contents": "Title: Braid is undecidable Abstract: Braid is a 2008 puzzle game centered around the ability to reverse time. We\nshow that Braid can simulate an arbitrary computation. Our construction makes\nno use of Braid's unique time mechanics, and therefore may apply to many other\nvideo games.\n  We also show that a plausible \"bounded\" variant of Braid lies within\n2-EXPSPACE. Our proof relies on a technical lemma about Turing machines which\nmay be of independent interest. Namely, define a braidlike Turing machine to be\na Turing machine that, when it writes to the tape, deletes all data on the tape\nto the right of the head. We prove that deciding the behavior of such a machine\nlies in EXPSPACE. \n\n"}
{"id": "1412.1505", "contents": "Title: Symmetric Weighted First-Order Model Counting Abstract: The FO Model Counting problem (FOMC) is the following: given a sentence\n$\\Phi$ in FO and a number $n$, compute the number of models of $\\Phi$ over a\ndomain of size $n$; the Weighted variant (WFOMC) generalizes the problem by\nassociating a weight to each tuple and defining the weight of a model to be the\nproduct of weights of its tuples. In this paper we study the complexity of the\nsymmetric WFOMC, where all tuples of a given relation have the same weight. Our\nmotivation comes from an important application, inference in Knowledge Bases\nwith soft constraints, like Markov Logic Networks, but the problem is also of\nindependent theoretical interest. We study both the data complexity, and the\ncombined complexity of FOMC and WFOMC. For the data complexity we prove the\nexistence of an FO$^{3}$ formula for which FOMC is #P$_1$-complete, and the\nexistence of a Conjunctive Query for which WFOMC is #P$_1$-complete. We also\nprove that all $\\gamma$-acyclic queries have polynomial time data complexity.\nFor the combined complexity, we prove that, for every fragment FO$^{k}$, $k\\geq\n2$, the combined complexity of FOMC (or WFOMC) is #P-complete. \n\n"}
{"id": "1412.2954", "contents": "Title: Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample\n  Complexity Abstract: We present a simple, general technique for reducing the sample complexity of\nmatrix and tensor decomposition algorithms applied to distributions. We use the\ntechnique to give a polynomial-time algorithm for standard ICA with sample\ncomplexity nearly linear in the dimension, thereby improving substantially on\nprevious bounds. The analysis is based on properties of random polynomials,\nnamely the spacings of an ensemble of polynomials. Our technique also applies\nto other applications of tensor decompositions, including spherical Gaussian\nmixture models. \n\n"}
{"id": "1412.3334", "contents": "Title: Computational Complexity of Competitive Diffusion on (Un)weighted Graphs Abstract: Consider an undirected graph modeling a social network, where the vertices\nrepresent users, and the edges do connections among them. In the competitive\ndiffusion game, each of a number of players chooses a vertex as a seed to\npropagate his/her opinion, and then it spreads along the edges in the graphs.\nThe objective of every player is to maximize the number of vertices the opinion\ninfects. In this paper, we investigate a computational problem of asking\nwhether a pure Nash equilibrium exists in the competitive diffusion game on\nunweighed and weighted graphs, and present several negative and positive\nresults. We first prove that the problem is W[1]-hard when parameterized by the\nnumber of players even for unweighted graphs. We also show that the problem is\nNP-hard even for series-parallel graphs with positive integer weights, and is\nNP-hard even for forests with arbitrary integer weights. Furthermore, we show\nthat the problem for forest of paths with arbitrary weights is solvable in\npseudo-polynomial time; and it is solvable in quadratic time if a given graph\nis unweighted. We also prove that the problem for chain, cochain, and threshold\ngraphs with arbitrary integer weights is solvable in polynomial time. \n\n"}
{"id": "1412.3570", "contents": "Title: Bounded-degree factors of lacunary multivariate polynomials Abstract: In this paper, we present a new method for computing bounded-degree factors\nof lacunary multivariate polynomials. In particular for polynomials over number\nfields, we give a new algorithm that takes as input a multivariate polynomial f\nin lacunary representation and a degree bound d and computes the irreducible\nfactors of degree at most d of f in time polynomial in the lacunary size of f\nand in d. Our algorithm, which is valid for any field of zero characteristic,\nis based on a new gap theorem that enables reducing the problem to several\ninstances of (a) the univariate case and (b) low-degree multivariate\nfactorization.\n  The reduction algorithms we propose are elementary in that they only\nmanipulate the exponent vectors of the input polynomial. The proof of\ncorrectness and the complexity bounds rely on the Newton polytope of the\npolynomial, where the underlying valued field consists of Puiseux series in a\nsingle variable. \n\n"}
{"id": "1412.4203", "contents": "Title: On the computational complexity and generalization properties of\n  multi-stage and recursive scenario programs Abstract: We discuss the computational complexity and feasibility properties of\nscenario based techniques for uncertain optimization programs. We consider\ndifferent solution alternatives ranging from the standard scenario approach to\nrecursive variants, and compare feasibility as a function of the total\ncomputation burden. We identify trade-offs between the different methods\ndepending on the problem structure and the desired probability of constraint\nsatisfaction. Our motivation for this work stems from the applicability and\ncomplexity reduction when making decisions by means of recursive algorithms. We\nillustrate our results on an example from the area of approximate dynamic\nprogramming \n\n"}
{"id": "1412.4646", "contents": "Title: Fewer runs than word length Abstract: The work takes another look at the number of runs that a string might contain\nand provides an alternative proof for the bound. We also propose another\nstronger conjecture that states that, for a fixed order on the alphabet, within\nevery factor of a word there are at most as many occurrences of Lyndon roots\ncorresponding to runs in a word as the length of the factor (only first such\noccurrences for each run are considered). \n\n"}
{"id": "1412.5249", "contents": "Title: The switch Markov chain for sampling irregular graphs Abstract: The problem of efficiently sampling from a set of(undirected) graphs with a\ngiven degree sequence has many applications. One approach to this problem uses\na simple Markov chain, which we call the switch chain, to perform the sampling.\nThe switch chain is known to be rapidly mixing for regular degree sequences. We\nprove that the switch chain is rapidly mixing for any degree sequence with\nminimum degree at least 1 and with maximum degree $d_{\\max}$ which satisfies\n$3\\leq d_{\\max}\\leq \\frac{1}{4}\\, \\sqrt{M}$, where $M$ is the sum of the\ndegrees. The mixing time bound obtained is only an order of $n$ larger than\nthat established in the regular case, where $n$ is the number of vertices. \n\n"}
{"id": "1412.6705", "contents": "Title: On the Shadow Simplex Method for Curved Polyhedra Abstract: We study the simplex method over polyhedra satisfying certain \"discrete\ncurvature\" lower bounds, which enforce that the boundary always meets vertices\nat sharp angles. Motivated by linear programs with totally unimodular\nconstraint matrices, recent results of Bonifas et al (SOCG 2012), Brunsch and\nR\\\"oglin (ICALP 2013), and Eisenbrand and Vempala (2014) have improved our\nunderstanding of such polyhedra.\n  We develop a new type of dual analysis of the shadow simplex method which\nprovides a clean and powerful tool for improving all previously mentioned\nresults. Our methods are inspired by the recent work of Bonifas and the first\nnamed author (SODA 2015), who analyzed a remarkably similar process as part of\nan algorithm for the Closest Vector Problem with Preprocessing.\n  For our first result, we obtain a constructive diameter bound of\n$O(\\frac{n^2}{\\delta} \\ln \\frac{n}{\\delta})$ for $n$-dimensional polyhedra with\ncurvature parameter $\\delta \\in [0,1]$. For the class of polyhedra arising from\ntotally unimodular constraint matrices, this implies a bound of $O(n^3 \\ln n)$.\nFor linear optimization, given an initial feasible vertex, we show that an\noptimal vertex can be found using an expected $O(\\frac{n^3}{\\delta} \\ln\n\\frac{n}{\\delta})$ simplex pivots, each requiring $O(m n)$ time to compute. An\ninitial feasible solution can be found using $O(\\frac{m n^3}{\\delta} \\ln\n\\frac{n}{\\delta})$ pivot steps. \n\n"}
{"id": "1412.7558", "contents": "Title: A Polynomial Kernel for Trivially Perfect Editing Abstract: We give a kernel with $O(k^7)$ vertices for Trivially Perfect Editing, the\nproblem of adding or removing at most $k$ edges in order to make a given graph\ntrivially perfect. This answers in affirmative an open question posed by Nastos\nand Gao, and by Liu et al. Our general technique implies also the existence of\nkernels of the same size for the related problems Trivially Perfect Completion\nand Trivially Perfect Deletion. Whereas for the former an $O(k^3)$ kernel was\ngiven by Guo, for the latter no polynomial kernel was known.\n  We complement our study of Trivially Perfect Editing by proving that,\ncontrary to Trivially Perfect Completion, it cannot be solved in time $2^{o(k)}\n\\cdot n^{O(1)}$ unless the Exponential Time Hypothesis fails. In this manner we\ncomplete the picture of the parameterized and kernelization complexity of the\nclassic edge modification problems for the class of trivially perfect graphs. \n\n"}
{"id": "1412.8347", "contents": "Title: Online Packing and Covering Framework with Convex Objectives Abstract: We consider online fractional covering problems with a convex objective,\nwhere the covering constraints arrive over time. Formally, we want to solve\n$\\min\\,\\{f(x) \\mid Ax\\ge \\mathbf{1},\\, x\\ge 0\\},$ where the objective function\n$f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$ is convex, and the constraint matrix\n$A_{m\\times n}$ is non-negative. The rows of $A$ arrive online over time, and\nwe wish to maintain a feasible solution $x$ at all times while only increasing\ncoordinates of $x$. We also consider \"dual\" packing problems of the form\n$\\max\\,\\{c^\\intercal y - g(\\mu) \\mid A^\\intercal y \\le \\mu,\\, y\\ge 0\\}$, where\n$g$ is a convex function. In the online setting, variables $y$ and columns of\n$A^\\intercal$ arrive over time, and we wish to maintain a non-decreasing\nsolution $(y,\\mu)$.\n  We provide an online primal-dual framework for both classes of problems with\ncompetitive ratio depending on certain \"monotonicity\" and \"smoothness\"\nparameters of $f$; our results match or improve on guarantees for some special\nclasses of functions $f$ considered previously.\n  Using this fractional solver with problem-dependent randomized rounding\nprocedures, we obtain competitive algorithms for the following problems: online\ncovering LPs minimizing $\\ell_p$-norms of arbitrary packing constraints, set\ncover with multiple cost functions, capacity constrained facility location,\ncapacitated multicast problem, set cover with set requests, and profit\nmaximization with non-separable production costs. Some of these results are new\nand others provide a unified view of previous results, with matching or\nslightly worse competitive ratios. \n\n"}
{"id": "1501.00624", "contents": "Title: Maximal Noise in Interactive Communication over Erasure Channels and\n  Channels with Feedback Abstract: We provide tight upper and lower bounds on the noise resilience of\ninteractive communication over noisy channels with feedback. In this setting,\nwe show that the maximal fraction of noise that any robust protocol can resist\nis 1/3. Additionally, we provide a simple and efficient robust protocol that\nsucceeds as long as the fraction of noise is at most 1/3 - \\epsilon.\nSurprisingly, both bounds hold regardless of whether the parties send bits or\nsymbols from an arbitrarily large alphabet. We also consider interactive\ncommunication over erasure channels. We provide a protocol that matches the\noptimal tolerable erasure rate of 1/2 - \\epsilon of previous protocols\n(Franklin et al., CRYPTO '13) but operates in a much simpler and more efficient\nway. Our protocol works with an alphabet of size 4, in contrast to prior\nprotocols in which the alphabet size grows as epsilon goes to zero. Building on\nthe above algorithm with a fixed alphabet size, we are able to devise a\nprotocol for binary erasure channels that tolerates erasure rates of up to 1/3\n- \\epsilon. \n\n"}
{"id": "1501.01391", "contents": "Title: Rigidity of frameworks on expanding spheres Abstract: A rigidity theory is developed for bar-joint frameworks in $\\mathbb{R}^{d+1}$\nwhose vertices are constrained to lie on concentric $d$-spheres with\nindependently variable radii. In particular, combinatorial characterisations\nare established for the rigidity of generic frameworks for $d=1$ with an\narbitrary number of independently variable radii, and for $d=2$ with at most\ntwo variable radii. This includes a characterisation of the rigidity or\nflexibility of uniformly expanding spherical frameworks in $\\mathbb{R}^{3}$.\nDue to the equivalence of the generic rigidity between Euclidean space and\nspherical space, these results interpolate between rigidity in 1D and 2D and to\nsome extent between rigidity in 2D and 3D. Symmetry-adapted counts for the\ndetection of symmetry-induced continuous flexibility in frameworks on spheres\nwith variable radii are also provided. \n\n"}
{"id": "1501.05296", "contents": "Title: Output-sensitive algorithms for sumset and sparse polynomial\n  multiplication Abstract: We present randomized algorithms to compute the sumset (Minkowski sum) of two\ninteger sets, and to multiply two univariate integer polynomials given by\nsparse representations. Our algorithm for sumset has cost softly linear in the\ncombined size of the inputs and output. This is used as part of our sparse\nmultiplication algorithm, whose cost is softly linear in the combined size of\nthe inputs, output, and the sumset of the supports of the inputs. As a\nsubroutine, we present a new method for computing the coefficients of a sparse\npolynomial, given a set containing its support. Our multiplication algorithm\nextends to multivariate Laurent polynomials over finite fields and rational\nnumbers. Our techniques are based on sparse interpolation algorithms and\nresults from analytic number theory. \n\n"}
{"id": "1501.06521", "contents": "Title: Noisy Tensor Completion via the Sum-of-Squares Hierarchy Abstract: In the noisy tensor completion problem we observe $m$ entries (whose location\nis chosen uniformly at random) from an unknown $n_1 \\times n_2 \\times n_3$\ntensor $T$. We assume that $T$ is entry-wise close to being rank $r$. Our goal\nis to fill in its missing entries using as few observations as possible. Let $n\n= \\max(n_1, n_2, n_3)$. We show that if $m = n^{3/2} r$ then there is a\npolynomial time algorithm based on the sixth level of the sum-of-squares\nhierarchy for completing it. Our estimate agrees with almost all of $T$'s\nentries almost exactly and works even when our observations are corrupted by\nnoise. This is also the first algorithm for tensor completion that works in the\novercomplete case when $r > n$, and in fact it works all the way up to $r =\nn^{3/2-\\epsilon}$.\n  Our proofs are short and simple and are based on establishing a new\nconnection between noisy tensor completion (through the language of Rademacher\ncomplexity) and the task of refuting random constant satisfaction problems.\nThis connection seems to have gone unnoticed even in the context of matrix\ncompletion. Furthermore, we use this connection to show matching lower bounds.\nOur main technical result is in characterizing the Rademacher complexity of the\nsequence of norms that arise in the sum-of-squares relaxations to the tensor\nnuclear norm. These results point to an interesting new direction: Can we\nexplore computational vs. sample complexity tradeoffs through the\nsum-of-squares hierarchy? \n\n"}
{"id": "1501.06561", "contents": "Title: Improved Practical Matrix Sketching with Guarantees Abstract: Matrices have become essential data representations for many large-scale\nproblems in data analytics, and hence matrix sketching is a critical task.\nAlthough much research has focused on improving the error/size tradeoff under\nvarious sketching paradigms, the many forms of error bounds make these\napproaches hard to compare in theory and in practice. This paper attempts to\ncategorize and compare most known methods under row-wise streaming updates with\nprovable guarantees, and then to tweak some of these methods to gain practical\nimprovements while retaining guarantees.\n  For instance, we observe that a simple heuristic iSVD, with no guarantees,\ntends to outperform all known approaches in terms of size/error trade-off. We\nmodify the best performing method with guarantees FrequentDirections under the\nsize/error trade-off to match the performance of iSVD and retain its\nguarantees. We also demonstrate some adversarial datasets where iSVD performs\nquite poorly. In comparing techniques in the time/error trade-off, techniques\nbased on hashing or sampling tend to perform better. In this setting we modify\nthe most studied sampling regime to retain error guarantee but obtain dramatic\nimprovements in the time/error trade-off.\n  Finally, we provide easy replication of our studies on APT, a new testbed\nwhich makes available not only code and datasets, but also a computing platform\nwith fixed environmental settings. \n\n"}
{"id": "1501.06794", "contents": "Title: Computing Functions of Random Variables via Reproducing Kernel Hilbert\n  Space Representations Abstract: We describe a method to perform functional operations on probability\ndistributions of random variables. The method uses reproducing kernel Hilbert\nspace representations of probability distributions, and it is applicable to all\noperations which can be applied to points drawn from the respective\ndistributions. We refer to our approach as {\\em kernel probabilistic\nprogramming}. We illustrate it on synthetic data, and show how it can be used\nfor nonparametric structural equation models, with an application to causal\ninference. \n\n"}
{"id": "1501.07053", "contents": "Title: Quadratic-Time Hardness of LCS and other Sequence Similarity Measures Abstract: Two important similarity measures between sequences are the longest common\nsubsequence (LCS) and the dynamic time warping distance (DTWD). The\ncomputations of these measures for two given sequences are central tasks in a\nvariety of applications. Simple dynamic programming algorithms solve these\ntasks in $O(n^2)$ time, and despite an extensive amount of research, no\nalgorithms with significantly better worst case upper bounds are known.\n  In this paper, we show that an $O(n^{2-\\epsilon})$ time algorithm, for some\n$\\epsilon>0$, for computing the LCS or the DTWD of two sequences of length $n$\nover a constant size alphabet, refutes the popular Strong Exponential Time\nHypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings over\nan alphabet of size $O(k)$ cannot be done in $O(n^{k-\\epsilon})$ time, for any\n$\\epsilon>0$, under SETH. Finally, we also address the time complexity of\napproximating the DTWD of two strings in truly subquadratic time. \n\n"}
{"id": "1502.00067", "contents": "Title: Quantum interpretations of AWPP and APP Abstract: AWPP is a complexity class introduced by Fenner, Fortnow, Kurtz, and Li,\nwhich is defined using GapP functions. Although it is an important class as the\nbest upperbound of BQP, its definition seems to be somehow artificial, and\ntherefore it would be better if we have some \"physical interpretation\" of AWPP.\nHere we provide a quantum physical interpretation of AWPP: we show that AWPP is\nequal to the class of problems efficiently solved by a quantum computer with\nthe ability of postselecting an event whose probability is close to an FP\nfunction. This result is applied to also obtain a quantum physical\ninterpretation of APP. In addition, we consider \"classical physical analogue\"\nof these results, and show that a restricted version of ${\\rm BPP}_{\\rm path}$\ncontains ${\\rm UP}\\cap{\\rm coUP}$ and is contained in WAPP. \n\n"}
{"id": "1502.00827", "contents": "Title: On the Duality of Additivity and Tensorization Abstract: A function is said to be additive if, similar to mutual information, expands\nby a factor of $n$, when evaluated on $n$ i.i.d. repetitions of a source or\nchannel. On the other hand, a function is said to satisfy the tensorization\nproperty if it remains unchanged when evaluated on i.i.d. repetitions. Additive\nrate regions are of fundamental importance in network information theory,\nserving as capacity regions or upper bounds thereof. Tensorizing measures of\ncorrelation have also found applications in distributed source and channel\ncoding problems as well as the distribution simulation problem. Prior to our\nwork only two measures of correlation, namely the hypercontractivity ribbon and\nmaximal correlation (and their derivatives), were known to have the\ntensorization property. In this paper, we provide a general framework to obtain\na region with the tensorization property from any additive rate region. We\nobserve that hypercontractivity ribbon indeed comes from the dual of the rate\nregion of the Gray-Wyner source coding problem, and generalize it to the\nmultipartite case. Then we define other measures of correlation with similar\nproperties from other source coding problems. We also present some applications\nof our results. \n\n"}
{"id": "1502.01063", "contents": "Title: Quadratic Conditional Lower Bounds for String Problems and Dynamic Time\n  Warping Abstract: Classic similarity measures of strings are longest common subsequence and\nLevenshtein distance (i.e., the classic edit distance). A classic similarity\nmeasure of curves is dynamic time warping. These measures can be computed by\nsimple $O(n^2)$ dynamic programming algorithms, and despite much effort no\nalgorithms with significantly better running time are known.\n  We prove that, even restricted to binary strings or one-dimensional curves,\nrespectively, these measures do not have strongly subquadratic time algorithms,\ni.e., no algorithms with running time $O(n^{2-\\varepsilon})$ for any\n$\\varepsilon > 0$, unless the Strong Exponential Time Hypothesis fails. We\ngeneralize the result to edit distance for arbitrary fixed costs of the four\noperations (deletion in one of the two strings, matching, substitution), by\nidentifying trivial cases that can be solved in constant time, and proving\nquadratic-time hardness on binary strings for all other cost choices. This\nimproves and generalizes the known hardness result for Levenshtein distance\n[Backurs, Indyk STOC'15] by the restriction to binary strings and the\ngeneralization to arbitrary costs, and adds important problems to a recent line\nof research showing conditional lower bounds for a growing number of quadratic\ntime problems.\n  As our main technical contribution, we introduce a framework for proving\nquadratic-time hardness of similarity measures. To apply the framework it\nsuffices to construct a single gadget, which encapsulates all the expressive\npower necessary to emulate a reduction from satisfiability.\n  Finally, we prove quadratic-time hardness for longest palindromic subsequence\nand longest tandem subsequence via reductions from longest common subsequence,\nshowing that conditional lower bounds based on the Strong Exponential Time\nHypothesis also apply to string problems that are not necessarily similarity\nmeasures. \n\n"}
{"id": "1502.03032", "contents": "Title: Implementing Randomized Matrix Algorithms in Parallel and Distributed\n  Environments Abstract: In this era of large-scale data, distributed systems built on top of clusters\nof commodity hardware provide cheap and reliable storage and scalable\nprocessing of massive data. Here, we review recent work on developing and\nimplementing randomized matrix algorithms in large-scale parallel and\ndistributed environments. Randomized algorithms for matrix problems have\nreceived a great deal of attention in recent years, thus far typically either\nin theory or in machine learning applications or with implementations on a\nsingle machine. Our main focus is on the underlying theory and practical\nimplementation of random projection and random sampling algorithms for very\nlarge very overdetermined (i.e., overconstrained) $\\ell_1$ and $\\ell_2$\nregression problems. Randomization can be used in one of two related ways:\neither to construct sub-sampled problems that can be solved, exactly or\napproximately, with traditional numerical methods; or to construct\npreconditioned versions of the original full problem that are easier to solve\nwith traditional iterative algorithms. Theoretical results demonstrate that in\nnear input-sparsity time and with only a few passes through the data one can\nobtain very strong relative-error approximate solutions, with high probability.\nEmpirical results highlight the importance of various trade-offs (e.g., between\nthe time to construct an embedding and the conditioning quality of the\nembedding, between the relative importance of computation versus communication,\netc.) and demonstrate that $\\ell_1$ and $\\ell_2$ regression problems can be\nsolved to low, medium, or high precision in existing distributed systems on up\nto terabyte-sized data. \n\n"}
{"id": "1502.03316", "contents": "Title: The Hardness of Approximation of Euclidean k-means Abstract: The Euclidean $k$-means problem is a classical problem that has been\nextensively studied in the theoretical computer science, machine learning and\nthe computational geometry communities. In this problem, we are given a set of\n$n$ points in Euclidean space $R^d$, and the goal is to choose $k$ centers in\n$R^d$ so that the sum of squared distances of each point to its nearest center\nis minimized. The best approximation algorithms for this problem include a\npolynomial time constant factor approximation for general $k$ and a\n$(1+\\epsilon)$-approximation which runs in time $poly(n) 2^{O(k/\\epsilon)}$. At\nthe other extreme, the only known computational complexity result for this\nproblem is NP-hardness [ADHP'09]. The main difficulty in obtaining hardness\nresults stems from the Euclidean nature of the problem, and the fact that any\npoint in $R^d$ can be a potential center. This gap in understanding left open\nthe intriguing possibility that the problem might admit a PTAS for all $k,d$.\n  In this paper we provide the first hardness of approximation for the\nEuclidean $k$-means problem. Concretely, we show that there exists a constant\n$\\epsilon > 0$ such that it is NP-hard to approximate the $k$-means objective\nto within a factor of $(1+\\epsilon)$. We show this via an efficient reduction\nfrom the vertex cover problem on triangle-free graphs: given a triangle-free\ngraph, the goal is to choose the fewest number of vertices which are incident\non all the edges. Additionally, we give a proof that the current best hardness\nresults for vertex cover can be carried over to triangle-free graphs. To show\nthis we transform $G$, a known hard vertex cover instance, by taking a graph\nproduct with a suitably chosen graph $H$, and showing that the size of the\n(normalized) maximum independent set is almost exactly preserved in the product\ngraph using a spectral analysis, which might be of independent interest. \n\n"}
{"id": "1502.04022", "contents": "Title: Local Computation Algorithms for Graphs of Non-Constant Degrees Abstract: In the model of \\emph{local computation algorithms} (LCAs), we aim to compute\nthe queried part of the output by examining only a small (sublinear) portion of\nthe input. Many recently developed LCAs on graph problems achieve time and\nspace complexities with very low dependence on $n$, the number of vertices.\nNonetheless, these complexities are generally at least exponential in $d$, the\nupper bound on the degree of the input graph. Instead, we consider the case\nwhere parameter $d$ can be moderately dependent on $n$, and aim for\ncomplexities with subexponential dependence on $d$, while maintaining\npolylogarithmic dependence on $n$. We present: a randomized LCA for computing\nmaximal independent sets whose time and space complexities are quasi-polynomial\nin $d$ and polylogarithmic in $n$; for constant $\\epsilon > 0$, a randomized\nLCA that provides a $(1-\\epsilon)$-approximation to maximum matching whose time\nand space complexities are polynomial in $d$ and polylogarithmic in $n$. \n\n"}
{"id": "1502.04482", "contents": "Title: A new proof of Friedman's second eigenvalue Theorem and its extension to\n  random lifts Abstract: It was conjectured by Alon and proved by Friedman that a random $d$-regular\ngraph has nearly the largest possible spectral gap, more precisely, the largest\nabsolute value of the non-trivial eigenvalues of its adjacency matrix is at\nmost $2\\sqrt{d-1} +o(1)$ with probability tending to one as the size of the\ngraph tends to infinity. We give a new proof of this statement. We also study\nrelated questions on random $n$-lifts of graphs and improve a recent result by\nFriedman and Kohler. \n\n"}
{"id": "1502.04803", "contents": "Title: Reconfiguration on sparse graphs Abstract: A vertex-subset graph problem Q defines which subsets of the vertices of an\ninput graph are feasible solutions. A reconfiguration variant of a\nvertex-subset problem asks, given two feasible solutions S and T of size k,\nwhether it is possible to transform S into T by a sequence of vertex additions\nand deletions such that each intermediate set is also a feasible solution of\nsize bounded by k. We study reconfiguration variants of two classical\nvertex-subset problems, namely Independent Set and Dominating Set. We denote\nthe former by ISR and the latter by DSR. Both ISR and DSR are PSPACE-complete\non graphs of bounded bandwidth and W[1]-hard parameterized by k on general\ngraphs. We show that ISR is fixed-parameter tractable parameterized by k when\nthe input graph is of bounded degeneracy or nowhere-dense. As a corollary, we\nanswer positively an open question concerning the parameterized complexity of\nthe problem on graphs of bounded treewidth. Moreover, our techniques generalize\nrecent results showing that ISR is fixed-parameter tractable on planar graphs\nand graphs of bounded degree. For DSR, we show the problem fixed-parameter\ntractable parameterized by k when the input graph does not contain large\nbicliques, a class of graphs which includes graphs of bounded degeneracy and\nnowhere-dense graphs. \n\n"}
{"id": "1502.05204", "contents": "Title: Clustered Integer 3SUM via Additive Combinatorics Abstract: We present a collection of new results on problems related to 3SUM,\nincluding:\n  1. The first truly subquadratic algorithm for\n  $\\ \\ \\ \\ \\ $ 1a. computing the (min,+) convolution for monotone increasing\nsequences with integer values bounded by $O(n)$,\n  $\\ \\ \\ \\ \\ $1b. solving 3SUM for monotone sets in 2D with integer coordinates\nbounded by $O(n)$, and\n  $\\ \\ \\ \\ \\ $1c. preprocessing a binary string for histogram indexing (also\ncalled jumbled indexing).\n  The running time is:\n$O(n^{(9+\\sqrt{177})/12}\\,\\textrm{polylog}\\,n)=O(n^{1.859})$ with\nrandomization, or $O(n^{1.864})$ deterministically. This greatly improves the\nprevious $n^2/2^{\\Omega(\\sqrt{\\log n})}$ time bound obtained from Williams'\nrecent result on all-pairs shortest paths [STOC'14], and answers an open\nquestion raised by several researchers studying the histogram indexing problem.\n  2. The first algorithm for histogram indexing for any constant alphabet size\nthat achieves truly subquadratic preprocessing time and truly sublinear query\ntime.\n  3. A truly subquadratic algorithm for integer 3SUM in the case when the given\nset can be partitioned into $n^{1-\\delta}$ clusters each covered by an interval\nof length $n$, for any constant $\\delta>0$.\n  4. An algorithm to preprocess any set of $n$ integers so that subsequently\n3SUM on any given subset can be solved in $O(n^{13/7}\\,\\textrm{polylog}\\,n)$\ntime.\n  All these results are obtained by a surprising new technique, based on the\nBalog--Szemer\\'edi--Gowers Theorem from additive combinatorics. \n\n"}
{"id": "1502.05983", "contents": "Title: Sorting Networks: The Final Countdown Abstract: In this paper we extend the knowledge on the problem of empirically searching\nfor sorting networks of minimal depth. We present new search space pruning\ntechniques for the last four levels of a candidate sorting network by\nconsidering only the output set representation of a network. We present an\nalgorithm for checking whether an $n$-input sorting network of depth $d$ exists\nby considering the minimal up to permutation and reflection itemsets at each\nlevel and using the pruning at the last four levels. We experimentally\nevaluated this algorithm to find the optimal depth sorting networks for all $n\n\\leq 12$. \n\n"}
{"id": "1502.06590", "contents": "Title: Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden\n  Submatrix Problems Abstract: Given a large data matrix $A\\in\\mathbb{R}^{n\\times n}$, we consider the\nproblem of determining whether its entries are i.i.d. with some known marginal\ndistribution $A_{ij}\\sim P_0$, or instead $A$ contains a principal submatrix\n$A_{{\\sf Q},{\\sf Q}}$ whose entries have marginal distribution $A_{ij}\\sim\nP_1\\neq P_0$. As a special case, the hidden (or planted) clique problem\nrequires to find a planted clique in an otherwise uniformly random graph.\n  Assuming unbounded computational resources, this hypothesis testing problem\nis statistically solvable provided $|{\\sf Q}|\\ge C \\log n$ for a suitable\nconstant $C$. However, despite substantial effort, no polynomial time algorithm\nis known that succeeds with high probability when $|{\\sf Q}| = o(\\sqrt{n})$.\nRecently Meka and Wigderson \\cite{meka2013association}, proposed a method to\nestablish lower bounds within the Sum of Squares (SOS) semidefinite hierarchy.\n  Here we consider the degree-$4$ SOS relaxation, and study the construction of\n\\cite{meka2013association} to prove that SOS fails unless $k\\ge C\\,\nn^{1/3}/\\log n$. An argument presented by Barak implies that this lower bound\ncannot be substantially improved unless the witness construction is changed in\nthe proof. Our proof uses the moments method to bound the spectrum of a certain\nrandom association scheme, i.e. a symmetric random matrix whose rows and\ncolumns are indexed by the edges of an Erd\\\"os-Renyi random graph. \n\n"}
{"id": "1502.07888", "contents": "Title: Is Nearly-linear the same in Theory and Practice? A Case Study with a\n  Combinatorial Laplacian Solver Abstract: Linear system solving is one of the main workhorses in applied mathematics.\nRecently, theoretical computer scientists have contributed sophisticated\nalgorithms for solving linear systems with symmetric diagonally dominant\nmatrices (a class to which Laplacian matrices belong) in provably nearly-linear\ntime. While these algorithms are highly interesting from a theoretical\nperspective, there are no published results how they perform in practice.\n  With this paper we address this gap. We provide the first implementation of\nthe combinatorial solver by [Kelner et al., STOC 2013], which is particularly\nappealing for implementation due to its conceptual simplicity. The algorithm\nexploits that a Laplacian matrix corresponds to a graph; solving Laplacian\nlinear systems amounts to finding an electrical flow in this graph with the\nhelp of cycles induced by a spanning tree with the low-stretch property.\n  The results of our comprehensive experimental study are ambivalent. They\nconfirm a nearly-linear running time, but for reasonable inputs the constant\nfactors make the solver much slower than methods with higher asymptotic\ncomplexity. One other aspect predicted by theory is confirmed by our findings,\nthough: Spanning trees with lower stretch indeed reduce the solver's running\ntime. Yet, simple spanning tree algorithms perform in practice better than\nthose with a guaranteed low stretch. \n\n"}
{"id": "1502.07930", "contents": "Title: Combinatorial approximation of maximum $k$-vertex cover in bipartite\n  graphs within ratio~0.7 Abstract: We propose a \\textit{purely combinatorial algorithm} for \\mkvc{} in bipartite\ngraphs, achieving approximation ratio~0.7. The only combinatorial algorithms\ncurrently known until now for this problem are the natural greedy algorithm,\nthat achieves ratio 0.632, and an easy~$2/3$-approximation algorithm presented\nin \\cite{DBLP:journals/corr/BonnetEPS14}. \n\n"}
{"id": "1503.00260", "contents": "Title: Parameter Compilation Abstract: In resolving instances of a computational problem, if multiple instances of\ninterest share a feature in common, it may be fruitful to compile this feature\ninto a format that allows for more efficient resolution, even if the\ncompilation is relatively expensive. In this article, we introduce a formal\nframework for classifying problems according to their compilability. The basic\nobject in our framework is that of a parameterized problem, which here is a\nlanguage along with a parameterization---a map which provides, for each\ninstance, a so-called parameter on which compilation may be performed. Our\nframework is positioned within the paradigm of parameterized complexity, and\nour notions are relatable to established concepts in the theory of\nparameterized complexity. Indeed, we view our framework as playing a unifying\nrole, integrating together parameterized complexity and compilability theory. \n\n"}
{"id": "1503.01663", "contents": "Title: Dimensionality Reduction of Massive Sparse Datasets Using Coresets Abstract: In this paper we present a practical solution with performance guarantees to\nthe problem of dimensionality reduction for very large scale sparse matrices.\nWe show applications of our approach to computing the low rank approximation\n(reduced SVD) of such matrices. Our solution uses coresets, which is a subset\nof $O(k/\\eps^2)$ scaled rows from the $n\\times d$ input matrix, that\napproximates the sub of squared distances from its rows to every\n$k$-dimensional subspace in $\\REAL^d$, up to a factor of $1\\pm\\eps$. An open\ntheoretical problem has been whether we can compute such a coreset that is\nindependent of the input matrix and also a weighted subset of its rows. %An\nopen practical problem has been whether we can compute a non-trivial\napproximation to the reduced SVD of very large databases such as the Wikipedia\ndocument-term matrix in a reasonable time. We answer this question\naffirmatively. % and demonstrate an algorithm that efficiently computes a low\nrank approximation of the entire English Wikipedia. Our main technical result\nis a novel technique for deterministic coreset construction that is based on a\nreduction to the problem of $\\ell_2$ approximation for item frequencies. \n\n"}
{"id": "1503.04099", "contents": "Title: Algorithms and complexity for Turaev-Viro invariants Abstract: The Turaev-Viro invariants are a powerful family of topological invariants\nfor distinguishing between different 3-manifolds. They are invaluable for\nmathematical software, but current algorithms to compute them require\nexponential time.\n  The invariants are parameterised by an integer $r \\geq 3$. We resolve the\nquestion of complexity for $r=3$ and $r=4$, giving simple proofs that computing\nTuraev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \\#P-hard.\nMoreover, we give an explicit fixed-parameter tractable algorithm for arbitrary\n$r$, and show through concrete implementation and experimentation that this\nalgorithm is practical---and indeed preferable---to the prior state of the art\nfor real computation. \n\n"}
{"id": "1503.05225", "contents": "Title: Sketching, Embedding, and Dimensionality Reduction for Information\n  Spaces Abstract: Information distances like the Hellinger distance and the Jensen-Shannon\ndivergence have deep roots in information theory and machine learning. They are\nused extensively in data analysis especially when the objects being compared\nare high dimensional empirical probability distributions built from data.\nHowever, we lack common tools needed to actually use information distances in\napplications efficiently and at scale with any kind of provable guarantees. We\ncan't sketch these distances easily, or embed them in better behaved spaces, or\neven reduce the dimensionality of the space while maintaining the probability\nstructure of the data.\n  In this paper, we build these tools for information distances---both for the\nHellinger distance and Jensen--Shannon divergence, as well as related measures,\nlike the $\\chi^2$ divergence. We first show that they can be sketched\nefficiently (i.e. up to multiplicative error in sublinear space) in the\naggregate streaming model. This result is exponentially stronger than known\nupper bounds for sketching these distances in the strict turnstile streaming\nmodel. Second, we show a finite dimensionality embedding result for the\nJensen-Shannon and $\\chi^2$ divergences that preserves pair wise distances.\nFinally we prove a dimensionality reduction result for the Hellinger,\nJensen--Shannon, and $\\chi^2$ divergences that preserves the information\ngeometry of the distributions (specifically, by retaining the simplex structure\nof the space). While our second result above already implies that these\ndivergences can be explicitly embedded in Euclidean space, retaining the\nsimplex structure is important because it allows us to continue doing inference\nin the reduced space. In essence, we preserve not just the distance structure\nbut the underlying geometry of the space. \n\n"}
{"id": "1503.06116", "contents": "Title: Quantitative Tverberg, Helly, & Carath\\'eodory theorems Abstract: This paper presents sixteen quantitative versions of the classic Tverberg,\nHelly, & Caratheodory theorems in combinatorial convexity. Our results include\nmeasurable or enumerable information in the hypothesis and the conclusion.\nTypical measurements include the volume, the diameter, or the number of points\nin a lattice. \n\n"}
{"id": "1503.06271", "contents": "Title: Binary Coding in Stream Abstract: Big data is becoming ever more ubiquitous, ranging over massive video\nrepositories, document corpuses, image sets and Internet routing history.\nProximity search and clustering are two algorithmic primitives fundamental to\ndata analysis, but suffer from the \"curse of dimensionality\" on these gigantic\ndatasets. A popular attack for this problem is to convert object\nrepresentations into short binary codewords, while approximately preserving\nnear neighbor structure. However, there has been limited research on\nconstructing codewords in the \"streaming\" or \"online\" settings often applicable\nto this scale of data, where one may only make a single pass over data too\nmassive to fit in local memory.\n  In this paper, we apply recent advances in matrix sketching techniques to\nconstruct binary codewords in both streaming and online setting. Our\nexperimental results compete outperform several of the most popularly used\nalgorithms, and we prove theoretical guarantees on performance in the streaming\nsetting under mild assumptions on the data and randomness of the training set. \n\n"}
{"id": "1503.06447", "contents": "Title: Sum-of-squares lower bounds for planted clique Abstract: Finding cliques in random graphs and the closely related \"planted\" clique\nvariant, where a clique of size k is planted in a random G(n, 1/2) graph, have\nbeen the focus of substantial study in algorithm design. Despite much effort,\nthe best known polynomial-time algorithms only solve the problem for k ~\nsqrt(n).\n  In this paper we study the complexity of the planted clique problem under\nalgorithms from the Sum-of-squares hierarchy. We prove the first average case\nlower bound for this model: for almost all graphs in G(n,1/2), r rounds of the\nSOS hierarchy cannot find a planted k-clique unless k > n^{1/2r} (up to\nlogarithmic factors). Thus, for any constant number of rounds planted cliques\nof size n^{o(1)} cannot be found by this powerful class of algorithms. This is\nshown via an integrability gap for the natural formulation of maximum clique\nproblem on random graphs for SOS and Lasserre hierarchies, which in turn follow\nfrom degree lower bounds for the Positivestellensatz proof system.\n  We follow the usual recipe for such proofs. First, we introduce a natural\n\"dual certificate\" (also known as a \"vector-solution\" or \"pseudo-expectation\")\nfor the given system of polynomial equations representing the problem for every\nfixed input graph. Then we show that the matrix associated with this dual\ncertificate is PSD (positive semi-definite) with high probability over the\nchoice of the input graph.This requires the use of certain tools. One is the\ntheory of association schemes, and in particular the eigenspaces and\neigenvalues of the Johnson scheme. Another is a combinatorial method we develop\nto compute (via traces) norm bounds for certain random matrices whose entries\nare highly dependent; we hope this method will be useful elsewhere. \n\n"}
{"id": "1503.07563", "contents": "Title: Mind the Gap Abstract: We examine the complexity of the online Dictionary Matching with One Gap\nProblem (DMOG) which is the following. Preprocess a dictionary $D$ of $d$\npatterns, where each pattern contains a special gap symbol that can match any\nstring, so that given a text that arrives online, a character at a time, we can\nreport all of the patterns from $D$ that are suffixes of the text that has\narrived so far, before the next character arrives. In more general versions the\ngap symbols are associated with bounds determining the possible lengths of\nmatching strings. Finding efficient algorithmic solutions for (online) DMOG has\nproven to be a difficult algorithmic challenge. We demonstrate that the\ndifficulty in obtaining efficient solutions for the DMOG problem even, in the\noffline setting, can be traced back to the infamous 3SUM conjecture.\nInterestingly, our reduction deviates from the known reduction paths that\nfollow from 3SUM. In particular, most reductions from 3SUM go through the\nset-disjointness problem, which corresponds to the problem of preprocessing a\ngraph to answer edge-triangles queries. We use a new path of reductions by\nconsidering the complementary, although structurally very different,\nvertex-triangles queries. Using this new path we show a conditional lower bound\nof $\\Omega(\\delta(G_D)+op)$ time per text character, where $G_D$ is a bipartite\ngraph that captures the structure of $D$, $\\delta(G_D)$ is the degeneracy of\nthis graph, and $op$ is the output size. We also provide matching upper-bounds\n(up to sub-polynomial factors) for the vertex-triangles problem, and then\nextend these techniques to the online DMOG problem. In particular, we introduce\nalgorithms whose time cost depends linearly on $\\delta(G_D)$. Our algorithms\nmake use of graph orientations, together with some additional techniques.\nFinally, when $\\delta(G_D)$ is large we are able to obtain even more efficient\nsolutions. \n\n"}
{"id": "1504.00681", "contents": "Title: Approximation of non-boolean 2CSP Abstract: We develop a polynomial time $\\Omega\\left ( \\frac 1R \\log R \\right)$\napproximate algorithm for Max 2CSP-$R$, the problem where we are given a\ncollection of constraints, each involving two variables, where each variable\nranges over a set of size $R$, and we want to find an assignment to the\nvariables that maximizes the number of satisfied constraints. Assuming the\nUnique Games Conjecture, this is the best possible approximation up to constant\nfactors.\n  Previously, a $1/R$-approximate algorithm was known, based on linear\nprogramming. Our algorithm is based on semidefinite programming (SDP) and on a\nnovel rounding technique. The SDP that we use has an almost-matching\nintegrality gap. \n\n"}
{"id": "1504.01033", "contents": "Title: Watch and Learn: Optimizing from Revealed Preferences Feedback Abstract: A Stackelberg game is played between a leader and a follower. The leader\nfirst chooses an action, then the follower plays his best response. The goal of\nthe leader is to pick the action that will maximize his payoff given the\nfollower's best response. In this paper we present an approach to solving for\nthe leader's optimal strategy in certain Stackelberg games where the follower's\nutility function (and thus the subsequent best response of the follower) is\nunknown.\n  Stackelberg games capture, for example, the following interaction between a\nproducer and a consumer. The producer chooses the prices of the goods he\nproduces, and then a consumer chooses to buy a utility maximizing bundle of\ngoods. The goal of the seller here is to set prices to maximize his\nprofit---his revenue, minus the production cost of the purchased bundle. It is\nquite natural that the seller in this example should not know the buyer's\nutility function. However, he does have access to revealed preference\nfeedback---he can set prices, and then observe the purchased bundle and his own\nprofit. We give algorithms for efficiently solving, in terms of both\ncomputational and query complexity, a broad class of Stackelberg games in which\nthe follower's utility function is unknown, using only \"revealed preference\"\naccess to it. This class includes in particular the profit maximization\nproblem, as well as the optimal tolling problem in nonatomic congestion games,\nwhen the latency functions are unknown. Surprisingly, we are able to solve\nthese problems even though the optimization problems are non-convex in the\nleader's actions. \n\n"}
{"id": "1504.01431", "contents": "Title: If the Current Clique Algorithms are Optimal, so is Valiant's Parser Abstract: The CFG recognition problem is: given a context-free grammar $\\mathcal{G}$\nand a string $w$ of length $n$, decide if $w$ can be obtained from\n$\\mathcal{G}$. This is the most basic parsing question and is a core computer\nscience problem. Valiant's parser from 1975 solves the problem in\n$O(n^{\\omega})$ time, where $\\omega<2.373$ is the matrix multiplication\nexponent. Dozens of parsing algorithms have been proposed over the years, yet\nValiant's upper bound remains unbeaten. The best combinatorial algorithms have\nmildly subcubic $O(n^3/\\log^3{n})$ complexity.\n  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for\nCFG parsing, and that very efficient and practical algorithms might be hard or\neven impossible to obtain. Lee showed that any algorithm for a more general\nparsing problem with running time $O(|\\mathcal{G}|\\cdot n^{3-\\varepsilon})$ can\nbe converted into a surprising subcubic algorithm for Boolean Matrix\nMultiplication. Unfortunately, Lee's hardness result required that the grammar\nsize be $|\\mathcal{G}|=\\Omega(n^6)$. Nothing was known for the more relevant\ncase of constant size grammars.\n  In this work, we prove that any improvement on Valiant's algorithm, even for\nconstant size grammars, either in terms of runtime or by avoiding the\ninefficiencies of fast matrix multiplication, would imply a breakthrough\nalgorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if\nthere are $k$ that form a clique.\n  Besides classifying the complexity of a fundamental problem, our reduction\nhas led us to similar lower bounds for more modern and well-studied cubic time\nproblems for which faster algorithms are highly desirable in practice: RNA\nFolding, a central problem in computational biology, and Dyck Language Edit\nDistance, answering an open question of Saha (FOCS'14). \n\n"}
{"id": "1504.01642", "contents": "Title: Quantitative $(p,q)$ theorems in combinatorial geometry Abstract: We show quantitative versions of classic results in discrete geometry, where\nthe size of a convex set is determined by some non-negative function. We give\nversions of this kind for the selection theorem of B\\'ar\\'any, the existence of\nweak epsilon-nets for convex sets and the $(p,q)$ theorem of Alon and Kleitman.\nThese methods can be applied to functions such as the volume, surface area or\nnumber of points of a discrete set. We also give general quantitative versions\nof the colorful Helly theorem for continuous functions. \n\n"}
{"id": "1504.01836", "contents": "Title: New Unconditional Hardness Results for Dynamic and Online Problems Abstract: There has been a resurgence of interest in lower bounds whose truth rests on\nthe conjectured hardness of well known computational problems. These\nconditional lower bounds have become important and popular due to the painfully\nslow progress on proving strong unconditional lower bounds. Nevertheless, the\nlong term goal is to replace these conditional bounds with unconditional ones.\nIn this paper we make progress in this direction by studying the cell probe\ncomplexity of two conjectured to be hard problems of particular importance:\nmatrix-vector multiplication and a version of dynamic set disjointness known as\nPatrascu's Multiphase Problem. We give improved unconditional lower bounds for\nthese problems as well as introducing new proof techniques of independent\ninterest. These include a technique capable of proving strong threshold lower\nbounds of the following form: If we insist on having a very fast query time,\nthen the update time has to be slow enough to compute a lookup table with the\nanswer to every possible query. This is the first time a lower bound of this\ntype has been proven. \n\n"}
{"id": "1504.02526", "contents": "Title: Learning Arbitrary Statistical Mixtures of Discrete Distributions Abstract: We study the problem of learning from unlabeled samples very general\nstatistical mixture models on large finite sets. Specifically, the model to be\nlearned, $\\vartheta$, is a probability distribution over probability\ndistributions $p$, where each such $p$ is a probability distribution over $[n]\n= \\{1,2,\\dots,n\\}$. When we sample from $\\vartheta$, we do not observe $p$\ndirectly, but only indirectly and in very noisy fashion, by sampling from $[n]$\nrepeatedly, independently $K$ times from the distribution $p$. The problem is\nto infer $\\vartheta$ to high accuracy in transportation (earthmover) distance.\n  We give the first efficient algorithms for learning this mixture model\nwithout making any restricting assumptions on the structure of the distribution\n$\\vartheta$. We bound the quality of the solution as a function of the size of\nthe samples $K$ and the number of samples used. Our model and results have\napplications to a variety of unsupervised learning scenarios, including\nlearning topic models and collaborative filtering. \n\n"}
{"id": "1504.03856", "contents": "Title: Sparse multivariate polynomial interpolation in the basis of Schubert\n  polynomials Abstract: Schubert polynomials were discovered by A. Lascoux and M. Sch\\\"utzenberger in\nthe study of cohomology rings of flag manifolds in 1980's. These polynomials\ngeneralize Schur polynomials, and form a linear basis of multivariate\npolynomials. In 2003, Lenart and Sottile introduced skew Schubert polynomials,\nwhich generalize skew Schur polynomials, and expand in the Schubert basis with\nthe generalized Littlewood-Richardson coefficients.\n  In this paper we initiate the study of these two families of polynomials from\nthe perspective of computational complexity theory. We first observe that skew\nSchubert polynomials, and therefore Schubert polynomials, are in $\\CountP$\n(when evaluating on non-negative integral inputs) and $\\VNP$.\n  Our main result is a deterministic algorithm that computes the expansion of a\npolynomial $f$ of degree $d$ in $\\Z[x_1, \\dots, x_n]$ in the basis of Schubert\npolynomials, assuming an oracle computing Schubert polynomials. This algorithm\nruns in time polynomial in $n$, $d$, and the bit size of the expansion. This\ngeneralizes, and derandomizes, the sparse interpolation algorithm of symmetric\npolynomials in the Schur basis by Barvinok and Fomin (Advances in Applied\nMathematics, 18(3):271--285). In fact, our interpolation algorithm is general\nenough to accommodate any linear basis satisfying certain natural properties.\n  Applications of the above results include a new algorithm that computes the\ngeneralized Littlewood-Richardson coefficients. \n\n"}
{"id": "1504.05140", "contents": "Title: Route Planning in Transportation Networks Abstract: We survey recent advances in algorithms for route planning in transportation\nnetworks. For road networks, we show that one can compute driving directions in\nmilliseconds or less even at continental scale. A variety of techniques provide\ndifferent trade-offs between preprocessing effort, space requirements, and\nquery time. Some algorithms can answer queries in a fraction of a microsecond,\nwhile others can deal efficiently with real-time traffic. Journey planning on\npublic transportation systems, although conceptually similar, is a\nsignificantly harder problem due to its inherent time-dependent and\nmulticriteria nature. Although exact algorithms are fast enough for interactive\nqueries on metropolitan transit systems, dealing with continent-sized instances\nrequires simplifications or heavy preprocessing. The multimodal route planning\nproblem, which seeks journeys combining schedule-based transportation (buses,\ntrains) with unrestricted modes (walking, driving), is even harder, relying on\napproximate solutions even for metropolitan inputs. \n\n"}
{"id": "1504.05795", "contents": "Title: Gromov meets Phylogenetics - new Animals for the Zoo of Biocomputable\n  Metrics on Tree Space Abstract: We present a new class of metrics for unrooted phylogenetic $X$-trees derived\nfrom the Gromov-Hausdorff distance for (compact) metric spaces. These metrics\ncan be efficiently computed by linear or quadratic programming. They are robust\nunder NNI-operations, too. The local behavior of the metrics shows that they\nare different from any formerly introduced metrics. The performance of the\nmetrics is briefly analised on random weighted and unweighted trees as well as\nrandom caterpillars. \n\n"}
{"id": "1504.06122", "contents": "Title: Random projections for Bayesian regression Abstract: This article deals with random projections applied as a data reduction\ntechnique for Bayesian regression analysis. We show sufficient conditions under\nwhich the entire $d$-dimensional distribution is approximately preserved under\nrandom projections by reducing the number of data points from $n$ to $k\\in\nO(\\operatorname{poly}(d/\\varepsilon))$ in the case $n\\gg d$. Under mild\nassumptions, we prove that evaluating a Gaussian likelihood function based on\nthe projected data instead of the original data yields a\n$(1+O(\\varepsilon))$-approximation in terms of the $\\ell_2$ Wasserstein\ndistance. Our main result shows that the posterior distribution of Bayesian\nlinear regression is approximated up to a small error depending on only an\n$\\varepsilon$-fraction of its defining parameters. This holds when using\narbitrary Gaussian priors or the degenerate case of uniform distributions over\n$\\mathbb{R}^d$ for $\\beta$. Our empirical evaluations involve different\nsimulated settings of Bayesian linear regression. Our experiments underline\nthat the proposed method is able to recover the regression model up to small\nerror while considerably reducing the total running time. \n\n"}
{"id": "1505.00731", "contents": "Title: Generic algorithms for halting problem and optimal machines revisited Abstract: The halting problem is undecidable --- but can it be solved for \"most\"\ninputs? This natural question was considered in a number of papers, in\ndifferent settings. We revisit their results and show that most of them can be\neasily proven in a natural framework of optimal machines (considered in\nalgorithmic information theory) using the notion of Kolmogorov complexity. We\nalso consider some related questions about this framework and about asymptotic\nproperties of the halting problem. In particular, we show that the fraction of\nterminating programs cannot have a limit, and all limit points are Martin-L\\\"of\nrandom reals. We then consider mass problems of finding an approximate solution\nof halting problem and probabilistic algorithms for them, proving both positive\nand negative results. We consider the fraction of terminating programs that\nrequire a long time for termination, and describe this fraction using the busy\nbeaver function. We also consider approximate versions of separation problems,\nand revisit Schnorr's results about optimal numberings showing how they can be\ngeneralized. \n\n"}
{"id": "1505.00828", "contents": "Title: Dynamic Consistency of Conditional Simple Temporal Networks via Mean\n  Payoff Games: a Singly-Exponential Time DC-Checking Abstract: Conditional Simple Temporal Network (CSTN) is a constraint-based\ngraph-formalism for conditional temporal planning. It offers a more flexible\nformalism than the equivalent CSTP model of Tsamardinos, Vidal and Pollack,\nfrom which it was derived mainly as a sound formalization. Three notions of\nconsistency arise for CSTNs and CSTPs: weak, strong, and dynamic. Dynamic\nconsistency is the most interesting notion, but it is also the most challenging\nand it was conjectured to be hard to assess. Tsamardinos, Vidal and Pollack\ngave a doubly-exponential time algorithm for deciding whether a CSTN is\ndynamically-consistent and to produce, in the positive case, a dynamic\nexecution strategy of exponential size. In the present work we offer a proof\nthat deciding whether a CSTN is dynamically-consistent is coNP-hard and provide\nthe first singly-exponential time algorithm for this problem, also producing a\ndynamic execution strategy whenever the input CSTN is dynamically-consistent.\nThe algorithm is based on a novel connection with Mean Payoff Games, a family\nof two-player combinatorial games on graphs well known for having applications\nin model-checking and formal verification. The presentation of such connection\nis mediated by the Hyper Temporal Network model, a tractable generalization of\nSimple Temporal Networks whose consistency checking is equivalent to\ndetermining Mean Payoff Games. In order to analyze the algorithm we introduce a\nrefined notion of dynamic-consistency, named \\epsilon-dynamic-consistency, and\npresent a sharp lower bounding analysis on the critical value of the reaction\ntime \\hat{\\varepsilon} where the CSTN transits from being, to not being,\ndynamically-consistent. The proof technique introduced in this analysis of\n\\hat{\\varepsilon} is applicable more in general when dealing with linear\ndifference constraints which include strict inequalities. \n\n"}
{"id": "1505.01616", "contents": "Title: Colouring graphs with constraints on connectivity Abstract: A graph $G$ has maximal local edge-connectivity $k$ if the maximum number of\nedge-disjoint paths between every pair of distinct vertices $x$ and $y$ is at\nmost $k$. We prove Brooks-type theorems for $k$-connected graphs with maximal\nlocal edge-connectivity $k$, and for any graph with maximal local\nedge-connectivity 3. We also consider several related graph classes defined by\nconstraints on connectivity. In particular, we show that there is a\npolynomial-time algorithm that, given a 3-connected graph $G$ with maximal\nlocal connectivity 3, outputs an optimal colouring for $G$. On the other hand,\nwe prove, for $k \\ge 3$, that $k$-colourability is NP-complete when restricted\nto minimally $k$-connected graphs, and 3-colourability is NP-complete when\nrestricted to $(k-1)$-connected graphs with maximal local connectivity $k$.\nFinally, we consider a parameterization of $k$-colourability based on the\nnumber of vertices of degree at least $k+1$, and prove that, even when $k$ is\npart of the input, the corresponding parameterized problem is FPT. \n\n"}
{"id": "1505.02993", "contents": "Title: A Holant Dichotomy: Is the FKT Algorithm Universal? Abstract: We prove a complexity dichotomy for complex-weighted Holant problems with an\narbitrary set of symmetric constraint functions on Boolean variables. This\ndichotomy is specifically to answer the question: Is the FKT algorithm under a\nholographic transformation a \\emph{universal} strategy to obtain\npolynomial-time algorithms for problems over planar graphs that are intractable\nin general? This dichotomy is a culmination of previous ones, including those\nfor Spin Systems, Holant, and #CSP. A recurring theme has been that a\nholographic reduction to FKT is a universal strategy. Surprisingly, for planar\nHolant, we discover new planar tractable problems that are not expressible by a\nholographic reduction to FKT.\n  In previous work, an important tool was a dichotomy for #CSP^d, which denotes\n#CSP where every variable appears a multiple of d times. However its proof\nviolates planarity. We prove a dichotomy for planar #CSP^2. We apply this\nplanar #CSP^2 dichotomy in the proof of the planar Holant dichotomy.\n  As a special case of our new planar tractable problems, counting perfect\nmatchings (#PM) over k-uniform hypergraphs is polynomial-time computable when\nthe incidence graph is planar and k >= 5. The same problem is #P-hard when k=3\nor k=4, which is also a consequence of our dichotomy. When k=2, it becomes #PM\nover planar graphs and is tractable again. More generally, over hypergraphs\nwith specified hyperedge sizes and the same planarity assumption, #PM is\npolynomial-time computable if the greatest common divisor of all hyperedge\nsizes is at least 5. \n\n"}
{"id": "1505.03424", "contents": "Title: Beating the random assignment on constraint satisfaction problems of\n  bounded degree Abstract: We show that for any odd $k$ and any instance of the Max-kXOR constraint\nsatisfaction problem, there is an efficient algorithm that finds an assignment\nsatisfying at least a $\\frac{1}{2} + \\Omega(1/\\sqrt{D})$ fraction of\nconstraints, where $D$ is a bound on the number of constraints that each\nvariable occurs in. This improves both qualitatively and quantitatively on the\nrecent work of Farhi, Goldstone, and Gutmann (2014), which gave a\n\\emph{quantum} algorithm to find an assignment satisfying a $\\frac{1}{2} +\n\\Omega(D^{-3/4})$ fraction of the equations.\n  For arbitrary constraint satisfaction problems, we give a similar result for\n\"triangle-free\" instances; i.e., an efficient algorithm that finds an\nassignment satisfying at least a $\\mu + \\Omega(1/\\sqrt{D})$ fraction of\nconstraints, where $\\mu$ is the fraction that would be satisfied by a uniformly\nrandom assignment. \n\n"}
{"id": "1505.05800", "contents": "Title: Complexity Theoretic Limitations on Learning Halfspaces Abstract: We study the problem of agnostically learning halfspaces which is defined by\na fixed but unknown distribution $\\mathcal{D}$ on $\\mathbb{Q}^n\\times \\{\\pm\n1\\}$. We define $\\mathrm{Err}_{\\mathrm{HALF}}(\\mathcal{D})$ as the least error\nof a halfspace classifier for $\\mathcal{D}$. A learner who can access\n$\\mathcal{D}$ has to return a hypothesis whose error is small compared to\n$\\mathrm{Err}_{\\mathrm{HALF}}(\\mathcal{D})$.\n  Using the recently developed method of the author, Linial and Shalev-Shwartz\nwe prove hardness of learning results under a natural assumption on the\ncomplexity of refuting random $K$-$\\mathrm{XOR}$ formulas. We show that no\nefficient learning algorithm has non-trivial worst-case performance even under\nthe guarantees that $\\mathrm{Err}_{\\mathrm{HALF}}(\\mathcal{D}) \\le \\eta$ for\narbitrarily small constant $\\eta>0$, and that $\\mathcal{D}$ is supported in\n$\\{\\pm 1\\}^n\\times \\{\\pm 1\\}$. Namely, even under these favorable conditions\nits error must be $\\ge \\frac{1}{2}-\\frac{1}{n^c}$ for every $c>0$. In\nparticular, no efficient algorithm can achieve a constant approximation ratio.\nUnder a stronger version of the assumption (where $K$ can be poly-logarithmic\nin $n$), we can take $\\eta = 2^{-\\log^{1-\\nu}(n)}$ for arbitrarily small\n$\\nu>0$. Interestingly, this is even stronger than the best known lower bounds\n(Arora et. al. 1993, Feldamn et. al. 2006, Guruswami and Raghavendra 2006) for\nthe case that the learner is restricted to return a halfspace classifier (i.e.\nproper learning). \n\n"}
{"id": "1505.06032", "contents": "Title: Variable Neighborhood Search for solving Bandwidth Coloring Problem Abstract: This paper presents a variable neighborhood search (VNS) algorithm for\nsolving bandwidth coloring problem (BCP) and bandwidth multicoloring problem\n(BMCP). BCP and BMCP are generalizations of the well known vertex coloring\nproblem and they are of a great interest from both theoretical and practical\npoints of view. Presented VNS combines a shaking procedure which perturbs the\ncolors for an increasing number of vertices and a specific variable\nneighborhood descent (VND) procedure, based on the specially designed\narrangement of the vertices which are the subject of re-coloring. By this\napproach, local search is split in a series of disjoint procedures, enabling\nbetter choice of the vertices which are addressed to re-color. The experiments\nshow that proposed method is highly competitive with the state-of-the-art\nalgorithms and improves 2 out of 33 previous best known solutions for BMCP. \n\n"}
{"id": "1505.07807", "contents": "Title: Injective Hulls of Infinite Totally Split-Decomposable Metric Spaces Abstract: We consider the class of (possibly) infinite metric spaces with\ninteger-valued totally split-decomposable metric and possessing an injective\nhull which has the structure of a polyhedral complex. For this class, we give a\ncharacterization for the injective hull to be combinatorially equivalent to a\nCAT(0) cube complex. In order to obtain these results, we extend the\ndecomposition theory introduced by Bandelt and Dress in 1992 as well as results\non the tight span of totally split-decomposable metric spaces proved by Huber,\nKoolen and Moulton in 2006.\n  As an application, and using results of Lang of 2013, we obtain proper\nactions on CAT(0) cube complexes for finitely generated groups endowed with a\ntotally split-decomposable word metric whose associated splits satisfy an easy\ncombinatorial property. In the case of Gromov hyperbolic groups, the action is\nproper as well as cocompact. \n\n"}
{"id": "1506.01695", "contents": "Title: Polynomial-time Algorithm for Isomorphism of Graphs with Clique-width at\n  most Three Abstract: The clique-width is a measure of complexity of decomposing graphs into\ncertain tree-like structures. The class of graphs with bounded clique-width\ncontains bounded tree-width graphs. We give a polynomial time graph isomorphism\nalgorithm for graphs with clique-width at most three. Our work is independent\nof the work by Grohe et al. \\cite{grohe2015isomorphism} showing that the\nisomorphism problem for graphs of bounded clique-width is polynomial time. \n\n"}
{"id": "1506.02191", "contents": "Title: Positive-fraction intersection results and variations of weak\n  epsilon-nets Abstract: Given a finite set $X$ of points in $R^n$ and a family $F$ of sets generated\nby the pairs of points of $X$, we determine volumetric and structural\nconditions for the sets that allow us to guarantee the existence of a\npositive-fraction subfamily $F'$ of $F$ for which the sets have non-empty\nintersection. This allows us to show the existence of weak epsilon-nets for\nthese families. We also prove a topological variation of the existence of weak\nepsilon-nets for convex sets. \n\n"}
{"id": "1506.02243", "contents": "Title: On approximating tree spanners that are breadth first search trees Abstract: A tree $t$-spanner $T$ of a graph $G$ is a spanning tree of $G$ such that the\ndistance in $T$ between every pair of verices is at most $t$ times the distance\nin $G$ between them. There are efficient algorithms that find a tree $t\\cdot\nO(\\log n)$-spanner of a graph $G$, when $G$ admits a tree $t$-spanner. In this\npaper, the search space is narrowed to $v$-concentrated spanning trees, a\nsimple family that includes all the breadth first search trees starting from\nvertex $v$. In this case, it is not easy to find approximate tree spanners\nwithin factor almost $o(\\log n)$. Specifically, let $m$ and $t$ be integers,\nsuch that $m>0$ and $t\\geq 7$. If there is an efficient algorithm that receives\nas input a graph $G$ and a vertex $v$ and returns a $v$-concentrated tree\n$t\\cdot o((\\log n)^{m/(m+1)})$-spanner of $G$, when $G$ admits a\n$v$-concentrated tree $t$-spanner, then there is an algorithm that decides\n3-SAT in quasi-polynomial time. \n\n"}
{"id": "1506.02442", "contents": "Title: NP-hardness of sortedness constraints Abstract: In Constraint Programming, global constraints allow to model and solve many\ncombinatorial problems. Among these constraints, several sortedness constraints\nhave been defined, for which propagation algorithms are available, but for\nwhich the tractability is not settled. We show that the sort(U,V) constraint\n(Older et. al, 1995) is intractable for integer variables whose domains are not\nlimited to intervals. As a consequence, the similar result holds for the\nsort(U,V, P) constraint (Zhou, 1996). Moreover, the intractability holds even\nunder the stability condition present in the recently introduced\nkeysorting(U,V,Keys,P) constraint (Carlsson et al., 2014), and requiring that\nthe order of the variables with the same value in the list U be preserved in\nthe list V. Therefore, keysorting(U,V,Keys,P) is intractable as well. \n\n"}
{"id": "1506.04862", "contents": "Title: EERTREE: An Efficient Data Structure for Processing Palindromes in\n  Strings Abstract: We propose a new linear-size data structure which provides a fast access to\nall palindromic substrings of a string or a set of strings. This structure\ninherits some ideas from the construction of both the suffix trie and suffix\ntree. Using this structure, we present simple and efficient solutions for a\nnumber of problems involving palindromes. \n\n"}
{"id": "1506.06302", "contents": "Title: Inapproximability of $H$-Transversal/Packing Abstract: Given an undirected graph $G = (V_G, E_G)$ and a fixed \"pattern\" graph $H =\n(V_H, E_H)$ with $k$ vertices, we consider the $H$-Transversal and $H$-Packing\nproblems. The former asks to find the smallest $S \\subseteq V_G$ such that the\nsubgraph induced by $V_G \\setminus S$ does not have $H$ as a subgraph, and the\nlatter asks to find the maximum number of pairwise disjoint $k$-subsets $S_1,\n..., S_m \\subseteq V_G$ such that the subgraph induced by each $S_i$ has $H$ as\na subgraph.\n  We prove that if $H$ is 2-connected, $H$-Transversal and $H$-Packing are\nalmost as hard to approximate as general $k$-Hypergraph Vertex Cover and\n$k$-Set Packing, so it is NP-hard to approximate them within a factor of\n$\\Omega (k)$ and $\\widetilde \\Omega (k)$ respectively. We also show that there\nis a 1-connected $H$ where $H$-Transversal admits an $O(\\log k)$-approximation\nalgorithm, so that the connectivity requirement cannot be relaxed from 2 to 1.\nFor a special case of $H$-Transversal where $H$ is a (family of) cycles, we\nmention the implication of our result to the related Feedback Vertex Set\nproblem, and give a different hardness proof for directed graphs. \n\n"}
{"id": "1506.07773", "contents": "Title: Maximum weighted independent sets with a budget Abstract: Given a graph $G$, a non-negative integer $k$, and a weight function that\nmaps each vertex in $G$ to a positive real number, the \\emph{Maximum Weighted\nBudgeted Independent Set (MWBIS) problem} is about finding a maximum weighted\nindependent set in $G$ of cardinality at most $k$. A special case of MWBIS,\nwhen the weight assigned to each vertex is equal to its degree in $G$, is\ncalled the \\emph{Maximum Independent Vertex Coverage (MIVC)} problem. In other\nwords, the MIVC problem is about finding an independent set of cardinality at\nmost $k$ with maximum coverage.\n  Since it is a generalization of the well-known Maximum Weighted Independent\nSet (MWIS) problem, MWBIS too does not have any constant factor polynomial time\napproximation algorithm assuming $P \\neq NP$. In this paper, we study MWBIS in\nthe context of bipartite graphs. We show that, unlike MWIS, the MIVC (and\nthereby the MWBIS) problem in bipartite graphs is NP-hard. Then, we show that\nthe MWBIS problem admits a $\\frac{1}{2}$-factor approximation algorithm in the\nclass of bipartite graphs, which matches the integrality gap of a natural LP\nrelaxation. \n\n"}
{"id": "1506.07776", "contents": "Title: An Experimental Evaluation of the Best-of-Many Christofides' Algorithm\n  for the Traveling Salesman Problem Abstract: Recent papers on approximation algorithms for the traveling salesman problem\n(TSP) have given a new variant on the well-known Christofides' algorithm for\nthe TSP, called the Best-of-Many Christofides' algorithm. The algorithm\ninvolves sampling a spanning tree from the solution the standard LP relaxation\nof the TSP, subject to the condition that each edge is sampled with probability\nat most its value in the LP relaxation. One then runs Christofides' algorithm\non the tree by computing a minimum-cost matching on the odd-degree vertices in\nthe tree, and shortcutting the resulting Eulerian graph to a tour. In this\npaper we perform an experimental evaluation of the Best-of-Many Christofides'\nalgorithm to see if there are empirical reasons to believe its performance is\nbetter than that of Christofides' algorithm. Furthermore, several different\nsampling schemes have been proposed; we implement several different schemes to\ndetermine which ones might be the most promising for obtaining improved\nperformance guarantees over that of Christofides' algorithm. In our\nexperiments, all of the implemented methods perform significantly better than\nthe Christofides' algorithm; an algorithm that samples from a maximum entropy\ndistribution over spanning trees seems to be particularly good, though there\nare others that perform almost as well. \n\n"}
{"id": "1506.08204", "contents": "Title: Sparsified Cholesky Solvers for SDD linear systems Abstract: We show that Laplacian and symmetric diagonally dominant (SDD) matrices can\nbe well approximated by linear-sized sparse Cholesky factorizations. We show\nthat these matrices have constant-factor approximations of the form $L L^{T}$,\nwhere $L$ is a lower-triangular matrix with a number of nonzero entries linear\nin its dimension. Furthermore linear systems in $L$ and $L^{T}$ can be solved\nin $O (n)$ work and $O(\\log{n}\\log^2\\log{n})$ depth, where $n$ is the dimension\nof the matrix.\n  We present nearly linear time algorithms that construct solvers that are\nalmost this efficient. In doing so, we give the first nearly-linear work\nroutine for constructing spectral vertex sparsifiers---that is, spectral\napproximations of Schur complements of Laplacian matrices. \n\n"}
{"id": "1507.00843", "contents": "Title: Optimal linear Bernoulli factories for small mean problems Abstract: Suppose a coin with unknown probability $p$ of heads can be flipped as often\nas desired. A Bernoulli factory for a function $f$ is an algorithm that uses\nflips of the coin together with auxiliary randomness to flip a single coin with\nprobability $f(p)$ of heads. Applications include near perfect sampling from\nthe stationary distribution of regenerative processes. When $f$ is analytic,\nthe problem can be reduced to a Bernoulli factory of the form $f(p) = Cp$ for\nconstant $C$. Presented here is a new algorithm where for small values of $Cp$,\nrequires roughly only $C$ coin flips to generate a $Cp$ coin. From information\ntheory considerations, this is also conjectured to be (to first order) the\nminimum number of flips needed by any such algorithm.\n  For $Cp$ large, the new algorithm can also be used to build a new Bernoulli\nfactory that uses only 80\\% of the expected coin flips of the older method, and\napplies to the more general problem of a multivariate Bernoulli factory, where\nthere are $k$ coins, the $k$th coin has unknown probability $p_k$ of heads, and\nthe goal is to simulate a coin flip with probability $C_1 p_1 + \\cdots + C_k\np_k$ of heads. \n\n"}
{"id": "1507.01768", "contents": "Title: The Restricted Isometry Property of Subsampled Fourier Matrices Abstract: A matrix $A \\in \\mathbb{C}^{q \\times N}$ satisfies the restricted isometry\nproperty of order $k$ with constant $\\varepsilon$ if it preserves the $\\ell_2$\nnorm of all $k$-sparse vectors up to a factor of $1\\pm \\varepsilon$. We prove\nthat a matrix $A$ obtained by randomly sampling $q = O(k \\cdot \\log^2 k \\cdot\n\\log N)$ rows from an $N \\times N$ Fourier matrix satisfies the restricted\nisometry property of order $k$ with a fixed $\\varepsilon$ with high\nprobability. This improves on Rudelson and Vershynin (Comm. Pure Appl. Math.,\n2008), its subsequent improvements, and Bourgain (GAFA Seminar Notes, 2014). \n\n"}
{"id": "1507.02089", "contents": "Title: Zero-free regions of partition functions with applications to algorithms\n  and graph limits Abstract: Based on a technique of Barvinok and Barvinok and Sober\\'on we identify a\nclass of edge-coloring models whose partition functions do not evaluate to zero\non bounded degree graphs. Subsequently we give a quasi-polynomial time\napproximation scheme for computing these partition functions. As another\napplication we show that the normalised partition functions of these models are\ncontinuous with respect the Benjamini-Schramm topology on bounded degree\ngraphs. We moreover give quasi-polynomial time approximation schemes for\nevaluating a large class of graph polynomials, including the Tutte polynomial,\non bounded degree graphs. \n\n"}
{"id": "1507.03046", "contents": "Title: An efficient tree decomposition method for permanents and mixed\n  discriminants Abstract: We present an efficient algorithm to compute permanents, mixed discriminants\nand hyperdeterminants of structured matrices and multidimensional arrays\n(tensors). We describe the sparsity structure of an array in terms of a graph,\nand we assume that its treewidth, denoted as $\\omega$, is small. Our algorithm\nrequires $O(n 2^\\omega)$ arithmetic operations to compute permanents, and\n$O(n^2 + n 3^\\omega)$ for mixed discriminants and hyperdeterminants. We finally\nshow that mixed volume computation continues to be hard under bounded treewidth\nassumptions. \n\n"}
{"id": "1507.03269", "contents": "Title: Tensor principal component analysis via sum-of-squares proofs Abstract: We study a statistical model for the tensor principal component analysis\nproblem introduced by Montanari and Richard: Given a order-$3$ tensor $T$ of\nthe form $T = \\tau \\cdot v_0^{\\otimes 3} + A$, where $\\tau \\geq 0$ is a\nsignal-to-noise ratio, $v_0$ is a unit vector, and $A$ is a random noise\ntensor, the goal is to recover the planted vector $v_0$. For the case that $A$\nhas iid standard Gaussian entries, we give an efficient algorithm to recover\n$v_0$ whenever $\\tau \\geq \\omega(n^{3/4} \\log(n)^{1/4})$, and certify that the\nrecovered vector is close to a maximum likelihood estimator, all with high\nprobability over the random choice of $A$. The previous best algorithms with\nprovable guarantees required $\\tau \\geq \\Omega(n)$.\n  In the regime $\\tau \\leq o(n)$, natural tensor-unfolding-based spectral\nrelaxations for the underlying optimization problem break down (in the sense\nthat their integrality gap is large). To go beyond this barrier, we use convex\nrelaxations based on the sum-of-squares method. Our recovery algorithm proceeds\nby rounding a degree-$4$ sum-of-squares relaxations of the\nmaximum-likelihood-estimation problem for the statistical model. To complement\nour algorithmic results, we show that degree-$4$ sum-of-squares relaxations\nbreak down for $\\tau \\leq O(n^{3/4}/\\log(n)^{1/4})$, which demonstrates that\nimproving our current guarantees (by more than logarithmic factors) would\nrequire new techniques or might even be intractable.\n  Finally, we show how to exploit additional problem structure in order to\nsolve our sum-of-squares relaxations, up to some approximation, very\nefficiently. Our fastest algorithm runs in nearly-linear time using shifted\n(matrix) power iteration and has similar guarantees as above. The analysis of\nthis algorithm also confirms a variant of a conjecture of Montanari and Richard\nabout singular vectors of tensor unfoldings. \n\n"}
{"id": "1507.03738", "contents": "Title: Tight Bounds for Subgraph Isomorphism and Graph Homomorphism Abstract: We prove that unless Exponential Time Hypothesis (ETH) fails, deciding if\nthere is a homomorphism from graph $G$ to graph $H$ cannot be done in time\n$|V(H)|^{o(|V(G)|)}$. Combined with the reduction of Cygan, Pachocki, and\nSoca{\\l}a, our result rules out (subject to ETH) a possibility of\n$|V(G)|^{o(|V(G)|)}$-time algorithm deciding if graph $H$ is a subgraph of $G$.\nFor both problems our lower bounds asymptotically match the running time of\nbrute-force algorithms trying all possible mappings of one graph into another.\nThus, our work closes the gap in the known complexity of these fundamental\nproblems. \n\n"}
{"id": "1507.05136", "contents": "Title: Tight Lower Bounds for Planted Clique in the Degree-4 SOS Program Abstract: We give a lower bound of $\\tilde{\\Omega}(\\sqrt{n})$ for the degree-4\nSum-of-Squares SDP relaxation for the planted clique problem. Specifically, we\nshow that on an Erd\\\"os-R\\'enyi graph $G(n,\\tfrac{1}{2})$, with high\nprobability there is a feasible point for the degree-4 SOS relaxation of the\nclique problem with an objective value of $\\tilde{\\Omega}(\\sqrt{n})$, so that\nthe program cannot distinguish between a random graph and a random graph with a\nplanted clique of size $\\tilde{O}(\\sqrt{n})$. This bound is tight.\n  We build on the works of Deshpande and Montanari and Meka et al., who give\nlower bounds of $\\tilde{\\Omega}(n^{1/3})$ and $\\tilde{\\Omega}(n^{1/4})$\nrespectively. We improve on their results by making a perturbation to the SDP\nsolution proposed in their work, then showing that this perturbation remains\nPSD as the objective value approaches $\\tilde{\\Omega}(n^{1/2})$.\n  In an independent work, Hopkins, Kothari and Potechin [HKP15] have obtained a\nsimilar lower bound for the degree-$4$ SOS relaxation. \n\n"}
{"id": "1507.05230", "contents": "Title: SoS and Planted Clique: Tight Analysis of MPW Moments at all Degrees and\n  an Optimal Lower Bound at Degree Four Abstract: The problem of finding large cliques in random graphs and its \"planted\"\nvariant, where one wants to recover a clique of size $\\omega \\gg \\log{(n)}$\nadded to an \\Erdos-\\Renyi graph $G \\sim G(n,\\frac{1}{2})$, have been intensely\nstudied. Nevertheless, existing polynomial time algorithms can only recover\nplanted cliques of size $\\omega = \\Omega(\\sqrt{n})$. By contrast, information\ntheoretically, one can recover planted cliques so long as $\\omega \\gg\n\\log{(n)}$. In this work, we continue the investigation of algorithms from the\nsum of squares hierarchy for solving the planted clique problem begun by Meka,\nPotechin, and Wigderson (MPW, 2015) and Deshpande and Montanari (DM,2015). Our\nmain results improve upon both these previous works by showing:\n  1. Degree four SoS does not recover the planted clique unless $\\omega \\gg\n\\sqrt n poly \\log n$, improving upon the bound $\\omega \\gg n^{1/3}$ due to DM.\nA similar result was obtained independently by Raghavendra and Schramm (2015).\n  2. For $2 < d = o(\\sqrt{\\log{(n)}})$, degree $2d$ SoS does not recover the\nplanted clique unless $\\omega \\gg n^{1/(d + 1)} /(2^d poly \\log n)$, improving\nupon the bound due to MPW.\n  Our proof for the second result is based on a fine spectral analysis of the\ncertificate used in the prior works MPW,DM and Feige and Krauthgamer (2003) by\ndecomposing it along an appropriately chosen basis. Along the way, we develop\ncombinatorial tools to analyze the spectrum of random matrices with dependent\nentries and to understand the symmetries in the eigenspaces of the set\nsymmetric matrices inspired by work of Grigoriev (2001).\n  An argument of Kelner shows that the first result cannot be proved using the\nsame certificate. Rather, our proof involves constructing and analyzing a new\ncertificate that yields the nearly tight lower bound by \"correcting\" the\ncertificate of previous works. \n\n"}
{"id": "1507.05854", "contents": "Title: Global Convergence of Non-Convex Gradient Descent for Computing Matrix\n  Squareroot Abstract: While there has been a significant amount of work studying gradient descent\ntechniques for non-convex optimization problems over the last few years, all\nexisting results establish either local convergence with good rates or global\nconvergence with highly suboptimal rates, for many problems of interest. In\nthis paper, we take the first step in getting the best of both worlds --\nestablishing global convergence and obtaining a good rate of convergence for\nthe problem of computing squareroot of a positive definite (PD) matrix, which\nis a widely studied problem in numerical linear algebra with applications in\nmachine learning and statistics among others. Given a PD matrix $M$ and a PD\nstarting point $U_0$, we show that gradient descent with appropriately chosen\nstep-size finds an $\\epsilon$-accurate squareroot of $M$ in $O(\\alpha \\log\n(\\|M-U_0^2\\|_F /\\epsilon))$ iterations, where $\\alpha =\n(\\max\\{\\|U_0\\|_2^2,\\|M\\|_2\\} / \\min \\{\\sigma_{\\min}^2(U_0),\\sigma_{\\min}(M) \\}\n)^{3/2}$. Our result is the first to establish global convergence for this\nproblem and that it is robust to errors in each iteration. A key contribution\nof our work is the general proof technique which we believe should further\nexcite research in understanding deterministic and stochastic variants of\nsimple non-convex gradient descent algorithms with good global convergence\nrates for other problems in machine learning and numerical linear algebra. \n\n"}
{"id": "1507.05950", "contents": "Title: On the Worst-Case Approximability of Sparse PCA Abstract: It is well known that Sparse PCA (Sparse Principal Component Analysis) is\nNP-hard to solve exactly on worst-case instances. What is the complexity of\nsolving Sparse PCA approximately? Our contributions include: 1) a simple and\nefficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness\nof approximation to within $(1-\\varepsilon)$, for some small constant\n$\\varepsilon > 0$; 3) SSE-hardness of approximation to within any constant\nfactor; and 4) an $\\exp\\exp\\left(\\Omega\\left(\\sqrt{\\log \\log n}\\right)\\right)$\n(\"quasi-quasi-polynomial\") gap for the standard semidefinite program. \n\n"}
{"id": "1507.06370", "contents": "Title: Sum-of-Squares Lower Bounds for Sparse PCA Abstract: This paper establishes a statistical versus computational trade-off for\nsolving a basic high-dimensional machine learning problem via a basic convex\nrelaxation method. Specifically, we consider the {\\em Sparse Principal\nComponent Analysis} (Sparse PCA) problem, and the family of {\\em\nSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well\nknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em\nin principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli)\nsamples, but all {\\em efficient} (polynomial time) algorithms known require $n\n\\approx k^2$ samples. It was also known that this quadratic gap cannot be\nimproved by the the most basic {\\em semi-definite} (SDP, aka spectral)\nrelaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also\ndegree-4 SoS algorithms cannot improve this quadratic gap. This average-case\nlower bound adds to the small collection of hardness results in machine\nlearning for this powerful family of convex relaxation algorithms. Moreover,\nour design of moments (or \"pseudo-expectations\") for this lower bound is quite\ndifferent than previous lower bounds. Establishing lower bounds for higher\ndegree SoS algorithms for remains a challenging problem. \n\n"}
{"id": "1507.06370", "contents": "Title: Sum-of-Squares Lower Bounds for Sparse PCA Abstract: This paper establishes a statistical versus computational trade-off for\nsolving a basic high-dimensional machine learning problem via a basic convex\nrelaxation method. Specifically, we consider the {\\em Sparse Principal\nComponent Analysis} (Sparse PCA) problem, and the family of {\\em\nSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well\nknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em\nin principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli)\nsamples, but all {\\em efficient} (polynomial time) algorithms known require $n\n\\approx k^2$ samples. It was also known that this quadratic gap cannot be\nimproved by the the most basic {\\em semi-definite} (SDP, aka spectral)\nrelaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also\ndegree-4 SoS algorithms cannot improve this quadratic gap. This average-case\nlower bound adds to the small collection of hardness results in machine\nlearning for this powerful family of convex relaxation algorithms. Moreover,\nour design of moments (or \"pseudo-expectations\") for this lower bound is quite\ndifferent than previous lower bounds. Establishing lower bounds for higher\ndegree SoS algorithms for remains a challenging problem. \n\n"}
{"id": "1507.06638", "contents": "Title: A Geometric Lower Bound Theorem Abstract: We resolve a conjecture of Kalai relating approximation theory of convex\nbodies by simplicial polytopes to the face numbers and primitive Betti numbers\nof these polytopes and their toric varieties. The proof uses higher notions of\nchordality. Further, for C^2-convex bodies, asymptotically tight lower bounds\non the g-numbers of the approximating polytopes are given, in terms of their\nHausdorff distance from the convex body. \n\n"}
{"id": "1507.06970", "contents": "Title: Perturbed Iterate Analysis for Asynchronous Stochastic Optimization Abstract: We introduce and analyze stochastic optimization methods where the input to\neach gradient update is perturbed by bounded noise. We show that this framework\nforms the basis of a unified approach to analyze asynchronous implementations\nof stochastic optimization algorithms.In this framework, asynchronous\nstochastic optimization algorithms can be thought of as serial methods\noperating on noisy inputs. Using our perturbed iterate framework, we provide\nnew analyses of the Hogwild! algorithm and asynchronous stochastic coordinate\ndescent, that are simpler than earlier analyses, remove many assumptions of\nprevious models, and in some cases yield improved upper bounds on the\nconvergence rates. We proceed to apply our framework to develop and analyze\nKroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient\n(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the\nsparse and parallel version of SVRG is in some cases more than four orders of\nmagnitude faster than the standard SVRG algorithm. \n\n"}
{"id": "1507.07080", "contents": "Title: Range Predecessor and Lempel-Ziv Parsing Abstract: The Lempel-Ziv parsing of a string (LZ77 for short) is one of the most\nimportant and widely-used algorithmic tools in data compression and string\nprocessing. We show that the Lempel-Ziv parsing of a string of length $n$ on an\nalphabet of size $\\sigma$ can be computed in $O(n\\log\\log\\sigma)$ time ($O(n)$\ntime if we allow randomization) using $O(n\\log\\sigma)$ bits of working space;\nthat is, using space proportional to that of the input string in bits. The\nprevious fastest algorithm using $O(n\\log\\sigma)$ space takes\n$O(n(\\log\\sigma+\\log\\log n))$ time. We also consider the important rightmost\nvariant of the problem, where the goal is to associate with each phrase of the\nparsing its most recent occurrence in the input string. We solve this problem\nin $O(n(1 + (\\log\\sigma/\\sqrt{\\log n}))$ time, using the same working space as\nabove. The previous best solution for rightmost parsing uses\n$O(n(1+\\log\\sigma/\\log\\log n))$ time and $O(n\\log n)$ space. As a bonus, in our\nsolution for rightmost parsing we provide a faster construction method for\nefficient 2D orthogonal range reporting, which is of independent interest. \n\n"}
{"id": "1507.07495", "contents": "Title: Estimating an Activity Driven Hidden Markov Model Abstract: We define a Hidden Markov Model (HMM) in which each hidden state has\ntime-dependent $\\textit{activity levels}$ that drive transitions and emissions,\nand show how to estimate its parameters. Our construction is motivated by the\nproblem of inferring human mobility on sub-daily time scales from, for example,\nmobile phone records. \n\n"}
{"id": "1507.08158", "contents": "Title: Fast Biclustering by Dual Parameterization Abstract: We study two clustering problems, Starforest Editing, the problem of adding\nand deleting edges to obtain a disjoint union of stars, and the generalization\nBicluster Editing. We show that, in addition to being NP-hard, none of the\nproblems can be solved in subexponential time unless the exponential time\nhypothesis fails.\n  Misra, Panolan, and Saurabh (MFCS 2013) argue that introducing a bound on the\nnumber of connected components in the solution should not make the problem\neasier: In particular, they argue that the subexponential time algorithm for\nediting to a fixed number of clusters (p-Cluster Editing) by Fomin et al. (J.\nComput. Syst. Sci., 80(7) 2014) is an exception rather than the rule. Here, p\nis a secondary parameter, bounding the number of components in the solution.\n  However, upon bounding the number of stars or bicliques in the solution, we\nobtain algorithms which run in time $2^{5 \\sqrt{pk}} + O(n+m)$ for p-Starforest\nEditing and $2^{O(p \\sqrt{k} \\log(pk))} + O(n+m)$ for p-Bicluster Editing. We\nobtain a similar result for the more general case of t-Partite p-Cluster\nEditing. This is subexponential in k for fixed number of clusters, since p is\nthen considered a constant.\n  Our results even out the number of multivariate subexponential time\nalgorithms and give reasons to believe that this area warrants further study. \n\n"}
{"id": "1508.01110", "contents": "Title: Symmetries of matrix multiplication algorithms. I Abstract: In this work the algorithms of fast multiplication of matrices are\nconsidered. To any algorithm there associated a certain group of automorphisms.\nThese automorphism groups are found for some well-known algorithms, including\nalgorithms of Hopcroft, Laderman, and Pan. The automorphism group is isomorphic\nto $S_3\\times Z_2$ and $S_4$ for Hopcroft anf Laderman algorithms,\nrespectively. The studying of symmetry of algorithms may be a fruitful idea for\nfinding fast algorithms, by an analogy with well-known optimization problems\nfor codes, lattices, and graphs.\n  {\\em Keywords}: Strassen algorithm, symmetry, fast matrix multiplication. \n\n"}
{"id": "1508.01504", "contents": "Title: Resource Oblivious Sorting on Multicores Abstract: We present a deterministic sorting algorithm, SPMS (Sample, Partition, and\nMerge Sort), that interleaves the partitioning of a sample sort with merging.\nSequentially, it sorts $n$ elements in $O(n \\log n)$ time cache-obliviously\nwith an optimal number of cache misses. The parallel complexity (or critical\npath length) of the algorithm is $O(\\log n \\cdot \\log\\log n)$, which improves\non previous bounds for optimal cache oblivious sorting. The algorithm also has\nlow false sharing costs. When scheduled by a work-stealing scheduler in a\nmulticore computing environment with a global shared memory and $p$ cores, each\nhaving a cache of size $M$ organized in blocks of size $B$, the costs of the\nadditional cache misses and false sharing misses due to this parallel execution\nare bounded by the cost of $O(S\\cdot M/B)$ and $O(S \\cdot B)$ cache misses\nrespectively, where $S$ is the number of steals performed during the execution.\nFinally, SPMS is resource oblivious in Athat the dependence on machine\nparameters appear only in the analysis of its performance, and not within the\nalgorithm itself. \n\n"}
{"id": "1508.03537", "contents": "Title: Scribability problems for polytopes Abstract: In this paper we study various scribability problems for polytopes. We begin\nwith the classical $k$-scribability problem proposed by Steiner and generalized\nby Schulte, which asks about the existence of $d$-polytopes that cannot be\nrealized with all $k$-faces tangent to a sphere. We answer this problem for\nstacked and cyclic polytopes for all values of $d$ and $k$. We then continue\nwith the weak scribability problem proposed by Gr\\\"unbaum and Shephard, for\nwhich we complete the work of Schulte by presenting non weakly circumscribable\n$3$-polytopes. Finally, we propose new $(i,j)$-scribability problems, in a\nstrong and a weak version, which generalize the classical ones. They ask about\nthe existence of $d$-polytopes that can not be realized with all their\n$i$-faces \"avoiding\" the sphere and all their $j$-faces \"cutting\" the sphere.\nWe provide such examples for all the cases where $j-i \\le d-3$. \n\n"}
{"id": "1508.05013", "contents": "Title: Message Passing and Combinatorial Optimization Abstract: Graphical models use the intuitive and well-studied methods of graph theory\nto implicitly represent dependencies between variables in large systems. They\ncan model the global behaviour of a complex system by specifying only local\nfactors. This thesis studies inference in discrete graphical models from an\nalgebraic perspective and the ways inference can be used to express and\napproximate NP-hard combinatorial problems.\n  We investigate the complexity and reducibility of various inference problems,\nin part by organizing them in an inference hierarchy. We then investigate\ntractable approximations for a subset of these problems using distributive law\nin the form of message passing. The quality of the resulting message passing\nprocedure, called Belief Propagation (BP), depends on the influence of loops in\nthe graphical model. We contribute to three classes of approximations that\nimprove BP for loopy graphs A) loop correction techniques; B) survey\npropagation, another message passing technique that surpasses BP in some\nsettings; and C) hybrid methods that interpolate between deterministic message\npassing and Markov Chain Monte Carlo inference.\n  We then review the existing message passing solutions and provide novel\ngraphical models and inference techniques for combinatorial problems under\nthree broad classes: A) constraint satisfaction problems such as\nsatisfiability, coloring, packing, set / clique-cover and dominating /\nindependent set and their optimization counterparts; B) clustering problems\nsuch as hierarchical clustering, K-median, K-clustering, K-center and\nmodularity optimization; C) problems over permutations including assignment,\ngraph morphisms and alignment, finding symmetries and traveling salesman\nproblem. In many cases we show that message passing is able to find solutions\nthat are either near optimal or favourably compare with today's\nstate-of-the-art approaches. \n\n"}
{"id": "1508.05282", "contents": "Title: Lower bounds for the parameterized complexity of Minimum Fill-in and\n  other completion problems Abstract: In this work, we focus on several completion problems for subclasses of\nchordal graphs: Minimum Fill-In, Interval Completion, Proper Interval\nCompletion, Threshold Completion, and Trivially Perfect Completion. In these\nproblems, the task is to add at most k edges to a given graph in order to\nobtain a chordal, interval, proper interval, threshold, or trivially perfect\ngraph, respectively. We prove the following lower bounds for all these\nproblems, as well as for the related Chain Completion problem: Assuming the\nExponential Time Hypothesis, none of these problems can be solved in time\n2^O(n^(1/2) / log^c n) or 2^O(k^(1/4) / log^c k) n^O(1), for some integer c.\nAssuming the non-existence of a subexponential-time approximation scheme for\nMin Bisection on d-regular graphs, for some constant d, none of these problems\ncan be solved in time 2^o(n) or 2^o(sqrt(k)) n^O(1).\n  For all the aforementioned completion problems, apart from Proper Interval\nCompletion, FPT algorithms with running time of the form 2^O(sqrt(k) log k)\nn^O(1) are known. Thus, the second result proves that a significant improvement\nof any of these algorithms would lead to a surprising breakthrough in the\ndesign of approximation algorithms for Min Bisection.\n  To prove our results, we use a reduction methodology based on combining the\nclassic approach of starting with a sparse instance of 3-Sat, prepared using\nthe Sparsification Lemma, with the existence of almost linear-size\nProbabilistically Checkable Proofs (PCPs). Apart from our main results, we also\nobtain lower bounds excluding the existence of subexponential algorithms for\nthe Optimum Linear Arrangement problem, as well as improved, yet still not\ntight, lower bounds for Feedback Arc Set in Tournaments. \n\n"}
{"id": "1508.05573", "contents": "Title: A Dichotomy Theorem for Circular Colouring Reconfiguration Abstract: The \"reconfiguration problem\" for circular colourings asks, given two\n$(p,q)$-colourings $f$ and $g$ of a graph $G$, is it possible to transform $f$\ninto $g$ by changing the colour of one vertex at a time such that every\nintermediate mapping is a $(p,q)$-colouring? We show that this problem can be\nsolved in polynomial time for $2\\leq p/q <4$ and is PSPACE-complete for\n$p/q\\geq 4$. This generalizes a known dichotomy theorem for reconfiguring\nclassical graph colourings. \n\n"}
{"id": "1508.06420", "contents": "Title: The Stable Fixtures Problem with Payments Abstract: We generalize two well-known game-theoretic models by introducing multiple\npartners matching games, defined by a graph $G=(N,E)$, with an integer vertex\ncapacity function $b$ and an edge weighting $w$. The set $N$ consists of a\nnumber of players that are to form a set $M\\subseteq E$ of 2-player coalitions\n$ij$ with value $w(ij)$, such that each player $i$ is in at most $b(i)$\ncoalitions. A payoff vector is a mapping $p: N \\times N \\rightarrow {\\mathbb\nR}$ with $p(i,j)+p(j,i)=w(ij)$ if $ij\\in M$ and $p(i,j)=p(j,i)=0$ if $ij\\notin\nM$. The pair $(M,p)$ is called a solution. A pair of players $i,j$ with $ij\\in\nE\\setminus M$ blocks a solution $(M,p)$ if $i,j$ can form, possibly only after\nwithdrawing from one of their existing 2-player coalitions, a new 2-player\ncoalition in which they are mutually better off. A solution is stable if it has\nno blocking pairs.\n  We give a polynomial-time algorithm that either finds that a given multiple\npartners matching game has no stable solution, or obtains a stable solution for\nit. We characterize the set of stable solutions of a multiple partners matching\ngame in two different ways and show how this leads to simple proofs for a\nnumber of known results of Sotomayor (1992,1999,2007) for multiple partners\nssignment games and to generalizations of some of these results to multiple\npartners matching games. We also perform a study on the core of the\ncorresponding cooperative game, where coalitions of any size may be formed. In\nparticular we show that the standard relation between the existence of a stable\nsolution and the non-emptiness of the core, which holds in the other models\nwith payments, is no longer valid for our (most general) model. We also prove\nthat the problem of deciding if an allocation belongs to the core jumps from\nbeing polynomial-time solvable for $b\\leq 2$ to NP-complete for $b\\equiv 3$. \n\n"}
{"id": "1508.07338", "contents": "Title: A linear time algorithm for quantum 2-SAT Abstract: The Boolean constraint satisfaction problem 3-SAT is arguably the canonical\nNP-complete problem. In contrast, 2-SAT can not only be decided in polynomial\ntime, but in fact in deterministic linear time. In 2006, Bravyi proposed a\nphysically motivated generalization of k-SAT to the quantum setting, defining\nthe problem \"quantum k-SAT\". He showed that quantum 2-SAT is also solvable in\npolynomial time on a classical computer, in particular in deterministic time\nO(n^4), assuming unit-cost arithmetic over a field extension of the rational\nnumbers, where n is number of variables. In this paper, we present an algorithm\nfor quantum 2-SAT which runs in linear time, i.e. deterministic time O(n+m) for\nn and m the number of variables and clauses, respectively. Our approach\nexploits the transfer matrix techniques of Laumann et al. [QIC, 2010] used in\nthe study of phase transitions for random quantum 2-SAT, and bears similarities\nwith both the linear time 2-SAT algorithms of Even, Itai, and Shamir (based on\nbacktracking) [SICOMP, 1976] and Aspvall, Plass, and Tarjan (based on strongly\nconnected components) [IPL, 1979]. \n\n"}
{"id": "1508.07606", "contents": "Title: Helly's Theorem: New Variations and Applications Abstract: This survey presents recent Helly-type geometric theorems published since the\nappearance of the last comprehensive survey, more than ten years ago. We\ndiscuss how such theorems continue to be influential in computational geometry\nand in optimization. \n\n"}
{"id": "1509.00092", "contents": "Title: Explicit resilient functions matching Ajtai-Linial Abstract: A Boolean function on n variables is q-resilient if for any subset of at most\nq variables, the function is very likely to be determined by a uniformly random\nassignment to the remaining n-q variables; in other words, no coalition of at\nmost q variables has significant influence on the function. Resilient functions\nhave been extensively studied with a variety of applications in cryptography,\ndistributed computing, and pseudorandomness. The best known balanced resilient\nfunction on n variables due to Ajtai and Linial ([AL93]) is Omega(n/(log^2\nn))-resilient. However, the construction of Ajtai and Linial is by the\nprobabilistic method and does not give an efficiently computable function.\n  In this work we give an explicit monotone depth three almost-balanced Boolean\nfunction on n bits that is Omega(n/(log^2 n))-resilient matching the work of\nAjtai and Linial. The best previous explicit construction due to Meka [Meka09]\n(which only gives a logarithmic depth function) and Chattopadhyay and\nZuckermman [CZ15] were only n^{1-c}-resilient for any constant c < 1. Our\nconstruction and analysis are motivated by (and simplifies parts of) the recent\nbreakthrough of [CZ15] giving explicit two-sources extractors for\npolylogarithmic min-entropy; a key ingredient in their result was the\nconstruction of explicit constant-depth resilient functions.\n  An important ingredient in our construction is a new randomness optimal\noblivious sampler which preserves moment generating functions of sums of\nvariables and could be useful elsewhere. \n\n"}
{"id": "1509.00824", "contents": "Title: A note on Probably Certifiably Correct algorithms Abstract: Many optimization problems of interest are known to be intractable, and while\nthere are often heuristics that are known to work on typical instances, it is\nusually not easy to determine a posteriori whether the optimal solution was\nfound. In this short note, we discuss algorithms that not only solve the\nproblem on typical instances, but also provide a posteriori certificates of\noptimality, probably certifiably correct (PCC) algorithms. As an illustrative\nexample, we present a fast PCC algorithm for minimum bisection under the\nstochastic block model and briefly discuss other examples. \n\n"}
{"id": "1509.01844", "contents": "Title: Sparsification of Two-Variable Valued CSPs Abstract: A valued constraint satisfaction problem (VCSP) instance $(V,\\Pi,w)$ is a set\nof variables $V$ with a set of constraints $\\Pi$ weighted by $w$. Given a VCSP\ninstance, we are interested in a re-weighted sub-instance $(V,\\Pi'\\subset\n\\Pi,w')$ such that preserves the value of the given instance (under every\nassignment to the variables) within factor $1\\pm\\epsilon$. A well-studied\nspecial case is cut sparsification in graphs, which has found various\napplications.\n  We show that a VCSP instance consisting of a single boolean predicate\n$P(x,y)$ (e.g., for cut, $P=\\mbox{XOR}$) can be sparsified into\n$O(|V|/\\epsilon^2)$ constraints if and only if the number of inputs that\nsatisfy $P$ is anything but one (i.e., $|P^{-1}(1)| \\neq 1$). Furthermore, this\nsparsity bound is tight unless $P$ is a relatively trivial predicate. We\nconclude that also systems of 2SAT (or 2LIN) constraints can be sparsified. \n\n"}
{"id": "1509.05896", "contents": "Title: On space efficiency of algorithms working on structural decompositions\n  of graphs Abstract: Dynamic programming on path and tree decompositions of graphs is a technique\nthat is ubiquitous in the field of parameterized and exponential-time\nalgorithms. However, one of its drawbacks is that the space usage is\nexponential in the decomposition's width. Following the work of Allender et al.\n[Theory of Computing, '14], we investigate whether this space complexity\nexplosion is unavoidable. Using the idea of reparameterization of Cai and\nJuedes [J. Comput. Syst. Sci., '03], we prove that the question is closely\nrelated to a conjecture that the Longest Common Subsequence problem\nparameterized by the number of input strings does not admit an algorithm that\nsimultaneously uses XP time and FPT space. Moreover, we complete the complexity\nlandscape sketched for pathwidth and treewidth by Allender et al. by\nconsidering the parameter tree-depth. We prove that computations on tree-depth\ndecompositions correspond to a model of non-deterministic machines that work in\npolynomial time and logarithmic space, with access to an auxiliary stack of\nmaximum height equal to the decomposition's depth. Together with the results of\nAllender et al., this describes a hierarchy of complexity classes for\npolynomial-time non-deterministic machines with different restrictions on the\naccess to working space, which mirrors the classic relations between treewidth,\npathwidth, and tree-depth. \n\n"}
{"id": "1509.07466", "contents": "Title: Anchored parallel repetition for nonlocal games Abstract: We introduce a simple transformation on two-player nonlocal games, called\n\"anchoring\", and prove an exponential-decay parallel repetition theorem for all\nanchored games in the setting of quantum entangled players. This transformation\nis inspired in part by the Feige-Kilian transformation (SICOMP 2000), and has\nthe property that if the quantum value of the original game $G$ is $v$ then the\nquantum value of the anchored game $G_\\bot$ is $1 - (1 - \\alpha)^2 \\cdot (1 -\nv)$ where $\\alpha$ is a parameter of the transformation. In particular the\nanchored game has quantum value $1$ if and only if the original game $G$ has\nquantum value $1$. This provides the first gap amplification technique for\ngeneral two-player nonlocal games that achieves exponential decay of the\nquantum value. \n\n"}
{"id": "1510.00598", "contents": "Title: On independent set on B1-EPG graphs Abstract: In this paper we consider the Maximum Independent Set problem (MIS) on\n$B_1$-EPG graphs. EPG (for Edge intersection graphs of Paths on a Grid) was\nintroduced in ~\\cite{edgeintersinglebend} as the class of graphs whose vertices\ncan be represented as simple paths on a rectangular grid so that two vertices\nare adjacent if and only if the corresponding paths share at least one edge of\nthe underlying grid. The restricted class $B_k$-EPG denotes EPG-graphs where\nevery path has at most $k$ bends. The study of MIS on $B_1$-EPG graphs has been\ninitiated in~\\cite{wadsMIS} where authors prove that MIS is NP-complete on\n$B_1$-EPG graphs, and provide a polynomial $4$-approximation. In this article\nwe study the approximability and the fixed parameter tractability of MIS on\n$B_1$-EPG. We show that there is no PTAS for MIS on $B_1$-EPG unless P$=$NP,\neven if there is only one shape of path, and even if each path has its vertical\npart or its horizontal part of length at most $3$. This is optimal, as we show\nthat if all paths have their horizontal part bounded by a constant, then MIS\nadmits a PTAS. Finally, we show that MIS is FPT in the standard\nparameterization on $B_1$-EPG restricted to only three shapes of path, and\n$W_1$-hard on $B_2$-EPG. The status for general $B_1$-EPG (with the four\nshapes) is left open. \n\n"}
{"id": "1510.01891", "contents": "Title: On the Hardest Problem Formulations for the 0/1 Lasserre Hierarchy Abstract: The Lasserre/Sum-of-Squares (SoS) hierarchy is a systematic procedure for\nconstructing a sequence of increasingly tight semidefinite relaxations. It is\nknown that the hierarchy converges to the 0/1 polytope in n levels and captures\nthe convex relaxations used in the best available approximation algorithms for\na wide variety of optimization problems.\n  In this paper we characterize the set of 0/1 integer linear problems and\nunconstrained 0/1 polynomial optimization problems that can still have an\nintegrality gap at level n-1. These problems are the hardest for the Lasserre\nhierarchy in this sense. \n\n"}
{"id": "1510.02195", "contents": "Title: Structure and automorphisms of primitive coherent configurations Abstract: Coherent configurations (CCs) are highly regular colorings of the set of\nordered pairs of a \"vertex set\"; each color represents a \"constituent digraph.\"\nCCs arise in the study of permutation groups, combinatorial structures such as\npartially balanced designs, and the analysis of algorithms; their history goes\nback to Schur in the 1930s. A CC is primitive (PCC) if all its constituent\ndigraphs are connected.\n  We address the problem of classifying PCCs with large automorphism groups.\nThis project was started in Babai's 1981 paper in which he showed that only the\ntrivial PCC admits more than $\\exp(\\tilde{O}(n^{1/2}))$ automorphisms. (Here,\n$n$ is the number of vertices and the $\\tilde{O}$ hides polylogarithmic\nfactors.)\n  In the present paper we classify all PCCs with more than\n$\\exp(\\tilde{O}(n^{1/3}))$ automorphisms, making the first progress on Babai's\nconjectured classification of all PCCs with more than $\\exp(n^{\\epsilon})$\nautomorphisms.\n  A corollary to Babai's 1981 result solved a then 100-year-old problem on\nprimitive but not doubly transitive permutation groups, giving an\n$\\exp(\\tilde{O}(n^{1/2}))$ bound on their order. In a similar vein, our result\nimplies an $\\exp(\\tilde{O}(n^{1/3}))$ upper bound on the order of such groups,\nwith known exceptions. This improvement of Babai's result was previously known\nonly through the Classification of Finite Simple Groups (Cameron, 1981), while\nour proof, like Babai's, is elementary and almost purely combinatorial.\n  Our analysis relies on a new combinatorial structure theory we develop for\nPCCs. In particular, we demonstrate the presence of \"asymptotically uniform\nclique geometries\" on PCCs in a certain range of the parameters. \n\n"}
{"id": "1510.02824", "contents": "Title: On the Complexity of Inner Product Similarity Join Abstract: A number of tasks in classification, information retrieval, recommendation\nsystems, and record linkage reduce to the core problem of inner product\nsimilarity join (IPS join): identifying pairs of vectors in a collection that\nhave a sufficiently large inner product. IPS join is well understood when\nvectors are normalized and some approximation of inner products is allowed.\nHowever, the general case where vectors may have any length appears much more\nchallenging. Recently, new upper bounds based on asymmetric locality-sensitive\nhashing (ALSH) and asymmetric embeddings have emerged, but little has been\nknown on the lower bound side. In this paper we initiate a systematic study of\ninner product similarity join, showing new lower and upper bounds. Our main\nresults are:\n  * Approximation hardness of IPS join in subquadratic time, assuming the\nstrong exponential time hypothesis.\n  * New upper and lower bounds for (A)LSH-based algorithms. In particular, we\nshow that asymmetry can be avoided by relaxing the LSH definition to only\nconsider the collision probability of distinct elements.\n  * A new indexing method for IPS based on linear sketches, implying that our\nhardness results are not far from being tight.\n  Our technical contributions include new asymmetric embeddings that may be of\nindependent interest. At the conceptual level we strive to provide greater\nclarity, for example by distinguishing among signed and unsigned variants of\nIPS join and shedding new light on the effect of asymmetry. \n\n"}
{"id": "1510.04991", "contents": "Title: Honest signaling in zero-sum games is hard, and lying is even harder Abstract: We prove that, assuming the exponential time hypothesis, finding an\n\\epsilon-approximately optimal symmetric signaling scheme in a two-player\nzero-sum game requires quasi-polynomial time. This is tight by [Cheng et al.,\nFOCS'15] and resolves an open question of [Dughmi, FOCS'14]. We also prove that\nfinding a multiplicative approximation is NP-hard.\n  We also introduce a new model where a dishonest signaler may publicly commit\nto use one scheme, but post signals according to a different scheme. For this\nmodel, we prove that even finding a (1-2^{-n})-approximately optimal scheme is\nNP-hard. \n\n"}
{"id": "1510.05433", "contents": "Title: Fast Parallel Operations on Search Trees Abstract: Using (a,b)-trees as an example, we show how to perform a parallel split with\nlogarithmic latency and parallel join, bulk updates, intersection, union (or\nmerge), and (symmetric) set difference with logarithmic latency and with\ninformation theoretically optimal work. We present both asymptotically optimal\nsolutions and simplified versions that perform well in practice - they are\nseveral times faster than previous implementations. \n\n"}
{"id": "1510.06128", "contents": "Title: Products of Independent Gaussian Random Matrices Abstract: This thesis reviews recent progress on products of random matrices from the\nperspective of exactly solved Gaussian random matrix models. We derive exact\nformulae for the correlation functions for the eigen- and singular values at\narbitrary matrix dimension and for an arbitrary number of factors. These exact\nresults are used to study asymptotic limits for the macroscopic densities and\nthe microscopic correlations as either the matrix dimension or the number of\nfactors tends to infinity. \n\n"}
{"id": "1510.07185", "contents": "Title: Memory-Adjustable Navigation Piles with Applications to Sorting and\n  Convex Hulls Abstract: We consider space-bounded computations on a random-access machine (RAM) where\nthe input is given on a read-only random-access medium, the output is to be\nproduced to a write-only sequential-access medium, and the available workspace\nallows random reads and writes but is of limited capacity. The length of the\ninput is $N$ elements, the length of the output is limited by the computation,\nand the capacity of the workspace is $O(S)$ bits for some predetermined\nparameter $S$. We present a state-of-the-art priority queue---called an\nadjustable navigation pile---for this restricted RAM model. Under some\nreasonable assumptions, our priority queue supports $\\mathit{minimum}$ and\n$\\mathit{insert}$ in $O(1)$ worst-case time and $\\mathit{extract}$ in $O(N/S +\n\\lg{} S)$ worst-case time for any $S \\geq \\lg{} N$. We show how to use this\ndata structure to sort $N$ elements and to compute the convex hull of $N$\npoints in the two-dimensional Euclidean space in $O(N^2/S + N \\lg{} S)$\nworst-case time for any $S \\geq \\lg{} N$. Following a known lower bound for the\nspace-time product of any branching program for finding unique elements, both\nour sorting and convex-hull algorithms are optimal. The adjustable navigation\npile has turned out to be useful when designing other space-efficient\nalgorithms, and we expect that it will find its way to yet other applications. \n\n"}
{"id": "1510.07825", "contents": "Title: Span-program-based quantum algorithms for graph bipartiteness and\n  connectivity Abstract: Span program is a linear-algebraic model of computation which can be used to\ndesign quantum algorithms. For any Boolean function there exists a span program\nthat leads to a quantum algorithm with optimal quantum query complexity. In\ngeneral, finding such span programs is not an easy task. In this work, given a\nquery access to the adjacency matrix of a simple graph $G$ with $n$ vertices,\nwe provide two new span-program-based quantum algorithms: an algorithm for\ntesting if the graph is bipartite that uses $O(n\\sqrt{n})$ quantum queries; an\nalgorithm for testing if the graph is connected that uses $O(n\\sqrt{n})$\nquantum queries. \n\n"}
{"id": "1511.00778", "contents": "Title: Basis Collapse for Holographic Algorithms Over All Domain Sizes Abstract: The theory of holographic algorithms introduced by Valiant represents a novel\napproach to achieving polynomial-time algorithms for seemingly intractable\ncounting problems via a reduction to counting planar perfect matchings and a\nlinear change of basis. Two fundamental parameters in holographic algorithms\nare the \\emph{domain size} and the \\emph{basis size}. Roughly, the domain size\nis the range of colors involved in the counting problem at hand (e.g. counting\ngraph $k$-colorings is a problem over domain size $k$), while the basis size\n$\\ell$ captures the dimensionality of the representation of those colors. A\nmajor open problem has been: for a given $k$, what is the smallest $\\ell$ for\nwhich any holographic algorithm for a problem over domain size $k$ \"collapses\nto\" (can be simulated by) a holographic algorithm with basis size $\\ell$? Cai\nand Lu showed in 2008 that over domain size 2, basis size 1 suffices, opening\nthe door to an extensive line of work on the structural theory of holographic\nalgorithms over the Boolean domain. Cai and Fu later showed for signatures of\nfull rank that over domain sizes 3 and 4, basis sizes 1 and 2, respectively,\nsuffice, and they conjectured that over domain size $k$ there is a collapse to\nbasis size $\\lfloor\\log_2 k\\rfloor$. In this work, we resolve this conjecture\nin the affirmative for signatures of full rank for all $k$. \n\n"}
{"id": "1511.01699", "contents": "Title: Low Rank Approximation of Binary Matrices: Column Subset Selection and\n  Generalizations Abstract: Low rank matrix approximation is an important tool in machine learning. Given\na data matrix, low rank approximation helps to find factors, patterns and\nprovides concise representations for the data. Research on low rank\napproximation usually focus on real matrices. However, in many applications\ndata are binary (categorical) rather than continuous. This leads to the problem\nof low rank approximation of binary matrix. Here we are given a $d \\times n$\nbinary matrix $A$ and a small integer $k$. The goal is to find two binary\nmatrices $U$ and $V$ of sizes $d \\times k$ and $k \\times n$ respectively, so\nthat the Frobenius norm of $A - U V$ is minimized. There are two models of this\nproblem, depending on the definition of the dot product of binary vectors: The\n$\\mathrm{GF}(2)$ model and the Boolean semiring model. Unlike low rank\napproximation of real matrix which can be efficiently solved by Singular Value\nDecomposition, approximation of binary matrix is $NP$-hard even for $k=1$.\n  In this paper, we consider the problem of Column Subset Selection (CSS), in\nwhich one low rank matrix must be formed by $k$ columns of the data matrix. We\ncharacterize the approximation ratio of CSS for binary matrices. For $GF(2)$\nmodel, we show the approximation ratio of CSS is bounded by\n$\\frac{k}{2}+1+\\frac{k}{2(2^k-1)}$ and this bound is asymptotically tight. For\nBoolean model, it turns out that CSS is no longer sufficient to obtain a bound.\nWe then develop a Generalized CSS (GCSS) procedure in which the columns of one\nlow rank matrix are generated from Boolean formulas operating bitwise on\ncolumns of the data matrix. We show the approximation ratio of GCSS is bounded\nby $2^{k-1}+1$, and the exponential dependency on $k$ is inherent. \n\n"}
{"id": "1511.01770", "contents": "Title: Pattern matching in $(213,231)$-avoiding permutations Abstract: Given permutations $\\sigma \\in S_k$ and $\\pi \\in S_n$ with $k<n$, the\n\\emph{pattern matching} problem is to decide whether $\\pi$ matches $\\sigma$ as\nan order-isomorphic subsequence. We give a linear-time algorithm in case both\n$\\pi$ and $\\sigma$ avoid the two size-$3$ permutations $213$ and $231$. For the\nspecial case where only $\\sigma$ avoids $213$ and $231$, we present a\n$O(max(kn^2,n^2\\log(\\log(n)))$ time algorithm. We extend our research to\nbivincular patterns that avoid $213$ and $231$ and present a $O(kn^4)$ time\nalgorithm. Finally we look at the related problem of the longest subsequence\nwhich avoids $213$ and $231$. \n\n"}
{"id": "1511.02235", "contents": "Title: NAND-Trees, Average Choice Complexity, and Effective Resistance Abstract: We show that the quantum query complexity of evaluating NAND-tree instances\nwith average choice complexity at most $W$ is $O(W)$, where average choice\ncomplexity is a measure of the difficulty of winning the associated two-player\ngame. This generalizes a superpolynomial speedup over classical query\ncomplexity due to Zhan et al. [Zhan et al., ITCS 2012, 249-265]. We further\nshow that the player with a winning strategy for the two-player game associated\nwith the NAND-tree can win the game with an expected\n$\\widetilde{O}(N^{1/4}\\sqrt{{\\cal C}(x)})$ quantum queries against a random\nopponent, where ${\\cal C }(x)$ is the average choice complexity of the\ninstance. This gives an improvement over the query complexity of the naive\nstrategy, which costs $\\widetilde{O}(\\sqrt{N})$ queries.\n  The results rely on a connection between NAND-tree evaluation and\n$st$-connectivity problems on certain graphs, and span programs for\n$st$-connectivity problems. Our results follow from relating average choice\ncomplexity to the effective resistance of these graphs, which itself\ncorresponds to the span program witness size. \n\n"}
{"id": "1511.03700", "contents": "Title: The Volume of the Trace-Nonnegative Polytope via the Irwin-Hall\n  Distribution Abstract: In this work, we find an explicit expression for the volume of the trace\nnonnegative polytope via a generalization of the Irwin-Hall distribution. This\nvolume is an upper bound for the volume of all projected, normalized realizable\nspectra. We provide ancillary results on realizable trace-zero spectra and pose\nseveral problems suitable for further inquiry. \n\n"}
{"id": "1511.04066", "contents": "Title: Properly Learning Poisson Binomial Distributions in Almost Polynomial\n  Time Abstract: We give an algorithm for properly learning Poisson binomial distributions. A\nPoisson binomial distribution (PBD) of order $n$ is the discrete probability\ndistribution of the sum of $n$ mutually independent Bernoulli random variables.\nGiven $\\widetilde{O}(1/\\epsilon^2)$ samples from an unknown PBD $\\mathbf{p}$,\nour algorithm runs in time $(1/\\epsilon)^{O(\\log \\log (1/\\epsilon))}$, and\noutputs a hypothesis PBD that is $\\epsilon$-close to $\\mathbf{p}$ in total\nvariation distance. The previously best known running time for properly\nlearning PBDs was $(1/\\epsilon)^{O(\\log(1/\\epsilon))}$.\n  As one of our main contributions, we provide a novel structural\ncharacterization of PBDs. We prove that, for all $\\epsilon >0,$ there exists an\nexplicit collection $\\cal{M}$ of $(1/\\epsilon)^{O(\\log \\log (1/\\epsilon))}$\nvectors of multiplicities, such that for any PBD $\\mathbf{p}$ there exists a\nPBD $\\mathbf{q}$ with $O(\\log(1/\\epsilon))$ distinct parameters whose\nmultiplicities are given by some element of ${\\cal M}$, such that $\\mathbf{q}$\nis $\\epsilon$-close to $\\mathbf{p}$. Our proof combines tools from Fourier\nanalysis and algebraic geometry.\n  Our approach to the proper learning problem is as follows: Starting with an\naccurate non-proper hypothesis, we fit a PBD to this hypothesis. More\nspecifically, we essentially start with the hypothesis computed by the\ncomputationally efficient non-proper learning algorithm in our recent\nwork~\\cite{DKS15}. Our aforementioned structural characterization allows us to\nreduce the corresponding fitting problem to a collection of\n$(1/\\epsilon)^{O(\\log \\log(1/\\epsilon))}$ systems of low-degree polynomial\ninequalities. We show that each such system can be solved in time\n$(1/\\epsilon)^{O(\\log \\log(1/\\epsilon))}$, which yields the overall running\ntime of our algorithm. \n\n"}
{"id": "1511.04731", "contents": "Title: Hardness of RNA Folding Problem with Four Symbols Abstract: An RNA sequence is a string composed of four types of nucleotides, $A, C, G$,\nand $U$. The goal of the RNA folding problem is to find a maximum cardinality\nset of crossing-free pairs of the form $\\{A,U\\}$ or $\\{C,G\\}$ in a given RNA\nsequence. The problem is central in bioinformatics and has received much\nattention over the years. Abboud, Backurs, and Williams (FOCS 2015)\ndemonstrated a conditional lower bound for a generalized version of the RNA\nfolding problem based on a conjectured hardness of the $k$-clique problem.\nTheir lower bound requires the RNA sequence to have at least 36 types of\nsymbols, making the result not applicable to the RNA folding problem in real\nlife (i.e., alphabet size 4). In this paper, we present an improved lower bound\nthat works for the alphabet size 4 case.\n  We also investigate the Dyck edit distance problem, which is a string problem\nclosely related to RNA folding. We demonstrate a reduction from RNA folding to\nDyck edit distance with alphabet size 10. This leads to a much simpler proof of\nthe conditional lower bound for Dyck edit distance problem given by Abboud,\nBackurs, and Williams (FOCS 2015), and lowers the alphabet size requirement. \n\n"}
{"id": "1511.05053", "contents": "Title: A Polynomial Lower Bound for Testing Monotonicity Abstract: We show that every algorithm for testing $n$-variate Boolean functions for\nmonotonicity must have query complexity $\\tilde{\\Omega}(n^{1/4})$. All previous\nlower bounds for this problem were designed for non-adaptive algorithms and, as\na result, the best previous lower bound for general (possibly adaptive)\nmonotonicity testers was only $\\Omega(\\log n)$. Combined with the query\ncomplexity of the non-adaptive monotonicity tester of Khot, Minzer, and Safra\n(FOCS 2015), our lower bound shows that adaptivity can result in at most a\nquadratic reduction in the query complexity for testing monotonicity.\n  By contrast, we show that there is an exponential gap between the query\ncomplexity of adaptive and non-adaptive algorithms for testing regular linear\nthreshold functions (LTFs) for monotonicity. Chen, De, Servedio, and Tan (STOC\n2015) recently showed that non-adaptive algorithms require almost\n$\\Omega(n^{1/2})$ queries for this task. We introduce a new adaptive\nmonotonicity testing algorithm which has query complexity $O(\\log n)$ when the\ninput is a regular LTF. \n\n"}
{"id": "1511.05514", "contents": "Title: Better $s$-$t$-Tours by Gao Trees Abstract: We consider the $s$-$t$-path TSP: given a finite metric space with two\nelements $s$ and $t$, we look for a path from $s$ to $t$ that contains all the\nelements and has minimum total distance. We improve the approximation ratio for\nthis problem from 1.599 to 1.566. Like previous algorithms, we solve the\nnatural LP relaxation and represent an optimum solution $x^*$ as a convex\ncombination of spanning trees. Gao showed that there exists a spanning tree in\nthe support of $x^*$ that has only one edge in each narrow cut (i.e., each cut\n$C$ with $x^*(C)<2$). Our main theorem says that the spanning trees in the\nconvex combination can be chosen such that many of them are such \"Gao trees''. \n\n"}
{"id": "1511.05546", "contents": "Title: Complexity and Approximability of Parameterized MAX-CSPs Abstract: We study the optimization version of constraint satisfaction problems\n(Max-CSPs) in the framework of parameterized complexity; the goal is to compute\nthe maximum fraction of constraints that can be satisfied simultaneously. In\nstandard CSPs, we want to decide whether this fraction equals one. The\nparameters we investigate are structural measures, such as the treewidth or the\nclique-width of the variable-constraint incidence graph of the CSP instance.\n  We consider Max-CSPs with the constraint types AND, OR, PARITY, and MAJORITY,\nand with various parameters k, and we attempt to fully classify them into the\nfollowing three cases: 1. The exact optimum can be computed in FPT time. 2. It\nis W[1]-hard to compute the exact optimum, but there is a randomized FPT\napproximation scheme (FPTAS), which computes a $(1-\\epsilon)$-approximation in\ntime $f(k,\\epsilon)\\cdot poly(n)$. 3. There is no FPTAS unless FPT=W[1].\n  For the corresponding standard CSPs, we establish FPT vs. W[1]-hardness\nresults. \n\n"}
{"id": "1511.06037", "contents": "Title: Enumeration and Random Generation of Unlabeled Classes of Graphs: A\n  Practical Study of Cycle Pointing and the Dissymmetry Theorem Abstract: Our work studies the enumeration and random generation of unlabeled\ncombinatorial classes of unrooted graphs. While the technique of vertex\npointing provides a straightforward procedure for analyzing a labeled class of\nunrooted graphs by first studying its rooted counterpart, the existence of\nnontrivial symmetries in the unlabeled case causes this technique to break\ndown. Instead, techniques such as the dissymmetry theorem (of Otter) and cycle\npointing (of Bodirsky et al.) have emerged in the unlabeled case, with the\nformer providing an enumeration of the class and the latter providing both an\nenumeration and an unbiased sampler. In this work, we extend the power of the\ndissymmetry theorem by showing that it in fact provides a Boltzmann sampler for\nthe class in question. We then present an exposition of the cycle pointing\ntechnique, with a focus on the enumeration and random generation of the\nunderlying unpointed class. Finally, we apply cycle pointing to enumerate and\nimplement samplers for the classes of distance-hereditary graphs and three-leaf\npower graphs. \n\n"}
{"id": "1511.06558", "contents": "Title: Near-Optimal UGC-hardness of Approximating Max k-CSP_R Abstract: In this paper, we prove an almost-optimal hardness for Max $k$-CSP$_R$ based\non Khot's Unique Games Conjecture (UGC). In Max $k$-CSP$_R$, we are given a set\nof predicates each of which depends on exactly $k$ variables. Each variable can\ntake any value from $1, 2, \\dots, R$. The goal is to find an assignment to\nvariables that maximizes the number of satisfied predicates.\n  Assuming the Unique Games Conjecture, we show that it is NP-hard to\napproximate Max $k$-CSP$_R$ to within factor $2^{O(k \\log k)}(\\log\nR)^{k/2}/R^{k - 1}$ for any $k, R$. To the best of our knowledge, this result\nimproves on all the known hardness of approximation results when $3 \\leq k =\no(\\log R/\\log \\log R)$. In this case, the previous best hardness result was\nNP-hardness of approximating within a factor $O(k/R^{k-2})$ by Chan. When $k =\n2$, our result matches the best known UGC-hardness result of Khot, Kindler,\nMossel and O'Donnell.\n  In addition, by extending an algorithm for Max 2-CSP$_R$ by Kindler, Kolla\nand Trevisan, we provide an $\\Omega(\\log R/R^{k - 1})$-approximation algorithm\nfor Max $k$-CSP$_R$. This algorithm implies that our inapproximability result\nis tight up to a factor of $2^{O(k \\log k)}(\\log R)^{k/2 - 1}$. In comparison,\nwhen $3 \\leq k$ is a constant, the previously known gap was $O(R)$, which is\nsignificantly larger than our gap of $O(\\text{polylog } R)$.\n  Finally, we show that we can replace the Unique Games Conjecture assumption\nwith Khot's $d$-to-1 Conjecture and still get asymptotically the same hardness\nof approximation. \n\n"}
{"id": "1511.07070", "contents": "Title: Which Regular Expression Patterns are Hard to Match? Abstract: Regular expressions constitute a fundamental notion in formal language theory\nand are frequently used in computer science to define search patterns. A\nclassic algorithm for these problems constructs and simulates a\nnon-deterministic finite automaton corresponding to the expression, resulting\nin an $O(mn)$ running time (where $m$ is the length of the pattern and $n$ is\nthe length of the text). This running time can be improved slightly (by a\npolylogarithmic factor), but no significantly faster solutions are known. At\nthe same time, much faster algorithms exist for various special cases of\nregular expressions, including dictionary matching, wildcard matching, subset\nmatching, word break problem etc.\n  In this paper, we show that the complexity of regular expression matching can\nbe characterized based on its {\\em depth} (when interpreted as a formula). Our\nresults hold for expressions involving concatenation, OR, Kleene star and\nKleene plus. For regular expressions of depth two (involving any combination of\nthe above operators), we show the following dichotomy: matching and membership\ntesting can be solved in near-linear time, except for \"concatenations of\nstars\", which cannot be solved in strongly sub-quadratic time assuming the\nStrong Exponential Time Hypothesis (SETH). For regular expressions of depth\nthree the picture is more complex. Nevertheless, we show that all problems can\neither be solved in strongly sub-quadratic time, or cannot be solved in\nstrongly sub-quadratic time assuming SETH.\n  An intriguing special case of membership testing involves regular expressions\nof the form \"a star of an OR of concatenations\", e.g., $[a|ab|bc]^*$. This\ncorresponds to the so-called {\\em word break} problem, for which a dynamic\nprogramming algorithm with a runtime of (roughly) $O(n\\sqrt{m})$ is known. We\nshow that the latter bound is not tight and improve the runtime to\n$O(nm^{0.44\\ldots})$. \n\n"}
{"id": "1511.07527", "contents": "Title: Tradeoffs for nearest neighbors on the sphere Abstract: We consider tradeoffs between the query and update complexities for the\n(approximate) nearest neighbor problem on the sphere, extending the recent\nspherical filters to sparse regimes and generalizing the scheme and analysis to\naccount for different tradeoffs. In a nutshell, for the sparse regime the\ntradeoff between the query complexity $n^{\\rho_q}$ and update complexity\n$n^{\\rho_u}$ for data sets of size $n$ is given by the following equation in\nterms of the approximation factor $c$ and the exponents $\\rho_q$ and $\\rho_u$:\n$$c^2\\sqrt{\\rho_q}+(c^2-1)\\sqrt{\\rho_u}=\\sqrt{2c^2-1}.$$\n  For small $c=1+\\epsilon$, minimizing the time for updates leads to a linear\nspace complexity at the cost of a query time complexity $n^{1-4\\epsilon^2}$.\nBalancing the query and update costs leads to optimal complexities\n$n^{1/(2c^2-1)}$, matching bounds from [Andoni-Razenshteyn, 2015] and [Dubiner,\nIEEE-TIT'10] and matching the asymptotic complexities of [Andoni-Razenshteyn,\nSTOC'15] and [Andoni-Indyk-Laarhoven-Razenshteyn-Schmidt, NIPS'15]. A\nsubpolynomial query time complexity $n^{o(1)}$ can be achieved at the cost of a\nspace complexity of the order $n^{1/(4\\epsilon^2)}$, matching the bound\n$n^{\\Omega(1/\\epsilon^2)}$ of [Andoni-Indyk-Patrascu, FOCS'06] and\n[Panigrahy-Talwar-Wieder, FOCS'10] and improving upon results of\n[Indyk-Motwani, STOC'98] and [Kushilevitz-Ostrovsky-Rabani, STOC'98].\n  For large $c$, minimizing the update complexity results in a query complexity\nof $n^{2/c^2+O(1/c^4)}$, improving upon the related exponent for large $c$ of\n[Kapralov, PODS'15] by a factor $2$, and matching the bound $n^{\\Omega(1/c^2)}$\nof [Panigrahy-Talwar-Wieder, FOCS'08]. Balancing the costs leads to optimal\ncomplexities $n^{1/(2c^2-1)}$, while a minimum query time complexity can be\nachieved with update complexity $n^{2/c^2+O(1/c^4)}$, improving upon the\nprevious best exponents of Kapralov by a factor $2$. \n\n"}
{"id": "1511.07605", "contents": "Title: On the Computational Complexity of Limit Cycles in Dynamical Systems Abstract: We study the Poincare-Bendixson theorem for two-dimensional continuous\ndynamical systems in compact domains from the point of view of computation,\nseeking algorithms for finding the limit cycle promised by this classical\nresult. We start by considering a discrete analogue of this theorem and show\nthat both finding a point on a limit cycle, and determining if a given point is\non one, are PSPACE-complete.\n  For the continuous version, we show that both problems are uncomputable in\nthe real complexity sense; i.e., their complexity is arbitrarily high.\nSubsequently, we introduce a notion of an \"approximate cycle\" and prove an\n\"approximate\" Poincar\\'e-Bendixson theorem guaranteeing that some orbits come\nvery close to forming a cycle in the absence of approximate fixpoints;\nsurprisingly, it holds for all dimensions. The corresponding computational\nproblem defined in terms of arithmetic circuits is PSPACE-complete. \n\n"}
{"id": "1512.01968", "contents": "Title: Partition bound is quadratically tight for product distributions Abstract: Let $f : \\{0,1\\}^n \\times \\{0,1\\}^n \\rightarrow \\{0,1\\}$ be a 2-party\nfunction. For every product distribution $\\mu$ on $\\{0,1\\}^n \\times \\{0,1\\}^n$,\nwe show that $$\\mathsf{CC}^\\mu_{0.49}(f) = O\\left(\\left(\\log\n\\mathsf{prt}_{1/8}(f) \\cdot \\log \\log \\mathsf{prt}_{1/8}(f)\\right)^2\\right),$$\nwhere $\\mathsf{CC}^\\mu_\\varepsilon(f)$ is the distributional communication\ncomplexity of $f$ with error at most $\\varepsilon$ under the distribution $\\mu$\nand $\\mathsf{prt}_{1/8}(f)$ is the {\\em partition bound} of $f$, as defined by\nJain and Klauck [{\\em Proc. 25th CCC}, 2010]. We also prove a similar bound in\nterms of $\\mathsf{IC}_{1/8}(f)$, the {\\em information complexity} of $f$,\nnamely, $$\\mathsf{CC}^\\mu_{0.49}(f) = O\\left(\\left(\\mathsf{IC}_{1/8}(f) \\cdot\n\\log \\mathsf{IC}_{1/8}(f)\\right)^2\\right).$$ The latter bound was recently and\nindependently established by Kol [{\\em Proc. 48th STOC}, 2016] using a\ndifferent technique.\n  We show a similar result for query complexity under product distributions.\nLet $g : \\{0,1\\}^n \\rightarrow \\{0,1\\}$ be a function. For every bit-wise\nproduct distribution $\\mu$ on $\\{0,1\\}^n$, we show that\n$$\\mathsf{QC}^\\mu_{0.49}(g) = O\\left(\\left( \\log \\mathsf{qprt}_{1/8}(g) \\cdot\n\\log \\log\\mathsf{qprt}_{1/8}(g) \\right)^2 \\right),$$ where\n$\\mathsf{QC}^\\mu_{\\varepsilon}(g)$ is the distributional query complexity of\n$f$ with error at most $\\varepsilon$ under the distribution $\\mu$ and\n$\\mathsf{qprt}_{1/8}(g))$ is the {\\em query partition bound} of the function\n$g$.\n  Partition bounds were introduced (in both communication complexity and query\ncomplexity models) to provide LP-based lower bounds for randomized\ncommunication complexity and randomized query complexity. Our results\ndemonstrate that these lower bounds are polynomially tight for {\\em product}\ndistributions. \n\n"}
{"id": "1512.02068", "contents": "Title: Minimum Cut of Directed Planar Graphs in O(nloglogn) Time Abstract: We give an $O(n \\log \\log n)$ time algorithm for computing the minimum cut\n(or equivalently, the shortest cycle) of a weighted directed planar graph. This\nimproves the previous fastest $O(n\\log^3 n)$ solution. Interestingly, while in\nundirected planar graphs both min-cut and min $st$-cut have $O(n \\log \\log n)$\nsolutions, in directed planar graphs our result makes min-cut faster than min\n$st$-cut, which currently requires $O(n \\log n)$. \n\n"}
{"id": "1512.02337", "contents": "Title: Fast spectral algorithms from sum-of-squares proofs: tensor\n  decomposition and planted sparse vectors Abstract: We consider two problems that arise in machine learning applications: the\nproblem of recovering a planted sparse vector in a random linear subspace and\nthe problem of decomposing a random low-rank overcomplete 3-tensor. For both\nproblems, the best known guarantees are based on the sum-of-squares method. We\ndevelop new algorithms inspired by analyses of the sum-of-squares method. Our\nalgorithms achieve the same or similar guarantees as sum-of-squares for these\nproblems but the running time is significantly faster.\n  For the planted sparse vector problem, we give an algorithm with running time\nnearly linear in the input size that approximately recovers a planted sparse\nvector with up to constant relative sparsity in a random subspace of $\\mathbb\nR^n$ of dimension up to $\\tilde \\Omega(\\sqrt n)$. These recovery guarantees\nmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to\nlogarithmic factors.\n  For tensor decomposition, we give an algorithm with running time close to\nlinear in the input size (with exponent $\\approx 1.086$) that approximately\nrecovers a component of a random 3-tensor over $\\mathbb R^n$ of rank up to\n$\\tilde \\Omega(n^{4/3})$. The best previous algorithm for this problem due to\nGe and Ma (RANDOM 2015) works up to rank $\\tilde \\Omega(n^{3/2})$ but requires\nquasipolynomial time. \n\n"}
{"id": "1512.02563", "contents": "Title: The combinatorial geometry of stresses in frameworks Abstract: In this paper we formulate and prove necessary and sufficient geometric\nconditions for existence of generic tensegrities in the plane for arbitrary\ngraphs. The conditions are written in terms of \"meet-join\" relations for the\nconfiguration spaces of fixed points and non-fixed lines through fixed points. \n\n"}
{"id": "1512.03531", "contents": "Title: Constructive noncommutative rank computation is in deterministic\n  polynomial time Abstract: We extend our techniques developed in our earlier paper appeared in\nComputational Complexity, 2017 (preprint: arXiv:1508.00690) to obtain a\ndeterministic polynomial time algorithm for computing the non-commutative rank\ntogether with certificates of linear spaces of matrices over sufficiently large\nbase fields.\n  The key new idea is a reduction procedure that keeps the blow-up parameter\nsmall, and there are two methods to implement this idea: the first one is a\ngreedy argument that removes certain rows and columns, and the second one is an\nefficient algorithmic version of a result of Derksen and Makam. Both methods\nrely crucially on the regularity lemma in our aforementioned paper, and in this\nmanuscript we also improve that lemma by removing a coprime condition there. \n\n"}
{"id": "1512.05279", "contents": "Title: Improved Bounds for 3SUM, $k$-SUM, and Linear Degeneracy Abstract: Given a set of $n$ real numbers, the 3SUM problem is to decide whether there\nare three of them that sum to zero. Until a recent breakthrough by Gr{\\o}nlund\nand Pettie [FOCS'14], a simple $\\Theta(n^2)$-time deterministic algorithm for\nthis problem was conjectured to be optimal. Over the years many algorithmic\nproblems have been shown to be reducible from the 3SUM problem or its variants,\nincluding the more generalized forms of the problem, such as $k$-SUM and\n$k$-variate linear degeneracy testing ($k$-LDT). The conjectured hardness of\nthese problems have become extremely popular for basing conditional lower\nbounds for numerous algorithmic problems in P.\n  In this paper, we show that the randomized $4$-linear decision tree\ncomplexity of 3SUM is $O(n^{3/2})$, and that the randomized $(2k-2)$-linear\ndecision tree complexity of $k$-SUM and $k$-LDT is $O(n^{k/2})$, for any odd\n$k\\ge 3$. These bounds improve (albeit randomized) the corresponding\n$O(n^{3/2}\\sqrt{\\log n})$ and $O(n^{k/2}\\sqrt{\\log n})$ decision tree bounds\nobtained by Gr{\\o}nlund and Pettie. Our technique includes a specialized\nrandomized variant of fractional cascading data structure. Additionally, we\ngive another deterministic algorithm for 3SUM that runs in $O(n^2 \\log\\log n /\n\\log n )$ time. The latter bound matches a recent independent bound by Freund\n[Algorithmica 2017], but our algorithm is somewhat simpler, due to a better use\nof word-RAM model. \n\n"}
{"id": "1512.05996", "contents": "Title: Advice Complexity of the Online Induced Subgraph Problem Abstract: Several well-studied graph problems aim to select a largest (or smallest)\ninduced subgraph with a given property of the input graph. Examples of such\nproblems include maximum independent set, maximum planar graph, and many\nothers. We consider these problems, where the vertices are presented online.\nWith each vertex, the online algorithm must decide whether to include it into\nthe constructed subgraph, based only on the subgraph induced by the vertices\npresented so far. We study the properties that are common to all these problems\nby investigating the generalized problem: for a hereditary property \\pty, find\nsome maximal induced subgraph having \\pty. We study this problem from the point\nof view of advice complexity. Using a result from Boyar et al. [STACS 2015], we\ngive a tight trade-off relationship stating that for inputs of length n roughly\nn/c bits of advice are both needed and sufficient to obtain a solution with\ncompetitive ratio c, regardless of the choice of \\pty, for any c (possibly a\nfunction of n). Surprisingly, a similar result cannot be obtained for the\nsymmetric problem: for a given cohereditary property \\pty, find a minimum\nsubgraph having \\pty. We show that the advice complexity of this problem varies\nsignificantly with the choice of \\pty.\n  We also consider preemptive online model, where the decision of the algorithm\nis not completely irreversible. In particular, the algorithm may discard some\nvertices previously assigned to the constructed set, but discarded vertices\ncannot be reinserted into the set again. We show that, for the maximum induced\nsubgraph problem, preemption cannot help much, giving a lower bound of\n$\\Omega(n/(c^2\\log c))$ bits of advice needed to obtain competitive ratio $c$,\nwhere $c$ is any increasing function bounded by \\sqrt{n/log n}. We also give a\nlinear lower bound for c close to 1. \n\n"}
{"id": "1512.06143", "contents": "Title: Algorithms for Provisioning Queries and Analytics Abstract: Provisioning is a technique for avoiding repeated expensive computations in\nwhat-if analysis. Given a query, an analyst formulates $k$ hypotheticals, each\nretaining some of the tuples of a database instance, possibly overlapping, and\nshe wishes to answer the query under scenarios, where a scenario is defined by\na subset of the hypotheticals that are \"turned on\". We say that a query admits\ncompact provisioning if given any database instance and any $k$ hypotheticals,\none can create a poly-size (in $k$) sketch that can then be used to answer the\nquery under any of the $2^{k}$ possible scenarios without accessing the\noriginal instance.\n  In this paper, we focus on provisioning complex queries that combine\nrelational algebra (the logical component), grouping, and statistics/analytics\n(the numerical component). We first show that queries that compute quantiles or\nlinear regression (as well as simpler queries that compute count and\nsum/average of positive values) can be compactly provisioned to provide\n(multiplicative) approximate answers to an arbitrary precision. In contrast,\nexact provisioning for each of these statistics requires the sketch size to be\nexponential in $k$. We then establish that for any complex query whose logical\ncomponent is a positive relational algebra query, as long as the numerical\ncomponent can be compactly provisioned, the complex query itself can be\ncompactly provisioned. On the other hand, introducing negation or recursion in\nthe logical component again requires the sketch size to be exponential in $k$.\nWhile our positive results use algorithms that do not access the original\ninstance after a scenario is known, we prove our lower bounds even for the case\nwhen, knowing the scenario, limited access to the instance is allowed. \n\n"}
{"id": "1512.09170", "contents": "Title: Statistical Query Algorithms for Mean Vector Estimation and Stochastic\n  Convex Optimization Abstract: Stochastic convex optimization, where the objective is the expectation of a\nrandom convex function, is an important and widely used method with numerous\napplications in machine learning, statistics, operations research and other\nareas. We study the complexity of stochastic convex optimization given only\nstatistical query (SQ) access to the objective function. We show that\nwell-known and popular first-order iterative methods can be implemented using\nonly statistical queries. For many cases of interest we derive nearly matching\nupper and lower bounds on the estimation (sample) complexity including linear\noptimization in the most general setting. We then present several consequences\nfor machine learning, differential privacy and proving concrete lower bounds on\nthe power of convex optimization based methods.\n  The key ingredient of our work is SQ algorithms and lower bounds for\nestimating the mean vector of a distribution over vectors supported on a convex\nbody in $\\mathbb{R}^d$. This natural problem has not been previously studied\nand we show that our solutions can be used to get substantially improved SQ\nversions of Perceptron and other online algorithms for learning halfspaces. \n\n"}
{"id": "1601.00479", "contents": "Title: Motivating Time-Inconsistent Agents: A Computational Approach Abstract: In this paper we investigate the computational complexity of motivating\ntime-inconsistent agents to complete long term projects. We resort to an\nelegant graph-theoretic model, introduced by Kleinberg and Oren, which consists\nof a task graph $G$ with $n$ vertices, including a source $s$ and target $t$,\nand an agent that incrementally constructs a path from $s$ to $t$ in order to\ncollect rewards. The twist is that the agent is present-biased and discounts\nfuture costs and rewards by a factor $\\beta\\in [0,1]$. Our design objective is\nto ensure that the agent reaches $t$ i.e.\\ completes the project, for as little\nreward as possible. Such graphs are called motivating. We consider two\nstrategies. First, we place a single reward $r$ at $t$ and try to guide the\nagent by removing edges from $G$. We prove that deciding the existence of such\nmotivating subgraphs is NP-complete if $r$ is fixed. More importantly, we\ngeneralize our reduction to a hardness of approximation result for computing\nthe minimum $r$ that admits a motivating subgraph. In particular, we show that\nno polynomial-time approximation to within a ratio of $\\sqrt{n}/4$ or less is\npossible, unless ${\\rm P}={\\rm NP}$. Furthermore, we develop a\n$(1+\\sqrt{n})$-approximation algorithm and thus settle the approximability of\ncomputing motivating subgraphs. Secondly, we study motivating reward\nconfigurations, where non-negative rewards $r(v)$ may be placed on arbitrary\nvertices $v$ of $G$. The agent only receives the rewards of visited vertices.\nAgain we give an NP-completeness result for deciding the existence of a\nmotivating reward configuration within a fixed budget $b$. This result even\nholds if $b=0$, which in turn implies that no efficient approximation of a\nminimum $b$ within a ration grater or equal to $1$ is possible, unless ${\\rm\nP}={\\rm NP}$. \n\n"}
{"id": "1601.01744", "contents": "Title: Performance of QAOA on Typical Instances of Constraint Satisfaction\n  Problems with Bounded Degree Abstract: We consider constraint satisfaction problems of bounded degree, with a good\nnotion of \"typicality\", e.g. the negation of the variables in each constraint\nis taken independently at random. Using the quantum approximate optimization\nalgorithm (QAOA), we show that $ \\mu+\\Omega(1/\\sqrt{D}) $ fraction of the\nconstraints can be satisfied for typical instances, with the assignment\nefficiently produced by QAOA. We do so by showing that the averaged fraction of\nconstraints being satisfied is $ \\mu+\\Omega(1/\\sqrt{D}) $, with small variance.\nHere $ \\mu $ is the fraction that would be satisfied by a uniformly random\nassignment, and $ D $ is the number of constraints that each variable can\nappear. CSPs with typicality include Max-$ k $XOR and Max-$ k $SAT. We point\nout how it can be applied to determine the typical ground-state energy of some\nlocal Hamiltonians. We also give a similar result for instances with \"no\noverlapping constraints\", using the quantum algorithm. We sketch how the\nclassical algorithm might achieve some partial result. \n\n"}
{"id": "1601.02416", "contents": "Title: Extension complexity and realization spaces of hypersimplices Abstract: The (n,k)-hypersimplex is the convex hull of all 0/1-vectors of length n with\ncoordinate sum k. We explicitly determine the extension complexity of all\nhypersimplices as well as of certain classes of combinatorial hypersimplices.\nTo that end, we investigate the projective realization spaces of hypersimplices\nand their (refined) rectangle covering numbers. Our proofs combine ideas from\ngeometry and combinatorics and are partly computer assisted. \n\n"}
{"id": "1601.02481", "contents": "Title: Approximation algorithms for node-weighted prize-collecting Steiner tree\n  problems on planar graphs Abstract: We study the prize-collecting version of the Node-weighted Steiner Tree\nproblem (NWPCST) restricted to planar graphs. We give a new primal-dual\nLagrangian-multiplier-preserving (LMP) 3-approximation algorithm for planar\nNWPCST. We then show a ($2.88 + \\epsilon$)-approximation which establishes a\nnew best approximation guarantee for planar NWPCST. This is done by combining\nour LMP algorithm with a threshold rounding technique and utilizing the\n2.4-approximation of Berman and Yaroslavtsev for the version without penalties.\nWe also give a primal-dual 4-approximation algorithm for the more general\nforest version using techniques introduced by Hajiaghay and Jain. \n\n"}
{"id": "1601.02939", "contents": "Title: The minimal hitting set generation problem: algorithms and computation Abstract: Finding inclusion-minimal \"hitting sets\" for a given collection of sets is a\nfundamental combinatorial problem with applications in domains as diverse as\nBoolean algebra, computational biology, and data mining. Much of the\nalgorithmic literature focuses on the problem of *recognizing* the collection\nof minimal hitting sets; however, in many of the applications, it is more\nimportant to *generate* these hitting sets. We survey twenty algorithms from\nacross a variety of domains, considering their history, classification, useful\nfeatures, and computational performance on a variety of synthetic and\nreal-world inputs. We also provide a suite of implementations of these\nalgorithms with a ready-to-use, platform-agnostic interface based on Docker\ncontainers and the AlgoRun framework, so that interested computational\nscientists can easily perform similar tests with inputs from their own research\nareas on their own computers or through a convenient Web interface. \n\n"}
{"id": "1601.02958", "contents": "Title: Measurable equidecompositions for group actions with an expansion\n  property Abstract: Given an action of a group $\\Gamma$ on a measure space $\\Omega$, we provide a\nsufficient criterion under which two sets $A, B\\subseteq \\Omega$ are measurably\nequidecomposable, i.e., $A$ can be partitioned into finitely many measurable\npieces which can be rearranged using the elements of $\\Gamma$ to form a\npartition of $B$. In particular, we prove that every bounded measurable subset\nof $R^n$, $n\\ge 3$, with non-empty interior is measurably equidecomposable to a\nball via isometries. The analogous result also holds for some other spaces,\nsuch as the sphere or the hyperbolic space of dimension $n\\ge 2$. \n\n"}
{"id": "1601.03095", "contents": "Title: Submodular Optimization under Noise Abstract: We consider the problem of maximizing a monotone submodular function under\nnoise. There has been a great deal of work on optimization of submodular\nfunctions under various constraints, resulting in algorithms that provide\ndesirable approximation guarantees. In many applications, however, we do not\nhave access to the submodular function we aim to optimize, but rather to some\nerroneous or noisy version of it. This raises the question of whether provable\nguarantees are obtainable in presence of error and noise. We provide initial\nanswers, by focusing on the question of maximizing a monotone submodular\nfunction under a cardinality constraint when given access to a noisy oracle of\nthe function. We show that:\n  - For a cardinality constraint $k \\geq 2$, there is an approximation\nalgorithm whose approximation ratio is arbitrarily close to $1-1/e$;\n  - For $k=1$ there is an algorithm whose approximation ratio is arbitrarily\nclose to $1/2$. No randomized algorithm can obtain an approximation ratio\nbetter than $1/2+o(1)$;\n  -If the noise is adversarial, no non-trivial approximation guarantee can be\nobtained. \n\n"}
{"id": "1601.03676", "contents": "Title: Arbitrary Overlap Constraints in Graph Packing Problems Abstract: In earlier versions of the community discovering problem, the overlap between\ncommunities was restricted by a simple count upper-bound [17,5,11,8]. In this\npaper, we introduce the $\\Pi$-Packing with $\\alpha()$-Overlap problem to allow\nfor more complex constraints in the overlap region than those previously\nstudied. Let $\\mathcal{V}^r$ be all possible subsets of vertices of $V(G)$ each\nof size at most $r$, and $\\alpha: \\mathcal{V}^r \\times \\mathcal{V}^r \\to\n\\{0,1\\}$ be a function. The $\\Pi$-Packing with $\\alpha()$-Overlap problem seeks\nat least $k$ induced subgraphs in a graph $G$ subject to: (i) each subgraph has\nat most $r$ vertices and obeys a property $\\Pi$, and (ii) for any pair\n$H_i,H_j$, with $i\\neq j$, $\\alpha(H_i, H_j) = 0$ (i.e., $H_i,H_j$ do not\nconflict). We also consider a variant that arises in clustering applications:\neach subgraph of a solution must contain a set of vertices from a given\ncollection of sets $\\mathcal{C}$, and no pair of subgraphs may share vertices\nfrom the sets of $\\mathcal{C}$. In addition, we propose similar formulations\nfor packing hypergraphs. We give an $O(r^{rk} k^{(r+1)k} n^{cr})$ algorithm for\nour problems where $k$ is the parameter and $c$ and $r$ are constants, provided\nthat: i) $\\Pi$ is computable in polynomial time in $n$ and ii) the function\n$\\alpha()$ satisfies specific conditions. Specifically, $\\alpha()$ is\nhereditary, applicable only to overlapping subgraphs, and computable in\npolynomial time in $n$. Motivated by practical applications we give several\nexamples of $\\alpha()$ functions which meet those conditions. \n\n"}
{"id": "1601.04974", "contents": "Title: Nonbinary tree-based phylogenetic networks Abstract: Rooted phylogenetic networks are used to describe evolutionary histories that\ncontain non-treelike evolutionary events such as hybridization and horizontal\ngene transfer. In some cases, such histories can be described by a phylogenetic\nbase-tree with additional linking arcs, which can for example represent gene\ntransfer events. Such phylogenetic networks are called tree-based. Here, we\nconsider two possible generalizations of this concept to nonbinary networks,\nwhich we call tree-based and strictly-tree-based nonbinary phylogenetic\nnetworks. We give simple graph-theoretic characterizations of tree-based and\nstrictly-tree-based nonbinary phylogenetic networks. Moreover, we show for each\nof these two classes that it can be decided in polynomial time whether a given\nnetwork is contained in the class. Our approach also provides a new view on\ntree-based binary phylogenetic networks. Finally, we discuss two examples of\nnonbinary phylogenetic networks in biology and show how our results can be\napplied to them. \n\n"}
{"id": "1601.07629", "contents": "Title: The Computational Complexity of Duality Abstract: We show that for any given norm ball or proper cone, weak membership in its\ndual ball or dual cone is polynomial-time reducible to weak membership in the\ngiven ball or cone. A consequence is that the weak membership or membership\nproblem for a ball or cone is NP-hard if and only if the corresponding problem\nfor the dual ball or cone is NP-hard. In a similar vein, we show that\ncomputation of the dual norm of a given norm is polynomial-time reducible to\ncomputation of the given norm. This extends to convex functions satisfying a\npolynomial growth condition: for such a given function, computation of its\nFenchel dual/conjugate is polynomial-time reducible to computation of the given\nfunction. Hence the computation of a norm or a convex function of\npolynomial-growth is NP-hard if and only if the computation of its dual norm or\nFenchel dual is NP-hard. We discuss implications of these results on the weak\nmembership problem for a symmetric convex body and its polar dual, the\npolynomial approximability of Mahler volume, and the weak membership problem\nfor the epigraph of a convex function with polynomial growth and that of its\nFenchel dual. \n\n"}
{"id": "1601.08229", "contents": "Title: On the geometry of border rank algorithms for matrix multiplication and\n  other tensors with symmetry Abstract: We establish basic information about border rank algorithms for the matrix\nmultiplication tensor and other tensors with symmetry. We prove that border\nrank algorithms for tensors with symmetry (such as matrix multiplication and\nthe determinant polynomial) come in families that include representatives with\nnormal forms. These normal forms will be useful both to develop new efficient\nalgorithms and to prove lower complexity bounds. We derive a border rank\nversion of the substitution method used in proving lower bounds for tensor\nrank. We use this border-substitution method and a normal form to improve the\nlower bound on the border rank of matrix multiplication by one, to 2n^2- n+1.\nWe also point out difficulties that will be formidable obstacles to future\nprogress on lower complexity bounds for tensors because of the \"wild\" structure\nof the Hilbert scheme of points. \n\n"}
{"id": "1602.01819", "contents": "Title: A short note on Merlin-Arthur protocols for subset sum Abstract: In the subset sum problem we are given n positive integers along with a\ntarget integer t. A solution is a subset of these integers summing to t. In\nthis short note we show that for a given subset sum instance there is a proof\nof size $O^*(\\sqrt{t})$ of what the number of solutions is that can be\nconstructed in $O^*(t)$ time and can be probabilistically verified in time\n$O^*(\\sqrt{t})$ with at most constant error probability. Here, the $O^*()$\nnotation omits factors polynomial in the input size $n\\log(t)$. \n\n"}
{"id": "1602.04478", "contents": "Title: Subgraph Counting: Color Coding Beyond Trees Abstract: The problem of counting occurrences of query graphs in a large data graph,\nknown as subgraph counting, is fundamental to several domains such as genomics\nand social network analysis. Many important special cases (e.g. triangle\ncounting) have received significant attention. Color coding is a very general\nand powerful algorithmic technique for subgraph counting. Color coding has been\nshown to be effective in several applications, but scalable implementations are\nonly known for the special case of {\\em tree queries} (i.e. queries of\ntreewidth one).\n  In this paper we present the first efficient distributed implementation for\ncolor coding that goes beyond tree queries: our algorithm applies to any query\ngraph of treewidth $2$. Since tree queries can be solved in time linear in the\nsize of the data graph, our contribution is the first step into the realm of\ncolour coding for queries that require superlinear running time in the worst\ncase. This superlinear complexity leads to significant load balancing problems\non graphs with heavy tailed degree distributions. Our algorithm structures the\ncomputation to work around high degree nodes in the data graph, and achieves\nvery good runtime and scalability on a diverse collection of data and query\ngraph pairs as a result. We also provide theoretical analysis of our\nalgorithmic techniques, showing asymptotic improvements in runtime on random\ngraphs with power law degree distributions, a popular model for real world\ngraphs. \n\n"}
{"id": "1602.05073", "contents": "Title: Dependence of the heavily covered point on parameters Abstract: We examine Gromov's method of selecting a point \"heavily covered\" by\nsimplices formed by a given finite point sets, in order to understand the\ndependence of the heavily covered point on parameters. We have no continuous\ndependence, but manage to utilize the \"homological continuous dependence\" of\nthe heavily covered point. This allows us to infer some corollaries in a usual\nway. We also give an elementary argument to prove the simplest of these\ncorollaries. \n\n"}
{"id": "1602.05263", "contents": "Title: Scheduling MapReduce Jobs under Multi-Round Precedences Abstract: We consider non-preemptive scheduling of MapReduce jobs with multiple tasks\nin the practical scenario where each job requires several map-reduce rounds. We\nseek to minimize the average weighted completion time and consider scheduling\non identical and unrelated parallel processors. For identical processors, we\npresent LP-based O(1)-approximation algorithms. For unrelated processors, the\napproximation ratio naturally depends on the maximum number of rounds of any\njob. Since the number of rounds per job in typical MapReduce algorithms is a\nsmall constant, our scheduling algorithms achieve a small approximation ratio\nin practice. For the single-round case, we substantially improve on previously\nbest known approximation guarantees for both identical and unrelated\nprocessors. Moreover, we conduct an experimental analysis and compare the\nperformance of our algorithms against a fast heuristic and a lower bound on the\noptimal solution, thus demonstrating their promising practical performance. \n\n"}
{"id": "1602.05856", "contents": "Title: TwoPaCo: An efficient algorithm to build the compacted de Bruijn graph\n  from many complete genomes Abstract: Motivation: De Bruijn graphs have been proposed as a data structure to\nfacilitate the analysis of related whole genome sequences, in both a population\nand comparative genomic settings. However, current approaches do not scale well\nto many genomes of large size (such as mammalian genomes). Results: In this\npaper, we present TwoPaCo, a simple and scalable low memory algorithm for the\ndirect construction of the compacted de Bruijn graph from a set of complete\ngenomes. We demonstrate that it can construct the graph for 100 simulated human\ngenomes in less then a day and eight real primates in less than two hours, on a\ntypical shared-memory machine. We believe that this progress will enable novel\nbiological analyses of hundreds of mammalian-sized genomes. Availability: Our\ncode and data is available for download from github.com/medvedevgroup/TwoPaCo\nContact: ium125@psu.edu \n\n"}
{"id": "1602.05897", "contents": "Title: Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity Abstract: We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power. \n\n"}
{"id": "1602.06174", "contents": "Title: A Constant Approximation Algorithm for Scheduling Packets on Line\n  Networks Abstract: In this paper we improve the approximation ratio for the problem of\nscheduling packets on line networks with bounded buffers, where the aim is that\nof maximizing the throughput. Each node in the network has a local buffer of\nbounded size $B$, and each edge (or link) can transmit a limited number, $c$,\nof packets in every time unit. The input to the problem consists of a set of\npacket requests, each defined by a source node, a destination node, and a\nrelease time. We denote by $n$ the size of the network. A solution for this\nproblem is a schedule that delivers (some of the) packets to their destinations\nwithout violating the capacity constraints of the network (buffers or edges).\nOur goal is to design an efficient algorithm that computes a schedule that\nmaximizes the number of packets that arrive to their respective destinations.\n  We give a randomized approximation algorithm with constant approximation\nratio for the case where $B=\\Theta(c)$. This improves over the previously best\nresult of $O(\\log^* n)$ (R\\\"acke and Ros\\'en, Theory Comput. Syst., 49(4),\n2011). Our improvement is based on a new combinatorial lemma that we prove,\nstating, roughly speaking, that if packets are allowed to stay put in buffers\nonly a limited number of time steps, $2d$, where $d$ is the longest\nsource-destination distance of any input packet, then the cardinality of the\noptimal solution is decreased by only a constant factor. This claim was not\npreviously known in the directed integral (i.e., unsplittable, zero-one) case,\nand may find additional applications for routing and scheduling algorithms. \n\n"}
{"id": "1602.07422", "contents": "Title: The robust recoverable spanning tree problem with interval costs is\n  polynomially solvable Abstract: In this paper the robust recoverable spanning tree problem with interval edge\ncosts is considered. The complexity of this problem has remained open to date.\nIt is shown that the problem is polynomially solvable, by using an iterative\nrelaxation method. A generalization of this idea to the robust recoverable\nmatroid basis problem is also presented. Polynomial algorithms for both robust\nrecoverable problems are proposed. \n\n"}
{"id": "1602.08393", "contents": "Title: Exact Weighted Minwise Hashing in Constant Time Abstract: Weighted minwise hashing (WMH) is one of the fundamental subroutine, required\nby many celebrated approximation algorithms, commonly adopted in industrial\npractice for large scale-search and learning. The resource bottleneck of the\nalgorithms is the computation of multiple (typically a few hundreds to\nthousands) independent hashes of the data. The fastest hashing algorithm is by\nIoffe \\cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data\nvector, $O(d)$ ($d$ is the number of non-zeros), for computing one hash.\nHowever, the requirement of multiple hashes demands hundreds or thousands\npasses over the data. This is very costly for modern massive dataset.\n  In this work, we break this expensive barrier and show an expected constant\namortized time algorithm which computes $k$ independent and unbiased WMH in\ntime $O(k)$ instead of $O(dk)$ required by Ioffe's method. Moreover, our\nproposal only needs a few bits (5 - 9 bits) of storage per hash value compared\nto around $64$ bits required by the state-of-art-methodologies. Experimental\nevaluations, on real datasets, show that for computing 500 WMH, our proposal\ncan be 60000x faster than the Ioffe's method without losing any accuracy. Our\nmethod is also around 100x faster than approximate heuristics capitalizing on\nthe efficient \"densified\" one permutation hashing schemes\n\\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its\nsignificant advantages, we hope that it will replace existing implementations\nin practice. \n\n"}
{"id": "1603.01191", "contents": "Title: A fixed-parameter algorithm for a routing open shop problem: unit\n  processing times, few machines and locations Abstract: The open shop problem is to find a minimum makespan schedule to process each\njob $J_i$ on each machine $M_q$ for $p_{iq}$ time such that, at any time, each\nmachine processes at most one job and each job is processed by at most one\nmachine. We study a problem variant in which the jobs are located in the\nvertices of an edge-weighted graph. The weights determine the time needed for\nthe machines to travel between jobs in different vertices. We show that the\nproblem with $m$ machines and $n$ unit-time jobs in $g$ vertices is solvable in\n$2^{O(gm^2\\log gm)}+O(mn\\log n)$ time. \n\n"}
{"id": "1603.01799", "contents": "Title: Noise Stability and Correlation with Half Spaces Abstract: Benjamini, Kalai and Schramm showed that a monotone function $f : \\{-1,1\\}^n\n\\to \\{-1,1\\}$ is noise stable if and only if it is correlated with a half-space\n(a set of the form $\\{x: \\langle x, a\\rangle \\le b\\}$).\n  We study noise stability in terms of correlation with half-spaces for general\n(not necessarily monotone) functions. We show that a function $f: \\{-1, 1\\}^n\n\\to \\{-1, 1\\}$ is noise stable if and only if it becomes correlated with a\nhalf-space when we modify $f$ by randomly restricting a constant fraction of\nits coordinates.\n  Looking at random restrictions is necessary: we construct noise stable\nfunctions whose correlation with any half-space is $o(1)$. The examples further\nsatisfy that different restrictions are correlated with different half-spaces:\nfor any fixed half-space, the probability that a random restriction is\ncorrelated with it goes to zero.\n  We also provide quantitative versions of the above statements, and versions\nthat apply for the Gaussian measure on $\\mathbb{R}^n$ instead of the discrete\ncube. Our work is motivated by questions in learning theory and a recent\nquestion of Khot and Moshkovitz. \n\n"}
{"id": "1603.01925", "contents": "Title: On the Complexity of Detecting Constrained Negative Cost Cycles Abstract: Given a positive integer $k$ and a directed graph with a cost on each edge,\nthe $k$-length negative cost cycle ($k$\\emph{LNCC}) problem is to determine\nwhether there exists a negative cost cycle with at least $k$ edges, and the\nfixed-point \\emph{$k$-}length negative cost cycle \\emph{trail (FP$k$LNCCT)}\nproblem is to determine whether there exists a negative trail enrouting a given\nvertex (as the fixed point) and containing only cycles with at least $k$ edges.\nThe $k$\\emph{LNCC} problem first emerged in deadlock avoidance in synchronized\nstreaming computing network \\cite{spaa10}, generalizing two famous problems:\nnegative cycle detection and the $k$-cycle problem. As a warmup by-production,\nthe paper first shows that \\emph{FP$k$LNCCT is }${\\cal NP}$-complete in\nmultigraph\\emph{ }even for\\emph{ $k=3$} by reducing from the \\emph{3SAT}\nproblem. Then as the main result, we prove the ${\\cal NP}$-completeness of\n$k$\\emph{LNCC} by giving a sophisticated reduction from the 3 Occurrence\n3-Satisfiability (\\emph{3O3SAT}) problem, a known ${\\cal NP}$-complete special\ncase of 3SAT in which a variable occurs at most three times. The complexity\nresult is interesting, since polynomial time algorithms are known for both\n$2$\\emph{LNCC} (essentially no restriction on the value of $k$) and the\n$k$-cycle problem of fixed $k$. This paper closes the open problem proposed by\nLi et al. in \\cite{spaa10} whether $k$\\emph{LNCC} admits polynomial-time\nalgorithms. \n\n"}
{"id": "1603.05523", "contents": "Title: Quantitative combinatorial geometry for continuous parameters Abstract: We prove variations of Carath\\'eodory's, Helly's and Tverberg's theorems\nwhere the sets involved are measured according to continuous functions such as\nthe volume or diameter. Among our results, we present continuous quantitative\nversions of Lov\\'asz's colorful Helly theorem, B\\'ar\\'any's colorful\nCarath\\'eodory's theorem, and the colorful Tverberg theorem. \n\n"}
{"id": "1603.05525", "contents": "Title: Quantitative Tverberg theorems over lattices and other discrete sets Abstract: This paper presents a new variation of Tverberg's theorem. Given a discrete\nset $S$ of $R^d$, we study the number of points of $S$ needed to guarantee the\nexistence of an $m$-partition of the points such that the intersection of the\n$m$ convex hulls of the parts contains at least $k$ points of $S$. The proofs\nof the main results require new quantitative versions of Helly's and\nCarath\\'eodory's theorems. \n\n"}
{"id": "1603.06505", "contents": "Title: Characterizations of symmetrically partial Boolean functions with exact\n  quantum query complexity Abstract: We give and prove an optimal exact quantum query algorithm with complexity\n$k+1$ for computing the promise problem (i.e., symmetric and partial Boolean\nfunction) $DJ_n^k$ defined as: $DJ_n^k(x)=1$ for $|x|=n/2$, $DJ_n^k(x)=0$ for\n$|x|$ in the set $\\{0, 1,\\ldots, k, n-k, n-k+1,\\ldots,n\\}$, and it is undefined\nfor the rest cases, where $n$ is even, $|x|$ is the Hamming weight of $x$. The\ncase of $k=0$ is the well-known Deutsch-Jozsa problem. We outline all symmetric\n(and partial) Boolean functions with degrees 1 and 2, and prove their exact\nquantum query complexity. Then we prove that any symmetrical (and partial)\nBoolean function $f$ has exact quantum 1-query complexity if and only if $f$\ncan be computed by the Deutsch-Jozsa algorithm. We also discover the optimal\nexact quantum 2-query complexity for distinguishing between inputs of Hamming\nweight $\\{ \\lfloor n/2\\rfloor, \\lceil n/2\\rceil \\}$ and Hamming weight in the\nset $\\{ 0, n\\}$ for all odd $n$. In addition, a method is provided to determine\nthe degree of any symmetrical (and partial) Boolean function. \n\n"}
{"id": "1603.06985", "contents": "Title: A Quantum Version of Sch\\\"oning's Algorithm Applied to Quantum 2-SAT Abstract: We study a quantum algorithm that consists of a simple quantum Markov\nprocess, and we analyze its behavior on restricted versions of Quantum 2-SAT.\nWe prove that the algorithm solves this decision problem with high probability\nfor n qubits, L clauses, and promise gap c in time O(n^2 L^2 c^{-2}). If the\nHamiltonian is additionally polynomially gapped, our algorithm efficiently\nproduces a state that has high overlap with the satisfying subspace. The Markov\nprocess we study is a quantum analogue of Sch\\\"oning's probabilistic algorithm\nfor k-SAT. \n\n"}
{"id": "1603.08151", "contents": "Title: Binary search trees and rectangulations Abstract: We revisit the classical problem of searching in a binary search tree (BST)\nusing rotations, and present novel connections of this problem to a number of\ngeometric and combinatorial structures. In particular, we show that the\nexecution trace of a BST that serves a sequence of queries is in close\ncorrespondence with the flip-sequence between two rectangulations.\n(Rectangulations are well-studied combinatorial objects also known as mosaic\nfloorplans.) We also reinterpret Small Manhattan Network, a problem with known\nconnections to the BST problem, in terms of flips in rectangulations. We apply\nfurther transformations to the obtained geometric model, to arrive at a\nparticularly simple view of the BST problem that resembles sequences of\nedge-relaxations in a shortest path algorithm.\n  Our connections yield new results and observations for all structures\nconcerned. In this draft we present some preliminary findings. BSTs with\nrotations are among the most fundamental and most thoroughly studied objects in\ncomputer science, nonetheless they pose long-standing open questions, such as\nthe dynamic optimality conjecture of Sleator and Tarjan (STOC 1983). Our hope\nis that the correspondences presented in this paper provide a new perspective\non this old problem and bring new tools to the study of dynamic optimality. \n\n"}
{"id": "1603.08430", "contents": "Title: A Note on the Computational Complexity of Unsmoothened Vertex Attack\n  Tolerance Abstract: We have previously introduced vertex attack tolerance (VAT) and unsmoothened\nVAT (UVAT), denoted respectively as $\\tau(G) = \\min_{S \\subset V}\n\\frac{|S|}{|V-S-C_{max}(V-S)|+1}$ and $\\hat{\\tau}(G) = \\min_{S \\subset V}\n\\frac{|S|}{|V-S-C_{max}(V-S)|}$, where $C_{max}(V-S)$ is the largest connected\ncomponent in $V-S$, as appropriate mathematical measures of resilience in the\nface of targeted node attacks for arbitrary degree networks. Here we prove the\nhardness of approximating $\\hat{\\tau}$ under various plausible computational\ncomplexity hypotheses. \n\n"}
{"id": "1604.00357", "contents": "Title: Beyond matroids: Secretary Problem and Prophet Inequality with general\n  constraints Abstract: We study generalizations of the \"Prophet Inequality\" and \"Secretary Problem\",\nwhere the algorithm is restricted to an arbitrary downward-closed set system.\nFor {0,1}-values, we give O(log n)-competitive algorithms for both problems.\nThis is close to the \\Omega(log n / loglog n) lower bound due to Babaioff,\nImmorlica, and Kleinberg. For general values, our results translate to O(log n\nlog r)-competitive algorithms, where r is the cardinality of the largest\nfeasible set. This resolves (up to the O(log r loglog n) factors) an open\nquestion posed to us by Bobby Kleinberg. \n\n"}
{"id": "1604.02557", "contents": "Title: The Complexity of Computing (Almost) Unitary Matrices With $\\eps$-Copies\n  of the Fourier Transform Abstract: The complexity of computing the Fourier transform is a longstanding open\nproblem. Very recently, Ailon (2013, 2014, 2015) showed in a collection of\npapers that, roughly speaking, a speedup of the Fourier transform computation\nimplies numerical ill-condition. The papers also quantify this tradeoff. The\nmain method for proving these results is via a potential function called\nquasi-entropy, reminiscent of Shannon entropy. The quasi-entropy method opens\nnew doors to understanding the computational complexity of the important\nFourier transformation. However, it suffers from various obvious limitations.\nThis paper, motivated by one such limitation, partly overcomes it, while at the\nsame time sheds llight on new interesting, and problems on the intersection of\ncomputational complexity and group theory. The paper also explains why this\nresearch direction, if fruitful, has a chance of solving much bigger questions\nabout the complexity of the Fourier transform. \n\n"}
{"id": "1604.03030", "contents": "Title: Amortized Dynamic Cell-Probe Lower Bounds from Four-Party Communication Abstract: This paper develops a new technique for proving amortized, randomized\ncell-probe lower bounds on dynamic data structure problems. We introduce a new\nrandomized nondeterministic four-party communication model that enables\n\"accelerated\", error-preserving simulations of dynamic data structures.\n  We use this technique to prove an $\\Omega(n(\\log n/\\log\\log n)^2)$ cell-probe\nlower bound for the dynamic 2D weighted orthogonal range counting problem\n(2D-ORC) with $n/\\mathrm{poly}\\log n$ updates and $n$ queries, that holds even\nfor data structures with $\\exp(-\\tilde{\\Omega}(n))$ success probability. This\nresult not only proves the highest amortized lower bound to date, but is also\ntight in the strongest possible sense, as a matching upper bound can be\nobtained by a deterministic data structure with worst-case operational time.\nThis is the first demonstration of a \"sharp threshold\" phenomenon for dynamic\ndata structures.\n  Our broader motivation is that cell-probe lower bounds for exponentially\nsmall success facilitate reductions from dynamic to static data structures. As\na proof-of-concept, we show that a slightly strengthened version of our lower\nbound would imply an $\\Omega((\\log n /\\log\\log n)^2)$ lower bound for the\nstatic 3D-ORC problem with $O(n\\log^{O(1)}n)$ space. Such result would give a\nnear quadratic improvement over the highest known static cell-probe lower\nbound, and break the long standing $\\Omega(\\log n)$ barrier for static data\nstructures. \n\n"}
{"id": "1604.04111", "contents": "Title: Lossy Kernelization Abstract: In this paper we propose a new framework for analyzing the performance of\npreprocessing algorithms. Our framework builds on the notion of kernelization\nfrom parameterized complexity. However, as opposed to the original notion of\nkernelization, our definitions combine well with approximation algorithms and\nheuristics. The key new definition is that of a polynomial size\n$\\alpha$-approximate kernel. Loosely speaking, a polynomial size\n$\\alpha$-approximate kernel is a polynomial time pre-processing algorithm that\ntakes as input an instance $(I,k)$ to a parameterized problem, and outputs\nanother instance $(I',k')$ to the same problem, such that $|I'|+k' \\leq\nk^{O(1)}$. Additionally, for every $c \\geq 1$, a $c$-approximate solution $s'$\nto the pre-processed instance $(I',k')$ can be turned in polynomial time into a\n$(c \\cdot \\alpha)$-approximate solution $s$ to the original instance $(I,k)$.\n  Our main technical contribution are $\\alpha$-approximate kernels of\npolynomial size for three problems, namely Connected Vertex Cover, Disjoint\nCycle Packing and Disjoint Factors. These problems are known not to admit any\npolynomial size kernels unless $NP \\subseteq coNP/poly$. Our approximate\nkernels simultaneously beat both the lower bounds on the (normal) kernel size,\nand the hardness of approximation lower bounds for all three problems. On the\nnegative side we prove that Longest Path parameterized by the length of the\npath and Set Cover parameterized by the universe size do not admit even an\n$\\alpha$-approximate kernel of polynomial size, for any $\\alpha \\geq 1$, unless\n$NP \\subseteq coNP/poly$. In order to prove this lower bound we need to combine\nin a non-trivial way the techniques used for showing kernelization lower bounds\nwith the methods for showing hardness of approximation \n\n"}
{"id": "1604.05863", "contents": "Title: An isoperimetric inequality for planar triangulations Abstract: We prove a discrete analogue to a classical isoperimetric theorem of Weil for\nsurfaces with non-positive curvature. It is shown that hexagons in the\ntriangular lattice have maximal volume among all sets of a given boundary in\nany triangulation with minimal degree 6. \n\n"}
{"id": "1604.07062", "contents": "Title: Extension Complexity of Independent Set Polytopes Abstract: We exhibit an $n$-node graph whose independent set polytope requires extended\nformulations of size exponential in $\\Omega(n/\\log n)$. Previously, no explicit\nexamples of $n$-dimensional $0/1$-polytopes were known with extension\ncomplexity larger than exponential in $\\Theta(\\sqrt{n})$. Our construction is\ninspired by a relatively little-known connection between extended formulations\nand (monotone) circuit depth. \n\n"}
{"id": "1604.07134", "contents": "Title: New minimal (4; n)-regular matchstick graphs Abstract: A matchstick graph is a graph drawn with straight edges in the plane such\nthat the edges have unit length, and non-adjacent edges do not intersect. We\ncall a matchstick graph ($m;n)$-regular if every vertex has only degree $m$ or\n$n$. In this article the authors present the latest known $(4;n)$-regular\nmatchstick graphs for $4\\leq n\\leq11$ with a minimum number of vertices. \n\n"}
{"id": "1604.07286", "contents": "Title: About the Structure of the Integer Cone and its Application to Bin\n  Packing Abstract: We consider the bin packing problem with $d$ different item sizes and revisit\nthe structure theorem given by Goemans and Rothvo\\ss [6] about solutions of the\ninteger cone. We present new techniques on how solutions can be modified and\ngive a new structure theorem that relies on the set of vertices of the\nunderlying integer polytope. As a result of our new structure theorem, we\nobtain an algorithm for the bin packing problem with running time\n$|V|^{2^{O(d)}} \\cdot enc(I)^{O(1)}$, where $V$ is the set of vertices of the\ninteger knapsack polytope and $enc(I)$ is the encoding length of the bin\npacking instance. The algorithm is fixed parameter tractable, parameterized by\nthe number of vertices of the integer knapsack polytope $|V|$. This shows that\nthe bin packing problem can be solved efficiently when the underlying integer\nknapsack polytope has an easy structure, i.e. has a small number of vertices.\n  Furthermore, we show that the presented bounds of the structure theorem are\nasymptotically tight. We give a construction of bin packing instances using new\nstructural insights and classical number theoretical theorems which yield the\ndesired lower bound. \n\n"}
{"id": "1604.08860", "contents": "Title: Designing optimal- and fast-on-average pattern matching algorithms Abstract: Given a pattern $w$ and a text $t$, the speed of a pattern matching algorithm\nover $t$ with regard to $w$, is the ratio of the length of $t$ to the number of\ntext accesses performed to search $w$ into $t$. We first propose a general\nmethod for computing the limit of the expected speed of pattern matching\nalgorithms, with regard to $w$, over iid texts. Next, we show how to determine\nthe greatest speed which can be achieved among a large class of algorithms,\naltogether with an algorithm running this speed. Since the complexity of this\ndetermination make it impossible to deal with patterns of length greater than\n4, we propose a polynomial heuristic. Finally, our approaches are compared with\n9 pre-existing pattern matching algorithms from both a theoretical and a\npractical point of view, i.e. both in terms of limit expected speed on iid\ntexts, and in terms of observed average speed on real data. In all cases, the\npre-existing algorithms are outperformed. \n\n"}
{"id": "1605.00058", "contents": "Title: Strongly Refuting Random CSPs Below the Spectral Threshold Abstract: Random constraint satisfaction problems (CSPs) are known to exhibit threshold\nphenomena: given a uniformly random instance of a CSP with $n$ variables and\n$m$ clauses, there is a value of $m = \\Omega(n)$ beyond which the CSP will be\nunsatisfiable with high probability. Strong refutation is the problem of\ncertifying that no variable assignment satisfies more than a constant fraction\nof clauses; this is the natural algorithmic problem in the unsatisfiable regime\n(when $m/n = \\omega(1)$).\n  Intuitively, strong refutation should become easier as the clause density\n$m/n$ grows, because the contradictions introduced by the random clauses become\nmore locally apparent. For CSPs such as $k$-SAT and $k$-XOR, there is a\nlong-standing gap between the clause density at which efficient strong\nrefutation algorithms are known, $m/n \\ge \\widetilde O(n^{k/2-1})$, and the\nclause density at which instances become unsatisfiable with high probability,\n$m/n = \\omega (1)$.\n  In this paper, we give spectral and sum-of-squares algorithms for strongly\nrefuting random $k$-XOR instances with clause density $m/n \\ge \\widetilde\nO(n^{(k/2-1)(1-\\delta)})$ in time $\\exp(\\widetilde O(n^{\\delta}))$ or in\n$\\widetilde O(n^{\\delta})$ rounds of the sum-of-squares hierarchy, for any\n$\\delta \\in [0,1)$ and any integer $k \\ge 3$. Our algorithms provide a smooth\ntransition between the clause density at which polynomial-time algorithms are\nknown at $\\delta = 0$, and brute-force refutation at the satisfiability\nthreshold when $\\delta = 1$. We also leverage our $k$-XOR results to obtain\nstrong refutation algorithms for SAT (or any other Boolean CSP) at similar\nclause densities. Our algorithms match the known sum-of-squares lower bounds\ndue to Grigoriev and Schonebeck, up to logarithmic factors.\n  Additionally, we extend our techniques to give new results for certifying\nupper bounds on the injective tensor norm of random tensors. \n\n"}
{"id": "1605.00443", "contents": "Title: On densities of lattice arrangements intersecting every i-dimensional\n  affine subspace Abstract: In 1978, Makai Jr. established a remarkable connection between the\nvolume-product of a convex body, its maximal lattice packing density and the\nminimal density of a lattice arrangement of its polar body intersecting every\naffine hyperplane. Consequently, he formulated a conjecture that can be seen as\na dual analog of Minkowski's fundamental theorem, and which is strongly linked\nto the well-known Mahler-conjecture.\n  Based on the covering minima of Kannan & Lov\\'asz and a problem posed by\nFejes T\\'oth, we arrange Makai Jr.'s conjecture into a wider context and\ninvestigate densities of lattice arrangements of convex bodies intersecting\nevery i-dimensional affine subspace. Then it becomes natural also to formulate\nand study a dual analog to Minkowski's second fundamental theorem. As our main\nresults, we derive meaningful asymptotic lower bounds for the densities of such\narrangements, and furthermore, we solve the problems exactly for the special,\nyet important, class of unconditional convex bodies. \n\n"}
{"id": "1605.00903", "contents": "Title: Sum-of-Squares Certificates for Maxima of Random Tensors on the Sphere Abstract: For an $n$-variate order-$d$ tensor $A$, define $ A_{\\max} := \\sup_{\\| x \\|_2\n= 1} \\langle A , x^{\\otimes d} \\rangle$ to be the maximum value taken by the\ntensor on the unit sphere. It is known that for a random tensor with i.i.d $\\pm\n1$ entries, $A_{\\max} \\lesssim \\sqrt{n\\cdot d\\cdot\\log d}$ w.h.p. We study the\nproblem of efficiently certifying upper bounds on $A_{\\max}$ via the natural\nrelaxation from the Sum of Squares (SoS) hierarchy. Our results include:\n  - When $A$ is a random order-$q$ tensor, we prove that $q$ levels of SoS\ncertifies an upper bound $B$ on $A_{\\max}$ that satisfies \\[ B ~~~~\\leq~~\n  A_{\\max} \\cdot \\biggl(\\frac{n}{q^{\\,1-o(1)}}\\biggr)^{q/4-1/2} \\quad\n\\text{w.h.p.} \\] Our upper bound improves a result of Montanari and Richard\n(NIPS 2014) when $q$ is large.\n  - We show the above bound is the best possible up to lower order terms,\nnamely the optimum of the level-$q$ SoS relaxation is at least \\[ A_{\\max}\n\\cdot \\biggl(\\frac{n}{q^{\\,1+o(1)}}\\biggr)^{q/4-1/2} \\ . \\]\n  - When $A$ is a random order-$d$ tensor, we prove that $q$ levels of SoS\ncertifies an upper bound $B$ on $A_{\\max}$ that satisfies \\[\n  B ~~\\leq ~~ A_{\\max} \\cdot \\biggl(\\frac{\\widetilde{O}(n)}{q}\\biggr)^{d/4 -\n1/2} \\quad \\text{w.h.p.} \\] For growing $q$, this improves upon the bound\ncertified by constant levels of SoS. This answers in part, a question posed by\nHopkins, Shi, and Steurer (COLT 2015), who established the tight\ncharacterization for constant levels of SoS. \n\n"}
{"id": "1605.01172", "contents": "Title: Approximate Euclidean Steiner Trees Abstract: An approximate Steiner tree is a Steiner tree on a given set of terminals in\nEuclidean space such that the angles at the Steiner points are within a\nspecified error e from 120 degrees.This notion arises in numerical\napproximations of minimum Steiner trees (W. D. Smith, Algorithmica, 7 (1992),\n137--177). We investigate the worst-case relative error of the length of an\napproximate Steiner tree compared to the shortest tree with the same\ntopology.Rubinstein, Weng and Wormald (J. Global Optim. 35 (2006), 573--592)\nconjectured that this relative error is at most linear in $e$, independent of\nthe number of terminals. We verify their conjecture for the two-dimensional\ncase as long as the error $e$ is sufficiently small in terms of the number of\nterminals. We derive a lower bound linear in $e$ for the relative error in the\ntwo-dimensional case when $e$ is sufficiently small in terms of the number of\nterminals. We find improved estimates of the relative error for larger values\nof $e$, and calculate exact values in the plane for three and four terminals. \n\n"}
{"id": "1605.01539", "contents": "Title: Rank and select: Another lesson learned Abstract: Rank and select queries on bitmaps are essential building bricks of many\ncompressed data structures, including text indexes, membership and range\nsupporting spatial data structures, compressed graphs, and more. Theoretically\nconsidered yet in 1980s, these primitives have also been a subject of vivid\nresearch concerning their practical incarnations in the last decade. We present\na few novel rank/select variants, focusing mostly on speed, obtaining\ncompetitive space-time results in the compressed setting. Our findings can be\nsummarized as follows: $(i)$ no single rank/select solution works best on any\nkind of data (ours are optimized for concatenated bit arrays obtained from\nwavelet trees for real text datasets), $(ii)$ it pays to efficiently handle\nblocks consisting of all 0 or all 1 bits, $(iii)$ compressed select does not\nhave to be significantly slower than compressed rank at a comparable memory\nuse. \n\n"}
{"id": "1605.01718", "contents": "Title: The Complexity of Translationally-Invariant Spin Chains with Low Local\n  Dimension Abstract: We prove that estimating the ground state energy of a\ntranslationally-invariant, nearest-neighbour Hamiltonian on a 1D spin chain is\nQMAEXP-complete, even for systems of low local dimension (roughly 40). This is\nan improvement over the best previously-known result by several orders of\nmagnitude, and it shows that spin-glass-like frustration can occur in\ntranslationally-invariant quantum systems with a local dimension comparable to\nthe smallest-known non-translationally-invariant systems with similar\nbehaviour.\n  While previous constructions of such systems rely on standard models of\nquantum computation, we construct a new model that is particularly well-suited\nfor encoding quantum computation into the ground state of a\ntranslationally-invariant system. This allows us to shift the proof burden from\noptimizing the Hamiltonian encoding a standard computational model to proving\nuniversality of a simple model.\n  Previous techniques for encoding quantum computation into the ground state of\na local Hamiltonian allow only a linear sequence of gates, hence only a linear\n(or nearly linear) path in the graph of all computational states. We extend\nthese techniques by allowing significantly more general paths, including\nbranching and cycles, thus enabling a highly efficient encoding of our\ncomputational model. However, this requires more sophisticated techniques for\nanalysing the spectrum of the resulting Hamiltonian. To address this, we\nintroduce a framework of graphs with unitary edge labels. After relating our\nHamiltonian to the Laplacian of such a unitary labelled graph, we analyse its\nspectrum by combining matrix analysis and spectral graph theory techniques. \n\n"}
{"id": "1605.03019", "contents": "Title: Tight Sum-of-Squares lower bounds for binary polynomial optimization\n  problems Abstract: We give two results concerning the power of the Sum-of-Squares(SoS)/Lasserre\nhierarchy. For binary polynomial optimization problems of degree $2d$ and an\nodd number of variables $n$, we prove that $\\frac{n+2d-1}{2}$ levels of the\nSoS/Lasserre hierarchy are necessary to provide the exact optimal value. This\nmatches the recent upper bound result by Sakaue, Takeda, Kim and Ito.\n  Additionally, we study a conjecture by Laurent, who considered the linear\nrepresentation of a set with no integral points. She showed that the\nSherali-Adams hierarchy requires $n$ levels to detect the empty integer hull,\nand conjectured that the SoS/Lasserre rank for the same problem is $n-1$. We\ndisprove this conjecture and derive lower and upper bounds for the rank. \n\n"}
{"id": "1605.03243", "contents": "Title: On \"Exponential Lower Bounds for Polytopes in Combinatorial\n  Optimization\" by Fiorini et al. (2015): A Refutation For Models With Disjoint\n  Sets of Descriptive Variables Abstract: We provide a numerical refutation of the developments of Fiorini et al.\n(2015)* for models with disjoint sets of descriptive variables. We also provide\nan insight into the meaning of the existence of a one-to-one linear map between\nsolutions of such models.\n  *: Fiorini, S., S. Massar, S. Pokutta, H.R. Tiwary, and R. de Wolf (2015).\nExponential Lower Bounds for Polytopes in Combinatorial Optimization. Journal\nof the ACM 62:2, Article No. 17. \n\n"}
{"id": "1605.03266", "contents": "Title: A Quantum Approach to the Unique Sink Orientation Problem Abstract: We consider quantum algorithms for the unique sink orientation problem on\ncubes. This problem is widely considered to be of intermediate computational\ncomplexity. This is because there no known polynomial algorithm (classical or\nquantum) from the problem and yet it arrises as part of a series of problems\nfor which it being intractable would imply complexity theoretic collapses. We\ngive a reduction which proves that if one can efficiently evaluate the kth\npower of the unique sink orientation outmap, then there exists a polynomial\ntime quantum algorithm for the unique sink orientation problem on cubes. \n\n"}
{"id": "1605.04031", "contents": "Title: Robin Hood Hashing really has constant average search cost and variance\n  in full tables Abstract: Thirty years ago, the Robin Hood collision resolution strategy was introduced\nfor open addressing hash tables, and a recurrence equation was found for the\ndistribution of its search cost. Although this recurrence could not be solved\nanalytically, it allowed for numerical computations that, remarkably, suggested\nthat the variance of the search cost approached a value of $1.883$ when the\ntable was full. Furthermore, by using a non-standard mean-centered search\nalgorithm, this would imply that searches could be performed in expected\nconstant time even in a full table.\n  In spite of the time elapsed since these observations were made, no progress\nhas been made in proving them. In this paper we introduce a technique to work\naround the intractability of the recurrence equation by solving instead an\nassociated differential equation. While this does not provide an exact\nsolution, it is sufficiently powerful to prove a bound for the variance, and\nthus obtain a proof that the variance of Robin Hood is bounded by a small\nconstant for load factors arbitrarily close to 1. As a corollary, this proves\nthat the mean-centered search algorithm runs in expected constant time.\n  We also use this technique to study the performance of Robin Hood hash tables\nunder a long sequence of insertions and deletions, where deletions are\nimplemented by marking elements as deleted. We prove that, in this case, the\nvariance is bounded by $1/(1-\\alpha)+O(1)$, where $\\alpha$ is the load factor.\n  To model the behavior of these hash tables, we use a unified approach that\ncan be applied also to study the First-Come-First-Served and\nLast-Come-First-Served collision resolution disciplines, both with and without\ndeletions. \n\n"}
{"id": "1605.04568", "contents": "Title: The bellows conjecture for small flexible polyhedra in non-Euclidean\n  spaces Abstract: The bellows conjecture claims that the volume of any flexible polyhedron of\ndimension 3 or higher is constant during the flexion. The bellows conjecture\nwas proved for flexible polyhedra in the Euclidean spaces of dimensions 3 and\nhigher, and for bounded flexible polyhedra in the odd-dimensional Lobachevsky\nspaces. Counterexamples to the bellows conjecture are known in all open\nhemispheres of dimensions 3 and higher. The aim of this paper is to prove that,\nnonetheless, the bellows conjecture is true for all flexible polyhedra in\neither spheres or Lobachevsky spaces of dimensions greater than or equal to 3\nwith sufficiently small edge lengths. \n\n"}
{"id": "1605.08023", "contents": "Title: Online Placement of Multi-Component Applications in Edge Computing\n  Environments Abstract: Mobile edge computing is a new cloud computing paradigm which makes use of\nsmall-sized edge-clouds to provide real-time services to users. These mobile\nedge-clouds (MECs) are located in close proximity to users, thus enabling users\nto seamlessly access applications running on MECs. Due to the co-existence of\nthe core (centralized) cloud, users, and one or multiple layers of MECs, an\nimportant problem is to decide where (on which computational entity) to place\ndifferent components of an application. This problem, known as the application\nor workload placement problem, is notoriously hard, and therefore, heuristic\nalgorithms without performance guarantees are generally employed in common\npractice, which may unknowingly suffer from poor performance as compared to the\noptimal solution. In this paper, we address the application placement problem\nand focus on developing algorithms with provable performance bounds. We model\nthe user application as an application graph and the physical computing system\nas a physical graph, with resource demands/availabilities annotated on these\ngraphs. We first consider the placement of a linear application graph and\npropose an algorithm for finding its optimal solution. Using this result, we\nthen generalize the formulation and obtain online approximation algorithms with\npolynomial-logarithmic (poly-log) competitive ratio for tree application graph\nplacement. We jointly consider node and link assignment, and incorporate\nmultiple types of computational resources at nodes. \n\n"}
{"id": "1605.08540", "contents": "Title: Induced Minor Free Graphs: Isomorphism and Clique-width Abstract: Given two graphs $G$ and $H$, we say that $G$ contains $H$ as an induced\nminor if a graph isomorphic to $H$ can be obtained from $G$ by a sequence of\nvertex deletions and edge contractions. We study the complexity of Graph\nIsomorphism on graphs that exclude a fixed graph as an induced minor. More\nprecisely, we determine for every graph $H$ that Graph Isomorphism is\npolynomial-time solvable on $H$-induced-minor-free graphs or that it is\nGI-complete. Additionally, we classify those graphs $H$ for which\n$H$-induced-minor-free graphs have bounded clique-width. These two results\ncomplement similar dichotomies for graphs that exclude a fixed graph as an\ninduced subgraph, minor, or subgraph. \n\n"}
{"id": "1606.00898", "contents": "Title: Factoring Polynomials over Finite Fields using Drinfeld Modules with\n  Complex Multiplication Abstract: We present novel algorithms to factor polynomials over a finite field $\\F_q$\nof odd characteristic using rank $2$ Drinfeld modules with complex\nmultiplication. The main idea is to compute a lift of the Hasse invariant\n(modulo the polynomial $f(x) \\in \\F_q[x]$ to be factored) with respect to a\nDrinfeld module $\\phi$ with complex multiplication. Factors of $f(x)$ supported\non prime ideals with supersingular reduction at $\\phi$ have vanishing Hasse\ninvariant and can be separated from the rest. A Drinfeld module analogue of\nDeligne's congruence plays a key role in computing the Hasse invariant lift. We\npresent two algorithms based on this idea. The first algorithm chooses Drinfeld\nmodules with complex multiplication at random and has a quadratic expected run\ntime. The second is a deterministic algorithm with $O(\\sqrt{p})$ run time\ndependence on the characteristic $p$ of $\\F_q$. \n\n"}
{"id": "1606.03168", "contents": "Title: Finding Low-Rank Solutions via Non-Convex Matrix Factorization,\n  Efficiently and Provably Abstract: A rank-$r$ matrix $X \\in \\mathbb{R}^{m \\times n}$ can be written as a product\n$U V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n\n\\times r}$. One could exploit this observation in optimization: e.g., consider\nthe minimization of a convex function $f(X)$ over rank-$r$ matrices, where the\nset of rank-$r$ matrices is modeled via the factorization $UV^\\top$. Though\nsuch parameterization reduces the number of variables, and is more\ncomputationally efficient (of particular interest is the case $r \\ll \\min\\{m,\nn\\}$), it comes at a cost: $f(UV^\\top)$ becomes a non-convex function w.r.t.\n$U$ and $V$.\n  We study such parameterization for optimization of generic convex objectives\n$f$, and focus on first-order, gradient descent algorithmic solutions. We\npropose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient\nfirst-order method that operates on the $U, V$ factors. We show that when $f$\nis (restricted) smooth, BFGD has local sublinear convergence, and linear\nconvergence when $f$ is both (restricted) smooth and (restricted) strongly\nconvex. For several key applications, we provide simple and efficient\ninitialization schemes that provide approximate solutions good enough for the\nabove convergence results to hold. \n\n"}
{"id": "1606.06636", "contents": "Title: Intriguingly Simple and Efficient Time-Dependent Routing in Road\n  Networks Abstract: We study the earliest arrival problem in road networks with static\ntime-dependent functions as arc weights. We propose and evaluate the following\nsimple algorithm: (1) average the travel time in k time windows, (2) compute a\nshortest time-independent path within each window and mark the edges in these\npaths, and (3) compute a shortest time-dependent path in the original graph\nrestricted to the marked edges. Our experimental evaluation shows that this\nsimple algorithm yields near optimal results on well-established benchmark\ninstances. We additionally demonstrate that the error can be further reduced by\nadditionally considering alternative routes at the expense of more marked\nedges. Finally, we show that the achieved subgraphs are small enough to be able\nto efficiently implement profile queries using a simple sampling-based\napproach. A highlight of our introduced algorithms is that they do not rely on\nlinking and merging profile functions. \n\n"}
{"id": "1606.08275", "contents": "Title: Near-Optimal Computation of Runs over General Alphabet via Non-Crossing\n  LCE Queries Abstract: Longest common extension queries (LCE queries) and runs are ubiquitous in\nalgorithmic stringology. Linear-time algorithms computing runs and\npreprocessing for constant-time LCE queries have been known for over a decade.\nHowever, these algorithms assume a linearly-sortable integer alphabet. A recent\nbreakthrough paper by Bannai et.\\ al.\\ (SODA 2015) showed a link between the\ntwo notions: all the runs in a string can be computed via a linear number of\nLCE queries. The first to consider these problems over a general ordered\nalphabet was Kosolobov (\\emph{Inf.\\ Process.\\ Lett.}, 2016), who presented an\n$O(n (\\log n)^{2/3})$-time algorithm for answering $O(n)$ LCE queries. This\nresult was improved by Gawrychowski et.\\ al.\\ (accepted to CPM 2016) to $O(n\n\\log \\log n)$ time. In this work we note a special \\emph{non-crossing} property\nof LCE queries asked in the runs computation. We show that any $n$ such\nnon-crossing queries can be answered on-line in $O(n \\alpha(n))$ time, which\nyields an $O(n \\alpha(n))$-time algorithm for computing runs. \n\n"}
{"id": "1606.08790", "contents": "Title: Robust Tverberg and colorful Carath\\'eodory results via random choice Abstract: We use the probabilistic method to obtain versions of the colorful\nCarath\\'eodory theorem and Tverberg's theorem with tolerance.\n  In particular, we give bounds for the smallest integer $N=N(t,d,r)$ such that\nfor any $N$ points in $R^d$, there is a partition of them into $r$ parts for\nwhich the following condition holds: after removing any $t$ points from the\nset, the convex hulls of what is left in each part intersect.\n  We prove the bound $N=rt+O(\\sqrt{t})$ for fixed $r,d$ which is polynomial in\neach parameters. Our bounds extend to colorful versions of Tverberg's theorem,\nas well as Reay-type variations of this theorem. \n\n"}
{"id": "1606.09449", "contents": "Title: Clique-Width and Directed Width Measures for Answer-Set Programming Abstract: Disjunctive Answer Set Programming (ASP) is a powerful declarative\nprogramming paradigm whose main decision problems are located on the second\nlevel of the polynomial hierarchy. Identifying tractable fragments and\ndeveloping efficient algorithms for such fragments are thus important\nobjectives in order to complement the sophisticated ASP systems available to\ndate. Hard problems can become tractable if some problem parameter is bounded\nby a fixed constant; such problems are then called fixed-parameter tractable\n(FPT). While several FPT results for ASP exist, parameters that relate to\ndirected or signed graphs representing the program at hand have been neglected\nso far. In this paper, we first give some negative observations showing that\ndirected width measures on the dependency graph of a program do not lead to FPT\nresults. We then consider the graph parameter of signed clique-width and\npresent a novel dynamic programming algorithm that is FPT w.r.t. this\nparameter. Clique-width is more general than the well-known treewidth, and, to\nthe best of our knowledge, ours is the first FPT algorithm for bounded\nclique-width for reasoning problems beyond SAT. \n\n"}
{"id": "1606.09481", "contents": "Title: Generating massive complex networks with hyperbolic geometry faster in\n  practice Abstract: Generative network models play an important role in algorithm development,\nscaling studies, network analysis, and realistic system benchmarks for graph\ndata sets. The commonly used graph-based benchmark model R-MAT has some\ndrawbacks concerning realism and the scaling behavior of network properties. A\ncomplex network model gaining considerable popularity builds random hyperbolic\ngraphs, generated by distributing points within a disk in the hyperbolic plane\nand then adding edges between points whose hyperbolic distance is below a\nthreshold.\n  We present in this paper a fast generation algorithm for such graphs. Our\nexperiments show that our new generator achieves speedup factors of 3-60 over\nthe best previous implementation. One billion edges can now be generated in\nunder one minute on a shared-memory workstation. Furthermore, we present a\ndynamic extension to model gradual network change, while preserving at each\nstep the point position probabilities. \n\n"}
{"id": "1607.01162", "contents": "Title: Unit Interval Vertex Deletion: Fewer Vertices are Relevant Abstract: The unit interval vertex deletion problem asks for a set of at most $k$\nvertices whose deletion from an $n$-vertex graph makes it a unit interval\ngraph. We develop an $O(k^4)$-vertex kernel for the problem, significantly\nimproving the $O(k^{53})$-vertex kernel of Fomin, Saurabh, and Villanger\n[ESA'12; SIAM J. Discrete Math 27(2013)]. We introduce a novel way of\norganizing cliques of a unit interval graph. Our constructive proof for the\ncorrectness of our algorithm, using interval models, greatly simplifies the\ndestructive proofs, based on forbidden induced subgraphs, for similar problems\nin literature. \n\n"}
{"id": "1607.01167", "contents": "Title: Deterministic polynomial-time approximation algorithms for partition\n  functions and graph polynomials Abstract: In this paper we show a new way of constructing deterministic polynomial-time\napproximation algorithms for computing complex-valued evaluations of a large\nclass of graph polynomials on bounded degree graphs. In particular, our\napproach works for the Tutte polynomial and independence polynomial, as well as\npartition functions of complex-valued spin and edge-coloring models.\n  More specifically, we define a large class of graph polynomials $\\mathcal C$\nand show that if $p\\in \\cal C$ and there is a disk $D$ centered at zero in the\ncomplex plane such that $p(G)$ does not vanish on $D$ for all bounded degree\ngraphs $G$, then for each $z$ in the interior of $D$ there exists a\ndeterministic polynomial-time approximation algorithm for evaluating $p(G)$ at\n$z$. This gives an explicit connection between absence of zeros of graph\npolynomials and the existence of efficient approximation algorithms, allowing\nus to show new relationships between well-known conjectures.\n  Our work builds on a recent line of work initiated by. Barvinok, which\nprovides a new algorithmic approach besides the existing Markov chain Monte\nCarlo method and the correlation decay method for these types of problems. \n\n"}
{"id": "1607.03183", "contents": "Title: How to calculate partition functions using convex programming\n  hierarchies: provable bounds for variational methods Abstract: We consider the problem of approximating partition functions for Ising\nmodels. We make use of recent tools in combinatorial optimization: the\nSherali-Adams and Lasserre convex programming hierarchies, in combination with\nvariational methods to get algorithms for calculating partition functions in\nthese families. These techniques give new, non-trivial approximation guarantees\nfor the partition function beyond the regime of correlation decay. They also\ngeneralize some classical results from statistical physics about the\nCurie-Weiss ferromagnetic Ising model, as well as provide a partition function\ncounterpart of classical results about max-cut on dense graphs\n\\cite{arora1995polynomial}. With this, we connect techniques from two\napparently disparate research areas -- optimization and counting/partition\nfunction approximations. (i.e. \\#-P type of problems).\n  Furthermore, we design to the best of our knowledge the first provable,\nconvex variational methods. Though in the literature there are a host of convex\nversions of variational methods \\cite{wainwright2003tree, wainwright2005new,\nheskes2006convexity, meshi2009convexifying}, they come with no guarantees\n(apart from some extremely special cases, like e.g. the graph has a single\ncycle \\cite{weiss2000correctness}). We consider dense and low threshold rank\ngraphs, and interestingly, the reason our approach works on these types of\ngraphs is because local correlations propagate to global correlations --\ncompletely the opposite of algorithms based on correlation decay. In the\nprocess we design novel entropy approximations based on the low-order moments\nof a distribution.\n  Our proof techniques are very simple and generic, and likely to be applicable\nto many other settings other than Ising models. \n\n"}
{"id": "1607.03718", "contents": "Title: Streaming Algorithms For Computing Edit Distance Without Exploiting\n  Suffix Trees Abstract: The edit distance is a way of quantifying how similar two strings are to one\nanother by counting the minimum number of character insertions, deletions, and\nsubstitutions required to transform one string into the other.\n  In this paper we study the computational problem of computing the edit\ndistance between a pair of strings where their distance is bounded by a\nparameter $k\\ll n$. We present two streaming algorithms for computing edit\ndistance: One runs in time $O(n+k^2)$ and the other $n+O(k^3)$. By writing\n$n+O(k^3)$ we want to emphasize that the number of operations per an input\nsymbol is a small constant. In particular, the running time does not depend on\nthe alphabet size, and the algorithm should be easy to implement.\n  Previously a streaming algorithm with running time $O(n+k^4)$ was given in\nthe paper by the current authors (STOC'16). The best off-line algorithm runs in\ntime $O(n+k^2)$ (Landau et al., 1998) which is known to be optimal under the\nStrong Exponential Time Hypothesis. \n\n"}
{"id": "1607.03961", "contents": "Title: Deleting and Testing Forbidden Patterns in Multi-Dimensional Arrays Abstract: Understanding the local behaviour of structured multi-dimensional data is a\nfundamental problem in various areas of computer science. As the amount of data\nis often huge, it is desirable to obtain sublinear time algorithms, and\nspecifically property testers, to understand local properties of the data.\n  We focus on the natural local problem of testing pattern freeness: given a\nlarge $d$-dimensional array $A$ and a fixed $d$-dimensional pattern $P$ over a\nfinite alphabet, we say that $A$ is $P$-free if it does not contain a copy of\nthe forbidden pattern $P$ as a consecutive subarray. The distance of $A$ to\n$P$-freeness is the fraction of entries of $A$ that need to be modified to make\nit $P$-free. For any $\\epsilon \\in [0,1]$ and any large enough pattern $P$ over\nany alphabet, other than a very small set of exceptional patterns, we design a\ntolerant tester that distinguishes between the case that the distance is at\nleast $\\epsilon$ and the case that it is at most $a_d \\epsilon$, with query\ncomplexity and running time $c_d \\epsilon^{-1}$, where $a_d < 1$ and $c_d$\ndepend only on $d$.\n  To analyze the testers we establish several combinatorial results, including\nthe following $d$-dimensional modification lemma, which might be of independent\ninterest: for any large enough pattern $P$ over any alphabet (excluding a small\nset of exceptional patterns for the binary case), and any array $A$ containing\na copy of $P$, one can delete this copy by modifying one of its locations\nwithout creating new $P$-copies in $A$.\n  Our results address an open question of Fischer and Newman, who asked whether\nthere exist efficient testers for properties related to tight substructures in\nmulti-dimensional structured data. They serve as a first step towards a general\nunderstanding of local properties of multi-dimensional arrays, as any such\nproperty can be characterized by a fixed family of forbidden patterns. \n\n"}
{"id": "1607.04346", "contents": "Title: Space-Efficient Construction of Compressed Indexes in Deterministic\n  Linear Time Abstract: We show that the compressed suffix array and the compressed suffix tree of a\nstring $T$ can be built in $O(n)$ deterministic time using $O(n\\log\\sigma)$\nbits of space, where $n$ is the string length and $\\sigma$ is the alphabet\nsize. Previously described deterministic algorithms either run in time that\ndepends on the alphabet size or need $\\omega(n\\log \\sigma)$ bits of working\nspace. Our result has immediate applications to other problems, such as\nyielding the first linear-time LZ77 and LZ78 parsing algorithms that use $O(n\n\\log\\sigma)$ bits. \n\n"}
{"id": "1607.04913", "contents": "Title: An Improved Algorithm for Incremental DFS Tree in Undirected Graphs Abstract: Depth first search (DFS) tree is one of the most well-known data structures\nfor designing efficient graph algorithms. Given an undirected graph $G=(V,E)$\nwith $n$ vertices and $m$ edges, the textbook algorithm takes $O(n+m)$ time to\nconstruct a DFS tree. In this paper, we study the problem of maintaining a DFS\ntree when the graph is undergoing incremental updates. Formally, we show: Given\nan arbitrary online sequence of edge or vertex insertions, there is an\nalgorithm that reports a DFS tree in $O(n)$ worst case time per operation, and\nrequires $O\\left(\\min\\{m \\log n, n^2\\}\\right)$ preprocessing time.\n  Our result improves the previous $O(n \\log^3 n)$ worst case update time\nalgorithm by Baswana et al. and the $O(n \\log n)$ time by Nakamura and\nSadakane, and matches the trivial $\\Omega(n)$ lower bound when it is required\nto explicitly output a DFS tree.\n  Our result builds on the framework introduced in the breakthrough work by\nBaswana et al., together with a novel use of a tree-partition lemma by Duan and\nZhan, and the celebrated fractional cascading technique by Chazelle and Guibas. \n\n"}
{"id": "1607.05112", "contents": "Title: Minimum cycle and homology bases of surface embedded graphs Abstract: We study the problems of finding a minimum cycle basis (a minimum weight set\nof cycles that form a basis for the cycle space) and a minimum homology basis\n(a minimum weight set of cycles that generates the $1$-dimensional\n($\\mathbb{Z}_2$)-homology classes) of an undirected graph embedded on a\nsurface. The problems are closely related, because the minimum cycle basis of a\ngraph contains its minimum homology basis, and the minimum homology basis of\nthe $1$-skeleton of any graph is exactly its minimum cycle basis.\n  For the minimum cycle basis problem, we give a deterministic\n$O(n^\\omega+2^{2g}n^2+m)$-time algorithm for graphs embedded on an orientable\nsurface of genus $g$. The best known existing algorithms for surface embedded\ngraphs are those for general graphs: an $O(m^\\omega)$ time Monte Carlo\nalgorithm and a deterministic $O(nm^2/\\log n + n^2 m)$ time algorithm. For the\nminimum homology basis problem, we give a deterministic $O((g+b)^3 n \\log n +\nm)$-time algorithm for graphs embedded on an orientable or non-orientable\nsurface of genus $g$ with $b$ boundary components, assuming shortest paths are\nunique, improving on existing algorithms for many values of $g$ and $n$. The\nassumption of unique shortest paths can be avoided with high probability using\nrandomization or deterministically by increasing the running time of the\nhomology basis algorithm by a factor of $O(\\log n)$. \n\n"}
{"id": "1607.05222", "contents": "Title: Information-theoretic bounds and phase transitions in clustering, sparse\n  PCA, and submatrix localization Abstract: We study the problem of detecting a structured, low-rank signal matrix\ncorrupted with additive Gaussian noise. This includes clustering in a Gaussian\nmixture model, sparse PCA, and submatrix localization. Each of these problems\nis conjectured to exhibit a sharp information-theoretic threshold, below which\nthe signal is too weak for any algorithm to detect. We derive upper and lower\nbounds on these thresholds by applying the first and second moment methods to\nthe likelihood ratio between these \"planted models\" and null models where the\nsignal matrix is zero. Our bounds differ by at most a factor of root two when\nthe rank is large (in the clustering and submatrix localization problems, when\nthe number of clusters or blocks is large) or the signal matrix is very sparse.\nMoreover, our upper bounds show that for each of these problems there is a\nsignificant regime where reliable detection is information- theoretically\npossible but where known algorithms such as PCA fail completely, since the\nspectrum of the observed matrix is uninformative. This regime is analogous to\nthe conjectured 'hard but detectable' regime for community detection in sparse\ngraphs. \n\n"}
{"id": "1607.06660", "contents": "Title: Fast Longest Common Extensions in Small Space Abstract: In this paper we address the longest common extension (LCE) problem: to\ncompute the length $\\ell$ of the longest common prefix between any two suffixes\nof $T\\in \\Sigma^n$ with $ \\Sigma = \\{0, \\ldots \\sigma-1\\} $. We present two\nfast and space-efficient solutions based on (Karp-Rabin)\n\\textit{fingerprinting} and \\textit{sampling}. Our first data structure\nexploits properties of Mersenne prime numbers when used as moduli of the\nKarp-Rabin hash function and takes $n\\lceil \\log_2\\sigma\\rceil$ bits of space.\nOur second structure works with any prime modulus and takes $n\\lceil\n\\log_2\\sigma\\rceil + n/w + w\\log_2 n$ bits of space ($ w $ memory-word size).\nBoth structures support $\\mathcal O\\left(m\\log\\sigma/w \\right)$-time extraction\nof any length-$m$ text substring, $\\mathcal O(\\log\\ell)$-time LCE queries with\nhigh probability, and can be built in optimal $\\mathcal O(n)$ time. In the\nfirst case, ours is the first result showing that it is possible to answer LCE\nqueries in $o(n)$ time while using only $\\mathcal O(1)$ words on top of the\nspace required to store the text. Our results improve the state of the art in\nspace usage, query times, and preprocessing times and are extremely practical:\nwe present a C++ implementation that is very fast and space-efficient in\npractice. \n\n"}
{"id": "1607.06665", "contents": "Title: Approximation Schemes for Geometric Coverage Problems Abstract: In their seminal work, Mustafa and Ray (2009) showed that a wide class of\ngeometric set cover (SC) problems admit a PTAS via local search -- this is one\nof the most general approaches known for such problems. Their result applies if\na naturally defined \"exchange graph\" for two feasible solutions is planar and\nis based on subdividing this graph via a planar separator theorem due to\nFrederickson (1987). Obtaining similar results for the related maximum\nk-coverage problem (MC) seems non-trivial due to the hard cardinality\nconstraint. In fact, while Badanidiyuru, Kleinberg, and Lee (2012) have shown\n(via a different analysis) that local search yields a PTAS for two-dimensional\nreal halfspaces, they only conjectured that the same holds true for dimension\nthree. Interestingly, at this point it was already known that local search\nprovides a PTAS for the corresponding set cover case and this followed directly\nfrom the approach of Mustafa and Ray.\n  In this work we provide a way to address the above-mentioned issue. First, we\npropose a color-balanced version of the planar separator theorem. The resulting\nsubdivision approximates locally in each part the global distribution of the\ncolors. Second, we show how this roughly balanced subdivision can be employed\nin a more careful analysis to strictly obey the hard cardinality constraint.\nMore specifically, we obtain a PTAS for any \"planarizable\" instance of MC and\nthus essentially for all cases where the corresponding SC instance can be\ntackled via the approach of Mustafa and Ray. As a corollary, we confirm the\nconjecture of Badanidiyuru, Kleinberg, and Lee regarding real half spaces in\ndimension three. We feel that our ideas could also be helpful in other\ngeometric settings involving a cardinality constraint. \n\n"}
{"id": "1607.06711", "contents": "Title: Algorithmic and optimization aspects of Brascamp-Lieb inequalities, via\n  Operator Scaling Abstract: The celebrated Brascamp-Lieb (BL) inequalities (and their extensions) are an\nimportant mathematical tool, unifying and generalizing numerous inequalities in\nanalysis, convex geometry and information theory. While their structural theory\nis very well understood, far less is known about computing their main\nparameters.\n  We give polynomial time algorithms to compute feasibility of BL-datum, the\noptimal BL-constant and a weak separation oracle for the BL-polytope. The same\nresult holds for the so-called Reverse BL inequalities of Barthe. The best\nknown algorithms for any of these tasks required at least exponential time.\n  The algorithms are obtained by a simple efficient reduction of a given\nBL-datum to an instance of the Operator Scaling problem defined by Gurvits, for\nwhich the present authors have provided a polynomial time algorithm. This\nreduction implies algorithmic versions of many of the known structural results,\nand in some cases provide proofs that are different or simpler than existing\nones.\n  Of particular interest is the fact that the operator scaling algorithm is\ncontinuous in its input. Thus as a simple corollary of our reduction we obtain\nexplicit bounds on the magnitude and continuity of the BL-constant in terms of\nthe BL-data. To the best of our knowledge no such bounds were known, as past\narguments relied on compactness. The continuity of BL-constants is important\nfor developing non-linear BL inequalities that have recently found so many\napplications. \n\n"}
{"id": "1607.07130", "contents": "Title: A No-Go Theorem for Derandomized Parallel Repetition: Beyond\n  Feige-Kilian Abstract: In this work we show a barrier towards proving a randomness-efficient\nparallel repetition, a promising avenue for achieving many tight\ninapproximability results. Feige and Kilian (STOC'95) proved an impossibility\nresult for randomness-efficient parallel repetition for two prover games with\nsmall degree, i.e., when each prover has only few possibilities for the\nquestion of the other prover. In recent years, there have been indications that\nrandomness-efficient parallel repetition (also called derandomized parallel\nrepetition) might be possible for games with large degree, circumventing the\nimpossibility result of Feige and Kilian. In particular, Dinur and Meir\n(CCC'11) construct games with large degree whose repetition can be derandomized\nusing a theorem of Impagliazzo, Kabanets and Wigderson (SICOMP'12). However,\nobtaining derandomized parallel repetition theorems that would yield optimal\ninapproximability results has remained elusive.\n  This paper presents an explanation for the current impasse in progress, by\nproving a limitation on derandomized parallel repetition. We formalize two\nproperties which we call \"fortification-friendliness\" and \"yields robust\nembeddings.\" We show that any proof of derandomized parallel repetition\nachieving almost-linear blow-up cannot both (a) be fortification-friendly and\n(b) yield robust embeddings. Unlike Feige and Kilian, we do not require the\nsmall degree assumption.\n  Given that virtually all existing proofs of parallel repetition, including\nthe derandomized parallel repetition result of Dinur and Meir, share these two\nproperties, our no-go theorem highlights a major barrier to achieving\nalmost-linear derandomized parallel repetition. \n\n"}
{"id": "1607.07156", "contents": "Title: Low growth equational complexity Abstract: The equational complexity function\n$\\beta_\\mathscr{V}:\\mathbb{N}\\to\\mathbb{N}$ of an equational class of algebras\n$\\mathscr{V}$ bounds the size of equation required to determine membership of\n$n$-element algebras in $\\mathscr{V}$. Known examples of finitely generated\nvarieties $\\mathscr{V}$ with unbounded equational complexity have growth in\n$\\Omega(n^c)$, usually for $c\\geq \\frac{1}{2}$. We show that much slower growth\nis possible, exhibiting $O(\\log_2^3(n))$ growth amongst varieties of\nsemilattice ordered inverse semigroups and additive idempotent semirings. We\nalso examine a quasivariety analogue of equational complexity, and show that a\nfinite group has polylogarithmic quasi-equational complexity function, bounded\nif and only if all Sylow subgroups are abelian. \n\n"}
{"id": "1607.07200", "contents": "Title: Approximating Multicut and the Demand Graph Abstract: In the minimum Multicut problem, the input is an edge-weighted supply graph\n$G=(V,E)$ and a simple demand graph $H=(V,F)$. Either $G$ and $H$ are directed\n(DMulC) or both are undirected (UMulC). The goal is to remove a minimum weight\nset of edges in $G$ such that there is no path from $s$ to $t$ in the remaining\ngraph for any $(s,t) \\in F$. UMulC admits an $O(\\log k)$-approximation where\n$k$ is the vertex cover size of $H$ while the best known approximation for\nDMulC is $\\min\\{k, \\tilde{O}(n^{11/23})\\}$. These approximations are obtained\nby proving corresponding results on the multicommodity flow-cut gap. In\ncontrast to these results some special cases of Multicut, such as the\nwell-studied Multiway Cut problem, admit a constant factor approximation in\nboth undirected and directed graphs. Motivated by both concrete instances from\napplications and abstract considerations, we consider the role that the\nstructure of the demand graph $H$ plays in determining the approximability of\nMulticut.\n  In undirected graphs our main result is a $2$-approximation in $n^{O(t)}$\ntime when the demand graph $H$ excludes an induced matching of size $t$. This\ngives a constant factor approximation for a specific demand graph that\nmotivated this work.\n  In contrast to undirected graphs, we prove that in directed graphs such\napproximation algorithms can not exist. Assuming the Unique Games Conjecture\n(UGC), for a large class of fixed demand graphs DMulC cannot be approximated to\na factor better than worst-case flow-cut gap. As a consequence we prove that\nfor any fixed $k$, assuming UGC, DMulC with $k$ demand pairs is hard to\napproximate to within a factor better than $k$. On the positive side, we prove\nan approximation of $k$ when the demand graph excludes certain graphs as an\ninduced subgraph. This generalizes the Multiway Cut result to a much larger\nclass of demand graphs. \n\n"}
{"id": "1607.07306", "contents": "Title: The Costs and Benefits of Sharing: Sequential Individual Rationality and\n  Sequential Fairness Abstract: In designing dynamic shared service systems that incentivize customers to opt\nfor shared rather than exclusive service, the traditional notion of individual\nrationality may be insufficient, as a customer's estimated utility could\nfluctuate arbitrarily during their time in the shared system, as long as their\nrealized utility at service completion is not worse than that for exclusive\nservice. In this work, within a model that explicitly considers the\n\"inconvenience costs\" incurred by customers due to sharing, we introduce the\nnotion of sequential individual rationality (SIR) that requires that the\ndisutility of existing customers is nonincreasing as the system state changes\ndue to new customer arrivals. Next, under SIR, we observe that cost sharing can\nalso be viewed as benefit sharing, which inspires a natural definition of\nsequential fairness (SF) - the total incremental benefit due to a new customer\nis shared among existing customers in proportion to the incremental\ninconvenience suffered.\n  We demonstrate the effectiveness of these notions by applying them to a\nridesharing system, where unexpected detours to pick up subsequent passengers\ninconvenience the existing passengers. Imposing SIR and SF reveals interesting\nand surprising results, including: (a) natural limits on the incremental\ndetours permissible, (b) exact characterization of \"SIR-feasible\" routes, which\nboast sublinear upper and lower bounds on the fractional detours, (c) exact\ncharacterization of sequentially fair cost sharing schemes, which includes a\nstrong requirement that passengers must compensate each other for the detour\ninconveniences that they cause, and (d) new algorithmic problems related to and\nmotivated by SIR. \n\n"}
{"id": "1607.07673", "contents": "Title: A Selectable Sloppy Heap Abstract: We study the selection problem, namely that of computing the $i$th order\nstatistic of $n$ given elements. Here we offer a data structure called\n\\emph{selectable sloppy heap} handling a dynamic version in which upon request:\n(i)~a new element is inserted or (ii)~an element of a prescribed quantile group\nis deleted from the data structure. Each operation is executed in (ideal!)\nconstant time---and is thus independent of $n$ (the number of elements stored\nin the data structure)---provided that the number of quantile groups is fixed.\nThis is the first result of this kind accommodating both insertion and deletion\nin constant time. As such, our data structure outperforms the soft heap data\nstructure of Chazelle (which only offers constant amortized complexity for a\nfixed error rate $0<\\varepsilon \\leq 1/2$) in applications such as dynamic\npercentile maintenance. The design demonstrates how slowing down a certain\ncomputation can speed up the data structure. \n\n"}
{"id": "1607.08077", "contents": "Title: Algorithmic statistics: forty years later Abstract: Algorithmic statistics has two different (and almost orthogonal) motivations.\nFrom the philosophical point of view, it tries to formalize how the statistics\nworks and why some statistical models are better than others. After this notion\nof a \"good model\" is introduced, a natural question arises: it is possible that\nfor some piece of data there is no good model? If yes, how often these bad\n(\"non-stochastic\") data appear \"in real life\"?\n  Another, more technical motivation comes from algorithmic information theory.\nIn this theory a notion of complexity of a finite object (=amount of\ninformation in this object) is introduced; it assigns to every object some\nnumber, called its algorithmic complexity (or Kolmogorov complexity).\nAlgorithmic statistic provides a more fine-grained classification: for each\nfinite object some curve is defined that characterizes its behavior. It turns\nout that several different definitions give (approximately) the same curve.\n  In this survey we try to provide an exposition of the main results in the\nfield (including full proofs for the most important ones), as well as some\nhistorical comments. We assume that the reader is familiar with the main\nnotions of algorithmic information (Kolmogorov complexity) theory. \n\n"}
{"id": "1607.08337", "contents": "Title: Efficient Algorithms for Constructing Very Sparse Spanners and Emulators Abstract: Miller et al. \\cite{MPVX15} devised a distributed\\footnote{They actually\nshowed a PRAM algorithm. The distributed algorithm with these properties is\nimplicit in \\cite{MPVX15}.} algorithm in the CONGEST model, that given a\nparameter $k = 1,2,\\ldots$, constructs an $O(k)$-spanner of an input unweighted\n$n$-vertex graph with $O(n^{1+1/k})$ expected edges in $O(k)$ rounds of\ncommunication. In this paper we improve the result of \\cite{MPVX15}, by showing\na $k$-round distributed algorithm in the same model, that constructs a\n$(2k-1)$-spanner with $O(n^{1+1/k}/\\epsilon)$ edges, with probability $1-\n\\epsilon$, for any $\\epsilon>0$. Moreover, when $k = \\omega(\\log n)$, our\nalgorithm produces (still in $k$ rounds) {\\em ultra-sparse} spanners, i.e.,\nspanners of size $n(1+ o(1))$, with probability $1- o(1)$. To our knowledge,\nthis is the first distributed algorithm in the CONGEST or in the PRAM models\nthat constructs spanners or skeletons (i.e., connected spanning subgraphs) that\nsparse. Our algorithm can also be implemented in linear time in the standard\ncentralized model, and for large $k$, it provides spanners that are sparser\nthan any other spanner given by a known (near-)linear time algorithm.\n  We also devise improved bounds (and algorithms realizing these bounds) for\n$(1+\\epsilon,\\beta)$-spanners and emulators. In particular, we show that for\nany unweighted $n$-vertex graph and any $\\epsilon > 0$, there exists a $(1+\n\\epsilon, ({{\\log\\log n} \\over \\epsilon})^{\\log\\log n})$-emulator with $O(n)$\nedges. All previous constructions of $(1+\\epsilon,\\beta)$-spanners and\nemulators employ a superlinear number of edges, for all choices of parameters.\n  Finally, we provide some applications of our results to approximate shortest\npaths' computation in unweighted graphs. \n\n"}
{"id": "1608.00196", "contents": "Title: An Approximation Algorithm for Maximum Internal Spanning Tree Abstract: Given a graph G, the {\\em maximum internal spanning tree problem} (MIST for\nshort) asks for computing a spanning tree T of G such that the number of\ninternal vertices in T is maximized. MIST has possible applications in the\ndesign of cost-efficient communication networks and water supply networks and\nhence has been extensively studied in the literature. MIST is NP-hard and hence\na number of polynomial-time approximation algorithms have been designed for\nMIST in the literature. The previously best polynomial-time approximation\nalgorithm for MIST achieves a ratio of 3/4. In this paper, we first design a\nsimpler algorithm that achieves the same ratio and the same time complexity as\nthe previous best. We then refine the algorithm into a new approximation\nalgorithm that achieves a better ratio (namely, 13/17) with the same time\ncomplexity. Our new algorithm explores much deeper structure of the problem\nthan the previous best. The discovered structure may be used to design even\nbetter approximation or parameterized algorithms for the problem in the future. \n\n"}
{"id": "1608.00497", "contents": "Title: From Weak to Strong LP Gaps for all CSPs Abstract: We study the approximability of constraint satisfaction problems (CSPs) by\nlinear programming (LP) relaxations. We show that for every CSP, the\napproximation obtained by a basic LP relaxation, is no weaker than the\napproximation obtained using relaxations given by $\\Omega\\left(\\frac{\\log\nn}{\\log \\log n}\\right)$ levels of the Sherali-Adams hierarchy on instances of\nsize $n$.\n  It was proved by Chan et al. [FOCS 2013] that any polynomial size LP extended\nformulation is no stronger than relaxations obtained by a super-constant levels\nof the Sherali-Adams hierarchy.. Combining this with our result also implies\nthat any polynomial size LP extended formulation is no stronger than the basic\nLP.\n  Using our techniques, we also simplify and strengthen the result by Khot et\nal. [STOC 2014] on (strong) approximation resistance for LPs. They provided a\nnecessary and sufficient condition under which $\\Omega(\\log \\log n)$ levels of\nthe Sherali-Adams hierarchy cannot achieve an approximation better than a\nrandom assignment. We simplify their proof and strengthen the bound to\n$\\Omega\\left(\\frac{\\log n}{\\log \\log n}\\right)$ levels. \n\n"}
{"id": "1608.02530", "contents": "Title: Border Ranks of Monomials Abstract: Young flattenings, introduced by Landsberg and Ottaviani, give determinantal\nequations for secant varieties and their non-vanishing provides lower bounds\nfor border ranks of tensors and in particular polynomials. We study\nmonomial-optimal shapes for Young flattenings, which exhibit the limits of the\nYoung flattening method. In particular, they provide the best possible lower\nbound for large classes of monomials including all monomials up to degree 6,\nmonomials in 3 variables, and any power of the product of variables. On the\nother hand, for degree 7 and higher there are monomials for which no Young\nflattening can give a lower bound that matches the conjecturally tight upper\nbound of Landsberg and Teitler. \n\n"}
{"id": "1608.02775", "contents": "Title: On sets with few distinct distances Abstract: It is widely believed that point sets in the plane which determine few\ndistinct distances must have some special structure. In particular, such sets\nare believed to be similar to a lattice. This note considers two different ways\nto quantify this idea.\n  Firstly, improving on a result of Hanson (see arXiv:1607.03442), it is proven\nthat if $P= A \\times A$ with $A \\subset \\mathbb R$ and $P$ determines\n$O(|A|^2)$ distinct distances, then $|A-A|=O\\left(|A|^{2-\\frac{2}{11}}\\right)$.\nThis result gives further evidence that cartesian products which determine few\ndistinct distances have some additive structure.\n  Secondly, it is shown that if a set $P \\subset \\mathbb R^2$ of $N$ points\ndetermines $O(N/\\sqrt {\\log N})$ distinct distances, then there exists a\nreflection $\\mathcal R$ and a set $P' \\subset P$ with $|P'| =\\Omega (\n\\log^{3/2} N)$ such that $\\mathcal R(P') \\subset P$. In other words, sets with\nfew distinct distances have some degree of reflexive symmetry. \n\n"}
{"id": "1608.03245", "contents": "Title: On the Complexity of Closest Pair via Polar-Pair of Point-Sets Abstract: Every graph $G$ can be represented by a collection of equi-radii spheres in a\n$d$-dimensional metric $\\Delta$ such that there is an edge $uv$ in $G$ if and\nonly if the spheres corresponding to $u$ and $v$ intersect. The smallest\ninteger $d$ such that $G$ can be represented by a collection of spheres (all of\nthe same radius) in $\\Delta$ is called the sphericity of $G$, and if the\ncollection of spheres are non-overlapping, then the value $d$ is called the\ncontact-dimension of $G$. In this paper, we study the sphericity and contact\ndimension of the complete bipartite graph $K_{n,n}$ in various $L^p$-metrics\nand consequently connect the complexity of the monochromatic closest pair and\nbichromatic closest pair problems. \n\n"}
{"id": "1608.04639", "contents": "Title: Arrangements of homothets of a convex body Abstract: Answering a question of F\\\"uredi and Loeb (1994), we show that the maximum\nnumber of pairwise intersecting homothets of a $d$-dimensional centrally\nsymmetric convex body $K$, none of which contains the center of another in its\ninterior, is at most $O(3^d d\\log d)$. If $K$ is not necessarily centrally\nsymmetric and the role of its center is played by its centroid, then the above\nbound can be replaced by $O(3^d\\binom{2d}{d}d\\log d)$. We establish analogous\nresults for the case where the center is defined as an arbitrary point in the\ninterior of $K$. We also show that in the latter case, one can always find\nfamilies of at least $\\Omega((2/\\sqrt{3})^d)$ translates of $K$ with the above\nproperty. \n\n"}
{"id": "1608.07568", "contents": "Title: Graphic TSP in cubic graphs Abstract: We present a polynomial-time 9/7-approximation algorithm for the graphic TSP\nfor cubic graphs, which improves the previously best approximation factor of\n1.3 for 2-connected cubic graphs and drops the requirement of 2-connectivity at\nthe same time. To design our algorithm, we prove that every simple 2-connected\ncubic n-vertex graph contains a spanning closed walk of length at most 9n/7-1,\nand that such a walk can be found in polynomial time. \n\n"}
{"id": "1608.07647", "contents": "Title: Elementary polytopes with high lift-and-project ranks for strong\n  positive semidefinite operators Abstract: We consider operators acting on convex subsets of the unit hypercube. These\noperators are used in constructing convex relaxations of combinatorial\noptimization problems presented as a 0,1 integer programming problem or a 0,1\npolynomial optimization problem. Our focus is mostly on operators that, when\nexpressed as a lift-and-project operator, involve the use of semidefiniteness\nconstraints in the lifted space, including operators due to Lasserre and\nvariants of the Sherali--Adams and Bienstock--Zuckerberg operators. We study\nthe performance of these semidefinite-optimization-based lift-and-project\noperators on some elementary polytopes --- hypercubes that are chipped (at\nleast one vertex of the hypercube removed by intersection with a closed\nhalfspace) or cropped (all $2^n$ vertices of the hypercube removed by\nintersection with $2^n$ closed halfspaces) to varying degrees of severity\n$\\rho$. We prove bounds on $\\rho$ where these operators would perform badly on\nthe aforementioned examples. We also show that the integrality gap of the\nchipped hypercube is invariant under the application of several\nlift-and-project operators of varying strengths. \n\n"}
{"id": "1608.08545", "contents": "Title: Dynamic Controllability of Conditional Simple Temporal Networks is\n  PSPACE-complete Abstract: Even after the proposal of various solution algorithms, the precise\ncomputational complexity of checking whether a Conditional Temporal Network is\nDynamically Controllable had still remained widely open. This issue gets\nsettled in this paper which provides constructions, algorithms, and bridging\nlemmas and arguments to formally prove that: (1) the problem is PSPACE-hard,\nand (2) the problem lies in PSPACE. \n\n"}
{"id": "1609.00161", "contents": "Title: Parallel Clustering of Graphs for Anonymization and Recommender Systems Abstract: Graph clustering is widely used in many data analysis applications. In this\npaper we propose several parallel graph clustering algorithms based on Monte\nCarlo simulations and expectation maximization in the context of stochastic\nblock models. We apply those algorithms to the specific problems of recommender\nsystems and social network anonymization. We compare the experimental results\nto previous propositions. \n\n"}
{"id": "1609.00750", "contents": "Title: Predicting Signed Edges with $O(n^{1+o(1)} \\log{n})$ Queries Abstract: Social networks and interactions in social media involve both positive and\nnegative relationships. Signed graphs capture both types of relationships:\npositive edges correspond to pairs of \"friends\", and negative edges to pairs of\n\"foes\". The {\\em edge sign prediction problem}, which aims to predict whether\nan interaction between a pair of nodes will be positive or negative, is an\nimportant graph mining task for which many heuristics have recently been\nproposed \\cite{leskovec2010predicting,leskovec2010signed}.\n  Motivated by social balance theory, we model the edge sign prediction problem\nas a noisy correlation clustering problem with two clusters. We are allowed to\nquery each pair of nodes whether they belong to the same cluster or not, but\nthe answer to the query is corrupted with some probability $0<q<\\frac{1}{2}$.\nLet $c=\\frac{1}{2}-q$ be the gap. We provide an algorithm that recovers the\nclustering with high probability in the presence of noise for any constant gap\n$c$ with $O(n^{1+\\tfrac{1}{\\log\\log{n}}}\\log{n})$ queries. Our algorithm uses\nsimple breadth first search as its main algorithmic primitive. Finally, we\nprovide a novel generalization to $k \\geq 3$ clusters and prove that our\ntechniques can recover the clustering if the gap is constant in this\ngeneralized setting. \n\n"}
{"id": "1609.03650", "contents": "Title: Extreme Quantum Advantage when Simulating Strongly Coupled Classical\n  Systems Abstract: Classical stochastic processes can be generated by quantum simulators instead\nof the more standard classical ones, such as hidden Markov models. One reason\nfor using quantum simulators is that they generally require less memory than\ntheir classical counterparts. Here, we examine this quantum advantage for\nstrongly coupled spin systems---the Dyson-like one-dimensional Ising spin chain\nwith variable interaction length. We find that the advantage scales with both\ninteraction range and temperature, growing without bound as interaction\nincreases. Thus, quantum systems can very efficiently simulate strongly coupled\nclassical systems. \n\n"}
{"id": "1609.03769", "contents": "Title: Analysis of Kelner and Levin graph sparsification algorithm for a\n  streaming setting Abstract: We derive a new proof to show that the incremental resparsification algorithm\nproposed by Kelner and Levin (2013) produces a spectral sparsifier in high\nprobability. We rigorously take into account the dependencies across subsequent\nresparsifications using martingale inequalities, fixing a flaw in the original\nanalysis. \n\n"}
{"id": "1609.04618", "contents": "Title: From H&M to Gap for Lightweight BWT Merging Abstract: Recently, Holt and McMillan [Bionformatics 2014, ACM-BCB 2014] have proposed\na simple and elegant algorithm to merge the Burrows-Wheeler transforms of a\nfamily of strings. In this paper we show that the H&M algorithm can be improved\nso that, in addition to merging the BWTs, it can also merge the Longest Common\nPrefix (LCP) arrays. The new algorithm, called Gap because of how it operates,\nhas the same asymptotic cost as the H&M algorithm and requires additional space\nonly for storing the LCP values. \n\n"}
{"id": "1609.05885", "contents": "Title: Matrix Norms in Data Streams: Faster, Multi-Pass and Row-Order Abstract: A central problem in data streams is to characterize which functions of an\nunderlying frequency vector can be approximated efficiently. Recently there has\nbeen considerable effort in extending this problem to that of estimating\nfunctions of a matrix that is presented as a data-stream. This setting\ngeneralizes classical problems to the analogous ones for matrices. For example,\ninstead of estimating frequent-item counts, we now wish to estimate\n\"frequent-direction\" counts. A related example is to estimate norms, which now\ncorrespond to estimating a vector norm on the singular values of the matrix.\nDespite recent efforts, the current understanding for such matrix problems is\nconsiderably weaker than that for vector problems.\n  We study a number of aspects of estimating matrix norms in a stream that have\nnot previously been considered: (1) multi-pass algorithms, (2) algorithms that\nsee the underlying matrix one row at a time, and (3) time-efficient algorithms.\nOur multi-pass and row-order algorithms use less memory than what is provably\nrequired in the single-pass and entrywise-update models, and thus give\nseparations between these models (in terms of memory). Moreover, all of our\nalgorithms are considerably faster than previous ones. We also prove a number\nof lower bounds, and obtain for instance, a near-complete characterization of\nthe memory required of row-order algorithms for estimating Schatten $p$-norms\nof sparse matrices. \n\n"}
{"id": "1609.06736", "contents": "Title: Improving and extending the testing of distributions for\n  shape-restricted properties Abstract: Distribution testing deals with what information can be deduced about an\nunknown distribution over $\\{1,\\ldots,n\\}$, where the algorithm is only allowed\nto obtain a relatively small number of independent samples from the\ndistribution. In the extended conditional sampling model, the algorithm is also\nallowed to obtain samples from the restriction of the original distribution on\nsubsets of $\\{1,\\ldots,n\\}$.\n  In 2015, Canonne, Diakonikolas, Gouleakis and Rubinfeld unified several\nprevious results, and showed that for any property of distributions satisfying\na \"decomposability\" criterion, there exists an algorithm (in the basic model)\nthat can distinguish with high probability distributions satisfying the\nproperty from distributions that are far from it in the variation distance.\n  We present here a more efficient yet simpler algorithm for the basic model,\nas well as very efficient algorithms for the conditional model, which until now\nwas not investigated under the umbrella of decomposable properties.\nAdditionally, we provide an algorithm for the conditional model that handles a\nmuch larger class of properties.\n  Our core mechanism is a way of efficiently producing an interval-partition of\n$\\{1,\\ldots,n\\}$ that satisfies a \"fine-grain\" quality. We show that with such\na partition at hand we can directly move forward with testing individual\nintervals, instead of first searching for the \"correct\" partition of\n$\\{1,\\ldots,n\\}$. \n\n"}
{"id": "1609.08253", "contents": "Title: On the Group and Color Isomorphism Problems Abstract: In this paper, we prove results on the relationship between the complexity of\nthe group and color isomorphism problems. The difficulty of color isomorphism\nproblems is known to be closely linked to the the composition factors of the\npermutation group involved. Previous works are primarily concerned with\napplying color isomorphism to bou nded degree graph isomorphism, and have\ntherefore focused on the alternating composit ion factors, since those are the\nbottleneck in the case of graph isomorphism.\n  We consider the color isomorphism problem with composition factors restricted\nto those other than the alternating group, show that group isomorphism reduces\nin n^(O(log log n)) time to this problem, and, conversely, that a special case\nof this color isomorphism problem reduces to a slight generalization of group\nisomorphism. We then sharpen our results by identifying the projective special\nlinear group as the main obstacle to faster algorithms for group isomorphism\nand prove that the aforementioned reduc tion from group isomorphism to color\nisomorphism in fact produces only cyclic and projective special linear factors.\nOur results demonstrate that, just as the alternatin g group was a barrier to\nfaster algorithms for graph isomorphism for three decades, the projective\nspecial linear group is an obstacle to faster algorithms for group isomorphism. \n\n"}
{"id": "1610.01185", "contents": "Title: Recursion-Theoretic Ranking and Compression Abstract: For which sets A does there exist a mapping, computed by a total or partial\nrecursive function, such that the mapping, when its domain is restricted to A,\nis a 1-to-1, onto mapping to $\\Sigma^*$? And for which sets A does there exist\nsuch a mapping that respects the lexicographical ordering within A? Both cases\nare types of perfect, minimal hash functions. The complexity-theoretic versions\nof these notions are known as compression functions and ranking functions. The\npresent paper defines and studies the recursion-theoretic versions of\ncompression and ranking functions, and in particular studies the question of\nwhich sets have, or lack, such functions. Thus, this is a case where, in\ncontrast to the usual direction of notion transferal, notions from complexity\ntheory are inspiring notions, and an investigation, in computability theory.\n  We show that the rankable and compressible sets broadly populate the\n1-truth-table degrees, and we prove that every nonempty coRE cylinder is\nrecursively compressible. \n\n"}
{"id": "1610.01980", "contents": "Title: Polynomial-time Tensor Decompositions with Sum-of-Squares Abstract: We give new algorithms based on the sum-of-squares method for tensor\ndecomposition. Our results improve the best known running times from\nquasi-polynomial to polynomial for several problems, including decomposing\nrandom overcomplete 3-tensors and learning overcomplete dictionaries with\nconstant relative sparsity. We also give the first robust analysis for\ndecomposing overcomplete 4-tensors in the smoothed analysis model. A key\ningredient of our analysis is to establish small spectral gaps in moment\nmatrices derived from solutions to sum-of-squares relaxations. To enable this\nanalysis we augment sum-of-squares relaxations with spectral analogs of maximum\nentropy constraints. \n\n"}
{"id": "1610.02361", "contents": "Title: Energy-efficient Delivery by Heterogeneous Mobile Agents Abstract: We consider the problem of delivering $m$ messages between specified\nsource-target pairs in a weighted undirected graph, by $k$ mobile agents\ninitially located at distinct nodes of the graph. Each agent consumes energy\nproportional to the distance it travels in the graph and we are interested in\noptimizing the total energy consumption for the team of agents. Unlike previous\nrelated work, we consider heterogeneous agents with different rates of energy\nconsumption (weights~$w_i$). To solve the delivery problem, agents face three\nmajor challenges: \\emph{Collaboration} (how to work together on each message),\n\\emph{Planning} (which route to take) and \\emph{Coordination} (how to assign\nagents to messages).\n  We first show that the delivery problem can be 2-approximated \\emph{without}\ncollaborating and that this is best possible, i.e., we show that the\n\\emph{benefit of collaboration} is 2 in general. We also show that the benefit\nof collaboration for a single message is~$1/\\ln 2 \\approx 1.44$. Planning turns\nout to be \\NP-hard to approximate even for a single agent, but can be\n2-approximated in polynomial time if agents have unit capacities and do not\ncollaborate. We further show that coordination is \\NP-hard even for agents with\nunit capacity, but can be efficiently solved exactly if they have uniform\nweights. Finally, we give a polynomial-time\n$(4\\max\\tfrac{w_i}{w_j})$-approximation for message delivery with unit\ncapacities. \n\n"}
{"id": "1610.04430", "contents": "Title: Improved approximation for two dimensional strip packing with polynomial\n  bounded width Abstract: We study the well-known two-dimensional strip packing problem. Given is a set\nof rectangular axis-parallel items and a strip of width $W$ with infinite\nheight. The objective is to find a packing of these items into the strip, which\nminimizes the packing height. Lately, it has been shown that the lower bound of\n$3/2$ of the absolute approximation ratio can be beaten when we allow a\npseudo-polynomial running-time of type $(n W)^{f(1/\\varepsilon)}$. If $W$ is\npolynomially bounded by the number of items, this is a polynomial running-time.\nWe present a pseudo-polynomial algorithm with approximation ratio $4/3\n+\\varepsilon$ and running time $(n\nW)^{1/\\varepsilon^{\\mathcal{O}(2^{1/\\varepsilon})}}$. \n\n"}
{"id": "1610.05350", "contents": "Title: How Well Do Local Algorithms Solve Semidefinite Programs? Abstract: Several probabilistic models from high-dimensional statistics and machine\nlearning reveal an intriguing --and yet poorly understood-- dichotomy. Either\nsimple local algorithms succeed in estimating the object of interest, or even\nsophisticated semi-definite programming (SDP) relaxations fail.\n  In order to explore this phenomenon, we study a classical SDP relaxation of\nthe minimum graph bisection problem, when applied to Erd\\H{o}s-Renyi random\ngraphs with bounded average degree $d>1$, and obtain several types of results.\nFirst, we use a dual witness construction (using the so-called non-backtracking\nmatrix of the graph) to upper bound the SDP value. Second, we prove that a\nsimple local algorithm approximately solves the SDP to within a factor\n$2d^2/(2d^2+d-1)$ of the upper bound. In particular, the local algorithm is at\nmost $8/9$ suboptimal, and $1+O(1/d)$ suboptimal for large degree.\n  We then analyze a more sophisticated local algorithm, which aggregates\ninformation according to the harmonic measure on the limiting Galton-Watson\n(GW) tree. The resulting lower bound is expressed in terms of the conductance\nof the GW tree and matches surprisingly well the empirically determined SDP\nvalues on large-scale Erd\\H{o}s-Renyi graphs.\n  We finally consider the planted partition model. In this case, purely local\nalgorithms are known to fail, but they do succeed if a small amount of side\ninformation is available. Our results imply quantitative bounds on the\nthreshold for partial recovery using SDP in this model. \n\n"}
{"id": "1610.05656", "contents": "Title: Asymptotic expansions for moments of number of comparisons used by the\n  randomized quick sort algorithm Abstract: We calculate asymptotic expansions for the moments of number of comparisons\nused by the randomized quick sort algorithm using the singularity analysis of\ncertain generating functions. \n\n"}
{"id": "1610.05710", "contents": "Title: Feasibility Based Large Margin Nearest Neighbor Metric Learning Abstract: Large margin nearest neighbor (LMNN) is a metric learner which optimizes the\nperformance of the popular $k$NN classifier. However, its resulting metric\nrelies on pre-selected target neighbors. In this paper, we address the\nfeasibility of LMNN's optimization constraints regarding these target points,\nand introduce a mathematical measure to evaluate the size of the feasible\nregion of the optimization problem. We enhance the optimization framework of\nLMNN by a weighting scheme which prefers data triplets which yield a larger\nfeasible region. This increases the chances to obtain a good metric as the\nsolution of LMNN's problem. We evaluate the performance of the resulting\nfeasibility-based LMNN algorithm using synthetic and real datasets. The\nempirical results show an improved accuracy for different types of datasets in\ncomparison to regular LMNN. \n\n"}
{"id": "1611.00889", "contents": "Title: Designing Sparse Reliable Pose-Graph SLAM: A Graph-Theoretic Approach Abstract: In this paper, we aim to design sparse D-optimal (determinantoptimal)\npose-graph SLAM problems through the synthesis of sparse graphs with the\nmaximum weighted number of spanning trees. Characterizing graphs with the\nmaximum number of spanning trees is an open problem in general. To tackle this\nproblem, several new theoretical results are established in this paper,\nincluding the monotone log-submodularity of the weighted number of spanning\ntrees. By exploiting these structures, we design a complementary pair of\nnear-optimal efficient approximation algorithms with provable guarantees. Our\ntheoretical results are validated using random graphs and a publicly available\npose-graph SLAM dataset. \n\n"}
{"id": "1611.01403", "contents": "Title: Searching Trees with Permanently Noisy Advice: Walking and Query\n  Algorithms Abstract: We consider a search problem on trees in which the goal is to find an\nadversarially placed treasure, while relying on local, partial information.\nSpecifically, each node in the tree holds a pointer to one of its neighbors,\ntermed \\emph{advice}. A node is faulty with probability $q$. The advice at a\nnon-faulty node points to the neighbor that is closer to the treasure, and the\nadvice at a faulty node points to a uniformly random neighbor. Crucially, the\nadvice is {\\em permanent}, in the sense that querying the same node again would\nyield the same answer. Let $\\Delta$ denote the maximal degree. Roughly\nspeaking, when considering the expected number of {\\em moves}, i.e., edge\ntraversals, we show that a phase transition occurs when the {\\em noise\nparameter} $q$ is about $1/\\sqrt{\\Delta}$. Below the threshold, there exists an\nalgorithm with expected move complexity $O(D\\sqrt{\\Delta})$, where $D$ is the\ndepth of the treasure, whereas above the threshold, every search algorithm has\nexpected number of moves which is both exponential in $D$ and polynomial in the\nnumber of nodes~$n$. In contrast, if we require to find the treasure with\nprobability at least $1-\\delta$, then for every fixed $\\varepsilon > 0$, if\n$q<1/\\Delta^{\\varepsilon}$ then there exists a search strategy that with\nprobability $1-\\delta$ finds the treasure using $(\\delta^{-1}D)^{O(\\frac 1\n\\varepsilon)}$ moves. Moreover, we show that $(\\delta^{-1}D)^{\\Omega(\\frac 1\n\\varepsilon)}$ moves are necessary. Besides the number of moves, we also study\nthe number of advice {\\em queries} required to find the treasure. Roughly\nspeaking, for this complexity, we show similar threshold results to those\npreviously stated, where the parameter $D$ is replaced by $\\log n$. \n\n"}
{"id": "1611.01491", "contents": "Title: Understanding Deep Neural Networks with Rectified Linear Units Abstract: In this paper we investigate the family of functions representable by deep\nneural networks (DNN) with rectified linear units (ReLU). We give an algorithm\nto train a ReLU DNN with one hidden layer to *global optimality* with runtime\npolynomial in the data size albeit exponential in the input dimension. Further,\nwe improve on the known lower bounds on size (from exponential to super\nexponential) for approximating a ReLU deep net function by a shallower ReLU\nnet. Our gap theorems hold for smoothly parametrized families of \"hard\"\nfunctions, contrary to countable, discrete families known in the literature. An\nexample consequence of our gap theorems is the following: for every natural\nnumber $k$ there exists a function representable by a ReLU DNN with $k^2$\nhidden layers and total size $k^3$, such that any ReLU DNN with at most $k$\nhidden layers will require at least $\\frac{1}{2}k^{k+1}-1$ total nodes.\nFinally, for the family of $\\mathbb{R}^n\\to \\mathbb{R}$ DNNs with ReLU\nactivations, we show a new lowerbound on the number of affine pieces, which is\nlarger than previous constructions in certain regimes of the network\narchitecture and most distinctively our lowerbound is demonstrated by an\nexplicit construction of a *smoothly parameterized* family of functions\nattaining this scaling. Our construction utilizes the theory of zonotopes from\npolyhedral theory. \n\n"}
{"id": "1611.01805", "contents": "Title: Algorithmic Discrepancy Beyond Partial Coloring Abstract: The partial coloring method is one of the most powerful and widely used\nmethod in combinatorial discrepancy problems. However, in many cases it leads\nto sub-optimal bounds as the partial coloring step must be iterated a\nlogarithmic number of times, and the errors can add up in an adversarial way.\nWe give a new and general algorithmic framework that overcomes the limitations\nof the partial coloring method and can be applied in a black-box manner to\nvarious problems. Using this framework, we give new improved bounds and\nalgorithms for several classic problems in discrepancy. In particular, for\nTusnady's problem, we give an improved $O(\\log^2 n)$ bound for discrepancy of\naxis-parallel rectangles and more generally an $O_d(\\log^dn)$ bound for\n$d$-dimensional boxes in $\\mathbb{R}^d$. Previously, even non-constructively,\nthe best bounds were $O(\\log^{2.5} n)$ and $O_d(\\log^{d+0.5}n)$ respectively.\nSimilarly, for the Steinitz problem we give the first algorithm that matches\nthe best known non-constructive bounds due to Banaszczyk [Banaszczyk 2012] in\nthe $\\ell_\\infty$ case, and improves the previous algorithmic bounds\nsubstantially in the $\\ell_2$ case. Our framework is based upon a substantial\ngeneralization of the techniques developed recently in the context of the\nKoml\\'os discrepancy problem [BDG16]. \n\n"}
{"id": "1611.03251", "contents": "Title: Helly-type theorem for eigenvectors Abstract: We prove that if any $\\lfloor3d/2 \\rfloor$ or fewer elements of a finite\nfamily of linear operators $\\mathbb K^d\\to \\mathbb K^d$ ($\\mathbb K$ is an\narbitrary field) have a common eigenvector then all operators in the family\nhave a common eigenvector. Moreover, $\\lfloor 3d/2\\rfloor$ cannot be replaced\nby a smaller number. Also, we study the following problem, achieving partial\nresults: prove that if any $l=O(d)$ or fewer elements of a finite family of\nlinear operators $\\mathbb K^d\\to \\mathbb K^d$ have a common non-trivial\ninvariant subspace then all operators in the family have a common non-trivial\ninvariant subspace. \n\n"}
{"id": "1611.03473", "contents": "Title: Statistical Query Lower Bounds for Robust Estimation of High-dimensional\n  Gaussians and Gaussian Mixtures Abstract: We describe a general technique that yields the first {\\em Statistical Query\nlower bounds} for a range of fundamental high-dimensional learning problems\ninvolving Gaussian distributions. Our main results are for the problems of (1)\nlearning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of\na single unknown Gaussian distribution. For each of these problems, we show a\n{\\em super-polynomial gap} between the (information-theoretic) sample\ncomplexity and the computational complexity of {\\em any} Statistical Query\nalgorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively\nmatched by known learning algorithms for GMMs. Our lower bound for Problem (2)\nimplies that the accuracy of the robust learning algorithm\nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all\npolynomial-time SQ algorithms.\n  Our SQ lower bounds are attained via a unified moment-matching technique that\nis useful in other contexts and may be of broader interest. Our technique\nyields nearly-tight lower bounds for a number of related unsupervised\nestimation problems. Specifically, for the problems of (3) robust covariance\nestimation in spectral norm, and (4) robust sparse mean estimation, we\nestablish a quadratic {\\em statistical--computational tradeoff} for SQ\nalgorithms, matching known upper bounds. Finally, our technique can be used to\nobtain tight sample complexity lower bounds for high-dimensional {\\em testing}\nproblems. Specifically, for the classical problem of robustly {\\em testing} an\nunknown mean (known covariance) Gaussian, our technique implies an\ninformation-theoretic sample lower bound that scales {\\em linearly} in the\ndimension. Our sample lower bound matches the sample complexity of the\ncorresponding robust {\\em learning} problem and separates the sample complexity\nof robust testing from standard (non-robust) testing. \n\n"}
{"id": "1611.03819", "contents": "Title: Recovery Guarantee of Non-negative Matrix Factorization via Alternating\n  Updates Abstract: Non-negative matrix factorization is a popular tool for decomposing data into\nfeature and weight matrices under non-negativity constraints. It enjoys\npractical success but is poorly understood theoretically. This paper proposes\nan algorithm that alternates between decoding the weights and updating the\nfeatures, and shows that assuming a generative model of the data, it provably\nrecovers the ground-truth under fairly mild conditions. In particular, its only\nessential requirement on features is linear independence. Furthermore, the\nalgorithm uses ReLU to exploit the non-negativity for decoding the weights, and\nthus can tolerate adversarial noise that can potentially be as large as the\nsignal, and can tolerate unbiased noise much larger than the signal. The\nanalysis relies on a carefully designed coupling between two potential\nfunctions, which we believe is of independent interest. \n\n"}
{"id": "1611.06104", "contents": "Title: Spectrahedrality of hyperbolicity cones of multivariate matching\n  polynomials Abstract: The generalized Lax conjecture asserts that each hyperbolicity cone is a\nlinear slice of the cone of positive semidefinite matrices. We prove the\nconjecture for a multivariate generalization of the matching polynomial. This\nis further extended (albeit in a weaker sense) to a multivariate version of the\nindependence polynomial for simplicial graphs. As an application we give a new\nproof of the conjecture for elementary symmetric polynomials (originally due to\nBr\\\"and\\'en). Finally we consider a hyperbolic convolution of determinant\npolynomials generalizing an identity of Godsil and Gutman. \n\n"}
{"id": "1611.06650", "contents": "Title: Trading information complexity for error Abstract: We consider the standard two-party communication model. The central problem\nstudied in this article is how much one can save in information complexity by\nallowing an error of $\\epsilon$.\n  For arbitrary functions, we obtain lower bounds and upper bounds indicating a\ngain that is of order $\\Omega(h(\\epsilon))$ and $O(h(\\sqrt{\\epsilon}))$. Here\n$h$ denotes the binary entropy function. We analyze the case of the two-bit AND\nfunction in detail to show that for this function the gain is\n$\\Theta(h(\\epsilon))$. This answers a question of [M. Braverman, A. Garg, D.\nPankratov, and O. Weinstein, From information to exact communication (extended\nabstract), STOC'13].\n  We obtain sharp bounds for the set disjointness function of order $n$. For\nthe case of the distributional error, we introduce a new protocol that achieves\na gain of $\\Theta(\\sqrt{h(\\epsilon)})$ provided that $n$ is sufficiently large.\nWe apply these results to answer another of question of Braverman et al.\nregarding the randomized communication complexity of the set disjointness\nfunction.\n  Answering a question of [Mark Braverman, Interactive information complexity,\nSTOC'12], we apply our analysis of the set disjointness function to establish a\ngap between the two different notions of the prior-free information cost. This\nimplies that amortized randomized communication complexity is not necessarily\nequal to the amortized distributional communication complexity with respect to\nthe hardest distribution. \n\n"}
{"id": "1611.06795", "contents": "Title: A randomized polynomial kernelization for Vertex Cover with a smaller\n  parameter Abstract: In the Vertex Cover problem we are given a graph $G=(V,E)$ and an integer $k$\nand have to determine whether there is a set $X\\subseteq V$ of size at most $k$\nsuch that each edge in $E$ has at least one endpoint in $X$. The problem can be\neasily solved in time $O^*(2^k)$, making it fixed-parameter tractable (FPT)\nwith respect to $k$. While the fastest known algorithm takes only time\n$O^*(1.2738^k)$, much stronger improvements have been obtained by studying\nparameters that are smaller than $k$. Apart from treewidth-related results, the\narguably best algorithm for Vertex Cover runs in time $O^*(2.3146^p)$, where\n$p=k-LP(G)$ is only the excess of the solution size $k$ over the best\nfractional vertex cover (Lokshtanov et al.\\ TALG 2014). Since $p\\leq k$ but $k$\ncannot be bounded in terms of $p$ alone, this strictly increases the range of\ntractable instances.\n  Recently, Garg and Philip (SODA 2016) greatly contributed to understanding\nthe parameterized complexity of the Vertex Cover problem. They prove that\n$2LP(G)-MM(G)$ is a lower bound for the vertex cover size of $G$, where $MM(G)$\nis the size of a largest matching of $G$, and proceed to study parameter\n$\\ell=k-(2LP(G)-MM(G))$. They give an algorithm of running time $O^*(3^\\ell)$,\nproving that Vertex Cover is FPT in $\\ell$. It can be easily observed that\n$\\ell\\leq p$ whereas $p$ cannot be bounded in terms of $\\ell$ alone. We\ncomplement the work of Garg and Philip by proving that Vertex Cover admits a\nrandomized polynomial kernelization in terms of $\\ell$, i.e., an efficient\npreprocessing to size polynomial in $\\ell$. This improves over parameter\n$p=k-LP(G)$ for which this was previously known (Kratsch and Wahlstr\\\"om FOCS\n2012). \n\n"}
{"id": "1611.07489", "contents": "Title: Approximating k-Forest with Resource Augmentation: A Primal-Dual\n  Approach Abstract: In this paper, we study the $k$-forest problem in the model of resource\naugmentation. In the $k$-forest problem, given an edge-weighted graph $G(V,E)$,\na parameter $k$, and a set of $m$ demand pairs $\\subseteq V \\times V$, the\nobjective is to construct a minimum-cost subgraph that connects at least $k$\ndemands. The problem is hard to approximate---the best-known approximation\nratio is $O(\\min\\{\\sqrt{n}, \\sqrt{k}\\})$. Furthermore, $k$-forest is as hard to\napproximate as the notoriously-hard densest $k$-subgraph problem.\n  While the $k$-forest problem is hard to approximate in the worst-case, we\nshow that with the use of resource augmentation, we can efficiently approximate\nit up to a constant factor.\n  First, we restate the problem in terms of the number of demands that are {\\em\nnot} connected. In particular, the objective of the $k$-forest problem can be\nviewed as to remove at most $m-k$ demands and find a minimum-cost subgraph that\nconnects the remaining demands. We use this perspective of the problem to\nexplain the performance of our algorithm (in terms of the augmentation) in a\nmore intuitive way.\n  Specifically, we present a polynomial-time algorithm for the $k$-forest\nproblem that, for every $\\epsilon>0$, removes at most $m-k$ demands and has\ncost no more than $O(1/\\epsilon^{2})$ times the cost of an optimal algorithm\nthat removes at most $(1-\\epsilon)(m-k)$ demands. \n\n"}
{"id": "1611.07682", "contents": "Title: Special cases of the quadratic shortest path problem Abstract: The quadratic shortest path problem (QSPP) is \\textcolor{black}{the problem\nof finding a path with prespecified start vertex $s$ and end vertex $t$ in a\ndigraph} such that the sum of weights of arcs and the sum of interaction costs\nover all pairs of arcs on the path is minimized. We first consider a variant of\nthe QSPP known as the adjacent QSPP. It was recently proven that the adjacent\nQSPP on cyclic digraphs cannot be approximated unless P=NP. Here, we give a\nsimple proof for the same result.\n  We also show that if the quadratic cost matrix is a symmetric weak sum matrix\n\\textcolor{black}{ and all $s$-$t$ paths have the same length,} then an optimal\nsolution for the QSPP can be obtained by solving the corresponding instance of\nthe shortest path problem. Similarly, it is shown that the QSPP with a\nsymmetric product cost matrix is solvable in polynomial time.\n  Further, we provide sufficient and necessary conditions for a QSPP instance\non a complete symmetric digraph with four vertices to be linearizable. We also\ncharacterize linearizable QSPP instances on complete symmetric digraphs with\nmore than four vertices. Finally, we derive an algorithm that examines whether\na QSPP instance on the directed grid graph $G_{pq}$ ($p,q\\geq 2$) is\nlinearizable. The complexity of this algorithm is\n${\\mathcal{O}(p^{3}q^{2}+p^{2}q^{3})}$. \n\n"}
{"id": "1611.08326", "contents": "Title: Detecting communities is hard, and counting them is even harder Abstract: We consider the algorithmic problem of community detection in networks. Given\nan undirected friendship graph $G=\\left(V,E\\right)$, a subset $S\\subseteq V$ is\nan $\\left(\\alpha,\\beta\\right)$-community if:\n  * Every member of the community is friends with an $\\alpha$-fraction of the\ncommunity;\n  * Every non-member is friends with at most a $\\beta$-fraction of the\ncommunity.\n  Arora et al [AGSS12] gave a quasi-polynomial time algorithm for enumerating\nall the $\\left(\\alpha,\\beta\\right)$-communities for any constants\n$\\alpha>\\beta$.\n  Here, we prove that, assuming the Exponential Time Hypothesis (ETH),\nquasi-polynomial time is in fact necessary - and even for a much weaker\napproximation desideratum. Namely, distinguishing between:\n  * $G$ contains an $\\left(1,o\\left(1\\right)\\right)$-community; and\n  * $G$ does not contain an\n$\\left(\\beta+o\\left(1\\right),\\beta\\right)$-community for any\n$\\beta\\in\\left[0,1\\right]$.\n  We also prove that counting the number of\n$\\left(1,o\\left(1\\right)\\right)$-communities requires quasi-polynomial time\nassuming the weaker #ETH. \n\n"}
{"id": "1611.09479", "contents": "Title: Upper bounds for $s$-distance sets and equiangular lines Abstract: The set of points in a metric space is called an $s$-distance set if pairwise\ndistances between these points admit only $s$ distinct values. Two-distance\nspherical sets with the set of scalar products $\\{\\alpha, -\\alpha\\}$,\n$\\alpha\\in[0,1)$, are called equiangular. The problem of determining the\nmaximum size of $s$-distance sets in various spaces has a long history in\nmathematics. We suggest a new method of bounding the size of an $s$-distance\nset in compact two-point homogeneous spaces via zonal spherical functions. This\nmethod allows us to prove that the maximum size of a spherical two-distance set\nin $\\mathbb{R}^n$, $n\\geq 7$, is $\\frac{n(n+1)}2$ with possible exceptions for\nsome $n=(2k+1)^2-3$, $k \\in \\mathbb{N}$. We also prove the universal upper\nbound $\\sim \\frac 2 3 n a^2$ for equiangular sets with $\\alpha=\\frac 1 a$ and,\nemploying this bound, prove a new upper bound on the size of equiangular sets\nin all dimensions. Finally, we classify all equiangular sets reaching this new\nbound. \n\n"}
{"id": "1612.01360", "contents": "Title: Completely regular codes in the infinite hexagonal grid Abstract: A set $C$ of vertices of a simple graph is called a completely regular code\nif for each $i=0$, $1$, $2$, \\ldots and $j = i-1$, $i$, $i+1$, all vertices at\ndistance $i$ from $C$ have the same number $s_{ij}$ of neighbors at distance\n$j$ from $C$. We characterize the completely regular codes in the infinite\nhexagonal grid graph. \n\n"}
{"id": "1612.01527", "contents": "Title: Matrix multiplication algorithms from group orbits Abstract: We show how to construct highly symmetric algorithms for matrix\nmultiplication. In particular, we consider algorithms which decompose the\nmatrix multiplication tensor into a sum of rank-1 tensors, where the\ndecomposition itself consists of orbits under some finite group action. We show\nhow to use the representation theory of the corresponding group to derive\nsimple constraints on the decomposition, which we solve by hand for n=2,3,4,5,\nrecovering Strassen's algorithm (in a particularly symmetric form) and new\nalgorithms for larger n. While these new algorithms do not improve the known\nupper bounds on tensor rank or the matrix multiplication exponent, they are\nbeautiful in their own right, and we point out modifications of this idea that\ncould plausibly lead to further improvements. Our constructions also suggest\nfurther patterns that could be mined for new algorithms, including a\ntantalizing connection with lattices. In particular, using lattices we give the\nmost transparent proof to date of Strassen's algorithm; the same proof works\nfor all n, to yield a decomposition with $n^3 - n + 1$ terms. \n\n"}
{"id": "1612.02384", "contents": "Title: Subquadratic Algorithms for Algebraic Generalizations of 3SUM Abstract: The 3SUM problem asks if an input $n$-set of real numbers contains a triple\nwhose sum is zero. We consider the 3POL problem, a natural generalization of\n3SUM where we replace the sum function by a constant-degree polynomial in three\nvariables. The motivations are threefold. Raz, Sharir, and de Zeeuw gave a\n$O(n^{11/6})$ upper bound on the number of solutions of trivariate polynomial\nequations when the solutions are taken from the cartesian product of three\n$n$-sets of real numbers. We give algorithms for the corresponding problem of\ncounting such solutions. Gr\\o nlund and Pettie recently designed subquadratic\nalgorithms for 3SUM. We generalize their results to 3POL. Finally, we shed\nlight on the General Position Testing (GPT) problem: \"Given $n$ points in the\nplane, do three of them lie on a line?\", a key problem in computational\ngeometry.\n  We prove that there exist bounded-degree algebraic decision trees of depth\n$O(n^{\\frac{12}{7}+\\varepsilon})$ that solve 3POL, and that 3POL can be solved\nin $O(n^2 {(\\log \\log n)}^\\frac{3}{2} / {(\\log n)}^\\frac{1}{2})$ time in the\nreal-RAM model. Among the possible applications of those results, we show how\nto solve GPT in subquadratic time when the input points lie on $o({(\\log\nn)}^\\frac{1}{6}/{(\\log \\log n)}^\\frac{1}{2})$ constant-degree polynomial\ncurves. This constitutes a first step towards closing the major open question\nof whether GPT can be solved in subquadratic time.\n  To obtain these results, we generalize important tools --- such as batch\nrange searching and dominance reporting --- to a polynomial setting. We expect\nthese new tools to be useful in other applications. \n\n"}
{"id": "1612.02712", "contents": "Title: Scalable Influence Maximization for Multiple Products in Continuous-Time\n  Diffusion Networks Abstract: A typical viral marketing model identifies influential users in a social\nnetwork to maximize a single product adoption assuming unlimited user\nattention, campaign budgets, and time. In reality, multiple products need\ncampaigns, users have limited attention, convincing users incurs costs, and\nadvertisers have limited budgets and expect the adoptions to be maximized soon.\nFacing these user, monetary, and timing constraints, we formulate the problem\nas a submodular maximization task in a continuous-time diffusion model under\nthe intersection of a matroid and multiple knapsack constraints. We propose a\nrandomized algorithm estimating the user influence in a network\n($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$\nwith $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and\n$\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By\nexploiting the influence estimation algorithm as a subroutine, we develop an\nadaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2\nk)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.\nExtensive experiments on networks of millions of nodes demonstrate that the\nproposed algorithms achieve the state-of-the-art in terms of effectiveness and\nscalability. \n\n"}
{"id": "1612.03456", "contents": "Title: Baby-Step Giant-Step Algorithms for the Symmetric Group Abstract: We study discrete logarithms in the setting of group actions. Suppose that\n$G$ is a group that acts on a set $S$. When $r,s \\in S$, a solution $g \\in G$\nto $r^g = s$ can be thought of as a kind of logarithm. In this paper, we study\nthe case where $G = S_n$, and develop analogs to the Shanks baby-step /\ngiant-step procedure for ordinary discrete logarithms. Specifically, we compute\ntwo sets $A, B \\subseteq S_n$ such that every permutation of $S_n$ can be\nwritten as a product $ab$ of elements $a \\in A$ and $b \\in B$. Our\ndeterministic procedure is optimal up to constant factors, in the sense that\n$A$ and $B$ can be computed in optimal asymptotic complexity, and $|A|$ and\n$|B|$ are a small constant from $\\sqrt{n!}$ in size. We also analyze randomized\n\"collision\" algorithms for the same problem. \n\n"}
{"id": "1612.05665", "contents": "Title: PAM: Parallel Augmented Maps Abstract: Ordered (key-value) maps are an important and widely-used data type for\nlarge-scale data processing frameworks. Beyond simple search, insertion and\ndeletion, more advanced operations such as range extraction, filtering, and\nbulk updates form a critical part of these frameworks.\n  We describe an interface for ordered maps that is augmented to support fast\nrange queries and sums, and introduce a parallel and concurrent library called\nPAM (Parallel Augmented Maps) that implements the interface. The interface\nincludes a wide variety of functions on augmented maps ranging from basic\ninsertion and deletion to more interesting functions such as union,\nintersection, filtering, extracting ranges, splitting, and range-sums. We\ndescribe algorithms for these functions that are efficient both in theory and\npractice.\n  As examples of the use of the interface and the performance of PAM, we apply\nthe library to four applications: simple range sums, interval trees, 2D range\ntrees, and ranked word index searching. The interface greatly simplifies the\nimplementation of these data structures over direct implementations.\nSequentially the code achieves performance that matches or exceeds existing\nlibraries designed specially for a single application, and in parallel our\nimplementation gets speedups ranging from 40 to 90 on 72 cores with 2-way\nhyperthreading. \n\n"}
{"id": "1612.07710", "contents": "Title: Set Similarity Search Beyond MinHash Abstract: We consider the problem of approximate set similarity search under\nBraun-Blanquet similarity $B(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} \\cap\n\\mathbf{y}| / \\max(|\\mathbf{x}|, |\\mathbf{y}|)$. The $(b_2, b_2)$-approximate\nBraun-Blanquet similarity search problem is to preprocess a collection of sets\n$P$ such that, given a query set $\\mathbf{q}$, if there exists $\\mathbf{x} \\in\nP$ with $B(\\mathbf{q}, \\mathbf{x}) \\geq b_1$, then we can efficiently return\n$\\mathbf{x}' \\in P$ with $B(\\mathbf{q}, \\mathbf{x}') > b_2$.\n  We present a simple data structure that solves this problem with space usage\n$O(n^{1+\\rho}\\log n + \\sum_{\\mathbf{x} \\in P}|\\mathbf{x}|)$ and query time\n$O(|\\mathbf{q}|n^{\\rho} \\log n)$ where $n = |P|$ and $\\rho =\n\\log(1/b_1)/\\log(1/b_2)$. Making use of existing lower bounds for\nlocality-sensitive hashing by O'Donnell et al. (TOCT 2014) we show that this\nvalue of $\\rho$ is tight across the parameter space, i.e., for every choice of\nconstants $0 < b_2 < b_1 < 1$.\n  In the case where all sets have the same size our solution strictly improves\nupon the value of $\\rho$ that can be obtained through the use of\nstate-of-the-art data-independent techniques in the Indyk-Motwani\nlocality-sensitive hashing framework (STOC 1998) such as Broder's MinHash (CCS\n1997) for Jaccard similarity and Andoni et al.'s cross-polytope LSH (NIPS 2015)\nfor cosine similarity. Surprisingly, even though our solution is\ndata-independent, for a large part of the parameter space we outperform the\ncurrently best data-dependent method by Andoni and Razenshteyn (STOC 2015). \n\n"}
{"id": "1612.07728", "contents": "Title: Statistical limits of spiked tensor models Abstract: We study the statistical limits of both detecting and estimating a rank-one\ndeformation of a symmetric random Gaussian tensor. We establish upper and lower\nbounds on the critical signal-to-noise ratio, under a variety of priors for the\nplanted vector: (i) a uniformly sampled unit vector, (ii) i.i.d. $\\pm 1$\nentries, and (iii) a sparse vector where a constant fraction $\\rho$ of entries\nare i.i.d. $\\pm 1$ and the rest are zero. For each of these cases, our upper\nand lower bounds match up to a $1+o(1)$ factor as the order $d$ of the tensor\nbecomes large. For sparse signals (iii), our bounds are also asymptotically\ntight in the sparse limit $\\rho \\to 0$ for any fixed $d$ (including the $d=2$\ncase of sparse PCA). Our upper bounds for (i) demonstrate a phenomenon\nreminiscent of the work of Baik, Ben Arous and P\\'ech\\'e: an `eigenvalue' of a\nperturbed tensor emerges from the bulk at a strictly lower signal-to-noise\nratio than when the perturbation itself exceeds the bulk; we quantify the size\nof this effect. We also provide some general results for larger classes of\npriors. In particular, the large $d$ asymptotics of the threshold location\ndiffers between problems with discrete priors versus continuous priors.\nFinally, for priors (i) and (ii) we carry out the replica prediction from\nstatistical physics, which is conjectured to give the exact\ninformation-theoretic threshold for any fixed $d$.\n  Of independent interest, we introduce a new improvement to the second moment\nmethod for contiguity, on which our lower bounds are based. Our technique\nconditions away from rare `bad' events that depend on interactions between the\nsignal and noise. This enables us to close $\\sqrt{2}$-factor gaps present in\nseveral previous works. \n\n"}
{"id": "1612.07866", "contents": "Title: Spectral algorithms for tensor completion Abstract: In the tensor completion problem, one seeks to estimate a low-rank tensor\nbased on a random sample of revealed entries. In terms of the required sample\nsize, earlier work revealed a large gap between estimation with unbounded\ncomputational resources (using, for instance, tensor nuclear norm minimization)\nand polynomial-time algorithms. Among the latter, the best statistical\nguarantees have been proved, for third-order tensors, using the sixth level of\nthe sum-of-squares (SOS) semidefinite programming hierarchy (Barak and Moitra,\n2014). However, the SOS approach does not scale well to large problem\ninstances. By contrast, spectral methods --- based on unfolding or matricizing\nthe tensor --- are attractive for their low complexity, but have been believed\nto require a much larger sample size.\n  This paper presents two main contributions. First, we propose a new\nunfolding-based method, which outperforms naive ones for symmetric $k$-th order\ntensors of rank $r$. For this result we make a study of singular space\nestimation for partially revealed matrices of large aspect ratio, which may be\nof independent interest. For third-order tensors, our algorithm matches the SOS\nmethod in terms of sample size (requiring about $rd^{3/2}$ revealed entries),\nsubject to a worse rank condition ($r\\ll d^{3/4}$ rather than $r\\ll d^{3/2}$).\nWe complement this result with a different spectral algorithm for third-order\ntensors in the overcomplete ($r\\ge d$) regime. Under a random model, this\nsecond approach succeeds in estimating tensors of rank $d\\le r \\ll d^{3/2}$\nfrom about $rd^{3/2}$ revealed entries. \n\n"}
{"id": "1701.01483", "contents": "Title: Noise Stability is computable and low dimensional Abstract: Questions of noise stability play an important role in hardness of\napproximation in computer science as well as in the theory of voting. In many\napplications, the goal is to find an optimizer of noise stability among all\npossible partitions of $\\mathbb{R}^n$ for $n \\geq 1$ to $k$ parts with given\nGaussian measures $\\mu_1,\\ldots,\\mu_k$. We call a partition $\\epsilon$-optimal,\nif its noise stability is optimal up to an additive $\\epsilon$. In this paper,\nwe give an explicit, computable function $n(\\epsilon)$ such that an\n$\\epsilon$-optimal partition exists in $\\mathbb{R}^{n(\\epsilon)}$. This result\nhas implications for the computability of certain problems in non-interactive\nsimulation, which are addressed in a subsequent work. \n\n"}
{"id": "1701.01485", "contents": "Title: Non interactive simulation of correlated distributions is decidable Abstract: A basic problem in information theory is the following: Let $\\mathbf{P} =\n(\\mathbf{X}, \\mathbf{Y})$ be an arbitrary distribution where the marginals\n$\\mathbf{X}$ and $\\mathbf{Y}$ are (potentially) correlated. Let Alice and Bob\nbe two players where Alice gets samples $\\{x_i\\}_{i \\ge 1}$ and Bob gets\nsamples $\\{y_i\\}_{i \\ge 1}$ and for all $i$, $(x_i, y_i) \\sim \\mathbf{P}$. What\njoint distributions $\\mathbf{Q}$ can be simulated by Alice and Bob without any\ninteraction?\n  Classical works in information theory by G{\\'a}cs-K{\\\"o}rner and Wyner answer\nthis question when at least one of $\\mathbf{P}$ or $\\mathbf{Q}$ is the\ndistribution on $\\{0,1\\} \\times \\{0,1\\}$ where each marginal is unbiased and\nidentical. However, other than this special case, the answer to this question\nis understood in very few cases. Recently, Ghazi, Kamath and Sudan showed that\nthis problem is decidable for $\\mathbf{Q}$ supported on $\\{0,1\\} \\times\n\\{0,1\\}$. We extend their result to $\\mathbf{Q}$ supported on any finite\nalphabet.\n  We rely on recent results in Gaussian geometry (by the authors) as well as a\nnew \\emph{smoothing argument} inspired by the method of \\emph{boosting} from\nlearning theory and potential function arguments from complexity theory and\nadditive combinatorics. \n\n"}
{"id": "1701.01539", "contents": "Title: Algorithms for Optimal Replica Placement Under Correlated Failure in\n  Hierarchical Failure Domains Abstract: In data centers, data replication is the primary method used to ensure\navailability of customer data. To avoid correlated failure, cloud storage\ninfrastructure providers model hierarchical failure domains using a tree, and\navoid placing a large number of data replicas within the same failure domain\n(i.e. on the same branch of the tree). Typical best practices ensure that\nreplicas are distributed across failure domains, but relatively little is known\nconcerning optimization algorithms for distributing data replicas. Using a\nhierarchical model, we answer how to distribute replicas across failure domains\noptimally. We formulate a novel optimization problem for replica placement in\ndata centers. As part of our problem, we formalize and explain a new criterion\nfor optimizing a replica placement. Our overall goal is to choose placements in\nwhich correlated failures disable as few replicas as possible. We provide two\noptimization algorithms for dependency models represented by trees. We first\npresent an $O(n + \\rho \\log \\rho)$ time dynamic programming algorithm for\nplacing $\\rho$ replicas of a single file on the leaves (representing servers)\nof a tree with $n$ vertices. We next consider the problem of placing replicas\nof $m$ blocks of data, where each block may have different replication factors.\nFor this problem, we give an exact algorithm which runs in polynomial time when\nthe skew, the difference in the number of replicas between the largest and\nsmallest blocks of data, is constant. \n\n"}
{"id": "1701.03263", "contents": "Title: An EPTAS for Scheduling on Unrelated Machines of Few Different Types Abstract: In the classical problem of scheduling on unrelated parallel machines, a set\nof jobs has to be assigned to a set of machines. The jobs have a processing\ntime depending on the machine and the goal is to minimize the makespan, that is\nthe maximum machine load. It is well known that this problem is NP-hard and\ndoes not allow polynomial time approximation algorithms with approximation\nguarantees smaller than $1.5$ unless P$=$NP. We consider the case that there\nare only a constant number $K$ of machine types. Two machines have the same\ntype if all jobs have the same processing time for them. This variant of the\nproblem is strongly NP-hard already for $K=1$. We present an efficient\npolynomial time approximation scheme (EPTAS) for the problem, that is, for any\n$\\varepsilon > 0$ an assignment with makespan of length at most\n$(1+\\varepsilon)$ times the optimum can be found in polynomial time in the\ninput length and the exponent is independent of $1/\\varepsilon$. In particular\nwe achieve a running time of $2^{\\mathcal{O}(K\\log(K)\n\\frac{1}{\\varepsilon}\\log^4 \\frac{1}{\\varepsilon})}+\\mathrm{poly}(|I|)$, where\n$|I|$ denotes the input length. Furthermore, we study three other problem\nvariants and present an EPTAS for each of them: The Santa Claus problem, where\nthe minimum machine load has to be maximized; the case of scheduling on\nunrelated parallel machines with a constant number of uniform types, where\nmachines of the same type behave like uniformly related machines; and the\nmultidimensional vector scheduling variant of the problem where both the\ndimension and the number of machine types are constant. For the Santa Claus\nproblem we achieve the same running time. The results are achieved, using mixed\ninteger linear programming and rounding techniques. \n\n"}
{"id": "1701.03318", "contents": "Title: Comparing MapReduce and Pipeline Implementations for Counting Triangles Abstract: A common method to define a parallel solution for a computational problem\nconsists in finding a way to use the Divide and Conquer paradigm in order to\nhave processors acting on its own data and scheduled in a parallel fashion.\nMapReduce is a programming model that follows this paradigm, and allows for the\ndefinition of efficient solutions by both decomposing a problem into steps on\nsubsets of the input data and combining the results of each step to produce\nfinal results. Albeit used for the implementation of a wide variety of\ncomputational problems, MapReduce performance can be negatively affected\nwhenever the replication factor grows or the size of the input is larger than\nthe resources available at each processor. In this paper we show an alternative\napproach to implement the Divide and Conquer paradigm, named dynamic pipeline.\nThe main features of dynamic pipelines are illustrated on a parallel\nimplementation of the well-known problem of counting triangles in a graph. This\nproblem is especially interesting either when the input graph does not fit in\nmemory or is dynamically generated. To evaluate the properties of pipeline, a\ndynamic pipeline of processes and an ad-hoc version of MapReduce are\nimplemented in the language Go, exploiting its ability to deal with channels\nand spawned processes. An empirical evaluation is conducted on graphs of\ndifferent topologies, sizes, and densities. Observed results suggest that\ndynamic pipelines allows for an efficient implementation of the problem of\ncounting triangles in a graph, particularly, in dense and large graphs,\ndrastically reducing the execution time with respect to the MapReduce\nimplementation. \n\n"}
{"id": "1701.03493", "contents": "Title: Subgaussian Tail Bounds via Stability Arguments Abstract: Sums of independent, bounded random variables concentrate around their\nexpectation approximately as well a Gaussian of the same variance. Well known\nresults of this form include the Bernstein, Hoeffding, and Chernoff\ninequalities and many others. We present an alternative proof of these tail\nbounds based on what we call a stability argument, which avoids bounding the\nmoment generating function or higher-order moments of the distribution. Our\nstability argument is inspired by recent work on the generalization properties\nof differential privacy and their connection to adaptive data analysis (Bassily\net al., STOC 2016). \n\n"}
{"id": "1701.03990", "contents": "Title: Quantum algorithm for multivariate polynomial interpolation Abstract: How many quantum queries are required to determine the coefficients of a\ndegree-$d$ polynomial in $n$ variables? We present and analyze quantum\nalgorithms for this multivariate polynomial interpolation problem over the\nfields $\\mathbb{F}_q$, $\\mathbb{R}$, and $\\mathbb{C}$. We show that\n$k_{\\mathbb{C}}$ and $2k_{\\mathbb{C}}$ queries suffice to achieve probability\n$1$ for $\\mathbb{C}$ and $\\mathbb{R}$, respectively, where\n$k_{\\mathbb{C}}=\\smash{\\lceil\\frac{1}{n+1}{n+d\\choose d}\\rceil}$ except for\n$d=2$ and four other special cases. For $\\mathbb{F}_q$, we show that\n$\\smash{\\lceil\\frac{d}{n+d}{n+d\\choose d}\\rceil}$ queries suffice to achieve\nprobability approaching $1$ for large field order $q$. The classical query\ncomplexity of this problem is $\\smash{n+d\\choose d}$, so our result provides a\nspeedup by a factor of $n+1$, $\\frac{n+1}{2}$, and $\\frac{n+d}{d}$ for\n$\\mathbb{C}$, $\\mathbb{R}$, and $\\mathbb{F}_q$, respectively. Thus we find a\nmuch larger gap between classical and quantum algorithms than the univariate\ncase, where the speedup is by a factor of $2$. For the case of $\\mathbb{F}_q$,\nwe conjecture that $2k_{\\mathbb{C}}$ queries also suffice to achieve\nprobability approaching $1$ for large field order $q$, although we leave this\nas an open problem. \n\n"}
{"id": "1701.04148", "contents": "Title: SF-sketch: A Two-stage Sketch for Data Streams Abstract: A sketch is a probabilistic data structure used to record frequencies of\nitems in a multi-set. Sketches are widely used in various fields, especially\nthose that involve processing and storing data streams. In streaming\napplications with high data rates, a sketch \"fills up\" very quickly. Thus, its\ncontents are periodically transferred to the remote collector, which is\nresponsible for answering queries. In this paper, we propose a new sketch,\ncalled Slim-Fat (SF) sketch, which has a significantly higher accuracy compared\nto prior art, a much smaller memory footprint, and at the same time achieves\nthe same speed as the best prior sketch. The key idea behind our proposed\nSF-sketch is to maintain two separate sketches: a small sketch called\nSlim-subsketch and a large sketch called Fat-subsketch. The Slim-subsketch is\nperiodically transferred to the remote collector for answering queries quickly\nand accurately. The Fat-subsketch, however, is not transferred to the remote\ncollector because it is used only to assist the Slim-subsketch during the\ninsertions and deletions and is not used to answer queries. We implemented and\nextensively evaluated SF-sketch along with several prior sketches and compared\nthem side by side. Our experimental results show that SF-sketch outperforms the\nmost widely used CM-sketch by up to 33.1 times in terms of accuracy. We have\nreleased the source codes of our proposed sketch as well as existing sketches\nat Github. The short version of this paper will appear in ICDE 2017. \n\n"}
{"id": "1701.04521", "contents": "Title: Sum of squares lower bounds for refuting any CSP Abstract: Let $P:\\{0,1\\}^k \\to \\{0,1\\}$ be a nontrivial $k$-ary predicate. Consider a\nrandom instance of the constraint satisfaction problem $\\mathrm{CSP}(P)$ on $n$\nvariables with $\\Delta n$ constraints, each being $P$ applied to $k$ randomly\nchosen literals. Provided the constraint density satisfies $\\Delta \\gg 1$, such\nan instance is unsatisfiable with high probability. The \\emph{refutation}\nproblem is to efficiently find a proof of unsatisfiability.\n  We show that whenever the predicate $P$ supports a $t$-\\emph{wise uniform}\nprobability distribution on its satisfying assignments, the sum of squares\n(SOS) algorithm of degree $d = \\Theta(\\frac{n}{\\Delta^{2/(t-1)} \\log \\Delta})$\n(which runs in time $n^{O(d)}$) \\emph{cannot} refute a random instance of\n$\\mathrm{CSP}(P)$. In particular, the polynomial-time SOS algorithm requires\n$\\widetilde{\\Omega}(n^{(t+1)/2})$ constraints to refute random instances of\nCSP$(P)$ when $P$ supports a $t$-wise uniform distribution on its satisfying\nassignments. Together with recent work of Lee et al. [LRS15], our result also\nimplies that \\emph{any} polynomial-size semidefinite programming relaxation for\nrefutation requires at least $\\widetilde{\\Omega}(n^{(t+1)/2})$ constraints.\n  Our results (which also extend with no change to CSPs over larger alphabets)\nsubsume all previously known lower bounds for semialgebraic refutation of\nrandom CSPs. For every constraint predicate~$P$, they give a three-way hardness\ntradeoff between the density of constraints, the SOS degree (hence running\ntime), and the strength of the refutation. By recent algorithmic results of\nAllen et al. [AOW15] and Raghavendra et al. [RRS16], this full three-way\ntradeoff is \\emph{tight}, up to lower-order factors. \n\n"}
{"id": "1701.04545", "contents": "Title: Point distribution in compact metric spaces, III. Two-point homogeneous\n  spaces Abstract: We consider point distributions in compact connected two-point homogeneous\nspaces (Riemannian symmetric spaces of rank one). All such spaces are known,\nthey are the spheres in the Euclidean spaces, the real, complex and\nquaternionic projective spaces and the octonionic projective plane. Our concern\nis with discrepancies of distributions in metric balls and sums of pairwise\ndistances between points of distributions in such spaces.\n  Using the geometric features of two-point spaces, we show that Stolarsky's\ninvariance principle, well-known for the Euclidean spheres, can be extended to\nall projective spaces and the octonionic projective plane (Theorem 2.1 and\nCorollary 2.1). We obtain the spherical function expansions for discrepancies\nand sums of distances (Theorem 9.1). Relying on these expansions, we prove in\nall such spaces the best possible bounds for quadratic discrepancies and sums\nof pairwise distances (Theorem 2.2). Applications to $t$-designs on such\ntwo-point homogeneous spaces are also considered. It is shown that the optimal\n$t$-designs meet the best possible bounds for quadratic discrepancies and sums\nof pairwise distances. (Corollaries 3.1 and 3.2). \n\n"}
{"id": "1701.05328", "contents": "Title: Succinct Hitting Sets and Barriers to Proving Algebraic Circuits Lower\n  Bounds Abstract: We formalize a framework of algebraically natural lower bounds for algebraic\ncircuits. Just as with the natural proofs notion of Razborov and Rudich for\nboolean circuit lower bounds, our notion of algebraically natural lower bounds\ncaptures nearly all lower bound techniques known. However, unlike the boolean\nsetting, there has been no concrete evidence demonstrating that this is a\nbarrier to obtaining super-polynomial lower bounds for general algebraic\ncircuits, as there is little understanding whether algebraic circuits are\nexpressive enough to support \"cryptography\" secure against algebraic circuits.\n  Following a similar result of Williams in the boolean setting, we show that\nthe existence of an algebraic natural proofs barrier is equivalent to the\nexistence of succinct derandomization of the polynomial identity testing\nproblem. That is, whether the coefficient vectors of polylog(N)-degree\npolylog(N)-size circuits is a hitting set for the class of poly(N)-degree\npoly(N)-size circuits. Further, we give an explicit universal construction\nshowing that if such a succinct hitting set exists, then our universal\nconstruction suffices.\n  Further, we assess the existing literature constructing hitting sets for\nrestricted classes of algebraic circuits and observe that none of them are\nsuccinct as given. Yet, we show how to modify some of these constructions to\nobtain succinct hitting sets. This constitutes the first evidence supporting\nthe existence of an algebraic natural proofs barrier.\n  Our framework is similar to the Geometric Complexity Theory (GCT) program of\nMulmuley and Sohoni, except that here we emphasize constructiveness of the\nproofs while the GCT program emphasizes symmetry. Nevertheless, our succinct\nhitting sets have relevance to the GCT program as they imply lower bounds for\nthe complexity of the defining equations of polynomials computed by small\ncircuits. \n\n"}
{"id": "1701.06064", "contents": "Title: On Recoverable and Two-Stage Robust Selection Problems with Budgeted\n  Uncertainty Abstract: In this paper the problem of selecting $p$ out of $n$ available items is\ndiscussed, such that their total cost is minimized. We assume that costs are\nnot known exactly, but stem from a set of possible outcomes.\n  Robust recoverable and two-stage models of this selection problem are\nanalyzed. In the two-stage problem, up to $p$ items is chosen in the first\nstage, and the solution is completed once the scenario becomes revealed in the\nsecond stage. In the recoverable problem, a set of $p$ items is selected in the\nfirst stage, and can be modified by exchanging up to $k$ items in the second\nstage, after a scenario reveals.\n  We assume that uncertain costs are modeled through bounded uncertainty sets,\ni.e., the interval uncertainty sets with an additional linear (budget)\nconstraint, in their discrete and continuous variants. Polynomial algorithms\nfor recoverable and two-stage selection problems with continuous bounded\nuncertainty, and compact mixed integer formulations in the case of discrete\nbounded uncertainty are constructed. \n\n"}
{"id": "1701.06321", "contents": "Title: Quantum entanglement, sum of squares, and the log rank conjecture Abstract: For every $\\epsilon>0$, we give an\n$\\exp(\\tilde{O}(\\sqrt{n}/\\epsilon^2))$-time algorithm for the $1$ vs\n$1-\\epsilon$ \\emph{Best Separable State (BSS)} problem of distinguishing, given\nan $n^2\\times n^2$ matrix $\\mathcal{M}$ corresponding to a quantum measurement,\nbetween the case that there is a separable (i.e., non-entangled) state $\\rho$\nthat $\\mathcal{M}$ accepts with probability $1$, and the case that every\nseparable state is accepted with probability at most $1-\\epsilon$.\nEquivalently, our algorithm takes the description of a subspace $\\mathcal{W}\n\\subseteq \\mathbb{F}^{n^2}$ (where $\\mathbb{F}$ can be either the real or\ncomplex field) and distinguishes between the case that $\\mathcal{W}$ contains a\nrank one matrix, and the case that every rank one matrix is at least $\\epsilon$\nfar (in $\\ell_2$ distance) from $\\mathcal{W}$.\n  To the best of our knowledge, this is the first improvement over the\nbrute-force $\\exp(n)$-time algorithm for this problem. Our algorithm is based\non the \\emph{sum-of-squares} hierarchy and its analysis is inspired by Lovett's\nproof (STOC '14, JACM '16) that the communication complexity of every rank-$n$\nBoolean matrix is bounded by $\\tilde{O}(\\sqrt{n})$. \n\n"}
{"id": "1701.06446", "contents": "Title: Algorithm for an arbitrary-order cumulant tensor calculation in a\n  sliding window of data streams Abstract: High order cumulant tensors carry information about statistics of\nnon-normally distributed multivariate data. In this work we present a new\nefficient algorithm for calculation of cumulants of arbitrary order in a\nsliding window for data streams. We showed that this algorithms enables\nspeedups of cumulants updates compared to current algorithms. This algorithm\ncan be used for processing on-line high-frequency multivariate data and can\nfind applications in, e.g., on-line signal filtering and classification of data\nstreams.\n  To present an application of this algorithm, we propose an estimator of\nnon-Gaussianity of a data stream based on the norms of high-order cumulant\ntensors.\n  We show how to detect the transition from Gaussian distributed data to\nnon-Gaussian ones in a~data stream. In order to achieve high implementation\nefficiency of operations on super-symmetric tensors, such as cumulant tensors,\nwe employ the block structure to store and calculate only one hyper-pyramid\npart of such tensors. \n\n"}
{"id": "1702.00066", "contents": "Title: Combinatorial distance geometry in normed spaces Abstract: We survey problems and results from combinatorial geometry in normed spaces,\nconcentrating on problems that involve distances. These include various\nproperties of unit-distance graphs, minimum-distance graphs, diameter graphs,\nas well as minimum spanning trees and Steiner minimum trees. In particular, we\ndiscuss translative kissing (or Hadwiger) numbers, equilateral sets, and the\nBorsuk problem in normed spaces. We show how to use the angular measure of\nPeter Brass to prove various statements about Hadwiger and blocking numbers of\nconvex bodies in the plane, including some new results. We also include some\nnew results on thin cones and their application to distinct distances and other\ncombinatorial problems for normed spaces. \n\n"}
{"id": "1702.00767", "contents": "Title: A new Holant dichotomy inspired by quantum computation Abstract: Holant problems are a framework for the analysis of counting complexity\nproblems on graphs. This framework is simultaneously general enough to\nencompass many other counting problems on graphs and specific enough to allow\nthe derivation of dichotomy results, partitioning all problem instances into\nthose which can be solved in polynomial time and those which are #P-hard. The\nHolant framework is based on the theory of holographic algorithms, which was\noriginally inspired by concepts from quantum computation, but this connection\nappears not to have been explored before.\n  Here, we employ quantum information theory to explain existing results in a\nconcise way and to derive a dichotomy for a new family of problems, which we\ncall Holant$^+$. This family sits in between the known families of Holant$^*$,\nfor which a full dichotomy is known, and Holant$^c$, for which only a\nrestricted dichotomy is known. Using knowledge from entanglement theory -- both\npreviously existing work and new results of our own -- we prove a full\ndichotomy theorem for Holant$^+$, which is very similar to the restricted\nHolant$^c$ dichotomy. Other than the dichotomy for #R$_3$-CSP, ours is the\nfirst Holant dichotomy in which the allowed functions are not restricted and in\nwhich only a finite number of functions are assumed to be freely available. \n\n"}
{"id": "1702.00948", "contents": "Title: Ranking vertices for active module recovery problem Abstract: Selecting a connected subnetwork enriched in individually important vertices\nis an approach commonly used in many areas of bioinformatics, including\nanalysis of gene expression data, mutations, metabolomic profiles and others.\nIt can be formulated as a recovery of an active module from which an\nexperimental signal is generated. Commonly, methods for solving this problem\nresult in a single subnetwork that is considered to be a good candidate.\nHowever, it is usually useful to consider not one but multiple candidate\nmodules at different significance threshold levels. Therefore, in this paper we\nsuggest to consider a problem of finding a vertex ranking instead of finding a\nsingle module. We also propose two algorithms for solving this problem: one\nthat we consider to be optimal but computationally expensive for real-world\nnetworks and one that works close to the optimal in practice and is also able\nto work with big networks. \n\n"}
{"id": "1702.00961", "contents": "Title: Liouville theorem for bounded harmonic functions on manifolds and graphs\n  satisfying non-negative curvature dimension condition Abstract: Brighton [Bri13] proved the Liouville theorem for bounded harmonic functions\non weighted manifolds satisfying non-negative curvature dimension condition,\ni.e. $CD(0,\\infty).$ In this paper, we provide a new proof of this result by\nusing the reverse Poincar\\'e inequality. Moreover, we adopt this approach to\nprove the Liouville theorem for bounded harmonic functions on graphs satisfying\nthe $CD(0,\\infty)$ condition. \n\n"}
{"id": "1702.01719", "contents": "Title: A 2-Approximation for the Height of Maximal Outerplanar Graph Drawings Abstract: In this paper, we study planar drawings of maximal outerplanar graphs with\nthe objective of achieving small height. A recent paper gave an algorithm for\nsuch drawings that is within a factor of 4 of the optimum height. In this\npaper, we substantially improve the approximation factor to become 2. The main\ningredient is to define a new parameter of outerplanar graphs (the so-called\numbrella depth, obtained by recursively splitting the graph into graphs called\numbrellas). We argue that the height of any poly-line drawing must be at least\nthe umbrella depth, and then devise an algorithm that achieves height at most\ntwice the umbrella depth. \n\n"}
{"id": "1702.02017", "contents": "Title: A Tight I/O Lower Bound for Matrix Multiplication Abstract: A tight lower bound for required I/O when computing an ordinary matrix-matrix\nmultiplication on a processor with two layers of memory is established. Prior\nwork obtained weaker lower bounds by reasoning about the number of segments\nneeded to perform $C:=AB$, for distinct matrices $A$, $B$, and $C$, where each\nsegment is a series of operations involving $M$ reads and writes to and from\nfast memory, and $M$ is the size of fast memory. A lower bound on the number of\nsegments was then determined by obtaining an upper bound on the number of\nelementary multiplications performed per segment. This paper follows the same\nhigh level approach, but improves the lower bound by (1) transforming\nalgorithms for MMM so that they perform all computation via fused multiply-add\ninstructions (FMAs) and using this to reason about only the cost associated\nwith reading the matrices, and (2) decoupling the per-segment I/O cost from the\nsize of fast memory. For $n \\times n$ matrices, the lower bound's leading-order\nterm is $2n^3/\\sqrt{M}$. A theoretical algorithm whose leading terms attains\nthis is introduced. To what extent the state-of-the-art Goto's Algorithm\nattains the lower bound is discussed. \n\n"}
{"id": "1702.02133", "contents": "Title: A New Graph Parameter To Measure Linearity Abstract: Consider a sequence of LexBFS vertex orderings {\\sigma}1, {\\sigma}2, . . .\nwhere each ordering {\\sigma}i is used to break ties for {\\sigma}i+1. Since the\ntotal number of vertex orderings of a finite graph is finite, this sequence\nmust end in a cycle of vertex orderings. The possible length of this cycle is\nthe main subject of this work. Intuitively, we prove for graphs with a known\nnotion of linearity (e.g., interval graphs with their interval representation\non the real line), this cycle cannot be too big, no matter which vertex\nordering we start with. More precisely, it was conjectured in [9] that for\ncocomparability graphs, the size of this cycle is always 2, independent of the\nstarting order. Furthermore [27] asked whether for arbitrary graphs, the size\nof such a cycle is always bounded by the asteroidal number of the graph. In\nthis work, while we answer this latter question negatively, we provide support\nfor the conjecture on cocomparability graphs by proving it for the subclass of\ndomino-free cocomparability graphs. This subclass contains cographs, proper\ninterval, interval, and cobipartite graphs. We also provide simpler independent\nproofs for each of these cases which lead to stronger results on this\nsubclasses. \n\n"}
{"id": "1702.03106", "contents": "Title: A Las Vegas approximation algorithm for metric $1$-median selection Abstract: Given an $n$-point metric space, consider the problem of finding a point with\nthe minimum sum of distances to all points. We show that this problem has a\nrandomized algorithm that {\\em always} outputs a $(2+\\epsilon)$-approximate\nsolution in an expected $O(n/\\epsilon^2)$ time for each constant $\\epsilon>0$.\nInheriting Indyk's algorithm, our algorithm outputs a\n$(1+\\epsilon)$-approximate $1$-median in $O(n/\\epsilon^2)$ time with\nprobability $\\Omega(1)$. \n\n"}
{"id": "1702.04536", "contents": "Title: A $(2+\\epsilon)$-Approximation for Maximum Weight Matching in the\n  Semi-Streaming Model Abstract: We present a simple deterministic single-pass $(2+\\epsilon)$-approximation\nalgorithm for the maximum weight matching problem in the semi-streaming model.\nThis improves upon the currently best known approximation ratio of\n$(4+\\epsilon)$.\n  Our algorithm uses $O(n\\log^2 n)$ bits of space for constant values of\n$\\epsilon$. It relies on a variation of the local-ratio theorem, which may be\nof use for other algorithms in the semi-streaming model as well. \n\n"}
{"id": "1702.05456", "contents": "Title: LCL problems on grids Abstract: LCLs or locally checkable labelling problems (e.g. maximal independent set,\nmaximal matching, and vertex colouring) in the LOCAL model of computation are\nvery well-understood in cycles (toroidal 1-dimensional grids): every problem\nhas a complexity of $O(1)$, $\\Theta(\\log^* n)$, or $\\Theta(n)$, and the design\nof optimal algorithms can be fully automated.\n  This work develops the complexity theory of LCL problems for toroidal\n2-dimensional grids. The complexity classes are the same as in the\n1-dimensional case: $O(1)$, $\\Theta(\\log^* n)$, and $\\Theta(n)$. However, given\nan LCL problem it is undecidable whether its complexity is $\\Theta(\\log^* n)$\nor $\\Theta(n)$ in 2-dimensional grids.\n  Nevertheless, if we correctly guess that the complexity of a problem is\n$\\Theta(\\log^* n)$, we can completely automate the design of optimal\nalgorithms. For any problem we can find an algorithm that is of a normal form\n$A' \\circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for\nfinding a maximal independent set in $k$th power of the grid, and $k$ is a\nconstant.\n  Finally, partially with the help of automated design tools, we classify the\ncomplexity of several concrete LCL problems related to colourings and\norientations. \n\n"}
{"id": "1702.06237", "contents": "Title: Exact tensor completion with sum-of-squares Abstract: We obtain the first polynomial-time algorithm for exact tensor completion\nthat improves over the bound implied by reduction to matrix completion. The\nalgorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal\ncomponents in $\\mathbb R^n$ from $r\\cdot \\tilde O(n^{1.5})$ randomly observed\nentries of the tensor. This bound improves over the previous best one of\n$r\\cdot \\tilde O(n^{2})$ by reduction to exact matrix completion. Our bound\nalso matches the best known results for the easier problem of approximate\ntensor completion (Barak & Moitra, 2015).\n  Our algorithm and analysis extends seminal results for exact matrix\ncompletion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares\nmethod. The main technical challenge is to show that a small number of randomly\nchosen monomials are enough to construct a degree-3 polynomial with precisely\nplanted orthogonal global optima over the sphere and that this fact can be\ncertified within the sum-of-squares proof system. \n\n"}
{"id": "1702.06237", "contents": "Title: Exact tensor completion with sum-of-squares Abstract: We obtain the first polynomial-time algorithm for exact tensor completion\nthat improves over the bound implied by reduction to matrix completion. The\nalgorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal\ncomponents in $\\mathbb R^n$ from $r\\cdot \\tilde O(n^{1.5})$ randomly observed\nentries of the tensor. This bound improves over the previous best one of\n$r\\cdot \\tilde O(n^{2})$ by reduction to exact matrix completion. Our bound\nalso matches the best known results for the easier problem of approximate\ntensor completion (Barak & Moitra, 2015).\n  Our algorithm and analysis extends seminal results for exact matrix\ncompletion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares\nmethod. The main technical challenge is to show that a small number of randomly\nchosen monomials are enough to construct a degree-3 polynomial with precisely\nplanted orthogonal global optima over the sphere and that this fact can be\ncertified within the sum-of-squares proof system. \n\n"}
{"id": "1702.06365", "contents": "Title: Automatic implementation of material laws: Jacobian calculation in a\n  finite element code with TAPENADE Abstract: In an effort to increase the versatility of finite element codes, we explore\nthe possibility of automatically creating the Jacobian matrix necessary for the\ngradient-based solution of nonlinear systems of equations. Particularly, we aim\nto assess the feasibility of employing the automatic differentiation tool\nTAPENADE for this purpose on a large Fortran codebase that is the result of\nmany years of continuous development. As a starting point we will describe the\nspecial structure of finite element codes and the implications that this code\ndesign carries for an efficient calculation of the Jacobian matrix. We will\nalso propose a first approach towards improving the efficiency of such a\nmethod. Finally, we will present a functioning method for the automatic\nimplementation of the Jacobian calculation in a finite element software, but\nwill also point out important shortcomings that will have to be addressed in\nthe future. \n\n"}
{"id": "1702.06503", "contents": "Title: When can Graph Hyperbolicity be computed in Linear Time? Abstract: Hyperbolicity measures, in terms of (distance) metrics, how close a given\ngraph is to being a tree. Due to its relevance in modeling real-world networks,\nhyperbolicity has seen intensive research over the last years. Unfortunately,\nthe best known algorithms for computing the hyperbolicity number of a graph\n(the smaller, the more tree-like) have running time $O(n^4)$, where $n$ is the\nnumber of graph vertices. Exploiting the framework of parameterized complexity\nanalysis, we explore possibilities for \"linear-time FPT\" algorithms to compute\nhyperbolicity. For instance, we show that hyperbolicity can be computed in time\n$O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same\ntime, unless the SETH fails, there is no $2^{o(k)}n^2$-time algorithm. \n\n"}
{"id": "1702.06723", "contents": "Title: Compact linear programs for 2SAT Abstract: For each integer $n$ we present an explicit formulation of a compact linear\nprogram, with $O(n^3)$ variables and constraints, which determines the\nsatisfiability of any 2SAT formula with $n$ boolean variables by a single\nlinear optimization. This contrasts with the fact that the natural polytope for\nthis problem, formed from the convex hull of all satisfiable formulas and their\nsatisfying assignments, has superpolynomial extension complexity. Our\nformulation is based on multicommodity flows. We also discuss connections of\nthese results to the stable matching problem. \n\n"}
{"id": "1702.07134", "contents": "Title: Diverse Weighted Bipartite b-Matching Abstract: Bipartite matching, where agents on one side of a market are matched to\nagents or items on the other, is a classical problem in computer science and\neconomics, with widespread application in healthcare, education, advertising,\nand general resource allocation. A practitioner's goal is typically to maximize\na matching market's economic efficiency, possibly subject to some fairness\nrequirements that promote equal access to resources. A natural balancing act\nexists between fairness and efficiency in matching markets, and has been the\nsubject of much research.\n  In this paper, we study a complementary goal---balancing diversity and\nefficiency---in a generalization of bipartite matching where agents on one side\nof the market can be matched to sets of agents on the other. Adapting a\nclassical definition of the diversity of a set, we propose a quadratic\nprogramming-based approach to solving a supermodular minimization problem that\nbalances diversity and total weight of the solution. We also provide a scalable\ngreedy algorithm with theoretical performance bounds. We then define the price\nof diversity, a measure of the efficiency loss due to enforcing diversity, and\ngive a worst-case theoretical bound. Finally, we demonstrate the efficacy of\nour methods on three real-world datasets, and show that the price of diversity\nis not bad in practice. \n\n"}
{"id": "1702.08299", "contents": "Title: Independent Set Size Approximation in Graph Streams Abstract: We study the problem of estimating the size of independent sets in a graph\n$G$ defined by a stream of edges. Our approach relies on the Caro-Wei bound,\nwhich expresses the desired quantity in terms of a sum over nodes of the\nreciprocal of their degrees, denoted by $\\beta(G)$. Our results show that\n$\\beta(G)$ can be approximated accurately, based on a provided lower bound on\n$\\beta$. Stronger results are possible when the edges are promised to arrive\ngrouped by an incident node. In this setting, we obtain a value that is at most\na logarithmic factor below the true value of $\\beta$ and no more than the true\nindependent set size. To justify the form of this bound, we also show an\n$\\Omega(n/\\beta)$ lower bound on any algorithm that approximates $\\beta$ up to\na constant factor. \n\n"}
{"id": "1702.08483", "contents": "Title: The computational landscape of general physical theories Abstract: There is good evidence that quantum computers are more powerful than\nclassical computers, and that various simple modifications of quantum theory\nyield computational power that is dramatically greater still. However, these\nmodifications also violate fundamental physical principles. This raises the\nquestion of whether there exists a physical theory, allowing computation more\npowerful than quantum, but which still respects those fundamental physical\nprinciples. Prior work by two of us introduced this question within a suitable\nframework for theories that make good operational sense, and showed that in any\ntheory satisfying tomographic locality, the class of problems that can be\nsolved efficiently is contained in the complexity class AWPP. Here, we show\nthat this bound is tight, in the sense that there exists a theory, satisfying\ntomographic locality, as well as a basic principle of causality, which can\nefficiently decide everything in AWPP. Hence this theory can efficiently\nsimulate any computation in this framework, including quantum computation. \n\n"}
{"id": "1702.08791", "contents": "Title: Robust Budget Allocation via Continuous Submodular Functions Abstract: The optimal allocation of resources for maximizing influence, spread of\ninformation or coverage, has gained attention in the past years, in particular\nin machine learning and data mining. But in applications, the parameters of the\nproblem are rarely known exactly, and using wrong parameters can lead to\nundesirable outcomes. We hence revisit a continuous version of the Budget\nAllocation or Bipartite Influence Maximization problem introduced by Alon et\nal. (2012) from a robust optimization perspective, where an adversary may\nchoose the least favorable parameters within a confidence set. The resulting\nproblem is a nonconvex-concave saddle point problem (or game). We show that\nthis nonconvex problem can be solved exactly by leveraging connections to\ncontinuous submodular functions, and by solving a constrained submodular\nminimization problem. Although constrained submodular minimization is hard in\ngeneral, here, we establish conditions under which such a problem can be solved\nto arbitrary precision $\\epsilon$. \n\n"}
{"id": "1703.00066", "contents": "Title: On the Power of Learning from $k$-Wise Queries Abstract: Several well-studied models of access to data samples, including statistical\nqueries, local differential privacy and low-communication algorithms rely on\nqueries that provide information about a function of a single sample. (For\nexample, a statistical query (SQ) gives an estimate of $Ex_{x \\sim D}[q(x)]$\nfor any choice of the query function $q$ mapping $X$ to the reals, where $D$ is\nan unknown data distribution over $X$.) Yet some data analysis algorithms rely\non properties of functions that depend on multiple samples. Such algorithms\nwould be naturally implemented using $k$-wise queries each of which is\nspecified by a function $q$ mapping $X^k$ to the reals. Hence it is natural to\nask whether algorithms using $k$-wise queries can solve learning problems more\nefficiently and by how much.\n  Blum, Kalai and Wasserman (2003) showed that for any weak PAC learning\nproblem over a fixed distribution, the complexity of learning with $k$-wise SQs\nis smaller than the (unary) SQ complexity by a factor of at most $2^k$. We show\nthat for more general problems over distributions the picture is substantially\nricher. For every $k$, the complexity of distribution-independent PAC learning\nwith $k$-wise queries can be exponentially larger than learning with\n$(k+1)$-wise queries. We then give two approaches for simulating a $k$-wise\nquery using unary queries. The first approach exploits the structure of the\nproblem that needs to be solved. It generalizes and strengthens (exponentially)\nthe results of Blum et al.. It allows us to derive strong lower bounds for\nlearning DNF formulas and stochastic constraint satisfaction problems that hold\nagainst algorithms using $k$-wise queries. The second approach exploits the\n$k$-party communication complexity of the $k$-wise query function. \n\n"}
{"id": "1703.00830", "contents": "Title: Robust Communication-Optimal Distributed Clustering Algorithms Abstract: In this work, we study the $k$-median and $k$-means clustering problems when\nthe data is distributed across many servers and can contain outliers. While\nthere has been a lot of work on these problems for worst-case instances, we\nfocus on gaining a finer understanding through the lens of beyond worst-case\nanalysis. Our main motivation is the following: for many applications such as\nclustering proteins by function or clustering communities in a social network,\nthere is some unknown target clustering, and the hope is that running a\n$k$-median or $k$-means algorithm will produce clusterings which are close to\nmatching the target clustering. Worst-case results can guarantee constant\nfactor approximations to the optimal $k$-median or $k$-means objective value,\nbut not closeness to the target clustering.\n  Our first result is a distributed algorithm which returns a near-optimal\nclustering assuming a natural notion of stability, namely, approximation\nstability [Balcan et. al 2013], even when a constant fraction of the data are\noutliers. The communication complexity is $\\tilde O(sk+z)$ where $s$ is the\nnumber of machines, $k$ is the number of clusters, and $z$ is the number of\noutliers.\n  Next, we show this amount of communication cannot be improved even in the\nsetting when the input satisfies various non-worst-case assumptions. We give a\nmatching $\\Omega(sk+z)$ lower bound on the communication required both for\napproximating the optimal $k$-means or $k$-median cost up to any constant, and\nfor returning a clustering that is close to the target clustering in Hamming\ndistance. These lower bounds hold even when the data satisfies approximation\nstability or other common notions of stability, and the cluster sizes are\nbalanced. Therefore, $\\Omega(sk+z)$ is a communication bottleneck, even for\nreal-world instances. \n\n"}
{"id": "1703.00941", "contents": "Title: On the Fine-grained Complexity of One-Dimensional Dynamic Programming Abstract: In this paper, we investigate the complexity of one-dimensional dynamic\nprogramming, or more specifically, of the Least-Weight Subsequence (LWS)\nproblem: Given a sequence of $n$ data items together with weights for every\npair of the items, the task is to determine a subsequence $S$ minimizing the\ntotal weight of the pairs adjacent in $S$. A large number of natural problems\ncan be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.\n  In many interesting instances, the $O(n^2)$-many weights can be succinctly\nrepresented. Yet except for near-linear time algorithms for some specific\nspecial cases, little is known about when an LWS instantiation admits a\nsubquadratic-time algorithm and when it does not. In particular, no lower\nbounds for LWS instantiations have been known before. In an attempt to remedy\nthis situation, we provide a general approach to study the fine-grained\ncomplexity of succinct instantiations of the LWS problem. In particular, given\nan LWS instantiation we identify a highly parallel core problem that is\nsubquadratically equivalent. This provides either an explanation for the\napparent hardness of the problem or an avenue to find improved algorithms as\nthe case may be.\n  More specifically, we prove subquadratic equivalences between the following\npairs (an LWS instantiation and the corresponding core problem) of problems: a\nlow-rank version of LWS and minimum inner product, finding the longest chain of\nnested boxes and vector domination, and a coin change problem which is closely\nrelated to the knapsack problem and (min,+)-convolution. Using these\nequivalences and known SETH-hardness results for some of the core problems, we\ndeduce tight conditional lower bounds for the corresponding LWS instantiations.\nWe also establish the (min,+)-convolution-hardness of the knapsack problem. \n\n"}
{"id": "1703.01913", "contents": "Title: Near-Optimal Closeness Testing of Discrete Histogram Distributions Abstract: We investigate the problem of testing the equivalence between two discrete\nhistograms. A {\\em $k$-histogram} over $[n]$ is a probability distribution that\nis piecewise constant over some set of $k$ intervals over $[n]$. Histograms\nhave been extensively studied in computer science and statistics. Given a set\nof samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to\ndistinguish (with high probability) between the cases that $p = q$ and\n$\\|p-q\\|_1 \\geq \\epsilon$. The main contribution of this paper is a new\nalgorithm for this testing problem and a nearly matching information-theoretic\nlower bound. Specifically, the sample complexity of our algorithm matches our\nlower bound up to a logarithmic factor, improving on previous work by\npolynomial factors in the relevant parameters. Our algorithmic approach applies\nin a more general setting and yields improved sample upper bounds for testing\ncloseness of other structured distributions as well. \n\n"}
{"id": "1703.04143", "contents": "Title: Bernoulli Factories and Black-Box Reductions in Mechanism Design Abstract: We provide a polynomial time reduction from Bayesian incentive compatible\nmechanism design to Bayesian algorithm design for welfare maximization\nproblems. Unlike prior results, our reduction achieves exact incentive\ncompatibility for problems with multi-dimensional and continuous type spaces.\nThe key technical barrier preventing exact incentive compatibility in prior\nblack-box reductions is that repairing violations of incentive constraints\nrequires understanding the distribution of the mechanism's output, which is\ntypically #P-hard to compute. Reductions that instead estimate the output\ndistribution by sampling inevitably suffer from sampling error, which typically\nprecludes exact incentive compatibility.\n  We overcome this barrier by employing and generalizing the computational\nmodel in the literature on $\\textit{Bernoulli factories}$. In a Bernoulli\nfactory problem, one is given a function mapping the bias of an \"input coin\" to\nthat of an \"output coin\", and the challenge is to efficiently simulate the\noutput coin given only sample access to the input coin. This is the key\ningredient in designing an incentive compatible mechanism for bipartite\nmatching, which can be used to make the approximately incentive compatible\nreduction of Hartline et al. (2015) exactly incentive compatible. \n\n"}
{"id": "1703.04505", "contents": "Title: A remark on a construction of D.S. Asche Abstract: It is shown that a classical construction of D.S. Asche of 72 equiangular\nlines in the 19 dimensional Euclidean space contains a subset of 54 equiangular\nlines embedded in an 18 dimensional subspace. \n\n"}
{"id": "1703.06040", "contents": "Title: Towards a Topology-Shape-Metrics Framework for Ortho-Radial Drawings Abstract: Ortho-Radial drawings are a generalization of orthogonal drawings to grids\nthat are formed by concentric circles and straight-line spokes emanating from\nthe circles' center. Such drawings have applications in schematic graph\nlayouts, e.g., for metro maps and destination maps.\n  A plane graph is a planar graph with a fixed planar embedding. We give a\ncombinatorial characterization of the plane graphs that admit a planar\northo-radial drawing without bends. Previously, such a characterization was\nonly known for paths, cycles, and theta graphs, and in the special case of\nrectangular drawings for cubic graphs, where the contour of each face is\nrequired to be a rectangle.\n  The characterization is expressed in terms of an ortho-radial representation\nthat, similar to Tamassia's orthogonal representations for orthogonal drawings\ndescribes such a drawing combinatorially in terms of angles around vertices and\nbends on the edges. In this sense our characterization can be seen as a first\nstep towards generalizing the Topology-Shape-Metrics framework of Tamassia to\northo-radial drawings. \n\n"}
{"id": "1703.06048", "contents": "Title: An FPTAS for the Knapsack Problem with Parametric Weights Abstract: In this paper, we investigate the parametric weight knapsack problem, in\nwhich the item weights are affine functions of the form $w_i(\\lambda) = a_i +\n\\lambda \\cdot b_i$ for $i \\in \\{1,\\ldots,n\\}$ depending on a real-valued\nparameter $\\lambda$. The aim is to provide a solution for all values of the\nparameter. It is well-known that any exact algorithm for the problem may need\nto output an exponential number of knapsack solutions. We present the first\nfully polynomial-time approximation scheme (FPTAS) for the problem that, for\nany desired precision $\\varepsilon \\in (0,1)$, computes\n$(1-\\varepsilon)$-approximate solutions for all values of the parameter. Our\nFPTAS is based on two different approaches and achieves a running time of\n$\\mathcal{O}(n^3/\\varepsilon^2 \\cdot \\min\\{ \\log^2 P, n^2 \\} \\cdot \\min\\{\\log\nM, n \\log (n/\\varepsilon) / \\log(n \\log (n/\\varepsilon) )\\})$ where $P$ is an\nupper bound on the optimal profit and $M := \\max\\{W, n \\cdot \\max\\{a_i,b_i: i\n\\in \\{1,\\ldots,n\\}\\}\\}$ for a knapsack with capacity $W$. \n\n"}
{"id": "1703.10550", "contents": "Title: Proof of L\\'aszl\\'o Fejes T\\'oth's zone conjecture Abstract: A zone of width $\\omega$ on the unit sphere is the set of points within\nspherical distance $\\omega/2$ of a given great circle. We show that the total\nwidth of any collection of zones covering the unit sphere is at least $\\pi$,\nanswering a question of Fejes T\\'oth from 1973. \n\n"}
{"id": "1704.00249", "contents": "Title: Complexity of short Presburger arithmetic Abstract: We study complexity of short sentences in Presburger arithmetic (Short-PA).\nHere by \"short\" we mean sentences with a bounded number of variables,\nquantifiers, inequalities and Boolean operations; the input consists only of\nthe integers involved in the inequalities. We prove that assuming Kannan's\npartition can be found in polynomial time, the satisfiability of Short-PA\nsentences can be decided in polynomial time. Furthermore, under the same\nassumption, we show that the numbers of satisfying assignments of short\nPresburger sentences can also be computed in polynomial time. \n\n"}
{"id": "1704.00633", "contents": "Title: Optimal lower bounds for universal relation, and for samplers and\n  finding duplicates in streams Abstract: In the communication problem $\\mathbf{UR}$ (universal relation) [KRW95],\nAlice and Bob respectively receive $x, y \\in\\{0,1\\}^n$ with the promise that\n$x\\neq y$. The last player to receive a message must output an index $i$ such\nthat $x_i\\neq y_i$. We prove that the randomized one-way communication\ncomplexity of this problem in the public coin model is exactly\n$\\Theta(\\min\\{n,\\log(1/\\delta)\\log^2(\\frac n{\\log(1/\\delta)})\\})$ for failure\nprobability $\\delta$. Our lower bound holds even if promised\n$\\mathop{support}(y)\\subset \\mathop{support}(x)$. As a corollary, we obtain\noptimal lower bounds for $\\ell_p$-sampling in strict turnstile streams for\n$0\\le p < 2$, as well as for the problem of finding duplicates in a stream. Our\nlower bounds do not need to use large weights, and hold even if promised\n$x\\in\\{0,1\\}^n$ at all points in the stream.\n  We give two different proofs of our main result. The first proof demonstrates\nthat any algorithm $\\mathcal A$ solving sampling problems in turnstile streams\nin low memory can be used to encode subsets of $[n]$ of certain sizes into a\nnumber of bits below the information theoretic minimum. Our encoder makes\nadaptive queries to $\\mathcal A$ throughout its execution, but done carefully\nso as to not violate correctness. This is accomplished by injecting random\nnoise into the encoder's interactions with $\\mathcal A$, which is loosely\nmotivated by techniques in differential privacy. Our second proof is via a\nnovel randomized reduction from Augmented Indexing [MNSW98] which needs to\ninteract with $\\mathcal A$ adaptively. To handle the adaptivity we identify\ncertain likely interaction patterns and union bound over them to guarantee\ncorrect interaction on all of them. To guarantee correctness, it is important\nthat the interaction hides some of its randomness from $\\mathcal A$ in the\nreduction. \n\n"}
{"id": "1704.00765", "contents": "Title: Quantum Algorithms for Graph Connectivity and Formula Evaluation Abstract: We give a new upper bound on the quantum query complexity of deciding\n$st$-connectivity on certain classes of planar graphs, and show the bound is\nsometimes exponentially better than previous results. We then show Boolean\nformula evaluation reduces to deciding connectivity on just such a class of\ngraphs. Applying the algorithm for $st$-connectivity to Boolean formula\nevaluation problems, we match the $O(\\sqrt{N})$ bound on the quantum query\ncomplexity of evaluating formulas on $N$ variables, give a quadratic speed-up\nover the classical query complexity of a certain class of promise Boolean\nformulas, and show this approach can yield superpolynomial quantum/classical\nseparations. These results indicate that this $st$-connectivity-based approach\nmay be the \"right\" way of looking at quantum algorithms for formula evaluation. \n\n"}
{"id": "1704.00777", "contents": "Title: The Unbounded-Error Communication Complexity of symmetric XOR functions Abstract: Settling a conjecture of Shi and Zhang, we determine the unbounded-error\ncommunication complexity of the symmetric XOR functions up to a\npoly-logarithmic factor. Our proof is by a simple reduction to an earlier\nresult of Sherstov. \n\n"}
{"id": "1704.01460", "contents": "Title: Comparison Based Nearest Neighbor Search Abstract: We consider machine learning in a comparison-based setting where we are given\na set of points in a metric space, but we have no access to the actual\ndistances between the points. Instead, we can only ask an oracle whether the\ndistance between two points $i$ and $j$ is smaller than the distance between\nthe points $i$ and $k$. We are concerned with data structures and algorithms to\nfind nearest neighbors based on such comparisons. We focus on a simple yet\neffective algorithm that recursively splits the space by first selecting two\nrandom pivot points and then assigning all other points to the closer of the\ntwo (comparison tree). We prove that if the metric space satisfies certain\nexpansion conditions, then with high probability the height of the comparison\ntree is logarithmic in the number of points, leading to efficient search\nperformance. We also provide an upper bound for the failure probability to\nreturn the true nearest neighbor. Experiments show that the comparison tree is\ncompetitive with algorithms that have access to the actual distance values, and\nneeds less triplet comparisons than other competitors. \n\n"}
{"id": "1704.02958", "contents": "Title: On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel\n  Methods and Neural Networks Abstract: Empirical risk minimization (ERM) is ubiquitous in machine learning and\nunderlies most supervised learning methods. While there has been a large body\nof work on algorithms for various ERM problems, the exact computational\ncomplexity of ERM is still not understood. We address this issue for multiple\npopular ERM problems including kernel SVMs, kernel ridge regression, and\ntraining the final layer of a neural network. In particular, we give\nconditional hardness results for these problems based on complexity-theoretic\nassumptions such as the Strong Exponential Time Hypothesis. Under these\nassumptions, we show that there are no algorithms that solve the aforementioned\nERM problems to high accuracy in sub-quadratic time. We also give similar\nhardness results for computing the gradient of the empirical loss, which is the\nmain computational burden in many non-convex learning tasks. \n\n"}
{"id": "1704.03664", "contents": "Title: Approximating Optimization Problems using EAs on Scale-Free Networks Abstract: It has been observed that many complex real-world networks have certain\nproperties, such as a high clustering coefficient, a low diameter, and a\npower-law degree distribution. A network with a power-law degree distribution\nis known as scale-free network. In order to study these networks, various\nrandom graph models have been proposed, e.g. Preferential Attachment, Chung-Lu,\nor Hyperbolic.\n  We look at the interplay between the power-law degree distribution and the\nrun time of optimization techniques for well known combinatorial problems. We\nobserve that on scale-free networks, simple evolutionary algorithms (EAs)\nquickly reach a constant-factor approximation ratio on common covering problems\n  We prove that the single-objective (1+1)EA reaches a constant-factor\napproximation ratio on the Minimum Dominating Set problem, the Minimum Vertex\nCover problem, the Minimum Connected Dominating Set problem, and the Maximum\nIndependent Set problem in expected polynomial number of calls to the fitness\nfunction.\n  Furthermore, we prove that the multi-objective GSEMO algorithm reaches a\nbetter approximation ratio than the (1+1)EA on those problems, within\npolynomial fitness evaluations. \n\n"}
{"id": "1704.03864", "contents": "Title: A Matrix Expander Chernoff Bound Abstract: We prove a Chernoff-type bound for sums of matrix-valued random variables\nsampled via a random walk on an expander, confirming a conjecture due to\nWigderson and Xiao. Our proof is based on a new multi-matrix extension of the\nGolden-Thompson inequality which improves in some ways the inequality of\nSutter, Berta, and Tomamichel, and may be of independent interest, as well as\nan adaptation of an argument for the scalar case due to Healy. Secondarily, we\nalso provide a generic reduction showing that any concentration inequality for\nvector-valued martingales implies a concentration inequality for the\ncorresponding expander walk, with a weakening of parameters proportional to the\nsquared mixing time. \n\n"}
{"id": "1704.04546", "contents": "Title: SETH-Based Lower Bounds for Subset Sum and Bicriteria Path Abstract: Subset-Sum and k-SAT are two of the most extensively studied problems in\ncomputer science, and conjectures about their hardness are among the\ncornerstones of fine-grained complexity. One of the most intriguing open\nproblems in this area is to base the hardness of one of these problems on the\nother.\n  Our main result is a tight reduction from k-SAT to Subset-Sum on dense\ninstances, proving that Bellman's 1962 pseudo-polynomial $O^{*}(T)$-time\nalgorithm for Subset-Sum on $n$ numbers and target $T$ cannot be improved to\ntime $T^{1-\\varepsilon}\\cdot 2^{o(n)}$ for any $\\varepsilon>0$, unless the\nStrong Exponential Time Hypothesis (SETH) fails. This is one of the strongest\nknown connections between any two of the core problems of fine-grained\ncomplexity.\n  As a corollary, we prove a \"Direct-OR\" theorem for Subset-Sum under SETH,\noffering a new tool for proving conditional lower bounds: It is now possible to\nassume that deciding whether one out of $N$ given instances of Subset-Sum is a\nYES instance requires time $(N T)^{1-o(1)}$. As an application of this\ncorollary, we prove a tight SETH-based lower bound for the classical Bicriteria\ns,t-Path problem, which is extensively studied in Operations Research. We\nseparate its complexity from that of Subset-Sum: On graphs with $m$ edges and\nedge lengths bounded by $L$, we show that the $O(Lm)$ pseudo-polynomial time\nalgorithm by Joksch from 1966 cannot be improved to $\\tilde{O}(L+m)$, in\ncontrast to a recent improvement for Subset Sum (Bringmann, SODA 2017). \n\n"}
{"id": "1704.05303", "contents": "Title: The Robot Routing Problem for Collecting Aggregate Stochastic Rewards Abstract: We propose a new model for formalizing reward collection problems on graphs\nwith dynamically generated rewards which may appear and disappear based on a\nstochastic model. The *robot routing problem* is modeled as a graph whose nodes\nare stochastic processes generating potential rewards over discrete time. The\nrewards are generated according to the stochastic process, but at each step, an\nexisting reward disappears with a given probability. The edges in the graph\nencode the (unit-distance) paths between the rewards' locations. On visiting a\nnode, the robot collects the accumulated reward at the node at that time, but\ntraveling between the nodes takes time. The optimization question asks to\ncompute an optimal (or epsilon-optimal) path that maximizes the expected\ncollected rewards.\n  We consider the finite and infinite-horizon robot routing problems. For\nfinite-horizon, the goal is to maximize the total expected reward, while for\ninfinite horizon we consider limit-average objectives. We study the\ncomputational and strategy complexity of these problems, establish NP-lower\nbounds and show that optimal strategies require memory in general. We also\nprovide an algorithm for computing epsilon-optimal infinite paths for arbitrary\nepsilon > 0. \n\n"}
{"id": "1704.06774", "contents": "Title: Quantum algorithm for tree size estimation, with applications to\n  backtracking and 2-player games Abstract: We study quantum algorithms on search trees of unknown structure, in a model\nwhere the tree can be discovered by local exploration. That is, we are given\nthe root of the tree and access to a black box which, given a vertex $v$,\noutputs the children of $v$.\n  We construct a quantum algorithm which, given such access to a search tree of\ndepth at most $n$, estimates the size of the tree $T$ within a factor of $1\\pm\n\\delta$ in $\\tilde{O}(\\sqrt{nT})$ steps. More generally, the same algorithm can\nbe used to estimate size of directed acyclic graphs (DAGs) in a similar model.\n  We then show two applications of this result: a) We show how to transform a\nclassical backtracking search algorithm which examines $T$ nodes of a search\ntree into an $\\tilde{O}(\\sqrt{T}n^{3/2})$ time quantum algorithm, improving\nover an earlier quantum backtracking algorithm of Montanaro (arXiv:1509.02374).\nb) We give a quantum algorithm for evaluating AND-OR formulas in a model where\nthe formula can be discovered by local exploration (modeling position trees in\n2-player games). We show that, in this setting, formulas of size $T$ and depth\n$T^{o(1)}$ can be evaluated in quantum time $O(T^{1/2+o(1)})$. Thus, the\nquantum speedup is essentially the same as in the case when the formula is\nknown in advance. \n\n"}
{"id": "1704.06847", "contents": "Title: A hybrid primal heuristic for Robust Multiperiod Network Design Abstract: We investigate the Robust Multiperiod Network Design Problem, a\ngeneralization of the classical Capacitated Network Design Problem that\nadditionally considers multiple design periods and provides solutions protected\nagainst traffic uncertainty. Given the intrinsic difficulty of the problem,\nwhich proves challenging even for state-of-the art commercial solvers, we\npropose a hybrid primal heuristic based on the combination of ant colony\noptimization and an exact large neighborhood search. Computational experiments\non a set of realistic instances from the SNDlib show that our heuristic can\nfind solutions of extremely good quality with low optimality gap. \n\n"}
{"id": "1704.07852", "contents": "Title: Sub-string/Pattern Matching in Sub-linear Time Using a Sparse Fourier\n  Transform Approach Abstract: We consider the problem of querying a string (or, a database) of length $N$\nbits to determine all the locations where a substring (query) of length $M$\nappears either exactly or is within a Hamming distance of $K$ from the query.\nWe assume that sketches of the original signal can be computed off line and\nstored. Using the sparse Fourier transform computation based approach\nintroduced by Pawar and Ramchandran, we show that all such matches can be\ndetermined with high probability in sub-linear time. Specifically, if the query\nlength $M = O(N^\\mu)$ and the number of matches $L=O(N^\\lambda)$, we show that\nfor $\\lambda < 1-\\mu$ all the matching positions can be determined with a\nprobability that approaches 1 as $N \\rightarrow \\infty$ for $K \\leq\n\\frac{1}{6}M$. More importantly our scheme has a worst-case computational\ncomplexity that is only $O\\left(\\max\\{N^{1-\\mu}\\log^2 N, N^{\\mu+\\lambda}\\log N\n\\}\\right)$, which means we can recover all the matching positions in {\\it\nsub-linear} time for $\\lambda<1-\\mu$. This is a substantial improvement over\nthe best known computational complexity of $O\\left(N^{1-0.359 \\mu} \\right)$ for\nrecovering one matching position by Andoni {\\em et al.} \\cite{andoni2013shift}.\nFurther, the number of Fourier transform coefficients that need to be computed,\nstored and accessed, i.e., the sketching complexity of this algorithm is only\n$O\\left(N^{1-\\mu}\\log N\\right)$. Several extensions of the main theme are also\ndiscussed. \n\n"}
{"id": "1704.08462", "contents": "Title: Communication complexity of approximate maximum matching in the\n  message-passing model Abstract: We consider the communication complexity of finding an approximate maximum\nmatching in a graph in a multi-party message-passing communication model. The\nmaximum matching problem is one of the most fundamental graph combinatorial\nproblems, with a variety of applications.\n  The input to the problem is a graph $G$ that has $n$ vertices and the set of\nedges partitioned over $k$ sites, and an approximation ratio parameter\n$\\alpha$. The output is required to be a matching in $G$ that has to be\nreported by one of the sites, whose size is at least factor $\\alpha$ of the\nsize of a maximum matching in $G$.\n  We show that the communication complexity of this problem is $\\Omega(\\alpha^2\nk n)$ information bits. This bound is shown to be tight up to a $\\log n$\nfactor, by constructing an algorithm, establishing its correctness, and an\nupper bound on the communication cost. The lower bound also applies to other\ngraph combinatorial problems in the message-passing communication model,\nincluding max-flow and graph sparsification. \n\n"}
{"id": "1704.08683", "contents": "Title: Matrix Completion and Related Problems via Strong Duality Abstract: This work studies the strong duality of non-convex matrix factorization\nproblems: we show that under certain dual conditions, these problems and its\ndual have the same optimum. This has been well understood for convex\noptimization, but little was known for non-convex problems. We propose a novel\nanalytical framework and show that under certain dual conditions, the optimal\nsolution of the matrix factorization program is the same as its bi-dual and\nthus the global optimality of the non-convex program can be achieved by solving\nits bi-dual which is convex. These dual conditions are satisfied by a wide\nclass of matrix factorization problems, although matrix factorization problems\nare hard to solve in full generality. This analytical framework may be of\nindependent interest to non-convex optimization more broadly.\n  We apply our framework to two prototypical matrix factorization problems:\nmatrix completion and robust Principal Component Analysis (PCA). These are\nexamples of efficiently recovering a hidden matrix given limited reliable\nobservations of it. Our framework shows that exact recoverability and strong\nduality hold with nearly-optimal sample complexity guarantees for matrix\ncompletion and robust PCA. \n\n"}
{"id": "1704.08868", "contents": "Title: Structural Parameters, Tight Bounds, and Approximation for (k,r)-Center Abstract: In $(k,r)$-Center we are given a (possibly edge-weighted) graph and are asked\nto select at most $k$ vertices (centers), so that all other vertices are at\ndistance at most $r$ from a center. In this paper we provide a number of tight\nfine-grained bounds on the complexity of this problem with respect to various\nstandard graph parameters. Specifically:\n  - For any $r\\ge 1$, we show an algorithm that solves the problem in\n$O^*((3r+1)^{\\textrm{cw}})$ time, where $\\textrm{cw}$ is the clique-width of\nthe input graph, as well as a tight SETH lower bound matching this algorithm's\nperformance. As a corollary, for $r=1$, this closes the gap that previously\nexisted on the complexity of Dominating Set parameterized by $\\textrm{cw}$.\n  - We strengthen previously known FPT lower bounds, by showing that\n$(k,r)$-Center is W[1]-hard parameterized by the input graph's vertex cover (if\nedge weights are allowed), or feedback vertex set, even if $k$ is an additional\nparameter. Our reductions imply tight ETH-based lower bounds. Finally, we\ndevise an algorithm parameterized by vertex cover for unweighted graphs.\n  - We show that the complexity of the problem parameterized by tree-depth is\n$2^{\\Theta(\\textrm{td}^2)}$ by showing an algorithm of this complexity and a\ntight ETH-based lower bound.\n  We complement these mostly negative results by providing FPT approximation\nschemes parameterized by clique-width or treewidth which work efficiently\nindependently of the values of $k,r$. In particular, we give algorithms which,\nfor any $\\epsilon>0$, run in time\n$O^*((\\textrm{tw}/\\epsilon)^{O(\\textrm{tw})})$,\n$O^*((\\textrm{cw}/\\epsilon)^{O(\\textrm{cw})})$ and return a\n$(k,(1+\\epsilon)r)$-center, if a $(k,r)$-center exists, thus circumventing the\nproblem's W-hardness. \n\n"}
{"id": "1705.00055", "contents": "Title: Charting the Complexity Landscape of Waypoint Routing Abstract: Modern computer networks support interesting new routing models in which\ntraffic flows from a source s to a destination t can be flexibly steered\nthrough a sequence of waypoints, such as (hardware) middleboxes or\n(virtualized) network functions, to create innovative network services like\nservice chains or segment routing. While the benefits and technological\nchallenges of providing such routing models have been articulated and studied\nintensively over the last years, much less is known about the underlying\nalgorithmic traffic routing problems. This paper shows that the waypoint\nrouting problem features a deep combinatorial structure, and we establish\ninteresting connections to several classic graph theoretical problems. We find\nthat the difficulty of the waypoint routing problem depends on the specific\nsetting, and chart a comprehensive landscape of the computational complexity.\nIn particular, we derive several NP-hardness results, but we also demonstrate\nthat exact polynomial-time algorithms exist for a wide range of practically\nrelevant scenarios. \n\n"}
{"id": "1705.01240", "contents": "Title: Consistency of orthology and paralogy constraints in the presence of\n  gene transfers Abstract: Orthology and paralogy relations are often inferred by methods based on gene\nsimilarity, which usually yield a graph depicting the relationships between\ngene pairs. Such relation graphs are known to frequently contain errors, as\nthey cannot be explained via a gene tree that both contains the depicted\northologs/paralogs, and that is consistent with a species tree $S$. This idea\nof detecting errors through inconsistency with a species tree has mostly been\nstudied in the presence of speciation and duplication events only. In this\nwork, we ask: could the given set of relations be consistent if we allow\nlateral gene transfers in the evolutionary model? We formalize this question\nand provide a variety of algorithmic results regarding the underlying problems.\nNamely, we show that deciding if a relation graph $R$ is consistent with a\ngiven species network $N$ is NP-hard, and that it is W[1]-hard under the\nparameter \"minimum number of transfers\". However, we present an FPT algorithm\nbased on the degree of the $DS$-tree associated with $R$. We also study\nanalogous problems in the case that the transfer highways on a species tree are\nunknown. \n\n"}
{"id": "1705.01414", "contents": "Title: Covering Small Independent Sets and Separators with Applications to\n  Parameterized Algorithms Abstract: We present two new combinatorial tools for the design of parameterized\nalgorithms. The first is a simple linear time randomized algorithm that given\nas input a $d$-degenerate graph $G$ and an integer $k$, outputs an independent\nset $Y$, such that for every independent set $X$ in $G$ of size at most $k$,\nthe probability that $X$ is a subset of $Y$ is at least $\\left({(d+1)k \\choose\nk} \\cdot k(d+1)\\right)^{-1}$.The second is a new (deterministic) polynomial\ntime graph sparsification procedure that given a graph $G$, a set $T = \\{\\{s_1,\nt_1\\}, \\{s_2, t_2\\}, \\ldots, \\{s_\\ell, t_\\ell\\}\\}$ of terminal pairs and an\ninteger $k$, returns an induced subgraph $G^\\star$ of $G$ that maintains all\nthe inclusion minimal multicuts of $G$ of size at most $k$, and does not\ncontain any $(k+2)$-vertex connected set of size $2^{{\\cal O}(k)}$. In\nparticular, $G^\\star$ excludes a clique of size $2^{{\\cal O}(k)}$ as a\ntopological minor. Put together, our new tools yield new randomized fixed\nparameter tractable (FPT) algorithms for Stable $s$-$t$ Separator, Stable Odd\nCycle Transversal and Stable Multicut on general graphs, and for Stable\nDirected Feedback Vertex Set on $d$-degenerate graphs, resolving two problems\nleft open by Marx et al. [ACM Transactions on Algorithms, 2013]. All of our\nalgorithms can be derandomized at the cost of a small overhead in the running\ntime. \n\n"}
{"id": "1705.01843", "contents": "Title: Quantum SDP-Solvers: Better upper and lower bounds Abstract: Brand\\~ao and Svore very recently gave quantum algorithms for approximately\nsolving semidefinite programs, which in some regimes are faster than the\nbest-possible classical algorithms in terms of the dimension $n$ of the problem\nand the number $m$ of constraints, but worse in terms of various other\nparameters. In this paper we improve their algorithms in several ways, getting\nbetter dependence on those other parameters. To this end we develop new\ntechniques for quantum algorithms, for instance a general way to efficiently\nimplement smooth functions of sparse Hamiltonians, and a generalized\nminimum-finding procedure.\n  We also show limits on this approach to quantum SDP-solvers, for instance for\ncombinatorial optimizations problems that have a lot of symmetry. Finally, we\nprove some general lower bounds showing that in the worst case, the complexity\nof every quantum LP-solver (and hence also SDP-solver) has to scale linearly\nwith $mn$ when $m\\approx n$, which is the same as classical. \n\n"}
{"id": "1705.02944", "contents": "Title: Hardness Results for Structured Linear Systems Abstract: We show that if the nearly-linear time solvers for Laplacian matrices and\ntheir generalizations can be extended to solve just slightly larger families of\nlinear systems, then they can be used to quickly solve all systems of linear\nequations over the reals. This result can be viewed either positively or\nnegatively: either we will develop nearly-linear time algorithms for solving\nall systems of linear equations over the reals, or progress on the families we\ncan solve in nearly-linear time will soon halt. \n\n"}
{"id": "1705.03283", "contents": "Title: An exponential lower bound for Individualization-Refinement algorithms\n  for Graph Isomorphism Abstract: The individualization-refinement paradigm provides a strong toolbox for\ntesting isomorphism of two graphs and indeed, the currently fastest\nimplementations of isomorphism solvers all follow this approach. While these\nsolvers are fast in practice, from a theoretical point of view, no general\nlower bounds concerning the worst case complexity of these tools are known. In\nfact, it is an open question whether individualization-refinement algorithms\ncan achieve upper bounds on the running time similar to the more theoretical\ntechniques based on a group theoretic approach.\n  In this work we give a negative answer to this question and construct a\nfamily of graphs on which algorithms based on the individualization-refinement\nparadigm require exponential time. Contrary to a previous construction of\nMiyazaki, that only applies to a specific implementation within the\nindividualization-refinement framework, our construction is immune to changing\nthe cell selector, or adding various heuristic invariants to the algorithm.\nFurthermore, our graphs also provide exponential lower bounds in the case when\nthe $k$-dimensional Weisfeiler-Leman algorithm is used to replace the standard\ncolor refinement operator and the arguments even work when the entire\nautomorphism group of the inputs is initially provided to the algorithm. \n\n"}
{"id": "1705.05735", "contents": "Title: Comparison-Based Choices Abstract: A broad range of on-line behaviors are mediated by interfaces in which people\nmake choices among sets of options. A rich and growing line of work in the\nbehavioral sciences indicate that human choices follow not only from the\nutility of alternatives, but also from the choice set in which alternatives are\npresented. In this work we study comparison-based choice functions, a simple\nbut surprisingly rich class of functions capable of exhibiting so-called\nchoice-set effects. Motivated by the challenge of predicting complex choices,\nwe study the query complexity of these functions in a variety of settings. We\nconsider settings that allow for active queries or passive observation of a\nstream of queries, and give analyses both at the granularity of individuals or\npopulations that might exhibit heterogeneous choice behavior. Our main result\nis that any comparison-based choice function in one dimension can be inferred\nas efficiently as a basic maximum or minimum choice function across many query\ncontexts, suggesting that choice-set effects need not entail any fundamental\nalgorithmic barriers to inference. We also introduce a class of choice\nfunctions we call distance-comparison-based functions, and briefly discuss the\nanalysis of such functions. The framework we outline provides intriguing\nconnections between human choice behavior and a range of questions in the\ntheory of sorting. \n\n"}
{"id": "1705.07728", "contents": "Title: Improved method for finding optimal formulae for bilinear maps in a\n  finite field Abstract: In 2012, Barbulescu, Detrey, Estibals and Zimmermann proposed a new framework\nto exhaustively search for optimal formulae for evaluating bilinear maps, such\nas Strassen or Karatsuba formulae. The main contribution of this work is a new\ncriterion to aggressively prune useless branches in the exhaustive search, thus\nleading to the computation of new optimal formulae, in particular for the short\nproduct modulo X 5 and the circulant product modulo (X 5 -- 1). Moreover , we\nare able to prove that there is essentially only one optimal decomposition of\nthe product of 3 x 2 by 2 x 3 matrices up to the action of some group of\nautomorphisms. \n\n"}
{"id": "1705.08213", "contents": "Title: Parallel Accelerated Custom Correlation Coefficient Calculations for\n  Genomics Applications Abstract: The massive quantities of genomic data being made available through gene\nsequencing techniques are enabling breakthroughs in genomic science in many\nareas such as medical advances in the diagnosis and treatment of diseases.\nAnalyzing this data, however, is a computational challenge insofar as the\ncomputational costs of the relevant algorithms can grow with quadratic, cubic\nor higher complexity-leading to the need for leadership scale computing. In\nthis paper we describe a new approach to calculations of the Custom Correlation\nCoefficient (CCC) between Single Nucleotide Polymorphisms (SNPs) across a\npopulation, suitable for parallel systems equipped with graphics processing\nunits (GPUs) or Intel Xeon Phi processors. We describe the mapping of the\nalgorithms to accelerated processors, techniques used for eliminating redundant\ncalculations due to symmetries, and strategies for efficient mapping of the\ncalculations to many-node parallel systems. Results are presented demonstrating\nhigh per-node performance and near-ideal parallel scalability with rates of\nmore than nine quadrillion elementwise comparisons achieved per second with the\nlatest optimized code on the ORNL Titan system, this being orders of magnitude\nfaster than rates achieved using other codes and platforms as reported in the\nliterature. Also it is estimated that as many as 90 quadrillion comparisons per\nsecond may be achievable on the upcoming ORNL Summit system, an additional 10X\nperformance increase. In a companion paper we describe corresponding techniques\napplied to calculations of the Proportional Similarity metric for comparative\ngenomics applications. \n\n"}
{"id": "1705.09253", "contents": "Title: Arrangements of homothets of a convex body II Abstract: A family of homothets of an o-symmetric convex body K in d-dimensional\nEuclidean space is called a Minkowski arrangement if no homothet contains the\ncenter of any other homothet in its interior. We show that any pairwise\nintersecting Minkowski arrangement of a d-dimensional convex body has at most\n$2\\cdot 3^d$ members. This improves a result of Polyanskii (arXiv:1610.04400).\nUsing similar ideas, we also give a proof the following result of Polyanskii:\nLet $K_1,\\dots,K_n$ be a sequence of homothets of the o-symmetric convex body\n$K$, such that for any $i<j$, the center of $K_j$ lies on the boundary of\n$K_i$. Then $n\\leq O(3^d d)$. \n\n"}
{"id": "1706.01260", "contents": "Title: The Classical Complexity of Boson Sampling Abstract: We study the classical complexity of the exact Boson Sampling problem where\nthe objective is to produce provably correct random samples from a particular\nquantum mechanical distribution. The computational framework was proposed by\nAaronson and Arkhipov in 2011 as an attainable demonstration of `quantum\nsupremacy', that is a practical quantum computing experiment able to produce\noutput at a speed beyond the reach of classical (that is non-quantum) computer\nhardware. Since its introduction Boson Sampling has been the subject of intense\ninternational research in the world of quantum computing. On the face of it,\nthe problem is challenging for classical computation. Aaronson and Arkhipov\nshow that exact Boson Sampling is not efficiently solvable by a classical\ncomputer unless $P^{\\#P} = BPP^{NP}$ and the polynomial hierarchy collapses to\nthe third level.\n  The fastest known exact classical algorithm for the standard Boson Sampling\nproblem takes $O({m + n -1 \\choose n} n 2^n )$ time to produce samples for a\nsystem with input size $n$ and $m$ output modes, making it infeasible for\nanything but the smallest values of $n$ and $m$. We give an algorithm that is\nmuch faster, running in $O(n 2^n + \\operatorname{poly}(m,n))$ time and $O(m)$\nadditional space. The algorithm is simple to implement and has low constant\nfactor overheads. As a consequence our classical algorithm is able to solve the\nexact Boson Sampling problem for system sizes far beyond current photonic\nquantum computing experimentation, thereby significantly reducing the\nlikelihood of achieving near-term quantum supremacy in the context of Boson\nSampling. \n\n"}
{"id": "1706.02205", "contents": "Title: Compression, inversion, and approximate PCA of dense kernel matrices at\n  near-linear computational complexity Abstract: Dense kernel matrices $\\Theta \\in \\mathbb{R}^{N \\times N}$ obtained from\npoint evaluations of a covariance function $G$ at locations $\\{ x_{i} \\}_{1\n\\leq i \\leq N} \\subset \\mathbb{R}^{d}$ arise in statistics, machine learning,\nand numerical analysis. For covariance functions that are Green's functions of\nelliptic boundary value problems and homogeneously-distributed sampling points,\nwe show how to identify a subset $S \\subset \\{ 1 , \\dots , N \\}^2$, with $\\# S\n= O ( N \\log (N) \\log^{d} ( N /\\epsilon ) )$, such that the zero fill-in\nincomplete Cholesky factorisation of the sparse matrix $\\Theta_{ij} 1_{( i, j )\n\\in S}$ is an $\\epsilon$-approximation of $\\Theta$. This factorisation can\nprovably be obtained in complexity $O ( N \\log( N ) \\log^{d}( N /\\epsilon) )$\nin space and $O ( N \\log^{2}( N ) \\log^{2d}( N /\\epsilon) )$ in time, improving\nupon the state of the art for general elliptic operators; we further present\nnumerical evidence that $d$ can be taken to be the intrinsic dimension of the\ndata set rather than that of the ambient space. The algorithm only needs to\nknow the spatial configuration of the $x_{i}$ and does not require an analytic\nrepresentation of $G$. Furthermore, this factorization straightforwardly\nprovides an approximate sparse PCA with optimal rate of convergence in the\noperator norm. Hence, by using only subsampling and the incomplete Cholesky\nfactorization, we obtain, at nearly linear complexity, the compression,\ninversion and approximate PCA of a large class of covariance matrices. By\ninverting the order of the Cholesky factorization we also obtain a solver for\nelliptic PDE with complexity $O ( N \\log^{d}( N /\\epsilon) )$ in space and $O (\nN \\log^{2d}( N /\\epsilon) )$ in time, improving upon the state of the art for\ngeneral elliptic operators. \n\n"}
{"id": "1706.04889", "contents": "Title: Improved Set-based Symbolic Algorithms for Parity Games Abstract: Graph games with {\\omega}-regular winning conditions provide a mathematical\nframework to analyze a wide range of problems in the analysis of reactive\nsystems and programs (such as the synthesis of reactive systems, program\nrepair, and the verification of branching time properties). Parity conditions\nare canonical forms to specify {\\omega}-regular winning conditions. Graph games\nwith parity conditions are equivalent to {\\mu}-calculus model checking, and\nthus a very important algorithmic problem. Symbolic algorithms are of great\nsignificance because they provide scalable algorithms for the analysis of large\nfinite-state systems, as well as algorithms for the analysis of infinite-state\nsystems with finite quotient. A set-based symbolic algorithm uses the basic set\noperations and the one-step predecessor operators. We consider graph games with\n$n$ vertices and parity conditions with $c$ priorities. While many explicit\nalgorithms exist for graph games with parity conditions, for set-based symbolic\nalgorithms there are only two algorithms (notice that we use space to refer to\nthe number of sets stored by a symbolic algorithm): (a) the basic algorithm\nthat requires $O(n^c)$ symbolic operations and linear space; and (b) an\nimproved algorithm that requires $O(n^{c/2+1})$ symbolic operations but also\n$O(n^{c/2+1})$ space (i.e., exponential space). In this work we present two\nset-based symbolic algorithms for parity games: (a) our first algorithm\nrequires $O(n^{c/2+1})$ symbolic operations and only requires linear space; and\n(b) developing on our first algorithm, we present an algorithm that requires\n$O(n^{c/3+1})$ symbolic operations and only linear space. We also present the\nfirst linear space set-based symbolic algorithm for parity games that requires\nat most a sub-exponential number of symbolic operations. \n\n"}
{"id": "1706.06375", "contents": "Title: Almost-equidistant sets Abstract: For a positive integer $d$, a set of points in $d$-dimensional Euclidean\nspace is called almost-equidistant if for any three points from the set, some\ntwo are at unit distance. Let $f(d)$ denote the largest size of an\nalmost-equidistant set in $d$-space. It is known that $f(2)=7$, $f(3)=10$, and\nthat the extremal almost-equidistant sets are unique. We give independent,\ncomputer-assisted proofs of these statements. It is also known that $f(5) \\ge\n16$. We further show that $12\\leq f(4)\\leq 13$, $f(5)\\leq 20$, $18\\leq f(6)\\leq\n26$, $20\\leq f(7)\\leq 34$, and $f(9)\\geq f(8)\\geq 24$. Up to dimension $7$, our\nwork is based on various computer searches, and in dimensions $6$ to $9$, we\ngive constructions based on the known construction for $d=5$. For every\ndimension $d \\ge 3$, we give an example of an almost-equidistant set of $2d+4$\npoints in the $d$-space and we prove the asymptotic upper bound $f(d) \\le\nO(d^{3/2})$. \n\n"}
{"id": "1706.09339", "contents": "Title: Lossy Kernels for Connected Dominating Set on Sparse Graphs Abstract: For $\\alpha > 1$, an $\\alpha$-approximate (bi-)kernel is a polynomial-time\nalgorithm that takes as input an instance $(I, k)$ of a problem $\\mathcal{Q}$\nand outputs an instance $(I',k')$ (of a problem $\\mathcal{Q}'$) of size bounded\nby a function of $k$ such that, for every $c\\geq 1$, a $c$-approximate solution\nfor the new instance can be turned into a $(c\\cdot\\alpha)$-approximate solution\nof the original instance in polynomial time. This framework of lossy\nkernelization was recently introduced by Lokshtanov et al. We study Connected\nDominating Set (and its distance-$r$ variant) parameterized by solution size on\nsparse graph classes like biclique-free graphs, classes of bounded expansion,\nand nowhere dense classes. We prove that for every $\\alpha>1$, Connected\nDominating Set admits a polynomial-size $\\alpha$-approximate (bi-)kernel on all\nthe aforementioned classes. Our results are in sharp contrast to the\nkernelization complexity of Connected Dominating Set, which is known to not\nadmit a polynomial kernel even on $2$-degenerate graphs and graphs of bounded\nexpansion, unless $\\textsf{NP} \\subseteq \\textsf{coNP/poly}$. We complement our\nresults by the following conditional lower bound. We show that if a class\n$\\mathcal{C}$ is somewhere dense and closed under taking subgraphs, then for\nsome value of $r\\in\\mathbb{N}$ there cannot exist an $\\alpha$-approximate\nbi-kernel for the (Connected) Distance-$r$ Dominating Set problem on\n$\\mathcal{C}$ for any $\\alpha>1$ (assuming the Gap Exponential Time\nHypothesis). \n\n"}
{"id": "1706.10110", "contents": "Title: On Using Toeplitz and Circulant Matrices for Johnson-Lindenstrauss\n  Transforms Abstract: The Johnson-Lindenstrauss lemma is one of the corner stone results in\ndimensionality reduction. It says that given $N$, for any set of $N$ vectors $X\n\\subset \\mathbb{R}^n$, there exists a mapping $f : X \\to \\mathbb{R}^m$ such\nthat $f(X)$ preserves all pairwise distances between vectors in $X$ to within\n$(1 \\pm \\varepsilon)$ if $m = O(\\varepsilon^{-2} \\lg N)$. Much effort has gone\ninto developing fast embedding algorithms, with the Fast Johnson-Lindenstrauss\ntransform of Ailon and Chazelle being one of the most well-known techniques.\nThe current fastest algorithm that yields the optimal $m =\nO(\\varepsilon^{-2}\\lg N)$ dimensions has an embedding time of $O(n \\lg n +\n\\varepsilon^{-2} \\lg^3 N)$. An exciting approach towards improving this, due to\nHinrichs and Vyb\\'iral, is to use a random $m \\times n$ Toeplitz matrix for the\nembedding. Using Fast Fourier Transform, the embedding of a vector can then be\ncomputed in $O(n \\lg m)$ time. The big question is of course whether $m =\nO(\\varepsilon^{-2} \\lg N)$ dimensions suffice for this technique. If so, this\nwould end a decades long quest to obtain faster and faster\nJohnson-Lindenstrauss transforms. The current best analysis of the embedding of\nHinrichs and Vyb\\'iral shows that $m = O(\\varepsilon^{-2}\\lg^2 N)$ dimensions\nsuffices. The main result of this paper, is a proof that this analysis\nunfortunately cannot be tightened any further, i.e., there exists a set of $N$\nvectors requiring $m = \\Omega(\\varepsilon^{-2} \\lg^2 N)$ for the Toeplitz\napproach to work. \n\n"}
{"id": "1706.10209", "contents": "Title: Storage, Communication, and Load Balancing Trade-off in Distributed\n  Cache Networks Abstract: We consider load balancing in a network of caching servers delivering\ncontents to end users. Randomized load balancing via the so-called power of two\nchoices is a well-known approach in parallel and distributed systems. In this\nframework, we investigate the tension between storage resources, communication\ncost, and load balancing performance. To this end, we propose a randomized load\nbalancing scheme which simultaneously considers cache size limitation and\nproximity in the server redirection process.\n  In contrast to the classical power of two choices setup, since the memory\nlimitation and the proximity constraint cause correlation in the server\nselection process, we may not benefit from the power of two choices. However,\nwe prove that in certain regimes of problem parameters, our scheme results in\nthe maximum load of order $\\Theta(\\log\\log n)$ (here $n$ is the network size).\nThis is an exponential improvement compared to the scheme which assigns each\nrequest to the nearest available replica. Interestingly, the extra\ncommunication cost incurred by our proposed scheme, compared to the nearest\nreplica strategy, is small. Furthermore, our extensive simulations show that\nthe trade-off trend does not depend on the network topology and library\npopularity profile details. \n\n"}
{"id": "1707.00362", "contents": "Title: Dynamic Parameterized Problems and Algorithms Abstract: Fixed-parameter algorithms and kernelization are two powerful methods to\nsolve $\\mathsf{NP}$-hard problems. Yet, so far those algorithms have been\nlargely restricted to static inputs.\n  In this paper we provide fixed-parameter algorithms and kernelizations for\nfundamental $\\mathsf{NP}$-hard problems with dynamic inputs. We consider a\nvariety of parameterized graph and hitting set problems which are known to have\n$f(k)n^{1+o(1)}$ time algorithms on inputs of size $n$, and we consider the\nquestion of whether there is a data structure that supports small updates (such\nas edge/vertex/set/element insertions and deletions) with an update time of\n$g(k)n^{o(1)}$; such an update time would be essentially optimal. Update and\nquery times independent of $n$ are particularly desirable. Among many other\nresults, we show that Feedback Vertex Set and $k$-Path admit dynamic algorithms\nwith $f(k)\\log^{O(1)}n$ update and query times for some function $f$ depending\non the solution size $k$ only.\n  We complement our positive results by several conditional and unconditional\nlower bounds. For example, we show that unlike their undirected counterparts,\nDirected Feedback Vertex Set and Directed $k$-Path do not admit dynamic\nalgorithms with $n^{o(1)}$ update and query times even for constant solution\nsizes $k\\leq 3$, assuming popular hardness hypotheses. We also show that\nunconditionally, in the cell probe model, Directed Feedback Vertex Set cannot\nbe solved with update time that is purely a function of $k$. \n\n"}
{"id": "1707.01242", "contents": "Title: Learning Geometric Concepts with Nasty Noise Abstract: We study the efficient learnability of geometric concept classes -\nspecifically, low-degree polynomial threshold functions (PTFs) and\nintersections of halfspaces - when a fraction of the data is adversarially\ncorrupted. We give the first polynomial-time PAC learning algorithms for these\nconcept classes with dimension-independent error guarantees in the presence of\nnasty noise under the Gaussian distribution. In the nasty noise model, an\nomniscient adversary can arbitrarily corrupt a small fraction of both the\nunlabeled data points and their labels. This model generalizes well-studied\nnoise models, including the malicious noise model and the agnostic (adversarial\nlabel noise) model. Prior to our work, the only concept class for which\nefficient malicious learning algorithms were known was the class of\norigin-centered halfspaces.\n  Specifically, our robust learning algorithm for low-degree PTFs succeeds\nunder a number of tame distributions -- including the Gaussian distribution\nand, more generally, any log-concave distribution with (approximately) known\nlow-degree moments. For LTFs under the Gaussian distribution, we give a\npolynomial-time algorithm that achieves error $O(\\epsilon)$, where $\\epsilon$\nis the noise rate. At the core of our PAC learning results is an efficient\nalgorithm to approximate the low-degree Chow-parameters of any bounded function\nin the presence of nasty noise. To achieve this, we employ an iterative\nspectral method for outlier detection and removal, inspired by recent work in\nrobust unsupervised learning. Our aforementioned algorithm succeeds for a range\nof distributions satisfying mild concentration bounds and moment assumptions.\nThe correctness of our robust learning algorithm for intersections of\nhalfspaces makes essential use of a novel robust inverse independence lemma\nthat may be of broader interest. \n\n"}
{"id": "1707.04295", "contents": "Title: Approximation Schemes for Clustering with Outliers Abstract: Clustering problems are well-studied in a variety of fields such as data\nscience, operations research, and computer science. Such problems include\nvariants of centre location problems, $k$-median, and $k$-means to name a few.\nIn some cases, not all data points need to be clustered; some may be discarded\nfor various reasons.\n  We study clustering problems with outliers. More specifically, we look at\nUncapacitated Facility Location (UFL), $k$-Median, and $k$-Means. In UFL with\noutliers, we have to open some centres, discard up to $z$ points of $\\cal X$\nand assign every other point to the nearest open centre, minimizing the total\nassignment cost plus centre opening costs. In $k$-Median and $k$-Means, we have\nto open up to $k$ centres but there are no opening costs. In $k$-Means, the\ncost of assigning $j$ to $i$ is $\\delta^2(j,i)$. We present several results.\nOur main focus is on cases where $\\delta$ is a doubling metric or is the\nshortest path metrics of graphs from a minor-closed family of graphs. For\nuniform-cost UFL with outliers on such metrics we show that a multiswap simple\nlocal search heuristic yields a PTAS. With a bit more work, we extend this to\nbicriteria approximations for the $k$-Median and $k$-Means problems in the same\nmetrics where, for any constant $\\epsilon > 0$, we can find a solution using\n$(1+\\epsilon)k$ centres whose cost is at most a $(1+\\epsilon)$-factor of the\noptimum and uses at most $z$ outliers. We also show that natural local search\nheuristics that do not violate the number of clusters and outliers for\n$k$-Median (or $k$-Means) will have unbounded gap even in Euclidean metrics.\nFurthermore, we show how our analysis can be extended to general metrics for\n$k$-Means with outliers to obtain a $(25+\\epsilon,1+\\epsilon)$ bicriteria. \n\n"}
{"id": "1707.05867", "contents": "Title: Reconciling Graphs and Sets of Sets Abstract: We explore a generalization of set reconciliation, where the goal is to\nreconcile sets of sets. Alice and Bob each have a parent set consisting of $s$\nchild sets, each containing at most $h$ elements from a universe of size $u$.\nThey want to reconcile their sets of sets in a scenario where the total number\nof differences between all of their child sets (under the minimum difference\nmatching between their child sets) is $d$. We give several algorithms for this\nproblem, and discuss applications to reconciliation problems on graphs,\ndatabases, and collections of documents. We specifically focus on graph\nreconciliation, providing protocols based on set of sets reconciliation for\nrandom graphs from $G(n,p)$ and for forests of rooted trees. \n\n"}
{"id": "1707.05945", "contents": "Title: First-Order Query Evaluation with Cardinality Conditions Abstract: We study an extension of first-order logic that allows to express cardinality\nconditions in a similar way as SQL's COUNT operator. The corresponding logic\nFOC(P) was introduced by Kuske and Schweikardt (LICS'17), who showed that query\nevaluation for this logic is fixed-parameter tractable on classes of structures\n(or databases) of bounded degree. In the present paper, we first show that the\nfixed-parameter tractability of FOC(P) cannot even be generalised to very\nsimple classes of structures of unbounded degree such as unranked trees or\nstrings with a linear order relation.\n  Then we identify a fragment FOC1(P) of FOC(P) which is still sufficiently\nstrong to express standard applications of SQL's COUNT operator. Our main\nresult shows that query evaluation for FOC1(P) is fixed-parameter tractable\nwith almost linear running time on nowhere dense classes of structures. As a\ncorollary, we also obtain a fixed-parameter tractable algorithm for counting\nthe number of tuples satisfying a query over nowhere dense classes of\nstructures. \n\n"}
{"id": "1707.06364", "contents": "Title: An Alon-Boppana Type Bound for Weighted Graphs and Lowerbounds for\n  Spectral Sparsification Abstract: We prove the following Alon-Boppana type theorem for general (not necessarily\nregular) weighted graphs: if $G$ is an $n$-node weighted undirected graph of\naverage combinatorial degree $d$ (that is, $G$ has $dn/2$ edges) and girth $g>\n2d^{1/8}+1$, and if $\\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\lambda_n$ are the\neigenvalues of the (non-normalized) Laplacian of $G$, then \\[ \\frac\n{\\lambda_n}{\\lambda_2} \\geq 1 + \\frac 4{\\sqrt d} - O \\left( \\frac 1{d^{\\frac\n58} }\\right) \\] (The Alon-Boppana theorem implies that if $G$ is unweighted and\n$d$-regular, then $\\frac {\\lambda_n}{\\lambda_2} \\geq 1 + \\frac 4{\\sqrt d} -\nO\\left( \\frac 1 d \\right)$ if the diameter is at least $d^{1.5}$.)\n  Our result implies a lower bound for spectral sparsifiers. A graph $H$ is a\nspectral $\\epsilon$-sparsifier of a graph $G$ if \\[ L(G) \\preceq L(H) \\preceq\n(1+\\epsilon) L(G) \\] where $L(G)$ is the Laplacian matrix of $G$ and $L(H)$ is\nthe Laplacian matrix of $H$. Batson, Spielman and Srivastava proved that for\nevery $G$ there is an $\\epsilon$-sparsifier $H$ of average degree $d$ where\n$\\epsilon \\approx \\frac {4\\sqrt 2}{\\sqrt d}$ and the edges of $H$ are a\n(weighted) subset of the edges of $G$. Batson, Spielman and Srivastava also\nshow that the bound on $\\epsilon$ cannot be reduced below $\\approx \\frac\n2{\\sqrt d}$ when $G$ is a clique; our Alon-Boppana-type result implies that\n$\\epsilon$ cannot be reduced below $\\approx \\frac 4{\\sqrt d}$ when $G$ comes\nfrom a family of expanders of super-constant degree and super-constant girth.\n  The method of Batson, Spielman and Srivastava proves a more general result,\nabout sparsifying sums of rank-one matrices, and their method applies to an\n\"online\" setting. We show that for the online matrix setting the $4\\sqrt 2 /\n\\sqrt d$ bound is tight, up to lower order terms. \n\n"}
{"id": "1707.08557", "contents": "Title: Congestion Barcodes: Exploring the Topology of Urban Congestion Using\n  Persistent Homology Abstract: This work presents a new method to quantify connectivity in transportation\nnetworks. Inspired by the field of topological data analysis, we propose a\nnovel approach to explore the robustness of road network connectivity in the\npresence of congestion on the roadway. The robustness of the pattern is\nsummarized in a congestion barcode, which can be constructed directly from\ntraffic datasets commonly used for navigation. As an initial demonstration, we\nillustrate the main technique on a publicly available traffic dataset in a\nneighborhood in New York City. \n\n"}
{"id": "1707.08730", "contents": "Title: A Quantum Approach to Subset-Sum and Similar Problems Abstract: In this paper, we study the subset-sum problem by using a quantum heuristic\napproach similar to the verification circuit of quantum Arthur-Merlin games.\nUnder described certain assumptions, we show that the exact solution of the\nsubset sum problem my be obtained in polynomial time and the exponential\nspeed-up over the classical algorithms may be possible. We give a numerical\nexample and discuss the complexity of the approach and its further application\nto the knapsack problem. \n\n"}
{"id": "1707.09150", "contents": "Title: A spectrahedral representation of the first derivative relaxation of the\n  positive semidefinite cone Abstract: If $X$ is an $n\\times n$ symmetric matrix, then the directional derivative of\n$X \\mapsto \\det(X)$ in the direction $I$ is the elementary symmetric polynomial\nof degree $n-1$ in the eigenvalues of $X$. This is a polynomial in the entries\nof $X$ with the property that it is hyperbolic with respect to the direction\n$I$. The corresponding hyperbolicity cone is a relaxation of the positive\nsemidefinite (PSD) cone known as the first derivative relaxation (or Renegar\nderivative) of the PSD cone. A spectrahedal cone is a convex cone that has a\nrepresentation as the intersection of a subspace with the cone of PSD matrices\nin some dimension. We show that the first derivative relaxation of the PSD cone\nis a spectrahedral cone, and give an explicit spectrahedral description of size\n$\\binom{n+1}{2}-1$. The construction provides a new explicit example of a\nhyperbolicity cone that is also a spectrahedron. This is consistent with the\ngeneralized Lax conjecture, which conjectures that every hyperbolicity cone is\na spectrahedron. \n\n"}
{"id": "1707.09863", "contents": "Title: Dimensionality reduction of SDPs through sketching Abstract: We show how to sketch semidefinite programs (SDPs) using positive maps in\norder to reduce their dimension. More precisely, we use\nJohnson\\hyp{}Lindenstrauss transforms to produce a smaller SDP whose solution\npreserves feasibility or approximates the value of the original problem with\nhigh probability. These techniques allow to improve both complexity and storage\nspace requirements. They apply to problems in which the Schatten 1-norm of the\nmatrices specifying the SDP and also of a solution to the problem is constant\nin the problem size. Furthermore, we provide some results which clarify the\nlimitations of positive, linear sketches in this setting. \n\n"}
{"id": "1708.00822", "contents": "Title: Quadratically Tight Relations for Randomized Query Complexity Abstract: Let $f:\\{0,1\\}^n \\rightarrow \\{0,1\\}$ be a Boolean function. The certificate\ncomplexity $C(f)$ is a complexity measure that is quadratically tight for the\nzero-error randomized query complexity $R_0(f)$: $C(f) \\leq R_0(f) \\leq\nC(f)^2$. In this paper we study a new complexity measure that we call\nexpectational certificate complexity $EC(f)$, which is also a quadratically\ntight bound on $R_0(f)$: $EC(f) \\leq R_0(f) = O(EC(f)^2)$. We prove that $EC(f)\n\\leq C(f) \\leq EC(f)^2$ and show that there is a quadratic separation between\nthe two, thus $EC(f)$ gives a tighter upper bound for $R_0(f)$. The measure is\nalso related to the fractional certificate complexity $FC(f)$ as follows:\n$FC(f) \\leq EC(f) = O(FC(f)^{3/2})$. This also connects to an open question by\nAaronson whether $FC(f)$ is a quadratically tight bound for $R_0(f)$, as\n$EC(f)$ is in fact a relaxation of $FC(f)$.\n  In the second part of the work, we upper bound the distributed query\ncomplexity $D^\\mu_\\epsilon(f)$ for product distributions $\\mu$ by the square of\nthe query corruption bound ($\\mathrm{corr}_\\epsilon(f)$) which improves upon a\nresult of Harsha, Jain and Radhakrishnan [2015]. A similar statement for\ncommunication complexity is open. \n\n"}
{"id": "1708.01212", "contents": "Title: Polynomial tuning of multiparametric combinatorial samplers Abstract: Boltzmann samplers and the recursive method are prominent algorithmic\nframeworks for the approximate-size and exact-size random generation of large\ncombinatorial structures, such as maps, tilings, RNA sequences or various\ntree-like structures. In their multiparametric variants, these samplers allow\nto control the profile of expected values corresponding to multiple\ncombinatorial parameters. One can control, for instance, the number of leaves,\nprofile of node degrees in trees or the number of certain subpatterns in\nstrings. However, such a flexible control requires an additional non-trivial\ntuning procedure. In this paper, we propose an efficient polynomial-time, with\nrespect to the number of tuned parameters, tuning algorithm based on convex\noptimisation techniques. Finally, we illustrate the efficiency of our approach\nusing several applications of rational, algebraic and P\\'olya structures\nincluding polyomino tilings with prescribed tile frequencies, planar trees with\na given specific node degree distribution, and weighted partitions. \n\n"}
{"id": "1708.04109", "contents": "Title: Solving Hard Stable Matching Problems Involving Groups of Similar Agents Abstract: Many important stable matching problems are known to be NP-hard, even when\nstrong restrictions are placed on the input. In this paper we seek to identify\nstructural properties of instances of stable matching problems which will allow\nus to design efficient algorithms using elementary techniques. We focus on the\nsetting in which all agents involved in some matching problem can be\npartitioned into k different types, where the type of an agent determines his\nor her preferences, and agents have preferences over types (which may be\nrefined by more detailed preferences within a single type). This situation\nwould arise in practice if agents form preferences solely based on some small\ncollection of agents' attributes. We also consider a generalisation in which\neach agent may consider some small collection of other agents to be\nexceptional, and rank these in a way that is not consistent with their types;\nthis could happen in practice if agents have prior contact with a small number\nof candidates. We show that (for the case without exceptions), several\nwell-studied NP-hard stable matching problems including Max SMTI (that of\nfinding the maximum cardinality stable matching in an instance of stable\nmarriage with ties and incomplete lists) belong to the parameterised complexity\nclass FPT when parameterised by the number of different types of agents needed\nto describe the instance. For Max SMTI this tractability result can be extended\nto the setting in which each agent promotes at most one `exceptional' candidate\nto the top of his/her list (when preferences within types are not refined), but\nthe problem remains NP-hard if preference lists can contain two or more\nexceptions and the exceptional candidates can be placed anywhere in the\npreference lists, even if the number of types is bounded by a constant. \n\n"}
{"id": "1708.04381", "contents": "Title: Streaming Periodicity with Mismatches Abstract: We study the problem of finding all $k$-periods of a length-$n$ string $S$,\npresented as a data stream. $S$ is said to have $k$-period $p$ if its prefix of\nlength $n-p$ differs from its suffix of length $n-p$ in at most $k$ locations.\n  We give a one-pass streaming algorithm that computes the $k$-periods of a\nstring $S$ using $\\text{poly}(k, \\log n)$ bits of space, for $k$-periods of\nlength at most $\\frac{n}{2}$. We also present a two-pass streaming algorithm\nthat computes $k$-periods of $S$ using $\\text{poly}(k, \\log n)$ bits of space,\nregardless of period length. We complement these results with comparable lower\nbounds. \n\n"}
{"id": "1708.04974", "contents": "Title: A fast coset-translation algorithm for computing the cycle structure of\n  Comer relation algebras over $\\mathbb{Z}/p\\mathbb{Z}$ Abstract: Proper relation algebras can be constructed using $\\mathbb{Z}/p\\mathbb{Z}$ as\na base set using a method due to Comer. The cycle structure of such an algebra\nmust, in general, be determined \\emph{a posteriori}, normally with the aid of a\ncomputer. In this paper, we give an improved algorithm for checking the cycle\nstructure that reduces the time complexity from $\\mathcal{O}(p^2)$ to\n$\\mathcal{O}(p)$. \n\n"}
{"id": "1708.09398", "contents": "Title: Designing Strassen's algorithm Abstract: In 1969, Strassen shocked the world by showing that two n x n matrices could\nbe multiplied in time asymptotically less than $O(n^3)$. While the recursive\nconstruction in his algorithm is very clear, the key gain was made by showing\nthat 2 x 2 matrix multiplication could be performed with only 7 multiplications\ninstead of 8. The latter construction was arrived at by a process of\nelimination and appears to come out of thin air. Here, we give the simplest and\nmost transparent proof of Strassen's algorithm that we are aware of, using only\na simple unitary 2-design and a few easy lines of calculation. Moreover, using\nbasic facts from the representation theory of finite groups, we use 2-designs\ncoming from group orbits to generalize our construction to all n (although the\nresulting algorithms aren't optimal for n at least 3). \n\n"}
{"id": "1709.00228", "contents": "Title: Learning Multi-item Auctions with (or without) Samples Abstract: We provide algorithms that learn simple auctions whose revenue is\napproximately optimal in multi-item multi-bidder settings, for a wide range of\nvaluations including unit-demand, additive, constrained additive, XOS, and\nsubadditive. We obtain our learning results in two settings. The first is the\ncommonly studied setting where sample access to the bidders' distributions over\nvaluations is given, for both regular distributions and arbitrary distributions\nwith bounded support. Our algorithms require polynomially many samples in the\nnumber of items and bidders. The second is a more general max-min learning\nsetting that we introduce, where we are given \"approximate distributions,\" and\nwe seek to compute an auction whose revenue is approximately optimal\nsimultaneously for all \"true distributions\" that are close to the given ones.\nThese results are more general in that they imply the sample-based results, and\nare also applicable in settings where we have no sample access to the\nunderlying distributions but have estimated them indirectly via market research\nor by observation of previously run, potentially non-truthful auctions.\n  Our results hold for valuation distributions satisfying the standard (and\nnecessary) independence-across-items property. They also generalize and improve\nupon recent works, which have provided algorithms that learn approximately\noptimal auctions in more restricted settings with additive, subadditive and\nunit-demand valuations using sample access to distributions. We generalize\nthese results to the complete unit-demand, additive, and XOS setting, to i.i.d.\nsubadditive bidders, and to the max-min setting.\n  Our results are enabled by new uniform convergence bounds for hypotheses\nclasses under product measures. Our bounds result in exponential savings in\nsample complexity compared to bounds derived by bounding the VC dimension, and\nare of independent interest. \n\n"}
{"id": "1709.01670", "contents": "Title: Parameterized complexity of machine scheduling: 15 open problems Abstract: Machine scheduling problems are a long-time key domain of algorithms and\ncomplexity research. A novel approach to machine scheduling problems are\nfixed-parameter algorithms. To stimulate this thriving research direction, we\npropose 15 open questions in this area whose resolution we expect to lead to\nthe discovery of new approaches and techniques both in scheduling and\nparameterized complexity theory. \n\n"}
{"id": "1709.05282", "contents": "Title: On the Difference Between Closest, Furthest, and Orthogonal Pairs:\n  Nearly-Linear vs Barely-Subquadratic Complexity in Computational Geometry Abstract: Point location problems for $n$ points in $d$-dimensional Euclidean space\n(and $\\ell_p$ spaces more generally) have typically had two kinds of\nrunning-time solutions:\n  * (Nearly-Linear) less than $d^{poly(d)} \\cdot n \\log^{O(d)} n$ time, or\n  * (Barely-Subquadratic) $f(d) \\cdot n^{2-1/\\Theta(d)}$ time, for various $f$.\n  For small $d$ and large $n$, \"nearly-linear\" running times are generally\nfeasible, while \"barely-subquadratic\" times are generally infeasible. For\nexample, in the Euclidean metric, finding a Closest Pair among $n$ points in\n${\\mathbb R}^d$ is nearly-linear, solvable in $2^{O(d)} \\cdot n \\log^{O(1)} n$\ntime, while known algorithms for Furthest Pair (the diameter of the point set)\nare only barely-subquadratic, requiring $\\Omega(n^{2-1/\\Theta(d)})$ time. Why\ndo these proximity problems have such different time complexities? Is there a\nbarrier to obtaining nearly-linear algorithms for problems which are currently\nonly barely-subquadratic?\n  We give a novel exact and deterministic self-reduction for the Orthogonal\nVectors problem on $n$ vectors in $\\{0,1\\}^d$ to $n$ vectors in ${\\mathbb\nZ}^{\\omega(\\log d)}$ that runs in $2^{o(d)}$ time. As a consequence,\nbarely-subquadratic problems such as Euclidean diameter, Euclidean bichromatic\nclosest pair, ray shooting, and incidence detection do not have\n$O(n^{2-\\epsilon})$ time algorithms (in Turing models of computation) for\ndimensionality $d = \\omega(\\log \\log n)^2$, unless the popular Orthogonal\nVectors Conjecture and the Strong Exponential Time Hypothesis are false. That\nis, while poly-log-log-dimensional Closest Pair is in $n^{1+o(1)}$ time, the\nanalogous case of Furthest Pair can encode larger-dimensional problems\nconjectured to require $n^{2-o(1)}$ time. We also show that the All-Nearest\nNeighbors problem in $\\omega(\\log n)$ dimensions requires $n^{2-o(1)}$ time to\nsolve, assuming either of the above conjectures. \n\n"}
{"id": "1709.05510", "contents": "Title: The Geometric Block Model Abstract: To capture the inherent geometric features of many community detection\nproblems, we propose to use a new random graph model of communities that we\ncall a Geometric Block Model. The geometric block model generalizes the random\ngeometric graphs in the same way that the well-studied stochastic block model\ngeneralizes the Erdos-Renyi random graphs. It is also a natural extension of\nrandom community models inspired by the recent theoretical and practical\nadvancement in community detection. While being a topic of fundamental\ntheoretical interest, our main contribution is to show that many practical\ncommunity structures are better explained by the geometric block model. We also\nshow that a simple triangle-counting algorithm to detect communities in the\ngeometric block model is near-optimal. Indeed, even in the regime where the\naverage degree of the graph grows only logarithmically with the number of\nvertices (sparse-graph), we show that this algorithm performs extremely well,\nboth theoretically and practically. In contrast, the triangle-counting\nalgorithm is far from being optimum for the stochastic block model. We simulate\nour results on both real and synthetic datasets to show superior performance of\nboth the new model as well as our algorithm. \n\n"}
{"id": "1709.05876", "contents": "Title: Approximations for Generalized Unsplittable Flow on Paths with\n  Application to Power Systems Optimization Abstract: The Unsplittable Flow on a Path (UFP) problem has garnered considerable\nattention as a challenging combinatorial optimization problem with notable\npractical implications. Steered by its pivotal applications in power\nengineering, the present work formulates a novel generalization of UFP, wherein\ndemands and capacities in the input instance are monotone step functions over\nthe set of edges. As an initial step towards tackling this generalization, we\ndraw on and extend ideas from prior research to devise a quasi-polynomial time\napproximation scheme (QPTAS) under the premise that the demands and capacities\nlie in a quasi-polynomial range. Second, retaining the same assumption, an\nefficient logarithmic approximation is introduced for the single-source variant\nof the problem. Finally, we round up the contributions by designing a (kind of)\nblack-box reduction that, under some mild conditions, allows to translate\nLP-based approximation algorithms for the studied problem into their\ncounterparts for the Alternating Current Optimal Power Flow (AC OPF) problem --\na fundamental workflow in operation and control of power systems. \n\n"}
{"id": "1709.06349", "contents": "Title: Double-distance frameworks and mixed sparsity graphs Abstract: A rigidity theory is developed for frameworks in a metric space with two\ntypes of distance constraints. Mixed sparsity graph characterisations are\nobtained for the infinitesimal and continuous rigidity of completely regular\nbar-joint frameworks in a variety of such contexts. The main results are\ncombinatorial characterisations for (i) frameworks restricted to surfaces with\nboth Euclidean and geodesic distance constraints, (ii) frameworks in the plane\nwith Euclidean and non-Euclidean distance constraints, and (iii)\ndirection-length frameworks in the non-Euclidean plane. \n\n"}
{"id": "1709.08459", "contents": "Title: Crowdsourced correlation clustering with relative distance comparisons Abstract: Crowdsourced, or human computation based clustering algorithms usually rely\non relative distance comparisons, as these are easier to elicit from human\nworkers than absolute distance information. A relative distance comparison is a\nstatement of the form \"item A is closer to item B than to item C\". However,\nmany existing clustering algorithms that use relative distances are rather\ncomplex. They are often based on a two-step approach, where the relative\ndistances are first used to learn either a distance matrix, or an embedding of\nthe items, and then some standard clustering method is applied in a second\nstep. In this paper we argue that it should be possible to compute a clustering\ndirectly from relative distance comparisons. Our ideas are built upon existing\nwork on correlation clustering, a well-known non-parametric approach to\nclustering. The technical contribution of this work is twofold. We first define\na novel variant of correlation clustering that is based on relative distance\ncomparisons, and hence suitable for human computation. We go on to show that\nour new problem is closely related to basic correlation clustering, and use\nthis property to design an approximation algorithm for our problem. As a second\ncontribution, we propose a more practical algorithm, which we empirically\ncompare against existing methods from literature. Experiments with synthetic\ndata suggest that our approach can outperform more complex methods. Also, our\nmethod efficiently finds good and intuitive clusterings from real relative\ndistance comparison data. \n\n"}
{"id": "1709.09307", "contents": "Title: On the construction of converging hierarchies for polynomial\n  optimization based on certificates of global positivity Abstract: In recent years, techniques based on convex optimization and real algebra\nthat produce converging hierarchies of lower bounds for polynomial minimization\nproblems have gained much popularity. At their heart, these hierarchies rely\ncrucially on Positivstellens\\\"atze from the late 20th century (e.g., due to\nStengle, Putinar, or Schm\\\"udgen) that certify positivity of a polynomial on an\narbitrary closed basic semialgebraic set. In this paper, we show that such\nhierarchies could in fact be designed from much more limited\nPositivstellens\\\"atze dating back to the early 20th century that only certify\npositivity of a polynomial globally. More precisely, we show that any inner\napproximation to the cone of positive homogeneous polynomials that is\narbitrarily tight can be turned into a converging hierarchy of lower bounds for\ngeneral polynomial minimization problems with compact feasible sets. This in\nparticular leads to a semidefinite programming-based hierarchy that relies\nsolely on Artin's solution to Hilbert's 17th problem. We also use a classical\nresult of Poly\\'a on global positivity of even forms to construct an\n\"optimization-free\" converging hierarchy for general polynomial minimization\nproblems. This hierarchy only requires polynomial multiplication and checking\nnonnegativity of coefficients of certain fixed polynomials. As a corollary, we\nobtain new linear programming and second-order cone programming-based\nhierarchies for polynomial minimization problems that rely on the recently\nintroduced concepts of dsos and sdsos polynomials. We remark that the scope of\nthis paper is theoretical at this stage as our hierarchies-though they involve\nat most two sum of squares constraints or only basic arithmetic at each\nlevel-require the use of bisection and increase the number of variables (resp.\ndegree) of the problem by the number of inequality constraints plus three\n(resp. by a factor of two). \n\n"}
{"id": "1709.10258", "contents": "Title: An improved algorithm for recognizing matroids Abstract: Let $M$ be a matroid defined on a finite set $E$ and $L\\subset E$. $L$ is\nlocked in $M$ if $M|L$ and $M^*|(E\\backslash L)$ are 2-connected, and\n$min\\{r(L), r^*(E\\backslash L)\\} \\geq 2$. Locked subsets characterize\nnontrivial facets of the bases polytope. In this paper, we give a new axiom\nsystem for matroids based on locked subsets. We deduce an algorithm for\nrecognizing matroids improving the running time complexity of the best known\ntill today. This algorithm induces a polynomial time algorithm for recognizing\nuniform matroids. This latter problem is intractable if we use an independence\noracle. \n\n"}
{"id": "1710.00264", "contents": "Title: Bayesian estimation from few samples: community detection and related\n  problems Abstract: We propose an efficient meta-algorithm for Bayesian estimation problems that\nis based on low-degree polynomials, semidefinite programming, and tensor\ndecomposition. The algorithm is inspired by recent lower bound constructions\nfor sum-of-squares and related to the method of moments. Our focus is on sample\ncomplexity bounds that are as tight as possible (up to additive lower-order\nterms) and often achieve statistical thresholds or conjectured computational\nthresholds.\n  Our algorithm recovers the best known bounds for community detection in the\nsparse stochastic block model, a widely-studied class of estimation problems\nfor community detection in graphs. We obtain the first recovery guarantees for\nthe mixed-membership stochastic block model (Airoldi et el.) in constant\naverage degree graphs---up to what we conjecture to be the computational\nthreshold for this model. We show that our algorithm exhibits a sharp\ncomputational threshold for the stochastic block model with multiple\ncommunities beyond the Kesten--Stigum bound---giving evidence that this task\nmay require exponential time.\n  The basic strategy of our algorithm is strikingly simple: we compute the\nbest-possible low-degree approximation for the moments of the posterior\ndistribution of the parameters and use a robust tensor decomposition algorithm\nto recover the parameters from these approximate posterior moments. \n\n"}
{"id": "1710.01969", "contents": "Title: Simultaneous Multiparty Communication Complexity of Composed Functions Abstract: In the Number On the Forehead (NOF) multiparty communication model, $k$\nplayers want to evaluate a function $F : X_1 \\times\\cdots\\times X_k\\rightarrow\nY$ on some input $(x_1,\\dots,x_k)$ by broadcasting bits according to a\npredetermined protocol. The input is distributed in such a way that each player\n$i$ sees all of it except $x_i$. In the simultaneous setting, the players\ncannot speak to each other but instead send information to a referee. The\nreferee does not know the players' input, and cannot give any information back.\nAt the end, the referee must be able to recover $F(x_1,\\dots,x_k)$ from what\nshe obtained.\n  A central open question, called the $\\log n$ barrier, is to find a function\nwhich is hard to compute for $polylog(n)$ or more players (where the $x_i$'s\nhave size $poly(n)$) in the simultaneous NOF model. This has important\napplications in circuit complexity, as it could help to separate $ACC^0$ from\nother complexity classes. One of the candidates belongs to the family of\ncomposed functions. The input to these functions is represented by a $k\\times\n(t\\cdot n)$ boolean matrix $M$, whose row $i$ is the input $x_i$ and $t$ is a\nblock-width parameter. A symmetric composed function acting on $M$ is specified\nby two symmetric $n$- and $kt$-variate functions $f$ and $g$, that output\n$f\\circ g(M)=f(g(B_1),\\dots,g(B_n))$ where $B_j$ is the $j$-th block of width\n$t$ of $M$. As the majority function $MAJ$ is conjectured to be outside of\n$ACC^0$, Babai et. al. suggested to study $MAJ\\circ MAJ_t$, with $t$ large\nenough.\n  So far, it was only known that $t=1$ is not enough for $MAJ\\circ MAJ_t$ to\nbreak the $\\log n$ barrier in the simultaneous deterministic NOF model. In this\npaper, we extend this result to any constant block-width $t>1$, by giving a\nprotocol of cost $2^{O(2^t)}\\log^{2^{t+1}}(n)$ for any symmetric composed\nfunction when there are $2^{\\Omega(2^t)}\\log n$ players. \n\n"}
{"id": "1710.02690", "contents": "Title: Unique Entity Estimation with Application to the Syrian Conflict Abstract: Entity resolution identifies and removes duplicate entities in large, noisy\ndatabases and has grown in both usage and new developments as a result of\nincreased data availability. Nevertheless, entity resolution has tradeoffs\nregarding assumptions of the data generation process, error rates, and\ncomputational scalability that make it a difficult task for real applications.\nIn this paper, we focus on a related problem of unique entity estimation, which\nis the task of estimating the unique number of entities and associated standard\nerrors in a data set with duplicate entities. Unique entity estimation shares\nmany fundamental challenges of entity resolution, namely, that the\ncomputational cost of all-to-all entity comparisons is intractable for large\ndatabases. To circumvent this computational barrier, we propose an efficient\n(near-linear time) estimation algorithm based on locality sensitive hashing.\nOur estimator, under realistic assumptions, is unbiased and has provably low\nvariance compared to existing random sampling based approaches. In addition, we\nempirically show its superiority over the state-of-the-art estimators on three\nreal applications. The motivation for our work is to derive an accurate\nestimate of the documented, identifiable deaths in the ongoing Syrian conflict.\nOur methodology, when applied to the Syrian data set, provides an estimate of\n$191,874 \\pm 1772$ documented, identifiable deaths, which is very close to the\nHuman Rights Data Analysis Group (HRDAG) estimate of 191,369. Our work provides\nan example of challenges and efforts involved in solving a real, noisy\nchallenging problem where modeling assumptions may not hold. \n\n"}
{"id": "1710.06025", "contents": "Title: Quantum query complexity of entropy estimation Abstract: Estimation of Shannon and R\\'enyi entropies of unknown discrete distributions\nis a fundamental problem in statistical property testing and an active research\ntopic in both theoretical computer science and information theory. Tight bounds\non the number of samples to estimate these entropies have been established in\nthe classical setting, while little is known about their quantum counterparts.\nIn this paper, we give the first quantum algorithms for estimating\n$\\alpha$-R\\'enyi entropies (Shannon entropy being 1-Renyi entropy). In\nparticular, we demonstrate a quadratic quantum speedup for Shannon entropy\nestimation and a generic quantum speedup for $\\alpha$-R\\'enyi entropy\nestimation for all $\\alpha\\geq 0$, including a tight bound for the\ncollision-entropy (2-R\\'enyi entropy). We also provide quantum upper bounds for\nextreme cases such as the Hartley entropy (i.e., the logarithm of the support\nsize of a distribution, corresponding to $\\alpha=0$) and the min-entropy case\n(i.e., $\\alpha=+\\infty$), as well as the Kullback-Leibler divergence between\ntwo distributions. Moreover, we complement our results with quantum lower\nbounds on $\\alpha$-R\\'enyi entropy estimation for all $\\alpha\\geq 0$. \n\n"}
{"id": "1710.07774", "contents": "Title: A Unified PTAS for Prize Collecting TSP and Steiner Tree Problem in\n  Doubling Metrics Abstract: We present a unified polynomial-time approximation scheme (PTAS) for the\nprize collecting traveling salesman problem (PCTSP) and the prize collecting\nSteiner tree problem (PCSTP) in doubling metrics. Given a metric space and a\npenalty function on a subset of points known as terminals, a solution is a\nsubgraph on points in the metric space, whose cost is the weight of its edges\nplus the penalty due to terminals not covered by the subgraph. Under our\nunified framework, the solution subgraph needs to be Eulerian for PCTSP, while\nit needs to be connected for PCSTP. Before our work, even a QPTAS for the\nproblems in doubling metrics is not known.\n  Our unified PTAS is based on the previous dynamic programming frameworks\nproposed in [Talwar STOC 2004] and [Bartal, Gottlieb, Krauthgamer STOC 2012].\nHowever, since it is unknown which part of the optimal cost is due to edge\nlengths and which part is due to penalties of uncovered terminals, we need to\ndevelop new techniques to apply previous divide-and-conquer strategies and\nsparse instance decompositions. \n\n"}
{"id": "1710.08607", "contents": "Title: Provable and practical approximations for the degree distribution using\n  sublinear graph samples Abstract: The degree distribution is one of the most fundamental properties used in the\nanalysis of massive graphs. There is a large literature on graph sampling,\nwhere the goal is to estimate properties (especially the degree distribution)\nof a large graph through a small, random sample. The degree distribution\nestimation poses a significant challenge, due to its heavy-tailed nature and\nthe large variance in degrees.\n  We design a new algorithm, SADDLES, for this problem, using recent\nmathematical techniques from the field of sublinear algorithms. The SADDLES\nalgorithm gives provably accurate outputs for all values of the degree\ndistribution. For the analysis, we define two fatness measures of the degree\ndistribution, called the $h$-index and the $z$-index. We prove that SADDLES is\nsublinear in the graph size when these indices are large. A corollary of this\nresult is a provably sublinear algorithm for any degree distribution bounded\nbelow by a power law.\n  We deploy our new algorithm on a variety of real datasets and demonstrate its\nexcellent empirical behavior. In all instances, we get extremely accurate\napproximations for all values in the degree distribution by observing at most\n$1\\%$ of the vertices. This is a major improvement over the state-of-the-art\nsampling algorithms, which typically sample more than $10\\%$ of the vertices to\ngive comparable results. We also observe that the $h$ and $z$-indices of real\ngraphs are large, validating our theoretical analysis. \n\n"}
{"id": "1710.09595", "contents": "Title: Quantum versus Classical Online Streaming Algorithms with Logarithmic\n  Size of Memory Abstract: We consider online algorithms with respect to the competitive ratio. Here, we\ninvestigate quantum and classical one-way automata with non-constant size of\nmemory (streaming algorithms) as a model for online algorithms. We construct\nproblems that can be solved by quantum online streaming algorithms better than\nby classical ones in a case of logarithmic or sublogarithmic size of memory. \n\n"}
{"id": "1710.11462", "contents": "Title: Manipulation Strategies for the Rank Maximal Matching Problem Abstract: We consider manipulation strategies for the rank-maximal matching problem. In\nthe rank-maximal matching problem we are given a bipartite graph $G = (A \\cup\nP, E)$ such that $A$ denotes a set of applicants and $P$ a set of posts. Each\napplicant $a \\in A$ has a preference list over the set of his neighbours in\n$G$, possibly involving ties. Preference lists are represented by ranks on the\nedges - an edge $(a,p)$ has rank $i$, denoted as $rank(a,p)=i$, if post $p$\nbelongs to one of $a$'s $i$-th choices. A rank-maximal matching is one in which\nthe maximum number of applicants is matched to their rank one posts and subject\nto this condition, the maximum number of applicants is matched to their rank\ntwo posts, and so on. A rank-maximal matching can be computed in $O(\\min(c\n\\sqrt{n},n) m)$ time, where $n$ denotes the number of applicants, $m$ the\nnumber of edges and $c$ the maximum rank of an edge in an optimal solution.\n  A central authority matches applicants to posts. It does so using one of the\nrank-maximal matchings. Since there may be more than one rank- maximal matching\nof $G$, we assume that the central authority chooses any one of them randomly.\nLet $a_1$ be a manipulative applicant, who knows the preference lists of all\nthe other applicants and wants to falsify his preference list so that he has a\nchance of getting better posts than if he were truthful. In the first problem\naddressed in this paper the manipulative applicant $a_1$ wants to ensure that\nhe is never matched to any post worse than the most preferred among those of\nrank greater than one and obtainable when he is truthful. In the second problem\nthe manipulator wants to construct such a preference list that the worst post\nhe can become matched to by the central authority is best possible or in other\nwords, $a_1$ wants to minimize the maximal rank of a post he can become matched\nto. \n\n"}
{"id": "1711.00571", "contents": "Title: Efficient $\\widetilde{O}(n/\\epsilon)$ Spectral Sketches for the\n  Laplacian and its Pseudoinverse Abstract: In this paper we consider the problem of efficiently computing\n$\\epsilon$-sketches for the Laplacian and its pseudoinverse. Given a Laplacian\nand an error tolerance $\\epsilon$, we seek to construct a function $f$ such\nthat for any vector $x$ (chosen obliviously from $f$), with high probability\n$(1-\\epsilon) x^\\top A x \\leq f(x) \\leq (1 + \\epsilon) x^\\top A x$ where $A$ is\neither the Laplacian or its pseudoinverse. Our goal is to construct such a\nsketch $f$ efficiently and to store it in the least space possible.\n  We provide nearly-linear time algorithms that, when given a Laplacian matrix\n$\\mathcal{L} \\in \\mathbb{R}^{n \\times n}$ and an error tolerance $\\epsilon$,\nproduce $\\tilde{O}(n/\\epsilon)$-size sketches of both $\\mathcal{L}$ and its\npseudoinverse. Our algorithms improve upon the previous best sketch size of\n$\\widetilde{O}(n / \\epsilon^{1.6})$ for sketching the Laplacian form by Andoni\net al (2015) and $O(n / \\epsilon^2)$ for sketching the Laplacian pseudoinverse\nby Batson, Spielman, and Srivastava (2008).\n  Furthermore we show how to compute all-pairs effective resistances from\n$\\widetilde{O}(n/\\epsilon)$ size sketch in $\\widetilde{O}(n^2/\\epsilon)$ time.\nThis improves upon the previous best running time of\n$\\widetilde{O}(n^2/\\epsilon^2)$ by Spielman and Srivastava (2008). \n\n"}
{"id": "1711.00788", "contents": "Title: On the complexity of optimal homotopies Abstract: In this article, we provide new structural results and algorithms for the\nHomotopy Height problem. In broad terms, this problem quantifies how much a\ncurve on a surface needs to be stretched to sweep continuously between two\npositions. More precisely, given two homotopic curves $\\gamma_1$ and $\\gamma_2$\non a combinatorial (say, triangulated) surface, we investigate the problem of\ncomputing a homotopy between $\\gamma_1$ and $\\gamma_2$ where the length of the\nlongest intermediate curve is minimized. Such optimal homotopies are relevant\nfor a wide range of purposes, from very theoretical questions in quantitative\nhomotopy theory to more practical applications such as similarity measures on\nmeshes and graph searching problems.\n  We prove that Homotopy Height is in the complexity class NP, and the\ncorresponding exponential algorithm is the best one known for this problem.\nThis result builds on a structural theorem on monotonicity of optimal\nhomotopies, which is proved in a companion paper. Then we show that this\nproblem encompasses the Homotopic Fr\\'echet distance problem which we therefore\nalso establish to be in NP, answering a question which has previously been\nconsidered in several different settings. We also provide an O(log\nn)-approximation algorithm for Homotopy Height on surfaces by adapting an\nearlier algorithm of Har-Peled, Nayyeri, Salvatipour and Sidiropoulos in the\nplanar setting. \n\n"}
{"id": "1711.02860", "contents": "Title: Constructive Discrepancy Minimization with Hereditary L2 Guarantees Abstract: In discrepancy minimization problems, we are given a family of sets\n$\\mathcal{S} = \\{S_1,\\dots,S_m\\}$, with each $S_i \\in \\mathcal{S}$ a subset of\nsome universe $U = \\{u_1,\\dots,u_n\\}$ of $n$ elements. The goal is to find a\ncoloring $\\chi : U \\to \\{-1,+1\\}$ of the elements of $U$ such that each set $S\n\\in \\mathcal{S}$ is colored as evenly as possible. Two classic measures of\ndiscrepancy are $\\ell_\\infty$-discrepancy defined as\n$\\textrm{disc}_\\infty(\\mathcal{S},\\chi):=\\max_{S \\in \\mathcal{S}} | \\sum_{u_i\n\\in S} \\chi(u_i) |$ and $\\ell_2$-discrepancy defined as\n$\\textrm{disc}_2(\\mathcal{S},\\chi):=\\sqrt{(1/|\\mathcal{S}|)\\sum_{S \\in\n\\mathcal{S}} \\left(\\sum_{u_i \\in S}\\chi(u_i)\\right)^2}$. Breakthrough work by\nBansal gave a polynomial time algorithm, based on rounding an SDP, for finding\na coloring $\\chi$ such that $\\textrm{disc}_\\infty(\\mathcal{S},\\chi) = O(\\lg n\n\\cdot \\textrm{herdisc}_\\infty(\\mathcal{S}))$ where\n$\\textrm{herdisc}_\\infty(\\mathcal{S})$ is the hereditary\n$\\ell_\\infty$-discrepancy of $\\mathcal{S}$. We complement his work by giving a\nsimple $O((m+n)n^2)$ time algorithm for finding a coloring $\\chi$ such\n$\\textrm{disc}_2(\\mathcal{S},\\chi) = O(\\sqrt{\\lg n} \\cdot\n\\textrm{herdisc}_2(\\mathcal{S}))$ where $\\textrm{herdisc}_2(\\mathcal{S})$ is\nthe hereditary $\\ell_2$-discrepancy of $\\mathcal{S}$. Interestingly, our\nalgorithm avoids solving an SDP and instead relies on computing\neigendecompositions of matrices. Moreover, we use our ideas to speed up the\nEdge-Walk algorithm by Lovett and Meka [SICOMP'15]. To prove that our algorithm\nhas the claimed guarantees, we show new inequalities relating\n$\\textrm{herdisc}_\\infty$ and $\\textrm{herdisc}_2$ to the eigenvalues of the\nmatrix corresponding to $\\mathcal{S}$. Our inequalities improve over previous\nwork by Chazelle and Lvov, and by Matousek et al. Finally, we also implement\nour algorithm and show that it far outperforms random sampling. \n\n"}
{"id": "1711.03336", "contents": "Title: Toward perfect reads: self-correction of short reads via mapping on de\n  Bruijn graphs Abstract: Motivations Short-read accuracy is important for downstream analyses such as\ngenome assembly and hybrid long-read correction. Despite much work on\nshort-read correction, present-day correctors either do not scale well on large\ndata sets or consider reads as mere suites of k-mers, without taking into\naccount their full-length read information. Results We propose a new method to\ncorrect short reads using de Bruijn graphs, and implement it as a tool called\nBcool. As a first st ep, Bcool constructs a compacted de Bruijn graph from the\nreads. This graph is filtered on the basis of k-mer abundance then of unitig\nabundance, thereby removing from most sequencing errors. The cleaned graph is\nthen used as a reference on which the reads are mapped to correct them. We show\nthat this approach yields more accurate reads than k-mer-spectrum correctors\nwhile being scalable to human-size genomic datasets and beyond. Availability\nand Implementation The implementation is open source and available at\nhttp://github.com/Malfoy/BCOOL under the Affero GPL license. Contact Antoine\nLimasset antoine.limasset@gmail.com & Jean-Fran\\c{c}ois Flot jflot@ulb.ac.be &\nPierre Peterlongo pierre.peterlongo@inria.fr \n\n"}
{"id": "1711.04712", "contents": "Title: Randomized Near Neighbor Graphs, Giant Components, and Applications in\n  Data Science Abstract: If we pick $n$ random points uniformly in $[0,1]^d$ and connect each point to\nits $k-$nearest neighbors, then it is well known that there exists a giant\nconnected component with high probability. We prove that in $[0,1]^d$ it\nsuffices to connect every point to $ c_{d,1} \\log{\\log{n}}$ points chosen\nrandomly among its $ c_{d,2} \\log{n}-$nearest neighbors to ensure a giant\ncomponent of size $n - o(n)$ with high probability. This construction yields a\nmuch sparser random graph with $\\sim n \\log\\log{n}$ instead of $\\sim n \\log{n}$\nedges that has comparable connectivity properties. This result has nontrivial\nimplications for problems in data science where an affinity matrix is\nconstructed: instead of picking the $k-$nearest neighbors, one can often pick\n$k' \\ll k$ random points out of the $k-$nearest neighbors without sacrificing\nefficiency. This can massively simplify and accelerate computation, we\nillustrate this with several numerical examples. \n\n"}
{"id": "1711.05157", "contents": "Title: A note on the complexity of Feedback Vertex Set parameterized by\n  mim-width Abstract: We complement the recent algorithmic result that Feedback Vertex Set is\nXP-time solvable parameterized by the mim-width of a given branch decomposition\nof the input graph [3] by showing that the problem is W[1]-hard in this\nparameterization. The hardness holds even for linear mim-width, as well as for\nH-graphs, where the parameter is the number of edges in H. To obtain this\nresult, we adapt a reduction due to Fomin, Golovach and Raymond [2], following\nthe same line of reasoning but adding a new gadget. \n\n"}
{"id": "1711.05408", "contents": "Title: Recurrent Neural Networks as Weighted Language Recognizers Abstract: We investigate the computational complexity of various problems for simple\nrecurrent neural networks (RNNs) as formal models for recognizing weighted\nlanguages. We focus on the single-layer, ReLU-activation, rational-weight RNNs\nwith softmax, which are commonly used in natural language processing\napplications. We show that most problems for such RNNs are undecidable,\nincluding consistency, equivalence, minimization, and the determination of the\nhighest-weighted string. However, for consistent RNNs the last problem becomes\ndecidable, although the solution length can surpass all computable bounds. If\nadditionally the string is limited to polynomial length, the problem becomes\nNP-complete and APX-hard. In summary, this shows that approximations and\nheuristic algorithms are necessary in practical applications of those RNNs. \n\n"}
{"id": "1711.05796", "contents": "Title: A rank 18 Waring decomposition of $sM_{\\langle 3\\rangle}$ with 432\n  symmetries Abstract: The recent discovery that the exponent of matrix multiplication is determined\nby the rank of the symmetrized matrix multiplication tensor has invigorated\ninterest in better understanding symmetrized matrix multiplication. I present\nan explicit rank 18 Waring decomposition of $sM_{\\langle 3\\rangle}$ and\ndescribe its symmetry group. \n\n"}
{"id": "1711.05812", "contents": "Title: Iteration complexity of inexact augmented Lagrangian methods for\n  constrained convex programming Abstract: Augmented Lagrangian method (ALM) has been popularly used for solving\nconstrained optimization problems. Practically, subproblems for updating primal\nvariables in the framework of ALM usually can only be solved inexactly. The\nconvergence and local convergence speed of ALM have been extensively studied.\nHowever, the global convergence rate of inexact ALM is still open for problems\nwith nonlinear inequality constraints. In this paper, we work on general convex\nprograms with both equality and inequality constraints. For these problems, we\nestablish the global convergence rate of inexact ALM and estimate its iteration\ncomplexity in terms of the number of gradient evaluations to produce a solution\nwith a specified accuracy.\n  We first establish an ergodic convergence rate result of inexact ALM that\nuses constant penalty parameters or geometrically increasing penalty\nparameters. Based on the convergence rate result, we apply Nesterov's optimal\nfirst-order method on each primal subproblem and estimate the iteration\ncomplexity of the inexact ALM. We show that if the objective is convex, then\n$O(\\varepsilon^{-1})$ gradient evaluations are sufficient to guarantee an\n$\\varepsilon$-optimal solution in terms of both primal objective and\nfeasibility violation. If the objective is strongly convex, the result can be\nimproved to $O(\\varepsilon^{-\\frac{1}{2}}|\\log\\varepsilon|)$. Finally, by\nrelating to the inexact proximal point algorithm, we establish a nonergodic\nconvergence rate result of inexact ALM that uses geometrically increasing\npenalty parameters. We show that the nonergodic iteration complexity result is\nin the same order as that for the ergodic result. Numerical experiments on\nquadratically constrained quadratic programming are conducted to compare the\nperformance of the inexact ALM with different settings. \n\n"}
{"id": "1711.06883", "contents": "Title: Fully Dynamic Almost-Maximal Matching: Breaking the Polynomial Barrier\n  for Worst-Case Time Bounds Abstract: Despite significant research efforts, the state-of-the-art algorithm for\nmaintaining an approximate matching in fully dynamic graphs has a polynomial\n{worst-case} update time, even for very poor approximation guarantees. In a\nrecent breakthrough, Bhattacharya, Henzinger and Nanongkai showed how to\nmaintain a constant approximation to the minimum vertex cover, and thus also a\nconstant-factor estimate of the maximum matching size, with polylogarithmic\nworst-case update time. Later (in SODA'17 Proc.) they improved the\napproximation factor all the way to $2+\\epsilon$. Nevertheless, the\nlongstanding fundamental problem of {maintaining} an approximate matching with\nsub-polynomial worst-case time bounds remained open.\n  We present a randomized algorithm for maintaining an {almost-maximal}\nmatching in fully dynamic graphs with polylogarithmic worst-case update time.\nSuch a matching provides $(2+\\epsilon)$-approximations for both the maximum\nmatching and the minimum vertex cover, for any $\\epsilon > 0$. Our result was\ndone independently of the $(2+\\epsilon)$-approximation result of Bhattacharya\net al., so it provides the first $(2+\\epsilon)$-approximation for minimum\nvertex cover (together with Bhattacharya et al.'s result) and the first\n$(2+\\epsilon)$-approximation for maximum (integral) matching.\n  The polylogarithmic worst-case update time of our algorithm holds\ndeterministically, while the almost-maximality guarantee holds with high\nprobability. This result not only settles the aforementioned problem on dynamic\nmatchings, but also provides essentially the best possible approximation\nguarantee for dynamic vertex cover (assuming the unique games conjecture). \n\n"}
{"id": "1711.07285", "contents": "Title: Quantum Query Algorithms are Completely Bounded Forms Abstract: We prove a characterization of $t$-query quantum algorithms in terms of the\nunit ball of a space of degree-$2t$ polynomials. Based on this, we obtain a\nrefined notion of approximate polynomial degree that equals the quantum query\ncomplexity, answering a question of Aaronson et al. (CCC'16). Our proof is\nbased on a fundamental result of Christensen and Sinclair (J. Funct. Anal.,\n1987) that generalizes the well-known Stinespring representation for quantum\nchannels to multilinear forms. Using our characterization, we show that many\npolynomials of degree four are far from those coming from two-query quantum\nalgorithms. We also give a simple and short proof of one of the results of\nAaronson et al. showing an equivalence between one-query quantum algorithms and\nbounded quadratic polynomials.\n  Revision note: A mistake was found in the proof of the second result on\ndegree-4 polynomials far from 2-query quantum algorithms. An explanation of the\nissue, a corrected proof and stronger examples are presented in work of\nEscudero Guti\\'errez and the second author. \n\n"}
{"id": "1711.08041", "contents": "Title: The Set Cover Conjecture and Subgraph Isomorphism with a Tree Pattern Abstract: In the Set Cover problem, the input is a ground set of $n$ elements and a\ncollection of $m$ sets, and the goal is to find the smallest sub-collection of\nsets whose union is the entire ground set. The fastest algorithm known runs in\ntime $O(mn2^n)$ [Fomin et al., WG 2004], and the Set Cover Conjecture (SeCoCo)\n[Cygan et al., TALG 2016] asserts that for every fixed $\\varepsilon>0$, no\nalgorithm can solve Set Cover in time $2^{(1-\\varepsilon)n}poly(m)$, even if\nset sizes are bounded by $\\Delta=\\Delta(\\varepsilon)$. We show strong\nconnections between this problem and kTree, a special case of Subgraph\nIsomorphism where the input is an $n$-node graph $G$ and a $k$-node tree $T$,\nand the goal is to determine whether $G$ has a subgraph isomorphic to $T$.\n  First, we propose a weaker conjecture Log-SeCoCo, that allows input sets of\nsize $\\Delta=O(1/\\varepsilon \\cdot\\log n)$, and show that an algorithm breaking\nLog-SeCoCo would imply a faster algorithm than the currently known $2^n\npoly(n)$-time algorithm [Koutis and Williams, TALG 2016] for Directed nTree,\nwhich is kTree with $k=n$ and arbitrary directions to the edges of $G$ and $T$.\nThis would also improve the running time for Directed Hamiltonicity, for which\nno algorithm significantly faster than $2^n poly(n)$ is known despite extensive\nresearch.\n  Second, we prove that if Set Cover cannot be solved significantly faster than\n$2^npoly(m)$ (an assumption even weaker than Log-SeCoCo), then kTree cannot be\ncomputed significantly faster than $2^kpoly(n)$, the running time of the Koutis\nand Williams' algorithm. Applying the same techniques to the p-Partial Cover\nproblem, a parameterized version of Set Cover that requires covering at least\n$p$ elements, we obtain a new algorithm with running time $(2+\\varepsilon)^p\n(m+n)^{O(1/\\varepsilon)}$ for arbitrary $\\varepsilon>0$, which improves\nprevious work and is nearly optimal assuming say Log-SeCoCo. \n\n"}
{"id": "1711.08421", "contents": "Title: Relief-Based Feature Selection: Introduction and Review Abstract: Feature selection plays a critical role in biomedical data mining, driven by\nincreasing feature dimensionality in target problems and growing interest in\nadvanced but computationally expensive methodologies able to model complex\nassociations. Specifically, there is a need for feature selection methods that\nare computationally efficient, yet sensitive to complex patterns of\nassociation, e.g. interactions, so that informative features are not mistakenly\neliminated prior to downstream modeling. This paper focuses on Relief-based\nalgorithms (RBAs), a unique family of filter-style feature selection algorithms\nthat have gained appeal by striking an effective balance between these\nobjectives while flexibly adapting to various data characteristics, e.g.\nclassification vs. regression. First, this work broadly examines types of\nfeature selection and defines RBAs within that context. Next, we introduce the\noriginal Relief algorithm and associated concepts, emphasizing the intuition\nbehind how it works, how feature weights generated by the algorithm can be\ninterpreted, and why it is sensitive to feature interactions without evaluating\ncombinations of features. Lastly, we include an expansive review of RBA\nmethodological research beyond Relief and its popular descendant, ReliefF. In\nparticular, we characterize branches of RBA research, and provide comparative\nsummaries of RBA algorithms including contributions, strategies, functionality,\ntime complexity, adaptation to key data characteristics, and software\navailability. \n\n"}
{"id": "1711.08494", "contents": "Title: Deterministic parallel algorithms for bilinear objective functions Abstract: Many randomized algorithms can be derandomized efficiently using either the\nmethod of conditional expectations or probability spaces with low independence.\nA series of papers, beginning with work by Luby (1988), showed that in many\ncases these techniques can be combined to give deterministic parallel (NC)\nalgorithms for a variety of combinatorial optimization problems, with low time-\nand processor-complexity.\n  We extend and generalize a technique of Luby for efficiently handling\nbilinear objective functions. One noteworthy application is an NC algorithm for\nmaximal independent set. On a graph $G$ with $m$ edges and $n$ vertices, this\ntakes $\\tilde O(\\log^2 n)$ time and $(m + n) n^{o(1)}$ processors, nearly\nmatching the best randomized parallel algorithms. Other applications include\nreduced processor counts for algorithms of Berger (1997) for maximum acyclic\nsubgraph and Gale-Berlekamp switching games.\n  This bilinear factorization also gives better algorithms for problems\ninvolving discrepancy. An important application of this is to automata-fooling\nprobability spaces, which are the basis of a notable derandomization technique\nof Sivakumar (2002). Our method leads to large reduction in processor\ncomplexity for a number of derandomization algorithms based on\nautomata-fooling, including set discrepancy and the Johnson-Lindenstrauss\nLemma. \n\n"}
{"id": "1711.08921", "contents": "Title: Automated Algorithm Selection on Continuous Black-Box Problems By\n  Combining Exploratory Landscape Analysis and Machine Learning Abstract: In this paper, we build upon previous work on designing informative and\nefficient Exploratory Landscape Analysis features for characterizing problems'\nlandscapes and show their effectiveness in automatically constructing algorithm\nselection models in continuous black-box optimization problems. Focussing on\nalgorithm performance results of the COCO platform of several years, we\nconstruct a representative set of high-performing complementary solvers and\npresent an algorithm selection model that - compared to the portfolio's single\nbest solver - on average requires less than half of the resources for solving a\ngiven problem. Therefore, there is a huge gain in efficiency compared to\nclassical ensemble methods combined with an increased insight into problem\ncharacteristics and algorithm properties by using informative features. Acting\non the assumption that the function set of the Black-Box Optimization Benchmark\nis representative enough for practical applications the model allows for\nselecting the best suited optimization algorithm within the considered set for\nunseen problems prior to the optimization itself based on a small sample of\nfunction evaluations. Note that such a sample can even be reused for the\ninitial population of an evolutionary (optimization) algorithm so that even the\nfeature costs become negligible. \n\n"}
{"id": "1711.10634", "contents": "Title: Active Betweenness Cardinality: Algorithms and Applications Abstract: Centrality rankings such as degree, closeness, betweenness, Katz, PageRank,\netc. are commonly used to identify critical nodes in a graph. These methods are\nbased on two assumptions that restrict their wider applicability. First, they\nassume the exact topology of the network is available. Secondly, they do not\ntake into account the activity over the network and only rely on its topology.\nHowever, in many applications, the network is autonomous, vast, and\ndistributed, and it is hard to collect the exact topology. At the same time,\nthe underlying pairwise activity between node pairs is not uniform and node\ncriticality strongly depends on the activity on the underlying network.\n  In this paper, we propose active betweenness cardinality, as a new measure,\nwhere the node criticalities are based on not the static structure, but the\nactivity of the network. We show how this metric can be computed efficiently by\nusing only local information for a given node and how we can find the most\ncritical nodes starting from only a few nodes. We also show how this metric can\nbe used to monitor a network and identify failed nodes.We present experimental\nresults to show effectiveness by demonstrating how the failed nodes can be\nidentified by measuring active betweenness cardinality of a few nodes in the\nsystem. \n\n"}
{"id": "1711.11469", "contents": "Title: Sum of squares lower bounds from symmetry and a good story Abstract: In this paper, we develop machinery which makes it much easier to prove sum\nof squares lower bounds when the problem is symmetric under permutations of\n$[1,n]$ and the unsatisfiability of our problem comes from integrality\narguments, i.e. arguments that an expression must be an integer. Roughly\nspeaking, to prove SOS lower bounds with our machinery it is sufficient to\nverify that the answer to the following three questions is yes:\n  1. Are there natural pseudo-expectation values for the problem?\n  2. Are these pseudo-expectation values rational functions of the problem\nparameters?\n  3. Are there sufficiently many values of the parameters for which these\npseudo-expectation values correspond to the actual expected values over a\ndistribution of solutions which is the uniform distribution over permutations\nof a single solution?\n  We demonstrate our machinery on three problems, the knapsack problem analyzed\nby Grigoriev, the MOD 2 principle (which says that the complete graph $K_n$ has\nno perfect matching when $n$ is odd), and the following Turan type problem:\nMinimize the number of triangles in a graph $G$ with a given edge density. For\nknapsack, we recover Grigoriev's lower bound exactly. For the MOD 2 principle,\nwe tighten Grigoriev's linear degree sum of squares lower bound, making it\nexact. Finally, for the triangle problem, we prove a sum of squares lower bound\nfor finding the minimum triangle density. This lower bound is completely new\nand gives a simple example where constant degree sum of squares methods have a\nconstant factor error in estimating graph densities. \n\n"}
{"id": "1712.00810", "contents": "Title: The Complexity of Satisfiability in Non-Iterated and Iterated\n  Probabilistic Logics Abstract: Let L be some extension of classical propositional logic. The non-iterated\nprobabilistic logic over L, is the logic PL that is defined by adding\nnon-nested probabilistic operators in the language of L. For example in PL we\ncan express a statement like \"the probability of truthfulness of A is at 0.3\"\nwhere A is a formula of L. The iterated probabilistic logic over L is the logic\nPPL, where the probabilistic operators may be iterated (nested). For example,\nin PPL we can express a statement like \"this coin is counterfeit with\nprobability 0.6\". In this paper we investigate the influence of probabilistic\noperators in the complexity of satisfiability in PL and PPL. We obtain\ncomplexity bounds, for the aforementioned satisfiability problem, which are\nparameterized in the complexity of satisfiability of conjunctions of positive\nand negative formulas that have neither a probabilistic nor a classical\noperator as a top-connective. As an application of our results we obtain tight\ncomplexity bounds for the satisfiability problem in PL and PPL when L is\nclassical propositional logic or justification logic. \n\n"}
{"id": "1712.02447", "contents": "Title: On Colouring $(2P_2,H)$-Free and $(P_5,H)$-Free Graphs Abstract: The Colouring problem asks whether the vertices of a graph can be coloured\nwith at most $k$ colours for a given integer $k$ in such a way that no two\nadjacent vertices receive the same colour. A graph is $(H_1,H_2)$-free if it\nhas no induced subgraph isomorphic to $H_1$ or $H_2$. A connected graph $H_1$\nis almost classified if Colouring on $(H_1,H_2)$-free graphs is known to be\npolynomial-time solvable or NP-complete for all but finitely many connected\ngraphs $H_2$. We show that every connected graph $H_1$ apart from the claw\n$K_{1,3}$ and the $5$-vertex path $P_5$ is almost classified. We also prove a\nnumber of new hardness results for Colouring on $(2P_2,H)$-free graphs. This\nenables us to list all graphs $H$ for which the complexity of Colouring is open\non $(2P_2,H)$-free graphs and all graphs $H$ for which the complexity of\nColouring is open on $(P_5,H)$-free graphs. In fact we show that these two\nlists coincide. Moreover, we show that the complexities of Colouring for\n$(2P_2,H)$-free graphs and for $(P_5,H)$-free graphs are the same for all known\ncases. \n\n"}
{"id": "1712.04043", "contents": "Title: How to navigate through obstacles? Abstract: Given a set of obstacles and two points, is there a path between the two\npoints that does not cross more than $k$ different obstacles? This is a\nfundamental problem that has undergone a tremendous amount of work. It is known\nto be NP-hard, even when the obstacles are very simple geometric shapes (e.g.,\nunit-length line segments). The problem can be generalized into the following\ngraph problem: Given a planar graph $G$ whose vertices are colored by color\nsets, two designated vertices $s, t \\in V(G)$, and $k \\in \\mathbb{N}$, is there\nan $s$-$t$ path in $G$ that uses at most $k$ colors? If each obstacle is\nconnected, the resulting graph satisfies the color-connectivity property,\nnamely that each color induces a connected subgraph.\n  We study the complexity and design algorithms for the above graph problem\nwith an eye on its geometric applications. We prove that without the\ncolor-connectivity property, the problem is W[SAT]-hard parameterized by $k$. A\ncorollary of this result is that, unless W[2] $=$ FPT, the problem cannot be\napproximated in FPT time to within a factor that is a function of $k$. By\ndescribing a generic plane embedding of the graph instances, we show that our\nhardness results translate to the geometric instances of the problem.\n  We then focus on graphs satisfying the color-connectivity property. By\nexploiting the planarity of the graph and the connectivity of the colors, we\ndevelop topological results to \"represent\" the valid $s$-$t$ paths containing\nsubsets of colors from any vertex $v$. We employ these results to design an FPT\nalgorithm for the problem parameterized by both $k$ and the treewidth of the\ngraph, and extend this result to obtain an FPT algorithm for the\nparameterization by both $k$ and the length of the path. The latter result\ndirectly implies previous FPT results for various obstacle shapes, such as unit\ndisks and fat regions. \n\n"}
{"id": "1712.04099", "contents": "Title: Towards a proof of the 24-cell conjecture Abstract: This review paper is devoted to the problems of sphere packings in 4\ndimensions. The main goal is to find reasonable approaches for solutions to\nproblems related to densest sphere packings in 4-dimensional Euclidean space.\nWe consider two long-standing open problems: the uniqueness of maximum kissing\narrangements in 4 dimensions and the 24-cell conjecture. Note that a proof of\nthe 24-cell conjecture also proves that the checkerboard lattice packing D4 is\nthe densest sphere packing in 4 dimensions. \n\n"}
{"id": "1712.05010", "contents": "Title: QPTAS and Subexponential Algorithm for Maximum Clique on Disk Graphs Abstract: A (unit) disk graph is the intersection graph of closed (unit) disks in the\nplane. Almost three decades ago, an elegant polynomial-time algorithm was found\nfor \\textsc{Maximum Clique} on unit disk graphs [Clark, Colbourn, Johnson;\nDiscrete Mathematics '90]. Since then, it has been an intriguing open question\nwhether or not tractability can be extended to general disk graphs. We show the\nrather surprising structural result that a disjoint union of cycles is the\ncomplement of a disk graph if and only if at most one of those cycles is of odd\nlength. From that, we derive the first QPTAS and subexponential algorithm\nrunning in time $2^{\\tilde{O}(n^{2/3})}$ for \\textsc{Maximum Clique} on disk\ngraphs. In stark contrast, \\textsc{Maximum Clique} on intersection graphs of\nfilled ellipses or filled triangles is unlikely to have such algorithms, even\nwhen the ellipses are close to unit disks. Indeed, we show that there is a\nconstant approximation which is not attainable even in time\n$2^{n^{1-\\varepsilon}}$, unless the Exponential Time Hypothesis fails. \n\n"}
{"id": "1712.05142", "contents": "Title: Completeness for the Complexity Class $\\forall \\exists \\mathbb{R}$ and\n  Area-Universality Abstract: Exhibiting a deep connection between purely geometric problems and real\nalgebra, the complexity class $\\exists \\mathbb{R}$ plays a crucial role in the\nstudy of geometric problems. Sometimes $\\exists \\mathbb{R}$ is referred to as\nthe 'real analog' of NP. While NP is a class of computational problems that\ndeals with existentially quantified boolean variables, $\\exists \\mathbb{R}$\ndeals with existentially quantified real variables.\n  In analogy to $\\Pi_2^p$ and $\\Sigma_2^p$ in the famous polynomial hierarchy,\nwe study the complexity classes $\\forall \\exists \\mathbb{R}$ and $\\exists\n\\forall \\mathbb{R}$ with real variables. Our main interest is the\narea-universality problem, where we are given a plane graph $G$, and ask if for\neach assignment of areas to the inner faces of $G$, there exists a\nstraight-line drawing of $G$ realizing the assigned areas. We conjecture that\narea-universality is $\\forall \\exists \\mathbb{R}$-complete and support this\nconjecture by proving $\\exists \\mathbb{R}$- and $\\forall \\exists\n\\mathbb{R}$-completeness of two variants of area-universality. To this end, we\nintroduce tools to prove $\\forall \\exists \\mathbb{R}$-hardness and membership.\nFinally, we present geometric problems as candidates for $\\forall \\exists\n\\mathbb{R}$-complete problems. These problems have connections to the concepts\nof imprecision, robustness, and extendability. \n\n"}
{"id": "1712.06309", "contents": "Title: FPT-algorithms for some problems related to integer programming Abstract: In this paper, we present FPT-algorithms for special cases of the shortest\nlattice vector, integer linear programming, and simplex width computation\nproblems, when matrices included in the problems' formulations are near square.\nThe parameter is the maximum absolute value of rank minors of the corresponding\nmatrices. Additionally, we present FPT-algorithms with respect to the same\nparameter for the problems, when the matrices have no singular rank\nsub-matrices. \n\n"}
{"id": "1712.07431", "contents": "Title: Text Indexing and Searching in Sublinear Time Abstract: We introduce the first index that can be built in $o(n)$ time for a text of\nlength $n$, and can also be queried in $o(q)$ time for a pattern of length $q$.\nOn an alphabet of size $\\sigma$, our index uses $O(n\\sqrt{\\log n\\log\\sigma})$\nbits, is built in $O(n((\\log\\log n)^2+\\sqrt{\\log\\sigma})/\\sqrt{\\log_\\sigma n})$\ndeterministic time, and computes the number $\\mathrm{occ}$ of occurrences of\nthe pattern in time $O(q/\\log_\\sigma n+\\log n)$. Each such occurrence can then\nbe found in $O(\\sqrt{\\log n\\log\\sigma})$ time. By slightly increasing the space\nand construction time, to $O(n(\\sqrt{\\log n\\log\\sigma}+\n\\log\\sigma\\log^\\varepsilon n))$ and $O(n\\log^{3/2}\\sigma/\\log^{1/2-\\varepsilon}\nn)$, respectively, for any constant $0<\\varepsilon<1/2$, we can find the\n$\\mathrm{occ}$ pattern occurrences in time $O(q/\\log_\\sigma n +\n\\sqrt{\\log_\\sigma n}\\log\\log n + \\mathrm{occ})$. We build on a novel text\nsampling based on difference covers, which enjoys properties that allow us\nefficiently computing longest common prefixes in constant time. We extend our\nresults to the secondary memory model as well, where we give the first\nconstruction in $o(\\mathit{Sort}(n))$ I/Os of a data structure with suffix\narray functionality; this data structure supports pattern matching queries with\noptimal or nearly-optimal cost. \n\n"}
{"id": "1712.08205", "contents": "Title: Practically-Self-Stabilizing Vector Clocks in the Absence of Execution\n  Fairness Abstract: Vector clock algorithms are basic wait-free building blocks that facilitate\ncausal ordering of events. As wait-free algorithms, they are guaranteed to\ncomplete their operations within a finite number of steps. Stabilizing\nalgorithms allow the system to recover after the occurrence of transient\nfaults, such as soft errors and arbitrary violations of the assumptions\naccording to which the system was designed to behave. We present the first, to\nthe best of our knowledge, stabilizing vector clock algorithm for asynchronous\ncrash-prone message-passing systems that can recover in a wait-free manner\nafter the occurrence of transient faults. In these settings, it is challenging\nto demonstrate a finite and wait-free recovery from (communication and crash\nfailures as well as) transient faults, bound the message and storage sizes,\ndeal with the removal of all stale information without blocking, and deal with\ncounter overflow events (which occur at different network nodes concurrently).\n  We present an algorithm that never violates safety in the absence of\ntransient faults and provides bounded time recovery during fair executions that\nfollow the last transient fault. The novelty is that in the absence of\nexecution fairness, the algorithm guarantees a bound on the number of times in\nwhich the system might violate safety (while existing algorithms might block\nforever due to the presence of both transient faults and crash failures).\n  Since vector clocks facilitate a number of elementary synchronization\nbuilding blocks (without requiring remote replica synchronization) in\nasynchronous systems, we believe that our analytical insights are useful for\nthe design of other systems that cannot guarantee execution fairness. \n\n"}
{"id": "1712.08373", "contents": "Title: Notes on complexity of packing coloring Abstract: A packing $k$-coloring for some integer $k$ of a graph $G=(V,E)$ is a mapping\n  $\\varphi:V\\to\\{1,\\ldots,k\\}$ such that any two vertices $u, v$ of color\n$\\varphi(u)=\\varphi(v)$ are in distance at least $\\varphi(u)+1$. This concept\nis motivated by frequency assignment problems. The \\emph{packing chromatic\nnumber} of $G$ is the smallest $k$ such that there exists a packing\n$k$-coloring of $G$.\n  Fiala and Golovach showed that determining the packing chromatic number for\nchordal graphs is \\NP-complete for diameter exactly 5. While the problem is\neasy to solve for diameter 2, we show \\NP-completeness for any diameter at\nleast 3. Our reduction also shows that the packing chromatic number is hard to\napproximate within $n^{{1/2}-\\varepsilon}$ for any $\\varepsilon > 0$.\n  In addition, we design an \\FPT algorithm for interval graphs of bounded\ndiameter. This leads us to exploring the problem of finding a partial coloring\nthat maximizes the number of colored vertices. \n\n"}
{"id": "1712.08709", "contents": "Title: Pruning based Distance Sketches with Provable Guarantees on Random\n  Graphs Abstract: Measuring the distances between vertices on graphs is one of the most\nfundamental components in network analysis. Since finding shortest paths\nrequires traversing the graph, it is challenging to obtain distance information\non large graphs very quickly. In this work, we present a preprocessing\nalgorithm that is able to create landmark based distance sketches efficiently,\nwith strong theoretical guarantees. When evaluated on a diverse set of social\nand information networks, our algorithm significantly improves over existing\napproaches by reducing the number of landmarks stored, preprocessing time, or\nstretch of the estimated distances.\n  On Erd\\\"{o}s-R\\'{e}nyi graphs and random power law graphs with degree\ndistribution exponent $2 < \\beta < 3$, our algorithm outputs an exact distance\ndata structure with space between $\\Theta(n^{5/4})$ and $\\Theta(n^{3/2})$\ndepending on the value of $\\beta$, where $n$ is the number of vertices. We\ncomplement the algorithm with tight lower bounds for Erdos-Renyi graphs and the\ncase when $\\beta$ is close to two. \n\n"}
{"id": "1712.10197", "contents": "Title: Interesting Paths in the Mapper Abstract: The Mapper produces a compact summary of high dimensional data as a\nsimplicial complex. We study the problem of quantifying the interestingness of\nsubpopulations in a Mapper, which appear as long paths, flares, or loops.\nFirst, we create a weighted directed graph G using the 1-skeleton of the\nMapper. We use the average values at the vertices of a target function to\ndirect edges (from low to high). The difference between the average values at\nvertices (high-low) is set as the edge's weight. Covariation of the remaining h\nfunctions (independent variables) is captured by a h-bit binary signature\nassigned to the edge. An interesting path in G is a directed path whose edges\nall have the same signature. We define the interestingness score of such a path\nas a sum of its edge weights multiplied by a nonlinear function of their ranks\nin the path.\n  Second, we study three optimization problems on this graph G. In the problem\nMax-IP, we seek an interesting path in G with the maximum interestingness\nscore. We show that Max-IP is NP-complete. For the special case when G is a\ndirected acyclic graph (DAG), we show that Max-IP can be solved in polynomial\ntime - in O(mnd_i) where d_i is the maximum indegree of a vertex in G.\n  In the more general problem IP, the goal is to find a collection of\nedge-disjoint interesting paths such that the overall sum of their\ninterestingness scores is maximized. We also study a variant of IP termed k-IP,\nwhere the goal is to identify a collection of edge-disjoint interesting paths\neach with k edges, and their total interestingness score is maximized. While\nk-IP can be solved in polynomial time for k <= 2, we show k-IP is NP-complete\nfor k >= 3 even when G is a DAG. We develop polynomial time heuristics for IP\nand k-IP on DAGs. \n\n"}
{"id": "1712.10261", "contents": "Title: Optimal Lower Bounds for Sketching Graph Cuts Abstract: We study the space complexity of sketching cuts and Laplacian quadratic forms\nof graphs. We show that any data structure which approximately stores the sizes\nof all cuts in an undirected graph on $n$ vertices up to a $1+\\epsilon$ error\nmust use $\\Omega(n\\log n/\\epsilon^2)$ bits of space in the worst case,\nimproving the $\\Omega(n/\\epsilon^2)$ bound of Andoni et al. and matching the\nbest known upper bound achieved by spectral sparsifiers. Our proof is based on\na rigidity phenomenon for cut (and spectral) approximation which may be of\nindependent interest: any two $d-$regular graphs which approximate each other's\ncuts significantly better than a random graph approximates the complete graph\nmust overlap in a constant fraction of their edges. \n\n"}
{"id": "1801.04497", "contents": "Title: Near-optimal approximation algorithm for simultaneous Max-Cut Abstract: In the simultaneous Max-Cut problem, we are given $k$ weighted graphs on the\nsame set of $n$ vertices, and the goal is to find a cut of the vertex set so\nthat the minimum, over the $k$ graphs, of the cut value is as large as\npossible. Previous work [BKS15] gave a polynomial time algorithm which achieved\nan approximation factor of $1/2 - o(1)$ for this problem (and an approximation\nfactor of $1/2 + \\epsilon_k$ in the unweighted case, where $\\epsilon_k\n\\rightarrow 0$ as $k \\rightarrow \\infty$).\n  In this work, we give a polynomial time approximation algorithm for\nsimultaneous Max-Cut with an approximation factor of $0.8780$ (for all constant\n$k$). The natural SDP formulation for simultaneous Max-Cut was shown to have an\nintegrality gap of $1/2+\\epsilon_k$ in [BKS15]. In achieving the better\napproximation guarantee, we use a stronger Sum-of-Squares hierarchy SDP\nrelaxation and a rounding algorithm based on Raghavendra-Tan [RT12], in\naddition to techniques from [BKS15]. \n\n"}
{"id": "1801.04696", "contents": "Title: Robust capacitated trees and networks with uniform demands Abstract: We are interested in the design of robust (or resilient) capacitated rooted\nSteiner networks in case of terminals with uniform demands. Formally, we are\ngiven a graph, capacity and cost functions on the edges, a root, a subset of\nnodes called terminals, and a bound k on the number of edge failures. We first\nstudy the problem where k = 1 and the network that we want to design must be a\ntree covering the root and the terminals: we give complexity results and\npropose models to optimize both the cost of the tree and the number of\nterminals disconnected from the root in the worst case of an edge failure,\nwhile respecting the capacity constraints on the edges. Second, we consider the\nproblem of computing a minimum-cost survivable network, i.e., a network that\ncovers the root and terminals even after the removal of any k edges, while\nstill respecting the capacity constraints on the edges. We also consider the\npossibility of protecting a given number of edges. We propose three different\nformulations: a cut-set based formulation, a flow based one, and a bilevel one\n(with an attacker and a defender). We propose algorithms to solve each\nformulation and compare their efficiency. \n\n"}
{"id": "1801.05717", "contents": "Title: Exact quantum query complexity of weight decision problems via Chebyshev\n  polynomials Abstract: The weight decision problem, which requires to determine the Hamming weight\nof a given binary string, is a natural and important problem, with applications\nin cryptanalysis, coding theory, fault-tolerant circuit design and so on. In\nparticular, both Deutsch-Jozsa problem and Grover search problem can be\ninterpreted as special cases of weight decision problems. In this work, we\ninvestigate the exact quantum query complexity of weight decision problems,\nwhere the quantum algorithm must always output the correct answer. More\nspecifically we consider a partial Boolean function which distinguishes whether\nthe Hamming weight of the length-$n$ input is $k$ or it is $l$. Our\ncontribution includes both upper bounds and lower bounds for the precise number\nof queries. Furthermore, for most choices of $(\\frac{k}{n},\\frac{l}{n})$ and\nsufficiently large $n$, the gap between our upper and lower bounds is no more\nthan one. To get the results, we first build the connection between Chebyshev\npolynomials and our problem, then determine all the boundary cases of\n$(\\frac{k}{n},\\frac{l}{n})$ with matching upper and lower bounds, and finally\nwe generalize to other cases via a new \\emph{quantum padding} technique. This\nquantum padding technique can be of independent interest in designing other\nquantum algorithms. \n\n"}
{"id": "1801.06620", "contents": "Title: A high-performance analog Max-SAT solver and its application to Ramsey\n  numbers Abstract: We introduce a continuous-time analog solver for MaxSAT, a quintessential\nclass of NP-hard discrete optimization problems, where the task is to find a\ntruth assignment for a set of Boolean variables satisfying the maximum number\nof given logical constraints. We show that the scaling of an invariant of the\nsolver's dynamics, the escape rate, as function of the number of unsatisfied\nclauses can predict the global optimum value, often well before reaching the\ncorresponding state. We demonstrate the performance of the solver on hard\nMaxSAT competition problems. We then consider the two-color Ramsey number\n$R(m,m)$ problem, translate it to SAT, and apply our algorithm to the still\nunknown $R(5,5)$. We find edge colorings without monochromatic 5-cliques for\ncomplete graphs up to 42 vertices, while on 43 vertices we find colorings with\nonly two monochromatic 5-cliques, the best coloring found so far, supporting\nthe conjecture that $R(5,5) = 43$. \n\n"}
{"id": "1801.08098", "contents": "Title: A Chronological Edge-Driven Approach to Temporal Subgraph Isomorphism Abstract: Many real world networks are considered temporal networks, in which the\nchronological ordering of the edges has importance to the meaning of the data.\nPerforming temporal subgraph matching on such graphs requires the edges in the\nsubgraphs to match the order of the temporal graph motif we are searching for.\nPrevious methods for solving this rely on the use of static subgraph matching\nto find potential matches first, before filtering them based on edge order to\nfind the true temporal matches. We present a new algorithm for temporal\nsubgraph isomorphism that performs the subgraph matching directly on the\nchronologically sorted edges. By restricting our search to only the subgraphs\nwith chronologically correct edges, we can improve the performance of the\nalgorithm significantly. We present experimental timing results to show\nsignificant performance improvements on publicly available datasets for a\nnumber of different temporal query graph motifs with four or more nodes. We\nalso demonstrate a practical example of how temporal subgraph isomorphism can\nproduce more meaningful results than traditional static subgraph searches. \n\n"}
{"id": "1801.08709", "contents": "Title: Adaptive Lower Bound for Testing Monotonicity on the Line Abstract: In the property testing model, the task is to distinguish objects possessing\nsome property from the objects that are far from it. One of such properties is\nmonotonicity, when the objects are functions from one poset to another. This is\nan active area of research. In this paper we study query complexity of\n$\\epsilon$-testing monotonicity of a function $f\\colon [n]\\to[r]$. All our\nlower bounds are for adaptive two-sided testers.\n  * We prove a nearly tight lower bound for this problem in terms of $r$. The\nbound is $\\Omega(\\frac{\\log r}{\\log \\log r})$ when $\\epsilon = 1/2$. No\nprevious satisfactory lower bound in terms of $r$ was known.\n  * We completely characterise query complexity of this problem in terms of $n$\nfor smaller values of $\\epsilon$. The complexity is $\\Theta(\\epsilon^{-1} \\log\n(\\epsilon n))$. Apart from giving the lower bound, this improves on the best\nknown upper bound.\n  Finally, we give an alternative proof of the $\\Omega(\\epsilon^{-1}d\\log n -\n\\epsilon^{-1}\\log\\epsilon^{-1})$ lower bound for testing monotonicity on the\nhypergrid $[n]^d$ due to Chakrabarty and Seshadhri (RANDOM'13). \n\n"}
{"id": "1802.02050", "contents": "Title: Optimal Data Reduction for Graph Coloring Using Low-Degree Polynomials Abstract: The theory of kernelization can be used to rigorously analyze data reduction\nfor graph coloring problems. Here, the aim is to reduce a q-Coloring input to\nan equivalent but smaller input whose size is provably bounded in terms of\nstructural properties, such as the size of a minimum vertex cover. In this\npaper we settle two open problems about data reduction for q-Coloring.\n  First, we obtain a kernel of bitsize $O(k^{q-1}\\log{k})$ for q-Coloring\nparameterized by Vertex Cover, for any q >= 3. This size bound is optimal up to\n$k^{o(1)}$ factors assuming NP is not a subset of coNP/poly, and improves on\nthe previous-best kernel of size $O(k^q)$. We generalize this result for\ndeciding q-colorability of a graph G, to deciding the existence of a\nhomomorphism from G to an arbitrary fixed graph H. Furthermore, we can replace\nthe parameter vertex cover by the less restrictive parameter twin-cover. We\nprove that H-Coloring parameterized by Twin-Cover has a kernel of size\n$O(k^{\\Delta(H)}\\log k)$.\n  Our second result shows that 3-Coloring does not admit non-trivial\nsparsification: assuming NP is not a subset of coNP/poly, the parameterization\nby the number of vertices n admits no (generalized) kernel of size $O(n^{2-e})$\nfor any e > 0. Previously, such a lower bound was only known for coloring with\nq >= 4 colors. \n\n"}
{"id": "1802.02325", "contents": "Title: On The Hardness of Approximate and Exact (Bichromatic) Maximum Inner\n  Product Abstract: In this paper we study the (Bichromatic) Maximum Inner Product Problem\n(Max-IP), in which we are given sets $A$ and $B$ of vectors, and the goal is to\nfind $a \\in A$ and $b \\in B$ maximizing inner product $a \\cdot b$. Max-IP is\nvery basic and serves as the base problem in the recent breakthrough of [Abboud\net al., FOCS 2017] on hardness of approximation for polynomial-time problems.\nIt is also used (implicitly) in the argument for hardness of exact\n$\\ell_2$-Furthest Pair (and other important problems in computational geometry)\nin poly-log-log dimensions in [Williams, SODA 2018]. We have three main results\nregarding this problem.\n  First, we study the best multiplicative approximation ratio for Boolean\nMax-IP in sub-quadratic time. We show that, for Max-IP with two sets of $n$\nvectors from $\\{0,1\\}^{d}$, there is an $n^{2 - \\Omega(1)}$ time $\\left( d/\\log\nn \\right)^{\\Omega(1)}$-multiplicative-approximating algorithm, and we show this\nis conditionally optimal, as such a $\\left(d/\\log\nn\\right)^{o(1)}$-approximating algorithm would refute SETH.\n  Second, we achieve a similar characterization for the best additive\napproximation error to Boolean Max-IP. We show that, for Max-IP with two sets\nof $n$ vectors from $\\{0,1\\}^{d}$, there is an $n^{2 - \\Omega(1)}$ time\n$\\Omega(d)$-additive-approximating algorithm, and this is conditionally\noptimal, as such an $o(d)$-approximating algorithm would refute SETH\n[Rubinstein, STOC 2018].\n  Last, we revisit the hardness of solving Max-IP exactly for vectors with\ninteger entries. We show that, under SETH, for Max-IP with sets of $n$ vectors\nfrom $\\mathbb{Z}^{d}$ for some $d = 2^{O(\\log^{*} n)}$, every exact algorithm\nrequires $n^{2 - o(1)}$ time. With the reduction from [Williams, SODA 2018], it\nfollows that $\\ell_2$-Furthest Pair and Bichromatic $\\ell_2$-Closest Pair in\n$2^{O(\\log^{*} n)}$ dimensions require $n^{2 - o(1)}$ time. \n\n"}
{"id": "1802.02336", "contents": "Title: A Schematic Definition of Quantum Polynomial Time Computability Abstract: In the past four decades, the notion of quantum polynomial-time computability\nhas been mathematically modeled by quantum Turing machines as well as quantum\ncircuits. This paper seeks the third model, which is a quantum analogue of the\nschematic (inductive or constructive) definition of (primitive) recursive\nfunctions. For quantum functions mapping finite-dimensional Hilbert spaces to\nthemselves, we present such a schematic definition, composed of a small set of\ninitial quantum functions and a few construction rules that dictate how to\nbuild a new quantum function from the existing ones. We prove that our\nschematic definition precisely characterizes all functions that can be\ncomputable with high success probabilities on well-formed quantum Turing\nmachines in polynomial time, or equivalently uniform families of\npolynomial-size quantum circuits. Our new, schematic definition is quite simple\nand intuitive and, more importantly, it avoids the cumbersome introduction of\nthe well-formedness condition imposed on a quantum Turing machine model as well\nas of the uniformity condition necessary for a quantum circuit model. Our new\napproach can further open a door to the descriptional complexity of quantum\nfunctions, to the theory of higher-type quantum functionals, to the development\nof new first-order theories for quantum computing, and to the designing of\nprogramming languages for real-life quantum computers. \n\n"}
{"id": "1802.02547", "contents": "Title: Learning One Convolutional Layer with Overlapping Patches Abstract: We give the first provably efficient algorithm for learning a one hidden\nlayer convolutional network with respect to a general class of (potentially\noverlapping) patches. Additionally, our algorithm requires only mild conditions\non the underlying distribution. We prove that our framework captures commonly\nused schemes from computer vision, including one-dimensional and\ntwo-dimensional \"patch and stride\" convolutions.\n  Our algorithm-- $Convotron$ -- is inspired by recent work applying isotonic\nregression to learning neural networks. Convotron uses a simple, iterative\nupdate rule that is stochastic in nature and tolerant to noise (requires only\nthat the conditional mean function is a one layer convolutional network, as\nopposed to the realizable setting). In contrast to gradient descent, Convotron\nrequires no special initialization or learning-rate tuning to converge to the\nglobal optimum.\n  We also point out that learning one hidden convolutional layer with respect\nto a Gaussian distribution and just $one$ disjoint patch $P$ (the other patches\nmay be arbitrary) is $easy$ in the following sense: Convotron can efficiently\nrecover the hidden weight vector by updating $only$ in the direction of $P$. \n\n"}
{"id": "1802.03235", "contents": "Title: The $b$-bibranching Problem: TDI System, Packing, and Discrete Convexity Abstract: In this paper, we introduce the $b$-bibranching problem in digraphs, which is\na common generalization of the bibranching and $b$-branching problems. The\nbibranching problem, introduced by Schrijver (1982), is a common generalization\nof the branching and bipartite edge cover problems. Previous results on\nbibranchings include polynomial algorithms, a linear programming formulation\nwith total dual integrality, a packing theorem, and an M-convex submodular flow\nformulation. The $b$-branching problem, recently introduced by Kakimura,\nKamiyama, and Takazawa (2018), is a generalization of the branching problem\nadmitting higher indegree, i.e., each vertex $v$ can have indegree at most\n$b(v)$. For $b$-branchings, a combinatorial algorithm, a linear programming\nformulation with total dual integrality, and a packing theorem for branchings\nare extended. A main contribution of this paper is to extend those previous\nresults on bibranchings and $b$-branchings to $b$-bibranchings. That is, we\npresent a linear programming formulation with total dual integrality, a packing\ntheorem, and an M-convex submodular flow formulation for $b$-bibranchings. In\nparticular, the linear program and M-convex submodular flow formulations\nrespectively imply polynomial algorithms for finding a shortest\n$b$-bibranching. \n\n"}
{"id": "1802.03700", "contents": "Title: Stochastic Non-preemptive Co-flow Scheduling with Time-Indexed\n  Relaxation Abstract: Co-flows model a modern scheduling setting that is commonly found in a\nvariety of applications in distributed and cloud computing. A stochastic\nco-flow task contains a set of parallel flows with randomly distributed sizes.\nFurther, many applications require non-preemptive scheduling of co-flow tasks.\nThis paper gives an approximation algorithm for stochastic non-preemptive\nco-flow scheduling. The proposed approach uses a time-indexed linear\nrelaxation, and uses its solution to come up with a feasible schedule. This\nalgorithm is shown to achieve a competitive ratio of\n$(2\\log{m}+1)(1+\\sqrt{m}\\Delta)(1+m{\\Delta}){(3+\\Delta)}/{2}$ for zero-release\ntimes, and $(2\\log{m}+1)(1+\\sqrt{m}\\Delta)(1+m\\Delta)(2+\\Delta)$ for general\nrelease times, where $\\Delta$ represents the upper bound of squared coefficient\nof variation of processing times, and $m$ is the number of servers. \n\n"}
{"id": "1802.05399", "contents": "Title: Competitive caching with machine learned advice Abstract: Traditional online algorithms encapsulate decision making under uncertainty,\nand give ways to hedge against all possible future events, while guaranteeing a\nnearly optimal solution as compared to an offline optimum. On the other hand,\nmachine learning algorithms are in the business of extrapolating patterns found\nin the data to predict the future, and usually come with strong guarantees on\nthe expected generalization error.\n  In this work we develop a framework for augmenting online algorithms with a\nmachine learned oracle to achieve competitive ratios that provably improve upon\nunconditional worst case lower bounds when the oracle has low error. Our\napproach treats the oracle as a complete black box, and is not dependent on its\ninner workings, or the exact distribution of its errors.\n  We apply this framework to the traditional caching problem -- creating an\neviction strategy for a cache of size $k$. We demonstrate that naively\nfollowing the oracle's recommendations may lead to very poor performance, even\nwhen the average error is quite low. Instead we show how to modify the Marker\nalgorithm to take into account the oracle's predictions, and prove that this\ncombined approach achieves a competitive ratio that both (i) decreases as the\noracle's error decreases, and (ii) is always capped by $O(\\log k)$, which can\nbe achieved without any oracle input. We complement our results with an\nempirical evaluation of our algorithm on real world datasets, and show that it\nperforms well empirically even using simple off-the-shelf predictions. \n\n"}
{"id": "1802.05791", "contents": "Title: A Faster FPTAS for #Knapsack Abstract: Given a set $W = \\{w_1,\\ldots, w_n\\}$ of non-negative integer weights and an\ninteger $C$, the #Knapsack problem asks to count the number of distinct subsets\nof $W$ whose total weight is at most $C$. In the more general integer version\nof the problem, the subsets are multisets. That is, we are also given a set $\n\\{u_1,\\ldots, u_n\\}$ and we are allowed to take up to $u_i$ items of weight\n$w_i$.\n  We present a deterministic FPTAS for #Knapsack running in\n$O(n^{2.5}\\varepsilon^{-1.5}\\log(n \\varepsilon^{-1})\\log (n \\varepsilon))$\ntime. The previous best deterministic algorithm [FOCS 2011] runs in $O(n^3\n\\varepsilon^{-1} \\log(n\\varepsilon^{-1}))$ time (see also [ESA 2014] for a\nlogarithmic factor improvement). The previous best randomized algorithm [STOC\n2003] runs in $O(n^{2.5} \\sqrt{\\log (n\\varepsilon^{-1}) } + \\varepsilon^{-2}\nn^2 )$ time. Therefore, in the natural setting of constant $\\varepsilon$, we\nclose the gap between the $\\tilde O(n^{2.5})$ randomized algorithm and the\n$\\tilde O(n^3)$ deterministic algorithm.\n  For the integer version with $U = \\max_i \\{u_i\\}$, we present a deterministic\nFPTAS running in $O(n^{2.5}\\varepsilon^{-1.5}\\log(n\\varepsilon^{-1} \\log U)\\log\n(n \\varepsilon) \\log^2 U)$ time. The previous best deterministic algorithm\n[APPROX 2016] runs in $O(n^3\\varepsilon^{-1}\\log(n \\varepsilon^{-1} \\log U)\n\\log^2 U)$ time. \n\n"}
{"id": "1802.05905", "contents": "Title: Assigning times to minimise reachability in temporal graphs Abstract: Temporal graphs (in which edges are active at specified times) are of\nparticular relevance for spreading processes on graphs, e.g.~the spread of\ndisease or dissemination of information. Motivated by real-world applications,\nmodification of static graphs to control this spread has proven a rich topic\nfor previous research. Here, we introduce a new type of modification for\ntemporal graphs: the number of active times for each edge is fixed, but we can\nchange the relative order in which (sets of) edges are active. We investigate\nthe problem of determining an ordering of edges that minimises the maximum\nnumber of vertices reachable from any single starting vertex;\nepidemiologically, this corresponds to the worst-case number of vertices\ninfected in a single disease outbreak. We study two versions of this problem,\nboth of which we show to be $\\NP$-hard, and identify cases in which the problem\ncan be solved or approximated efficiently. \n\n"}
{"id": "1802.06030", "contents": "Title: Improving the Florentine algorithms: recovering algorithms for Motzkin\n  and Schr\\\"oder paths Abstract: We present random sampling procedures for Motzkin and Schr\\\"oder paths,\nfollowing previous work on Dyck paths. Our algorithms follow the anticipated\nrejection method of the Florentine algorithms (Barcucci et al. 1994+), but\nintroduce a recovery idea to greatly reduce the probability of rejection. They\nuse an optimal amount of randomness and achieve a better time complexity than\nthe Florentine algorithms. \n\n"}
{"id": "1802.06052", "contents": "Title: Online Continuous Submodular Maximization Abstract: In this paper, we consider an online optimization process, where the\nobjective functions are not convex (nor concave) but instead belong to a broad\nclass of continuous submodular functions. We first propose a variant of the\nFrank-Wolfe algorithm that has access to the full gradient of the objective\nfunctions. We show that it achieves a regret bound of $O(\\sqrt{T})$ (where $T$\nis the horizon of the online optimization problem) against a\n$(1-1/e)$-approximation to the best feasible solution in hindsight. However, in\nmany scenarios, only an unbiased estimate of the gradients are available. For\nsuch settings, we then propose an online stochastic gradient ascent algorithm\nthat also achieves a regret bound of $O(\\sqrt{T})$ regret, albeit against a\nweaker $1/2$-approximation to the best feasible solution in hindsight. We also\ngeneralize our results to $\\gamma$-weakly submodular functions and prove the\nsame sublinear regret bounds. Finally, we demonstrate the efficiency of our\nalgorithms on a few problem instances, including non-convex/non-concave\nquadratic programs, multilinear extensions of submodular set functions, and\nD-optimal design. \n\n"}
{"id": "1802.06212", "contents": "Title: Multi-Pass Streaming Algorithms for Monotone Submodular Function\n  Maximization Abstract: We consider maximizing a monotone submodular function under a cardinality\nconstraint or a knapsack constraint in the streaming setting. In particular,\nthe elements arrive sequentially and at any point of time, the algorithm has\naccess to only a small fraction of the data stored in primary memory. We\npropose the following streaming algorithms taking $O(\\varepsilon^{-1})$ passes:\n  ----a $(1-e^{-1}-\\varepsilon)$-approximation algorithm for the\ncardinality-constrained problem ---- a $(0.5-\\varepsilon)$-approximation\nalgorithm for the knapsack-constrained problem.\n  Both of our algorithms run in $O^\\ast(n)$ time, using $O^\\ast(K)$ space,\nwhere $n$ is the size of the ground set and $K$ is the size of the knapsack.\nHere the term $O^\\ast$ hides a polynomial of $\\log K$ and $\\varepsilon^{-1}$.\nOur streaming algorithms can also be used as fast approximation algorithms. In\nparticular, for the cardinality-constrained problem, our algorithm takes\n$O(n\\varepsilon^{-1} \\log (\\varepsilon^{-1}\\log K) )$ time, improving on the\nalgorithm of Badanidiyuru and Vondr\\'{a}k that takes $O(n \\varepsilon^{-1} \\log\n(\\varepsilon^{-1} K) )$ time. \n\n"}
{"id": "1802.06905", "contents": "Title: Communication-Optimal Convolutional Neural Nets Abstract: Efficiently executing convolutional neural nets (CNNs) is important in many\nmachine-learning tasks. Since the cost of moving a word of data, either between\nlevels of a memory hierarchy or between processors over a network, is much\nhigher than the cost of an arithmetic operation, minimizing data movement is\ncritical to performance optimization. In this paper, we present both new lower\nbounds on data movement needed for CNNs, and optimal sequential algorithms that\nattain these lower bounds. In most common cases, our optimal algorithms can\nattain significantly more data reuse than matrix multiplication. \n\n"}
{"id": "1802.07209", "contents": "Title: Distributed Symmetry-Breaking Algorithms for Congested Cliques Abstract: The {Congested Clique} is a distributed-computing model for single-hop\nnetworks with restricted bandwidth that has been very intensively studied\nrecently. It models a network by an $n$-vertex graph in which any pair of\nvertices can communicate one with another by transmitting $O(\\log n )$ bits in\neach round. Various problems have been studied in this setting, but for some of\nthem the best-known results are those for general networks. In this paper we\ndevise significantly improved algorithms for various symmetry-breaking\nproblems, such as forests-decompositions, vertex-colorings, and maximal\nindependent set.\n  We analyze the running time of our algorithms as a function of the arboricity\n$a$ of a clique subgraph that is given as input. Our algorithms are especially\nefficient in Trees, planar graphs, graphs with constant genus, and many other\ngraphs that have bounded arboricity, but unbounded size. We obtain\n$O(a)$-forest-decomposition algorithm with $O(\\log a)$ time that improves the\npreviously-known $O(\\log n)$ time, $O(a^{2 + \\epsilon})$-coloring in $O(\\log^*\nn)$ time that improves upon an $O(\\log n)$-time algorithm, $O(a)$-coloring in\n$O(a^{\\epsilon})$-time that improves upon several previous algorithms, and a\nmaximal independent set algorithm with $O(\\sqrt a)$ time that improves at least\nquadratically upon the state-of-the-art for small and moderate values of $a$.\n  Those results are achieved using several techniques. First, we produce a\nforest decomposition with a helpful structure called {$H$-partition} within\n$O(\\log a)$ rounds. In general graphs this structure requires $\\Theta(\\log n)$\ntime, but in Congested Cliques we are able to compute it faster. We employ this\nstructure in conjunction with partitioning techniques that allow us to solve\nvarious symmetry-breaking problems efficiently. \n\n"}
{"id": "1802.08252", "contents": "Title: The iisignature library: efficient calculation of iterated-integral\n  signatures and log signatures Abstract: Iterated-integral signatures and log signatures are vectors calculated from a\npath that characterise its shape. They come from the theory of differential\nequations driven by rough paths, and also have applications in statistics and\nmachine learning. We present algorithms for efficiently calculating these\nsignatures, and benchmark their performance. We release the methods as a Python\npackage. \n\n"}
{"id": "1802.08563", "contents": "Title: The Parameterized Hardness of the k-Center Problem in Transportation\n  Networks Abstract: In this paper we study the hardness of the $k$-Center problem on inputs that\nmodel transportation networks. For the problem, a graph $G=(V,E)$ with edge\nlengths and an integer $k$ are given and a center set $C\\subseteq V$ needs to\nbe chosen such that $|C|\\leq k$. The aim is to minimize the maximum distance of\nany vertex in the graph to the closest center. This problem arises in many\napplications of logistics, and thus it is natural to consider inputs that model\ntransportation networks. Such inputs are often assumed to be planar graphs, low\ndoubling metrics, or bounded highway dimension graphs. For each of these\nmodels, parameterized approximation algorithms have been shown to exist. We\ncomplement these results by proving that the $k$-Center problem is W[1]-hard on\nplanar graphs of constant doubling dimension, where the parameter is the\ncombination of the number of centers $k$, the highway dimension $h$, and the\npathwidth $p$. Moreover, under the Exponential Time Hypothesis there is no\n$f(k,p,h)\\cdot n^{o(p+\\sqrt{k+h})}$ time algorithm for any computable function\n$f$. Thus it is unlikely that the optimum solution to $k$-Center can be found\nefficiently, even when assuming that the input graph abides to all of the above\nmodels for transportation networks at once!\n  Additionally we give a simple parameterized $(1+\\varepsilon)$-approximation\nalgorithm for inputs of doubling dimension $d$ with runtime\n$(k^k/\\varepsilon^{O(kd)})\\cdot n^{O(1)}$. This generalizes a previous result,\nwhich considered inputs in $D$-dimensional $L_q$ metrics. \n\n"}
{"id": "1802.09001", "contents": "Title: The Complexity of the Possible Winner Problem over Partitioned\n  Preferences Abstract: The Possible-Winner problem asks, given an election where the voters'\npreferences over the set of candidates is partially specified, whether a\ndistinguished candidate can become a winner. In this work, we consider the\ncomputational complexity of Possible-Winner under the assumption that the voter\npreferences are $partitioned$. That is, we assume that every voter provides a\ncomplete order over sets of incomparable candidates (e.g., candidates are\nranked by their level of education). We consider elections with partitioned\nprofiles over positional scoring rules, with an unbounded number of candidates,\nand unweighted voters. Our first result is a polynomial time algorithm for\nvoting rules with $2$ distinct values, which include the well-known\n$k$-approval voting rule. We then go on to prove NP-hardness for a class of\nrules that contain all voting rules that produce scoring vectors with at least\n$4$ distinct values. \n\n"}
{"id": "1802.09111", "contents": "Title: Dynamic Effective Resistances and Approximate Schur Complement on\n  Separable Graphs Abstract: We consider the problem of dynamically maintaining (approximate) all-pairs\neffective resistances in separable graphs, which are those that admit an\n$n^{c}$-separator theorem for some $c<1$. We give a fully dynamic algorithm\nthat maintains $(1+\\varepsilon)$-approximations of the all-pairs effective\nresistances of an $n$-vertex graph $G$ undergoing edge insertions and deletions\nwith $\\tilde{O}(\\sqrt{n}/\\varepsilon^2)$ worst-case update time and\n$\\tilde{O}(\\sqrt{n}/\\varepsilon^2)$ worst-case query time, if $G$ is guaranteed\nto be $\\sqrt{n}$-separable (i.e., it is taken from a class satisfying a\n$\\sqrt{n}$-separator theorem) and its separator can be computed in\n$\\tilde{O}(n)$ time. Our algorithm is built upon a dynamic algorithm for\nmaintaining \\emph{approximate Schur complement} that approximately preserves\npairwise effective resistances among a set of terminals for separable graphs,\nwhich might be of independent interest.\n  We complement our result by proving that for any two fixed vertices $s$ and\n$t$, no incremental or decremental algorithm can maintain the $s-t$ effective\nresistance for $\\sqrt{n}$-separable graphs with worst-case update time\n$O(n^{1/2-\\delta})$ and query time $O(n^{1-\\delta})$ for any $\\delta>0$, unless\nthe Online Matrix Vector Multiplication (OMv) conjecture is false.\n  We further show that for \\emph{general} graphs, no incremental or decremental\nalgorithm can maintain the $s-t$ effective resistance problem with worst-case\nupdate time $O(n^{1-\\delta})$ and query-time $O(n^{2-\\delta})$ for any $\\delta\n>0$, unless the OMv conjecture is false. \n\n"}
{"id": "1802.09617", "contents": "Title: Multiscale Planar Graph Generation Abstract: The study of network representations of physical, biological, and social\nphenomena can help us better understand the structural and functional dynamics\nof their networks and formulate predictive models of these phenomena. However,\ndue to the scarcity of real-world network data owing to factors such as cost\nand effort required in collection of network data and the sensitivity of this\ndata towards theft and misuse, engineers and researchers often rely on\nsynthetic data for simulations, hypothesis testing, decision making, and\nalgorithm engineering. An important characteristic of infrastructure networks\nsuch as roads, water distribution and other utility systems is that they can be\nembedded in a plane, therefore to simulate these system we need realistic\nnetworks which are also planar. While the currently-available synthetic network\ngenerators can model networks that exhibit realism, they do not guarantee or\nachieve planarity. Therefore, in this paper we present a flexible algorithm\nthat can synthesize realistic networks that are planar. The method follows a\nmulti-scale randomized editing approach generating a hierarchy of coarsened\nnetworks of a given planar graph and introducing edits at various levels in the\nhierarchy. The method preserves the structural properties with minimal bias\nincluding the planarity of the network, while introducing realistic variability\nat multiple scales. \n\n"}
{"id": "1803.00796", "contents": "Title: Fine-Grained Complexity of Analyzing Compressed Data: Quantifying\n  Improvements over Decompress-And-Solve Abstract: Can we analyze data without decompressing it? As our data keeps growing,\nunderstanding the time complexity of problems on compressed inputs, rather than\nin convenient uncompressed forms, becomes more and more relevant. Suppose we\nare given a compression of size $n$ of data that originally has size $N$, and\nwe want to solve a problem with time complexity $T(\\cdot)$. The naive strategy\nof \"decompress-and-solve\" gives time $T(N)$, whereas \"the gold standard\" is\ntime $T(n)$: to analyze the compression as efficiently as if the original data\nwas small.\n  We restrict our attention to data in the form of a string (text, files,\ngenomes, etc.) and study the most ubiquitous tasks. While the challenge might\nseem to depend heavily on the specific compression scheme, most methods of\npractical relevance (Lempel-Ziv-family, dictionary methods, and others) can be\nunified under the elegant notion of Grammar Compressions. A vast literature,\nacross many disciplines, established this as an influential notion for\nAlgorithm design.\n  We introduce a framework for proving (conditional) lower bounds in this\nfield, allowing us to assess whether decompress-and-solve can be improved, and\nby how much. Our main results are:\n  - The $O(nN\\sqrt{\\log{N/n}})$ bound for LCS and the $O(\\min\\{N \\log N, nM\\})$\nbound for Pattern Matching with Wildcards are optimal up to $N^{o(1)}$ factors,\nunder the Strong Exponential Time Hypothesis. (Here, $M$ denotes the\nuncompressed length of the compressed pattern.)\n  - Decompress-and-solve is essentially optimal for Context-Free Grammar\nParsing and RNA Folding, under the $k$-Clique conjecture.\n  - We give an algorithm showing that decompress-and-solve is not optimal for\nDisjointness. \n\n"}
{"id": "1803.00938", "contents": "Title: Multivariate Fine-Grained Complexity of Longest Common Subsequence Abstract: We revisit the classic combinatorial pattern matching problem of finding a\nlongest common subsequence (LCS). For strings $x$ and $y$ of length $n$, a\ntextbook algorithm solves LCS in time $O(n^2)$, but although much effort has\nbeen spent, no $O(n^{2-\\varepsilon})$-time algorithm is known. Recent work\nindeed shows that such an algorithm would refute the Strong Exponential Time\nHypothesis (SETH) [Abboud, Backurs, Vassilevska Williams + Bringmann,\nK\\\"unnemann FOCS'15].\n  Despite the quadratic-time barrier, for over 40 years an enduring scientific\ninterest continued to produce fast algorithms for LCS and its variations.\nParticular attention was put into identifying and exploiting input parameters\nthat yield strongly subquadratic time algorithms for special cases of interest,\ne.g., differential file comparison. This line of research was successfully\npursued until 1990, at which time significant improvements came to a halt. In\nthis paper, using the lens of fine-grained complexity, our goal is to (1)\njustify the lack of further improvements and (2) determine whether some special\ncases of LCS admit faster algorithms than currently known.\n  To this end, we provide a systematic study of the multivariate complexity of\nLCS, taking into account all parameters previously discussed in the literature:\nthe input size $n:=\\max\\{|x|,|y|\\}$, the length of the shorter string\n$m:=\\min\\{|x|,|y|\\}$, the length $L$ of an LCS of $x$ and $y$, the numbers of\ndeletions $\\delta := m-L$ and $\\Delta := n-L$, the alphabet size, as well as\nthe numbers of matching pairs $M$ and dominant pairs $d$. For any class of\ninstances defined by fixing each parameter individually to a polynomial in\nterms of the input size, we prove a SETH-based lower bound matching one of\nthree known algorithms. Specifically, we determine the optimal running time for\nLCS under SETH as $(n+\\min\\{d, \\delta \\Delta, \\delta m\\})^{1\\pm o(1)}$.\n  [...] \n\n"}
{"id": "1803.01474", "contents": "Title: Optimizing Learned Bloom Filters by Sandwiching Abstract: We provide a simple method for improving the performance of the recently\nintroduced learned Bloom filters, by showing that they perform better when the\nlearned function is sandwiched between two Bloom filters. \n\n"}
{"id": "1803.02289", "contents": "Title: Testing the complexity of a valued CSP language Abstract: A Valued Constraint Satisfaction Problem (VCSP) provides a common framework\nthat can express a wide range of discrete optimization problems. A VCSP\ninstance is given by a finite set of variables, a finite domain of labels, and\nan objective function to be minimized. This function is represented as a sum of\nterms where each term depends on a subset of the variables. To obtain different\nclasses of optimization problems, one can restrict all terms to come from a\nfixed set $\\Gamma$ of cost functions, called a language.\n  Recent breakthrough results have established a complete complexity\nclassification of such classes with respect to language $\\Gamma$: if all cost\nfunctions in $\\Gamma$ satisfy a certain algebraic condition then all\n$\\Gamma$-instances can be solved in polynomial time, otherwise the problem is\nNP-hard. Unfortunately, testing this condition for a given language $\\Gamma$ is\nknown to be NP-hard. We thus study exponential algorithms for this\nmeta-problem. We show that the tractability condition of a finite-valued\nlanguage $\\Gamma$ can be tested in $O(\\sqrt[3]{3}^{\\,|D|}\\cdot\npoly(size(\\Gamma)))$ time, where $D$ is the domain of $\\Gamma$ and\n$poly(\\cdot)$ is some fixed polynomial. We also obtain a matching lower bound\nunder the Strong Exponential Time Hypothesis (SETH). More precisely, we prove\nthat for any constant $\\delta<1$ there is no $O(\\sqrt[3]{3}^{\\,\\delta|D|})$\nalgorithm, assuming that SETH holds. \n\n"}
{"id": "1803.02702", "contents": "Title: On kissing numbers and spherical codes in high dimensions Abstract: We prove a lower bound of $\\Omega (d^{3/2} \\cdot (2/\\sqrt{3})^d)$ on the\nkissing number in dimension $d$. This improves the classical lower bound of\nChabauty, Shannon, and Wyner by a linear factor in the dimension. We obtain a\nsimilar linear factor improvement to the best known lower bound on the maximal\nsize of a spherical code of acute angle $\\theta$ in high dimensions. \n\n"}
{"id": "1803.03239", "contents": "Title: Fairness Through Computationally-Bounded Awareness Abstract: We study the problem of fair classification within the versatile framework of\nDwork et al. [ITCS '12], which assumes the existence of a metric that measures\nsimilarity between pairs of individuals. Unlike earlier work, we do not assume\nthat the entire metric is known to the learning algorithm; instead, the learner\ncan query this arbitrary metric a bounded number of times. We propose a new\nnotion of fairness called metric multifairness and show how to achieve this\nnotion in our setting. Metric multifairness is parameterized by a similarity\nmetric $d$ on pairs of individuals to classify and a rich collection ${\\cal C}$\nof (possibly overlapping) \"comparison sets\" over pairs of individuals. At a\nhigh level, metric multifairness guarantees that similar subpopulations are\ntreated similarly, as long as these subpopulations are identified within the\nclass ${\\cal C}$. \n\n"}
{"id": "1803.03728", "contents": "Title: Geodesic nets with three boundary vertices Abstract: We prove that a geodesic net with three boundary (= unbalanced) vertices on a\nnon-positively curved plane has at most one balanced vertex. We do not assume\nany a priori bound for the degrees of unbalanced vertices. The result seems to\nbe new even in the Euclidean case. We demonstrate by examples that the result\nis not true for metrics of positive curvature on the plane, and that there are\nno immediate generalizations of this result for geodesic nets with four\nunbalanced vertices. \n\n"}
{"id": "1803.03833", "contents": "Title: Submodular Hypergraphs: p-Laplacians, Cheeger Inequalities and Spectral\n  Clustering Abstract: We introduce submodular hypergraphs, a family of hypergraphs that have\ndifferent submodular weights associated with different cuts of hyperedges.\nSubmodular hypergraphs arise in clustering applications in which higher-order\nstructures carry relevant information. For such hypergraphs, we define the\nnotion of p-Laplacians and derive corresponding nodal domain theorems and k-way\nCheeger inequalities. We conclude with the description of algorithms for\ncomputing the spectra of 1- and 2-Laplacians that constitute the basis of new\nspectral hypergraph clustering methods. \n\n"}
{"id": "1803.03922", "contents": "Title: Scalable Breadth-First Search on a GPU Cluster Abstract: On a GPU cluster, the ratio of high computing power to communication\nbandwidth makes scaling breadth-first search (BFS) on a scale-free graph\nextremely challenging. By separating high and low out-degree vertices, we\npresent an implementation with scalable computation and a model for scalable\ncommunication for BFS and direction-optimized BFS. Our communication model uses\nglobal reduction for high-degree vertices, and point-to-point transmission for\nlow-degree vertices. Leveraging the characteristics of degree separation, we\nreduce the graph size to one third of the conventional edge list\nrepresentation. With several other optimizations, we observe linear weak\nscaling as we increase the number of GPUs, and achieve 259.8 GTEPS on a\nscale-33 Graph500 RMAT graph with 124 GPUs on the latest CORAL early access\nsystem. \n\n"}
{"id": "1803.05084", "contents": "Title: Local Partition in Rich Graphs Abstract: Local graph partitioning is a key graph mining tool that allows researchers\nto identify small groups of interrelated nodes (e.g. people) and their\nconnective edges (e.g. interactions). Because local graph partitioning is\nprimarily focused on the network structure of the graph (vertices and edges),\nit often fails to consider the additional information contained in the\nattributes. In this paper we propose---(i) a scalable algorithm to improve\nlocal graph partitioning by taking into account both the network structure of\nthe graph and the attribute data and (ii) an application of the proposed local\ngraph partitioning algorithm (AttriPart) to predict the evolution of local\ncommunities (LocalForecasting). Experimental results show that our proposed\nAttriPart algorithm finds up to 1.6$\\times$ denser local partitions, while\nrunning approximately 43$\\times$ faster than traditional local partitioning\ntechniques (PageRank-Nibble). In addition, our LocalForecasting algorithm shows\na significant improvement in the number of nodes and edges correctly predicted\nover baseline methods. \n\n"}
{"id": "1803.05933", "contents": "Title: Some Closure Results for Polynomial Factorization and Applications Abstract: In a sequence of seminal results in the 80's, Kaltofen showed that the\ncomplexity class VP is closed under taking factors. A natural question in this\ncontext is to understand if other natural classes of multivariate polynomials,\nfor instance, arithmetic formulas, algebraic branching programs, bounded depth\narithmetic circuits or the class VNP, are closed under taking factors.\n  In this paper, we show that all factors of degree at most $\\log^a n$ of\npolynomials with poly(n) size depth $k$ circuits have poly(n) size circuits of\ndepth at most $O(k + a)$. This partially answers a question of\nShpilka-Yehudayoff and has applications to hardness-randomness tradeoffs for\nbounded depth arithmetic circuits. More precisely, this shows that a\nsuperpolynomial lower bound for bounded depth arithmetic circuits, for a family\nof explicit polynomials of degree poly$(\\log n)$ implies deterministic\nsub-exponential time algorithms for polynomial identity testing (PIT) for\nbounded depth arithmetic circuits. This is incomparable to a beautiful result\nof Dvir et al., where they showed that super-polynomial lower bounds for\nconstant depth arithmetic circuits for any explicit family of polynomials (of\npotentially high degree) implies sub-exponential time deterministic PIT for\nbounded depth circuits of bounded individual degree. Thus, we remove the\n\"bounded individual degree\" condition in [DSY09] at the cost of strengthening\nthe hardness assumption to hold for polynomials of low degree.\n  As direct applications of our techniques, we also show that the complexity\nclass VNP is closed under taking factors, thereby confirming a conjecture of\nB\\\"urgisser and get an alternate proof of the fact (first shown by Dutta et\nal.) that if a polynomial $Q$ of degree at most $d$ divides a polynomial $P$\ncomputable by a formula of size $s$, then $Q$ has a formula of size at most\npoly$(s, d^{\\log d}, deg(P))$. \n\n"}
{"id": "1803.06010", "contents": "Title: Ridge Regression and Provable Deterministic Ridge Leverage Score\n  Sampling Abstract: Ridge leverage scores provide a balance between low-rank approximation and\nregularization, and are ubiquitous in randomized linear algebra and machine\nlearning. Deterministic algorithms are also of interest in the moderately big\ndata regime, because deterministic algorithms provide interpretability to the\npractitioner by having no failure probability and always returning the same\nresults.\n  We provide provable guarantees for deterministic column sampling using ridge\nleverage scores. The matrix sketch returned by our algorithm is a column subset\nof the original matrix, yielding additional interpretability. Like the\nrandomized counterparts, the deterministic algorithm provides (1 + {\\epsilon})\nerror column subset selection, (1 + {\\epsilon}) error projection-cost\npreservation, and an additive-multiplicative spectral bound. We also show that\nunder the assumption of power-law decay of ridge leverage scores, this\ndeterministic algorithm is provably as accurate as randomized algorithms.\n  Lastly, ridge regression is frequently used to regularize ill-posed linear\nleast-squares problems. While ridge regression provides shrinkage for the\nregression coefficients, many of the coefficients remain small but non-zero.\nPerforming ridge regression with the matrix sketch returned by our algorithm\nand a particular regularization parameter forces coefficients to zero and has a\nprovable (1 + {\\epsilon}) bound on the statistical risk. As such, it is an\ninteresting alternative to elastic net regularization. \n\n"}
{"id": "1803.06521", "contents": "Title: Beyond the Low-Degree Algorithm: Mixtures of Subcubes and Their\n  Applications Abstract: We introduce the problem of learning mixtures of $k$ subcubes over\n$\\{0,1\\}^n$, which contains many classic learning theory problems as a special\ncase (and is itself a special case of others). We give a surprising $n^{O(\\log\nk)}$-time learning algorithm based on higher-order multilinear moments. It is\nnot possible to learn the parameters because the same distribution can be\nrepresented by quite different models. Instead, we develop a framework for\nreasoning about how multilinear moments can pinpoint essential features of the\nmixture, like the number of components.\n  We also give applications of our algorithm to learning decision trees with\nstochastic transitions (which also capture interesting scenarios where the\ntransitions are deterministic but there are latent variables). Using our\nalgorithm for learning mixtures of subcubes, we can approximate the Bayes\noptimal classifier within additive error $\\epsilon$ on $k$-leaf decision trees\nwith at most $s$ stochastic transitions on any root-to-leaf path in $n^{O(s +\n\\log k)}\\cdot\\text{poly}(1/\\epsilon)$ time. In this stochastic setting, the\nclassic Occam algorithms for learning decision trees with zero stochastic\ntransitions break down, while the low-degree algorithm of Linial et al.\ninherently has a quasipolynomial dependence on $1/\\epsilon$.\n  In contrast, as we will show, mixtures of $k$ subcubes are uniquely\ndetermined by their degree $2 \\log k$ moments and hence provide a useful\nabstraction for simultaneously achieving the polynomial dependence on\n$1/\\epsilon$ of the classic Occam algorithms for decision trees and the\nflexibility of the low-degree algorithm in being able to accommodate stochastic\ntransitions. Using our multilinear moment techniques, we also give the first\nimproved upper and lower bounds since the work of Feldman et al. for the\nrelated but harder problem of learning mixtures of binary product\ndistributions. \n\n"}
{"id": "1803.06636", "contents": "Title: Complexity problems in enumerative combinatorics Abstract: We give a broad survey of recent results in Enumerative Combinatorics and\ntheir complexity aspects. \n\n"}
{"id": "1803.06858", "contents": "Title: An improved isomorphism test for bounded-tree-width graphs Abstract: We give a new fpt algorithm testing isomorphism of $n$-vertex graphs of tree\nwidth $k$ in time $2^{k\\operatorname{polylog} (k)}\\operatorname{poly} (n)$,\nimproving the fpt algorithm due to Lokshtanov, Pilipczuk, Pilipczuk, and\nSaurabh (FOCS 2014), which runs in time $2^{\\mathcal{O}(k^5\\log\nk)}\\operatorname{poly} (n)$. Based on an improved version of the\nisomorphism-invariant graph decomposition technique introduced by Lokshtanov et\nal., we prove restrictions on the structure of the automorphism groups of\ngraphs of tree width $k$. Our algorithm then makes heavy use of the group\ntheoretic techniques introduced by Luks (JCSS 1982) in his isomorphism test for\nbounded degree graphs and Babai (STOC 2016) in his quasipolynomial isomorphism\ntest. In fact, we even use Babai's algorithm as a black box in one place.\n  We also give a second algorithm which, at the price of a slightly worse\nrunning time $2^{\\mathcal{O}(k^2 \\log k)}\\operatorname{poly} (n)$, avoids the\nuse of Babai's algorithm and, more importantly, has the additional benefit that\nit can also used as a canonization algorithm. \n\n"}
{"id": "1803.06878", "contents": "Title: Parameterized Complexity of Fair Vertex Evaluation Problems Abstract: A prototypical graph problem is centered around a graph-theoretic property\nfor a set of vertices and a solution to it is a set of vertices for which the\ndesired property holds. The task is to decide whether, in the given graph,\nthere exists a solution of a certain quality, where we use size as a quality\nmeasure. In this work, we are changing the measure to the fair measure\n[Lin&Sahni: Fair edge deletion problems. IEEE Trans. Comput. 89]. The measure\nis k if the number of solution neighbors does not exceed k for any vertex in\nthe graph. One possible way to study graph problems is by defining the property\nin a certain logic. For a given objective an evaluation problem is to find a\nset (of vertices) that simultaneously minimizes the assumed measure and\nsatisfies an appropriate formula.\n  In the presented paper we show that there is an FPT algorithm for the MSO\nFair Vertex Evaluation problem for formulas with one free variable\nparameterized by the twin cover number of the input graph. Here, the free\nvariable corresponds to the solution sought. One may define an extended variant\nof MSO Fair Vertex Evaluation for formulas with l free variables; here we\nmeasure a maximum number of neighbors in each of the l sets. However, such\nvariant is W[1]-hard for parameter l even on graphs with twin cover one.\nFurthermore, we study the Fair Vertex Cover (Fair VC) problem. Fair VC is among\nthe simplest problems with respect to the demanded property (i.e., the rest\nforms an edgeless graph). On the negative side, Fair VC is W[1]-hard when\nparameterized by both treedepth and feedback vertex set of the input graph. On\nthe positive side, we provide an FPT algorithm for the parameter modular width. \n\n"}
{"id": "1803.09289", "contents": "Title: Minmax Centered k-Partitioning of Trees and Applications to Sink\n  Evacuation with Dynamic Confluent Flows Abstract: Let $T=(V,E)$ be a tree with associated costs on its subtrees. A minmax\n$k$-partition of $T$ is a partition into $k$ subtrees, minimizing the maximum\ncost of a subtree over all possible partitions. In the centered version of the\nproblem, the cost of a subtree cost is defined as the minimum cost of\n\"servicing\" that subtree using a center located within it.\n  The problem motivating this work was the sink-evacuation problem on trees,\ni.e., finding a collection of $k$-sinks that minimize the time required by a\nconfluent dynamic network flow to evacuate all supplies to sinks.\n  This paper provides the first polynomial-time algorithm for solving this\nproblem, running in $O\\Bigl(\\max(k,\\log n) k n \\log^4 n\\Bigr)$ time. The\ntechnique developed can be used to solve any Minmax Centered $k$-Partitioning\nproblem on trees in which the servicing costs satisfy some very general\nconditions. Solutions can be found for both the discrete case, in which centers\nmust be on vertices, and the continuous case, in which centers may also be\nplaced on edges. The technique developed also improves previous results for\nfinding a minmax cost $k$-partition of a tree given the location of the sinks\nin advance. \n\n"}
{"id": "1803.09370", "contents": "Title: Popular Matching in Roommates Setting is NP-hard Abstract: An input to the Popular Matching problem, in the roommates setting, consists\nof a graph $G$ and each vertex ranks its neighbors in strict order, known as\nits preference. In the Popular Matching problem the objective is to test\nwhether there exists a matching $M^\\star$ such that there is no matching $M$\nwhere more people are happier with $M$ than with $M^\\star$. In this paper we\nsettle the computational complexity of the Popular Matching problem in the\nroommates setting by showing that the problem is NP-complete. Thus, we resolve\nan open question that has been repeatedly, explicitly asked over the last\ndecade. \n\n"}
{"id": "1803.09675", "contents": "Title: Extra Space during Initialization of Succinct Data Structures and\n  Dynamical Initializable Arrays Abstract: Many succinct data structures on the word RAM require precomputed tables to\nstart operating. Usually, the tables can be constructed in sublinear time. In\nthis time, most of a data structure is not initialized, i.e., there is plenty\nof unused space allocated for the data structure. We present a general\nframework to store temporarily extra buffers between the real data so that the\ndata can be processed immediately, stored first in the buffers, and then moved\ninto the real data structure after finishing the tables. As an application, we\napply our framework to Dodis, Patrascu, and Thorup's data structure (STOC 2010)\nthat emulates c-ary memory and to Farzan and Munro's succinct encoding of\narbitrary graphs (TCS 2013). We also use our framework to present an in-place\ndynamical initializable array. \n\n"}
{"id": "1803.10366", "contents": "Title: Smoothed Online Convex Optimization in High Dimensions via Online\n  Balanced Descent Abstract: We study Smoothed Online Convex Optimization, a version of online convex\noptimization where the learner incurs a penalty for changing her actions\nbetween rounds. Given a $\\Omega(\\sqrt{d})$ lower bound on the competitive ratio\nof any online algorithm, where $d$ is the dimension of the action space, we ask\nunder what conditions this bound can be beaten. We introduce a novel\nalgorithmic framework for this problem, Online Balanced Descent (OBD), which\nworks by iteratively projecting the previous point onto a carefully chosen\nlevel set of the current cost function so as to balance the switching costs and\nhitting costs. We demonstrate the generality of the OBD framework by showing\nhow, with different choices of \"balance,\" OBD can improve upon state-of-the-art\nperformance guarantees for both competitive ratio and regret, in particular,\nOBD is the first algorithm to achieve a dimension-free competitive ratio, $3 +\nO(1/\\alpha)$, for locally polyhedral costs, where $\\alpha$ measures the\n\"steepness\" of the costs. We also prove bounds on the dynamic regret of OBD\nwhen the balance is performed in the dual space that are dimension-free and\nimply that OBD has sublinear static regret. \n\n"}
{"id": "1803.11132", "contents": "Title: Notes on computational-to-statistical gaps: predictions using\n  statistical physics Abstract: In these notes we describe heuristics to predict computational-to-statistical\ngaps in certain statistical problems. These are regimes in which the underlying\nstatistical problem is information-theoretically possible although no efficient\nalgorithm exists, rendering the problem essentially unsolvable for large\ninstances. The methods we describe here are based on mature, albeit\nnon-rigorous, tools from statistical physics.\n  These notes are based on a lecture series given by the authors at the Courant\nInstitute of Mathematical Sciences in New York City, on May 16th, 2017. \n\n"}
{"id": "1804.00141", "contents": "Title: The Popular Roommates problem Abstract: We consider the popular matching problem in a roommates instance with strict\npreference lists. While popular matchings always exist in a bipartite instance,\nthey need not exist in a roommates instance. The complexity of the popular\nmatching problem in a roommates instance has been an open problem for several\nyears and here we show it is NP-hard. A sub-class of max-size popular matchings\ncalled dominant matchings has been well-studied in bipartite graphs. We show\nthat the dominant matching problem in a roommates instance is also NP-hard and\nthis is the case even when the instance admits a stable matching. \n\n"}
{"id": "1804.01366", "contents": "Title: Losing Treewidth by Separating Subsets Abstract: We study the problem of deleting the smallest set $S$ of vertices (resp.\nedges) from a given graph $G$ such that the induced subgraph (resp. subgraph)\n$G \\setminus S$ belongs to some class $\\mathcal{H}$. We consider the case where\ngraphs in $\\mathcal{H}$ have treewidth bounded by $t$, and give a general\nframework to obtain approximation algorithms for both vertex and edge-deletion\nsettings from approximation algorithms for certain natural graph partitioning\nproblems called $k$-Subset Vertex Separator and $k$-Subset Edge Separator,\nrespectively.\n  For the vertex deletion setting, our framework combined with the current best\nresult for $k$-Subset Vertex Separator, yields a significant improvement in the\napproximation ratios for basic problems such as $k$-Treewidth Vertex Deletion\nand Planar-$F$ Vertex Deletion. Our algorithms are simpler than previous works\nand give the first uniform approximation algorithms under the natural\nparameterization.\n  For the edge deletion setting, we give improved approximation algorithms for\n$k$-Subset Edge Separator combining ideas from LP relaxations and important\nseparators. We present their applications in bounded-degree graphs, and also\ngive an APX-hardness result for the edge deletion problems. \n\n"}
{"id": "1804.02273", "contents": "Title: BFS Enumeration for Breaking Symmetries in Graphs Abstract: There are numerous NP-hard combinatorial problems which involve searching for\nan undirected graph satisfying a certain property. One way to solve such\nproblems is to translate a problem into an instance of the boolean\nsatisfiability (SAT) or constraint satisfaction (CSP) problem. Such reduction\nusually can give rise to numerous isomorphic representations of the same graph.\nOne way to reduce the search space and speed up the search under these\nconditions is to introduce symmetrybreaking predicates. In this paper we\nintroduce three novel and practically effective symmetry-breaking predicates\nfor an undirected connected graph search based on breadth-first search (BFS)\nenumeration and compare with existing symmetry-breaking methods on several\ngraph problems. \n\n"}
{"id": "1804.02394", "contents": "Title: An Accelerated Directional Derivative Method for Smooth Stochastic\n  Convex Optimization Abstract: We consider smooth stochastic convex optimization problems in the context of\nalgorithms which are based on directional derivatives of the objective\nfunction. This context can be considered as an intermediate one between\nderivative-free optimization and gradient-based optimization. We assume that at\nany given point and for any given direction, a stochastic approximation for the\ndirectional derivative of the objective function at this point and in this\ndirection is available with some additive noise. The noise is assumed to be of\nan unknown nature, but bounded in the absolute value. We underline that we\nconsider directional derivatives in any direction, as opposed to coordinate\ndescent methods which use only derivatives in coordinate directions. For this\nsetting, we propose a non-accelerated and an accelerated directional derivative\nmethod and provide their complexity bounds. Our non-accelerated algorithm has a\ncomplexity bound which is similar to the gradient-based algorithm, that is,\nwithout any dimension-dependent factor. Our accelerated algorithm has a\ncomplexity bound which coincides with the complexity bound of the accelerated\ngradient-based algorithm up to a factor of square root of the problem\ndimension. We extend these results to strongly convex problems. \n\n"}
{"id": "1804.02949", "contents": "Title: Personalized PageRank dimensionality and algorithmic implications Abstract: Many systems, including the Internet, social networks, and the power grid,\ncan be represented as graphs. When analyzing graphs, it is often useful to\ncompute scores describing the relative importance or distance between nodes.\nOne example is Personalized PageRank (PPR), which assigns to each node $v$ a\nvector whose $i$-th entry describes the importance of the $i$-th node from the\nperspective of $v$. PPR has proven useful in many applications, such as\nrecommending who users should follow on social networks (if this $i$-th entry\nis large, $v$ may be interested in following the $i$-th user). Unfortunately,\ncomputing $n$ such PPR vectors (where $n$ is the number of nodes) is infeasible\nfor many graphs of interest.\n  In this work, we argue that the situation is not so dire. Our main result\nshows that the dimensionality of the set of PPR vectors scales sublinearly in\n$n$ with high probability, for a certain class of random graphs and for a\nnotion of dimensionality similar to rank. Put differently, we argue that the\neffective dimension of this set is much less than $n$, despite the fact that\nthe matrix containing these vectors has rank $n$. Furthermore, we show this\ndimensionality measure relates closely to the complexity of a PPR estimation\nscheme that was proposed (but not analyzed) by Jeh and Widom. This allows us to\nargue that accurately estimating all $n$ PPR vectors amounts to computing a\nvanishing fraction of the $n^2$ vector elements (when the technical assumptions\nof our main result are satisfied). Finally, we demonstrate empirically that\nsimilar conclusions hold when considering real-world networks, despite the\nassumptions of our theory not holding. \n\n"}
{"id": "1804.04005", "contents": "Title: Non-Malleable Extractors and Non-Malleable Codes: Partially Optimal\n  Constructions Abstract: The recent line of study on randomness extractors has been a great success,\nresulting in exciting new techniques, new connections, and breakthroughs to\nlong standing open problems in several seemingly different topics. These\ninclude seeded non-malleable extractors, privacy amplification protocols with\nan active adversary, independent source extractors (and explicit Ramsey\ngraphs), and non-malleable codes in the split state model. However, in all\ncases there is still a gap to optimum and the motivation to close this gap\nremains strong.\n  In this paper, we introduce a set of new techniques to further push the\nfrontier in the above questions. Our techniques lead to improvements in all of\nthe above questions, and in several cases partially optimal constructions.\nSpecifically, we obtain: 1. A seeded non-malleable extractor with seed length\n$O(log n)+log^{1+o(1)}(1/\\epsilon) and entropy requirement O(log log\nn+log(1/\\epsilon)), where the entropy requirement is asymptotically optimal by\na recent result of Gur and Shinkar \\cite{GurS17}; 2. A two-round privacy\namplification protocol with optimal entropy loss for security parameter up to\n\\Omega(k), which solves the privacy amplification problem completely; 3. A\ntwo-source extractor for entropy O(\\frac{log n log log n}{log log log n}),\nwhich also gives an explicit Ramsey graph on N vertices with no clique or\nindependent set of size (log N)^{O(\\frac{log log log N}{log log log log N})};\nand 4. The first explicit non-malleable code in the 2-split state model with\n\\emph{constant} rate, which has been a major goal in the study of non-malleable\ncodes for quite some time. One small caveat is that the error of this code is\nonly (an arbitrarily small) constant, but we can also achieve negligible error\nwith rate \\Omega(log log log n/log log n), which already improves the rate in\n\\cite{Li17} exponentially. \n\n"}
{"id": "1804.05345", "contents": "Title: Data-Dependent Coresets for Compressing Neural Networks with\n  Applications to Generalization Bounds Abstract: We present an efficient coresets-based neural network compression algorithm\nthat sparsifies the parameters of a trained fully-connected neural network in a\nmanner that provably approximates the network's output. Our approach is based\non an importance sampling scheme that judiciously defines a sampling\ndistribution over the neural network parameters, and as a result, retains\nparameters of high importance while discarding redundant ones. We leverage a\nnovel, empirical notion of sensitivity and extend traditional coreset\nconstructions to the application of compressing parameters. Our theoretical\nanalysis establishes guarantees on the size and accuracy of the resulting\ncompressed network and gives rise to generalization bounds that may provide new\ninsights into the generalization properties of neural networks. We demonstrate\nthe practical effectiveness of our algorithm on a variety of neural network\nconfigurations and real-world data sets. \n\n"}
{"id": "1804.05436", "contents": "Title: Hidden Hamiltonian Cycle Recovery via Linear Programming Abstract: We introduce the problem of hidden Hamiltonian cycle recovery, where there is\nan unknown Hamiltonian cycle in an $n$-vertex complete graph that needs to be\ninferred from noisy edge measurements. The measurements are independent and\ndistributed according to $\\calP_n$ for edges in the cycle and $\\calQ_n$\notherwise. This formulation is motivated by a problem in genome assembly, where\nthe goal is to order a set of contigs (genome subsequences) according to their\npositions on the genome using long-range linking measurements between the\ncontigs. Computing the maximum likelihood estimate in this model reduces to a\nTraveling Salesman Problem (TSP). Despite the NP-hardness of TSP, we show that\na simple linear programming (LP) relaxation, namely the fractional $2$-factor\n(F2F) LP, recovers the hidden Hamiltonian cycle with high probability as $n \\to\n\\infty$ provided that $\\alpha_n - \\log n \\to \\infty$, where $\\alpha_n\n\\triangleq -2 \\log \\int \\sqrt{d P_n d Q_n}$ is the R\\'enyi divergence of order\n$\\frac{1}{2}$. This condition is information-theoretically optimal in the sense\nthat, under mild distributional assumptions, $\\alpha_n \\geq (1+o(1)) \\log n$ is\nnecessary for any algorithm to succeed regardless of the computational cost.\n  Departing from the usual proof techniques based on dual witness construction,\nthe analysis relies on the combinatorial characterization (in particular, the\nhalf-integrality) of the extreme points of the F2F polytope. Represented as\nbicolored multi-graphs, these extreme points are further decomposed into\nsimpler \"blossom-type\" structures for the large deviation analysis and counting\narguments. Evaluation of the algorithm on real data shows improvements over\nexisting approaches. \n\n"}
{"id": "1804.06040", "contents": "Title: Constructions of maximum few-distance sets in Euclidean spaces Abstract: A finite set of distinct vectors $\\mathcal{X}$ in the $d$-dimensional\nEuclidean space $\\mathbb{R}^d$ is called an $s$-distance set if the set of\nmutual distances between distinct elements of $\\mathcal{X}$ has cardinality\n$s$. In this paper we present a combined approach of isomorph-free exhaustive\ngeneration of graphs and Gr\\\"obner basis computation to classify the largest\n$3$-distance sets in $\\mathbb{R}^4$, the largest $4$-distance sets in\n$\\mathbb{R}^3$, and the largest $6$-distance sets in $\\mathbb{R}^2$. We also\nconstruct new examples of large $s$-distance sets for $d\\leq 8$ and $s\\leq 6$,\nand independently verify several earlier results from the literature. \n\n"}
{"id": "1804.06601", "contents": "Title: Independent Distributions on a Multi-Branching AND-OR Tree of Height 2 Abstract: We investigate an AND-OR tree T and a probability distribution d on the truth\nassignments to the leaves. Tarsi (1983) showed that if d is an independent and\nidentical distribution (IID) such that probability of a leaf having value 0 is\nneither 0 nor 1 then, under a certain assumptions, there exists an optimal\nalgorithm that is depth-first. We investigate the case where d is an\nindependent distribution (ID) and probability depends on each leaf. It is known\nthat in this general case, if height is greater than or equal to 3, Tarsi-type\nresult does not hold. It is also known that for a complete binary tree of\nheight 2, Tarsi-type result certainly holds. In this paper, we ask whether\nTarsi-type result holds for an AND-OR tree of height 2. Here, a child node of\nthe root is either an OR-gate or a leaf: The number of child nodes of an\ninternal node is arbitrary, and depends on an internal node. We give an\naffirmative answer. Our strategy of the proof is to reduce the problem to the\ncase of directional algorithms. We perform induction on the number of leaves,\nand modify Tarsi's method to suite height 2 trees. We discuss why our proof\ndoes not apply to height 3 trees. \n\n"}
{"id": "1804.07799", "contents": "Title: Enumeration in Incremental FPT-Time Abstract: In this paper, we study the relationship of parametrised enumeration\ncomplexity classes defined by Creignou et al. (MFCS 2013). Specifically, we\nintroduce two hierarchies (IncFPTa and CapIncFPTa) of enumeration complexity\nclasses for incremental fpt-time in terms of exponent slices and show how they\ninterleave. Furthermore, we define several parametrised function classes and,\nin particular, introduce the parametrised counterpart of the class of\nnondeterministic multivalued functions with values that are polynomially\nverifiable and guaranteed to exist, TFNP, known from Megiddo and Papadimitriou\n(TCS 1991). We show that TF(para-NP) collapsing to F(FPT) is equivalent to\nOutputFPT coinciding with IncFPT. This result is in turn connected to a\ncollapse in the classical function setting and eventually to the collapse of\nIncP and OutputP which proves the first direct connection of classical to\nparametrised enumeration. \n\n"}
{"id": "1804.07869", "contents": "Title: Designing Practical PTASes for Minimum Feedback Vertex Set in Planar\n  Graphs Abstract: We present two algorithms for the minimum feedback vertex set problem in\nplanar graphs: an $O(n \\log n)$ PTAS using a linear kernel and balanced\nseparator, and a heuristic algorithm using kernelization and local search. We\nimplemented these algorithms and compared their performance with Becker and\nGeiger's 2-approximation algorithm. We observe that while our PTAS is\ncompetitive with the 2-approximation algorithm on large planar graphs, its\nrunning time is much longer. And our heuristic algorithm can outperform the\n2-approximation algorithm on most large planar graphs and provide a trade-off\nbetween running time and solution quality, i.e. a \"PTAS behavior\". \n\n"}
{"id": "1804.08008", "contents": "Title: Sufficient conditions for the global rigidity of periodic graphs Abstract: Tanigawa (2016) showed that vertex-redundant rigidity of a graph implies its\nglobal rigidity in arbitrary dimension. We extend this result to periodic\ngraphs under fixed lattice representations. A periodic graph is\nvertex-redundantly rigid if the deletion of a single vertex orbit under the\nperiodicity results in a periodically rigid graph. Our proof is similar to the\none of Tanigawa, but there are some added difficulties. First, it is not known\nwhether periodic global rigidity is a generic property. This issue is resolved\nvia a slight modification of a recent result of Kaszanitzy, Schulze and\nTanigawa (2016). Secondly, while the rigidity of finite graphs in\n$\\mathbb{R}^d$ on at most $d$ vertices obviously implies their global rigidity,\nit is non-trivial to prove a similar result for periodic graphs. This is\naccomplished by extending a result of Bezdek and Connelly (2002) on the\nexistence of a continuous movement between two equivalent $d$-dimensional\nrealisations of a single graph in $\\mathbb{R}^{2d}$ to periodic frameworks.\n  As an application of our result, we give a necessary and sufficient condition\nfor the global rigidity of generic periodic body-bar frameworks in arbitrary\ndimension. This provides a periodic counterpart to a result of Connelly, Jordan\nand Whiteley (2013) regarding the global rigidity of generic finite body-bar\nframeworks. \n\n"}
{"id": "1804.08645", "contents": "Title: Individual Sensitivity Preprocessing for Data Privacy Abstract: The sensitivity metric in differential privacy, which is informally defined\nas the largest marginal change in output between neighboring databases, is of\nsubstantial significance in determining the accuracy of private data analyses.\nTechniques for improving accuracy when the average sensitivity is much smaller\nthan the worst-case sensitivity have been developed within the differential\nprivacy literature, including tools such as smooth sensitivity,\nSample-and-Aggregate, Propose-Test-Release, and Lipschitz extensions.\n  In this work, we provide a new and general Sensitivity-Preprocessing\nframework for reducing sensitivity, where efficient application gives\nstate-of-the-art accuracy for privately outputting the important statistical\nmetrics median and mean when no underlying assumptions are made about the\ndatabase. In particular, our framework compares favorably to smooth sensitivity\nfor privately outputting median, in terms of both running time and accuracy.\nFurthermore, because our framework is a preprocessing step, it can also be\ncomplementary to smooth sensitivity and any other private mechanism, where\napplying both can achieve further gains in accuracy.\n  We additionally introduce a new notion of individual sensitivity and show\nthat it is an important metric in the variant definition of personalized\ndifferential privacy. We show that our algorithm can extend to this context and\nserve as a useful tool for this variant definition and its applications in\nmarkets for privacy. \n\n"}
{"id": "1804.08978", "contents": "Title: Tighter Connections Between Formula-SAT and Shaving Logs Abstract: A noticeable fraction of Algorithms papers in the last few decades improve\nthe running time of well-known algorithms for fundamental problems by\nlogarithmic factors. For example, the $O(n^2)$ dynamic programming solution to\nthe Longest Common Subsequence problem (LCS) was improved to $O(n^2/\\log^2 n)$\nin several ways and using a variety of ingenious tricks. This line of research,\nalso known as \"the art of shaving log factors\", lacks a tool for proving\nnegative results. Specifically, how can we show that it is unlikely that LCS\ncan be solved in time $O(n^2/\\log^3 n)$?\n  Perhaps the only approach for such results was suggested in a recent paper of\nAbboud, Hansen, Vassilevska W. and Williams (STOC'16). The authors blame the\nhardness of shaving logs on the hardness of solving satisfiability on Boolean\nformulas (Formula-SAT) faster than exhaustive search. They show that an\n$O(n^2/\\log^{1000} n)$ algorithm for LCS would imply a major advance in circuit\nlower bounds. Whether this approach can lead to tighter barriers was unclear.\n  In this paper, we push this approach to its limit and, in particular, prove\nthat a well-known barrier from complexity theory stands in the way for shaving\nfive additional log factors for fundamental combinatorial problems. For LCS,\nregular expression pattern matching, as well as the Fr\\'echet distance problem\nfrom Computational Geometry, we show that an $O(n^2/\\log^{7+\\varepsilon} n)$\nruntime would imply new Formula-SAT algorithms.\n  Our main result is a reduction from SAT on formulas of size $s$ over $n$\nvariables to LCS on sequences of length $N=2^{n/2} \\cdot s^{1+o(1)}$. Our\nreduction is essentially as efficient as possible, and it greatly improves the\npreviously known reduction for LCS with $N=2^{n/2} \\cdot s^c$, for some $c \\geq\n100$. \n\n"}
{"id": "1804.10173", "contents": "Title: Efficient and adaptive parameterized algorithms on modular\n  decompositions Abstract: We study the influence of a graph parameter called modular-width on the time\ncomplexity for optimally solving well-known polynomial problems such as Maximum\nMatching, Triangle Counting, and Maximum $s$-$t$ Vertex-Capacitated Flow. The\nmodular-width of a graph depends on its (unique) modular decomposition tree,\nand can be computed in linear time $O(n+m)$ for graphs with $n$ vertices and\n$m$ edges. Modular decompositions are an important tool for graph algorithms,\ne.g., for linear-time recognition of certain graph classes. Throughout, we\nobtain efficient parameterized algorithms of running times $O(f(mw)n+m)$,\n$O(n+f(mw)m)$ , or $O(f(mw)+n+m)$ for graphs of modular-width $mw$. Our\nalgorithm for Maximum Matching, running in time $O(mw^2\\log mw \\cdot n+m)$, is\nboth faster and simpler than the recent $O(mw^4n+m)$ time algorithm of Coudert\net al. (SODA 2018). For several other problems, e.g., Triangle Counting and\nMaximum $b$-Matching, we give adaptive algorithms, meaning that their running\ntimes match the best unparameterized algorithms for worst-case modular-width of\n$mw=\\Theta(n)$ and they outperform them already for $mw=o(n)$, until reaching\nlinear time for $mw=O(1)$. \n\n"}
{"id": "1804.10827", "contents": "Title: On Euclidean $k$-Means Clustering with $\\alpha$-Center Proximity Abstract: $k$-means clustering is NP-hard in the worst case but previous work has shown\nefficient algorithms assuming the optimal $k$-means clusters are \\emph{stable}\nunder additive or multiplicative perturbation of data. This has two caveats.\nFirst, we do not know how to efficiently verify this property of optimal\nsolutions that are NP-hard to compute in the first place. Second, the stability\nassumptions required for polynomial time $k$-means algorithms are often\nunreasonable when compared to the ground-truth clusters in real-world data. A\nconsequence of multiplicative perturbation resilience is \\emph{center\nproximity}, that is, every point is closer to the center of its own cluster\nthan the center of any other cluster, by some multiplicative factor $\\alpha >\n1$.\n  We study the problem of minimizing the Euclidean $k$-means objective only\nover clusterings that satisfy $\\alpha$-center proximity. We give a simple\nalgorithm to find the optimal $\\alpha$-center-proximal $k$-means clustering in\nrunning time exponential in $k$ and $1/(\\alpha - 1)$ but linear in the number\nof points and the dimension. We define an analogous $\\alpha$-center proximity\ncondition for outliers, and give similar algorithmic guarantees for $k$-means\nwith outliers and $\\alpha$-center proximity. On the hardness side we show that\nfor any $\\alpha' > 1$, there exists an $\\alpha \\leq \\alpha'$, $(\\alpha >1)$,\nand an $\\varepsilon_0 > 0$ such that minimizing the $k$-means objective over\nclusterings that satisfy $\\alpha$-center proximity is NP-hard to approximate\nwithin a multiplicative $(1+\\varepsilon_0)$ factor. \n\n"}
{"id": "1805.00212", "contents": "Title: Nearly Optimal Distinct Elements and Heavy Hitters on Sliding Windows Abstract: We study the distinct elements and $\\ell_p$-heavy hitters problems in the\nsliding window model, where only the most recent $n$ elements in the data\nstream form the underlying set. We first introduce the composable histogram, a\nsimple twist on the exponential (Datar et al., SODA 2002) and smooth histograms\n(Braverman and Ostrovsky, FOCS 2007) that may be of independent interest. We\nthen show that the composable histogram along with a careful combination of\nexisting techniques to track either the identity or frequency of a few specific\nitems suffices to obtain algorithms for both distinct elements and\n$\\ell_p$-heavy hitters that are nearly optimal in both $n$ and $\\epsilon$.\n  Applying our new composable histogram framework, we provide an algorithm that\noutputs a $(1+\\epsilon)$-approximation to the number of distinct elements in\nthe sliding window model and uses $\\mathcal{O}\\left(\\frac{1}{\\epsilon^2}\\log\nn\\log\\frac{1}{\\epsilon}\\log\\log n+\\frac{1}{\\epsilon}\\log^2 n\\right)$ bits of\nspace. For $\\ell_p$-heavy hitters, we provide an algorithm using space\n$\\mathcal{O}\\left(\\frac{1}{\\epsilon^p}\\log^3 n\\left(\\log\\log\nn+\\log\\frac{1}{\\epsilon}\\right)\\right)$ for $0<p\\le 2$, improving upon the\nbest-known algorithm for $\\ell_2$-heavy hitters (Braverman et al., COCOON\n2014), which has space complexity $\\mathcal{O}\\left(\\frac{1}{\\epsilon^4}\\log^3\nn\\right)$. We also show lower bounds of $\\Omega\\left(\\frac{1}{\\epsilon}\\log^2\nn+\\frac{1}{\\epsilon^2}\\log n\\right)$ for distinct elements and\n$\\Omega\\left(\\frac{1}{\\epsilon^p}\\log^2 n\\right)$ for $\\ell_p$-heavy hitters. \n\n"}
{"id": "1805.00578", "contents": "Title: Greedy Bipartite Matching in Random Type Poisson Arrival Model Abstract: We introduce a new random input model for bipartite matching which we call\nthe Random Type Poisson Arrival Model. Just like in the known i.i.d. model\n(introduced by Feldman et al. 2009), online nodes have types in our model. In\ncontrast to the adversarial types studied in the known i.i.d. model, following\nthe random graphs studied in Mastin and Jaillet 2016, in our model each type\ngraph is generated randomly by including each offline node in the neighborhood\nof an online node with probability $c/n$ independently. In our model, nodes of\nthe same type appear consecutively in the input and the number of times each\ntype node appears is distributed according to the Poisson distribution with\nparameter 1. We analyze the performance of the simple greedy algorithm under\nthis input model. The performance is controlled by the parameter $c$ and we are\nable to exactly characterize the competitive ratio for the regimes $c = o(1)$\nand $c = \\omega(1)$. We also provide a precise bound on the expected size of\nthe matching in the remaining regime of constant $c$. We compare our results to\nthe previous work of Mastin and Jaillet who analyzed the simple greedy\nalgorithm in the $G_{n,n,p}$ model where each online node type occurs exactly\nonce. We essentially show that the approach of Mastin and Jaillet can be\nextended to work for the Random Type Poisson Arrival Model, although several\nnontrivial technical challenges need to be overcome. Intuitively, one can view\nthe Random Type Poisson Arrival Model as the $G_{n,n,p}$ model with less\nrandomness; that is, instead of each online node having a new type, each online\nnode has a chance of repeating the previous type. \n\n"}
{"id": "1805.01074", "contents": "Title: Lower Bounds for Tolerant Junta and Unateness Testing via Rejection\n  Sampling of Graphs Abstract: We introduce a new model for testing graph properties which we call the\n\\emph{rejection sampling model}. We show that testing bipartiteness of\n$n$-nodes graphs using rejection sampling queries requires complexity\n$\\widetilde{\\Omega}(n^2)$. Via reductions from the rejection sampling model, we\ngive three new lower bounds for tolerant testing of Boolean functions of the\nform $f\\colon\\{0,1\\}^n\\to \\{0,1\\}$:\n  $\\bullet$Tolerant $k$-junta testing with \\emph{non-adaptive} queries requires\n$\\widetilde{\\Omega}(k^2)$ queries.\n  $\\bullet$Tolerant unateness testing requires $\\widetilde{\\Omega}(n)$ queries.\n  $\\bullet$Tolerant unateness testing with \\emph{non-adaptive} queries requires\n$\\widetilde{\\Omega}(n^{3/2})$ queries.\n  Given the $\\widetilde{O}(k^{3/2})$-query non-adaptive junta tester of Blais\n\\cite{B08}, we conclude that non-adaptive tolerant junta testing requires more\nqueries than non-tolerant junta testing. In addition, given the\n$\\widetilde{O}(n^{3/4})$-query unateness tester of Chen, Waingarten, and Xie\n\\cite{CWX17b} and the $\\widetilde{O}(n)$-query non-adaptive unateness tester of\nBaleshzar, Chakrabarty, Pallavoor, Raskhodnikova, and Seshadhri \\cite{BCPRS17},\nwe conclude that tolerant unateness testing requires more queries than\nnon-tolerant unateness testing, in both adaptive and non-adaptive settings.\nThese lower bounds provide the first separation between tolerant and\nnon-tolerant testing for a natural property of Boolean functions. \n\n"}
{"id": "1805.02026", "contents": "Title: Beyond the Lovasz Local Lemma: Point to Set Correlations and Their\n  Algorithmic Applications Abstract: Following the groundbreaking algorithm of Moser and Tardos for the Lovasz\nLocal Lemma (LLL), there has been a plethora of results analyzing local search\nalgorithms for various constraint satisfaction problems. The algorithms\nconsidered fall into two broad categories: resampling algorithms, analyzed via\ndifferent algorithmic LLL conditions; and backtracking algorithms, analyzed via\nentropy compression arguments. This paper introduces a new convergence\ncondition that seamlessly handles resampling, backtracking, and hybrid\nalgorithms, i.e., algorithms that perform both resampling and backtracking\nsteps. Unlike all past LLL work, our condition replaces the notion of a\ndependency or causality graph by quantifying point-to-set correlations between\nbad events. As a result, our condition simultaneously: (i)~captures the most\ngeneral algorithmic LLL condition known as a special case; (ii)~significantly\nsimplifies the analysis of entropy compression applications; (iii)~relates\nbacktracking algorithms, which are conceptually very different from resampling\nalgorithms, to the LLL; and most importantly (iv)~allows for the analysis of\nhybrid algorithms, which were outside the scope of previous techniques. We give\nseveral applications of our condition, including a new hybrid vertex coloring\nalgorithm that extends the recent breakthrough result of Molloy for coloring\ntriangle-free graphs to arbitrary graphs. \n\n"}
{"id": "1805.02351", "contents": "Title: Fine-grained Complexity Meets IP = PSPACE Abstract: In this paper we study the fine-grained complexity of finding exact and\napproximate solutions to problems in P. Our main contribution is showing\nreductions from exact to approximate solution for a host of such problems.\n  As one (notable) example, we show that the Closest-LCS-Pair problem (Given\ntwo sets of strings $A$ and $B$, compute exactly the maximum $\\textsf{LCS}(a,\nb)$ with $(a, b) \\in A \\times B$) is equivalent to its approximation version\n(under near-linear time reductions, and with a constant approximation factor).\nMore generally, we identify a class of problems, which we call BP-Pair-Class,\ncomprising both exact and approximate solutions, and show that they are all\nequivalent under near-linear time reductions.\n  Exploring this class and its properties, we also show:\n  $\\bullet$ Under the NC-SETH assumption (a significantly more relaxed\nassumption than SETH), solving any of the problems in this class requires\nessentially quadratic time.\n  $\\bullet$ Modest improvements on the running time of known algorithms\n(shaving log factors) would imply that NEXP is not in non-uniform\n$\\textsf{NC}^1$.\n  $\\bullet$ Finally, we leverage our techniques to show new barriers for\ndeterministic approximation algorithms for LCS.\n  At the heart of these new results is a deep connection between interactive\nproof systems for bounded-space computations and the fine-grained complexity of\nexact and approximate solutions to problems in P. In particular, our results\nbuild on the proof techniques from the classical IP = PSPACE result. \n\n"}
{"id": "1805.03055", "contents": "Title: Parallel Graph Connectivity in Log Diameter Rounds Abstract: We study graph connectivity problem in MPC model. On an undirected graph with\n$n$ nodes and $m$ edges, $O(\\log n)$ round connectivity algorithms have been\nknown for over 35 years. However, no algorithms with better complexity bounds\nwere known. In this work, we give fully scalable, faster algorithms for the\nconnectivity problem, by parameterizing the time complexity as a function of\nthe diameter of the graph. Our main result is a $O(\\log D \\log\\log_{m/n} n)$\ntime connectivity algorithm for diameter-$D$ graphs, using $\\Theta(m)$ total\nmemory. If our algorithm can use more memory, it can terminate in fewer rounds,\nand there is no lower bound on the memory per processor.\n  We extend our results to related graph problems such as spanning forest,\nfinding a DFS sequence, exact/approximate minimum spanning forest, and\nbottleneck spanning forest. We also show that achieving similar bounds for\nreachability in directed graphs would imply faster boolean matrix\nmultiplication algorithms.\n  We introduce several new algorithmic ideas. We describe a general technique\ncalled double exponential speed problem size reduction which roughly means that\nif we can use total memory $N$ to reduce a problem from size $n$ to $n/k$, for\n$k=(N/n)^{\\Theta(1)}$ in one phase, then we can solve the problem in\n$O(\\log\\log_{N/n} n)$ phases. In order to achieve this fast reduction for graph\nconnectivity, we use a multistep algorithm. One key step is a carefully\nconstructed truncated broadcasting scheme where each node broadcasts neighbor\nsets to its neighbors in a way that limits the size of the resulting neighbor\nsets. Another key step is random leader contraction, where we choose a smaller\nset of leaders than many previous works do. \n\n"}
{"id": "1805.03192", "contents": "Title: The Computational Complexity of Finding Hamiltonian Cycles in Grid\n  Graphs of Semiregular Tessellations Abstract: Finding Hamitonian Cycles in square grid graphs is a well studied and\nimportant questions. More recent work has extended these results to triangular\nand hexagonal grids, as well as further restricted versions. In this paper, we\nexamine a class of more complex grids, as well as looking at the problem with\nrestricted types of paths. We investigate the hardness of Hamiltonian cycle\nproblem in grid graphs of semiregular tessellations. We give NP-hardness\nreductions for finding Hamiltonian paths in grid graphs based on all eight of\nthe semiregular tessilations. Next, we investigate variations on the problem of\nfinding Hamiltonian Paths in grid graphs when the path is forced to turn at\nevery vertex. We give a polynomial time algorithm for deciding if a square grid\ngraph admits a Hamiltonian cycle which turns at every vertex. We then show\ndeciding if cubic grid graphs, even if the height is restricted to $2$, admit a\nHamiltonian cycle is NP-complete. \n\n"}
{"id": "1805.05208", "contents": "Title: Theoretically Efficient Parallel Graph Algorithms Can Be Fast and\n  Scalable Abstract: There has been significant recent interest in parallel graph processing due\nto the need to quickly analyze the large graphs available today. Many graph\ncodes have been designed for distributed memory or external memory. However,\ntoday even the largest publicly-available real-world graph (the Hyperlink Web\ngraph with over 3.5 billion vertices and 128 billion edges) can fit in the\nmemory of a single commodity multicore server. Nevertheless, most experimental\nwork in the literature report results on much smaller graphs, and the ones for\nthe Hyperlink graph use distributed or external memory. Therefore, it is\nnatural to ask whether we can efficiently solve a broad class of graph problems\non this graph in memory.\n  This paper shows that theoretically-efficient parallel graph algorithms can\nscale to the largest publicly-available graphs using a single machine with a\nterabyte of RAM, processing them in minutes. We give implementations of\ntheoretically-efficient parallel algorithms for 20 important graph problems. We\nalso present the optimizations and techniques that we used in our\nimplementations, which were crucial in enabling us to process these large\ngraphs quickly. We show that the running times of our implementations\noutperform existing state-of-the-art implementations on the largest real-world\ngraphs. For many of the problems that we consider, this is the first time they\nhave been solved on graphs at this scale. We have made the implementations\ndeveloped in this work publicly-available as the Graph-Based Benchmark Suite\n(GBBS). \n\n"}
{"id": "1805.05305", "contents": "Title: Transforming graph states using single-qubit operations Abstract: Stabilizer states form an important class of states in quantum information,\nand are of central importance in quantum error correction. Here, we provide an\nalgorithm for deciding whether one stabilizer (target) state can be obtained\nfrom another stabilizer (source) state by single-qubit Clifford operations\n(LC), single-qubit Pauli measurements (LPM), and classical communication (CC)\nbetween sites holding the individual qubits. What's more, we provide a recipe\nto obtain the sequence of LC+LPM+CC operations which prepare the desired target\nstate from the source state, and show how these operations can be applied in\nparallel to reach the target state in constant time. Our algorithm has\napplications in quantum networks, quantum computing, and can also serve as a\ndesign tool - for example, to find transformations between quantum error\ncorrecting codes. We provide a software implementation of our algorithm that\nmakes this tool easier to apply.\n  A key insight leading to our algorithm is to show that the problem is\nequivalent to one in graph theory, which is to decide whether some graph G' is\na vertex-minor of another graph G. Here we show that the vertex-minor problem\ncan be solved in time O(|G|^3) where |G| is the size of the graph G, whenever\nthe rank-width of G and the size of G' are bounded. Our algorithm is based on\ntechniques by Courcelle for solving fixed parameter tractable problems, where\nhere the relevant fixed parameter is the rank width. The second half of this\npaper serves as an accessible but far from exhausting introduction to these\nconcepts, that could be useful for many other problems in quantum information. \n\n"}
{"id": "1805.05306", "contents": "Title: How to transform graph states using single-qubit operations:\n  computational complexity and algorithms Abstract: Graph states are ubiquitous in quantum information with diverse applications\nranging from quantum network protocols to measurement based quantum computing.\nHere we consider the question whether one graph (source) state can be\ntransformed into another graph (target) state, using a specific set of quantum\noperations (LC+LPM+CC): single-qubit Clifford operations (LC), single-qubit\nPauli measurements (LPM) and classical communication (CC) between sites holding\nthe individual qubits. We first show that deciding whether a graph state |G>\ncan be transformed into another graph state |G'> using LC+LPM+CC is\nNP-Complete, even if |G'> is restricted to be the GHZ-state. However, we also\nprovide efficient algorithms for two situations of practical interest:\n  1. |G> has Schmidt-rank width one and |G'> is a GHZ-state. The Schmidt-rank\nwidth is an entanglement measure of quantum states, meaning this algorithm is\nefficient if the original state has little entanglement. Our algorithm has\nruntime O(|V(G')||V(G)|^3), and is also efficient in practice even on small\ninstances as further showcased by a freely available software implementation.\n  2. |G> is in a certain class of states with unbounded Schmidt-rank width, and\n|G'> is a GHZ-state of a constant size. Here the runtime is O(poly(|V(G)|)),\nshowing that more efficient algorithms can in principle be found even for\nstates holding a large amount of entanglement, as long as the output state has\nconstant size.\n  Our results make use of the insight that deciding whether a graph state |G>\ncan be transformed to another graph state |G'> is equivalent to a known\ndecision problem in graph theory, namely the problem of deciding whether a\ngraph G' is a vertex-minor of a graph G. Many of the technical tools developed\nto obtain our results may be of independent interest. \n\n"}
{"id": "1805.05404", "contents": "Title: Congested Clique Algorithms for Graph Spanners Abstract: Graph spanners are sparse subgraphs that faithfully preserve the distances in\nthe original graph up to small stretch. Spanner have been studied extensively\nas they have a wide range of applications ranging from distance oracles,\nlabeling schemes and routing to solving linear systems and spectral\nsparsification. A $k$-spanner maintains pairwise distances up to multiplicative\nfactor of $k$. It is a folklore that for every $n$-vertex graph $G$, one can\nconstruct a $(2k-1)$ spanner with $O(n^{1+1/k})$ edges. In a distributed\nsetting, such spanners can be constructed in the standard CONGEST model using\n$O(k^2)$ rounds, when randomization is allowed.\n  In this work, we consider spanner constructions in the congested clique\nmodel, and show: (1) A randomized construction of a $(2k-1)$-spanner with\n$\\widetilde{O}(n^{1+1/k})$ edges in $O(\\log k)$ rounds. The previous best\nalgorithm runs in $O(k)$ rounds. (2) A deterministic construction of a\n$(2k-1)$-spanner with $\\widetilde{O}(n^{1+1/k})$ edges in $O(\\log k +(\\log\\log\nn)^3)$ rounds. The previous best algorithm runs in $O(k\\log n)$ rounds. This\nimprovement is achieved by a new derandomization theorem for hitting sets which\nmight be of independent interest. (3) A deterministic construction of a\n$O(k)$-spanner with $O(k \\cdot n^{1+1/k})$ edges in $O(\\log k)$ rounds. \n\n"}
{"id": "1805.06151", "contents": "Title: Improved Worst-Case Deterministic Parallel Dynamic Minimum Spanning\n  Forest Abstract: This paper gives a new deterministic algorithm for the dynamic Minimum\nSpanning Forest (MSF) problem in the EREW PRAM model, where the goal is to\nmaintain a MSF of a weighted graph with $n$ vertices and $m$ edges while\nsupporting edge insertions and deletions. We show that one can solve the\ndynamic MSF problem using $O(\\sqrt n)$ processors and $O(\\log n)$ worst-case\nupdate time, for a total of $O(\\sqrt n \\log n)$ work. This improves on the work\nof Ferragina [IPPS 1995] which costs $O(\\log n)$ worst-case update time and\n$O(n^{2/3} \\log{\\frac{m}{n}})$ work. \n\n"}
{"id": "1805.08255", "contents": "Title: Algorithmic and algebraic aspects of unshuffling permutations Abstract: A permutation is said to be a square if it can be obtained by shuffling two\norder-isomorphic patterns. The definition is intended to be the natural\ncounterpart to the ordinary shuffle of words and languages. In this paper, we\ntackle the problem of recognizing square permutations from both the point of\nview of algebra and algorithms. On the one hand, we present some algebraic and\ncombinatorial properties of the shuffle product of permutations. We follow an\nunusual line consisting in defining the shuffle of permutations by means of an\nunshuffling operator, known as a coproduct. This strategy allows to obtain easy\nproofs for algebraic and combinatorial properties of our shuffle product. We\nbesides exhibit a bijection between square $(213,231)$-avoiding permutations\nand square binary words. On the other hand, by using a pattern avoidance\ncriterion on directed perfect matchings, we prove that recognizing square\npermutations is {\\bf NP}-complete. \n\n"}
{"id": "1805.12065", "contents": "Title: A four vertex theorem for frieze patterns? Abstract: Given two Coxeter's frieze patterns with the same width and consisting of\npositive numbers, choose a row and consider the periodic sequence of the\ndifferences of the respective entries of the two friezes. We ask for which rows\nthis sequence must change sign at least four times over the period. We prove\nthat this is the case for the first and for the second non-trivial rows, and\nthat this is true, for all rows, for an infinitesimal version of the question.\nThe article also contains expository material on the four vertex theorem and on\nCoxeter's frieze patterns. \n\n"}
{"id": "1805.12238", "contents": "Title: High-Quality Disjoint and Overlapping Community Structure in Large-Scale\n  Complex Networks Abstract: In this paper, we propose an improved version of an agglomerative\nhierarchical clustering algorithm that performs disjoint community detection in\nlarge-scale complex networks. The improved algorithm is achieved after\nreplacing the local structural similarity used in the original algorithm, with\nthe recently proposed Dynamic Structural Similarity. Additionally, the improved\nalgorithm is extended to detect fuzzy and crisp overlapping community\nstructure. The extended algorithm leverages the disjoint community structure\ngenerated by itself and the dynamic structural similarity measures, to compute\na proposed membership probability function that defines the fuzzy communities.\nMoreover, an experimental evaluation is performed on reference benchmark graphs\nin order to compare the proposed algorithms with the state-of-the-art. \n\n"}
{"id": "1805.12400", "contents": "Title: Tropical Geometry of Phylogenetic Tree Space: A Statistical Perspective Abstract: Phylogenetic trees are the fundamental mathematical representation of\nevolutionary processes in biology. They are also objects of interest in pure\nmathematics, such as algebraic geometry and combinatorics, due to their\ndiscrete geometry. Although they are important data structures, they face the\nsignificant challenge that sets of trees form a non-Euclidean phylogenetic tree\nspace, which means that standard computational and statistical methods cannot\nbe directly applied. In this work, we explore the statistical feasibility of a\npure mathematical representation of the set of all phylogenetic trees based on\ntropical geometry for both descriptive and inferential statistics, and\nunsupervised and supervised machine learning. Our exploration is both\ntheoretical and practical. We show that the tropical geometric phylogenetic\ntree space endowed with a generalized Hilbert projective metric exhibits\nanalytic, geometric, and topological properties that are desirable for\ntheoretical studies in probability and statistics and allow for well-defined\nquestions to be posed. We illustrate the statistical feasibility of the\ntropical geometric perspective for phylogenetic trees with an example of both a\ndescriptive and inferential statistical task. Moreover, this approach exhibits\nincreased computational efficiency and statistical performance over the current\nstate-of-the-art, which we illustrate with a real data example on seasonal\ninfluenza. Our results demonstrate the viability of the tropical geometric\nsetting for parametric statistical and probabilistic studies of sets of\nphylogenetic trees. \n\n"}
{"id": "1806.00040", "contents": "Title: Efficient Algorithms and Lower Bounds for Robust Linear Regression Abstract: We study the problem of high-dimensional linear regression in a robust model\nwhere an $\\epsilon$-fraction of the samples can be adversarially corrupted. We\nfocus on the fundamental setting where the covariates of the uncorrupted\nsamples are drawn from a Gaussian distribution $\\mathcal{N}(0, \\Sigma)$ on\n$\\mathbb{R}^d$. We give nearly tight upper bounds and computational lower\nbounds for this problem. Specifically, our main contributions are as follows:\n  For the case that the covariance matrix is known to be the identity, we give\na sample near-optimal and computationally efficient algorithm that outputs a\ncandidate hypothesis vector $\\widehat{\\beta}$ which approximates the unknown\nregression vector $\\beta$ within $\\ell_2$-norm $O(\\epsilon \\log(1/\\epsilon)\n\\sigma)$, where $\\sigma$ is the standard deviation of the random observation\nnoise. An error of $\\Omega (\\epsilon \\sigma)$ is information-theoretically\nnecessary, even with infinite sample size. Prior work gave an algorithm for\nthis problem with sample complexity $\\tilde{\\Omega}(d^2/\\epsilon^2)$ whose\nerror guarantee scales with the $\\ell_2$-norm of $\\beta$.\n  For the case of unknown covariance, we show that we can efficiently achieve\nthe same error guarantee as in the known covariance case using an additional\n$\\tilde{O}(d^2/\\epsilon^2)$ unlabeled examples. On the other hand, an error of\n$O(\\epsilon \\sigma)$ can be information-theoretically attained with\n$O(d/\\epsilon^2)$ samples. We prove a Statistical Query (SQ) lower bound\nproviding evidence that this quadratic tradeoff in the sample size is inherent.\nMore specifically, we show that any polynomial time SQ learning algorithm for\nrobust linear regression (in Huber's contamination model) with estimation\ncomplexity $O(d^{2-c})$, where $c>0$ is an arbitrarily small constant, must\nincur an error of $\\Omega(\\sqrt{\\epsilon} \\sigma)$. \n\n"}
{"id": "1806.00534", "contents": "Title: Provably convergent acceleration in factored gradient descent with\n  applications in matrix sensing Abstract: We present theoretical results on the convergence of \\emph{non-convex}\naccelerated gradient descent in matrix factorization models with $\\ell_2$-norm\nloss. The purpose of this work is to study the effects of acceleration in\nnon-convex settings, where provable convergence with acceleration should not be\nconsidered a \\emph{de facto} property. The technique is applied to matrix\nsensing problems, for the estimation of a rank $r$ optimal solution $X^\\star\n\\in \\mathbb{R}^{n \\times n}$. Our contributions can be summarized as follows.\n$i)$ We show that acceleration in factored gradient descent converges at a\nlinear rate; this fact is novel for non-convex matrix factorization settings,\nunder common assumptions. $ii)$ Our proof technique requires the acceleration\nparameter to be carefully selected, based on the properties of the problem,\nsuch as the condition number of $X^\\star$ and the condition number of objective\nfunction. $iii)$ Currently, our proof leads to the same dependence on the\ncondition number(s) in the contraction parameter, similar to recent results on\nnon-accelerated algorithms. $iv)$ Acceleration is observed in practice, both in\nsynthetic examples and in two real applications: neuronal multi-unit activities\nrecovery from single electrode recordings, and quantum state tomography on\nquantum computing simulators. \n\n"}
{"id": "1806.01305", "contents": "Title: Towards the Practical Application of Near-Term Quantum Computers in\n  Quantum Chemistry Simulations: A Problem Decomposition Approach Abstract: With the aim of establishing a framework to efficiently perform the practical\napplication of quantum chemistry simulation on near-term quantum devices, we\nenvision a hybrid quantum--classical framework for leveraging problem\ndecomposition (PD) techniques in quantum chemistry. Specifically, we use PD\ntechniques to decompose a target molecular system into smaller subsystems\nrequiring fewer computational resources. In our framework, there are two levels\nof hybridization. At the first level, we use a classical algorithm to decompose\na target molecule into subsystems, and utilize a quantum algorithm to simulate\nthe quantum nature of the subsystems. The second level is in the quantum\nalgorithm. We consider the quantum--classical variational algorithm that\niterates between an expectation estimation using a quantum device and a\nparameter optimization using a classical device. We investigate three popular\nPD techniques for our hybrid approach: the fragment molecular-orbital (FMO)\nmethod, the divide-and-conquer (DC) technique, and the density matrix embedding\ntheory (DMET). We examine the efficacy of these techniques in correctly\ndifferentiating conformations of simple alkane molecules. In particular, we\nconsider the ratio between the number of qubits for PD and that of the full\nsystem; the mean absolute deviation; and the Pearson correlation coefficient\nand Spearman's rank correlation coefficient. Sampling error is introduced when\nexpectation values are measured on the quantum device. Therefore, we study how\nthis error affects the predictive performance of PD techniques. The present\nstudy is our first step to opening up the possibility of using quantum\nchemistry simulations at a scale close to the size of molecules relevant to\nindustry on near-term quantum hardware. \n\n"}
{"id": "1806.02484", "contents": "Title: Splitting loops and necklaces: Variants of the square peg problem Abstract: Toeplitz conjectured that any simple planar loop inscribes a square. Here we\nprove variants of Toeplitz' square peg problem. We prove Hadwiger's 1971\nconjecture that any simple loop in $3$-space inscribes a parallelogram. We show\nthat any simple planar loop inscribes sufficiently many rectangles that their\nvertices are dense in the loop (independently due to Schwartz). If the loop is\nrectifiable, there is a rectangle that cuts the loop into four pieces that can\nbe rearranged to form two loops of equal length. A rectifiable loop in\n$d$-space can be cut into $(r-1)(d+1)+1$ pieces that can be rearranged by\ntranslations to form $r$ loops of equal length. We relate our results to fair\ndivisions of necklaces in the sense of Alon and to Tverberg-type results. This\nprovides a new approach and a common framework to obtain variants of Toeplitz'\nsquare peg problem for the class of all continuous curves. \n\n"}
{"id": "1806.02771", "contents": "Title: Structural Rounding: Approximation Algorithms for Graphs Near an\n  Algorithmically Tractable Class Abstract: We develop a new framework for generalizing approximation algorithms from the\nstructural graph algorithm literature so that they apply to graphs somewhat\nclose to that class (a scenario we expect is common when working with\nreal-world networks) while still guaranteeing approximation ratios. The idea is\nto $\\textit{edit}$ a given graph via vertex- or edge-deletions to put the graph\ninto an algorithmically tractable class, apply known approximation algorithms\nfor that class, and then $\\textit{lift}$ the solution to apply to the original\ngraph. We give a general characterization of when an optimization problem is\namenable to this approach, and show that it includes many well-studied graph\nproblems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum\nMaximal Matching, Chromatic Number, ($\\ell$-)Dominating Set, Edge\n($\\ell$-)Dominating Set, and Connected Dominating Set.\n  To enable this framework, we develop new editing algorithms that find the\napproximately-fewest edits required to bring a given graph into one of several\nimportant graph classes (in some cases, also approximating the target parameter\nof the family). For bounded degeneracy, we obtain a bicriteria\n$(4,4)$-approximation which also extends to a smoother bicriteria trade-off.\nFor bounded treewidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log\nw}))$-approximation, and for bounded pathwidth, we obtain a bicriteria\n$(O(\\log^{1.5} n), O(\\sqrt{\\log w} \\cdot \\log n))$-approximation. For treedepth\n$2$ (also related to bounded expansion), we obtain a $4$-approximation. We also\nprove complementary hardness-of-approximation results assuming $\\mathrm{P} \\neq\n\\mathrm{NP}$: in particular, these problems are all log-factor inapproximable,\nexcept the last which is not approximable below some constant factor ($2$\nassuming UGC). \n\n"}
{"id": "1806.02969", "contents": "Title: List-decoding homomorphism codes with arbitrary codomains Abstract: The codewords of the homomorphism code $\\operatorname{aHom}(G,H)$ are the\naffine homomorphisms between two finite groups, $G$ and $H$, generalizing\nHadamard codes. Following the work of Goldreich--Levin (1989), Grigorescu et\nal. (2006), Dinur et al. (2008), and Guo and Sudan (2014), we further expand\nthe range of groups for which local list-decoding is possible up to\n$\\textsf{mindist}$, the minimum distance of the code. In particular, for the\nfirst time, we do not require either $G$ or $H$ to be solvable. Specifically,\nwe demonstrate a $\\operatorname{poly}(1/\\varepsilon)$ bound on the list size,\ni.e., on the number of codewords within distance\n$(\\textsf{mindist}-\\varepsilon)$ from any received word, when $G$ is either\nabelian or an alternating group, and $H$ is an arbitrary (finite or infinite)\ngroup. We conjecture that a similar bound holds for all finite simple groups as\ndomains; the alternating groups serve as the first test case.\n  The abelian vs. arbitrary result then permits us to adapt previous techniques\nto obtain efficient local list-decoding for this case. We also obtain efficient\nlocal list-decoding for the permutation representations of alternating groups\n(i.e., when the codomain is a symmetric group $S_m$) under the restriction that\nthe domain $G=A_n$ is paired with codomain $H=S_m$ satisfying $m <\n2^{n-1}/\\sqrt{n}$.\n  The limitations on the codomain in the latter case arise from severe\ntechnical difficulties stemming from the need to solve the homomorphism\nextension (HomExt) problem in certain cases; these are addressed in a separate\npaper (Wuu 2018).\n  However, we also introduce an intermediate \"semi-algorithmic\" model we call\nCertificate List-Decoding that bypasses the HomExt bottleneck and works in the\nalternating vs. arbitrary setting. A certificate list-decoder produces partial\nhomomorphisms that uniquely extend to the homomorphisms in the list. \n\n"}
{"id": "1806.04310", "contents": "Title: MISSION: Ultra Large-Scale Feature Selection using Count-Sketches Abstract: Feature selection is an important challenge in machine learning. It plays a\ncrucial role in the explainability of machine-driven decisions that are rapidly\npermeating throughout modern society. Unfortunately, the explosion in the size\nand dimensionality of real-world datasets poses a severe challenge to standard\nfeature selection algorithms. Today, it is not uncommon for datasets to have\nbillions of dimensions. At such scale, even storing the feature vector is\nimpossible, causing most existing feature selection methods to fail.\nWorkarounds like feature hashing, a standard approach to large-scale machine\nlearning, helps with the computational feasibility, but at the cost of losing\nthe interpretability of features. In this paper, we present MISSION, a novel\nframework for ultra large-scale feature selection that performs stochastic\ngradient descent while maintaining an efficient representation of the features\nin memory using a Count-Sketch data structure. MISSION retains the simplicity\nof feature hashing without sacrificing the interpretability of the features\nwhile using only O(log^2(p)) working memory. We demonstrate that MISSION\naccurately and efficiently performs feature selection on real-world,\nlarge-scale datasets with billions of dimensions. \n\n"}
{"id": "1806.06173", "contents": "Title: On the Complexity of Detecting Convexity over a Box Abstract: It has recently been shown that the problem of testing global convexity of\npolynomials of degree four is {strongly} NP-hard, answering an open question of\nN.Z. Shor. This result is minimal in the degree of the polynomial when global\nconvexity is of concern. In a number of applications however, one is interested\nin testing convexity only over a compact region, most commonly a box (i.e.,\nhyper-rectangle). In this paper, we show that this problem is also strongly\nNP-hard, in fact for polynomials of degree as low as three. This result is\nminimal in the degree of the polynomial and in some sense justifies why\nconvexity detection in nonlinear optimization solvers is limited to quadratic\nfunctions or functions with special structure. As a byproduct, our proof shows\nthat the problem of testing whether all matrices in an interval family are\npositive semidefinite is strongly NP-hard. This problem, which was previously\nshown to be (weakly) NP-hard by Nemirovski, is of independent interest in the\ntheory of robust control. \n\n"}
{"id": "1806.06996", "contents": "Title: Optimization over Nonnegative and Convex Polynomials With and Without\n  Semidefinite Programming Abstract: The problem of optimizing over the cone of nonnegative polynomials is a\nfundamental problem in computational mathematics, with applications to\npolynomial optimization, control, machine learning, game theory, and\ncombinatorics, among others. A number of breakthrough papers in the early 2000s\nshowed that this problem, long thought to be out of reach, could be tackled by\nusing sum of squares programming. This technique however has proved to be\nexpensive for large-scale problems, as it involves solving large semidefinite\nprograms (SDPs).\n  In the first part of this thesis, we present two methods for approximately\nsolving large-scale sum of squares programs that dispense altogether with\nsemidefinite programming and only involve solving a sequence of linear or\nsecond order cone programs generated in an adaptive fashion. We then focus on\nthe problem of finding tight lower bounds on polynomial optimization problems\n(POPs), a fundamental task in this area that is most commonly handled through\nthe use of SDP-based sum of squares hierarchies (e.g., due to Lasserre and\nParrilo). In contrast to previous approaches, we provide the first theoretical\nframework for constructing converging hierarchies of lower bounds on POPs whose\ncomputation simply requires the ability to multiply certain fixed polynomials\ntogether and to check nonnegativity of the coefficients of their product.\n  In the second part of this thesis, we focus on the theory and applications of\nthe problem of optimizing over convex polynomials, a subcase of the problem of\noptimizing over nonnegative polynomials. (See manuscript for the rest of the\nabstract.) \n\n"}
{"id": "1806.07861", "contents": "Title: The two-distance sets in dimension four Abstract: A finite set of distinct vectors $\\mathcal{X}$ in the $d$-dimensional\nEuclidean space $\\mathbb{R}^d$ is called a $2$-distance set, if the set of\nmutual distances between distinct elements of $\\mathcal{X}$ has cardinality\nexactly $2$. In this note we classify the $2$-distance sets in $\\mathbb{R}^4$\nup to isometry with computer-aided methods. \n\n"}
{"id": "1806.10952", "contents": "Title: Amortized Analysis of Asynchronous Price Dynamics Abstract: We extend a recently developed framework for analyzing asynchronous\ncoordinate descent algorithms to show that an asynchronous version of\ntatonnement, a fundamental price dynamic widely studied in general equilibrium\ntheory, converges toward a market equilibrium for Fisher markets with CES\nutilities or Leontief utilities, for which tatonnement is equivalent to\ncoordinate descent. \n\n"}
{"id": "1806.11282", "contents": "Title: Approximation Algorithms for Complex-Valued Ising Models on Bounded\n  Degree Graphs Abstract: We study the problem of approximating the Ising model partition function with\ncomplex parameters on bounded degree graphs. We establish a deterministic\npolynomial-time approximation scheme for the partition function when the\ninteractions and external fields are absolutely bounded close to zero.\nFurthermore, we prove that for this class of Ising models the partition\nfunction does not vanish. Our algorithm is based on an approach due to Barvinok\nfor approximating evaluations of a polynomial based on the location of the\ncomplex zeros and a technique due to Patel and Regts for efficiently computing\nthe leading coefficients of graph polynomials on bounded degree graphs.\nFinally, we show how our algorithm can be extended to approximate certain\noutput probability amplitudes of quantum circuits. \n\n"}
{"id": "1807.00878", "contents": "Title: Distributed Statistical Estimation of Matrix Products with Applications Abstract: We consider statistical estimations of a matrix product over the integers in\na distributed setting, where we have two parties Alice and Bob; Alice holds a\nmatrix $A$ and Bob holds a matrix $B$, and they want to estimate statistics of\n$A \\cdot B$. We focus on the well-studied $\\ell_p$-norm, distinct elements ($p\n= 0$), $\\ell_0$-sampling, and heavy hitter problems. The goal is to minimize\nboth the communication cost and the number of rounds of communication.\n  This problem is closely related to the fundamental set-intersection join\nproblem in databases: when $p = 0$ the problem corresponds to the size of the\nset-intersection join. When $p = \\infty$ the output is simply the pair of sets\nwith the maximum intersection size. When $p = 1$ the problem corresponds to the\nsize of the corresponding natural join. We also consider the heavy hitters\nproblem which corresponds to finding the pairs of sets with intersection size\nabove a certain threshold, and the problem of sampling an intersecting pair of\nsets uniformly at random. \n\n"}
{"id": "1807.01703", "contents": "Title: An efficient quantum circuits optimizing scheme compared with QISKit Abstract: Recently, the development of quantum chips has made great progress-- the\nnumber of qubits is increasing and the fidelity is getting higher. However,\nqubits of these chips are not always fully connected, which sets additional\nbarriers for implementing quantum algorithms and programming quantum programs.\nIn this paper, we introduce a general circuit optimizing scheme, which can\nefficiently adjust and optimize quantum circuits according to arbitrary given\nqubits' layout by adding additional quantum gates, exchanging qubits and\nmerging single-qubit gates. Compared with the optimizing algorithm of IBM's\nQISKit, the quantum gates consumed by our scheme is 74.7%, and the execution\ntime is only 12.9% on average. \n\n"}
{"id": "1807.05194", "contents": "Title: An Algorithmic Blend of LPs and Ring Equations for Promise CSPs Abstract: Promise CSPs are a relaxation of constraint satisfaction problems where the\ngoal is to find an assignment satisfying a relaxed version of the constraints.\nSeveral well-known problems can be cast as promise CSPs including approximate\ngraph coloring, discrepancy minimization, and interesting variants of\nsatisfiability. Similar to CSPs, the tractability of promise CSPs can be tied\nto the structure of operations on the solution space called polymorphisms,\nthough in the promise world these operations are much less constrained. Under\nthe thesis that non-trivial polymorphisms govern tractability, promise CSPs\ntherefore provide a fertile ground for the discovery of novel algorithms.\n  In previous work, we classified Boolean promise CSPs when the constraint\npredicates are symmetric. In this work, we vastly generalize these algorithmic\nresults. Specifically, we show that promise CSPs that admit a family of\n\"regional-periodic\" polymorphisms are in P, assuming that determining which\nregion a point is in can be computed in polynomial time. Such polymorphisms are\nquite general and are obtained by gluing together several functions that are\nperiodic in the Hamming weights in different blocks of the input.\n  Our algorithm is based on a novel combination of linear programming and\nsolving linear systems over rings. We also abstract a framework based on\nreducing a promise CSP to a CSP over an infinite domain, solving it there, and\nthen rounding the solution to an assignment for the promise CSP instance. The\nrounding step is intimately tied to the family of polymorphisms and clarifies\nthe connection between polymorphisms and algorithms in this context. As a key\ningredient, we introduce the technique of finding a solution to a linear\nprogram with integer coefficients that lies in a different ring (such as\n$\\mathbb Z[\\sqrt{2}]$) to bypass ad-hoc adjustments for lying on a rounding\nboundary. \n\n"}
{"id": "1807.06256", "contents": "Title: Classical lower bounds from quantum upper bounds Abstract: We prove lower bounds on complexity measures, such as the approximate degree\nof a Boolean function and the approximate rank of a Boolean matrix, using\nquantum arguments. We prove these lower bounds using a quantum query algorithm\nfor the combinatorial group testing problem.\n  We show that for any function f, the approximate degree of computing the OR\nof n copies of f is Omega(sqrt{n}) times the approximate degree of f, which is\noptimal. No such general result was known prior to our work, and even the lower\nbound for the OR of ANDs function was only resolved in 2013.\n  We then prove an analogous result in communication complexity, showing that\nthe logarithm of the approximate rank (or more precisely, the approximate\ngamma_2 norm) of F: X x Y -> {0,1} grows by a factor of Omega~(sqrt{n}) when we\ntake the OR of n copies of F, which is also essentially optimal. As a\ncorollary, we give a new proof of Razborov's celebrated Omega(sqrt{n}) lower\nbound on the quantum communication complexity of the disjointness problem.\n  Finally, we generalize both these results from composition with the OR\nfunction to composition with arbitrary symmetric functions, yielding nearly\noptimal lower bounds in this setting as well. \n\n"}
{"id": "1807.06456", "contents": "Title: Quantum Chebyshev's Inequality and Applications Abstract: In this paper we provide new quantum algorithms with polynomial speed-up for\na range of problems for which no such results were known, or we improve\nprevious algorithms. First, we consider the approximation of the frequency\nmoments $F_k$ of order $k \\geq 3$ in the multi-pass streaming model with\nupdates (turnstile model). We design a $P$-pass quantum streaming algorithm\nwith memory $M$ satisfying a tradeoff of $P^2 M = \\tilde{O}(n^{1-2/k})$,\nwhereas the best classical algorithm requires $P M = \\Theta(n^{1-2/k})$. Then,\nwe study the problem of estimating the number $m$ of edges and the number $t$\nof triangles given query access to an $n$-vertex graph. We describe optimal\nquantum algorithms that perform $\\tilde{O}(\\sqrt{n}/m^{1/4})$ and\n$\\tilde{O}(\\sqrt{n}/t^{1/6} + m^{3/4}/\\sqrt{t})$ queries respectively. This is\na quadratic speed-up compared to the classical complexity of these problems.\n  For this purpose we develop a new quantum paradigm that we call Quantum\nChebyshev's inequality. Namely we demonstrate that, in a certain model of\nquantum sampling, one can approximate with relative error the mean of any\nrandom variable with a number of quantum samples that is linear in the ratio of\nthe square root of the variance to the mean. Classically the dependency is\nquadratic. Our algorithm subsumes a previous result of Montanaro [Mon15]. This\nnew paradigm is based on a refinement of the Amplitude Estimation algorithm of\nBrassard et al. [BHMT02] and of previous quantum algorithms for the mean\nestimation problem. We show that this speed-up is optimal, and we identify\nanother common model of quantum sampling where it cannot be obtained. For our\napplications, we also adapt the variable-time amplitude amplification technique\nof Ambainis [Amb10] into a variable-time amplitude estimation algorithm. \n\n"}
{"id": "1807.06874", "contents": "Title: An Information-theoretic Framework for the Lossy Compression of Link\n  Streams Abstract: Graph compression is a data analysis technique that consists in the\nreplacement of parts of a graph by more general structural patterns in order to\nreduce its description length. It notably provides interesting exploration\ntools for the study of real, large-scale, and complex graphs which cannot be\ngrasped at first glance. This article proposes a framework for the compression\nof temporal graphs, that is for the compression of graphs that evolve with\ntime. This framework first builds on a simple and limited scheme, exploiting\nstructural equivalence for the lossless compression of static graphs, then\ngeneralises it to the lossy compression of link streams, a recent formalism for\nthe study of temporal graphs. Such generalisation relies on the natural\nextension of (bidimensional) relational data by the addition of a third\ntemporal dimension. Moreover, we introduce an information-theoretic measure to\nquantify and to control the information that is lost during compression, as\nwell as an algebraic characterisation of the space of possible compression\npatterns to enhance the expressiveness of the initial compression scheme. These\ncontributions lead to the definition of a combinatorial optimisation problem,\nthat is the Lossy Multistream Compression Problem, for which we provide an\nexact algorithm. \n\n"}
{"id": "1807.08678", "contents": "Title: Submodular Function Maximization in Parallel via the Multilinear\n  Relaxation Abstract: Balkanski and Singer [5] recently initiated the study of adaptivity (or\nparallelism) for constrained submodular function maximization, and studied the\nsetting of a cardinality constraint. Very recent improvements for this problem\nby Balkanski, Rubinstein, and Singer [6] and Ene and Nguyen [21] resulted in a\nnear-optimal $(1-1/e-\\epsilon)$-approximation in $O(\\log n/\\epsilon^2)$ rounds\nof adaptivity. Partly motivated by the goal of extending these results to more\ngeneral constraints, we describe parallel algorithms for approximately\nmaximizing the multilinear relaxation of a monotone submodular function subject\nto packing constraints. Formally our problem is to maximize $F(x)$ over $x \\in\n[0,1]^{n}$ subject to $Ax \\le 1$ where $F$ is the multilinear relaxation of a\nmonotone submodular function. Our algorithm achieves a near-optimal\n$(1-1/e-\\epsilon)$-approximation in $O(\\log^2 m \\log n/\\epsilon^4)$ rounds\nwhere $n$ is the cardinality of the ground set and $m$ is the number of packing\nconstraints. For many constraints of interest, the resulting fractional\nsolution can be rounded via known randomized rounding schemes that are\noblivious to the specific submodular function. We thus derive randomized\nalgorithms with poly-logarithmic adaptivity for a number of constraints\nincluding partition and laminar matroids, matchings, knapsack constraints, and\ntheir intersections. \n\n"}
{"id": "1807.11135", "contents": "Title: A Hybrid Quantum-Classical Paradigm to Mitigate Embedding Costs in\n  Quantum Annealing---Abridged Version Abstract: Quantum annealing has shown significant potential as an approach to near-term\nquantum computing. Despite promising progress towards obtaining a quantum\nspeedup, quantum annealers are limited by the need to embed problem instances\nwithin the (often highly restricted) connectivity graph of the annealer. This\nembedding can be costly to perform and may destroy any computational speedup.\nHere we present a hybrid quantum-classical paradigm to help mitigate this\nlimitation, and show how a raw speedup that is negated by the embedding time\ncan nonetheless be exploited in certain circumstances. We illustrate this\napproach with initial results on a proof-of-concept implementation of an\nalgorithm for the dynamically weighted maximum independent set problem. \n\n"}
{"id": "1807.11518", "contents": "Title: Parameterized Orientable Deletion Abstract: A graph is $d$-orientable if its edges can be oriented so that the maximum\nin-degree of the resulting digraph is at most $d$. $d$-orientability is a\nwell-studied concept with close connections to fundamental graph-theoretic\nnotions and applications as a load balancing problem. In this paper we consider\nthe d-ORIENTABLE DELETION problem: given a graph $G=(V,E)$, delete the minimum\nnumber of vertices to make $G$ $d$-orientable. We contribute a number of\nresults that improve the state of the art on this problem. Specifically:\n  - We show that the problem is W[2]-hard and $\\log n$-inapproximable with\nrespect to $k$, the number of deleted vertices. This closes the gap in the\nproblem's approximability.\n  - We completely characterize the parameterized complexity of the problem on\nchordal graphs: it is FPT parameterized by $d+k$, but W-hard for each of the\nparameters $d,k$ separately.\n  - We show that, under the SETH, for all $d,\\epsilon$, the problem does not\nadmit a $(d+2-\\epsilon)^{tw}$, algorithm where $tw$ is the graph's treewidth,\nresolving as a special case an open problem on the complexity of PSEUDOFOREST\nDELETION.\n  - We show that the problem is W-hard parameterized by the input graph's\nclique-width. Complementing this, we provide an algorithm running in time\n$d^{O(d\\cdot cw)}$, showing that the problem is FPT by $d+cw$, and improving\nthe previously best known algorithm for this case. \n\n"}
{"id": "1807.11869", "contents": "Title: On Exploring Temporal Graphs of Small Pathwidth Abstract: We show that the Temporal Graph Exploration Problem is NP-complete, even when\nthe underlying graph has pathwidth 2 and at each time step, the current graph\nis connected. \n\n"}
{"id": "1808.00838", "contents": "Title: Algorithms for Noisy Broadcast under Erasures Abstract: The noisy broadcast model was first studied in [Gallager, TranInf'88] where\nan $n$-character input is distributed among $n$ processors, so that each\nprocessor receives one input bit. Computation proceeds in rounds, where in each\nround each processor broadcasts a single character, and each reception is\ncorrupted independently at random with some probability $p$. [Gallager,\nTranInf'88] gave an algorithm for all processors to learn the input in\n$O(\\log\\log n)$ rounds with high probability. Later, a matching lower bound of\n$\\Omega(\\log\\log n)$ was given in [Goyal, Kindler, Saks; SICOMP'08].\n  We study a relaxed version of this model where each reception is erased and\nreplaced with a `?' independently with probability $p$. In this relaxed model,\nwe break past the lower bound of [Goyal, Kindler, Saks; SICOMP'08] and obtain\nan $O(\\log^* n)$-round algorithm for all processors to learn the input with\nhigh probability. We also show an $O(1)$-round algorithm for the same problem\nwhen the alphabet size is $\\Omega(\\mathrm{poly}(n))$. \n\n"}
{"id": "1808.02359", "contents": "Title: On the Computational Complexity of Length- and Neighborhood-Constrained\n  Path Problems Abstract: Finding paths in graphs is a fundamental graph-theoretic task. In this work,\nwe we are concerned with finding a path with some constraints on its length and\nthe number of vertices neighboring the path, that is, being outside of and\nincident with the path. Herein, we consider short and long paths on the one\nside, and small and large neighborhoods on the other side---yielding four\ndecision problems. We show that all four problems are NP-complete, even in\nplanar graphs with small maximum degree. Moreover, we study all four variants\nwhen parameterized by a bound $k$ on the length of the path, by a bound $\\ell$\non the size of neighborhood, and by $k + \\ell$. \n\n"}
{"id": "1808.02821", "contents": "Title: Positive 1-in-3-SAT admits a non-trivial Kernel Abstract: We illustrate the strength of Algebraic Methods, adapting Gaussian\nElimination and Substitution to the problem of Exact Boolean Satisfiability.\nFor 1-in-3 SAT with non-negated literals we are able to obtain considerably\nsmaller equivalent instances of 0/1 Integer Programming restricted to Equality\nonly.\n  Both Gaussian Elimination and Substitution may be used in a processing step,\nfollowed by a type of brute-force approach on the kernel thus obtained.\n  Our method shows that Positive instances of 1-in-3 SAT may be reduced to\nsignificantly smaller instances of I.P.E. in the following sense. Any such\ninstance of $|V|$ variables and $|C|$ clauses can be polynomial-time reduced to\nan instance of 0/1 Integer Programming with Equality, of size at most $2/3|V|$\nvariables and at most $|C|$ clauses.\n  We obtain an upper bound for the complexity of counting, $O(2\\kappa r\n2^{(1-\\kappa) r})$ for number of variables $r$ and clauses to variables ratio\n$\\kappa$.\n  We proceed to define formally the notion of a non-trivial kernel, defining\nthe problems considered as Constraint Satisfaction Problems.\n  We conclude showing the methods presented here, giving a non-trivial kernel\nfor positive 1-in-3 SAT, imply the existence of a non-trivial kernel for 1-in-3\nSAT. \n\n"}
{"id": "1808.02985", "contents": "Title: Sampling-Based Tour Generation of Arbitrarily Oriented Dubins Sensor\n  Platforms Abstract: This paper describes a formulation and develops a novel procedure for a fleet\nof unmanned aerial vehicles (UAVs) from the perspective of remotely executable\ntasks. In a complex mission environment, the characteristics of vehicles can be\ndifferent in terms of sensing capability, range, direction, or the motion\nconstraints. The purpose of this paper is to find a set of paths that minimizes\nthe sum of costs while every task region is visited exactly once under certain\nreasonable assumptions. The heterogeneous multi-UAV path planning problem is\nformulated as a generalized, heterogeneous, multiple depot traveling salesmen\nproblem (GHMDATSP), which is a variant of the traveling salesman problem. The\nproposed transformation procedure changes an instance of the GHMDATSP into a\nformat of an Asymmetric, Traveling Salesman Problem (ATSP) to obtain tours for\nwhich the total cost of a fleet of vehicles is minimized. The instance of the\nATSP is solved using the Lin-Kernighan-Helsgaun heuristic, and the result is\ninversely transformed to the GHMDATSP-formatted instance to obtain a set of\ntours. An additional local optimization based path refinement process helps\nobtain a high-quality solution. Numerical experiments investigate and confirm\nfor the validity and applicability of the proposed procedure. \n\n"}
{"id": "1808.04360", "contents": "Title: Stochastic on-time arrival problem in transit networks Abstract: This article considers the stochastic on-time arrival problem in transit\nnetworks where both the travel time and the waiting time for transit services\nare stochastic. A specific challenge of this problem is the combinatorial\nsolution space due to the unknown ordering of transit line arrivals. We propose\na network structure appropriate to the online decision-making of a passenger,\nincluding boarding, waiting and transferring. In this framework, we design a\ndynamic programming algorithm that is pseudo-polynomial in the number of\ntransit stations and travel time budget, and exponential in the number of\ntransit lines at a station, which is a small number in practice. To reduce the\nsearch space, we propose a definition of transit line dominance, and techniques\nto identify dominance, which decrease the computation time by up to 90% in\nnumerical experiments. Extensive numerical experiments are conducted on both a\nsynthetic network and the Chicago transit network. \n\n"}
{"id": "1808.04995", "contents": "Title: The Sketching Complexity of Graph and Hypergraph Counting Abstract: Subgraph counting is a fundamental primitive in graph processing, with\napplications in social network analysis (e.g., estimating the clustering\ncoefficient of a graph), database processing and other areas. The space\ncomplexity of subgraph counting has been studied extensively in the literature,\nbut many natural settings are still not well understood. In this paper we\nrevisit the subgraph (and hypergraph) counting problem in the sketching model,\nwhere the algorithm's state as it processes a stream of updates to the graph is\na linear function of the stream. This model has recently received a lot of\nattention in the literature, and has become a standard model for solving\ndynamic graph streaming problems.\n  In this paper we give a tight bound on the sketching complexity of counting\nthe number of occurrences of a small subgraph $H$ in a bounded degree graph $G$\npresented as a stream of edge updates. Specifically, we show that the space\ncomplexity of the problem is governed by the fractional vertex cover number of\nthe graph $H$. Our subgraph counting algorithm implements a natural vertex\nsampling approach, with sampling probabilities governed by the vertex cover of\n$H$. Our main technical contribution lies in a new set of Fourier analytic\ntools that we develop to analyze multiplayer communication protocols in the\nsimultaneous communication model, allowing us to prove a tight lower bound. We\nbelieve that our techniques are likely to find applications in other settings.\nBesides giving tight bounds for all graphs $H$, both our algorithm and lower\nbounds extend to the hypergraph setting, albeit with some loss in space\ncomplexity. \n\n"}
{"id": "1808.05676", "contents": "Title: Why did the shape of your network change? (On detecting network\n  anomalies via non-local curvatures) Abstract: $Anomaly$ $detection$ problems (also called $change$-$point$ $detection$\nproblems) have been studied in data mining, statistics and computer science\nover the last several decades in applications such as medical condition\nmonitoring and weather change detection. In recent days, however, anomaly\ndetection problems have become increasing more relevant in the context of\n$network$ $science$ since useful insights for many complex systems in biology,\nfinance and social science are often obtained by representing them via\nnetworks. Notions of local and non-local curvatures of higher-dimensional\ngeometric shapes and topological spaces play a $fundamental$ role in physics\nand mathematics in characterizing anomalous behaviours of these higher\ndimensional entities. However, using curvature measures to detect anomalies in\nnetworks is not yet very common. To this end, a main goal in this paper to\nformulate and analyze curvature analysis methods to provide the foundations of\nsystematic approaches to find $critical$ $components$ and $detect$ $anomalies$\nin networks. For this purpose, we use two measures of network curvatures which\ndepend on non-trivial global properties, such as distributions of geodesics and\nhigher-order correlations among nodes, of the given network. Based on these\nmeasures, we precisely formulate several computational problems related to\nanomaly detection in static or dynamic networks, and provide non-trivial\ncomputational complexity results for these problems. This paper must $not$ be\nviewed as delivering the final word on appropriateness and suitability of\nspecific curvature measures. Instead, it is our hope that this paper will\nstimulate and motivate further theoretical or empirical research concerning the\nexciting interplay between notions of curvatures from network and non-network\ndomains, a $much$ desired goal in our opinion. \n\n"}
{"id": "1808.05915", "contents": "Title: On Representations of Graphs as Two-Distance Sets Abstract: Let a \\neq b be two positive scalars. A Euclidean representation of a simple\ngraph G in R^r is a mapping of the nodes of G into points in R^r such that the\nsquared Euclidean distance between any two points is a if the corresponding\nnodes are adjacent and b otherwise. A Euclidean representation is spherical if\nthe points lie on an (r-1)-sphere, and is J-spherical if this sphere has radius\n1 and a=2 < b. Let dim_E(G), dim_S(G) and dim_J(G) denote, respectively, the\nsmallest dimension r for which G admits a Euclidean, spherical and J-spherical\nrepresentation.\n  In this paper, we extend and simplify the results of Roy[18] and Nozaki and\nshinohara[17] by deriving exact simple formulas for dim_E(G) and dim_S(G) in\nterms of the eigenvalues of V^TAV, where A is the adjacency matrix of G and V\nis the matrix whose columns form an orthonormal basis for the orthogonal\ncomplement of the vector of all 1's.\n  We also extend and simplify the results of Musin [16] by deriving explicit\nformulas for determining the J-spherical representation of G and for\ndetermining dim_J(G)in terms of the largest eigenvalue of \\bar{A}, the\nadjacency matrix of the complement graph \\bar{G}. As a byproduct, we obtain\nseveral related results and in particular we answer a question raised by Musin\nin [16]. \n\n"}
{"id": "1808.06265", "contents": "Title: Pseudorandom Generators for Read-Once Branching Programs, in any Order Abstract: A central question in derandomization is whether randomized logspace (RL)\nequals deterministic logspace (L). To show that RL=L, it suffices to construct\nexplicit pseudorandom generators (PRGs) that fool polynomial-size read-once\n(oblivious) branching programs (roBPs). Starting with the work of Nisan,\npseudorandom generators with seed-length $O(\\log^2 n)$ were constructed.\nUnfortunately, improving on this seed-length in general has proven challenging\nand seems to require new ideas.\n  A recent line of inquiry has suggested focusing on a particular limitation of\nthe existing PRGs, which is that they only fool roBPs when the variables are\nread in a particular known order, such as $x_1<\\cdots<x_n$. In comparison,\nexistentially one can obtain logarithmic seed-length for fooling the set of\npolynomial-size roBPs that read the variables under any fixed unknown\npermutation $x_{\\pi(1)}<\\cdots<x_{\\pi(n)}$. While recent works have established\nnovel PRGs in this setting for subclasses of roBPs, there were no known\n$n^{o(1)}$ seed-length explicit PRGs for general polynomial-size roBPs in this\nsetting.\n  In this work, we follow the \"bounded independence plus noise\" paradigm of\nHaramaty, Lee and Viola, and give an improved analysis in the general roBP\nunknown-order setting. With this analysis we obtain an explicit PRG with\nseed-length $O(\\log^3 n)$ for polynomial-size roBPs reading their bits in an\nunknown order. Plugging in a recent Fourier tail bound of Chattopadhyay,\nHatami, Reingold, and Tal, we can obtain a $\\widetilde{O}(\\log^2 n)$\nseed-length when the roBP is of constant width. \n\n"}
{"id": "1808.09495", "contents": "Title: Symmetries of 3-polytopes with fixed edge lengths Abstract: We consider an interesting class of combinatorial symmetries of polytopes\nwhich we call \\emph{edge-length preserving combinatorial symmetries}. These\nsymmetries not only preserve the combinatorial structure of a polytope but also\nmap each edge of the polytope to an edge of the same length. We prove a simple\nsufficient condition for a polytope to realize all edge-length preserving\ncombinatorial symmetries by isometries of ambient space. The proof of this\ncondition uses Cauchy's rigidity theorem in an unusual way. \n\n"}
{"id": "1808.10650", "contents": "Title: Graph reduction with spectral and cut guarantees Abstract: Can one reduce the size of a graph without significantly altering its basic\nproperties? The graph reduction problem is hereby approached from the\nperspective of restricted spectral approximation, a modification of the\nspectral similarity measure used for graph sparsification. This choice is\nmotivated by the observation that restricted approximation carries strong\nspectral and cut guarantees, and that it implies approximation results for\nunsupervised learning problems relying on spectral embeddings.\n  The paper then focuses on coarsening---the most common type of graph\nreduction. Sufficient conditions are derived for a small graph to approximate a\nlarger one in the sense of restricted similarity. These findings give rise to\nnearly-linear algorithms that, compared to both standard and advanced graph\nreduction methods, find coarse graphs of improved quality, often by a large\nmargin, without sacrificing speed. \n\n"}
{"id": "1809.00394", "contents": "Title: Mining Frequent Patterns in Evolving Graphs Abstract: Given a labeled graph, the frequent-subgraph mining (FSM) problem asks to\nfind all the $k$-vertex subgraphs that appear with frequency greater than a\ngiven threshold. FSM has numerous applications ranging from biology to network\nscience, as it provides a compact summary of the characteristics of the graph.\nHowever, the task is challenging, even more so for evolving graphs due to the\nstreaming nature of the input and the exponential time complexity of the\nproblem.\n  In this paper, we initiate the study of the approximate FSM problem in both\nincremental and fully-dynamic streaming settings, where arbitrary edges can be\nadded or removed from the graph. For each streaming setting, we propose\nalgorithms that can extract a high-quality approximation of the frequent\n$k$-vertex subgraphs for a given threshold, at any given time instance, with\nhigh probability. In contrast to the existing state-of-the-art solutions that\nrequire iterating over the entire set of subgraphs for any update, our\nalgorithms operate by maintaining a uniform sample of $k$-vertex subgraphs with\noptimized neighborhood-exploration procedures local to the updates. We provide\ntheoretical analysis of the proposed algorithms and empirically demonstrate\nthat the proposed algorithms generate high-quality results compared to\nbaselines. \n\n"}
{"id": "1809.02006", "contents": "Title: Rigidity for sticky disks Abstract: We study the combinatorial and rigidity properties of disk packings with\ngeneric radii. We show that a packing of $n$ disks in the plane with generic\nradii cannot have more than $2n-3$ pairs of disks in contact. The allowed\nmotions of a packing preserve the disjointness of the disk interiors and\ntangency between pairs already in contact (modeling a collection of sticky\ndisks). We show that if a packing has generic radii, then the allowed motions\nare all rigid body motions if and only if the packing has exactly $2n-3$\ncontacts. Our approach is to study the space of packings with a fixed contact\ngraph. The main technical step is to show that this space is a smooth manifold,\nwhich is done via a connection to the Cauchy-Alexandrov stress lemma. Our\nmethods also apply to jamming problems, in which contacts are allowed to break\nduring a motion. We give a simple proof of a finite variant of a recent result\nof Connelly, et al. on the number of contacts in a jammed packing of disks with\ngeneric radii. \n\n"}
{"id": "1809.02835", "contents": "Title: Multitasking Capacity: Hardness Results and Improved Constructions Abstract: We consider the problem of determining the maximal $\\alpha \\in (0,1]$ such\nthat every matching $M$ of size $k$ (or at most $k$) in a bipartite graph $G$\ncontains an induced matching of size at least $\\alpha |M|$. This measure was\nrecently introduced in Alon et al. (NIPS 2018) and is motivated by\nconnectionist models of cognition as well as modeling interference in wireless\nand communication networks.\n  We prove various hardness results for computing $\\alpha$ either exactly or\napproximately. En route to our results, we also consider the maximum connected\nmatching problem: determining the largest matching $N$ in a graph $G$ such that\nevery two edges in $N$ are connected by an edge. We prove a nearly optimal\n$n^{1-\\epsilon}$ hardness of approximation result (under randomized reductions)\nfor connected matching in bipartite graphs (with both sides of cardinality\n$n$). Towards this end we define bipartite half-covers: A new combinatorial\nobject that may be of independent interest. To the best of our knowledge, the\nbest previous hardness result for the connected matching problem was some\nconstant $\\beta>1$.\n  Finally, we demonstrate the existence of bipartite graphs with $n$ vertices\non each side of average degree $d$, that achieve $\\alpha=1/2-\\epsilon$ for\nmatchings of size sufficiently smaller than $n/poly(d)$. This nearly matches\nthe trivial upper bound of $1/2$ on $\\alpha$ which holds for any graph\ncontaining a path of length 3. \n\n"}
{"id": "1809.05419", "contents": "Title: Approximate Query Processing over Static Sets and Sliding Windows Abstract: Indexing of static and dynamic sets is fundamental to a large set of\napplications such as information retrieval and caching. Denoting the\ncharacteristic vector of the set by B, we consider the problem of encoding sets\nand multisets to support approximate versions of the operations rank(i) (i.e.,\ncomputing sum_{j <= i}B[j]) and select(i) (i.e., finding min{p | rank(p) >= i})\nqueries. We study multiple types of approximations (allowing an error in the\nquery or the result) and present lower bounds and succinct data structures for\nseveral variants of the problem. We also extend our model to sliding windows,\nin which we process a stream of elements and compute suffix sums. This is a\ngeneralization of the window summation problem that allows the user to specify\nthe window size at query time. Here, we provide an algorithm that supports\nupdates and queries in constant time while requiring just (1+o(1)) factor more\nspace than the fixed-window summation algorithms. \n\n"}
{"id": "1809.06171", "contents": "Title: Best-case and Worst-case Sparsifiability of Boolean CSPs Abstract: We continue the investigation of polynomial-time sparsification for\nNP-complete Boolean Constraint Satisfaction Problems (CSPs). The goal in\nsparsification is to reduce the number of constraints in a problem instance\nwithout changing the answer, such that a bound on the number of resulting\nconstraints can be given in terms of the number of variables n. We investigate\nhow the worst-case sparsification size depends on the types of constraints\nallowed in the problem formulation (the constraint language). Two algorithmic\nresults are presented. The first result essentially shows that for any arity k,\nthe only constraint type for which no nontrivial sparsification is possible has\nexactly one falsifying assignment, and corresponds to logical OR (up to\nnegations). Our second result concerns linear sparsification, that is, a\nreduction to an equivalent instance with O(n) constraints. Using linear algebra\nover rings of integers modulo prime powers, we give an elegant necessary and\nsufficient condition for a constraint type to be captured by a degree-1\npolynomial over such a ring, which yields linear sparsifications. The\ncombination of these algorithmic results allows us to prove two\ncharacterizations that capture the optimal sparsification sizes for a range of\nBoolean CSPs. For NP-complete Boolean CSPs whose constraints are symmetric (the\nsatisfaction depends only on the number of 1 values in the assignment, not on\ntheir positions), we give a complete characterization of which constraint\nlanguages allow for a linear sparsification. For Boolean CSPs in which every\nconstraint has arity at most three, we characterize the optimal size of\nsparsifications in terms of the largest OR that can be expressed by the\nconstraint language. \n\n"}
{"id": "1809.08858", "contents": "Title: Graph Pattern Polynomials Abstract: We study the time complexity of induced subgraph isomorphism problems where\nthe pattern graph is fixed. The earliest known example of an improvement over\ntrivial algorithms is by Itai and Rodeh (1978) who sped up triangle detection\nin graphs using fast matrix multiplication. This algorithm was generalized by\nNe\\v{s}et\\v{r}il and Poljak (1985) to speed up detection of k-cliques.\n  Improved algorithms are known for certain small-sized patterns. For example,\na linear-time algorithm is known for detecting length-4 paths. In this paper,\nwe give the first pattern detection algorithm that improves upon\nNe\\v{s}et\\v{r}il and Poljak's algorithm for arbitrarily large pattern graphs\n(not cliques). The algorithm is obtained by reducing the induced subgraph\nisomorphism problem to the problem of detecting multilinear terms in\nconstant-degree polynomials.\n  We show that the same technique can be used to reduce the induced subgraph\nisomorphism problem of many pattern graphs to constructing arithmetic circuits\ncomputing homomorphism polynomials of these pattern graphs. Using this, we\nobtain faster combinatorial algorithms (algorithms that do not use fast matrix\nmultiplication) for k-paths and k-cycles. We also obtain faster algorithms for\n5-paths and 5-cycles that match the runtime for triangle detection.\n  We show that these algorithms are expressible using polynomial families that\nwe call graph pattern polynomial families. We then define a notion of reduction\namong these polynomials that allows us to compare the complexity of various\npattern detection problems within this framework. For example, we show that the\ninduced subgraph isomorphism polynomial for any pattern that contains a\nk-clique is harder than the induced subgraph isomorphism polynomial for\nk-clique. An analogue of this theorem is not known with respect to general\nalgorithmic hardness. \n\n"}
{"id": "1809.09165", "contents": "Title: Locally Private Learning without Interaction Requires Separation Abstract: We consider learning under the constraint of local differential privacy\n(LDP). For many learning problems known efficient algorithms in this model\nrequire many rounds of communication between the server and the clients holding\nthe data points. Yet multi-round protocols are prohibitively slow in practice\ndue to network latency and, as a result, currently deployed large-scale systems\nare limited to a single round. Despite significant research interest, very\nlittle is known about which learning problems can be solved by such\nnon-interactive systems. The only lower bound we are aware of is for PAC\nlearning an artificial class of functions with respect to a uniform\ndistribution (Kasiviswanathan et al. 2011).\n  We show that the margin complexity of a class of Boolean functions is a lower\nbound on the complexity of any non-interactive LDP algorithm for\ndistribution-independent PAC learning of the class. In particular, the classes\nof linear separators and decision lists require exponential number of samples\nto learn non-interactively even though they can be learned in polynomial time\nby an interactive LDP algorithm. This gives the first example of a natural\nproblem that is significantly harder to solve without interaction and also\nresolves an open problem of Kasiviswanathan et al. (2011). We complement this\nlower bound with a new efficient learning algorithm whose complexity is\npolynomial in the margin complexity of the class. Our algorithm is\nnon-interactive on labeled samples but still needs interactive access to\nunlabeled samples. All of our results also apply to the statistical query model\nand any model in which the number of bits communicated about each data point is\nconstrained. \n\n"}
{"id": "1809.09819", "contents": "Title: Improved bounds on Fourier entropy and Min-entropy Abstract: Given a Boolean function $f:\\{-1,1\\}^n\\to \\{-1,1\\}$, the Fourier distribution\nassigns probability $\\widehat{f}(S)^2$ to $S\\subseteq [n]$. The Fourier\nEntropy-Influence (FEI) conjecture of Friedgut and Kalai asks if there exist a\nuniversal constant C>0 such that $H(\\hat{f}^2)\\leq C Inf(f)$, where\n$H(\\hat{f}^2)$ is the Shannon entropy of the Fourier distribution of $f$ and\n$Inf(f)$ is the total influence of $f$.\n  1) We consider the weaker Fourier Min-entropy-Influence (FMEI) conjecture.\nThis asks if $H_{\\infty}(\\hat{f}^2)\\leq C Inf(f)$, where\n$H_{\\infty}(\\hat{f}^2)$ is the min-entropy of the Fourier distribution. We show\n$H_{\\infty}(\\hat{f}^2)\\leq 2C_{\\min}^\\oplus(f)$, where $C_{\\min}^\\oplus(f)$ is\nthe minimum parity certificate complexity of $f$. We also show that for every\n$\\epsilon\\geq 0$, we have $H_{\\infty}(\\hat{f}^2)\\leq 2\\log\n(\\|\\hat{f}\\|_{1,\\epsilon}/(1-\\epsilon))$, where $\\|\\hat{f}\\|_{1,\\epsilon}$ is\nthe approximate spectral norm of $f$. As a corollary, we verify the FMEI\nconjecture for the class of read-$k$ $DNF$s (for constant $k$).\n  2) We show that $H(\\hat{f}^2)\\leq 2 aUC^\\oplus(f)$, where $aUC^\\oplus(f)$ is\nthe average unambiguous parity certificate complexity of $f$. This improves\nupon Chakraborty et al. An important consequence of the FEI conjecture is the\nlong-standing Mansour's conjecture. We show that a weaker version of FEI\nalready implies Mansour's conjecture: is $H(\\hat{f}^2)\\leq C\n\\min\\{C^0(f),C^1(f)\\}$?, where $C^0(f), C^1(f)$ are the 0- and 1-certificate\ncomplexities of $f$, respectively.\n  3) We study what FEI implies about the structure of polynomials that\n1/3-approximate a Boolean function. We pose a conjecture (which is implied by\nFEI): no \"flat\" degree-$d$ polynomial of sparsity $2^{\\omega(d)}$ can\n1/3-approximate a Boolean function. We prove this conjecture unconditionally\nfor a particular class of polynomials. \n\n"}
{"id": "1809.10325", "contents": "Title: Being Corrupt Requires Being Clever, But Detecting Corruption Doesn't Abstract: We consider a variation of the problem of corruption detection on networks\nposed by Alon, Mossel, and Pemantle '15. In this model, each vertex of a graph\ncan be either truthful or corrupt. Each vertex reports about the types\n(truthful or corrupt) of all its neighbors to a central agency, where truthful\nnodes report the true types they see and corrupt nodes report adversarially.\nThe central agency aggregates these reports and attempts to find a single\ntruthful node. Inspired by real auditing networks, we pose our problem for\narbitrary graphs and consider corruption through a computational lens. We\nidentify a key combinatorial parameter of the graph $m(G)$, which is the\nminimal number of corrupted agents needed to prevent the central agency from\nidentifying a single truthful node. We give an efficient (in fact, linear time)\nalgorithm for the central agency to identify a truthful node that is successful\nwhenever the number of corrupt nodes is less than $m(G)/2$. On the other hand,\nwe prove that for any constant $\\alpha > 1$, it is NP-hard to find a subset of\nnodes $S$ in $G$ such that corrupting $S$ prevents the central agency from\nfinding one truthful node and $|S| \\leq \\alpha m(G)$, assuming the Small Set\nExpansion Hypothesis (Raghavendra and Steurer, STOC '10). We conclude that\nbeing corrupt requires being clever, while detecting corruption does not.\n  Our main technical insight is a relation between the minimum number of\ncorrupt nodes required to hide all truthful nodes and a certain notion of\nvertex separability for the underlying graph. Additionally, this insight lets\nus design an efficient algorithm for a corrupt party to decide which graphs\nrequire the fewest corrupted nodes, up to a multiplicative factor of $O(\\log\nn)$. \n\n"}
{"id": "1809.10469", "contents": "Title: Probabilistic Analysis of Edge Elimination for Euclidean TSP Abstract: One way to speed up the calculation of optimal TSP tours in practice is\neliminating edges that are certainly not in the optimal tour as a preprocessing\nstep. In order to do so several edge elimination approaches have been proposed\nin the past. In this work we investigate two of them in the scenario where the\ninput consists of $n$ independently distributed random points in the\n2-dimensional unit square with bounded density function from above and below by\narbitrary positive constants. We show that after the edge elimination procedure\nof Hougardy and Schroeder the expected number of remaining edges is\n$\\Theta(n)$, while after that the non-recursive part of Jonker and Volgenant\nthe expected number of remaining edges is $\\Theta(n^2)$. \n\n"}
{"id": "1809.11086", "contents": "Title: Learning Recurrent Binary/Ternary Weights Abstract: Recurrent neural networks (RNNs) have shown excellent performance in\nprocessing sequence data. However, they are both complex and memory intensive\ndue to their recursive nature. These limitations make RNNs difficult to embed\non mobile devices requiring real-time processes with limited hardware\nresources. To address the above issues, we introduce a method that can learn\nbinary and ternary weights during the training phase to facilitate hardware\nimplementations of RNNs. As a result, using this approach replaces all\nmultiply-accumulate operations by simple accumulations, bringing significant\nbenefits to custom hardware in terms of silicon area and power consumption. On\nthe software side, we evaluate the performance (in terms of accuracy) of our\nmethod using long short-term memories (LSTMs) on various sequential models\nincluding sequence classification and language modeling. We demonstrate that\nour method achieves competitive results on the aforementioned tasks while using\nbinary/ternary weights during the runtime. On the hardware side, we present\ncustom hardware for accelerating the recurrent computations of LSTMs with\nbinary/ternary weights. Ultimately, we show that LSTMs with binary/ternary\nweights can achieve up to 12x memory saving and 10x inference speedup compared\nto the full-precision implementation on an ASIC platform. \n\n"}
{"id": "1810.00580", "contents": "Title: Slaying Hydrae: Improved Bounds for Generalized k-Server in Uniform\n  Metrics Abstract: The generalized $k$-server problem is an extension of the weighted $k$-server\nproblem, which in turn extends the classic $k$-server problem. In the\ngeneralized $k$-server problem, each of $k$ servers $s_1, \\dots, s_k$ remains\nin its own metric space $M_i$. A request is a tuple $(r_1,\\dots,r_k)$, where\n$r_i \\in M_i$, and to service it, an algorithm needs to move at least one\nserver $s_i$ to the point $r_i$. The objective is to minimize the total\ndistance traveled by all servers.\n  In this paper, we focus on the generalized $k$-server problem for the case\nwhere all $M_i$ are uniform metrics. We show an $O(k^2 \\cdot \\log\nk)$-competitive randomized algorithm improving over a recent result by Bansal\net al. [SODA 2018], who gave an $O(k^3 \\cdot \\log k)$-competitive algorithm. To\nthis end, we define an abstract online problem, called Hydra game, and we show\nthat a randomized solution of low cost to this game implies a randomized\nalgorithm to the generalized $k$-server problem with low competitive ratio.\n  We also show that no randomized algorithm can achieve competitive ratio lower\nthan $\\Omega(k)$, thus improving the lower bound of $\\Omega(k / \\log^2 k)$ by\nBansal et al. \n\n"}
{"id": "1810.01136", "contents": "Title: A deterministic polynomial kernel for Odd Cycle Transversal and Vertex\n  Multiway Cut in planar graphs Abstract: We show that Odd Cycle Transversal and Vertex Multiway Cut admit\ndeterministic polynomial kernels when restricted to planar graphs and\nparameterized by the solution size. This answers a question of Saurabh. On the\nway to these results, we provide an efficient sparsification routine in the\nflavor of the sparsification routine used for the Steiner Tree problem in\nplanar graphs (FOCS 2014). It differs from the previous work because it\npreserves the existence of low-cost subgraphs that are not necessarily Steiner\ntrees in the original plane graph, but structures that turn into (supergraphs\nof) Steiner trees after adding all edges between pairs of vertices that lie on\na common face. We also show connections between Vertex Multiway Cut and the\nVertex Planarization problem, where the existence of a polynomial kernel\nremains an important open problem. \n\n"}
{"id": "1810.02304", "contents": "Title: Polynomial-time Recognition of 4-Steiner Powers Abstract: The $k^{th}$-power of a given graph $G=(V,E)$ is obtained from $G$ by adding\nan edge between every two distinct vertices at a distance at most $k$ in $G$.\nWe call $G$ a $k$-Steiner power if it is an induced subgraph of the\n$k^{th}$-power of some tree. Our main contribution is a polynomial-time\nrecognition algorithm of $4$-Steiner powers, thereby extending the\ndecade-year-old results of (Lin, Kearney and Jiang, ISAAC'00) for $k=1,2$ and\n(Chang and Ko, WG'07) for $k=3$.\n  A graph $G$ is termed $k$-leaf power if there is some tree $T$ such that: all\nvertices in $V(G)$ are leaf-nodes of $T$, and $G$ is an induced subgraph of the\n$k^{th}$-power of $T$. As a byproduct of our main result, we give the first\nknown polynomial-time recognition algorithm for $6$-leaf powers. \n\n"}
{"id": "1810.04254", "contents": "Title: Extreme Classification in Log Memory Abstract: We present Merged-Averaged Classifiers via Hashing (MACH) for\nK-classification with ultra-large values of K. Compared to traditional\none-vs-all classifiers that require O(Kd) memory and inference cost, MACH only\nneed O(d log K) (d is dimensionality )memory while only requiring O(K log K + d\nlog K) operation for inference. MACH is a generic K-classification algorithm,\nwith provably theoretical guarantees, which requires O(log K) memory without\nany assumption on the relationship between classes. MACH uses universal hashing\nto reduce classification with a large number of classes to few independent\nclassification tasks with small (constant) number of classes. We provide\ntheoretical quantification of discriminability-memory tradeoff. With MACH we\ncan train ODP dataset with 100,000 classes and 400,000 features on a single\nTitan X GPU, with the classification accuracy of 19.28%, which is the\nbest-reported accuracy on this dataset. Before this work, the best performing\nbaseline is a one-vs-all classifier that requires 40 billion parameters (160 GB\nmodel size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracy\nwith 480x reduction in the model size (of mere 0.3GB). With MACH, we also\ndemonstrate complete training of fine-grained imagenet dataset (compressed size\n104GB), with 21,000 classes, on a single GPU. To the best of our knowledge,\nthis is the first work to demonstrate complete training of these extreme-class\ndatasets on a single Titan X. \n\n"}
{"id": "1810.05692", "contents": "Title: Linear Program Reconstruction in Practice Abstract: We briefly report on a successful linear program reconstruction attack\nperformed on a production statistical queries system and using a real dataset.\nThe attack was deployed in test environment in the course of the Aircloak\nChallenge bug bounty program and is based on the reconstruction algorithm of\nDwork, McSherry, and Talwar. We empirically evaluate the effectiveness of the\nalgorithm and a related algorithm by Dinur and Nissim with various dataset\nsizes, error rates, and numbers of queries in a Gaussian noise setting. \n\n"}
{"id": "1810.08345", "contents": "Title: A Matrix Chernoff Bound for Strongly Rayleigh Distributions and Spectral\n  Sparsifiers from a few Random Spanning Trees Abstract: Strongly Rayleigh distributions are a class of negatively dependent\ndistributions of binary-valued random variables [Borcea, Branden, Liggett JAMS\n09]. Recently, these distributions have played a crucial role in the analysis\nof algorithms for fundamental graph problems, e.g. Traveling Salesman Problem\n[Gharan, Saberi, Singh FOCS 11]. We prove a new matrix Chernoff bound for\nStrongly Rayleigh distributions.\n  As an immediate application, we show that adding together the Laplacians of\n$\\epsilon^{-2} \\log^2 n$ random spanning trees gives an $(1\\pm \\epsilon)$\nspectral sparsifiers of graph Laplacians with high probability. Thus, we\npositively answer an open question posed in [Baston, Spielman, Srivastava, Teng\nJACM 13]. Our number of spanning trees for spectral sparsifier matches the\nnumber of spanning trees required to obtain a cut sparsifier in [Fung,\nHariharan, Harvey, Panigraphi STOC 11]. The previous best result was by naively\napplying a classical matrix Chernoff bound which requires $\\epsilon^{-2} n \\log\nn$ spanning trees. For the tree averaging procedure to agree with the original\ngraph Laplacian in expectation, each edge of the tree should be reweighted by\nthe inverse of the edge leverage score in the original graph. We also show that\nwhen using this reweighting of the edges, the Laplacian of single random tree\nis bounded above in the PSD order by the original graph Laplacian times a\nfactor $\\log n$ with high probability, i.e. $L_T \\preceq O(\\log n) L_G$.\n  We show a lower bound that almost matches our last result, namely that in\nsome graphs, with high probability, the random spanning tree is $\\it{not}$\nbounded above in the spectral order by $\\frac{\\log n}{\\log\\log n}$ times the\noriginal graph Laplacian. We also show a lower bound that in $\\epsilon^{-2}\n\\log n$ spanning trees are necessary to get a $(1\\pm \\epsilon)$ spectral\nsparsifier. \n\n"}
{"id": "1810.08671", "contents": "Title: Limits on All Known (and Some Unknown) Approaches to Matrix\n  Multiplication Abstract: We study the known techniques for designing Matrix Multiplication algorithms.\nThe two main approaches are the Laser method of Strassen, and the Group\ntheoretic approach of Cohn and Umans. We define a generalization based on\nzeroing outs which subsumes these two approaches, which we call the Solar\nmethod, and an even more general method based on monomial degenerations, which\nwe call the Galactic method.\n  We then design a suite of techniques for proving lower bounds on the value of\n$\\omega$, the exponent of matrix multiplication, which can be achieved by\nalgorithms using many tensors $T$ and the Galactic method. Some of our\ntechniques exploit `local' properties of $T$, like finding a sub-tensor of $T$\nwhich is so `weak' that $T$ itself couldn't be used to achieve a good bound on\n$\\omega$, while others exploit `global' properties, like $T$ being a monomial\ndegeneration of the structural tensor of a group algebra.\n  Our main result is that there is a universal constant $\\ell>2$ such that a\nlarge class of tensors generalizing the Coppersmith-Winograd tensor $CW_q$\ncannot be used within the Galactic method to show a bound on $\\omega$ better\nthan $\\ell$, for any $q$. We give evidence that previous lower-bounding\ntechniques were not strong enough to show this. We also prove a number of\ncomplementary results along the way, including that for any group $G$, the\nstructural tensor of $\\mathbb{C}[G]$ can be used to recover the best bound on\n$\\omega$ which the Coppersmith-Winograd approach gets using $CW_{|G|-2}$ as\nlong as the asymptotic rank of the structural tensor is not too large. \n\n"}
{"id": "1810.10046", "contents": "Title: Approximating the Quadratic Transportation Metric in Near-Linear Time Abstract: Computing the quadratic transportation metric (also called the\n$2$-Wasserstein distance or root mean square distance) between two point\nclouds, or, more generally, two discrete distributions, is a fundamental\nproblem in machine learning, statistics, computer graphics, and theoretical\ncomputer science. A long line of work has culminated in a sophisticated\ngeometric algorithm due to Agarwal and Sharathkumar in 2014, which runs in time\n$\\tilde{O}(n^{3/2})$, where $n$ is the number of points. However, obtaining\nfaster algorithms has proven difficult since the $2$-Wasserstein distance is\nknown to have poor sketching and embedding properties, which limits the\neffectiveness of geometric approaches. In this paper, we give an extremely\nsimple deterministic algorithm with $\\tilde{O}(n)$ runtime by using a\ncompletely different approach based on entropic regularization, approximate\nSinkhorn scaling, and low-rank approximations of Gaussian kernel matrices. We\ngive explicit dependence of our algorithm on the dimension and precision of the\napproximation. \n\n"}
{"id": "1810.11700", "contents": "Title: Minimum Reload Cost Graph Factors Abstract: The concept of Reload cost in a graph refers to the cost that occurs while\ntraversing a vertex via two of its incident edges. This cost is uniquely\ndetermined by the colors of the two edges. This concept has various\napplications in transportation networks, communication networks, and energy\ndistribution networks. Various problems using this model are defined and\nstudied in the literature. The problem of finding a spanning tree whose\ndiameter with respect to the reload costs is the smallest possible, the\nproblems of finding a path, trail or walk with minimum total reload cost\nbetween two given vertices, problems about finding a proper edge coloring of a\ngraph such that the total reload cost is minimized, the problem of finding a\nspanning tree such that the sum of the reload costs of all paths between all\npairs of vertices is minimized, and the problem of finding a set of cycles of\nminimum reload cost, that cover all the vertices of a graph, are examples of\nsuch problems. % In this work we focus on the last problem. Noting that a cycle\ncover of a graph is a 2-factor of it, we generalize the problem to that of\nfinding an $r$-factor of minimum reload cost of an edge colored graph. We prove\nseveral NP-hardness results for special cases of the problem. Namely, bounded\ndegree graphs, planar graphs, bounded total cost, and bounded number of\ndistinct costs. For the special case of $r=2$, our results imply an improved\nNP-hardness result. On the positive side, we present a polynomial-time solvable\nspecial case which provides a tight boundary between the polynomial and hard\ncases in terms of $r$ and the maximum degree of the graph. We then investigate\nthe parameterized complexity of the problem, prove W[1]-hardness results and\npresent an FPT algorithm. \n\n"}
{"id": "1810.12030", "contents": "Title: Simon's problem for linear functions Abstract: Simon's problem asks the following: determine if a function $f: \\{0,1\\}^n\n\\rightarrow \\{0,1\\}^n$ is one-to-one or if there exists a unique $s \\in\n\\{0,1\\}^n$ such that $f(x) = f(x \\oplus s)$ for all $x \\in \\{0,1\\}^n$, given\nthe promise that exactly one of the two holds. A classical algorithm that can\nsolve this problem for every $f$ requires $2^{\\Omega(n)}$ queries to $f$. Simon\nshowed that there is a quantum algorithm that can solve this promise problem\nfor every $f$ using only $\\mathcal O(n)$ quantum queries to $f$. A matching\nlower bound on the number of quantum queries was given by Koiran et al., even\nfor functions $f: {\\mathbb{F}_p^n} \\to {\\mathbb{F}_p^n}$. We give a short proof\nthat $\\mathcal O(n)$ quantum queries is optimal even when we are additionally\npromised that $f$ is linear. This is somewhat surprising because for linear\nfunctions there even exists a classical $n$-query algorithm. \n\n"}
{"id": "1810.12792", "contents": "Title: Average-Case Quantum Advantage with Shallow Circuits Abstract: Recently Bravyi, Gosset and K\\\"onig (Science 2018) proved an unconditional\nseparation between the computational powers of small-depth quantum and\nclassical circuits for a relation. In this paper we show a similar separation\nin the average-case setting that gives stronger evidence of the superiority of\nsmall-depth quantum computation: we construct a computational task that can be\nsolved on all inputs by a quantum circuit of constant depth with bounded-fanin\ngates (a \"shallow\" quantum circuit) and show that any classical circuit with\nbounded-fanin gates solving this problem on a non-negligible fraction of the\ninputs must have logarithmic depth. Our results are obtained by introducing a\ntechnique to create quantum states exhibiting global quantum correlations from\nany graph, via a construction that we call the \\emph{extended graph}.\n  Similar results have been very recently (and independently) obtained by\nCoudron, Stark and Vidick (arXiv:1810.04233), and Bene Watts, Kothari,\nSchaeffer and Tal (STOC 2019). \n\n"}
{"id": "1810.13351", "contents": "Title: Stochastic Submodular Cover with Limited Adaptivity Abstract: In the submodular cover problem, we are given a non-negative monotone\nsubmodular function $f$ over a ground set $E$ of items, and the goal is to\nchoose a smallest subset $S \\subseteq E$ such that $f(S) = Q$ where $Q = f(E)$.\nIn the stochastic version of the problem, we are given $m$ stochastic items\nwhich are different random variables that independently realize to some item in\n$E$, and the goal is to find a smallest set of stochastic items whose\nrealization $R$ satisfies $f(R) = Q$. The problem captures as a special case\nthe stochastic set cover problem and more generally, stochastic covering\ninteger programs.\n  We define an $r$-round adaptive algorithm to be an algorithm that chooses a\npermutation of all available items in each round $k \\in [r]$, and a threshold\n$\\tau_k$, and realizes items in the order specified by the permutation until\nthe function value is at least $\\tau_k$. The permutation for each round $k$ is\nchosen adaptively based on the realization in the previous rounds, but the\nordering inside each round remains fixed regardless of the realizations seen\ninside the round. Our main result is that for any integer $r$, there exists a\npoly-time $r$-round adaptive algorithm for stochastic submodular cover whose\nexpected cost is $\\tilde{O}(Q^{{1}/{r}})$ times the expected cost of a fully\nadaptive algorithm. Prior to our work, such a result was not known even for the\ncase of $r=1$ and when $f$ is the coverage function. On the other hand, we show\nthat for any $r$, there exist instances of the stochastic submodular cover\nproblem where no $r$-round adaptive algorithm can achieve better than\n$\\Omega(Q^{{1}/{r}})$ approximation to the expected cost of a fully adaptive\nalgorithm. Our lower bound result holds even for coverage function and for\nalgorithms with unbounded computational power. \n\n"}
{"id": "1811.00955", "contents": "Title: Local search breaks 1.75 for Graph Balancing Abstract: Graph Balancing is the problem of orienting the edges of a weighted\nmultigraph so as to minimize the maximum weighted in-degree. Since the\nintroduction of the problem the best algorithm known achieves an approximation\nratio of $1.75$ and it is based on rounding a linear program with this exact\nintegrality gap. It is also known that there is no $(1.5 -\n\\epsilon)$-approximation algorithm, unless $\\mathrm{P}=\\mathrm{NP}$. Can we do\nbetter than $1.75$? We prove that a different LP formulation, the configuration\nLP, has a strictly smaller integrality gap. Graph Balancing was the last one in\na group of related problems from literature, for which it was open whether the\nconfiguration LP is stronger than previous, simple LP relaxations. We base our\nproof on a local search approach that has been applied successfully to the more\ngeneral Restricted Assignment problem, which in turn is a prominent special\ncase of makespan minimization on unrelated machines. With a number of technical\nnovelties we are able to obtain a bound of $1.749$ for the case of Graph\nBalancing. It is not clear whether the local search algorithm we present\nterminates in polynomial time, which means that the bound is non-constructive.\nHowever, it is a strong evidence that a better approximation algorithm is\npossible using the configuration LP and it allows the optimum to be estimated\nwithin a factor better than $1.75$. A particularly interesting aspect of our\ntechniques is the way we handle small edges in the local search. We manage to\nexploit the configuration constraints enforced on small edges in the LP. This\nmay be of interest to other problems such as Restricted Assignment as well. \n\n"}
{"id": "1811.01077", "contents": "Title: Dynamic Pricing (and Assortment) under a Static Calendar Abstract: This work is motivated by our collaboration with a large consumer packaged\ngoods (CPG) company. We have found that while the company appreciates the\nadvantages of dynamic pricing, they deem it operationally much easier to plan\nout a static price calendar in advance.\n  We investigate the efficacy of static control policies for revenue management\nproblems whose optimal solution is inherently dynamic. In these problems, a\nfirm has limited inventory to sell over a finite time horizon, over which\nheterogeneous customers stochastically arrive. We consider both pricing and\nassortment controls, and derive simple static policies in the form of a price\ncalendar or a planned sequence of assortments, respectively. In the assortment\nplanning problem, we also differentiate between the static vs. dynamic\nsubstitution models of customer demand. We show that our policies are within\n1-1/e (approximately 0.63) of the optimum under stationary (IID) demand, and\n1/2 of the optimum under non-stationary demand, with both guarantees\napproaching 1 if the starting inventories are large.\n  We adapt the technique of prophet inequalities from optimal stopping theory\nto pricing and assortment problems, and our results are relative to the linear\nprogramming relaxation. Under the special case of IID single-item pricing, our\nresults improve the understanding of irregular and discrete demand curves, by\nshowing that a static calendar can be (1-1/e)-approximate if the prices are\nsorted high-to-low.\n  Finally, we demonstrate on both data from the CPG company and synthetic data\nfrom the literature that our simple price and assortment calendars are\neffective. \n\n"}
{"id": "1811.01220", "contents": "Title: Sharp worst-case evaluation complexity bounds for arbitrary-order\n  nonconvex optimization with inexpensive constraints Abstract: We provide sharp worst-case evaluation complexity bounds for nonconvex\nminimization problems with general inexpensive constraints, i.e.\\ problems\nwhere the cost of evaluating/enforcing of the (possibly nonconvex or even\ndisconnected) constraints, if any, is negligible compared to that of evaluating\nthe objective function. These bounds unify, extend or improve all known upper\nand lower complexity bounds for unconstrained and convexly-constrained\nproblems. It is shown that, given an accuracy level $\\epsilon$, a degree of\nhighest available Lipschitz continuous derivatives $p$ and a desired optimality\norder $q$ between one and $p$, a conceptual regularization algorithm requires\nno more than $O(\\epsilon^{-\\frac{p+1}{p-q+1}})$ evaluations of the objective\nfunction and its derivatives to compute a suitably approximate $q$-th order\nminimizer. With an appropriate choice of the regularization, a similar result\nalso holds if the $p$-th derivative is merely H\\\"older rather than Lipschitz\ncontinuous. We provide an example that shows that the above complexity bound is\nsharp for unconstrained and a wide class of constrained problems, we also give\nreasons for the optimality of regularization methods from a worst-case\ncomplexity point of view, within a large class of algorithms that use the same\nderivative information. \n\n"}
{"id": "1811.01226", "contents": "Title: Multidimensional segment trees can do range updates in poly-logarithmic\n  time Abstract: Updating and querying on a range is a classical algorithmic problem with a\nmultitude of applications. The Segment Tree data structure is particularly\nnotable in handling the range query and update operations. A Segment Tree\ndivides the range into disjoint segments and merges them together to perform\nrange queries and range updates elegantly. Although this data structure is\nremarkably potent for 1-dimensional problems, it falls short in higher\ndimensions. Lazy Propagation enables the operations to be computed in $O(logn)$\ntime in a single dimension. However, the concept of lazy propagation could not\nbe translated to higher-dimensional cases, which imposes a time complexity of\n$O(n^{k-1} \\; logn)$ for operations on $k$-dimensional data. In this work, we\nhave made an attempt to emulate the idea of lazy propagation differently so\nthat it can be applied for 2-dimensional cases. Moreover, the proposed\nmodification should be capable of performing most general aggregate functions\nsimilar to the original Segment Tree, and can also be extended to even higher\ndimensions. Our proposed algorithm manages to perform range sum queries and\nupdates in $O(\\log^2 n)$ time for a 2-dimensional problem, which becomes\n$O(\\log^d n)$ for a $d$-dimensional situation. \n\n"}
{"id": "1811.01248", "contents": "Title: Compressed Multiple Pattern Matching Abstract: Given $d$ strings over the alphabet $\\{0,1,\\ldots,\\sigma{-}1\\}$, the\nclassical Aho--Corasick data structure allows us to find all $occ$ occurrences\nof the strings in any text $T$ in $O(|T| + occ)$ time using $O(m\\log m)$ bits\nof space, where $m$ is the number of edges in the trie containing the strings.\nFix any constant $\\varepsilon \\in (0, 2)$. We describe a compressed solution\nfor the problem that, provided $\\sigma \\le m^\\delta$ for a constant $\\delta <\n1$, works in $O(|T| \\frac{1}{\\varepsilon} \\log\\frac{1}{\\varepsilon} + occ)$\ntime, which is $O(|T| + occ)$ since $\\varepsilon$ is constant, and occupies\n$mH_k + 1.443 m + \\varepsilon m + O(d\\log\\frac{m}{d})$ bits of space, for all\n$0 \\le k \\le \\max\\{0,\\alpha\\log_\\sigma m - 2\\}$ simultaneously, where $\\alpha\n\\in (0,1)$ is an arbitrary constant and $H_k$ is the $k$th-order empirical\nentropy of the trie. Hence, we reduce the $3.443m$ term in the space bounds of\npreviously best succinct solutions to $(1.443 + \\varepsilon)m$, thus solving an\nopen problem posed by Belazzougui. Further, we notice that $L =\n\\log\\binom{\\sigma (m+1)}{m} - O(\\log(\\sigma m))$ is a worst-case space lower\nbound for any solution of the problem and, for $d = o(m)$ and constant\n$\\varepsilon$, our approach allows to achieve $L + \\varepsilon m$ bits of\nspace, which gives an evidence that, for $d = o(m)$, the space of our data\nstructure is theoretically optimal up to the $\\varepsilon m$ additive term and\nit is hardly possible to eliminate the term $1.443m$. In addition, we refine\nthe space analysis of previous works by proposing a more appropriate definition\nfor $H_k$. We also simplify the construction for practice adapting the fixed\nblock compression boosting technique, then implement our data structure, and\nconduct a number of experiments showing that it is comparable to the state of\nthe art in terms of time and is superior in space. \n\n"}
{"id": "1811.01259", "contents": "Title: QuickXsort - A Fast Sorting Scheme in Theory and Practice Abstract: QuickXsort is a highly efficient in-place sequential sorting scheme that\nmixes Hoare's Quicksort algorithm with X, where X can be chosen from a wider\nrange of other known sorting algorithms, like Heapsort, Insertionsort and\nMergesort. Its major advantage is that QuickXsort can be in-place even if X is\nnot. In this work we provide general transfer theorems expressing the number of\ncomparisons of QuickXsort in terms of the number of comparisons of X. More\nspecifically, if pivots are chosen as medians of (not too fast) growing size\nsamples, the average number of comparisons of QuickXsort and X differ only by\n$o(n)$-terms. For median-of-$k$ pivot selection for some constant $k$, the\ndifference is a linear term whose coefficient we compute precisely. For\ninstance, median-of-three QuickMergesort uses at most $n \\lg n - 0.8358n +\nO(\\log n)$ comparisons.\n  Furthermore, we examine the possibility of sorting base cases with some other\nalgorithm using even less comparisons. By doing so the average-case number of\ncomparisons can be reduced down to $n \\lg n- 1.4106n + o(n)$ for a remaining\ngap of only $0.0321n$ comparisons to the known lower bound (while using only\n$O(\\log n)$ additional space and $O(n \\log n)$ time overall).\n  Implementations of these sorting strategies show that the algorithms\nchallenge well-established library implementations like Musser's Introsort. \n\n"}
{"id": "1811.01885", "contents": "Title: Learning Two Layer Rectified Neural Networks in Polynomial Time Abstract: Consider the following fundamental learning problem: given input examples $x\n\\in \\mathbb{R}^d$ and their vector-valued labels, as defined by an underlying\ngenerative neural network, recover the weight matrices of this network. We\nconsider two-layer networks, mapping $\\mathbb{R}^d$ to $\\mathbb{R}^m$, with $k$\nnon-linear activation units $f(\\cdot)$, where $f(x) = \\max \\{x , 0\\}$ is the\nReLU. Such a network is specified by two weight matrices, $\\mathbf{U}^* \\in\n\\mathbb{R}^{m \\times k}, \\mathbf{V}^* \\in \\mathbb{R}^{k \\times d}$, such that\nthe label of an example $x \\in \\mathbb{R}^{d}$ is given by $\\mathbf{U}^*\nf(\\mathbf{V}^* x)$, where $f(\\cdot)$ is applied coordinate-wise. Given $n$\nsamples as a matrix $\\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and the (possibly\nnoisy) labels $\\mathbf{U}^* f(\\mathbf{V}^* \\mathbf{X}) + \\mathbf{E}$ of the\nnetwork on these samples, where $\\mathbf{E}$ is a noise matrix, our goal is to\nrecover the weight matrices $\\mathbf{U}^*$ and $\\mathbf{V}^*$.\n  In this work, we develop algorithms and hardness results under varying\nassumptions on the input and noise. Although the problem is NP-hard even for\n$k=2$, by assuming Gaussian marginals over the input $\\mathbf{X}$ we are able\nto develop polynomial time algorithms for the approximate recovery of\n$\\mathbf{U}^*$ and $\\mathbf{V}^*$. Perhaps surprisingly, in the noiseless case\nour algorithms recover $\\mathbf{U}^*,\\mathbf{V}^*$ exactly, i.e., with no\nerror. To the best of the our knowledge, this is the first algorithm to\naccomplish exact recovery. For the noisy case, we give the first polynomial\ntime algorithm that approximately recovers the weights in the presence of\nmean-zero noise $\\mathbf{E}$. Our algorithms generalize to a larger class of\nrectified activation functions, $f(x) = 0$ when $x\\leq 0$, and $f(x) > 0$\notherwise. \n\n"}
{"id": "1811.02944", "contents": "Title: Connecting Knowledge Compilation Classes and Width Parameters Abstract: The field of knowledge compilation establishes the tractability of many tasks\nby studying how to compile them to Boolean circuit classes obeying some\nrequirements such as structuredness, decomposability, and determinism. However,\nin other settings such as intensional query evaluation on databases, we obtain\nBoolean circuits that satisfy some width bounds, e.g., they have bounded\ntreewidth or pathwidth. In this work, we give a systematic picture of many\ncircuit classes considered in knowledge compilation and show how they can be\nsystematically connected to width measures, through upper and lower bounds. Our\nupper bounds show that bounded-treewidth circuits can be constructively\nconverted to d-SDNNFs, in time linear in the circuit size and singly\nexponential in the treewidth; and that bounded-pathwidth circuits can similarly\nbe converted to uOBDDs. We show matching lower bounds on the compilation of\nmonotone DNF or CNF formulas to structured targets, assuming a constant bound\non the arity (size of clauses) and degree (number of occurrences of each\nvariable): any d-SDNNF (resp., SDNNF) for such a DNF (resp., CNF) must be of\nexponential size in its treewidth, and the same holds for uOBDDs (resp.,\nn-OBDDs) when considering pathwidth. Unlike most previous work, our bounds\napply to any formula of this class, not just a well-chosen family. Hence, we\nshow that pathwidth and treewidth respectively characterize the efficiency of\ncompiling monotone DNFs to uOBDDs and d-SDNNFs with compilation being singly\nexponential in the corresponding width parameter. We also show that our lower\nbounds on CNFs extend to unstructured compilation targets, with an exponential\nlower bound in the treewidth (resp., pathwidth) when compiling monotone CNFs of\nconstant arity and degree to DNNFs (resp., nFBDDs). \n\n"}
{"id": "1811.03197", "contents": "Title: Private Continual Release of Real-Valued Data Streams Abstract: We present a differentially private mechanism to display statistics (e.g.,\nthe moving average) of a stream of real valued observations where the bound on\neach observation is either too conservative or unknown in advance. This is\nparticularly relevant to scenarios of real-time data monitoring and reporting,\ne.g., energy data through smart meters. Our focus is on real-world data streams\nwhose distribution is light-tailed, meaning that the tail approaches zero at\nleast as fast as the exponential distribution. For such data streams,\nindividual observations are expected to be concentrated below an unknown\nthreshold. Estimating this threshold from the data can potentially violate\nprivacy as it would reveal particular events tied to individuals [1]. On the\nother hand an overly conservative threshold may impact accuracy by adding more\nnoise than necessary. We construct a utility optimizing differentially private\nmechanism to release this threshold based on the input stream. Our main\nadvantage over the state-of-the-art algorithms is that the resulting noise\nadded to each observation of the stream is scaled to the threshold instead of a\npossibly much larger bound; resulting in considerable gain in utility when the\ndifference is significant. Using two real-world datasets, we demonstrate that\nour mechanism, on average, improves the utility by a factor of 3.5 on the first\ndataset, and 9 on the other. While our main focus is on continual release of\nstatistics, our mechanism for releasing the threshold can be used in various\nother applications where a (privacy-preserving) measure of the scale of the\ninput distribution is required. \n\n"}
{"id": "1811.03491", "contents": "Title: Degree-$d$ Chow Parameters Robustly Determine Degree-$d$ PTFs (and\n  Algorithmic Applications) Abstract: The degree-$d$ Chow parameters of a Boolean function $f: \\{-1,1\\}^n \\to\n\\mathbb{R}$ are its degree at most $d$ Fourier coefficients. It is well-known\nthat degree-$d$ Chow parameters uniquely characterize degree-$d$ polynomial\nthreshold functions (PTFs) within the space of all bounded functions. In this\npaper, we prove a robust version of this theorem: For $f$ any Boolean\ndegree-$d$ PTF and $g$ any bounded function, if the degree-$d$ Chow parameters\nof $f$ are close to the degree-$d$ Chow parameters of $g$ in $\\ell_2$-norm,\nthen $f$ is close to $g$ in $\\ell_1$-distance. Notably, our bound relating the\ntwo distances is completely independent of the dimension $n$. That is, we show\nthat Boolean degree-$d$ PTFs are {\\em robustly identifiable} from their\ndegree-$d$ Chow parameters. Results of this form had been shown for the $d=1$\ncase~\\cite{OS11:chow, DeDFS14}, but no non-trivial bound was previously known\nfor $d >1$.\n  Our robust identifiability result gives the following algorithmic\napplications: First, we show that Boolean degree-$d$ PTFs can be efficiently\napproximately reconstructed from approximations to their degree-$d$ Chow\nparameters. This immediately implies that degree-$d$ PTFs are efficiently\nlearnable in the uniform distribution $d$-RFA\nmodel~\\cite{BenDavidDichterman:98}. As a byproduct of our approach, we also\nobtain the first low integer-weight approximations of degree-$d$ PTFs, for\n$d>1$. As our second application, our robust identifiability result gives the\nfirst efficient algorithm, with dimension-independent error guarantees, for\nmalicious learning of Boolean degree-$d$ PTFs under the uniform distribution. \n\n"}
{"id": "1811.03841", "contents": "Title: Unique End of Potential Line Abstract: This paper studies the complexity of problems in PPAD $\\cap$ PLS that have\nunique solutions. Three well-known examples of such problems are the problem of\nfinding a fixpoint of a contraction map, finding the unique sink of a Unique\nSink Orientation (USO), and solving the P-matrix Linear Complementarity Problem\n(P-LCP). Each of these are promise-problems, and when the promise holds, they\nalways possess unique solutions.\n  We define the complexity class UEOPL to capture problems of this type. We\nfirst define a class that we call EOPL, which consists of all problems that can\nbe reduced to End-of-Potential-Line. This problem merges the canonical\nPPAD-complete problem End-of-Line, with the canonical PLS-complete problem\nSink-of-Dag, and so EOPL captures problems that can be solved by a\nline-following algorithm that also simultaneously decreases a potential\nfunction.\n  Promise-UEOPL is a promise-subclass of EOPL in which the line in the\nEnd-of-Potential-Line instance is guaranteed to be unique via a promise. We\nturn this into a non-promise class UEOPL, by adding an extra solution type to\nEOPL that captures any pair of points that are provably on two different lines.\n  We show that UEOPL $\\subseteq$ EOPL $\\subseteq$ CLS, and that all of our\nmotivating problems are contained in UEOPL: specifically USO, P-LCP, and\nfinding a fixpoint of a Piecewise-Linear Contraction under an $\\ell_p$-norm all\nlie in UEOPL. Our results also imply that parity games, mean-payoff games,\ndiscounted games, and simple-stochastic games lie in UEOPL.\n  All of our containment results are proved via a reduction to a problem that\nwe call One-Permutation Discrete Contraction (OPDC). This problem is motivated\nby a discretized version of contraction, but it is also closely related to the\nUSO problem. We show that OPDC lies in UEOPL, and we are also able to show that\nOPDC is UEOPL-complete. \n\n"}
{"id": "1811.04753", "contents": "Title: Sliding Window Temporal Graph Coloring Abstract: Graph coloring is one of the most famous computational problems with\napplications in a wide range of areas such as planning and scheduling, resource\nallocation, and pattern matching. So far coloring problems are mostly studied\non static graphs, which often stand in stark contrast to practice where data is\ninherently dynamic and subject to discrete changes over time. A temporal graph\nis a graph whose edges are assigned a set of integer time labels, indicating at\nwhich discrete time steps the edge is active. In this paper we present a\nnatural temporal extension of the classical graph coloring problem. Given a\ntemporal graph and a natural number $\\Delta$, we ask for a coloring sequence\nfor each vertex such that (i) in every sliding time window of $\\Delta$\nconsecutive time steps, in which an edge is active, this edge is properly\ncolored (i.e. its endpoints are assigned two different colors) at least once\nduring that time window, and (ii) the total number of different colors is\nminimized. This sliding window temporal coloring problem abstractly captures\nmany realistic graph coloring scenarios in which the underlying network changes\nover time, such as dynamically assigning communication channels to moving\nagents. We present a thorough investigation of the computational complexity of\nthis temporal coloring problem. More specifically, we prove strong\ncomputational hardness results, complemented by efficient exact and\napproximation algorithms. Some of our algorithms are linear-time\nfixed-parameter tractable with respect to appropriate parameters, while others\nare asymptotically almost optimal under the Exponential Time Hypothesis (ETH). \n\n"}
{"id": "1811.05511", "contents": "Title: Towards a Geometric Approach to Strassen's Asymptotic Rank Conjecture Abstract: We make a first geometric study of three varieties in $\\mathbb{C}^m \\otimes\n\\mathbb{C}^m \\otimes \\mathbb{C}^m$ (for each $m$), including the Zariski\nclosure of the set of tight tensors, the tensors with continuous regular\nsymmetry. Our motivation is to develop a geometric framework for Strassen's\nAsymptotic Rank Conjecture that the asymptotic rank of any tight tensor is\nminimal. In particular, we determine the dimension of the set of tight tensors.\nWe prove that this dimension equals the dimension of the set of oblique\ntensors, a less restrictive class introduced by Strassen. \n\n"}
{"id": "1811.05630", "contents": "Title: Memory-Efficient Quantum Circuit Simulation by Using Lossy Data\n  Compression Abstract: In order to evaluate, validate, and refine the design of new quantum\nalgorithms or quantum computers, researchers and developers need methods to\nassess their correctness and fidelity. This requires the capabilities of\nquantum circuit simulations. However, the number of quantum state amplitudes\nincreases exponentially with the number of qubits, leading to the exponential\ngrowth of the memory requirement for the simulations. In this work, we present\nour memory-efficient quantum circuit simulation by using lossy data\ncompression. Our empirical data shows that we reduce the memory requirement to\n16.5% and 2.24E-06 of the original requirement for QFT and Grover's search,\nrespectively. This finding further suggests that we can simulate deep quantum\ncircuits up to 63 qubits with 0.8 petabytes memory. \n\n"}
{"id": "1811.07821", "contents": "Title: Efficient random graph matching via degree profiles Abstract: Random graph matching refers to recovering the underlying vertex\ncorrespondence between two random graphs with correlated edges; a prominent\nexample is when the two random graphs are given by Erd\\H{o}s-R\\'{e}nyi graphs\n$G(n,\\frac{d}{n})$. This can be viewed as an average-case and noisy version of\nthe graph isomorphism problem. Under this model, the maximum likelihood\nestimator is equivalent to solving the intractable quadratic assignment\nproblem. This work develops an $\\tilde{O}(n d^2+n^2)$-time algorithm which\nperfectly recovers the true vertex correspondence with high probability,\nprovided that the average degree is at least $d = \\Omega(\\log^2 n)$ and the two\ngraphs differ by at most $\\delta = O( \\log^{-2}(n) )$ fraction of edges. For\ndense graphs and sparse graphs, this can be improved to $\\delta = O(\n\\log^{-2/3}(n) )$ and $\\delta = O( \\log^{-2}(d) )$ respectively, both in\npolynomial time. The methodology is based on appropriately chosen distance\nstatistics of the degree profiles (empirical distribution of the degrees of\nneighbors). Before this work, the best known result achieves $\\delta=O(1)$ and\n$n^{o(1)} \\leq d \\leq n^c$ for some constant $c$ with an $n^{O(\\log n)}$-time\nalgorithm \\cite{barak2018nearly} and $\\delta=\\tilde O((d/n)^4)$ and $d =\n\\tilde{\\Omega}(n^{4/5})$ with a polynomial-time algorithm\n\\cite{dai2018performance}. \n\n"}
{"id": "1811.08506", "contents": "Title: Tight Approximation Ratio for Minimum Maximal Matching Abstract: We study a combinatorial problem called Minimum Maximal Matching, where we\nare asked to find in a general graph the smallest that can not be extended. We\nshow that this problem is hard to approximate with a constant smaller than 2,\nassuming the Unique Games Conjecture.\n  As a corollary we show, that Minimum Maximal Matching in bipartite graphs is\nhard to approximate with constant smaller than $\\frac{4}{3}$, with the same\nassumption. With a stronger variant of the Unique Games Conjecture --- that is\nSmall Set Expansion Hypothesis --- we are able to improve the hardness result\nup to the factor of $\\frac{3}{2}$. \n\n"}
{"id": "1811.08539", "contents": "Title: Breaking symmetries to rescue Sum of Squares in the case of makespan\n  scheduling Abstract: The Sum of Squares (\\sos{}) hierarchy gives an automatized technique to\ncreate a family of increasingly tight convex relaxations for binary programs.\nThere are several problems for which a constant number of rounds of this\nhierarchy give integrality gaps matching the best known approximation\nalgorithms. For many other problems, however, ad-hoc techniques give better\napproximation ratios than \\sos{} in the worst case, as shown by corresponding\nlower bound instances. Notably, in many cases these instances are invariant\nunder the action of a large permutation group. This yields the question how\nsymmetries in a formulation degrade the performance of the relaxation obtained\nby the \\sos{} hierarchy. In this paper, we study this for the case of the\nminimum makespan problem on identical machines. Our first result is to show\nthat $\\Omega(n)$ rounds of \\sos{} applied over the \\emph{configuration linear\nprogram} yields an integrality gap of at least $1.0009$, where $n$ is the\nnumber of jobs. Our result is based on tools from representation theory of\nsymmetric groups. Then, we consider the weaker \\emph{assignment linear program}\nand add a well chosen set of symmetry breaking inequalities that removes a\nsubset of the machine permutation symmetries. We show that applying\n$2^{\\tilde{O}(1/\\varepsilon^2)}$ rounds of the SA hierarchy to this stronger\nlinear program reduces the integrality gap to $1+\\varepsilon$, which yields a\nlinear programming based polynomial time approximation scheme. Our results\nsuggest that for this classical problem, symmetries were the main barrier\npreventing the \\sos{}/ SA hierarchies to give relaxations of polynomial\ncomplexity with an integrality gap of~$1+\\varepsilon$. We leave as an open\nquestion whether this phenomenon occurs for other symmetric problems. \n\n"}
{"id": "1811.09136", "contents": "Title: REPT: A Streaming Algorithm of Approximating Global and Local Triangle\n  Counts in Parallel Abstract: Recently, considerable efforts have been devoted to approximately computing\nthe global and local (i.e., incident to each node) triangle counts of a large\ngraph stream represented as a sequence of edges. Existing approximate triangle\ncounting algorithms rely on sampling techniques to reduce the computational\ncost. However, their estimation errors are significantly determined by the\ncovariance between sampled triangles. Moreover, little attention has been paid\nto developing parallel one-pass streaming algorithms that can be used to fast\nand approximately count triangles on a multi-core machine or a cluster of\nmachines. To solve these problems, we develop a novel parallel method REPT to\nsignificantly reduce the covariance (even completely eliminate the covariance\nfor some cases) between sampled triangles. We theoretically prove that REPT is\nmore accurate than parallelizing existing triangle count estimation algorithms\nin a direct manner. In addition, we also conduct extensive experiments on a\nvariety of real-world graphs, and the results demonstrate that our method REPT\nis several times more accurate than state-of-the-art methods. \n\n"}
{"id": "1811.12017", "contents": "Title: An Equivalence Class for Orthogonal Vectors Abstract: The Orthogonal Vectors problem ($\\textsf{OV}$) asks: given $n$ vectors in\n$\\{0,1\\}^{O(\\log n)}$, are two of them orthogonal? $\\textsf{OV}$ is easily\nsolved in $O(n^2 \\log n)$ time, and it is a central problem in fine-grained\ncomplexity: dozens of conditional lower bounds are based on the popular\nhypothesis that $\\textsf{OV}$ cannot be solved in (say) $n^{1.99}$ time.\nHowever, unlike the APSP problem, few other problems are known to be\nnon-trivially equivalent to $\\textsf{OV}$.\n  We show $\\textsf{OV}$ is truly-subquadratic equivalent to several fundamental\nproblems, all of which (a priori) look harder than $\\textsf{OV}$. A partial\nlist is given below:\n  ($\\textsf{Min-IP}/\\textsf{Max-IP}$) Find a red-blue pair of vectors with\nminimum (respectively, maximum) inner product, among $n$ vectors in\n$\\{0,1\\}^{O(\\log n)}$.\n  ($\\textsf{Exact-IP}$) Find a red-blue pair of vectors with inner product\nequal to a given target integer, among $n$ vectors in $\\{0,1\\}^{O(\\log n)}$.\n  ($\\textsf{Apx-Min-IP}/\\textsf{Apx-Max-IP}$) Find a red-blue pair of vectors\nthat is a 100-approximation to the minimum (resp. maximum) inner product, among\n$n$ vectors in $\\{0,1\\}^{O(\\log n)}$.\n  (Approx. $\\textsf{Bichrom.-$\\ell_p$-Closest-Pair}$) Compute a $(1 +\n\\Omega(1))$-approximation to the $\\ell_p$-closest red-blue pair (for a constant\n$p \\in [1,2]$), among $n$ points in $\\mathbb{R}^d$, $d \\le n^{o(1)}$.\n  (Approx. $\\textsf{$\\ell_p$-Furthest-Pair}$) Compute a $(1 +\n\\Omega(1))$-approximation to the $\\ell_p$-furthest pair (for a constant $p \\in\n[1,2]$), among $n$ points in $\\mathbb{R}^d$, $d \\le n^{o(1)}$.\n  We also show that there is a $\\text{poly}(n)$ space, $n^{1-\\epsilon}$ query\ntime data structure for Partial Match with vectors from $\\{0,1\\}^{O(\\log n)}$\nif and only if such a data structure exists for $1+\\Omega(1)$ Approximate\nNearest Neighbor Search in Euclidean space. \n\n"}
{"id": "1811.12527", "contents": "Title: Algorithms and Hardness for Diameter in Dynamic Graphs Abstract: The diameter, radius and eccentricities are natural graph parameters. While\nthese problems have been studied extensively, there are no known dynamic\nalgorithms for them beyond the ones that follow from trivial recomputation\nafter each update or from solving dynamic All-Pairs Shortest Paths (APSP),\nwhich is very computationally intensive. This is the situation for dynamic\napproximation algorithms as well, and even if only edge insertions or edge\ndeletions need to be supported.\n  This paper provides a comprehensive study of the dynamic approximation of\nDiameter, Radius and Eccentricities, providing both conditional lower bounds,\nand new algorithms whose bounds are optimal under popular hypotheses in\nfine-grained complexity. Some of the highlights include:\n  - Under popular hardness hypotheses, there can be no significantly better\nfully dynamic approximation algorithms than recomputing the answer after each\nupdate, or maintaining full APSP.\n  - Nearly optimal partially dynamic (incremental/decremental) algorithms can\nbe achieved via efficient reductions to (incremental/decremental) maintenance\nof Single-Source Shortest Paths. For instance, a nearly\n$(3/2+\\epsilon)$-approximation to Diameter in directed or undirected graphs can\nbe maintained decrementally in total time $m^{1+o(1)}\\sqrt{n}/\\epsilon^2$. This\nnearly matches the static $3/2$-approximation algorithm for the problem that is\nknown to be conditionally optimal. \n\n"}
{"id": "1811.12547", "contents": "Title: The inverse Voronoi problem in graphs Abstract: We introduce the inverse Voronoi diagram problem in graphs: given a graph $G$\nwith positive edge-lengths and a collection $\\mathbb{U}$ of subsets of vertices\nof $V(G)$, decide whether $\\mathbb{U}$ is a Voronoi diagram in $G$ with respect\nto the shortest-path metric. We show that the problem is NP-hard, even for\nplanar graphs where all the edges have unit length. We also study the\nparameterized complexity of the problem and show that the problem is W[1]-hard\nwhen parameterized by the number of Voronoi cells or by the pathwidth of the\ngraph. For trees we show that the problem can be solved in $O(N+n \\log^2 n)$\ntime, where $n$ is the number of vertices in the tree and $N=n+\\sum_{U\\in\n\\mathbb{U}}|U|$ is the size of the description of the input. We also provide a\nlower bound of $\\Omega(n \\log n)$ time for trees with $n$ vertices. \n\n"}
{"id": "1812.00359", "contents": "Title: Locally Consistent Parsing for Text Indexing in Small Space Abstract: We consider two closely related problems of text indexing in a sub-linear\nworking space. The first problem is the Sparse Suffix Tree (SST) construction\nof a set of suffixes $B$ using only $O(|B|)$ words of space. The second problem\nis the Longest Common Extension (LCE) problem, where for some parameter\n$1\\le\\tau\\le n$, the goal is to construct a data structure that uses $O(\\frac\n{n}{\\tau})$ words of space and can compute the longest common prefix length of\nany pair of suffixes. We show how to use ideas based on the Locally Consistent\nParsing technique, that was introduced by Sahinalp and Vishkin [STOC '94], in\nsome non-trivial ways in order to improve the known results for the above\nproblems. We introduce new Las-Vegas and deterministic algorithms for both\nproblems.\n  We introduce the first Las-Vegas SST construction algorithm that takes $O(n)$\ntime. This is an improvement over the last result of Gawrychowski and Kociumaka\n[SODA '17] who obtained $O(n)$ time for Monte-Carlo algorithm, and\n$O(n\\sqrt{\\log |B|})$ time for Las-Vegas algorithm. In addition, we introduce a\nrandomized Las-Vegas construction for an LCE data structure that can be\nconstructed in linear time and answers queries in $O(\\tau)$ time.\n  For the deterministic algorithms, we introduce an SST construction algorithm\nthat takes $O(n\\log \\frac{n}{|B|})$ time (for $|B|=\\Omega(\\log n)$). This is\nthe first almost linear time, $O(n\\cdot poly\\log{n})$, deterministic SST\nconstruction algorithm, where all previous algorithms take at least\n$\\Omega\\left(\\min\\{n|B|,\\frac{n^2}{|B|}\\}\\right)$ time. For the LCE problem, we\nintroduce a data structure that answers LCE queries in $O(\\tau\\sqrt{\\log^*n})$\ntime, with $O(n\\log\\tau)$ construction time (for $\\tau=O(\\frac{n}{\\log n})$).\nThis data structure improves both query time and construction time upon the\nresults of Tanimura et al. [CPM '16]. \n\n"}
{"id": "1812.01482", "contents": "Title: Target Set Selection parameterized by vertex cover and more Abstract: Given a simple, undirected graph $G$ with a threshold function $\\tau:V(G)\n\\rightarrow \\mathbb{N}$, the \\textsc{Target Set Selection} (TSS) problem is\nabout choosing a minimum cardinality set, say $S \\subseteq V(G)$, such that\nstarting a diffusion process with $S$ as its seed set will eventually result in\nactivating all the nodes in $G$. For any non-negative integer $i$, we say a set\n$T\\subseteq V(G)$ is a \"degree-$i$ modulator\" of $G$ if the degree of any\nvertex in the graph $G-T$ is at most $i$. Degree-$0$ modulators of a graph are\nprecisely its vertex covers. Consider a graph $G$ on $n$ vertices and $m$\nedges. We have the following results on the TSS problem:\n  -> It was shown by Nichterlein et al. [Social Network Analysis and Mining,\n2013] that it is possible to compute an optimal-sized target set in\n$O(2^{(2^{t}+1)t}\\cdot m)$ time, where $t$ denotes the cardinality of a minimum\ndegree-$0$ modulator of $G$. We improve this result by designing an algorithm\nrunning in time $2^{O(t\\log t)}n^{O(1)}$.\n  -> We design a $2^{2^{O(t)}}n^{O(1)}$ time algorithm to compute an optimal\ntarget set for $G$, where $t$ is the size of a minimum degree-$1$ modulator of\n$G$. \n\n"}
{"id": "1812.01789", "contents": "Title: Hard combinatorial problems and minor embeddings on lattice graphs Abstract: Today, hardware constraints are an important limitation on quantum adiabatic\noptimization algorithms. Firstly, computational problems must be formulated as\nquadratic unconstrained binary optimization (QUBO) in the presence of noisy\ncoupling constants. Secondly, the interaction graph of the QUBO must have an\neffective minor embedding into a two-dimensional nonplanar lattice graph. We\ndescribe new strategies for constructing QUBOs for NP-complete/hard\ncombinatorial problems that address both of these challenges. Our results\ninclude asymptotically improved embeddings for number partitioning, filling\nknapsacks, graph coloring, and finding Hamiltonian cycles. These embeddings can\nbe also be found with reduced computational effort. Our new embedding for\nnumber partitioning may be more effective on next-generation hardware. \n\n"}
{"id": "1812.02570", "contents": "Title: Compression with wildcards: Abstract simplicial complexes Abstract: Despite the more handy terminology of abstract simplicial complexes SC, in\nits core this article is about antitone Boolean functions. Given the maximal\nfaces (=facets) of SC, our main algorithm, called Facets-To-Faces, outputs SC\nin a compressed format. The degree of compression of Facets-To-Faces, which is\nprogrammed in high-level Mathematica code, compares favorably to both the\nMathematica command BooleanConvert, and to the BDD's provided by Python. A\nnovel way to calculate the face-numbers from the facets is also presented. Both\nalgorithms can be parallelized and are applicable (e.g.) to reliability\nanalysis, combinatorial topology, and frequent set mining. \n\n"}
{"id": "1812.02696", "contents": "Title: Differentially Private Fair Learning Abstract: Motivated by settings in which predictive models may be required to be\nnon-discriminatory with respect to certain attributes (such as race), but even\ncollecting the sensitive attribute may be forbidden or restricted, we initiate\nthe study of fair learning under the constraint of differential privacy. We\ndesign two learning algorithms that simultaneously promise differential privacy\nand equalized odds, a 'fairness' condition that corresponds to equalizing false\npositive and negative rates across protected groups. Our first algorithm is a\nprivate implementation of the equalized odds post-processing approach of [Hardt\net al., 2016]. This algorithm is appealingly simple, but must be able to use\nprotected group membership explicitly at test time, which can be viewed as a\nform of 'disparate treatment'. Our second algorithm is a differentially private\nversion of the oracle-efficient in-processing approach of [Agarwal et al.,\n2018] that can be used to find the optimal fair classifier, given access to a\nsubroutine that can solve the original (not necessarily fair) learning problem.\nThis algorithm is more complex but need not have access to protected group\nmembership at test time. We identify new tradeoffs between fairness, accuracy,\nand privacy that emerge only when requiring all three properties, and show that\nthese tradeoffs can be milder if group membership may be used at test time. We\nconclude with a brief experimental evaluation. \n\n"}
{"id": "1812.07826", "contents": "Title: Two-stage Combinatorial Optimization Problems under Risk Abstract: In this paper a class of combinatorial optimization problems is discussed. It\nis assumed that a solution can be constructed in two stages. The current\nfirst-stage costs are precisely known, while the future second-stage costs are\nonly known to belong to an uncertainty set, which contains a finite number of\nscenarios with known probability distribution. A partial solution, chosen in\nthe first stage, can be completed by performing an optimal recourse action,\nafter the true second-stage scenario is revealed. A solution minimizing the\nConditional Value at Risk (CVaR) measure is computed. Since expectation and\nmaximum are boundary cases of CVaR, the model generalizes the traditional\nstochastic and robust two-stage approaches, previously discussed in the\nexisting literature. In this paper some new negative and positive results are\nprovided for basic combinatorial optimization problems such as the selection or\nnetwork problems. \n\n"}
{"id": "1812.08480", "contents": "Title: The Query Complexity of a Permutation-Based Variant of Mastermind Abstract: We study the query complexity of a permutation-based variant of the guessing\ngame Mastermind. In this variant, the secret is a pair $(z,\\pi)$ which consists\nof a binary string $z \\in \\{0,1\\}^n$ and a permutation $\\pi$ of $[n]$. The\nsecret must be unveiled by asking queries of the form $x \\in \\{0,1\\}^n$. For\neach such query, we are returned the score \\[ f_{z,\\pi}(x):= \\max \\{ i \\in\n[0..n]\\mid \\forall j \\leq i: z_{\\pi(j)} = x_{\\pi(j)}\\}\\,;\\] i.e., the score of\n$x$ is the length of the longest common prefix of $x$ and $z$ with respect to\nthe order imposed by $\\pi$. The goal is to minimize the number of queries\nneeded to identify $(z,\\pi)$. This problem originates from the study of\nblack-box optimization heuristics, where it is known as the\n\\textsc{LeadingOnes} problem.\n  In this work, we prove matching upper and lower bounds for the deterministic\nand randomized query complexity of this game, which are $\\Theta(n \\log n)$ and\n$\\Theta(n \\log \\log n)$, respectively. \n\n"}
{"id": "1812.08615", "contents": "Title: Temporal Matching Abstract: A link stream is a sequence of pairs of the form $(t,\\{u,v\\})$, where\n$t\\in\\mathbb N$ represents a time instant and $u\\neq v$. Given an integer\n$\\gamma$, the $\\gamma$-edge between vertices $u$ and $v$, starting at time $t$,\nis the set of temporally consecutive edges defined by $\\{(t',\\{u,v\\}) | t' \\in\n[t,t+\\gamma-1]\\}$. We introduce the notion of temporal matching of a link\nstream to be an independent $\\gamma$-edge set belonging to the link stream. We\nshow that the problem of computing a temporal matching of maximum size is\nNP-hard as soon as $\\gamma>1$. We depict a kernelization algorithm\nparameterized by the solution size for the problem. As a byproduct we also give\na $2$-approximation algorithm.\n  Both our $2$-approximation and kernelization algorithms are implemented and\nconfronted to link streams collected from real world graph data. We observe\nthat finding temporal matchings is a sensitive question when mining our data\nfrom such a perspective as: managing peer-working when any pair of peers $X$\nand $Y$ are to collaborate over a period of one month, at an average rate of at\nleast two email exchanges every week. We furthermore design a link stream\ngenerating process by mimicking the behaviour of a random moving group of\nparticles under natural simulation, and confront our algorithms to these\ngenerated instances of link streams. All the implementations are open source. \n\n"}
{"id": "1812.08731", "contents": "Title: Limits on the Universal Method for Matrix Multiplication Abstract: In this work, we prove limitations on the known methods for designing matrix\nmultiplication algorithms. Alman and Vassilevska Williams recently defined the\nUniversal Method, which substantially generalizes all the known approaches\nincluding Strassen's Laser Method and Cohn and Umans' Group Theoretic Method.\nWe prove concrete lower bounds on the algorithms one can design by applying the\nUniversal Method to many different tensors. Our proofs use new tools for upper\nbounding the asymptotic slice rank of a wide range of tensors. Our main result\nis that the Universal method applied to any Coppersmith-Winograd tensor $CW_q$\ncannot yield a bound on $\\omega$, the exponent of matrix multiplication, better\nthan $2.16805$. By comparison, it was previously only known that the weaker\n`Galactic Method' applied to $CW_q$ could not achieve an exponent of $2$.\n  We also study the Laser Method (which is, in principle, a highly special case\nof the Universal Method) and prove that it is \"complete\" for matrix\nmultiplication algorithms: when it applies to a tensor $T$, it achieves $\\omega\n= 2$ if and only if it is possible for the Universal method applied to $T$ to\nachieve $\\omega = 2$. Hence, the Laser Method, which was originally used as an\nalgorithmic tool, can also be seen as a lower bounding tool. For example, in\ntheir landmark paper, Coppersmith and Winograd achieved a bound of $\\omega \\leq\n2.376$, by applying the Laser Method to $CW_q$. By our result, the fact that\nthey did not achieve $\\omega=2$ implies a lower bound on the Universal Method\napplied to $CW_q$. Indeed, if it were possible for the Universal Method applied\nto $CW_q$ to achieve $\\omega=2$, then Coppersmith and Winograd's application of\nthe Laser Method would have achieved $\\omega=2$. \n\n"}
{"id": "1812.09428", "contents": "Title: Quantum query complexity of symmetric oracle problems Abstract: We study the query complexity of quantum learning problems in which the\noracles form a group $G$ of unitary matrices. In the simplest case, one wishes\nto identify the oracle, and we find a description of the optimal success\nprobability of a $t$-query quantum algorithm in terms of group characters. As\nan application, we show that $\\Omega(n)$ queries are required to identify a\nrandom permutation in $S_n$. More generally, suppose $H$ is a fixed subgroup of\nthe group $G$ of oracles, and given access to an oracle sampled uniformly from\n$G$, we want to learn which coset of $H$ the oracle belongs to. We call this\nproblem coset identification and it generalizes a number of well-known quantum\nalgorithms including the Bernstein-Vazirani problem, the van Dam problem and\nfinite field polynomial interpolation. We provide character-theoretic formulas\nfor the optimal success probability achieved by a $t$-query algorithm for this\nproblem. One application involves the Heisenberg group and provides a family of\nproblems depending on $n$ which require $n+1$ queries classically and only $1$\nquery quantumly. \n\n"}
{"id": "1812.09519", "contents": "Title: Enumeration on Trees with Tractable Combined Complexity and Efficient\n  Updates Abstract: We give an algorithm to enumerate the results on trees of monadic\nsecond-order (MSO) queries represented by nondeterministic tree automata. After\nlinear time preprocessing (in the input tree), we can enumerate answers with\nlinear delay (in each answer). We allow updates on the tree to take place at\nany time, and we can then restart the enumeration after logarithmic time in the\ntree. Further, all our combined complexities are polynomial in the automaton.\n  Our result follows our previous circuit-based enumeration algorithms based on\ndeterministic tree automata, and is also inspired by our earlier result on\nwords and nondeterministic sequential extended variable-set automata in the\ncontext of document spanners. We extend these results and combine them with a\nrecent tree balancing scheme by Niewerth, so that our enumeration structure\nsupports updates to the underlying tree in logarithmic time (with leaf\ninsertions, leaf deletions, and node relabelings). Our result implies that, for\nMSO queries with free first-order variables, we can enumerate the results with\nlinear preprocessing and constant-delay and update the underlying tree in\nlogarithmic time, which improves on several known results for words and trees.\n  Building on lower bounds from data structure research, we also show\nunconditionally that up to a doubly logarithmic factor the update time of our\nalgorithm is optimal. Thus, unlike other settings, there can be no algorithm\nwith constant update time. \n\n"}
{"id": "1812.09824", "contents": "Title: The Online Event-Detection Problem Abstract: Given a stream $S = (s_1, s_2, ..., s_N)$, a $\\phi$-heavy hitter is an item\n$s_i$ that occurs at least $\\phi N$ times in $S$. The problem of finding\nheavy-hitters has been extensively studied in the database literature. In this\npaper, we study a related problem. We say that there is a $\\phi$-event at time\n$t$ if $s_t$ occurs exactly $\\phi N$ times in $(s_1, s_2, ..., s_t)$. Thus, for\neach $\\phi$-heavy hitter there is a single $\\phi$-event which occurs when its\ncount reaches the reporting threshold $\\phi N$. We define the online\nevent-detection problem (OEDP) as: given $\\phi$ and a stream $S$, report all\n$\\phi$-events as soon as they occur.\n  Many real-world monitoring systems demand event detection where all events\nmust be reported (no false negatives), in a timely manner, with no non-events\nreported (no false positives), and a low reporting threshold. As a result, the\nOEDP requires a large amount of space (Omega(N) words) and is not solvable in\nthe streaming model or via standard sampling-based approaches.\n  Since OEDP requires large space, we focus on cache-efficient algorithms in\nthe external-memory model.\n  We provide algorithms for the OEDP that are within a log factor of optimal.\nOur algorithms are tunable: its parameters can be set to allow for a bounded\nfalse-positives and a bounded delay in reporting. None of our relaxations allow\nfalse negatives since reporting all events is a strict requirement of our\napplications. Finally, we show improved results when the count of items in the\ninput stream follows a power-law distribution. \n\n"}
{"id": "1812.09967", "contents": "Title: Sherali--Adams Strikes Back Abstract: Let $G$ be any $n$-vertex graph whose random walk matrix has its nontrivial\neigenvalues bounded in magnitude by $1/\\sqrt{\\Delta}$ (for example, a random\ngraph $G$ of average degree~$\\Theta(\\Delta)$ typically has this property). We\nshow that the $\\exp\\Big(c \\frac{\\log n}{\\log \\Delta}\\Big)$-round Sherali--Adams\nlinear programming hierarchy certifies that the maximum cut in such a~$G$ is at\nmost $50.1\\%$ (in fact, at most $\\tfrac12 + 2^{-\\Omega(c)}$). For example, in\nrandom graphs with $n^{1.01}$ edges, $O(1)$ rounds suffice; in random graphs\nwith $n \\cdot \\text{polylog}(n)$ edges, $n^{O(1/\\log \\log n)} = n^{o(1)}$\nrounds suffice.\n  Our results stand in contrast to the conventional beliefs that linear\nprogramming hierarchies perform poorly for \\maxcut and other CSPs, and that\neigenvalue/SDP methods are needed for effective refutation. Indeed, our results\nimply that constant-round Sherali--Adams can strongly refute random Boolean\n$k$-CSP instances with $n^{\\lceil k/2 \\rceil + \\delta}$ constraints; previously\nthis had only been done with spectral algorithms or the SOS SDP hierarchy. \n\n"}
{"id": "1901.01861", "contents": "Title: On the Parameterized Complexity of $k$-Edge Colouring Abstract: For every fixed integer $k \\geq 1$, we prove that $k$-Edge Colouring is\nfixed-parameter-tractable when parameterized by the number of vertices of\nmaximum degree. \n\n"}
{"id": "1901.02871", "contents": "Title: The Lingering of Gradients: Theory and Applications Abstract: Classically, the time complexity of a first-order method is estimated by its\nnumber of gradient computations. In this paper, we study a more refined\ncomplexity by taking into account the `lingering' of gradients: once a gradient\nis computed at $x_k$, the additional time to compute gradients at\n$x_{k+1},x_{k+2},\\dots$ may be reduced.\n  We show how this improves the running time of several first-order methods.\nFor instance, if the `additional time' scales linearly with respect to the\ntraveled distance, then the `convergence rate' of gradient descent can be\nimproved from $1/T$ to $\\exp(-T^{1/3})$. On the application side, we solve a\nhypothetical revenue management problem on the Yahoo! Front Page Today Module\nwith 4.6m users to $10^{-6}$ error using only 6 passes of the dataset; and\nsolve a real-life support vector machine problem to an accuracy that is two\norders of magnitude better comparing to the state-of-the-art algorithm. \n\n"}
{"id": "1901.06482", "contents": "Title: On Efficient Optimal Transport: An Analysis of Greedy and Accelerated\n  Mirror Descent Algorithms Abstract: We provide theoretical analyses for two algorithms that solve the regularized\noptimal transport (OT) problem between two discrete probability measures with\nat most $n$ atoms. We show that a greedy variant of the classical Sinkhorn\nalgorithm, known as the \\emph{Greenkhorn algorithm}, can be improved to\n$\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-2})$, improving on the best known\ncomplexity bound of $\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-3})$. Notably,\nthis matches the best known complexity bound for the Sinkhorn algorithm and\nhelps explain why the Greenkhorn algorithm can outperform the Sinkhorn\nalgorithm in practice. Our proof technique, which is based on a primal-dual\nformulation and a novel upper bound for the dual solution, also leads to a new\nclass of algorithms that we refer to as \\emph{adaptive primal-dual accelerated\nmirror descent} (APDAMD) algorithms. We prove that the complexity of these\nalgorithms is $\\widetilde{\\mathcal{O}}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$,\nwhere $\\delta > 0$ refers to the inverse of the strong convexity module of\nBregman divergence with respect to $\\|\\cdot\\|_\\infty$. This implies that the\nAPDAMD algorithm is faster than the Sinkhorn and Greenkhorn algorithms in terms\nof $\\varepsilon$. Experimental results on synthetic and real datasets\ndemonstrate the favorable performance of the Greenkhorn and APDAMD algorithms\nin practice. \n\n"}
{"id": "1901.06628", "contents": "Title: Efficiently factoring polynomials modulo $p^4$ Abstract: Polynomial factoring has famous practical algorithms over fields-- finite,\nrational \\& $p$-adic. However, modulo prime powers it gets hard as there is\nnon-unique factorization and a combinatorial blowup ensues. For example, $x^2+p\n\\bmod p^2$ is irreducible, but $x^2+px \\bmod p^2$ has exponentially many\nfactors! We present the first randomized poly(deg $f, \\log p$) time algorithm\nto factor a given univariate integral $f(x)$ modulo $p^k$, for a prime $p$ and\n$k \\leq 4$. Thus, we solve the open question of factoring modulo $p^3$ posed in\n(Sircana, ISSAC'17).\n  Our method reduces the general problem of factoring $f(x) \\bmod p^k$ to that\nof {\\em root finding} in a related polynomial $E(y) \\bmod\\langle p^k,\n\\varphi(x)^\\ell \\rangle$ for some irreducible $\\varphi \\bmod p$. We could\nefficiently solve the latter for $k\\le4$, by incrementally transforming $E(y)$.\nMoreover, we discover an efficient and strong generalization of Hensel lifting\nto lift factors of $f(x) \\bmod p$ to those $\\bmod\\ p^4$ (if possible). This was\npreviously unknown, as the case of repeated factors of $f(x) \\bmod p$ forbids\nclassical Hensel lifting. \n\n"}
{"id": "1901.06702", "contents": "Title: Deterministic constructions of high-dimensional sets with small\n  dispersion Abstract: The dispersion of a point set $P\\subset[0,1]^d$ is the volume of the largest\nbox with sides parallel to the coordinate axes, which does not intersect $P$.\nHere, we show a construction of low-dispersion point sets, which can be deduced\nfrom solutions of certain $k$-restriction problems, which are well-known in\ncoding theory.\n  It was observed only recently that, for any $\\varepsilon>0$, certain\nrandomized constructions provide point sets with dispersion smaller than\n$\\varepsilon$ and number of elements growing only logarithmically in $d$. Based\non deep results from coding theory, we present explicit, deterministic\nalgorithms to construct such point sets in time that is only polynomial in $d$.\nNote that, however, the running-time will be super-exponential in\n$\\varepsilon^{-1}$. \n\n"}
{"id": "1901.08575", "contents": "Title: Deterministic 2-Dimensional Temperature-1 Tile Assembly Systems Cannot\n  Compute Abstract: We consider non cooperative binding in so called `temperature 1', in\ndeterministic (here called {\\it confluent}) tile self-assembly systems (1-TAS)\nand prove the standing conjecture that such systems do not have universal\ncomputational power. We call a TAS whose maximal assemblies contain at least\none ultimately periodic assembly path {\\it para-periodic}. We observe that a\nconfluent 1-TAS has at most one maximal producible assembly, $\\alpha_{max}$,\nthat can be considered a union of path assemblies, and we show that such a\nsystem is always para-periodic. This result is obtained through a superposition\nand a combination of two paths that produce a new path with desired properties,\na technique that we call \\emph{co-grow} of two paths. Moreover we provide a\ncharacterization of an $\\alpha_{max}$ of a confluent 1-TAS as one of two\npossible cases, so called, a grid or a disjoint union of combs. To a given\n$\\alpha_{max}$ we can associate a finite labeled graph, called \\emph{quipu},\nsuch that the union of all labels of paths in the quipu equals $\\alpha_{max}$,\ntherefore giving a finite description for $\\alpha_{max}$. This finite\ndescription implies that $\\alpha_{max}$ is a union of semi-affine subsets of\n$\\mathbb{Z}^2$ and since such a finite description can be algorithmicly\ngenerated from any 1-TAS, 1-TAS cannot have universal computational power. \n\n"}
{"id": "1901.09515", "contents": "Title: Black Box Submodular Maximization: Discrete and Continuous Settings Abstract: In this paper, we consider the problem of black box continuous submodular\nmaximization where we only have access to the function values and no\ninformation about the derivatives is provided. For a monotone and continuous\nDR-submodular function, and subject to a bounded convex body constraint, we\npropose Black-box Continuous Greedy, a derivative-free algorithm that provably\nachieves the tight $[(1-1/e)OPT-\\epsilon]$ approximation guarantee with\n$O(d/\\epsilon^3)$ function evaluations. We then extend our result to the\nstochastic setting where function values are subject to stochastic zero-mean\nnoise. It is through this stochastic generalization that we revisit the\ndiscrete submodular maximization problem and use the multi-linear extension as\na bridge between discrete and continuous settings. Finally, we extensively\nevaluate the performance of our algorithm on continuous and discrete submodular\nobjective functions using both synthetic and real data. \n\n"}
{"id": "1901.09863", "contents": "Title: Efficient Multiparty Interactive Coding for Insertions, Deletions and\n  Substitutions Abstract: In the field of interactive coding, two or more parties wish to carry out a\ndistributed computation over a communication network that may be noisy. The\nultimate goal is to develop efficient coding schemes that can tolerate a high\nlevel of noise while increasing the communication by only a constant factor\n(i.e., constant rate).\n  In this work we consider synchronous communication networks over an arbitrary\ntopology, in the powerful adversarial insertion-deletion noise model. Namely,\nthe noisy channel may adversarially alter the content of any transmitted\nsymbol, as well as completely remove a transmitted symbol or inject a new\nsymbol into the channel. We provide efficient, constant rate schemes that\nsuccessfully conduct any computation with high probability as long as the\nadversary corrupts at most $\\varepsilon /m$ fraction of the total\ncommunication, where $m$ is the number of links in the network and\n$\\varepsilon$ is a small constant. This scheme assumes the parties share a\nrandom string to which the adversarial noise is oblivious. We can remove this\nassumption at the price of being resilient to $\\varepsilon / (m\\log m)$\nadversarial error.\n  While previous work considered the insertion-deletion noise model in the\ntwo-party setting, to the best of our knowledge, our scheme is the first\nmultiparty scheme that is resilient to insertions and deletions. Furthermore,\nour scheme is the first computationally efficient scheme in the multiparty\nsetting that is resilient to adversarial noise. \n\n"}
{"id": "1901.10084", "contents": "Title: A Parallel Projection Method for Metric Constrained Optimization Abstract: Many clustering applications in machine learning and data mining rely on\nsolving metric-constrained optimization problems. These problems are\ncharacterized by $O(n^3)$ constraints that enforce triangle inequalities on\ndistance variables associated with $n$ objects in a large dataset. Despite its\nusefulness, metric-constrained optimization is challenging in practice due to\nthe cubic number of constraints and the high-memory requirements of standard\noptimization software. Recent work has shown that iterative projection methods\nare able to solve metric-constrained optimization problems on a much larger\nscale than was previously possible, thanks to their comparatively low memory\nrequirement. However, the major limitation of projection methods is their slow\nconvergence rate. In this paper we present a parallel projection method for\nmetric-constrained optimization which allows us to speed up the convergence\nrate in practice. The key to our approach is a new parallel execution schedule\nthat allows us to perform projections at multiple metric constraints\nsimultaneously without any conflicts or locking of variables. We illustrate the\neffectiveness of this execution schedule by implementing and testing a parallel\nprojection method for solving the metric-constrained linear programming\nrelaxation of correlation clustering. We show numerous experimental results on\nproblems involving up to 2.9 trillion constraints. \n\n"}
{"id": "1901.10789", "contents": "Title: Optimal Minimal Margin Maximization with Boosting Abstract: Boosting algorithms produce a classifier by iteratively combining base\nhypotheses. It has been observed experimentally that the generalization error\nkeeps improving even after achieving zero training error. One popular\nexplanation attributes this to improvements in margins. A common goal in a long\nline of research, is to maximize the smallest margin using as few base\nhypotheses as possible, culminating with the AdaBoostV algorithm by (R{\\\"a}tsch\nand Warmuth [JMLR'04]). The AdaBoostV algorithm was later conjectured to yield\nan optimal trade-off between number of hypotheses trained and the minimal\nmargin over all training points (Nie et al. [JMLR'13]). Our main contribution\nis a new algorithm refuting this conjecture. Furthermore, we prove a lower\nbound which implies that our new algorithm is optimal. \n\n"}
{"id": "1901.11343", "contents": "Title: Algorithmic counting of nonequivalent compact Huffman codes Abstract: It is known that the following five counting problems lead to the same\ninteger sequence~$f_t(n)$: the number of nonequivalent compact Huffman codes of\nlength~$n$ over an alphabet of $t$ letters, the number of `nonequivalent'\ncanonical rooted $t$-ary trees (level-greedy trees) with $n$~leaves, the number\nof `proper' words, the number of bounded degree sequences, and the number of\nways of writing $1= \\frac{1}{t^{x_1}}+ \\dots + \\frac{1}{t^{x_n}}$ with integers\n$0 \\leq x_1 \\leq x_2 \\leq \\dots \\leq x_n$. In this work, we show that one can\ncompute this sequence for \\textbf{all} $n<N$ with essentially one power series\ndivision. In total we need at most $N^{1+\\varepsilon}$ additions and\nmultiplications of integers of $cN$ bits, $c<1$, or $N^{2+\\varepsilon}$ bit\noperations, respectively. This improves an earlier bound by Even and Lempel who\nneeded $O(N^3)$ operations in the integer ring or $O(N^4)$ bit operations,\nrespectively. \n\n"}
{"id": "cond-mat/0212451", "contents": "Title: Constraint Satisfaction by Survey Propagation Abstract: Survey Propagation is an algorithm designed for solving typical instances of\nrandom constraint satisfiability problems. It has been successfully tested on\nrandom 3-SAT and random $G(n,\\frac{c}{n})$ graph 3-coloring, in the hard region\nof the parameter space. Here we provide a generic formalism which applies to a\nwide class of discrete Constraint Satisfaction Problems. \n\n"}
{"id": "cond-mat/0312483", "contents": "Title: Survey Propagation as local equilibrium equations Abstract: It has been shown experimentally that a decimation algorithm based on Survey\nPropagation (SP) equations allows to solve efficiently some combinatorial\nproblems over random graphs. We show that these equations can be derived as\nsum-product equations for the computation of marginals in an extended space\nwhere the variables are allowed to take an additional value -- $*$ -- when they\nare not forced by the combinatorial constraints. An appropriate ``local\nequilibrium condition'' cost/energy function is introduced and its entropy is\nshown to coincide with the expected logarithm of the number of clusters of\nsolutions as computed by SP. These results may help to clarify the geometrical\nnotion of clusters assumed by SP for the random K-SAT or random graph coloring\n(where it is conjectured to be exact) and helps to explain which kind of\nclustering operation or approximation is enforced in general/small sized models\nin which it is known to be inexact. \n\n"}
{"id": "cs/0005032", "contents": "Title: Computational Complexity and Phase Transitions Abstract: Phase transitions in combinatorial problems have recently been shown to be\nuseful in locating \"hard\" instances of combinatorial problems. The connection\nbetween computational complexity and the existence of phase transitions has\nbeen addressed in Statistical Mechanics and Artificial Intelligence, but not\nstudied rigorously.\n  We take a step in this direction by investigating the existence of sharp\nthresholds for the class of generalized satisfiability problems defined by\nSchaefer. In the case when all constraints are clauses we give a complete\ncharacterization of such problems that have a sharp threshold.\n  While NP-completeness does not imply (even in this restricted case) the\nexistence of a sharp threshold, it \"almost implies\" this, since clausal\ngeneralized satisfiability problems that lack a sharp threshold are either\n  1. polynomial time solvable, or\n  2. predicted, with success probability lower bounded by some positive\nconstant by across all the probability range, by a single, trivial procedure. \n\n"}
{"id": "cs/0007029", "contents": "Title: Dimension-Dependent behavior in the satisfability of random k-Horn\n  formulae Abstract: We determine the asymptotical satisfiability probability of a random\nat-most-k-Horn formula, via a probabilistic analysis of a simple version,\ncalled PUR, of positive unit resolution. We show that for k=k(n)->oo the\nproblem can be ``reduced'' to the case k(n)=n, that was solved in\ncs.DS/9912001. On the other hand, in the case k= a constant the behavior of PUR\nis modeled by a simple queuing chain, leading to a closed-form solution when\nk=2. Our analysis predicts an ``easy-hard-easy'' pattern in this latter case.\nUnder a rescaled parameter, the graphs of satisfaction probability\ncorresponding to finite values of k converge to the one for the uniform case, a\n``dimension-dependent behavior'' similar to the one found experimentally by\nKirkpatrick and Selman (Science'94) for k-SAT. The phenomenon is qualitatively\nexplained by a threshold property for the number of iterations of PUR makes on\nrandom satisfiable Horn formulas. \n\n"}
{"id": "cs/0009006", "contents": "Title: Improved Algorithms for 3-Coloring, 3-Edge-Coloring, and Constraint\n  Satisfaction Abstract: We consider worst case time bounds for NP-complete problems including 3-SAT,\n3-coloring, 3-edge-coloring, and 3-list-coloring. Our algorithms are based on a\nconstraint satisfaction (CSP) formulation of these problems; 3-SAT is\nequivalent to (2,3)-CSP while the other problems above are special cases of\n(3,2)-CSP. We give a fast algorithm for (3,2)-CSP and use it to improve the\ntime bounds for solving the other problems listed above. Our techniques involve\na mixture of Davis-Putnam-style backtracking with more sophisticated matching\nand network flow based ideas. \n\n"}
{"id": "cs/0012017", "contents": "Title: Towards Robust Quantum Computation Abstract: Quantum computation is a subject of much theoretical promise, but has not\nbeen realized in large scale, despite the discovery of fault-tolerant\nprocedures to overcome decoherence. Part of the reason is that the\ntheoretically modest requirements still present daunting experimental\nchallenges. The goal of this Dissertation is to reduce various resources\nrequired for robust quantum computation, focusing on quantum error correcting\ncodes and solution NMR quantum computation. A variety of techniques have been\ndeveloped, including high rate quantum codes for amplitude damping, relaxed\ncriteria for quantum error correction, systematic construction of\nfault-tolerant gates, recipes for quantum process tomography, techniques in\nbulk thermal state computation, and efficient decoupling techniques to\nimplement selective coupled logic gates. A detailed experimental study of a\nquantum error correcting code in NMR is also presented. The Dissertation\nclarifies and extends results previously reported in quant-ph/9610043,\nquant-ph/9704002, quant-ph/9811068, quant-ph/9904100, quant-ph/9906112,\nquant-ph/0002039. Additionally, a procedure for quantum process tomography\nusing maximally entangled states, and a review on NMR quantum computation are\nincluded. \n\n"}
{"id": "cs/0102018", "contents": "Title: An effective Procedure for Speeding up Algorithms Abstract: The provably asymptotically fastest algorithm within a factor of 5 for\nformally described problems will be constructed. The main idea is to enumerate\nall programs provably equivalent to the original problem by enumerating all\nproofs. The algorithm could be interpreted as a generalization and improvement\nof Levin search, which is, within a multiplicative constant, the fastest\nalgorithm for inverting functions. Blum's speed-up theorem is avoided by taking\ninto account only programs for which a correctness proof exists. Furthermore,\nit is shown that the fastest program that computes a certain function is also\none of the shortest programs provably computing this function. To quantify this\nstatement, the definition of Kolmogorov complexity is extended, and two new\nnatural measures for the complexity of a function are defined. \n\n"}
{"id": "cs/0204037", "contents": "Title: Kolmogorov's Structure Functions and Model Selection Abstract: In 1974 Kolmogorov proposed a non-probabilistic approach to statistics and\nmodel selection. Let data be finite binary strings and models be finite sets of\nbinary strings. Consider model classes consisting of models of given maximal\n(Kolmogorov) complexity. The ``structure function'' of the given data expresses\nthe relation between the complexity level constraint on a model class and the\nleast log-cardinality of a model in the class containing the data. We show that\nthe structure function determines all stochastic properties of the data: for\nevery constrained model class it determines the individual best-fitting model\nin the class irrespective of whether the ``true'' model is in the model class\nconsidered or not. In this setting, this happens {\\em with certainty}, rather\nthan with high probability as is in the classical case. We precisely quantify\nthe goodness-of-fit of an individual model with respect to individual data. We\nshow that--within the obvious constraints--every graph is realized by the\nstructure function of some data. We determine the (un)computability properties\nof the various functions contemplated and of the ``algorithmic minimal\nsufficient statistic.'' \n\n"}
{"id": "cs/0209018", "contents": "Title: Probabilistic Reversible Automata and Quantum Automata Abstract: To study relationship between quantum finite automata and probabilistic\nfinite automata, we introduce a notion of probabilistic reversible automata\n(PRA, or doubly stochastic automata). We find that there is a strong\nrelationship between different possible models of PRA and corresponding models\nof quantum finite automata. We also propose a classification of reversible\nfinite 1-way automata. \n\n"}
{"id": "cs/0211013", "contents": "Title: Algorithmic scalability in globally constrained conservative parallel\n  discrete event simulations of asynchronous systems Abstract: We consider parallel simulations for asynchronous systems employing L\nprocessing elements that are arranged on a ring. Processors communicate only\namong the nearest neighbors and advance their local simulated time only if it\nis guaranteed that this does not violate causality. In simulations with no\nconstraints, in the infinite L-limit the utilization scales (Korniss et al, PRL\n84, 2000); but, the width of the virtual time horizon diverges (i.e., the\nmeasurement phase of the algorithm does not scale). In this work, we introduce\na moving window global constraint, which modifies the algorithm so that the\nmeasurement phase scales as well. We present results of systematic studies in\nwhich the system size (i.e., L and the volume load per processor) as well as\nthe constraint are varied. The constraint eliminates the extreme fluctuations\nin the virtual time horizon, provides a bound on its width, and controls the\naverage progress rate. The width of the window constraint can serve as a tuning\nparameter that, for a given volume load per processor, could be adjusted to\noptimize the utilization so as to maximize the efficiency. This result may find\nnumerous applications in modeling the evolution of general spatially extended\nshort-range interacting systems with asynchronous dynamics, including dynamic\nMonte Carlo studies. \n\n"}
{"id": "cs/0211025", "contents": "Title: Effective Strong Dimension, Algorithmic Information, and Computational\n  Complexity Abstract: The two most important notions of fractal dimension are {\\it Hausdorff\ndimension}, developed by Hausdorff (1919), and {\\it packing dimension},\ndeveloped by Tricot (1982).\n  Lutz (2000) has recently proven a simple characterization of Hausdorff\ndimension in terms of {\\it gales}, which are betting strategies that generalize\nmartingales. Imposing various computability and complexity constraints on these\ngales produces a spectrum of effective versions of Hausdorff dimension.\n  In this paper we show that packing dimension can also be characterized in\nterms of gales. Moreover, even though the usual definition of packing dimension\nis considerably more complex than that of Hausdorff dimension, our gale\ncharacterization of packing dimension is an exact dual of -- and every bit as\nsimple as -- the gale characterization of Hausdorff dimension.\n  Effectivizing our gale characterization of packing dimension produces a\nvariety of {\\it effective strong dimensions}, which are exact duals of the\neffective dimensions mentioned above.\n  We develop the basic properties of effective strong dimensions and prove a\nnumber of results relating them to fundamental aspects of randomness,\nKolmogorov complexity, prediction, Boolean circuit-size complexity,\npolynomial-time degrees, and data compression. \n\n"}
{"id": "cs/0309023", "contents": "Title: Efficient Algorithms for Citation Network Analysis Abstract: In the paper very efficient, linear in number of arcs, algorithms for\ndetermining Hummon and Doreian's arc weights SPLC and SPNP in citation network\nare proposed, and some theoretical properties of these weights are presented.\nThe nonacyclicity problem in citation networks is discussed. An approach to\nidentify on the basis of arc weights an important small subnetwork is proposed\nand illustrated on the citation networks of SOM (self organizing maps)\nliterature and US patents. \n\n"}
{"id": "cs/0401003", "contents": "Title: Randomized selection with tripartitioning Abstract: We show that several versions of Floyd and Rivest's algorithm Select [Comm.\\\nACM {\\bf 18} (1975) 173] for finding the $k$th smallest of $n$ elements require\nat most $n+\\min\\{k,n-k\\}+o(n)$ comparisons on average, even when equal elements\noccur. This parallels our recent analysis of another variant due to Floyd and\nRivest [Comm. ACM {\\bf 18} (1975) 165--172]. Our computational results suggest\nthat both variants perform well in practice, and may compete with other\nselection methods, such as Hoare's Find or quickselect with median-of-3 pivots. \n\n"}
{"id": "cs/0405053", "contents": "Title: Synchronous Relaxation for Parallel Ising Spin Simulations Abstract: A new parallel algorithm for simulating Ising spin systems is presented. The\nsequential prototype is the n-fold way algorithm cite{BKL75}, which is\nefficient but is hard to parallelize using conservative methods. Our parallel\nalgorithm is optimistic. Unlike other optimistic algorithms, e.g., Time Warp,\nour algorithm is synchronous. It also belongs to the class of simulations known\nas ``relaxation'' cite{CS8 hence it is named ``synchronous relaxation.'' We\nderive performance guarantees for this algorithm. If N is the number of PEs,\nthen under weak assumptions we show that the number of correct events processed\nper unit of time is, on average, at least of order N/log(N). All communication\ndelays, processing time, and busy waits are taken into account. \n\n"}
{"id": "cs/0407056", "contents": "Title: On the hardness of distinguishing mixed-state quantum computations Abstract: This paper considers the following problem. Two mixed-state quantum circuits\nQ and R are given, and the goal is to determine which of two possibilities\nholds: (i) Q and R act nearly identically on all possible quantum state inputs,\nor (ii) there exists some input state that Q and R transform into almost\nperfectly distinguishable outputs. This problem may be viewed as an abstraction\nof the following problem: given two physical processes described by sequences\nof local interactions, are the processes effectively the same or are they\ndifferent? We prove that this problem is a complete promise problem for the\nclass QIP of problems having quantum interactive proof systems, and is\ntherefore PSPACE-hard. This is in sharp contrast to the fact that the analogous\nproblem for classical (probabilistic) circuits is in AM, and for unitary\nquantum circuits is in QMA. \n\n"}
{"id": "cs/0412022", "contents": "Title: Zeno machines and hypercomputation Abstract: This paper reviews the Church-Turing Thesis (or rather, theses) with\nreference to their origin and application and considers some models of\n\"hypercomputation\", concentrating on perhaps the most straight-forward option:\nZeno machines (Turing machines with accelerating clock). The halting problem is\nbriefly discussed in a general context and the suggestion that it is an\ninevitable companion of any reasonable computational model is emphasised. It is\nhinted that claims to have \"broken the Turing barrier\" could be toned down and\nthat the important and well-founded role of Turing computability in the\nmathematical sciences stands unchallenged. \n\n"}
{"id": "cs/0506059", "contents": "Title: Existentially Restricted Quantified Constraint Satisfaction Abstract: The quantified constraint satisfaction problem (QCSP) is a powerful framework\nfor modelling computational problems. The general intractability of the QCSP\nhas motivated the pursuit of restricted cases that avoid its maximal\ncomplexity. In this paper, we introduce and study a new model for investigating\nQCSP complexity in which the types of constraints given by the existentially\nquantified variables, is restricted. Our primary technical contribution is the\ndevelopment and application of a general technology for proving positive\nresults on parameterizations of the model, of inclusion in the complexity class\ncoNP. \n\n"}
{"id": "cs/0603043", "contents": "Title: Time-Space Trade-Offs for Predecessor Search Abstract: We develop a new technique for proving cell-probe lower bounds for static\ndata structures. Previous lower bounds used a reduction to communication games,\nwhich was known not to be tight by counting arguments. We give the first lower\nbound for an explicit problem which breaks this communication complexity\nbarrier. In addition, our bounds give the first separation between polynomial\nand near linear space. Such a separation is inherently impossible by\ncommunication complexity.\n  Using our lower bound technique and new upper bound constructions, we obtain\ntight bounds for searching predecessors among a static set of integers. Given a\nset Y of n integers of l bits each, the goal is to efficiently find\npredecessor(x) = max{y in Y | y <= x}, by representing Y on a RAM using space\nS.\n  In external memory, it follows that the optimal strategy is to use either\nstandard B-trees, or a RAM algorithm ignoring the larger block size. In the\nimportant case of l = c*lg n, for c>1 (i.e. polynomial universes), and near\nlinear space (such as S = n*poly(lg n)), the optimal search time is Theta(lg\nl). Thus, our lower bound implies the surprising conclusion that van Emde Boas'\nclassic data structure from [FOCS'75] is optimal in this case. Note that for\nspace n^{1+eps}, a running time of O(lg l / lglg l) was given by Beame and Fich\n[STOC'99]. \n\n"}
{"id": "cs/0603084", "contents": "Title: Random 3CNF formulas elude the Lovasz theta function Abstract: Let $\\phi$ be a 3CNF formula with n variables and m clauses. A simple\nnonconstructive argument shows that when m is sufficiently large compared to n,\nmost 3CNF formulas are not satisfiable. It is an open question whether there is\nan efficient refutation algorithm that for most such formulas proves that they\nare not satisfiable. A possible approach to refute a formula $\\phi$ is: first,\ntranslate it into a graph $G_{\\phi}$ using a generic reduction from 3-SAT to\nmax-IS, then bound the maximum independent set of $G_{\\phi}$ using the Lovasz\n$\\vartheta$ function. If the $\\vartheta$ function returns a value $< m$, this\nis a certificate for the unsatisfiability of $\\phi$. We show that for random\nformulas with $m < n^{3/2 -o(1)}$ clauses, the above approach fails, i.e. the\n$\\vartheta$ function is likely to return a value of m. \n\n"}
{"id": "cs/0610042", "contents": "Title: A Polynomial Time Algorithm for The Traveling Salesman Problem Abstract: The ATSP polytope can be expressed by asymmetric polynomial size linear\nprogram. \n\n"}
{"id": "cs/9904019", "contents": "Title: Bounds for Small-Error and Zero-Error Quantum Algorithms Abstract: We present a number of results related to quantum algorithms with small error\nprobability and quantum algorithms that are zero-error. First, we give a tight\nanalysis of the trade-offs between the number of queries of quantum search\nalgorithms, their error probability, the size of the search space, and the\nnumber of solutions in this space. Using this, we deduce new lower and upper\nbounds for quantum versions of amplification problems. Next, we establish\nnearly optimal quantum-classical separations for the query complexity of\nmonotone functions in the zero-error model (where our quantum zero-error model\nis defined so as to be robust when the quantum gates are noisy). Also, we\npresent a communication complexity problem related to a total function for\nwhich there is a quantum-classical communication complexity gap in the\nzero-error model. Finally, we prove separations for monotone graph properties\nin the zero-error and other error models which imply that the evasiveness\nconjecture for such properties does not hold for quantum computers. \n\n"}
{"id": "math/0201011", "contents": "Title: Data mining for cones of metrics, quasi-metrics, hemi-metrics and\n  super-metrics Abstract: Using some adaptations of the adjacency decomposition method \\cite{CR} and\nthe program {\\it cdd} (~\\cite{Fu}), we compute the first computationally\ndifficult cases of convex cones of $m$-ary and oriented analogs of semi-metrics\nand cut semi-metrics, which were introduced in \\cite{DR2} and \\cite{DP}. We\nconsidered also more general notion of $(m,s)$-super-metric and corresponding\ncones. The data on related cones - the number of facets, of extreme rays, of\ntheir orbits and diameters - are collected in Table \\ref{tab:MainLovelyTable}.\nWe study also criterion of adjacency for skeletons of those cones and their\nduals. Some families of extreme rays and operations on them are also given. \n\n"}
{"id": "math/0207126", "contents": "Title: On the Number of Embeddings of Minimally Rigid Graphs Abstract: Rigid frameworks in some Euclidian space are embedded graphs having a unique\nlocal realization (up to Euclidian motions) for the given edge lengths,\nalthough globally they may have several. We study the number of distinct planar\nembeddings of minimally rigid graphs with $n$ vertices. We show that, modulo\nplanar rigid motions, this number is at most ${{2n-4}\\choose {n-2}} \\approx\n4^n$. We also exhibit several families which realize lower bounds of the order\nof $2^n$, $2.21^n$ and $2.88^n$.\n  For the upper bound we use techniques from complex algebraic geometry, based\non the (projective) Cayley-Menger variety $CM^{2,n}(C)\\subset P_{{{n}\\choose\n{2}}-1}(C)$ over the complex numbers $C$. In this context, point configurations\nare represented by coordinates given by squared distances between all pairs of\npoints. Sectioning the variety with $2n-4$ hyperplanes yields at most\n$deg(CM^{2,n})$ zero-dimensional components, and one finds this degree to be\n$D^{2,n}={1/2}{{2n-4}\\choose {n-2}}$. The lower bounds are related to inductive\nconstructions of minimally rigid graphs via Henneberg sequences.\n  The same approach works in higher dimensions. In particular we show that it\nleads to an upper bound of $2 D^{3,n}=\n{\\frac{2^{n-3}}{n-2}}{{n-6}\\choose{n-3}}$ for the number of spatial embeddings\nwith generic edge lengths of the 1-skeleton of a simplicial polyhedron, up to\nrigid motions. \n\n"}
{"id": "math/0208073", "contents": "Title: Face numbers of 4-Polytopes and 3-Spheres Abstract: In this paper, we discuss f- and flag-vectors of 4-dimensional convex\npolytopes and cellular 3-spheres. We put forward two crucial parameters of\nfatness and complexity: Fatness F(P) := (f_1+f_2-20)/(f_0+f_3-10) is large if\nthere are many more edges and 2-faces than there are vertices and facets, while\ncomplexity C(P) := (f_{03}-20)/(f_0+f_3-10) is large if every facet has many\nvertices, and every vertex is in many facets. Recent results suggest that these\nparameters might allow one to differentiate between the cones of f- or\nflag-vectors of -- connected Eulerian lattices of length 5 (combinatorial\nobjects), -- strongly regular CW 3-spheres (topological objects), -- convex\n4-polytopes (discrete geometric objects), and -- rational convex 4-polytopes\n(whose study involves arithmetic aspects). Further progress will depend on the\nderivation of tighter f-vector inequalities for convex 4-polytopes. On the\nother hand, we will need new construction methods that produce interesting\npolytopes which are far from being simplicial or simple -- for example, very\n``fat'' or ``complex'' 4-polytopes. In this direction, I will report about\nconstructions (from joint work with Michael Joswig, David Eppstein and Greg\nKuperberg) that yield -- strongly regular CW 3-spheres of arbitrarily large\nfatness, -- convex 4-polytopes of fatness larger than 5.048, and -- rational\nconvex 4-polytopes of fatness larger than 5-epsilon. \n\n"}
{"id": "math/0210133", "contents": "Title: Beneath-and-Beyond revisited Abstract: It is shown how the Beneath-and-Beyond algorithm can be used to yield another\nproof of the equivalence of V- and H-representations of convex polytopes. In\nthis sense this paper serves as the sketch of an introduction to polytope\ntheory with a focus on algorithmic aspects. Moreover, computational results are\npresented to compare Beneath-and-Beyond to other convex hull implementations. \n\n"}
{"id": "math/0212353", "contents": "Title: The six-dimensional Delaunay polytopes Abstract: Given a lattice $L$, a full dimensional polytope $P$ is called a {\\em\nDelaunay polytope} if the set of its vertices is $S\\cap L$ with $S$ being an\n{\\em empty sphere} of the lattice. Extending our previous work \\cite{DD-hyp} on\nthe {\\em hypermetric cone} $HYP_7$, we classify the six-dimensional Delaunay\npolytopes according to their {\\em combinatorial type}. The list of 6241\ncombinatorial types is obtained by a study of the set of faces of the\npolyhedral cone $HYP_7$. \n\n"}
{"id": "math/0304492", "contents": "Title: The $E_t$-Construction for Lattices, Spheres and Polytopes Abstract: We describe and analyze a new construction that produces new Eulerian\nlattices from old ones. It specializes to a construction that produces new\nstrongly regular cellular spheres (whose face lattices are Eulerian). The\nconstruction does not always specialize to convex polytopes; however, in a\nnumber of cases where we can realize it, it produces interesting classes of\npolytopes. Thus we produce an infinite family of rational 2-simplicial 2-simple\n4-polytopes, as requested by Eppstein, Kuperberg and Ziegler. We also construct\nfor each $d\\ge3$ an infinite family of $(d-2)$-simplicial 2-simple\n$d$-polytopes, thus solving a problem of Gr\\\"unbaum. \n\n"}
{"id": "math/0309156", "contents": "Title: Non-crossing frameworks with non-crossing reciprocals Abstract: We study non-crossing frameworks in the plane for which the classical\nreciprocal on the dual graph is also non-crossing. We give a complete\ndescription of the self-stresses on non-crossing frameworks whose reciprocals\nare non-crossing, in terms of: the types of faces (only pseudo-triangles and\npseudo-quadrangles are allowed); the sign patterns in the self-stress; and a\ngeometric condition on the stress vectors at some of the vertices.\n  As in other recent papers where the interplay of non-crossingness and\nrigidity of straight-line plane graphs is studied, pseudo-triangulations show\nup as objects of special interest. For example, it is known that all planar\nLaman circuits can be embedded as a pseudo-triangulation with one non-pointed\nvertex. We show that if such an embedding is sufficiently generic, then the\nreciprocal is non-crossing and again a pseudo-triangulation embedding of a\nplanar Laman circuit. For a singular (i.e., non-generic) pseudo-triangulation\nembedding of a planar Laman circuit, the reciprocal is still non-crossing and a\npseudo-triangulation, but its underlying graph may not be a Laman circuit.\nMoreover, all the pseudo-triangulations which admit a non-crossing reciprocal\narise as the reciprocals of such, possibly singular, stresses on\npseudo-triangulation embeddings of Laman circuits.\n  All self-stresses on a planar graph correspond to liftings to piece-wise\nlinear surfaces in 3-space. We prove characteristic geometric properties of the\nlifts of such non-crossing reciprocal pairs. \n\n"}
{"id": "math/0402053", "contents": "Title: A Note on Space Tiling Zonotopes Abstract: In 1908 Voronoi conjectured that every convex polytope which tiles space\nface-to-face by translations is affinely equivalent to the Dirichlet-Voronoi\npolytope of some lattice. In 1999 Erdahl proved this conjecture for the special\ncase of zonotopes. A zonotope is a projection of a regular cube under some\naffine transformation. In 1975 McMullen showed several equivalent conditions\nfor a zonotope to be a space tiling zonotope, i.e. a zonotope which admits a\nface-to-face tiling of space by translations. Implicitly, he related space\ntiling zonotopes to a special class of oriented matroids (regular matroids). We\nwill extend his result to give a new proof of Voronoi's conjecture for\nzonotopes using oriented matroids. This enables us to distinguish between\ncombinatorial and metrical properties and to apply the fact that oriented\nmatroids considered here have an essentially unique realization. Originally,\nthis is a theorem due to Brylawski and Lucas. By using oriented matroid duality\nwe interpret a part of McMullen's arguments as an elegant geometric proof of\nthis theorem in the special case of real numbers. \n\n"}
{"id": "math/0403272", "contents": "Title: Computational Approaches to Lattice Packing and Covering Problems Abstract: We describe algorithms which address two classical problems in lattice\ngeometry: the lattice covering and the simultaneous lattice packing-covering\nproblem. Theoretically our algorithms solve the two problems in any fixed\ndimension d in the sense that they approximate optimal covering lattices and\noptimal packing-covering lattices within any desired accuracy. Both algorithms\ninvolve semidefinite programming and are based on Voronoi's reduction theory\nfor positive definite quadratic forms, which describes all possible Delone\ntriangulations of Z^d.\n  In practice, our implementations reproduce known results in dimensions d <= 5\nand in particular solve the two problems in these dimensions. For d = 6 our\ncomputations produce new best known covering as well as packing-covering\nlattices, which are closely related to the lattice (E6)*. For d = 7, 8 our\napproach leads to new best known covering lattices. Although we use numerical\nmethods, we made some effort to transform numerical evidences into rigorous\nproofs. We provide rigorous error bounds and prove that some of the new\nlattices are locally optimal. \n\n"}
{"id": "math/0405441", "contents": "Title: Local Covering Optimality of Lattices: Leech Lattice versus Root Lattice\n  E8 Abstract: We show that the Leech lattice gives a sphere covering which is locally least\ndense among lattice coverings. We show that a similar result is false for the\nroot lattice E8. For this we construct a less dense covering lattice whose\nDelone subdivision has a common refinement with the Delone subdivision of E8.\nThe new lattice yields a sphere covering which is more than 12% less dense than\nthe formerly best known given by the lattice A8*. Currently, the Leech lattice\nis the first and only known example of a locally optimal lattice covering\nhaving a non-simplicial Delone subdivision. We hereby in particular answer a\nquestion of Dickson posed in 1968. By showing that the Leech lattice is rigid\nour answer is even strongest possible in a sense. \n\n"}
{"id": "math/0412093", "contents": "Title: Polyhedral surfaces of high genus Abstract: The construction of the COMBINATORIAL data for a surface with n vertices of\nmaximal genus is a classical problem: The maximal genus g=[(n-3)(n-4)/12] was\nachieved in the famous ``Map Color Theorem'' by Ringel et al. (1968). We\npresent the nicest one of Ringel's constructions, for the case when n is\ncongruent to 7 mod 12, but also an alternative construction, essentially due to\nHeffter (1898), which easily and explicitly yields surfaces of genus g ~ 1/16\nn^2.\n  For GEOMETRIC (polyhedral) surfaces with n vertices the maximal genus is not\nknown. The current record is g ~ n log n, due to McMullen, Schulz & Wills\n(1983). We present these surfaces with a new construction: We find them in\nSchlegel diagrams of ``neighborly cubical 4-polytopes,'' as constructed by\nJoswig & Ziegler (2000). \n\n"}
{"id": "math/0507528", "contents": "Title: Ehrhart polynomial and Successive Minima Abstract: We investigate the Ehrhart polynomial for the class of 0-symmetric convex\nlattice polytopes in Euclidean $n$-space $\\mathbb{R}^n$. It turns out that the\nroots of the Ehrhart polynomial and Minkowski's successive minima are closely\nrelated by their geometric and arithmetic mean. We also show that the roots of\nlattice $n$-polytopes with or without interior lattice points differ\nessentially. Furthermore, we study the structure of the roots in the planar\ncase. Here it turns out that their distribution reflects basic properties of\nlattice polygons. \n\n"}
{"id": "math/0605222", "contents": "Title: Solution of the coincidence problem in dimensions $d\\le 4$ Abstract: Discrete point sets $\\mathcal{S}$ such as lattices or quasiperiodic Delone\nsets may permit, beyond their symmetries, certain isometries $R$ such that\n$\\mathcal{S}\\cap R\\mathcal{S}$ is a subset of $\\mathcal{S}$ of finite density.\nThese are the so-called coincidence isometrie. They are important in\nunderstanding and classifying grain boundaries and twins in crystals and\nquasicrystals. It is the purpose of this contribution to introduce the\ncorresponding coincidence problem in a mathematical setting and to demonstrate\nhow it can be solved algebraically in dimensions 2, 3 and 4. Various examples\nboth from crystals and quasicrystals are treated explicitly, in particular\n(hyper-)cubic lattices and quasicrystals with non-crystallographic point groups\nof type $H_2$, $H_3$ and $H_4$. We derive parametrizations of all linear\ncoincidence isometries, determine the corresponding coincidence index (the\nreciprocal of the density of coinciding points, also called\n$\\varSigma$-factor), and finally encapsulate their statistics in suitable\nDirichlet series generating functions. \n\n"}
{"id": "math/0606089", "contents": "Title: Notes on the roots of Ehrhart polynomials Abstract: We determine lattice polytopes of smallest volume with a given number of\ninterior lattice points. We show that the Ehrhart polynomials of those with one\ninterior lattice point have largest roots with norm of order n^2, where n is\nthe dimension. This improves on the previously best known bound n and\ncomplements a recent result of Braun where it is shown that the norm of a root\nof an Ehrhart polynomial is at most of order n^2.\n  For the class of 0-symmetric lattice polytopes we present a conjecture on the\nsmallest volume for a given number of interior lattice points and prove the\nconjecture for crosspolytopes.\n  We further give a characterisation of the roots of the Ehrhart polyomials in\nthe 3-dimensional case and we classify for n\\leq 4 all lattice polytopes whose\nroots of their Ehrhart polynomials have all real part -1/2. These polytopes\nbelong to the class of reflexive polytopes. \n\n"}
{"id": "math/0610325", "contents": "Title: The computational complexity of convex bodies Abstract: We discuss how well a given convex body B in a real d-dimensional vector\nspace V can be approximated by a set X for which the membership question:\n``given an x in V, does x belong to X?'' can be answered efficiently (in time\npolynomial in d). We discuss approximations of a convex body by an ellipsoid,\nby an algebraic hypersurface, by a projection of a polytope with a controlled\nnumber of facets, and by a section of the cone of positive semidefinite\nquadratic forms. We illustrate some of the results on the Traveling Salesman\nPolytope, an example of a complicated convex body studied in combinatorial\noptimization. \n\n"}
{"id": "physics/0302034", "contents": "Title: Power and beauty of interval methods Abstract: Interval calculus is a relatively new branch of mathematics. Initially\nunderstood as a set of tools to assess the quality of numerical calculations\n(rigorous control of rounding errors), it became a discipline in its own rights\ntoday. Interval methods are usefull whenever we have to deal with\nuncertainties, which can be rigorously bounded. Fuzzy sets, rough sets and\nprobability calculus can perform similar tasks, yet only the interval methods\nare able to (dis)prove, with mathematical rigor, the (non)existence of desired\nsolution(s). Known are several problems, not presented here, which cannot be\neffectively solved by any other means.\n  This paper presents basic notions and main ideas of interval calculus and two\nexamples of useful algorithms. \n\n"}
{"id": "q-bio/0311037", "contents": "Title: Hierarchical Clustering Using Mutual Information Abstract: We present a method for hierarchical clustering of data called {\\it mutual\ninformation clustering} (MIC) algorithm. It uses mutual information (MI) as a\nsimilarity measure and exploits its grouping property: The MI between three\nobjects $X, Y,$ and $Z$ is equal to the sum of the MI between $X$ and $Y$, plus\nthe MI between $Z$ and the combined object $(XY)$. We use this both in the\nShannon (probabilistic) version of information theory and in the Kolmogorov\n(algorithmic) version. We apply our method to the construction of phylogenetic\ntrees from mitochondrial DNA sequences and to the output of independent\ncomponents analysis (ICA) as illustrated with the ECG of a pregnant woman. \n\n"}
{"id": "quant-ph/0110006", "contents": "Title: Quantum Certificate Verification: Single versus Multiple Quantum\n  Certificates Abstract: The class MA consists of languages that can be efficiently verified by\nclassical probabilistic verifiers using a single classical certificate, and the\nclass QMA consists of languages that can be efficiently verified by quantum\nverifiers using a single quantum certificate. Suppose that a verifier receives\nnot only one but multiple certificates. In the classical setting, it is obvious\nthat a classical verifier with multiple classical certificates is essentially\nthe same with the one with a single classical certificate. However, in the\nquantum setting where a quantum verifier is given a set of quantum certificates\nin tensor product form (i.e. each quantum certificate is not entangled with\nothers), the situation is different, because the quantum verifier might utilize\nthe structure of the tensor product form. This suggests a possibility of\nanother hierarchy of complexity classes, namely the QMA hierarchy. From this\npoint of view, we extend the definition of QMA to QMA(k) for the case quantum\nverifiers use k quantum certificates, and analyze the properties of QMA(k).\n  To compare the power of QMA(2) with that of QMA(1) = QMA, we show one\ninteresting property of ``quantum indistinguishability''. This gives a strong\nevidence that QMA(2) is more powerful than QMA(1). Furthermore, we show that,\nfor any fixed positive integer $k \\geq 2$, if a language L has a one-sided\nbounded error QMA(k) protocol with a quantum verifier using k quantum\ncertificates, L necessarily has a one-sided bounded error QMA(2) protocol with\na quantum verifier using only two quantum certificates. \n\n"}
{"id": "quant-ph/0110067", "contents": "Title: Frontier between separability and quantum entanglement in a many spin\n  system Abstract: We discuss the critical point $x_c$ separating the quantum entangled and\nseparable states in two series of N spins S in the simple mixed state\ncharacterized by the matrix operator $\\rho=x|\\tilde{\\phi}><\\tilde{\\phi}| +\n\\frac{1-x}{D^N} I_{D^N}$ where $x \\in [0,1]$, $D =2S+1$, ${\\bf I}_{D^N}$ is the\n$D^N \\times D^N$ unity matrix and $|\\tilde {\\phi}>$ is a special entangled\nstate. The cases x=0 and x=1 correspond respectively to fully random spins and\nto a fully entangled state. In the first of these series we consider special\nstates $|\\tilde{\\phi}>$ invariant under charge conjugation, that generalizes\nthe N=2 spin S=1/2 Einstein-Podolsky-Rosen state, and in the second one we\nconsider generalizations of the Weber density matrices. The evaluation of the\ncritical point $x_c$ was done through bounds coming from the partial\ntransposition method of Peres and the conditional nonextensive entropy\ncriterion. Our results suggest the conjecture that whenever the bounds coming\nfrom both methods coincide the result of $x_c$ is the exact one. The results we\npresent are relevant for the discussion of quantum computing, teleportation and\ncryptography. \n\n"}
{"id": "quant-ph/0204010", "contents": "Title: Quantum Optimization Problems Abstract: Krentel [J. Comput. System. Sci., 36, pp.490--509] presented a framework for\nan NP optimization problem that searches an optimal value among\nexponentially-many outcomes of polynomial-time computations. This paper expands\nhis framework to a quantum optimization problem using polynomial-time quantum\ncomputations and introduces the notion of an ``universal'' quantum optimization\nproblem similar to a classical ``complete'' optimization problem. We exhibit a\ncanonical quantum optimization problem that is universal for the class of\npolynomial-time quantum optimization problems. We show in a certain relativized\nworld that all quantum optimization problems cannot be approximated closely by\nquantum polynomial-time computations. We also study the complexity of quantum\noptimization problems in connection to well-known complexity classes. \n\n"}
{"id": "quant-ph/0506200", "contents": "Title: Solving Satisfiability Problems by the Ground-State Quantum Computer Abstract: A quantum algorithm is proposed to solve the Satisfiability problems by the\nground-state quantum computer. The scale of the energy gap of the ground-state\nquantum computer is analyzed for the 3-bit Exact Cover problem. The time cost\nof this algorithm on the general SAT problems is discussed. \n\n"}
{"id": "quant-ph/0604056", "contents": "Title: Quantum Versus Classical Proofs and Advice Abstract: This paper studies whether quantum proofs are more powerful than classical\nproofs, or in complexity terms, whether QMA=QCMA. We prove three results about\nthis question. First, we give a \"quantum oracle separation\" between QMA and\nQCMA. More concretely, we show that any quantum algorithm needs\n$\\Omega(\\sqrt{2^n/(m+1)})$ queries to find an $n$-qubit \"marked state\"\n$\\lvert\\psi\\rangle$, even if given an $m$-bit classical description of\n$\\lvert\\psi\\rangle$ together with a quantum black box that recognizes\n$\\lvert\\psi\\rangle$. Second, we give an explicit QCMA protocol that nearly\nachieves this lower bound. Third, we show that, in the one previously-known\ncase where quantum proofs seemed to provide an exponential advantage, classical\nproofs are basically just as powerful. In particular, Watrous gave a QMA\nprotocol for verifying non-membership in finite groups. Under plausible\ngroup-theoretic assumptions, we give a QCMA protocol for the same problem. Even\nwith no assumptions, our protocol makes only polynomially many queries to the\ngroup oracle. We end with some conjectures about quantum versus classical\noracles, and about the possibility of a classical oracle separation between QMA\nand QCMA. \n\n"}
{"id": "quant-ph/0608026", "contents": "Title: Search via Quantum Walk Abstract: We propose a new method for designing quantum search algorithms for finding a\n\"marked\" element in the state space of a classical Markov chain. The algorithm\nis based on a quantum walk \\'a la Szegedy (2004) that is defined in terms of\nthe Markov chain. The main new idea is to apply quantum phase estimation to the\nquantum walk in order to implement an approximate reflection operator. This\noperator is then used in an amplitude amplification scheme. As a result we\nconsiderably expand the scope of the previous approaches of Ambainis (2004) and\nSzegedy (2004). Our algorithm combines the benefits of these approaches in\nterms of being able to find marked elements, incurring the smaller cost of the\ntwo, and being applicable to a larger class of Markov chains. In addition, it\nis conceptually simple and avoids some technical difficulties in the previous\nanalyses of several algorithms based on quantum walk. \n\n"}
