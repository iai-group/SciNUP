{"id": "0704.3938", "contents": "Title: Classical dispersion-cancellation interferometry Abstract: Even-order dispersion cancellation, an effect previously identified with\nfrequency-entangled photons, is demonstrated experimentally for the first time\nwith a linear, classical interferometer. A combination of a broad bandwidth\nlaser and a high resolution spectrometer was used to measure the intensity\ncorrelations between anti-correlated optical frequencies. Only 14% broadening\nof the correlation signal is observed when significant material dispersion,\nenough to broaden the regular interferogram by 4250%, is introduced into one\narm of the interferometer. \n\n"}
{"id": "0708.0201", "contents": "Title: Four-wave mixing, quantum control and compensating losses in doped\n  negative-index photonic metamaterials Abstract: The possibility of compensating absorption in negative-index metatamterials\n(NIMs) doped by resonant nonlinear-optical centers is shown. The role of\nquantum interference and extraordinary properties of four-wave parametric\namplification of counter-propagating electromagnetic waves in NIMs are\ndiscussed. \n\n"}
{"id": "0812.2633", "contents": "Title: Ghost imaging with a single detector Abstract: We experimentally demonstrate pseudothermal ghost imaging and ghost\ndiffraction using only a single single-pixel detector. We achieve this by\nreplacing the high resolution detector of the reference beam with a computation\nof the propagating field, following a recent proposal by Shapiro [J. H.\nShapiro, arXiv:0807.2614 (2008)]. Since only a single detector is used, this\nprovides an experimental evidence that pseudothermal ghost imaging does not\nrely on non-local quantum correlations. In addition, we show the\ndepth-resolving capability of this ghost imaging technique. \n\n"}
{"id": "0903.2301", "contents": "Title: Dipole emission from a real cavity in a random medium. Fluorescence in a\n  homogeneous medium II Abstract: In this paper we derive a general expression for the emission rate of an\nemitter which is placed at the center of a real cavity drilled in a generic\nrandom medium. We apply our formalism to the computation of the decay rate of a\nsingle atom which seats within a small molecule embedded in a continuous\nmedium. \n\n"}
{"id": "0903.4706", "contents": "Title: Broad-band spectral control of single photon sources using a nonlinear\n  photonic crystal cavity Abstract: Motivated by developments in quantum information science, much recent effort\nhas been directed toward coupling individual quantum emitters to optical\nmicrocavities. Such systems can be used to produce single photons on demand,\nenable nonlinear optical switching at a single photon level, and implement\nfunctional nodes of a quantum network, where the emitters serve as processing\nnodes and photons are used for long-distance quantum communication. For many of\nthese practical applications, it is important to develop techniques that allow\none to generate outgoing single photons of desired frequency and bandwidth,\nenabling hybrid networks connecting different types of emitters and\nlong-distance transmission over telecommunications wavelengths. Here, we\npropose a novel approach that makes use of a nonlinear optical resonator, in\nwhich the single photon originating from the atom-like emitter is directly\nconverted into a photon with desired frequency and bandwidth using the\nintracavity nonlinearity. As specific examples, we discuss a high-finesse,\nTE-TM double-mode photonic crystal cavity design that allows for direct\ngeneration of single photons at telecom wavelengths starting from an InAs/GaAs\nquantum dot with a 950 nm transition wavelength, and a scheme for direct\noptical coupling of such a quantum dot with a diamond nitrogen-vacancy center\nat 637 nm. \n\n"}
{"id": "0905.1576", "contents": "Title: Pulse shape effects on photon-photon interactions in non-linear optical\n  quantum gates Abstract: Ideally, strong non-linearities could be used to implement quantum gates for\nphotonic qubits by well controlled two photon interactions. However, the\ndependence of the non-linear interaction on frequency and time makes it\ndifficult to preserve a coherent pulse shape that could justify a single mode\nmodel for the time-frequency degree of freedom of the photons. In this paper,\nwe analyze the problem of temporal multi-mode effects by considering the pulse\nshape of the average output field obtained from a coherent input pulse. It is\nshown that a significant part of the two photon state transformation can be\nderived from this semi-classical description of the optical non-linearity. The\neffect of a non-linear system on a two photon state can then be determined from\nthe density matrix dynamics of the coherently driven system using input-output\ntheory. As an example, the resonant non-linearity of a single two level atom is\ncharacterized. The results indicate that the most efficient non-linear effect\nmay not be the widely studied single mode phase shift, but the transfer of one\nof the photons to an orthogonal mode distinguished by its temporal and spectral\nproperties. \n\n"}
{"id": "0907.0265", "contents": "Title: Negative Refraction Gives Rise to the Klein Paradox Abstract: Electromagnetic negative refraction in metamaterials has attracted\nincreasingly great interest, since its first experimental verification in 2001.\nIt potentially leads to the applications superior to conventional devices\nincluding compact antennas for mobile stations, imaging beyond the diffraction\nlimit, and high-resolution radars, not to mention the anamolous wave\npropagation in fundamental optics. Here, we report how metamaterials could be\nused to simulate the \"negative refraction of spin-zero particles interacting\nwith a strong potential barrier\", which gives rise to the Klein paradox--a\ncounterintuitive relativistic process. We address the underlying physics of\nanalogous wave propagation behaviours in those two entirely different domains\nof quantum and classical. \n\n"}
{"id": "0911.1372", "contents": "Title: Low-loss nonlinear polaritonics Abstract: We propose large low-loss cross-phase modulation between two coupled surface\npolaritons propagating through a double electromagnetically-induced\ntransparency medium situated close to a negative-index metamaterial. In\nparticular a mutual $\\pi$ phase shift is attainable between the two pulses at\nthe single photon level. \n\n"}
{"id": "1005.4974", "contents": "Title: Cavity opto-electromechanical system combining strong electrical\n  actuation with ultrasensitive transduction Abstract: A cavity opto-electromechanical system is reported which combines the\nultrasensitive transduction of cavity optomechanical systems with the\nelectrical actuation of nanoelectromechanical systems. Ultrasensitive\nmechanical transduction is achieved via opto-mechanical coupling. Electrical\ngradient forces as large as 0.40 $\\mu$N are realized, facilitating strong\nactuation with ultralow dissipation. A scanning probe microscope is\nimplemented, capable of characterizing the mechanical modes. The integration of\nelectrical actuation into optomechanical devices is an enabling step towards\nthe regime of quantum nonlinear dynamics, and provides new capabilities for\nquantum control of mechanical motion. \n\n"}
{"id": "1007.3622", "contents": "Title: A generalized risk approach to path inference based on hidden Markov\n  models Abstract: Motivated by the unceasing interest in hidden Markov models (HMMs), this\npaper re-examines hidden path inference in these models, using primarily a\nrisk-based framework. While the most common maximum a posteriori (MAP), or\nViterbi, path estimator and the minimum error, or Posterior Decoder (PD), have\nlong been around, other path estimators, or decoders, have been either only\nhinted at or applied more recently and in dedicated applications generally\nunfamiliar to the statistical learning community. Over a decade ago, however, a\nfamily of algorithmically defined decoders aiming to hybridize the two standard\nones was proposed (Brushe et al., 1998). The present paper gives a careful\nanalysis of this hybridization approach, identifies several problems and issues\nwith it and other previously proposed approaches, and proposes practical\nresolutions of those. Furthermore, simple modifications of the classical\ncriteria for hidden path recognition are shown to lead to a new class of\ndecoders. Dynamic programming algorithms to compute these decoders in the usual\nforward-backward manner are presented. A particularly interesting subclass of\nsuch estimators can be also viewed as hybrids of the MAP and PD estimators.\nSimilar to previously proposed MAP-PD hybrids, the new class is parameterized\nby a small number of tunable parameters. Unlike their algorithmic predecessors,\nthe new risk-based decoders are more clearly interpretable, and, most\nimportantly, work \"out of the box\" in practice, which is demonstrated on some\nreal bioinformatics tasks and data. Some further generalizations and\napplications are discussed in conclusion. \n\n"}
{"id": "1009.2417", "contents": "Title: Toward third order ghost imaging with thermal light Abstract: Recently it has been suggested that an enhancement in the visibility of ghost\nimages obtained with thermal light can be achieved exploiting higher order\ncorrelations [3]. This paper reports on the status of an higher order ghost\nimaging experiment carried on at INRIM labs exploiting a pseudo-thermal source\nand a CCD camera. \n\n"}
{"id": "1009.2646", "contents": "Title: Efficient Bayesian Community Detection using Non-negative Matrix\n  Factorisation Abstract: Identifying overlapping communities in networks is a challenging task. In\nthis work we present a novel approach to community detection that utilises the\nBayesian non-negative matrix factorisation (NMF) model to produce a\nprobabilistic output for node memberships. The scheme has the advantage of\ncomputational efficiency, soft community membership and an intuitive\nfoundation. We present the performance of the method against a variety of\nbenchmark problems and compare and contrast it to several other algorithms for\ncommunity detection. Our approach performs favourably compared to other methods\nat a fraction of the computational costs. \n\n"}
{"id": "1009.5005", "contents": "Title: Canonical quantization of macroscopic electromagnetism Abstract: Application of the standard canonical quantization rules of quantum field\ntheory to macroscopic electromagnetism has encountered obstacles due to\nmaterial dispersion and absorption. This has led to a phenomenological approach\nto macroscopic quantum electrodynamics where no canonical formulation is\nattempted. In this paper macroscopic electromagnetism is canonically quantized.\nThe results apply to any linear, inhomogeneous, magnetodielectric medium with\ndielectric functions that obey the Kramers-Kronig relations. The prescriptions\nof the phenomenological approach are derived from the canonical theory. \n\n"}
{"id": "1011.3296", "contents": "Title: Input-Output Formalism For Few-Photon Transport in One-Dimensional\n  Nanophotonic Waveguides Coupled to a Qubit Abstract: We extend the input-output formalism of quantum optics to analyze few-photon\ntransport in waveguides with an embedded qubit. We provide explicit analytical\nderivations for one and two-photon scattering matrix elements based on operator\nequations in the Heisenberg picture. \n\n"}
{"id": "1012.3795", "contents": "Title: Estimating Networks With Jumps Abstract: We study the problem of estimating a temporally varying coefficient and\nvarying structure (VCVS) graphical model underlying nonstationary time series\ndata, such as social states of interacting individuals or microarray expression\nprofiles of gene networks, as opposed to i.i.d. data from an invariant model\nwidely considered in current literature of structural estimation. In\nparticular, we consider the scenario in which the model evolves in a piece-wise\nconstant fashion. We propose a procedure that minimizes the so-called TESLA\nloss (i.e., temporally smoothed L1 regularized regression), which allows\njointly estimating the partition boundaries of the VCVS model and the\ncoefficient of the sparse precision matrix on each block of the partition. A\nhighly scalable proximal gradient method is proposed to solve the resultant\nconvex optimization problem; and the conditions for sparsistent estimation and\nthe convergence rate of both the partition boundaries and the network structure\nare established for the first time for such estimators. \n\n"}
{"id": "1102.5372", "contents": "Title: Low-temperature tapered-fiber probing of diamond NV ensembles coupled to\n  GaP microcavities Abstract: In this work we present a platform for testing the device performance of a\ncavity-emitter system, using an ensemble of emitters and a tapered optical\nfiber. This method provides high-contrast spectra of the cavity modes,\nselective detection of emitters coupled to the cavity, and an estimate of the\ndevice performance in the single- emitter case. Using nitrogen-vacancy (NV)\ncenters in diamond and a GaP optical microcavity, we are able to tune the\ncavity onto the NV resonance at 10 K, couple the cavity-coupled emission to a\ntapered fiber, and measure the fiber-coupled NV spontaneous emission decay.\nTheoretically we show that the fiber-coupled average Purcell factor is 2-3\ntimes greater than that of free-space collection; although due to ensemble\naveraging it is still a factor of 3 less than the Purcell factor of a single,\nideally placed center. \n\n"}
{"id": "1103.5202", "contents": "Title: Fast Learning Rate of lp-MKL and its Minimax Optimality Abstract: In this paper, we give a new sharp generalization bound of lp-MKL which is a\ngeneralized framework of multiple kernel learning (MKL) and imposes\nlp-mixed-norm regularization instead of l1-mixed-norm regularization. We\nutilize localization techniques to obtain the sharp learning rate. The bound is\ncharacterized by the decay rate of the eigenvalues of the associated kernels. A\nlarger decay rate gives a faster convergence rate. Furthermore, we give the\nminimax learning rate on the ball characterized by lp-mixed-norm in the product\nspace. Then we show that our derived learning rate of lp-MKL achieves the\nminimax optimal rate on the lp-mixed-norm ball. \n\n"}
{"id": "1105.1153", "contents": "Title: On the Superradiant Phase in Field-Matter Interactions Abstract: We show that semi-classical states adapted to the symmetry of the Hamiltonian\nare an excellent approximation to the exact quantum solution of the ground and\nfirst excited states of the Dicke model. Their overlap to the exact quantum\nstates is very close to 1 except in a close vicinity of the quantum phase\ntransition. Furthermore, they have analytic forms in terms of the model\nparameters and allow us to calculate analytically the expectation values of\nfield and matter observables. Some of these differ considerably from results\nobtained via the standard coherent states, and by means of Holstein-Primakoff\nseries expansion of the Dicke Hamiltonian. Comparison with exact solutions\nobtained numerically support our results. In particular, it is shown that the\nexpectation values of the number of photons and of the number of excited atoms\nhave no singularities at the phase transition. We comment on why other authors\nhave previously found otherwise. \n\n"}
{"id": "1106.0800", "contents": "Title: Optimal Reinforcement Learning for Gaussian Systems Abstract: The exploration-exploitation trade-off is among the central challenges of\nreinforcement learning. The optimal Bayesian solution is intractable in\ngeneral. This paper studies to what extent analytic statements about optimal\nlearning are possible if all beliefs are Gaussian processes. A first order\napproximation of learning of both loss and dynamics, for nonlinear,\ntime-varying systems in continuous time and space, subject to a relatively weak\nrestriction on the dynamics, is described by an infinite-dimensional partial\ndifferential equation. An approximate finite-dimensional projection gives an\nimpression for how this result may be helpful. \n\n"}
{"id": "1106.4234", "contents": "Title: Properties of a Polarization based Phase Operator Abstract: We define a Hermitian phase operator for zero mass spin one particles\n(photons) by taking account polarization. The Hilbert space includes the\npositive helicity states and negative helicity states with opposite circular\npolarization. We define an operator which corresponds to the physical process\nof reversing the sense of polarization and acts as a bridge between positive\nhelicity states and negative helicity states. The exponential phase operator\nobtained using the entire set is unitary and acts as ladder operator over all\nthe states. The phase operator derived from this exponential operator satisfies\nthe canonical commutation relations with the number operator. We have\ncalculated the density matrix and the phase probability distribution of various\nstates like coherent states, squeezed states and thermal states, to illustrate\nthe utility of our operator. \n\n"}
{"id": "1107.2021", "contents": "Title: Multi-Instance Learning with Any Hypothesis Class Abstract: In the supervised learning setting termed Multiple-Instance Learning (MIL),\nthe examples are bags of instances, and the bag label is a function of the\nlabels of its instances. Typically, this function is the Boolean OR. The\nlearner observes a sample of bags and the bag labels, but not the instance\nlabels that determine the bag labels. The learner is then required to emit a\nclassification rule for bags based on the sample. MIL has numerous\napplications, and many heuristic algorithms have been used successfully on this\nproblem, each adapted to specific settings or applications. In this work we\nprovide a unified theoretical analysis for MIL, which holds for any underlying\nhypothesis class, regardless of a specific application or problem domain. We\nshow that the sample complexity of MIL is only poly-logarithmically dependent\non the size of the bag, for any underlying hypothesis class. In addition, we\nintroduce a new PAC-learning algorithm for MIL, which uses a regular supervised\nlearning algorithm as an oracle. We prove that efficient PAC-learning for MIL\ncan be generated from any efficient non-MIL supervised learning algorithm that\nhandles one-sided error. The computational complexity of the resulting\nalgorithm is only polynomially dependent on the bag size. \n\n"}
{"id": "1109.1762", "contents": "Title: A high-speed tunable beam splitter for feed-forward photonic quantum\n  information processing Abstract: We realize quantum gates for path qubits with a high-speed,\npolarization-independent and tunable beam splitter. Two electro-optical\nmodulators act in a Mach-Zehnder interferometer as high-speed phase shifters\nand rapidly tune its splitting ratio. We test its performance with heralded\nsingle photons, observing a polarization-independent interference contrast\nabove 95%. The switching time is about 5.6 ns, and a maximal repetition rate is\n2.5 MHz. We demonstrate tunable feed-forward operations of a single-qubit gate\nof path-encoded qubits and a two-qubit gate via measurement-induced interaction\nbetween two photons. \n\n"}
{"id": "1111.3781", "contents": "Title: Fast Learning Rate of Non-Sparse Multiple Kernel Learning and Optimal\n  Regularization Strategies Abstract: In this paper, we give a new generalization error bound of Multiple Kernel\nLearning (MKL) for a general class of regularizations, and discuss what kind of\nregularization gives a favorable predictive accuracy. Our main target in this\npaper is dense type regularizations including \\ellp-MKL. According to the\nrecent numerical experiments, the sparse regularization does not necessarily\nshow a good performance compared with dense type regularizations. Motivated by\nthis fact, this paper gives a general theoretical tool to derive fast learning\nrates of MKL that is applicable to arbitrary mixed-norm-type regularizations in\na unifying manner. This enables us to compare the generalization performances\nof various types of regularizations. As a consequence, we observe that the\nhomogeneity of the complexities of candidate reproducing kernel Hilbert spaces\n(RKHSs) affects which regularization strategy (\\ell1 or dense) is preferred. In\nfact, in homogeneous complexity settings where the complexities of all RKHSs\nare evenly same, \\ell1-regularization is optimal among all isotropic norms. On\nthe other hand, in inhomogeneous complexity settings, dense type\nregularizations can show better learning rate than sparse \\ell1-regularization.\nWe also show that our learning rate achieves the minimax lower bound in\nhomogeneous complexity settings. \n\n"}
{"id": "1111.4226", "contents": "Title: Joint Modeling of Multiple Related Time Series via the Beta Process Abstract: We propose a Bayesian nonparametric approach to the problem of jointly\nmodeling multiple related time series. Our approach is based on the discovery\nof a set of latent, shared dynamical behaviors. Using a beta process prior, the\nsize of the set and the sharing pattern are both inferred from data. We develop\nefficient Markov chain Monte Carlo methods based on the Indian buffet process\nrepresentation of the predictive distribution of the beta process, without\nrelying on a truncated model. In particular, our approach uses the sum-product\nalgorithm to efficiently compute Metropolis-Hastings acceptance probabilities,\nand explores new dynamical behaviors via birth and death proposals. We examine\nthe benefits of our proposed feature-based model on several synthetic datasets,\nand also demonstrate promising results on unsupervised segmentation of visual\nmotion capture data. \n\n"}
{"id": "1203.1455", "contents": "Title: Macroscopic displaced thermal field as the entanglement catalyst Abstract: We show that entanglement of multiple atoms can arise via resonant\ninteraction with a displaced thermal field with a macroscopic photon-number.\nThe cavity field acts as the catalyst, which is disentangled with the atomic\nsystem after the operation. Remarkably, the entanglement speed does not\ndecrease as the average photon-number of the mixed thermal state increases. The\natoms may evolve to a highly entangled state even when the photon-number of the\ncavity mode approaches infinity. \n\n"}
{"id": "1203.4523", "contents": "Title: On the Equivalence between Herding and Conditional Gradient Algorithms Abstract: We show that the herding procedure of Welling (2009) takes exactly the form\nof a standard convex optimization algorithm--namely a conditional gradient\nalgorithm minimizing a quadratic moment discrepancy. This link enables us to\ninvoke convergence results from convex optimization and to consider faster\nalternatives for the task of approximating integrals in a reproducing kernel\nHilbert space. We study the behavior of the different variants through\nnumerical simulations. The experiments indicate that while we can improve over\nherding on the task of approximating integrals, the original herding algorithm\ntends to approach more often the maximum entropy distribution, shedding more\nlight on the learning bias behind herding. \n\n"}
{"id": "1204.5721", "contents": "Title: Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit\n  Problems Abstract: Multi-armed bandit problems are the most basic examples of sequential\ndecision problems with an exploration-exploitation trade-off. This is the\nbalance between staying with the option that gave highest payoffs in the past\nand exploring new options that might give higher payoffs in the future.\nAlthough the study of bandit problems dates back to the Thirties,\nexploration-exploitation trade-offs arise in several modern applications, such\nas ad placement, website optimization, and packet routing. Mathematically, a\nmulti-armed bandit is defined by the payoff process associated with each\noption. In this survey, we focus on two extreme cases in which the analysis of\nregret is particularly simple and elegant: i.i.d. payoffs and adversarial\npayoffs. Besides the basic setting of finitely many actions, we also analyze\nsome of the most important variants and extensions, such as the contextual\nbandit model. \n\n"}
{"id": "1205.5075", "contents": "Title: Efficient Sparse Group Feature Selection via Nonconvex Optimization Abstract: Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance. \n\n"}
{"id": "1206.0930", "contents": "Title: All-Optical Switching Demonstration using Two-Photon Absorption and the\n  Classical Zeno Effect Abstract: Low-contrast all-optical Zeno switching has been demonstrated in a silicon\nnitride microdisk resonator coupled to a hot atomic vapor. The device is based\non the suppression of the field build-up within a microcavity due to\nnon-degenerate two-photon absorption. This experiment used one beam in a\nresonator and one in free-space due to limitations related to device physics.\nThese results suggest that a similar scheme with both beams resonant in the\ncavity would correspond to input power levels near 20 nW. \n\n"}
{"id": "1206.3548", "contents": "Title: Quantum Key Distribution with Fibonacci Orbital Angular Momentum States Abstract: Quantum cryptography and quantum key distribution (QKD) have been the most\nsuccessful applications of quantum information processing, highlighting the\nunique capability of quantum mechanics, through the no-cloning theorem, to\nprotect the security of shared encryption keys. Here we present a new and\nfundamentally different approach to high-capacity, high-efficiency QKD by\nexploiting interplay between cross-disciplinary ideas from quantum information\nand light scattering of aperiodic photonic media. The novelty of the proposed\napproach relies on a unique type of entangled-photon source and a new physical\nmechanism for efficiently sharing keys. The new source produces entangled\nphotons with orbital angular momenta (OAM) randomly distributed among Fibonacci\nnumbers. Combining entanglement with the mathematical properties of Fibonacci\nsequences leads to a new QKD protocol. This Fibonacci protocol is immune to\nphoton-number-splitting attacks and allows secure generation of long keys from\nfew photons. Unlike other protocols, reference frame alignment and active\nmodulation of production and detection bases are unnecessary, since security\ndoes not require use of non-orthogonal polarization measurements. \n\n"}
{"id": "1206.6381", "contents": "Title: Shortest path distance in random k-nearest neighbor graphs Abstract: Consider a weighted or unweighted k-nearest neighbor graph that has been\nbuilt on n data points drawn randomly according to some density p on R^d. We\nstudy the convergence of the shortest path distance in such graphs as the\nsample size tends to infinity. We prove that for unweighted kNN graphs, this\ndistance converges to an unpleasant distance function on the underlying space\nwhose properties are detrimental to machine learning. We also study the\nbehavior of the shortest path distance in weighted kNN graphs. \n\n"}
{"id": "1207.3031", "contents": "Title: Distributed Strongly Convex Optimization Abstract: A lot of effort has been invested into characterizing the convergence rates\nof gradient based algorithms for non-linear convex optimization. Recently,\nmotivated by large datasets and problems in machine learning, the interest has\nshifted towards distributed optimization. In this work we present a distributed\nalgorithm for strongly convex constrained optimization. Each node in a network\nof n computers converges to the optimum of a strongly convex, L-Lipchitz\ncontinuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is the\nnumber of iterations. This rate is achieved in the online setting where the\ndata is revealed one at a time to the nodes, and in the batch setting where\neach node has access to its full local dataset from the start. The same\nconvergence rate is achieved in expectation when the subgradients used at each\nnode are corrupted with additive zero-mean noise. \n\n"}
{"id": "1207.4747", "contents": "Title: Block-Coordinate Frank-Wolfe Optimization for Structural SVMs Abstract: We propose a randomized block-coordinate variant of the classic Frank-Wolfe\nalgorithm for convex optimization with block-separable constraints. Despite its\nlower iteration cost, we show that it achieves a similar convergence rate in\nduality gap as the full Frank-Wolfe algorithm. We also show that, when applied\nto the dual structural support vector machine (SVM) objective, this yields an\nonline algorithm that has the same low iteration complexity as primal\nstochastic subgradient methods. However, unlike stochastic subgradient methods,\nthe block-coordinate Frank-Wolfe algorithm allows us to compute the optimal\nstep-size and yields a computable duality gap guarantee. Our experiments\nindicate that this simple algorithm outperforms competing structural SVM\nsolvers. \n\n"}
{"id": "1208.4575", "contents": "Title: Multi-photon quantum interference in a multi-port integrated photonic\n  device Abstract: Increasing the complexity of quantum photonic devices is essential for many\noptical information processing applications to reach a regime beyond what can\nbe classically simulated, and integrated photonics has emerged as a leading\nplatform for achieving this. Here, we demonstrate three-photon quantum\noperation of an integrated device containing three coupled interferometers,\neight spatial modes and many classical and nonclassical interferences. This\nrepresents a critical advance over previous complexities and the first on-chip\nnonclassical interference with more than two photonic inputs. We introduce a\nnew scheme to verify quantum behaviour, using classically characterised device\nelements and hierarchies of photon correlation functions. We accurately predict\nthe device's quantum behaviour and show operation inconsistent with both\nclassical and bi-separable quantum models. Such methods for verifying\nmultiphoton quantum behaviour are vital for achieving increased circuit\ncomplexity. Our experiment paves the way for the next generation of integrated\nphotonic quantum simulation and computing devices. \n\n"}
{"id": "1209.1688", "contents": "Title: Rank Centrality: Ranking from Pair-wise Comparisons Abstract: The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal. \n\n"}
{"id": "1209.1873", "contents": "Title: Stochastic Dual Coordinate Ascent Methods for Regularized Loss\n  Minimization Abstract: Stochastic Gradient Descent (SGD) has become popular for solving large scale\nsupervised machine learning optimization problems such as SVM, due to their\nstrong theoretical guarantees. While the closely related Dual Coordinate Ascent\n(DCA) method has been implemented in various software packages, it has so far\nlacked good convergence analysis. This paper presents a new analysis of\nStochastic Dual Coordinate Ascent (SDCA) showing that this class of methods\nenjoy strong theoretical guarantees that are comparable or better than SGD.\nThis analysis justifies the effectiveness of SDCA for practical applications. \n\n"}
{"id": "1210.0805", "contents": "Title: Robust PCA and subspace tracking from incomplete observations using\n  L0-surrogates Abstract: Many applications in data analysis rely on the decomposition of a data matrix\ninto a low-rank and a sparse component. Existing methods that tackle this task\nuse the nuclear norm and L1-cost functions as convex relaxations of the rank\nconstraint and the sparsity measure, respectively, or employ thresholding\ntechniques. We propose a method that allows for reconstructing and tracking a\nsubspace of upper-bounded dimension from incomplete and corrupted observations.\nIt does not require any a priori information about the number of outliers. The\ncore of our algorithm is an intrinsic Conjugate Gradient method on the set of\northogonal projection matrices, the so-called Grassmannian. Non-convex sparsity\nmeasures are used for outlier detection, which leads to improved performance in\nterms of robustly recovering and tracking the low-rank matrix. In particular,\nour approach can cope with more outliers and with an underlying matrix of\nhigher rank than other state-of-the-art methods. \n\n"}
{"id": "1210.0814", "contents": "Title: Micro-resonator based all-optical transistor Abstract: We present theoretical estimates for a high-speed, low-loss, all-optical\ntransistor using a micro-resonator device, whose fields interact evanescently\nwith Rubidium vapor. We use a four-level electromagnetically induced absorption\nscheme to couple the light fields of the transistor. We show results indicating\nthat a weak control beam can switch a much stronger signal beam, with contrast\nof greater than 25 dB and loss less than 0.5 dB. The switching timescale is on\nthe order of 100 ps. \n\n"}
{"id": "1210.2289", "contents": "Title: A Fast Distributed Proximal-Gradient Method Abstract: We present a distributed proximal-gradient method for optimizing the average\nof convex functions, each of which is the private local objective of an agent\nin a network with time-varying topology. The local objectives have distinct\ndifferentiable components, but they share a common nondifferentiable component,\nwhich has a favorable structure suitable for effective computation of the\nproximal operator. In our method, each agent iteratively updates its estimate\nof the global minimum by optimizing its local objective function, and\nexchanging estimates with others via communication in the network. Using\nNesterov-type acceleration techniques and multiple communication steps per\niteration, we show that this method converges at the rate 1/k (where k is the\nnumber of communication rounds between the agents), which is faster than the\nconvergence rate of the existing distributed methods for solving this problem.\nThe superior convergence rate of our method is also verified by numerical\nexperiments. \n\n"}
{"id": "1210.2747", "contents": "Title: Experimental quantification of non-Gaussianity of phase-randomized\n  coherent states Abstract: We present the experimental investigation of the non-Gaussian nature of some\nmixtures of Fock states by reconstructing their Wigner function and exploiting\ntwo recently introduced measures of non-Gaussianity. In particular, we\ndemonstrate the consistency between the different approaches and the\nmonotonicity of the two measures for states belonging to the class of phase\nrandomized coherent states. Moreover, we prove that the exact behavior of one\nmeasure with respect to the other depends on the states under investigation and\ndevise possible criteria to discriminate which measure is more useful for the\ncharacterization of the states in realistic applications. \n\n"}
{"id": "1210.3212", "contents": "Title: Schmidt-like coherent mode decomposition and spatial intensity\n  correlations of thermal light Abstract: We experimentally study the properties of coherent mode decomposition for\nintensity correlation function of quasi-thermal light. We use the technique of\nspatial mode selection developed for studying transverse entanglement of photon\npairs, and show that it can be extended to characterize classical spatial\ncorrelations. Our results demonstrate the existence of a unique for a given\nthermal source basis of coherent modes, correlated in a way much resembling the\nSchmidt modes of spatially entangled photons. \n\n"}
{"id": "1210.7492", "contents": "Title: Unveiling the Hanbury Brown and Twiss effect through Renyi entropy\n  correlations Abstract: Adopting a quantum information perspective, we analyse the correlations in\nthe thermal light beams used to demonstrate the Hanbury Brown and Twiss effect.\nWe find that the total correlations measured by the Renyi mutual information\nmatch the normalised intensity correlations in the regime of low source\nintensity. Genuine quantum correlations in the form of discord are relevant in\nsuch regime but get washed out with increasing source intensity. This provides\na new angle on the issue about the nature--quantum versus classical--of the\neffect. \n\n"}
{"id": "1211.0358", "contents": "Title: Deep Gaussian Processes Abstract: In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a\ndeep belief network based on Gaussian process mappings. The data is modeled as\nthe output of a multivariate GP. The inputs to that Gaussian process are then\ngoverned by another GP. A single layer model is equivalent to a standard GP or\nthe GP latent variable model (GP-LVM). We perform inference in the model by\napproximate variational marginalization. This results in a strict lower bound\non the marginal likelihood of the model which we use for model selection\n(number of layers and nodes per layer). Deep belief networks are typically\napplied to relatively large data sets using stochastic gradient descent for\noptimization. Our fully Bayesian treatment allows for the application of deep\nmodels even when data is scarce. Model selection by our variational bound shows\nthat a five layer hierarchy is justified even when modelling a digit data set\ncontaining only 150 examples. \n\n"}
{"id": "1211.1043", "contents": "Title: Soft (Gaussian CDE) regression models and loss functions Abstract: Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution. \n\n"}
{"id": "1211.1082", "contents": "Title: Active and passive learning of linear separators under log-concave\n  distributions Abstract: We provide new results concerning label efficient, polynomial time, passive\nand active learning of linear separators. We prove that active learning\nprovides an exponential improvement over PAC (passive) learning of homogeneous\nlinear separators under nearly log-concave distributions. Building on this, we\nprovide a computationally efficient PAC algorithm with optimal (up to a\nconstant factor) sample complexity for such problems. This resolves an open\nquestion concerning the sample complexity of efficient PAC algorithms under the\nuniform distribution in the unit ball. Moreover, it provides the first bound\nfor a polynomial-time PAC algorithm that is tight for an interesting infinite\nclass of hypothesis functions under a general and natural class of\ndata-distributions, providing significant progress towards a longstanding open\nquestion.\n  We also provide new bounds for active and passive learning in the case that\nthe data might not be linearly separable, both in the agnostic case and and\nunder the Tsybakov low-noise condition. To derive our results, we provide new\nstructural results for (nearly) log-concave distributions, which might be of\nindependent interest as well. \n\n"}
{"id": "1211.2227", "contents": "Title: Efficient learning of simplices Abstract: We show an efficient algorithm for the following problem: Given uniformly\nrandom points from an arbitrary n-dimensional simplex, estimate the simplex.\nThe size of the sample and the number of arithmetic operations of our algorithm\nare polynomial in n. This answers a question of Frieze, Jerrum and Kannan\n[FJK]. Our result can also be interpreted as efficiently learning the\nintersection of n+1 half-spaces in R^n in the model where the intersection is\nbounded and we are given polynomially many uniform samples from it. Our proof\nuses the local search technique from Independent Component Analysis (ICA), also\nused by [FJK]. Unlike these previous algorithms, which were based on analyzing\nthe fourth moment, ours is based on the third moment.\n  We also show a direct connection between the problem of learning a simplex\nand ICA: a simple randomized reduction to ICA from the problem of learning a\nsimplex. The connection is based on a known representation of the uniform\nmeasure on a simplex. Similar representations lead to a reduction from the\nproblem of learning an affine transformation of an n-dimensional l_p ball to\nICA. \n\n"}
{"id": "1211.2717", "contents": "Title: Proximal Stochastic Dual Coordinate Ascent Abstract: We introduce a proximal version of dual coordinate ascent method. We\ndemonstrate how the derived algorithmic framework can be used for numerous\nregularized loss minimization problems, including $\\ell_1$ regularization and\nstructured output SVM. The convergence rates we obtain match, and sometimes\nimprove, state-of-the-art results. \n\n"}
{"id": "1212.0451", "contents": "Title: Semi-blind Source Separation via Sparse Representations and Online\n  Dictionary Learning Abstract: This work examines a semi-blind single-channel source separation problem. Our\nspecific aim is to separate one source whose local structure is approximately\nknown, from another a priori unspecified background source, given only a single\nlinear combination of the two sources. We propose a separation technique based\non local sparse approximations along the lines of recent efforts in sparse\nrepresentations and dictionary learning. A key feature of our procedure is the\nonline learning of dictionaries (using only the data itself) to sparsely model\nthe background source, which facilitates its separation from the\npartially-known source. Our approach is applicable to source separation\nproblems in various application domains; here, we demonstrate the performance\nof our proposed approach via simulation on a stylized audio source separation\ntask. \n\n"}
{"id": "1212.5156", "contents": "Title: Nonparametric ridge estimation Abstract: We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate. \n\n"}
{"id": "1301.0858", "contents": "Title: A New Geometric Approach to Latent Topic Modeling and Discovery Abstract: A new geometrically-motivated algorithm for nonnegative matrix factorization\nis developed and applied to the discovery of latent \"topics\" for text and image\n\"document\" corpora. The algorithm is based on robustly finding and clustering\nextreme points of empirical cross-document word-frequencies that correspond to\nnovel \"words\" unique to each topic. In contrast to related approaches that are\nbased on solving non-convex optimization problems using suboptimal\napproximations, locally-optimal methods, or heuristics, the new algorithm is\nconvex, has polynomial complexity, and has competitive qualitative and\nquantitative performance compared to the current state-of-the-art approaches on\nsynthetic and real-world datasets. \n\n"}
{"id": "1301.1722", "contents": "Title: Linear Bandits in High Dimension and Recommendation Systems Abstract: A large number of online services provide automated recommendations to help\nusers to navigate through a large collection of items. New items (products,\nvideos, songs, advertisements) are suggested on the basis of the user's past\nhistory and --when available-- her demographic profile. Recommendations have to\nsatisfy the dual goal of helping the user to explore the space of available\nitems, while allowing the system to probe the user's preferences.\n  We model this trade-off using linearly parametrized multi-armed bandits,\npropose a policy and prove upper and lower bounds on the cumulative \"reward\"\nthat coincide up to constants in the data poor (high-dimensional) regime. Prior\nwork on linear bandits has focused on the data rich (low-dimensional) regime\nand used cumulative \"risk\" as the figure of merit. For this data rich regime,\nwe provide a simple modification for our policy that achieves near-optimal risk\nperformance under more restrictive assumptions on the geometry of the problem.\nWe test (a variation of) the scheme used for establishing achievability on the\nNetflix and MovieLens datasets and obtain good agreement with the qualitative\npredictions of the theory we develop. \n\n"}
{"id": "1301.2084", "contents": "Title: Single photon-added coherent states: estimation of parameters and\n  fidelity of the optical homodyne detection Abstract: Travelling modes of single-photon-added coherent states (SPACS) are\ncharacterized via optical homodyne tomography. Given a set of experimentally\nmeasured quadrature distributions, we estimate parameters of the state and also\nextract information about the detector efficiency. The method used is a minimal\ndistance estimation between theoretical and experimental quantities, which\nadditionally allows to evaluate the precision of estimated parameters. Given\nexperimental data, we also estimate the lower and upper bounds on fidelity. The\nresults are believed to encourage preciser engineering and detection of SPACS. \n\n"}
{"id": "1301.2194", "contents": "Title: Network-based clustering with mixtures of L1-penalized Gaussian\n  graphical models: an empirical investigation Abstract: In many applications, multivariate samples may harbor previously unrecognized\nheterogeneity at the level of conditional independence or network structure.\nFor example, in cancer biology, disease subtypes may differ with respect to\nsubtype-specific interplay between molecular components. Then, both subtype\ndiscovery and estimation of subtype-specific networks present important and\nrelated challenges. To enable such analyses, we put forward a mixture model\nwhose components are sparse Gaussian graphical models. This brings together\nmodel-based clustering and graphical modeling to permit simultaneous estimation\nof cluster assignments and cluster-specific networks. We carry out estimation\nwithin an L1-penalized framework, and investigate several specific penalization\nregimes. We present empirical results on simulated data and provide general\nrecommendations for the formulation and use of mixtures of L1-penalized\nGaussian graphical models. \n\n"}
{"id": "1301.5584", "contents": "Title: Improved Cheeger's Inequality: Analysis of Spectral Partitioning\n  Algorithms through Higher Order Spectral Gap Abstract: Let \\phi(G) be the minimum conductance of an undirected graph G, and let\n0=\\lambda_1 <= \\lambda_2 <=... <= \\lambda_n <= 2 be the eigenvalues of the\nnormalized Laplacian matrix of G. We prove that for any graph G and any k >= 2,\n  \\phi(G) = O(k) \\lambda_2 / \\sqrt{\\lambda_k}, and this performance guarantee\nis achieved by the spectral partitioning algorithm. This improves Cheeger's\ninequality, and the bound is optimal up to a constant factor for any k. Our\nresult shows that the spectral partitioning algorithm is a constant factor\napproximation algorithm for finding a sparse cut if \\lambda_k$ is a constant\nfor some constant k. This provides some theoretical justification to its\nempirical performance in image segmentation and clustering problems. We extend\nthe analysis to other graph partitioning problems, including multi-way\npartition, balanced separator, and maximum cut. \n\n"}
{"id": "1301.5650", "contents": "Title: Regularization and nonlinearities for neural language models: when are\n  they needed? Abstract: Neural language models (LMs) based on recurrent neural networks (RNN) are\nsome of the most successful word and character-level LMs. Why do they work so\nwell, in particular better than linear neural LMs? Possible explanations are\nthat RNNs have an implicitly better regularization or that RNNs have a higher\ncapacity for storing patterns due to their nonlinearities or both. Here we\nargue for the first explanation in the limit of little training data and the\nsecond explanation for large amounts of text data. We show state-of-the-art\nperformance on the popular and small Penn dataset when RNN LMs are regularized\nwith random dropout. Nonetheless, we show even better performance from a\nsimplified, much less expressive linear RNN model without off-diagonal entries\nin the recurrent matrix. We call this model an impulse-response LM (IRLM).\nUsing random dropout, column normalization and annealed learning rates, IRLMs\ndevelop neurons that keep a memory of up to 50 words in the past and achieve a\nperplexity of 102.5 on the Penn dataset. On two large datasets however, the\nsame regularization methods are unsuccessful for both models and the RNN's\nexpressivity allows it to overtake the IRLM by 10 and 20 percent perplexity,\nrespectively. Despite the perplexity gap, IRLMs still outperform RNNs on the\nMicrosoft Research Sentence Completion (MRSC) task. We develop a slightly\nmodified IRLM that separates long-context units (LCUs) from short-context units\nand show that the LCUs alone achieve a state-of-the-art performance on the MRSC\ntask of 60.8%. Our analysis indicates that a fruitful direction of research for\nneural LMs lies in developing more accessible internal representations, and\nsuggests an optimization regime of very high momentum terms for effectively\ntraining such models. \n\n"}
{"id": "1302.2576", "contents": "Title: The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking Abstract: We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models. \n\n"}
{"id": "1302.4245", "contents": "Title: Gaussian Process Kernels for Pattern Discovery and Extrapolation Abstract: Gaussian processes are rich distributions over functions, which provide a\nBayesian nonparametric approach to smoothing and interpolation. We introduce\nsimple closed form kernels that can be used with Gaussian processes to discover\npatterns and enable extrapolation. These kernels are derived by modelling a\nspectral density -- the Fourier transform of a kernel -- with a Gaussian\nmixture. The proposed kernels support a broad class of stationary covariances,\nbut Gaussian process inference remains simple and analytic. We demonstrate the\nproposed kernels by discovering patterns and performing long range\nextrapolation on synthetic examples, as well as atmospheric CO2 trends and\nairline passenger data. We also show that we can reconstruct standard\ncovariances within our framework. \n\n"}
{"id": "1302.4922", "contents": "Title: Structure Discovery in Nonparametric Regression through Compositional\n  Kernel Search Abstract: Despite its importance, choosing the structural form of the kernel in\nnonparametric regression remains a black art. We define a space of kernel\nstructures which are built compositionally by adding and multiplying a small\nnumber of base kernels. We present a method for searching over this space of\nstructures which mirrors the scientific discovery process. The learned\nstructures can often decompose functions into interpretable components and\nenable long-range extrapolation on time-series datasets. Our structure search\nmethod outperforms many widely used kernels and kernel combination methods on a\nvariety of prediction tasks. \n\n"}
{"id": "1302.6009", "contents": "Title: On learning parametric-output HMMs Abstract: We present a novel approach for learning an HMM whose outputs are distributed\naccording to a parametric family. This is done by {\\em decoupling} the learning\ntask into two steps: first estimating the output parameters, and then\nestimating the hidden states transition probabilities. The first step is\naccomplished by fitting a mixture model to the output stationary distribution.\nGiven the parameters of this mixture model, the second step is formulated as\nthe solution of an easily solvable convex quadratic program. We provide an\nerror analysis for the estimated transition probabilities and show they are\nrobust to small perturbations in the estimates of the mixture parameters.\nFinally, we support our analysis with some encouraging empirical results. \n\n"}
{"id": "1303.1925", "contents": "Title: High-bandwidth squeezed light at 1550 nm from a compact monolithic PPKTP\n  cavity Abstract: We report the generation of squeezed vacuum states of light at 1550 nm with a\nbroadband quantum noise reduction of up to 4.8 dB ranging from 5 MHz to 1.2 GHz\nsideband frequency. We used a custom-designed 2.6 mm long biconvex\nperiodically-poled potassium titanyl phosphate (PPKTP) crystal. It featured\nreflectively coated end surfaces, 2.26 GHz of linewidth and generated the\nsqueezing via optical parametric amplification. Two homodyne detectors with\ndifferent quantum efficiencies and bandwidths were used to characterize the\nnon-classical noise suppression. We measured squeezing values of up to 4.8 dB\nfrom 5 to 100 MHz and up to 3 dB from 100 MHz to 1.2 GHz. The squeezed vacuum\nmeasurements were limited by detection loss. We propose an improved detection\nscheme to measure up to 10 dB squeezing over 1 GHz. Our results of GHz\nbandwidth squeezed light generation provide new prospects for high-speed\nquantum key distribution. \n\n"}
{"id": "1303.3240", "contents": "Title: A Unified Framework for Probabilistic Component Analysis Abstract: We present a unifying framework which reduces the construction of\nprobabilistic component analysis techniques to a mere selection of the latent\nneighbourhood, thus providing an elegant and principled framework for creating\nnovel component analysis models as well as constructing probabilistic\nequivalents of deterministic component analysis methods. Under our framework,\nwe unify many very popular and well-studied component analysis algorithms, such\nas Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA),\nLocality Preserving Projections (LPP) and Slow Feature Analysis (SFA), some of\nwhich have no probabilistic equivalents in literature thus far. We firstly\ndefine the Markov Random Fields (MRFs) which encapsulate the latent\nconnectivity of the aforementioned component analysis techniques; subsequently,\nwe show that the projection directions produced by all PCA, LDA, LPP and SFA\nare also produced by the Maximum Likelihood (ML) solution of a single joint\nprobability density function, composed by selecting one of the defined MRF\npriors while utilising a simple observation model. Furthermore, we propose\nnovel Expectation Maximization (EM) algorithms, exploiting the proposed joint\nPDF, while we generalize the proposed methodologies to arbitrary connectivities\nvia parameterizable MRF products. Theoretical analysis and experiments on both\nsimulated and real world data show the usefulness of the proposed framework, by\nderiving methods which well outperform state-of-the-art equivalents. \n\n"}
{"id": "1303.4634", "contents": "Title: Experimental distribution of entanglement with separable carriers Abstract: The key requirement for quantum networking is the distribution of\nentanglement between nodes. Surprisingly, entanglement can be generated across\na network without direct transfer - or communication - of entanglement. In\ncontrast to information gain, which cannot exceed the communicated information,\nthe entanglement gain is bounded by the communicated quantum discord, a more\ngeneral measure of quantum correlation that includes but is not limited to\nentanglement. Here, we experimentally entangle two communicating parties\nsharing three initially separable photonic qubits by exchange of a carrier\nphoton that is unentangled with either party at all times. We show that\ndistributing entanglement with separable carriers is resilient to noise and in\nsome cases becomes the only way of distributing entanglement through noisy\nenvironments. \n\n"}
{"id": "1303.7138", "contents": "Title: Unequivocal differentiation of coherent and chaotic light through\n  interferometric photon correlation measurements Abstract: We present a novel experimental technique that can differentiate\nunequivocally between chaotic light and coherent light with amplitude\nfluctuations, and thus permits to characterize unambiguously the output of a\nlaser. This technique consists of measuring the second-order intensity\ncross-correlation at the outputs of an unbalanced Michelson interferometer. It\nis applied to a chaotic light source and to the output of a semiconductor\nnanolaser whose \"standard\" intensity correlation function above-threshold\ndisplays values compatible with a mixture of coherent and chaotic light. Our\nexperimental results demonstrate that the output of such lasers is not\npartially chaotic but is indeed a coherent state with amplitude fluctuations. \n\n"}
{"id": "1304.4208", "contents": "Title: On Chip Manipulation of Single Photons from a Diamond Defect Abstract: Operating reconfigurable quantum circuits with single photon sources is a key\ngoal of photonic quantum information science and technology. We use an\nintegrated waveguide device comprising of directional couplers and a\nreconfigurable thermal phase controller to manipulate single photons emitted\nfrom a chromium related colour centre in diamond. Observation of both a\nwave-like interference pattern and particle-like sub-Poissionian\nautocorrelation functions demonstrates coherent manipulation of single photons\nemitted from the chromium related centre and verifies wave particle duality. \n\n"}
{"id": "1304.5583", "contents": "Title: Distributed Low-rank Subspace Segmentation Abstract: Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups. \n\n"}
{"id": "1304.6679", "contents": "Title: Cavity cooling of an optically levitated nanoparticle Abstract: The ability to trap and to manipulate individual atoms is at the heart of\ncurrent implementations of quantum simulations, quantum computing, and\nlong-distance quantum communication. Controlling the motion of larger particles\nopens up yet new avenues for quantum science, both for the study of fundamental\nquantum phenomena in the context of matter wave interference, and for new\nsensing and transduction applications in the context of quantum optomechanics.\nSpecifically, it has been suggested that cavity cooling of a single\nnanoparticle in high vacuum allows for the generation of quantum states of\nmotion in a room-temperature environment as well as for unprecedented force\nsensitivity. Here, we take the first steps into this regime. We demonstrate\ncavity cooling of an optically levitated nanoparticle consisting of\napproximately 10e9 atoms. The particle is trapped at modest vacuum levels of a\nfew millibar in the standing-wave field of an optical cavity and is cooled\nthrough coherent scattering into the modes of the same cavity. We estimate that\nour cooling rates are sufficient for ground-state cooling, provided that\noptical trapping at a vacuum level of 10e-7 millibar can be realized in the\nfuture, e.g., by employing additional active-feedback schemes to stabilize the\noptical trap in three dimensions. This paves the way for a new light-matter\ninterface enabling room-temperature quantum experiments with mesoscopic\nmechanical systems. \n\n"}
{"id": "1304.7717", "contents": "Title: The Randomized Dependence Coefficient Abstract: We introduce the Randomized Dependence Coefficient (RDC), a measure of\nnon-linear dependence between random variables of arbitrary dimension based on\nthe Hirschfeld-Gebelein-R\\'enyi Maximum Correlation Coefficient. RDC is defined\nin terms of correlation of random non-linear copula projections; it is\ninvariant with respect to marginal distribution transformations, has low\ncomputational cost and is easy to implement: just five lines of R code,\nincluded at the end of the paper. \n\n"}
{"id": "1305.1040", "contents": "Title: On the Convergence and Consistency of the Blurring Mean-Shift Process Abstract: The mean-shift algorithm is a popular algorithm in computer vision and image\nprocessing. It can also be cast as a minimum gamma-divergence estimation. In\nthis paper we focus on the \"blurring\" mean shift algorithm, which is one\nversion of the mean-shift process that successively blurs the dataset. The\nanalysis of the blurring mean-shift is relatively more complicated compared to\nthe nonblurring version, yet the algorithm convergence and the estimation\nconsistency have not been well studied in the literature. In this paper we\nprove both the convergence and the consistency of the blurring mean-shift. We\nalso perform simulation studies to compare the efficiency of the blurring and\nthe nonblurring versions of the mean-shift algorithms. Our results show that\nthe blurring mean-shift has more efficiency. \n\n"}
{"id": "1305.5306", "contents": "Title: A Supervised Neural Autoregressive Topic Model for Simultaneous Image\n  Classification and Annotation Abstract: Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to perform scene recognition and annotation. Recently, a\nnew type of topic model called the Document Neural Autoregressive Distribution\nEstimator (DocNADE) was proposed and demonstrated state-of-the-art performance\nfor document modeling. In this work, we show how to successfully apply and\nextend this model to the context of visual scene modeling. Specifically, we\npropose SupDocNADE, a supervised extension of DocNADE, that increases the\ndiscriminative power of the hidden topic features by incorporating label\ninformation into the training objective of the model. We also describe how to\nleverage information about the spatial position of the visual words and how to\nembed additional image annotations, so as to simultaneously perform image\nclassification and annotation. We test our model on the Scene15, LabelMe and\nUIUC-Sports datasets and show that it compares favorably to other topic models\nsuch as the supervised variant of LDA. \n\n"}
{"id": "1305.5782", "contents": "Title: Adapting the Stochastic Block Model to Edge-Weighted Networks Abstract: We generalize the stochastic block model to the important case in which edges\nare annotated with weights drawn from an exponential family distribution. This\ngeneralization introduces several technical difficulties for model estimation,\nwhich we solve using a Bayesian approach. We introduce a variational algorithm\nthat efficiently approximates the model's posterior distribution for dense\ngraphs. In specific numerical experiments on edge-weighted networks, this\nweighted stochastic block model outperforms the common approach of first\napplying a single threshold to all weights and then applying the classic\nstochastic block model, which can obscure latent block structure in networks.\nThis model will enable the recovery of latent structure in a broader range of\nnetwork data than was previously possible. \n\n"}
{"id": "1305.6627", "contents": "Title: High quantum-efficiency photon-number-resolving detector for photonic\n  on-chip information processing Abstract: The integrated optical circuit is a promising architecture for the\nrealization of complex quantum optical states and information networks. One\nelement that is required for many of these applications is a high-efficiency\nphoton detector capable of photon-number discrimination. We present an\nintegrated photonic system in the telecom band at 1550 nm based on UV-written\nsilica-on-silicon waveguides and modified transition-edge sensors capable of\nnumber resolution and over 40% efficiency. Exploiting the mode transmission\nfailure of these devices, we multiplex three detectors in series to demonstrate\na combined 79% +/- 2% detection efficiency with a single pass, and 88% +/- 3%\nat the operating wavelength of an on-chip terminal reflection grating.\nFurthermore, our optical measurements clearly demonstrate no significant\nunexplained loss in this system due to scattering or reflections. This\nwaveguide and detector design therefore allows the placement of\nnumber-resolving single-photon detectors of predictable efficiency at arbitrary\nlocations within a photonic circuit - a capability that offers great potential\nfor many quantum optical applications. \n\n"}
{"id": "1306.0308", "contents": "Title: Probabilistic Solutions to Differential Equations and their Application\n  to Riemannian Statistics Abstract: We study a probabilistic numerical method for the solution of both boundary\nand initial value problems that returns a joint Gaussian process posterior over\nthe solution. Such methods have concrete value in the statistics on Riemannian\nmanifolds, where non-analytic ordinary differential equations are involved in\nvirtually all computations. The probabilistic formulation permits marginalising\nthe uncertainty of the numerical solution such that statistics are less\nsensitive to inaccuracies. This leads to new Riemannian algorithms for mean\nvalue computations and principal geodesic analysis. Marginalisation also means\nresults can be less precise than point estimates, enabling a noticeable\nspeed-up over the state of the art. Our approach is an argument for a wider\npoint that uncertainty caused by numerical calculations should be tracked\nthroughout the pipeline of machine learning algorithms. \n\n"}
{"id": "1306.0940", "contents": "Title: (More) Efficient Reinforcement Learning via Posterior Sampling Abstract: Most provably-efficient learning algorithms introduce optimism about\npoorly-understood states and actions to encourage exploration. We study an\nalternative approach for efficient exploration, posterior sampling for\nreinforcement learning (PSRL). This algorithm proceeds in repeated episodes of\nknown duration. At the start of each episode, PSRL updates a prior distribution\nover Markov decision processes and takes one sample from this posterior. PSRL\nthen follows the policy that is optimal for this sample during the episode. The\nalgorithm is conceptually simple, computationally efficient and allows an agent\nto encode prior knowledge in a natural way. We establish an $\\tilde{O}(\\tau S\n\\sqrt{AT})$ bound on the expected regret, where $T$ is time, $\\tau$ is the\nepisode length and $S$ and $A$ are the cardinalities of the state and action\nspaces. This bound is one of the first for an algorithm not based on optimism,\nand close to the state of the art for any reinforcement learning algorithm. We\nshow through simulation that PSRL significantly outperforms existing algorithms\nwith similar regret bounds. \n\n"}
{"id": "1306.1587", "contents": "Title: Spectral Convergence of the connection Laplacian from random samples Abstract: Spectral methods that are based on eigenvectors and eigenvalues of discrete\ngraph Laplacians, such as Diffusion Maps and Laplacian Eigenmaps are often used\nfor manifold learning and non-linear dimensionality reduction. It was\npreviously shown by Belkin and Niyogi \\cite{belkin_niyogi:2007} that the\neigenvectors and eigenvalues of the graph Laplacian converge to the\neigenfunctions and eigenvalues of the Laplace-Beltrami operator of the manifold\nin the limit of infinitely many data points sampled independently from the\nuniform distribution over the manifold. Recently, we introduced Vector\nDiffusion Maps and showed that the connection Laplacian of the tangent bundle\nof the manifold can be approximated from random samples. In this paper, we\npresent a unified framework for approximating other connection Laplacians over\nthe manifold by considering its principle bundle structure. We prove that the\neigenvectors and eigenvalues of these Laplacians converge in the limit of\ninfinitely many independent random samples. We generalize the spectral\nconvergence results to the case where the data points are sampled from a\nnon-uniform distribution, and for manifolds with and without boundary. \n\n"}
{"id": "1306.2270", "contents": "Title: Compressive Object Tracking using Entangled Photons Abstract: We present a compressive sensing protocol that tracks a moving object by\nremoving static components from a scene. The implementation is carried out on a\nghost imaging scheme to minimize both the number of photons and the number of\nmeasurements required to form a quantum image of the tracked object. This\nprocedure tracks an object at low light levels with fewer than 3% of the\nmeasurements required for a raster scan, permitting us to more effectively use\nthe information content in each photon. \n\n"}
{"id": "1307.1827", "contents": "Title: Loss minimization and parameter estimation with heavy tails Abstract: This work studies applications and generalizations of a simple estimation\ntechnique that provides exponential concentration under heavy-tailed\ndistributions, assuming only bounded low-order moments. We show that the\ntechnique can be used for approximate minimization of smooth and strongly\nconvex losses, and specifically for least squares linear regression. For\ninstance, our $d$-dimensional estimator requires just\n$\\tilde{O}(d\\log(1/\\delta))$ random samples to obtain a constant factor\napproximation to the optimal least squares loss with probability $1-\\delta$,\nwithout requiring the covariates or noise to be bounded or subgaussian. We\nprovide further applications to sparse linear regression and low-rank\ncovariance matrix estimation with similar allowances on the noise and covariate\ndistributions. The core technique is a generalization of the median-of-means\nestimator to arbitrary metric spaces. \n\n"}
{"id": "1308.0850", "contents": "Title: Generating Sequences With Recurrent Neural Networks Abstract: This paper shows how Long Short-term Memory recurrent neural networks can be\nused to generate complex sequences with long-range structure, simply by\npredicting one data point at a time. The approach is demonstrated for text\n(where the data are discrete) and online handwriting (where the data are\nreal-valued). It is then extended to handwriting synthesis by allowing the\nnetwork to condition its predictions on a text sequence. The resulting system\nis able to generate highly realistic cursive handwriting in a wide variety of\nstyles. \n\n"}
{"id": "1308.2853", "contents": "Title: When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor\n  Tucker Decompositions with Structured Sparsity Abstract: Overcomplete latent representations have been very popular for unsupervised\nfeature learning in recent years. In this paper, we specify which overcomplete\nmodels can be identified given observable moments of a certain order. We\nconsider probabilistic admixture or topic models in the overcomplete regime,\nwhere the number of latent topics can greatly exceed the size of the observed\nword vocabulary. While general overcomplete topic models are not identifiable,\nwe establish generic identifiability under a constraint, referred to as topic\npersistence. Our sufficient conditions for identifiability involve a novel set\nof \"higher order\" expansion conditions on the topic-word matrix or the\npopulation structure of the model. This set of higher-order expansion\nconditions allow for overcomplete models, and require the existence of a\nperfect matching from latent topics to higher order observed words. We\nestablish that random structured topic models are identifiable w.h.p. in the\novercomplete regime. Our identifiability results allows for general\n(non-degenerate) distributions for modeling the topic proportions, and thus, we\ncan handle arbitrarily correlated topics in our framework. Our identifiability\nresults imply uniqueness of a class of tensor decompositions with structured\nsparsity which is contained in the class of Tucker decompositions, but is more\ngeneral than the Candecomp/Parafac (CP) decomposition. \n\n"}
{"id": "1308.4568", "contents": "Title: Distributed Online Learning via Cooperative Contextual Bandits Abstract: In this paper we propose a novel framework for decentralized, online learning\nby many learners. At each moment of time, an instance characterized by a\ncertain context may arrive to each learner; based on the context, the learner\ncan select one of its own actions (which gives a reward and provides\ninformation) or request assistance from another learner. In the latter case,\nthe requester pays a cost and receives the reward but the provider learns the\ninformation. In our framework, learners are modeled as cooperative contextual\nbandits. Each learner seeks to maximize the expected reward from its arrivals,\nwhich involves trading off the reward received from its own actions, the\ninformation learned from its own actions, the reward received from the actions\nrequested of others and the cost paid for these actions - taking into account\nwhat it has learned about the value of assistance from each other learner. We\ndevelop distributed online learning algorithms and provide analytic bounds to\ncompare the efficiency of these with algorithms with the complete knowledge\n(oracle) benchmark (in which the expected reward of every action in every\ncontext is known by every learner). Our estimates show that regret - the loss\nincurred by the algorithm - is sublinear in time. Our theoretical framework can\nbe used in many practical applications including Big Data mining, event\ndetection in surveillance sensor networks and distributed online recommendation\nsystems. \n\n"}
{"id": "1308.5038", "contents": "Title: Group-Sparse Signal Denoising: Non-Convex Regularization, Convex\n  Optimization Abstract: Convex optimization with sparsity-promoting convex regularization is a\nstandard approach for estimating sparse signals in noise. In order to promote\nsparsity more strongly than convex regularization, it is also standard practice\nto employ non-convex optimization. In this paper, we take a third approach. We\nutilize a non-convex regularization term chosen such that the total cost\nfunction (consisting of data consistency and regularization terms) is convex.\nTherefore, sparsity is more strongly promoted than in the standard convex\nformulation, but without sacrificing the attractive aspects of convex\noptimization (unique minimum, robust algorithms, etc.). We use this idea to\nimprove the recently developed 'overlapping group shrinkage' (OGS) algorithm\nfor the denoising of group-sparse signals. The algorithm is applied to the\nproblem of speech enhancement with favorable results in terms of both SNR and\nperceptual quality. \n\n"}
{"id": "1308.5712", "contents": "Title: The Generalized Mean Information Coefficient Abstract: Reshef & Reshef recently published a paper in which they present a method\ncalled the Maximal Information Coefficient (MIC) that can detect all forms of\nstatistical dependence between pairs of variables as sample size goes to\ninfinity. While this method has been praised by some, it has also been\ncriticized for its lack of power in finite samples. We seek to modify MIC so\nthat it has higher power in detecting associations for limited sample sizes.\nHere we present the Generalized Mean Information Coefficient (GMIC), a\ngeneralization of MIC which incorporates a tuning parameter that can be used to\nmodify the complexity of the association favored by the measure. We define GMIC\nand prove it maintains several key asymptotic properties of MIC. Its increased\npower over MIC is demonstrated using a simulation of eight different functional\nrelationships at sixty different noise levels. The results are compared to the\nPearson correlation, distance correlation, and MIC. Simulation results suggest\nthat while generally GMIC has slightly lower power than the distance\ncorrelation measure, it achieves higher power than MIC for many forms of\nunderlying association. For some functional relationships, GMIC surpasses all\nother statistics calculated. Preliminary results suggest choosing a moderate\nvalue of the tuning parameter for GMIC will yield a test that is robust across\nunderlying relationships. GMIC is a promising new method that mitigates the\npower issues suffered by MIC, at the possible expense of equitability.\nNonetheless, distance correlation was in our simulations more powerful for many\nforms of underlying relationships. At a minimum, this work motivates further\nconsideration of maximal information-based nonparametric exploration (MINE)\nmethods as statistical tests of independence. \n\n"}
{"id": "1308.6774", "contents": "Title: Separable Approximations and Decomposition Methods for the Augmented\n  Lagrangian Abstract: In this paper we study decomposition methods based on separable\napproximations for minimizing the augmented Lagrangian. In particular, we study\nand compare the Diagonal Quadratic Approximation Method (DQAM) of Mulvey and\nRuszczy\\'{n}ski and the Parallel Coordinate Descent Method (PCDM) of\nRicht\\'arik and Tak\\'a\\v{c}. We show that the two methods are equivalent for\nfeasibility problems up to the selection of a single step-size parameter.\nFurthermore, we prove an improved complexity bound for PCDM under strong\nconvexity, and show that this bound is at least $8(L'/\\bar{L})(\\omega-1)^2$\ntimes better than the best known bound for DQAM, where $\\omega$ is the degree\nof partial separability and $L'$ and $\\bar{L}$ are the maximum and average of\nthe block Lipschitz constants of the gradient of the quadratic penalty\nappearing in the augmented Lagrangian. \n\n"}
{"id": "1309.1193", "contents": "Title: Confidence-constrained joint sparsity recovery under the Poisson noise\n  model Abstract: Our work is focused on the joint sparsity recovery problem where the common\nsparsity pattern is corrupted by Poisson noise. We formulate the\nconfidence-constrained optimization problem in both least squares (LS) and\nmaximum likelihood (ML) frameworks and study the conditions for perfect\nreconstruction of the original row sparsity and row sparsity pattern. However,\nthe confidence-constrained optimization problem is non-convex. Using convex\nrelaxation, an alternative convex reformulation of the problem is proposed. We\nevaluate the performance of the proposed approach using simulation results on\nsynthetic data and show the effectiveness of proposed row sparsity and row\nsparsity pattern recovery framework. \n\n"}
{"id": "1309.1901", "contents": "Title: Variational Bayes Approximations for Clustering via Mixtures of Normal\n  Inverse Gaussian Distributions Abstract: Parameter estimation for model-based clustering using a finite mixture of\nnormal inverse Gaussian (NIG) distributions is achieved through variational\nBayes approximations. Univariate NIG mixtures and multivariate NIG mixtures are\nconsidered. The use of variational Bayes approximations here is a substantial\ndeparture from the traditional EM approach and alleviates some of the\nassociated computational complexities and uncertainties. Our variational\nalgorithm is applied to simulated and real data. The paper concludes with\ndiscussion and suggestions for future work. \n\n"}
{"id": "1309.3223", "contents": "Title: Partitioning into Expanders Abstract: Let G=(V,E) be an undirected graph, lambda_k be the k-th smallest eigenvalue\nof the normalized laplacian matrix of G. There is a basic fact in algebraic\ngraph theory that lambda_k > 0 if and only if G has at most k-1 connected\ncomponents. We prove a robust version of this fact. If lambda_k>0, then for\nsome 1\\leq \\ell\\leq k-1, V can be {\\em partitioned} into l sets P_1,\\ldots,P_l\nsuch that each P_i is a low-conductance set in G and induces a high conductance\ninduced subgraph. In particular, \\phi(P_i)=O(l^3\\sqrt{\\lambda_l}) and\n\\phi(G[P_i]) >= \\lambda_k/k^2).\n  We make our results algorithmic by designing a simple polynomial time\nspectral algorithm to find such partitioning of G with a quadratic loss in the\ninside conductance of P_i's. Unlike the recent results on higher order\nCheeger's inequality [LOT12,LRTV12], our algorithmic results do not use higher\norder eigenfunctions of G. If there is a sufficiently large gap between\nlambda_k and lambda_{k+1}, more precisely, if \\lambda_{k+1} >= \\poly(k)\nlambda_{k}^{1/4} then our algorithm finds a k partitioning of V into sets\nP_1,...,P_k such that the induced subgraph G[P_i] has a significantly larger\nconductance than the conductance of P_i in G. Such a partitioning may represent\nthe best k clustering of G. Our algorithm is a simple local search that only\nuses the Spectral Partitioning algorithm as a subroutine. We expect to see\nfurther applications of this simple algorithm in clustering applications. \n\n"}
{"id": "1309.5979", "contents": "Title: Asymptotic Analysis of LASSOs Solution Path with Implications for\n  Approximate Message Passing Abstract: This paper concerns the performance of the LASSO (also knows as basis pursuit\ndenoising) for recovering sparse signals from undersampled, randomized, noisy\nmeasurements. We consider the recovery of the signal $x_o \\in \\mathbb{R}^N$\nfrom $n$ random and noisy linear observations $y= Ax_o + w$, where $A$ is the\nmeasurement matrix and $w$ is the noise. The LASSO estimate is given by the\nsolution to the optimization problem $x_o$ with $\\hat{x}_{\\lambda} = \\arg\n\\min_x \\frac{1}{2} \\|y-Ax\\|_2^2 + \\lambda \\|x\\|_1$. Despite major progress in\nthe theoretical analysis of the LASSO solution, little is known about its\nbehavior as a function of the regularization parameter $\\lambda$. In this paper\nwe study two questions in the asymptotic setting (i.e., where $N \\rightarrow\n\\infty$, $n \\rightarrow \\infty$ while the ratio $n/N$ converges to a fixed\nnumber in $(0,1)$): (i) How does the size of the active set\n$\\|\\hat{x}_\\lambda\\|_0/N$ behave as a function of $\\lambda$, and (ii) How does\nthe mean square error $\\|\\hat{x}_{\\lambda} - x_o\\|_2^2/N$ behave as a function\nof $\\lambda$? We then employ these results in a new, reliable algorithm for\nsolving LASSO based on approximate message passing (AMP). \n\n"}
{"id": "1309.6172", "contents": "Title: Generation of tunable wavelength coherent states and heralded single\n  photons for quantum optics applications Abstract: Quantum optics experiments frequently involve interfering single photons and\ncoherent states. In the case of multi-photon experiments this requires that all\nphotons are frequency degenerate. We report a simple and practical approach to\ngenerate coherent states that can be readily tuned to any wavelength required,\nfor example by non-degenerate photon pair creation. We demonstrate this by\nperforming a two-photon (Hong-Ou-Mandel) interference experiment between a\ncoherent state and a pure heralded single photon source. No spectral filtering\nis required on either source, the coherent state constrained by the pump and\nseed lasers and the heralded photon exploits non-local filtering. We expect\nthat such an approach can find a wide range of applications in photonic based\nquantum information science. \n\n"}
{"id": "1309.6487", "contents": "Title: A Unified Framework for Representation-based Subspace Clustering of\n  Out-of-sample and Large-scale Data Abstract: Under the framework of spectral clustering, the key of subspace clustering is\nbuilding a similarity graph which describes the neighborhood relations among\ndata points. Some recent works build the graph using sparse, low-rank, and\n$\\ell_2$-norm-based representation, and have achieved state-of-the-art\nperformance. However, these methods have suffered from the following two\nlimitations. First, the time complexities of these methods are at least\nproportional to the cube of the data size, which make those methods inefficient\nfor solving large-scale problems. Second, they cannot cope with out-of-sample\ndata that are not used to construct the similarity graph. To cluster each\nout-of-sample datum, the methods have to recalculate the similarity graph and\nthe cluster membership of the whole data set. In this paper, we propose a\nunified framework which makes representation-based subspace clustering\nalgorithms feasible to cluster both out-of-sample and large-scale data. Under\nour framework, the large-scale problem is tackled by converting it as\nout-of-sample problem in the manner of \"sampling, clustering, coding, and\nclassifying\". Furthermore, we give an estimation for the error bounds by\ntreating each subspace as a point in a hyperspace. Extensive experimental\nresults on various benchmark data sets show that our methods outperform several\nrecently-proposed scalable methods in clustering large-scale data set. \n\n"}
{"id": "1310.0740", "contents": "Title: Pseudo-Marginal Bayesian Inference for Gaussian Processes Abstract: The main challenges that arise when adopting Gaussian Process priors in\nprobabilistic modeling are how to carry out exact Bayesian inference and how to\naccount for uncertainty on model parameters when making model-based predictions\non out-of-sample data. Using probit regression as an illustrative working\nexample, this paper presents a general and effective methodology based on the\npseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses\nboth of these issues. The results presented in this paper show improvements\nover existing sampling methods to simulate from the posterior distribution over\nthe parameters defining the covariance function of the Gaussian Process prior.\nThis is particularly important as it offers a powerful tool to carry out full\nBayesian inference of Gaussian Process based hierarchic statistical models in\ngeneral. The results also demonstrate that Monte Carlo based integration of all\nmodel parameters is actually feasible in this class of models providing a\nsuperior quantification of uncertainty in predictions. Extensive comparisons\nwith respect to state-of-the-art probabilistic classifiers confirm this\nassertion. \n\n"}
{"id": "1310.1533", "contents": "Title: CAM: Causal additive models, high-dimensional order search and penalized\n  regression Abstract: We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data. \n\n"}
{"id": "1310.1562", "contents": "Title: Dependence Measure for non-additive model Abstract: We proposed a new statistical dependency measure called Copula Dependency\nCoefficient(CDC) for two sets of variables based on copula. It is robust to\noutliers, easy to implement, powerful and appropriate to high-dimensional\nvariables. These properties are important in many applications. Experimental\nresults show that CDC can detect the dependence between variables in both\nadditive and non-additive models. \n\n"}
{"id": "1310.8485", "contents": "Title: SU(2)-invariant depolarization of quantum states of light Abstract: We develop an SU(2)-invariant approach to the depolarization of quantum\nsystems as the effect of random unitary SU(2) transformations. From it we\nderive an SU(2)-invariant Markovian master equation. This is applied to several\nquantum states examining whether nonclassical states are more sensible to\ndepolarization than the classical ones. Furthermore, we show that this\ndepolarization model provides a nontrivial generalization of depolarization\nchannels to states of arbitrary dimension. \n\n"}
{"id": "1311.1644", "contents": "Title: The Maximum Entropy Relaxation Path Abstract: The relaxed maximum entropy problem is concerned with finding a probability\ndistribution on a finite set that minimizes the relative entropy to a given\nprior distribution, while satisfying relaxed max-norm constraints with respect\nto a third observed multinomial distribution. We study the entire relaxation\npath for this problem in detail. We show existence and a geometric description\nof the relaxation path. Specifically, we show that the maximum entropy\nrelaxation path admits a planar geometric description as an increasing,\npiecewise linear function in the inverse relaxation parameter. We derive fast\nalgorithms for tracking the path. In various realistic settings, our algorithms\nrequire $O(n\\log(n))$ operations for probability distributions on $n$ points,\nmaking it possible to handle large problems. Once the path has been recovered,\nwe show that given a validation set, the family of admissible models is reduced\nfrom an infinite family to a small, discrete set. We demonstrate the merits of\nour approach in experiments with synthetic data and discuss its potential for\nthe estimation of compact n-gram language models. \n\n"}
{"id": "1311.2079", "contents": "Title: Nonparametric Multi-group Membership Model for Dynamic Networks Abstract: Relational data-like graphs, networks, and matrices-is often dynamic, where\nthe relational structure evolves over time. A fundamental problem in the\nanalysis of time-varying network data is to extract a summary of the common\nstructure and the dynamics of the underlying relations between the entities.\nHere we build on the intuition that changes in the network structure are driven\nby the dynamics at the level of groups of nodes. We propose a nonparametric\nmulti-group membership model for dynamic networks. Our model contains three\nmain components: We model the birth and death of individual groups with respect\nto the dynamics of the network structure via a distance dependent Indian Buffet\nProcess. We capture the evolution of individual node group memberships via a\nFactorial Hidden Markov model. And, we explain the dynamics of the network\nstructure by explicitly modeling the connectivity structure of groups. We\ndemonstrate our model's capability of identifying the dynamics of latent groups\nin a number of different types of network data. Experimental results show that\nour model provides improved predictive performance over existing dynamic\nnetwork models on future network forecasting and missing link prediction. \n\n"}
{"id": "1311.2236", "contents": "Title: Fast Distribution To Real Regression Abstract: We study the problem of distribution to real-value regression, where one aims\nto regress a mapping $f$ that takes in a distribution input covariate $P\\in\n\\mathcal{I}$ (for a non-parametric family of distributions $\\mathcal{I}$) and\noutputs a real-valued response $Y=f(P) + \\epsilon$. This setting was recently\nstudied, and a \"Kernel-Kernel\" estimator was introduced and shown to have a\npolynomial rate of convergence. However, evaluating a new prediction with the\nKernel-Kernel estimator scales as $\\Omega(N)$. This causes the difficult\nsituation where a large amount of data may be necessary for a low estimation\nrisk, but the computation cost of estimation becomes infeasible when the\ndata-set is too large. To this end, we propose the Double-Basis estimator,\nwhich looks to alleviate this big data problem in two ways: first, the\nDouble-Basis estimator is shown to have a computation complexity that is\nindependent of the number of of instances $N$ when evaluating new predictions\nafter training; secondly, the Double-Basis estimator is shown to have a fast\nrate of convergence for a general class of mappings $f\\in\\mathcal{F}$. \n\n"}
{"id": "1311.3287", "contents": "Title: Nonparametric Estimation of Multi-View Latent Variable Models Abstract: Spectral methods have greatly advanced the estimation of latent variable\nmodels, generating a sequence of novel and efficient algorithms with strong\ntheoretical guarantees. However, current spectral algorithms are largely\nrestricted to mixtures of discrete or Gaussian distributions. In this paper, we\npropose a kernel method for learning multi-view latent variable models,\nallowing each mixture component to be nonparametric. The key idea of the method\nis to embed the joint distribution of a multi-view latent variable into a\nreproducing kernel Hilbert space, and then the latent parameters are recovered\nusing a robust tensor power method. We establish that the sample complexity for\nthe proposed method is quadratic in the number of latent components and is a\nlow order polynomial in the other relevant parameters. Thus, our non-parametric\ntensor approach to learning latent variable models enjoys good sample and\ncomputational efficiencies. Moreover, the non-parametric tensor power method\ncompares favorably to EM algorithm and other existing spectral algorithms in\nour experiments. \n\n"}
{"id": "1311.3492", "contents": "Title: High-dimensional learning of linear causal networks via inverse\n  covariance estimation Abstract: We establish a new framework for statistical estimation of directed acyclic\ngraphs (DAGs) when data are generated from a linear, possibly non-Gaussian\nstructural equation model. Our framework consists of two parts: (1) inferring\nthe moralized graph from the support of the inverse covariance matrix; and (2)\nselecting the best-scoring graph amongst DAGs that are consistent with the\nmoralized graph. We show that when the error variances are known or estimated\nto close enough precision, the true DAG is the unique minimizer of the score\ncomputed using the reweighted squared l_2-loss. Our population-level results\nhave implications for the identifiability of linear SEMs when the error\ncovariances are specified up to a constant multiple. On the statistical side,\nwe establish rigorous conditions for high-dimensional consistency of our\ntwo-part algorithm, defined in terms of a \"gap\" between the true DAG and the\nnext best candidate. Finally, we demonstrate that dynamic programming may be\nused to select the optimal DAG in linear time when the treewidth of the\nmoralized graph is bounded. \n\n"}
{"id": "1311.5479", "contents": "Title: Learning Pairwise Graphical Models with Nonlinear Sufficient Statistics Abstract: We investigate a generic problem of learning pairwise exponential family\ngraphical models with pairwise sufficient statistics defined by a global\nmapping function, e.g., Mercer kernels. This subclass of pairwise graphical\nmodels allow us to flexibly capture complex interactions among variables beyond\npairwise product. We propose two $\\ell_1$-norm penalized maximum likelihood\nestimators to learn the model parameters from i.i.d. samples. The first one is\na joint estimator which estimates all the parameters simultaneously. The second\none is a node-wise conditional estimator which estimates the parameters\nindividually for each node. For both estimators, we show that under proper\nconditions the extra flexibility gained in our model comes at almost no cost of\nstatistical and computational efficiency. We demonstrate the advantages of our\nmodel over state-of-the-art methods on synthetic and real datasets. \n\n"}
{"id": "1311.6107", "contents": "Title: Off-policy reinforcement learning for $ H_\\infty $ control design Abstract: The $H_\\infty$ control design problem is considered for nonlinear systems\nwith unknown internal system model. It is known that the nonlinear $ H_\\infty $\ncontrol problem can be transformed into solving the so-called\nHamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial\ndifferential equation that is generally impossible to be solved analytically.\nEven worse, model-based approaches cannot be used for approximately solving HJI\nequation, when the accurate system model is unavailable or costly to obtain in\npractice. To overcome these difficulties, an off-policy reinforcement leaning\n(RL) method is introduced to learn the solution of HJI equation from real\nsystem data instead of mathematical system model, and its convergence is\nproved. In the off-policy RL method, the system data can be generated with\narbitrary policies rather than the evaluating policy, which is extremely\nimportant and promising for practical systems. For implementation purpose, a\nneural network (NN) based actor-critic structure is employed and a least-square\nNN weight update algorithm is derived based on the method of weighted\nresiduals. Finally, the developed NN-based off-policy RL method is tested on a\nlinear F16 aircraft plant, and further applied to a rotational/translational\nactuator system. \n\n"}
{"id": "1311.7320", "contents": "Title: Bayesian Inference for Gaussian Process Classifiers with Annealing and\n  Pseudo-Marginal MCMC Abstract: Kernel methods have revolutionized the fields of pattern recognition and\nmachine learning. Their success, however, critically depends on the choice of\nkernel parameters. Using Gaussian process (GP) classification as a working\nexample, this paper focuses on Bayesian inference of covariance (kernel)\nparameters using Markov chain Monte Carlo (MCMC) methods. The motivation is\nthat, compared to standard optimization of kernel parameters, they have been\nsystematically demonstrated to be superior in quantifying uncertainty in\npredictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as a\npractical inference tool for GP models. In particular, it amounts in replacing\nthe analytically intractable marginal likelihood by an unbiased estimate\nobtainable by approximate methods and importance sampling. After discussing the\npotential drawbacks in employing importance sampling, this paper proposes the\napplication of annealed importance sampling. The results empirically\ndemonstrate that compared to importance sampling, annealed importance sampling\ncan reduce the variance of the estimate of the marginal likelihood\nexponentially in the number of data at a computational cost that scales only\npolynomially. The results on real data demonstrate that employing annealed\nimportance sampling in the Pseudo-Marginal MCMC approach represents a step\nforward in the development of fully automated exact inference engines for GP\nmodels. \n\n"}
{"id": "1312.4426", "contents": "Title: Optimization for Compressed Sensing: the Simplex Method and Kronecker\n  Sparsification Abstract: In this paper we present two new approaches to efficiently solve large-scale\ncompressed sensing problems. These two ideas are independent of each other and\ncan therefore be used either separately or together. We consider all\npossibilities.\n  For the first approach, we note that the zero vector can be taken as the\ninitial basic (infeasible) solution for the linear programming problem and\ntherefore, if the true signal is very sparse, some variants of the simplex\nmethod can be expected to take only a small number of pivots to arrive at a\nsolution. We implemented one such variant and demonstrate a dramatic\nimprovement in computation time on very sparse signals.\n  The second approach requires a redesigned sensing mechanism in which the\nvector signal is stacked into a matrix. This allows us to exploit the Kronecker\ncompressed sensing (KCS) mechanism. We show that the Kronecker sensing requires\nstronger conditions for perfect recovery compared to the original vector\nproblem. However, the Kronecker sensing, modeled correctly, is a much sparser\nlinear optimization problem. Hence, algorithms that benefit from sparse problem\nrepresentation, such as interior-point methods, can solve the Kronecker sensing\nproblems much faster than the corresponding vector problem. In our numerical\nstudies, we demonstrate a ten-fold improvement in the computation time. \n\n"}
{"id": "1312.4469", "contents": "Title: Measurement of sub-pulse-width temporal delays via spectral interference\n  induced by weak value amplification Abstract: We demonstrate experimentally a scheme to measure small temporal delays, much\nsmaller than the pulse width, between optical pulses. Specifically, we observe\nan interference effect, based on the concepts of quantum weak measurements and\nweak value amplification, through which a sub-pulsewidth temporal delay between\ntwo femtosecond pulses induces a measurable shift of the central frequency of\nthe pulse. The amount of frequency shift, and the accompanying losses of the\nmeasurement, can be tailored by post-selecting different states of\npolarization. Our scheme requires only spectrum measurements and linear optics\nelements, hence greatly facilitating its implementation. Thus it appears as a\npromising technique for measuring small and rapidly varying temporal delays. \n\n"}
{"id": "1312.6026", "contents": "Title: How to Construct Deep Recurrent Neural Networks Abstract: In this paper, we explore different ways to extend a recurrent neural network\n(RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in\nan RNN is not as clear as it is in feedforward neural networks. By carefully\nanalyzing and understanding the architecture of an RNN, however, we find three\npoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)\nhidden-to-hidden transition and (3) hidden-to-output function. Based on this\nobservation, we propose two novel architectures of a deep RNN which are\northogonal to an earlier attempt of stacking multiple recurrent layers to build\na deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an\nalternative interpretation of these deep RNNs using a novel framework based on\nneural operators. The proposed deep RNNs are empirically evaluated on the tasks\nof polyphonic music prediction and language modeling. The experimental result\nsupports our claim that the proposed deep RNNs benefit from the depth and\noutperform the conventional, shallow RNNs. \n\n"}
{"id": "1312.6137", "contents": "Title: An electrically injected photon-pair source at room temperature Abstract: One of the main challenges for future quantum information technologies is\nminiaturization and integration of high performance components in a single\nchip. In this context, electrically driven sources of non-classical states of\nlight have a clear advantage over optically driven ones. Here we demonstrate\nthe first electrically driven semiconductor source of photon pairs working at\nroom temperature and telecom wavelength. The device is based on type-II\nintracavity Spontaneous Parametric Down-Conversion in an AlGaAs laser diode and\ngenerates pairs at 1.57 $\\mu$m. Time-correlation measurements of the emitted\npairs give an internal generation efficiency of $7 \\times 10^{-11}$\npairs/injected electron. The capability of our platform to support generation,\nmanipulation and detection of photons opens the way to the demonstration of\nmassively parallel systems for complex quantum operations. \n\n"}
{"id": "1312.6211", "contents": "Title: An Empirical Investigation of Catastrophic Forgetting in Gradient-Based\n  Neural Networks Abstract: Catastrophic forgetting is a problem faced by many machine learning models\nand algorithms. When trained on one task, then trained on a second task, many\nmachine learning models \"forget\" how to perform the first task. This is widely\nbelieved to be a serious problem for neural networks. Here, we investigate the\nextent to which the catastrophic forgetting problem occurs for modern neural\nnetworks, comparing both established and recent gradient-based training\nalgorithms and activation functions. We also examine the effect of the\nrelationship between the first task and the second task on catastrophic\nforgetting. We find that it is always best to train using the dropout\nalgorithm--the dropout algorithm is consistently best at adapting to the new\ntask, remembering the old task, and has the best tradeoff curve between these\ntwo extremes. We find that different tasks and relationships between tasks\nresult in very different rankings of activation function performance. This\nsuggests the choice of activation function should always be cross-validated. \n\n"}
{"id": "1312.6607", "contents": "Title: Using Latent Binary Variables for Online Reconstruction of Large Scale\n  Systems Abstract: We propose a probabilistic graphical model realizing a minimal encoding of\nreal variables dependencies based on possibly incomplete observation and an\nempirical cumulative distribution function per variable. The target application\nis a large scale partially observed system, like e.g. a traffic network, where\na small proportion of real valued variables are observed, and the other\nvariables have to be predicted. Our design objective is therefore to have good\nscalability in a real-time setting. Instead of attempting to encode the\ndependencies of the system directly in the description space, we propose a way\nto encode them in a latent space of binary variables, reflecting a rough\nperception of the observable (congested/non-congested for a traffic road). The\nmethod relies in part on message passing algorithms, i.e. belief propagation,\nbut the core of the work concerns the definition of meaningful latent variables\nassociated to the variables of interest and their pairwise dependencies.\nNumerical experiments demonstrate the applicability of the method in practice. \n\n"}
{"id": "1401.0852", "contents": "Title: Concave Penalized Estimation of Sparse Gaussian Bayesian Networks Abstract: We develop a penalized likelihood estimation framework to estimate the\nstructure of Gaussian Bayesian networks from observational data. In contrast to\nrecent methods which accelerate the learning problem by restricting the search\nspace, our main contribution is a fast algorithm for score-based structure\nlearning which does not restrict the search space in any way and works on\nhigh-dimensional datasets with thousands of variables. Our use of concave\nregularization, as opposed to the more popular $\\ell_0$ (e.g. BIC) penalty, is\nnew. Moreover, we provide theoretical guarantees which generalize existing\nasymptotic results when the underlying distribution is Gaussian. Most notably,\nour framework does not require the existence of a so-called faithful DAG\nrepresentation, and as a result the theory must handle the inherent\nnonidentifiability of the estimation problem in a novel way. Finally, as a\nmatter of independent interest, we provide a comprehensive comparison of our\napproach to several standard structure learning methods using open-source\npackages developed for the R language. Based on these experiments, we show that\nour algorithm is significantly faster than other competing methods while\nobtaining higher sensitivity with comparable false discovery rates for\nhigh-dimensional data. In particular, the total runtime for our method to\ngenerate a solution path of 20 estimates for DAGs with 8000 nodes is around one\nhour. \n\n"}
{"id": "1401.1842", "contents": "Title: Robust Large Scale Non-negative Matrix Factorization using Proximal\n  Point Algorithm Abstract: A robust algorithm for non-negative matrix factorization (NMF) is presented\nin this paper with the purpose of dealing with large-scale data, where the\nseparability assumption is satisfied. In particular, we modify the Linear\nProgramming (LP) algorithm of [9] by introducing a reduced set of constraints\nfor exact NMF. In contrast to the previous approaches, the proposed algorithm\ndoes not require the knowledge of factorization rank (extreme rays [3] or\ntopics [7]). Furthermore, motivated by a similar problem arising in the context\nof metabolic network analysis [13], we consider an entirely different regime\nwhere the number of extreme rays or topics can be much larger than the\ndimension of the data vectors. The performance of the algorithm for different\nsynthetic data sets are provided. \n\n"}
{"id": "1401.7376", "contents": "Title: An intensity-dependent quantum Rabi model: Spectrum, SUSY partner and\n  optical simulation Abstract: We study an intensity-dependent quantum Rabi model that can be written in\nterms of SU(1,1) group elements and is related to the Buck-Sukumar model for\nthe Bargmann parameter $k=1/2$. The spectrum seems to present avoiding\ncrossings for all valid parameter sets and, thus, may be integrable. For a\ndegenerate qubit, the model is soluble and we construct an unbroken\nsupersymmetric parter for it. We discuss the classical simulation of the\ngeneral model in photonic lattices and show that it presents quasi-periodic\nreconstruction for a given initial state and parameter set. \n\n"}
{"id": "1401.7702", "contents": "Title: A Spectral Framework for Anomalous Subgraph Detection Abstract: A wide variety of application domains are concerned with data consisting of\nentities and their relationships or connections, formally represented as\ngraphs. Within these diverse application areas, a common problem of interest is\nthe detection of a subset of entities whose connectivity is anomalous with\nrespect to the rest of the data. While the detection of such anomalous\nsubgraphs has received a substantial amount of attention, no\napplication-agnostic framework exists for analysis of signal detectability in\ngraph-based data. In this paper, we describe a framework that enables such\nanalysis using the principal eigenspace of a graph's residuals matrix, commonly\ncalled the modularity matrix in community detection. Leveraging this analytical\ntool, we show that the framework has a natural power metric in the spectral\nnorm of the anomalous subgraph's adjacency matrix (signal power) and of the\nbackground graph's residuals matrix (noise power). We propose several\nalgorithms based on spectral properties of the residuals matrix, with more\ncomputationally expensive techniques providing greater detection power.\nDetection and identification performance are presented for a number of signal\nand noise models, including clusters and bipartite foregrounds embedded into\nsimple random backgrounds as well as graphs with community structure and\nrealistic degree distributions. The trends observed verify intuition gleaned\nfrom other signal processing areas, such as greater detection power when the\nsignal is embedded within a less active portion of the background. We\ndemonstrate the utility of the proposed techniques in detecting small, highly\nanomalous subgraphs in real graphs derived from Internet traffic and product\nco-purchases. \n\n"}
{"id": "1402.1700", "contents": "Title: On the Prediction Performance of the Lasso Abstract: Although the Lasso has been extensively studied, the relationship between its\nprediction performance and the correlations of the covariates is not fully\nunderstood. In this paper, we give new insights into this relationship in the\ncontext of multiple linear regression. We show, in particular, that the\nincorporation of a simple correlation measure into the tuning parameter can\nlead to a nearly optimal prediction performance of the Lasso even for highly\ncorrelated covariates. However, we also reveal that for moderately correlated\ncovariates, the prediction performance of the Lasso can be mediocre\nirrespective of the choice of the tuning parameter. We finally show that our\nresults also lead to near-optimal rates for the least-squares estimator with\ntotal variation penalty. \n\n"}
{"id": "1402.1783", "contents": "Title: Active Clustering with Model-Based Uncertainty Reduction Abstract: Semi-supervised clustering seeks to augment traditional clustering methods by\nincorporating side information provided via human expertise in order to\nincrease the semantic meaningfulness of the resulting clusters. However, most\ncurrent methods are \\emph{passive} in the sense that the side information is\nprovided beforehand and selected randomly. This may require a large number of\nconstraints, some of which could be redundant, unnecessary, or even detrimental\nto the clustering results. Thus in order to scale such semi-supervised\nalgorithms to larger problems it is desirable to pursue an \\emph{active}\nclustering method---i.e. an algorithm that maximizes the effectiveness of the\navailable human labor by only requesting human input where it will have the\ngreatest impact. Here, we propose a novel online framework for active\nsemi-supervised spectral clustering that selects pairwise constraints as\nclustering proceeds, based on the principle of uncertainty reduction. Using a\nfirst-order Taylor expansion, we decompose the expected uncertainty reduction\nproblem into a gradient and a step-scale, computed via an application of matrix\nperturbation theory and cluster-assignment entropy, respectively. The resulting\nmodel is used to estimate the uncertainty reduction potential of each sample in\nthe dataset. We then present the human user with pairwise queries with respect\nto only the best candidate sample. We evaluate our method using three different\nimage datasets (faces, leaves and dogs), a set of common UCI machine learning\ndatasets and a gene dataset. The results validate our decomposition formulation\nand show that our method is consistently superior to existing state-of-the-art\ntechniques, as well as being robust to noise and to unknown numbers of\nclusters. \n\n"}
{"id": "1402.1869", "contents": "Title: On the Number of Linear Regions of Deep Neural Networks Abstract: We study the complexity of functions computable by deep feedforward neural\nnetworks with piecewise linear activations in terms of the symmetries and the\nnumber of linear regions that they have. Deep networks are able to sequentially\nmap portions of each layer's input-space to the same output. In this way, deep\nmodels compute functions that react equally to complicated patterns of\ndifferent inputs. The compositional structure of these functions enables them\nto re-use pieces of computation exponentially often in terms of the network's\ndepth. This paper investigates the complexity of such compositional maps and\ncontributes new theoretical results regarding the advantage of depth for neural\nnetworks with piecewise linear activation functions. In particular, our\nanalysis is not specific to a single family of models, and as an example, we\nemploy it for rectifier and maxout networks. We improve complexity bounds from\npre-existing work and investigate the behavior of units in higher layers. \n\n"}
{"id": "1402.2423", "contents": "Title: Interface between path and OAM entanglement for high-dimensional\n  photonic quantum information Abstract: Photonics has become a mature field of quantum information science, where\nintegrated optical circuits offer a way to scale the complexity of the setup as\nwell as the dimensionality of the quantum state. On photonic chips, paths are\nthe natural way to encode information. To distribute those high-dimensional\nquantum states over large distances, transverse spatial modes, like orbital\nangular momentum (OAM) possessing Laguerre Gauss modes, are favourable as\nflying information carriers. Here we demonstrate a quantum interface between\nthese two vibrant photonic fields. We create three-dimensional path\nentanglement between two photons in a non-linear crystal and use a mode sorter\nas the quantum interface to transfer the entanglement to the OAM degree of\nfreedom. Thus our results show a novel, flexible way to create high-dimensional\nspatial mode entanglement. Moreover, they pave the way to implement broad\ncomplex quantum networks where high-dimensionally entangled states could be\ndistributed over distant photonic chips. \n\n"}
{"id": "1402.5077", "contents": "Title: Group-sparse Matrix Recovery Abstract: We apply the OSCAR (octagonal selection and clustering algorithms for\nregression) in recovering group-sparse matrices (two-dimensional---2D---arrays)\nfrom compressive measurements. We propose a 2D version of OSCAR (2OSCAR)\nconsisting of the $\\ell_1$ norm and the pair-wise $\\ell_{\\infty}$ norm, which\nis convex but non-differentiable. We show that the proximity operator of 2OSCAR\ncan be computed based on that of OSCAR. The 2OSCAR problem can thus be\nefficiently solved by state-of-the-art proximal splitting algorithms.\nExperiments on group-sparse 2D array recovery show that 2OSCAR regularization\nsolved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm\n(with debiasing) yields the most accurate results. \n\n"}
{"id": "1402.5836", "contents": "Title: Avoiding pathologies in very deep networks Abstract: Choosing appropriate architectures and regularization strategies for deep\nnetworks is crucial to good predictive performance. To shed light on this\nproblem, we analyze the analogous problem of constructing useful priors on\ncompositions of functions. Specifically, we study the deep Gaussian process, a\ntype of infinitely-wide, deep neural network. We show that in standard\narchitectures, the representational capacity of the network tends to capture\nfewer degrees of freedom as the number of layers increases, retaining only a\nsingle degree of freedom in the limit. We propose an alternate network\narchitecture which does not suffer from this pathology. We also examine deep\ncovariance functions, obtained by composing infinitely many feature transforms.\nLastly, we characterize the class of models obtained by performing dropout on\nGaussian processes. \n\n"}
{"id": "1402.7344", "contents": "Title: An Incidence Geometry approach to Dictionary Learning Abstract: We study the Dictionary Learning (aka Sparse Coding) problem of obtaining a\nsparse representation of data points, by learning \\emph{dictionary vectors}\nupon which the data points can be written as sparse linear combinations. We\nview this problem from a geometry perspective as the spanning set of a subspace\narrangement, and focus on understanding the case when the underlying hypergraph\nof the subspace arrangement is specified. For this Fitted Dictionary Learning\nproblem, we completely characterize the combinatorics of the associated\nsubspace arrangements (i.e.\\ their underlying hypergraphs). Specifically, a\ncombinatorial rigidity-type theorem is proven for a type of geometric incidence\nsystem. The theorem characterizes the hypergraphs of subspace arrangements that\ngenerically yield (a) at least one dictionary (b) a locally unique dictionary\n(i.e.\\ at most a finite number of isolated dictionaries) of the specified size.\nWe are unaware of prior application of combinatorial rigidity techniques in the\nsetting of Dictionary Learning, or even in machine learning. We also provide a\nsystematic classification of problems related to Dictionary Learning together\nwith various algorithms, their assumptions and performance. \n\n"}
{"id": "1403.0657", "contents": "Title: PT-Symmetric Phonon Laser Abstract: By exploiting recent developments associated with coupled microcavities, we\nintroduce the concept of PT-symmetric phonon laser with balanced gain and loss.\nThis is accomplished by introducing gain to one of the microcavities such that\nit balances the passive loss of the other. In the vicinity of the gain-loss\nbalance, a strong nonlinear relation emerges between the intracavity photon\nintensity and the input power. This then leads to a giant enhancement of both\noptical pressure and mechanical gain, resulting in a highly efficient\nphonon-lasing action. These results provide a promising approach for\nmanipulating optomechanical systems through PT-symmetric concepts. Potential\napplications range from enhancing mechanical cooling to designing phonon-laser\namplifiers. \n\n"}
{"id": "1403.4204", "contents": "Title: PT-symmetry breaking with divergent potentials: lattice and continuum\n  cases Abstract: We investigate the parity- and time-reversal ($\\mathcal{PT}$)-symmetry\nbreaking in lattice models in the presence of long-ranged, non-hermitian,\n$\\mathcal{PT}$-symmetric potentials that remain finite or become divergent in\nthe continuum limit. By scaling analysis of the fragile $\\mathcal{PT}$\nthreshold for an open finite lattice, we show that continuum loss-gain\npotentials $V_\\alpha(x)\\propto i |x|^\\alpha \\mathrm{sign}(x)$ have a positive\n$\\mathcal{PT}$-breaking threshold for $\\alpha>-2$, and a zero threshold for\n$\\alpha\\leq -2$. When $\\alpha<0$ localized states with complex (conjugate)\nenergies in the continuum energy-band occur at higher loss-gain strengths. We\ninvestigate the signatures of $\\mathcal{PT}$-symmetry breaking in coupled\nwaveguides, and show that the emergence of localized states dramatically\nshortens the relevant time-scale in the $\\mathcal{PT}$-symmetry broken region. \n\n"}
{"id": "1404.0593", "contents": "Title: Highly efficient generation of single-mode photon pairs using a\n  crystalline whispering gallery mode resonator Abstract: We report a highly efficient source of narrow-band photon pairs based on\nparametric down-conversion in a crystalline whispering gallery mode resonator.\nRemarkably, each photon of a pair is strictly emitted into a single spatial and\ntemporal mode, as witnessed by Glaubers autocorrelation function. We explore\nthe phase-matching conditions in spherical geometries, and determine the\nrequirements of the single-mode operation. Understanding these conditions has\nallowed us to experimentally demonstrate a single-mode pair-detection rate of\n$0.97 \\cdot 10^6$ pairs/s per mW pump power per 20 MHz bandwidth without the\nneed of additional filter cavities. \n\n"}
{"id": "1404.4412", "contents": "Title: Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness Abstract: Nonnegative Tucker decomposition (NTD) is a powerful tool for the extraction\nof nonnegative parts-based and physically meaningful latent components from\nhigh-dimensional tensor data while preserving the natural multilinear structure\nof data. However, as the data tensor often has multiple modes and is\nlarge-scale, existing NTD algorithms suffer from a very high computational\ncomplexity in terms of both storage and computation time, which has been one\nmajor obstacle for practical applications of NTD. To overcome these\ndisadvantages, we show how low (multilinear) rank approximation (LRA) of\ntensors is able to significantly simplify the computation of the gradients of\nthe cost function, upon which a family of efficient first-order NTD algorithms\nare developed. Besides dramatically reducing the storage complexity and running\ntime, the new algorithms are quite flexible and robust to noise because any\nwell-established LRA approaches can be applied. We also show how nonnegativity\nincorporating sparsity substantially improves the uniqueness property and\npartially alleviates the curse of dimensionality of the Tucker decompositions.\nSimulation results on synthetic and real-world data justify the validity and\nhigh efficiency of the proposed NTD algorithms. \n\n"}
{"id": "1404.6000", "contents": "Title: Robust and computationally feasible community detection in the presence\n  of arbitrary outlier nodes Abstract: Community detection, which aims to cluster $N$ nodes in a given graph into\n$r$ distinct groups based on the observed undirected edges, is an important\nproblem in network data analysis. In this paper, the popular stochastic block\nmodel (SBM) is extended to the generalized stochastic block model (GSBM) that\nallows for adversarial outlier nodes, which are connected with the other nodes\nin the graph in an arbitrary way. Under this model, we introduce a procedure\nusing convex optimization followed by $k$-means algorithm with $k=r$. Both\ntheoretical and numerical properties of the method are analyzed. A theoretical\nguarantee is given for the procedure to accurately detect the communities with\nsmall misclassification rate under the setting where the number of clusters can\ngrow with $N$. This theoretical result admits to the best-known result in the\nliterature of computationally feasible community detection in SBM without\noutliers. Numerical results show that our method is both computationally fast\nand robust to different kinds of outliers, while some popular computationally\nfast community detection algorithms, such as spectral clustering applied to\nadjacency matrices or graph Laplacians, may fail to retrieve the major clusters\ndue to a small portion of outliers. We apply a slight modification of our\nmethod to a political blogs data set, showing that our method is competent in\npractice and comparable to existing computationally feasible methods in the\nliterature. To the best of the authors' knowledge, our result is the first in\nthe literature in terms of clustering communities with fast growing numbers\nunder the GSBM where a portion of arbitrary outlier nodes exist. \n\n"}
{"id": "1404.6473", "contents": "Title: Quantifying Uncertainty in Random Forests via Confidence Intervals and\n  Hypothesis Tests Abstract: This work develops formal statistical inference procedures for machine\nlearning ensemble methods. Ensemble methods based on bootstrapping, such as\nbagging and random forests, have improved the predictive accuracy of individual\ntrees, but fail to provide a framework in which distributional results can be\neasily determined. Instead of aggregating full bootstrap samples, we consider\npredicting by averaging over trees built on subsamples of the training set and\ndemonstrate that the resulting estimator takes the form of a U-statistic. As\nsuch, predictions for individual feature vectors are asymptotically normal,\nallowing for confidence intervals to accompany predictions. In practice, a\nsubset of subsamples is used for computational speed; here our estimators take\nthe form of incomplete U-statistics and equivalent results are derived. We\nfurther demonstrate that this setup provides a framework for testing the\nsignificance of features. Moreover, the internal estimation method we develop\nallows us to estimate the variance parameters and perform these inference\nprocedures at no additional computational cost. Simulations and illustrations\non a real dataset are provided. \n\n"}
{"id": "1405.2690", "contents": "Title: Policy Gradients for CVaR-Constrained MDPs Abstract: We study a risk-constrained version of the stochastic shortest path (SSP)\nproblem, where the risk measure considered is Conditional Value-at-Risk (CVaR).\nWe propose two algorithms that obtain a locally risk-optimal policy by\nemploying four tools: stochastic approximation, mini batches, policy gradients\nand importance sampling. Both the algorithms incorporate a CVaR estimation\nprocedure, along the lines of Bardou et al. [2009], which in turn is based on\nRockafellar-Uryasev's representation for CVaR and utilize the likelihood ratio\nprinciple for estimating the gradient of the sum of one cost function\n(objective of the SSP) and the gradient of the CVaR of the sum of another cost\nfunction (in the constraint of SSP). The algorithms differ in the manner in\nwhich they approximate the CVaR estimates/necessary gradients - the first\nalgorithm uses stochastic approximation, while the second employ mini-batches\nin the spirit of Monte Carlo methods. We establish asymptotic convergence of\nboth the algorithms. Further, since estimating CVaR is related to rare-event\nsimulation, we incorporate an importance sampling based variance reduction\nscheme into our proposed algorithms. \n\n"}
{"id": "1405.2694", "contents": "Title: Strain-optic active control for quantum integrated photonics Abstract: We present a practical method for active phase control on a photonic chip\nthat has immediate applications in quantum photonics. Our approach uses\nstrain-optic modification of the refractive index of individual waveguides,\neffected by a millimeter-scale mechanical actuator. The resulting phase change\nof propagating optical fields is rapid and polarization-dependent, enabling\nquantum applications that require active control and polarization encoding. We\ndemonstrate strain-optic control of non-classical states of light in silica,\nshowing the generation of 2-photon polarisation N00N states by manipulating\nHong-Ou-Mandel interference. We also demonstrate switching times of a few\nmicroseconds, which are sufficient for silica-based feed-forward control of\nphotonic quantum states. \n\n"}
{"id": "1405.2959", "contents": "Title: Type I parametric down conversion of highly focused Gaussian beams in\n  finite length crystals Abstract: We study the correlations in wave vector space of photon pairs generated by\ntype I spontaneous parametric down conversion using a Gaussian pump beam. We\nanalyze both moderate focused and highly focused regimes taking special\nattention to the angular spectrum and the conditional angular spectrum. Simple\nanalytic expressions are derived that allow us to study in detail the\ndependence of these spectra on the waist of the source and the length of the\nnonlinear crystal. These expressions are in good agreement with numerical\nexpectations and reported experimental results. They are used to make a\nsystematic search of optimization parameters that improve the feasibility of\nusing highly focused Gaussian beams to generate idler and signal photons with\npredetermined mean values and spread of their transverse wave vectors. \n\n"}
{"id": "1405.4404", "contents": "Title: Generation and delayed retrieval of spatially multimode Raman scattering\n  in warm rubidium vapors Abstract: We apply collective Raman scattering to create, store and retrieve spatially\nmultimode light in warm rubidium-87 vapors. The light is created in a\nspontaneous Stokes scattering process. This is accompanied by the creation of\ncounterpart collective excitations in the atomic ensemble -- the spin waves.\nAfter a certain storage time we coherently convert the spin waves into the\nlight in deterministic anti-Stokes scattering. The whole process can be\nregarded as a delayed four-wave mixing which produces pairs of correlated,\ndelayed random images. Storage of higher order spatial modes up to microseconds\nis possible owing to usage of a buffer gas. We study the performance of the\nRaman scattering, storage and retrieval of collective excitations focusing on\nspatial effects and the influence of decoherence caused by diffusion of\nrubidium atoms in different buffer gases. We quantify the number of modes\ncreated and retrieved by analyzing statistical correlations of intensity\nfluctuations between portions of the light scattered in the far field. \n\n"}
{"id": "1405.6070", "contents": "Title: Empirical Bayes Estimation for the Stochastic Blockmodel Abstract: Inference for the stochastic blockmodel is currently of burgeoning interest\nin the statistical community, as well as in various application domains as\ndiverse as social networks, citation networks, brain connectivity networks\n(connectomics), etc. Recent theoretical developments have shown that spectral\nembedding of graphs yields tractable distributional results; in particular, a\nrandom dot product latent position graph formulation of the stochastic\nblockmodel informs a mixture of normal distributions for the adjacency spectral\nembedding. We employ this new theory to provide an empirical Bayes methodology\nfor estimation of block memberships of vertices in a random graph drawn from\nthe stochastic blockmodel, and demonstrate its practical utility. The posterior\ninference is conducted using a Metropolis-within-Gibbs algorithm. The theory\nand methods are illustrated through Monte Carlo simulation studies, both within\nthe stochastic blockmodel and beyond, and experimental results on a Wikipedia\ndata set are presented. \n\n"}
{"id": "1406.0764", "contents": "Title: Constructing Dynamic Treatment Regimes in Infinite-Horizon Settings Abstract: The application of existing methods for constructing optimal dynamic\ntreatment regimes is limited to cases where investigators are interested in\noptimizing a utility function over a fixed period of time (finite horizon). In\nthis manuscript, we develop an inferential procedure based on temporal\ndifference residuals for optimal dynamic treatment regimes in infinite-horizon\nsettings, where there is no a priori fixed end of follow-up point. The proposed\nmethod can be used to determine the optimal regime in chronic diseases where\npatients are monitored and treated throughout their life. We derive large\nsample results necessary for conducting inference. We also simulate a cohort of\npatients with diabetes to mimic the third wave of the National Health and\nNutrition Examination Survey, and we examine the performance of the proposed\nmethod in controlling the level of hemoglobin A1c. Supplementary materials for\nthis article are available online. \n\n"}
{"id": "1406.1078", "contents": "Title: Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation Abstract: In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases. \n\n"}
{"id": "1406.1650", "contents": "Title: Strong photon antibunching of symmetric and antisymmetric modes in\n  weakly nonlinear photonic molecules Abstract: We study the photon statistics of symmetric and antisymmetric modes in a\nphotonic molecule consisting of two linearly coupled nonlinear cavity modes.\nOur calculations show that strong photon antibunching of both symmetric and\nantisymmetric modes can be obtained even when the nonlinearity in the photonic\nmolecule is weak. The strong antibunching effect results from the destructive\ninterference between different paths for two-photon excitation. Moreover, we\nfind that the optimal frequency detunings for strong photon antibunching in the\nsymmetric and antisymmetric modes are linearly dependent on the coupling\nstrength between the cavity modes in the photonic molecule. This implies that\nthe photonic molecules can be used to generate tunable single-photon sources by\ntuning the values of the coupling strength between the cavity modes with weak\nnonlinearity. \n\n"}
{"id": "1406.2602", "contents": "Title: Graph Approximation and Clustering on a Budget Abstract: We consider the problem of learning from a similarity matrix (such as\nspectral clustering and lowd imensional embedding), when computing pairwise\nsimilarities are costly, and only a limited number of entries can be observed.\nWe provide a theoretical analysis using standard notions of graph\napproximation, significantly generalizing previous results (which focused on\nspectral clustering with two clusters). We also propose a new algorithmic\napproach based on adaptive sampling, which experimentally matches or improves\non previous methods, while being considerably more general and computationally\ncheaper. \n\n"}
{"id": "1406.3955", "contents": "Title: Increasing the photon collection rate from a single NV center with a\n  silver mirror Abstract: In the pursuit of realizing quantum optical networks, a large variety of\ndifferent approaches have been studied to achieve a single photon source\non-demand. The common goal for these approaches is to harvest all the emission\nfrom a quantum emitter into a single spatial optical mode while maintaining a\nhigh signal-to-noise ratio. In this work, we use a single nitrogen vacancy\ncenter in diamond as a quantum emitter operating at ambient conditions and we\ndemonstrate an increased photon count rate up to a factor of 1.76 by placing a\nsilver mirror fabricated on the end facet of an optical fiber near the emitter. \n\n"}
{"id": "1406.5986", "contents": "Title: A Statistical Perspective on Randomized Sketching for Ordinary\n  Least-Squares Abstract: We consider statistical as well as algorithmic aspects of solving large-scale\nleast-squares (LS) problems using randomized sketching algorithms. For a LS\nproblem with input data $(X, Y) \\in \\mathbb{R}^{n \\times p} \\times\n\\mathbb{R}^n$, sketching algorithms use a sketching matrix, $S\\in\\mathbb{R}^{r\n\\times n}$ with $r \\ll n$. Then, rather than solving the LS problem using the\nfull data $(X,Y)$, sketching algorithms solve the LS problem using only the\nsketched data $(SX, SY)$. Prior work has typically adopted an algorithmic\nperspective, in that it has made no statistical assumptions on the input $X$\nand $Y$, and instead it has been assumed that the data $(X,Y)$ are fixed and\nworst-case (WC). Prior results show that, when using sketching matrices such as\nrandom projections and leverage-score sampling algorithms, with $p < r \\ll n$,\nthe WC error is the same as solving the original problem, up to a small\nconstant. From a statistical perspective, we typically consider the\nmean-squared error performance of randomized sketching algorithms, when data\n$(X, Y)$ are generated according to a statistical model $Y = X \\beta +\n\\epsilon$, where $\\epsilon$ is a noise process. We provide a rigorous\ncomparison of both perspectives leading to insights on how they differ. To do\nthis, we first develop a framework for assessing algorithmic and statistical\naspects of randomized sketching methods. We then consider the statistical\nprediction efficiency (PE) and the statistical residual efficiency (RE) of the\nsketched LS estimator; and we use our framework to provide upper bounds for\nseveral types of random projection and random sampling sketching algorithms.\nAmong other results, we show that the RE can be upper bounded when $p < r \\ll\nn$ while the PE typically requires the sample size $r$ to be substantially\nlarger. Lower bounds developed in subsequent results show that our upper bounds\non PE can not be improved. \n\n"}
{"id": "1407.1970", "contents": "Title: Dipole-Induced Electromagnetic Transparency Abstract: We determine the optical response of a thin and dense layer of interacting\nquantum emitters. We show that in such a dense system, the Lorentz redshift and\nthe associated interaction broadening can be used to control the transmission\nand reflection spectra. In the presence of overlapping resonances, a\nDipole-Induced Electromagnetic Transparency (DIET) regime, similar to\nElectromagnetically Induced Transparency (EIT), may be achieved. DIET relies on\ndestructive interference between the electromagnetic waves emitted by quantum\nemitters. Carefully tuning material parameters allows to achieve narrow\ntransmission windows in otherwise completely opaque media. We analyze in\ndetails this coherent and collective effect using a generalized Lorentz model\nand show how it can be controlled. Several potential applications of the\nphenomenon, such as slow light, are proposed. \n\n"}
{"id": "1407.4416", "contents": "Title: In Defense of MinHash Over SimHash Abstract: MinHash and SimHash are the two widely adopted Locality Sensitive Hashing\n(LSH) algorithms for large-scale data processing applications. Deciding which\nLSH to use for a particular problem at hand is an important question, which has\nno clear answer in the existing literature. In this study, we provide a\ntheoretical answer (validated by experiments) that MinHash virtually always\noutperforms SimHash when the data are binary, as common in practice such as\nsearch.\n  The collision probability of MinHash is a function of resemblance similarity\n($\\mathcal{R}$), while the collision probability of SimHash is a function of\ncosine similarity ($\\mathcal{S}$). To provide a common basis for comparison, we\nevaluate retrieval results in terms of $\\mathcal{S}$ for both MinHash and\nSimHash. This evaluation is valid as we can prove that MinHash is a valid LSH\nwith respect to $\\mathcal{S}$, by using a general inequality $\\mathcal{S}^2\\leq\n\\mathcal{R}\\leq \\frac{\\mathcal{S}}{2-\\mathcal{S}}$. Our worst case analysis can\nshow that MinHash significantly outperforms SimHash in high similarity region.\n  Interestingly, our intensive experiments reveal that MinHash is also\nsubstantially better than SimHash even in datasets where most of the data\npoints are not too similar to each other. This is partly because, in practical\ndata, often $\\mathcal{R}\\geq \\frac{\\mathcal{S}}{z-\\mathcal{S}}$ holds where $z$\nis only slightly larger than 2 (e.g., $z\\leq 2.1$). Our restricted worst case\nanalysis by assuming $\\frac{\\mathcal{S}}{z-\\mathcal{S}}\\leq \\mathcal{R}\\leq\n\\frac{\\mathcal{S}}{2-\\mathcal{S}}$ shows that MinHash indeed significantly\noutperforms SimHash even in low similarity region.\n  We believe the results in this paper will provide valuable guidelines for\nsearch in practice, especially when the data are sparse. \n\n"}
{"id": "1407.6538", "contents": "Title: Optimized multifrequency light collection by adaptive self-ordering of\n  scatterers in optical resonators Abstract: Mobile light scatterers in a high-Q optical cavity transversely illuminated\nby laser light close to a cavity resonance form ordered patterns, which\nmaximize light scattering into the cavity and induce optical self-trapping. We\nshow that a generalized form of such crystallization dynamics appears in\nmulticolored pump fields with several cavity modes. Here the particles arrange\nin spatial patterns maximizing total light collection into the resonator. For\nchanging input frequencies and strengths the particles dynamically adapt to the\ncurrent illumination. Interestingly the system keeps some memory on past\nconfigurations, so that a later renewed application of the same pattern\nexhibits faster adaptation towards optimal collective scattering. In a noisy\nenvironment particles explore larger regions of configuration space spending\nmost of the time close to optimum scattering configurations. This adaptive\nself-ordering dynamics should be implementable in a wide range of systems\nranging from cold atoms in multimode cavities or nano-fiber traps to molecules\nor mobile nano-particles within an optical resonator. \n\n"}
{"id": "1408.5352", "contents": "Title: Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in\n  Polynomial Time Abstract: Sparse principal component analysis (PCA) involves nonconvex optimization for\nwhich the global solution is hard to obtain. To address this issue, one popular\napproach is convex relaxation. However, such an approach may produce suboptimal\nestimators due to the relaxation effect. To optimally estimate sparse principal\nsubspaces, we propose a two-stage computational framework named \"tighten after\nrelax\": Within the 'relax' stage, we approximately solve a convex relaxation of\nsparse PCA with early stopping to obtain a desired initial estimator; For the\n'tighten' stage, we propose a novel algorithm called sparse orthogonal\niteration pursuit (SOAP), which iteratively refines the initial estimator by\ndirectly solving the underlying nonconvex problem. A key concept of this\ntwo-stage framework is the basin of attraction. It represents a local region\nwithin which the `tighten' stage has desired computational and statistical\nguarantees. We prove that, the initial estimator obtained from the 'relax'\nstage falls into such a region, and hence SOAP geometrically converges to a\nprincipal subspace estimator which is minimax-optimal within a certain model\nclass. Unlike most existing sparse PCA estimators, our approach applies to the\nnon-spiked covariance models, and adapts to non-Gaussianity as well as\ndependent data settings. Moreover, through analyzing the computational\ncomplexity of the two stages, we illustrate an interesting phenomenon that\nlarger sample size can reduce the total iteration complexity. Our framework\nmotivates a general paradigm for solving many complex statistical problems\nwhich involve nonconvex optimization with provable guarantees. \n\n"}
{"id": "1409.0272", "contents": "Title: Multi-task Sparse Structure Learning Abstract: Multi-task learning (MTL) aims to improve generalization performance by\nlearning multiple related tasks simultaneously. While sometimes the underlying\ntask relationship structure is known, often the structure needs to be estimated\nfrom data at hand. In this paper, we present a novel family of models for MTL,\napplicable to regression and classification problems, capable of learning the\nstructure of task relationships. In particular, we consider a joint estimation\nproblem of the task relationship structure and the individual task parameters,\nwhich is solved using alternating minimization. The task relationship structure\nlearning component builds on recent advances in structure learning of Gaussian\ngraphical models based on sparse estimators of the precision (inverse\ncovariance) matrix. We illustrate the effectiveness of the proposed model on a\nvariety of synthetic and benchmark datasets for regression and classification.\nWe also consider the problem of combining climate model outputs for better\nprojections of future climate, with focus on temperature in South America, and\nshow that the proposed model outperforms several existing methods for the\nproblem. \n\n"}
{"id": "1409.2935", "contents": "Title: Nonlinear optical magnetometry with accessible in situ optical squeezing Abstract: We demonstrate compact and accessible squeezed-light magnetometry using\nfour-wave mixing in a single hot rubidium vapor cell. The strong intrinsic\ncoherence of the four wave mixing process results in nonlinear magneto-optical\nrotation (NMOR) on each mode of a two mode relative-intensity squeezed state.\nThis framework enables 4.7 dB of quantum noise reduction while the opposing\npolarization rotation signals of the probe and conjugate fields add to increase\nthe total signal to noise ratio. \n\n"}
{"id": "1409.3660", "contents": "Title: 10,000+ Times Accelerated Robust Subset Selection (ARSS) Abstract: Subset selection from massive data with noised information is increasingly\npopular for various applications. This problem is still highly challenging as\ncurrent methods are generally slow in speed and sensitive to outliers. To\naddress the above two issues, we propose an accelerated robust subset selection\n(ARSS) method. Specifically in the subset selection area, this is the first\nattempt to employ the $\\ell_{p}(0<p\\leq1)$-norm based measure for the\nrepresentation loss, preventing large errors from dominating our objective. As\na result, the robustness against outlier elements is greatly enhanced.\nActually, data size is generally much larger than feature length, i.e. $N\\gg\nL$. Based on this observation, we propose a speedup solver (via ALM and\nequivalent derivations) to highly reduce the computational cost, theoretically\nfrom $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark\ndatasets verify that our method not only outperforms state of the art methods,\nbut also runs 10,000+ times faster than the most related method. \n\n"}
{"id": "1409.7480", "contents": "Title: Generalized Twin Gaussian Processes using Sharma-Mittal Divergence Abstract: There has been a growing interest in mutual information measures due to their\nwide range of applications in Machine Learning and Computer Vision. In this\npaper, we present a generalized structured regression framework based on\nShama-Mittal divergence, a relative entropy measure, which is introduced to the\nMachine Learning community in this work. Sharma-Mittal (SM) divergence is a\ngeneralized mutual information measure for the widely used R\\'enyi, Tsallis,\nBhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, we\nstudy Sharma-Mittal divergence as a cost function in the context of the Twin\nGaussian Processes (TGP)~\\citep{Bo:2010}, which generalizes over the\nKL-divergence without computational penalty. We show interesting properties of\nSharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missing\ninsights in the traditional TGP formulation. However, we generalize this theory\nbased on SM-divergence instead of KL-divergence which is a special case.\nExperimentally, we evaluated the proposed SMTGP framework on several datasets.\nThe results show that SMTGP reaches better predictions than KL-based TGP, since\nit offers a bigger class of models through its parameters that we learn from\nthe data. \n\n"}
{"id": "1410.0723", "contents": "Title: A Lower Bound for the Optimization of Finite Sums Abstract: This paper presents a lower bound for optimizing a finite sum of $n$\nfunctions, where each function is $L$-smooth and the sum is $\\mu$-strongly\nconvex. We show that no algorithm can reach an error $\\epsilon$ in minimizing\nall functions from this class in fewer than $\\Omega(n +\n\\sqrt{n(\\kappa-1)}\\log(1/\\epsilon))$ iterations, where $\\kappa=L/\\mu$ is a\nsurrogate condition number. We then compare this lower bound to upper bounds\nfor recently developed methods specializing to this setting. When the functions\ninvolved in this sum are not arbitrary, but based on i.i.d. random data, then\nwe further contrast these complexity results with those for optimal first-order\nmethods to directly optimize the sum. The conclusion we draw is that a lot of\ncaution is necessary for an accurate comparison, and identify machine learning\nscenarios where the new methods help computationally. \n\n"}
{"id": "1410.8034", "contents": "Title: Latent Feature Based FM Model For Rating Prediction Abstract: Rating Prediction is a basic problem in Recommender System, and one of the\nmost widely used method is Factorization Machines(FM). However, traditional\nmatrix factorization methods fail to utilize the benefit of implicit feedback,\nwhich has been proved to be important in Rating Prediction problem. In this\nwork, we consider a specific situation, movie rating prediction, where we\nassume that watching history has a big influence on his/her rating behavior on\nan item. We introduce two models, Latent Dirichlet Allocation(LDA) and\nword2vec, both of which perform state-of-the-art results in training latent\nfeatures. Based on that, we propose two feature based models. One is the\nTopic-based FM Model which provides the implicit feedback to the matrix\nfactorization. The other is the Vector-based FM Model which expresses the order\ninfo of watching history. Empirical results on three datasets demonstrate that\nour method performs better than the baseline model and confirm that\nVector-based FM Model usually works better as it contains the order info. \n\n"}
{"id": "1410.8275", "contents": "Title: Bootstrap-Based Regularization for Low-Rank Matrix Estimation Abstract: We develop a flexible framework for low-rank matrix estimation that allows us\nto transform noise models into regularization schemes via a simple bootstrap\nalgorithm. Effectively, our procedure seeks an autoencoding basis for the\nobserved matrix that is stable with respect to the specified noise model; we\ncall the resulting procedure a stable autoencoder. In the simplest case, with\nan isotropic noise model, our method is equivalent to a classical singular\nvalue shrinkage estimator. For non-isotropic noise models, e.g., Poisson noise,\nthe method does not reduce to singular value shrinkage, and instead yields new\nestimators that perform well in experiments. Moreover, by iterating our stable\nautoencoding scheme, we can automatically generate low-rank estimates without\nspecifying the target rank as a tuning parameter. \n\n"}
{"id": "1410.8722", "contents": "Title: Divergence of an orbital-angular-momentum-carrying beam upon propagation Abstract: There is recent interest in the use of light beams carrying orbital angular\nmomentum (OAM) for creating multiple channels within free-space optical\ncommunication systems. One limiting issue is that, for a given beam size at the\ntransmitter, the beam divergence angle increases with increasing OAM, thus\nrequiring a larger aperture at the receiving optical system if the efficiency\nof detection is to be maintained. Confusion exists as to whether this\ndivergence scales linarly with, or with the square root of, the beam's OAM. We\nclarify how both these scaling laws are valid, depending upon whether it is the\nradius of the Gaussian beam waist or the rms intensity which is kept constant\nwhile varying the OAM. \n\n"}
{"id": "1411.0023", "contents": "Title: Validation of Matching Abstract: We introduce a technique to compute probably approximately correct (PAC)\nbounds on precision and recall for matching algorithms. The bounds require some\nverified matches, but those matches may be used to develop the algorithms. The\nbounds can be applied to network reconciliation or entity resolution\nalgorithms, which identify nodes in different networks or values in a data set\nthat correspond to the same entity. For network reconciliation, the bounds do\nnot require knowledge of the network generation process. \n\n"}
{"id": "1411.2581", "contents": "Title: Deep Exponential Families Abstract: We describe \\textit{deep exponential families} (DEFs), a class of latent\nvariable models that are inspired by the hidden structures used in deep neural\nnetworks. DEFs capture a hierarchy of dependencies between latent variables,\nand are easily generalized to many settings through exponential families. We\nperform inference using recent \"black box\" variational inference techniques. We\nthen evaluate various DEFs on text and combine multiple DEFs into a model for\npairwise recommendation data. In an extensive study, we show that going beyond\none layer improves predictions for DEFs. We demonstrate that DEFs find\ninteresting exploratory structure in large data sets, and give better\npredictive performance than state-of-the-art models. \n\n"}
{"id": "1411.3499", "contents": "Title: Better Randomness with Single Photons Abstract: Randomness is one of the most important resources in modern information\nscience, since encryption founds upon the trust in random numbers. Since it is\nimpossible to prove if an existing random bit string is truly random, it is\nrelevant that they be generated in a trust worthy process. This requires\nspecialized hardware for random numbers, for example a die or a tossed coin.\nBut when all input parameters are known, their outcome might still be\npredicted. A quantum mechanical superposition allows for provably true random\nbit generation. In the past decade many quantum random number generators\n(QRNGs) were realized. A photonic implementation is described as a photon which\nimpinges on a beam splitter, but such a protocol is rarely realized with\nnon-classical light or anti-bunched single photons. Instead, laser sources or\nlight emitting diodes are used. Here we analyze the difference in generating a\ntrue random bit string with a laser and with anti-bunched light. We show that a\nsingle photon source provides more randomness than even a brighter laser. This\ngain of usable entropy proves the advantages of true single photons versus\ncoherent input states of light in an experimental implementation. The\nunderlying advantage can be adapted to microscopy and sensing. \n\n"}
{"id": "1411.3784", "contents": "Title: Deep Narrow Boltzmann Machines are Universal Approximators Abstract: We show that deep narrow Boltzmann machines are universal approximators of\nprobability distributions on the activities of their visible units, provided\nthey have sufficiently many hidden layers, each containing the same number of\nunits as the visible layer. We show that, within certain parameter domains,\ndeep Boltzmann machines can be studied as feedforward networks. We provide\nupper and lower bounds on the sufficient depth and width of universal\napproximators. These results settle various intuitions regarding undirected\nnetworks and, in particular, they show that deep narrow Boltzmann machines are\nat least as compact universal approximators as narrow sigmoid belief networks\nand restricted Boltzmann machines, with respect to the currently available\nbounds for those models. \n\n"}
{"id": "1412.1740", "contents": "Title: Image Data Compression for Covariance and Histogram Descriptors Abstract: Covariance and histogram image descriptors provide an effective way to\ncapture information about images. Both excel when used in combination with\nspecial purpose distance metrics. For covariance descriptors these metrics\nmeasure the distance along the non-Euclidean Riemannian manifold of symmetric\npositive definite matrices. For histogram descriptors the Earth Mover's\ndistance measures the optimal transport between two histograms. Although more\nprecise, these distance metrics are very expensive to compute, making them\nimpractical in many applications, even for data sets of only a few thousand\nexamples. In this paper we present two methods to compress the size of\ncovariance and histogram datasets with only marginal increases in test error\nfor k-nearest neighbor classification. Specifically, we show that we can reduce\ndata sets to 16% and in some cases as little as 2% of their original size,\nwhile approximately matching the test error of kNN classification on the full\ntraining set. In fact, because the compressed set is learned in a supervised\nfashion, it sometimes even outperforms the full data set, while requiring only\na fraction of the space and drastically reducing test-time computation. \n\n"}
{"id": "1412.2309", "contents": "Title: Visual Causal Feature Learning Abstract: We provide a rigorous definition of the visual cause of a behavior that is\nbroadly applicable to the visually driven behavior in humans, animals, neurons,\nrobots and other perceiving systems. Our framework generalizes standard\naccounts of causal learning to settings in which the causal variables need to\nbe constructed from micro-variables. We prove the Causal Coarsening Theorem,\nwhich allows us to gain causal knowledge from observational data with minimal\nexperimental effort. The theorem provides a connection to standard inference\ntechniques in machine learning that identify features of an image that\ncorrelate with, but may not cause, the target behavior. Finally, we propose an\nactive learning scheme to learn a manipulator function that performs optimal\nmanipulations on the image to automatically identify the visual cause of a\ntarget behavior. We illustrate our inference and learning algorithms in\nexperiments based on both synthetic and real data. \n\n"}
{"id": "1412.2349", "contents": "Title: Field theory of monochromatic optical beams. II Classical and quantum\n  paraxial fields Abstract: This work is the second part of an investigation aiming at the study of\noptical wave equations from a field-theoretic point of view. Here, we study\nclassical and quantum aspects of scalar fields satisfying the paraxial wave\nequation. First, we determine conservation laws for energy, linear and angular\nmomentum of paraxial fields in a classical context. Then, we proceed with the\nquantization of the field. Finally, we compare our result with the traditional\nones. \n\n"}
{"id": "1412.2432", "contents": "Title: MLitB: Machine Learning in the Browser Abstract: With few exceptions, the field of Machine Learning (ML) research has largely\nignored the browser as a computational engine. Beyond an educational resource\nfor ML, the browser has vast potential to not only improve the state-of-the-art\nin ML research, but also, inexpensively and on a massive scale, to bring\nsophisticated ML learning and prediction to the public at large. This paper\nintroduces MLitB, a prototype ML framework written entirely in JavaScript,\ncapable of performing large-scale distributed computing with heterogeneous\nclasses of devices. The development of MLitB has been driven by several\nunderlying objectives whose aim is to make ML learning and usage ubiquitous (by\nusing ubiquitous compute devices), cheap and effortlessly distributed, and\ncollaborative. This is achieved by allowing every internet capable device to\nrun training algorithms and predictive models with no software installation and\nby saving models in universally readable formats. Our prototype library is\ncapable of training deep neural networks with synchronized, distributed\nstochastic gradient descent. MLitB offers several important opportunities for\nnovel ML research, including: development of distributed learning algorithms,\nadvancement of web GPU algorithms, novel field and mobile applications, privacy\npreserving computing, and green grid-computing. MLitB is available as open\nsource software. \n\n"}
{"id": "1412.5192", "contents": "Title: Giant Narrowband Twin-Beam Generation along the Pump Energy Propagation Abstract: Walk-off effects, originating from the difference between the group and phase\nvelocities, limit the efficiency of nonlinear optical interactions. While\ntransverse walk-off can be eliminated by proper medium engineering,\nlongitudinal walk-off is harder to avoid. In particular, ultrafast twin- beam\ngeneration via pulsed parametric down-conversion (PDC) and four-wave mixing\n(FWM) is only possible in short crystals or fibres or in double-path schemes.\nHere we show that in high-gain PDC, one can overcome the destructive role of\nboth effects and even turn them into useful tools for shaping the emission. In\nour experiment, one of the twin beams is emitted along the pump Poynting vector\nor its group velocity matches that of the pump. The result is dramatically\nenhanced generation of both twin beams, with the simultaneous narrowing of\nangular and frequency spectrum. The effect will enable efficient generation of\nultrafast twin photons and beams in cavities, waveguides, and\nwhispering-gallery mode resonators. \n\n"}
{"id": "1412.6622", "contents": "Title: Deep metric learning using Triplet network Abstract: Deep learning has proven itself as a successful set of models for learning\nuseful semantic representations of data. These, however, are mostly implicitly\nlearned as part of a classification task. In this paper we propose the triplet\nnetwork model, which aims to learn useful representations by distance\ncomparisons. A similar model was defined by Wang et al. (2014), tailor made for\nlearning a ranking for image information retrieval. Here we demonstrate using\nvarious datasets that our model learns a better representation than that of its\nimmediate competitor, the Siamese network. We also discuss future possible\nusage as a framework for unsupervised learning. \n\n"}
{"id": "1412.6830", "contents": "Title: Learning Activation Functions to Improve Deep Neural Networks Abstract: Artificial neural networks typically have a fixed, non-linear activation\nfunction at each neuron. We have designed a novel form of piecewise linear\nactivation function that is learned independently for each neuron using\ngradient descent. With this adaptive activation function, we are able to\nimprove upon deep neural network architectures composed of static rectified\nlinear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),\nCIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs\nboson decay modes. \n\n"}
{"id": "1412.7149", "contents": "Title: Deep Fried Convnets Abstract: The fully connected layers of a deep convolutional neural network typically\ncontain over 90% of the network parameters, and consume the majority of the\nmemory required to store the network parameters. Reducing the number of\nparameters while preserving essentially the same predictive performance is\ncritically important for operating deep neural networks in memory constrained\nenvironments such as GPUs or embedded devices.\n  In this paper we show how kernel methods, in particular a single Fastfood\nlayer, can be used to replace all fully connected layers in a deep\nconvolutional neural network. This novel Fastfood layer is also end-to-end\ntrainable in conjunction with convolutional layers, allowing us to combine them\ninto a new architecture, named deep fried convolutional networks, which\nsubstantially reduces the memory footprint of convolutional networks trained on\nMNIST and ImageNet with no drop in predictive performance. \n\n"}
{"id": "1501.02320", "contents": "Title: On model misspecification and KL separation for Gaussian graphical\n  models Abstract: We establish bounds on the KL divergence between two multivariate Gaussian\ndistributions in terms of the Hamming distance between the edge sets of the\ncorresponding graphical models. We show that the KL divergence is bounded below\nby a constant when the graphs differ by at least one edge; this is essentially\nthe tightest possible bound, since classes of graphs exist for which the edge\ndiscrepancy increases but the KL divergence remains bounded above by a\nconstant. As a natural corollary to our KL lower bound, we also establish a\nsample size requirement for correct model selection via maximum likelihood\nestimation. Our results rigorize the notion that it is essential to estimate\nthe edge structure of a Gaussian graphical model accurately in order to\napproximate the true distribution to close precision. \n\n"}
{"id": "1501.02629", "contents": "Title: Scaling-up Empirical Risk Minimization: Optimization of Incomplete\n  U-statistics Abstract: In a wide range of statistical learning problems such as ranking, clustering\nor metric learning among others, the risk is accurately estimated by\n$U$-statistics of degree $d\\geq 1$, i.e. functionals of the training data with\nlow variance that take the form of averages over $k$-tuples. From a\ncomputational perspective, the calculation of such statistics is highly\nexpensive even for a moderate sample size $n$, as it requires averaging\n$O(n^d)$ terms. This makes learning procedures relying on the optimization of\nsuch data functionals hardly feasible in practice. It is the major goal of this\npaper to show that, strikingly, such empirical risks can be replaced by\ndrastically computationally simpler Monte-Carlo estimates based on $O(n)$ terms\nonly, usually referred to as incomplete $U$-statistics, without damaging the\n$O_{\\mathbb{P}}(1/\\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM)\nprocedures. For this purpose, we establish uniform deviation results describing\nthe error made when approximating a $U$-process by its incomplete version under\nappropriate complexity assumptions. Extensions to model selection, fast rate\nsituations and various sampling techniques are also considered, as well as an\napplication to stochastic gradient descent for ERM. Finally, numerical examples\nare displayed in order to provide strong empirical evidence that the approach\nwe promote largely surpasses more naive subsampling techniques. \n\n"}
{"id": "1501.04080", "contents": "Title: Differentially Private Bayesian Optimization Abstract: Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters\nof a wide variety of machine learning models. The success of machine learning\nhas led practitioners in diverse real-world settings to learn classifiers for\npractical problems. As machine learning becomes commonplace, Bayesian\noptimization becomes an attractive method for practitioners to automate the\nprocess of classifier hyper-parameter tuning. A key observation is that the\ndata used for tuning models in these settings is often sensitive. Certain data\nsuch as genetic predisposition, personal email statistics, and car accident\nhistory, if not properly private, may be at risk of being inferred from\nBayesian optimization outputs. To address this, we introduce methods for\nreleasing the best hyper-parameters and classifier accuracy privately.\nLeveraging the strong theoretical guarantees of differential privacy and known\nBayesian optimization convergence bounds, we prove that under a GP assumption\nthese private quantities are also near-optimal. Finally, even if this\nassumption is not satisfied, we can use different smoothness guarantees to\nprotect privacy. \n\n"}
{"id": "1501.04080", "contents": "Title: Differentially Private Bayesian Optimization Abstract: Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters\nof a wide variety of machine learning models. The success of machine learning\nhas led practitioners in diverse real-world settings to learn classifiers for\npractical problems. As machine learning becomes commonplace, Bayesian\noptimization becomes an attractive method for practitioners to automate the\nprocess of classifier hyper-parameter tuning. A key observation is that the\ndata used for tuning models in these settings is often sensitive. Certain data\nsuch as genetic predisposition, personal email statistics, and car accident\nhistory, if not properly private, may be at risk of being inferred from\nBayesian optimization outputs. To address this, we introduce methods for\nreleasing the best hyper-parameters and classifier accuracy privately.\nLeveraging the strong theoretical guarantees of differential privacy and known\nBayesian optimization convergence bounds, we prove that under a GP assumption\nthese private quantities are also near-optimal. Finally, even if this\nassumption is not satisfied, we can use different smoothness guarantees to\nprotect privacy. \n\n"}
{"id": "1501.06243", "contents": "Title: Poisson Matrix Completion Abstract: We extend the theory of matrix completion to the case where we make Poisson\nobservations for a subset of entries of a low-rank matrix. We consider the\n(now) usual matrix recovery formulation through maximum likelihood with proper\nconstraints on the matrix $M$, and establish theoretical upper and lower bounds\non the recovery error. Our bounds are nearly optimal up to a factor on the\norder of $\\mathcal{O}(\\log(d_1 d_2))$. These bounds are obtained by adapting\nthe arguments used for one-bit matrix completion \\cite{davenport20121}\n(although these two problems are different in nature) and the adaptation\nrequires new techniques exploiting properties of the Poisson likelihood\nfunction and tackling the difficulties posed by the locally sub-Gaussian\ncharacteristic of the Poisson distribution. Our results highlight a few\nimportant distinctions of Poisson matrix completion compared to the prior work\nin matrix completion including having to impose a minimum signal-to-noise\nrequirement on each observed entry. We also develop an efficient iterative\nalgorithm and demonstrate its good performance in recovering solar flare\nimages. \n\n"}
{"id": "1502.00725", "contents": "Title: Cheaper and Better: Selecting Good Workers for Crowdsourcing Abstract: Crowdsourcing provides a popular paradigm for data collection at scale. We\nstudy the problem of selecting subsets of workers from a given worker pool to\nmaximize the accuracy under a budget constraint. One natural question is\nwhether we should hire as many workers as the budget allows, or restrict on a\nsmall number of top-quality workers. By theoretically analyzing the error rate\nof a typical setting in crowdsourcing, we frame the worker selection problem\ninto a combinatorial optimization problem and propose an algorithm to solve it\nefficiently. Empirical results on both simulated and real-world datasets show\nthat our algorithm is able to select a small number of high-quality workers,\nand performs as good as, sometimes even better than, the much larger crowds as\nthe budget allows. \n\n"}
{"id": "1502.01425", "contents": "Title: Provable Sparse Tensor Decomposition Abstract: We propose a novel sparse tensor decomposition method, namely Tensor\nTruncated Power (TTP) method, that incorporates variable selection into the\nestimation of decomposition components. The sparsity is achieved via an\nefficient truncation step embedded in the tensor power iteration. Our method\napplies to a broad family of high dimensional latent variable models, including\nhigh dimensional Gaussian mixture and mixtures of sparse regressions. A\nthorough theoretical investigation is further conducted. In particular, we show\nthat the final decomposition estimator is guaranteed to achieve a local\nstatistical rate, and further strengthen it to the global statistical rate by\nintroducing a proper initialization procedure. In high dimensional regimes, the\nobtained statistical rate significantly improves those shown in the existing\nnon-sparse decomposition methods. The empirical advantages of TTP are confirmed\nin extensive simulated results and two real applications of click-through rate\nprediction and high-dimensional gene clustering. \n\n"}
{"id": "1502.02362", "contents": "Title: Counterfactual Risk Minimization: Learning from Logged Bandit Feedback Abstract: We develop a learning principle and an efficient algorithm for batch learning\nfrom logged bandit feedback. This learning setting is ubiquitous in online\nsystems (e.g., ad placement, web search, recommendation), where an algorithm\nmakes a prediction (e.g., ad ranking) for a given input (e.g., query) and\nobserves bandit feedback (e.g., user clicks on presented ads). We first address\nthe counterfactual nature of the learning problem through propensity scoring.\nNext, we prove generalization error bounds that account for the variance of the\npropensity-weighted empirical risk estimator. These constructive bounds give\nrise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM\ncan be used to derive a new learning method -- called Policy Optimizer for\nExponential Models (POEM) -- for learning stochastic linear rules for\nstructured output prediction. We present a decomposition of the POEM objective\nthat enables efficient stochastic gradient optimization. POEM is evaluated on\nseveral multi-label classification problems showing substantially improved\nrobustness and generalization performance compared to the state-of-the-art. \n\n"}
{"id": "1502.03391", "contents": "Title: Fast Embedding for JOFC Using the Raw Stress Criterion Abstract: The Joint Optimization of Fidelity and Commensurability (JOFC) manifold\nmatching methodology embeds an omnibus dissimilarity matrix consisting of\nmultiple dissimilarities on the same set of objects. One approach to this\nembedding optimizes the preservation of fidelity to each individual\ndissimilarity matrix together with commensurability of each given observation\nacross modalities via iterative majorization of a raw stress error criterion by\nsuccessive Guttman transforms. In this paper, we exploit the special structure\ninherent to JOFC to exactly and efficiently compute the successive Guttman\ntransforms, and as a result we are able to greatly speed up the JOFC procedure\nfor both in-sample and out-of-sample embedding. We demonstrate the scalability\nof our implementation on both real and simulated data examples. \n\n"}
{"id": "1502.05336", "contents": "Title: Probabilistic Backpropagation for Scalable Learning of Bayesian Neural\n  Networks Abstract: Large multilayer neural networks trained with backpropagation have recently\nachieved state-of-the-art results in a wide range of problems. However, using\nbackprop for neural net learning still has some disadvantages, e.g., having to\ntune a large number of hyperparameters to the data, lack of calibrated\nprobabilistic predictions, and a tendency to overfit the training data. In\nprinciple, the Bayesian approach to learning neural networks does not have\nthese problems. However, existing Bayesian techniques lack scalability to large\ndataset and network sizes. In this work we present a novel scalable method for\nlearning Bayesian neural networks, called probabilistic backpropagation (PBP).\nSimilar to classical backpropagation, PBP works by computing a forward\npropagation of probabilities through the network and then doing a backward\ncomputation of gradients. A series of experiments on ten real-world datasets\nshow that PBP is significantly faster than other techniques, while offering\ncompetitive predictive abilities. Our experiments also show that PBP provides\naccurate estimates of the posterior variance on the network weights. \n\n"}
{"id": "1502.05698", "contents": "Title: Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks Abstract: One long-term goal of machine learning research is to produce methods that\nare applicable to reasoning and natural language, in particular building an\nintelligent dialogue agent. To measure progress towards that goal, we argue for\nthe usefulness of a set of proxy tasks that evaluate reading comprehension via\nquestion answering. Our tasks measure understanding in several ways: whether a\nsystem is able to answer questions via chaining facts, simple induction,\ndeduction and many more. The tasks are designed to be prerequisites for any\nsystem that aims to be capable of conversing with a human. We believe many\nexisting learning systems can currently not solve them, and hence our aim is to\nclassify these tasks into skill sets, so that researchers can identify (and\nthen rectify) the failings of their systems. We also extend and improve the\nrecently introduced Memory Networks model, and show it is able to solve some,\nbut not all, of the tasks. \n\n"}
{"id": "1502.06689", "contents": "Title: 1-Bit Matrix Completion under Exact Low-Rank Constraint Abstract: We consider the problem of noisy 1-bit matrix completion under an exact rank\nconstraint on the true underlying matrix $M^*$. Instead of observing a subset\nof the noisy continuous-valued entries of a matrix $M^*$, we observe a subset\nof noisy 1-bit (or binary) measurements generated according to a probabilistic\nmodel. We consider constrained maximum likelihood estimation of $M^*$, under a\nconstraint on the entry-wise infinity-norm of $M^*$ and an exact rank\nconstraint. This is in contrast to previous work which has used convex\nrelaxations for the rank. We provide an upper bound on the matrix estimation\nerror under this model. Compared to the existing results, our bound has faster\nconvergence rate with matrix dimensions when the fraction of revealed 1-bit\nobservations is fixed, independent of the matrix dimensions. We also propose an\niterative algorithm for solving our nonconvex optimization with a certificate\nof global optimality of the limiting point. This algorithm is based on low rank\nfactorization of $M^*$. We validate the method on synthetic and real data with\nimproved performance over existing methods. \n\n"}
{"id": "1502.06919", "contents": "Title: Low Rank Matrix Completion with Exponential Family Noise Abstract: The matrix completion problem consists in reconstructing a matrix from a\nsample of entries, possibly observed with noise. A popular class of estimator,\nknown as nuclear norm penalized estimators, are based on minimizing the sum of\na data fitting term and a nuclear norm penalization. Here, we investigate the\ncase where the noise distribution belongs to the exponential family and is\nsub-exponential. Our framework alllows for a general sampling scheme. We first\nconsider an estimator defined as the minimizer of the sum of a log-likelihood\nterm and a nuclear norm penalization and prove an upper bound on the Frobenius\nprediction risk. The rate obtained improves on previous works on matrix\ncompletion for exponential family. When the sampling distribution is known, we\npropose another estimator and prove an oracle inequality w.r.t. the\nKullback-Leibler prediction risk, which translates immediatly into an upper\nbound on the Frobenius prediction risk. Finally, we show that all the rates\nobtained are minimax optimal up to a logarithmic factor. \n\n"}
{"id": "1503.01158", "contents": "Title: A Meta-Analysis of the Anomaly Detection Problem Abstract: This article provides a thorough meta-analysis of the anomaly detection\nproblem. To accomplish this we first identify approaches to benchmarking\nanomaly detection algorithms across the literature and produce a large corpus\nof anomaly detection benchmarks that vary in their construction across several\ndimensions we deem important to real-world applications: (a) point difficulty,\n(b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d)\nrelevance of features. We apply a representative set of anomaly detection\nalgorithms to this corpus, yielding a very large collection of experimental\nresults. We analyze these results to understand many phenomena observed in\nprevious work. First we observe the effects of experimental design on\nexperimental results. Second, results are evaluated with two metrics, ROC Area\nUnder the Curve and Average Precision. We employ statistical hypothesis testing\nto demonstrate the value (or lack thereof) of our benchmarks. We then offer\nseveral approaches to summarizing our experimental results, drawing several\nconclusions about the impact of our methodology as well as the strengths and\nweaknesses of some algorithms. Last, we compare results against a trivial\nsolution as an alternate means of normalizing the reported performance of\nalgorithms. The intended contributions of this article are many; in addition to\nproviding a large publicly-available corpus of anomaly detection benchmarks, we\nprovide an ontology for describing anomaly detection contexts, a methodology\nfor controlling various aspects of benchmark creation, guidelines for future\nexperimental design and a discussion of the many potential pitfalls of trying\nto measure success in this field. \n\n"}
{"id": "1503.01521", "contents": "Title: Jointly Learning Multiple Measures of Similarities from Triplet\n  Comparisons Abstract: Similarity between objects is multi-faceted and it can be easier for human\nannotators to measure it when the focus is on a specific aspect. We consider\nthe problem of mapping objects into view-specific embeddings where the distance\nbetween them is consistent with the similarity comparisons of the form \"from\nthe t-th view, object A is more similar to B than to C\". Our framework jointly\nlearns view-specific embeddings exploiting correlations between views.\nExperiments on a number of datasets, including one of multi-view crowdsourced\ncomparison on bird images, show the proposed method achieves lower triplet\ngeneralization error when compared to both learning embeddings independently\nfor each view and all views pooled into one view. Our method can also be used\nto learn multiple measures of similarity over input features taking class\nlabels into account and compares favorably to existing approaches for\nmulti-task metric learning on the ISOLET dataset. \n\n"}
{"id": "1503.02531", "contents": "Title: Distilling the Knowledge in a Neural Network Abstract: A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel. \n\n"}
{"id": "1503.02533", "contents": "Title: A Smoothed Dual Approach for Variational Wasserstein Problems Abstract: Variational problems that involve Wasserstein distances have been recently\nproposed to summarize and learn from probability measures. Despite being\nconceptually simple, such problems are computationally challenging because they\ninvolve minimizing over quantities (Wasserstein distances) that are themselves\nhard to compute. We show that the dual formulation of Wasserstein variational\nproblems introduced recently by Carlier et al. (2014) can be regularized using\nan entropic smoothing, which leads to smooth, differentiable, convex\noptimization problems that are simpler to implement and numerically more\nstable. We illustrate the versatility of this approach by applying it to the\ncomputation of Wasserstein barycenters and gradient flows of spacial\nregularization functionals. \n\n"}
{"id": "1503.03200", "contents": "Title: Nano-optomechanical measurement in the photon counting regime Abstract: Optically measuring in the photon counting regime is a recurrent challenge in\nmodern physics and a guarantee to develop weakly invasive probes. Here we\ninvestigate this idea on a hybrid nano-optomechanical system composed of a\nnanowire hybridized to a single Nitrogen-Vacancy (NV) defect. The vibrations of\nthe nanoresonator grant a spatial degree of freedom to the quantum emitter and\nthe photon emission event can now vary in space and time. We investigate how\nthe nanomotion is encoded on the detected photon statistics and explore their\nspatio-temporal correlation properties. This allows a quantitative measurement\nof the vibrations of the nanomechanical oscillator at unprecedentedly low light\nintensities in the photon counting regime when less than one photon is detected\nper oscillation period, where standard detectors are dark-noise-limited. These\nresults have implications for probing weakly interacting nanoresonators, for\nlow temperature experiments and for investigating single moving markers. \n\n"}
{"id": "1503.03231", "contents": "Title: Adaptive-Rate Sparse Signal Reconstruction With Application in\n  Compressive Background Subtraction Abstract: We propose and analyze an online algorithm for reconstructing a sequence of\nsignals from a limited number of linear measurements. The signals are assumed\nsparse, with unknown support, and evolve over time according to a generic\nnonlinear dynamical model. Our algorithm, based on recent theoretical results\nfor $\\ell_1$-$\\ell_1$ minimization, is recursive and computes the number of\nmeasurements to be taken at each time on-the-fly. As an example, we apply the\nalgorithm to compressive video background subtraction, a problem that can be\nstated as follows: given a set of measurements of a sequence of images with a\nstatic background, simultaneously reconstruct each image while separating its\nforeground from the background. The performance of our method is illustrated on\nsequences of real images: we observe that it allows a dramatic reduction in the\nnumber of measurements with respect to state-of-the-art compressive background\nsubtraction schemes. \n\n"}
{"id": "1503.04567", "contents": "Title: Learning Mixed Membership Community Models in Social Tagging Networks\n  through Tensor Methods Abstract: Community detection in graphs has been extensively studied both in theory and\nin applications. However, detecting communities in hypergraphs is more\nchallenging. In this paper, we propose a tensor decomposition approach for\nguaranteed learning of communities in a special class of hypergraphs modeling\nsocial tagging systems or folksonomies. A folksonomy is a tripartite 3-uniform\nhypergraph consisting of (user, tag, resource) hyperedges. We posit a\nprobabilistic mixed membership community model, and prove that the tensor\nmethod consistently learns the communities under efficient sample complexity\nand separation requirements. \n\n"}
{"id": "1503.05135", "contents": "Title: Pulsed excitation dynamics of an optomechanical crystal resonator near\n  its quantum ground-state of motion Abstract: Using pulsed optical excitation and read-out along with single phonon\ncounting techniques, we measure the transient back-action, heating, and damping\ndynamics of a nanoscale silicon optomechanical crystal cavity mounted in a\ndilution refrigerator at a base temperature of 11mK. In addition to observing a\nslow (~740ns) turn-on time for the optical-absorption-induced hot phonon bath,\nwe measure for the 5.6GHz `breathing' acoustic mode of the cavity an initial\nphonon occupancy as low as 0.021 +- 0.007 (mode temperature = 70mK) and an\nintrinsic mechanical decay rate of 328 +- 14 Hz (mechanical Q-factor =\n1.7x10^7). These measurements demonstrate the feasibility of using short pulsed\nmeasurements for a variety of quantum optomechanical applications despite the\npresence of steady-state optical heating. \n\n"}
{"id": "1503.08727", "contents": "Title: A Parzen-based distance between probability measures as an alternative\n  of summary statistics in Approximate Bayesian Computation Abstract: Approximate Bayesian Computation (ABC) are likelihood-free Monte Carlo\nmethods. ABC methods use a comparison between simulated data, using different\nparameters drew from a prior distribution, and observed data. This comparison\nprocess is based on computing a distance between the summary statistics from\nthe simulated data and the observed data. For complex models, it is usually\ndifficult to define a methodology for choosing or constructing the summary\nstatistics. Recently, a nonparametric ABC has been proposed, that uses a\ndissimilarity measure between discrete distributions based on empirical kernel\nembeddings as an alternative for summary statistics. The nonparametric ABC\noutperforms other methods including ABC, kernel ABC or synthetic likelihood\nABC. However, it assumes that the probability distributions are discrete, and\nit is not robust when dealing with few observations. In this paper, we propose\nto apply kernel embeddings using an smoother density estimator or Parzen\nestimator for comparing the empirical data distributions, and computing the ABC\nposterior. Synthetic data and real data were used to test the Bayesian\ninference of our method. We compare our method with respect to state-of-the-art\nmethods, and demonstrate that our method is a robust estimator of the posterior\ndistribution in terms of the number of observations. \n\n"}
{"id": "1504.02247", "contents": "Title: Projective simulation with generalization Abstract: The ability to generalize is an important feature of any intelligent agent.\nNot only because it may allow the agent to cope with large amounts of data, but\nalso because in some environments, an agent with no generalization capabilities\ncannot learn. In this work we outline several criteria for generalization, and\npresent a dynamic and autonomous machinery that enables projective simulation\nagents to meaningfully generalize. Projective simulation, a novel, physical\napproach to artificial intelligence, was recently shown to perform well in\nstandard reinforcement learning problems, with applications in advanced\nrobotics as well as quantum experiments. Both the basic projective simulation\nmodel and the presented generalization machinery are based on very simple\nprinciples. This allows us to provide a full analytical analysis of the agent's\nperformance and to illustrate the benefit the agent gains by generalizing.\nSpecifically, we show that already in basic (but extreme) environments,\nlearning without generalization may be impossible, and demonstrate how the\npresented generalization machinery enables the projective simulation agent to\nlearn. \n\n"}
{"id": "1504.04343", "contents": "Title: Caffe con Troll: Shallow Ideas to Speed Up Deep Learning Abstract: We present Caffe con Troll (CcT), a fully compatible end-to-end version of\nthe popular framework Caffe with rebuilt internals. We built CcT to examine the\nperformance characteristics of training and deploying general-purpose\nconvolutional neural networks across different hardware architectures. We find\nthat, by employing standard batching optimizations for CPU training, we achieve\na 4.5x throughput improvement over Caffe on popular networks like CaffeNet.\nMoreover, with these improvements, the end-to-end training time for CNNs is\ndirectly proportional to the FLOPS delivered by the CPU, which enables us to\nefficiently train hybrid CPU-GPU systems for CNNs. \n\n"}
{"id": "1504.05006", "contents": "Title: Partition MCMC for inference on acyclic digraphs Abstract: Acyclic digraphs are the underlying representation of Bayesian networks, a\nwidely used class of probabilistic graphical models. Learning the underlying\ngraph from data is a way of gaining insights about the structural properties of\na domain. Structure learning forms one of the inference challenges of\nstatistical graphical models.\n  MCMC methods, notably structure MCMC, to sample graphs from the posterior\ndistribution given the data are probably the only viable option for Bayesian\nmodel averaging. Score modularity and restrictions on the number of parents of\neach node allow the graphs to be grouped into larger collections, which can be\nscored as a whole to improve the chain's convergence. Current examples of\nalgorithms taking advantage of grouping are the biased order MCMC, which acts\non the alternative space of permuted triangular matrices, and non ergodic edge\nreversal moves.\n  Here we propose a novel algorithm, which employs the underlying combinatorial\nstructure of DAGs to define a new grouping. As a result convergence is improved\ncompared to structure MCMC, while still retaining the property of producing an\nunbiased sample. Finally the method can be combined with edge reversal moves to\nimprove the sampler further. \n\n"}
{"id": "1504.05359", "contents": "Title: Tunable multi-channel inverse optomechanically induced transparency Abstract: In contrast to the optomechanically induced transparency (OMIT) defined\nconventionally, the inverse OMIT behaves as coherent absorption of the input\nlights in the optomechanical systems. We characterize a feasible inverse OMIT\nin a multi-channel fashion with a double-sided optomechanical cavity system\ncoupled to a nearby charged nanomechanical resonator via Coulomb interaction,\nwhere two counter-propagating probe lights can be absorbed via one of the\nchannels or even via three channels simultaneously with the assistance of a\nstrong pump light. Under realistic conditions, we demonstrate the experimental\nfeasibility of our model using two slightly different nanomechanical resonators\nand the possibility of detecting the energy dissipation of the system. In\nparticular, we find that our model turns to be an unilateral inverse OMIT once\nthe two probe lights are different with a relative phase, and in this case we\nshow the possibility to measure the relative phase precisely. \n\n"}
{"id": "1504.06937", "contents": "Title: Algorithms with Logarithmic or Sublinear Regret for Constrained\n  Contextual Bandits Abstract: We study contextual bandits with budget and time constraints, referred to as\nconstrained contextual bandits.The time and budget constraints significantly\ncomplicate the exploration and exploitation tradeoff because they introduce\ncomplex coupling among contexts over time.Such coupling effects make it\ndifficult to obtain oracle solutions that assume known statistics of bandits.\nTo gain insight, we first study unit-cost systems with known context\ndistribution. When the expected rewards are known, we develop an approximation\nof the oracle, referred to Adaptive-Linear-Programming (ALP), which achieves\nnear-optimality and only requires the ordering of expected rewards. With these\nhighly desirable features, we then combine ALP with the upper-confidence-bound\n(UCB) method in the general case where the expected rewards are unknown {\\it a\npriori}. We show that the proposed UCB-ALP algorithm achieves logarithmic\nregret except for certain boundary cases. Further, we design algorithms and\nobtain similar regret analysis results for more general systems with unknown\ncontext distribution and heterogeneous costs. To the best of our knowledge,\nthis is the first work that shows how to achieve logarithmic regret in\nconstrained contextual bandits. Moreover, this work also sheds light on the\nstudy of computationally efficient algorithms for general constrained\ncontextual bandits. \n\n"}
{"id": "1504.07235", "contents": "Title: Sign Stable Random Projections for Large-Scale Learning Abstract: We study the use of \"sign $\\alpha$-stable random projections\" (where\n$0<\\alpha\\leq 2$) for building basic data processing tools in the context of\nlarge-scale machine learning applications (e.g., classification, regression,\nclustering, and near-neighbor search). After the processing by sign stable\nrandom projections, the inner products of the processed data approximate\nvarious types of nonlinear kernels depending on the value of $\\alpha$. Thus,\nthis approach provides an effective strategy for approximating nonlinear\nlearning algorithms essentially at the cost of linear learning. When $\\alpha\n=2$, it is known that the corresponding nonlinear kernel is the arc-cosine\nkernel. When $\\alpha=1$, the procedure approximates the arc-cos-$\\chi^2$ kernel\n(under certain condition). When $\\alpha\\rightarrow0+$, it corresponds to the\nresemblance kernel.\n  From practitioners' perspective, the method of sign $\\alpha$-stable random\nprojections is ready to be tested for large-scale learning applications, where\n$\\alpha$ can be simply viewed as a tuning parameter. What is missing in the\nliterature is an extensive empirical study to show the effectiveness of sign\nstable random projections, especially for $\\alpha\\neq 2$ or 1. The paper\nsupplies such a study on a wide variety of classification datasets. In\nparticular, we compare shoulder-by-shoulder sign stable random projections with\nthe recently proposed \"0-bit consistent weighted sampling (CWS)\" (Li 2015). \n\n"}
{"id": "1504.07550", "contents": "Title: Deep Neural Networks Regularization for Structured Output Prediction Abstract: A deep neural network model is a powerful framework for learning\nrepresentations. Usually, it is used to learn the relation $x \\to y$ by\nexploiting the regularities in the input $x$. In structured output prediction\nproblems, $y$ is multi-dimensional and structural relations often exist between\nthe dimensions. The motivation of this work is to learn the output dependencies\nthat may lie in the output data in order to improve the prediction accuracy.\nUnfortunately, feedforward networks are unable to exploit the relations between\nthe outputs. In order to overcome this issue, we propose in this paper a\nregularization scheme for training neural networks for these particular tasks\nusing a multi-task framework. Our scheme aims at incorporating the learning of\nthe output representation $y$ in the training process in an unsupervised\nfashion while learning the supervised mapping function $x \\to y$.\n  We evaluate our framework on a facial landmark detection problem which is a\ntypical structured output task. We show over two public challenging datasets\n(LFPW and HELEN) that our regularization scheme improves the generalization of\ndeep neural networks and accelerates their training. The use of unlabeled data\nand label-only data is also explored, showing an additional improvement of the\nresults. We provide an opensource implementation\n(https://github.com/sbelharbi/structured-output-ae) of our framework. \n\n"}
{"id": "1504.08025", "contents": "Title: Note on Equivalence Between Recurrent Neural Network Time Series Models\n  and Variational Bayesian Models Abstract: We observe that the standard log likelihood training objective for a\nRecurrent Neural Network (RNN) model of time series data is equivalent to a\nvariational Bayesian training objective, given the proper choice of generative\nand inference models. This perspective may motivate extensions to both RNNs and\nvariational Bayesian models. We propose one such extension, where multiple\nparticles are used for the hidden state of an RNN, allowing a natural\nrepresentation of uncertainty or multimodality. \n\n"}
{"id": "1504.08069", "contents": "Title: Controllable optical output fields from an optomechanical system with a\n  mechanical driving Abstract: We investigate the properties of the optical output fields from a cavity\noptomechanical system, where the cavity is driven by a strong coupling and a\nweak probe optical fields and the mechanical resonator is driven by a coherent\nmechanical pump. When the frequency of the mechanical pump matches the\nfrequency difference between the coupling and probe optical fields, due to the\ninterference between the different optical components at the same frequency, we\ndemonstrate that the large positive or negative group delay of the output field\nat the frequency of probe field can be achieved and tuned by adjusting the\nphase and amplitude of the mechanical driving field. Moreover, the strength of\nthe output field at the frequency of optical four-wave-mixing (FWM) field also\ncan be controlled (enhanced and suppressed) by tuning the phase and amplitude\nof the mechanical pump. We show that the power of the output field at the\nfrequency of the optical FWM field can be suppressed to zero or enhanced so\nmuch that it can be comparable with and even larger than the power of the input\nprobe optical field. \n\n"}
{"id": "1505.01164", "contents": "Title: Achieving a Hyperlocal Housing Price Index: Overcoming Data Sparsity by\n  Bayesian Dynamical Modeling of Multiple Data Streams Abstract: Understanding how housing values evolve over time is important to policy\nmakers, consumers and real estate professionals. Existing methods for\nconstructing housing indices are computed at a coarse spatial granularity, such\nas metropolitan regions, which can mask or distort price dynamics apparent in\nlocal markets, such as neighborhoods and census tracts. A challenge in moving\nto estimates at, for example, the census tract level is the sparsity of\nspatiotemporally localized house sales observations. Our work aims at\naddressing this challenge by leveraging observations from multiple census\ntracts discovered to have correlated valuation dynamics. Our proposed Bayesian\nnonparametric approach builds on the framework of latent factor models to\nenable a flexible, data-driven method for inferring the clustering of\ncorrelated census tracts. We explore methods for scalability and\nparallelizability of computations, yielding a housing valuation index at the\nlevel of census tract rather than zip code, and on a monthly basis rather than\nquarterly. Our analysis is provided on a large Seattle metropolitan housing\ndataset. \n\n"}
{"id": "1505.01371", "contents": "Title: Re-scale boosting for regression and classification Abstract: Boosting is a learning scheme that combines weak prediction rules to produce\na strong composite estimator, with the underlying intuition that one can obtain\naccurate prediction rules by combining \"rough\" ones. Although boosting is\nproved to be consistent and overfitting-resistant, its numerical convergence\nrate is relatively slow. The aim of this paper is to develop a new boosting\nstrategy, called the re-scale boosting (RBoosting), to accelerate the numerical\nconvergence rate and, consequently, improve the learning performance of\nboosting. Our studies show that RBoosting possesses the almost optimal\nnumerical convergence rate in the sense that, up to a logarithmic factor, it\ncan reach the minimax nonlinear approximation rate. We then use RBoosting to\ntackle both the classification and regression problems, and deduce a tight\ngeneralization error estimate. The theoretical and experimental results show\nthat RBoosting outperforms boosting in terms of generalization. \n\n"}
{"id": "1505.04406", "contents": "Title: Hinge-Loss Markov Random Fields and Probabilistic Soft Logic Abstract: A fundamental challenge in developing high-impact machine learning\ntechnologies is balancing the need to model rich, structured domains with the\nability to scale to big data. Many important problem areas are both richly\nstructured and large scale, from social and biological networks, to knowledge\ngraphs and the Web, to images, video, and natural language. In this paper, we\nintroduce two new formalisms for modeling structured data, and show that they\ncan both capture rich structure and scale to big data. The first, hinge-loss\nMarkov random fields (HL-MRFs), is a new kind of probabilistic graphical model\nthat generalizes different approaches to convex inference. We unite three\napproaches from the randomized algorithms, probabilistic graphical models, and\nfuzzy logic communities, showing that all three lead to the same inference\nobjective. We then define HL-MRFs by generalizing this unified objective. The\nsecond new formalism, probabilistic soft logic (PSL), is a probabilistic\nprogramming language that makes HL-MRFs easy to define using a syntax based on\nfirst-order logic. We introduce an algorithm for inferring most-probable\nvariable assignments (MAP inference) that is much more scalable than\ngeneral-purpose convex optimization methods, because it uses message passing to\ntake advantage of sparse dependency structures. We then show how to learn the\nparameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous\ndiscrete models, but much more scalable. Together, these algorithms enable\nHL-MRFs and PSL to model rich, structured data at scales not previously\npossible. \n\n"}
{"id": "1505.08052", "contents": "Title: Batch Bayesian Optimization via Local Penalization Abstract: The popularity of Bayesian optimization methods for efficient exploration of\nparameter spaces has lead to a series of papers applying Gaussian processes as\nsurrogates in the optimization of functions. However, most proposed approaches\nonly allow the exploration of the parameter space to occur sequentially. Often,\nit is desirable to simultaneously propose batches of parameter values to\nexplore. This is particularly the case when large parallel processing\nfacilities are available. These facilities could be computational or physical\nfacets of the process being optimized. E.g. in biological experiments many\nexperimental set ups allow several samples to be simultaneously processed.\nBatch methods, however, require modeling of the interaction between the\nevaluations in the batch, which can be expensive in complex scenarios. We\ninvestigate a simple heuristic based on an estimate of the Lipschitz constant\nthat captures the most important aspect of this interaction (i.e. local\nrepulsion) at negligible computational overhead. The resulting algorithm\ncompares well, in running time, with much more elaborate alternatives. The\napproach assumes that the function of interest, $f$, is a Lipschitz continuous\nfunction. A wrap-loop around the acquisition function is used to collect\nbatches of points of certain size minimizing the non-parallelizable\ncomputational effort. The speed-up of our method with respect to previous\napproaches is significant in a set of computationally expensive experiments. \n\n"}
{"id": "1506.00117", "contents": "Title: Enhancing the bandwidth of gravitational-wave detectors with unstable\n  optomechanical filters Abstract: For gravitational-wave interferometric detectors, there is a tradeoff between\nthe detector bandwidth and peak sensitivity when focusing on the shot noise\nlevel. This has to do with the frequency-dependent propagation phase lag\n(positive dispersion) of the signal. We consider embedding an active unstable\nfilter---a cavity-assisted optomechanical device operating in the instability\nregime---inside the interferometer to compensate the phase, and using feedback\ncontrol to stabilize the entire system. We show that this scheme in principle\ncan enhance the bandwidth without sacrificing the peak sensitivity. However,\nthere is one practical difficulty for implementing it due to the thermal\nfluctuation of the mechanical oscillator in the optomechanical filter, which\nputs a very stringent requirement on the environmental temperature and the\nmechanical quality factor. \n\n"}
{"id": "1506.00354", "contents": "Title: Learning with hidden variables Abstract: Learning and inferring features that generate sensory input is a task\ncontinuously performed by cortex. In recent years, novel algorithms and\nlearning rules have been proposed that allow neural network models to learn\nsuch features from natural images, written text, audio signals, etc. These\nnetworks usually involve deep architectures with many layers of hidden neurons.\nHere we review recent advancements in this area emphasizing, amongst other\nthings, the processing of dynamical inputs by networks with hidden nodes and\nthe role of single neuron models. These points and the questions they arise can\nprovide conceptual advancements in understanding of learning in the cortex and\nthe relationship between machine learning approaches to learning with hidden\nnodes and those in cortical circuits. \n\n"}
{"id": "1506.01305", "contents": "Title: Shifting the Quantum-Classical Boundary: Theory and Experiment for\n  Statistically Classical Optical Fields Abstract: The growing recognition that entanglement is not exclusively a quantum\nproperty, and does not even originate with Schr\\\"odinger's famous remark about\nit [Proc. Camb. Phil. Soc. 31, 555 (1935)], prompts examination of its role in\nmarking the quantum-classical boundary. We have done this by subjecting\ncorrelations of classical optical fields to new Bell-analysis experiments, and\nreport here values of the Bell parameter greater than ${\\cal B} = 2.54$. This\nis many standard deviations outside the limit ${\\cal B} = 2$ established by the\nClauser-Horne-Shimony-Holt (CHSH) Bell inequality [Phys. Rev. Lett. 23, 880\n(1969)], in agreement with our theoretical classical prediction, and not far\nfrom the Tsirelson limit ${\\cal B} = 2.828...$. These results cast a new light\non the standard quantum-classical boundary description, and suggest a\nreinterpretation of it. \n\n"}
{"id": "1506.02142", "contents": "Title: Dropout as a Bayesian Approximation: Representing Model Uncertainty in\n  Deep Learning Abstract: Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout's uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout's uncertainty in deep reinforcement learning. \n\n"}
{"id": "1506.02169", "contents": "Title: Approximating Likelihood Ratios with Calibrated Discriminative\n  Classifiers Abstract: In many fields of science, generalized likelihood ratio tests are established\ntools for statistical inference. At the same time, it has become increasingly\ncommon that a simulator (or generative model) is used to describe complex\nprocesses that tie parameters $\\theta$ of an underlying theory and measurement\napparatus to high-dimensional observations $\\mathbf{x}\\in \\mathbb{R}^p$.\nHowever, simulator often do not provide a way to evaluate the likelihood\nfunction for a given observation $\\mathbf{x}$, which motivates a new class of\nlikelihood-free inference algorithms. In this paper, we show that likelihood\nratios are invariant under a specific class of dimensionality reduction maps\n$\\mathbb{R}^p \\mapsto \\mathbb{R}$. As a direct consequence, we show that\ndiscriminative classifiers can be used to approximate the generalized\nlikelihood ratio statistic when only a generative model for the data is\navailable. This leads to a new machine learning-based approach to\nlikelihood-free inference that is complementary to Approximate Bayesian\nComputation, and which does not require a prior on the model parameters.\nExperimental results on artificial problems with known exact likelihoods\nillustrate the potential of the proposed method. \n\n"}
{"id": "1506.02267", "contents": "Title: Computationally Efficient Bayesian Learning of Gaussian Process State\n  Space Models Abstract: Gaussian processes allow for flexible specification of prior assumptions of\nunknown dynamics in state space models. We present a procedure for efficient\nBayesian learning in Gaussian process state space models, where the\nrepresentation is formed by projecting the problem onto a set of approximate\neigenfunctions derived from the prior covariance structure. Learning under this\nfamily of models can be conducted using a carefully crafted particle MCMC\nalgorithm. This scheme is computationally efficient and yet allows for a fully\nBayesian treatment of the problem. Compared to conventional system\nidentification tools or existing learning methods, we show competitive\nperformance and reliable quantification of uncertainties in the model. \n\n"}
{"id": "1506.02544", "contents": "Title: Learning with Group Invariant Features: A Kernel Perspective Abstract: We analyze in this paper a random feature map based on a theory of invariance\nI-theory introduced recently. More specifically, a group invariant signal\nsignature is obtained through cumulative distributions of group transformed\nrandom projections. Our analysis bridges invariant feature learning with kernel\nmethods, as we show that this feature map defines an expected Haar integration\nkernel that is invariant to the specified group action. We show how this\nnon-linear random feature map approximates this group invariant kernel\nuniformly on a set of $N$ points. Moreover, we show that it defines a function\nspace that is dense in the equivalent Invariant Reproducing Kernel Hilbert\nSpace. Finally, we quantify error rates of the convergence of the empirical\nrisk minimization, as well as the reduction in the sample complexity of a\nlearning algorithm using such an invariant representation for signal\nclassification, in a classical supervised learning setting. \n\n"}
{"id": "1506.02550", "contents": "Title: Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem Abstract: We study the $K$-armed dueling bandit problem, a variation of the standard\nstochastic bandit problem where the feedback is limited to relative comparisons\nof a pair of arms. We introduce a tight asymptotic regret lower bound that is\nbased on the information divergence. An algorithm that is inspired by the\nDeterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010)\nis proposed, and its regret is analyzed. The proposed algorithm is found to be\nthe first one with a regret upper bound that matches the lower bound.\nExperimental comparisons of dueling bandit algorithms show that the proposed\nalgorithm significantly outperforms existing ones. \n\n"}
{"id": "1506.02785", "contents": "Title: On the Error of Random Fourier Features Abstract: Kernel methods give powerful, flexible, and theoretically grounded approaches\nto solving many problems in machine learning. The standard approach, however,\nrequires pairwise evaluations of a kernel function, which can lead to\nscalability issues for very large datasets. Rahimi and Recht (2007) suggested a\npopular approach to handling this problem, known as random Fourier features.\nThe quality of this approximation, however, is not well understood. We improve\nthe uniform error bound of that paper, as well as giving novel understandings\nof the embedding's variance, approximation error, and use in some machine\nlearning methods. We also point out that surprisingly, of the two main variants\nof those features, the more widely used is strictly higher-variance for the\nGaussian kernel and has worse bounds. \n\n"}
{"id": "1506.02903", "contents": "Title: Mixing Time Estimation in Reversible Markov Chains from a Single Sample\n  Path Abstract: This article provides the first procedure for computing a fully\ndata-dependent interval that traps the mixing time $t_{\\text{mix}}$ of a finite\nreversible ergodic Markov chain at a prescribed confidence level. The interval\nis computed from a single finite-length sample path from the Markov chain, and\ndoes not require the knowledge of any parameters of the chain. This stands in\ncontrast to previous approaches, which either only provide point estimates, or\nrequire a reset mechanism, or additional prior knowledge. The interval is\nconstructed around the relaxation time $t_{\\text{relax}}$, which is strongly\nrelated to the mixing time, and the width of the interval converges to zero\nroughly at a $\\sqrt{n}$ rate, where $n$ is the length of the sample path. Upper\nand lower bounds are given on the number of samples required to achieve\nconstant-factor multiplicative accuracy. The lower bounds indicate that, unless\nfurther restrictions are placed on the chain, no procedure can achieve this\naccuracy level before seeing each state at least $\\Omega(t_{\\text{relax}})$\ntimes on the average. Finally, future directions of research are identified. \n\n"}
{"id": "1506.03016", "contents": "Title: Accelerated Stochastic Gradient Descent for Minimizing Finite Sums Abstract: We propose an optimization method for minimizing the finite sums of smooth\nconvex functions. Our method incorporates an accelerated gradient descent (AGD)\nand a stochastic variance reduction gradient (SVRG) in a mini-batch setting.\nUnlike SVRG, our method can be directly applied to non-strongly and strongly\nconvex problems. We show that our method achieves a lower overall complexity\nthan the recently proposed methods that supports non-strongly convex problems.\nMoreover, this method has a fast rate of convergence for strongly convex\nproblems. Our experiments show the effectiveness of our method. \n\n"}
{"id": "1506.03039", "contents": "Title: Measuring Sample Quality with Stein's Method Abstract: To improve the efficiency of Monte Carlo estimation, practitioners are\nturning to biased Markov chain Monte Carlo procedures that trade off asymptotic\nexactness for computational speed. The reasoning is sound: a reduction in\nvariance due to more rapid sampling can outweigh the bias introduced. However,\nthe inexactness creates new challenges for sampler and parameter selection,\nsince standard measures of sample quality like effective sample size do not\naccount for asymptotic bias. To address these challenges, we introduce a new\ncomputable quality measure based on Stein's method that quantifies the maximum\ndiscrepancy between sample and target expectations over a large class of test\nfunctions. We use our tool to compare exact, biased, and deterministic sample\nsequences and illustrate applications to hyperparameter selection, convergence\nrate assessment, and quantifying bias-variance tradeoffs in posterior\ninference. \n\n"}
{"id": "1506.03509", "contents": "Title: Convolutional Dictionary Learning through Tensor Factorization Abstract: Tensor methods have emerged as a powerful paradigm for consistent learning of\nmany latent variable models such as topic models, independent component\nanalysis and dictionary learning. Model parameters are estimated via CP\ndecomposition of the observed higher order input moments. However, in many\ndomains, additional invariances such as shift invariances exist, enforced via\nmodels such as convolutional dictionary learning. In this paper, we develop\nnovel tensor decomposition algorithms for parameter estimation of convolutional\nmodels. Our algorithm is based on the popular alternating least squares method,\nbut with efficient projections onto the space of stacked circulant matrices.\nOur method is embarrassingly parallel and consists of simple operations such as\nfast Fourier transforms and matrix multiplications. Our algorithm converges to\nthe dictionary much faster and more accurately compared to the alternating\nminimization over filters and activation maps. \n\n"}
{"id": "1506.04158", "contents": "Title: A Spectral Algorithm with Additive Clustering for the Recovery of\n  Overlapping Communities in Networks Abstract: This paper presents a novel spectral algorithm with additive clustering\ndesigned to identify overlapping communities in networks. The algorithm is\nbased on geometric properties of the spectrum of the expected adjacency matrix\nin a random graph model that we call stochastic blockmodel with overlap (SBMO).\nAn adaptive version of the algorithm, that does not require the knowledge of\nthe number of hidden communities, is proved to be consistent under the SBMO\nwhen the degrees in the graph are (slightly more than) logarithmic. The\nalgorithm is shown to perform well on simulated data and on real-world graphs\nwith known overlapping communities. \n\n"}
{"id": "1506.05936", "contents": "Title: Sampling constrained probability distributions using Spherical\n  Augmentation Abstract: Statistical models with constrained probability distributions are abundant in\nmachine learning. Some examples include regression models with norm constraints\n(e.g., Lasso), probit, many copula models, and latent Dirichlet allocation\n(LDA). Bayesian inference involving probability distributions confined to\nconstrained domains could be quite challenging for commonly used sampling\nalgorithms. In this paper, we propose a novel augmentation technique that\nhandles a wide range of constraints by mapping the constrained domain to a\nsphere in the augmented space. By moving freely on the surface of this sphere,\nsampling algorithms handle constraints implicitly and generate proposals that\nremain within boundaries when mapped back to the original space. Our proposed\nmethod, called {Spherical Augmentation}, provides a mathematically natural and\ncomputationally efficient framework for sampling from constrained probability\ndistributions. We show the advantages of our method over state-of-the-art\nsampling algorithms, such as exact Hamiltonian Monte Carlo, using several\nexamples including truncated Gaussian distributions, Bayesian Lasso, Bayesian\nbridge regression, reconstruction of quantized stationary Gaussian process, and\nLDA for topic modeling. \n\n"}
{"id": "1506.06081", "contents": "Title: A Convergent Gradient Descent Algorithm for Rank Minimization and\n  Semidefinite Programming from Random Linear Measurements Abstract: We propose a simple, scalable, and fast gradient descent algorithm to\noptimize a nonconvex objective for the rank minimization problem and a closely\nrelated family of semidefinite programs. With $O(r^3 \\kappa^2 n \\log n)$ random\nmeasurements of a positive semidefinite $n \\times n$ matrix of rank $r$ and\ncondition number $\\kappa$, our method is guaranteed to converge linearly to the\nglobal optimum. \n\n"}
{"id": "1506.08858", "contents": "Title: Machine learning for many-body physics: efficient solution of dynamical\n  mean-field theory Abstract: Machine learning methods for solving the equations of dynamical mean-field\ntheory are developed. The method is demonstrated on the three dimensional\nHubbard model. The key technical issues are defining a mapping of an input\nfunction to an output function, and distinguishing metallic from insulating\nsolutions. Both metallic and Mott insulator solutions can be predicted. The\nvalidity of the machine learning scheme is assessed by comparing predictions of\nfull correlation functions, of quasi-particle weight and particle density to\nvalues directly computed. The results indicate that with modest further\ndevelopment, machine learning approach may be an attractive computational\nefficient option for real materials predictions for strongly correlated\nsystems. \n\n"}
{"id": "1507.01784", "contents": "Title: Rethinking LDA: moment matching for discrete ICA Abstract: We consider moment matching techniques for estimation in Latent Dirichlet\nAllocation (LDA). By drawing explicit links between LDA and discrete versions\nof independent component analysis (ICA), we first derive a new set of\ncumulant-based tensors, with an improved sample complexity. Moreover, we reuse\nstandard ICA techniques such as joint diagonalization of tensors to improve\nover existing methods based on the tensor power method. In an extensive set of\nexperiments on both synthetic and real datasets, we show that our new\ncombination of tensors and orthogonal joint diagonalization techniques\noutperforms existing moment matching methods. \n\n"}
{"id": "1507.02216", "contents": "Title: Robust Sparse Blind Source Separation Abstract: Blind Source Separation is a widely used technique to analyze multichannel\ndata. In many real-world applications, its results can be significantly\nhampered by the presence of unknown outliers. In this paper, a novel algorithm\ncoined rGMCA (robust Generalized Morphological Component Analysis) is\nintroduced to retrieve sparse sources in the presence of outliers. It\nexplicitly estimates the sources, the mixing matrix, and the outliers. It also\ntakes advantage of the estimation of the outliers to further implement a\nweighting scheme, which provides a highly robust separation procedure.\nNumerical experiments demonstrate the efficiency of rGMCA to estimate the\nmixing matrix in comparison with standard BSS techniques. \n\n"}
{"id": "1507.03092", "contents": "Title: On the use of Harrell's C for clinical risk prediction via random\n  survival forests Abstract: Random survival forests (RSF) are a powerful method for risk prediction of\nright-censored outcomes in biomedical research. RSF use the log-rank split\ncriterion to form an ensemble of survival trees. The most common approach to\nevaluate the prediction accuracy of a RSF model is Harrell's concordance index\nfor survival data ('C index'). Conceptually, this strategy implies that the\nsplit criterion in RSF is different from the evaluation criterion of interest.\nThis discrepancy can be overcome by using Harrell's C for both node splitting\nand evaluation. We compare the difference between the two split criteria\nanalytically and in simulation studies with respect to the preference of more\nunbalanced splits, termed end-cut preference (ECP). Specifically, we show that\nthe log-rank statistic has a stronger ECP compared to the C index. In\nsimulation studies and with the help of two medical data sets we demonstrate\nthat the accuracy of RSF predictions, as measured by Harrell's C, can be\nimproved if the log-rank statistic is replaced by the C index for node\nsplitting. This is especially true in situations where the censoring rate or\nthe fraction of informative continuous predictor variables is high. Conversely,\nlog-rank splitting is preferable in noisy scenarios. Both C-based and log-rank\nsplitting are implemented in the R~package ranger. We recommend Harrell's C as\nsplit criterion for use in smaller scale clinical studies and the log-rank\nsplit criterion for use in large-scale 'omics' studies. \n\n"}
{"id": "1507.05601", "contents": "Title: Ladder-type electromagnetically induced transparency using\n  nanofiber-guided light in a warm atomic vapor Abstract: We demonstrate ladder-type electromagnetically induced transparency (EIT)\nusing an optical nanofiber suspended in a warm rubidium vapor. The signal and\ncontrol fields are both guided along the nanofiber, which enables strong\nnonlinear interactions with the surrounding atoms at relatively low powers.\nTransit-time broadening is found to be a significant EIT decoherence mechanism\nin this tightly-confined waveguiding geometry. Nonetheless, we observe\nsignificant EIT and controlled polarization rotation using control-field powers\nof only a few microWatts in this relatively robust warm-atom nanofiber system. \n\n"}
{"id": "1508.00317", "contents": "Title: Time-series modeling with undecimated fully convolutional neural\n  networks Abstract: We present a new convolutional neural network-based time-series model.\nTypical convolutional neural network (CNN) architectures rely on the use of\nmax-pooling operators in between layers, which leads to reduced resolution at\nthe top layers. Instead, in this work we consider a fully convolutional network\n(FCN) architecture that uses causal filtering operations, and allows for the\nrate of the output signal to be the same as that of the input signal. We\nfurthermore propose an undecimated version of the FCN, which we refer to as the\nundecimated fully convolutional neural network (UFCNN), and is motivated by the\nundecimated wavelet transform. Our experimental results verify that using the\nundecimated version of the FCN is necessary in order to allow for effective\ntime-series modeling. The UFCNN has several advantages compared to other\ntime-series models such as the recurrent neural network (RNN) and long\nshort-term memory (LSTM), since it does not suffer from either the vanishing or\nexploding gradients problems, and is therefore easier to train. Convolution\noperations can also be implemented more efficiently compared to the recursion\nthat is involved in RNN-based models. We evaluate the performance of our model\nin a synthetic target tracking task using bearing only measurements generated\nfrom a state-space model, a probabilistic modeling of polyphonic music\nsequences problem, and a high frequency trading task using a time-series of\nask/bid quotes and their corresponding volumes. Our experimental results using\nsynthetic and real datasets verify the significant advantages of the UFCNN\ncompared to the RNN and LSTM baselines. \n\n"}
{"id": "1508.03253", "contents": "Title: Obtaining tight bounds on higher-order interferences with a 5-path\n  interferometer Abstract: Within the established theoretical framework of quantum mechanics,\ninterference always occurs between pairs of trajectories. Higher order\ninterferences with multiple constituents are, however, excluded by Born's rule\nand can only exist in generalized probabilistic theories. Thus, high-precision\nexperiments searching for such higher order interferences are a powerful method\nto distinguish between quantum mechanics and more general theories. Here, we\nperform such a test in optical multi-path interferometers. Our results rule out\nthe existence of higher order interference terms to an extent which is more\nthan four orders of magnitude smaller than the expected pairwise interference,\nrefining previous bounds by two orders of magnitude. This establishes the\nhitherto tightest constraints on generalized interference theories. \n\n"}
{"id": "1508.03390", "contents": "Title: Doubly Stochastic Primal-Dual Coordinate Method for Bilinear\n  Saddle-Point Problem Abstract: We propose a doubly stochastic primal-dual coordinate optimization algorithm\nfor empirical risk minimization, which can be formulated as a bilinear\nsaddle-point problem. In each iteration, our method randomly samples a block of\ncoordinates of the primal and dual solutions to update. The linear convergence\nof our method could be established in terms of 1) the distance from the current\niterate to the optimal solution and 2) the primal-dual objective gap. We show\nthat the proposed method has a lower overall complexity than existing\ncoordinate methods when either the data matrix has a factorized structure or\nthe proximal mapping on each block is computationally expensive, e.g.,\ninvolving an eigenvalue decomposition. The efficiency of the proposed method is\nconfirmed by empirical studies on several real applications, such as the\nmulti-task large margin nearest neighbor problem. \n\n"}
{"id": "1508.04306", "contents": "Title: Deep clustering: Discriminative embeddings for segmentation and\n  separation Abstract: We address the problem of acoustic source separation in a deep learning\nframework we call \"deep clustering.\" Rather than directly estimating signals or\nmasking functions, we train a deep network to produce spectrogram embeddings\nthat are discriminative for partition labels given in training data. Previous\ndeep network approaches provide great advantages in terms of learning power and\nspeed, but previously it has been unclear how to use them to separate signals\nin a class-independent way. In contrast, spectral clustering approaches are\nflexible with respect to the classes and number of items to be segmented, but\nit has been unclear how to leverage the learning power and speed of deep\nnetworks. To obtain the best of both worlds, we use an objective function that\nto train embeddings that yield a low-rank approximation to an ideal pairwise\naffinity matrix, in a class-independent way. This avoids the high cost of\nspectral factorization and instead produces compact clusters that are amenable\nto simple clustering methods. The segmentations are therefore implicitly\nencoded in the embeddings, and can be \"decoded\" by clustering. Preliminary\nexperiments show that the proposed method can separate speech: when trained on\nspectrogram features containing mixtures of two speakers, and tested on\nmixtures of a held-out set of speakers, it can infer masking functions that\nimprove signal quality by around 6dB. We show that the model can generalize to\nthree-speaker mixtures despite training only on two-speaker mixtures. The\nframework can be used without class labels, and therefore has the potential to\nbe trained on a diverse set of sound types, and to generalize to novel sources.\nWe hope that future work will lead to segmentation of arbitrary sounds, with\nextensions to microphone array methods as well as image segmentation and other\ndomains. \n\n"}
{"id": "1509.00114", "contents": "Title: Multi-Sensor Slope Change Detection Abstract: We develop a mixture procedure for multi-sensor systems to monitor data\nstreams for a change-point that causes a gradual degradation to a subset of the\nstreams. Observations are assumed to be initially normal random variables with\nknown constant means and variances. After the change-point, observations in the\nsubset will have increasing or decreasing means. The subset and the\nrate-of-changes are unknown. Our procedure uses a mixture statistics, which\nassumes that each sensor is affected by the change-point with probability\n$p_0$. Analytic expressions are obtained for the average run length (ARL) and\nthe expected detection delay (EDD) of the mixture procedure, which are\ndemonstrated to be quite accurate numerically. We establish the asymptotic\noptimality of the mixture procedure. Numerical examples demonstrate the good\nperformance of the proposed procedure. We also discuss an adaptive mixture\nprocedure using empirical Bayes. This paper extends our earlier work on\ndetecting an abrupt change-point that causes a mean-shift, by tackling the\nchallenges posed by the non-stationarity of the slope-change problem. \n\n"}
{"id": "1509.01168", "contents": "Title: Semi-described and semi-supervised learning with Gaussian processes Abstract: Propagating input uncertainty through non-linear Gaussian process (GP)\nmappings is intractable. This hinders the task of training GPs using uncertain\nand partially observed inputs. In this paper we refer to this task as\n\"semi-described learning\". We then introduce a GP framework that solves both,\nthe semi-described and the semi-supervised learning problems (where missing\nvalues occur in the outputs). Auto-regressive state space simulation is also\nrecognised as a special case of semi-described learning. To achieve our goal we\ndevelop variational methods for handling semi-described inputs in GPs, and\ncouple them with algorithms that allow for imputing the missing values while\ntreating the uncertainty in a principled, Bayesian manner. Extensive\nexperiments on simulated and real-world data study the problems of iterative\nforecasting and regression/classification with missing values. The results\nsuggest that the principled propagation of uncertainty stemming from our\nframework can significantly improve performance in these tasks. \n\n"}
{"id": "1509.02347", "contents": "Title: Modelling time evolving interactions in networks through a non\n  stationary extension of stochastic block models Abstract: In this paper, we focus on the stochastic block model (SBM),a probabilistic\ntool describing interactions between nodes of a network using latent clusters.\nThe SBM assumes that the networkhas a stationary structure, in which\nconnections of time varying intensity are not taken into account. In other\nwords, interactions between two groups are forced to have the same features\nduring the whole observation time. To overcome this limitation,we propose a\npartition of the whole time horizon, in which interactions are observed, and\ndevelop a non stationary extension of the SBM,allowing to simultaneously\ncluster the nodes in a network along with fixed time intervals in which the\ninteractions take place. The number of clusters (K for nodes, D for time\nintervals) as well as the class memberships are finallyobtained through\nmaximizing the complete-data integrated likelihood by means of a greedy search\napproach. After showing that the model works properly with simulated data, we\nfocus on a real data set. We thus consider the three days ACM Hypertext\nconference held in Turin,June 29th - July 1st 2009. Proximity interactions\nbetween attendees during the first day are modelled and an\ninterestingclustering of the daily hours is finally obtained, with times of\nsocial gathering (e.g. coffee breaks) recovered by the approach. Applications\nto large networks are limited due to the computational complexity of the greedy\nsearch which is dominated bythe number $K\\_{max}$ and $D\\_{max}$ of clusters\nused in the initialization. Therefore,advanced clustering tools are considered\nto reduce the number of clusters expected in the data, making the greedy search\napplicable to large networks. \n\n"}
{"id": "1509.05090", "contents": "Title: Rotational excitation of molecules with long sequences of intense\n  femtosecond pulses Abstract: We investigate the prospects of creating broad rotational wave packets by\nmeans of molecular interaction with long sequences of intense femtosecond\npulses. Using state-resolved rotational Raman spectroscopy of oxygen, subject\nto a sequence of more than 20 laser pulses with peak intensities exceeding\n$10^{13}$ W/cm$^{2}$ per pulse, we show that the centrifugal distortion is the\nmain obstacle on the way to reaching high rotational states. We demonstrate\nthat the timing of the pulses can be optimized to partially mitigate the\ncentrifugal limit. The cumulative effect of a long pulse sequence results in\nhigh degree of rotational coherence, which is shown to cause an efficient\nspectral broadening of probe light via cascaded Raman transitions. \n\n"}
{"id": "1509.05789", "contents": "Title: BLC: Private Matrix Factorization Recommenders via Automatic Group\n  Learning Abstract: We propose a privacy-enhanced matrix factorization recommender that exploits\nthe fact that users can often be grouped together by interest. This allows a\nform of \"hiding in the crowd\" privacy. We introduce a novel matrix\nfactorization approach suited to making recommendations in a shared group (or\nnym) setting and the BLC algorithm for carrying out this matrix factorization\nin a privacy-enhanced manner. We demonstrate that the increased privacy does\nnot come at the cost of reduced recommendation accuracy. \n\n"}
{"id": "1509.06875", "contents": "Title: Physical meaning of the radial index of Laguerre-Gauss beams Abstract: The Laguerre-Gauss modes are a class of fundamental and well-studied optical\nfields. These stable, shape-invariant photons - exhibiting circular-cylindrical\nsymmetry - are familiar from laser optics, micro-mechanical manipulation,\nquantum optics, communication, and foundational studies in both classical\noptics and quantum physics. They are characterized, chiefly, by two modes\nnumbers: the azimuthal index indicating the orbital angular momentum of the\nbeam - which itself has spawned a burgeoning and vibrant sub-field - and the\nradial index, which up until recently, has largely been ignored. In this\nmanuscript we develop a differential operator formalism for dealing with the\nradial modes in both the position and momentum representations, and - more\nimportantly - give for the first time the meaning of this quantum number in\nterms of a well-defined physical parameter: the \"intrinsic hyperbolic momentum\ncharge\". \n\n"}
{"id": "1510.01722", "contents": "Title: Structured Transforms for Small-Footprint Deep Learning Abstract: We consider the task of building compact deep learning pipelines suitable for\ndeployment on storage and power constrained mobile devices. We propose a\nunified framework to learn a broad family of structured parameter matrices that\nare characterized by the notion of low displacement rank. Our structured\ntransforms admit fast function and gradient evaluation, and span a rich range\nof parameter sharing configurations whose statistical modeling capacity can be\nexplicitly tuned along a continuum from structured to unstructured.\nExperimental results show that these transforms can significantly accelerate\ninference and forward/backward passes during training, and offer superior\naccuracy-compactness-speed tradeoffs in comparison to a number of existing\ntechniques. In keyword spotting applications in mobile speech recognition, our\nmethods are much more effective than standard linear low-rank bottleneck layers\nand nearly retain the performance of state of the art models, while providing\nmore than 3.5-fold compression. \n\n"}
{"id": "1510.04905", "contents": "Title: Robust Partially-Compressed Least-Squares Abstract: Randomized matrix compression techniques, such as the Johnson-Lindenstrauss\ntransform, have emerged as an effective and practical way for solving\nlarge-scale problems efficiently. With a focus on computational efficiency,\nhowever, forsaking solutions quality and accuracy becomes the trade-off. In\nthis paper, we investigate compressed least-squares problems and propose new\nmodels and algorithms that address the issue of error and noise introduced by\ncompression. While maintaining computational efficiency, our models provide\nrobust solutions that are more accurate--relative to solutions of uncompressed\nleast-squares--than those of classical compressed variants. We introduce tools\nfrom robust optimization together with a form of partial compression to improve\nthe error-time trade-offs of compressed least-squares solvers. We develop an\nefficient solution algorithm for our Robust Partially-Compressed (RPC) model\nbased on a reduction to a one-dimensional search. We also derive the first\napproximation error bounds for Partially-Compressed least-squares solutions.\nEmpirical results comparing numerous alternatives suggest that robust and\npartially compressed solutions are effectively insulated against aggressive\nrandomized transforms. \n\n"}
{"id": "1510.07740", "contents": "Title: The Wilson Machine for Image Modeling Abstract: Learning the distribution of natural images is one of the hardest and most\nimportant problems in machine learning. The problem remains open, because the\nenormous complexity of the structures in natural images spans all length\nscales. We break down the complexity of the problem and show that the hierarchy\nof structures in natural images fuels a new class of learning algorithms based\non the theory of critical phenomena and stochastic processes. We approach this\nproblem from the perspective of the theory of critical phenomena, which was\ndeveloped in condensed matter physics to address problems with infinite\nlength-scale fluctuations, and build a framework to integrate the criticality\nof natural images into a learning algorithm. The problem is broken down by\nmapping images into a hierarchy of binary images, called bitplanes. In this\nrepresentation, the top bitplane is critical, having fluctuations in structures\nover a vast range of scales. The bitplanes below go through a gradual\nstochastic heating process to disorder. We turn this representation into a\ndirected probabilistic graphical model, transforming the learning problem into\nthe unsupervised learning of the distribution of the critical bitplane and the\nsupervised learning of the conditional distributions for the remaining\nbitplanes. We learnt the conditional distributions by logistic regression in a\nconvolutional architecture. Conditioned on the critical binary image, this\nsimple architecture can generate large, natural-looking images, with many\nshades of gray, without the use of hidden units, unprecedented in the studies\nof natural images. The framework presented here is a major step in bringing\ncriticality and stochastic processes to machine learning and in studying\nnatural image statistics. \n\n"}
{"id": "1510.07786", "contents": "Title: A Framework to Adjust Dependency Measure Estimates for Chance Abstract: Estimating the strength of dependency between two variables is fundamental\nfor exploratory analysis and many other applications in data mining. For\nexample: non-linear dependencies between two continuous variables can be\nexplored with the Maximal Information Coefficient (MIC); and categorical\nvariables that are dependent to the target class are selected using Gini gain\nin random forests. Nonetheless, because dependency measures are estimated on\nfinite samples, the interpretability of their quantification and the accuracy\nwhen ranking dependencies become challenging. Dependency estimates are not\nequal to 0 when variables are independent, cannot be compared if computed on\ndifferent sample size, and they are inflated by chance on variables with more\ncategories. In this paper, we propose a framework to adjust dependency measure\nestimates on finite samples. Our adjustments, which are simple and applicable\nto any dependency measure, are helpful in improving interpretability when\nquantifying dependency and in improving accuracy on the task of ranking\ndependencies. In particular, we demonstrate that our approach enhances the\ninterpretability of MIC when used as a proxy for the amount of noise between\nvariables, and to gain accuracy when ranking variables during the splitting\nprocedure in random forests. \n\n"}
{"id": "1511.00897", "contents": "Title: Programmable two-photon quantum interference in $10^3$ channels in\n  opaque scattering media Abstract: We investigate two-photon quantum interference in an opaque scattering medium\nthat intrinsically supports $10^6$ transmission channels. By adaptive spatial\nphase-modulation of the incident wavefronts, the photons are directed at\ntargeted speckle spots or output channels. From $10^3$ experimentally available\ncoupled channels, we select two channels and enhance their transmission, to\nrealize the equivalent of a fully programmable $2\\times2$ beam splitter. By\nsending pairs of single photons from a parametric down-conversion source\nthrough the opaque scattering medium, we observe two-photon quantum\ninterference. The programmed beam splitter need not fulfill energy conservation\nover the two selected output channels and hence could be non-unitary.\nConsequently, we have the freedom to tune the quantum interference from\nbunching (Hong-Ou-Mandel-like) to antibunching. Our results establish opaque\nscattering media as a platform for high-dimensional quantum interference that\nis notably relevant for boson sampling and physical-key-based authentication. \n\n"}
{"id": "1511.04408", "contents": "Title: Scalable Gaussian Processes for Characterizing Multidimensional Change\n  Surfaces Abstract: We present a scalable Gaussian process model for identifying and\ncharacterizing smooth multidimensional changepoints, and automatically learning\nchanges in expressive covariance structure. We use Random Kitchen Sink features\nto flexibly define a change surface in combination with expressive spectral\nmixture kernels to capture the complex statistical structure. Finally, through\nthe use of novel methods for additive non-separable kernels, we can scale the\nmodel to large datasets. We demonstrate the model on numerical and real world\ndata, including a large spatio-temporal disease dataset where we identify\npreviously unknown heterogeneous changes in space and time. \n\n"}
{"id": "1511.04590", "contents": "Title: Oracle performance for visual captioning Abstract: The task of associating images and videos with a natural language description\nhas attracted a great amount of attention recently. Rapid progress has been\nmade in terms of both developing novel algorithms and releasing new datasets.\nIndeed, the state-of-the-art results on some of the standard datasets have been\npushed into the regime where it has become more and more difficult to make\nsignificant improvements. Instead of proposing new models, this work\ninvestigates the possibility of empirically establishing performance upper\nbounds on various visual captioning datasets without extra data labelling\neffort or human evaluation. In particular, it is assumed that visual captioning\nis decomposed into two steps: from visual inputs to visual concepts, and from\nvisual concepts to natural language descriptions. One would be able to obtain\nan upper bound when assuming the first step is perfect and only requiring\ntraining a conditional language model for the second step. We demonstrate the\nconstruction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination\nof M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used\nfor visual concept extraction in the first step and the simplicity of the\nlanguage model for the second step, we show that current state-of-the-art\nmodels fall short when being compared with the learned upper bounds.\nFurthermore, with such a bound, we quantify several important factors\nconcerning image and video captioning: the number of visual concepts captured\nby different models, the trade-off between the amount of visual elements\ncaptured and their accuracy, and the intrinsic difficulty and blessing of\ndifferent datasets. \n\n"}
{"id": "1511.05650", "contents": "Title: Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models Abstract: Normalized random measures (NRMs) provide a broad class of discrete random\nmeasures that are often used as priors for Bayesian nonparametric models.\nDirichlet process is a well-known example of NRMs. Most of posterior inference\nmethods for NRM mixture models rely on MCMC methods since they are easy to\nimplement and their convergence is well studied. However, MCMC often suffers\nfrom slow convergence when the acceptance rate is low. Tree-based inference is\nan alternative deterministic posterior inference method, where Bayesian\nhierarchical clustering (BHC) or incremental Bayesian hierarchical clustering\n(IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively.\nAlthough IBHC is a promising method for posterior inference for NRMM models due\nto its efficiency and applicability to online inference, its convergence is not\nguaranteed since it uses heuristics that simply selects the best solution after\nmultiple trials are made. In this paper, we present a hybrid inference\nalgorithm for NRMM models, which combines the merits of both MCMC and IBHC.\nTrees built by IBHC outlines partitions of data, which guides\nMetropolis-Hastings procedure to employ appropriate proposals. Inheriting the\nnature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and\nenjoys the fast convergence thanks to the effective proposals guided by trees.\nExperiments on both synthetic and real-world datasets demonstrate the benefit\nof our method. \n\n"}
{"id": "1511.06067", "contents": "Title: Convolutional neural networks with low-rank regularization Abstract: Large CNNs have delivered impressive performance in various computer vision\napplications. But the storage and computation requirements make it problematic\nfor deploying these models on mobile devices. Recently, tensor decompositions\nhave been used for speeding up CNNs. In this paper, we further develop the\ntensor decomposition technique. We propose a new algorithm for computing the\nlow-rank tensor decomposition for removing the redundancy in the convolution\nkernels. The algorithm finds the exact global optimizer of the decomposition\nand is more effective than iterative methods. Based on the decomposition, we\nfurther propose a new method for training low-rank constrained CNNs from\nscratch. Interestingly, while achieving a significant speedup, sometimes the\nlow-rank constrained CNNs delivers significantly better performance than their\nnon-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank\nNIN model achieves $91.31\\%$ accuracy (without data augmentation), which also\nimproves upon state-of-the-art result. We evaluated the proposed method on\nCIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet,\nNIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is\nreduced by half while the performance is still comparable. Empirical success\nsuggests that low-rank tensor decompositions can be a very useful tool for\nspeeding up large CNNs. \n\n"}
{"id": "1511.06443", "contents": "Title: Neural Network Matrix Factorization Abstract: Data often comes in the form of an array or matrix. Matrix factorization\ntechniques attempt to recover missing or corrupted entries by assuming that the\nmatrix can be written as the product of two low-rank matrices. In other words,\nmatrix factorization approximates the entries of the matrix by a simple, fixed\nfunction---namely, the inner product---acting on the latent feature vectors for\nthe corresponding row and column. Here we consider replacing the inner product\nby an arbitrary function that we learn from the data at the same time as we\nlearn the latent feature vectors. In particular, we replace the inner product\nby a multi-layer feed-forward neural network, and learn by alternating between\noptimizing the network for fixed latent features, and optimizing the latent\nfeatures for a fixed network. The resulting approach---which we call neural\nnetwork matrix factorization or NNMF, for short---dominates standard low-rank\ntechniques on a suite of benchmark but is dominated by some recent proposals\nthat take advantage of the graph features. Given the vast range of\narchitectures, activation functions, regularizers, and optimization techniques\nthat could be used within the NNMF framework, it seems likely the true\npotential of the approach has yet to be reached. \n\n"}
{"id": "1511.06909", "contents": "Title: BlackOut: Speeding up Recurrent Neural Network Language Models With Very\n  Large Vocabularies Abstract: We propose BlackOut, an approximation algorithm to efficiently train massive\nrecurrent neural network language models (RNNLMs) with million word\nvocabularies. BlackOut is motivated by using a discriminative loss, and we\ndescribe a new sampling strategy which significantly reduces computation while\nimproving stability, sample efficiency, and rate of convergence. One way to\nunderstand BlackOut is to view it as an extension of the DropOut strategy to\nthe output layer, wherein we use a discriminative training loss and a weighted\nsampling scheme. We also establish close connections between BlackOut,\nimportance sampling, and noise contrastive estimation (NCE). Our experiments,\non the recently released one billion word language modeling benchmark,\ndemonstrate scalability and accuracy of BlackOut; we outperform the\nstate-of-the art, and achieve the lowest perplexity scores on this dataset.\nMoreover, unlike other established methods which typically require GPUs or CPU\nclusters, we show that a carefully implemented version of BlackOut requires\nonly 1-10 days on a single machine to train a RNNLM with a million word\nvocabulary and billions of parameters on one billion words. Although we\ndescribe BlackOut in the context of RNNLM training, it can be used to any\nnetworks with large softmax output layers. \n\n"}
{"id": "1511.07944", "contents": "Title: Maximum Likelihood Estimation for Single Linkage Hierarchical Clustering Abstract: We derive a statistical model for estimation of a dendrogram from single\nlinkage hierarchical clustering (SLHC) that takes account of uncertainty\nthrough noise or corruption in the measurements of separation of data. Our\nfocus is on just the estimation of the hierarchy of partitions afforded by the\ndendrogram, rather than the heights in the latter. The concept of estimating\nthis \"dendrogram structure'' is introduced, and an approximate maximum\nlikelihood estimator (MLE) for the dendrogram structure is described. These\nideas are illustrated by a simple Monte Carlo simulation that, at least for\nsmall data sets, suggests the method outperforms SLHC in the presence of noise. \n\n"}
{"id": "1511.08768", "contents": "Title: Gradient Estimation with Simultaneous Perturbation and Compressive\n  Sensing Abstract: This paper aims at achieving a \"good\" estimator for the gradient of a\nfunction on a high-dimensional space. Often such functions are not sensitive in\nall coordinates and the gradient of the function is almost sparse. We propose a\nmethod for gradient estimation that combines ideas from Spall's Simultaneous\nPerturbation Stochastic Approximation with compressive sensing. The aim is to\nobtain \"good\" estimator without too many function evaluations. Application to\nestimating gradient outer product matrix as well as standard optimization\nproblems are illustrated via simulations. \n\n"}
{"id": "1512.01131", "contents": "Title: Near-complete polarization Bell-state analysis based on symmetry-broken\n  scheme with linear optics Abstract: Bell-state analysis is a considerable challenge and an essential requirement\nfor reliable implementation of quantum communication proposals. An open\nquestion is the one for the maximal fraction of successful Bell measurements.\nIt has been pointed out that no scheme using only linear elements can implement\na Bell state analyzer. Some effort has paid attention to the complete\npolarization-entangled Bell-state analysis using linear optics, with the aid of\nauxiliary means. Here we present a symmetry-broken scheme with linear optics\nonly, without any aid of other auxiliary means, for discriminating\npolarization-entangled Bell states. Although our scheme is unable of realizing\ncomplete Bell-state measurement for less photon-pairs situation, it can\ndeterministically identify four Bell states with success probabilities beyond\n99.2% provided that photon-pairs are not less than 8. Our scheme as a\nsignificant breakthrough is simpler and feasible with respect to the current\ntechnology for the near-complete Bell-state analysis. Symmetry breaking is\nindispensable in our scheme as in other physical systems. \n\n"}
{"id": "1512.03081", "contents": "Title: Gamma Belief Networks Abstract: To infer multilayer deep representations of high-dimensional discrete and\nnonnegative real vectors, we propose an augmentable gamma belief network (GBN)\nthat factorizes each of its hidden layers into the product of a sparse\nconnection weight matrix and the nonnegative real hidden units of the next\nlayer. The GBN's hidden layers are jointly trained with an upward-downward\nGibbs sampler that solves each layer with the same subroutine. The\ngamma-negative binomial process combined with a layer-wise training strategy\nallows inferring the width of each layer given a fixed budget on the width of\nthe first layer. Example results illustrate interesting relationships between\nthe width of the first layer and the inferred network structure, and\ndemonstrate that the GBN can add more layers to improve its performance in both\nunsupervisedly extracting features and predicting heldout data. For exploratory\ndata analysis, we extract trees and subnetworks from the learned deep network\nto visualize how the very specific factors discovered at the first hidden layer\nand the increasingly more general factors discovered at deeper hidden layers\nare related to each other, and we generate synthetic data by propagating random\nvariables through the deep network from the top hidden layer back to the bottom\ndata layer. \n\n"}
{"id": "1512.07329", "contents": "Title: Super-resolution microscopy of single atoms in optical lattices Abstract: We report on image processing techniques and experimental procedures to\ndetermine the lattice-site positions of single atoms in an optical lattice with\nhigh reliability, even for limited acquisition time or optical resolution.\nDetermining the positions of atoms beyond the diffraction limit relies on\nparametric deconvolution in close analogy to methods employed in\nsuper-resolution microscopy. We develop a deconvolution method that makes\neffective use of the prior knowledge of the optical transfer function, noise\nproperties, and discreteness of the optical lattice. We show that accurate\nknowledge of the image formation process enables a dramatic improvement on the\nlocalization reliability. This allows us to demonstrate super-resolution of the\natoms' position in closely packed ensembles where the separation between\nparticles cannot be directly optically resolved. Furthermore, we demonstrate\nexperimental methods to precisely reconstruct the point spread function with\nsub-pixel resolution from fluorescence images of single atoms, and we give a\nmathematical foundation thereof. We also discuss discretized image sampling in\npixel detectors and provide a quantitative model of noise sources in electron\nmultiplying CCD cameras. The techniques developed here are not only beneficial\nto neutral atom experiments, but could also be employed to improve the\nlocalization precision of trapped ions for ultra precise force sensing. \n\n"}
{"id": "1601.01663", "contents": "Title: Measurement-induced macroscopic superposition states in cavity\n  optomechanics Abstract: We present a novel proposal for generating quantum superpositions of\nmacroscopically distinct states of a bulk mechanical oscillator, compatible\nwith existing optomechanical devices operating in the readily achievable\nbad-cavity limit. The scheme is based on a pulsed cavity optomechanical quantum\nnon-demolition (QND) interaction, driven by displaced non-Gaussian states, and\nmeasurement-induced feedback, avoiding the need for strong single-photon\noptomechanical coupling. Furthermore, we show that single-quadrature cooling of\nthe mechanical oscillator is sufficient for efficient state preparation, and we\noutline a three-pulse protocol comprising a sequence of QND interactions for\nsqueezing-enhanced cooling, state preparation, and tomography. \n\n"}
{"id": "1601.04738", "contents": "Title: Sub-Sampled Newton Methods II: Local Convergence Rates Abstract: Many data-fitting applications require the solution of an optimization\nproblem involving a sum of large number of functions of high dimensional\nparameter. Here, we consider the problem of minimizing a sum of $n$ functions\nover a convex constraint set $\\mathcal{X} \\subseteq \\mathbb{R}^{p}$ where both\n$n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$\ncan offer great amount of computational efficiency.\n  Within the context of second order methods, we first give quantitative local\nconvergence results for variants of Newton's method where the Hessian is\nuniformly sub-sampled. Using random matrix concentration inequalities, one can\nsub-sample in a way that the curvature information is preserved. Using such\nsub-sampling strategy, we establish locally Q-linear and Q-superlinear\nconvergence rates. We also give additional convergence results for when the\nsub-sampled Hessian is regularized by modifying its spectrum or Levenberg-type\nregularization.\n  Finally, in addition to Hessian sub-sampling, we consider sub-sampling the\ngradient as way to further reduce the computational complexity per iteration.\nWe use approximate matrix multiplication results from randomized numerical\nlinear algebra (RandNLA) to obtain the proper sampling strategy and we\nestablish locally R-linear convergence rates. In such a setting, we also show\nthat a very aggressive sample size increase results in a R-superlinearly\nconvergent algorithm.\n  While the sample size depends on the condition number of the problem, our\nconvergence rates are problem-independent, i.e., they do not depend on the\nquantities related to the problem. Hence, our analysis here can be used to\ncomplement the results of our basic framework from the companion paper, [38],\nby exploring algorithmic trade-offs that are important in practice. \n\n"}
{"id": "1602.01132", "contents": "Title: Interactive algorithms: from pool to stream Abstract: We consider interactive algorithms in the pool-based setting, and in the\nstream-based setting. Interactive algorithms observe suggested elements\n(representing actions or queries), and interactively select some of them and\nreceive responses. Pool-based algorithms can select elements at any order,\nwhile stream-based algorithms observe elements in sequence, and can only select\nelements immediately after observing them. We assume that the suggested\nelements are generated independently from some source distribution, and ask\nwhat is the stream size required for emulating a pool algorithm with a given\npool size. We provide algorithms and matching lower bounds for general pool\nalgorithms, and for utility-based pool algorithms. We further show that a\nmaximal gap between the two settings exists also in the special case of active\nlearning for binary classification. \n\n"}
{"id": "1602.02018", "contents": "Title: Compressive Spectral Clustering Abstract: Spectral clustering has become a popular technique due to its high\nperformance in many contexts. It comprises three main steps: create a\nsimilarity graph between N objects to cluster, compute the first k eigenvectors\nof its Laplacian matrix to define a feature vector for each object, and run\nk-means on these features to separate objects into k classes. Each of these\nthree steps becomes computationally intensive for large N and/or k. We propose\nto speed up the last two steps based on recent results in the emerging field of\ngraph signal processing: graph filtering of random signals, and random sampling\nof bandlimited graph signals. We prove that our method, with a gain in\ncomputation time that can reach several orders of magnitude, is in fact an\napproximation of spectral clustering, for which we are able to control the\nerror. We test the performance of our method on artificial and real-world\nnetwork data. \n\n"}
{"id": "1602.03571", "contents": "Title: High Dimensional Inference with Random Maximum A-Posteriori\n  Perturbations Abstract: This paper presents a new approach, called perturb-max, for high-dimensional\nstatistical inference that is based on applying random perturbations followed\nby optimization. This framework injects randomness to maximum a-posteriori\n(MAP) predictors by randomly perturbing the potential function for the input. A\nclassic result from extreme value statistics asserts that perturb-max\noperations generate unbiased samples from the Gibbs distribution using\nhigh-dimensional perturbations. Unfortunately, the computational cost of\ngenerating so many high-dimensional random variables can be prohibitive.\nHowever, when the perturbations are of low dimension, sampling the perturb-max\nprediction is as efficient as MAP optimization. This paper shows that the\nexpected value of perturb-max inference with low dimensional perturbations can\nbe used sequentially to generate unbiased samples from the Gibbs distribution.\nFurthermore the expected value of the maximal perturbations is a natural bound\non the entropy of such perturb-max models. A measure concentration result for\nperturb-max values shows that the deviation of their sampled average from its\nexpectation decays exponentially in the number of samples, allowing effective\napproximation of the expectation. \n\n"}
{"id": "1602.03600", "contents": "Title: Data-Driven Online Decision Making with Costly Information Acquisition Abstract: In most real-world settings such as recommender systems, finance, and\nhealthcare, collecting useful information is costly and requires an active\nchoice on the part of the decision maker. The decision-maker needs to learn\nsimultaneously what observations to make and what actions to take. This paper\nincorporates the information acquisition decision into an online learning\nframework. We propose two different algorithms for this dual learning problem:\nSim-OOS and Seq-OOS where observations are made simultaneously and\nsequentially, respectively. We prove that both algorithms achieve a regret that\nis sublinear in time. The developed framework and algorithms can be used in\nmany applications including medical informatics, recommender systems and\nactionable intelligence in transportation, finance, cyber-security etc., in\nwhich collecting information prior to making decisions is costly. We validate\nour algorithms in a breast cancer example setting in which we show substantial\nperformance gains for our proposed algorithms. \n\n"}
{"id": "1602.04283", "contents": "Title: Deep Learning on FPGAs: Past, Present, and Future Abstract: The rapid growth of data size and accessibility in recent years has\ninstigated a shift of philosophy in algorithm design for artificial\nintelligence. Instead of engineering algorithms by hand, the ability to learn\ncomposable systems automatically from massive amounts of data has led to\nground-breaking performance in important domains such as computer vision,\nspeech recognition, and natural language processing. The most popular class of\ntechniques used in these domains is called deep learning, and is seeing\nsignificant attention from industry. However, these models require incredible\namounts of data and compute power to train, and are limited by the need for\nbetter hardware acceleration to accommodate scaling beyond current data and\nmodel sizes. While the current solution has been to use clusters of graphics\nprocessing units (GPU) as general purpose processors (GPGPU), the use of field\nprogrammable gate arrays (FPGA) provide an interesting alternative. Current\ntrends in design tools for FPGAs have made them more compatible with the\nhigh-level software practices typically practiced in the deep learning\ncommunity, making FPGAs more accessible to those who build and deploy models.\nSince FPGA architectures are flexible, this could also allow researchers the\nability to explore model-level optimizations beyond what is possible on fixed\narchitectures such as GPUs. As well, FPGAs tend to provide high performance per\nwatt of power consumption, which is of particular importance for application\nscientists interested in large scale server-based deployment or\nresource-limited embedded applications. This review takes a look at deep\nlearning and FPGAs from a hardware acceleration perspective, identifying trends\nand innovations that make these technologies a natural fit, and motivates a\ndiscussion on how FPGAs may best serve the needs of the deep learning community\nmoving forward. \n\n"}
{"id": "1602.06516", "contents": "Title: Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling\n  Techniques Abstract: In a series of recent works, we have generalised the consistency results in\nthe stochastic block model literature to the case of uniform and non-uniform\nhypergraphs. The present paper continues the same line of study, where we focus\non partitioning weighted uniform hypergraphs---a problem often encountered in\ncomputer vision. This work is motivated by two issues that arise when a\nhypergraph partitioning approach is used to tackle computer vision problems:\n(i) The uniform hypergraphs constructed for higher-order learning contain all\nedges, but most have negligible weights. Thus, the adjacency tensor is nearly\nsparse, and yet, not binary. (ii) A more serious concern is that standard\npartitioning algorithms need to compute all edge weights, which is\ncomputationally expensive for hypergraphs. This is usually resolved in practice\nby merging the clustering algorithm with a tensor sampling strategy---an\napproach that is yet to be analysed rigorously. We build on our earlier work on\npartitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati,\nICML, 2015), and address the aforementioned issues by proposing provable and\nefficient partitioning algorithms. Our analysis justifies the empirical success\nof practical sampling techniques. We also complement our theoretical findings\nby elaborate empirical comparison of various hypergraph partitioning schemes. \n\n"}
{"id": "1602.07109", "contents": "Title: Variational Inference for On-line Anomaly Detection in High-Dimensional\n  Time Series Abstract: Approximate variational inference has shown to be a powerful tool for\nmodeling unknown complex probability distributions. Recent advances in the\nfield allow us to learn probabilistic models of sequences that actively exploit\nspatial and temporal structure. We apply a Stochastic Recurrent Network (STORN)\nto learn robot time series data. Our evaluation demonstrates that we can\nrobustly detect anomalies both off- and on-line. \n\n"}
{"id": "1603.00929", "contents": "Title: A Kernel Test for Three-Variable Interactions with Random Processes Abstract: We apply a wild bootstrap method to the Lancaster three-variable interaction\nmeasure in order to detect factorisation of the joint distribution on three\nvariables forming a stationary random process, for which the existing\npermutation bootstrap method fails. As in the i.i.d. case, the Lancaster test\nis found to outperform existing tests in cases for which two independent\nvariables individually have a weak influence on a third, but that when\nconsidered jointly the influence is strong. The main contributions of this\npaper are twofold: first, we prove that the Lancaster statistic satisfies the\nconditions required to estimate the quantiles of the null distribution using\nthe wild bootstrap; second, the manner in which this is proved is novel,\nsimpler than existing methods, and can further be applied to other statistics. \n\n"}
{"id": "1603.03266", "contents": "Title: Topological Phenomena in Classical Optical Networks Abstract: We propose a scheme to realize a topological insulator with optical-passive\nelements, and analyze the effects of Kerr-nonlinearities in its topological\nbehavior. In the linear regime, our design gives rise to an optical spectrum\nwith topological features and where the bandwidths and bandgaps are\ndramatically broadened. The resulting edge modes cover a very wide frequency\nrange. We relate this behavior to the fact that the effective Hamiltonian\ndescribing the system's amplitudes is long-range. We also develop a method to\nanalyze the scheme in the presence of a Kerr medium. We assess robustness and\nstability of the topological features, and predict the presence of chiral\nsqueezed fluctuations at the edges in some parameter regimes. \n\n"}
{"id": "1603.04136", "contents": "Title: On the Influence of Momentum Acceleration on Online Learning Abstract: The article examines in some detail the convergence rate and\nmean-square-error performance of momentum stochastic gradient methods in the\nconstant step-size and slow adaptation regime. The results establish that\nmomentum methods are equivalent to the standard stochastic gradient method with\na re-scaled (larger) step-size value. The size of the re-scaling is determined\nby the value of the momentum parameter. The equivalence result is established\nfor all time instants and not only in steady-state. The analysis is carried out\nfor general strongly convex and smooth risk functions, and is not limited to\nquadratic risks. One notable conclusion is that the well-known bene ts of\nmomentum constructions for deterministic optimization problems do not\nnecessarily carry over to the adaptive online setting when small constant\nstep-sizes are used to enable continuous adaptation and learn- ing in the\npresence of persistent gradient noise. From simulations, the equivalence\nbetween momentum and standard stochastic gradient methods is also observed for\nnon-differentiable and non-convex problems. \n\n"}
{"id": "1603.04549", "contents": "Title: Matching while Learning Abstract: We consider the problem faced by a service platform that needs to match\nlimited supply with demand but also to learn the attributes of new users in\norder to match them better in the future. We introduce a benchmark model with\nheterogeneous \"workers\" (demand) and a limited supply of \"jobs\" that arrive\nover time. Job types are known to the platform, but worker types are unknown\nand must be learned by observing match outcomes. Workers depart after\nperforming a certain number of jobs. The expected payoff from a match depends\non the pair of types and the goal is to maximize the steady-state rate of\naccumulation of payoff. Though we use terminology inspired by labor markets,\nour framework applies more broadly to platforms where a limited supply of\nheterogeneous products is matched to users over time.\n  Our main contribution is a complete characterization of the structure of the\noptimal policy in the limit that each worker performs many jobs. The platform\nfaces a trade-off for each worker between myopically maximizing payoffs\n(exploitation) and learning the type of the worker (exploration). This creates\na multitude of multi-armed bandit problems, one for each worker, coupled\ntogether by the constraint on availability of jobs of different types (capacity\nconstraints). We find that the platform should estimate a shadow price for each\njob type, and use the payoffs adjusted by these prices, first, to determine its\nlearning goals and then, for each worker, (i) to balance learning with payoffs\nduring the \"exploration phase,\" and (ii) to myopically match after it has\nachieved its learning goals during the \"exploitation phase.\" \n\n"}
{"id": "1603.04904", "contents": "Title: Turing learning: a metric-free approach to inferring behavior and its\n  application to swarms Abstract: We propose Turing Learning, a novel system identification method for\ninferring the behavior of natural or artificial systems. Turing Learning\nsimultaneously optimizes two populations of computer programs, one representing\nmodels of the behavior of the system under investigation, and the other\nrepresenting classifiers. By observing the behavior of the system as well as\nthe behaviors produced by the models, two sets of data samples are obtained.\nThe classifiers are rewarded for discriminating between these two sets, that\nis, for correctly categorizing data samples as either genuine or counterfeit.\nConversely, the models are rewarded for 'tricking' the classifiers into\ncategorizing their data samples as genuine. Unlike other methods for system\nidentification, Turing Learning does not require predefined metrics to quantify\nthe difference between the system and its models. We present two case studies\nwith swarms of simulated robots and prove that the underlying behaviors cannot\nbe inferred by a metric-based system identification method. By contrast, Turing\nLearning infers the behaviors with high accuracy. It also produces a useful\nby-product - the classifiers - that can be used to detect abnormal behavior in\nthe swarm. Moreover, we show that Turing Learning also successfully infers the\nbehavior of physical robot swarms. The results show that collective behaviors\ncan be directly inferred from motion trajectories of individuals in the swarm,\nwhich may have significant implications for the study of animal collectives.\nFurthermore, Turing Learning could prove useful whenever a behavior is not\neasily characterizable using metrics, making it suitable for a wide range of\napplications. \n\n"}
{"id": "1603.09638", "contents": "Title: Detection under Privileged Information Abstract: For well over a quarter century, detection systems have been driven by models\nlearned from input features collected from real or simulated environments. An\nartifact (e.g., network event, potential malware sample, suspicious email) is\ndeemed malicious or non-malicious based on its similarity to the learned model\nat runtime. However, the training of the models has been historically limited\nto only those features available at runtime. In this paper, we consider an\nalternate learning approach that trains models using \"privileged\"\ninformation--features available at training time but not at runtime--to improve\nthe accuracy and resilience of detection systems. In particular, we adapt and\nextend recent advances in knowledge transfer, model influence, and distillation\nto enable the use of forensic or other data unavailable at runtime in a range\nof security domains. An empirical evaluation shows that privileged information\nincreases precision and recall over a system with no privileged information: we\nobserve up to 7.7% relative decrease in detection error for fast-flux bot\ndetection, 8.6% for malware traffic detection, 7.3% for malware classification,\nand 16.9% for face recognition. We explore the limitations and applications of\ndifferent privileged information techniques in detection systems. Such\ntechniques provide a new means for detection systems to learn from data that\nwould otherwise not be available at runtime. \n\n"}
{"id": "1604.00974", "contents": "Title: Writer-independent Feature Learning for Offline Signature Verification\n  using Deep Convolutional Neural Networks Abstract: Automatic Offline Handwritten Signature Verification has been researched over\nthe last few decades from several perspectives, using insights from graphology,\ncomputer vision, signal processing, among others. In spite of the advancements\non the field, building classifiers that can separate between genuine signatures\nand skilled forgeries (forgeries made targeting a particular signature) is\nstill hard. We propose approaching the problem from a feature learning\nperspective. Our hypothesis is that, in the absence of a good model of the data\ngeneration process, it is better to learn the features from data, instead of\nusing hand-crafted features that have no resemblance to the signature\ngeneration process. To this end, we use Deep Convolutional Neural Networks to\nlearn features in a writer-independent format, and use this model to obtain a\nfeature representation on another set of users, where we train writer-dependent\nclassifiers. We tested our method in two datasets: GPDS-960 and Brazilian\nPUC-PR. Our experimental results show that the features learned in a subset of\nthe users are discriminative for the other users, including across different\ndatasets, reaching close to the state-of-the-art in the GPDS dataset, and\nimproving the state-of-the-art in the Brazilian PUC-PR dataset. \n\n"}
{"id": "1604.01662", "contents": "Title: A Survey on Bayesian Deep Learning Abstract: A comprehensive artificial intelligence system needs to not only perceive the\nenvironment with different `senses' (e.g., seeing and hearing) but also infer\nthe world's conditional (or even causal) relations and corresponding\nuncertainty. The past decade has seen major advances in many perception tasks\nsuch as visual object recognition and speech recognition using deep learning\nmodels. For higher-level inference, however, probabilistic graphical models\nwith their Bayesian nature are still more powerful and flexible. In recent\nyears, Bayesian deep learning has emerged as a unified probabilistic framework\nto tightly integrate deep learning and Bayesian models. In this general\nframework, the perception of text or images using deep learning can boost the\nperformance of higher-level inference and in turn, the feedback from the\ninference process is able to enhance the perception of text or images. This\nsurvey provides a comprehensive introduction to Bayesian deep learning and\nreviews its recent applications on recommender systems, topic models, control,\netc. Besides, we also discuss the relationship and differences between Bayesian\ndeep learning and other related topics such as Bayesian treatment of neural\nnetworks. For a constantly updating project page, please refer to\nhttps://github.com/js05212/BayesianDeepLearning-Survey. \n\n"}
{"id": "1604.02459", "contents": "Title: Bandwidth manipulation of quantum light by an electro-optic time lens Abstract: The ability to manipulate the spectral-temporal waveform of optical pulses\nhas enabled a wide range of applications from ultrafast spectroscopy to\nhigh-speed communications. Extending these concepts to quantum light has the\npotential to enable breakthroughs in optical quantum science and technology.\nHowever, filtering and amplifying often employed in classical pulse shaping\ntechniques are incompatible with non-classical light. Controlling the pulsed\nmode structure of quantum light requires efficient means to achieve\ndeterministic, unitary manipulation that preserves fragile quantum coherences.\nHere we demonstrate an electro-optic method for modifying the spectrum of\nnon-classical light by employing a time lens. In particular we show\nhighly-efficient wavelength-preserving six-fold compression of single-photon\nspectral intensity bandwidth, enabling over a two-fold increase of\nsingle-photon flux into a spectrally narrowband absorber. These results pave\nthe way towards spectral-temporal photonic quantum information processing and\nfacilitate interfacing of different physical platforms where quantum\ninformation can be stored or manipulated. \n\n"}
{"id": "1604.02492", "contents": "Title: Challenges in Bayesian Adaptive Data Analysis Abstract: Traditional statistical analysis requires that the analysis process and data\nare independent. By contrast, the new field of adaptive data analysis hopes to\nunderstand and provide algorithms and accuracy guarantees for research as it is\ncommonly performed in practice, as an iterative process of interacting\nrepeatedly with the same data set, such as repeated tests against a holdout\nset. Previous work has defined a model with a rather strong lower bound on\nsample complexity in terms of the number of queries, $n\\sim\\sqrt q$, arguing\nthat adaptive data analysis is much harder than static data analysis, where\n$n\\sim\\log q$ is possible. Instead, we argue that those strong lower bounds\npoint to a limitation of the previous model in that it must consider wildly\nasymmetric scenarios which do not hold in typical applications.\n  To better understand other difficulties of adaptivity, we propose a new\nBayesian version of the problem that mandates symmetry. Since the other lower\nbound techniques are ruled out, we can more effectively see difficulties that\nmight otherwise be overshadowed. As a first contribution to this model, we\nproduce a new problem using error-correcting codes on which a large family of\nmethods, including all previously proposed algorithms, require roughly\n$n\\sim\\sqrt[4]q$. These early results illustrate new difficulties in adaptive\ndata analysis regarding slightly correlated queries on problems with\nconcentrated uncertainty. \n\n"}
{"id": "1604.04939", "contents": "Title: Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor\n  Analysis Abstract: Factor analysis aims to determine latent factors, or traits, which summarize\na given data set. Inter-battery factor analysis extends this notion to multiple\nviews of the data. In this paper we show how a nonlinear, nonparametric version\nof these models can be recovered through the Gaussian process latent variable\nmodel. This gives us a flexible formalism for multi-view learning where the\nlatent variables can be used both for exploratory purposes and for learning\nrepresentations that enable efficient inference for ambiguous estimation tasks.\nLearning is performed in a Bayesian manner through the formulation of a\nvariational compression scheme which gives a rigorous lower bound on the log\nlikelihood. Our Bayesian framework provides strong regularization during\ntraining, allowing the structure of the latent space to be determined\nefficiently and automatically. We demonstrate this by producing the first (to\nour knowledge) published results of learning from dozens of views, even when\ndata is scarce. We further show experimental results on several different types\nof multi-view data sets and for different kinds of tasks, including exploratory\ndata analysis, generation, ambiguity modelling through latent priors and\nclassification. \n\n"}
{"id": "1604.04939", "contents": "Title: Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor\n  Analysis Abstract: Factor analysis aims to determine latent factors, or traits, which summarize\na given data set. Inter-battery factor analysis extends this notion to multiple\nviews of the data. In this paper we show how a nonlinear, nonparametric version\nof these models can be recovered through the Gaussian process latent variable\nmodel. This gives us a flexible formalism for multi-view learning where the\nlatent variables can be used both for exploratory purposes and for learning\nrepresentations that enable efficient inference for ambiguous estimation tasks.\nLearning is performed in a Bayesian manner through the formulation of a\nvariational compression scheme which gives a rigorous lower bound on the log\nlikelihood. Our Bayesian framework provides strong regularization during\ntraining, allowing the structure of the latent space to be determined\nefficiently and automatically. We demonstrate this by producing the first (to\nour knowledge) published results of learning from dozens of views, even when\ndata is scarce. We further show experimental results on several different types\nof multi-view data sets and for different kinds of tasks, including exploratory\ndata analysis, generation, ambiguity modelling through latent priors and\nclassification. \n\n"}
{"id": "1604.07081", "contents": "Title: Optomechanical Multi-Mode Hamiltonian for Nanophotonic Waveguides Abstract: We develop a systematic method for deriving a quantum optical multi-mode\nHamiltonian for the interaction of photons and phonons in nanophotonic\ndielectric materials by applying perturbation theory to the electromagnetic\nHamiltonian. The Hamiltonian covers radiation pressure and electrostrictive\ninteractions on equal footing. As a paradigmatic example, we apply our method\nto a cylindrical nanoscale waveguide, and derive a Hamiltonian description of\nBrillouin quantum optomechanics. We show analytically that in nanoscale\nwaveguides radiation pressure dominates over electrostriction, in agreement\nwith recent experiments. The calculated photon-phonon coupling parameters are\nused to infer gain parameters of Stokes Brillouin scattering in good agreement\nwith experimental observations. \n\n"}
{"id": "1604.07101", "contents": "Title: Double Thompson Sampling for Dueling Bandits Abstract: In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for\ndueling bandit problems. As indicated by its name, D-TS selects both the first\nand the second candidates according to Thompson Sampling. Specifically, D-TS\nmaintains a posterior distribution for the preference matrix, and chooses the\npair of arms for comparison by sampling twice from the posterior distribution.\nThis simple algorithm applies to general Copeland dueling bandits, including\nCondorcet dueling bandits as its special case. For general Copeland dueling\nbandits, we show that D-TS achieves $O(K^2 \\log T)$ regret. For Condorcet\ndueling bandits, we further simplify the D-TS algorithm and show that the\nsimplified D-TS algorithm achieves $O(K \\log T + K^2 \\log \\log T)$ regret.\nSimulation results based on both synthetic and real-world data demonstrate the\nefficiency of the proposed D-TS algorithm. \n\n"}
{"id": "1605.00201", "contents": "Title: Further properties of the forward-backward envelope with applications to\n  difference-of-convex programming Abstract: In this paper, we further study the forward-backward envelope first\nintroduced in [28] and [30] for problems whose objective is the sum of a proper\nclosed convex function and a twice continuously differentiable possibly\nnonconvex function with Lipschitz continuous gradient. We derive sufficient\nconditions on the original problem for the corresponding forward-backward\nenvelope to be a level-bounded and Kurdyka-{\\L}ojasiewicz function with an\nexponent of $\\frac12$; these results are important for the efficient\nminimization of the forward-backward envelope by classical optimization\nalgorithms. In addition, we demonstrate how to minimize some\ndifference-of-convex regularized least squares problems by minimizing a\nsuitably constructed forward-backward envelope. Our preliminary numerical\nresults on randomly generated instances of large-scale $\\ell_{1-2}$ regularized\nleast squares problems [37] illustrate that an implementation of this approach\nwith a limited-memory BFGS scheme usually outperforms standard first-order\nmethods such as the nonmonotone proximal gradient method in [35]. \n\n"}
{"id": "1605.01384", "contents": "Title: Multilevel Monte Carlo methods for the approximation of invariant\n  measures of stochastic differential equations Abstract: We develop a framework that allows the use of the multi-level Monte Carlo\n(MLMC) methodology (Giles2015) to calculate expectations with respect to the\ninvariant measure of an ergodic SDE. In that context, we study the\n(over-damped) Langevin equations with a strongly concave potential. We show\nthat, when appropriate contracting couplings for the numerical integrators are\navailable, one can obtain a uniform in time estimate of the MLMC variance in\ncontrast to the majority of the results in the MLMC literature. As a\nconsequence, a root mean square error of $\\mathcal{O}(\\varepsilon)$ is achieved\nwith $\\mathcal{O}(\\varepsilon^{-2})$ complexity on par with Markov Chain Monte\nCarlo (MCMC) methods, which however can be computationally intensive when\napplied to large data sets. Finally, we present a multi-level version of the\nrecently introduced Stochastic Gradient Langevin Dynamics (SGLD) method\n(Welling and Teh, 2011) built for large datasets applications. We show that\nthis is the first stochastic gradient MCMC method with complexity\n$\\mathcal{O}(\\varepsilon^{-2}|\\log {\\varepsilon}|^{3})$, in contrast to the\ncomplexity $\\mathcal{O}(\\varepsilon^{-3})$ of currently available methods.\nNumerical experiments confirm our theoretical findings. \n\n"}
{"id": "1605.02268", "contents": "Title: Rate-Distortion Bounds on Bayes Risk in Supervised Learning Abstract: We present an information-theoretic framework for bounding the number of\nlabeled samples needed to train a classifier in a parametric Bayesian setting.\nWe derive bounds on the average $L_p$ distance between the learned classifier\nand the true maximum a posteriori classifier, which are well-established\nsurrogates for the excess classification error due to imperfect learning. We\nprovide lower and upper bounds on the rate-distortion function, using $L_p$\nloss as the distortion measure, of a maximum a priori classifier in terms of\nthe differential entropy of the posterior distribution and a quantity called\nthe interpolation dimension, which characterizes the complexity of the\nparametric distribution family. In addition to expressing the information\ncontent of a classifier in terms of lossy compression, the rate-distortion\nfunction also expresses the minimum number of bits a learning machine needs to\nextract from training data to learn a classifier to within a specified $L_p$\ntolerance. We use results from universal source coding to express the\ninformation content in the training data in terms of the Fisher information of\nthe parametric family and the number of training samples available. The result\nis a framework for computing lower bounds on the Bayes $L_p$ risk. This\nframework complements the well-known probably approximately correct (PAC)\nframework, which provides minimax risk bounds involving the Vapnik-Chervonenkis\ndimension or Rademacher complexity. Whereas the PAC framework provides upper\nbounds the risk for the worst-case data distribution, the proposed\nrate-distortion framework lower bounds the risk averaged over the data\ndistribution. We evaluate the bounds for a variety of data models, including\ncategorical, multinomial, and Gaussian models. In each case the bounds are\nprovably tight orderwise, and in two cases we prove that the bounds are tight\nup to multiplicative constants. \n\n"}
{"id": "1605.02633", "contents": "Title: Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace\n  Clustering Abstract: State-of-the-art subspace clustering methods are based on expressing each\ndata point as a linear combination of other data points while regularizing the\nmatrix of coefficients with $\\ell_1$, $\\ell_2$ or nuclear norms. $\\ell_1$\nregularization is guaranteed to give a subspace-preserving affinity (i.e.,\nthere are no connections between points from different subspaces) under broad\ntheoretical conditions, but the clusters may not be connected. $\\ell_2$ and\nnuclear norm regularization often improve connectivity, but give a\nsubspace-preserving affinity only for independent subspaces. Mixed $\\ell_1$,\n$\\ell_2$ and nuclear norm regularizations offer a balance between the\nsubspace-preserving and connectedness properties, but this comes at the cost of\nincreased computational complexity. This paper studies the geometry of the\nelastic net regularizer (a mixture of the $\\ell_1$ and $\\ell_2$ norms) and uses\nit to derive a provably correct and scalable active set method for finding the\noptimal coefficients. Our geometric analysis also provides a theoretical\njustification and a geometric interpretation for the balance between the\nconnectedness (due to $\\ell_2$ regularization) and subspace-preserving (due to\n$\\ell_1$ regularization) properties for elastic net subspace clustering. Our\nexperiments show that the proposed active set method not only achieves\nstate-of-the-art clustering performance, but also efficiently handles\nlarge-scale datasets. \n\n"}
{"id": "1605.03391", "contents": "Title: Unbiased split variable selection for random survival forests using\n  maximally selected rank statistics Abstract: The most popular approach for analyzing survival data is the Cox regression\nmodel. The Cox model may, however, be misspecified, and its proportionality\nassumption may not always be fulfilled. An alternative approach for survival\nprediction is random forests for survival outcomes. The standard split\ncriterion for random survival forests is the log-rank test statistics, which\nfavors splitting variables with many possible split points. Conditional\ninference forests avoid this split variable selection bias. However, linear\nrank statistics are utilized by default in conditional inference forests to\nselect the optimal splitting variable, which cannot detect non-linear effects\nin the independent variables. An alternative is to use maximally selected rank\nstatistics for the split point selection. As in conditional inference forests,\nsplitting variables are compared on the p-value scale. However, instead of the\nconditional Monte-Carlo approach used in conditional inference forests, p-value\napproximations are employed. We describe several p-value approximations and the\nimplementation of the proposed random forest approach. A simulation study\ndemonstrates that unbiased split variable selection is possible. However, there\nis a trade-off between unbiased split variable selection and runtime. In\nbenchmark studies of prediction performance on simulated and real datasets the\nnew method performs better than random survival forests if informative\ndichotomous variables are combined with uninformative variables with more\ncategories and better than conditional inference forests if non-linear\ncovariate effects are included. In a runtime comparison the method proves to be\ncomputationally faster than both alternatives, if a simple p-value\napproximation is used. \n\n"}
{"id": "1605.06451", "contents": "Title: Fixed Points of Belief Propagation -- An Analysis via Polynomial\n  Homotopy Continuation Abstract: Belief propagation (BP) is an iterative method to perform approximate\ninference on arbitrary graphical models. Whether BP converges and if the\nsolution is a unique fixed point depends on both the structure and the\nparametrization of the model. To understand this dependence it is interesting\nto find \\emph{all} fixed points. In this work, we formulate a set of polynomial\nequations, the solutions of which correspond to BP fixed points. To solve such\na nonlinear system we present the numerical polynomial-homotopy-continuation\n(NPHC) method. Experiments on binary Ising models and on error-correcting codes\nshow how our method is capable of obtaining all BP fixed points. On Ising\nmodels with fixed parameters we show how the structure influences both the\nnumber of fixed points and the convergence properties. We further asses the\naccuracy of the marginals and weighted combinations thereof. Weighting\nmarginals with their respective partition function increases the accuracy in\nall experiments. Contrary to the conjecture that uniqueness of BP fixed points\nimplies convergence, we find graphs for which BP fails to converge, even though\na unique fixed point exists. Moreover, we show that this fixed point gives a\ngood approximation, and the NPHC method is able to obtain this fixed point. \n\n"}
{"id": "1605.06619", "contents": "Title: Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic\n  Gradient Descent Abstract: Asynchronous parallel optimization algorithms for solving large-scale machine\nlearning problems have drawn significant attention from academia to industry\nrecently. This paper proposes a novel algorithm, decoupled asynchronous\nproximal stochastic gradient descent (DAP-SGD), to minimize an objective\nfunction that is the composite of the average of multiple empirical losses and\na regularization term. Unlike the traditional asynchronous proximal stochastic\ngradient descent (TAP-SGD) in which the master carries much of the computation\nload, the proposed algorithm off-loads the majority of computation tasks from\nthe master to workers, and leaves the master to conduct simple addition\noperations. This strategy yields an easy-to-parallelize algorithm, whose\nperformance is justified by theoretical convergence analyses. To be specific,\nDAP-SGD achieves an $O(\\log T/T)$ rate when the step-size is diminishing and an\nergodic $O(1/\\sqrt{T})$ rate when the step-size is constant, where $T$ is the\nnumber of total iterations. \n\n"}
{"id": "1605.07174", "contents": "Title: Kernel-based Reconstruction of Graph Signals Abstract: A number of applications in engineering, social sciences, physics, and\nbiology involve inference over networks. In this context, graph signals are\nwidely encountered as descriptors of vertex attributes or features in\ngraph-structured data. Estimating such signals in all vertices given noisy\nobservations of their values on a subset of vertices has been extensively\nanalyzed in the literature of signal processing on graphs (SPoG). This paper\nadvocates kernel regression as a framework generalizing popular SPoG modeling\nand reconstruction and expanding their capabilities. Formulating signal\nreconstruction as a regression task on reproducing kernel Hilbert spaces of\ngraph signals permeates benefits from statistical learning, offers fresh\ninsights, and allows for estimators to leverage richer forms of prior\ninformation than existing alternatives. A number of SPoG notions such as\nbandlimitedness, graph filters, and the graph Fourier transform are naturally\naccommodated in the kernel framework. Additionally, this paper capitalizes on\nthe so-called representer theorem to devise simpler versions of existing\nThikhonov regularized estimators, and offers a novel probabilistic\ninterpretation of kernel methods on graphs based on graphical models. Motivated\nby the challenges of selecting the bandwidth parameter in SPoG estimators or\nthe kernel map in kernel-based methods, the present paper further proposes two\nmulti-kernel approaches with complementary strengths. Whereas the first enables\nestimation of the unknown bandwidth of bandlimited signals, the second allows\nfor efficient graph filter selection. Numerical tests with synthetic as well as\nreal data demonstrate the merits of the proposed methods relative to\nstate-of-the-art alternatives. \n\n"}
{"id": "1605.07717", "contents": "Title: Deep Structured Energy Based Models for Anomaly Detection Abstract: In this paper, we attack the anomaly detection problem by directly modeling\nthe data distribution with deep architectures. We propose deep structured\nenergy based models (DSEBMs), where the energy function is the output of a\ndeterministic deep neural network with structure. We develop novel model\narchitectures to integrate EBMs with different types of data such as static\ndata, sequential data, and spatial data, and apply appropriate model\narchitectures to adapt to the data structure. Our training algorithm is built\nupon the recent development of score matching \\cite{sm}, which connects an EBM\nwith a regularized autoencoder, eliminating the need for complicated sampling\nmethod. Statistically sound decision criterion can be derived for anomaly\ndetection purpose from the perspective of the energy landscape of the data\ndistribution. We investigate two decision criteria for performing anomaly\ndetection: the energy score and the reconstruction error. Extensive empirical\nstudies on benchmark tasks demonstrate that our proposed model consistently\nmatches or outperforms all the competing methods. \n\n"}
{"id": "1605.08618", "contents": "Title: Variational Bayesian Inference for Hidden Markov Models With\n  Multivariate Gaussian Output Distributions Abstract: Hidden Markov Models (HMM) have been used for several years in many time\nseries analysis or pattern recognitions tasks. HMM are often trained by means\nof the Baum-Welch algorithm which can be seen as a special variant of an\nexpectation maximization (EM) algorithm. Second-order training techniques such\nas Variational Bayesian Inference (VI) for probabilistic models regard the\nparameters of the probabilistic models as random variables and define\ndistributions over these distribution parameters, hence the name of this\ntechnique. VI can also bee regarded as a special case of an EM algorithm. In\nthis article, we bring both together and train HMM with multivariate Gaussian\noutput distributions with VI. The article defines the new training technique\nfor HMM. An evaluation based on some case studies and a comparison to related\napproaches is part of our ongoing work. \n\n"}
{"id": "1605.09477", "contents": "Title: A Neural Autoregressive Approach to Collaborative Filtering Abstract: This paper proposes CF-NADE, a neural autoregressive architecture for\ncollaborative filtering (CF) tasks, which is inspired by the Restricted\nBoltzmann Machine (RBM) based CF model and the Neural Autoregressive\nDistribution Estimator (NADE). We first describe the basic CF-NADE model for CF\ntasks. Then we propose to improve the model by sharing parameters between\ndifferent ratings. A factored version of CF-NADE is also proposed for better\nscalability. Furthermore, we take the ordinal nature of the preferences into\nconsideration and propose an ordinal cost to optimize CF-NADE, which shows\nsuperior performance. Finally, CF-NADE can be extended to a deep model, with\nonly moderately increased computational complexity. Experimental results show\nthat CF-NADE with a single hidden layer beats all previous state-of-the-art\nmethods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more\nhidden layers can further improve the performance. \n\n"}
{"id": "1606.00318", "contents": "Title: Discovering Phase Transitions with Unsupervised Learning Abstract: Unsupervised learning is a discipline of machine learning which aims at\ndiscovering patterns in big data sets or classifying the data into several\ncategories without being trained explicitly. We show that unsupervised learning\ntechniques can be readily used to identify phases and phases transitions of\nmany body systems. Starting with raw spin configurations of a prototypical\nIsing model, we use principal component analysis to extract relevant low\ndimensional representations the original data and use clustering analysis to\nidentify distinct phases in the feature space. This approach successfully finds\nout physical concepts such as order parameter and structure factor to be\nindicators of the phase transition. We discuss future prospects of discovering\nmore complex phases and phase transitions using unsupervised learning\ntechniques. \n\n"}
{"id": "1606.00451", "contents": "Title: Graph-Guided Banding of the Covariance Matrix Abstract: Regularization has become a primary tool for developing reliable estimators\nof the covariance matrix in high-dimensional settings. To curb the curse of\ndimensionality, numerous methods assume that the population covariance (or\ninverse covariance) matrix is sparse, while making no particular structural\nassumptions on the desired pattern of sparsity. A highly-related, yet\ncomplementary, literature studies the specific setting in which the measured\nvariables have a known ordering, in which case a banded population matrix is\noften assumed. While the banded approach is conceptually and computationally\neasier than asking for \"patternless sparsity,\" it is only applicable in very\nspecific situations (such as when data are measured over time or\none-dimensional space). This work proposes a generalization of the notion of\nbandedness that greatly expands the range of problems in which banded\nestimators apply.\n  We develop convex regularizers occupying the broad middle ground between the\nformer approach of \"patternless sparsity\" and the latter reliance on having a\nknown ordering. Our framework defines bandedness with respect to a known graph\non the measured variables. Such a graph is available in diverse situations, and\nwe provide a theoretical, computational, and applied treatment of two new\nestimators. An R package, called ggb, implements these new methods. \n\n"}
{"id": "1606.00603", "contents": "Title: Quantum limit for two-dimensional resolution of two incoherent optical\n  point sources Abstract: We obtain the multiple-parameter quantum Cram\\'er-Rao bound for estimating\nthe transverse Cartesian components of the centroid and separation of two\nincoherent optical point sources using an imaging system with finite spatial\nbandwidth. Under quite general and realistic assumptions on the point-spread\nfunction of the imaging system, and for weak source strengths, we show that the\nCram\\'er-Rao bounds for the $x$ and $y$ components of the separation are\nindependent of the values of those components, which may be well below the\nconventional Rayleigh resolution limit. We also propose two linear optics-based\nmeasurement methods that approach the quantum bound for the estimation of the\nCartesian components of the separation once the centroid has been located. One\nof the methods is an interferometric scheme that approaches the quantum bound\nfor sub-Rayleigh separations. The other method using fiber coupling can in\nprinciple attain the bound regardless of the distance between the two sources. \n\n"}
{"id": "1606.01245", "contents": "Title: Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization Abstract: The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard\nnuclear norm in order to approximate the rank function more accurately.\nHowever, existing Schatten-p quasi-norm minimization algorithms involve\nsingular value decomposition (SVD) or eigenvalue decomposition (EVD) in each\niteration, and thus may become very slow and impractical for large-scale\nproblems. In this paper, we first define two tractable Schatten quasi-norms,\ni.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove\nthat they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively,\nwhich lead to the design of very efficient algorithms that only need to update\ntwo much smaller factor matrices. We also design two efficient proximal\nalternating linearized minimization algorithms for solving representative\nmatrix completion problems. Finally, we provide the global convergence and\nperformance guarantees for our algorithms, which have better convergence\nproperties than existing algorithms. Experimental results on synthetic and\nreal-world data show that our algorithms are more accurate than the\nstate-of-the-art methods, and are orders of magnitude faster. \n\n"}
{"id": "1606.02275", "contents": "Title: Measuring the reliability of MCMC inference with bidirectional Monte\n  Carlo Abstract: Markov chain Monte Carlo (MCMC) is one of the main workhorses of\nprobabilistic inference, but it is notoriously hard to measure the quality of\napproximate posterior samples. This challenge is particularly salient in black\nbox inference methods, which can hide details and obscure inference failures.\nIn this work, we extend the recently introduced bidirectional Monte Carlo\ntechnique to evaluate MCMC-based posterior inference algorithms. By running\nannealed importance sampling (AIS) chains both from prior to posterior and vice\nversa on simulated data, we upper bound in expectation the symmetrized KL\ndivergence between the true posterior distribution and the distribution of\napproximate samples. We present Bounding Divergences with REverse Annealing\n(BREAD), a protocol for validating the relevance of simulated data experiments\nto real datasets, and integrate it into two probabilistic programming\nlanguages: WebPPL and Stan. As an example of how BREAD can be used to guide the\ndesign of inference algorithms, we apply it to study the effectiveness of\ndifferent model representations in both WebPPL and Stan. \n\n"}
{"id": "1606.02401", "contents": "Title: On clustering network-valued data Abstract: Community detection, which focuses on clustering nodes or detecting\ncommunities in (mostly) a single network, is a problem of considerable\npractical interest and has received a great deal of attention in the research\ncommunity. While being able to cluster within a network is important, there are\nemerging needs to be able to cluster multiple networks. This is largely\nmotivated by the routine collection of network data that are generated from\npotentially different populations. These networks may or may not have node\ncorrespondence. When node correspondence is present, we cluster networks by\nsummarizing a network by its graphon estimate, whereas when node correspondence\nis not present, we propose a novel solution for clustering such networks by\nassociating a computationally feasible feature vector to each network based on\ntrace of powers of the adjacency matrix. We illustrate our methods using both\nsimulated and real data sets, and theoretical justifications are provided in\nterms of consistency. \n\n"}
{"id": "1606.03631", "contents": "Title: A Stern-Gerlach-like approach to electron orbital angular momentum\n  measurement Abstract: Many methods now exist to prepare free electrons into orbital angular\nmomentum states, and the predicted applications of these electron states as\nprobes of materials and scattering processes are numerous. The development of\nelectron orbital angular momentum measurement techniques has lagged behind. We\nshow that coupling between electron orbital angular momentum and a spatially\nvarying magnetic field produces an angular momentum-dependent focusing effect.\nWe propose a design for an orbital angular momentum measurement device built on\nthis principle. As the method of measurement is non-interferometric, the device\nworks equally well for mixed, superposed and pure final orbital angular\nmomentum states. The energy and orbital angular momentum distributions of\ninelastically scattered electrons may be simultaneously measurable with this\ntechnique. \n\n"}
{"id": "1606.04523", "contents": "Title: Quantum-coherent mixtures of causal relations Abstract: Understanding the causal influences that hold among parts of a system is\ncritical both to explaining that system's natural behaviour and to controlling\nit through targeted interventions. In a quantum world, understanding causal\nrelations is equally important, but the set of possibilities is far richer. The\ntwo basic ways in which a pair of time-ordered quantum systems may be causally\nrelated are by a cause-effect mechanism or by a common cause acting on both.\nHere, we show a coherent mixture of these two possibilities. We realize this\nnonclassical causal relation in a quantum optics experiment and derive a set of\ncriteria for witnessing the coherence based on a quantum version of Berkson's\neffect, whereby two independent causes can become correlated upon observation\nof their common effect. The interplay of causality and quantum theory lies at\nthe heart of challenging foundational puzzles, including Bell's theorem and the\nsearch for quantum gravity. \n\n"}
{"id": "1606.04671", "contents": "Title: Progressive Neural Networks Abstract: Learning to solve complex sequences of tasks--while both leveraging transfer\nand avoiding catastrophic forgetting--remains a key obstacle to achieving\nhuman-level intelligence. The progressive networks approach represents a step\nforward in this direction: they are immune to forgetting and can leverage prior\nknowledge via lateral connections to previously learned features. We evaluate\nthis architecture extensively on a wide variety of reinforcement learning tasks\n(Atari and 3D maze games), and show that it outperforms common baselines based\non pretraining and finetuning. Using a novel sensitivity measure, we\ndemonstrate that transfer occurs at both low-level sensory and high-level\ncontrol layers of the learned policy. \n\n"}
{"id": "1606.05158", "contents": "Title: CLEAR: Covariant LEAst-square Re-fitting with applications to image\n  restoration Abstract: In this paper, we propose a new framework to remove parts of the systematic\nerrors affecting popular restoration algorithms, with a special focus for image\nprocessing tasks. Generalizing ideas that emerged for $\\ell_1$ regularization,\nwe develop an approach re-fitting the results of standard methods towards the\ninput data. Total variation regularizations and non-local means are special\ncases of interest. We identify important covariant information that should be\npreserved by the re-fitting method, and emphasize the importance of preserving\nthe Jacobian (w.r.t. the observed signal) of the original estimator. Then, we\nprovide an approach that has a \"twicing\" flavor and allows re-fitting the\nrestored signal by adding back a local affine transformation of the residual\nterm. We illustrate the benefits of our method on numerical simulations for\nimage restoration tasks. \n\n"}
{"id": "1606.05798", "contents": "Title: Interpretable Two-level Boolean Rule Learning for Classification Abstract: As a contribution to interpretable machine learning research, we develop a\nnovel optimization framework for learning accurate and sparse two-level Boolean\nrules. We consider rules in both conjunctive normal form (AND-of-ORs) and\ndisjunctive normal form (OR-of-ANDs). A principled objective function is\nproposed to trade classification accuracy and interpretability, where we use\nHamming loss to characterize accuracy and sparsity to characterize\ninterpretability. We propose efficient procedures to optimize these objectives\nbased on linear programming (LP) relaxation, block coordinate descent, and\nalternating minimization. Experiments show that our new algorithms provide very\ngood tradeoffs between accuracy and interpretability. \n\n"}
{"id": "1607.00669", "contents": "Title: Understanding the Energy and Precision Requirements for Online Learning Abstract: It is well-known that the precision of data, hyperparameters, and internal\nrepresentations employed in learning systems directly impacts its energy,\nthroughput, and latency. The precision requirements for the training algorithm\nare also important for systems that learn on-the-fly. Prior work has shown that\nthe data and hyperparameters can be quantized heavily without incurring much\npenalty in classification accuracy when compared to floating point\nimplementations. These works suffer from two key limitations. First, they\nassume uniform precision for the classifier and for the training algorithm and\nthus miss out on the opportunity to further reduce precision. Second, prior\nworks are empirical studies. In this article, we overcome both these\nlimitations by deriving analytical lower bounds on the precision requirements\nof the commonly employed stochastic gradient descent (SGD) on-line learning\nalgorithm in the specific context of a support vector machine (SVM). Lower\nbounds on the data precision are derived in terms of the the desired\nclassification accuracy and precision of the hyperparameters used in the\nclassifier. Additionally, lower bounds on the hyperparameter precision in the\nSGD training algorithm are obtained. These bounds are validated using both\nsynthetic and the UCI breast cancer dataset. Additionally, the impact of these\nprecisions on the energy consumption of a fixed-point SVM with on-line training\nis studied. \n\n"}
{"id": "1607.07270", "contents": "Title: A Statistical Test for Joint Distributions Equivalence Abstract: We provide a distribution-free test that can be used to determine whether any\ntwo joint distributions $p$ and $q$ are statistically different by inspection\nof a large enough set of samples. Following recent efforts from Long et al.\n[1], we rely on joint kernel distribution embedding to extend the kernel\ntwo-sample test of Gretton et al. [2] to the case of joint probability\ndistributions. Our main result can be directly applied to verify if a\ndataset-shift has occurred between training and test distributions in a\nlearning framework, without further assuming the shift has occurred only in the\ninput, in the target or in the conditional distribution. \n\n"}
{"id": "1608.00446", "contents": "Title: Chiral Quantum Optics Abstract: At the most fundamental level, the interaction between light and matter is\nmanifested by the emission and absorption of single photons by single quantum\nemitters. Controlling light--matter interaction is the basis for diverse\napplications ranging from light technology to quantum--information processing.\nMany of these applications are nowadays based on photonic nanostructures\nstrongly benefitting from their scalability and integrability. The confinement\nof light in such nanostructures imposes an inherent link between the local\npolarization and propagation direction of light. This leads to {\\em chiral\nlight--matter interaction}, i.e., the emission and absorption of photons depend\non the propagation direction and local polarization of light as well as the\npolarization of the emitter transition. The burgeoning research field of {\\em\nchiral quantum optics} offers fundamentally new functionalities and\napplications both for single emitters and ensembles thereof. For instance, a\nchiral light--matter interface enables the realization of integrated\nnon--reciprocal single--photon devices and deterministic spin--photon\ninterfaces. Moreover, engineering directional photonic reservoirs opens new\navenues for constructing complex quantum circuits and networks, which may be\napplied to simulate a new class of quantum many--body systems. \n\n"}
{"id": "1608.00686", "contents": "Title: Clinical Tagging with Joint Probabilistic Models Abstract: We describe a method for parameter estimation in bipartite probabilistic\ngraphical models for joint prediction of clinical conditions from the\nelectronic medical record. The method does not rely on the availability of\ngold-standard labels, but rather uses noisy labels, called anchors, for\nlearning. We provide a likelihood-based objective and a moments-based\ninitialization that are effective at learning the model parameters. The learned\nmodel is evaluated in a task of assigning a heldout clinical condition to\npatients based on retrospective analysis of the records, and outperforms\nbaselines which do not account for the noisiness in the labels or do not model\nthe conditions jointly. \n\n"}
{"id": "1608.01976", "contents": "Title: Kernel Ridge Regression via Partitioning Abstract: In this paper, we investigate a divide and conquer approach to Kernel Ridge\nRegression (KRR). Given n samples, the division step involves separating the\npoints based on some underlying disjoint partition of the input space (possibly\nvia clustering), and then computing a KRR estimate for each partition. The\nconquering step is simple: for each partition, we only consider its own local\nestimate for prediction. We establish conditions under which we can give\ngeneralization bounds for this estimator, as well as achieve optimal minimax\nrates. We also show that the approximation error component of the\ngeneralization error is lesser than when a single KRR estimate is fit on the\ndata: thus providing both statistical and computational advantages over a\nsingle KRR estimate over the entire data (or an averaging over random\npartitions as in other recent work, [30]). Lastly, we provide experimental\nvalidation for our proposed estimator and our assumptions. \n\n"}
{"id": "1608.02554", "contents": "Title: Sparse recovery via Orthogonal Least-Squares under presence of Noise Abstract: We consider the Orthogonal Least-Squares (OLS) algorithm for the recovery of\na $m$-dimensional $k$-sparse signal from a low number of noisy linear\nmeasurements. The Exact Recovery Condition (ERC) in bounded noisy scenario is\nestablished for OLS under certain condition on nonzero elements of the signal.\nThe new result also improves the existing guarantees for Orthogonal Matching\nPursuit (OMP) algorithm. In addition, This framework is employed to provide\nprobabilistic guarantees for the case that the coefficient matrix is drawn at\nrandom according to Gaussian or Bernoulli distribution where we exploit some\nconcentration properties. It is shown that under certain conditions, OLS\nrecovers the true support in $k$ iterations with high probability. This in turn\ndemonstrates that ${\\cal O}\\left(k\\log m\\right)$ measurements is sufficient for\nexact recovery of sparse signals via OLS. \n\n"}
{"id": "1608.03333", "contents": "Title: Temporal Learning and Sequence Modeling for a Job Recommender System Abstract: We present our solution to the job recommendation task for RecSys Challenge\n2016. The main contribution of our work is to combine temporal learning with\nsequence modeling to capture complex user-item activity patterns to improve job\nrecommendations. First, we propose a time-based ranking model applied to\nhistorical observations and a hybrid matrix factorization over time re-weighted\ninteractions. Second, we exploit sequence properties in user-items activities\nand develop a RNN-based recommendation model. Our solution achieved 5$^{th}$\nplace in the challenge among more than 100 participants. Notably, the strong\nperformance of our RNN approach shows a promising new direction in employing\nsequence modeling for recommendation systems. \n\n"}
{"id": "1608.04245", "contents": "Title: The Bayesian Low-Rank Determinantal Point Process Mixture Model Abstract: Determinantal point processes (DPPs) are an elegant model for encoding\nprobabilities over subsets, such as shopping baskets, of a ground set, such as\nan item catalog. They are useful for a number of machine learning tasks,\nincluding product recommendation. DPPs are parametrized by a positive\nsemi-definite kernel matrix. Recent work has shown that using a low-rank\nfactorization of this kernel provides remarkable scalability improvements that\nopen the door to training on large-scale datasets and computing online\nrecommendations, both of which are infeasible with standard DPP models that use\na full-rank kernel. In this paper we present a low-rank DPP mixture model that\nallows us to represent the latent structure present in observed subsets as a\nmixture of a number of component low-rank DPPs, where each component DPP is\nresponsible for representing a portion of the observed data. The mixture model\nallows us to effectively address the capacity constraints of the low-rank DPP\nmodel. We present an efficient and scalable Markov Chain Monte Carlo (MCMC)\nlearning algorithm for our model that uses Gibbs sampling and stochastic\ngradient Hamiltonian Monte Carlo (SGHMC). Using an evaluation on several\nreal-world product recommendation datasets, we show that our low-rank DPP\nmixture model provides substantially better predictive performance than is\npossible with a single low-rank or full-rank DPP, and significantly better\nperformance than several other competing recommendation methods in many cases. \n\n"}
{"id": "1608.04846", "contents": "Title: A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation Abstract: Finding the most effective way to aggregate multi-subject fMRI data is a\nlong-standing and challenging problem. It is of increasing interest in\ncontemporary fMRI studies of human cognition due to the scarcity of data per\nsubject and the variability of brain anatomy and functional response across\nsubjects. Recent work on latent factor models shows promising results in this\ntask but this approach does not preserve spatial locality in the brain. We\nexamine two ways to combine the ideas of a factor model and a searchlight based\nanalysis to aggregate multi-subject fMRI data while preserving spatial\nlocality. We first do this directly by combining a recent factor method known\nas a shared response model with searchlight analysis. Then we design a\nmulti-view convolutional autoencoder for the same task. Both approaches\npreserve spatial locality and have competitive or better performance compared\nwith standard searchlight analysis and the shared response model applied across\nthe whole brain. We also report a system design to handle the computational\nchallenge of training the convolutional autoencoder. \n\n"}
{"id": "1608.05081", "contents": "Title: BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for\n  Task-Oriented Dialogue Systems Abstract: We present a new algorithm that significantly improves the efficiency of\nexploration for deep Q-learning agents in dialogue systems. Our agents explore\nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop\nneural network. Our algorithm learns much faster than common exploration\nstrategies such as $\\epsilon$-greedy, Boltzmann, bootstrapping, and\nintrinsic-reward-based ones. Additionally, we show that spiking the replay\nbuffer with experiences from just a few successful episodes can make Q-learning\nfeasible when it might otherwise fail. \n\n"}
{"id": "1608.05889", "contents": "Title: Online Feature Selection with Group Structure Analysis Abstract: Online selection of dynamic features has attracted intensive interest in\nrecent years. However, existing online feature selection methods evaluate\nfeatures individually and ignore the underlying structure of feature stream.\nFor instance, in image analysis, features are generated in groups which\nrepresent color, texture and other visual information. Simply breaking the\ngroup structure in feature selection may degrade performance. Motivated by this\nfact, we formulate the problem as an online group feature selection. The\nproblem assumes that features are generated individually but there are group\nstructure in the feature stream. To the best of our knowledge, this is the\nfirst time that the correlation among feature stream has been considered in the\nonline feature selection process. To solve this problem, we develop a novel\nonline group feature selection method named OGFS. Our proposed approach\nconsists of two stages: online intra-group selection and online inter-group\nselection. In the intra-group selection, we design a criterion based on\nspectral analysis to select discriminative features in each group. In the\ninter-group selection, we utilize a linear regression model to select an\noptimal subset. This two-stage procedure continues until there are no more\nfeatures arriving or some predefined stopping conditions are met. %Our method\nhas been applied Finally, we apply our method to multiple tasks including image\nclassification %, face verification and face verification. Extensive empirical\nstudies performed on real-world and benchmark data sets demonstrate that our\nmethod outperforms other state-of-the-art online feature selection %method\nmethods. \n\n"}
{"id": "1608.06879", "contents": "Title: AIDE: Fast and Communication Efficient Distributed Optimization Abstract: In this paper, we present two new communication-efficient methods for\ndistributed minimization of an average of functions. The first algorithm is an\ninexact variant of the DANE algorithm that allows any local algorithm to return\nan approximate solution to a local subproblem. We show that such a strategy\ndoes not affect the theoretical guarantees of DANE significantly. In fact, our\napproach can be viewed as a robustification strategy since the method is\nsubstantially better behaved than DANE on data partition arising in practice.\nIt is well known that DANE algorithm does not match the communication\ncomplexity lower bounds. To bridge this gap, we propose an accelerated variant\nof the first method, called AIDE, that not only matches the communication lower\nbounds but can also be implemented using a purely first-order oracle. Our\nempirical results show that AIDE is superior to other communication efficient\nalgorithms in settings that naturally arise in machine learning applications. \n\n"}
{"id": "1608.07636", "contents": "Title: Learning Temporal Dependence from Time-Series Data with Latent Variables Abstract: We consider the setting where a collection of time series, modeled as random\nprocesses, evolve in a causal manner, and one is interested in learning the\ngraph governing the relationships of these processes. A special case of wide\ninterest and applicability is the setting where the noise is Gaussian and\nrelationships are Markov and linear. We study this setting with two additional\nfeatures: firstly, each random process has a hidden (latent) state, which we\nuse to model the internal memory possessed by the variables (similar to hidden\nMarkov models). Secondly, each variable can depend on its latent memory state\nthrough a random lag (rather than a fixed lag), thus modeling memory recall\nwith differing lags at distinct times. Under this setting, we develop an\nestimator and prove that under a genericity assumption, the parameters of the\nmodel can be learned consistently. We also propose a practical adaption of this\nestimator, which demonstrates significant performance gains in both synthetic\nand real-world datasets. \n\n"}
{"id": "1609.01596", "contents": "Title: Direct Feedback Alignment Provides Learning in Deep Neural Networks Abstract: Artificial neural networks are most commonly trained with the\nback-propagation algorithm, where the gradient for learning is provided by\nback-propagating the error, layer by layer, from the output layer to the hidden\nlayers. A recently discovered method called feedback-alignment shows that the\nweights used for propagating the error backward don't have to be symmetric with\nthe weights used for propagation the activation forward. In fact, random\nfeedback weights work evenly well, because the network learns how to make the\nfeedback useful. In this work, the feedback alignment principle is used for\ntraining hidden layers more independently from the rest of the network, and\nfrom a zero initial condition. The error is propagated through fixed random\nfeedback connections directly from the output layer to each hidden layer. This\nsimple method is able to achieve zero training error even in convolutional\nnetworks and very deep networks, completely without error back-propagation. The\nmethod is a step towards biologically plausible machine learning because the\nerror signal is almost local, and no symmetric or reciprocal weights are\nrequired. Experiments show that the test performance on MNIST and CIFAR is\nalmost as good as those obtained with back-propagation for fully connected\nnetworks. If combined with dropout, the method achieves 1.45% error on the\npermutation invariant MNIST task. \n\n"}
{"id": "1609.02845", "contents": "Title: Distributed Online Optimization in Dynamic Environments Using Mirror\n  Descent Abstract: This work addresses decentralized online optimization in non-stationary\nenvironments. A network of agents aim to track the minimizer of a global\ntime-varying convex function. The minimizer evolves according to a known\ndynamics corrupted by an unknown, unstructured noise. At each time, the global\nfunction can be cast as a sum of a finite number of local functions, each of\nwhich is assigned to one agent in the network. Moreover, the local functions\nbecome available to agents sequentially, and agents do not have a prior\nknowledge of the future cost functions. Therefore, agents must communicate with\neach other to build an online approximation of the global function. We propose\na decentralized variation of the celebrated Mirror Descent, developed by\nNemirovksi and Yudin. Using the notion of Bregman divergence in lieu of\nEuclidean distance for projection, Mirror Descent has been shown to be a\npowerful tool in large-scale optimization. Our algorithm builds on Mirror\nDescent, while ensuring that agents perform a consensus step to follow the\nglobal function and take into account the dynamics of the global minimizer. To\nmeasure the performance of the proposed online algorithm, we compare it to its\noffline counterpart, where the global functions are available a priori. The gap\nbetween the two is called dynamic regret. We establish a regret bound that\nscales inversely in the spectral gap of the network, and more notably it\nrepresents the deviation of minimizer sequence with respect to the given\ndynamics. We then show that our results subsume a number of results in\ndistributed optimization. We demonstrate the application of our method to\ndecentralized tracking of dynamic parameters and verify the results via\nnumerical experiments. \n\n"}
{"id": "1609.03416", "contents": "Title: Characterization of two distant double-slits by chaotic light\n  second-order interference Abstract: We present the experimental characterization of two distant double-slit masks\nilluminated by chaotic light, in the absence of first-order imaging and\ninterference. The scheme exploits second-order interference of light\npropagating through two indistinguishable pairs of {\\it disjoint} optical paths\npassing through the masks of interest. The proposed technique leads to a deeper\nunderstanding of biphoton interference and coherence, and opens the way to the\ndevelopment of novel schemes for retrieving information on the relative\nposition and the spatial structure of distant objects, which is of interest in\nremote sensing, biomedical imaging, as well as monitoring of laser ablation,\nwhen first-order imaging and interference are not feasible. \n\n"}
{"id": "1609.05388", "contents": "Title: ADAGIO: Fast Data-aware Near-Isometric Linear Embeddings Abstract: Many important applications, including signal reconstruction, parameter\nestimation, and signal processing in a compressed domain, rely on a\nlow-dimensional representation of the dataset that preserves {\\em all} pairwise\ndistances between the data points and leverages the inherent geometric\nstructure that is typically present. Recently Hedge, Sankaranarayanan, Yin and\nBaraniuk \\cite{hedge2015} proposed the first data-aware near-isometric linear\nembedding which achieves the best of both worlds. However, their method NuMax\ndoes not scale to large-scale datasets.\n  Our main contribution is a simple, data-aware, near-isometric linear\ndimensionality reduction method which significantly outperforms a\nstate-of-the-art method \\cite{hedge2015} with respect to scalability while\nachieving high quality near-isometries. Furthermore, our method comes with\nstrong worst-case theoretical guarantees that allow us to guarantee the quality\nof the obtained near-isometry. We verify experimentally the efficiency of our\nmethod on numerous real-world datasets, where we find that our method ($<$10\nsecs) is more than 3\\,000$\\times$ faster than the state-of-the-art method\n\\cite{hedge2015} ($>$9 hours) on medium scale datasets with 60\\,000 data points\nin 784 dimensions. Finally, we use our method as a preprocessing step to\nincrease the computational efficiency of a classification application and for\nspeeding up approximate nearest neighbor queries. \n\n"}
{"id": "1609.06826", "contents": "Title: Bibliographic Analysis with the Citation Network Topic Model Abstract: Bibliographic analysis considers author's research areas, the citation\nnetwork and paper content among other things. In this paper, we combine these\nthree in a topic model that produces a bibliographic model of authors, topics\nand documents using a non-parametric extension of a combination of the Poisson\nmixed-topic link model and the author-topic model. We propose a novel and\nefficient inference algorithm for the model to explore subsets of research\npublications from CiteSeerX. Our model demonstrates improved performance in\nboth model fitting and a clustering task compared to several baselines. \n\n"}
{"id": "1609.07087", "contents": "Title: (Bandit) Convex Optimization with Biased Noisy Gradient Oracles Abstract: Algorithms for bandit convex optimization and online learning often rely on\nconstructing noisy gradient estimates, which are then used in appropriately\nadjusted first-order algorithms, replacing actual gradients. Depending on the\nproperties of the function to be optimized and the nature of ``noise'' in the\nbandit feedback, the bias and variance of gradient estimates exhibit various\ntradeoffs. In this paper we propose a novel framework that replaces the\nspecific gradient estimation methods with an abstract oracle. With the help of\nthe new framework we unify previous works, reproducing their results in a clean\nand concise fashion, while, perhaps more importantly, the framework also allows\nus to formally show that to achieve the optimal root-$n$ rate either the\nalgorithms that use existing gradient estimators, or the proof techniques used\nto analyze them have to go beyond what exists today. \n\n"}
{"id": "1609.07405", "contents": "Title: Spatial localization and pattern formation in discrete optomechanical\n  cavities and arrays Abstract: We investigate theoretically the generation of nonlinear dissipative\nstructures in optomechanical (OM) systems containing discrete arrays of\nmechanical resonators. We consider both hybrid models in which the optical\nsystem is a continuous multimode field, as it would happen in an OM cavity\ncontaining an array of micro-mirrors, and also fully discrete models in which\neach mechanical resonator interacts with a single optical mode, making contact\nwith Ludwig & Marquardt [Phys. Rev. Lett. 101, 073603 (2013)]. Also, we study\nthe connections between both types of models and continuous OM models. While\nall three types of models merge naturally in the limit of a large number of\ndensely distributed mechanical resonators, we show that the spatial\nlocalization and the pattern formation found in continuous OM models can be\nstill observed for a small number of mechanical elements, even in the presence\nof finite-size effects, which we discuss. This opens new venues for\nexperimental approaches to the subject. \n\n"}
{"id": "1609.07574", "contents": "Title: Dynamic Pricing in High-dimensions Abstract: We study the pricing problem faced by a firm that sells a large number of\nproducts, described via a wide range of features, to customers that arrive over\ntime. Customers independently make purchasing decisions according to a general\nchoice model that includes products features and customers' characteristics,\nencoded as $d$-dimensional numerical vectors, as well as the price offered. The\nparameters of the choice model are a priori unknown to the firm, but can be\nlearned as the (binary-valued) sales data accrues over time. The firm's\nobjective is to minimize the regret, i.e., the expected revenue loss against a\nclairvoyant policy that knows the parameters of the choice model in advance,\nand always offers the revenue-maximizing price. This setting is motivated in\npart by the prevalence of online marketplaces that allow for real-time pricing.\nWe assume a structured choice model, parameters of which depend on $s_0$ out of\nthe $d$ product features. We propose a dynamic policy, called Regularized\nMaximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of\nthe high-dimensional model and obtains a logarithmic regret in $T$. More\nspecifically, the regret of our algorithm is of $O(s_0 \\log d \\cdot \\log T)$.\nFurthermore, we show that no policy can obtain regret better than $O(s_0 (\\log\nd + \\log T))$. \n\n"}
{"id": "1609.08203", "contents": "Title: Variational Inference with Hamiltonian Monte Carlo Abstract: Variational inference lies at the core of many state-of-the-art algorithms.\nTo improve the approximation of the posterior beyond parametric families, it\nwas proposed to include MCMC steps into the variational lower bound. In this\nwork we explore this idea using steps of the Hamiltonian Monte Carlo (HMC)\nalgorithm, an efficient MCMC method. In particular, we incorporate the\nacceptance step of the HMC algorithm, guaranteeing asymptotic convergence to\nthe true posterior. Additionally, we introduce some extensions to the HMC\nalgorithm geared towards faster convergence. The theoretical advantages of\nthese modifications are reflected by performance improvements in our\nexperimental results. \n\n"}
{"id": "1610.02143", "contents": "Title: Stochastic Averaging for Constrained Optimization with Application to\n  Online Resource Allocation Abstract: Existing approaches to resource allocation for nowadays stochastic networks\nare challenged to meet fast convergence and tolerable delay requirements. The\npresent paper leverages online learning advances to facilitate stochastic\nresource allocation tasks. By recognizing the central role of Lagrange\nmultipliers, the underlying constrained optimization problem is formulated as a\nmachine learning task involving both training and operational modes, with the\ngoal of learning the sought multipliers in a fast and efficient manner. To this\nend, an order-optimal offline learning approach is developed first for batch\ntraining, and it is then generalized to the online setting with a procedure\ntermed learn-and-adapt. The novel resource allocation protocol permeates\nbenefits of stochastic approximation and statistical learning to obtain\nlow-complexity online updates with learning errors close to the statistical\naccuracy limits, while still preserving adaptation performance, which in the\nstochastic network optimization context guarantees queue stability. Analysis\nand simulated tests demonstrate that the proposed data-driven approach improves\nthe delay and convergence performance of existing resource allocation schemes. \n\n"}
{"id": "1610.02920", "contents": "Title: Generative Adversarial Nets from a Density Ratio Estimation Perspective Abstract: Generative adversarial networks (GANs) are successful deep generative models.\nGANs are based on a two-player minimax game. However, the objective function\nderived in the original motivation is changed to obtain stronger gradients when\nlearning the generator. We propose a novel algorithm that repeats the density\nratio estimation and f-divergence minimization. Our algorithm offers a new\nperspective toward the understanding of GANs and is able to make use of\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\nwhat divergence is stable and relative density ratio is useful. \n\n"}
{"id": "1610.04210", "contents": "Title: Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex\n  Relaxation Abstract: We propose a flexible convex relaxation for the phase retrieval problem that\noperates in the natural domain of the signal. Therefore, we avoid the\nprohibitive computational cost associated with \"lifting\" and semidefinite\nprogramming (SDP) in methods such as PhaseLift and compete with recently\ndeveloped non-convex techniques for phase retrieval. We relax the quadratic\nequations for phaseless measurements to inequality constraints each of which\nrepresenting a symmetric \"slab\". Through a simple convex program, our proposed\nestimator finds an extreme point of the intersection of these slabs that is\nbest aligned with a given anchor vector. We characterize geometric conditions\nthat certify success of the proposed estimator. Furthermore, using classic\nresults in statistical learning theory, we show that for random measurements\nthe geometric certificates hold with high probability at an optimal sample\ncomplexity. Phase transition of our estimator is evaluated through simulations.\nOur numerical experiments also suggest that the proposed method can solve phase\nretrieval problems with coded diffraction measurements as well. \n\n"}
{"id": "1610.04574", "contents": "Title: Generalization Error of Invariant Classifiers Abstract: This paper studies the generalization error of invariant classifiers. In\nparticular, we consider the common scenario where the classification task is\ninvariant to certain transformations of the input, and that the classifier is\nconstructed (or learned) to be invariant to these transformations. Our approach\nrelies on factoring the input space into a product of a base space and a set of\ntransformations. We show that whereas the generalization error of a\nnon-invariant classifier is proportional to the complexity of the input space,\nthe generalization error of an invariant classifier is proportional to the\ncomplexity of the base space. We also derive a set of sufficient conditions on\nthe geometry of the base space and the set of transformations that ensure that\nthe complexity of the base space is much smaller than the complexity of the\ninput space. Our analysis applies to general classifiers such as convolutional\nneural networks. We demonstrate the implications of the developed theory for\nsuch classifiers with experiments on the MNIST and CIFAR-10 datasets. \n\n"}
{"id": "1610.04782", "contents": "Title: An Adaptive Test of Independence with Analytic Kernel Embeddings Abstract: A new computationally efficient dependence measure, and an adaptive\nstatistical test of independence, are proposed. The dependence measure is the\ndifference between analytic embeddings of the joint distribution and the\nproduct of the marginals, evaluated at a finite set of locations (features).\nThese features are chosen so as to maximize a lower bound on the test power,\nresulting in a test that is data-efficient, and that runs in linear time (with\nrespect to the sample size n). The optimized features can be interpreted as\nevidence to reject the null hypothesis, indicating regions in the joint domain\nwhere the joint distribution and the product of the marginals differ most.\nConsistency of the independence test is established, for an appropriate choice\nof features. In real-world benchmarks, independence tests using the optimized\nfeatures perform comparably to the state-of-the-art quadratic-time HSIC test,\nand outperform competing O(n) and O(n log n) tests. \n\n"}
{"id": "1610.04804", "contents": "Title: Dynamic Stacked Generalization for Node Classification on Networks Abstract: We propose a novel stacked generalization (stacking) method as a dynamic\nensemble technique using a pool of heterogeneous classifiers for node label\nclassification on networks. The proposed method assigns component models a set\nof functional coefficients, which can vary smoothly with certain topological\nfeatures of a node. Compared to the traditional stacking model, the proposed\nmethod can dynamically adjust the weights of individual models as we move\nacross the graph and provide a more versatile and significantly more accurate\nstacking model for label prediction on a network. We demonstrate the benefits\nof the proposed model using both a simulation study and real data analysis. \n\n"}
{"id": "1610.05246", "contents": "Title: BET on Independence Abstract: We study the problem of nonparametric dependence detection. Many existing\nmethods may suffer severe power loss due to non-uniform consistency, which we\nillustrate with a paradox. To avoid such power loss, we approach the\nnonparametric test of independence through the new framework of binary\nexpansion statistics (BEStat) and binary expansion testing (BET), which examine\ndependence through a novel binary expansion filtration approximation of the\ncopula. Through a Hadamard transform, we find that the symmetry statistics in\nthe filtration are complete sufficient statistics for dependence. These\nstatistics are also uncorrelated under the null. By utilizing symmetry\nstatistics, the BET avoids the problem of non-uniform consistency and improves\nupon a wide class of commonly used methods (a) by achieving the minimax rate in\nsample size requirement for reliable power and (b) by providing clear\ninterpretations of global relationships upon rejection of independence. The\nbinary expansion approach also connects the symmetry statistics with the\ncurrent computing system to facilitate efficient bitwise implementation. We\nillustrate the BET with a study of the distribution of stars in the night sky\nand with an exploratory data analysis of the TCGA breast cancer data. \n\n"}
{"id": "1610.05507", "contents": "Title: Analysis and Implementation of an Asynchronous Optimization Algorithm\n  for the Parameter Server Abstract: This paper presents an asynchronous incremental aggregated gradient algorithm\nand its implementation in a parameter server framework for solving regularized\noptimization problems. The algorithm can handle both general convex (possibly\nnon-smooth) regularizers and general convex constraints. When the empirical\ndata loss is strongly convex, we establish linear convergence rate, give\nexplicit expressions for step-size choices that guarantee convergence to the\noptimum, and bound the associated convergence factors. The expressions have an\nexplicit dependence on the degree of asynchrony and recover classical results\nunder synchronous operation. Simulations and implementations on commercial\ncompute clouds validate our findings. \n\n"}
{"id": "1610.05735", "contents": "Title: Deep Amortized Inference for Probabilistic Programs Abstract: Probabilistic programming languages (PPLs) are a powerful modeling tool, able\nto represent any computable probability distribution. Unfortunately,\nprobabilistic program inference is often intractable, and existing PPLs mostly\nrely on expensive, approximate sampling-based methods. To alleviate this\nproblem, one could try to learn from past inferences, so that future inferences\nrun faster. This strategy is known as amortized inference; it has recently been\napplied to Bayesian networks and deep generative models. This paper proposes a\nsystem for amortized inference in PPLs. In our system, amortization comes in\nthe form of a parameterized guide program. Guide programs have similar\nstructure to the original program, but can have richer data flow, including\nneural network components. These networks can be optimized so that the guide\napproximately samples from the posterior distribution defined by the original\nprogram. We present a flexible interface for defining guide programs and a\nstochastic gradient-based scheme for optimizing guide parameters, as well as\nsome preliminary results on automatically deriving guide programs. We explore\nin detail the common machine learning pattern in which a 'local' model is\nspecified by 'global' random values and used to generate independent observed\ndata points; this gives rise to amortized local inference supporting global\nmodel learning. \n\n"}
{"id": "1610.06700", "contents": "Title: End-to-End Training Approaches for Discriminative Segmental Models Abstract: Recent work on discriminative segmental models has shown that they can\nachieve competitive speech recognition performance, using features based on\ndeep neural frame classifiers. However, segmental models can be more\nchallenging to train than standard frame-based approaches. While some segmental\nmodels have been successfully trained end to end, there is a lack of\nunderstanding of their training under different settings and with different\nlosses.\n  We investigate a model class based on recent successful approaches,\nconsisting of a linear model that combines segmental features based on an LSTM\nframe classifier. Similarly to hybrid HMM-neural network models, segmental\nmodels of this class can be trained in two stages (frame classifier training\nfollowed by linear segmental model weight training), end to end (joint training\nof both frame classifier and linear weights), or with end-to-end fine-tuning\nafter two-stage training.\n  We study segmental models trained end to end with hinge loss, log loss,\nlatent hinge loss, and marginal log loss. We consider several losses for the\ncase where training alignments are available as well as where they are not.\n  We find that in general, marginal log loss provides the most consistent\nstrong performance without requiring ground-truth alignments. We also find that\ntraining with dropout is very important in obtaining good performance with\nend-to-end training. Finally, the best results are typically obtained by a\ncombination of two-stage training and fine-tuning. \n\n"}
{"id": "1610.07524", "contents": "Title: Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments Abstract: Recidivism prediction instruments provide decision makers with an assessment\nof the likelihood that a criminal defendant will reoffend at a future point in\ntime. While such instruments are gaining increasing popularity across the\ncountry, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses a fairness criterion originating in the\nfield of educational and psychological testing that has recently been applied\nto assess the fairness of recidivism prediction instruments. We demonstrate how\nadherence to the criterion may lead to considerable disparate impact when\nrecidivism prevalence differs across groups. \n\n"}
{"id": "1610.07797", "contents": "Title: Frank-Wolfe Algorithms for Saddle Point Problems Abstract: We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained\nsmooth convex-concave saddle point (SP) problems. Remarkably, the method only\nrequires access to linear minimization oracles. Leveraging recent advances in\nFW optimization, we provide the first proof of convergence of a FW-type saddle\npoint solver over polytopes, thereby partially answering a 30 year-old\nconjecture. We also survey other convergence results and highlight gaps in the\ntheoretical underpinnings of FW-style algorithms. Motivating applications\nwithout known efficient alternatives are explored through structured prediction\nwith combinatorial penalties as well as games over matching polytopes involving\nan exponential number of constraints. \n\n"}
{"id": "1610.08733", "contents": "Title: GPflow: A Gaussian process library using TensorFlow Abstract: GPflow is a Gaussian process library that uses TensorFlow for its core\ncomputations and Python for its front end. The distinguishing features of\nGPflow are that it uses variational inference as the primary approximation\nmethod, provides concise code through the use of automatic differentiation, has\nbeen engineered with a particular emphasis on software testing and is able to\nexploit GPU hardware. \n\n"}
{"id": "1610.09038", "contents": "Title: Professor Forcing: A New Algorithm for Training Recurrent Networks Abstract: The Teacher Forcing algorithm trains recurrent networks by supplying observed\nsequence values as inputs during training and using the network's own\none-step-ahead predictions to do multi-step sampling. We introduce the\nProfessor Forcing algorithm, which uses adversarial domain adaptation to\nencourage the dynamics of the recurrent network to be the same when training\nthe network and when sampling from the network over multiple time steps. We\napply Professor Forcing to language modeling, vocal synthesis on raw waveforms,\nhandwriting generation, and image generation. Empirically we find that\nProfessor Forcing acts as a regularizer, improving test likelihood on character\nlevel Penn Treebank and sequential MNIST. We also find that the model\nqualitatively improves samples, especially when sampling for a large number of\ntime steps. This is supported by human evaluation of sample quality. Trade-offs\nbetween Professor Forcing and Scheduled Sampling are discussed. We produce\nT-SNEs showing that Professor Forcing successfully makes the dynamics of the\nnetwork during training and sampling more similar. \n\n"}
{"id": "1610.09641", "contents": "Title: Auxiliary gradient-based sampling algorithms Abstract: We introduce a new family of MCMC samplers that combine auxiliary variables,\nGibbs sampling and Taylor expansions of the target density. Our approach\npermits the marginalisation over the auxiliary variables yielding marginal\nsamplers, or the augmentation of the auxiliary variables, yielding auxiliary\nsamplers. The well-known Metropolis-adjusted Langevin algorithm (MALA) and\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm are shown to be special\ncases. We prove that marginal samplers are superior in terms of asymptotic\nvariance and demonstrate cases where they are slower in computing time compared\nto auxiliary samplers. In the context of latent Gaussian models we propose new\nauxiliary and marginal samplers whose implementation requires a single tuning\nparameter, which can be found automatically during the transient phase.\nExtensive experimentation shows that the increase in efficiency (measured as\neffective sample size per unit of computing time) relative to (optimised\nimplementations of) pCNL, elliptical slice sampling and MALA ranges from\n10-fold in binary classification problems to 25-fold in log-Gaussian Cox\nprocesses to 100-fold in Gaussian process regression, and it is on par with\nRiemann manifold Hamiltonian Monte Carlo in an example where the latter has the\nsame complexity as the aforementioned algorithms. We explain this remarkable\nimprovement in terms of the way alternative samplers try to approximate the\neigenvalues of the target. We introduce a novel MCMC sampling scheme for\nhyperparameter learning that builds upon the auxiliary samplers. The MATLAB\ncode for reproducing the experiments in the article is publicly available and a\nSupplement to this article contains additional experiments and implementation\ndetails. \n\n"}
{"id": "1611.00448", "contents": "Title: Natural-Parameter Networks: A Class of Probabilistic Neural Networks Abstract: Neural networks (NN) have achieved state-of-the-art performance in various\napplications. Unfortunately in applications where training data is\ninsufficient, they are often prone to overfitting. One effective way to\nalleviate this problem is to exploit the Bayesian approach by using Bayesian\nneural networks (BNN). Another shortcoming of NN is the lack of flexibility to\ncustomize different distributions for the weights and neurons according to the\ndata, as is often done in probabilistic graphical models. To address these\nproblems, we propose a class of probabilistic neural networks, dubbed\nnatural-parameter networks (NPN), as a novel and lightweight Bayesian treatment\nof NN. NPN allows the usage of arbitrary exponential-family distributions to\nmodel the weights and neurons. Different from traditional NN and BNN, NPN takes\ndistributions as input and goes through layers of transformation before\nproducing distributions to match the target output distributions. As a Bayesian\ntreatment, efficient backpropagation (BP) is performed to learn the natural\nparameters for the distributions over both the weights and neurons. The output\ndistributions of each layer, as byproducts, may be used as second-order\nrepresentations for the associated tasks such as link prediction. Experiments\non real-world datasets show that NPN can achieve state-of-the-art performance. \n\n"}
{"id": "1611.01129", "contents": "Title: Cross: Efficient Low-rank Tensor Completion Abstract: The completion of tensors, or high-order arrays, attracts significant\nattention in recent research. Current literature on tensor completion primarily\nfocuses on recovery from a set of uniformly randomly measured entries, and the\nrequired number of measurements to achieve recovery is not guaranteed to be\noptimal. In addition, the implementation of some previous methods is NP-hard.\nIn this article, we propose a framework for low-rank tensor completion via a\nnovel tensor measurement scheme we name Cross. The proposed procedure is\nefficient and easy to implement. In particular, we show that a third order\ntensor of Tucker rank-$(r_1, r_2, r_3)$ in $p_1$-by-$p_2$-by-$p_3$ dimensional\nspace can be recovered from as few as $r_1r_2r_3 + r_1(p_1-r_1) + r_2(p_2-r_2)\n+ r_3(p_3-r_3)$ noiseless measurements, which matches the sample complexity\nlower-bound. In the case of noisy measurements, we also develop a theoretical\nupper bound and the matching minimax lower bound for recovery error over\ncertain classes of low-rank tensors for the proposed procedure. The results can\nbe further extended to fourth or higher-order tensors. Simulation studies show\nthat the method performs well under a variety of settings. Finally, the\nprocedure is illustrated through a real dataset in neuroimaging. \n\n"}
{"id": "1611.01838", "contents": "Title: Entropy-SGD: Biasing Gradient Descent Into Wide Valleys Abstract: This paper proposes a new optimization algorithm called Entropy-SGD for\ntraining deep neural networks that is motivated by the local geometry of the\nenergy landscape. Local extrema with low generalization error have a large\nproportion of almost-zero eigenvalues in the Hessian with very few positive or\nnegative eigenvalues. We leverage upon this observation to construct a\nlocal-entropy-based objective function that favors well-generalizable solutions\nlying in large flat regions of the energy landscape, while avoiding\npoorly-generalizable solutions located in the sharp valleys. Conceptually, our\nalgorithm resembles two nested loops of SGD where we use Langevin dynamics in\nthe inner loop to compute the gradient of the local entropy before each update\nof the weights. We show that the new objective has a smoother energy landscape\nand show improved generalization over SGD using uniform stability, under\ncertain assumptions. Our experiments on convolutional and recurrent networks\ndemonstrate that Entropy-SGD compares favorably to state-of-the-art techniques\nin terms of generalization error and training time. \n\n"}
{"id": "1611.01843", "contents": "Title: Learning to Perform Physics Experiments via Deep Reinforcement Learning Abstract: When encountering novel objects, humans are able to infer a wide range of\nphysical properties such as mass, friction and deformability by interacting\nwith them in a goal driven way. This process of active interaction is in the\nsame spirit as a scientist performing experiments to discover hidden facts.\nRecent advances in artificial intelligence have yielded machines that can\nachieve superhuman performance in Go, Atari, natural language processing, and\ncomplex control problems; however, it is not clear that these systems can rival\nthe scientific intuition of even a young child. In this work we introduce a\nbasic set of tasks that require agents to estimate properties such as mass and\ncohesion of objects in an interactive simulated environment where they can\nmanipulate the objects and observe the consequences. We found that state of art\ndeep reinforcement learning methods can learn to perform the experiments\nnecessary to discover such hidden properties. By systematically manipulating\nthe problem difficulty and the cost incurred by the agent for performing\nexperiments, we found that agents learn different strategies that balance the\ncost of gathering information against the cost of making mistakes in different\nsituations. \n\n"}
{"id": "1611.02010", "contents": "Title: Convergence Analysis of Distributed Inference with Vector-Valued\n  Gaussian Belief Propagation Abstract: This paper considers inference over distributed linear Gaussian models using\nfactor graphs and Gaussian belief propagation (BP). The distributed inference\nalgorithm involves only local computation of the information matrix and of the\nmean vector, and message passing between neighbors. Under broad conditions, it\nis shown that the message information matrix converges to a unique positive\ndefinite limit matrix for arbitrary positive semidefinite initialization, and\nit approaches an arbitrarily small neighborhood of this limit matrix at a\ndoubly exponential rate. A necessary and sufficient convergence condition for\nthe belief mean vector to converge to the optimal centralized estimator is\nprovided under the assumption that the message information matrix is\ninitialized as a positive semidefinite matrix. Further, it is shown that\nGaussian BP always converges when the underlying factor graph is given by the\nunion of a forest and a single loop. The proposed convergence condition in the\nsetup of distributed linear Gaussian models is shown to be strictly weaker than\nother existing convergence conditions and requirements, including the Gaussian\nMarkov random field based walk-summability condition, and applicable to a large\nclass of scenarios. \n\n"}
{"id": "1611.02101", "contents": "Title: Distributed Coordinate Descent for Generalized Linear Models with\n  Regularization Abstract: Generalized linear model with $L_1$ and $L_2$ regularization is a widely used\ntechnique for solving classification, class probability estimation and\nregression problems. With the numbers of both features and examples growing\nrapidly in the fields like text mining and clickstream data analysis\nparallelization and the use of cluster architectures becomes important. We\npresent a novel algorithm for fitting regularized generalized linear models in\nthe distributed environment. The algorithm splits data between nodes by\nfeatures, uses coordinate descent on each node and line search to merge results\nglobally. Convergence proof is provided. A modifications of the algorithm\naddresses slow node problem. For an important particular case of logistic\nregression we empirically compare our program with several state-of-the art\napproaches that rely on different algorithmic and data spitting methods.\nExperiments demonstrate that our approach is scalable and superior when\ntraining on large and sparse datasets. \n\n"}
{"id": "1611.03979", "contents": "Title: Kernel regression, minimax rates and effective dimensionality: beyond\n  the regular case Abstract: We investigate if kernel regularization methods can achieve minimax\nconvergence rates over a source condition regularity assumption for the target\nfunction. These questions have been considered in past literature, but only\nunder specific assumptions about the decay, typically polynomial, of the\nspectrum of the the kernel mapping covariance operator. In the perspective of\ndistribution-free results, we investigate this issue under much weaker\nassumption on the eigenvalue decay, allowing for more complex behavior that can\nreflect different structure of the data at different scales. \n\n"}
{"id": "1611.03993", "contents": "Title: Riemannian Tensor Completion with Side Information Abstract: By restricting the iterate on a nonlinear manifold, the recently proposed\nRiemannian optimization methods prove to be both efficient and effective in low\nrank tensor completion problems. However, existing methods fail to exploit the\neasily accessible side information, due to their format mismatch. Consequently,\nthere is still room for improvement in such methods. To fill the gap, in this\npaper, a novel Riemannian model is proposed to organically integrate the\noriginal model and the side information by overcoming their inconsistency. For\nthis particular model, an efficient Riemannian conjugate gradient descent\nsolver is devised based on a new metric that captures the curvature of the\nobjective.Numerical experiments suggest that our solver is more accurate than\nthe state-of-the-art without compromising the efficiency. \n\n"}
{"id": "1611.04701", "contents": "Title: Errors-in-variables models with dependent measurements Abstract: Suppose that we observe $y \\in \\mathbb{R}^n$ and $X \\in \\mathbb{R}^{n \\times\nm}$ in the following errors-in-variables model: \\begin{eqnarray*} y & = & X_0\n\\beta^* +\\epsilon \\\\ X & = & X_0 + W, \\end{eqnarray*} where $X_0$ is an $n\n\\times m$ design matrix with independent subgaussian row vectors, $\\epsilon \\in\n\\mathbb{R}^n$ is a noise vector and $W$ is a mean zero $n \\times m$ random\nnoise matrix with independent subgaussian column vectors, independent of $X_0$\nand $\\epsilon$. This model is significantly different from those analyzed in\nthe literature in the sense that we allow the measurement error for each\ncovariate to be a dependent vector across its $n$ observations. Such error\nstructures appear in the science literature when modeling the trial-to-trial\nfluctuations in response strength shared across a set of neurons.\n  Under sparsity and restrictive eigenvalue type of conditions, we show that\none is able to recover a sparse vector $\\beta^* \\in \\mathbb{R}^m$ from the\nmodel given a single observation matrix $X$ and the response vector $y$. We\nestablish consistency in estimating $\\beta^*$ and obtain the rates of\nconvergence in the $\\ell_q$ norm, where $q = 1, 2$. We show error bounds which\napproach that of the regular Lasso and the Dantzig selector in case the errors\nin $W$ are tending to 0. We analyze the convergence rates of the gradient\ndescent methods for solving the nonconvex programs and show that the composite\ngradient descent algorithm is guaranteed to converge at a geometric rate to a\nneighborhood of the global minimizers: the size of the neighborhood is bounded\nby the statistical error in the $\\ell_2$ norm. Our analysis reveals interesting\nconnections between computational and statistical efficiency and the\nconcentration of measure phenomenon in random matrix theory. We provide\nsimulation evidence illuminating the theoretical predictions. \n\n"}
{"id": "1611.05089", "contents": "Title: Entangled two photon absorption cross section on the 808 nm region for\n  the common dyes Zinc tetraphenylporphyrin and Rhodamine B Abstract: We report the measurement of the entangled two photon absorption cross\nsection, $\\sigma_E$, at 808 nm on organic chromophores in solution in a low\nphoton flux regime. We performed measurements on Zinc tetraphenylporphyrin\n(ZnTPP) in Toluene and Rhodamine B (RhB) in Methanol. This is, to the best of\nour knowledge, the first time that $\\sigma_E$ is measured for RhB.\nAdditionally, we report a systematic study of the dependence of $\\sigma_E$ on\nthe molecular concentration for both molecular systems. In contrast to previous\nexperiments, our measurements are based on detecting the pairs of photons that\nare transmitted by the molecular system. By using a coincidence count circuit\nit was possible to improve the signal to noise ratio. This type of work is\nimportant for the development of spectroscopic and microscopic techniques using\nentangled photons. \n\n"}
{"id": "1611.05722", "contents": "Title: GENESIM: genetic extraction of a single, interpretable model Abstract: Models obtained by decision tree induction techniques excel in being\ninterpretable.However, they can be prone to overfitting, which results in a low\npredictive performance. Ensemble techniques are able to achieve a higher\naccuracy. However, this comes at a cost of losing interpretability of the\nresulting model. This makes ensemble techniques impractical in applications\nwhere decision support, instead of decision making, is crucial.\n  To bridge this gap, we present the GENESIM algorithm that transforms an\nensemble of decision trees to a single decision tree with an enhanced\npredictive performance by using a genetic algorithm. We compared GENESIM to\nprevalent decision tree induction and ensemble techniques using twelve publicly\navailable data sets. The results show that GENESIM achieves a better predictive\nperformance on most of these data sets than decision tree induction techniques\nand a predictive performance in the same order of magnitude as the ensemble\ntechniques. Moreover, the resulting model of GENESIM has a very low complexity,\nmaking it very interpretable, in contrast to ensemble techniques. \n\n"}
{"id": "1611.06972", "contents": "Title: Measuring Sample Quality with Diffusions Abstract: Stein's method for measuring convergence to a continuous target distribution\nrelies on an operator characterizing the target and Stein factor bounds on the\nsolutions of an associated differential equation. While such operators and\nbounds are readily available for a diversity of univariate targets, few\nmultivariate targets have been analyzed. We introduce a new class of\ncharacterizing operators based on Ito diffusions and develop explicit\nmultivariate Stein factor bounds for any target with a fast-coupling Ito\ndiffusion. As example applications, we develop computable and\nconvergence-determining diffusion Stein discrepancies for log-concave,\nheavy-tailed, and multimodal targets and use these quality measures to select\nthe hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare\nrandom and deterministic quadrature rules, and quantify bias-variance tradeoffs\nin approximate MCMC. Our results establish a near-linear relationship between\ndiffusion Stein discrepancies and Wasserstein distances, improving upon past\nwork even for strongly log-concave targets. The exposed relationship between\nStein factors and Markov process coupling may be of independent interest. \n\n"}
{"id": "1611.07056", "contents": "Title: The Recycling Gibbs Sampler for Efficient Learning Abstract: Monte Carlo methods are essential tools for Bayesian inference. Gibbs\nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively\nused in signal processing, machine learning, and statistics, employed to draw\nsamples from complicated high-dimensional posterior distributions. The key\npoint for the successful application of the Gibbs sampler is the ability to\ndraw efficiently samples from the full-conditional probability density\nfunctions. Since in the general case this is not possible, in order to speed up\nthe convergence of the chain, it is required to generate auxiliary samples\nwhose information is eventually disregarded. In this work, we show that these\nauxiliary samples can be recycled within the Gibbs estimators, improving their\nefficiency with no extra cost. This novel scheme arises naturally after\npointing out the relationship between the standard Gibbs sampler and the chain\nrule used for sampling purposes. Numerical simulations involving simple and\nreal inference problems confirm the excellent performance of the proposed\nscheme in terms of accuracy and computational efficiency. In particular we give\nempirical evidence of performance in a toy example, inference of Gaussian\nprocesses hyperparameters, and learning dependence graphs through regression. \n\n"}
{"id": "1611.08568", "contents": "Title: Bottleneck Conditional Density Estimation Abstract: We introduce a new framework for training deep generative models for\nhigh-dimensional conditional density estimation. The Bottleneck Conditional\nDensity Estimator (BCDE) is a variant of the conditional variational\nautoencoder (CVAE) that employs layer(s) of stochastic variables as the\nbottleneck between the input $x$ and target $y$, where both are\nhigh-dimensional. Crucially, we propose a new hybrid training method that\nblends the conditional generative model with a joint generative model. Hybrid\nblending is the key to effective training of the BCDE, which avoids overfitting\nand provides a novel mechanism for leveraging unlabeled data. We show that our\nhybrid training procedure enables models to achieve competitive results in the\nMNIST quadrant prediction task in the fully-supervised setting, and sets new\nbenchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA. \n\n"}
{"id": "1611.08903", "contents": "Title: Should I use TensorFlow Abstract: Google's Machine Learning framework TensorFlow was open-sourced in November\n2015 [1] and has since built a growing community around it. TensorFlow is\nsupposed to be flexible for research purposes while also allowing its models to\nbe deployed productively. This work is aimed towards people with experience in\nMachine Learning considering whether they should use TensorFlow in their\nenvironment. Several aspects of the framework important for such a decision are\nexamined, such as the heterogenity, extensibility and its computation graph. A\npure Python implementation of linear classification is compared with an\nimplementation utilizing TensorFlow. I also contrast TensorFlow to other\npopular frameworks with respect to modeling capability, deployment and\nperformance and give a brief description of the current adaption of the\nframework. \n\n"}
{"id": "1611.09226", "contents": "Title: Robust Variational Inference Abstract: Variational inference is a powerful tool for approximate inference. However,\nit mainly focuses on the evidence lower bound as variational objective and the\ndevelopment of other measures for variational inference is a promising area of\nresearch. This paper proposes a robust modification of evidence and a lower\nbound for the evidence, which is applicable when the majority of the training\nset samples are random noise objects. We provide experiments for variational\nautoencoders to show advantage of the objective over the evidence lower bound\non synthetic datasets obtained by adding uninformative noise objects to MNIST\nand OMNIGLOT. Additionally, for the original MNIST and OMNIGLOT datasets we\nobserve a small improvement over the non-robust evidence lower bound. \n\n"}
{"id": "1612.00367", "contents": "Title: Large-scale Validation of Counterfactual Learning Methods: A Test-Bed Abstract: The ability to perform effective off-policy learning would revolutionize the\nprocess of building better interactive systems, such as search engines and\nrecommendation systems for e-commerce, computational advertising and news.\nRecent approaches for off-policy evaluation and learning in these settings\nappear promising. With this paper, we provide real-world data and a\nstandardized test-bed to systematically investigate these algorithms using data\nfrom display advertising. In particular, we consider the problem of filling a\nbanner ad with an aggregate of multiple products the user may want to purchase.\nThis paper presents our test-bed, the sanity checks we ran to ensure its\nvalidity, and shows results comparing state-of-the-art off-policy learning\nmethods like doubly robust optimization, POEM, and reductions to supervised\nlearning using regression baselines. Our results show experimental evidence\nthat recent off-policy learning methods can improve upon state-of-the-art\nsupervised learning techniques on a large-scale real-world data set. \n\n"}
{"id": "1612.02842", "contents": "Title: Tensor-Dictionary Learning with Deep Kruskal-Factor Analysis Abstract: A multi-way factor analysis model is introduced for tensor-variate data of\nany order. Each data item is represented as a (sparse) sum of Kruskal\ndecompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can\ninfer both the tensor-rank of each dictionary atom and the number of dictionary\natoms. The model is adapted for online learning, which allows dictionary\nlearning on large data sets. After KFA is introduced, the model is extended to\na deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The\nexperiments section demonstrates the improvement of KFA over vectorized\napproaches (e.g., BPFA), tensor decompositions, and convolutional neural\nnetworks (CNN) in multi-way denoising, blind inpainting, and image\nclassification. The improvement in PSNR for the inpainting results over other\nmethods exceeds 1dB in several cases and we achieve state of the art results on\nCaltech101 image classification. \n\n"}
{"id": "1612.03770", "contents": "Title: Neurogenesis Deep Learning Abstract: Neural machine learning methods, such as deep neural networks (DNN), have\nachieved remarkable success in a number of complex data processing tasks. These\nmethods have arguably had their strongest impact on tasks such as image and\naudio processing - data processing domains in which humans have long held clear\nadvantages over conventional algorithms. In contrast to biological neural\nsystems, which are capable of learning continuously, deep artificial networks\nhave a limited ability for incorporating new information in an already trained\nnetwork. As a result, methods for continuous learning are potentially highly\nimpactful in enabling the application of deep networks to dynamic data sets.\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\nexplore the potential for adding new neurons to deep layers of artificial\nneural networks in order to facilitate their acquisition of novel information\nwhile preserving previously trained data representations. Our results on the\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\nlower and upper case letters and digits, demonstrate that neurogenesis is well\nsuited for addressing the stability-plasticity dilemma that has long challenged\nadaptive machine learning algorithms. \n\n"}
{"id": "1612.04315", "contents": "Title: Towards Adaptive Training of Agent-based Sparring Partners for Fighter\n  Pilots Abstract: A key requirement for the current generation of artificial decision-makers is\nthat they should adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, must optimize behavior with respect to an\nobjective function that is evaluated and learned through simulations. Bayesian\noptimization with a Gaussian Process surrogate is used as the method for\ninvestigating the objective function. One key benefit is that during\noptimization, the Gaussian Process learns a global estimate of the true\nobjective function, with predicted outcomes and a statistical measure of\nconfidence in areas that haven't been investigated yet. Having a model of the\nobjective function is important for being able to understand possible outcomes\nin the decision space; for example this is crucial for training and providing\nfeedback to human pilots. However, standard Bayesian optimization does not\nperform consistently or provide an accurate Gaussian Process surrogate function\nfor highly volatile objective functions. We treat these problems by introducing\na novel sampling technique called Hybrid Repeat/Multi-point Sampling. This\ntechnique gives the AI ability to learn optimum behaviors in a highly uncertain\nenvironment. More importantly, it not only improves the reliability of the\noptimization, but also creates a better model of the entire objective surface.\nWith this improved model the agent is equipped to more accurately/efficiently\npredict performance in unexplored scenarios. \n\n"}
{"id": "1612.04717", "contents": "Title: Network cross-validation by edge sampling Abstract: While many statistical models and methods are now available for network\nanalysis, resampling network data remains a challenging problem.\nCross-validation is a useful general tool for model selection and parameter\ntuning, but is not directly applicable to networks since splitting network\nnodes into groups requires deleting edges and destroys some of the network\nstructure. Here we propose a new network resampling strategy based on splitting\nnode pairs rather than nodes applicable to cross-validation for a wide range of\nnetwork model selection tasks. We provide a theoretical justification for our\nmethod in a general setting and examples of how our method can be used in\nspecific network model selection and parameter tuning tasks. Numerical results\non simulated networks and on a citation network of statisticians show that this\ncross-validation approach works well for model selection. \n\n"}
{"id": "1612.05086", "contents": "Title: Coupling Adaptive Batch Sizes with Learning Rates Abstract: Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available. \n\n"}
{"id": "1612.05356", "contents": "Title: Projected Semi-Stochastic Gradient Descent Method with Mini-Batch Scheme\n  under Weak Strong Convexity Assumption Abstract: We propose a projected semi-stochastic gradient descent method with\nmini-batch for improving both the theoretical complexity and practical\nperformance of the general stochastic gradient descent method (SGD). We are\nable to prove linear convergence under weak strong convexity assumption. This\nrequires no strong convexity assumption for minimizing the sum of smooth convex\nfunctions subject to a compact polyhedral set, which remains popular across\nmachine learning community. Our PS2GD preserves the low-cost per iteration and\nhigh optimization accuracy via stochastic gradient variance-reduced technique,\nand admits a simple parallel implementation with mini-batches. Moreover, PS2GD\nis also applicable to dual problem of SVM with hinge loss. \n\n"}
{"id": "1612.05708", "contents": "Title: Mutual information for fitting deep nonlinear models Abstract: Deep nonlinear models pose a challenge for fitting parameters due to lack of\nknowledge of the hidden layer and the potentially non-affine relation of the\ninitial and observed layers. In the present work we investigate the use of\ninformation theoretic measures such as mutual information and Kullback-Leibler\n(KL) divergence as objective functions for fitting such models without\nknowledge of the hidden layer. We investigate one model as a proof of concept\nand one application of cogntive performance. We further investigate the use of\noptimizers with these methods. Mutual information is largely successful as an\nobjective, depending on the parameters. KL divergence is found to be similarly\nsuccesful, given some knowledge of the statistics of the hidden layer. \n\n"}
{"id": "1612.06299", "contents": "Title: Simple Black-Box Adversarial Perturbations for Deep Networks Abstract: Deep neural networks are powerful and popular learning models that achieve\nstate-of-the-art pattern recognition performance on many computer vision,\nspeech, and language processing tasks. However, these networks have also been\nshown susceptible to carefully crafted adversarial perturbations which force\nmisclassification of the inputs. Adversarial examples enable adversaries to\nsubvert the expected system behavior leading to undesired consequences and\ncould pose a security risk when these systems are deployed in the real world.\n  In this work, we focus on deep convolutional neural networks and demonstrate\nthat adversaries can easily craft adversarial examples even without any\ninternal knowledge of the target network. Our attacks treat the network as an\noracle (black-box) and only assume that the output of the network can be\nobserved on the probed inputs. Our first attack is based on a simple idea of\nadding perturbation to a randomly selected single pixel or a small set of them.\nWe then improve the effectiveness of this attack by carefully constructing a\nsmall set of pixels to perturb by using the idea of greedy local-search. Our\nproposed attacks also naturally extend to a stronger notion of\nmisclassification. Our extensive experimental results illustrate that even\nthese elementary attacks can reveal a deep neural network's vulnerabilities.\nThe simplicity and effectiveness of our proposed schemes mean that they could\nserve as a litmus test for designing robust networks. \n\n"}
{"id": "1612.07640", "contents": "Title: Deep Learning and Its Applications to Machine Health Monitoring: A\n  Survey Abstract: Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed. \n\n"}
{"id": "1612.09007", "contents": "Title: A Deep Learning Approach To Multiple Kernel Fusion Abstract: Kernel fusion is a popular and effective approach for combining multiple\nfeatures that characterize different aspects of data. Traditional approaches\nfor Multiple Kernel Learning (MKL) attempt to learn the parameters for\ncombining the kernels through sophisticated optimization procedures. In this\npaper, we propose an alternative approach that creates dense embeddings for\ndata using the kernel similarities and adopts a deep neural network\narchitecture for fusing the embeddings. In order to improve the effectiveness\nof this network, we introduce the kernel dropout regularization strategy\ncoupled with the use of an expanded set of composition kernels. Experiment\nresults on a real-world activity recognition dataset show that the proposed\narchitecture is effective in fusing kernels and achieves state-of-the-art\nperformance. \n\n"}
{"id": "1612.09034", "contents": "Title: Geometric descent method for convex composite minimization Abstract: In this paper, we extend the geometric descent method recently proposed by\nBubeck, Lee and Singh to tackle nonsmooth and strongly convex composite\nproblems. We prove that our proposed algorithm, dubbed geometric proximal\ngradient method (GeoPG), converges with a linear rate $(1-1/\\sqrt{\\kappa})$ and\nthus achieves the optimal rate among first-order methods, where $\\kappa$ is the\ncondition number of the problem. Numerical results on linear regression and\nlogistic regression with elastic net regularization show that GeoPG compares\nfavorably with Nesterov's accelerated proximal gradient method, especially when\nthe problem is ill-conditioned. \n\n"}
{"id": "1612.09596", "contents": "Title: Counterfactual Prediction with Deep Instrumental Variables Networks Abstract: We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine. \n\n"}
{"id": "1701.01140", "contents": "Title: Learning causal effects from many randomized experiments using\n  regularized instrumental variables Abstract: Scientific and business practices are increasingly resulting in large\ncollections of randomized experiments. Analyzed together, these collections can\ntell us things that individual experiments in the collection cannot. We study\nhow to learn causal relationships between variables from the kinds of\ncollections faced by modern data scientists: the number of experiments is\nlarge, many experiments have very small effects, and the analyst lacks metadata\n(e.g., descriptions of the interventions). Here we use experimental groups as\ninstrumental variables (IV) and show that a standard method (two-stage least\nsquares) is biased even when the number of experiments is infinite. We show how\na sparsity-inducing l0 regularization can --- in a reversal of the standard\nbias--variance tradeoff in regularization --- reduce bias (and thus error) of\ninterventional predictions. Because we are interested in interventional loss\nminimization we also propose a modified cross-validation procedure (IVCV) to\nfeasibly select the regularization parameter. We show, using a trick from Monte\nCarlo sampling, that IVCV can be done using summary statistics instead of raw\ndata. This makes our full procedure simple to use in many real-world\napplications. \n\n"}
{"id": "1701.02804", "contents": "Title: Similarity Function Tracking using Pairwise Comparisons Abstract: Recent work in distance metric learning has focused on learning\ntransformations of data that best align with specified pairwise similarity and\ndissimilarity constraints, often supplied by a human observer. The learned\ntransformations lead to improved retrieval, classification, and clustering\nalgorithms due to the better adapted distance or similarity measures. Here, we\naddress the problem of learning these transformations when the underlying\nconstraint generation process is nonstationary. This nonstationarity can be due\nto changes in either the ground-truth clustering used to generate constraints\nor changes in the feature subspaces in which the class structure is apparent.\nWe propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD),\na general adaptive, online approach for learning and tracking optimal metrics\nas they change over time that is highly robust to a variety of nonstationary\nbehaviors in the changing metric. We apply the OCELAD framework to an ensemble\nof online learners. Specifically, we create a retro-initialized composite\nobjective mirror descent (COMID) ensemble (RICE) consisting of a set of\nparallel COMID learners with different learning rates, and demonstrate\nparameter-free RICE-OCELAD metric learning on both synthetic data and a highly\nnonstationary Twitter dataset. We show significant performance improvements and\nincreased robustness to nonstationary effects relative to previously proposed\nbatch and online distance metric learning algorithms. \n\n"}
{"id": "1701.02815", "contents": "Title: Stochastic Generative Hashing Abstract: Learning-based binary hashing has become a powerful paradigm for fast search\nand retrieval in massive databases. However, due to the requirement of discrete\noutputs for the hash functions, learning such functions is known to be very\nchallenging. In addition, the objective functions adopted by existing hashing\ntechniques are mostly chosen heuristically. In this paper, we propose a novel\ngenerative approach to learn hash functions through Minimum Description Length\nprinciple such that the learned hash codes maximally compress the dataset and\ncan also be used to regenerate the inputs. We also develop an efficient\nlearning algorithm based on the stochastic distributional gradient, which\navoids the notorious difficulty caused by binary output constraints, to jointly\noptimize the parameters of the hash function and the associated generative\nmodel. Extensive experiments on a variety of large-scale datasets show that the\nproposed method achieves better retrieval results than the existing\nstate-of-the-art methods. \n\n"}
{"id": "1701.03436", "contents": "Title: Fast Stability Scanning for Future Grid Scenario Analysis Abstract: Future grid scenario analysis requires a major departure from conventional\npower system planning, where only a handful of most critical conditions is\ntypically analyzed. To capture the inter-seasonal variations in renewable\ngeneration of a future grid scenario necessitates the use of computationally\nintensive time-series analysis. In this paper, we propose a planning framework\nfor fast stability scanning of future grid scenarios using a novel feature\nselection algorithm and a novel self-adaptive PSO-k-means clustering algorithm.\nTo achieve the computational speed-up, the stability analysis is performed only\non small number of representative cluster centroids instead of on the full set\nof operating conditions. As a case study, we perform small-signal stability and\nsteady-state voltage stability scanning of a simplified model of the Australian\nNational Electricity Market with significant penetration of renewable\ngeneration. The simulation results show the effectiveness of the proposed\napproach. Compared to an exhaustive time series scanning, the proposed\nframework reduced the computational burden up to ten times, with an acceptable\nlevel of accuracy. \n\n"}
{"id": "1701.03449", "contents": "Title: Manifold Alignment Determination: finding correspondences across\n  different data views Abstract: We present Manifold Alignment Determination (MAD), an algorithm for learning\nalignments between data points from multiple views or modalities. The approach\nis capable of learning correspondences between views as well as correspondences\nbetween individual data-points. The proposed method requires only a few aligned\nexamples from which it is capable to recover a global alignment through a\nprobabilistic model. The strong, yet flexible regularization provided by the\ngenerative model is sufficient to align the views. We provide experiments on\nboth synthetic and real data to highlight the benefit of the proposed approach. \n\n"}
{"id": "1701.04968", "contents": "Title: Multilayer Perceptron Algebra Abstract: Artificial Neural Networks(ANN) has been phenomenally successful on various\npattern recognition tasks. However, the design of neural networks rely heavily\non the experience and intuitions of individual developers. In this article, the\nauthor introduces a mathematical structure called MLP algebra on the set of all\nMultilayer Perceptron Neural Networks(MLP), which can serve as a guiding\nprinciple to build MLPs accommodating to the particular data sets, and to build\ncomplex MLPs from simpler ones. \n\n"}
{"id": "1701.05230", "contents": "Title: Surrogate Aided Unsupervised Recovery of Sparse Signals in Single Index\n  Models for Binary Outcomes Abstract: We consider the recovery of regression coefficients, denoted by\n$\\boldsymbol{\\beta}_0$, for a single index model (SIM) relating a binary\noutcome $Y$ to a set of possibly high dimensional covariates $\\boldsymbol{X}$,\nbased on a large but 'unlabeled' dataset $\\mathcal{U}$, with $Y$ never\nobserved. On $\\mathcal{U}$, we fully observe $\\boldsymbol{X}$ and additionally,\na surrogate $S$ which, while not being strongly predictive of $Y$ throughout\nthe entirety of its support, can forecast it with high accuracy when it assumes\nextreme values. Such datasets arise naturally in modern studies involving large\ndatabases such as electronic medical records (EMR) where $Y$, unlike\n$(\\boldsymbol{X}, S)$, is difficult and/or expensive to obtain. In EMR studies,\nan example of $Y$ and $S$ would be the true disease phenotype and the count of\nthe associated diagnostic codes respectively. Assuming another SIM for $S$\ngiven $\\boldsymbol{X}$, we show that under sparsity assumptions, we can recover\n$\\boldsymbol{\\beta}_0$ proportionally by simply fitting a least squares LASSO\nestimator to the subset of the observed data on $(\\boldsymbol{X}, S)$\nrestricted to the extreme sets of $S$, with $Y$ imputed using the surrogacy of\n$S$. We obtain sharp finite sample performance bounds for our estimator,\nincluding deterministic deviation bounds and probabilistic guarantees. We\ndemonstrate the effectiveness of our approach through multiple simulation\nstudies, as well as by application to real data from an EMR study conducted at\nthe Partners HealthCare Systems. \n\n"}
{"id": "1701.05512", "contents": "Title: Fisher consistency for prior probability shift Abstract: We introduce Fisher consistency in the sense of unbiasedness as a desirable\nproperty for estimators of class prior probabilities. Lack of Fisher\nconsistency could be used as a criterion to dismiss estimators that are\nunlikely to deliver precise estimates in test datasets under prior probability\nand more general dataset shift. The usefulness of this unbiasedness concept is\ndemonstrated with three examples of classifiers used for quantification:\nAdjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted\nClassify & Count and EM-algorithm are Fisher consistent. A counter-example\nshows that CDE-Iterate is not Fisher consistent and, therefore, cannot be\ntrusted to deliver reliable estimates of class probabilities. \n\n"}
{"id": "1701.05927", "contents": "Title: Learning Particle Physics by Example: Location-Aware Generative\n  Adversarial Networks for Physics Synthesis Abstract: We provide a bridge between generative modeling in the Machine Learning\ncommunity and simulated physical processes in High Energy Particle Physics by\napplying a novel Generative Adversarial Network (GAN) architecture to the\nproduction of jet images -- 2D representations of energy depositions from\nparticles interacting with a calorimeter. We propose a simple architecture, the\nLocation-Aware Generative Adversarial Network, that learns to produce realistic\nradiation patterns from simulated high energy particle collisions. The pixel\nintensities of GAN-generated images faithfully span over many orders of\nmagnitude and exhibit the desired low-dimensional physical properties (i.e.,\njet mass, n-subjettiness, etc.). We shed light on limitations, and provide a\nnovel empirical validation of image quality and validity of GAN-produced\nsimulations of the natural world. This work provides a base for further\nexplorations of GANs for use in faster simulation in High Energy Particle\nPhysics. \n\n"}
{"id": "1701.08528", "contents": "Title: Self-Adaptation of Activity Recognition Systems to New Sensors Abstract: Traditional activity recognition systems work on the basis of training,\ntaking a fixed set of sensors into account. In this article, we focus on the\nquestion how pattern recognition can leverage new information sources without\nany, or with minimal user input. Thus, we present an approach for opportunistic\nactivity recognition, where ubiquitous sensors lead to dynamically changing\ninput spaces. Our method is a variation of well-established principles of\nmachine learning, relying on unsupervised clustering to discover structure in\ndata and inferring cluster labels from a small number of labeled dates in a\nsemi-supervised manner. Elaborating the challenges, evaluations of over 3000\nsensor combinations from three multi-user experiments are presented in detail\nand show the potential benefit of our approach. \n\n"}
{"id": "1702.01209", "contents": "Title: Probabilistic Sensor Fusion for Ambient Assisted Living Abstract: There is a widely-accepted need to revise current forms of health-care\nprovision, with particular interest in sensing systems in the home. Given a\nmultiple-modality sensor platform with heterogeneous network connectivity, as\nis under development in the Sensor Platform for HEalthcare in Residential\nEnvironment (SPHERE) Interdisciplinary Research Collaboration (IRC), we face\nspecific challenges relating to the fusion of the heterogeneous sensor\nmodalities.\n  We introduce Bayesian models for sensor fusion, which aims to address the\nchallenges of fusion of heterogeneous sensor modalities. Using this approach we\nare able to identify the modalities that have most utility for each particular\nactivity, and simultaneously identify which features within that activity are\nmost relevant for a given activity.\n  We further show how the two separate tasks of location prediction and\nactivity recognition can be fused into a single model, which allows for\nsimultaneous learning an prediction for both tasks.\n  We analyse the performance of this model on data collected in the SPHERE\nhouse, and show its utility. We also compare against some benchmark models\nwhich do not have the full structure,and show how the proposed model compares\nfavourably to these methods \n\n"}
{"id": "1702.02526", "contents": "Title: Deep Kernelized Autoencoders Abstract: In this paper we introduce the deep kernelized autoencoder, a neural network\nmodel that allows an explicit approximation of (i) the mapping from an input\nspace to an arbitrary, user-specified kernel space and (ii) the back-projection\nfrom such a kernel space to input space. The proposed method is based on\ntraditional autoencoders and is trained through a new unsupervised loss\nfunction. During training, we optimize both the reconstruction accuracy of\ninput samples and the alignment between a kernel matrix given as prior and the\ninner products of the hidden representations computed by the autoencoder.\nKernel alignment provides control over the hidden representation learned by the\nautoencoder. Experiments have been performed to evaluate both reconstruction\nand kernel alignment performance. Additionally, we applied our method to\nemulate kPCA on a denoising task obtaining promising results. \n\n"}
{"id": "1702.04126", "contents": "Title: Gaussian-Dirichlet Posterior Dominance in Sequential Learning Abstract: We consider the problem of sequential learning from categorical observations\nbounded in [0,1]. We establish an ordering between the Dirichlet posterior over\ncategorical outcomes and a Gaussian posterior under observations with N(0,1)\nnoise. We establish that, conditioned upon identical data with at least two\nobservations, the posterior mean of the categorical distribution will always\nsecond-order stochastically dominate the posterior mean of the Gaussian\ndistribution. These results provide a useful tool for the analysis of\nsequential learning under categorical outcomes. \n\n"}
{"id": "1702.04267", "contents": "Title: On Detecting Adversarial Perturbations Abstract: Machine learning and deep learning in particular has advanced tremendously on\nperceptual tasks in recent years. However, it remains vulnerable against\nadversarial perturbations of the input that have been crafted specifically to\nfool the system while being quasi-imperceptible to a human. In this work, we\npropose to augment deep neural networks with a small \"detector\" subnetwork\nwhich is trained on the binary classification task of distinguishing genuine\ndata from data containing adversarial perturbations. Our method is orthogonal\nto prior work on addressing adversarial perturbations, which has mostly focused\non making the classification network itself more robust. We show empirically\nthat adversarial perturbations can be detected surprisingly well even though\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\ntrained to detect only a specific adversary, they generalize to similar and\nweaker adversaries. In addition, we propose an adversarial attack that fools\nboth the classifier and the detector and a novel training procedure for the\ndetector that counteracts this attack. \n\n"}
{"id": "1702.05575", "contents": "Title: A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics Abstract: We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for\nnon-convex optimization. The algorithm performs stochastic gradient descent,\nwhere in each step it injects appropriately scaled Gaussian noise to the\nupdate. We analyze the algorithm's hitting time to an arbitrary subset of the\nparameter space. Two results follow from our general theory: First, we prove\nthat for empirical risk minimization, if the empirical risk is point-wise close\nto the (smooth) population risk, then the algorithm achieves an approximate\nlocal minimum of the population risk in polynomial time, escaping suboptimal\nlocal minima that only exist in the empirical risk. Second, we show that SGLD\nimproves on one of the best known learnability results for learning linear\nclassifiers under the zero-one loss. \n\n"}
{"id": "1702.05870", "contents": "Title: Cosine Normalization: Using Cosine Similarity Instead of Dot Product in\n  Neural Networks Abstract: Traditionally, multi-layer neural networks use dot product between the output\nvector of previous layer and the incoming weight vector as the input to\nactivation function. The result of dot product is unbounded, thus increases the\nrisk of large variance. Large variance of neuron makes the model sensitive to\nthe change of input distribution, thus results in poor generalization, and\naggravates the internal covariate shift which slows down the training. To bound\ndot product and decrease the variance, we propose to use cosine similarity or\ncentered cosine similarity (Pearson Correlation Coefficient) instead of dot\nproduct in neural networks, which we call cosine normalization. We compare\ncosine normalization with batch, weight and layer normalization in\nfully-connected neural networks as well as convolutional networks on the data\nsets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that\ncosine normalization achieves better performance than other normalization\ntechniques. \n\n"}
{"id": "1702.06602", "contents": "Title: Exemplar-Centered Supervised Shallow Parametric Data Embedding Abstract: Metric learning methods for dimensionality reduction in combination with\nk-Nearest Neighbors (kNN) have been extensively deployed in many\nclassification, data embedding, and information retrieval applications.\nHowever, most of these approaches involve pairwise training data comparisons,\nand thus have quadratic computational complexity with respect to the size of\ntraining set, preventing them from scaling to fairly big datasets. Moreover,\nduring testing, comparing test data against all the training data points is\nalso expensive in terms of both computational cost and resources required.\nFurthermore, previous metrics are either too constrained or too expressive to\nbe well learned. To effectively solve these issues, we present an\nexemplar-centered supervised shallow parametric data embedding model, using a\nMaximally Collapsing Metric Learning (MCML) objective. Our strategy learns a\nshallow high-order parametric embedding function and compares training/test\ndata only with learned or precomputed exemplars, resulting in a cost function\nwith linear computational complexity for both training and testing. We also\nempirically demonstrate, using several benchmark datasets, that for\nclassification in two-dimensional embedding space, our approach not only gains\nspeedup of kNN by hundreds of times, but also outperforms state-of-the-art\nsupervised embedding approaches. \n\n"}
{"id": "1702.07013", "contents": "Title: Learning Hawkes Processes from Short Doubly-Censored Event Sequences Abstract: Many real-world applications require robust algorithms to learn point\nprocesses based on a type of incomplete data --- the so-called short\ndoubly-censored (SDC) event sequences. We study this critical problem of\nquantitative asynchronous event sequence analysis under the framework of Hawkes\nprocesses by leveraging the idea of data synthesis. Given SDC event sequences\nobserved in a variety of time intervals, we propose a sampling-stitching data\nsynthesis method --- sampling predecessors and successors for each SDC event\nsequence from potential candidates and stitching them together to synthesize\nlong training sequences. The rationality and the feasibility of our method are\ndiscussed in terms of arguments based on likelihood. Experiments on both\nsynthetic and real-world data demonstrate that the proposed data synthesis\nmethod improves learning results indeed for both time-invariant and\ntime-varying Hawkes processes. \n\n"}
{"id": "1702.07274", "contents": "Title: Rotting Bandits Abstract: The Multi-Armed Bandits (MAB) framework highlights the tension between\nacquiring new knowledge (Exploration) and leveraging available knowledge\n(Exploitation). In the classical MAB problem, a decision maker must choose an\narm at each time step, upon which she receives a reward. The decision maker's\nobjective is to maximize her cumulative expected reward over the time horizon.\nThe MAB problem has been studied extensively, specifically under the assumption\nof the arms' rewards distributions being stationary, or quasi-stationary, over\ntime. We consider a variant of the MAB framework, which we termed Rotting\nBandits, where each arm's expected reward decays as a function of the number of\ntimes it has been pulled. We are motivated by many real-world scenarios such as\nonline advertising, content recommendation, crowdsourcing, and more. We present\nalgorithms, accompanied by simulations, and derive theoretical guarantees. \n\n"}
{"id": "1702.07790", "contents": "Title: Activation Ensembles for Deep Neural Networks Abstract: Many activation functions have been proposed in the past, but selecting an\nadequate one requires trial and error. We propose a new methodology of\ndesigning activation functions within a neural network at each layer. We call\nthis technique an \"activation ensemble\" because it allows the use of multiple\nactivation functions at each layer. This is done by introducing additional\nvariables, $\\alpha$, at each activation layer of a network to allow for\nmultiple activation functions to be active at each neuron. By design,\nactivations with larger $\\alpha$ values at a neuron is equivalent to having the\nlargest magnitude. Hence, those higher magnitude activations are \"chosen\" by\nthe network. We implement the activation ensembles on a variety of datasets\nusing an array of Feed Forward and Convolutional Neural Networks. By using the\nactivation ensemble, we achieve superior results compared to traditional\ntechniques. In addition, because of the flexibility of this methodology, we\nmore deeply explore activation functions and the features that they capture. \n\n"}
{"id": "1702.07811", "contents": "Title: Adaptive Neural Networks for Efficient Inference Abstract: We present an approach to adaptively utilize deep neural networks in order to\nreduce the evaluation time on new examples without loss of accuracy. Rather\nthan attempting to redesign or approximate existing networks, we propose two\nschemes that adaptively utilize networks. We first pose an adaptive network\nevaluation scheme, where we learn a system to adaptively choose the components\nof a deep network to be evaluated for each example. By allowing examples\ncorrectly classified using early layers of the system to exit, we avoid the\ncomputational time associated with full evaluation of the network. We extend\nthis to learn a network selection system that adaptively selects the network to\nbe evaluated for each example. We show that computational time can be\ndramatically reduced by exploiting the fact that many examples can be correctly\nclassified using relatively efficient networks and that complex,\ncomputationally costly networks are only necessary for a small fraction of\nexamples. We pose a global objective for learning an adaptive early exit or\nnetwork selection policy and solve it by reducing the policy learning problem\nto a layer-by-layer weighted binary classification problem. Empirically, these\napproaches yield dramatic reductions in computational cost, with up to a 2.8x\nspeedup on state-of-the-art networks from the ImageNet image recognition\nchallenge with minimal (<1%) loss of top5 accuracy. \n\n"}
{"id": "1702.07958", "contents": "Title: Efficient Online Bandit Multiclass Learning with $\\tilde{O}(\\sqrt{T})$\n  Regret Abstract: We present an efficient second-order algorithm with\n$\\tilde{O}(\\frac{1}{\\eta}\\sqrt{T})$ regret for the bandit online multiclass\nproblem. The regret bound holds simultaneously with respect to a family of loss\nfunctions parameterized by $\\eta$, for a range of $\\eta$ restricted by the norm\nof the competitor. The family of loss functions ranges from hinge loss\n($\\eta=0$) to squared hinge loss ($\\eta=1$). This provides a solution to the\nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for\n$\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our\nalgorithm experimentally, showing that it also performs favorably against\nearlier algorithms. \n\n"}
{"id": "1702.08259", "contents": "Title: Adaptive Ensemble Prediction for Deep Neural Networks based on\n  Confidence Level Abstract: Ensembling multiple predictions is a widely used technique for improving the\naccuracy of various machine learning tasks. One obvious drawback of ensembling\nis its higher execution cost during inference. In this paper, we first describe\nour insights on the relationship between the probability of prediction and the\neffect of ensembling with current deep neural networks; ensembling does not\nhelp mispredictions for inputs predicted with a high probability even when\nthere is a non-negligible number of mispredicted inputs. This finding motivated\nus to develop a way to adaptively control the ensembling. If the prediction for\nan input reaches a high enough probability, i.e., the output from the softmax\nfunction, on the basis of the confidence level, we stop ensembling for this\ninput to avoid wasting computation power. We evaluated the adaptive ensembling\nby using various datasets and showed that it reduces the computation cost\nsignificantly while achieving accuracy similar to that of static ensembling\nusing a pre-defined number of local predictions. We also show that our\nstatistically rigorous confidence-level-based early-exit condition reduces the\nburden of task-dependent threshold tuning better compared with naive early exit\nbased on a pre-defined threshold in addition to yielding a better accuracy with\nthe same cost. \n\n"}
{"id": "1702.08503", "contents": "Title: SGD Learns the Conjugate Kernel Class of the Network Abstract: We show that the standard stochastic gradient decent (SGD) algorithm is\nguaranteed to learn, in polynomial time, a function that is competitive with\nthe best function in the conjugate kernel space of the network, as defined in\nDaniely, Frostig and Singer. The result holds for log-depth networks from a\nrich family of architectures. To the best of our knowledge, it is the first\npolynomial-time guarantee for the standard neural network learning algorithm\nfor networks of depth more that two.\n  As corollaries, it follows that for neural networks of any depth between $2$\nand $\\log(n)$, SGD is guaranteed to learn, in polynomial time, constant degree\npolynomials with polynomially bounded coefficients. Likewise, it follows that\nSGD on large enough networks can learn any continuous function (not in\npolynomial time), complementing classical expressivity results. \n\n"}
{"id": "1702.08835", "contents": "Title: Deep Forest Abstract: Current deep learning models are mostly build upon neural networks, i.e.,\nmultiple layers of parameterized differentiable nonlinear modules that can be\ntrained by backpropagation. In this paper, we explore the possibility of\nbuilding deep models based on non-differentiable modules. We conjecture that\nthe mystery behind the success of deep neural networks owes much to three\ncharacteristics, i.e., layer-by-layer processing, in-model feature\ntransformation and sufficient model complexity. We propose the gcForest\napproach, which generates \\textit{deep forest} holding these characteristics.\nThis is a decision tree ensemble approach, with much less hyper-parameters than\ndeep neural networks, and its model complexity can be automatically determined\nin a data-dependent way. Experiments show that its performance is quite robust\nto hyper-parameter settings, such that in most cases, even across different\ndata from different domains, it is able to get excellent performance by using\nthe same default setting. This study opens the door of deep learning based on\nnon-differentiable modules, and exhibits the possibility of constructing deep\nmodels without using backpropagation. \n\n"}
{"id": "1702.08882", "contents": "Title: Deep Semi-Random Features for Nonlinear Function Approximation Abstract: We propose semi-random features for nonlinear function approximation. The\nflexibility of semi-random feature lies between the fully adjustable units in\ndeep learning and the random features used in kernel methods. For one hidden\nlayer models with semi-random features, we prove with no unrealistic\nassumptions that the model classes contain an arbitrarily good function as the\nwidth increases (universality), and despite non-convexity, we can find such a\ngood function (optimization theory) that generalizes to unseen new data\n(generalization bound). For deep models, with no unrealistic assumptions, we\nprove universal approximation ability, a lower bound on approximation error, a\npartial optimization guarantee, and a generalization bound. Depending on the\nproblems, the generalization bound of deep semi-random features can be\nexponentially better than the known bounds of deep ReLU nets; our\ngeneralization error bound can be independent of the depth, the number of\ntrainable weights as well as the input dimensionality. In experiments, we show\nthat semi-random features can match the performance of neural networks by using\nslightly more units, and it outperforms random features by using significantly\nfewer units. Moreover, we introduce a new implicit ensemble method by using\nsemi-random features. \n\n"}
{"id": "1703.00060", "contents": "Title: Achieving non-discrimination in prediction Abstract: Discrimination-aware classification is receiving an increasing attention in\ndata science fields. The pre-process methods for constructing a\ndiscrimination-free classifier first remove discrimination from the training\ndata, and then learn the classifier from the cleaned data. However, they lack a\ntheoretical guarantee for the potential discrimination when the classifier is\ndeployed for prediction. In this paper, we fill this gap by mathematically\nbounding the probability of the discrimination in prediction being within a\ngiven interval in terms of the training data and classifier. We adopt the\ncausal model for modeling the data generation mechanism, and formally defining\ndiscrimination in population, in a dataset, and in prediction. We obtain two\nimportant theoretical results: (1) the discrimination in prediction can still\nexist even if the discrimination in the training data is completely removed;\nand (2) not all pre-process methods can ensure non-discrimination in prediction\neven though they can achieve non-discrimination in the modified training data.\nBased on the results, we develop a two-phase framework for constructing a\ndiscrimination-free classifier with a theoretical guarantee. The experiments\ndemonstrate the theoretical results and show the effectiveness of our two-phase\nframework. \n\n"}
{"id": "1703.00381", "contents": "Title: The Statistical Recurrent Unit Abstract: Sophisticated gated recurrent neural network architectures like LSTMs and\nGRUs have been shown to be highly effective in a myriad of applications. We\ndevelop an un-gated unit, the statistical recurrent unit (SRU), that is able to\nlearn long term dependencies in data by only keeping moving averages of\nstatistics. The SRU's architecture is simple, un-gated, and contains a\ncomparable number of parameters to LSTMs; yet, SRUs perform favorably to more\nsophisticated LSTM and GRU alternatives, often outperforming one or both in\nvarious tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an\nunbiased manner by optimizing respective architectures' hyperparameters in a\nBayesian optimization scheme for both synthetic and real-world tasks. \n\n"}
{"id": "1703.00868", "contents": "Title: Using Synthetic Data to Train Neural Networks is Model-Based Reasoning Abstract: We draw a formal connection between using synthetic training data to optimize\nneural network parameters and approximate, Bayesian, model-based reasoning. In\nparticular, training a neural network using synthetic data can be viewed as\nlearning a proposal distribution generator for approximate inference in the\nsynthetic-data generative model. We demonstrate this connection in a\nrecognition task where we develop a novel Captcha-breaking architecture and\ntrain it using synthetic data, demonstrating both state-of-the-art performance\nand a way of computing task-specific posterior uncertainty. Using a neural\nnetwork trained this way, we also demonstrate successful breaking of real-world\nCaptchas currently used by Facebook and Wikipedia. Reasoning from these\nempirical results and drawing connections with Bayesian modeling, we discuss\nthe robustness of synthetic data results and suggest important considerations\nfor ensuring good neural network generalization when training with synthetic\ndata. \n\n"}
{"id": "1703.01014", "contents": "Title: Active Learning for Cost-Sensitive Classification Abstract: We design an active learning algorithm for cost-sensitive multiclass\nclassification: problems where different errors have different costs. Our\nalgorithm, COAL, makes predictions by regressing to each label's cost and\npredicting the smallest. On a new example, it uses a set of regressors that\nperform well on past data to estimate possible costs for each label. It queries\nonly the labels that could be the best, ignoring the sure losers. We prove COAL\ncan be efficiently implemented for any regression family that admits squared\nloss optimization; it also enjoys strong guarantees with respect to predictive\nperformance and labeling effort. We empirically compare COAL to passive\nlearning and several active learning baselines, showing significant\nimprovements in labeling effort and test cost on real-world datasets. \n\n"}
{"id": "1703.01253", "contents": "Title: Machine Learning on Sequential Data Using a Recurrent Weighted Average Abstract: Recurrent Neural Networks (RNN) are a type of statistical model designed to\nhandle sequential data. The model reads a sequence one symbol at a time. Each\nsymbol is processed based on information collected from the previous symbols.\nWith existing RNN architectures, each symbol is processed using only\ninformation from the previous processing step. To overcome this limitation, we\npropose a new kind of RNN model that computes a recurrent weighted average\n(RWA) over every past processing step. Because the RWA can be computed as a\nrunning average, the computational overhead scales like that of any other RNN\narchitecture. The approach essentially reformulates the attention mechanism\ninto a stand-alone model. The performance of the RWA model is assessed on the\nvariable copy problem, the adding problem, classification of artificial\ngrammar, classification of sequences by length, and classification of the MNIST\nimages (where the pixels are read sequentially one at a time). On almost every\ntask, the RWA model is found to outperform a standard LSTM model. \n\n"}
{"id": "1703.01340", "contents": "Title: Generative Poisoning Attack Method Against Neural Networks Abstract: Poisoning attack is identified as a severe security threat to machine\nlearning algorithms. In many applications, for example, deep neural network\n(DNN) models collect public data as the inputs to perform re-training, where\nthe input data can be poisoned. Although poisoning attack against support\nvector machines (SVM) has been extensively studied before, there is still very\nlimited knowledge about how such attack can be implemented on neural networks\n(NN), especially DNNs. In this work, we first examine the possibility of\napplying traditional gradient-based method (named as the direct gradient\nmethod) to generate poisoned data against NNs by leveraging the gradient of the\ntarget model w.r.t. the normal data. We then propose a generative method to\naccelerate the generation rate of the poisoned data: an auto-encoder\n(generator) used to generate poisoned data is updated by a reward function of\nthe loss, and the target NN model (discriminator) receives the poisoned data to\ncalculate the loss w.r.t. the normal data. Our experiment results show that the\ngenerative method can speed up the poisoned data generation rate by up to\n239.38x compared with the direct gradient method, with slightly lower model\naccuracy degradation. A countermeasure is also designed to detect such\npoisoning attack methods by checking the loss of the target model. \n\n"}
{"id": "1703.01619", "contents": "Title: Neural Machine Translation and Sequence-to-sequence Models: A Tutorial Abstract: This tutorial introduces a new and powerful set of techniques variously\ncalled \"neural machine translation\" or \"neural sequence-to-sequence models\".\nThese techniques have been used in a number of tasks regarding the handling of\nhuman language, and can be a powerful tool in the toolbox of anyone who wants\nto model sequential data of some sort. The tutorial assumes that the reader\nknows the basics of math and programming, but does not assume any particular\nexperience with neural networks or natural language processing. It attempts to\nexplain the intuition behind the various methods covered, then delves into them\nwith enough mathematical detail to understand them concretely, and culiminates\nwith a suggestion for an implementation exercise, where readers can test that\nthey understood the content in practice. \n\n"}
{"id": "1703.01988", "contents": "Title: Neural Episodic Control Abstract: Deep reinforcement learning methods attain super-human performance in a wide\nrange of environments. Such methods are grossly inefficient, often taking\norders of magnitudes more data than humans to achieve reasonable performance.\nWe propose Neural Episodic Control: a deep reinforcement learning agent that is\nable to rapidly assimilate new experiences and act upon them. Our agent uses a\nsemi-tabular representation of the value function: a buffer of past experience\ncontaining slowly changing state representations and rapidly updated estimates\nof the value function. We show across a wide range of environments that our\nagent learns significantly faster than other state-of-the-art, general purpose\ndeep reinforcement learning agents. \n\n"}
{"id": "1703.02000", "contents": "Title: Activation Maximization Generative Adversarial Nets Abstract: Class labels have been empirically shown useful in improving the sample\nquality of generative adversarial nets (GANs). In this paper, we mathematically\nstudy the properties of the current variants of GANs that make use of class\nlabel information. With class aware gradient and cross-entropy decomposition,\nwe reveal how class labels and associated losses influence GAN's training.\nBased on that, we propose Activation Maximization Generative Adversarial\nNetworks (AM-GAN) as an advanced solution. Comprehensive experiments have been\nconducted to validate our analysis and evaluate the effectiveness of our\nsolution, where AM-GAN outperforms other strong baselines and achieves\nstate-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we\ndemonstrate that, with the Inception ImageNet classifier, Inception Score\nmainly tracks the diversity of the generator, and there is, however, no\nreliable evidence that it can reflect the true sample quality. We thus propose\na new metric, called AM Score, to provide a more accurate estimation of the\nsample quality. Our proposed model also outperforms the baseline methods in the\nnew metric. \n\n"}
{"id": "1703.03722", "contents": "Title: Recovery of Sparse and Low Rank Components of Matrices Using Iterative\n  Method with Adaptive Thresholding Abstract: In this letter, we propose an algorithm for recovery of sparse and low rank\ncomponents of matrices using an iterative method with adaptive thresholding. In\neach iteration, the low rank and sparse components are obtained using a\nthresholding operator. This algorithm is fast and can be implemented easily. We\ncompare it with one of the most common fast methods in which the rank and\nsparsity are approximated by $\\ell_1$ norm. We also apply it to some real\napplications where the noise is not so sparse. The simulation results show that\nit has a suitable performance with low run-time. \n\n"}
{"id": "1703.06222", "contents": "Title: A unified treatment of multiple testing with prior knowledge using the\n  p-filter Abstract: There is a significant literature on methods for incorporating knowledge into\nmultiple testing procedures so as to improve their power and precision. Some\ncommon forms of prior knowledge include (a) beliefs about which hypotheses are\nnull, modeled by non-uniform prior weights; (b) differing importances of\nhypotheses, modeled by differing penalties for false discoveries; (c) multiple\narbitrary partitions of the hypotheses into (possibly overlapping) groups; and\n(d) knowledge of independence, positive or arbitrary dependence between\nhypotheses or groups, suggesting the use of more aggressive or conservative\nprocedures. We present a unified algorithmic framework called p-filter for\nglobal null testing and false discovery rate (FDR) control that allows the\nscientist to incorporate all four types of prior knowledge (a)-(d)\nsimultaneously, recovering a variety of known algorithms as special cases. \n\n"}
{"id": "1703.08581", "contents": "Title: Sequence-to-Sequence Models Can Directly Translate Foreign Speech Abstract: We present a recurrent encoder-decoder deep neural network architecture that\ndirectly translates speech in one language into text in another. The model does\nnot explicitly transcribe the speech into text in the source language, nor does\nit require supervision from the ground truth source language transcription\nduring training. We apply a slightly modified sequence-to-sequence with\nattention architecture that has previously been used for speech recognition and\nshow that it can be repurposed for this more complex task, illustrating the\npower of attention-based models. A single model trained end-to-end obtains\nstate-of-the-art performance on the Fisher Callhome Spanish-English speech\ntranslation task, outperforming a cascade of independently trained\nsequence-to-sequence speech recognition and machine translation models by 1.8\nBLEU points on the Fisher test set. In addition, we find that making use of the\ntraining data in both languages by multi-task training sequence-to-sequence\nspeech translation and recognition models with a shared encoder network can\nimprove performance by a further 1.4 BLEU points. \n\n"}
{"id": "1703.09207", "contents": "Title: Fairness in Criminal Justice Risk Assessments: The State of the Art Abstract: Objectives: Discussions of fairness in criminal justice risk assessments\ntypically lack conceptual precision. Rhetoric too often substitutes for careful\nanalysis. In this paper, we seek to clarify the tradeoffs between different\nkinds of fairness and between fairness and accuracy.\n  Methods: We draw on the existing literatures in criminology, computer science\nand statistics to provide an integrated examination of fairness and accuracy in\ncriminal justice risk assessments. We also provide an empirical illustration\nusing data from arraignments.\n  Results: We show that there are at least six kinds of fairness, some of which\nare incompatible with one another and with accuracy.\n  Conclusions: Except in trivial cases, it is impossible to maximize accuracy\nand fairness at the same time, and impossible simultaneously to satisfy all\nkinds of fairness. In practice, a major complication is different base rates\nacross different legally protected groups. There is a need to consider\nchallenging tradeoffs. \n\n"}
{"id": "1703.09947", "contents": "Title: Efficient Private ERM for Smooth Objectives Abstract: In this paper, we consider efficient differentially private empirical risk\nminimization from the viewpoint of optimization algorithms. For strongly convex\nand smooth objectives, we prove that gradient descent with output perturbation\nnot only achieves nearly optimal utility, but also significantly improves the\nrunning time of previous state-of-the-art private optimization algorithms, for\nboth $\\epsilon$-DP and $(\\epsilon, \\delta)$-DP. For non-convex but smooth\nobjectives, we propose an RRPSGD (Random Round Private Stochastic Gradient\nDescent) algorithm, which provably converges to a stationary point with privacy\nguarantee. Besides the expected utility bounds, we also provide guarantees in\nhigh probability form. Experiments demonstrate that our algorithm consistently\noutperforms existing method in both utility and running time. \n\n"}
{"id": "1703.10444", "contents": "Title: On Fundamental Limits of Robust Learning Abstract: We consider the problems of robust PAC learning from distributed and\nstreaming data, which may contain malicious errors and outliers, and analyze\ntheir fundamental complexity questions. In particular, we establish lower\nbounds on the communication complexity for distributed robust learning\nperformed on multiple machines, and on the space complexity for robust learning\nfrom streaming data on a single machine. These results demonstrate that gaining\nrobustness of learning algorithms is usually at the expense of increased\ncomplexities. As far as we know, this work gives the first complexity results\nfor distributed and online robust PAC learning. \n\n"}
{"id": "1704.00520", "contents": "Title: Efficient acquisition rules for model-based approximate Bayesian\n  computation Abstract: Approximate Bayesian computation (ABC) is a method for Bayesian inference\nwhen the likelihood is unavailable but simulating from the model is possible.\nHowever, many ABC algorithms require a large number of simulations, which can\nbe costly. To reduce the computational cost, Bayesian optimisation (BO) and\nsurrogate models such as Gaussian processes have been proposed. Bayesian\noptimisation enables one to intelligently decide where to evaluate the model\nnext but common BO strategies are not designed for the goal of estimating the\nposterior distribution. Our paper addresses this gap in the literature. We\npropose to compute the uncertainty in the ABC posterior density, which is due\nto a lack of simulations to estimate this quantity accurately, and define a\nloss function that measures this uncertainty. We then propose to select the\nnext evaluation location to minimise the expected loss. Experiments show that\nthe proposed method often produces the most accurate approximations as compared\nto common BO strategies. \n\n"}
{"id": "1704.02124", "contents": "Title: Jet Constituents for Deep Neural Network Based Top Quark Tagging Abstract: Recent literature on deep neural networks for tagging of highly energetic\njets resulting from top quark decays has focused on image based techniques or\nmultivariate approaches using high-level jet substructure variables. Here, a\nsequential approach to this task is taken by using an ordered sequence of jet\nconstituents as training inputs. Unlike the majority of previous approaches,\nthis strategy does not result in a loss of information during pixelisation or\nthe calculation of high level features. The jet classification method achieves\na background rejection of 45 at a 50% efficiency operating point for\nreconstruction level jets with transverse momentum range of 600 to 2500 GeV and\nis insensitive to multiple proton-proton interactions at the levels expected\nthroughout Run 2 of the LHC. \n\n"}
{"id": "1704.04235", "contents": "Title: Close Yet Distinctive Domain Adaptation Abstract: Domain adaptation is transfer learning which aims to generalize a learning\nmodel across training and testing data with different distributions. Most\nprevious research tackle this problem in seeking a shared feature\nrepresentation between source and target domains while reducing the mismatch of\ntheir data distributions. In this paper, we propose a close yet discriminative\ndomain adaptation method, namely CDDA, which generates a latent feature\nrepresentation with two interesting properties. First, the discrepancy between\nthe source and target domain, measured in terms of both marginal and\nconditional probability distribution via Maximum Mean Discrepancy is minimized\nso as to attract two domains close to each other. More importantly, we also\ndesign a repulsive force term, which maximizes the distances between each label\ndependent sub-domain to all others so as to drag different class dependent\nsub-domains far away from each other and thereby increase the discriminative\npower of the adapted domain. Moreover, given the fact that the underlying data\nmanifold could have complex geometric structure, we further propose the\nconstraints of label smoothness and geometric structure consistency for label\npropagation. Extensive experiments are conducted on 36 cross-domain image\nclassification tasks over four public datasets. The comprehensive results show\nthat the proposed method consistently outperforms the state-of-the-art methods\nwith significant margins. \n\n"}
{"id": "1704.05409", "contents": "Title: Ranking to Learn: Feature Ranking and Selection via Eigenvector\n  Centrality Abstract: In an era where accumulating data is easy and storing it inexpensive, feature\nselection plays a central role in helping to reduce the high-dimensionality of\nhuge amounts of otherwise meaningless data. In this paper, we propose a\ngraph-based method for feature selection that ranks features by identifying the\nmost important ones into arbitrary set of cues. Mapping the problem on an\naffinity graph-where features are the nodes-the solution is given by assessing\nthe importance of nodes through some indicators of centrality, in particular,\nthe Eigen-vector Centrality (EC). The gist of EC is to estimate the importance\nof a feature as a function of the importance of its neighbors. Ranking central\nnodes individuates candidate features, which turn out to be effective from a\nclassification point of view, as proved by a thoroughly experimental section.\nOur approach has been tested on 7 diverse datasets from recent literature\n(e.g., biological data and object recognition, among others), and compared\nagainst filter, embedded and wrappers methods. The results are remarkable in\nterms of accuracy, stability and low execution time. \n\n"}
{"id": "1704.05712", "contents": "Title: Universal Adversarial Perturbations Against Semantic Image Segmentation Abstract: While deep learning is remarkably successful on perceptual tasks, it was also\nshown to be vulnerable to adversarial perturbations of the input. These\nperturbations denote noise added to the input that was generated specifically\nto fool the system while being quasi-imperceptible for humans. More severely,\nthere even exist universal perturbations that are input-agnostic but fool the\nnetwork on the majority of inputs. While recent work has focused on image\nclassification, this work proposes attacks against semantic image segmentation:\nwe present an approach for generating (universal) adversarial perturbations\nthat make the network yield a desired target segmentation as output. We show\nempirically that there exist barely perceptible universal noise patterns which\nresult in nearly the same predicted segmentation for arbitrary inputs.\nFurthermore, we also show the existence of universal noise which removes a\ntarget class (e.g., all pedestrians) from the segmentation while leaving the\nsegmentation mostly unchanged otherwise. \n\n"}
{"id": "1704.06363", "contents": "Title: Hard Mixtures of Experts for Large Scale Weakly Supervised Vision Abstract: Training convolutional networks (CNN's) that fit on a single GPU with\nminibatch stochastic gradient descent has become effective in practice.\nHowever, there is still no effective method for training large CNN's that do\nnot fit in the memory of a few GPU cards, or for parallelizing CNN training. In\nthis work we show that a simple hard mixture of experts model can be\nefficiently trained to good effect on large scale hashtag (multilabel)\nprediction tasks. Mixture of experts models are not new (Jacobs et. al. 1991,\nCollobert et. al. 2003), but in the past, researchers have had to devise\nsophisticated methods to deal with data fragmentation. We show empirically that\nmodern weakly supervised data sets are large enough to support naive\npartitioning schemes where each data point is assigned to a single expert.\nBecause the experts are independent, training them in parallel is easy, and\nevaluation is cheap for the size of the model. Furthermore, we show that we can\nuse a single decoding layer for all the experts, allowing a unified feature\nembedding space. We demonstrate that it is feasible (and in fact relatively\npainless) to train far larger models than could be practically trained with\nstandard CNN architectures, and that the extra capacity can be well used on\ncurrent datasets. \n\n"}
{"id": "1704.06656", "contents": "Title: Feature selection algorithm based on Catastrophe model to improve the\n  performance of regression analysis Abstract: In this paper we introduce a new feature selection algorithm to remove the\nirrelevant or redundant features in the data sets. In this algorithm the\nimportance of a feature is based on its fitting to the Catastrophe model.\nAkaike information crite- rion value is used for ranking the features in the\ndata set. The proposed algorithm is compared with well-known RELIEF feature\nselection algorithm. Breast Cancer, Parkinson Telemonitoring data and Slice\nlocality data sets are used to evaluate the model. \n\n"}
{"id": "1704.07520", "contents": "Title: Stein Variational Gradient Descent as Gradient Flow Abstract: Stein variational gradient descent (SVGD) is a deterministic sampling\nalgorithm that iteratively transports a set of particles to approximate given\ndistributions, based on an efficient gradient-based update that guarantees to\noptimally decrease the KL divergence within a function space. This paper\ndevelops the first theoretical analysis on SVGD, discussing its weak\nconvergence properties and showing that its asymptotic behavior is captured by\na gradient flow of the KL divergence functional under a new metric structure\ninduced by Stein operator. We also provide a number of results on Stein\noperator and Stein's identity using the notion of weak derivative, including a\nnew proof of the distinguishability of Stein discrepancy under weak conditions. \n\n"}
{"id": "1704.08165", "contents": "Title: A Generalization of Convolutional Neural Networks to Graph-Structured\n  Data Abstract: This paper introduces a generalization of Convolutional Neural Networks\n(CNNs) from low-dimensional grid data, such as images, to graph-structured\ndata. We propose a novel spatial convolution utilizing a random walk to uncover\nthe relations within the input, analogous to the way the standard convolution\nuses the spatial neighborhood of a pixel on the grid. The convolution has an\nintuitive interpretation, is efficient and scalable and can also be used on\ndata with varying graph structure. Furthermore, this generalization can be\napplied to many standard regression or classification problems, by learning the\nthe underlying graph. We empirically demonstrate the performance of the\nproposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular\nactivity data set. \n\n"}
{"id": "1704.08727", "contents": "Title: Structured Sparse Modelling with Hierarchical GP Abstract: In this paper a new Bayesian model for sparse linear regression with a\nspatio-temporal structure is proposed. It incorporates the structural\nassumptions based on a hierarchical Gaussian process prior for spike and slab\ncoefficients. We design an inference algorithm based on Expectation Propagation\nand evaluate the model over the real data. \n\n"}
{"id": "1704.08770", "contents": "Title: Detecting Casimir torque with an optically levitated nanorod Abstract: The linear momentum and angular momentum of virtual photons of quantum vacuum\nfluctuations can induce the Casimir force and the Casimir torque, respectively.\nWhile the Casimir force has been measured extensively, the Casimir torque has\nnot been observed experimentally though it was predicted over forty years ago.\nHere we propose to detect the Casimir torque with an optically levitated\nnanorod near a birefringent plate in vacuum. The axis of the nanorod tends to\nalign with the polarization direction of the linearly polarized optical\ntweezer. When its axis is not parallel or perpendicular to the optical axis of\nthe birefringent crystal, it will experience a Casimir torque that shifts its\norientation slightly. We calculate the Casimir torque and Casimir force acting\non a levitated nanorod near a birefringent crystal. We also investigate the\neffects of thermal noise and photon recoils on the torque and force detection.\nWe prove that a levitated nanorod in vacuum will be capable of detecting the\nCasimir torque under realistic conditions. \n\n"}
{"id": "1704.08792", "contents": "Title: DeepArchitect: Automatically Designing and Training Deep Architectures Abstract: In deep learning, performance is strongly affected by the choice of\narchitecture and hyperparameters. While there has been extensive work on\nautomatic hyperparameter optimization for simple spaces, complex spaces such as\nthe space of deep architectures remain largely unexplored. As a result, the\nchoice of architecture is done manually by the human expert through a slow\ntrial and error process guided mainly by intuition. In this paper we describe a\nframework for automatically designing and training deep models. We propose an\nextensible and modular language that allows the human expert to compactly\nrepresent complex search spaces over architectures and their hyperparameters.\nThe resulting search spaces are tree-structured and therefore easy to traverse.\nModels can be automatically compiled to computational graphs once values for\nall hyperparameters have been chosen. We can leverage the structure of the\nsearch space to introduce different model search algorithms, such as random\nsearch, Monte Carlo tree search (MCTS), and sequential model-based optimization\n(SMBO). We present experiments comparing the different algorithms on CIFAR-10\nand show that MCTS and SMBO outperform random search. In addition, these\nexperiments show that our framework can be used effectively for model\ndiscovery, as it is possible to describe expressive search spaces and discover\ncompetitive models without much effort from the human expert. Code for our\nframework and experiments has been made publicly available. \n\n"}
{"id": "1704.08829", "contents": "Title: Deep Feature Learning for Graphs Abstract: This paper presents a general graph representation learning framework called\nDeepGL for learning deep node and edge representations from large (attributed)\ngraphs. In particular, DeepGL begins by deriving a set of base features (e.g.,\ngraphlet features) and automatically learns a multi-layered hierarchical graph\nrepresentation where each successive layer leverages the output from the\nprevious layer to learn features of a higher-order. Contrary to previous work,\nDeepGL learns relational functions (each representing a feature) that\ngeneralize across-networks and therefore useful for graph-based transfer\nlearning tasks. Moreover, DeepGL naturally supports attributed graphs, learns\ninterpretable features, and is space-efficient (by learning sparse feature\nvectors). In addition, DeepGL is expressive, flexible with many interchangeable\ncomponents, efficient with a time complexity of $\\mathcal{O}(|E|)$, and\nscalable for large networks via an efficient parallel implementation. Compared\nwith the state-of-the-art method, DeepGL is (1) effective for across-network\ntransfer learning tasks and attributed graph representation learning, (2)\nspace-efficient requiring up to 6x less memory, (3) fast with up to 182x\nspeedup in runtime performance, and (4) accurate with an average improvement of\n20% or more on many learning tasks. \n\n"}
{"id": "1705.02928", "contents": "Title: Cross-label Suppression: A Discriminative and Fast Dictionary Learning\n  with Group Regularization Abstract: This paper addresses image classification through learning a compact and\ndiscriminative dictionary efficiently. Given a structured dictionary with each\natom (columns in the dictionary matrix) related to some label, we propose\ncross-label suppression constraint to enlarge the difference among\nrepresentations for different classes. Meanwhile, we introduce group\nregularization to enforce representations to preserve label properties of\noriginal samples, meaning the representations for the same class are encouraged\nto be similar. Upon the cross-label suppression, we don't resort to\nfrequently-used $\\ell_0$-norm or $\\ell_1$-norm for coding, and obtain\ncomputational efficiency without losing the discriminative power for\ncategorization. Moreover, two simple classification schemes are also developed\nto take full advantage of the learnt dictionary. Extensive experiments on six\ndata sets including face recognition, object categorization, scene\nclassification, texture recognition and sport action categorization are\nconducted, and the results show that the proposed approach can outperform lots\nof recently presented dictionary algorithms on both recognition accuracy and\ncomputational efficiency. \n\n"}
{"id": "1705.06412", "contents": "Title: Sample-Efficient Algorithms for Recovering Structured Signals from\n  Magnitude-Only Measurements Abstract: We consider the problem of recovering a signal $\\mathbf{x}^* \\in\n\\mathbf{R}^n$, from magnitude-only measurements $y_i =\n|\\left\\langle\\mathbf{a}_i,\\mathbf{x}^*\\right\\rangle|$ for $i=[m]$. Also called\nthe phase retrieval, this is a fundamental challenge in bio-,astronomical\nimaging and speech processing. The problem above is ill-posed; additional\nassumptions on the signal and/or the measurements are necessary. In this paper\nwe first study the case where the signal $\\mathbf{x}^*$ is $s$-sparse. We\ndevelop a novel algorithm that we call Compressive Phase Retrieval with\nAlternating Minimization, or CoPRAM. Our algorithm is simple; it combines the\nclassical alternating minimization approach for phase retrieval with the CoSaMP\nalgorithm for sparse recovery. Despite its simplicity, we prove that CoPRAM\nachieves a sample complexity of $O(s^2\\log n)$ with Gaussian measurements\n$\\mathbf{a}_i$, matching the best known existing results; moreover, it\ndemonstrates linear convergence in theory and practice. Additionally, it\nrequires no extra tuning parameters other than signal sparsity $s$ and is\nrobust to noise. When the sorted coefficients of the sparse signal exhibit a\npower law decay, we show that CoPRAM achieves a sample complexity of $O(s\\log\nn)$, which is close to the information-theoretic limit. We also consider the\ncase where the signal $\\mathbf{x}^*$ arises from structured sparsity models. We\nspecifically examine the case of block-sparse signals with uniform block size\nof $b$ and block sparsity $k=s/b$. For this problem, we design a recovery\nalgorithm Block CoPRAM that further reduces the sample complexity to $O(ks\\log\nn)$. For sufficiently large block lengths of $b=\\Theta(s)$, this bound equates\nto $O(s\\log n)$. To our knowledge, this constitutes the first end-to-end\nalgorithm for phase retrieval where the Gaussian sample complexity has a\nsub-quadratic dependence on the signal sparsity level. \n\n"}
{"id": "1705.07109", "contents": "Title: Deep adversarial neural decoding Abstract: Here, we present a novel approach to solve the problem of reconstructing\nperceived stimuli from brain responses by combining probabilistic inference\nwith deep learning. Our approach first inverts the linear transformation from\nlatent features to brain responses with maximum a posteriori estimation and\nthen inverts the nonlinear transformation from perceived stimuli to latent\nfeatures with adversarial training of convolutional neural networks. We test\nour approach with a functional magnetic resonance imaging experiment and show\nthat it can generate state-of-the-art reconstructions of perceived faces from\nbrain activations. \n\n"}
{"id": "1705.07606", "contents": "Title: Guide Actor-Critic for Continuous Control Abstract: Actor-critic methods solve reinforcement learning problems by updating a\nparameterized policy known as an actor in a direction that increases an\nestimate of the expected return known as a critic. However, existing\nactor-critic methods only use values or gradients of the critic to update the\npolicy parameter. In this paper, we propose a novel actor-critic method called\nthe guide actor-critic (GAC). GAC firstly learns a guide actor that locally\nmaximizes the critic and then it updates the policy parameter based on the\nguide actor by supervised learning. Our main theoretical contributions are two\nfolds. First, we show that GAC updates the guide actor by performing\nsecond-order optimization in the action space where the curvature matrix is\nbased on the Hessians of the critic. Second, we show that the deterministic\npolicy gradient method is a special case of GAC when the Hessians are ignored.\nThrough experiments, we show that our method is a promising reinforcement\nlearning method for continuous controls. \n\n"}
{"id": "1705.07815", "contents": "Title: Minimax Statistical Learning with Wasserstein Distances Abstract: As opposed to standard empirical risk minimization (ERM), distributionally\nrobust optimization aims to minimize the worst-case risk over a larger\nambiguity set containing the original empirical distribution of the training\ndata. In this work, we describe a minimax framework for statistical learning\nwith ambiguity sets given by balls in Wasserstein space. In particular, we\nprove generalization bounds that involve the covering number properties of the\noriginal ERM problem. As an illustrative example, we provide generalization\nguarantees for transport-based domain adaptation problems where the Wasserstein\ndistance between the source and target domain distributions can be reliably\nestimated from unlabeled samples. \n\n"}
{"id": "1705.08564", "contents": "Title: Towards Interrogating Discriminative Machine Learning Models Abstract: It is oftentimes impossible to understand how machine learning models reach a\ndecision. While recent research has proposed various technical approaches to\nprovide some clues as to how a learning model makes individual decisions, they\ncannot provide users with ability to inspect a learning model as a complete\nentity. In this work, we propose a new technical approach that augments a\nBayesian regression mixture model with multiple elastic nets. Using the\nenhanced mixture model, we extract explanations for a target model through\nglobal approximation. To demonstrate the utility of our approach, we evaluate\nit on different learning models covering the tasks of text mining and image\nrecognition. Our results indicate that the proposed approach not only\noutperforms the state-of-the-art technique in explaining individual decisions\nbut also provides users with an ability to discover the vulnerabilities of a\nlearning model. \n\n"}
{"id": "1705.08848", "contents": "Title: Joint Distribution Optimal Transportation for Domain Adaptation Abstract: This paper deals with the unsupervised domain adaptation problem, where one\nwants to estimate a prediction function $f$ in a given target domain without\nany labeled sample by exploiting the knowledge available from a source domain\nwhere labels are known. Our work makes the following assumption: there exists a\nnon-linear transformation between the joint feature/label space distributions\nof the two domain $\\mathcal{P}_s$ and $\\mathcal{P}_t$. We propose a solution of\nthis problem with optimal transport, that allows to recover an estimated target\n$\\mathcal{P}^f_t=(X,f(X))$ by optimizing simultaneously the optimal coupling\nand $f$. We show that our method corresponds to the minimization of a bound on\nthe target error, and provide an efficient algorithmic solution, for which\nconvergence is proved. The versatility of our approach, both in terms of class\nof hypothesis or loss functions is demonstrated with real world classification\nand regression problems, for which we reach or surpass state-of-the-art\nresults. \n\n"}
{"id": "1705.08918", "contents": "Title: Unsupervised Learning Layers for Video Analysis Abstract: This paper presents two unsupervised learning layers (UL layers) for\nlabel-free video analysis: one for fully connected layers, and the other for\nconvolutional ones. The proposed UL layers can play two roles: they can be the\ncost function layer for providing global training signal; meanwhile they can be\nadded to any regular neural network layers for providing local training signals\nand combined with the training signals backpropagated from upper layers for\nextracting both slow and fast changing features at layers of different depths.\nTherefore, the UL layers can be used in either pure unsupervised or\nsemi-supervised settings. Both a closed-form solution and an online learning\nalgorithm for two UL layers are provided. Experiments with unlabeled synthetic\nand real-world videos demonstrated that the neural networks equipped with UL\nlayers and trained with the proposed online learning algorithm can extract\nshape and motion information from video sequences of moving objects. The\nexperiments demonstrated the potential applications of UL layers and online\nlearning algorithm to head orientation estimation and moving object\nlocalization. \n\n"}
{"id": "1705.08997", "contents": "Title: State Space Decomposition and Subgoal Creation for Transfer in Deep\n  Reinforcement Learning Abstract: Typical reinforcement learning (RL) agents learn to complete tasks specified\nby reward functions tailored to their domain. As such, the policies they learn\ndo not generalize even to similar domains. To address this issue, we develop a\nframework through which a deep RL agent learns to generalize policies from\nsmaller, simpler domains to more complex ones using a recurrent attention\nmechanism. The task is presented to the agent as an image and an instruction\nspecifying the goal. This meta-controller guides the agent towards its goal by\ndesigning a sequence of smaller subtasks on the part of the state space within\nthe attention, effectively decomposing it. As a baseline, we consider a setup\nwithout attention as well. Our experiments show that the meta-controller learns\nto create subgoals within the attention. \n\n"}
{"id": "1705.09056", "contents": "Title: Can Decentralized Algorithms Outperform Centralized Algorithms? A Case\n  Study for Decentralized Parallel Stochastic Gradient Descent Abstract: Most distributed machine learning systems nowadays, including TensorFlow and\nCNTK, are built in a centralized fashion. One bottleneck of centralized\nalgorithms lies on high communication cost on the central node. Motivated by\nthis, we ask, can decentralized algorithms be faster than its centralized\ncounterpart?\n  Although decentralized PSGD (D-PSGD) algorithms have been studied by the\ncontrol community, existing analysis and theory do not show any advantage over\ncentralized PSGD (C-PSGD) algorithms, simply assuming the application scenario\nwhere only the decentralized network is available. In this paper, we study a\nD-PSGD algorithm and provide the first theoretical analysis that indicates a\nregime in which decentralized algorithms might outperform centralized\nalgorithms for distributed stochastic gradient descent. This is because D-PSGD\nhas comparable total computational complexities to C-PSGD but requires much\nless communication cost on the busiest node. We further conduct an empirical\nstudy to validate our theoretical analysis across multiple frameworks (CNTK and\nTorch), different network configurations, and computation platforms up to 112\nGPUs. On network configurations with low bandwidth or high latency, D-PSGD can\nbe up to one order of magnitude faster than its well-optimized centralized\ncounterparts. \n\n"}
{"id": "1705.09280", "contents": "Title: Implicit Regularization in Matrix Factorization Abstract: We study implicit regularization when optimizing an underdetermined quadratic\nobjective over a matrix $X$ with gradient descent on a factorization of $X$. We\nconjecture and provide empirical and theoretical evidence that with small\nenough step sizes and initialization close enough to the origin, gradient\ndescent on a full dimensional factorization converges to the minimum nuclear\nnorm solution. \n\n"}
{"id": "1705.09558", "contents": "Title: Bayesian GAN Abstract: Generative adversarial networks (GANs) can implicitly learn rich\ndistributions over images, audio, and data which are hard to model with an\nexplicit likelihood. We present a practical Bayesian formulation for\nunsupervised and semi-supervised learning with GANs. Within this framework, we\nuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of\nthe generator and discriminator networks. The resulting approach is\nstraightforward and obtains good performance without any standard interventions\nsuch as feature matching, or mini-batch discrimination. By exploring an\nexpressive posterior over the parameters of the generator, the Bayesian GAN\navoids mode-collapse, produces interpretable and diverse candidate samples, and\nprovides state-of-the-art quantitative results for semi-supervised learning on\nbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,\nWasserstein GANs, and DCGAN ensembles. \n\n"}
{"id": "1705.09605", "contents": "Title: Combinatorial Multi-Armed Bandits with Filtered Feedback Abstract: Motivated by problems in search and detection we present a solution to a\nCombinatorial Multi-Armed Bandit (CMAB) problem with both heavy-tailed reward\ndistributions and a new class of feedback, filtered semibandit feedback. In a\nCMAB problem an agent pulls a combination of arms from a set $\\{1,...,k\\}$ in\neach round, generating random outcomes from probability distributions\nassociated with these arms and receiving an overall reward. Under semibandit\nfeedback it is assumed that the random outcomes generated are all observed.\nFiltered semibandit feedback allows the outcomes that are observed to be\nsampled from a second distribution conditioned on the initial random outcomes.\nThis feedback mechanism is valuable as it allows CMAB methods to be applied to\nsequential search and detection problems where combinatorial actions are made,\nbut the true rewards (number of objects of interest appearing in the round) are\nnot observed, rather a filtered reward (the number of objects the searcher\nsuccessfully finds, which must by definition be less than the number that\nappear). We present an upper confidence bound type algorithm, Robust-F-CUCB,\nand associated regret bound of order $\\mathcal{O}(\\ln(n))$ to balance\nexploration and exploitation in the face of both filtering of reward and heavy\ntailed reward distributions. \n\n"}
{"id": "1705.09620", "contents": "Title: Discriminative Metric Learning with Deep Forest Abstract: A Discriminative Deep Forest (DisDF) as a metric learning algorithm is\nproposed in the paper. It is based on the Deep Forest or gcForest proposed by\nZhou and Feng and can be viewed as a gcForest modification. The case of the\nfully supervised learning is studied when the class labels of individual\ntraining examples are known. The main idea underlying the algorithm is to\nassign weights to decision trees in random forest in order to reduce distances\nbetween objects from the same class and to increase them between objects from\ndifferent classes. The weights are training parameters. A specific objective\nfunction which combines Euclidean and Manhattan distances and simplifies the\noptimization problem for training the DisDF is proposed. The numerical\nexperiments illustrate the proposed distance metric algorithm. \n\n"}
{"id": "1705.09640", "contents": "Title: Photodetector figures of merit in terms of POVMs Abstract: A photodetector may be characterized by various figures of merit such as\nresponse time, bandwidth, dark count rate, efficiency, wavelength resolution,\nand photon-number resolution. On the other hand, quantum theory says that any\nmeasurement device is fully described by its POVM, which stands for\nPositive-Operator-Valued Measure, and which generalizes the textbook notion of\nthe eigenstates of the appropriate hermitian operator (the \"observable\") as\nmeasurement outcomes. Here we show how to define a multitude of photodetector\nfigures of merit in terms of a given POVM. We distinguish classical and quantum\nfigures of merit and issue a conjecture regarding trade-off relations between\nthem. We discuss the relationship between POVM elements and photodetector\nclicks, and how models of photodetectors may be tested by measuring either POVM\nelements or figures of merit. Finally, the POVM is advertised as a\nplatform-independent way of comparing different types of photodetectors, since\nany such POVM refers to the Hilbert space of the incoming light, and not to any\nHilbert space internal to the detector. \n\n"}
{"id": "1705.09952", "contents": "Title: Optimal sequential treatment allocation Abstract: In treatment allocation problems the individuals to be treated often arrive\nsequentially. We study a problem in which the policy maker is not only\ninterested in the expected cumulative welfare but is also concerned about the\nuncertainty/risk of the treatment outcomes. At the outset, the total number of\ntreatment assignments to be made may even be unknown. A sequential treatment\npolicy which attains the minimax optimal regret is proposed. We also\ndemonstrate that the expected number of suboptimal treatments only grows slowly\nin the number of treatments. Finally, we study a setting where outcomes are\nonly observed with delay. \n\n"}
{"id": "1705.10762", "contents": "Title: Generative Models of Visually Grounded Imagination Abstract: It is easy for people to imagine what a man with pink hair looks like, even\nif they have never seen such a person before. We call the ability to create\nimages of novel semantic concepts visually grounded imagination. In this paper,\nwe show how we can modify variational auto-encoders to perform this task. Our\nmethod uses a novel training objective, and a novel product-of-experts\ninference network, which can handle partially specified (abstract) concepts in\na principled and efficient way. We also propose a set of easy-to-compute\nevaluation metrics that capture our intuitive notions of what it means to have\ngood visual imagination, namely correctness, coverage, and compositionality\n(the 3 C's). Finally, we perform a detailed comparison of our method with two\nexisting joint image-attribute VAE methods (the JMVAE method of Suzuki et.al.\nand the BiVCCA method of Wang et.al.) by applying them to two datasets: the\nMNIST-with-attributes dataset (which we introduce here), and the CelebA\ndataset. \n\n"}
{"id": "1706.01566", "contents": "Title: Open Loop Hyperparameter Optimization and Determinantal Point Processes Abstract: Driven by the need for parallelizable hyperparameter optimization methods,\nthis paper studies \\emph{open loop} search methods: sequences that are\npredetermined and can be generated before a single configuration is evaluated.\nExamples include grid search, uniform random search, low discrepancy sequences,\nand other sampling distributions. In particular, we propose the use of\n$k$-determinantal point processes in hyperparameter optimization via random\nsearch. Compared to conventional uniform random search where hyperparameter\nsettings are sampled independently, a $k$-DPP promotes diversity. We describe\nan approach that transforms hyperparameter search spaces for efficient use with\na $k$-DPP. In addition, we introduce a novel Metropolis-Hastings algorithm\nwhich can sample from $k$-DPPs defined over any space from which uniform\nsamples can be drawn, including spaces with a mixture of discrete and\ncontinuous dimensions or tree structure. Our experiments show significant\nbenefits in realistic scenarios with a limited budget for training supervised\nlearners, whether in serial or parallel. \n\n"}
{"id": "1706.01724", "contents": "Title: Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic\n  Gradient Riemannian MCMC Abstract: It is challenging to develop stochastic gradient based scalable inference for\ndeep discrete latent variable models (LVMs), due to the difficulties in not\nonly computing the gradients, but also adapting the step sizes to different\nlatent factors and hidden layers. For the Poisson gamma belief network (PGBN),\na recently proposed deep discrete LVM, we derive an alternative representation\nthat is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data\naugmentation and marginalization techniques, we derive a block-diagonal Fisher\ninformation matrix and its inverse for the simplex-constrained global model\nparameters of DLDA. Exploiting that Fisher information matrix with stochastic\ngradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian\n(TLASGR) MCMC that jointly learns simplex-constrained global parameters across\nall layers and topics, with topic and layer specific learning rates.\nState-of-the-art results are demonstrated on big data sets. \n\n"}
{"id": "1706.01983", "contents": "Title: Deep Learning: Generalization Requires Deep Compositional Feature Space\n  Design Abstract: Generalization error defines the discriminability and the representation\npower of a deep model. In this work, we claim that feature space design using\ndeep compositional function plays a significant role in generalization along\nwith explicit and implicit regularizations. Our claims are being established\nwith several image classification experiments. We show that the information\nloss due to convolution and max pooling can be marginalized with the\ncompositional design, improving generalization performance. Also, we will show\nthat learning rate decay acts as an implicit regularizer in deep model\ntraining. \n\n"}
{"id": "1706.02419", "contents": "Title: Estimating Mixture Entropy with Pairwise Distances Abstract: Mixture distributions arise in many parametric and non-parametric settings --\nfor example, in Gaussian mixture models and in non-parametric estimation. It is\noften necessary to compute the entropy of a mixture, but, in most cases, this\nquantity has no closed-form expression, making some form of approximation\nnecessary. We propose a family of estimators based on a pairwise distance\nfunction between mixture components, and show that this estimator class has\nmany attractive properties. For many distributions of interest, the proposed\nestimators are efficient to compute, differentiable in the mixture parameters,\nand become exact when the mixture components are clustered. We prove this\nfamily includes lower and upper bounds on the mixture entropy. The Chernoff\n$\\alpha$-divergence gives a lower bound when chosen as the distance function,\nwith the Bhattacharyya distance providing the tightest lower bound for\ncomponents that are symmetric and members of a location family. The\nKullback-Leibler divergence gives an upper bound when used as the distance\nfunction. We provide closed-form expressions of these bounds for mixtures of\nGaussians, and discuss their applications to the estimation of mutual\ninformation. We then demonstrate that our bounds are significantly tighter than\nwell-known existing bounds using numeric simulations. This estimator class is\nvery useful in optimization problems involving maximization/minimization of\nentropy and mutual information, such as MaxEnt and rate distortion problems. \n\n"}
{"id": "1706.02582", "contents": "Title: Clustering with t-SNE, provably Abstract: t-distributed Stochastic Neighborhood Embedding (t-SNE), a clustering and\nvisualization method proposed by van der Maaten & Hinton in 2008, has rapidly\nbecome a standard tool in a number of natural sciences. Despite its\noverwhelming success, there is a distinct lack of mathematical foundations and\nthe inner workings of the algorithm are not well understood. The purpose of\nthis paper is to prove that t-SNE is able to recover well-separated clusters;\nmore precisely, we prove that t-SNE in the `early exaggeration' phase, an\noptimization technique proposed by van der Maaten & Hinton (2008) and van der\nMaaten (2014), can be rigorously analyzed. As a byproduct, the proof suggests\nnovel ways for setting the exaggeration parameter $\\alpha$ and step size $h$.\nNumerical examples illustrate the effectiveness of these rules: in particular,\nthe quality of embedding of topological structures (e.g. the swiss roll)\nimproves. We also discuss a connection to spectral clustering methods. \n\n"}
{"id": "1706.02761", "contents": "Title: Gated Orthogonal Recurrent Units: On Learning to Forget Abstract: We present a novel recurrent neural network (RNN) based model that combines\nthe remembering ability of unitary RNNs with the ability of gated RNNs to\neffectively forget redundant/irrelevant information in its memory. We achieve\nthis by extending unitary RNNs with a gating mechanism. Our model is able to\noutperform LSTMs, GRUs and Unitary RNNs on several long-term dependency\nbenchmark tasks. We empirically both show the orthogonal/unitary RNNs lack the\nability to forget and also the ability of GORU to simultaneously remember long\nterm dependencies while forgetting irrelevant information. This plays an\nimportant role in recurrent neural networks. We provide competitive results\nalong with an analysis of our model on many natural sequential tasks including\nthe bAbI Question Answering, TIMIT speech spectrum prediction, Penn TreeBank,\nand synthetic tasks that involve long-term dependencies such as algorithmic,\nparenthesis, denoising and copying tasks. \n\n"}
{"id": "1706.02999", "contents": "Title: Symmetry Learning for Function Approximation in Reinforcement Learning Abstract: In this paper we explore methods to exploit symmetries for ensuring sample\nefficiency in reinforcement learning (RL), this problem deserves ever\nincreasing attention with the recent advances in the use of deep networks for\ncomplex RL tasks which require large amount of training data. We introduce a\nnovel method to detect symmetries using reward trails observed during episodic\nexperience and prove its completeness. We also provide a framework to\nincorporate the discovered symmetries for functional approximation. Finally we\nshow that the use of potential based reward shaping is especially effective for\nour symmetry exploitation mechanism. Experiments on various classical problems\nshow that our method improves the learning performance significantly by\nutilizing symmetry information. \n\n"}
{"id": "1706.03149", "contents": "Title: An Expectation-Maximization Algorithm for the Fractal Inverse Problem Abstract: We present an Expectation-Maximization algorithm for the fractal inverse\nproblem: the problem of fitting a fractal model to data. In our setting the\nfractals are Iterated Function Systems (IFS), with similitudes as the family of\ntransformations. The data is a point cloud in ${\\mathbb R}^H$ with arbitrary\ndimension $H$. Each IFS defines a probability distribution on ${\\mathbb R}^H$,\nso that the fractal inverse problem can be cast as a problem of parameter\nestimation. We show that the algorithm reconstructs well-known fractals from\ndata, with the model converging to high precision parameters. We also show the\nutility of the model as an approximation for datasources outside the IFS model\nclass. \n\n"}
{"id": "1706.04572", "contents": "Title: Deep Learning Methods for Efficient Large Scale Video Labeling Abstract: We present a solution to \"Google Cloud and YouTube-8M Video Understanding\nChallenge\" that ranked 5th place. The proposed model is an ensemble of three\nmodel families, two frame level and one video level. The training was performed\non augmented dataset, with cross validation. \n\n"}
{"id": "1706.05446", "contents": "Title: Adversarial Variational Bayes Methods for Tweedie Compound Poisson Mixed\n  Models Abstract: The Tweedie Compound Poisson-Gamma model is routinely used for modeling\nnon-negative continuous data with a discrete probability mass at zero. Mixed\nmodels with random effects account for the covariance structure related to the\ngrouping hierarchy in the data. An important application of Tweedie mixed\nmodels is pricing the insurance policies, e.g. car insurance. However, the\nintractable likelihood function, the unknown variance function, and the\nhierarchical structure of mixed effects have presented considerable challenges\nfor drawing inferences on Tweedie. In this study, we tackle the Bayesian\nTweedie mixed-effects models via variational inference approaches. In\nparticular, we empower the posterior approximation by implicit models trained\nin an adversarial setting. To reduce the variance of gradients, we\nreparameterize random effects, and integrate out one local latent variable of\nTweedie. We also employ a flexible hyper prior to ensure the richness of the\napproximation. Our method is evaluated on both simulated and real-world data.\nResults show that the proposed method has smaller estimation bias on the random\neffects compared to traditional inference methods including MCMC; it also\nachieves a state-of-the-art predictive performance, meanwhile offering a richer\nestimation of the variance function. \n\n"}
{"id": "1706.05599", "contents": "Title: Sample, computation vs storage tradeoffs for classification using tensor\n  subspace models Abstract: In this paper, we exhibit the tradeoffs between the (training) sample,\ncomputation and storage complexity for the problem of supervised classification\nusing signal subspace estimation. Our main tool is the use of tensor subspaces,\ni.e. subspaces with a Kronecker structure, for embedding the data into lower\ndimensions. Among the subspaces with a Kronecker structure, we show that using\nsubspaces with a hierarchical structure for representing data leads to improved\ntradeoffs. One of the main reasons for the improvement is that embedding data\ninto these hierarchical Kronecker structured subspaces prevents overfitting at\nhigher latent dimensions. \n\n"}
{"id": "1706.08757", "contents": "Title: Extrinsic Gaussian processes for regression and classification on\n  manifolds Abstract: Gaussian processes (GPs) are very widely used for modeling of unknown\nfunctions or surfaces in applications ranging from regression to classification\nto spatial processes. Although there is an increasingly vast literature on\napplications, methods, theory and algorithms related to GPs, the overwhelming\nmajority of this literature focuses on the case in which the input domain\ncorresponds to a Euclidean space. However, particularly in recent years with\nthe increasing collection of complex data, it is commonly the case that the\ninput domain does not have such a simple form. For example, it is common for\nthe inputs to be restricted to a non-Euclidean manifold, a case which forms the\nmotivation for this article. In particular, we propose a general extrinsic\nframework for GP modeling on manifolds, which relies on embedding of the\nmanifold into a Euclidean space and then constructing extrinsic kernels for GPs\non their images. These extrinsic Gaussian processes (eGPs) are used as prior\ndistributions for unknown functions in Bayesian inferences. Our approach is\nsimple and general, and we show that the eGPs inherit fine theoretical\nproperties from GP models in Euclidean spaces. We consider applications of our\nmodels to regression and classification problems with predictors lying in a\nlarge class of manifolds, including spheres, planar shape spaces, a space of\npositive definite matrices, and Grassmannians. Our models can be readily used\nby practitioners in biological sciences for various regression and\nclassification problems, such as disease diagnosis or detection. Our work is\nalso likely to have impact in spatial statistics when spatial locations are on\nthe sphere or other geometric spaces. \n\n"}
{"id": "1706.09451", "contents": "Title: (Machine) Learning to Do More with Less Abstract: Determining the best method for training a machine learning algorithm is\ncritical to maximizing its ability to classify data. In this paper, we compare\nthe standard \"fully supervised\" approach (that relies on knowledge of\nevent-by-event truth-level labels) with a recent proposal that instead utilizes\nclass ratios as the only discriminating information provided during training.\nThis so-called \"weakly supervised\" technique has access to less information\nthan the fully supervised method and yet is still able to yield impressive\ndiscriminating power. In addition, weak supervision seems particularly well\nsuited to particle physics since quantum mechanics is incompatible with the\nnotion of mapping an individual event onto any single Feynman diagram. We\nexamine the technique in detail -- both analytically and numerically -- with a\nfocus on the robustness to issues of mischaracterizing the training samples.\nWeakly supervised networks turn out to be remarkably insensitive to systematic\nmismodeling. Furthermore, we demonstrate that the event level outputs for\nweakly versus fully supervised networks are probing different kinematics, even\nthough the numerical quality metrics are essentially identical. This implies\nthat it should be possible to improve the overall classification ability by\ncombining the output from the two types of networks. For concreteness, we apply\nthis technology to a signature of beyond the Standard Model physics to\ndemonstrate that all these impressive features continue to hold in a scenario\nof relevance to the LHC. \n\n"}
{"id": "1707.00372", "contents": "Title: Deep Convolutional Framelets: A General Deep Learning Framework for\n  Inverse Problems Abstract: Recently, deep learning approaches with various network architectures have\nachieved significant performance improvement over existing iterative\nreconstruction methods in various imaging problems. However, it is still\nunclear why these deep learning architectures work for specific inverse\nproblems. To address these issues, here we show that the long-searched-for\nmissing link is the convolution framelets for representing a signal by\nconvolving local and non-local bases. The convolution framelets was originally\ndeveloped to generalize the theory of low-rank Hankel matrix approaches for\ninverse problems, and this paper further extends the idea so that we can obtain\na deep neural network using multilayer convolution framelets with perfect\nreconstruction (PR) under rectilinear linear unit nonlinearity (ReLU). Our\nanalysis also shows that the popular deep network components such as residual\nblock, redundant filter channels, and concatenated ReLU (CReLU) do indeed help\nto achieve the PR, while the pooling and unpooling layers should be augmented\nwith high-pass branches to meet the PR condition. Moreover, by changing the\nnumber of filter channels and bias, we can control the shrinkage behaviors of\nthe neural network. This discovery leads us to propose a novel theory for deep\nconvolutional framelets neural network. Using numerical experiments with\nvarious inverse problems, we demonstrated that our deep convolution framelets\nnetwork shows consistent improvement over existing deep architectures.This\ndiscovery suggests that the success of deep learning is not from a magical\npower of a black-box, but rather comes from the power of a novel signal\nrepresentation using non-local basis combined with data-driven local basis,\nwhich is indeed a natural extension of classical signal processing theory. \n\n"}
{"id": "1707.00703", "contents": "Title: Automated Problem Identification: Regression vs Classification via\n  Evolutionary Deep Networks Abstract: Regression or classification? This is perhaps the most basic question faced\nwhen tackling a new supervised learning problem. We present an Evolutionary\nDeep Learning (EDL) algorithm that automatically solves this by identifying the\nquestion type with high accuracy, along with a proposed deep architecture.\nTypically, a significant amount of human insight and preparation is required\nprior to executing machine learning algorithms. For example, when creating deep\nneural networks, the number of parameters must be selected in advance and\nfurthermore, a lot of these choices are made based upon pre-existing knowledge\nof the data such as the use of a categorical cross entropy loss function.\nHumans are able to study a dataset and decide whether it represents a\nclassification or a regression problem, and consequently make decisions which\nwill be applied to the execution of the neural network. We propose the\nAutomated Problem Identification (API) algorithm, which uses an evolutionary\nalgorithm interface to TensorFlow to manipulate a deep neural network to decide\nif a dataset represents a classification or a regression problem. We test API\non 16 different classification, regression and sentiment analysis datasets with\nup to 10,000 features and up to 17,000 unique target values. API achieves an\naverage accuracy of $96.3\\%$ in identifying the problem type without hardcoding\nany insights about the general characteristics of regression or classification\nproblems. For example, API successfully identifies classification problems even\nwith 1000 target values. Furthermore, the algorithm recommends which loss\nfunction to use and also recommends a neural network architecture. Our work is\ntherefore a step towards fully automated machine learning. \n\n"}
{"id": "1707.01207", "contents": "Title: The Sup-norm Perturbation of HOSVD and Low Rank Tensor Denoising Abstract: The higher order singular value decomposition (HOSVD) of tensors is a\ngeneralization of matrix SVD. The perturbation analysis of HOSVD under random\nnoise is more delicate than its matrix counterpart. Recently, polynomial time\nalgorithms have been proposed where statistically optimal estimates of the\nsingular subspaces and the low rank tensors are attainable in the Euclidean\nnorm. In this article, we analyze the sup-norm perturbation bounds of HOSVD and\nintroduce estimators of the singular subspaces with sharp deviation bounds in\nthe sup-norm. We also investigate a low rank tensor denoising estimator and\ndemonstrate its fast convergence rate with respect to the entry-wise errors.\nThe sup-norm perturbation bounds reveal unconventional phase transitions for\nstatistical learning applications such as the exact clustering in high\ndimensional Gaussian mixture model and the exact support recovery in sub-tensor\nlocalizations. In addition, the bounds established for HOSVD also elaborate the\none-sided sup-norm perturbation bounds for the singular subspaces of unbalanced\n(or fat) matrices. \n\n"}
{"id": "1707.01809", "contents": "Title: Entangled coherent states by mixing squeezed vacuum and coherent light Abstract: Entangled coherent states are shown to emerge, with high fidelity, when\nmixing coherent and squeezed vacuum states of light on a beam-splitter. These\nmaximally entangled states, where photons bunch at the exit of a beamsplitter,\nare measured experimentally by Fock-state projections. Entanglement is examined\ntheoretically using a Bell-type nonlocality test and compared with ideal\nentangled coherent states. We experimentally show nearly perfect similarity\nwith entangled coherent states for an optimal ratio of coherent and squeezed\nvacuum light. In our scheme, entangled coherent states are generated\ndeterministically with small amplitudes, which could be beneficial, for\nexample, in deterministic distribution of entanglement over long distances. \n\n"}
{"id": "1707.03017", "contents": "Title: Learning Visual Reasoning Without Strong Priors Abstract: Achieving artificial visual reasoning - the ability to answer image-related\nquestions which require a multi-step, high-level process - is an important step\ntowards artificial general intelligence. This multi-modal task requires\nlearning a question-dependent, structured reasoning process over images from\nlanguage. Standard deep learning approaches tend to exploit biases in the data\nrather than learn this underlying structure, while leading methods learn to\nvisually reason successfully but are hand-crafted for reasoning. We show that a\ngeneral-purpose, Conditional Batch Normalization approach achieves\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\nerror rate. We outperform the next best end-to-end method (4.5%) and even\nmethods that use extra supervision (3.1%). We probe our model to shed light on\nhow it reasons, showing it has learned a question-dependent, multi-step\nprocess. Previous work has operated under the assumption that visual reasoning\ncalls for a specialized architecture, but we show that a general architecture\nwith proper conditioning can learn to visually reason effectively. \n\n"}
{"id": "1707.03141", "contents": "Title: A Simple Neural Attentive Meta-Learner Abstract: Deep neural networks excel in regimes with large amounts of data, but tend to\nstruggle when data is scarce or when they need to adapt quickly to changes in\nthe task. In response, recent work in meta-learning proposes training a\nmeta-learner on a distribution of similar tasks, in the hopes of generalization\nto novel but related tasks by learning a high-level strategy that captures the\nessence of the problem it is asked to solve. However, many recent meta-learning\napproaches are extensively hand-designed, either using architectures\nspecialized to a particular application, or hard-coding algorithmic components\nthat constrain how the meta-learner solves the task. We propose a class of\nsimple and generic meta-learner architectures that use a novel combination of\ntemporal convolutions and soft attention; the former to aggregate information\nfrom past experience and the latter to pinpoint specific pieces of information.\nIn the most extensive set of meta-learning experiments to date, we evaluate the\nresulting Simple Neural AttentIve Learner (or SNAIL) on several\nheavily-benchmarked tasks. On all tasks, in both supervised and reinforcement\nlearning, SNAIL attains state-of-the-art performance by significant margins. \n\n"}
{"id": "1707.03321", "contents": "Title: A deep learning architecture for temporal sleep stage classification\n  using multivariate and multimodal time series Abstract: Sleep stage classification constitutes an important preliminary exam in the\ndiagnosis of sleep disorders. It is traditionally performed by a sleep expert\nwho assigns to each 30s of signal a sleep stage, based on the visual inspection\nof signals such as electroencephalograms (EEG), electrooculograms (EOG),\nelectrocardiograms (ECG) and electromyograms (EMG). We introduce here the first\ndeep learning approach for sleep stage classification that learns end-to-end\nwithout computing spectrograms or extracting hand-crafted features, that\nexploits all multivariate and multimodal Polysomnography (PSG) signals (EEG,\nEMG and EOG), and that can exploit the temporal context of each 30s window of\ndata. For each modality the first layer learns linear spatial filters that\nexploit the array of sensors to increase the signal-to-noise ratio, and the\nlast layer feeds the learnt representation to a softmax classifier. Our model\nis compared to alternative automatic approaches based on convolutional networks\nor decisions trees. Results obtained on 61 publicly available PSG records with\nup to 20 EEG channels demonstrate that our network architecture yields\nstate-of-the-art performance. Our study reveals a number of insights on the\nspatio-temporal distribution of the signal of interest: a good trade-off for\noptimal classification performance measured with balanced accuracy is to use 6\nEEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one\nminute of data before and after each data segment offers the strongest\nimprovement when a limited number of channels is available. As sleep experts,\nour system exploits the multivariate and multimodal nature of PSG signals in\norder to deliver state-of-the-art classification performance with a small\ncomputational cost. \n\n"}
{"id": "1707.03386", "contents": "Title: DeepCodec: Adaptive Sensing and Recovery via Deep Convolutional Neural\n  Networks Abstract: In this paper we develop a novel computational sensing framework for sensing\nand recovering structured signals. When trained on a set of representative\nsignals, our framework learns to take undersampled measurements and recover\nsignals from them using a deep convolutional neural network. In other words, it\nlearns a transformation from the original signals to a near-optimal number of\nundersampled measurements and the inverse transformation from measurements to\nsignals. This is in contrast to traditional compressive sensing (CS) systems\nthat use random linear measurements and convex optimization or iterative\nalgorithms for signal recovery. We compare our new framework with\n$\\ell_1$-minimization from the phase transition point of view and demonstrate\nthat it outperforms $\\ell_1$-minimization in the regions of phase transition\nplot where $\\ell_1$-minimization cannot recover the exact solution. In\naddition, we experimentally demonstrate how learning measurements enhances the\noverall recovery performance, speeds up training of recovery framework, and\nleads to having fewer parameters to learn. \n\n"}
{"id": "1707.04025", "contents": "Title: On Measuring and Quantifying Performance: Error Rates, Surrogate Loss,\n  and an Example in SSL Abstract: In various approaches to learning, notably in domain adaptation, active\nlearning, learning under covariate shift, semi-supervised learning, learning\nwith concept drift, and the like, one often wants to compare a baseline\nclassifier to one or more advanced (or at least different) strategies. In this\nchapter, we basically argue that if such classifiers, in their respective\ntraining phases, optimize a so-called surrogate loss that it may also be\nvaluable to compare the behavior of this loss on the test set, next to the\nregular classification error rate. It can provide us with an additional view on\nthe classifiers' relative performances that error rates cannot capture. As an\nexample, limited but convincing empirical results demonstrates that we may be\nable to find semi-supervised learning strategies that can guarantee performance\nimprovements with increasing numbers of unlabeled data in terms of\nlog-likelihood. In contrast, the latter may be impossible to guarantee for the\nclassification error rate. \n\n"}
{"id": "1707.05010", "contents": "Title: Deep Learning to Attend to Risk in ICU Abstract: Modeling physiological time-series in ICU is of high clinical importance.\nHowever, data collected within ICU are irregular in time and often contain\nmissing measurements. Since absence of a measure would signify its lack of\nimportance, the missingness is indeed informative and might reflect the\ndecision making by the clinician. Here we propose a deep learning architecture\nthat can effectively handle these challenges for predicting ICU mortality\noutcomes. The model is based on Long Short-Term Memory, and has layered\nattention mechanisms. At the sensing layer, the model decides whether to\nobserve and incorporate parts of the current measurements. At the reasoning\nlayer, evidences across time steps are weighted and combined. The model is\nevaluated on the PhysioNet 2012 dataset showing competitive and interpretable\nresults. \n\n"}
{"id": "1707.05807", "contents": "Title: Improving Gibbs Sampler Scan Quality with DoGS Abstract: The pairwise influence matrix of Dobrushin has long been used as an\nanalytical tool to bound the rate of convergence of Gibbs sampling. In this\nwork, we use Dobrushin influence as the basis of a practical tool to certify\nand efficiently improve the quality of a discrete Gibbs sampler. Our\nDobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection\norders for a given sampling budget and variable subset of interest, explicit\nbounds on total variation distance to stationarity, and certifiable\nimprovements over the standard systematic and uniform random scan Gibbs\nsamplers. In our experiments with joint image segmentation and object\nrecognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising\nmodel inference, DoGS consistently deliver higher-quality inferences with\nsignificantly smaller sampling budgets than standard Gibbs samplers. \n\n"}
{"id": "1707.05823", "contents": "Title: Employing coupled cavities to increase the cooling rate of a levitated\n  nanosphere in the resolved sideband regime Abstract: In this paper we investigate cooling of a levitated nanosphere in a system of\ncoupled cavities in the resolved sideband regime. Thanks to the presence of an\nextra resonance in the coupled cavity cooling system, the coupling strength can\nbe maximized at the optimum detuning. In this fashion, the intra-cavity photon\nnumber is increased and thereby the cooling rate is enhanced and the strong\ncoupling regime is achieved without resorting to increased driving laser power.\nThe underlying physics of the increased cooling efficiency in the here-proposed\nsystem of coupled cavities in the resolved sideband regime and that of the\nalready reported system of coupled cavities in the unresolved sideband regime\nare significantly different from each other. Since the spectral density of the\ndisplacement of the particle can no longer be accurately approximated by the\nconventional Lorentzian lineshape in the strong coupling regime, a double\nLorentzian lineshape is employed to accurately approximate the spectral density\nof the displacement of the particle and to provide analytical formulations for\nthe cooling rate. The analytical expression given for the cooling rate is\nvalidated by numerical simulations. \n\n"}
{"id": "1707.06219", "contents": "Title: Acceleration and Averaging in Stochastic Mirror Descent Dynamics Abstract: We formulate and study a general family of (continuous-time) stochastic\ndynamics for accelerated first-order minimization of smooth convex functions.\nBuilding on an averaging formulation of accelerated mirror descent, we propose\na stochastic variant in which the gradient is contaminated by noise, and study\nthe resulting stochastic differential equation. We prove a bound on the rate of\nchange of an energy function associated with the problem, then use it to derive\nestimates of convergence rates of the function values, (a.s. and in\nexpectation) both for persistent and asymptotically vanishing noise. We discuss\nthe interaction between the parameters of the dynamics (learning rate and\naveraging weights) and the covariation of the noise process, and show, in\nparticular, how the asymptotic rate of covariation affects the choice of\nparameters and, ultimately, the convergence rate. \n\n"}
{"id": "1707.08475", "contents": "Title: DARLA: Improving Zero-Shot Transfer in Reinforcement Learning Abstract: Domain adaptation is an important open problem in deep reinforcement learning\n(RL). In many scenarios of interest data is hard to obtain, so agents may learn\na source policy in a setting where data is readily available, with the hope\nthat it generalises well to the target domain. We propose a new multi-stage RL\nagent, DARLA (DisentAngled Representation Learning Agent), which learns to see\nbefore learning to act. DARLA's vision is based on learning a disentangled\nrepresentation of the observed environment. Once DARLA can see, it is able to\nacquire source policies that are robust to many domain shifts - even with no\naccess to the target domain. DARLA significantly outperforms conventional\nbaselines in zero-shot domain adaptation scenarios, an effect that holds across\na variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms\n(DQN, A3C and EC). \n\n"}
{"id": "1707.08712", "contents": "Title: Signal and Noise Statistics Oblivious Sparse Reconstruction using\n  OMP/OLS Abstract: Orthogonal matching pursuit (OMP) and orthogonal least squares (OLS) are\nwidely used for sparse signal reconstruction in under-determined linear\nregression problems. The performance of these compressed sensing (CS)\nalgorithms depends crucially on the \\textit{a priori} knowledge of either the\nsparsity of the signal ($k_0$) or noise variance ($\\sigma^2$). Both $k_0$ and\n$\\sigma^2$ are unknown in general and extremely difficult to estimate in under\ndetermined models. This limits the application of OMP and OLS in many practical\nsituations. In this article, we develop two computationally efficient\nframeworks namely TF-IGP and RRT-IGP for using OMP and OLS even when $k_0$ and\n$\\sigma^2$ are unavailable. Both TF-IGP and RRT-IGP are analytically shown to\naccomplish successful sparse recovery under the same set of restricted isometry\nconditions on the design matrix required for OMP/OLS with \\textit{a priori}\nknowledge of $k_0$ and $\\sigma^2$. Numerical simulations also indicate a highly\ncompetitive performance of TF-IGP and RRT-IGP in comparison to OMP/OLS with\n\\textit{a priori} knowledge of $k_0$ and $\\sigma^2$. \n\n"}
{"id": "1707.09118", "contents": "Title: Counterfactual Learning from Bandit Feedback under Deterministic\n  Logging: A Case Study in Statistical Machine Translation Abstract: The goal of counterfactual learning for statistical machine translation (SMT)\nis to optimize a target SMT system from logged data that consist of user\nfeedback to translations that were predicted by another, historic SMT system. A\nchallenge arises by the fact that risk-averse commercial SMT systems\ndeterministically log the most probable translation. The lack of sufficient\nexploration of the SMT output space seemingly contradicts the theoretical\nrequirements for counterfactual learning. We show that counterfactual learning\nfrom deterministic bandit logs is possible nevertheless by smoothing out\ndeterministic components in learning. This can be achieved by additive and\nmultiplicative control variates that avoid degenerate behavior in empirical\nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU\npoints by counterfactual learning from deterministic bandit feedback. \n\n"}
{"id": "1707.09938", "contents": "Title: Deep Convolutional Framelet Denosing for Low-Dose CT via Wavelet\n  Residual Network Abstract: Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT\nare computationally expensive. To address this problem, we recently proposed a\ndeep convolutional neural network (CNN) for low-dose X-ray CT and won the\nsecond place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the\ntexture were not fully recovered. To address this problem, here we propose a\nnovel framelet-based denoising algorithm using wavelet residual network which\nsynergistically combines the expressive power of deep learning and the\nperformance guarantee from the framelet-based denoising algorithms. The new\nalgorithms were inspired by the recent interpretation of the deep convolutional\nneural network (CNN) as a cascaded convolution framelet signal representation.\nExtensive experimental results confirm that the proposed networks have\nsignificantly improved performance and preserves the detail texture of the\noriginal images. \n\n"}
{"id": "1708.00549", "contents": "Title: Improved Representation Learning for Predicting Commonsense Ontologies Abstract: Recent work in learning ontologies (hierarchical and partially-ordered\nstructures) has leveraged the intrinsic geometry of spaces of learned\nrepresentations to make predictions that automatically obey complex structural\nconstraints. We explore two extensions of one such model, the order-embedding\nmodel for hierarchical relation learning, with an aim towards improved\nperformance on text data for commonsense knowledge representation. Our first\nmodel jointly learns ordering relations and non-hierarchical knowledge in the\nform of raw text. Our second extension exploits the partial order structure of\nthe training data to find long-distance triplet constraints among embeddings\nwhich are poorly enforced by the pairwise training procedure. We find that both\nincorporating free text and augmented training constraints improve over the\noriginal order-embedding model and other strong baselines. \n\n"}
{"id": "1708.01886", "contents": "Title: Probabilistic Generative Adversarial Networks Abstract: We introduce the Probabilistic Generative Adversarial Network (PGAN), a new\nGAN variant based on a new kind of objective function. The central idea is to\nintegrate a probabilistic model (a Gaussian Mixture Model, in our case) into\nthe GAN framework which supports a new kind of loss function (based on\nlikelihood rather than classification loss), and at the same time gives a\nmeaningful measure of the quality of the outputs generated by the network.\nExperiments with MNIST show that the model learns to generate realistic images,\nand at the same time computes likelihoods that are correlated with the quality\nof the generated images. We show that PGAN is better able to cope with\ninstability problems that are usually observed in the GAN training procedure.\nWe investigate this from three aspects: the probability landscape of the\ndiscriminator, gradients of the generator, and the perfect discriminator\nproblem. \n\n"}
{"id": "1708.03735", "contents": "Title: Sparse Coding and Autoencoders Abstract: In \"Dictionary Learning\" one tries to recover incoherent matrices $A^* \\in\n\\mathbb{R}^{n \\times h}$ (typically overcomplete and whose columns are assumed\nto be normalized) and sparse vectors $x^* \\in \\mathbb{R}^h$ with a small\nsupport of size $h^p$ for some $0 <p < 1$ while having access to observations\n$y \\in \\mathbb{R}^n$ where $y = A^*x^*$. In this work we undertake a rigorous\nanalysis of whether gradient descent on the squared loss of an autoencoder can\nsolve the dictionary learning problem. The \"Autoencoder\" architecture we\nconsider is a $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ mapping with a single\nReLU activation layer of size $h$.\n  Under very mild distributional assumptions on $x^*$, we prove that the norm\nof the expected gradient of the standard squared loss function is\nasymptotically (in sparse code dimension) negligible for all points in a small\nneighborhood of $A^*$. This is supported with experimental evidence using\nsynthetic data. We also conduct experiments to suggest that $A^*$ is a local\nminimum. Along the way we prove that a layer of ReLU gates can be set up to\nautomatically recover the support of the sparse codes. This property holds\nindependent of the loss function. We believe that it could be of independent\ninterest. \n\n"}
{"id": "1708.04692", "contents": "Title: GANs for Biological Image Synthesis Abstract: In this paper, we propose a novel application of Generative Adversarial\nNetworks (GAN) to the synthesis of cells imaged by fluorescence microscopy.\nCompared to natural images, cells tend to have a simpler and more geometric\nglobal structure that facilitates image generation. However, the correlation\nbetween the spatial pattern of different fluorescent proteins reflects\nimportant biological functions, and synthesized images have to capture these\nrelationships to be relevant for biological applications. We adapt GANs to the\ntask at hand and propose new models with casual dependencies between image\nchannels that can generate multi-channel images, which would be impossible to\nobtain experimentally. We evaluate our approach using two independent\ntechniques and compare it against sensible baselines. Finally, we demonstrate\nthat by interpolating across the latent space we can mimic the known changes in\nprotein localization that occur through time during the cell cycle, allowing us\nto predict temporal evolution from static images. \n\n"}
{"id": "1708.04781", "contents": "Title: Racing Thompson: an Efficient Algorithm for Thompson Sampling with\n  Non-conjugate Priors Abstract: Thompson sampling has impressive empirical performance for many multi-armed\nbandit problems. But current algorithms for Thompson sampling only work for the\ncase of conjugate priors since these algorithms require to infer the posterior,\nwhich is often computationally intractable when the prior is not conjugate. In\nthis paper, we propose a novel algorithm for Thompson sampling which only\nrequires to draw samples from a tractable distribution, so our algorithm is\nefficient even when the prior is non-conjugate. To do this, we reformulate\nThompson sampling as an optimization problem via the Gumbel-Max trick. After\nthat we construct a set of random variables and our goal is to identify the one\nwith highest mean. Finally, we solve it with techniques in best arm\nidentification. \n\n"}
{"id": "1708.05487", "contents": "Title: Debiased distributed learning for sparse partial linear models in high\n  dimensions Abstract: Although various distributed machine learning schemes have been proposed\nrecently for pure linear models and fully nonparametric models, little\nattention has been paid on distributed optimization for semi-paramemetric\nmodels with multiple-level structures (e.g. sparsity, linearity and\nnonlinearity). To address these issues, the current paper proposes a new\ncommunication-efficient distributed learning algorithm for partially sparse\nlinear models with an increasing number of features. The proposed method is\nbased on the classical divide and conquer strategy for handing big data and\neach sub-method defined on each subsample consists of a debiased estimation of\nthe double-regularized least squares approach. With the proposed method, we\ntheoretically prove that our global parametric estimator can achieve optimal\nparametric rate in our semi-parametric model given an appropriate partition on\nthe total data. Specially, the choice of data partition relies on the\nunderlying smoothness of the nonparametric component, but it is adaptive to the\nsparsity parameter. Even under the non-distributed setting, we develop a new\nand easily-read proof for optimal estimation of the parametric error in high\ndimensional partial linear model. Finally, several simulated experiments are\nimplemented to indicate comparable empirical performance of our debiased\ntechnique under the distributed setting. \n\n"}
{"id": "1708.06101", "contents": "Title: Twisted Photons: New Quantum Perspectives in High Dimensions Abstract: Quantum information science and quantum information technology have seen a\nvirtual explosion world-wide. It is all based on the observation that\nfundamental quantum phenomena on the individual particle or system-level lead\nto completely novel ways of encoding, processing and transmitting information.\nQuantum mechanics, a child of the first third of the 20th century, has found\nnumerous realizations and technical applications, much more than was thought at\nthe beginning. Decades later, it became possible to do experiments with\nindividual quantum particles and quantum systems. This was due to technological\nprogress, and for light in particular, the development of the laser. Hitherto,\nnearly all experiments and also nearly all realizations in the fields have been\nperformed with qubits, which are two-level quantum systems. We suggest that\nthis limitation is again mainly a technological one, because it is very\ndifficult to create, manipulate and measure more complex quantum systems. Here,\nwe provide a specific overview of some recent developments with\nhigher-dimensional quantum systems. We mainly focus on Orbital Angular Momentum\n(OAM) states of photons and possible applications in quantum information\nprotocols. Such states form discrete higher-dimensional quantum systems, also\ncalled qudits. Specifically, we will first address the question what kind of\nnew fundamental properties exist and the quantum information applications which\nare opened up by such novel systems. Then we give an overview of recent\ndevelopments in the field by discussing several notable experiments over the\npast 2-3 years. Finally, we conclude with several important open questions\nwhich will be interesting for investigations in the future. \n\n"}
{"id": "1708.06302", "contents": "Title: A general framework for Vecchia approximations of Gaussian processes Abstract: Gaussian processes (GPs) are commonly used as models for functions, time\nseries, and spatial fields, but they are computationally infeasible for large\ndatasets. Focusing on the typical setting of modeling data as a GP plus an\nadditive noise term, we propose a generalization of the Vecchia (1988) approach\nas a framework for GP approximations. We show that our general Vecchia approach\ncontains many popular existing GP approximations as special cases, allowing for\ncomparisons among the different methods within a unified framework.\nRepresenting the models by directed acyclic graphs, we determine the sparsity\nof the matrices necessary for inference, which leads to new insights regarding\nthe computational properties. Based on these results, we propose a novel sparse\ngeneral Vecchia approximation, which ensures computational feasibility for\nlarge spatial datasets but can lead to considerable improvements in\napproximation accuracy over Vecchia's original approach. We provide several\ntheoretical results and conduct numerical comparisons. We conclude with\nguidelines for the use of Vecchia approximations in spatial statistics. \n\n"}
{"id": "1708.08155", "contents": "Title: ByRDiE: Byzantine-resilient distributed coordinate descent for\n  decentralized learning Abstract: Distributed machine learning algorithms enable learning of models from\ndatasets that are distributed over a network without gathering the data at a\ncentralized location. While efficient distributed algorithms have been\ndeveloped under the assumption of faultless networks, failures that can render\nthese algorithms nonfunctional occur frequently in the real world. This paper\nfocuses on the problem of Byzantine failures, which are the hardest to\nsafeguard against in distributed algorithms. While Byzantine fault tolerance\nhas a rich history, existing work does not translate into efficient and\npractical algorithms for high-dimensional learning in fully distributed (also\nknown as decentralized) settings. In this paper, an algorithm termed\nByzantine-resilient distributed coordinate descent (ByRDiE) is developed and\nanalyzed that enables distributed learning in the presence of Byzantine\nfailures. Theoretical analysis (convex settings) and numerical experiments\n(convex and nonconvex settings) highlight its usefulness for high-dimensional\ndistributed learning in the presence of Byzantine failures. \n\n"}
{"id": "1708.08705", "contents": "Title: Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary\n  Learning Abstract: The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model,\nconsisting of a cascade of convolutional sparse layers, provides a new\ninterpretation of Convolutional Neural Networks (CNNs). Under this framework,\nthe computation of the forward pass in a CNN is equivalent to a pursuit\nalgorithm aiming to estimate the nested sparse representation vectors -- or\nfeature maps -- from a given input signal. Despite having served as a pivotal\nconnection between CNNs and sparse modeling, a deeper understanding of the\nML-CSC is still lacking: there are no pursuit algorithms that can serve this\nmodel exactly, nor are there conditions to guarantee a non-empty model. While\none can easily obtain signals that approximately satisfy the ML-CSC\nconstraints, it remains unclear how to simply sample from the model and, more\nimportantly, how one can train the convolutional filters from real data.\n  In this work, we propose a sound pursuit algorithm for the ML-CSC model by\nadopting a projection approach. We provide new and improved bounds on the\nstability of the solution of such pursuit and we analyze different practical\nalternatives to implement this in practice. We show that the training of the\nfilters is essential to allow for non-trivial signals in the model, and we\nderive an online algorithm to learn the dictionaries from real data,\neffectively resulting in cascaded sparse convolutional layers. Last, but not\nleast, we demonstrate the applicability of the ML-CSC model for several\napplications in an unsupervised setting, providing competitive results. Our\nwork represents a bridge between matrix factorization, sparse dictionary\nlearning and sparse auto-encoders, and we analyze these connections in detail. \n\n"}
{"id": "1709.00137", "contents": "Title: Mutual Unbiasedness in Coarse-grained Continuous Variables Abstract: The notion of mutual unbiasedness for coarse-grained measurements of quantum\ncontinuous variable systems is considered. It is shown that while the procedure\nof \"standard\" coarse graining breaks the mutual unbiasedness between conjugate\nvariables, this desired feature can be theoretically established and\nexperimentally observed in periodic coarse graining. We illustrate our results\nin an optics experiment implementing Fraunhofer diffraction through a periodic\ndiffraction grating, finding excellent agreement with the derived theory. Our\nresults are an important step in developing a formal connection between\ndiscrete and continuous variable quantum mechanics. \n\n"}
{"id": "1709.00572", "contents": "Title: XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification Abstract: In recent years, there have been numerous developments towards solving\nmultimodal tasks, aiming to learn a stronger representation than through a\nsingle modality. Certain aspects of the data can be particularly useful in this\ncase - for example, correlations in the space or time domain across modalities\n- but should be wisely exploited in order to benefit from their full predictive\npotential. We propose two deep learning architectures with multimodal\ncross-connections that allow for dataflow between several feature extractors\n(XFlow). Our models derive more interpretable features and achieve better\nperformances than models which do not exchange representations, usefully\nexploiting correlations between audio and visual data, which have a different\ndimensionality and are nontrivially exchangeable. Our work improves on existing\nmultimodal deep learning algorithms in two essential ways: (1) it presents a\nnovel method for performing cross-modality (before features are learned from\nindividual modalities) and (2) extends the previously proposed\ncross-connections which only transfer information between streams that process\ncompatible data. Illustrating some of the representations learned by the\nconnections, we analyse their contribution to the increase in discrimination\nability and reveal their compatibility with a lip-reading network intermediate\nrepresentation. We provide the research community with Digits, a new dataset\nconsisting of three data types extracted from videos of people saying the\ndigits 0-9. Results show that both cross-modal architectures outperform their\nbaselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits\ndatasets, achieving state-of-the-art results. \n\n"}
{"id": "1709.01006", "contents": "Title: Learning Implicit Generative Models Using Differentiable Graph Tests Abstract: Recently, there has been a growing interest in the problem of learning rich\nimplicit models - those from which we can sample, but can not evaluate their\ndensity. These models apply some parametric function, such as a deep network,\nto a base measure, and are learned end-to-end using stochastic optimization.\nOne strategy of devising a loss function is through the statistics of two\nsample tests - if we can fool a statistical test, the learned distribution\nshould be a good model of the true data. However, not all tests can easily fit\ninto this framework, as they might not be differentiable with respect to the\ndata points, and hence with respect to the parameters of the implicit model.\nMotivated by this problem, in this paper we show how two such classical tests,\nthe Friedman-Rafsky and k-nearest neighbour tests, can be effectively smoothed\nusing ideas from undirected graphical models - the matrix tree theorem and\ncardinality potentials. Moreover, as we show experimentally, smoothing can\nsignificantly increase the power of the test, which might of of independent\ninterest. Finally, we apply our method to learn implicit models. \n\n"}
{"id": "1709.01062", "contents": "Title: A hierarchical loss and its problems when classifying non-hierarchically Abstract: Failing to distinguish between a sheepdog and a skyscraper should be worse\nand penalized more than failing to distinguish between a sheepdog and a poodle;\nafter all, sheepdogs and poodles are both breeds of dogs. However, existing\nmetrics of failure (so-called \"loss\" or \"win\") used in textual or visual\nclassification/recognition via neural networks seldom leverage a-priori\ninformation, such as a sheepdog being more similar to a poodle than to a\nskyscraper. We define a metric that, inter alia, can penalize failure to\ndistinguish between a sheepdog and a skyscraper more than failure to\ndistinguish between a sheepdog and a poodle. Unlike previously employed\npossibilities, this metric is based on an ultrametric tree associated with any\ngiven tree organization into a semantically meaningful hierarchy of a\nclassifier's classes. An ultrametric tree is a tree with a so-called\nultrametric distance metric such that all leaves are at the same distance from\nthe root. Unfortunately, extensive numerical experiments indicate that the\nstandard practice of training neural networks via stochastic gradient descent\nwith random starting points often drives down the hierarchical loss nearly as\nmuch when minimizing the standard cross-entropy loss as when trying to minimize\nthe hierarchical loss directly. Thus, this hierarchical loss is unreliable as\nan objective for plain, randomly started stochastic gradient descent to\nminimize; the main value of the hierarchical loss may be merely as a meaningful\nmetric of success of a classifier. \n\n"}
{"id": "1709.01215", "contents": "Title: ALICE: Towards Understanding Adversarial Learning for Joint Distribution\n  Matching Abstract: We investigate the non-identifiability issues associated with bidirectional\nadversarial training for joint distribution matching. Within a framework of\nconditional entropy, we propose both adversarial and non-adversarial approaches\nto learn desirable matched joint distributions for unsupervised and supervised\ntasks. We unify a broad family of adversarial models as joint distribution\nmatching problems. Our approach stabilizes learning of unsupervised\nbidirectional adversarial learning methods. Further, we introduce an extension\nfor semi-supervised learning tasks. Theoretical results are validated in\nsynthetic data and real-world applications. \n\n"}
{"id": "1709.01870", "contents": "Title: Clustering of Data with Missing Entries using Non-convex Fusion\n  Penalties Abstract: The presence of missing entries in data often creates challenges for pattern\nrecognition algorithms. Traditional algorithms for clustering data assume that\nall the feature values are known for every data point. We propose a method to\ncluster data in the presence of missing information. Unlike conventional\nclustering techniques where every feature is known for each point, our\nalgorithm can handle cases where a few feature values are unknown for every\npoint. For this more challenging problem, we provide theoretical guarantees for\nclustering using a $\\ell_0$ fusion penalty based optimization problem.\nFurthermore, we propose an algorithm to solve a relaxation of this problem\nusing saturating non-convex fusion penalties. It is observed that this\nalgorithm produces solutions that degrade gradually with an increase in the\nfraction of missing feature values. We demonstrate the utility of the proposed\nmethod using a simulated dataset, the Wine dataset and also an under-sampled\ncardiac MRI dataset. It is shown that the proposed method is a promising\nclustering technique for datasets with large fractions of missing entries. \n\n"}
{"id": "1709.03741", "contents": "Title: Learning Graph-Level Representation for Drug Discovery Abstract: Predicating macroscopic influences of drugs on human body, like efficacy and\ntoxicity, is a central problem of small-molecule based drug discovery.\nMolecules can be represented as an undirected graph, and we can utilize graph\nconvolution networks to predication molecular properties. However, graph\nconvolutional networks and other graph neural networks all focus on learning\nnode-level representation rather than graph-level representation. Previous\nworks simply sum all feature vectors for all nodes in the graph to obtain the\ngraph feature vector for drug predication. In this paper, we introduce a dummy\nsuper node that is connected with all nodes in the graph by a directed edge as\nthe representation of the graph and modify the graph operation to help the\ndummy super node learn graph-level feature. Thus, we can handle graph-level\nclassification and regression in the same way as node-level classification and\nregression. In addition, we apply focal loss to address class imbalance in drug\ndatasets. The experiments on MoleculeNet show that our method can effectively\nimprove the performance of molecular properties predication. \n\n"}
{"id": "1709.04574", "contents": "Title: Towards personalized human AI interaction - adapting the behavior of AI\n  agents using neural signatures of subjective interest Abstract: Reinforcement Learning AI commonly uses reward/penalty signals that are\nobjective and explicit in an environment -- e.g. game score, completion time,\netc. -- in order to learn the optimal strategy for task performance. However,\nHuman-AI interaction for such AI agents should include additional reinforcement\nthat is implicit and subjective -- e.g. human preferences for certain AI\nbehavior -- in order to adapt the AI behavior to idiosyncratic human\npreferences. Such adaptations would mirror naturally occurring processes that\nincrease trust and comfort during social interactions. Here, we show how a\nhybrid brain-computer-interface (hBCI), which detects an individual's level of\ninterest in objects/events in a virtual environment, can be used to adapt the\nbehavior of a Deep Reinforcement Learning AI agent that is controlling a\nvirtual autonomous vehicle. Specifically, we show that the AI learns a driving\nstrategy that maintains a safe distance from a lead vehicle, and most novelly,\npreferentially slows the vehicle when the human passengers of the vehicle\nencounter objects of interest. This adaptation affords an additional 20\\%\nviewing time for subjectively interesting objects. This is the first\ndemonstration of how an hBCI can be used to provide implicit reinforcement to\nan AI agent in a way that incorporates user preferences into the control\nsystem. \n\n"}
{"id": "1709.04875", "contents": "Title: Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework\n  for Traffic Forecasting Abstract: Timely accurate traffic forecast is crucial for urban traffic control and\nguidance. Due to the high nonlinearity and complexity of traffic flow,\ntraditional methods cannot satisfy the requirements of mid-and-long term\nprediction tasks and often neglect spatial and temporal dependencies. In this\npaper, we propose a novel deep learning framework, Spatio-Temporal Graph\nConvolutional Networks (STGCN), to tackle the time series prediction problem in\ntraffic domain. Instead of applying regular convolutional and recurrent units,\nwe formulate the problem on graphs and build the model with complete\nconvolutional structures, which enable much faster training speed with fewer\nparameters. Experiments show that our model STGCN effectively captures\ncomprehensive spatio-temporal correlations through modeling multi-scale traffic\nnetworks and consistently outperforms state-of-the-art baselines on various\nreal-world traffic datasets. \n\n"}
{"id": "1709.06010", "contents": "Title: Learning Neural Networks with Two Nonlinear Layers in Polynomial Time Abstract: We give a polynomial-time algorithm for learning neural networks with one\nlayer of sigmoids feeding into any Lipschitz, monotone activation function\n(e.g., sigmoid or ReLU). We make no assumptions on the structure of the\nnetwork, and the algorithm succeeds with respect to {\\em any} distribution on\nthe unit ball in $n$ dimensions (hidden weight vectors also have unit norm).\nThis is the first assumption-free, provably efficient algorithm for learning\nneural networks with two nonlinear layers.\n  Our algorithm-- {\\em Alphatron}-- is a simple, iterative update rule that\ncombines isotonic regression with kernel methods. It outputs a hypothesis that\nyields efficient oracle access to interpretable features. It also suggests a\nnew approach to Boolean learning problems via real-valued conditional-mean\nfunctions, sidestepping traditional hardness results from computational\nlearning theory.\n  Along these lines, we subsume and improve many longstanding results for PAC\nlearning Boolean functions to the more general, real-valued setting of {\\em\nprobabilistic concepts}, a model that (unlike PAC learning) requires non-i.i.d.\nnoise-tolerance. \n\n"}
{"id": "1709.06622", "contents": "Title: Distributed Training Large-Scale Deep Architectures Abstract: Scale of data and scale of computation infrastructures together enable the\ncurrent deep learning renaissance. However, training large-scale deep\narchitectures demands both algorithmic improvement and careful system\nconfiguration. In this paper, we focus on employing the system approach to\nspeed up large-scale training. Via lessons learned from our routine\nbenchmarking effort, we first identify bottlenecks and overheads that hinter\ndata parallelism. We then devise guidelines that help practitioners to\nconfigure an effective system and fine-tune parameters to achieve desired\nspeedup. Specifically, we develop a procedure for setting minibatch size and\nchoosing computation algorithms. We also derive lemmas for determining the\nquantity of key components such as the number of GPUs and parameter servers.\nExperiments and examples show that these guidelines help effectively speed up\nlarge-scale deep learning training. \n\n"}
{"id": "1709.06990", "contents": "Title: Text Compression for Sentiment Analysis via Evolutionary Algorithms Abstract: Can textual data be compressed intelligently without losing accuracy in\nevaluating sentiment? In this study, we propose a novel evolutionary\ncompression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),\nwhich makes use of Parts-of-Speech tags to compress text in a way that\nsacrifices minimal classification accuracy when used in conjunction with\nsentiment analysis algorithms. An analysis of PARSEC with eight commercial and\nnon-commercial sentiment analysis algorithms on twelve English sentiment data\nsets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss\nin sentiment classification accuracy for (20%, 50%, 75%) data compression with\nPARSEC using LingPipe, the most accurate of the sentiment algorithms. Other\nsentiment analysis algorithms are more severely affected by compression. We\nconclude that significant compression of text data is possible for sentiment\nanalysis depending on the accuracy demands of the specific application and the\nspecific sentiment analysis algorithm used. \n\n"}
{"id": "1709.08519", "contents": "Title: Enhanced Quantum Synchronization via Quantum Machine Learning Abstract: We study the quantum synchronization between a pair of two-level systems\ninside two coupled cavities. By using a digital-analog decomposition of the\nmaster equation that rules the system dynamics, we show that this approach\nleads to quantum synchronization between both two-level systems. Moreover, we\ncan identify in this digital-analog block decomposition the fundamental\nelements of a quantum machine learning protocol, in which the agent and the\nenvironment (learning units) interact through a mediating system, namely, the\nregister. If we can additionally equip this algorithm with a classical feedback\nmechanism, which consists of projective measurements in the register,\nreinitialization of the register state and local conditional operations on the\nagent and environment subspace, a powerful and flexible quantum machine\nlearning protocol emerges. Indeed, numerical simulations show that this\nprotocol enhances the synchronization process, even when every subsystem\nexperience different loss/decoherence mechanisms, and give us the flexibility\nto choose the synchronization state. Finally, we propose an implementation\nbased on current technologies in superconducting circuits. \n\n"}
{"id": "1709.08568", "contents": "Title: The Consciousness Prior Abstract: A new prior is proposed for learning representations of high-level concepts\nof the kind we manipulate with language. This prior can be combined with other\npriors in order to help disentangling abstract factors from each other. It is\ninspired by cognitive neuroscience theories of consciousness, seen as a\nbottleneck through which just a few elements, after having been selected by\nattention from a broader pool, are then broadcast and condition further\nprocessing, both in perception and decision-making. The set of recently\nselected elements one becomes aware of is seen as forming a low-dimensional\nconscious state. This conscious state is combining the few concepts\nconstituting a conscious thought, i.e., what one is immediately conscious of at\na particular moment. We claim that this architectural and\ninformation-processing constraint corresponds to assumptions about the joint\ndistribution between high-level concepts. To the extent that these assumptions\nare generally true (and the form of natural language seems consistent with\nthem), they can form a useful prior for representation learning. A\nlow-dimensional thought or conscious state is analogous to a sentence: it\ninvolves only a few variables and yet can make a statement with very high\nprobability of being true. This is consistent with a joint distribution (over\nhigh-level concepts) which has the form of a sparse factor graph, i.e., where\nthe dependencies captured by each factor of the factor graph involve only very\nfew variables while creating a strong dip in the overall energy function. The\nconsciousness prior also makes it natural to map conscious states to natural\nlanguage utterances or to express classical AI knowledge in a form similar to\nfacts and rules, albeit capturing uncertainty as well as efficient search\nmechanisms implemented by attention mechanisms. \n\n"}
{"id": "1710.02238", "contents": "Title: How Much Chemistry Does a Deep Neural Network Need to Know to Make\n  Accurate Predictions? Abstract: The meteoric rise of deep learning models in computer vision research, having\nachieved human-level accuracy in image recognition tasks is firm evidence of\nthe impact of representation learning of deep neural networks. In the chemistry\ndomain, recent advances have also led to the development of similar CNN models,\nsuch as Chemception, that is trained to predict chemical properties using\nimages of molecular drawings. In this work, we investigate the effects of\nsystematically removing and adding localized domain-specific information to the\nimage channels of the training data. By augmenting images with only 3\nadditional basic information, and without introducing any architectural\nchanges, we demonstrate that an augmented Chemception (AugChemception)\noutperforms the original model in the prediction of toxicity, activity, and\nsolvation free energy. Then, by altering the information content in the images,\nand examining the resulting model's performance, we also identify two distinct\nlearning patterns in predicting toxicity/activity as compared to solvation free\nenergy. These patterns suggest that Chemception is learning about its tasks in\nthe manner that is consistent with established knowledge. Thus, our work\ndemonstrates that advanced chemical knowledge is not a pre-requisite for deep\nlearning models to accurately predict complex chemical properties. \n\n"}
{"id": "1710.04045", "contents": "Title: Neural-Network Quantum States, String-Bond States, and Chiral\n  Topological States Abstract: Neural-Network Quantum States have been recently introduced as an Ansatz for\ndescribing the wave function of quantum many-body systems. We show that there\nare strong connections between Neural-Network Quantum States in the form of\nRestricted Boltzmann Machines and some classes of Tensor-Network states in\narbitrary dimensions. In particular we demonstrate that short-range Restricted\nBoltzmann Machines are Entangled Plaquette States, while fully connected\nRestricted Boltzmann Machines are String-Bond States with a nonlocal geometry\nand low bond dimension. These results shed light on the underlying architecture\nof Restricted Boltzmann Machines and their efficiency at representing many-body\nquantum states. String-Bond States also provide a generic way of enhancing the\npower of Neural-Network Quantum States and a natural generalization to systems\nwith larger local Hilbert space. We compare the advantages and drawbacks of\nthese different classes of states and present a method to combine them\ntogether. This allows us to benefit from both the entanglement structure of\nTensor Networks and the efficiency of Neural-Network Quantum States into a\nsingle Ansatz capable of targeting the wave function of strongly correlated\nsystems. While it remains a challenge to describe states with chiral\ntopological order using traditional Tensor Networks, we show that\nNeural-Network Quantum States and their String-Bond States extension can\ndescribe a lattice Fractional Quantum Hall state exactly. In addition, we\nprovide numerical evidence that Neural-Network Quantum States can approximate a\nchiral spin liquid with better accuracy than Entangled Plaquette States and\nlocal String-Bond States. Our results demonstrate the efficiency of neural\nnetworks to describe complex quantum wave functions and pave the way towards\nthe use of String-Bond States as a tool in more traditional machine-learning\napplications. \n\n"}
{"id": "1710.04340", "contents": "Title: Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition Abstract: Spectral decomposition of the Koopman operator is attracting attention as a\ntool for the analysis of nonlinear dynamical systems. Dynamic mode\ndecomposition is a popular numerical algorithm for Koopman spectral analysis;\nhowever, we often need to prepare nonlinear observables manually according to\nthe underlying dynamics, which is not always possible since we may not have any\na priori knowledge about them. In this paper, we propose a fully data-driven\nmethod for Koopman spectral analysis based on the principle of learning Koopman\ninvariant subspaces from observed data. To this end, we propose minimization of\nthe residual sum of squares of linear least-squares regression to estimate a\nset of functions that transforms data into a form in which the linear\nregression fits well. We introduce an implementation with neural networks and\nevaluate performance empirically using nonlinear dynamical systems and\napplications. \n\n"}
{"id": "1710.04934", "contents": "Title: RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE\n  detection in CT Scans Abstract: We describe a deep learning approach for automated brain hemorrhage detection\nfrom computed tomography (CT) scans. Our model emulates the procedure followed\nby radiologists to analyse a 3D CT scan in real-world. Similar to radiologists,\nthe model sifts through 2D cross-sectional slices while paying close attention\nto potential hemorrhagic regions. Further, the model utilizes 3D context from\nneighboring slices to improve predictions at each slice and subsequently,\naggregates the slice-level predictions to provide diagnosis at CT level. We\nrefer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it\nemploys original DenseNet architecture along with adding the components of\nattention for slice level predictions and recurrent neural network layer for\nincorporating 3D context. The real-world performance of RADnet has been\nbenchmarked against independent analysis performed by three senior radiologists\nfor 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at\nCT level that is comparable to radiologists. Further, RADnet achieves higher\nrecall than two of the three radiologists, which is remarkable. \n\n"}
{"id": "1710.05948", "contents": "Title: Experimentally bounding deviations from quantum theory in the landscape\n  of generalized probabilistic theories Abstract: Many experiments in the field of quantum foundations seek to adjudicate\nbetween quantum theory and speculative alternatives to it. This requires one to\nanalyze the experimental data in a manner that does not presume the correctness\nof the quantum formalism. The mathematical framework of generalized\nprobabilistic theories (GPTs) provides a means of doing so. We present a scheme\nfor determining which GPTs are consistent with a given set of experimental\ndata. It proceeds by performing tomography on the preparations and measurements\nin a self-consistent manner, i.e., without presuming a prior characterization\nof either. We illustrate the scheme by analyzing experimental data for a large\nset of preparations and measurements on the polarization degree of freedom of a\nsingle photon. We find that the smallest and largest GPT state spaces\nconsistent with our data are a pair of polytopes, each approximating the shape\nof the Bloch Sphere and having a volume ratio of $0.977 \\pm 0.001$, which\nprovides a quantitative bound on the scope for deviations from quantum theory.\nWe also demonstrate how our scheme can be used to bound the extent to which\nnature might be more nonlocal than quantum theory predicts, as well as the\nextent to which it might be more or less contextual. Specifically, we find that\nthe maximal violation of the CHSH inequality can be at most $1.3\\% \\pm 0.1$\ngreater than the quantum prediction, and the maximal violation of a particular\ninequality for universal noncontextuality can not differ from the quantum\nprediction by more than this factor on either side. The most significant\nloophole in this sort of analysis is that the set of preparations and\nmeasurements one implements might fail to be tomographically complete for the\nsystem of interest. \n\n"}
{"id": "1710.06736", "contents": "Title: Temporal-mode-selective optical Ramsey interferometry via cascaded\n  frequency conversion Abstract: Temporal modes (TM) are a new basis for storage and retrieval of quantum\ninformation in states of light. The full TM manipulation toolkit requires a\npractical quantum pulse gate (QPG), which is a device that unitarily maps any\ngiven TM component of the optical input field onto a different, easily\nseparable subspace or degree of freedom. An ideal QPG must \"separate\" the\nselected TM component with unit efficiency, whilst avoiding crosstalk from\northogonal TMs. All attempts at implementing QPGs in pulsed-pump traveling-wave\nsystems have been unable to satisfy both conditions simultaneously. This is due\nto a known selectivity limit in processes that rely on spatio-temporally local,\nnonlinear interactions between pulsed modes traveling at independent group\nvelocities. This limit is a consequence of time ordering in the quantum\ndynamical evolution, which is predicted to be overcome by coherently cascading\nmultiple stages of low-efficiency, but highly TM-discriminatory QPGs.\nMulti-stage interferometric quantum frequency conversion in nonlinear\nwaveguides was first proposed for precisely this purpose. TM-nonselective\ncascaded frequency conversion, also called optical Ramsey interferometry, has\nrecently been demonstrated with continuous-wave (CW) fields. Here, we present\nthe first experimental demonstration of TM-selective optical Ramsey\ninterferometry and show a significant enhancement in TM selectivity over\nsingle-stage schemes. \n\n"}
{"id": "1710.07437", "contents": "Title: Distributed Deep Transfer Learning by Basic Probability Assignment Abstract: Transfer learning is a popular practice in deep neural networks, but\nfine-tuning of large number of parameters is a hard task due to the complex\nwiring of neurons between splitting layers and imbalance distributions of data\nin pretrained and transferred domains. The reconstruction of the original\nwiring for the target domain is a heavy burden due to the size of\ninterconnections across neurons. We propose a distributed scheme that tunes the\nconvolutional filters individually while backpropagates them jointly by means\nof basic probability assignment. Some of the most recent advances in evidence\ntheory show that in a vast variety of the imbalanced regimes, optimizing of\nsome proper objective functions derived from contingency matrices prevents\nbiases towards high-prior class distributions. Therefore, the original filters\nget gradually transferred based on individual contributions to overall\nperformance of the target domain. This largely reduces the expected complexity\nof transfer learning whilst highly improves precision. Our experiments on\nstandard benchmarks and scenarios confirm the consistent improvement of our\ndistributed deep transfer learning strategy. \n\n"}
{"id": "1710.07702", "contents": "Title: On the Consistency of Graph-based Bayesian Learning and the Scalability\n  of Sampling Algorithms Abstract: A popular approach to semi-supervised learning proceeds by endowing the input\ndata with a graph structure in order to extract geometric information and\nincorporate it into a Bayesian framework. We introduce new theory that gives\nappropriate scalings of graph parameters that provably lead to a well-defined\nlimiting posterior as the size of the unlabeled data set grows. Furthermore, we\nshow that these consistency results have profound algorithmic implications.\nWhen consistency holds, carefully designed graph-based Markov chain Monte Carlo\nalgorithms are proved to have a uniform spectral gap, independent of the number\nof unlabeled inputs. Several numerical experiments corroborate both the\nstatistical consistency and the algorithmic scalability established by the\ntheory. \n\n"}
{"id": "1710.07783", "contents": "Title: A Novel Stochastic Stratified Average Gradient Method: Convergence Rate\n  and Its Complexity Abstract: SGD (Stochastic Gradient Descent) is a popular algorithm for large scale\noptimization problems due to its low iterative cost. However, SGD can not\nachieve linear convergence rate as FGD (Full Gradient Descent) because of the\ninherent gradient variance. To attack the problem, mini-batch SGD was proposed\nto get a trade-off in terms of convergence rate and iteration cost. In this\npaper, a general CVI (Convergence-Variance Inequality) equation is presented to\nstate formally the interaction of convergence rate and gradient variance. Then\na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is\nintroduced to reduce gradient variance based on two techniques, stratified\nsampling and averaging over iterations that is a key idea in SAG (Stochastic\nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of\n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs,\nwhere $C\\geq 2$ is the category number of training data. This convergence rate\ndepends mainly on the variance between classes, but not on the variance within\nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's\nconvergence rate is much better than SAG's convergence rate of $\\mathcal\n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG\nand many other algorithms. \n\n"}
{"id": "1710.07797", "contents": "Title: Optimal Rates for Learning with Nystr\\\"om Stochastic Gradient Methods Abstract: In the setting of nonparametric regression, we propose and study a\ncombination of stochastic gradient methods with Nystr\\\"om subsampling, allowing\nmultiple passes over the data and mini-batches. Generalization error bounds for\nthe studied algorithm are provided. Particularly, optimal learning rates are\nderived considering different possible choices of the step-size, the mini-batch\nsize, the number of iterations/passes, and the subsampling level. In comparison\nwith state-of-the-art algorithms such as the classic stochastic gradient\nmethods and kernel ridge regression with Nystr\\\"om, the studied algorithm has\nadvantages on the computational complexity, while achieving the same optimal\nlearning rates. Moreover, our results indicate that using mini-batches can\nreduce the total computational cost while achieving the same optimal\nstatistical results. \n\n"}
{"id": "1710.07991", "contents": "Title: Rethinking Convolutional Semantic Segmentation Learning Abstract: Deep convolutional semantic segmentation (DCSS) learning doesn't converge to\nan optimal local minimum with random parameters initializations; a pre-trained\nmodel on the same domain becomes necessary to achieve convergence.In this work,\nwe propose a joint cooperative end-to-end learning method for DCSS. It\naddresses many drawbacks with existing deep semantic segmentation learning; the\nproposed approach simultaneously learn both segmentation and classification;\ntaking away the essential need of the pre-trained model for learning\nconvergence. We present an improved inception based architecture with partial\nattention gating (PAG) over encoder information. The PAG also adds to achieve\nfaster convergence and better accuracy for segmentation task. We will show the\neffectiveness of this learning on a diabetic retinopathy classification and\nsegmentation dataset. \n\n"}
{"id": "1710.08310", "contents": "Title: AutoEncoder Inspired Unsupervised Feature Selection Abstract: High-dimensional data in many areas such as computer vision and machine\nlearning tasks brings in computational and analytical difficulty. Feature\nselection which selects a subset from observed features is a widely used\napproach for improving performance and effectiveness of machine learning models\nwith high-dimensional data. In this paper, we propose a novel AutoEncoder\nFeature Selector (AEFS) for unsupervised feature selection which combines\nautoencoder regression and group lasso tasks. Compared to traditional feature\nselection methods, AEFS can select the most important features by excavating\nboth linear and nonlinear information among features, which is more flexible\nthan the conventional self-representation method for unsupervised feature\nselection with only linear assumptions. Experimental results on benchmark\ndataset show that the proposed method is superior to the state-of-the-art\nmethod. \n\n"}
{"id": "1710.08531", "contents": "Title: Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets Abstract: Deep learning models (aka Deep Neural Networks) have revolutionized many\nfields including computer vision, natural language processing, speech\nrecognition, and is being increasingly used in clinical healthcare\napplications. However, few works exist which have benchmarked the performance\nof the deep learning models with respect to the state-of-the-art machine\nlearning models and prognostic scoring systems on publicly available healthcare\ndatasets. In this paper, we present the benchmarking results for several\nclinical prediction tasks such as mortality prediction, length of stay\nprediction, and ICD-9 code group prediction using Deep Learning models,\nensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA\nscores. We used the Medical Information Mart for Intensive Care III (MIMIC-III)\n(v1.4) publicly available dataset, which includes all patients admitted to an\nICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the\nbenchmarking tasks. Our results show that deep learning models consistently\noutperform all the other approaches especially when the `raw' clinical time\nseries data is used as input features to the models. \n\n"}
{"id": "1710.08816", "contents": "Title: Algorithmic infeasibility of community detection in higher-order\n  networks Abstract: In principle, higher-order networks that have multiple edge types are more\ninformative than their lower-order counterparts. In practice, however,\nexcessively rich information may be algorithmically infeasible to extract. It\nrequires an algorithm that assumes a high-dimensional model and such an\nalgorithm may perform poorly or be extremely sensitive to the initial estimate\nof the model parameters. Herein, we address this problem of community detection\nthrough a detectability analysis. We focus on the expectation-maximization (EM)\nalgorithm with belief propagation (BP), and analytically derive its algorithmic\ndetectability threshold, i.e., the limit of the modular structure strength\nbelow which the algorithm can no longer detect any modular structures. The\nresults indicate the existence of a phase in which the community detection of a\nlower-order network outperforms its higher-order counterpart. \n\n"}
{"id": "1710.10196", "contents": "Title: Progressive Growing of GANs for Improved Quality, Stability, and\n  Variation Abstract: We describe a new training methodology for generative adversarial networks.\nThe key idea is to grow both the generator and discriminator progressively:\nstarting from a low resolution, we add new layers that model increasingly fine\ndetails as training progresses. This both speeds the training up and greatly\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\nCelebA images at 1024^2. We also propose a simple way to increase the variation\nin generated images, and achieve a record inception score of 8.80 in\nunsupervised CIFAR10. Additionally, we describe several implementation details\nthat are important for discouraging unhealthy competition between the generator\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\nboth in terms of image quality and variation. As an additional contribution, we\nconstruct a higher-quality version of the CelebA dataset. \n\n"}
{"id": "1710.10403", "contents": "Title: Trainable back-propagated functional transfer matrices Abstract: Connections between nodes of fully connected neural networks are usually\nrepresented by weight matrices. In this article, functional transfer matrices\nare introduced as alternatives to the weight matrices: Instead of using real\nweights, a functional transfer matrix uses real functions with trainable\nparameters to represent connections between nodes. Multiple functional transfer\nmatrices are then stacked together with bias vectors and activations to form\ndeep functional transfer neural networks. These neural networks can be trained\nwithin the framework of back-propagation, based on a revision of the delta\nrules and the error transmission rule for functional connections. In\nexperiments, it is demonstrated that the revised rules can be used to train a\nrange of functional connections: 20 different functions are applied to neural\nnetworks with up to 10 hidden layers, and most of them gain high test\naccuracies on the MNIST database. It is also demonstrated that a functional\ntransfer matrix with a memory function can roughly memorise a non-cyclical\nsequence of 400 digits. \n\n"}
{"id": "1710.10513", "contents": "Title: Crime incidents embedding using restricted Boltzmann machines Abstract: We present a new approach for detecting related crime series, by unsupervised\nlearning of the latent feature embeddings from narratives of crime record via\nthe Gaussian-Bernoulli Restricted Boltzmann Machines (RBM). This is a\ndrastically different approach from prior work on crime analysis, which\ntypically considers only time and location and at most category information.\nAfter the embedding, related cases are closer to each other in the Euclidean\nfeature space, and the unrelated cases are far apart, which is a good property\ncan enable subsequent analysis such as detection and clustering of related\ncases. Experiments over several series of related crime incidents hand labeled\nby the Atlanta Police Department reveal the promise of our embedding methods. \n\n"}
{"id": "1710.10547", "contents": "Title: Interpretation of Neural Networks is Fragile Abstract: In order for machine learning to be deployed and trusted in many\napplications, it is crucial to be able to reliably explain why the machine\nlearning algorithm makes certain predictions. For example, if an algorithm\nclassifies a given pathology image to be a malignant tumor, then the doctor may\nneed to know which parts of the image led the algorithm to this classification.\nHow to interpret black-box predictors is thus an important and active area of\nresearch. A fundamental question is: how much can we trust the interpretation\nitself? In this paper, we show that interpretation of deep learning predictions\nis extremely fragile in the following sense: two perceptively indistinguishable\ninputs with the same predicted label can be assigned very different\ninterpretations. We systematically characterize the fragility of several\nwidely-used feature-importance interpretation methods (saliency maps, relevance\npropagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that\neven small random perturbation can change the feature importance and new\nsystematic perturbations can lead to dramatically different interpretations\nwithout changing the label. We extend these results to show that\ninterpretations based on exemplars (e.g. influence functions) are similarly\nfragile. Our analysis of the geometry of the Hessian matrix gives insight on\nwhy fragility could be a fundamental challenge to the current interpretation\napproaches. \n\n"}
{"id": "1710.10720", "contents": "Title: Globally Optimal Symbolic Regression Abstract: In this study we introduce a new technique for symbolic regression that\nguarantees global optimality. This is achieved by formulating a mixed integer\nnon-linear program (MINLP) whose solution is a symbolic mathematical expression\nof minimum complexity that explains the observations. We demonstrate our\napproach by rediscovering Kepler's law on planetary motion using exoplanet data\nand Galileo's pendulum periodicity equation using experimental data. \n\n"}
{"id": "1710.10772", "contents": "Title: Tensorizing Generative Adversarial Nets Abstract: Generative Adversarial Network (GAN) and its variants exhibit\nstate-of-the-art performance in the class of generative models. To capture\nhigher-dimensional distributions, the common learning procedure requires high\ncomputational complexity and a large number of parameters. The problem of\nemploying such massive framework arises when deploying it on a platform with\nlimited computational power such as mobile phones. In this paper, we present a\nnew generative adversarial framework by representing each layer as a tensor\nstructure connected by multilinear operations, aiming to reduce the number of\nmodel parameters by a large factor while preserving the generative performance\nand sample quality. To learn the model, we employ an efficient algorithm which\nalternatively optimizes both discriminator and generator. Experimental outcomes\ndemonstrate that our model can achieve high compression rate for model\nparameters up to $35$ times when compared to the original GAN for MNIST\ndataset. \n\n"}
{"id": "1710.11272", "contents": "Title: Empirical analysis of non-linear activation functions for Deep Neural\n  Networks in classification tasks Abstract: We provide an overview of several non-linear activation functions in a neural\nnetwork architecture that have proven successful in many machine learning\napplications. We conduct an empirical analysis on the effectiveness of using\nthese function on the MNIST classification task, with the aim of clarifying\nwhich functions produce the best results overall. Based on this first set of\nresults, we examine the effects of building deeper architectures with an\nincreasing number of hidden layers. We also survey the impact of using, on the\nsame task, different initialisation schemes for the weights of our neural\nnetwork. Using these sets of experiments as a base, we conclude by providing a\noptimal neural network architecture that yields impressive results in accuracy\non the MNIST classification task. \n\n"}
{"id": "1710.11298", "contents": "Title: Effective Tensor Sketching via Sparsification Abstract: In this paper, we investigate effective sketching schemes via sparsification\nfor high dimensional multilinear arrays or tensors. More specifically, we\npropose a novel tensor sparsification algorithm that retains a subset of the\nentries of a tensor in a judicious way, and prove that it can attain a given\nlevel of approximation accuracy in terms of tensor spectral norm with a much\nsmaller sample complexity when compared with existing approaches. In\nparticular, we show that for a $k$th order $d\\times\\cdots\\times d$ cubic tensor\nof {\\it stable rank} $r_s$, the sample size requirement for achieving a\nrelative error $\\varepsilon$ is, up to a logarithmic factor, of the order\n$r_s^{1/2} d^{k/2} /\\varepsilon$ when $\\varepsilon$ is relatively large, and\n$r_s d /\\varepsilon^2$ and essentially optimal when $\\varepsilon$ is\nsufficiently small. It is especially noteworthy that the sample size\nrequirement for achieving a high accuracy is of an order independent of $k$. To\nfurther demonstrate the utility of our techniques, we also study how higher\norder singular value decomposition (HOSVD) of large tensors can be efficiently\napproximated via sparsification. \n\n"}
{"id": "1710.11439", "contents": "Title: Statistical Speech Enhancement Based on Probabilistic Integration of\n  Variational Autoencoder and Non-Negative Matrix Factorization Abstract: This paper presents a statistical method of single-channel speech enhancement\nthat uses a variational autoencoder (VAE) as a prior distribution on clean\nspeech. A standard approach to speech enhancement is to train a deep neural\nnetwork (DNN) to take noisy speech as input and output clean speech. Although\nthis supervised approach requires a very large amount of pair data for\ntraining, it is not robust against unknown environments. Another approach is to\nuse non-negative matrix factorization (NMF) based on basis spectra trained on\nclean speech in advance and those adapted to noise on the fly. This\nsemi-supervised approach, however, causes considerable signal distortion in\nenhanced speech due to the unrealistic assumption that speech spectrograms are\nlinear combinations of the basis spectra. Replacing the poor linear generative\nmodel of clean speech in NMF with a VAE---a powerful nonlinear deep generative\nmodel---trained on clean speech, we formulate a unified probabilistic\ngenerative model of noisy speech. Given noisy speech as observed data, we can\nsample clean speech from its posterior distribution. The proposed method\noutperformed the conventional DNN-based method in unseen noisy environments. \n\n"}
{"id": "1711.00066", "contents": "Title: Fraternal Dropout Abstract: Recurrent neural networks (RNNs) are important class of architectures among\nneural networks useful for language modeling and sequential prediction.\nHowever, optimizing RNNs is known to be harder compared to feed-forward neural\nnetworks. A number of techniques have been proposed in literature to address\nthis problem. In this paper we propose a simple technique called fraternal\ndropout that takes advantage of dropout to achieve this goal. Specifically, we\npropose to train two identical copies of an RNN (that share parameters) with\ndifferent dropout masks while minimizing the difference between their\n(pre-softmax) predictions. In this way our regularization encourages the\nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We\nshow that our regularization term is upper bounded by the expectation-linear\ndropout objective which has been shown to address the gap due to the difference\nbetween the train and inference phases of dropout. We evaluate our model and\nachieve state-of-the-art results in sequence modeling tasks on two benchmark\ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads\nto performance improvement by a significant margin in image captioning\n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks. \n\n"}
{"id": "1711.01204", "contents": "Title: Metrics for Deep Generative Models Abstract: Neural samplers such as variational autoencoders (VAEs) or generative\nadversarial networks (GANs) approximate distributions by transforming samples\nfrom a simple random source---the latent space---to samples from a more complex\ndistribution represented by a dataset. While the manifold hypothesis implies\nthat the density induced by a dataset contains large regions of low density,\nthe training criterions of VAEs and GANs will make the latent space densely\ncovered. Consequently points that are separated by low-density regions in\nobservation space will be pushed together in latent space, making stationary\ndistances poor proxies for similarity. We transfer ideas from Riemannian\ngeometry to this setting, letting the distance between two points be the\nshortest path on a Riemannian manifold induced by the transformation. The\nmethod yields a principled distance measure, provides a tool for visual\ninspection of deep generative models, and an alternative to linear\ninterpolation in latent space. In addition, it can be applied for robot\nmovement generalization using previously learned skills. The method is\nevaluated on a synthetic dataset with known ground truth; on a simulated robot\narm dataset; on human motion capture data; and on a generative model of\nhandwritten digits. \n\n"}
{"id": "1711.01558", "contents": "Title: Wasserstein Auto-Encoders Abstract: We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building\na generative model of the data distribution. WAE minimizes a penalized form of\nthe Wasserstein distance between the model distribution and the target\ndistribution, which leads to a different regularizer than the one used by the\nVariational Auto-Encoder (VAE). This regularizer encourages the encoded\ntraining distribution to match the prior. We compare our algorithm with several\nother techniques and show that it is a generalization of adversarial\nauto-encoders (AAE). Our experiments show that WAE shares many of the\nproperties of VAEs (stable training, encoder-decoder architecture, nice latent\nmanifold structure) while generating samples of better quality, as measured by\nthe FID score. \n\n"}
{"id": "1711.01847", "contents": "Title: Extracting low-dimensional dynamics from multiple large-scale neural\n  population recordings by learning to predict correlations Abstract: A powerful approach for understanding neural population dynamics is to\nextract low-dimensional trajectories from population recordings using\ndimensionality reduction methods. Current approaches for dimensionality\nreduction on neural data are limited to single population recordings, and can\nnot identify dynamics embedded across multiple measurements. We propose an\napproach for extracting low-dimensional dynamics from multiple, sequential\nrecordings. Our algorithm scales to data comprising millions of observed\ndimensions, making it possible to access dynamics distributed across large\npopulations or multiple brain areas. Building on subspace-identification\napproaches for dynamical systems, we perform parameter estimation by minimizing\na moment-matching objective using a scalable stochastic gradient descent\nalgorithm: The model is optimized to predict temporal covariations across\nneurons and across time. We show how this approach naturally handles missing\ndata and multiple partial recordings, and can identify dynamics and predict\ncorrelations even in the presence of severe subsampling and small overlap\nbetween recordings. We demonstrate the effectiveness of the approach both on\nsimulated data and a whole-brain larval zebrafish imaging dataset. \n\n"}
{"id": "1711.02283", "contents": "Title: Large-Scale Optimal Transport and Mapping Estimation Abstract: This paper presents a novel two-step approach for the fundamental problem of\nlearning an optimal map from one distribution to another. First, we learn an\noptimal transport (OT) plan, which can be thought as a one-to-many map between\nthe two distributions. To that end, we propose a stochastic dual approach of\nregularized OT, and show empirically that it scales better than a recent\nrelated approach when the amount of samples is very large. Second, we estimate\na \\textit{Monge map} as a deep neural network learned by approximating the\nbarycentric projection of the previously-obtained OT plan. This\nparameterization allows generalization of the mapping outside the support of\nthe input measure. We prove two theoretical stability results of regularized OT\nwhich show that our estimations converge to the OT plan and Monge map between\nthe underlying continuous measures. We showcase our proposed approach on two\napplications: domain adaptation and generative modeling. \n\n"}
{"id": "1711.02301", "contents": "Title: Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games? Abstract: Deep reinforcement learning has achieved many recent successes, but our\nunderstanding of its strengths and limitations is hampered by the lack of rich\nenvironments in which we can fully characterize optimal behavior, and\ncorrespondingly diagnose individual actions against such a characterization.\nHere we consider a family of combinatorial games, arising from work of Erdos,\nSelfridge, and Spencer, and we propose their use as environments for evaluating\nand comparing different approaches to reinforcement learning. These games have\na number of appealing features: they are challenging for current learning\napproaches, but they form (i) a low-dimensional, simply parametrized\nenvironment where (ii) there is a linear closed form solution for optimal\nbehavior from any state, and (iii) the difficulty of the game can be tuned by\nchanging environment parameters in an interpretable way. We use these\nErdos-Selfridge-Spencer games not only to compare different algorithms, but\ntest for generalization, make comparisons to supervised learning, analyse\nmultiagent play, and even develop a self play algorithm. Code can be found at:\nhttps://github.com/rubai5/ESS_Game \n\n"}
{"id": "1711.02448", "contents": "Title: Cortical microcircuits as gated-recurrent neural networks Abstract: Cortical circuits exhibit intricate recurrent architectures that are\nremarkably similar across different brain areas. Such stereotyped structure\nsuggests the existence of common computational principles. However, such\nprinciples have remained largely elusive. Inspired by gated-memory networks,\nnamely long short-term memory networks (LSTMs), we introduce a recurrent neural\nnetwork in which information is gated through inhibitory cells that are\nsubtractive (subLSTM). We propose a natural mapping of subLSTMs onto known\ncanonical excitatory-inhibitory cortical microcircuits. Our empirical\nevaluation across sequential image classification and language modelling tasks\nshows that subLSTM units can achieve similar performance to LSTM units. These\nresults suggest that cortical circuits can be optimised to solve complex\ncontextual problems and proposes a novel view on their computational function.\nOverall our work provides a step towards unifying recurrent networks as used in\nmachine learning with their biological counterparts. \n\n"}
{"id": "1711.02653", "contents": "Title: Neural system identification for large populations separating \"what\" and\n  \"where\" Abstract: Neuroscientists classify neurons into different types that perform similar\ncomputations at different locations in the visual field. Traditional methods\nfor neural system identification do not capitalize on this separation of 'what'\nand 'where'. Learning deep convolutional feature spaces that are shared among\nmany neurons provides an exciting path forward, but the architectural design\nneeds to account for data limitations: While new experimental techniques enable\nrecordings from thousands of neurons, experimental time is limited so that one\ncan sample only a small fraction of each neuron's response space. Here, we show\nthat a major bottleneck for fitting convolutional neural networks (CNNs) to\nneural data is the estimation of the individual receptive field locations, a\nproblem that has been scratched only at the surface thus far. We propose a CNN\narchitecture with a sparse readout layer factorizing the spatial (where) and\nfeature (what) dimensions. Our network scales well to thousands of neurons and\nshort recordings and can be trained end-to-end. We evaluate this architecture\non ground-truth data to explore the challenges and limitations of CNN-based\nsystem identification. Moreover, we show that our network model outperforms\ncurrent state-of-the art system identification models of mouse primary visual\ncortex. \n\n"}
{"id": "1711.03712", "contents": "Title: Quantized Memory-Augmented Neural Networks Abstract: Memory-augmented neural networks (MANNs) refer to a class of neural network\nmodels equipped with external memory (such as neural Turing machines and memory\nnetworks). These neural networks outperform conventional recurrent neural\nnetworks (RNNs) in terms of learning long-term dependency, allowing them to\nsolve intriguing AI tasks that would otherwise be hard to address. This paper\nconcerns the problem of quantizing MANNs. Quantization is known to be effective\nwhen we deploy deep models on embedded systems with limited resources.\nFurthermore, quantization can substantially reduce the energy consumption of\nthe inference procedure. These benefits justify recent developments of\nquantized multi layer perceptrons, convolutional networks, and RNNs. However,\nno prior work has reported the successful quantization of MANNs. The in-depth\nanalysis presented here reveals various challenges that do not appear in the\nquantization of the other networks. Without addressing them properly, quantized\nMANNs would normally suffer from excessive quantization error which leads to\ndegraded performance. In this paper, we identify memory addressing\n(specifically, content-based addressing) as the main reason for the performance\ndegradation and propose a robust quantization method for MANNs to address the\nchallenge. In our experiments, we achieved a computation-energy gain of 22x\nwith 8-bit fixed-point and binary quantization compared to the floating-point\nimplementation. Measured on the bAbI dataset, the resulting model, named the\nquantized MANN (Q-MANN), improved the error rate by 46% and 30% with 8-bit\nfixed-point and binary quantization, respectively, compared to the MANN\nquantized using conventional techniques. \n\n"}
{"id": "1711.04126", "contents": "Title: Adversarial Training for Disease Prediction from Electronic Health\n  Records with Missing Data Abstract: Electronic health records (EHRs) have contributed to the computerization of\npatient records and can thus be used not only for efficient and systematic\nmedical services, but also for research on biomedical data science. However,\nthere are many missing values in EHRs when provided in matrix form, which is an\nimportant issue in many biomedical EHR applications. In this paper, we propose\na two-stage framework that includes missing data imputation and disease\nprediction to address the missing data problem in EHRs. We compared the disease\nprediction performance of generative adversarial networks (GANs) and\nconventional learning algorithms in combination with missing data prediction\nmethods. As a result, we obtained a level of accuracy of 0.9777, sensitivity of\n0.9521, specificity of 0.9925, area under the receiver operating characteristic\ncurve (AUC-ROC) of 0.9889, and F-score of 0.9688 with a stacked autoencoder as\nthe missing data prediction method and an auxiliary classifier GAN (AC-GAN) as\nthe disease prediction method. The comparison results show that a combination\nof a stacked autoencoder and an AC-GAN significantly outperforms other existing\napproaches. Our results suggest that the proposed framework is more robust for\ndisease prediction from EHRs with missing data. \n\n"}
{"id": "1711.04340", "contents": "Title: Data Augmentation Generative Adversarial Networks Abstract: Effective training of neural networks requires much data. In the low-data\nregime, parameters are underdetermined, and learnt networks generalise poorly.\nData Augmentation alleviates this by using existing data more effectively.\nHowever standard data augmentation produces only limited plausible alternative\ndata. Given there is potential to generate a much broader set of augmentations,\nwe design and train a generative model to do data augmentation. The model,\nbased on image conditional Generative Adversarial Networks, takes data from a\nsource domain and learns to take any data item and generalise it to generate\nother within-class data items. As this generative process does not depend on\nthe classes themselves, it can be applied to novel unseen classes of data. We\nshow that a Data Augmentation Generative Adversarial Network (DAGAN) augments\nstandard vanilla classifiers well. We also show a DAGAN can enhance few-shot\nlearning systems such as Matching Networks. We demonstrate these approaches on\nOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In\nour experiments we can see over 13% increase in accuracy in the low-data regime\nexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face\n(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%\n(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%). \n\n"}
{"id": "1711.04528", "contents": "Title: Simple And Efficient Architecture Search for Convolutional Neural\n  Networks Abstract: Neural networks have recently had a lot of success for many tasks. However,\nneural network architectures that perform well are still typically designed\nmanually by experts in a cumbersome trial-and-error process. We propose a new\nmethod to automatically search for well-performing CNN architectures based on a\nsimple hill climbing procedure whose operators apply network morphisms,\nfollowed by short optimization runs by cosine annealing. Surprisingly, this\nsimple method yields competitive results, despite only requiring resources in\nthe same order of magnitude as training a single network. E.g., on CIFAR-10,\nour method designs and trains networks with an error rate below 6% in only 12\nhours on a single GPU; training for one day reduces this error further, to\nalmost 5%. \n\n"}
{"id": "1711.05136", "contents": "Title: Deep Rewiring: Training very sparse deep networks Abstract: Neuromorphic hardware tends to pose limits on the connectivity of deep\nnetworks that one can run on them. But also generic hardware and software\nimplementations of deep learning run more efficiently for sparse networks.\nSeveral methods exist for pruning connections of a neural network after it was\ntrained without connectivity constraints. We present an algorithm, DEEP R, that\nenables us to train directly a sparsely connected neural network. DEEP R\nautomatically rewires the network during supervised training so that\nconnections are there where they are most needed for the task, while its total\nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used\nto train very sparse feedforward and recurrent neural networks on standard\nbenchmark tasks with just a minor loss in performance. DEEP R is based on a\nrigorous theoretical foundation that views rewiring as stochastic sampling of\nnetwork configurations from a posterior. \n\n"}
{"id": "1711.06821", "contents": "Title: Acquiring Common Sense Spatial Knowledge through Implicit Spatial\n  Templates Abstract: Spatial understanding is a fundamental problem with wide-reaching real-world\napplications. The representation of spatial knowledge is often modeled with\nspatial templates, i.e., regions of acceptability of two objects under an\nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with\nprior work that restricts spatial templates to explicit spatial prepositions\n(e.g., \"glass on table\"), here we extend this concept to implicit spatial\nlanguage, i.e., those relationships (generally actions) for which the spatial\narrangement of the objects is only implicitly implied (e.g., \"man riding\nhorse\"). In contrast with explicit relationships, predicting spatial\narrangements from implicit spatial language requires significant common sense\nspatial understanding. Here, we introduce the task of predicting spatial\ntemplates for two objects under a relationship, which can be seen as a spatial\nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t.\na horse when the man is walking the horse?\"). We present two simple\nneural-based models that leverage annotated images and structured text to learn\nthis task. The good performance of these models reveals that spatial locations\nare to a large extent predictable from implicit spatial language. Crucially,\nthe models attain similar performance in a challenging generalized setting,\nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have\nnever been seen before. Next, we go one step further by presenting the models\nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging\nword embeddings enables the models to output accurate spatial predictions,\nproving that the models acquire solid common sense spatial knowledge allowing\nfor such generalization. \n\n"}
{"id": "1711.07076", "contents": "Title: Does mitigating ML's impact disparity require treatment disparity? Abstract: Following related work in law and policy, two notions of disparity have come\nto shape the study of fairness in algorithmic decision-making. Algorithms\nexhibit treatment disparity if they formally treat members of protected\nsubgroups differently; algorithms exhibit impact disparity when outcomes differ\nacross subgroups, even if the correlation arises unintentionally. Naturally, we\ncan achieve impact parity through purposeful treatment disparity. In one thread\nof technical work, papers aim to reconcile the two forms of parity proposing\ndisparate learning processes (DLPs). Here, the learning algorithm can see group\nmembership during training but produce a classifier that is group-blind at test\ntime. In this paper, we show theoretically that: (i) When other features\ncorrelate to group membership, DLPs will (indirectly) implement treatment\ndisparity, undermining the policy desiderata they are designed to address; (ii)\nWhen group membership is partly revealed by other features, DLPs induce\nwithin-class discrimination; and (iii) In general, DLPs provide a suboptimal\ntrade-off between accuracy and impact parity. Based on our technical analysis,\nwe argue that transparent treatment disparity is preferable to occluded methods\nfor achieving impact parity. Experimental results on several real-world\ndatasets highlight the practical consequences of applying DLPs vs. per-group\nthresholds. \n\n"}
{"id": "1711.07476", "contents": "Title: Virtual Adversarial Ladder Networks For Semi-supervised Learning Abstract: Semi-supervised learning (SSL) partially circumvents the high cost of\nlabeling data by augmenting a small labeled dataset with a large and relatively\ncheap unlabeled dataset drawn from the same distribution. This paper offers a\nnovel interpretation of two deep learning-based SSL approaches, ladder networks\nand virtual adversarial training (VAT), as applying distributional smoothing to\ntheir respective latent spaces. We propose a class of models that fuse these\napproaches. We achieve near-supervised accuracy with high consistency on the\nMNIST dataset using just 5 labels per class: our best model, ladder with\nlayer-wise virtual adversarial noise (LVAN-LW), achieves 1.42% +/- 0.12 average\nerror rate on the MNIST test set, in comparison with 1.62% +/- 0.65 reported\nfor the ladder network. On adversarial examples generated with L2-normalized\nfast gradient method, LVAN-LW trained with 5 examples per class achieves\naverage error rate 2.4% +/- 0.3 compared to 68.6% +/- 6.5 for the ladder\nnetwork and 9.9% +/- 7.5 for VAT. \n\n"}
{"id": "1711.08014", "contents": "Title: The Riemannian Geometry of Deep Generative Models Abstract: Deep generative models learn a mapping from a low dimensional latent space to\na high-dimensional data space. Under certain regularity conditions, these\nmodels parameterize nonlinear manifolds in the data space. In this paper, we\ninvestigate the Riemannian geometry of these generated manifolds. First, we\ndevelop efficient algorithms for computing geodesic curves, which provide an\nintrinsic notion of distance between points on the manifold. Second, we develop\nan algorithm for parallel translation of a tangent vector along a path on the\nmanifold. We show how parallel translation can be used to generate analogies,\ni.e., to transport a change in one data point into a semantically similar\nchange of another data point. Our experiments on real image data show that the\nmanifolds learned by deep generative models, while nonlinear, are surprisingly\nclose to zero curvature. The practical implication is that linear paths in the\nlatent space closely approximate geodesics on the generated manifold. However,\nfurther investigation into this phenomenon is warranted, to identify if there\nare other architectures or datasets where curvature plays a more prominent\nrole. We believe that exploring the Riemannian geometry of deep generative\nmodels, using the tools developed in this paper, will be an important step in\nunderstanding the high-dimensional, nonlinear spaces these models learn. \n\n"}
{"id": "1711.08172", "contents": "Title: Run-and-Inspect Method for Nonconvex Optimization and Global Optimality\n  Bounds for R-Local Minimizers Abstract: Many optimization algorithms converge to stationary points. When the\nunderlying problem is nonconvex, they may get trapped at local minimizers and\noccasionally stagnate near saddle points. We propose the Run-and-Inspect\nMethod, which adds an \"inspect\" phase to existing algorithms that helps escape\nfrom non-global stationary points. The inspection samples a set of points in a\nradius $R$ around the current point. When a sample point yields a sufficient\ndecrease in the objective, we move there and resume an existing algorithm. If\nno sufficient decrease is found, the current point is called an approximate\n$R$-local minimizer. We show that an $R$-local minimizer is globally optimal,\nup to a specific error depending on $R$, if the objective function can be\nimplicitly decomposed into a smooth convex function plus a restricted function\nthat is possibly nonconvex, nonsmooth. For high-dimensional problems, we\nintroduce blockwise inspections to overcome the curse of dimensionality while\nstill maintaining optimality bounds up to a factor equal to the number of\nblocks. Our method performs well on a set of artificial and realistic nonconvex\nproblems by coupling with gradient descent, coordinate descent, EM, and\nprox-linear algorithms. \n\n"}
{"id": "1711.08442", "contents": "Title: From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine\n  Training Through Stopping Sets Abstract: We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC)\nestimators of Restricted Boltzmann Machines (RBMs). We denote our approach\nMarkov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange\nfor random running times. MCLV uses a stopping set built from the training data\nand has maximum number of Markov chain steps K (referred as MCLV-K). We present\na MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and\ndifferences between LVS-K and Contrastive Divergence (CD-K), with LVS-K\nsignificantly outperforming CD-K training RBMs over the MNIST dataset,\nindicating MCLV to be a promising direction in learning generative models. \n\n"}
{"id": "1711.10462", "contents": "Title: Plan, Attend, Generate: Planning for Sequence-to-Sequence Models Abstract: We investigate the integration of a planning mechanism into\nsequence-to-sequence models using attention. We develop a model which can plan\nahead in the future when it computes its alignments between input and output\nsequences, constructing a matrix of proposed future alignments and a commitment\nvector that governs whether to follow or recompute the plan. This mechanism is\ninspired by the recently proposed strategic attentive reader and writer (STRAW)\nmodel for Reinforcement Learning. Our proposed model is end-to-end trainable\nusing primarily differentiable operations. We show that it outperforms a strong\nbaseline on character-level translation tasks from WMT'15, the algorithmic task\nof finding Eulerian circuits of graphs, and question generation from the text.\nOur analysis demonstrates that the model computes qualitatively intuitive\nalignments, converges faster than the baselines, and achieves superior\nperformance with fewer parameters. \n\n"}
{"id": "1712.00409", "contents": "Title: Deep Learning Scaling is Predictable, Empirically Abstract: Deep learning (DL) creates impactful advances following a virtuous recipe:\nmodel architecture search, creating large training data sets, and scaling\ncomputation. It is widely believed that growing training sets and models should\nimprove accuracy and result in better products. As DL application domains grow,\nwe would like a deeper understanding of the relationships between training set\nsize, computational scale, and model accuracy improvements to advance the\nstate-of-the-art.\n  This paper presents a large scale empirical characterization of\ngeneralization error and model size growth as training sets grow. We introduce\na methodology for this measurement and test four machine learning domains:\nmachine translation, language modeling, image processing, and speech\nrecognition. Our empirical results show power-law generalization error scaling\nacross a breadth of factors, resulting in power-law exponents---the \"steepness\"\nof the learning curve---yet to be explained by theoretical work. Further, model\nimprovements only shift the error but do not appear to affect the power-law\nexponent. We also show that model size scales sublinearly with data size. These\nscaling relationships have significant implications on deep learning research,\npractice, and systems. They can assist model debugging, setting accuracy\ntargets, and decisions about data set growth. They can also guide computing\nsystem design and underscore the importance of continued computational scaling. \n\n"}
{"id": "1712.01158", "contents": "Title: Statistical Inference for Incomplete Ranking Data: The Case of\n  Rank-Dependent Coarsening Abstract: We consider the problem of statistical inference for ranking data,\nspecifically rank aggregation, under the assumption that samples are incomplete\nin the sense of not comprising all choice alternatives. In contrast to most\nexisting methods, we explicitly model the process of turning a full ranking\ninto an incomplete one, which we call the coarsening process. To this end, we\npropose the concept of rank-dependent coarsening, which assumes that incomplete\nrankings are produced by projecting a full ranking to a random subset of ranks.\nFor a concrete instantiation of our model, in which full rankings are drawn\nfrom a Plackett-Luce distribution and observations take the form of pairwise\npreferences, we study the performance of various rank aggregation methods. In\naddition to predictive accuracy in the finite sample setting, we address the\ntheoretical question of consistency, by which we mean the ability to recover a\ntarget ranking when the sample size goes to infinity, despite a potential bias\nin the observations caused by the (unknown) coarsening. \n\n"}
{"id": "1712.01378", "contents": "Title: Linearly-Recurrent Autoencoder Networks for Learning Dynamics Abstract: This paper describes a method for learning low-dimensional approximations of\nnonlinear dynamical systems, based on neural-network approximations of the\nunderlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD)\nprovides a useful data-driven approximation of the Koopman operator for\nanalyzing dynamical systems. This paper addresses a fundamental problem\nassociated with EDMD: a trade-off between representational capacity of the\ndictionary and over-fitting due to insufficient data. A new neural network\narchitecture combining an autoencoder with linear recurrent dynamics in the\nencoded state is used to learn a low-dimensional and highly informative\nKoopman-invariant subspace of observables. A method is also presented for\nbalanced model reduction of over-specified EDMD systems in feature space.\nNonlinear reconstruction using partially linear multi-kernel regression aims to\nimprove reconstruction accuracy from the low-dimensional state when the data\nhas complex but intrinsically low-dimensional structure. The techniques\ndemonstrate the ability to identify Koopman eigenfunctions of the unforced\nDuffing equation, create accurate low-dimensional models of an unstable\ncylinder wake flow, and make short-time predictions of the chaotic\nKuramoto-Sivashinsky equation. \n\n"}
{"id": "1712.01887", "contents": "Title: Deep Gradient Compression: Reducing the Communication Bandwidth for\n  Distributed Training Abstract: Large-scale distributed training requires significant communication bandwidth\nfor gradient exchange that limits the scalability of multi-node training, and\nrequires expensive high-bandwidth network infrastructure. The situation gets\neven worse with distributed training on mobile devices (federated learning),\nwhich suffers from higher latency, lower throughput, and intermittent poor\nconnections. In this paper, we find 99.9% of the gradient exchange in\ndistributed SGD is redundant, and propose Deep Gradient Compression (DGC) to\ngreatly reduce the communication bandwidth. To preserve accuracy during\ncompression, DGC employs four methods: momentum correction, local gradient\nclipping, momentum factor masking, and warm-up training. We have applied Deep\nGradient Compression to image classification, speech recognition, and language\nmodeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and\nLibrispeech Corpus. On these scenarios, Deep Gradient Compression achieves a\ngradient compression ratio from 270x to 600x without losing accuracy, cutting\nthe gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from\n488MB to 0.74MB. Deep gradient compression enables large-scale distributed\ntraining on inexpensive commodity 1Gbps Ethernet and facilitates distributed\ntraining on mobile. Code is available at:\nhttps://github.com/synxlin/deep-gradient-compression. \n\n"}
{"id": "1712.02412", "contents": "Title: Estimating the error variance in a high-dimensional linear model Abstract: The lasso has been studied extensively as a tool for estimating the\ncoefficient vector in the high-dimensional linear model; however, considerably\nless is known about estimating the error variance in this context. In this\npaper, we propose the natural lasso estimator for the error variance, which\nmaximizes a penalized likelihood objective. A key aspect of the natural lasso\nis that the likelihood is expressed in terms of the natural parameterization of\nthe multiparameter exponential family of a Gaussian with unknown mean and\nvariance. The result is a remarkably simple estimator of the error variance\nwith provably good performance in terms of mean squared error. These\ntheoretical results do not require placing any assumptions on the design matrix\nor the true regression coefficients. We also propose a companion estimator,\ncalled the organic lasso, which theoretically does not require tuning of the\nregularization parameter. Both estimators do well empirically compared to\npreexisting methods, especially in settings where successful recovery of the\ntrue support of the coefficient vector is hard. Finally, we show that existing\nmethods can do well under fewer assumptions than previously known, thus\nproviding a fuller story about the problem of estimating the error variance in\nhigh-dimensional linear models. \n\n"}
{"id": "1712.02734", "contents": "Title: Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for\n  Transferable Chemical Property Prediction Abstract: With access to large datasets, deep neural networks (DNN) have achieved\nhuman-level accuracy in image and speech recognition tasks. However, in\nchemistry, data is inherently small and fragmented. In this work, we develop an\napproach of using rule-based knowledge for training ChemNet, a transferable and\ngeneralizable deep neural network for chemical property prediction that learns\nin a weak-supervised manner from large unlabeled chemical databases. When\ncoupled with transfer learning approaches to predict other smaller datasets for\nchemical properties that it was not originally trained on, we show that\nChemNet's accuracy outperforms contemporary DNN models that were trained using\nconventional supervised learning. Furthermore, we demonstrate that the ChemNet\npre-training approach is equally effective on both CNN (Chemception) and RNN\n(SMILES2vec) models, indicating that this approach is network architecture\nagnostic and is effective across multiple data modalities. Our results indicate\na pre-trained ChemNet that incorporates chemistry domain knowledge, enables the\ndevelopment of generalizable neural networks for more accurate prediction of\nnovel chemical properties. \n\n"}
{"id": "1712.02902", "contents": "Title: Multiple Adaptive Bayesian Linear Regression for Scalable Bayesian\n  Optimization with Warm Start Abstract: Bayesian optimization (BO) is a model-based approach for gradient-free\nblack-box function optimization. Typically, BO is powered by a Gaussian process\n(GP), whose algorithmic complexity is cubic in the number of evaluations.\nHence, GP-based BO cannot leverage large amounts of past or related function\nevaluations, for example, to warm start the BO procedure. We develop a multiple\nadaptive Bayesian linear regression model as a scalable alternative whose\ncomplexity is linear in the number of observations. The multiple Bayesian\nlinear regression models are coupled through a shared feedforward neural\nnetwork, which learns a joint representation and transfers knowledge across\nmachine learning problems. \n\n"}
{"id": "1712.03563", "contents": "Title: DGCNN: Disordered Graph Convolutional Neural Network Based on the\n  Gaussian Mixture Model Abstract: Convolutional neural networks (CNNs) can be applied to graph similarity\nmatching, in which case they are called graph CNNs. Graph CNNs are attracting\nincreasing attention due to their effectiveness and efficiency. However, the\nexisting convolution approaches focus only on regular data forms and require\nthe transfer of the graph or key node neighborhoods of the graph into the same\nfixed form. During this transfer process, structural information of the graph\ncan be lost, and some redundant information can be incorporated. To overcome\nthis problem, we propose the disordered graph convolutional neural network\n(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a\npreprocessing layer called the disordered graph convolutional layer (DGCL). The\nDGCL uses a mixed Gaussian function to realize the mapping between the\nconvolution kernel and the nodes in the neighborhood of the graph. The output\nof the DGCL is the input of the CNN. We further implement a\nbackward-propagation optimization process of the convolutional layer by which\nwe incorporate the feature-learning model of the irregular node neighborhood\nstructure into the network. Thereafter, the optimization of the convolution\nkernel becomes part of the neural network learning process. The DGCNN can\naccept arbitrary scaled and disordered neighborhood graph structures as the\nreceptive fields of CNNs, which reduces information loss during graph\ntransformation. Finally, we perform experiments on multiple standard graph\ndatasets. The results show that the proposed method outperforms the\nstate-of-the-art methods in graph classification and retrieval. \n\n"}
{"id": "1712.04407", "contents": "Title: Logo Synthesis and Manipulation with Clustered Generative Adversarial\n  Networks Abstract: Designing a logo for a new brand is a lengthy and tedious back-and-forth\nprocess between a designer and a client. In this paper we explore to what\nextent machine learning can solve the creative task of the designer. For this,\nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web.\nTraining Generative Adversarial Networks (GANs) for logo synthesis on such\nmulti-modal data is not straightforward and results in mode collapse for some\nstate-of-the-art methods. We propose the use of synthetic labels obtained\nthrough clustering to disentangle and stabilize GAN training. We are able to\ngenerate a high diversity of plausible logos and we demonstrate latent space\nexploration techniques to ease the logo design task in an interactive manner.\nMoreover, we validate the proposed clustered GAN training on CIFAR 10,\nachieving state-of-the-art Inception scores when using synthetic labels\nobtained via clustering the features of an ImageNet classifier. GANs can cope\nwith multi-modal data by means of synthetic labels achieved through clustering,\nand our results show the creative potential of such techniques for logo\nsynthesis and manipulation. Our dataset and models will be made publicly\navailable at https://data.vision.ee.ethz.ch/cvl/lld/. \n\n"}
{"id": "1712.05134", "contents": "Title: Learning Compact Recurrent Neural Networks with Block-Term Tensor\n  Decomposition Abstract: Recurrent Neural Networks (RNNs) are powerful sequence modeling tools.\nHowever, when dealing with high dimensional inputs, the training of RNNs\nbecomes computational expensive due to the large number of model parameters.\nThis hinders RNNs from solving many important computer vision tasks, such as\nAction Recognition in Videos and Image Captioning. To overcome this problem, we\npropose a compact and flexible structure, namely Block-Term tensor\ndecomposition, which greatly reduces the parameters of RNNs and improves their\ntraining efficiency. Compared with alternative low-rank approximations, such as\ntensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only\nmore concise (when using the same rank), but also able to attain a better\napproximation to the original RNNs with much fewer parameters. On three\nchallenging tasks, including Action Recognition in Videos, Image Captioning and\nImage Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of\nboth prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes\n17,388 times fewer parameters than the standard LSTM to achieve an accuracy\nimprovement over 15.6\\% in the Action Recognition task on the UCF11 dataset. \n\n"}
{"id": "1712.06006", "contents": "Title: How well does your sampler really work? Abstract: We present a new data-driven benchmark system to evaluate the performance of\nnew MCMC samplers. Taking inspiration from the COCO benchmark in optimization,\nwe view this task as having critical importance to machine learning and\nstatistics given the rate at which new samplers are proposed. The common\nhand-crafted examples to test new samplers are unsatisfactory; we take a\nmeta-learning-like approach to generate benchmark examples from a large corpus\nof data sets and models. Surrogates of posteriors found in real problems are\ncreated using highly flexible density models including modern neural network\nbased approaches. We provide new insights into the real effective sample size\nof various samplers per unit time and the estimation efficiency of the samplers\nper sample. Additionally, we provide a meta-analysis to assess the predictive\nutility of various MCMC diagnostics and perform a nonparametric regression to\ncombine them. \n\n"}
{"id": "1712.06657", "contents": "Title: Towards the Augmented Pathologist: Challenges of Explainable-AI in\n  Digital Pathology Abstract: Digital pathology is not only one of the most promising fields of diagnostic\nmedicine, but at the same time a hot topic for fundamental research. Digital\npathology is not just the transfer of histopathological slides into digital\nrepresentations. The combination of different data sources (images, patient\nrecords, and *omics data) together with current advances in artificial\nintelligence/machine learning enable to make novel information accessible and\nquantifiable to a human expert, which is not yet available and not exploited in\ncurrent medical settings. The grand goal is to reach a level of usable\nintelligence to understand the data in the context of an application task,\nthereby making machine decisions transparent, interpretable and explainable.\nThe foundation of such an \"augmented pathologist\" needs an integrated approach:\nWhile machine learning algorithms require many thousands of training examples,\na human expert is often confronted with only a few data points. Interestingly,\nhumans can learn from such few examples and are able to instantly interpret\ncomplex patterns. Consequently, the grand goal is to combine the possibilities\nof artificial intelligence with human intelligence and to find a well-suited\nbalance between them to enable what neither of them could do on their own. This\ncan raise the quality of education, diagnosis, prognosis and prediction of\ncancer and other diseases. In this paper we describe some (incomplete) research\nissues which we believe should be addressed in an integrated and concerted\neffort for paving the way towards the augmented pathologist. \n\n"}
{"id": "1712.06715", "contents": "Title: Deformable Classifiers Abstract: Geometric variations of objects, which do not modify the object class, pose a\nmajor challenge for object recognition. These variations could be rigid as well\nas non-rigid transformations. In this paper, we design a framework for training\ndeformable classifiers, where latent transformation variables are introduced,\nand a transformation of the object image to a reference instantiation is\ncomputed in terms of the classifier output, separately for each class. The\nclassifier outputs for each class, after transformation, are compared to yield\nthe final decision. As a by-product of the classification this yields a\ntransformation of the input object to a reference pose, which can be used for\ndownstream tasks such as the computation of object support. We apply a two-step\ntraining mechanism for our framework, which alternates between optimizing over\nthe latent transformation variables and the classifier parameters to minimize\nthe loss function. We show that multilayer perceptrons, also known as deep\nnetworks, are well suited for this approach and achieve state of the art\nresults on the rotated MNIST and the Google Earth dataset, and produce\ncompetitive results on MNIST and CIFAR-10 when training on smaller subsets of\ntraining data. \n\n"}
{"id": "1712.07177", "contents": "Title: Approximate Profile Maximum Likelihood Abstract: We propose an efficient algorithm for approximate computation of the profile\nmaximum likelihood (PML), a variant of maximum likelihood maximizing the\nprobability of observing a sufficient statistic rather than the empirical\nsample. The PML has appealing theoretical properties, but is difficult to\ncompute exactly. Inspired by observations gleaned from exactly solvable cases,\nwe look for an approximate PML solution, which, intuitively, clumps comparably\nfrequent symbols into one symbol. This amounts to lower-bounding a certain\nmatrix permanent by summing over a subgroup of the symmetric group rather than\nthe whole group during the computation. We extensively experiment with the\napproximate solution, and find the empirical performance of our approach is\ncompetitive and sometimes significantly better than state-of-the-art\nperformance for various estimation problems. \n\n"}
{"id": "1712.07436", "contents": "Title: Incremental Adversarial Domain Adaptation for Continually Changing\n  Environments Abstract: Continuous appearance shifts such as changes in weather and lighting\nconditions can impact the performance of deployed machine learning models.\nWhile unsupervised domain adaptation aims to address this challenge, current\napproaches do not utilise the continuity of the occurring shifts. In\nparticular, many robotics applications exhibit these conditions and thus\nfacilitate the potential to incrementally adapt a learnt model over minor\nshifts which integrate to massive differences over time. Our work presents an\nadversarial approach for lifelong, incremental domain adaptation which benefits\nfrom unsupervised alignment to a series of intermediate domains which\nsuccessively diverge from the labelled source domain. We empirically\ndemonstrate that our incremental approach improves handling of large appearance\nchanges, e.g. day to night, on a traversable-path segmentation task compared\nwith a direct, single alignment step approach. Furthermore, by approximating\nthe feature distribution for the source domain with a generative adversarial\nnetwork, the deployment module can be rendered fully independent of retaining\npotentially large amounts of the related source training data for only a minor\nreduction in performance. \n\n"}
{"id": "1712.09005", "contents": "Title: Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding Abstract: t-distributed Stochastic Neighborhood Embedding (t-SNE) is a method for\ndimensionality reduction and visualization that has become widely popular in\nrecent years. Efficient implementations of t-SNE are available, but they scale\npoorly to datasets with hundreds of thousands to millions of high dimensional\ndata-points. We present Fast Fourier Transform-accelerated Interpolation-based\nt-SNE (FIt-SNE), which dramatically accelerates the computation of t-SNE. The\nmost time-consuming step of t-SNE is a convolution that we accelerate by\ninterpolating onto an equispaced grid and subsequently using the fast Fourier\ntransform to perform the convolution. We also optimize the computation of input\nsimilarities in high dimensions using multi-threaded approximate nearest\nneighbors. We further present a modification to t-SNE called \"late\nexaggeration,\" which allows for easier identification of clusters in t-SNE\nembeddings. Finally, for datasets that cannot be loaded into the memory, we\npresent out-of-core randomized principal component analysis (oocPCA), so that\nthe top principal components of a dataset can be computed without ever fully\nloading the matrix, hence allowing for t-SNE of large datasets to be computed\non resource-limited machines. \n\n"}
{"id": "1712.09203", "contents": "Title: Algorithmic Regularization in Over-parameterized Matrix Sensing and\n  Neural Networks with Quadratic Activations Abstract: We show that the gradient descent algorithm provides an implicit\nregularization effect in the learning of over-parameterized matrix\nfactorization models and one-hidden-layer neural networks with quadratic\nactivations. Concretely, we show that given $\\tilde{O}(dr^{2})$ random linear\nmeasurements of a rank $r$ positive semidefinite matrix $X^{\\star}$, we can\nrecover $X^{\\star}$ by parameterizing it by $UU^\\top$ with $U\\in \\mathbb\nR^{d\\times d}$ and minimizing the squared loss, even if $r \\ll d$. We prove\nthat starting from a small initialization, gradient descent recovers\n$X^{\\star}$ in $\\tilde{O}(\\sqrt{r})$ iterations approximately. The results\nsolve the conjecture of Gunasekar et al.'17 under the restricted isometry\nproperty. The technique can be applied to analyzing neural networks with\none-hidden-layer quadratic activations with some technical modifications. \n\n"}
{"id": "1712.09983", "contents": "Title: Random Feature-based Online Multi-kernel Learning in Environments with\n  Unknown Dynamics Abstract: Kernel-based methods exhibit well-documented performance in various nonlinear\nlearning tasks. Most of them rely on a preselected kernel, whose prudent choice\npresumes task-specific prior information. Especially when the latter is not\navailable, multi-kernel learning has gained popularity thanks to its\nflexibility in choosing kernels from a prescribed kernel dictionary. Leveraging\nthe random feature approximation and its recent orthogonality-promoting\nvariant, the present contribution develops a scalable multi-kernel learning\nscheme (termed Raker) to obtain the sought nonlinear learning function `on the\nfly,' first for static environments. To further boost performance in dynamic\nenvironments, an adaptive multi-kernel learning scheme (termed AdaRaker) is\ndeveloped. AdaRaker accounts not only for data-driven learning of kernel\ncombination, but also for the unknown dynamics. Performance is analyzed in\nterms of both static and dynamic regrets. AdaRaker is uniquely capable of\ntracking nonlinear learning functions in environments with unknown dynamics,\nand with with analytic performance guarantees. Tests with synthetic and real\ndatasets are carried out to showcase the effectiveness of the novel algorithms. \n\n"}
{"id": "1801.00905", "contents": "Title: Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space Abstract: Recently, Neural networks have seen a huge surge in its adoption due to their\nability to provide high accuracy on various tasks. On the other hand, the\nexistence of adversarial examples have raised suspicions regarding the\ngeneralization capabilities of neural networks. In this work, we focus on the\nweight matrix learnt by the neural networks and hypothesize that ill\nconditioned weight matrix is one of the contributing factors in neural\nnetwork's susceptibility towards adversarial examples. For ensuring that the\nlearnt weight matrix's condition number remains sufficiently low, we suggest\nusing orthogonal regularizer. We show that this indeed helps in increasing the\nadversarial accuracy on MNIST and F-MNIST datasets. \n\n"}
{"id": "1801.03744", "contents": "Title: Which Neural Net Architectures Give Rise To Exploding and Vanishing\n  Gradients? Abstract: We give a rigorous analysis of the statistical behavior of gradients in a\nrandomly initialized fully connected network N with ReLU activations. Our\nresults show that the empirical variance of the squares of the entries in the\ninput-output Jacobian of N is exponential in a simple architecture-dependent\nconstant beta, given by the sum of the reciprocals of the hidden layer widths.\nWhen beta is large, the gradients computed by N at initialization vary wildly.\nOur approach complements the mean field theory analysis of random networks.\nFrom this point of view, we rigorously compute finite width corrections to the\nstatistics of gradients at the edge of chaos. \n\n"}
{"id": "1801.04055", "contents": "Title: A3T: Adversarially Augmented Adversarial Training Abstract: Recent research showed that deep neural networks are highly sensitive to\nso-called adversarial perturbations, which are tiny perturbations of the input\ndata purposely designed to fool a machine learning classifier. Most\nclassification models, including deep learning models, are highly vulnerable to\nadversarial attacks. In this work, we investigate a procedure to improve\nadversarial robustness of deep neural networks through enforcing representation\ninvariance. The idea is to train the classifier jointly with a discriminator\nattached to one of its hidden layer and trained to filter the adversarial\nnoise. We perform preliminary experiments to test the viability of the approach\nand to compare it to other standard adversarial training methods. \n\n"}
{"id": "1801.04212", "contents": "Title: Multinomial logistic model for coinfection diagnosis between arbovirus\n  and malaria in Kedougou Abstract: In tropical regions, populations continue to suffer morbidity and mortality\nfrom malaria and arboviral diseases. In Kedougou (Senegal), these illnesses are\nall endemic due to the climate and its geographical position. The\nco-circulation of malaria parasites and arboviruses can explain the observation\nof coinfected cases. Indeed there is strong resemblance in symptoms between\nthese diseases making problematic targeted medical care of coinfected cases.\nThis is due to the fact that the origin of illness is not obviously known. Some\ncases could be immunized against one or the other of the pathogens, immunity\ntypically acquired with factors like age and exposure as usual for endemic\narea. Then, coinfection needs to be better diagnosed. Using data collected from\npatients in Kedougou region, from 2009 to 2013, we adjusted a multinomial\nlogistic model and selected relevant variables in explaining coinfection\nstatus. We observed specific sets of variables explaining each of the diseases\nexclusively and the coinfection. We tested the independence between arboviral\nand malaria infections and derived coinfection probabilities from the model\nfitting. In case of a coinfection probability greater than a threshold value to\nbe calibrated on the data, duration of illness above 3 days and age above 10\nyears-old are mostly indicative of arboviral disease while body temperature\nhigher than 40{\\textdegree}C and presence of nausea or vomiting symptoms during\nthe rainy season are mostly indicative of malaria disease. \n\n"}
{"id": "1801.04918", "contents": "Title: Time invariant $\\mathcal{PT}$-product and phase locking in\n  $\\mathcal{PT}$-symmetric lattice models Abstract: Over the past decade, non-Hermitian, $\\mathcal{PT}$-symmetric Hamiltonians\nhave been investigated as candidates for both, a fundamental, unitary, quantum\ntheory, and open systems with a non-unitary time evolution. In this paper, we\ninvestigate the implications of the former approach in the context of the\nlatter. Motivated by the invariance of the $\\mathcal{PT}$ (inner) product under\ntime evolution, we discuss the dynamics of wave-function phases in a wide range\nof $\\mathcal{PT}$-symmetric lattice models. In particular, we numerically show\nthat, starting with a random initial state, a universal, gain-site location\ndependent locking between wave function phases at adjacent sites occurs in the\n$\\mathcal{PT}$-symmetry broken region. Our results pave the way towards\nunderstanding the physically observable implications of time-invariants in the\nnon-unitary dynamics produced by $\\mathcal{PT}$-symmetric Hamiltonians. \n\n"}
{"id": "1801.05558", "contents": "Title: Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace Abstract: Gradient-based meta-learning methods leverage gradient descent to learn the\ncommonalities among various tasks. While previous such methods have been\nsuccessful in meta-learning tasks, they resort to simple gradient descent\nduring meta-testing. Our primary contribution is the {\\em MT-net}, which\nenables the meta-learner to learn on each layer's activation space a subspace\nthat the task-specific learner performs gradient descent on. Additionally, a\ntask-specific learner of an {\\em MT-net} performs gradient descent with respect\nto a meta-learned distance metric, which warps the activation space to be more\nsensitive to task identity. We demonstrate that the dimension of this learned\nsubspace reflects the complexity of the task-specific learner's adaptation\ntask, and also that our model is less sensitive to the choice of initial\nlearning rates than previous gradient-based meta-learning methods. Our method\nachieves state-of-the-art or comparable performance on few-shot classification\nand regression tasks. \n\n"}
{"id": "1801.07145", "contents": "Title: E-swish: Adjusting Activations to Different Network Depths Abstract: Activation functions have a notorious impact on neural networks on both\ntraining and testing the models against the desired problem. Currently, the\nmost used activation function is the Rectified Linear Unit (ReLU). This paper\nintroduces a new and novel activation function, closely related with the new\nactivation $Swish = x * sigmoid(x)$ (Ramachandran et al., 2017) which\ngeneralizes it. We call the new activation $E-swish = \\beta x * sigmoid(x)$. We\nshow that E-swish outperforms many other well-known activations including both\nReLU and Swish. For example, using E-swish provided 1.5% and 4.6% accuracy\nimprovements on Cifar10 and Cifar100 respectively for the WRN 10-2 when\ncompared to ReLU and 0.35% and 0.6% respectively when compared to Swish. The\ncode to reproduce all our experiments can be found at\nhttps://github.com/EricAlcaide/E-swish \n\n"}
{"id": "1801.07194", "contents": "Title: Optimizing Prediction Intervals by Tuning Random Forest via\n  Meta-Validation Abstract: Recent studies have shown that tuning prediction models increases prediction\naccuracy and that Random Forest can be used to construct prediction intervals.\nHowever, to our best knowledge, no study has investigated the need to, and the\nmanner in which one can, tune Random Forest for optimizing prediction intervals\n{ this paper aims to fill this gap. We explore a tuning approach that combines\nan effectively exhaustive search with a validation technique on a single Random\nForest parameter. This paper investigates which, out of eight validation\ntechniques, are beneficial for tuning, i.e., which automatically choose a\nRandom Forest configuration constructing prediction intervals that are reliable\nand with a smaller width than the default configuration. Additionally, we\npresent and validate three meta-validation techniques to determine which are\nbeneficial, i.e., those which automatically chose a beneficial validation\ntechnique. This study uses data from our industrial partner (Keymind Inc.) and\nthe Tukutuku Research Project, related to post-release defect prediction and\nWeb application effort estimation, respectively. Results from our study\nindicate that: i) the default configuration is frequently unreliable, ii) most\nof the validation techniques, including previously successfully adopted ones\nsuch as 50/50 holdout and bootstrap, are counterproductive in most of the\ncases, and iii) the 75/25 holdout meta-validation technique is always\nbeneficial; i.e., it avoids the likely counterproductive effects of validation\ntechniques. \n\n"}
{"id": "1801.07384", "contents": "Title: Hybrid Gradient Boosting Trees and Neural Networks for Forecasting\n  Operating Room Data Abstract: Time series data constitutes a distinct and growing problem in machine\nlearning. As the corpus of time series data grows larger, deep models that\nsimultaneously learn features and classify with these features can be\nintractable or suboptimal. In this paper, we present feature learning via long\nshort term memory (LSTM) networks and prediction via gradient boosting trees\n(XGB). Focusing on the consequential setting of electronic health record data,\nwe predict the occurrence of hypoxemia five minutes into the future based on\npast features. We make two observations: 1) long short term memory networks are\neffective at capturing long term dependencies based on a single feature and 2)\ngradient boosting trees are capable of tractably combining a large number of\nfeatures including static features like height and weight. With these\nobservations in mind, we generate features by performing \"supervised\"\nrepresentation learning with LSTM networks. Augmenting the original XGB model\nwith these features gives significantly better performance than either\nindividual method. \n\n"}
{"id": "1801.10579", "contents": "Title: Distinguishing Cause from Effect Using Quantiles: Bivariate Quantile\n  Causal Discovery Abstract: Causal inference using observational data is challenging, especially in the\nbivariate case. Through the minimum description length principle, we link the\npostulate of independence between the generating mechanisms of the cause and of\nthe effect given the cause to quantile regression. Based on this theory, we\ndevelop Bivariate Quantile Causal Discovery (bQCD), a new method to distinguish\ncause from effect assuming no confounding, selection bias or feedback. Because\nit uses multiple quantile levels instead of the conditional mean only, bQCD is\nadaptive not only to additive, but also to multiplicative or even\nlocation-scale generating mechanisms. To illustrate the effectiveness of our\napproach, we perform an extensive empirical comparison on both synthetic and\nreal datasets. This study shows that bQCD is robust across different\nimplementations of the method (i.e., the quantile regression), computationally\nefficient, and compares favorably to state-of-the-art methods. \n\n"}
{"id": "1802.00912", "contents": "Title: Active, Continual Fine Tuning of Convolutional Neural Networks for\n  Reducing Annotation Efforts Abstract: The splendid success of convolutional neural networks (CNNs) in computer\nvision is largely attributable to the availability of massive annotated\ndatasets, such as ImageNet and Places. However, in medical imaging, it is\nchallenging to create such large annotated datasets, as annotating medical\nimages is not only tedious, laborious, and time consuming, but it also demands\ncostly, specialty-oriented skills, which are not easily accessible. To\ndramatically reduce annotation cost, this paper presents a novel method to\nnaturally integrate active learning and transfer learning (fine-tuning) into a\nsingle framework, which starts directly with a pre-trained CNN to seek \"worthy\"\nsamples for annotation and gradually enhances the (fine-tuned) CNN via\ncontinual fine-tuning. We have evaluated our method using three distinct\nmedical imaging applications, demonstrating that it can reduce annotation\nefforts by at least half compared with random selection. \n\n"}
{"id": "1802.01610", "contents": "Title: Fast and accurate approximation of the full conditional for gamma shape\n  parameters Abstract: The gamma distribution arises frequently in Bayesian models, but there is not\nan easy-to-use conjugate prior for the shape parameter of a gamma. This\ninconvenience is usually dealt with by using either Metropolis-Hastings moves,\nrejection sampling methods, or numerical integration. However, in models with a\nlarge number of shape parameters, these existing methods are slower or more\ncomplicated than one would like, making them burdensome in practice. It turns\nout that the full conditional distribution of the gamma shape parameter is well\napproximated by a gamma distribution, even for small sample sizes, when the\nprior on the shape parameter is also a gamma distribution. This article\nintroduces a quick and easy algorithm for finding a gamma distribution that\napproximates the full conditional distribution of the shape parameter. We\nempirically demonstrate the speed and accuracy of the approximation across a\nwide range of conditions. If exactness is required, the approximation can be\nused as a proposal distribution for Metropolis-Hastings. \n\n"}
{"id": "1802.01697", "contents": "Title: Deep Learning with a Rethinking Structure for Multi-label Classification Abstract: Multi-label classification (MLC) is an important class of machine learning\nproblems that come with a wide spectrum of applications, each demanding a\npossibly different evaluation criterion. When solving the MLC problems, we\ngenerally expect the learning algorithm to take the hidden correlation of the\nlabels into account to improve the prediction performance. Extracting the\nhidden correlation is generally a challenging task. In this work, we propose a\nnovel deep learning framework to better extract the hidden correlation with the\nhelp of the memory structure within recurrent neural networks. The memory\nstores the temporary guesses on the labels and effectively allows the framework\nto rethink about the goodness and correlation of the guesses before making the\nfinal prediction. Furthermore, the rethinking process makes it easy to adapt to\ndifferent evaluation criteria to match real-world application needs. In\nparticular, the framework can be trained in an end-to-end style with respect to\nany given MLC evaluation criteria. The end-to-end design can be seamlessly\ncombined with other deep learning techniques to conquer challenging MLC\nproblems like image tagging. Experimental results across many real-world data\nsets justify that the rethinking framework indeed improves MLC performance\nacross different evaluation criteria and leads to superior performance over\nstate-of-the-art MLC algorithms. \n\n"}
{"id": "1802.02137", "contents": "Title: An Occluded Stacked Hourglass Approach to Facial Landmark Localization\n  and Occlusion Estimation Abstract: A key step to driver safety is to observe the driver's activities with the\nface being a key step in this process to extracting information such as head\npose, blink rate, yawns, talking to passenger which can then help derive higher\nlevel information such as distraction, drowsiness, intent, and where they are\nlooking. In the context of driving safety, it is important for the system\nperform robust estimation under harsh lighting and occlusion but also be able\nto detect when the occlusion occurs so that information predicted from occluded\nparts of the face can be taken into account properly. This paper introduces the\nOccluded Stacked Hourglass, based on the work of original Stacked Hourglass\nnetwork for body pose joint estimation, which is retrained to process a\ndetected face window and output 68 occlusion heat maps, each corresponding to a\nfacial landmark. Landmark location, occlusion levels and a refined face\ndetection score, to reject false positives, are extracted from these heat maps.\nUsing the facial landmark locations, features such as head pose and eye/mouth\nopenness can be extracted to derive driver attention and activity. The system\nis evaluated for face detection, head pose, and occlusion estimation on various\ndatasets in the wild, both quantitatively and qualitatively, and shows\nstate-of-the-art results. \n\n"}
{"id": "1802.02147", "contents": "Title: DeepTravel: a Neural Network Based Travel Time Estimation Model with\n  Auxiliary Supervision Abstract: Estimating the travel time of a path is of great importance to smart urban\nmobility. Existing approaches are either based on estimating the time cost of\neach road segment which are not able to capture many cross-segment complex\nfactors, or designed heuristically in a non-learning-based way which fail to\nutilize the existing abundant temporal labels of the data, i.e., the time stamp\nof each trajectory point. In this paper, we leverage on new development of deep\nneural networks and propose a novel auxiliary supervision model, namely\nDeepTravel, that can automatically and effectively extract different features,\nas well as make full use of the temporal labels of the trajectory data. We have\nconducted comprehensive experiments on real datasets to demonstrate the\nout-performance of DeepTravel over existing approaches. \n\n"}
{"id": "1802.02300", "contents": "Title: Quantum-optimal detection of one-versus-two incoherent optical sources\n  with arbitrary separation Abstract: We analyze the fundamental quantum limit of the resolution of an optical\nimaging system from the perspective of the detection problem of deciding\nwhether the optical field in the image plane is generated by one incoherent\non-axis source with brightness $\\epsilon$ or by two $\\epsilon/2$-brightness\nincoherent sources that are symmetrically disposed about the optical axis.\nUsing the exact thermal-state model of the field, we derive the quantum\nChernoff bound for the detection problem, which specifies the optimum rate of\ndecay of the error probability with increasing number of collected photons that\nis allowed by quantum mechanics. We then show that recently proposed\nlinear-optic schemes approach the quantum Chernoff bound---the method of binary\nspatial-mode demultiplexing (B-SPADE) is quantum-optimal for all values of\nseparation, while a method using image-inversion interferometry (SLIVER) is\nnear-optimal for sub-Rayleigh separations. We then simplify our model using a\nlow-brightness approximation that is very accurate for optical microscopy and\nastronomy, derive quantum Chernoff bounds conditional on the number of photons\ndetected, and show the optimality of our schemes in this conditional detection\nparadigm. For comparison, we analytically demonstrate the superior scaling of\nthe Chernoff bound for our schemes with source separation relative to that of\nspatially-resolved direct imaging. Our schemes have the advantages over the\nquantum-optimal (Helstrom) measurement in that they do not involve joint\nmeasurements over multiple modes, and that they do not require the angular\nseparation for the two-source hypothesis to be given \\emph{a priori} and can\noffer that information as a bonus in the event of a successful detection. \n\n"}
{"id": "1802.02498", "contents": "Title: Spectral Learning of Binomial HMMs for DNA Methylation Data Abstract: We consider learning parameters of Binomial Hidden Markov Models, which may\nbe used to model DNA methylation data. The standard algorithm for the problem\nis EM, which is computationally expensive for sequences of the scale of the\nmammalian genome. Recently developed spectral algorithms can learn parameters\nof latent variable models via tensor decomposition, and are highly efficient\nfor large data. However, these methods have only been applied to categorial\nHMMs, and the main challenge is how to extend them to Binomial HMMs while still\nretaining computational efficiency. We address this challenge by introducing a\nnew feature-map based approach that exploits specific properties of Binomial\nHMMs. We provide theoretical performance guarantees for our algorithm and\nevaluate it on real DNA methylation data. \n\n"}
{"id": "1802.03039", "contents": "Title: Few-shot learning of neural networks from scratch by pseudo example\n  optimization Abstract: In this paper, we propose a simple but effective method for training neural\nnetworks with a limited amount of training data. Our approach inherits the idea\nof knowledge distillation that transfers knowledge from a deep or wide\nreference model to a shallow or narrow target model. The proposed method\nemploys this idea to mimic predictions of reference estimators that are more\nrobust against overfitting than the network we want to train. Different from\nalmost all the previous work for knowledge distillation that requires a large\namount of labeled training data, the proposed method requires only a small\namount of training data. Instead, we introduce pseudo training examples that\nare optimized as a part of model parameters. Experimental results for several\nbenchmark datasets demonstrate that the proposed method outperformed all the\nother baselines, such as naive training of the target model and standard\nknowledge distillation. \n\n"}
{"id": "1802.03337", "contents": "Title: Large Scale Constrained Linear Regression Revisited: Faster Algorithms\n  via Preconditioning Abstract: In this paper, we revisit the large-scale constrained linear regression\nproblem and propose faster methods based on some recent developments in\nsketching and optimization. Our algorithms combine (accelerated) mini-batch SGD\nwith a new method called two-step preconditioning to achieve an approximate\nsolution with a time complexity lower than that of the state-of-the-art\ntechniques for the low precision case. Our idea can also be extended to the\nhigh precision case, which gives an alternative implementation to the Iterative\nHessian Sketch (IHS) method with significantly improved time complexity.\nExperiments on benchmark and synthetic datasets suggest that our methods indeed\noutperform existing ones considerably in both the low and high precision cases. \n\n"}
{"id": "1802.03832", "contents": "Title: Quadrature-based features for kernel approximation Abstract: We consider the problem of improving kernel approximation via randomized\nfeature maps. These maps arise as Monte Carlo approximation to integral\nrepresentations of kernel functions and scale up kernel methods for larger\ndatasets. Based on an efficient numerical integration technique, we propose a\nunifying approach that reinterprets the previous random features methods and\nextends to better estimates of the kernel approximation. We derive the\nconvergence behaviour and conduct an extensive empirical study that supports\nour hypothesis. \n\n"}
{"id": "1802.04034", "contents": "Title: Lipschitz-Margin Training: Scalable Certification of Perturbation\n  Invariance for Deep Neural Networks Abstract: High sensitivity of neural networks against malicious perturbations on inputs\ncauses security concerns. To take a steady step towards robust classifiers, we\naim to create neural network models provably defended from perturbations. Prior\ncertification work requires strong assumptions on network structures and\nmassive computational costs, and thus the range of their applications was\nlimited. From the relationship between the Lipschitz constants and prediction\nmargins, we present a computationally efficient calculation technique to\nlower-bound the size of adversarial perturbations that can deceive networks,\nand that is widely applicable to various complicated networks. Moreover, we\npropose an efficient training procedure that robustifies networks and\nsignificantly improves the provably guarded areas around data points. In\nexperimental evaluations, our method showed its ability to provide a\nnon-trivial guarantee and enhance robustness for even large networks. \n\n"}
{"id": "1802.04198", "contents": "Title: client2vec: Towards Systematic Baselines for Banking Applications Abstract: The workflow of data scientists normally involves potentially inefficient\nprocesses such as data mining, feature engineering and model selection. Recent\nresearch has focused on automating this workflow, partly or in its entirety, to\nimprove productivity. We choose the former approach and in this paper share our\nexperience in designing the client2vec: an internal library to rapidly build\nbaselines for banking applications. Client2vec uses marginalized stacked\ndenoising autoencoders on current account transactions data to create vector\nembeddings which represent the behaviors of our clients. These representations\ncan then be used in, and optimized against, a variety of tasks such as client\nsegmentation, profiling and targeting. Here we detail how we selected the\nalgorithmic machinery of client2vec and the data it works on and present\nexperimental results on several business cases. \n\n"}
{"id": "1802.04684", "contents": "Title: Unsupervised Evaluation and Weighted Aggregation of Ranked Predictions Abstract: Learning algorithms that aggregate predictions from an ensemble of diverse\nbase classifiers consistently outperform individual methods. Many of these\nstrategies have been developed in a supervised setting, where the accuracy of\neach base classifier can be empirically measured and this information is\nincorporated in the training process. However, the reliance on labeled data\nprecludes the application of ensemble methods to many real world problems where\nlabeled data has not been curated. To this end we developed a new theoretical\nframework for binary classification, the Strategy for Unsupervised Multiple\nMethod Aggregation (SUMMA), to estimate the performances of base classifiers\nand an optimal strategy for ensemble learning from unlabeled data. \n\n"}
{"id": "1802.04893", "contents": "Title: Uncertainty Estimation via Stochastic Batch Normalization Abstract: In this work, we investigate Batch Normalization technique and propose its\nprobabilistic interpretation. We propose a probabilistic model and show that\nBatch Normalization maximazes the lower bound of its marginalized\nlog-likelihood. Then, according to the new probabilistic model, we design an\nalgorithm which acts consistently during train and test. However, inference\nbecomes computationally inefficient. To reduce memory and computational cost,\nwe propose Stochastic Batch Normalization -- an efficient approximation of\nproper inference procedure. This method provides us with a scalable uncertainty\nestimation technique. We demonstrate the performance of Stochastic Batch\nNormalization on popular architectures (including deep convolutional\narchitectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets. \n\n"}
{"id": "1802.04908", "contents": "Title: Conditional Density Estimation with Bayesian Normalising Flows Abstract: Modeling complex conditional distributions is critical in a variety of\nsettings. Despite a long tradition of research into conditional density\nestimation, current methods employ either simple parametric forms or are\ndifficult to learn in practice. This paper employs normalising flows as a\nflexible likelihood model and presents an efficient method for fitting them to\ncomplex densities. These estimators must trade-off between modeling\ndistributional complexity, functional complexity and heteroscedasticity without\noverfitting. We recognize these trade-offs as modeling decisions and develop a\nBayesian framework for placing priors over these conditional density estimators\nusing variational Bayesian neural networks. We evaluate this method on several\nsmall benchmark regression datasets, on some of which it obtains state of the\nart performance. Finally, we apply the method to two spatial density modeling\ntasks with over 1 million datapoints using the New York City yellow taxi\ndataset and the Chicago crime dataset. \n\n"}
{"id": "1802.05074", "contents": "Title: L4: Practical loss-based stepsize adaptation for deep learning Abstract: We propose a stepsize adaptation scheme for stochastic gradient descent. It\noperates directly with the loss function and rescales the gradient in order to\nmake fixed predicted progress on the loss. We demonstrate its capabilities by\nconclusively improving the performance of Adam and Momentum optimizers. The\nenhanced optimizers with default hyperparameters consistently outperform their\nconstant stepsize counterparts, even the best ones, without a measurable\nincrease in computational cost. The performance is validated on multiple\narchitectures including dense nets, CNNs, ResNets, and the recurrent\nDifferential Neural Computer on classical datasets MNIST, fashion MNIST,\nCIFAR10 and others. \n\n"}
{"id": "1802.05394", "contents": "Title: Cost-Effective Training of Deep CNNs with Active Model Adaptation Abstract: Deep convolutional neural networks have achieved great success in various\napplications. However, training an effective DNN model for a specific task is\nrather challenging because it requires a prior knowledge or experience to\ndesign the network architecture, repeated trial-and-error process to tune the\nparameters, and a large set of labeled data to train the model. In this paper,\nwe propose to overcome these challenges by actively adapting a pre-trained\nmodel to a new task with less labeled examples. Specifically, the pre-trained\nmodel is iteratively fine tuned based on the most useful examples. The examples\nare actively selected based on a novel criterion, which jointly estimates the\npotential contribution of an instance on optimizing the feature representation\nas well as improving the classification model for the target task. On one hand,\nthe pre-trained model brings plentiful information from its original task,\navoiding redesign of the network architecture or training from scratch; and on\nthe other hand, the labeling cost can be significantly reduced by active label\nquerying. Experiments on multiple datasets and different pre-trained models\ndemonstrate that the proposed approach can achieve cost-effective training of\nDNNs. \n\n"}
{"id": "1802.05757", "contents": "Title: Stochastic Wasserstein Barycenters Abstract: We present a stochastic algorithm to compute the barycenter of a set of\nprobability distributions under the Wasserstein metric from optimal transport.\nUnlike previous approaches, our method extends to continuous input\ndistributions and allows the support of the barycenter to be adjusted in each\niteration. We tackle the problem without regularization, allowing us to recover\na sharp output whose support is contained within the support of the true\nbarycenter. We give examples where our algorithm recovers a more meaningful\nbarycenter than previous work. Our method is versatile and can be extended to\napplications such as generating super samples from a given distribution and\nrecovering blue noise approximations. \n\n"}
{"id": "1802.05872", "contents": "Title: Online Machine Learning in Big Data Streams Abstract: The area of online machine learning in big data streams covers algorithms\nthat are (1) distributed and (2) work from data streams with only a limited\npossibility to store past data. The first requirement mostly concerns software\narchitectures and efficient algorithms. The second one also imposes nontrivial\ntheoretical restrictions on the modeling methods: In the data stream model,\nolder data is no longer available to revise earlier suboptimal modeling\ndecisions as the fresh data arrives.\n  In this article, we provide an overview of distributed software architectures\nand libraries as well as machine learning models for online learning. We\nhighlight the most important ideas for classification, regression,\nrecommendation, and unsupervised modeling from streaming data, and we show how\nthey are implemented in various distributed data stream processing systems.\n  This article is a reference material and not a survey. We do not attempt to\nbe comprehensive in describing all existing methods and solutions; rather, we\ngive pointers to the most important resources in the field. All related\nsub-fields, online algorithms, online learning, and distributed data processing\nare hugely dominant in current research and development with conceptually new\nresearch results and software components emerging at the time of writing. In\nthis article, we refer to several survey results, both for distributed data\nprocessing and for online machine learning. Compared to past surveys, our\narticle is different because we discuss recommender systems in extended detail. \n\n"}
{"id": "1802.06093", "contents": "Title: Gradient descent with identity initialization efficiently learns\n  positive definite linear transformations by deep residual networks Abstract: We analyze algorithms for approximating a function $f(x) = \\Phi x$ mapping\n$\\Re^d$ to $\\Re^d$ using deep linear neural networks, i.e. that learn a\nfunction $h$ parameterized by matrices $\\Theta_1,...,\\Theta_L$ and defined by\n$h(x) = \\Theta_L \\Theta_{L-1} ... \\Theta_1 x$. We focus on algorithms that\nlearn through gradient descent on the population quadratic loss in the case\nthat the distribution over the inputs is isotropic.\n  We provide polynomial bounds on the number of iterations for gradient descent\nto approximate the least squares matrix $\\Phi$, in the case where the initial\nhypothesis $\\Theta_1 = ... = \\Theta_L = I$ has excess loss bounded by a small\nenough constant. On the other hand, we show that gradient descent fails to\nconverge for $\\Phi$ whose distance from the identity is a larger constant, and\nwe show that some forms of regularization toward the identity in each layer do\nnot help.\n  If $\\Phi$ is symmetric positive definite, we show that an algorithm that\ninitializes $\\Theta_i = I$ learns an $\\epsilon$-approximation of $f$ using a\nnumber of updates polynomial in $L$, the condition number of $\\Phi$, and\n$\\log(d/\\epsilon)$. In contrast, we show that if the least squares matrix\n$\\Phi$ is symmetric and has a negative eigenvalue, then all members of a class\nof algorithms that perform gradient descent with identity initialization, and\noptionally regularize toward the identity in each layer, fail to converge.\n  We analyze an algorithm for the case that $\\Phi$ satisfies $u^{\\top} \\Phi u >\n0$ for all $u$, but may not be symmetric. This algorithm uses two regularizers:\none that maintains the invariant $u^{\\top} \\Theta_L \\Theta_{L-1} ... \\Theta_1 u\n> 0$ for all $u$, and another that \"balances\" $\\Theta_1, ..., \\Theta_L$ so that\nthey have the same singular values. \n\n"}
{"id": "1802.06300", "contents": "Title: Exact and Robust Conformal Inference Methods for Predictive Machine\n  Learning With Dependent Data Abstract: We extend conformal inference to general settings that allow for time series\ndata. Our proposal is developed as a randomization method and accounts for\npotential serial dependence by including block structures in the permutation\nscheme. As a result, the proposed method retains the exact, model-free validity\nwhen the data are i.i.d. or more generally exchangeable, similar to usual\nconformal inference methods. When exchangeability fails, as is the case for\ncommon time series data, the proposed approach is approximately valid under\nweak assumptions on the conformity score. \n\n"}
{"id": "1802.06458", "contents": "Title: A Generative Modeling Approach to Limited Channel ECG Classification Abstract: Processing temporal sequences is central to a variety of applications in\nhealth care, and in particular multi-channel Electrocardiogram (ECG) is a\nhighly prevalent diagnostic modality that relies on robust sequence modeling.\nWhile Recurrent Neural Networks (RNNs) have led to significant advances in\nautomated diagnosis with time-series data, they perform poorly when models are\ntrained using a limited set of channels. A crucial limitation of existing\nsolutions is that they rely solely on discriminative models, which tend to\ngeneralize poorly in such scenarios. In order to combat this limitation, we\ndevelop a generative modeling approach to limited channel ECG classification.\nThis approach first uses a Seq2Seq model to implicitly generate the missing\nchannel information, and then uses the latent representation to perform the\nactual supervisory task. This decoupling enables the use of unsupervised data\nand also provides highly robust metric spaces for subsequent discriminative\nlearning. Our experiments with the Physionet dataset clearly evidence the\neffectiveness of our approach over standard RNNs in disease prediction. \n\n"}
{"id": "1802.06463", "contents": "Title: Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross\n  Entropy Abstract: We study model recovery for data classification, where the training labels\nare generated from a one-hidden-layer neural network with sigmoid activations,\nalso known as a single-layer feedforward network, and the goal is to recover\nthe weights of the neural network. We consider two network models, the\nfully-connected network (FCN) and the non-overlapping convolutional neural\nnetwork (CNN). We prove that with Gaussian inputs, the empirical risk based on\ncross entropy exhibits strong convexity and smoothness {\\em uniformly} in a\nlocal neighborhood of the ground truth, as soon as the sample complexity is\nsufficiently large. This implies that if initialized in this neighborhood,\ngradient descent converges linearly to a critical point that is provably close\nto the ground truth. Furthermore, we show such an initialization can be\nobtained via the tensor method. This establishes the global convergence\nguarantee for empirical risk minimization using cross entropy via gradient\ndescent for learning one-hidden-layer neural networks, at the near-optimal\nsample and computational complexity with respect to the network input dimension\nwithout unrealistic assumptions such as requiring a fresh set of samples at\neach iteration. \n\n"}
{"id": "1802.08334", "contents": "Title: Learning Without Mixing: Towards A Sharp Analysis of Linear System\n  Identification Abstract: We prove that the ordinary least-squares (OLS) estimator attains nearly\nminimax optimal performance for the identification of linear dynamical systems\nfrom a single observed trajectory. Our upper bound relies on a generalization\nof Mendelson's small-ball method to dependent data, eschewing the use of\nstandard mixing-time arguments. Our lower bounds reveal that these upper bounds\nmatch up to logarithmic factors. In particular, we capture the correct\nsignal-to-noise behavior of the problem, showing that more unstable linear\nsystems are easier to estimate. This behavior is qualitatively different from\narguments which rely on mixing-time calculations that suggest that unstable\nsystems are more difficult to estimate. We generalize our technique to provide\nbounds for a more general class of linear response time-series. \n\n"}
{"id": "1802.08471", "contents": "Title: Optimized Algorithms to Sample Determinantal Point Processes Abstract: In this technical report, we discuss several sampling algorithms for\nDeterminantal Point Processes (DPP). DPPs have recently gained a broad interest\nin the machine learning and statistics literature as random point processes\nwith negative correlation, i.e., ones that can generate a \"diverse\" sample from\na set of items. They are parametrized by a matrix $\\mathbf{L}$, called\n$L$-ensemble, that encodes the correlations between items. The standard\nsampling algorithm is separated in three phases: 1/~eigendecomposition of\n$\\mathbf{L}$, 2/~an eigenvector sampling phase where $\\mathbf{L}$'s\neigenvectors are sampled independently via a Bernoulli variable parametrized by\ntheir associated eigenvalue, 3/~a Gram-Schmidt-type orthogonalisation procedure\nof the sampled eigenvectors.\n  In a naive implementation, the computational cost of the third step is on\naverage $\\mathcal{O}(N\\mu^3)$ where $\\mu$ is the average number of samples of\nthe DPP. We give an algorithm which runs in $\\mathcal{O}(N\\mu^2)$ and is\nextremely simple to implement. If memory is a constraint, we also describe a\ndual variant with reduced memory costs. In addition, we discuss implementation\ndetails often missing in the literature. \n\n"}
{"id": "1802.08686", "contents": "Title: Adversarial vulnerability for any classifier Abstract: Despite achieving impressive performance, state-of-the-art classifiers remain\nhighly vulnerable to small, imperceptible, adversarial perturbations. This\nvulnerability has proven empirically to be very intricate to address. In this\npaper, we study the phenomenon of adversarial perturbations under the\nassumption that the data is generated with a smooth generative model. We derive\nfundamental upper bounds on the robustness to perturbations of any\nclassification function, and prove the existence of adversarial perturbations\nthat transfer well across different classifiers with small risk. Our analysis\nof the robustness also provides insights onto key properties of generative\nmodels, such as their smoothness and dimensionality of latent space. We\nconclude with numerical experimental results showing that our bounds provide\ninformative baselines to the maximal achievable robustness on several datasets. \n\n"}
{"id": "1802.09128", "contents": "Title: Averaging Stochastic Gradient Descent on Riemannian Manifolds Abstract: We consider the minimization of a function defined on a Riemannian manifold\n$\\mathcal{M}$ accessible only through unbiased estimates of its gradients. We\ndevelop a geometric framework to transform a sequence of slowly converging\niterates generated from stochastic gradient descent (SGD) on $\\mathcal{M}$ to\nan averaged iterate sequence with a robust and fast $O(1/n)$ convergence rate.\nWe then present an application of our framework to geodesically-strongly-convex\n(and possibly Euclidean non-convex) problems. Finally, we demonstrate how these\nideas apply to the case of streaming $k$-PCA, where we show how to accelerate\nthe slow rate of the randomized power method (without requiring knowledge of\nthe eigengap) into a robust algorithm achieving the optimal rate of\nconvergence. \n\n"}
{"id": "1802.09484", "contents": "Title: Disentangling the independently controllable factors of variation by\n  interacting with the world Abstract: It has been postulated that a good representation is one that disentangles\nthe underlying explanatory factors of variation. However, it remains an open\nquestion what kind of training framework could potentially achieve that.\nWhereas most previous work focuses on the static setting (e.g., with images),\nwe postulate that some of the causal factors could be discovered if the learner\nis allowed to interact with its environment. The agent can experiment with\ndifferent actions and observe their effects. More specifically, we hypothesize\nthat some of these factors correspond to aspects of the environment which are\nindependently controllable, i.e., that there exists a policy and a learnable\nfeature for each such aspect of the environment, such that this policy can\nyield changes in that feature with minimal changes to other features that\nexplain the statistical variations in the observed data. We propose a specific\nobjective function to find such factors, and verify experimentally that it can\nindeed disentangle independently controllable aspects of the environment\nwithout any extrinsic reward signal. \n\n"}
{"id": "1802.09691", "contents": "Title: Link Prediction Based on Graph Neural Networks Abstract: Link prediction is a key problem for network-structured data. Link prediction\nheuristics use some score functions, such as common neighbors and Katz index,\nto measure the likelihood of links. They have obtained wide practical uses due\nto their simplicity, interpretability, and for some of them, scalability.\nHowever, every heuristic has a strong assumption on when two nodes are likely\nto link, which limits their effectiveness on networks where these assumptions\nfail. In this regard, a more reasonable way should be learning a suitable\nheuristic from a given network instead of using predefined ones. By extracting\na local subgraph around each target link, we aim to learn a function mapping\nthe subgraph patterns to link existence, thus automatically learning a\n`heuristic' that suits the current network. In this paper, we study this\nheuristic learning paradigm for link prediction. First, we develop a novel\n$\\gamma$-decaying heuristic theory. The theory unifies a wide range of\nheuristics in a single framework, and proves that all these heuristics can be\nwell approximated from local subgraphs. Our results show that local subgraphs\nreserve rich information related to link existence. Second, based on the\n$\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from\nlocal subgraphs using a graph neural network (GNN). Its experimental results\nshow unprecedented performance, working consistently well on a wide range of\nproblems. \n\n"}
{"id": "1802.09914", "contents": "Title: High-Dimensional Vector Semantics Abstract: In this paper we explore the \"vector semantics\" problem from the perspective\nof \"almost orthogonal\" property of high-dimensional random vectors. We show\nthat this intriguing property can be used to \"memorize\" random vectors by\nsimply adding them, and we provide an efficient probabilistic solution to the\nset membership problem. Also, we discuss several applications to word context\nvector embeddings, document sentences similarity, and spam filtering. \n\n"}
{"id": "1803.00276", "contents": "Title: Model-Based Clustering and Classification of Functional Data Abstract: The problem of complex data analysis is a central topic of modern statistical\nscience and learning systems and is becoming of broader interest with the\nincreasing prevalence of high-dimensional data. The challenge is to develop\nstatistical models and autonomous algorithms that are able to acquire knowledge\nfrom raw data for exploratory analysis, which can be achieved through\nclustering techniques or to make predictions of future data via classification\n(i.e., discriminant analysis) techniques. Latent data models, including mixture\nmodel-based approaches are one of the most popular and successful approaches in\nboth the unsupervised context (i.e., clustering) and the supervised one (i.e,\nclassification or discrimination). Although traditionally tools of multivariate\nanalysis, they are growing in popularity when considered in the framework of\nfunctional data analysis (FDA). FDA is the data analysis paradigm in which the\nindividual data units are functions (e.g., curves, surfaces), rather than\nsimple vectors. In many areas of application, the analyzed data are indeed\noften available in the form of discretized values of functions or curves (e.g.,\ntime series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data).\nThis functional aspect of the data adds additional difficulties compared to the\ncase of a classical multivariate (non-functional) data analysis. We review and\npresent approaches for model-based clustering and classification of functional\ndata. We derive well-established statistical models along with efficient\nalgorithmic tools to address problems regarding the clustering and the\nclassification of these high-dimensional data, including their heterogeneity,\nmissing information, and dynamical hidden structure. The presented models and\nalgorithms are illustrated on real-world functional data analysis problems from\nseveral application area. \n\n"}
{"id": "1803.00676", "contents": "Title: Meta-Learning for Semi-Supervised Few-Shot Classification Abstract: In few-shot classification, we are interested in learning algorithms that\ntrain a classifier from only a handful of labeled examples. Recent progress in\nfew-shot classification has featured meta-learning, in which a parameterized\nmodel for a learning algorithm is defined and trained on episodes representing\ndifferent classification problems, each with a small labeled training set and\nits corresponding test set. In this work, we advance this few-shot\nclassification paradigm towards a scenario where unlabeled examples are also\navailable within each episode. We consider two situations: one where all\nunlabeled examples are assumed to belong to the same set of classes as the\nlabeled examples of the episode, as well as the more challenging situation\nwhere examples from other distractor classes are also provided. To address this\nparadigm, we propose novel extensions of Prototypical Networks (Snell et al.,\n2017) that are augmented with the ability to use unlabeled examples when\nproducing prototypes. These models are trained in an end-to-end way on\nepisodes, to learn to leverage the unlabeled examples successfully. We evaluate\nthese methods on versions of the Omniglot and miniImageNet benchmarks, adapted\nto this new framework augmented with unlabeled examples. We also propose a new\nsplit of ImageNet, consisting of a large set of classes, with a hierarchical\nstructure. Our experiments confirm that our Prototypical Networks can learn to\nimprove their predictions due to unlabeled examples, much like a\nsemi-supervised algorithm would. \n\n"}
{"id": "1803.02781", "contents": "Title: Fast Dawid-Skene: A Fast Vote Aggregation Scheme for Sentiment\n  Classification Abstract: Many real world problems can now be effectively solved using supervised\nmachine learning. A major roadblock is often the lack of an adequate quantity\nof labeled data for training. A possible solution is to assign the task of\nlabeling data to a crowd, and then infer the true label using aggregation\nmethods. A well-known approach for aggregation is the Dawid-Skene (DS)\nalgorithm, which is based on the principle of Expectation-Maximization (EM). We\npropose a new simple, yet effective, EM-based algorithm, which can be\ninterpreted as a `hard' version of DS, that allows much faster convergence\nwhile maintaining similar accuracy in aggregation. We show the use of this\nalgorithm as a quick and effective technique for online, real-time sentiment\nannotation. We also prove that our algorithm converges to the estimated labels\nat a linear rate. Our experiments on standard datasets show a significant\nspeedup in time taken for aggregation - upto $\\sim$8x over Dawid-Skene and\n$\\sim$6x over other fast EM methods, at competitive accuracy performance. The\ncode for the implementation of the algorithms can be found at\nhttps://github.com/GoodDeeds/Fast-Dawid-Skene \n\n"}
{"id": "1803.02782", "contents": "Title: A bag-to-class divergence approach to multiple-instance learning Abstract: In multi-instance (MI) learning, each object (bag) consists of multiple\nfeature vectors (instances), and is most commonly regarded as a set of points\nin a multidimensional space. A different viewpoint is that the instances are\nrealisations of random vectors with corresponding probability distribution, and\nthat a bag is the distribution, not the realisations. In MI classification,\neach bag in the training set has a class label, but the instances are\nunlabelled. By introducing the probability distribution space to bag-level\nclassification problems, dissimilarities between probability distributions\n(divergences) can be applied. The bag-to-bag Kullback-Leibler information is\nasymptotically the best classifier, but the typical sparseness of MI training\nsets is an obstacle. We introduce bag-to-class divergence to MI learning,\nemphasising the hierarchical nature of the random vectors that makes bags from\nthe same class different. We propose two properties for bag-to-class\ndivergences, and an additional property for sparse training sets. \n\n"}
{"id": "1803.02999", "contents": "Title: On First-Order Meta-Learning Algorithms Abstract: This paper considers meta-learning problems, where there is a distribution of\ntasks, and we would like to obtain an agent that performs well (i.e., learns\nquickly) when presented with a previously unseen task sampled from this\ndistribution. We analyze a family of algorithms for learning a parameter\ninitialization that can be fine-tuned quickly on a new task, using only\nfirst-order derivatives for the meta-learning updates. This family includes and\ngeneralizes first-order MAML, an approximation to MAML obtained by ignoring\nsecond-order derivatives. It also includes Reptile, a new algorithm that we\nintroduce here, which works by repeatedly sampling a task, training on it, and\nmoving the initialization towards the trained weights on that task. We expand\non the results from Finn et al. showing that first-order meta-learning\nalgorithms perform well on some well-established benchmarks for few-shot\nclassification, and we provide theoretical analysis aimed at understanding why\nthese algorithms work. \n\n"}
{"id": "1803.03289", "contents": "Title: Deep Neural Network Compression with Single and Multiple Level\n  Quantization Abstract: Network quantization is an effective solution to compress deep neural\nnetworks for practical usage. Existing network quantization methods cannot\nsufficiently exploit the depth information to generate low-bit compressed\nnetwork. In this paper, we propose two novel network quantization approaches,\nsingle-level network quantization (SLQ) for high-bit quantization and\nmulti-level network quantization (MLQ) for extremely low-bit quantization\n(ternary).We are the first to consider the network quantization from both width\nand depth level. In the width level, parameters are divided into two parts: one\nfor quantization and the other for re-training to eliminate the quantization\nloss. SLQ leverages the distribution of the parameters to improve the width\nlevel. In the depth level, we introduce incremental layer compensation to\nquantize layers iteratively which decreases the quantization loss in each\niteration. The proposed approaches are validated with extensive experiments\nbased on the state-of-the-art neural networks including AlexNet, VGG-16,\nGoogleNet and ResNet-18. Both SLQ and MLQ achieve impressive results. \n\n"}
{"id": "1803.03467", "contents": "Title: RippleNet: Propagating User Preferences on the Knowledge Graph for\n  Recommender Systems Abstract: To address the sparsity and cold start problem of collaborative filtering,\nresearchers usually make use of side information, such as social networks or\nitem attributes, to improve recommendation performance. This paper considers\nthe knowledge graph as the source of side information. To address the\nlimitations of existing embedding-based and path-based methods for\nknowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end\nframework that naturally incorporates the knowledge graph into recommender\nsystems. Similar to actual ripples propagating on the surface of water, Ripple\nNetwork stimulates the propagation of user preferences over the set of\nknowledge entities by automatically and iteratively extending a user's\npotential interests along links in the knowledge graph. The multiple \"ripples\"\nactivated by a user's historically clicked items are thus superposed to form\nthe preference distribution of the user with respect to a candidate item, which\ncould be used for predicting the final clicking probability. Through extensive\nexperiments on real-world datasets, we demonstrate that Ripple Network achieves\nsubstantial gains in a variety of scenarios, including movie, book and news\nrecommendation, over several state-of-the-art baselines. \n\n"}
{"id": "1803.04209", "contents": "Title: High Throughput Synchronous Distributed Stochastic Gradient Descent Abstract: We introduce a new, high-throughput, synchronous, distributed, data-parallel,\nstochastic-gradient-descent learning algorithm. This algorithm uses amortized\ninference in a compute-cluster-specific, deep, generative, dynamical model to\nperform joint posterior predictive inference of the mini-batch gradient\ncomputation times of all worker-nodes in a parallel computing cluster. We show\nthat a synchronous parameter server can, by utilizing such a model, choose an\noptimal cutoff time beyond which mini-batch gradient messages from slow workers\nare ignored that maximizes overall mini-batch gradient computations per second.\nIn keeping with earlier findings we observe that, under realistic conditions,\neagerly discarding the mini-batch gradient computations of stragglers not only\nincreases throughput but actually increases the overall rate of convergence as\na function of wall-clock time by virtue of eliminating idleness. The principal\nnovel contribution and finding of this work goes beyond this by demonstrating\nthat using the predicted run-times from a generative model of cluster worker\nperformance to dynamically adjust the cutoff improves substantially over the\nstatic-cutoff prior art, leading to, among other things, significantly reduced\ndeep neural net training times on large computer clusters. \n\n"}
{"id": "1803.04371", "contents": "Title: Optimal Rates of Sketched-regularized Algorithms for Least-Squares\n  Regression over Hilbert Spaces Abstract: We investigate regularized algorithms combining with projection for\nleast-squares regression problem over a Hilbert space, covering nonparametric\nregression over a reproducing kernel Hilbert space. We prove convergence\nresults with respect to variants of norms, under a capacity assumption on the\nhypothesis space and a regularity condition on the target function. As a\nresult, we obtain optimal rates for regularized algorithms with randomized\nsketches, provided that the sketch dimension is proportional to the effective\ndimension up to a logarithmic factor. As a byproduct, we obtain similar results\nfor Nystr\\\"{o}m regularized algorithms. Our results are the first ones with\noptimal, distribution-dependent rates that do not have any saturation effect\nfor sketched/Nystr\\\"{o}m regularized algorithms, considering both the\nattainable and non-attainable cases. \n\n"}
{"id": "1803.04383", "contents": "Title: Delayed Impact of Fair Machine Learning Abstract: Fairness in machine learning has predominantly been studied in static\nclassification settings without concern for how decisions change the underlying\npopulation over time. Conventional wisdom suggests that fairness criteria\npromote the long-term well-being of those groups they aim to protect.\n  We study how static fairness criteria interact with temporal indicators of\nwell-being, such as long-term improvement, stagnation, and decline in a\nvariable of interest. We demonstrate that even in a one-step feedback model,\ncommon fairness criteria in general do not promote improvement over time, and\nmay in fact cause harm in cases where an unconstrained objective would not.\n  We completely characterize the delayed impact of three standard criteria,\ncontrasting the regimes in which these exhibit qualitatively different\nbehavior. In addition, we find that a natural form of measurement error\nbroadens the regime in which fairness criteria perform favorably.\n  Our results highlight the importance of measurement and temporal modeling in\nthe evaluation of fairness criteria, suggesting a range of new challenges and\ntrade-offs. \n\n"}
{"id": "1803.04386", "contents": "Title: Flipout: Efficient Pseudo-Independent Weight Perturbations on\n  Mini-Batches Abstract: Stochastic neural net weights are used in a variety of contexts, including\nregularization, Bayesian neural nets, exploration in reinforcement learning,\nand evolution strategies. Unfortunately, due to the large number of weights,\nall the examples in a mini-batch typically share the same weight perturbation,\nthereby limiting the variance reduction effect of large mini-batches. We\nintroduce flipout, an efficient method for decorrelating the gradients within a\nmini-batch by implicitly sampling pseudo-independent weight perturbations for\neach example. Empirically, flipout achieves the ideal linear variance reduction\nfor fully connected networks, convolutional networks, and RNNs. We find\nsignificant speedups in training neural networks with multiplicative Gaussian\nperturbations. We show that flipout is effective at regularizing LSTMs, and\noutperforms previous methods. Flipout also enables us to vectorize evolution\nstrategies: in our experiments, a single GPU with flipout can handle the same\nthroughput as at least 40 CPU cores using existing methods, equivalent to a\nfactor-of-4 cost reduction on Amazon Web Services. \n\n"}
{"id": "1803.04848", "contents": "Title: Soft-Robust Actor-Critic Policy-Gradient Abstract: Robust Reinforcement Learning aims to derive optimal behavior that accounts\nfor model uncertainty in dynamical systems. However, previous studies have\nshown that by considering the worst case scenario, robust policies can be\noverly conservative. Our soft-robust framework is an attempt to overcome this\nissue. In this paper, we present a novel Soft-Robust Actor-Critic algorithm\n(SR-AC). It learns an optimal policy with respect to a distribution over an\nuncertainty set and stays robust to model uncertainty but avoids the\nconservativeness of robust strategies. We show the convergence of SR-AC and\ntest the efficiency of our approach on different domains by comparing it\nagainst regular learning methods and their robust formulations. \n\n"}
{"id": "1803.05011", "contents": "Title: A Probabilistic Disease Progression Model for Predicting Future Clinical\n  Outcome Abstract: In this work, we consider the problem of predicting the course of a\nprogressive disease, such as cancer or Alzheimer's. Progressive diseases often\nstart with mild symptoms that might precede a diagnosis, and each patient\nfollows their own trajectory. Patient trajectories exhibit wild variability,\nwhich can be associated with many factors such as genotype, age, or sex. An\nadditional layer of complexity is that, in real life, the amount and type of\ndata available for each patient can differ significantly. For example, for one\npatient we might have no prior history, whereas for another patient we might\nhave detailed clinical assessments obtained at multiple prior time-points. This\npaper presents a probabilistic model that can handle multiple modalities\n(including images and clinical assessments) and variable patient histories with\nirregular timings and missing entries, to predict clinical scores at future\ntime-points. We use a sigmoidal function to model latent disease progression,\nwhich gives rise to clinical observations in our generative model. We\nimplemented an approximate Bayesian inference strategy on the proposed model to\nestimate the parameters on data from a large population of subjects.\nFurthermore, the Bayesian framework enables the model to automatically\nfine-tune its predictions based on historical observations that might be\navailable on the test subject. We applied our method to a longitudinal\nAlzheimer's disease dataset with more than 3000 subjects [23] and present a\ndetailed empirical analysis of prediction performance under different\nscenarios, with comparisons against several benchmarks. We also demonstrate how\nthe proposed model can be interrogated to glean insights about temporal\ndynamics in Alzheimer's disease. \n\n"}
{"id": "1803.05598", "contents": "Title: Large Margin Deep Networks for Classification Abstract: We present a formulation of deep learning that aims at producing a large\nmargin classifier. The notion of margin, minimum distance to a decision\nboundary, has served as the foundation of several theoretically profound and\nempirically successful results for both classification and regression tasks.\nHowever, most large margin algorithms are applicable only to shallow models\nwith a preset feature representation; and conventional margin methods for\nneural networks only enforce margin at the output layer. Such methods are\ntherefore not well suited for deep networks.\n  In this work, we propose a novel loss function to impose a margin on any\nchosen set of layers of a deep network (including input and hidden layers). Our\nformulation allows choosing any norm on the metric measuring the margin. We\ndemonstrate that the decision boundary obtained by our loss has nice properties\ncompared to standard classification loss functions. Specifically, we show\nimproved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on\nmultiple tasks: generalization from small training sets, corrupted labels, and\nrobustness against adversarial perturbations. The resulting loss is general and\ncomplementary to existing data augmentation (such as random/adversarial input\ntransform) and regularization techniques (such as weight decay, dropout, and\nbatch norm). \n\n"}
{"id": "1803.05616", "contents": "Title: Strong light illumination on gain-switched semiconductor lasers helps\n  the eavesdropper in practical quantum key distribution systems Abstract: The temperature of the semiconductor diode increases under strong light\nillumination whether thermoelectric cooler is installed or not, which changes\nthe output wavelength of the laser (Lee M. S. et al., 2017). However, other\ncharacteristics also vary as temperature increases. These variations may help\nthe eavesdropper in practical quantum key distribution systems. We study the\neffects of temperature increase on gain-switched semiconductor lasers by\nsimulating temperature dependent rate equations. The results show that\ntemperature increase may cause large intensity fluctuation, decrease the output\nintensity and lead the signal state and decoy state distinguishable. We also\npropose a modified photon number splitting attack by exploiting the effects of\ntemperature increase. Countermeasures are also proposed. \n\n"}
{"id": "1803.06407", "contents": "Title: Deep Component Analysis via Alternating Direction Neural Networks Abstract: Despite a lack of theoretical understanding, deep neural networks have\nachieved unparalleled performance in a wide range of applications. On the other\nhand, shallow representation learning with component analysis is associated\nwith rich intuition and theory, but smaller capacity often limits its\nusefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA),\nan expressive multilayer model formulation that enforces hierarchical structure\nthrough constraints on latent variables in each layer. For inference, we\npropose a differentiable optimization algorithm implemented using recurrent\nAlternating Direction Neural Networks (ADNNs) that enable parameter learning\nusing standard backpropagation. By interpreting feed-forward networks as\nsingle-iteration approximations of inference in our model, we provide both a\nnovel theoretical perspective for understanding them and a practical technique\nfor constraining predictions with prior knowledge. Experimentally, we\ndemonstrate performance improvements on a variety of tasks, including\nsingle-image depth prediction with sparse output constraints. \n\n"}
{"id": "1803.06917", "contents": "Title: Universal features of price formation in financial markets: perspectives\n  from Deep Learning Abstract: Using a large-scale Deep Learning approach applied to a high-frequency\ndatabase containing billions of electronic market quotes and transactions for\nUS equities, we uncover nonparametric evidence for the existence of a universal\nand stationary price formation mechanism relating the dynamics of supply and\ndemand for a stock, as revealed through the order book, to subsequent\nvariations in its market price. We assess the model by testing its\nout-of-sample predictions for the direction of price moves given the history of\nprice and order flow, across a wide range of stocks and time periods. The\nuniversal price formation model is shown to exhibit a remarkably stable\nout-of-sample prediction accuracy across time, for a wide range of stocks from\ndifferent sectors. Interestingly, these results also hold for stocks which are\nnot part of the training sample, showing that the relations captured by the\nmodel are universal and not asset-specific.\n  The universal model --- trained on data from all stocks --- outperforms, in\nterms of out-of-sample prediction accuracy, asset-specific linear and nonlinear\nmodels trained on time series of any given stock, showing that the universal\nnature of price formation weighs in favour of pooling together financial data\nfrom various stocks, rather than designing asset- or sector-specific models as\ncommonly done. Standard data normalizations based on volatility, price level or\naverage spread, or partitioning the training data into sectors or categories\nsuch as large/small tick stocks, do not improve training results. On the other\nhand, inclusion of price and order flow history over many past observations is\nshown to improve forecasting performance, showing evidence of path-dependence\nin price dynamics. \n\n"}
{"id": "1803.07068", "contents": "Title: D$^2$: Decentralized Training over Decentralized Data Abstract: While training a machine learning model using multiple workers, each of which\ncollects data from their own data sources, it would be most useful when the\ndata collected from different workers can be {\\em unique} and {\\em different}.\nIronically, recent analysis of decentralized parallel stochastic gradient\ndescent (D-PSGD) relies on the assumption that the data hosted on different\nworkers are {\\em not too different}. In this paper, we ask the question: {\\em\nCan we design a decentralized parallel stochastic gradient descent algorithm\nthat is less sensitive to the data variance across workers?} In this paper, we\npresent D$^2$, a novel decentralized parallel stochastic gradient descent\nalgorithm designed for large data variance \\xr{among workers} (imprecisely,\n\"decentralized\" data). The core of D$^2$ is a variance blackuction extension of\nthe standard D-PSGD algorithm, which improves the convergence rate from\n$O\\left({\\sigma \\over \\sqrt{nT}} + {(n\\zeta^2)^{\\frac{1}{3}} \\over\nT^{2/3}}\\right)$ to $O\\left({\\sigma \\over \\sqrt{nT}}\\right)$ where $\\zeta^{2}$\ndenotes the variance among data on different workers. As a result, D$^2$ is\nrobust to data variance among workers. We empirically evaluated D$^2$ on image\nclassification tasks where each worker has access to only the data of a limited\nset of labels, and find that D$^2$ significantly outperforms D-PSGD. \n\n"}
{"id": "1803.07859", "contents": "Title: Efficient Sampling and Structure Learning of Bayesian Networks Abstract: Bayesian networks are probabilistic graphical models widely employed to\nunderstand dependencies in high dimensional data, and even to facilitate causal\ndiscovery. Learning the underlying network structure, which is encoded as a\ndirected acyclic graph (DAG) is highly challenging mainly due to the vast\nnumber of possible networks in combination with the acyclicity constraint.\nEfforts have focussed on two fronts: constraint-based methods that perform\nconditional independence tests to exclude edges and score and search approaches\nwhich explore the DAG space with greedy or MCMC schemes. Here we synthesise\nthese two fields in a novel hybrid method which reduces the complexity of MCMC\napproaches to that of a constraint-based method. Individual steps in the MCMC\nscheme only require simple table lookups so that very long chains can be\nefficiently obtained. Furthermore, the scheme includes an iterative procedure\nto correct for errors from the conditional independence tests. The algorithm\noffers markedly superior performance to alternatives, particularly because DAGs\ncan also be sampled from the posterior distribution, enabling full Bayesian\nmodel averaging for much larger Bayesian networks. \n\n"}
{"id": "1803.08784", "contents": "Title: Causal Modeling of Dynamical Systems Abstract: Dynamical systems are widely used in science and engineering to model systems\nconsisting of several interacting components. Often, they can be given a causal\ninterpretation in the sense that they not only model the evolution of the\nstates of the system's components over time, but also describe how their\nevolution is affected by external interventions on the system that perturb the\ndynamics. We introduce the formal framework of structural dynamical causal\nmodels (SDCMs) that explicates the causal semantics of the system's components\nas part of the model. SDCMs represent a dynamical system as a collection of\nstochastic processes and specify the basic causal mechanisms that govern the\ndynamics of each component as a structured system of random differential\nequations of arbitrary order. SDCMs extend the versatile causal modeling\nframework of structural causal models (SCMs), also known as structural equation\nmodels (SEMs), by explicitly allowing for time-dependence. An SDCM can be\nthought of as the stochastic-process version of an SCM, where the static random\nvariables of the SCM are replaced by dynamic stochastic processes and their\nderivatives. We provide the foundations for a theory of SDCMs, by (i) formally\ndefining SDCMs, their solutions, stochastic interventions, and a graphical\nrepresentation; (ii) studying existence and uniqueness of the solutions for\ngiven initial conditions; (iii) providing Markov properties for SDCMs with\ninitial conditions; (iv) discussing under which conditions SDCMs equilibrate to\nSCMs as time tends to infinity; (v) relating the properties of the SDCM to\nthose of the equilibrium SCM. This correspondence enables one to leverage the\nwealth of statistical tools and discovery methods available for SCMs when\nstudying the causal semantics of a large class of stochastic dynamical systems.\nThe theory is illustrated with examples from different scientific domains. \n\n"}
{"id": "1803.08841", "contents": "Title: The Convergence of Stochastic Gradient Descent in Asynchronous Shared\n  Memory Abstract: Stochastic Gradient Descent (SGD) is a fundamental algorithm in machine\nlearning, representing the optimization backbone for training several classic\nmodels, from regression to neural networks. Given the recent practical focus on\ndistributed machine learning, significant work has been dedicated to the\nconvergence properties of this algorithm under the inconsistent and noisy\nupdates arising from execution in a distributed environment. However,\nsurprisingly, the convergence properties of this classic algorithm in the\nstandard shared-memory model are still not well-understood.\n  In this work, we address this gap, and provide new convergence bounds for\nlock-free concurrent stochastic gradient descent, executing in the classic\nasynchronous shared memory model, against a strong adaptive adversary. Our\nresults give improved upper and lower bounds on the \"price of asynchrony\" when\nexecuting the fundamental SGD algorithm in a concurrent setting. They show that\nthis classic optimization tool can converge faster and with a wider range of\nparameters than previously known under asynchronous iterations. At the same\ntime, we exhibit a fundamental trade-off between the maximum delay in the\nsystem and the rate at which SGD can converge, which governs the set of\nparameters under which this algorithm can still work efficiently. \n\n"}
{"id": "1803.09357", "contents": "Title: On the Local Minima of the Empirical Risk Abstract: Population risk is always of primary interest in machine learning; however,\nlearning algorithms only have access to the empirical risk. Even for\napplications with nonconvex nonsmooth losses (such as modern deep networks),\nthe population risk is generally significantly more well-behaved from an\noptimization point of view than the empirical risk. In particular, sampling can\ncreate many spurious local minima. We consider a general framework which aims\nto optimize a smooth nonconvex function $F$ (population risk) given only access\nto an approximation $f$ (empirical risk) that is pointwise close to $F$ (i.e.,\n$\\|F-f\\|_{\\infty} \\le \\nu$). Our objective is to find the\n$\\epsilon$-approximate local minima of the underlying function $F$ while\navoiding the shallow local minima---arising because of the tolerance\n$\\nu$---which exist only in $f$. We propose a simple algorithm based on\nstochastic gradient descent (SGD) on a smoothed version of $f$ that is\nguaranteed to achieve our goal as long as $\\nu \\le O(\\epsilon^{1.5}/d)$. We\nalso provide an almost matching lower bound showing that our algorithm achieves\noptimal error tolerance $\\nu$ among all algorithms making a polynomial number\nof queries of $f$. As a concrete example, we show that our results can be\ndirectly used to give sample complexities for learning a ReLU unit. \n\n"}
{"id": "1803.09974", "contents": "Title: Inferring network connectivity from event timing patterns Abstract: Reconstructing network connectivity from the collective dynamics of a system\ntypically requires access to its complete continuous-time evolution although\nthese are often experimentally inaccessible. Here we propose a theory for\nrevealing physical connectivity of networked systems only from the event time\nseries their intrinsic collective dynamics generate. Representing the patterns\nof event timings in an event space spanned by inter-event and cross-event\nintervals, we reveal which other units directly influence the inter-event times\nof any given unit. For illustration, we linearize an event space mapping\nconstructed from the spiking patterns in model neural circuits to reveal the\npresence or absence of synapses between any pair of neurons as well as whether\nthe coupling acts in an inhibiting or activating (excitatory) manner. The\nproposed model-independent reconstruction theory is scalable to larger networks\nand may thus play an important role in the reconstruction of networks from\nbiology to social science and engineering. \n\n"}
{"id": "1803.10123", "contents": "Title: Task Agnostic Continual Learning Using Online Variational Bayes Abstract: Catastrophic forgetting is the notorious vulnerability of neural networks to\nthe change of the data distribution while learning. This phenomenon has long\nbeen considered a major obstacle for allowing the use of learning agents in\nrealistic continual learning settings. A large body of continual learning\nresearch assumes that task boundaries are known during training. However,\nresearch for scenarios in which task boundaries are unknown during training has\nbeen lacking. In this paper we present, for the first time, a method for\npreventing catastrophic forgetting (BGD) for scenarios with task boundaries\nthat are unknown during training --- task-agnostic continual learning. Code of\nour algorithm is available at https://github.com/igolan/bgd. \n\n"}
{"id": "1803.10736", "contents": "Title: Quantum Experiments and Graphs II: Quantum Interference, Computation and\n  State Generation Abstract: We present a conceptually new approach to describe state-of-the-art photonic\nquantum experiments using Graph Theory. There, the quantum states are given by\nthe coherent superpositions of perfect matchings. The crucial observation is\nthat introducing complex weights in graphs naturally leads to quantum\ninterference. The new viewpoint immediately leads to many interesting results,\nsome of which we present here. Firstly, we identify a new and experimentally\ncompletely unexplored multiphoton interference phenomenon. Secondly, we find\nthat computing the results of such experiments is #P-hard, which means it is a\nclassically intractable problem dealing with the computation of a matrix\nfunction Permanent and its generalization Hafnian. Thirdly, we explain how a\nrecent no-go result applies generally to linear optical quantum experiments,\nthus revealing important insights to quantum state generation with current\nphotonic technology. Fourthly, we show how to describe quantum protocols such\nas entanglement swapping in a graphical way. The uncovered bridge between\nquantum experiments and Graph Theory offers a novel perspective on a widely\nused technology, and immediately raises many follow-up questions. \n\n"}
{"id": "1803.10837", "contents": "Title: Learning Deep Representations with Probabilistic Knowledge Transfer Abstract: Knowledge Transfer (KT) techniques tackle the problem of transferring the\nknowledge from a large and complex neural network into a smaller and faster\none. However, existing KT methods are tailored towards classification tasks and\nthey cannot be used efficiently for other representation learning tasks. In\nthis paper a novel knowledge transfer technique, that is capable of training a\nstudent model that maintains the same amount of mutual information between the\nlearned representation and a set of (possible unknown) labels as the teacher\nmodel, is proposed. Apart from outperforming existing KT techniques, the\nproposed method allows for overcoming several limitations of existing methods\nproviding new insight into KT as well as novel KT applications, ranging from\nknowledge transfer from handcrafted feature extractors to {cross-modal} KT from\nthe textual modality into the representation extracted from the visual modality\nof the data. \n\n"}
{"id": "1803.11395", "contents": "Title: Contrast-Oriented Deep Neural Networks for Salient Object Detection Abstract: Deep convolutional neural networks have become a key element in the recent\nbreakthrough of salient object detection. However, existing CNN-based methods\nare based on either patch-wise (region-wise) training and inference or fully\nconvolutional networks. Methods in the former category are generally\ntime-consuming due to severe storage and computational redundancies among\noverlapping patches. To overcome this deficiency, methods in the second\ncategory attempt to directly map a raw input image to a predicted dense\nsaliency map in a single network forward pass. Though being very efficient, it\nis arduous for these methods to detect salient objects of different scales or\nsalient regions with weak semantic information. In this paper, we develop\nhybrid contrast-oriented deep neural networks to overcome the aforementioned\nlimitations. Each of our deep networks is composed of two complementary\ncomponents, including a fully convolutional stream for dense prediction and a\nsegment-level spatial pooling stream for sparse saliency inference. We further\npropose an attentional module that learns weight maps for fusing the two\nsaliency predictions from these two streams. A tailored alternate scheme is\ndesigned to train these deep networks by fine-tuning pre-trained baseline\nmodels. Finally, a customized fully connected CRF model incorporating a salient\ncontour feature embedding can be optionally applied as a post-processing step\nto improve spatial coherence and contour positioning in the fused result from\nthese two streams. Extensive experiments on six benchmark datasets demonstrate\nthat our proposed model can significantly outperform the state of the art in\nterms of all popular evaluation metrics. \n\n"}
{"id": "1804.00217", "contents": "Title: Fundamental Resource Trade-offs for Encoded Distributed Optimization Abstract: Dealing with the shear size and complexity of today's massive data sets\nrequires computational platforms that can analyze data in a parallelized and\ndistributed fashion. A major bottleneck that arises in such modern distributed\ncomputing environments is that some of the worker nodes may run slow. These\nnodes a.k.a.~stragglers can significantly slow down computation as the slowest\nnode may dictate the overall computational time. A recent computational\nframework, called encoded optimization, creates redundancy in the data to\nmitigate the effect of stragglers. In this paper we develop novel mathematical\nunderstanding for this framework demonstrating its effectiveness in much\nbroader settings than was previously understood. We also analyze the\nconvergence behavior of iterative encoded optimization algorithms, allowing us\nto characterize fundamental trade-offs between convergence rate, size of data\nset, accuracy, computational load (or data redundancy), and straggler\ntoleration in this framework. \n\n"}
{"id": "1804.00792", "contents": "Title: Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks Abstract: Data poisoning is an attack on machine learning models wherein the attacker\nadds examples to the training set to manipulate the behavior of the model at\ntest time. This paper explores poisoning attacks on neural nets. The proposed\nattacks use \"clean-labels\"; they don't require the attacker to have any control\nover the labeling of training data. They are also targeted; they control the\nbehavior of the classifier on a $\\textit{specific}$ test instance without\ndegrading overall classifier performance. For example, an attacker could add a\nseemingly innocuous image (that is properly labeled) to a training set for a\nface recognition engine, and control the identity of a chosen person at test\ntime. Because the attacker does not need to control the labeling function,\npoisons could be entered into the training set simply by leaving them on the\nweb and waiting for them to be scraped by a data collection bot.\n  We present an optimization-based method for crafting poisons, and show that\njust one single poison image can control classifier behavior when transfer\nlearning is used. For full end-to-end training, we present a \"watermarking\"\nstrategy that makes poisoning reliable using multiple ($\\approx$50) poisoned\ntraining instances. We demonstrate our method by generating poisoned frog\nimages from the CIFAR dataset and using them to manipulate image classifiers. \n\n"}
{"id": "1804.02086", "contents": "Title: Structured Disentangled Representations Abstract: Deep latent-variable models learn representations of high-dimensional data in\nan unsupervised manner. A number of recent efforts have focused on learning\nrepresentations that disentangle statistically independent axes of variation by\nintroducing modifications to the standard objective function. These approaches\ngenerally assume a simple diagonal Gaussian prior and as a result are not able\nto reliably disentangle discrete factors of variation. We propose a two-level\nhierarchical objective to control relative degree of statistical independence\nbetween blocks of variables and individual variables within blocks. We derive\nthis objective as a generalization of the evidence lower bound, which allows us\nto explicitly represent the trade-offs between mutual information between data\nand representation, KL divergence between representation and prior, and\ncoverage of the support of the empirical data distribution. Experiments on a\nvariety of datasets demonstrate that our objective can not only disentangle\ndiscrete variables, but that doing so also improves disentanglement of other\nvariables and, importantly, generalization even to unseen combinations of\nfactors. \n\n"}
{"id": "1804.02464", "contents": "Title: Differentiable plasticity: training plastic neural networks with\n  backpropagation Abstract: How can we build agents that keep learning from experience, quickly and\nefficiently, after their initial training? Here we take inspiration from the\nmain mechanism of learning in biological brains: synaptic plasticity, carefully\ntuned by evolution to produce efficient lifelong learning. We show that\nplasticity, just like connection weights, can be optimized by gradient descent\nin large (millions of parameters) recurrent networks with Hebbian plastic\nconnections. First, recurrent plastic networks with more than two million\nparameters can be trained to memorize and reconstruct sets of novel,\nhigh-dimensional 1000+ pixels natural images not seen during training.\nCrucially, traditional non-plastic recurrent networks fail to solve this task.\nFurthermore, trained plastic networks can also solve generic meta-learning\ntasks such as the Omniglot task, with competitive results and little parameter\noverhead. Finally, in reinforcement learning settings, plastic networks\noutperform a non-plastic equivalent in a maze exploration task. We conclude\nthat differentiable plasticity may provide a powerful novel approach to the\nlearning-to-learn problem. \n\n"}
{"id": "1804.02476", "contents": "Title: Associative Compression Networks for Representation Learning Abstract: This paper introduces Associative Compression Networks (ACNs), a new\nframework for variational autoencoding with neural networks. The system differs\nfrom existing variational autoencoders (VAEs) in that the prior distribution\nused to model each code is conditioned on a similar code from the dataset. In\ncompression terms this equates to sequentially transmitting the dataset using\nan ordering determined by proximity in latent space. Since the prior need only\naccount for local, rather than global variations in the latent space, the\ncoding cost is greatly reduced, leading to rich, informative codes. Crucially,\nthe codes remain informative when powerful, autoregressive decoders are used,\nwhich we argue is fundamentally difficult with normal VAEs. Experimental\nresults on MNIST, CIFAR-10, ImageNet and CelebA show that ACNs discover\nhigh-level latent features such as object class, writing style, pose and facial\nexpression, which can be used to cluster and classify the data, as well as to\ngenerate diverse and convincing samples. We conclude that ACNs are a promising\nnew direction for representation learning: one that steps away from IID\nmodelling, and towards learning a structured description of the dataset as a\nwhole. \n\n"}
{"id": "1804.03578", "contents": "Title: Towards Training Probabilistic Topic Models on Neuromorphic Multi-chip\n  Systems Abstract: Probabilistic topic models are popular unsupervised learning methods,\nincluding probabilistic latent semantic indexing (pLSI) and latent Dirichlet\nallocation (LDA). By now, their training is implemented on general purpose\ncomputers (GPCs), which are flexible in programming but energy-consuming.\nTowards low-energy implementations, this paper investigates their training on\nan emerging hardware technology called the neuromorphic multi-chip systems\n(NMSs). NMSs are very effective for a family of algorithms called spiking\nneural networks (SNNs). We present three SNNs to train topic models. The first\nSNN is a batch algorithm combining the conventional collapsed Gibbs sampling\n(CGS) algorithm and an inference SNN to train LDA. The other two SNNs are\nonline algorithms targeting at both energy- and storage-limited environments.\nThe two online algorithms are equivalent with training LDA by using\nmaximum-a-posterior estimation and maximizing the semi-collapsed likelihood,\nrespectively. They use novel, tailored ordinary differential equations for\nstochastic optimization. We simulate the new algorithms and show that they are\ncomparable with the GPC algorithms, while being suitable for NMS\nimplementation. We also propose an extension to train pLSI and a method to\nprune the network to obey the limited fan-in of some NMSs. \n\n"}
{"id": "1804.04205", "contents": "Title: Learning Topics using Semantic Locality Abstract: The topic modeling discovers the latent topic probability of the given text\ndocuments. To generate the more meaningful topic that better represents the\ngiven document, we proposed a new feature extraction technique which can be\nused in the data preprocessing stage. The method consists of three steps.\nFirst, it generates the word/word-pair from every single document. Second, it\napplies a two-way TF-IDF algorithm to word/word-pair for semantic filtering.\nThird, it uses the K-means algorithm to merge the word pairs that have the\nsimilar semantic meaning.\n  Experiments are carried out on the Open Movie Database (OMDb), Reuters\nDataset and 20NewsGroup Dataset. The mean Average Precision score is used as\nthe evaluation metric. Comparing our results with other state-of-the-art topic\nmodels, such as Latent Dirichlet allocation and traditional Restricted\nBoltzmann Machines. Our proposed data preprocessing can improve the generated\ntopic accuracy by up to 12.99\\%. \n\n"}
{"id": "1804.04241", "contents": "Title: Capsules for Object Segmentation Abstract: Convolutional neural networks (CNNs) have shown remarkable results over the\nlast several years for a wide range of computer vision tasks. A new\narchitecture recently introduced by Sabour et al., referred to as a capsule\nnetworks with dynamic routing, has shown great initial results for digit\nrecognition and small image classification. The success of capsule networks\nlies in their ability to preserve more information about the input by replacing\nmax-pooling layers with convolutional strides and dynamic routing, allowing for\npreservation of part-whole relationships in the data. This preservation of the\ninput is demonstrated by reconstructing the input from the output capsule\nvectors. Our work expands the use of capsule networks to the task of object\nsegmentation for the first time in the literature. We extend the idea of\nconvolutional capsules with locally-connected routing and propose the concept\nof deconvolutional capsules. Further, we extend the masked reconstruction to\nreconstruct the positive input class. The proposed\nconvolutional-deconvolutional capsule network, called SegCaps, shows strong\nresults for the task of object segmentation with substantial decrease in\nparameter space. As an example application, we applied the proposed SegCaps to\nsegment pathological lungs from low dose CT scans and compared its accuracy and\nefficiency with other U-Net-based architectures. SegCaps is able to handle\nlarge image sizes (512 x 512) as opposed to baseline capsules (typically less\nthan 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net\narchitecture by 95.4% while still providing a better segmentation accuracy. \n\n"}
{"id": "1804.04438", "contents": "Title: Pooling is neither necessary nor sufficient for appropriate deformation\n  stability in CNNs Abstract: Many of our core assumptions about how neural networks operate remain\nempirically untested. One common assumption is that convolutional neural\nnetworks need to be stable to small translations and deformations to solve\nimage recognition tasks. For many years, this stability was baked into CNN\narchitectures by incorporating interleaved pooling layers. Recently, however,\ninterleaved pooling has largely been abandoned. This raises a number of\nquestions: Are our intuitions about deformation stability right at all? Is it\nimportant? Is pooling necessary for deformation invariance? If not, how is\ndeformation invariance achieved in its absence? In this work, we rigorously\ntest these questions, and find that deformation stability in convolutional\nnetworks is more nuanced than it first appears: (1) Deformation invariance is\nnot a binary property, but rather that different tasks require different\ndegrees of deformation stability at different layers. (2) Deformation stability\nis not a fixed property of a network and is heavily adjusted over the course of\ntraining, largely through the smoothness of the convolutional filters. (3)\nInterleaved pooling layers are neither necessary nor sufficient for achieving\nthe optimal form of deformation stability for natural image classification. (4)\nPooling confers too much deformation stability for image classification at\ninitialization, and during training, networks have to learn to counteract this\ninductive bias. Together, these findings provide new insights into the role of\ninterleaved pooling and deformation invariance in CNNs, and demonstrate the\nimportance of rigorous empirical testing of even our most basic assumptions\nabout the working of neural networks. \n\n"}
{"id": "1804.04512", "contents": "Title: DLL: A Blazing Fast Deep Neural Network Library Abstract: Deep Learning Library (DLL) is a new library for machine learning with deep\nneural networks that focuses on speed. It supports feed-forward neural networks\nsuch as fully-connected Artificial Neural Networks (ANNs) and Convolutional\nNeural Networks (CNNs). It also has very comprehensive support for Restricted\nBoltzmann Machines (RBMs) and Convolutional RBMs. Our main motivation for this\nwork was to propose and evaluate novel software engineering strategies with\npotential to accelerate runtime for training and inference. Such strategies are\nmostly independent of the underlying deep learning algorithms. On three\ndifferent datasets and for four different neural network models, we compared\nDLL to five popular deep learning frameworks. Experimentally, it is shown that\nthe proposed framework is systematically and significantly faster on CPU and\nGPU. In terms of classification performance, similar accuracies as the other\nframeworks are reported. \n\n"}
{"id": "1804.04778", "contents": "Title: Understanding Community Structure in Layered Neural Networks Abstract: A layered neural network is now one of the most common choices for the\nprediction of high-dimensional practical data sets, where the relationship\nbetween input and output data is complex and cannot be represented well by\nsimple conventional models. Its effectiveness is shown in various tasks,\nhowever, the lack of interpretability of the trained result by a layered neural\nnetwork has limited its application area.\n  In our previous studies, we proposed methods for extracting a simplified\nglobal structure of a trained layered neural network by classifying the units\ninto communities according to their connection patterns with adjacent layers.\nThese methods provided us with knowledge about the strength of the relationship\nbetween communities from the existence of bundled connections, which are\ndetermined by threshold processing of the connection ratio between pairs of\ncommunities.\n  However, it has been difficult to understand the role of each community\nquantitatively by observing the modular structure. We could only know to which\nsets of the input and output dimensions each community was mainly connected, by\ntracing the bundled connections from the community to the input and output\nlayers. Another problem is that the finally obtained modular structure is\nchanged greatly depending on the setting of the threshold hyperparameter used\nfor determining bundled connections.\n  In this paper, we propose a new method for interpreting quantitatively the\nrole of each community in inference, by defining the effect of each input\ndimension on a community, and the effect of a community on each output\ndimension. We show experimentally that our proposed method can reveal the role\nof each part of a layered neural network by applying the neural networks to\nthree types of data sets, extracting communities from the trained network, and\napplying the proposed method to the community structure. \n\n"}
{"id": "1804.05012", "contents": "Title: Representing smooth functions as compositions of near-identity functions\n  with implications for deep network optimization Abstract: We show that any smooth bi-Lipschitz $h$ can be represented exactly as a\ncomposition $h_m \\circ ... \\circ h_1$ of functions $h_1,...,h_m$ that are close\nto the identity in the sense that each $\\left(h_i-\\mathrm{Id}\\right)$ is\nLipschitz, and the Lipschitz constant decreases inversely with the number $m$\nof functions composed. This implies that $h$ can be represented to any accuracy\nby a deep residual network whose nonlinear layers compute functions with a\nsmall Lipschitz constant. Next, we consider nonlinear regression with a\ncomposition of near-identity nonlinear maps. We show that, regarding Fr\\'echet\nderivatives with respect to the $h_1,...,h_m$, any critical point of a\nquadratic criterion in this near-identity region must be a global minimizer. In\ncontrast, if we consider derivatives with respect to parameters of a fixed-size\nresidual network with sigmoid activation functions, we show that there are\nnear-identity critical points that are suboptimal, even in the realizable case.\nInformally, this means that functional gradient methods for residual networks\ncannot get stuck at suboptimal critical points corresponding to near-identity\nlayers, whereas parametric gradient methods for sigmoidal residual networks\nsuffer from suboptimal critical points in the near-identity region. \n\n"}
{"id": "1804.05267", "contents": "Title: Low-Precision Floating-Point Schemes for Neural Network Training Abstract: The use of low-precision fixed-point arithmetic along with stochastic\nrounding has been proposed as a promising alternative to the commonly used\n32-bit floating point arithmetic to enhance training neural networks training\nin terms of performance and energy efficiency. In the first part of this paper,\nthe behaviour of the 12-bit fixed-point arithmetic when training a\nconvolutional neural network with the CIFAR-10 dataset is analysed, showing\nthat such arithmetic is not the most appropriate for the training phase. After\nthat, the paper presents and evaluates, under the same conditions, alternative\nlow-precision arithmetics, starting with the 12-bit floating-point arithmetic.\nThese two representations are then leveraged using local scaling in order to\nincrease accuracy and get closer to the baseline 32-bit floating-point\narithmetic. Finally, the paper introduces a simplified model in which both the\noutputs and the gradients of the neural networks are constrained to\npower-of-two values, just using 7 bits for their representation. The evaluation\ndemonstrates a minimal loss in accuracy for the proposed Power-of-Two neural\nnetwork, avoiding the use of multiplications and divisions and thereby,\nsignificantly reducing the training time as well as the energy consumption and\nmemory requirements during the training and inference phases. \n\n"}
{"id": "1804.05810", "contents": "Title: ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object\n  Detector Abstract: Given the ability to directly manipulate image pixels in the digital input\nspace, an adversary can easily generate imperceptible perturbations to fool a\nDeep Neural Network (DNN) image classifier, as demonstrated in prior work. In\nthis work, we propose ShapeShifter, an attack that tackles the more challenging\nproblem of crafting physical adversarial perturbations to fool image-based\nobject detectors like Faster R-CNN. Attacking an object detector is more\ndifficult than attacking an image classifier, as it needs to mislead the\nclassification results in multiple bounding boxes with different scales.\nExtending the digital attack to the physical world adds another layer of\ndifficulty, because it requires the perturbation to be robust enough to survive\nreal-world distortions due to different viewing distances and angles, lighting\nconditions, and camera limitations. We show that the Expectation over\nTransformation technique, which was originally proposed to enhance the\nrobustness of adversarial perturbations in image classification, can be\nsuccessfully adapted to the object detection setting. ShapeShifter can generate\nadversarially perturbed stop signs that are consistently mis-detected by Faster\nR-CNN as other objects, posing a potential threat to autonomous vehicles and\nother safety-critical computer vision systems. \n\n"}
{"id": "1804.05816", "contents": "Title: Models for Capturing Temporal Smoothness in Evolving Networks for\n  Learning Latent Representation of Nodes Abstract: In a dynamic network, the neighborhood of the vertices evolve across\ndifferent temporal snapshots of the network. Accurate modeling of this temporal\nevolution can help solve complex tasks involving real-life social and\ninteraction networks. However, existing models for learning latent\nrepresentation are inadequate for obtaining the representation vectors of the\nvertices for different time-stamps of a dynamic network in a meaningful way. In\nthis paper, we propose latent representation learning models for dynamic\nnetworks which overcome the above limitation by considering two different kinds\nof temporal smoothness: (i) retrofitted, and (ii) linear transformation. The\nretrofitted model tracks the representation vector of a vertex over time,\nfacilitating vertex-based temporal analysis of a network. On the other hand,\nlinear transformation based model provides a smooth transition operator which\nmaps the representation vectors of all vertices from one temporal snapshot to\nthe next (unobserved) snapshot-this facilitates prediction of the state of a\nnetwork in a future time-stamp. We validate the performance of our proposed\nmodels by employing them for solving the temporal link prediction task.\nExperiments on 9 real-life networks from various domains validate that the\nproposed models are significantly better than the existing models for\npredicting the dynamics of an evolving network. \n\n"}
{"id": "1804.05965", "contents": "Title: MaxGain: Regularisation of Neural Networks by Constraining Activation\n  Magnitudes Abstract: Effective regularisation of neural networks is essential to combat\noverfitting due to the large number of parameters involved. We present an\nempirical analogue to the Lipschitz constant of a feed-forward neural network,\nwhich we refer to as the maximum gain. We hypothesise that constraining the\ngain of a network will have a regularising effect, similar to how constraining\nthe Lipschitz constant of a network has been shown to improve generalisation. A\nsimple algorithm is provided that involves rescaling the weight matrix of each\nlayer after each parameter update. We conduct a series of studies on common\nbenchmark datasets, and also a novel dataset that we introduce to enable easier\nsignificance testing for experiments using convolutional networks. Performance\non these datasets compares favourably with other common regularisation\ntechniques. \n\n"}
{"id": "1804.06188", "contents": "Title: VC-Dimension Based Generalization Bounds for Relational Learning Abstract: In many applications of relational learning, the available data can be seen\nas a sample from a larger relational structure (e.g. we may be given a small\nfragment from some social network). In this paper we are particularly concerned\nwith scenarios in which we can assume that (i) the domain elements appearing in\nthe given sample have been uniformly sampled without replacement from the\n(unknown) full domain and (ii) the sample is complete for these domain elements\n(i.e. it is the full substructure induced by these elements). Within this\nsetting, we study bounds on the error of sufficient statistics of relational\nmodels that are estimated on the available data. As our main result, we prove a\nbound based on a variant of the Vapnik-Chervonenkis dimension which is suitable\nfor relational data. \n\n"}
{"id": "1804.06679", "contents": "Title: Understanding Neural Networks and Individual Neuron Importance via\n  Information-Ordered Cumulative Ablation Abstract: In this work, we investigate the use of three information-theoretic\nquantities -- entropy, mutual information with the class variable, and a class\nselectivity measure based on Kullback-Leibler divergence -- to understand and\nstudy the behavior of already trained fully-connected feed-forward neural\nnetworks. We analyze the connection between these information-theoretic\nquantities and classification performance on the test set by cumulatively\nablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our\nresults parallel those recently published by Morcos et al., indicating that\nclass selectivity is not a good indicator for classification performance.\nHowever, looking at individual layers separately, both mutual information and\nclass selectivity are positively correlated with classification performance, at\nleast for networks with ReLU activation functions. We provide explanations for\nthis phenomenon and conclude that it is ill-advised to compare the proposed\ninformation-theoretic quantities across layers. Furthermore, we show that\ncumulative ablation of neurons with ascending or descending\ninformation-theoretic quantities can be used to formulate hypotheses regarding\nthe joint behavior of multiple neurons, such as redundancy and synergy, with\ncomparably low computational cost. We also draw connections to the information\nbottleneck theory for neural networks. \n\n"}
{"id": "1804.06681", "contents": "Title: Contact interactions and Kronig-Penney Models in Hermitian and\n  PT-symmetric Quantum Mechanics Abstract: The delta function potential is a simple model of zero-range contact\ninteraction in one dimension. The Kronig-Penney model is a one-dimensional\nperiodic array of delta functions that models the energy bands in a crystal.\nHere we investigate contact interactions that generalize the delta function\npotential and corresponding generalizations of the Kronig-Penney model within\nconventional and parity-time symmetric quantum mechanics (PTQM). In\nconventional quantum mechanics we determine the most general contact\ninteraction compatible with self-adjointness; in PTQM we consider interactions\nthat are symmetric under the combined transformation PT. In both cases we find\nthat the most general interaction has four independent real parameters;\ndepending on the parameter values the interaction can support more bound states\nthan the conventional delta function. In the PT case the two bound state\nenergies can be both real or a complex conjugate pair, with the transition\ncorresponding to the breaking of PT-symmetry. The scattering states for the PT\ncase are also found to exhibit spontaneous breaking of PT-symmetry. We\ninvestigate the energy bands when the generalized contact interactions are\nrepeated periodically in space in one dimension. In the Hermitian case we find\nthat the two bound states result in two narrow bands generically separated by a\ngap. These bands intersect at a single point in the Brillouin zone as the\ninteraction parameters are varied. Near the intersection the bands form a\nmassless Dirac cone. In the PT-symmetric case, as the parameters of the contact\ninteraction are varied the two bound state bands undergo a PT-symmetry breaking\ntransition wherein the two band energies go from being real to being a complex\nconjugate pair. The PT-symmetric Kronig-Penney model provides a simple soluble\nexample of the transition which has the same form as in other models of\nPT-symmetric crystals. \n\n"}
{"id": "1804.06921", "contents": "Title: All-Optical control of linear and nonlinear energy transfer via Zeno\n  effect Abstract: Microresonator-based nonlinear processes are fundamental to applications\nincluding microcomb generation, parametric frequency conversion, and harmonics\ngeneration. While nonlinear processes involving either second- ($\\chi^{(2)}$)\nor third- $\\chi^{(3)}$) order nonlinearity have been extensively studied, the\ninteraction between these two basic nonlinear processes has seldom been\nreported. In this letter, we demonstrate a coherent interplay between second-\nand third- order nonlinear processes. The parametric ($\\chi^{(2)})$ coupling to\na lossy ancillary mode shortens the lifetime of the target photonic mode and\nsuppresses its density of states, preventing the photon emissions into the\ntarget photonic mode via Zeno effect. Such effect is then used to control the\nstimulated four-wave mixing process and realize a suppression ratio of $34.5$. \n\n"}
{"id": "1804.07193", "contents": "Title: Lipschitz Continuity in Model-based Reinforcement Learning Abstract: We examine the impact of learning Lipschitz continuous models in the context\nof model-based reinforcement learning. We provide a novel bound on multi-step\nprediction error of Lipschitz models where we quantify the error using the\nWasserstein metric. We go on to prove an error bound for the value-function\nestimate arising from Lipschitz models and show that the estimated value\nfunction is itself Lipschitz. We conclude with empirical results that show the\nbenefits of controlling the Lipschitz constant of neural-network models. \n\n"}
{"id": "1804.07209", "contents": "Title: NAIS-Net: Stable Deep Networks from Non-Autonomous Differential\n  Equations Abstract: This paper introduces Non-Autonomous Input-Output Stable Network(NAIS-Net), a\nvery deep architecture where each stacked processing block is derived from a\ntime-invariant non-autonomous dynamical system. Non-autonomy is implemented by\nskip connections from the block input to each of the unrolled processing stages\nand allows stability to be enforced so that blocks can be unrolled adaptively\nto a pattern-dependent processing depth. NAIS-Net induces non-trivial,\nLipschitz input-output maps, even for an infinite unroll length. We prove that\nthe network is globally asymptotically stable so that for every initial\ncondition there is exactly one input-dependent equilibrium assuming $tanh$\nunits, and incrementally stable for ReL units. An efficient implementation that\nenforces the stability under derived conditions for both fully-connected and\nconvolutional layers is also presented. Experimental results show how NAIS-Net\nexhibits stability in practice, yielding a significant reduction in\ngeneralization gap compared to ResNets. \n\n"}
{"id": "1804.09278", "contents": "Title: Opening the black box of neural nets: case studies in stop/top\n  discrimination Abstract: We introduce techniques for exploring the functionality of a neural network\nand extracting simple, human-readable approximations to its performance. By\nperforming gradient ascent on the input space of the network, we are able to\nproduce large populations of artificial events which strongly excite a given\nclassifier. By studying the populations of these events, we then directly\nproduce what are essentially contour maps of the network's classification\nfunction. Combined with a suite of tools for identifying the input dimensions\ndeemed most important by the network, we can utilize these maps to efficiently\ninterpret the dominant criteria by which the network makes its classification.\n  As a test case, we study networks trained to discriminate supersymmetric stop\nproduction in the dilepton channel from Standard Model backgrounds. In the case\nof a heavy stop decaying to a light neutralino, we find individual neurons with\nlarge mutual information with $m_{T2}^{\\ell\\ell}$, a human-designed variable\nfor optimizing the analysis. The network selects events with significant\nmissing $p_T$ oriented azimuthally away from both leptons, efficiently\nrejecting $t\\overline{t}$ background. In the case of a light stop with\nthree-body decays to $Wb{\\widetilde \\chi}$ and little phase space, we find\nneurons that smoothly interpolate between a similar top-rejection strategy and\nan ISR-tagging strategy allowing for more missing momentum. We also find that a\nneural network trained on a stealth stop parameter point learns novel angular\ncorrelations. \n\n"}
{"id": "1804.09530", "contents": "Title: Strong Baselines for Neural Semi-supervised Learning under Domain Shift Abstract: Novel neural models have been proposed in recent years for learning under\ndomain shift. Most models, however, only evaluate on a single task, on\nproprietary datasets, or compare to weak baselines, which makes comparison of\nmodels difficult. In this paper, we re-evaluate classic general-purpose\nbootstrapping approaches in the context of neural networks under domain shifts\nvs. recent neural approaches and propose a novel multi-task tri-training method\nthat reduces the time and space complexity of classic tri-training. Extensive\nexperiments on two benchmarks are negative: while our novel method establishes\na new state-of-the-art for sentiment analysis, it does not fare consistently\nthe best. More importantly, we arrive at the somewhat surprising conclusion\nthat classic tri-training, with some additions, outperforms the state of the\nart. We conclude that classic approaches constitute an important and strong\nbaseline. \n\n"}
{"id": "1804.09720", "contents": "Title: JUNIPR: a Framework for Unsupervised Machine Learning in Particle\n  Physics Abstract: In applications of machine learning to particle physics, a persistent\nchallenge is how to go beyond discrimination to learn about the underlying\nphysics. To this end, a powerful tool would be a framework for unsupervised\nlearning, where the machine learns the intricate high-dimensional contours of\nthe data upon which it is trained, without reference to pre-established labels.\nIn order to approach such a complex task, an unsupervised network must be\nstructured intelligently, based on a qualitative understanding of the data. In\nthis paper, we scaffold the neural network's architecture around a\nleading-order model of the physics underlying the data. In addition to making\nunsupervised learning tractable, this design actually alleviates existing\ntensions between performance and interpretability. We call the framework\nJUNIPR: \"Jets from UNsupervised Interpretable PRobabilistic models\". In this\napproach, the set of particle momenta composing a jet are clustered into a\nbinary tree that the neural network examines sequentially. Training is\nunsupervised and unrestricted: the network could decide that the data bears\nlittle correspondence to the chosen tree structure. However, when there is a\ncorrespondence, the network's output along the tree has a direct physical\ninterpretation. JUNIPR models can perform discrimination tasks, through the\nstatistically optimal likelihood-ratio test, and they permit visualizations of\ndiscrimination power at each branching in a jet's tree. Additionally, JUNIPR\nmodels provide a probability distribution from which events can be drawn,\nproviding a data-driven Monte Carlo generator. As a third application, JUNIPR\nmodels can reweight events from one (e.g. simulated) data set to agree with\ndistributions from another (e.g. experimental) data set. \n\n"}
{"id": "1804.10140", "contents": "Title: Securing Distributed Gradient Descent in High Dimensional Statistical\n  Learning Abstract: We consider unreliable distributed learning systems wherein the training data\nis kept confidential by external workers, and the learner has to interact\nclosely with those workers to train a model. In particular, we assume that\nthere exists a system adversary that can adaptively compromise some workers;\nthe compromised workers deviate from their local designed specifications by\nsending out arbitrarily malicious messages.\n  We assume in each communication round, up to $q$ out of the $m$ workers\nsuffer Byzantine faults. Each worker keeps a local sample of size $n$ and the\ntotal sample size is $N=nm$. We propose a secured variant of the gradient\ndescent method that can tolerate up to a constant fraction of Byzantine\nworkers, i.e., $q/m = O(1)$. Moreover, we show the statistical estimation error\nof the iterates converges in $O(\\log N)$ rounds to $O(\\sqrt{q/N} +\n\\sqrt{d/N})$, where $d$ is the model dimension. As long as $q=O(d)$, our\nproposed algorithm achieves the optimal error rate $O(\\sqrt{d/N})$. Our results\nare obtained under some technical assumptions. Specifically, we assume\nstrongly-convex population risk. Nevertheless, the empirical risk (sample\nversion) is allowed to be non-convex. The core of our method is to robustly\naggregate the gradients computed by the workers based on the filtering\nprocedure proposed by Steinhardt et al. On the technical front, deviating from\nthe existing literature on robustly estimating a finite-dimensional mean\nvector, we establish a {\\em uniform} concentration of the sample covariance\nmatrix of gradients, and show that the aggregated gradient, as a function of\nmodel parameter, converges uniformly to the true gradient function. To get a\nnear-optimal uniform concentration bound, we develop a new matrix concentration\ninequality, which might be of independent interest. \n\n"}
{"id": "1804.11271", "contents": "Title: Gaussian Process Behaviour in Wide Deep Neural Networks Abstract: Whilst deep neural networks have shown great empirical success, there is\nstill much work to be done to understand their theoretical properties. In this\npaper, we study the relationship between random, wide, fully connected,\nfeedforward networks with more than one hidden layer and Gaussian processes\nwith a recursive kernel definition. We show that, under broad conditions, as we\nmake the architecture increasingly wide, the implied random function converges\nin distribution to a Gaussian process, formalising and extending existing\nresults by Neal (1996) to deep networks. To evaluate convergence rates\nempirically, we use maximum mean discrepancy. We then compare finite Bayesian\ndeep networks from the literature to Gaussian processes in terms of the key\npredictive quantities of interest, finding that in some cases the agreement can\nbe very close. We discuss the desirability of Gaussian process behaviour and\nreview non-Gaussian alternative models from the literature. \n\n"}
{"id": "1805.00355", "contents": "Title: Sample-to-Sample Correspondence for Unsupervised Domain Adaptation Abstract: The assumption that training and testing samples are generated from the same\ndistribution does not always hold for real-world machine-learning applications.\nThe procedure of tackling this discrepancy between the training (source) and\ntesting (target) domains is known as domain adaptation. We propose an\nunsupervised version of domain adaptation that considers the presence of only\nunlabelled data in the target domain. Our approach centers on finding\ncorrespondences between samples of each domain. The correspondences are\nobtained by treating the source and target samples as graphs and using a convex\ncriterion to match them. The criteria used are first-order and second-order\nsimilarities between the graphs as well as a class-based regularization. We\nhave also developed a computationally efficient routine for the convex\noptimization, thus allowing the proposed method to be used widely. To verify\nthe effectiveness of the proposed method, computer simulations were conducted\non synthetic, image classification and sentiment classification datasets.\nResults validated that the proposed local sample-to-sample matching method\nout-performs traditional moment-matching methods and is competitive with\nrespect to current local domain-adaptation methods. \n\n"}
{"id": "1805.00452", "contents": "Title: Coupling and Convergence for Hamiltonian Monte Carlo Abstract: Based on a new coupling approach, we prove that the transition step of the\nHamiltonian Monte Carlo algorithm is contractive w.r.t. a carefully designed\nKantorovich (L1 Wasserstein) distance. The lower bound for the contraction rate\nis explicit. Global convexity of the potential is not required, and thus\nmultimodal target distributions are included. Explicit quantitative bounds for\nthe number of steps required to approximate the stationary distribution up to a\ngiven error are a direct consequence of contractivity. These bounds show that\nHMC can overcome diffusive behaviour if the duration of the Hamiltonian\ndynamics is adjusted appropriately. \n\n"}
{"id": "1805.00862", "contents": "Title: Spectral clustering algorithms for the detection of clusters in\n  block-cyclic and block-acyclic graphs Abstract: We propose two spectral algorithms for partitioning nodes in directed graphs\nrespectively with a cyclic and an acyclic pattern of connection between groups\nof nodes. Our methods are based on the computation of extremal eigenvalues of\nthe transition matrix associated to the directed graph. The two algorithms\noutperform state-of-the art methods for directed graph clustering on synthetic\ndatasets, including methods based on blockmodels, bibliometric symmetrization\nand random walks. Our algorithms have the same space complexity as classical\nspectral clustering algorithms for undirected graphs and their time complexity\nis also linear in the number of edges in the graph. One of our methods is\napplied to a trophic network based on predator-prey relationships. It\nsuccessfully extracts common categories of preys and predators encountered in\nfood chains. The same method is also applied to highlight the hierarchical\nstructure of a worldwide network of Autonomous Systems depicting business\nagreements between Internet Service Providers. \n\n"}
{"id": "1805.00932", "contents": "Title: Exploring the Limits of Weakly Supervised Pretraining Abstract: State-of-the-art visual perception models for a wide range of tasks rely on\nsupervised pretraining. ImageNet classification is the de facto pretraining\ntask for these models. Yet, ImageNet is now nearly ten years old and is by\nmodern standards \"small\". Even so, relatively little is known about the\nbehavior of pretraining with datasets that are multiple orders of magnitude\nlarger. The reasons are obvious: such datasets are difficult to collect and\nannotate. In this paper, we present a unique study of transfer learning with\nlarge convolutional networks trained to predict hashtags on billions of social\nmedia images. Our experiments demonstrate that training for large-scale hashtag\nprediction leads to excellent results. We show improvements on several image\nclassification and object detection tasks, and report the highest ImageNet-1k\nsingle-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform\nextensive experiments that provide novel empirical data on the relationship\nbetween large-scale pretraining and transfer learning performance. \n\n"}
{"id": "1805.01648", "contents": "Title: Sharp convergence rates for Langevin dynamics in the nonconvex setting Abstract: We study the problem of sampling from a distribution $p^*(x) \\propto\n\\exp\\left(-U(x)\\right)$, where the function $U$ is $L$-smooth everywhere and\n$m$-strongly convex outside a ball of radius $R$, but potentially nonconvex\ninside this ball. We study both overdamped and underdamped Langevin MCMC and\nestablish upper bounds on the number of steps required to obtain a sample from\na distribution that is within $\\epsilon$ of $p^*$ in $1$-Wasserstein distance.\nFor the first-order method (overdamped Langevin MCMC), the iteration complexity\nis $\\tilde{\\mathcal{O}}\\left(e^{cLR^2}d/\\epsilon^2\\right)$, where $d$ is the\ndimension of the underlying space. For the second-order method (underdamped\nLangevin MCMC), the iteration complexity is\n$\\tilde{\\mathcal{O}}\\left(e^{cLR^2}\\sqrt{d}/\\epsilon\\right)$ for an explicit\npositive constant $c$. Surprisingly, the iteration complexity for both these\nalgorithms is only polynomial in the dimension $d$ and the target accuracy\n$\\epsilon$. It is exponential, however, in the problem parameter $LR^2$, which\nis a measure of non-log-concavity of the target distribution. \n\n"}
{"id": "1805.01713", "contents": "Title: Metasurface imaging with entangled photons Abstract: Plasmonics and metamaterials have recently been shown to allow the control\nand interaction with non-classical states of light, a rather counterintuitive\nfinding given the high losses typically encountered in these systems. Here, we\ndemonstrate a range of functionalities that are allowed with correlated and\nentangled photons that are used to illuminate multiple, overlaid patterns on\nplasmonic metasurfaces. Correlated photons allow to nonlocally determine the\npattern that is imaged or, alternatively to un-scramble an image that is\notherwise blurred. Entangled photons allow a more important functionality\nwhereby the images imprinted on the metasurface are individually visible only\nwhen illuminated with one of the entangled photons. Correlated single photon\nimaging of functional metasurfaces could therefore promise advances towards the\nuse of nanostructured subwavelength thin devices in quantum information\nprotocols. \n\n"}
{"id": "1805.01891", "contents": "Title: Power Law in Sparsified Deep Neural Networks Abstract: The power law has been observed in the degree distributions of many\nbiological neural networks. Sparse deep neural networks, which learn an\neconomical representation from the data, resemble biological neural networks in\nmany ways. In this paper, we study if these artificial networks also exhibit\nproperties of the power law. Experimental results on two popular deep learning\nmodels, namely, multilayer perceptrons and convolutional neural networks, are\naffirmative. The power law is also naturally related to preferential\nattachment. To study the dynamical properties of deep networks in continual\nlearning, we propose an internal preferential attachment model to explain how\nthe network topology evolves. Experimental results show that with the arrival\nof a new task, the new connections made follow this preferential attachment\nprocess. \n\n"}
{"id": "1805.02161", "contents": "Title: Branching embedding: A heuristic dimensionality reduction algorithm\n  based on hierarchical clustering Abstract: This paper proposes a new dimensionality reduction algorithm named branching\nembedding (BE). It converts a dendrogram to a two-dimensional scatter plot, and\nvisualizes the inherent structures of the original high-dimensional data. Since\nthe conversion part is not computationally demanding, the BE algorithm would be\nbeneficial for the case where hierarchical clustering is already performed.\nNumerical experiments revealed that the outputs of the algorithm moderately\npreserve the original hierarchical structures. \n\n"}
{"id": "1805.02269", "contents": "Title: Incorporating Privileged Information to Unsupervised Anomaly Detection Abstract: We introduce a new unsupervised anomaly detection ensemble called SPI which\ncan harness privileged information - data available only for training examples\nbut not for (future) test examples. Our ideas build on the Learning Using\nPrivileged Information (LUPI) paradigm pioneered by Vapnik et al. [19,17],\nwhich we extend to unsupervised learning and in particular to anomaly\ndetection. SPI (for Spotting anomalies with Privileged Information) constructs\na number of frames/fragments of knowledge (i.e., density estimates) in the\nprivileged space and transfers them to the anomaly scoring space through\n\"imitation\" functions that use only the partial information available for test\nexamples. Our generalization of the LUPI paradigm to unsupervised anomaly\ndetection shepherds the field in several key directions, including (i) domain\nknowledge-augmented detection using expert annotations as PI, (ii) fast\ndetection using computationally-demanding data as PI, and (iii) early detection\nusing \"historical future\" data as PI. Through extensive experiments on\nsimulated and real datasets, we show that augmenting privileged information to\nanomaly detection significantly improves detection performance. We also\ndemonstrate the promise of SPI under all three settings (i-iii); with PI\ncapturing expert knowledge, computationally expensive features, and future data\non three real world detection tasks. \n\n"}
{"id": "1805.02296", "contents": "Title: DIRECT: Deep Discriminative Embedding for Clustering of LIGO Data Abstract: In this paper, benefiting from the strong ability of deep neural network in\nestimating non-linear functions, we propose a discriminative embedding function\nto be used as a feature extractor for clustering tasks. The trained embedding\nfunction transfers knowledge from the domain of a labeled set of\nmorphologically-distinct images, known as classes, to a new domain within which\nnew classes can potentially be isolated and identified. Our target application\nin this paper is the Gravity Spy Project, which is an effort to characterize\ntransient, non-Gaussian noise present in data from the Advanced Laser\nInterferometer Gravitational-wave Observatory, or LIGO. Accumulating large,\nlabeled sets of noise features and identifying of new classes of noise lead to\na better understanding of their origin, which makes their removal from the data\nand/or detectors possible. \n\n"}
{"id": "1805.02587", "contents": "Title: Sharp Analysis of a Simple Model for Random Forests Abstract: Random forests have become an important tool for improving accuracy in\nregression and classification problems since their inception by Leo Breiman in\n2001. In this paper, we revisit a historically important random forest model\noriginally proposed by Breiman in 2004 and later studied by G\\'erard Biau in\n2012, where a feature is selected at random and the splits occurs at the\nmidpoint of the node along the chosen feature. If the regression function is\nLipschitz and depends only on a small subset of $ S $ out of $ d $ features, we\nshow that, given access to $ n $ observations and properly tuned split\nprobabilities, the mean-squared prediction error is $ O((n(\\log\nn)^{(S-1)/2})^{-\\frac{1}{S\\log2+1}}) $. This positively answers an outstanding\nquestion of Biau about whether the rate of convergence for this random forest\nmodel could be improved. Furthermore, by a refined analysis of the\napproximation and estimation errors for linear models, we show that this rate\ncannot be improved in general. Finally, we generalize our analysis and improve\nextant prediction error bounds for another random forest model in which each\ntree is constructed from subsampled data and the splits are performed at the\nempirical median along a chosen feature. \n\n"}
{"id": "1805.02971", "contents": "Title: Multinomial Logit Bandit with Linear Utility Functions Abstract: Multinomial logit bandit is a sequential subset selection problem which\narises in many applications. In each round, the player selects a\n$K$-cardinality subset from $N$ candidate items, and receives a reward which is\ngoverned by a {\\it multinomial logit} (MNL) choice model considering both item\nutility and substitution property among items. The player's objective is to\ndynamically learn the parameters of MNL model and maximize cumulative reward\nover a finite horizon $T$. This problem faces the exploration-exploitation\ndilemma, and the involved combinatorial nature makes it non-trivial. In recent\nyears, there have developed some algorithms by exploiting specific\ncharacteristics of the MNL model, but all of them estimate the parameters of\nMNL model separately and incur a regret no better than\n$\\tilde{O}\\big(\\sqrt{NT}\\big)$ which is not preferred for large candidate set\nsize $N$. In this paper, we consider the {\\it linear utility} MNL choice model\nwhose item utilities are represented as linear functions of $d$-dimension item\nfeatures, and propose an algorithm, titled {\\bf LUMB}, to exploit the\nunderlying structure. It is proven that the proposed algorithm achieves\n$\\tilde{O}\\big(dK\\sqrt{T}\\big)$ regret which is free of candidate set size.\nExperiments show the superiority of the proposed algorithm. \n\n"}
{"id": "1805.03716", "contents": "Title: Long Short-Term Memory as a Dynamically Computed Element-wise Weighted\n  Sum Abstract: LSTMs were introduced to combat vanishing gradients in simple RNNs by\naugmenting them with gated additive recurrent connections. We present an\nalternative view to explain the success of LSTMs: the gates themselves are\nversatile recurrent models that provide more representational power than\npreviously appreciated. We do this by decoupling the LSTM's gates from the\nembedded simple RNN, producing a new class of RNNs where the recurrence\ncomputes an element-wise weighted sum of context-independent functions of the\ninput. Ablations on a range of problems demonstrate that the gating mechanism\nalone performs as well as an LSTM in most settings, strongly suggesting that\nthe gates are doing much more in practice than just alleviating vanishing\ngradients. \n\n"}
{"id": "1805.04272", "contents": "Title: An $O(N)$ Sorting Algorithm: Machine Learning Sort Abstract: We propose an $O(N\\cdot M)$ sorting algorithm by Machine Learning method,\nwhich shows a huge potential sorting big data. This sorting algorithm can be\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\nFurthermore, we discuss the application of this algorithm to sparse hash table. \n\n"}
{"id": "1805.04437", "contents": "Title: Cross-lingual Document Retrieval using Regularized Wasserstein Distance Abstract: Many information retrieval algorithms rely on the notion of a good distance\nthat allows to efficiently compare objects of different nature. Recently, a new\npromising metric called Word Mover's Distance was proposed to measure the\ndivergence between text passages. In this paper, we demonstrate that this\nmetric can be extended to incorporate term-weighting schemes and provide more\naccurate and computationally efficient matching between documents using\nentropic regularization. We evaluate the benefits of both extensions in the\ntask of cross-lingual document retrieval (CLDR). Our experimental results on\neight CLDR problems suggest that the proposed methods achieve remarkable\nimprovements in terms of Mean Reciprocal Rank compared to several baselines. \n\n"}
{"id": "1805.06431", "contents": "Title: Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided\n  Mixture Density Networks Abstract: In this paper, we focus on weakly supervised learning with noisy training\ndata for both classification and regression problems.We assume that the\ntraining outputs are collected from a mixture of a target and correlated noise\ndistributions.Our proposed method simultaneously estimates the target\ndistribution and the quality of each data which is defined as the correlation\nbetween the target and data generating distributions.The cornerstone of the\nproposed method is a Cholesky Block that enables modeling dependencies among\nmixture distributions in a differentiable manner where we maintain the\ndistribution over the network weights.We first provide illustrative examples in\nboth regression and classification tasks to show the effectiveness of the\nproposed method.Then, the proposed method is extensively evaluated in a number\nof experiments where we show that it constantly shows comparable or superior\nperformances compared to existing baseline methods in the handling of noisy\ndata. \n\n"}
{"id": "1805.07113", "contents": "Title: Change Point Methods on a Sequence of Graphs Abstract: Given a finite sequence of graphs, e.g., coming from technological,\nbiological, and social networks, the paper proposes a methodology to identify\npossible changes in stationarity in the stochastic process generating the\ngraphs. In order to cover a large class of applications, we consider the\ngeneral family of attributed graphs where both topology (number of vertexes and\nedge configuration) and related attributes are allowed to change also in the\nstationary case. Novel Change Point Methods (CPMs) are proposed, that (i) map\ngraphs into a vector domain; (ii) apply a suitable statistical test in the\nvector space; (iii) detect the change --if any-- according to a confidence\nlevel and provide an estimate for its time occurrence. Two specific\nmultivariate CPMs have been designed: one that detects shifts in the\ndistribution mean, the other addressing generic changes affecting the\ndistribution. We ground our proposal with theoretical results showing how to\nrelate the inference attained in the numerical vector space to the graph\ndomain, and vice versa. We also show how to extend the methodology for handling\nmultiple change points in the same sequence. Finally, the proposed CPMs have\nbeen validated on real data sets coming from epileptic-seizure detection\nproblems and on labeled data sets for graph classification. Results show the\neffectiveness of what proposed in relevant application scenarios. \n\n"}
{"id": "1805.07242", "contents": "Title: Siamese Capsule Networks Abstract: Capsule Networks have shown encouraging results on \\textit{defacto} benchmark\ncomputer vision datasets such as MNIST, CIFAR and smallNORB. Although, they are\nyet to be tested on tasks where (1) the entities detected inherently have more\ncomplex internal representations and (2) there are very few instances per class\nto learn from and (3) where point-wise classification is not suitable. Hence,\nthis paper carries out experiments on face verification in both controlled and\nuncontrolled settings that together address these points. In doing so we\nintroduce \\textit{Siamese Capsule Networks}, a new variant that can be used for\npairwise learning tasks. The model is trained using contrastive loss with\n$\\ell_2$-normalized capsule encoded pose features. We find that \\textit{Siamese\nCapsule Networks} perform well against strong baselines on both pairwise\nlearning datasets, yielding best results in the few-shot learning setting where\nimage pairs in the test set contain unseen subjects. \n\n"}
{"id": "1805.07810", "contents": "Title: Online Structured Laplace Approximations For Overcoming Catastrophic\n  Forgetting Abstract: We introduce the Kronecker factored online Laplace approximation for\novercoming catastrophic forgetting in neural networks. The method is grounded\nin a Bayesian online learning framework, where we recursively approximate the\nposterior after every task with a Gaussian, leading to a quadratic penalty on\nchanges to the weights. The Laplace approximation requires calculating the\nHessian around a mode, which is typically intractable for modern architectures.\nIn order to make our method scalable, we leverage recent block-diagonal\nKronecker factored approximations to the curvature. Our algorithm achieves over\n90% test accuracy across a sequence of 50 instantiations of the permuted MNIST\ndataset, substantially outperforming related methods for overcoming\ncatastrophic forgetting. \n\n"}
{"id": "1805.07836", "contents": "Title: Generalized Cross Entropy Loss for Training Deep Neural Networks with\n  Noisy Labels Abstract: Deep neural networks (DNNs) have achieved tremendous success in a variety of\napplications across many disciplines. Yet, their superior performance comes\nwith the expensive cost of requiring correctly annotated large-scale datasets.\nMoreover, due to DNNs' rich capacity, errors in training labels can hamper\nperformance. To combat this problem, mean absolute error (MAE) has recently\nbeen proposed as a noise-robust alternative to the commonly-used categorical\ncross entropy (CCE) loss. However, as we show in this paper, MAE can perform\npoorly with DNNs and challenging datasets. Here, we present a theoretically\ngrounded set of noise-robust loss functions that can be seen as a\ngeneralization of MAE and CCE. Proposed loss functions can be readily applied\nwith any existing DNN architecture and algorithm, while yielding good\nperformance in a wide range of noisy label scenarios. We report results from\nexperiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and\nsynthetically generated noisy labels. \n\n"}
{"id": "1805.07852", "contents": "Title: Accelerated Bayesian Optimization throughWeight-Prior Tuning Abstract: Bayesian optimization (BO) is a widely-used method for optimizing expensive\n(to evaluate) problems. At the core of most BO methods is the modeling of the\nobjective function using a Gaussian Process (GP) whose covariance is selected\nfrom a set of standard covariance functions. From a weight-space view, this\nmodels the objective as a linear function in a feature space implied by the\ngiven covariance K, with an arbitrary Gaussian weight prior ${\\bf w} \\sim\n\\mathcal{N} ({\\bf 0}, {\\bf I})$. In many practical applications there is data\navailable that has a similar (covariance) structure to the objective, but\nwhich, having different form, cannot be used directly in standard transfer\nlearning. In this paper we show how such auxiliary data may be used to\nconstruct a GP covariance corresponding to a more appropriate weight prior for\nthe objective function. Building on this, we show that we may accelerate BO by\nmodeling the objective function using this (learned) weight prior, which we\ndemonstrate on both test functions and a practical application to short-polymer\nfibre manufacture. \n\n"}
{"id": "1805.07883", "contents": "Title: How Many Samples are Needed to Estimate a Convolutional or Recurrent\n  Neural Network? Abstract: It is widely believed that the practical success of Convolutional Neural\nNetworks (CNNs) and Recurrent Neural Networks (RNNs) owes to the fact that CNNs\nand RNNs use a more compact parametric representation than their\nFully-Connected Neural Network (FNN) counterparts, and consequently require\nfewer training examples to accurately estimate their parameters. We initiate\nthe study of rigorously characterizing the sample-complexity of estimating CNNs\nand RNNs. We show that the sample-complexity to learn CNNs and RNNs scales\nlinearly with their intrinsic dimension and this sample-complexity is much\nsmaller than for their FNN counterparts. For both CNNs and RNNs, we also\npresent lower bounds showing our sample complexities are tight up to\nlogarithmic factors. Our main technical tools for deriving these results are a\nlocalized empirical process analysis and a new technical lemma characterizing\nthe convolutional and recurrent structure. We believe that these tools may\ninspire further developments in understanding CNNs and RNNs. \n\n"}
{"id": "1805.08006", "contents": "Title: Bidirectional Learning for Robust Neural Networks Abstract: A multilayer perceptron can behave as a generative classifier by applying\nbidirectional learning (BL). It consists of training an undirected neural\nnetwork to map input to output and vice-versa; therefore it can produce a\nclassifier in one direction, and a generator in the opposite direction for the\nsame data. The learning process of BL tries to reproduce the neuroplasticity\nstated in Hebbian theory using only backward propagation of errors. In this\npaper, two novel learning techniques are introduced which use BL for improving\nrobustness to white noise static and adversarial examples. The first method is\nbidirectional propagation of errors, which the error propagation occurs in\nbackward and forward directions. Motivated by the fact that its generative\nmodel receives as input a constant vector per class, we introduce as a second\nmethod the hybrid adversarial networks (HAN). Its generative model receives a\nrandom vector as input and its training is based on generative adversarial\nnetworks (GAN). To assess the performance of BL, we perform experiments using\nseveral architectures with fully and convolutional layers, with and without\nbias. Experimental results show that both methods improve robustness to white\nnoise static and adversarial examples, and even increase accuracy, but have\ndifferent behavior depending on the architecture and task, being more\nbeneficial to use the one or the other. Nevertheless, HAN using a convolutional\narchitecture with batch normalization presents outstanding robustness, reaching\nstate-of-the-art accuracy on adversarial examples of hand-written digits. \n\n"}
{"id": "1805.08749", "contents": "Title: A Tropical Approach to Neural Networks with Piecewise Linear Activations Abstract: We present a new, unifying approach following some recent developments on the\ncomplexity of neural networks with piecewise linear activations. We treat\nneural network layers with piecewise linear activations as tropical\npolynomials, which generalize polynomials in the so-called $(\\max, +)$ or\ntropical algebra, with possibly real-valued exponents. Motivated by the\ndiscussion in (arXiv:1402.1869), this approach enables us to refine their upper\nbounds on linear regions of layers with ReLU or leaky ReLU activations to\n$\\min\\left\\{ 2^m, \\sum_{j=0}^n \\binom{m}{j} \\right\\}$, where $n, m$ are the\nnumber of inputs and outputs, respectively. Additionally, we recover their\nupper bounds on maxout layers. Our work follows a novel path, exclusively under\nthe lens of tropical geometry, which is independent of the improvements\nreported in (arXiv:1611.01491, arXiv:1711.02114). Finally, we present a\ngeometric approach for effective counting of linear regions using random\nsampling in order to avoid the computational overhead of exact counting\napproaches \n\n"}
{"id": "1805.08882", "contents": "Title: Multi-task Maximum Entropy Inverse Reinforcement Learning Abstract: Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring\nmultiple reward functions from expert demonstrations. Prior work, built on\nBayesian IRL, is unable to scale to complex environments due to computational\nconstraints. This paper contributes a formulation of multi-task IRL in the more\ncomputationally efficient Maximum Causal Entropy (MCE) IRL framework.\nExperiments show our approach can perform one-shot imitation learning in a\ngridworld environment that single-task IRL algorithms need hundreds of\ndemonstrations to solve. We outline preliminary work using meta-learning to\nextend our method to the function approximator setting of modern MCE IRL\nalgorithms. Evaluating on multi-task variants of common simulated robotics\nbenchmarks, we discover serious limitations of these IRL algorithms, and\nconclude with suggestions for further work. \n\n"}
{"id": "1805.08920", "contents": "Title: Approximate Newton-based statistical inference using only stochastic\n  gradients Abstract: We present a novel statistical inference framework for convex empirical risk\nminimization, using approximate stochastic Newton steps. The proposed algorithm\nis based on the notion of finite differences and allows the approximation of a\nHessian-vector product from first-order information. In theory, our method\nefficiently computes the statistical error covariance in $M$-estimation, both\nfor unregularized convex learning problems and high-dimensional LASSO\nregression, without using exact second order information, or resampling the\nentire data set. We also present a stochastic gradient sampling scheme for\nstatistical inference in non-i.i.d. time series analysis, where we sample\ncontiguous blocks of indices. In practice, we demonstrate the effectiveness of\nour framework on large-scale machine learning problems, that go even beyond\nconvexity: as a highlight, our work can be used to detect certain adversarial\nattacks on neural networks. \n\n"}
{"id": "1805.09682", "contents": "Title: Phocas: dimensional Byzantine-resilient stochastic gradient descent Abstract: We propose a novel robust aggregation rule for distributed synchronous\nStochastic Gradient Descent~(SGD) under a general Byzantine failure model. The\nattackers can arbitrarily manipulate the data transferred between the servers\nand the workers in the parameter server~(PS) architecture. We prove the\nByzantine resilience of the proposed aggregation rules. Empirical analysis\nshows that the proposed techniques outperform current approaches for realistic\nuse cases and Byzantine attack scenarios. \n\n"}
{"id": "1805.10377", "contents": "Title: Ergodic Inference: Accelerate Convergence by Optimisation Abstract: Statistical inference methods are fundamentally important in machine\nlearning. Most state-of-the-art inference algorithms are variants of Markov\nchain Monte Carlo (MCMC) or variational inference (VI). However, both methods\nstruggle with limitations in practice: MCMC methods can be computationally\ndemanding; VI methods may have large bias. In this work, we aim to improve upon\nMCMC and VI by a novel hybrid method based on the idea of reducing simulation\nbias of finite-length MCMC chains using gradient-based optimisation. The\nproposed method can generate low-biased samples by increasing the length of\nMCMC simulation and optimising the MCMC hyper-parameters, which offers\nattractive balance between approximation bias and computational efficiency. We\nshow that our method produces promising results on popular benchmarks when\ncompared to recent hybrid methods of MCMC and VI. \n\n"}
{"id": "1805.10407", "contents": "Title: Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by\n  Minimizing Predictive Variance Abstract: Large amounts of labeled data are typically required to train deep learning\nmodels. For many real-world problems, however, acquiring additional data can be\nexpensive or even impossible. We present semi-supervised deep kernel learning\n(SSDKL), a semi-supervised regression model based on minimizing predictive\nvariance in the posterior regularization framework. SSDKL combines the\nhierarchical representation learning of neural networks with the probabilistic\nmodeling capabilities of Gaussian processes. By leveraging unlabeled data, we\nshow improvements on a diverse set of real-world regression tasks over\nsupervised deep kernel learning and semi-supervised methods such as VAT and\nmean teacher adapted for regression. \n\n"}
{"id": "1805.10561", "contents": "Title: Adversarial Constraint Learning for Structured Prediction Abstract: Constraint-based learning reduces the burden of collecting labels by having\nusers specify general properties of structured outputs, such as constraints\nimposed by physical laws. We propose a novel framework for simultaneously\nlearning these constraints and using them for supervision, bypassing the\ndifficulty of using domain expertise to manually specify constraints. Learning\nrequires a black-box simulator of structured outputs, which generates valid\nlabels, but need not model their corresponding inputs or the input-label\nrelationship. At training time, we constrain the model to produce outputs that\ncannot be distinguished from simulated labels by adversarial training.\nProviding our framework with a small number of labeled inputs gives rise to a\nnew semi-supervised structured prediction model; we evaluate this model on\nmultiple tasks --- tracking, pose estimation and time series prediction --- and\nfind that it achieves high accuracy with only a small number of labeled inputs.\nIn some cases, no labels are required at all. \n\n"}
{"id": "1805.10970", "contents": "Title: A Generative Model For Electron Paths Abstract: Chemical reactions can be described as the stepwise redistribution of\nelectrons in molecules. As such, reactions are often depicted using\n`arrow-pushing' diagrams which show this movement as a sequence of arrows. We\npropose an electron path prediction model (ELECTRO) to learn these sequences\ndirectly from raw reaction data. Instead of predicting product molecules\ndirectly from reactant molecules in one shot, learning a model of electron\nmovement has the benefits of (a) being easy for chemists to interpret, (b)\nincorporating constraints of chemistry, such as balanced atom counts before and\nafter the reaction, and (c) naturally encoding the sparsity of chemical\nreactions, which usually involve changes in only a small number of atoms in the\nreactants.We design a method to extract approximate reaction paths from any\ndataset of atom-mapped reaction SMILES strings. Our model achieves excellent\nperformance on an important subset of the USPTO reaction dataset, comparing\nfavorably to the strongest baselines. Furthermore, we show that our model\nrecovers a basic knowledge of chemistry without being explicitly trained to do\nso. \n\n"}
{"id": "1805.11233", "contents": "Title: Retraining-Based Iterative Weight Quantization for Deep Neural Networks Abstract: Model compression has gained a lot of attention due to its ability to reduce\nhardware resource requirements significantly while maintaining accuracy of\nDNNs. Model compression is especially useful for memory-intensive recurrent\nneural networks because smaller memory footprint is crucial not only for\nreducing storage requirement but also for fast inference operations.\nQuantization is known to be an effective model compression method and\nresearchers are interested in minimizing the number of bits to represent\nparameters. In this work, we introduce an iterative technique to apply\nquantization, presenting high compression ratio without any modifications to\nthe training algorithm. In the proposed technique, weight quantization is\nfollowed by retraining the model with full precision weights. We show that\niterative retraining generates new sets of weights which can be quantized with\ndecreasing quantization loss at each iteration. We also show that quantization\nis efficiently able to leverage pruning, another effective model compression\nmethod. Implementation issues on combining the two methods are also addressed.\nOur experimental results demonstrate that an LSTM model using 1-bit quantized\nweights is sufficient for PTB dataset without any accuracy degradation while\nprevious methods demand at least 2-4 bits for quantized weights. \n\n"}
{"id": "1805.11328", "contents": "Title: Hamiltonian Variational Auto-Encoder Abstract: Variational Auto-Encoders (VAEs) have become very popular techniques to\nperform inference and learning in latent variable models as they allow us to\nleverage the rich representational power of neural networks to obtain flexible\napproximations of the posterior of latent variables as well as tight evidence\nlower bounds (ELBOs). Combined with stochastic variational inference, this\nprovides a methodology scaling to large datasets. However, for this methodology\nto be practically efficient, it is necessary to obtain low-variance unbiased\nestimators of the ELBO and its gradients with respect to the parameters of\ninterest. While the use of Markov chain Monte Carlo (MCMC) techniques such as\nHamiltonian Monte Carlo (HMC) has been previously suggested to achieve this\n[23, 26], the proposed methods require specifying reverse kernels which have a\nlarge impact on performance. Additionally, the resulting unbiased estimator of\nthe ELBO for most MCMC kernels is typically not amenable to the\nreparameterization trick. We show here how to optimally select reverse kernels\nin this setting and, by building upon Hamiltonian Importance Sampling (HIS)\n[17], we obtain a scheme that provides low-variance unbiased estimators of the\nELBO and its gradients using the reparameterization trick. This allows us to\ndevelop a Hamiltonian Variational Auto-Encoder (HVAE). This method can be\nreinterpreted as a target-informed normalizing flow [20] which, within our\ncontext, only requires a few evaluations of the gradient of the sampled\nlikelihood and trivial Jacobian calculations at each iteration. \n\n"}
{"id": "1805.11604", "contents": "Title: How Does Batch Normalization Help Optimization? Abstract: Batch Normalization (BatchNorm) is a widely adopted technique that enables\nfaster and more stable training of deep neural networks (DNNs). Despite its\npervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly\nunderstood. The popular belief is that this effectiveness stems from\ncontrolling the change of the layers' input distributions during training to\nreduce the so-called \"internal covariate shift\". In this work, we demonstrate\nthat such distributional stability of layer inputs has little to do with the\nsuccess of BatchNorm. Instead, we uncover a more fundamental impact of\nBatchNorm on the training process: it makes the optimization landscape\nsignificantly smoother. This smoothness induces a more predictive and stable\nbehavior of the gradients, allowing for faster training. \n\n"}
{"id": "1805.11916", "contents": "Title: On the Spectrum of Random Features Maps of High Dimensional Data Abstract: Random feature maps are ubiquitous in modern statistical machine learning,\nwhere they generalize random projections by means of powerful, yet often\ndifficult to analyze nonlinear operators. In this paper, we leverage the\n\"concentration\" phenomenon induced by random matrix theory to perform a\nspectral analysis on the Gram matrix of these random feature maps, here for\nGaussian mixture models of simultaneously large dimension and size. Our results\nare instrumental to a deeper understanding on the interplay of the nonlinearity\nand the statistics of the data, thereby allowing for a better tuning of random\nfeature-based techniques. \n\n"}
{"id": "1805.11917", "contents": "Title: The Dynamics of Learning: A Random Matrix Approach Abstract: Understanding the learning dynamics of neural networks is one of the key\nissues for the improvement of optimization algorithms as well as for the\ntheoretical comprehension of why deep neural nets work so well today. In this\npaper, we introduce a random matrix-based framework to analyze the learning\ndynamics of a single-layer linear network on a binary classification problem,\nfor data of simultaneously large dimension and size, trained by gradient\ndescent. Our results provide rich insights into common questions in neural\nnets, such as overfitting, early stopping and the initialization of training,\nthereby opening the door for future studies of more elaborate structures and\nmodels appearing in today's neural networks. \n\n"}
{"id": "1805.11921", "contents": "Title: Anonymous Walk Embeddings Abstract: The task of representing entire graphs has seen a surge of prominent results,\nmainly due to learning convolutional neural networks (CNNs) on graph-structured\ndata. While CNNs demonstrate state-of-the-art performance in graph\nclassification task, such methods are supervised and therefore steer away from\nthe original problem of network representation in task-agnostic manner. Here,\nwe coherently propose an approach for embedding entire graphs and show that our\nfeature representations with SVM classifier increase classification accuracy of\nCNN algorithms and traditional graph kernels. For this we describe a recently\ndiscovered graph object, anonymous walk, on which we design task-independent\nalgorithms for learning graph representations in explicit and distributed way.\nOverall, our work represents a new scalable unsupervised learning of\nstate-of-the-art representations of entire graphs. \n\n"}
{"id": "1805.12062", "contents": "Title: Sobolev Descent Abstract: We study a simplification of GAN training: the problem of transporting\nparticles from a source to a target distribution. Starting from the Sobolev GAN\ncritic, part of the gradient regularized GAN family, we show a strong relation\nwith Optimal Transport (OT). Specifically with the less popular dynamic\nformulation of OT that finds a path of distributions from source to target\nminimizing a ``kinetic energy''. We introduce Sobolev descent that constructs\nsimilar paths by following gradient flows of a critic function in a kernel\nspace or parametrized by a neural network. In the kernel version, we show\nconvergence to the target distribution in the MMD sense. We show in theory and\nexperiments that regularization has an important role in favoring smooth\ntransitions between distributions, avoiding large gradients from the critic.\nThis analysis in a simplified particle setting provides insight in paths to\nequilibrium in GANs. \n\n"}
{"id": "1805.12244", "contents": "Title: Mining gold from implicit models to improve likelihood-free inference Abstract: Simulators often provide the best description of real-world phenomena.\nHowever, they also lead to challenging inverse problems because the density\nthey implicitly define is often intractable. We present a new suite of\nsimulation-based inference techniques that go beyond the traditional\nApproximate Bayesian Computation approach, which struggles in a\nhigh-dimensional setting, and extend methods that use surrogate models based on\nneural networks. We show that additional information, such as the joint\nlikelihood ratio and the joint score, can often be extracted from simulators\nand used to augment the training data for these surrogate models. Finally, we\ndemonstrate that these new techniques are more sample efficient and provide\nhigher-fidelity inference than traditional methods. \n\n"}
{"id": "1805.12514", "contents": "Title: Scaling provable adversarial defenses Abstract: Recent work has developed methods for learning deep network classifiers that\nare provably robust to norm-bounded adversarial perturbation; however, these\nmethods are currently only possible for relatively small feedforward networks.\nIn this paper, in an effort to scale these approaches to substantially larger\nmodels, we extend previous work in three main directions. First, we present a\ntechnique for extending these training procedures to much more general\nnetworks, with skip connections (such as ResNets) and general nonlinearities;\nthe approach is fully modular, and can be implemented automatically (analogous\nto automatic differentiation). Second, in the specific case of $\\ell_\\infty$\nadversarial perturbations and networks with ReLU nonlinearities, we adopt a\nnonlinear random projection for training, which scales linearly in the number\nof hidden units (previous approaches scaled quadratically). Third, we show how\nto further improve robust error through cascade models. On both MNIST and CIFAR\ndata sets, we train classifiers that improve substantially on the state of the\nart in provable robust adversarial error bounds: from 5.8% to 3.1% on MNIST\n(with $\\ell_\\infty$ perturbations of $\\epsilon=0.1$), and from 80% to 36.4% on\nCIFAR (with $\\ell_\\infty$ perturbations of $\\epsilon=2/255$). Code for all\nexperiments in the paper is available at\nhttps://github.com/locuslab/convex_adversarial/. \n\n"}
{"id": "1805.12549", "contents": "Title: Channel Gating Neural Networks Abstract: This paper introduces channel gating, a dynamic, fine-grained, and\nhardware-efficient pruning scheme to reduce the computation cost for\nconvolutional neural networks (CNNs). Channel gating identifies regions in the\nfeatures that contribute less to the classification result, and skips the\ncomputation on a subset of the input channels for these ineffective regions.\nUnlike static network pruning, channel gating optimizes CNN inference at\nrun-time by exploiting input-specific characteristics, which allows\nsubstantially reducing the compute cost with almost no accuracy loss. We\nexperimentally show that applying channel gating in state-of-the-art networks\nachieves 2.7-8.0$\\times$ reduction in floating-point operations (FLOPs) and\n2.0-4.4$\\times$ reduction in off-chip memory accesses with a minimal accuracy\nloss on CIFAR-10. Combining our method with knowledge distillation reduces the\ncompute cost of ResNet-18 by 2.6$\\times$ without accuracy drop on ImageNet. We\nfurther demonstrate that channel gating can be realized in hardware\nefficiently. Our approach exhibits sparsity patterns that are well-suited to\ndense systolic arrays with minimal additional hardware. We have designed an\naccelerator for channel gating networks, which can be implemented using either\nFPGAs or ASICs. Running a quantized ResNet-18 model for ImageNet, our\naccelerator achieves an encouraging speedup of 2.4$\\times$ on average, with a\ntheoretical FLOP reduction of 2.8$\\times$. \n\n"}
{"id": "1806.00101", "contents": "Title: Generative Ratio Matching Networks Abstract: Deep generative models can learn to generate realistic-looking images, but\nmany of the most effective methods are adversarial and involve a saddlepoint\noptimization, which requires a careful balancing of training between a\ngenerator network and a critic network. Maximum mean discrepancy networks\n(MMD-nets) avoid this issue by using kernel as a fixed adversary, but\nunfortunately, they have not on their own been able to match the generative\nquality of adversarial training. In this work, we take their insight of using\nkernels as fixed adversaries further and present a novel method for training\ndeep generative models that does not involve saddlepoint optimization. We call\nour method generative ratio matching or GRAM for short. In GRAM, the generator\nand the critic networks do not play a zero-sum game against each other,\ninstead, they do so against a fixed kernel. Thus GRAM networks are not only\nstable to train like MMD-nets but they also match and beat the generative\nquality of adversarially trained generative networks. \n\n"}
{"id": "1806.00144", "contents": "Title: Sea surface temperature prediction and reconstruction using patch-level\n  neural network representations Abstract: The forecasting and reconstruction of ocean and atmosphere dynamics from\nsatellite observation time series are key challenges. While model-driven\nrepresentations remain the classic approaches, data-driven representations\nbecome more and more appealing to benefit from available large-scale\nobservation and simulation datasets. In this work we investigate the relevance\nof recently introduced bilinear residual neural network representations, which\nmimic numerical integration schemes such as Runge-Kutta, for the forecasting\nand assimilation of geophysical fields from satellite-derived remote sensing\ndata. As a case-study, we consider satellite-derived Sea Surface Temperature\ntime series off South Africa, which involves intense and complex upper ocean\ndynamics. Our numerical experiments demonstrate that the proposed patch-level\nneural-network-based representations outperform other data-driven models,\nincluding analog schemes, both in terms of forecasting and missing data\ninterpolation performance with a relative gain up to 50\\% for highly dynamic\nareas. \n\n"}
{"id": "1806.00176", "contents": "Title: Reparameterization Gradient for Non-differentiable Models Abstract: We present a new algorithm for stochastic variational inference that targets\nat models with non-differentiable densities. One of the key challenges in\nstochastic variational inference is to come up with a low-variance estimator of\nthe gradient of a variational objective. We tackle the challenge by\ngeneralizing the reparameterization trick, one of the most effective techniques\nfor addressing the variance issue for differentiable models, so that the trick\nworks for non-differentiable models as well. Our algorithm splits the space of\nlatent variables into regions where the density of the variables is\ndifferentiable, and their boundaries where the density may fail to be\ndifferentiable. For each differentiable region, the algorithm applies the\nstandard reparameterization trick and estimates the gradient restricted to the\nregion. For each potentially non-differentiable boundary, it uses a form of\nmanifold sampling and computes the direction for variational parameters that,\nif followed, would increase the boundary's contribution to the variational\nobjective. The sum of all the estimates becomes the gradient estimate of our\nalgorithm. Our estimator enjoys the reduced variance of the reparameterization\ngradient while remaining unbiased even for non-differentiable models. The\nexperiments with our preliminary implementation confirm the benefit of reduced\nvariance and unbiasedness. \n\n"}
{"id": "1806.00179", "contents": "Title: The Nonlinearity Coefficient - Predicting Generalization in Deep Neural\n  Networks Abstract: For a long time, designing neural architectures that exhibit high performance\nwas considered a dark art that required expert hand-tuning. One of the few\nwell-known guidelines for architecture design is the avoidance of exploding\ngradients, though even this guideline has remained relatively vague and\ncircumstantial. We introduce the nonlinearity coefficient (NLC), a measurement\nof the complexity of the function computed by a neural network that is based on\nthe magnitude of the gradient. Via an extensive empirical study, we show that\nthe NLC is a powerful predictor of test error and that attaining a right-sized\nNLC is essential for optimal performance.\n  The NLC exhibits a range of intriguing and important properties. It is\nclosely tied to the amount of information gained from computing a single\nnetwork gradient. It is tied to the error incurred when replacing the\nnonlinearity operations in the network with linear operations. It is not\nsusceptible to the confounders of multiplicative scaling, additive bias and\nlayer width. It is stable from layer to layer. Hence, we argue that the NLC is\nthe first robust predictor of overfitting in deep networks. \n\n"}
{"id": "1806.00319", "contents": "Title: Learning convex bounds for linear quadratic control policy synthesis Abstract: Learning to make decisions from observed data in dynamic environments remains\na problem of fundamental importance in a number of fields, from artificial\nintelligence and robotics, to medicine and finance. This paper concerns the\nproblem of learning control policies for unknown linear dynamical systems so as\nto maximize a quadratic reward function. We present a method to optimize the\nexpected value of the reward over the posterior distribution of the unknown\nsystem parameters, given data. The algorithm involves sequential convex\nprograming, and enjoys reliable local convergence and robust stability\nguarantees. Numerical simulations and stabilization of a real-world inverted\npendulum are used to demonstrate the approach, with strong performance and\nrobustness properties observed in both. \n\n"}
{"id": "1806.00381", "contents": "Title: Persistence paths and signature features in topological data analysis Abstract: We introduce a new feature map for barcodes that arise in persistent homology\ncomputation. The main idea is to first realize each barcode as a path in a\nconvenient vector space, and to then compute its path signature which takes\nvalues in the tensor algebra of that vector space. The composition of these two\noperations - barcode to path, path to tensor series - results in a feature map\nthat has several desirable properties for statistical learning, such as\nuniversality and characteristicness, and achieves state-of-the-art results on\ncommon classification benchmarks. \n\n"}
{"id": "1806.00499", "contents": "Title: Backpropagation for Implicit Spectral Densities Abstract: Most successful machine intelligence systems rely on gradient-based learning,\nwhich is made possible by backpropagation. Some systems are designed to aid us\nin interpreting data when explicit goals cannot be provided. These unsupervised\nsystems are commonly trained by backpropagating through a likelihood function.\nWe introduce a tool that allows us to do this even when the likelihood is not\nexplicitly set, by instead using the implicit likelihood of the model.\nExplicitly defining the likelihood often entails making heavy-handed\nassumptions that impede our ability to solve challenging tasks. On the other\nhand, the implicit likelihood of the model is accessible without the need for\nsuch assumptions. Our tool, which we call spectral backpropagation, allows us\nto optimize it in much greater generality than what has been attempted before.\nGANs can also be viewed as a technique for optimizing implicit likelihoods. We\nstudy them using spectral backpropagation in order to demonstrate robustness\nfor high-dimensional problems, and identify two novel properties of the\ngenerator G: (1) there exist aberrant, nonsensical outputs to which G assigns\nvery high likelihood, and (2) the eigenvectors of the metric induced by G over\nlatent space correspond to quasi-disentangled explanatory factors. \n\n"}
{"id": "1806.00580", "contents": "Title: Detecting Adversarial Examples via Key-based Network Abstract: Though deep neural networks have achieved state-of-the-art performance in\nvisual classification, recent studies have shown that they are all vulnerable\nto the attack of adversarial examples. Small and often imperceptible\nperturbations to the input images are sufficient to fool the most powerful deep\nneural networks. Various defense methods have been proposed to address this\nissue. However, they either require knowledge on the process of generating\nadversarial examples, or are not robust against new attacks specifically\ndesigned to penetrate the existing defense. In this work, we introduce\nkey-based network, a new detection-based defense mechanism to distinguish\nadversarial examples from normal ones based on error correcting output codes,\nusing the binary code vectors produced by multiple binary classifiers applied\nto randomly chosen label-sets as signatures to match normal images and reject\nadversarial examples. In contrast to existing defense methods, the proposed\nmethod does not require knowledge of the process for generating adversarial\nexamples and can be applied to defend against different types of attacks. For\nthe practical black-box and gray-box scenarios, where the attacker does not\nknow the encoding scheme, we show empirically that key-based network can\neffectively detect adversarial examples generated by several state-of-the-art\nattacks. \n\n"}
{"id": "1806.00804", "contents": "Title: NAM: Non-Adversarial Unsupervised Domain Mapping Abstract: Several methods were recently proposed for the task of translating images\nbetween domains without prior knowledge in the form of correspondences. The\nexisting methods apply adversarial learning to ensure that the distribution of\nthe mapped source domain is indistinguishable from the target domain, which\nsuffers from known stability issues. In addition, most methods rely heavily on\n`cycle' relationships between the domains, which enforce a one-to-one mapping.\nIn this work, we introduce an alternative method: Non-Adversarial Mapping\n(NAM), which separates the task of target domain generative modeling from the\ncross-domain mapping task. NAM relies on a pre-trained generative model of the\ntarget domain, and aligns each source image with an image synthesized from the\ntarget domain, while jointly optimizing the domain mapping function. It has\nseveral key advantages: higher quality and resolution image translations,\nsimpler and more stable training and reusable target models. Extensive\nexperiments are presented validating the advantages of our method. \n\n"}
{"id": "1806.00880", "contents": "Title: Disconnected Manifold Learning for Generative Adversarial Networks Abstract: Natural images may lie on a union of disjoint manifolds rather than one\nglobally connected manifold, and this can cause several difficulties for the\ntraining of common Generative Adversarial Networks (GANs). In this work, we\nfirst show that single generator GANs are unable to correctly model a\ndistribution supported on a disconnected manifold, and investigate how sample\nquality, mode dropping and local convergence are affected by this. Next, we\nshow how using a collection of generators can address this problem, providing\nnew insights into the success of such multi-generator GANs. Finally, we explain\nthe serious issues caused by considering a fixed prior over the collection of\ngenerators and propose a novel approach for learning the prior and inferring\nthe necessary number of generators without any supervision. Our proposed\nmodifications can be applied on top of any other GAN model to enable learning\nof distributions supported on disconnected manifolds. We conduct several\nexperiments to illustrate the aforementioned shortcoming of GANs, its\nconsequences in practice, and the effectiveness of our proposed modifications\nin alleviating these issues. \n\n"}
{"id": "1806.01009", "contents": "Title: On the total variation regularized estimator over a class of tree graphs Abstract: We generalize to tree graphs obtained by connecting path graphs an oracle\nresult obtained for the Fused Lasso over the path graph. Moreover we show that\nit is possible to substitute in the oracle inequality the minimum of the\ndistances between jumps by their harmonic mean. In doing so we prove a lower\nbound on the compatibility constant for the total variation penalty. Our\nanalysis leverages insights obtained for the path graph with one branch to\nunderstand the case of more general tree graphs.\n  As a side result, we get insights into the irrepresentable condition for such\ntree graphs. \n\n"}
{"id": "1806.01203", "contents": "Title: Relational inductive bias for physical construction in humans and\n  machines Abstract: While current deep learning systems excel at tasks such as object\nclassification, language processing, and gameplay, few can construct or modify\na complex system such as a tower of blocks. We hypothesize that what these\nsystems lack is a \"relational inductive bias\": a capacity for reasoning about\ninter-object relations and making choices over a structured description of a\nscene. To test this hypothesis, we focus on a task that involves gluing pairs\nof blocks together to stabilize a tower, and quantify how well humans perform.\nWe then introduce a deep reinforcement learning agent which uses object- and\nrelation-centric scene and policy representations and apply it to the task. Our\nresults show that these structured representations allow the agent to\noutperform both humans and more naive approaches, suggesting that relational\ninductive bias is an important component in solving structured reasoning\nproblems and for building more intelligent, flexible machines. \n\n"}
{"id": "1806.01794", "contents": "Title: Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects Abstract: We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep\ngenerative model for videos of moving objects. It can reliably discover and\ntrack objects throughout the sequence of frames, and can also generate future\nframes conditioning on the current frame, thereby simulating expected motion of\nobjects. This is achieved by explicitly encoding object presence, locations and\nappearances in the latent variables of the model. SQAIR retains all strengths\nof its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al., 2016),\nincluding learning in an unsupervised manner, and addresses its shortcomings.\nWe use a moving multi-MNIST dataset to show limitations of AIR in detecting\noverlapping or partially occluded objects, and show how SQAIR overcomes them by\nleveraging temporal consistency of objects. Finally, we also apply SQAIR to\nreal-world pedestrian CCTV data, where it learns to reliably detect, track and\ngenerate walking pedestrians with no supervision. \n\n"}
{"id": "1806.02003", "contents": "Title: Deep Algorithms: designs for networks Abstract: A new design methodology for neural networks that is guided by traditional\nalgorithm design is presented. To prove our point, we present two heuristics\nand demonstrate an algorithmic technique for incorporating additional weights\nin their signal-flow graphs. We show that with training the performance of\nthese networks can not only exceed the performance of the initial network, but\ncan match the performance of more-traditional neural network architectures. A\nkey feature of our approach is that these networks are initialized with\nparameters that provide a known performance threshold for the architecture on a\ngiven task. \n\n"}
{"id": "1806.02012", "contents": "Title: A Peek Into the Hidden Layers of a Convolutional Neural Network Through\n  a Factorization Lens Abstract: Despite their increasing popularity and success in a variety of supervised\nlearning problems, deep neural networks are extremely hard to interpret and\ndebug: Given and already trained Deep Neural Net, and a set of test inputs, how\ncan we gain insight into how those inputs interact with different layers of the\nneural network? Furthermore, can we characterize a given deep neural network\nbased on it's observed behavior on different inputs? In this paper we propose a\nnovel factorization based approach on understanding how different deep neural\nnetworks operate. In our preliminary results, we identify fascinating patterns\nthat link the factorization rank (typically used as a measure of\ninterestingness in unsupervised data analysis) with how well or poorly the deep\nnetwork has been trained. Finally, our proposed approach can help provide\nvisual insights on how high-level. interpretable patterns of the network's\ninput behave inside the hidden layers of the deep network. \n\n"}
{"id": "1806.02322", "contents": "Title: Learning Kolmogorov Models for Binary Random Variables Abstract: We summarize our recent findings, where we proposed a framework for learning\na Kolmogorov model, for a collection of binary random variables. More\nspecifically, we derive conditions that link outcomes of specific random\nvariables, and extract valuable relations from the data. We also propose an\nalgorithm for computing the model and show its first-order optimality, despite\nthe combinatorial nature of the learning problem. We apply the proposed\nalgorithm to recommendation systems, although it is applicable to other\nscenarios. We believe that the work is a significant step toward interpretable\nmachine learning. \n\n"}
{"id": "1806.02338", "contents": "Title: Towards Dependability Metrics for Neural Networks Abstract: Artificial neural networks (NN) are instrumental in realizing\nhighly-automated driving functionality. An overarching challenge is to identify\nbest safety engineering practices for NN and other learning-enabled components.\nIn particular, there is an urgent need for an adequate set of metrics for\nmeasuring all-important NN dependability attributes. We address this challenge\nby proposing a number of NN-specific and efficiently computable metrics for\nmeasuring NN dependability attributes including robustness, interpretability,\ncompleteness, and correctness. \n\n"}
{"id": "1806.02389", "contents": "Title: Not All Attributes are Created Equal: $d_{\\mathcal{X}}$-Private\n  Mechanisms for Linear Queries Abstract: Differential privacy provides strong privacy guarantees simultaneously\nenabling useful insights from sensitive datasets. However, it provides the same\nlevel of protection for all elements (individuals and attributes) in the data.\nThere are practical scenarios where some data attributes need more/less\nprotection than others. In this paper, we consider $d_{\\mathcal{X}}$-privacy,\nan instantiation of the privacy notion introduced in\n\\cite{chatzikokolakis2013broadening}, which allows this flexibility by\nspecifying a separate privacy budget for each pair of elements in the data\ndomain. We describe a systematic procedure to tailor any existing\ndifferentially private mechanism that assumes a query set and a sensitivity\nvector as input into its $d_{\\mathcal{X}}$-private variant, specifically\nfocusing on linear queries. Our proposed meta procedure has broad applications\nas linear queries form the basis of a range of data analysis and machine\nlearning algorithms, and the ability to define a more flexible privacy budget\nacross the data domain results in improved privacy/utility tradeoff in these\napplications. We propose several $d_{\\mathcal{X}}$-private mechanisms, and\nprovide theoretical guarantees on the trade-off between utility and privacy. We\nalso experimentally demonstrate the effectiveness of our procedure, by\nevaluating our proposed $d_{\\mathcal{X}}$-private Laplace mechanism on both\nsynthetic and real datasets using a set of randomly generated linear queries. \n\n"}
{"id": "1806.02855", "contents": "Title: Scalable Natural Gradient Langevin Dynamics in Practice Abstract: Stochastic Gradient Langevin Dynamics (SGLD) is a sampling scheme for\nBayesian modeling adapted to large datasets and models. SGLD relies on the\ninjection of Gaussian Noise at each step of a Stochastic Gradient Descent (SGD)\nupdate. In this scheme, every component in the noise vector is independent and\nhas the same scale, whereas the parameters we seek to estimate exhibit strong\nvariations in scale and significant correlation structures, leading to poor\nconvergence and mixing times. We compare different preconditioning approaches\nto the normalization of the noise vector and benchmark these approaches on the\nfollowing criteria: 1) mixing times of the multivariate parameter vector, 2)\nregularizing effect on small dataset where it is easy to overfit, 3) covariate\nshift detection and 4) resistance to adversarial examples. \n\n"}
{"id": "1806.02997", "contents": "Title: q-Space Novelty Detection with Variational Autoencoders Abstract: In machine learning, novelty detection is the task of identifying novel\nunseen data. During training, only samples from the normal class are available.\nTest samples are classified as normal or abnormal by assignment of a novelty\nscore. Here we propose novelty detection methods based on training variational\nautoencoders (VAEs) on normal data. Since abnormal samples are not used during\ntraining, we define novelty metrics based on the (partially complementary)\nassumptions that the VAE is less capable of reconstructing abnormal samples\nwell; that abnormal samples more strongly violate the VAE regularizer; and that\nabnormal samples differ from normal samples not only in input-feature space,\nbut also in the VAE latent space and VAE output. These approaches, combined\nwith various possibilities of using (e.g. sampling) the probabilistic VAE to\nobtain scalar novelty scores, yield a large family of methods. We apply these\nmethods to magnetic resonance imaging, namely to the detection of\ndiffusion-space (q-space) abnormalities in diffusion MRI scans of multiple\nsclerosis patients, i.e. to detect multiple sclerosis lesions without using any\nlesion labels for training. Many of our methods outperform previously proposed\nq-space novelty detection methods. We also evaluate the proposed methods on the\nMNIST handwritten digits dataset and show that many of them are able to\noutperform the state of the art. \n\n"}
{"id": "1806.03107", "contents": "Title: Temporal Difference Variational Auto-Encoder Abstract: To act and plan in complex environments, we posit that agents should have a\nmental simulator of the world with three characteristics: (a) it should build\nan abstract state representing the condition of the world; (b) it should form a\nbelief which represents uncertainty on the world; (c) it should go beyond\nsimple step-by-step simulation, and exhibit temporal abstraction. Motivated by\nthe absence of a model satisfying all these requirements, we propose TD-VAE, a\ngenerative sequence model that learns representations containing explicit\nbeliefs about states several steps into the future, and that can be rolled out\ndirectly without single-step transitions. TD-VAE is trained on pairs of\ntemporally separated time points, using an analogue of temporal difference\nlearning used in reinforcement learning. \n\n"}
{"id": "1806.03551", "contents": "Title: An Estimation and Analysis Framework for the Rasch Model Abstract: The Rasch model is widely used for item response analysis in applications\nranging from recommender systems to psychology, education, and finance. While a\nnumber of estimators have been proposed for the Rasch model over the last\ndecades, the available analytical performance guarantees are mostly asymptotic.\nThis paper provides a framework that relies on a novel linear minimum\nmean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic,\nand closed-form analysis of the parameter estimation error under the Rasch\nmodel. The proposed framework provides guidelines on the number of items and\nresponses required to attain low estimation errors in tests or surveys. We\nfurthermore demonstrate its efficacy on a number of real-world collaborative\nfiltering datasets, which reveals that the proposed L-MMSE estimator performs\non par with state-of-the-art nonlinear estimators in terms of predictive\nperformance. \n\n"}
{"id": "1806.03925", "contents": "Title: Gear Training: A new way to implement high-performance model-parallel\n  training Abstract: The training of Deep Neural Networks usually needs tremendous computing\nresources. Therefore many deep models are trained in large cluster instead of\nsingle machine or GPU. Though major researchs at present try to run whole model\non all machines by using asynchronous asynchronous stochastic gradient descent\n(ASGD), we present a new approach to train deep model parallely -- split the\nmodel and then seperately train different parts of it in different speed. \n\n"}
{"id": "1806.03972", "contents": "Title: A Multi-task Deep Learning Architecture for Maritime Surveillance using\n  AIS Data Streams Abstract: In a world of global trading, maritime safety, security and efficiency are\ncrucial issues. We propose a multi-task deep learning framework for vessel\nmonitoring using Automatic Identification System (AIS) data streams. We combine\nrecurrent neural networks with latent variable modeling and an embedding of AIS\nmessages to a new representation space to jointly address key issues to be\ndealt with when considering AIS data streams: massive amount of streaming data,\nnoisy data and irregular timesampling. We demonstrate the relevance of the\nproposed deep learning framework on real AIS datasets for a three-task setting,\nnamely trajectory reconstruction, anomaly detection and vessel type\nidentification. \n\n"}
{"id": "1806.04166", "contents": "Title: Learning to Decompose and Disentangle Representations for Video\n  Prediction Abstract: Our goal is to predict future video frames given a sequence of input frames.\nDespite large amounts of video data, this remains a challenging task because of\nthe high-dimensionality of video frames. We address this challenge by proposing\nthe Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework\nthat combines structured probabilistic models and deep networks to\nautomatically (i) decompose the high-dimensional video that we aim to predict\ninto components, and (ii) disentangle each component to have low-dimensional\ntemporal dynamics that are easier to predict. Crucially, with an appropriately\nspecified generative model of video frames, our DDPAE is able to learn both the\nlatent decomposition and disentanglement without explicit supervision. For the\nMoving MNIST dataset, we show that DDPAE is able to recover the underlying\ncomponents (individual digits) and disentanglement (appearance and location) as\nwe would intuitively do. We further demonstrate that DDPAE can be applied to\nthe Bouncing Balls dataset involving complex interactions between multiple\nobjects to predict the video frame directly from the pixels and recover\nphysical states without explicit supervision. \n\n"}
{"id": "1806.04342", "contents": "Title: Focused Hierarchical RNNs for Conditional Sequence Processing Abstract: Recurrent Neural Networks (RNNs) with attention mechanisms have obtained\nstate-of-the-art results for many sequence processing tasks. Most of these\nmodels use a simple form of encoder with attention that looks over the entire\nsequence and assigns a weight to each token independently. We present a\nmechanism for focusing RNN encoders for sequence modelling tasks which allows\nthem to attend to key parts of the input as needed. We formulate this using a\nmulti-layer conditional sequence encoder that reads in one token at a time and\nmakes a discrete decision on whether the token is relevant to the context or\nquestion being asked. The discrete gating mechanism takes in the context\nembedding and the current hidden state as inputs and controls information flow\ninto the layer above. We train it using policy gradient methods. We evaluate\nthis method on several types of tasks with different attributes. First, we\nevaluate the method on synthetic tasks which allow us to evaluate the model for\nits generalization ability and probe the behavior of the gates in more\ncontrolled settings. We then evaluate this approach on large scale Question\nAnswering tasks including the challenging MS MARCO and SearchQA tasks. Our\nmodels shows consistent improvements for both tasks over prior work and our\nbaselines. It has also shown to generalize significantly better on synthetic\ntasks as compared to the baselines. \n\n"}
{"id": "1806.04542", "contents": "Title: Approximate inference with Wasserstein gradient flows Abstract: We present a novel approximate inference method for diffusion processes,\nbased on the Wasserstein gradient flow formulation of the diffusion. In this\nformulation, the time-dependent density of the diffusion is derived as the\nlimit of implicit Euler steps that follow the gradients of a particular free\nenergy functional. Existing methods for computing Wasserstein gradient flows\nrely on discretization of the domain of the diffusion, prohibiting their\napplication to domains in more than several dimensions. We propose instead a\ndiscretization-free inference method that computes the Wasserstein gradient\nflow directly in a space of continuous functions. We characterize approximation\nproperties of the proposed method and evaluate it on a nonlinear filtering\ntask, finding performance comparable to the state-of-the-art for filtering\ndiffusions. \n\n"}
{"id": "1806.04965", "contents": "Title: The streaming rollout of deep networks - towards fully model-parallel\n  execution Abstract: Deep neural networks, and in particular recurrent networks, are promising\ncandidates to control autonomous agents that interact in real-time with the\nphysical world. However, this requires a seamless integration of temporal\nfeatures into the network's architecture. For the training of and inference\nwith recurrent neural networks, they are usually rolled out over time, and\ndifferent rollouts exist. Conventionally during inference, the layers of a\nnetwork are computed in a sequential manner resulting in sparse temporal\nintegration of information and long response times. In this study, we present a\ntheoretical framework to describe rollouts, the level of model-parallelization\nthey induce, and demonstrate differences in solving specific tasks. We prove\nthat certain rollouts, also for networks with only skip and no recurrent\nconnections, enable earlier and more frequent responses, and show empirically\nthat these early responses have better performance. The streaming rollout\nmaximizes these properties and enables a fully parallel execution of the\nnetwork reducing runtime on massively parallel devices. Finally, we provide an\nopen-source toolbox to design, train, evaluate, and interact with streaming\nrollouts. \n\n"}
{"id": "1806.05393", "contents": "Title: Dynamical Isometry and a Mean Field Theory of CNNs: How to Train\n  10,000-Layer Vanilla Convolutional Neural Networks Abstract: In recent years, state-of-the-art methods in computer vision have utilized\nincreasingly deep convolutional neural network architectures (CNNs), with some\nof the most successful models employing hundreds or even thousands of layers. A\nvariety of pathologies such as vanishing/exploding gradients make training such\ndeep networks challenging. While residual connections and batch normalization\ndo enable training at these depths, it has remained unclear whether such\nspecialized architecture designs are truly necessary to train deep CNNs. In\nthis work, we demonstrate that it is possible to train vanilla CNNs with ten\nthousand layers or more simply by using an appropriate initialization scheme.\nWe derive this initialization scheme theoretically by developing a mean field\ntheory for signal propagation and by characterizing the conditions for\ndynamical isometry, the equilibration of singular values of the input-output\nJacobian matrix. These conditions require that the convolution operator be an\northogonal transformation in the sense that it is norm-preserving. We present\nan algorithm for generating such random initial orthogonal convolution kernels\nand demonstrate empirically that they enable efficient training of extremely\ndeep architectures. \n\n"}
{"id": "1806.05421", "contents": "Title: Selfless Sequential Learning Abstract: Sequential learning, also called lifelong learning, studies the problem of\nlearning tasks in a sequence with access restricted to only the data of the\ncurrent task. In this paper we look at a scenario with fixed model capacity,\nand postulate that the learning process should not be selfish, i.e. it should\naccount for future tasks to be added and thus leave enough capacity for them.\nTo achieve Selfless Sequential Learning we study different regularization\nstrategies and activation functions. We find that imposing sparsity at the\nlevel of the representation (i.e.~neuron activations) is more beneficial for\nsequential learning than encouraging parameter sparsity. In particular, we\npropose a novel regularizer, that encourages representation sparsity by means\nof neural inhibition. It results in few active neurons which in turn leaves\nmore free neurons to be utilized by upcoming tasks. As neural inhibition over\nan entire layer can be too drastic, especially for complex tasks requiring\nstrong representations, our regularizer only inhibits other neurons in a local\nneighbourhood, inspired by lateral inhibition processes in the brain. We\ncombine our novel regularizer, with state-of-the-art lifelong learning methods\nthat penalize changes to important previously learned parts of the network. We\nshow that our new regularizer leads to increased sparsity which translates in\nconsistent performance improvement %over alternative regularizers we studied on\ndiverse datasets. \n\n"}
{"id": "1806.05502", "contents": "Title: Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes Abstract: Visually predicting the stability of block towers is a popular task in the\ndomain of intuitive physics. While previous work focusses on prediction\naccuracy, a one-dimensional performance measure, we provide a broader analysis\nof the learned physical understanding of the final model and how the learning\nprocess can be guided. To this end, we introduce neural stethoscopes as a\ngeneral purpose framework for quantifying the degree of importance of specific\nfactors of influence in deep neural networks as well as for actively promoting\nand suppressing information as appropriate. In doing so, we unify concepts from\nmultitask learning as well as training with auxiliary and adversarial losses.\nWe apply neural stethoscopes to analyse the state-of-the-art neural network for\nstability prediction. We show that the baseline model is susceptible to being\nmisled by incorrect visual cues. This leads to a performance breakdown to the\nlevel of random guessing when training on scenarios where visual cues are\ninversely correlated with stability. Using stethoscopes to promote meaningful\nfeature extraction increases performance from 51% to 90% prediction accuracy.\nConversely, training on an easy dataset where visual cues are positively\ncorrelated with stability, the baseline model learns a bias leading to poor\nperformance on a harder dataset. Using an adversarial stethoscope, the network\nis successfully de-biased, leading to a performance increase from 66% to 88%. \n\n"}
{"id": "1806.05759", "contents": "Title: Insights on representational similarity in neural networks with\n  canonical correlation Abstract: Comparing different neural network representations and determining how\nrepresentations evolve over time remain challenging open questions in our\nunderstanding of the function of neural networks. Comparing representations in\nneural networks is fundamentally difficult as the structure of representations\nvaries greatly, even across groups of networks trained on identical tasks, and\nover the course of training. Here, we develop projection weighted CCA\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\nfirst improve the core method, showing how to differentiate between signal and\nnoise, and then apply this technique to compare across a group of CNNs,\ndemonstrating that networks which generalize converge to more similar\nrepresentations than networks which memorize, that wider networks converge to\nmore similar solutions than narrow networks, and that trained networks with\nidentical topology but different learning rates converge to distinct clusters\nwith diverse representations. We also investigate the representational dynamics\nof RNNs, across both training and sequential timesteps, finding that RNNs\nconverge in a bottom-up pattern over the course of training and that the hidden\nstate is highly variable over the course of a sequence, even when accounting\nfor linear transforms. Together, these results provide new insights into the\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\nunderstand representations. \n\n"}
{"id": "1806.05975", "contents": "Title: Structured Variational Learning of Bayesian Neural Networks with\n  Horseshoe Priors Abstract: Bayesian Neural Networks (BNNs) have recently received increasing attention\nfor their ability to provide well-calibrated posterior uncertainties. However,\nmodel selection---even choosing the number of nodes---remains an open question.\nRecent work has proposed the use of a horseshoe prior over node pre-activations\nof a Bayesian neural network, which effectively turns off nodes that do not\nhelp explain the data. In this work, we propose several modeling and inference\nadvances that consistently improve the compactness of the model learned while\nmaintaining predictive performance, especially in smaller-sample settings\nincluding reinforcement learning. \n\n"}
{"id": "1806.06003", "contents": "Title: On Machine Learning and Structure for Mobile Robots Abstract: Due to recent advances - compute, data, models - the role of learning in\nautonomous systems has expanded significantly, rendering new applications\npossible for the first time. While some of the most significant benefits are\nobtained in the perception modules of the software stack, other aspects\ncontinue to rely on known manual procedures based on prior knowledge on\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\nthese modules when data collection and curation become easier than manual rule\ndesign. Building on this coarse and broad survey of current research, the final\nsections aim to provide insights into future potentials and challenges as well\nas the necessity of structure in current practical applications. \n\n"}
{"id": "1806.06176", "contents": "Title: Learning Factorized Multimodal Representations Abstract: Learning multimodal representations is a fundamentally complex research\nproblem due to the presence of multiple heterogeneous sources of information.\nAlthough the presence of multiple modalities provides additional valuable\ninformation, there are two key challenges to address when learning from\nmultimodal data: 1) models must learn the complex intra-modal and cross-modal\ninteractions for prediction and 2) models must be robust to unexpected missing\nor noisy modalities during testing. In this paper, we propose to optimize for a\njoint generative-discriminative objective across multimodal data and labels. We\nintroduce a model that factorizes representations into two sets of independent\nfactors: multimodal discriminative and modality-specific generative factors.\nMultimodal discriminative factors are shared across all modalities and contain\njoint multimodal features required for discriminative tasks such as sentiment\nprediction. Modality-specific generative factors are unique for each modality\nand contain the information required for generating data. Experimental results\nshow that our model is able to learn meaningful multimodal representations that\nachieve state-of-the-art or competitive performance on six multimodal datasets.\nOur model demonstrates flexible generative capabilities by conditioning on\nindependent factors and can reconstruct missing modalities without\nsignificantly impacting performance. Lastly, we interpret our factorized\nrepresentations to understand the interactions that influence multimodal\nlearning. \n\n"}
{"id": "1806.06908", "contents": "Title: Designing Optimal Binary Rating Systems Abstract: Modern online platforms rely on effective rating systems to learn about\nitems. We consider the optimal design of rating systems that collect binary\nfeedback after transactions. We make three contributions. First, we formalize\nthe performance of a rating system as the speed with which it recovers the true\nunderlying ranking on items (in a large deviations sense), accounting for both\nitems' underlying match rates and the platform's preferences. Second, we\nprovide an efficient algorithm to compute the binary feedback system that\nyields the highest such performance. Finally, we show how this theoretical\nperspective can be used to empirically design an implementable, approximately\noptimal rating system, and validate our approach using real-world experimental\ndata collected on Amazon Mechanical Turk. \n\n"}
{"id": "1806.07247", "contents": "Title: Tensor-Tensor Product Toolbox Abstract: The tensor-tensor product (t-product) [M. E. Kilmer and C. D. Martin, 2011]\nis a natural generalization of matrix multiplication. Based on t-product, many\noperations on matrix can be extended to tensor cases, including tensor SVD,\ntensor spectral norm, tensor nuclear norm [C. Lu, et al., 2018] and many\nothers. The linear algebraic structure of tensors are similar to the matrix\ncases. We develop a Matlab toolbox to implement several basic operations on\ntensors based on t-product. The toolbox is available at\nhttps://github.com/canyilu/tproduct. \n\n"}
{"id": "1806.07385", "contents": "Title: Detecting and interpreting myocardial infarction using fully\n  convolutional neural networks Abstract: Objective: We aim to provide an algorithm for the detection of myocardial\ninfarction that operates directly on ECG data without any preprocessing and to\ninvestigate its decision criteria. Approach: We train an ensemble of fully\nconvolutional neural networks on the PTB ECG dataset and apply state-of-the-art\nattribution methods. Main results: Our classifier reaches 93.3% sensitivity and\n89.7% specificity evaluated using 10-fold cross-validation with sampling based\non patients. The presented method outperforms state-of-the-art approaches and\nreaches the performance level of human cardiologists for detection of\nmyocardial infarction. We are able to discriminate channel-specific regions\nthat contribute most significantly to the neural network's decision.\nInterestingly, the network's decision is influenced by signs also recognized by\nhuman cardiologists as indicative of myocardial infarction. Significance: Our\nresults demonstrate the high prospects of algorithmic ECG analysis for future\nclinical applications considering both its quantitative performance as well as\nthe possibility of assessing decision criteria on a per-example basis, which\nenhances the comprehensibility of the approach. \n\n"}
{"id": "1806.07498", "contents": "Title: Defining Locality for Surrogates in Post-hoc Interpretablity Abstract: Local surrogate models, to approximate the local decision boundary of a\nblack-box classifier, constitute one approach to generate explanations for the\nrationale behind an individual prediction made by the back-box. This paper\nhighlights the importance of defining the right locality, the neighborhood on\nwhich a local surrogate is trained, in order to approximate accurately the\nlocal black-box decision boundary. Unfortunately, as shown in this paper, this\nissue is not only a parameter or sampling distribution challenge and has a\nmajor impact on the relevance and quality of the approximation of the local\nblack-box decision boundary and thus on the meaning and accuracy of the\ngenerated explanation. To overcome the identified problems, quantified with an\nadapted measure and procedure, we propose to generate surrogate-based\nexplanations for individual predictions based on a sampling centered on\nparticular place of the decision boundary, relevant for the prediction to be\nexplained, rather than on the prediction itself as it is classically done. We\nevaluate the novel approach compared to state-of-the-art methods and a\nstraightforward improvement thereof on four UCI datasets. \n\n"}
{"id": "1806.07562", "contents": "Title: Efficient inference in stochastic block models with vertex labels Abstract: We study the stochastic block model with two communities where vertices\ncontain side information in the form of a vertex label. These vertex labels may\nhave arbitrary label distributions, depending on the community memberships. We\nanalyze a linearized version of the popular belief propagation algorithm. We\nshow that this algorithm achieves the highest accuracy possible whenever a\ncertain function of the network parameters has a unique fixed point. Whenever\nthis function has multiple fixed points, the belief propagation algorithm may\nnot perform optimally. We show that increasing the information in the vertex\nlabels may reduce the number of fixed points and hence lead to optimality of\nbelief propagation. \n\n"}
{"id": "1806.07568", "contents": "Title: Doubly Nested Network for Resource-Efficient Inference Abstract: We propose doubly nested network(DNNet) where all neurons represent their own\nsub-models that solve the same task. Every sub-model is nested both layer-wise\nand channel-wise. While nesting sub-models layer-wise is straight-forward with\ndeep-supervision as proposed in \\cite{xie2015holistically}, channel-wise\nnesting has not been explored in the literature to our best knowledge.\nChannel-wise nesting is non-trivial as neurons between consecutive layers are\nall connected to each other. In this work, we introduce a technique to solve\nthis problem by sorting channels topologically and connecting neurons\naccordingly. For the purpose, channel-causal convolutions are used. Slicing\ndoubly nested network gives a working sub-network. The most notable application\nof our proposed network structure with slicing operation is resource-efficient\ninference. At test time, computing resources such as time and memory available\nfor running the prediction algorithm can significantly vary across devices and\napplications. Given a budget constraint, we can slice the network accordingly\nand use a sub-model for inference within budget, requiring no additional\ncomputation such as training or fine-tuning after deployment. We demonstrate\nthe effectiveness of our approach in several practical scenarios of utilizing\navailable resource efficiently. \n\n"}
{"id": "1806.07569", "contents": "Title: A Distributed Second-Order Algorithm You Can Trust Abstract: Due to the rapid growth of data and computational resources, distributed\noptimization has become an active research area in recent years. While\nfirst-order methods seem to dominate the field, second-order methods are\nnevertheless attractive as they potentially require fewer communication rounds\nto converge. However, there are significant drawbacks that impede their wide\nadoption, such as the computation and the communication of a large Hessian\nmatrix. In this paper we present a new algorithm for distributed training of\ngeneralized linear models that only requires the computation of diagonal blocks\nof the Hessian matrix on the individual workers. To deal with this approximate\ninformation we propose an adaptive approach that - akin to trust-region methods\n- dynamically adapts the auxiliary model to compensate for modeling errors. We\nprovide theoretical rates of convergence for a wide class of problems including\nL1-regularized objectives. We also demonstrate that our approach achieves\nstate-of-the-art results on multiple large benchmark datasets. \n\n"}
{"id": "1806.07772", "contents": "Title: Accurate and Diverse Sampling of Sequences based on a \"Best of Many\"\n  Sample Objective Abstract: For autonomous agents to successfully operate in the real world, anticipation\nof future events and states of their environment is a key competence. This\nproblem has been formalized as a sequence extrapolation problem, where a number\nof observations are used to predict the sequence into the future. Real-world\nscenarios demand a model of uncertainty of such predictions, as predictions\nbecome increasingly uncertain -- in particular on long time horizons. While\nimpressive results have been shown on point estimates, scenarios that induce\nmulti-modal distributions over future sequences remain challenging. Our work\naddresses these challenges in a Gaussian Latent Variable model for sequence\nprediction. Our core contribution is a \"Best of Many\" sample objective that\nleads to more accurate and more diverse predictions that better capture the\ntrue variations in real-world sequence data. Beyond our analysis of improved\nmodel fit, our models also empirically outperform prior work on three diverse\ntasks ranging from traffic scenes to weather data. \n\n"}
{"id": "1806.08018", "contents": "Title: Quantum process tomography of a high-dimensional quantum communication\n  channel Abstract: The characterization of quantum processes, e.g. communication channels, is an\nessential ingredient for establishing quantum information systems. For quantum\nkey distribution protocols, the amount of overall noise in the channel\ndetermines the rate at which secret bits are distributed between authorized\npartners. In particular, tomographic protocols allow for the full\nreconstruction, and thus characterization, of the channel. Here, we perform\nquantum process tomography of high-dimensional quantum communication channels\nwith dimensions ranging from 2 to 5. We can thus explicitly demonstrate the\neffect of an eavesdropper performing an optimal cloning attack or an\nintercept-resend attack during a quantum cryptographic protocol. Moreover, our\nstudy shows that quantum process tomography enables a more detailed\nunderstanding of the channel conditions compared to a coarse-grained measure,\nsuch as quantum bit error rates. This full characterization technique allows us\nto optimize the performance of quantum key distribution under asymmetric\nexperimental conditions, which is particularly useful when considering\nhigh-dimensional encoding schemes. \n\n"}
{"id": "1806.08235", "contents": "Title: Semi-supervised Seizure Prediction with Generative Adversarial Networks Abstract: In this article, we propose an approach that can make use of not only labeled\nEEG signals but also the unlabeled ones which is more accessible. We also\nsuggest the use of data fusion to further improve the seizure prediction\naccuracy. Data fusion in our vision includes EEG signals, cardiogram signals,\nbody temperature and time. We use the short-time Fourier transform on 28-s EEG\nwindows as a pre-processing step. A generative adversarial network (GAN) is\ntrained in an unsupervised manner where information of seizure onset is\ndisregarded. The trained Discriminator of the GAN is then used as feature\nextractor. Features generated by the feature extractor are classified by two\nfully-connected layers (can be replaced by any classifier) for the labeled EEG\nsignals. This semi-supervised seizure prediction method achieves area under the\noperating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp\nEEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively.\nUnsupervised training without the need of labeling is important because not\nonly it can be performed in real-time during EEG signal recording, but also it\ndoes not require feature engineering effort for each patient. \n\n"}
{"id": "1806.08342", "contents": "Title: Quantizing deep convolutional networks for efficient inference: A\n  whitepaper Abstract: We present an overview of techniques for quantizing convolutional neural\nnetworks for inference with integer weights and activations. Per-channel\nquantization of weights and per-layer quantization of activations to 8-bits of\nprecision post-training produces classification accuracies within 2% of\nfloating point networks for a wide variety of CNN architectures. Model sizes\ncan be reduced by a factor of 4 by quantizing weights to 8-bits, even when\n8-bit arithmetic is not supported. This can be achieved with simple, post\ntraining quantization of weights.We benchmark latencies of quantized networks\non CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations\ncompared to floating point on CPUs. Speedups of up to 10x are observed on\nspecialized processors with fixed point SIMD capabilities, like the Qualcomm\nQDSPs with HVX.\n  Quantization-aware training can provide further improvements, reducing the\ngap to floating point to 1% at 8-bit precision. Quantization-aware training\nalso allows for reducing the precision of weights to four bits with accuracy\nlosses ranging from 2% to 10%, with higher accuracy drop for smaller\nnetworks.We introduce tools in TensorFlow and TensorFlowLite for quantizing\nconvolutional networks and review best practices for quantization-aware\ntraining to obtain high accuracy with quantized weights and activations. We\nrecommend that per-channel quantization of weights and per-layer quantization\nof activations be the preferred quantization scheme for hardware acceleration\nand kernel optimization. We also propose that future processors and hardware\naccelerators for optimized inference support precisions of 4, 8 and 16 bits. \n\n"}
{"id": "1806.08730", "contents": "Title: The Natural Language Decathlon: Multitask Learning as Question Answering Abstract: Deep learning has improved performance on many natural language processing\n(NLP) tasks individually. However, general NLP models cannot emerge within a\nparadigm that focuses on the particularities of a single metric, dataset, and\ntask. We introduce the Natural Language Decathlon (decaNLP), a challenge that\nspans ten tasks: question answering, machine translation, summarization,\nnatural language inference, sentiment analysis, semantic role labeling,\nzero-shot relation extraction, goal-oriented dialogue, semantic parsing, and\ncommonsense pronoun resolution. We cast all tasks as question answering over a\ncontext. Furthermore, we present a new Multitask Question Answering Network\n(MQAN) jointly learns all tasks in decaNLP without any task-specific modules or\nparameters in the multitask setting. MQAN shows improvements in transfer\nlearning for machine translation and named entity recognition, domain\nadaptation for sentiment analysis and natural language inference, and zero-shot\ncapabilities for text classification. We demonstrate that the MQAN's\nmulti-pointer-generator decoder is key to this success and performance further\nimproves with an anti-curriculum training strategy. Though designed for\ndecaNLP, MQAN also achieves state of the art results on the WikiSQL semantic\nparsing task in the single-task setting. We also release code for procuring and\nprocessing data, training and evaluating models, and reproducing all\nexperiments for decaNLP. \n\n"}
{"id": "1806.09228", "contents": "Title: Deep $k$-Means: Re-Training and Parameter Sharing with Harder Cluster\n  Assignments for Compressing Deep Convolutions Abstract: The current trend of pushing CNNs deeper with convolutions has created a\npressing demand to achieve higher compression gains on CNNs where convolutions\ndominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide\nResNet). Further, the high energy consumption of convolutions limits its\ndeployment on mobile devices. To this end, we proposed a simple yet effective\nscheme for compressing convolutions though applying k-means clustering on the\nweights, compression is achieved through weight-sharing, by only recording $K$\ncluster centers and weight assignment indexes. We then introduced a novel\nspectrally relaxed $k$-means regularization, which tends to make hard\nassignments of convolutional layer weights to $K$ learned cluster centers\nduring re-training. We additionally propose an improved set of metrics to\nestimate energy consumption of CNN hardware implementations, whose estimation\nresults are verified to be consistent with previously proposed energy\nestimation tool extrapolated from actual hardware measurements. We finally\nevaluated Deep $k$-Means across several CNN models in terms of both compression\nratio and energy consumption reduction, observing promising results without\nincurring accuracy loss. The code is available at\nhttps://github.com/Sandbox3aster/Deep-K-Means \n\n"}
{"id": "1806.09300", "contents": "Title: Improving Chemical Autoencoder Latent Space and Molecular De novo\n  Generation Diversity with Heteroencoders Abstract: Chemical autoencoders are attractive models as they combine chemical space\nnavigation with possibilities for de-novo molecule generation in areas of\ninterest. This enables them to produce focused chemical libraries around a\nsingle lead compound for employment early in a drug discovery project. Here it\nis shown that the choice of chemical representation, such as SMILES strings,\nhas a large influence on the properties of the latent space. It is further\nexplored to what extent translating between different chemical representations\ninfluences the latent space similarity to the SMILES strings or circular\nfingerprints. By employing SMILES enumeration for either the encoder or\ndecoder, it is found that the decoder has the largest influence on the\nproperties of the latent space. Training a sequence to sequence heteroencoder\nbased on recurrent neural networks(RNNs) with long short-term memory cells\n(LSTM) to predict different enumerated SMILES strings from the same canonical\nSMILES string gives the largest similarity between latent space distance and\nmolecular similarity measured as circular fingerprints similarity. Using the\noutput from the bottleneck in QSAR modelling of five molecular datasets shows\nthat heteroencoder derived vectors markedly outperforms autoencoder derived\nvectors as well as models built using ECFP4 fingerprints, underlining the\nincreased chemical relevance of the latent space. However, the use of\nenumeration during training of the decoder leads to a markedly increase in the\nrate of decoding to a different molecules than encoded, a tendency that can be\ncounteracted with more complex network architectures. \n\n"}
{"id": "1806.10206", "contents": "Title: Deep Feature Factorization For Concept Discovery Abstract: We propose Deep Feature Factorization (DFF), a method capable of localizing\nsimilar semantic concepts within an image or a set of images. We use DFF to\ngain insight into a deep convolutional neural network's learned features, where\nwe detect hierarchical cluster structures in feature space. This is visualized\nas heat maps, which highlight semantically matching regions across a set of\nimages, revealing what the network `perceives' as similar. DFF can also be used\nto perform co-segmentation and co-localization, and we report state-of-the-art\nresults on these tasks. \n\n"}
{"id": "1806.10728", "contents": "Title: Deep Echo State Networks with Uncertainty Quantification for\n  Spatio-Temporal Forecasting Abstract: Long-lead forecasting for spatio-temporal systems can often entail complex\nnonlinear dynamics that are difficult to specify it a priori. Current\nstatistical methodologies for modeling these processes are often highly\nparameterized and thus, challenging to implement from a computational\nperspective. One potential parsimonious solution to this problem is a method\nfrom the dynamical systems and engineering literature referred to as an echo\nstate network (ESN). ESN models use so-called {\\it reservoir computing} to\nefficiently compute recurrent neural network (RNN) forecasts. Moreover,\nso-called \"deep\" models have recently been shown to be successful at predicting\nhigh-dimensional complex nonlinear processes, particularly those with multiple\nspatial and temporal scales of variability (such as we often find in\nspatio-temporal environmental data). Here we introduce a deep ensemble ESN\n(D-EESN) model. We present two versions of this model for spatio-temporal\nprocesses that both produce forecasts and associated measures of uncertainty.\nThe first approach utilizes a bootstrap ensemble framework and the second is\ndeveloped within a hierarchical Bayesian framework (BD-EESN). This more general\nhierarchical Bayesian framework naturally accommodates non-Gaussian data types\nand multiple levels of uncertainties. The methodology is first applied to a\ndata set simulated from a novel non-Gaussian multiscale Lorenz-96 dynamical\nsystem simulation model and then to a long-lead United States (U.S.) soil\nmoisture forecasting application. \n\n"}
{"id": "1806.10787", "contents": "Title: How To Extract Fashion Trends From Social Media? A Robust Object\n  Detector With Support For Unsupervised Learning Abstract: With the proliferation of social media, fashion inspired from celebrities,\nreputed designers as well as fashion influencers has shortened the cycle of\nfashion design and manufacturing. However, with the explosion of fashion\nrelated content and large number of user generated fashion photos, it is an\narduous task for fashion designers to wade through social media photos and\ncreate a digest of trending fashion. This necessitates deep parsing of fashion\nphotos on social media to localize and classify multiple fashion items from a\ngiven fashion photo. While object detection competitions such as MSCOCO have\nthousands of samples for each of the object categories, it is quite difficult\nto get large labeled datasets for fast fashion items. Moreover,\nstate-of-the-art object detectors do not have any functionality to ingest large\namount of unlabeled data available on social media in order to fine tune object\ndetectors with labeled datasets. In this work, we show application of a generic\nobject detector, that can be pretrained in an unsupervised manner, on 24\ncategories from recently released Open Images V4 dataset. We first train the\nbase architecture of the object detector using unsupervisd learning on 60K\nunlabeled photos from 24 categories gathered from social media, and then\nsubsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.\nOn 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K\nphotos while performing 11% to 17% better as compared to the state-of-the-art\nobject detectors. We show that this improvement is due to our choice of\narchitecture that lets us do unsupervised learning and that performs\nsignificantly better in identifying small objects. \n\n"}
{"id": "1806.11311", "contents": "Title: Guaranteed Deterministic Bounds on the Total Variation Distance between\n  Univariate Mixtures Abstract: The total variation distance is a core statistical distance between\nprobability measures that satisfies the metric axioms, with value always\nfalling in $[0,1]$. This distance plays a fundamental role in machine learning\nand signal processing: It is a member of the broader class of $f$-divergences,\nand it is related to the probability of error in Bayesian hypothesis testing.\nSince the total variation distance does not admit closed-form expressions for\nstatistical mixtures (like Gaussian mixture models), one often has to rely in\npractice on costly numerical integrations or on fast Monte Carlo approximations\nthat however do not guarantee deterministic lower and upper bounds. In this\nwork, we consider two methods for bounding the total variation of univariate\nmixture models: The first method is based on the information monotonicity\nproperty of the total variation to design guaranteed nested deterministic lower\nbounds. The second method relies on computing the geometric lower and upper\nenvelopes of weighted mixture components to derive deterministic bounds based\non density ratio. We demonstrate the tightness of our bounds in a series of\nexperiments on Gaussian, Gamma and Rayleigh mixture models. \n\n"}
{"id": "1807.00042", "contents": "Title: Neural Networks Trained to Solve Differential Equations Learn General\n  Representations Abstract: We introduce a technique based on the singular vector canonical correlation\nanalysis (SVCCA) for measuring the generality of neural network layers across a\ncontinuously-parametrized set of tasks. We illustrate this method by studying\ngenerality in neural networks trained to solve parametrized boundary value\nproblems based on the Poisson partial differential equation. We find that the\nfirst hidden layer is general, and that deeper layers are successively more\nspecific. Next, we validate our method against an existing technique that\nmeasures layer generality using transfer learning experiments. We find\nexcellent agreement between the two methods, and note that our method is much\nfaster, particularly for continuously-parametrized problems. Finally, we\nvisualize the general representations of the first layers, and interpret them\nas generalized coordinates over the input domain. \n\n"}
{"id": "1807.02188", "contents": "Title: Implicit Generative Modeling of Random Noise during Training for\n  Adversarial Robustness Abstract: We introduce a Noise-based prior Learning (NoL) approach for training neural\nnetworks that are intrinsically robust to adversarial attacks. We find that the\nimplicit generative modeling of random noise with the same loss function used\nduring posterior maximization, improves a model's understanding of the data\nmanifold furthering adversarial robustness. We evaluate our approach's efficacy\nand provide a simplistic visualization tool for understanding adversarial data,\nusing Principal Component Analysis. Our analysis reveals that adversarial\nrobustness, in general, manifests in models with higher variance along the\nhigh-ranked principal components. We show that models learnt with our approach\nperform remarkably well against a wide-range of attacks. Furthermore, combining\nNoL with state-of-the-art adversarial training extends the robustness of a\nmodel, even beyond what it is adversarially trained for, in both white-box and\nblack-box attack scenarios. \n\n"}
{"id": "1807.02515", "contents": "Title: Blockchain as a Service: A Decentralized and Secure Computing Paradigm Abstract: Thanks to the advances in machine learning, data-driven analysis tools have\nbecome valuable solutions for various applications. However, there still remain\nessential challenges to develop effective data-driven methods because of the\nneed to acquire a large amount of data and to have sufficient computing power\nto handle the data. In many instances these challenges are addressed by relying\non a dominant cloud computing vendor, but, although commercial cloud vendors\nprovide valuable platforms for data analytics, they can suffer from a lack of\ntransparency, security, and privacy-perservation. Furthermore, reliance on\ncloud servers prevents applying big data analytics in environments where the\ncomputing power is scattered. To address these challenges, a decentralize,\nsecure, and privacy-preserving computing paradigm is proposed to enable an\nasynchronized cooperative computing process amongst scattered and untrustworthy\ncomputing nodes that may have limited computing power and computing\nintelligence. This paradigm is designed by exploring blockchain, decentralized\nlearning, homomorphic encryption, and software defined networking(SDN)\ntechniques. The performance of the proposed paradigm is evaluated via different\nscenarios in the simulation section. \n\n"}
{"id": "1807.02963", "contents": "Title: Jointly learning relevant subgraph patterns and nonlinear models of\n  their indicators Abstract: Classification and regression in which the inputs are graphs of arbitrary\nsize and shape have been paid attention in various fields such as computational\nchemistry and bioinformatics. Subgraph indicators are often used as the most\nfundamental features, but the number of possible subgraph patterns are\nintractably large due to the combinatorial explosion. We propose a novel\nefficient algorithm to jointly learn relevant subgraph patterns and nonlinear\nmodels of their indicators. Previous methods for such joint learning of\nsubgraph features and models are based on search for single best subgraph\nfeatures with specific pruning and boosting procedures of adding their\nindicators one by one, which result in linear models of subgraph indicators. In\ncontrast, the proposed approach is based on directly learning regression trees\nfor graph inputs using a newly derived bound of the total sum of squares for\ndata partitions by a given subgraph feature, and thus can learn nonlinear\nmodels through standard gradient boosting. An illustrative example we call the\nGraph-XOR problem to consider nonlinearity, numerical experiments with real\ndatasets, and scalability comparisons to naive approaches using explicit\npattern enumeration are also presented. \n\n"}
{"id": "1807.03257", "contents": "Title: Data Efficient Lithography Modeling with Transfer Learning and Active\n  Data Selection Abstract: Lithography simulation is one of the key steps in physical verification,\nenabled by the substantial optical and resist models. A resist model bridges\nthe aerial image simulation to printed patterns. While the effectiveness of\nlearning-based solutions for resist modeling has been demonstrated, they are\nconsiderably data-demanding. Meanwhile, a set of manufactured data for a\nspecific lithography configuration is only valid for the training of one single\nmodel, indicating low data efficiency. Due to the complexity of the\nmanufacturing process, obtaining enough data for acceptable accuracy becomes\nvery expensive in terms of both time and cost, especially during the evolution\nof technology generations when the design space is intensively explored. In\nthis work, we propose a new resist modeling framework for contact layers,\nutilizing existing data from old technology nodes and active selection of data\nin a target technology node, to reduce the amount of data required from the\ntarget lithography configuration. Our framework based on transfer learning and\nactive learning techniques is effective within a competitive range of accuracy,\ni.e., 3-10X reduction on the amount of training data with comparable accuracy\nto the state-of-the-art learning approach. \n\n"}
{"id": "1807.03523", "contents": "Title: DLOPT: Deep Learning Optimization Library Abstract: Deep learning hyper-parameter optimization is a tough task. Finding an\nappropriate network configuration is a key to success, however most of the\ntimes this labor is roughly done. In this work we introduce a novel library to\ntackle this problem, the Deep Learning Optimization Library: DLOPT. We briefly\ndescribe its architecture and present a set of use examples. This is an open\nsource project developed under the GNU GPL v3 license and it is freely\navailable at https://github.com/acamero/dlopt \n\n"}
{"id": "1807.03558", "contents": "Title: Bandits with Side Observations: Bounded vs. Logarithmic Regret Abstract: We consider the classical stochastic multi-armed bandit but where, from time\nto time and roughly with frequency $\\epsilon$, an extra observation is gathered\nby the agent for free. We prove that, no matter how small $\\epsilon$ is the\nagent can ensure a regret uniformly bounded in time.\n  More precisely, we construct an algorithm with a regret smaller than $\\sum_i\n\\frac{\\log(1/\\epsilon)}{\\Delta_i}$, up to multiplicative constant and loglog\nterms. We also prove a matching lower-bound, stating that no reasonable\nalgorithm can outperform this quantity. \n\n"}
{"id": "1807.03697", "contents": "Title: Deep Learning for Audio Transcription on Low-Resource Datasets Abstract: In training a deep learning system to perform audio transcription, two\npractical problems may arise. Firstly, most datasets are weakly labelled,\nhaving only a list of events present in each recording without any temporal\ninformation for training. Secondly, deep neural networks need a very large\namount of labelled training data to achieve good quality performance, yet in\npractice it is difficult to collect enough samples for most classes of\ninterest. In this paper, we propose factorising the final task of audio\ntranscription into multiple intermediate tasks in order to improve the training\nperformance when dealing with this kind of low-resource datasets. We evaluate\nthree data-efficient approaches of training a stacked convolutional and\nrecurrent neural network for the intermediate tasks. Our results show that\ndifferent methods of training have different advantages and disadvantages. \n\n"}
{"id": "1807.03710", "contents": "Title: Recurrent Auto-Encoder Model for Large-Scale Industrial Sensor Signal\n  Analysis Abstract: Recurrent auto-encoder model summarises sequential data through an encoder\nstructure into a fixed-length vector and then reconstructs the original\nsequence through the decoder structure. The summarised vector can be used to\nrepresent time series features. In this paper, we propose relaxing the\ndimensionality of the decoder output so that it performs partial\nreconstruction. The fixed-length vector therefore represents features in the\nselected dimensions only. In addition, we propose using rolling fixed window\napproach to generate training samples from unbounded time series data. The\nchange of time series features over time can be summarised as a smooth\ntrajectory path. The fixed-length vectors are further analysed using additional\nvisualisation and unsupervised clustering techniques. The proposed method can\nbe applied in large-scale industrial processes for sensors signal analysis\npurpose, where clusters of the vector representations can reflect the operating\nstates of the industrial system. \n\n"}
{"id": "1807.03729", "contents": "Title: Optimal mode configuration for multiple phase-matched four-wave mixing\n  processes Abstract: We demonstrate an unseeded, multimode four-wave mixing process in hot\n$^{85}$Rb vapor, using two pump beams of the same frequency that cross at a\nsmall angle. This results in the simultaneous fulfillment of multiple\nphase-matching conditions that reinforce one another to produce four\nintensity-stabilized bright output modes at two different frequencies. Each\ngenerated photon is directly correlated to exactly two others, resulting in the\npreferred four-mode output, in contrast to other multimode four-wave mixing\nexperiments. This provides significant insight into the optimal configuration\nof the output multimode squeezed and entangled states generated in such\nfour-wave mixing systems. We examine the power, temperature and frequency\ndependence of this new output and compare to the conical four-wave mixing\nemission from a single pump beam. The generated beams are spatially separated,\nallowing a natural distribution for potential use in quantum communication and\nsecret-sharing protocols. \n\n"}
{"id": "1807.03819", "contents": "Title: Universal Transformers Abstract: Recurrent neural networks (RNNs) sequentially process data by updating their\nstate with each new data point, and have long been the de facto choice for\nsequence modeling tasks. However, their inherently sequential computation makes\nthem slow to train. Feed-forward and convolutional architectures have recently\nbeen shown to achieve superior results on some sequence modeling tasks such as\nmachine translation, with the added advantage that they concurrently process\nall inputs in the sequence, leading to easy parallelization and faster training\ntimes. Despite these successes, however, popular feed-forward sequence models\nlike the Transformer fail to generalize in many simple tasks that recurrent\nmodels handle with ease, e.g. copying strings or even simple logical inference\nwhen the string or formula lengths exceed those observed at training time. We\npropose the Universal Transformer (UT), a parallel-in-time self-attentive\nrecurrent sequence model which can be cast as a generalization of the\nTransformer model and which addresses these issues. UTs combine the\nparallelizability and global receptive field of feed-forward sequence models\nlike the Transformer with the recurrent inductive bias of RNNs. We also add a\ndynamic per-position halting mechanism and find that it improves accuracy on\nseveral tasks. In contrast to the standard Transformer, under certain\nassumptions, UTs can be shown to be Turing-complete. Our experiments show that\nUTs outperform standard Transformers on a wide range of algorithmic and\nlanguage understanding tasks, including the challenging LAMBADA language\nmodeling task where UTs achieve a new state of the art, and machine translation\nwhere UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De\ndataset. \n\n"}
{"id": "1807.03845", "contents": "Title: Model-based free-breathing cardiac MRI reconstruction using deep learned\n  \\& STORM priors: MoDL-STORM Abstract: We introduce a model-based reconstruction framework with deep learned (DL)\nand smoothness regularization on manifolds (STORM) priors to recover free\nbreathing and ungated (FBU) cardiac MRI from highly undersampled measurements.\nThe DL priors enable us to exploit the local correlations, while the STORM\nprior enables us to make use of the extensive non-local similarities that are\nsubject dependent. We introduce a novel model-based formulation that allows the\nseamless integration of deep learning methods with available prior information,\nwhich current deep learning algorithms are not capable of. The experimental\nresults demonstrate the preliminary potential of this work in accelerating FBU\ncardiac MRI. \n\n"}
{"id": "1807.04511", "contents": "Title: Training Neural Networks Using Features Replay Abstract: Training a neural network using backpropagation algorithm requires passing\nerror gradients sequentially through the network. The backward locking prevents\nus from updating network layers in parallel and fully leveraging the computing\nresources. Recently, there are several works trying to decouple and parallelize\nthe backpropagation algorithm. However, all of them suffer from severe accuracy\nloss or memory explosion when the neural network is deep. To address these\nchallenging issues, we propose a novel parallel-objective formulation for the\nobjective function of the neural network. After that, we introduce features\nreplay algorithm and prove that it is guaranteed to converge to critical points\nfor the non-convex problem under certain conditions. Finally, we apply our\nmethod to training deep convolutional neural networks, and the experimental\nresults show that the proposed method achieves {faster} convergence, {lower}\nmemory consumption, and {better} generalization error than compared methods. \n\n"}
{"id": "1807.04919", "contents": "Title: TequilaGAN: How to easily identify GAN samples Abstract: In this paper we show strategies to easily identify fake samples generated\nwith the Generative Adversarial Network framework. One strategy is based on the\nstatistical analysis and comparison of raw pixel values and features extracted\nfrom them. The other strategy learns formal specifications from the real data\nand shows that fake samples violate the specifications of the real data. We\nshow that fake samples produced with GANs have a universal signature that can\nbe used to identify fake samples. We provide results on MNIST, CIFAR10, music\nand speech data. \n\n"}
{"id": "1807.05306", "contents": "Title: Generative Adversarial Privacy Abstract: We present a data-driven framework called generative adversarial privacy\n(GAP). Inspired by recent advancements in generative adversarial networks\n(GANs), GAP allows the data holder to learn the privatization mechanism\ndirectly from the data. Under GAP, finding the optimal privacy mechanism is\nformulated as a constrained minimax game between a privatizer and an adversary.\nWe show that for appropriately chosen adversarial loss functions, GAP provides\nprivacy guarantees against strong information-theoretic adversaries. We also\nevaluate GAP's performance on the GENKI face database. \n\n"}
{"id": "1807.05960", "contents": "Title: Meta-Learning with Latent Embedding Optimization Abstract: Gradient-based meta-learning techniques are both widely applicable and\nproficient at solving challenging few-shot learning and fast adaptation\nproblems. However, they have practical difficulties when operating on\nhigh-dimensional parameter spaces in extreme low-data regimes. We show that it\nis possible to bypass these limitations by learning a data-dependent latent\ngenerative representation of model parameters, and performing gradient-based\nmeta-learning in this low-dimensional latent space. The resulting approach,\nlatent embedding optimization (LEO), decouples the gradient-based adaptation\nprocedure from the underlying high-dimensional space of model parameters. Our\nevaluation shows that LEO can achieve state-of-the-art performance on the\ncompetitive miniImageNet and tieredImageNet few-shot classification tasks.\nFurther analysis indicates LEO is able to capture uncertainty in the data, and\ncan perform adaptation more effectively by optimizing in latent space. \n\n"}
{"id": "1807.06572", "contents": "Title: Explicating feature contribution using Random Forest proximity distances Abstract: In Random Forests, proximity distances are a metric representation of data\ninto decision space. By observing how changes in input map to the movement of\ninstances in this space we are able to determine the independent contribution\nof each feature to the decision-making process. For binary feature vectors,\nthis process is fully specified. As these changes in input move particular\ninstances nearer to the in-group or out-group, the independent contribution of\neach feature can be uncovered. Using this technique, we are able to calculate\nthe contribution of each feature in determining how black-box decisions were\nmade. This allows explication of the decision-making process, audit of the\nclassifier, and post-hoc analysis of errors in classification. \n\n"}
{"id": "1807.06576", "contents": "Title: Comparison of RNN Encoder-Decoder Models for Anomaly Detection Abstract: In this paper, we compare different types of Recurrent Neural Network (RNN)\nEncoder-Decoders in anomaly detection viewpoint. We focused on finding the\nmodel that can learn the same data more effectively. We compared multiple\nmodels under the same conditions, such as the number of parameters, optimizer,\nand learning rate. However, the difference is whether to predict the future\nsequence or restore the current sequence. We constructed the dataset with\nsimple vectors and used them for the experiment. Finally, we experimentally\nconfirmed that the model performs better when the model restores the current\nsequence, rather than predict the future sequence. \n\n"}
{"id": "1807.07525", "contents": "Title: Emulating malware authors for proactive protection using GANs over a\n  distributed image visualization of dynamic file behavior Abstract: Malware authors have always been at an advantage of being able to\nadversarially test and augment their malicious code, before deploying the\npayload, using anti-malware products at their disposal. The anti-malware\ndevelopers and threat experts, on the other hand, do not have such a privilege\nof tuning anti-malware products against zero-day attacks pro-actively. This\nallows the malware authors to being a step ahead of the anti-malware products,\nfundamentally biasing the cat and mouse game played by the two parties. In this\npaper, we propose a way that would enable machine learning based threat\nprevention models to bridge that gap by being able to tune against a deep\ngenerative adversarial network (GAN), which takes up the role of a malware\nauthor and generates new types of malware. The GAN is trained over a reversible\ndistributed RGB image representation of known malware behaviors, encoding the\nsequence of API call ngrams and the corresponding term frequencies. The\ngenerated images represent synthetic malware that can be decoded back to the\nunderlying API call sequence information. The image representation is not only\ndemonstrated as a general technique of incorporating necessary priors for\nexploiting convolutional neural network architectures for generative or\ndiscriminative modeling, but also as a visualization method for easy manual\nsoftware or malware categorization, by having individual API ngram information\ndistributed across the image space. In addition, we also propose using\nsmart-definitions for detecting malwares based on perceptual hashing of these\nimages. Such hashes are potentially more effective than cryptographic hashes\nthat do not carry any meaningful similarity metric, and hence, do not\ngeneralize well. \n\n"}
{"id": "1807.08091", "contents": "Title: Streaming Methods for Restricted Strongly Convex Functions with\n  Applications to Prototype Selection Abstract: In this paper, we show that if the optimization function is\nrestricted-strongly-convex (RSC) and restricted-smooth (RSM) -- a rich subclass\nof weakly submodular functions -- then a streaming algorithm with constant\nfactor approximation guarantee is possible. More generally, our results are\napplicable to any monotone weakly submodular function with submodularity ratio\nbounded from above. This (positive) result which provides a sufficient\ncondition for having a constant factor streaming guarantee for weakly\nsubmodular functions may be of special interest given the recent negative\nresult (Elenberg et al., 2017) for the general class of weakly submodular\nfunctions. We apply our streaming algorithms for creating compact synopsis of\nlarge complex datasets, by selecting $m$ representative elements, by optimizing\na suitable RSC and RSM objective function. Above results hold even with\nadditional constraints such as learning non-negative weights, for\ninterpretability, for each selected element indicative of its importance. We\nempirically evaluate our algorithms on two real datasets: MNIST- a handwritten\ndigits dataset and Letters- a UCI dataset containing the alphabet written in\ndifferent fonts and styles. We observe that our algorithms are orders of\nmagnitude faster than the state-of-the-art streaming algorithm for weakly\nsubmodular functions and with our main algorithm still providing equally good\nsolutions in practice. \n\n"}
{"id": "1807.08518", "contents": "Title: Implementing Neural Turing Machines Abstract: Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural\nNetworks, a new class of recurrent neural networks which decouple computation\nfrom memory by introducing an external memory unit. NTMs have demonstrated\nsuperior performance over Long Short-Term Memory Cells in several sequence\nlearning tasks. A number of open source implementations of NTMs exist but are\nunstable during training and/or fail to replicate the reported performance of\nNTMs. This paper presents the details of our successful implementation of a\nNTM. Our implementation learns to solve three sequential learning tasks from\nthe original NTM paper. We find that the choice of memory contents\ninitialization scheme is crucial in successfully implementing a NTM. Networks\nwith memory contents initialized to small constant values converge on average 2\ntimes faster than the next best memory contents initialization scheme. \n\n"}
{"id": "1807.09958", "contents": "Title: Rethinking the Form of Latent States in Image Captioning Abstract: RNNs and their variants have been widely adopted for image captioning. In\nRNNs, the production of a caption is driven by a sequence of latent states.\nExisting captioning models usually represent latent states as vectors, taking\nthis practice for granted. We rethink this choice and study an alternative\nformulation, namely using two-dimensional maps to encode latent states. This is\nmotivated by the curiosity about a question: how the spatial structures in the\nlatent states affect the resultant captions? Our study on MSCOCO and Flickr30k\nleads to two significant observations. First, the formulation with 2D states is\ngenerally more effective in captioning, consistently achieving higher\nperformance with comparable parameter sizes. Second, 2D states preserve spatial\nlocality. Taking advantage of this, we visually reveal the internal dynamics in\nthe process of caption generation, as well as the connections between input\nvisual domain and output linguistic domain. \n\n"}
{"id": "1807.10262", "contents": "Title: Seeded Graph Matching via Large Neighborhood Statistics Abstract: We study a well known noisy model of the graph isomorphism problem. In this\nmodel, the goal is to perfectly recover the vertex correspondence between two\nedge-correlated Erd\\H{o}s-R\\'{e}nyi random graphs, with an initial seed set of\ncorrectly matched vertex pairs revealed as side information. For seeded\nproblems, our result provides a significant improvement over previously known\nresults. We show that it is possible to achieve the information-theoretic limit\nof graph sparsity in time polynomial in the number of vertices $n$. Moreover,\nwe show the number of seeds needed for exact recovery in polynomial-time can be\nas low as $n^{3\\epsilon}$ in the sparse graph regime (with the average degree\nsmaller than $n^{\\epsilon}$) and $\\Omega(\\log n)$ in the dense graph regime.\n  Our results also shed light on the unseeded problem. In particular, we give\nsub-exponential time algorithms for sparse models and an $n^{O(\\log n)}$\nalgorithm for dense models for some parameters, including some that are not\ncovered by recent results of Barak et al. \n\n"}
{"id": "1807.11074", "contents": "Title: Visual Analogies between Atari Games for Studying Transfer Learning in\n  RL Abstract: In this work, we ask the following question: Can visual analogies, learned in\nan unsupervised way, be used in order to transfer knowledge between pairs of\ngames and even play one game using an agent trained for another game? We\nattempt to answer this research question by creating visual analogies between a\npair of games: a source game and a target game. For example, given a video\nframe in the target game, we map it to an analogous state in the source game\nand then attempt to play using a trained policy learned for the source game. We\ndemonstrate convincing visual mapping between four pairs of games (eight\nmappings), which are used to evaluate three transfer learning approaches. \n\n"}
{"id": "1807.11158", "contents": "Title: Robust Student Network Learning Abstract: Deep neural networks bring in impressive accuracy in various applications,\nbut the success often relies on the heavy network architecture. Taking\nwell-trained heavy networks as teachers, classical teacher-student learning\nparadigm aims to learn a student network that is lightweight yet accurate. In\nthis way, a portable student network with significantly fewer parameters can\nachieve a considerable accuracy which is comparable to that of teacher network.\nHowever, beyond accuracy, robustness of the learned student network against\nperturbation is also essential for practical uses. Existing teacher-student\nlearning frameworks mainly focus on accuracy and compression ratios, but ignore\nthe robustness. In this paper, we make the student network produce more\nconfident predictions with the help of the teacher network, and analyze the\nlower bound of the perturbation that will destroy the confidence of the student\nnetwork. Two important objectives regarding prediction scores and gradients of\nexamples are developed to maximize this lower bound, so as to enhance the\nrobustness of the student network without sacrificing the performance.\nExperiments on benchmark datasets demonstrate the efficiency of the proposed\napproach to learn robust student networks which have satisfying accuracy and\ncompact sizes. \n\n"}
{"id": "1807.11374", "contents": "Title: Weakly-Supervised Deep Learning of Heat Transport via Physics Informed\n  Loss Abstract: In typical machine learning tasks and applications, it is necessary to obtain\nor create large labeled datasets in order to to achieve high performance.\nUnfortunately, large labeled datasets are not always available and can be\nexpensive to source, creating a bottleneck towards more widely applicable\nmachine learning. The paradigm of weak supervision offers an alternative that\nallows for integration of domain-specific knowledge by enforcing constraints\nthat a correct solution to the learning problem will obey over the output\nspace. In this work, we explore the application of this paradigm to 2-D\nphysical systems governed by non-linear differential equations. We demonstrate\nthat knowledge of the partial differential equations governing a system can be\nencoded into the loss function of a neural network via an appropriately chosen\nconvolutional kernel. We demonstrate this by showing that the steady-state\nsolution to the 2-D heat equation can be learned directly from initial\nconditions by a convolutional neural network, in the absence of labeled\ntraining data. We also extend recent work in the progressive growing of fully\nconvolutional networks to achieve high accuracy (< 1.5% error) at multiple\nscales of the heat-flow problem, including at the very large scale (1024x1024).\nFinally, we demonstrate that this method can be used to speed up exact\ncalculation of the solution to the differential equations via finite\ndifference. \n\n"}
{"id": "1808.00563", "contents": "Title: Data Augmentation for Robust Keyword Spotting under Playback\n  Interference Abstract: Accurate on-device keyword spotting (KWS) with low false accept and false\nreject rate is crucial to customer experience for far-field voice control of\nconversational agents. It is particularly challenging to maintain low false\nreject rate in real world conditions where there is (a) ambient noise from\nexternal sources such as TV, household appliances, or other speech that is not\ndirected at the device (b) imperfect cancellation of the audio playback from\nthe device, resulting in residual echo, after being processed by the Acoustic\nEcho Cancellation (AEC) system. In this paper, we propose a data augmentation\nstrategy to improve keyword spotting performance under these challenging\nconditions. The training set audio is artificially corrupted by mixing in music\nand TV/movie audio, at different signal to interference ratios. Our results\nshow that we get around 30-45% relative reduction in false reject rates, at a\nrange of false alarm rates, under audio playback from such devices. \n\n"}
{"id": "1808.00831", "contents": "Title: Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes Abstract: We present an approximate Bayesian inference approach for estimating the\nintensity of an inhomogeneous Poisson process, where the intensity function is\nmodelled using a Gaussian process (GP) prior via a sigmoid link function.\nAugmenting the model using a latent marked Poisson process and P\\'olya--Gamma\nrandom variables we obtain a representation of the likelihood which is\nconjugate to the GP prior. We estimate the posterior using a variational\nfree--form mean field optimisation together with the framework of sparse GPs.\nFurthermore, as alternative approximation we suggest a sparse Laplace's method\nfor the posterior, for which an efficient expectation--maximisation algorithm\nis derived to find the posterior's mode. Both algorithms compare well against\nexact inference obtained by a Markov Chain Monte Carlo sampler and standard\nvariational Gauss approach solving the same model, while being one order of\nmagnitude faster. Furthermore, the performance and speed of our method is\ncompetitive with that of another recently proposed Poisson process model based\non a quadratic link function, while not being limited to GPs with squared\nexponential kernels and rectangular domains. \n\n"}
{"id": "1808.01517", "contents": "Title: DELIMIT PyTorch - An extension for Deep Learning in Diffusion Imaging Abstract: DELIMIT is a framework extension for deep learning in diffusion imaging,\nwhich extends the basic framework PyTorch towards spherical signals. Based on\nseveral novel layers, deep learning can be applied to spherical diffusion\nimaging data in a very convenient way. First, two spherical harmonic\ninterpolation layers are added to the extension, which allow to transform the\nsignal from spherical surface space into the spherical harmonic space, and vice\nversa. In addition, a local spherical convolution layer is introduced that adds\nthe possibility to include gradient neighborhood information within the\nnetwork. Furthermore, these extensions can also be utilized for the\npreprocessing of diffusion signals. \n\n"}
{"id": "1808.01550", "contents": "Title: Designing Adaptive Neural Networks for Energy-Constrained Image\n  Classification Abstract: As convolutional neural networks (CNNs) enable state-of-the-art computer\nvision applications, their high energy consumption has emerged as a key\nimpediment to their deployment on embedded and mobile devices. Towards\nefficient image classification under hardware constraints, prior work has\nproposed adaptive CNNs, i.e., systems of networks with different accuracy and\ncomputation characteristics, where a selection scheme adaptively selects the\nnetwork to be evaluated for each input image. While previous efforts have\ninvestigated different network selection schemes, we find that they do not\nnecessarily result in energy savings when deployed on mobile systems. The key\nlimitation of existing methods is that they learn only how data should be\nprocessed among the CNNs and not the network architectures, with each network\nbeing treated as a blackbox.\n  To address this limitation, we pursue a more powerful design paradigm where\nthe architecture settings of the CNNs are treated as hyper-parameters to be\nglobally optimized. We cast the design of adaptive CNNs as a hyper-parameter\noptimization problem with respect to energy, accuracy, and communication\nconstraints imposed by the mobile device. To efficiently solve this problem, we\nadapt Bayesian optimization to the properties of the design space, reaching\nnear-optimal configurations in few tens of function evaluations. Our method\nreduces the energy consumed for image classification on a mobile device by up\nto 6x, compared to the best previously published work that uses CNNs as\nblackboxes. Finally, we evaluate two image classification practices, i.e.,\nclassifying all images locally versus over the cloud under energy and\ncommunication constraints. \n\n"}
{"id": "1808.01574", "contents": "Title: Autoencoder Based Sample Selection for Self-Taught Learning Abstract: Self-taught learning is a technique that uses a large number of unlabeled\ndata as source samples to improve the task performance on target samples.\nCompared with other transfer learning techniques, self-taught learning can be\napplied to a broader set of scenarios due to the loose restrictions on the\nsource data. However, knowledge transferred from source samples that are not\nsufficiently related to the target domain may negatively influence the target\nlearner, which is referred to as negative transfer. In this paper, we propose a\nmetric for the relevance between a source sample and the target samples. To be\nmore specific, both source and target samples are reconstructed through a\nsingle-layer autoencoder with a linear relationship between source samples and\nreconstructed target samples being simultaneously enforced. An\n$\\ell_{2,1}$-norm sparsity constraint is imposed on the transformation matrix\nto identify source samples relevant to the target domain. Source domain samples\nthat are deemed relevant are assigned pseudo-labels reflecting their relevance\nto target domain samples, and are combined with target samples in order to\nprovide an expanded training set for classifier training. Local data structures\nare also preserved during source sample selection through spectral graph\nanalysis. Promising results in extensive experiments show the advantages of the\nproposed approach. \n\n"}
{"id": "1808.01975", "contents": "Title: A Survey on Surrogate Approaches to Non-negative Matrix Factorization Abstract: Motivated by applications in hyperspectral imaging we investigate methods for\napproximating a high-dimensional non-negative matrix $\\mathbf{\\mathit{Y}}$ by a\nproduct of two lower-dimensional, non-negative matrices $\\mathbf{\\mathit{K}}$\nand $\\mathbf{\\mathit{X}}.$ This so-called non-negative matrix factorization is\nbased on defining suitable Tikhonov functionals, which combine a discrepancy\nmeasure for $\\mathbf{\\mathit{Y}}\\approx\\mathbf{\\mathit{KX}}$ with penalty terms\nfor enforcing additional properties of $\\mathbf{\\mathit{K}}$ and\n$\\mathbf{\\mathit{X}}$. The minimization is based on alternating minimization\nwith respect to $\\mathbf{\\mathit{K}}$ or $\\mathbf{\\mathit{X}}$, where in each\niteration step one replaces the original Tikhonov functional by a locally\ndefined surrogate functional. The choice of surrogate functionals is crucial:\nIt should allow a comparatively simple minimization and simultaneously its\nfirst order optimality condition should lead to multiplicative update rules,\nwhich automatically preserve non-negativity of the iterates. We review the most\nstandard construction principles for surrogate functionals for Frobenius-norm\nand Kullback-Leibler discrepancy measures. We extend the known surrogate\nconstructions by a general framework, which allows to add a large variety of\npenalty terms. The paper finishes by deriving the corresponding alternating\nminimization schemes explicitely and by applying these methods to MALDI imaging\ndata. \n\n"}
{"id": "1808.01990", "contents": "Title: Hashing with Binary Matrix Pursuit Abstract: We propose theoretical and empirical improvements for two-stage hashing\nmethods. We first provide a theoretical analysis on the quality of the binary\ncodes and show that, under mild assumptions, a residual learning scheme can\nconstruct binary codes that fit any neighborhood structure with arbitrary\naccuracy. Secondly, we show that with high-capacity hash functions such as\nCNNs, binary code inference can be greatly simplified for many standard\nneighborhood definitions, yielding smaller optimization problems and more\nrobust codes. Incorporating our findings, we propose a novel two-stage hashing\nmethod that significantly outperforms previous hashing studies on widely used\nimage retrieval benchmarks. \n\n"}
{"id": "1808.03331", "contents": "Title: The Effectiveness of Multitask Learning for Phenotyping with Electronic\n  Health Records Data Abstract: Electronic phenotyping is the task of ascertaining whether an individual has\na medical condition of interest by analyzing their medical record and is\nfoundational in clinical informatics. Increasingly, electronic phenotyping is\nperformed via supervised learning. We investigate the effectiveness of\nmultitask learning for phenotyping using electronic health records (EHR) data.\nMultitask learning aims to improve model performance on a target task by\njointly learning additional auxiliary tasks and has been used in disparate\nareas of machine learning. However, its utility when applied to EHR data has\nnot been established, and prior work suggests that its benefits are\ninconsistent. We present experiments that elucidate when multitask learning\nwith neural nets improves performance for phenotyping using EHR data relative\nto neural nets trained for a single phenotype and to well-tuned logistic\nregression baselines. We find that multitask neural nets consistently\noutperform single-task neural nets for rare phenotypes but underperform for\nrelatively more common phenotypes. The effect size increases as more auxiliary\ntasks are added. Moreover, multitask learning reduces the sensitivity of neural\nnets to hyperparameter settings for rare phenotypes. Last, we quantify\nphenotype complexity and find that neural nets trained with or without\nmultitask learning do not improve on simple baselines unless the phenotypes are\nsufficiently complex. \n\n"}
{"id": "1808.03578", "contents": "Title: Dropout is a special case of the stochastic delta rule: faster and more\n  accurate deep learning Abstract: Multi-layer neural networks have lead to remarkable performance on many kinds\nof benchmark tasks in text, speech and image processing. Nonlinear parameter\nestimation in hierarchical models is known to be subject to overfitting and\nmisspecification. One approach to these estimation and related problems (local\nminima, colinearity, feature discovery etc.) is called Dropout (Hinton, et al\n2012, Baldi et al 2016). The Dropout algorithm removes hidden units according\nto a Bernoulli random variable with probability $p$ prior to each update,\ncreating random \"shocks\" to the network that are averaged over updates. In this\npaper we will show that Dropout is a special case of a more general model\npublished originally in 1990 called the Stochastic Delta Rule, or SDR (Hanson,\n1990). SDR redefines each weight in the network as a random variable with mean\n$\\mu_{w_{ij}}$ and standard deviation $\\sigma_{w_{ij}}$. Each weight random\nvariable is sampled on each forward activation, consequently creating an\nexponential number of potential networks with shared weights. Both parameters\nare updated according to prediction error, thus resulting in weight noise\ninjections that reflect a local history of prediction error and local model\naveraging. SDR therefore implements a more sensitive local gradient-dependent\nsimulated annealing per weight converging in the limit to a Bayes optimal\nnetwork. Tests on standard benchmarks (CIFAR) using a modified version of\nDenseNet shows the SDR outperforms standard Dropout in test error by approx.\n$17\\%$ with DenseNet-BC 250 on CIFAR-100 and approx. $12-14\\%$ in smaller\nnetworks. We also show that SDR reaches the same accuracy that Dropout attains\nin 100 epochs in as few as 35 epochs. \n\n"}
{"id": "1808.04362", "contents": "Title: A Domain Guided CNN Architecture for Predicting Age from Structural\n  Brain Images Abstract: Given the wide success of convolutional neural networks (CNNs) applied to\nnatural images, researchers have begun to apply them to neuroimaging data. To\ndate, however, exploration of novel CNN architectures tailored to neuroimaging\ndata has been limited. Several recent works fail to leverage the 3D structure\nof the brain, instead treating the brain as a set of independent 2D slices.\nApproaches that do utilize 3D convolutions rely on architectures developed for\nobject recognition tasks in natural 2D images. Such architectures make\nassumptions about the input that may not hold for neuroimaging. For example,\nexisting architectures assume that patterns in the brain exhibit translation\ninvariance. However, a pattern in the brain may have different meaning\ndepending on where in the brain it is located. There is a need to explore novel\narchitectures that are tailored to brain images. We present two simple\nmodifications to existing CNN architectures based on brain image structure.\nApplied to the task of brain age prediction, our network achieves a mean\nabsolute error (MAE) of 1.4 years and trains 30% faster than a CNN baseline\nthat achieves a MAE of 1.6 years. Our results suggest that lessons learned from\ndeveloping models on natural images may not directly transfer to neuroimaging\ntasks. Instead, there remains a large space of unexplored questions regarding\nmodel development in this area, whose answers may differ from conventional\nwisdom. \n\n"}
{"id": "1808.04475", "contents": "Title: Kernel Flows: from learning kernels from data into the abyss Abstract: Learning can be seen as approximating an unknown function by interpolating\nthe training data. Kriging offers a solution to this problem based on the prior\nspecification of a kernel. We explore a numerical approximation approach to\nkernel selection/construction based on the simple premise that a kernel must be\ngood if the number of interpolation points can be halved without significant\nloss in accuracy (measured using the intrinsic RKHS norm $\\|\\cdot\\|$ associated\nwith the kernel). We first test and motivate this idea on a simple problem of\nrecovering the Green's function of an elliptic PDE (with inhomogeneous\ncoefficients) from the sparse observation of one of its solutions. Next we\nconsider the problem of learning non-parametric families of deep kernels of the\nform $K_1(F_n(x),F_n(x'))$ with $F_{n+1}=(I_d+\\epsilon G_{n+1})\\circ F_n$ and\n$G_{n+1} \\in \\operatorname{Span}\\{K_1(F_n(x_i),\\cdot)\\}$. With the proposed\napproach constructing the kernel becomes equivalent to integrating a stochastic\ndata driven dynamical system, which allows for the training of very deep\n(bottomless) networks and the exploration of their properties. These networks\nlearn by constructing flow maps in the kernel and input spaces via incremental\ndata-dependent deformations/perturbations (appearing as the cooperative\ncounterpart of adversarial examples) and, at profound depths, they (1) can\nachieve accurate classification from only one data point per class (2) appear\nto learn archetypes of each class (3) expand distances between points that are\nin different classes and contract distances between points in the same class.\nFor kernels parameterized by the weights of Convolutional Neural Networks,\nminimizing approximation errors incurred by halving random subsets of\ninterpolation points, appears to outperform training (the same CNN\narchitecture) with relative entropy and dropout. \n\n"}
{"id": "1808.04538", "contents": "Title: Text-to-Image-to-Text Translation using Cycle Consistent Adversarial\n  Networks Abstract: Text-to-Image translation has been an active area of research in the recent\npast. The ability for a network to learn the meaning of a sentence and generate\nan accurate image that depicts the sentence shows ability of the model to think\nmore like humans. Popular methods on text to image translation make use of\nGenerative Adversarial Networks (GANs) to generate high quality images based on\ntext input, but the generated images don't always reflect the meaning of the\nsentence given to the model as input. We address this issue by using a\ncaptioning network to caption on generated images and exploit the distance\nbetween ground truth captions and generated captions to improve the network\nfurther. We show extensive comparisons between our method and existing methods. \n\n"}
{"id": "1808.04883", "contents": "Title: COLA: Decentralized Linear Learning Abstract: Decentralized machine learning is a promising emerging paradigm in view of\nglobal challenges of data ownership and privacy. We consider learning of linear\nclassification and regression models, in the setting where the training data is\ndecentralized over many user devices, and the learning algorithm must run\non-device, on an arbitrary communication network, without a central\ncoordinator. We propose COLA, a new decentralized training algorithm with\nstrong theoretical guarantees and superior practical performance. Our framework\novercomes many limitations of existing methods, and achieves communication\nefficiency, scalability, elasticity as well as resilience to changes in data\nand participating devices. \n\n"}
{"id": "1808.05355", "contents": "Title: Conceptual Domain Adaptation Using Deep Learning Abstract: Deep learning has recently been shown to be instrumental in the problem of\ndomain adaptation, where the goal is to learn a model on a target domain using\na similar --but not identical-- source domain. The rationale for coupling both\ntechniques is the possibility of extracting common concepts across domains.\nConsidering (strictly) local representations, traditional deep learning assumes\ncommon concepts must be captured in the same hidden units. We contend that\njointly training a model with source and target data using a single deep\nnetwork is prone to failure when there is inherently lower-level\nrepresentational discrepancy between the two domains; such discrepancy leads to\na misalignment of corresponding concepts in separate hidden units. We introduce\na search framework to correctly align high-level representations when training\ndeep networks; such framework leads to the notion of conceptual --as opposed to\nrepresentational-- domain adaptation. \n\n"}
{"id": "1808.05889", "contents": "Title: Data Consistency Approach to Model Validation Abstract: In scientific inference problems, the underlying statistical modeling\nassumptions have a crucial impact on the end results. There exist, however,\nonly a few automatic means for validating these fundamental modelling\nassumptions. The contribution in this paper is a general criterion to evaluate\nthe consistency of a set of statistical models with respect to observed data.\nThis is achieved by automatically gauging the models' ability to generate data\nthat is similar to the observed data. Importantly, the criterion follows from\nthe model class itself and is therefore directly applicable to a broad range of\ninference problems with varying data types, ranging from independent univariate\ndata to high-dimensional time-series. The proposed data consistency criterion\nis illustrated, evaluated and compared to several well-established methods\nusing three synthetic and two real data sets. \n\n"}
{"id": "1808.06148", "contents": "Title: Generalized Bregman and Jensen divergences which include some\n  f-divergences Abstract: In this paper, we introduce new classes of divergences by extending the\ndefinitions of the Bregman divergence and the skew Jensen divergence. These new\ndivergence classes (g-Bregman divergence and skew g-Jensen divergence) satisfy\nsome properties similar to the Bregman or skew Jensen divergence. We show these\ng-divergences include divergences which belong to a class of f-divergence (the\nHellinger distance, the chi-square divergence and the alpha-divergence in\naddition to the Kullback-Leibler divergence). Moreover, we derive an inequality\nbetween the g-Bregman divergence and the skew g-Jensen divergence and show this\ninequality is a generalization of Lin's inequality. \n\n"}
{"id": "1808.06508", "contents": "Title: Life-Long Disentangled Representation Learning with Cross-Domain Latent\n  Homologies Abstract: Intelligent behaviour in the real-world requires the ability to acquire new\nknowledge from an ongoing sequence of experiences while preserving and reusing\npast knowledge. We propose a novel algorithm for unsupervised representation\nlearning from piece-wise stationary visual data: Variational Autoencoder with\nShared Embeddings (VASE). Based on the Minimum Description Length principle,\nVASE automatically detects shifts in the data distribution and allocates spare\nrepresentational capacity to new knowledge, while simultaneously protecting\npreviously learnt representations from catastrophic forgetting. Our approach\nencourages the learnt representations to be disentangled, which imparts a\nnumber of desirable properties: VASE can deal sensibly with ambiguous inputs,\nit can enhance its own representations through imagination-based exploration,\nand most importantly, it exhibits semantically meaningful sharing of latents\nbetween different datasets. Compared to baselines with entangled\nrepresentations, our approach is able to reason beyond surface-level statistics\nand perform semantically meaningful cross-domain inference. \n\n"}
{"id": "1808.06671", "contents": "Title: Adversarial Sampling for Active Learning Abstract: This paper proposes asal, a new GAN based active learning method that\ngenerates high entropy samples. Instead of directly annotating the synthetic\nsamples, ASAL searches similar samples from the pool and includes them for\ntraining. Hence, the quality of new samples is high and annotations are\nreliable. To the best of our knowledge, ASAL is the first GAN based AL method\napplicable to multi-class problems that outperforms random sample selection.\nAnother benefit of ASAL is its small run-time complexity (sub-linear) compared\nto traditional uncertainty sampling (linear). We present a comprehensive set of\nexperiments on multiple traditional data sets and show that ASAL outperforms\nsimilar methods and clearly exceeds the established baseline (random sampling).\nIn the discussion section we analyze in which situations ASAL performs best and\nwhy it is sometimes hard to outperform random sample selection. \n\n"}
{"id": "1808.07105", "contents": "Title: Non-asymptotic bounds for sampling algorithms without log-concavity Abstract: Discrete time analogues of ergodic stochastic differential equations (SDEs)\nare one of the most popular and flexible tools for sampling high-dimensional\nprobability measures. Non-asymptotic analysis in the $L^2$ Wasserstein distance\nof sampling algorithms based on Euler discretisations of SDEs has been recently\ndeveloped by several authors for log-concave probability distributions. In this\nwork we replace the log-concavity assumption with a log-concavity at infinity\ncondition. We provide novel $L^2$ convergence rates for Euler schemes,\nexpressed explicitly in terms of problem parameters. From there we derive\nnon-asymptotic bounds on the distance between the laws induced by Euler schemes\nand the invariant laws of SDEs, both for schemes with standard and with\nrandomised (inaccurate) drifts. We also obtain bounds for the hierarchy of\ndiscretisation, which enables us to deploy a multi-level Monte Carlo estimator.\nOur proof relies on a novel construction of a coupling for the Markov chains\nthat can be used to control both the $L^1$ and $L^2$ Wasserstein distances\nsimultaneously. Finally, we provide a weak convergence analysis that covers\nboth the standard and the randomised (inaccurate) drift case. In particular, we\nreveal that the variance of the randomised drift does not influence the rate of\nweak convergence of the Euler scheme to the SDE. \n\n"}
{"id": "1808.07258", "contents": "Title: Escaping from Collapsing Modes in a Constrained Space Abstract: Generative adversarial networks (GANs) often suffer from unpredictable\nmode-collapsing during training. We study the issue of mode collapse of\nBoundary Equilibrium Generative Adversarial Network (BEGAN), which is one of\nthe state-of-the-art generative models. Despite its potential of generating\nhigh-quality images, we find that BEGAN tends to collapse at some modes after a\nperiod of training. We propose a new model, called \\emph{BEGAN with a\nConstrained Space} (BEGAN-CS), which includes a latent-space constraint in the\nloss function. We show that BEGAN-CS can significantly improve training\nstability and suppress mode collapse without either increasing the model\ncomplexity or degrading the image quality. Further, we visualize the\ndistribution of latent vectors to elucidate the effect of latent-space\nconstraint. The experimental results show that our method has additional\nadvantages of being able to train on small datasets and to generate images\nsimilar to a given real image yet with variations of designated attributes\non-the-fly. \n\n"}
{"id": "1808.07412", "contents": "Title: Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation\n  using Deep Neural Networks Abstract: Predicting the number of clock cycles a processor takes to execute a block of\nassembly instructions in steady state (the throughput) is important for both\ncompiler designers and performance engineers. Building an analytical model to\ndo so is especially complicated in modern x86-64 Complex Instruction Set\nComputer (CISC) machines with sophisticated processor microarchitectures in\nthat it is tedious, error prone, and must be performed from scratch for each\nprocessor generation. In this paper we present Ithemal, the first tool which\nlearns to predict the throughput of a set of instructions. Ithemal uses a\nhierarchical LSTM--based approach to predict throughput based on the opcodes\nand operands of instructions in a basic block. We show that Ithemal is more\naccurate than state-of-the-art hand-written tools currently used in compiler\nbackends and static machine code analyzers. In particular, our model has less\nthan half the error of state-of-the-art analytical models (LLVM's llvm-mca and\nIntel's IACA). Ithemal is also able to predict these throughput values just as\nfast as the aforementioned tools, and is easily ported across a variety of\nprocessor microarchitectures with minimal developer effort. \n\n"}
{"id": "1808.07510", "contents": "Title: XPCA: Extending PCA for a Combination of Discrete and Continuous\n  Variables Abstract: Principal component analysis (PCA) is arguably the most popular tool in\nmultivariate exploratory data analysis. In this paper, we consider the question\nof how to handle heterogeneous variables that include continuous, binary, and\nordinal. In the probabilistic interpretation of low-rank PCA, the data has a\nnormal multivariate distribution and, therefore, normal marginal distributions\nfor each column. If some marginals are continuous but not normal, the\nsemiparametric copula-based principal component analysis (COCA) method is an\nalternative to PCA that combines a Gaussian copula with nonparametric\nmarginals. If some marginals are discrete or semi-continuous, we propose a new\nextended PCA (XPCA) method that also uses a Gaussian copula and nonparametric\nmarginals and accounts for discrete variables in the likelihood calculation by\nintegrating over appropriate intervals. Like PCA, the factors produced by XPCA\ncan be used to find latent structure in data, build predictive models, and\nperform dimensionality reduction. We present the new model, its induced\nlikelihood function, and a fitting algorithm which can be applied in the\npresence of missing data. We demonstrate how to use XPCA to produce an\nestimated full conditional distribution for each data point, and use this to\nproduce to provide estimates for missing data that are automatically range\nrespecting. We compare the methods as applied to simulated and real-world data\nsets that have a mixture of discrete and continuous variables. \n\n"}
{"id": "1808.09744", "contents": "Title: Rule induction for global explanation of trained models Abstract: Understanding the behavior of a trained network and finding explanations for\nits outputs is important for improving the network's performance and\ngeneralization ability, and for ensuring trust in automated systems. Several\napproaches have previously been proposed to identify and visualize the most\nimportant features by analyzing a trained network. However, the relations\nbetween different features and classes are lost in most cases. We propose a\ntechnique to induce sets of if-then-else rules that capture these relations to\nglobally explain the predictions of a network. We first calculate the\nimportance of the features in the trained network. We then weigh the original\ninputs with these feature importance scores, simplify the transformed input\nspace, and finally fit a rule induction model to explain the model predictions.\nWe find that the output rule-sets can explain the predictions of a neural\nnetwork trained for 4-class text classification from the 20 newsgroups dataset\nto a macro-averaged F-score of 0.80. We make the code available at\nhttps://github.com/clips/interpret_with_rules. \n\n"}
{"id": "1808.10073", "contents": "Title: Rational Neural Networks for Approximating Jump Discontinuities of Graph\n  Convolution Operator Abstract: For node level graph encoding, a recent important state-of-art method is the\ngraph convolutional networks (GCN), which nicely integrate local vertex\nfeatures and graph topology in the spectral domain. However, current studies\nsuffer from several drawbacks: (1) graph CNNs relies on Chebyshev polynomial\napproximation which results in oscillatory approximation at jump\ndiscontinuities; (2) Increasing the order of Chebyshev polynomial can reduce\nthe oscillations issue, but also incurs unaffordable computational cost; (3)\nChebyshev polynomials require degree $\\Omega$(poly(1/$\\epsilon$)) to\napproximate a jump signal such as $|x|$, while rational function only needs\n$\\mathcal{O}$(poly log(1/$\\epsilon$))\\cite{liang2016deep,telgarsky2017neural}.\nHowever, it's non-trivial to apply rational approximation without increasing\ncomputational complexity due to the denominator. In this paper, the superiority\nof rational approximation is exploited for graph signal recovering. RatioanlNet\nis proposed to integrate rational function and neural networks. We show that\nrational function of eigenvalues can be rewritten as a function of graph\nLaplacian, which can avoid multiplication by the eigenvector matrix. Focusing\non the analysis of approximation on graph convolution operation, a graph signal\nregression task is formulated. Under graph signal regression task, its time\ncomplexity can be significantly reduced by graph Fourier transform. To overcome\nthe local minimum problem of neural networks model, a relaxed Remez algorithm\nis utilized to initialize the weight parameters. Convergence rate of\nRatioanlNet and polynomial based methods on jump signal is analyzed for a\ntheoretical guarantee. The extensive experimental results demonstrated that our\napproach could effectively characterize the jump discontinuities, outperforming\ncompeting methods by a substantial margin on both synthetic and real-world\ngraphs. \n\n"}
{"id": "1808.10340", "contents": "Title: A Coordinate-Free Construction of Scalable Natural Gradient Abstract: Most neural networks are trained using first-order optimization methods,\nwhich are sensitive to the parameterization of the model. Natural gradient\ndescent is invariant to smooth reparameterizations because it is defined in a\ncoordinate-free way, but tractable approximations are typically defined in\nterms of coordinate systems, and hence may lose the invariance properties. We\nanalyze the invariance properties of the Kronecker-Factored Approximate\nCurvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free\nway. We explicitly construct a Riemannian metric under which the natural\ngradient matches the K-FAC update; invariance to affine transformations of the\nactivations follows immediately. We extend our framework to analyze the\ninvariance properties of K-FAC applied to convolutional networks and recurrent\nneural networks, as well as metrics other than the usual Fisher metric. \n\n"}
{"id": "1808.10393", "contents": "Title: Learning End-to-end Autonomous Driving using Guided Auxiliary\n  Supervision Abstract: Learning to drive faithfully in highly stochastic urban settings remains an\nopen problem. To that end, we propose a Multi-task Learning from Demonstration\n(MT-LfD) framework which uses supervised auxiliary task prediction to guide the\nmain task of predicting the driving commands. Our framework involves an\nend-to-end trainable network for imitating the expert demonstrator's driving\ncommands. The network intermediately predicts visual affordances and action\nprimitives through direct supervision which provide the aforementioned\nauxiliary supervised guidance. We demonstrate that such joint learning and\nsupervised guidance facilitates hierarchical task decomposition, assisting the\nagent to learn faster, achieve better driving performance and increases\ntransparency of the otherwise black-box end-to-end network. We run our\nexperiments to validate the MT-LfD framework in CARLA, an open-source urban\ndriving simulator. We introduce multiple non-player agents in CARLA and induce\ntemporal noise in them for realistic stochasticity. \n\n"}
{"id": "1809.00101", "contents": "Title: Attentive Crowd Flow Machines Abstract: Traffic flow prediction is crucial for urban traffic management and public\nsafety. Its key challenges lie in how to adaptively integrate the various\nfactors that affect the flow changes. In this paper, we propose a unified\nneural network module to address this problem, called Attentive Crowd Flow\nMachine~(ACFM), which is able to infer the evolution of the crowd flow by\nlearning dynamic representations of temporally-varying data with an attention\nmechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units\nconnected with a convolutional layer for spatial weight prediction. The first\nLSTM takes the sequential flow density representation as input and generates a\nhidden state at each time-step for attention map inference, while the second\nLSTM aims at learning the effective spatial-temporal feature expression from\nattentionally weighted crowd flow features. Based on the ACFM, we further build\na deep architecture with the application to citywide crowd flow prediction,\nwhich naturally incorporates the sequential and periodic data as well as other\nexternal influences. Extensive experiments on two standard benchmarks (i.e.,\ncrowd flow in Beijing and New York City) show that the proposed method achieves\nsignificant improvements over the state-of-the-art methods. \n\n"}
{"id": "1809.00594", "contents": "Title: Adversarial Attack Type I: Cheat Classifiers by Significant Changes Abstract: Despite the great success of deep neural networks, the adversarial attack can\ncheat some well-trained classifiers by small permutations. In this paper, we\npropose another type of adversarial attack that can cheat classifiers by\nsignificant changes. For example, we can significantly change a face but\nwell-trained neural networks still recognize the adversarial and the original\nexample as the same person. Statistically, the existing adversarial attack\nincreases Type II error and the proposed one aims at Type I error, which are\nhence named as Type II and Type I adversarial attack, respectively. The two\ntypes of attack are equally important but are essentially different, which are\nintuitively explained and numerically evaluated. To implement the proposed\nattack, a supervised variation autoencoder is designed and then the classifier\nis attacked by updating the latent variables using gradient information.\n{Besides, with pre-trained generative models, Type I attack on latent spaces is\ninvestigated as well.} Experimental results show that our method is practical\nand effective to generate Type I adversarial examples on large-scale image\ndatasets. Most of these generated examples can pass detectors designed for\ndefending Type II attack and the strengthening strategy is only efficient with\na specific type attack, both implying that the underlying reasons for Type I\nand Type II attack are different. \n\n"}
{"id": "1809.00862", "contents": "Title: Handwriting styles: benchmarks and evaluation metrics Abstract: Evaluating the style of handwriting generation is a challenging problem,\nsince it is not well defined. It is a key component in order to develop in\ndeveloping systems with more personalized experiences with humans. In this\npaper, we propose baseline benchmarks, in order to set anchors to estimate the\nrelative quality of different handwriting style methods. This will be done\nusing deep learning techniques, which have shown remarkable results in\ndifferent machine learning tasks, learning classification, regression, and most\nrelevant to our work, generating temporal sequences. We discuss the challenges\nassociated with evaluating our methods, which is related to evaluation of\ngenerative models in general. We then propose evaluation metrics, which we find\nrelevant to this problem, and we discuss how we evaluate the evaluation\nmetrics. In this study, we use IRON-OFF dataset. To the best of our knowledge,\nthere is no work done before in generating handwriting (either in terms of\nmethodology or the performance metrics), our in exploring styles using this\ndataset. \n\n"}
{"id": "1809.01046", "contents": "Title: Group-Representative Functional Network Estimation from Multi-Subject\n  fMRI Data via MRF-based Image Segmentation Abstract: We propose a novel two-phase approach to functional network estimation of\nmulti-subject functional Magnetic Resonance Imaging (fMRI) data, which applies\nmodel-based image segmentation to determine a group-representative connectivity\nmap. In our approach, we first improve clustering-based Independent Component\nAnalysis (ICA) to generate maps of components occurring consistently across\nsubjects, and then estimate the group-representative map through MAP-MRF\n(Maximum a priori - Markov random field) labeling. For the latter, we provide a\nnovel and efficient variational Bayes algorithm. We study the performance of\nthe proposed method using synthesized data following a theoretical model, and\ndemonstrate its viability in blind extraction of group-representative\nfunctional networks using simulated fMRI data. We anticipate the proposed\nmethod will be applied in identifying common neuronal characteristics in a\npopulation, and could be further extended to real-world clinical diagnosis. \n\n"}
{"id": "1809.01129", "contents": "Title: Lipschitz Networks and Distributional Robustness Abstract: Robust risk minimisation has several advantages: it has been studied with\nregards to improving the generalisation properties of models and robustness to\nadversarial perturbation. We bound the distributionally robust risk for a model\nclass rich enough to include deep neural networks by a regularised empirical\nrisk involving the Lipschitz constant of the model. This allows us to\ninterpretand quantify the robustness properties of a deep neural network. As an\napplication we show the distributionally robust risk upperbounds the\nadversarial training risk. \n\n"}
{"id": "1809.02157", "contents": "Title: Scalable Learning in Reproducing Kernel Krein Spaces Abstract: We provide the first mathematically complete derivation of the Nystr\\\"om\nmethod for low-rank approximation of indefinite kernels and propose an\nefficient method for finding an approximate eigendecomposition of such kernel\nmatrices. Building on this result, we devise highly scalable methods for\nlearning in reproducing kernel Kre\\u{\\i}n spaces. The devised approaches\nprovide a principled and theoretically well-founded means to tackle large scale\nlearning problems with indefinite kernels. The main motivation for our work\ncomes from problems with structured representations (e.g., graphs, strings,\ntime-series), where it is relatively easy to devise a pairwise (dis)similarity\nfunction based on intuition and/or knowledge of domain experts. Such functions\nare typically not positive definite and it is often well beyond the expertise\nof practitioners to verify this condition. The effectiveness of the devised\napproaches is evaluated empirically using indefinite kernels defined on\nstructured and vectorial data representations. \n\n"}
{"id": "1809.02288", "contents": "Title: Tensor Ring Decomposition with Rank Minimization on Latent Space: An\n  Efficient Approach for Tensor Completion Abstract: In tensor completion tasks, the traditional low-rank tensor decomposition\nmodels suffer from the laborious model selection problem due to their high\nmodel sensitivity. In particular, for tensor ring (TR) decomposition, the\nnumber of model possibilities grows exponentially with the tensor order, which\nmakes it rather challenging to find the optimal TR decomposition. In this\npaper, by exploiting the low-rank structure of the TR latent space, we propose\na novel tensor completion method which is robust to model selection. In\ncontrast to imposing the low-rank constraint on the data space, we introduce\nnuclear norm regularization on the latent TR factors, resulting in the\noptimization step using singular value decomposition (SVD) being performed at a\nmuch smaller scale. By leveraging the alternating direction method of\nmultipliers (ADMM) scheme, the latent TR factors with optimal rank and the\nrecovered tensor can be obtained simultaneously. Our proposed algorithm is\nshown to effectively alleviate the burden of TR-rank selection, thereby greatly\nreducing the computational cost. The extensive experimental results on both\nsynthetic and real-world data demonstrate the superior performance and\nefficiency of the proposed approach against the state-of-the-art algorithms. \n\n"}
{"id": "1809.03137", "contents": "Title: Tracking by Animation: Unsupervised Learning of Multi-Object Attentive\n  Trackers Abstract: Online Multi-Object Tracking (MOT) from videos is a challenging computer\nvision task which has been extensively studied for decades. Most of the\nexisting MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm\ncombined with popular machine learning approaches which largely reduce the\nhuman effort to tune algorithm parameters. However, the commonly used\nsupervised learning approaches require the labeled data (e.g., bounding boxes),\nwhich is expensive for videos. Also, the TBD framework is usually suboptimal\nsince it is not end-to-end, i.e., it considers the task as detection and\ntracking, but not jointly. To achieve both label-free and end-to-end learning\nof MOT, we propose a Tracking-by-Animation framework, where a differentiable\nneural model first tracks objects from input frames and then animates these\nobjects into reconstructed frames. Learning is then driven by the\nreconstruction error through backpropagation. We further propose a\nReprioritized Attentive Tracking to improve the robustness of data association.\nExperiments conducted on both synthetic and real video datasets show the\npotential of the proposed model. Our project page is publicly available at:\nhttps://github.com/zhen-he/tracking-by-animation \n\n"}
{"id": "1809.03185", "contents": "Title: Shallow vs deep learning architectures for white matter lesion\n  segmentation in the early stages of multiple sclerosis Abstract: In this work, we present a comparison of a shallow and a deep learning\narchitecture for the automated segmentation of white matter lesions in MR\nimages of multiple sclerosis patients. In particular, we train and test both\nmethods on early stage disease patients, to verify their performance in\nchallenging conditions, more similar to a clinical setting than what is\ntypically provided in multiple sclerosis segmentation challenges. Furthermore,\nwe evaluate a prototype naive combination of the two methods, which refines the\nfinal segmentation. All methods were trained on 32 patients, and the evaluation\nwas performed on a pure test set of 73 cases. Results show low lesion-wise\nfalse positives (30%) for the deep learning architecture, whereas the shallow\narchitecture yields the best Dice coefficient (63%) and volume difference\n(19%). Combining both shallow and deep architectures further improves the\nlesion-wise metrics (69% and 26% lesion-wise true and false positive rate,\nrespectively). \n\n"}
{"id": "1809.04249", "contents": "Title: A Fast Globally Linearly Convergent Algorithm for the Computation of\n  Wasserstein Barycenters Abstract: We consider the problem of computing a Wasserstein barycenter for a set of\ndiscrete probability distributions with finite supports, which finds many\napplications in areas such as statistics, machine learning and image\nprocessing. When the support points of the barycenter are pre-specified, this\nproblem can be modeled as a linear programming (LP) problem whose size can be\nextremely large. To handle this large-scale LP, we analyse the structure of its\ndual problem, which is conceivably more tractable and can be reformulated as a\nwell-structured convex problem with 3 kinds of block variables and a coupling\nlinear equality constraint. We then adapt a symmetric Gauss-Seidel based\nalternating direction method of multipliers (sGS-ADMM) to solve the resulting\ndual problem and establish its global convergence and global linear convergence\nrate. As a critical component for efficient computation, we also show how all\nthe subproblems involved can be solved exactly and efficiently. This makes our\nmethod suitable for computing a Wasserstein barycenter on a large-scale data\nset, without introducing an entropy regularization term as is commonly\npracticed. In addition, our sGS-ADMM can be used as a subroutine in an\nalternating minimization method to compute a barycenter when its support points\nare not pre-specified. Numerical results on synthetic data sets and image data\nsets demonstrate that our method is highly competitive for solving large-scale\nWasserstein barycenter problems, in comparison to two existing representative\nmethods and the commercial software Gurobi. \n\n"}
{"id": "1809.04445", "contents": "Title: Structured and Unstructured Outlier Identification for Robust PCA: A Non\n  iterative, Parameter free Algorithm Abstract: Robust PCA, the problem of PCA in the presence of outliers has been\nextensively investigated in the last few years. Here we focus on Robust PCA in\nthe outlier model where each column of the data matrix is either an inlier or\nan outlier. Most of the existing methods for this model assumes either the\nknowledge of the dimension of the lower dimensional subspace or the fraction of\noutliers in the system. However in many applications knowledge of these\nparameters is not available. Motivated by this we propose a parameter free\noutlier identification method for robust PCA which a) does not require the\nknowledge of outlier fraction, b) does not require the knowledge of the\ndimension of the underlying subspace, c) is computationally simple and fast d)\ncan handle structured and unstructured outliers. Further, analytical guarantees\nare derived for outlier identification and the performance of the algorithm is\ncompared with the existing state of the art methods in both real and synthetic\ndata for various outlier structures. \n\n"}
{"id": "1809.04683", "contents": "Title: SAFE: A Neural Survival Analysis Model for Fraud Early Detection Abstract: Many online platforms have deployed anti-fraud systems to detect and prevent\nfraudulent activities. However, there is usually a gap between the time that a\nuser commits a fraudulent action and the time that the user is suspended by the\nplatform. How to detect fraudsters in time is a challenging problem. Most of\nthe existing approaches adopt classifiers to predict fraudsters given their\nactivity sequences along time. The main drawback of classification models is\nthat the prediction results between consecutive timestamps are often\ninconsistent. In this paper, we propose a survival analysis based fraud early\ndetection model, SAFE, which maps dynamic user activities to survival\nprobabilities that are guaranteed to be monotonically decreasing along time.\nSAFE adopts recurrent neural network (RNN) to handle user activity sequences\nand directly outputs hazard values at each timestamp, and then, survival\nprobability derived from hazard values is deployed to achieve consistent\npredictions. Because we only observe the user suspended time instead of the\nfraudulent activity time in the training data, we revise the loss function of\nthe regular survival model to achieve fraud early detection. Experimental\nresults on two real world datasets demonstrate that SAFE outperforms both the\nsurvival analysis model and recurrent neural network model alone as well as\nstate-of-the-art fraud early detection approaches. \n\n"}
{"id": "1809.04729", "contents": "Title: A Less Biased Evaluation of Out-of-distribution Sample Detectors Abstract: In the real world, a learning system could receive an input that is unlike\nanything it has seen during training. Unfortunately, out-of-distribution\nsamples can lead to unpredictable behaviour. We need to know whether any given\ninput belongs to the population distribution of the training/evaluation data to\nprevent unpredictable behaviour in deployed systems. A recent surge of interest\nin this problem has led to the development of sophisticated techniques in the\ndeep learning literature. However, due to the absence of a standard problem\ndefinition or an exhaustive evaluation, it is not evident if we can rely on\nthese methods. What makes this problem different from a typical supervised\nlearning setting is that the distribution of outliers used in training may not\nbe the same as the distribution of outliers encountered in the application.\nClassical approaches that learn inliers vs. outliers with only two datasets can\nyield optimistic results. We introduce OD-test, a three-dataset evaluation\nscheme as a more reliable strategy to assess progress on this problem. We\npresent an exhaustive evaluation of a broad set of methods from related areas\non image classification tasks. Contrary to the existing results, we show that\nfor realistic applications of high-dimensional images the previous techniques\nhave low accuracy and are not reliable in practice. \n\n"}
{"id": "1809.05043", "contents": "Title: PhD Dissertation: Generalized Independent Components Analysis Over\n  Finite Alphabets Abstract: Independent component analysis (ICA) is a statistical method for transforming\nan observable multi-dimensional random vector into components that are as\nstatistically independent as possible from each other. Usually the ICA\nframework assumes a model according to which the observations are generated\n(such as a linear transformation with additive noise). ICA over finite fields\nis a special case of ICA in which both the observations and the independent\ncomponents are over a finite alphabet. In this thesis we consider a formulation\nof the finite-field case in which an observation vector is decomposed to its\nindependent components (as much as possible) with no prior assumption on the\nway it was generated. This generalization is also known as Barlow's minimal\nredundancy representation and is considered an open problem. We propose several\ntheorems and show that this hard problem can be accurately solved with a branch\nand bound search tree algorithm, or tightly approximated with a series of\nlinear problems. Moreover, we show that there exists a simple transformation\n(namely, order permutation) which provides a greedy yet very effective\napproximation of the optimal solution. We further show that while not every\nrandom vector can be efficiently decomposed into independent components, the\nvast majority of vectors do decompose very well (that is, within a small\nconstant cost), as the dimension increases. In addition, we show that we may\npractically achieve this favorable constant cost with a complexity that is\nasymptotically linear in the alphabet size. Our contribution provides the first\nefficient set of solutions to Barlow's problem with theoretical and\ncomputational guarantees. Finally, we demonstrate our suggested framework in\nmultiple source coding applications. \n\n"}
{"id": "1809.05292", "contents": "Title: Efficient Rank Minimization via Solving Non-convexPenalties by Iterative\n  Shrinkage-Thresholding Algorithm Abstract: Rank minimization (RM) is a wildly investigated task of finding solutions by\nexploiting low-rank structure of parameter matrices. Recently, solving RM\nproblem by leveraging non-convex relaxations has received significant\nattention. It has been demonstrated by some theoretical and experimental work\nthat non-convex relaxation, e.g. Truncated Nuclear Norm Regularization (TNNR)\nand Reweighted Nuclear Norm Regularization (RNNR), can provide a better\napproximation of original problems than convex relaxations. However, designing\nan efficient algorithm with theoretical guarantee remains a challenging\nproblem. In this paper, we propose a simple but efficient proximal-type method,\nnamely Iterative Shrinkage-Thresholding Algorithm(ISTA), with concrete analysis\nto solve rank minimization problems with both non-convex weighted and\nreweighted nuclear norm as low-rank regularizers. Theoretically, the proposed\nmethod could converge to the critical point under very mild assumptions with\nthe rate in the order of $O(1/T)$. Moreover, the experimental results on both\nsynthetic data and real world data sets show that proposed algorithm\noutperforms state-of-arts in both efficiency and accuracy. \n\n"}
{"id": "1809.05323", "contents": "Title: Four-wave mixing in a silicon microring resonator using a self-pumping\n  geometry Abstract: We report on four-wave mixing in a silicon microring resonator using a\nself-pumping scheme instead of an external laser. The ring resonator is\ninserted in an external-loop cavity with a fibered semiconductor amplifier as a\nsource of gain. The silicon microring acts as a filter and we observe lasing in\none of the microring's resonances. We study correlations between signal and\nidler generated beams using a Joint Spectral Density experiment. \n\n"}
{"id": "1809.05504", "contents": "Title: Melding the Data-Decisions Pipeline: Decision-Focused Learning for\n  Combinatorial Optimization Abstract: Creating impact in real-world settings requires artificial intelligence\ntechniques to span the full pipeline from data, to predictive models, to\ndecisions. These components are typically approached separately: a machine\nlearning model is first trained via a measure of predictive accuracy, and then\nits predictions are used as input into an optimization algorithm which produces\na decision. However, the loss function used to train the model may easily be\nmisaligned with the end goal, which is to make the best decisions possible.\nHand-tuning the loss function to align with optimization is a difficult and\nerror-prone process (which is often skipped entirely).\n  We focus on combinatorial optimization problems and introduce a general\nframework for decision-focused learning, where the machine learning model is\ndirectly trained in conjunction with the optimization algorithm to produce\nhigh-quality decisions. Technically, our contribution is a means of integrating\ncommon classes of discrete optimization problems into deep learning or other\npredictive models, which are typically trained via gradient descent. The main\nidea is to use a continuous relaxation of the discrete problem to propagate\ngradients through the optimization procedure. We instantiate this framework for\ntwo broad classes of combinatorial problems: linear programs and submodular\nmaximization. Experimental results across a variety of domains show that\ndecision-focused learning often leads to improved optimization performance\ncompared to traditional methods. We find that standard measures of accuracy are\nnot a reliable proxy for a predictive model's utility in optimization, and our\nmethod's ability to specify the true goal as the model's training objective\nyields substantial dividends across a range of decision problems. \n\n"}
{"id": "1809.05693", "contents": "Title: apk2vec: Semi-supervised multi-view representation learning for\n  profiling Android applications Abstract: Building behavior profiles of Android applications (apps) with holistic, rich\nand multi-view information (e.g., incorporating several semantic views of an\napp such as API sequences, system calls, etc.) would help catering downstream\nanalytics tasks such as app categorization, recommendation and malware analysis\nsignificantly better. Towards this goal, we design a semi-supervised\nRepresentation Learning (RL) framework named apk2vec to automatically generate\na compact representation (aka profile/embedding) for a given app. More\nspecifically, apk2vec has the three following unique characteristics which make\nit an excellent choice for largescale app profiling: (1) it encompasses\ninformation from multiple semantic views such as API sequences, permissions,\netc., (2) being a semi-supervised embedding technique, it can make use of\nlabels associated with apps (e.g., malware family or app category labels) to\nbuild high quality app profiles, and (3) it combines RL and feature hashing\nwhich allows it to efficiently build profiles of apps that stream over time\n(i.e., online learning). The resulting semi-supervised multi-view hash\nembeddings of apps could then be used for a wide variety of downstream tasks\nsuch as the ones mentioned above. Our extensive evaluations with more than\n42,000 apps demonstrate that apk2vec's app profiles could significantly\noutperform state-of-the-art techniques in four app analytics tasks namely,\nmalware detection, familial clustering, app clone detection and app\nrecommendation. \n\n"}
{"id": "1809.05822", "contents": "Title: Aesthetic-based Clothing Recommendation Abstract: Recently, product images have gained increasing attention in clothing\nrecommendation since the visual appearance of clothing products has a\nsignificant impact on consumers' decision. Most existing methods rely on\nconventional features to represent an image, such as the visual features\nextracted by convolutional neural networks (CNN features) and the\nscale-invariant feature transform algorithm (SIFT features), color histograms,\nand so on. Nevertheless, one important type of features, the \\emph{aesthetic\nfeatures}, is seldom considered. It plays a vital role in clothing\nrecommendation since a users' decision depends largely on whether the clothing\nis in line with her aesthetics, however the conventional image features cannot\nportray this directly. To bridge this gap, we propose to introduce the\naesthetic information, which is highly relevant with user preference, into\nclothing recommender systems. To achieve this, we first present the aesthetic\nfeatures extracted by a pre-trained neural network, which is a brain-inspired\ndeep structure trained for the aesthetic assessment task. Considering that the\naesthetic preference varies significantly from user to user and by time, we\nthen propose a new tensor factorization model to incorporate the aesthetic\nfeatures in a personalized manner. We conduct extensive experiments on\nreal-world datasets, which demonstrate that our approach can capture the\naesthetic preference of users and significantly outperform several\nstate-of-the-art recommendation methods. \n\n"}
{"id": "1809.06367", "contents": "Title: Scattering Networks for Hybrid Representation Learning Abstract: Scattering networks are a class of designed Convolutional Neural Networks\n(CNNs) with fixed weights. We argue they can serve as generic representations\nfor modelling images. In particular, by working in scattering space, we achieve\ncompetitive results both for supervised and unsupervised learning tasks, while\nmaking progress towards constructing more interpretable CNNs. For supervised\nlearning, we demonstrate that the early layers of CNNs do not necessarily need\nto be learned, and can be replaced with a scattering network instead. Indeed,\nusing hybrid architectures, we achieve the best results with predefined\nrepresentations to-date, while being competitive with end-to-end learned CNNs.\nSpecifically, even applying a shallow cascade of small-windowed scattering\ncoefficients followed by 1$\\times$1-convolutions results in AlexNet accuracy on\nthe ILSVRC2012 classification task. Moreover, by combining scattering networks\nwith deep residual networks, we achieve a single-crop top-5 error of 11.4% on\nILSVRC2012. Also, we show they can yield excellent performance in the small\nsample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. For\nunsupervised learning, scattering coefficients can be a competitive\nrepresentation that permits image recovery. We use this fact to train hybrid\nGANs to generate images. Finally, we empirically analyze several properties\nrelated to stability and reconstruction of images from scattering coefficients. \n\n"}
{"id": "1809.06719", "contents": "Title: Improvements on Hindsight Learning Abstract: Sparse reward problems are one of the biggest challenges in Reinforcement\nLearning. Goal-directed tasks are one such sparse reward problems where a\nreward signal is received only when the goal is reached. One promising way to\ntrain an agent to perform goal-directed tasks is to use Hindsight Learning\napproaches. In these approaches, even when an agent fails to reach the desired\ngoal, the agent learns to reach the goal it achieved instead. Doing this over\nmultiple trajectories while generalizing the policy learned from the achieved\ngoals, the agent learns a goal conditioned policy to reach any goal. One such\napproach is Hindsight Experience replay which uses an off-policy Reinforcement\nLearning algorithm to learn a goal conditioned policy. In this approach, a\nreplay of the past transitions happens in a uniformly random fashion. Another\napproach is to use a Hindsight version of the policy gradients to directly\nlearn a policy. In this work, we discuss different ways to replay past\ntransitions to improve learning in hindsight experience replay focusing on\nprioritized variants in particular. Also, we implement the Hindsight Policy\ngradient methods to robotic tasks. \n\n"}
{"id": "1809.06781", "contents": "Title: Visual Diagnostics for Deep Reinforcement Learning Policy Development Abstract: Modern vision-based reinforcement learning techniques often use convolutional\nneural networks (CNN) as universal function approximators to choose which\naction to take for a given visual input. Until recently, CNNs have been treated\nlike black-box functions, but this mindset is especially dangerous when used\nfor control in safety-critical settings. In this paper, we present our\nextensions of CNN visualization algorithms to the domain of vision-based\nreinforcement learning. We use a simulated drone environment as an example\nscenario. These visualization algorithms are an important tool for behavior\nintrospection and provide insight into the qualities and flaws of trained\npolicies when interacting with the physical world. A video may be seen at\nhttps://sites.google.com/view/drlvisual . \n\n"}
{"id": "1809.06784", "contents": "Title: Adversarial Reinforcement Learning for Observer Design in Autonomous\n  Systems under Cyber Attacks Abstract: Complex autonomous control systems are subjected to sensor failures,\ncyber-attacks, sensor noise, communication channel failures, etc. that\nintroduce errors in the measurements. The corrupted information, if used for\nmaking decisions, can lead to degraded performance. We develop a framework for\nusing adversarial deep reinforcement learning to design observer strategies\nthat are robust to adversarial errors in information channels. We further show\nthrough simulation studies that the learned observation strategies perform\nremarkably well when the adversary's injected errors are bounded in some sense.\nWe use neural network as function approximator in our studies with the\nunderstanding that any other suitable function approximating class can be used\nwithin our framework. \n\n"}
{"id": "1809.06848", "contents": "Title: On the Learning Dynamics of Deep Neural Networks Abstract: While a lot of progress has been made in recent years, the dynamics of\nlearning in deep nonlinear neural networks remain to this day largely\nmisunderstood. In this work, we study the case of binary classification and\nprove various properties of learning in such networks under strong assumptions\nsuch as linear separability of the data. Extending existing results from the\nlinear case, we confirm empirical observations by proving that the\nclassification error also follows a sigmoidal shape in nonlinear architectures.\nWe show that given proper initialization, learning expounds parallel\nindependent modes and that certain regions of parameter space might lead to\nfailed training. We also demonstrate that input norm and features' frequency in\nthe dataset lead to distinct convergence speeds which might shed some light on\nthe generalization capabilities of deep neural networks. We provide a\ncomparison between the dynamics of learning with cross-entropy and hinge\nlosses, which could prove useful to understand recent progress in the training\nof generative adversarial networks. Finally, we identify a phenomenon that we\nbaptize gradient starvation where the most frequent features in a dataset\nprevent the learning of other less frequent but equally informative features. \n\n"}
{"id": "1809.06995", "contents": "Title: Interpretable Reinforcement Learning with Ensemble Methods Abstract: We propose to use boosted regression trees as a way to compute\nhuman-interpretable solutions to reinforcement learning problems. Boosting\ncombines several regression trees to improve their accuracy without\nsignificantly reducing their inherent interpretability. Prior work has focused\nindependently on reinforcement learning and on interpretable machine learning,\nbut there has been little progress in interpretable reinforcement learning. Our\nexperimental results show that boosted regression trees compute solutions that\nare both interpretable and match the quality of leading reinforcement learning\nmethods. \n\n"}
{"id": "1809.07196", "contents": "Title: Characterising Across-Stack Optimisations for Deep Convolutional Neural\n  Networks Abstract: Convolutional Neural Networks (CNNs) are extremely computationally demanding,\npresenting a large barrier to their deployment on resource-constrained devices.\nSince such systems are where some of their most useful applications lie (e.g.\nobstacle detection for mobile robots, vision-based medical assistive\ntechnology), significant bodies of work from both machine learning and systems\ncommunities have attempted to provide optimisations that will make CNNs\navailable to edge devices. In this paper we unify the two viewpoints in a Deep\nLearning Inference Stack and take an across-stack approach by implementing and\nevaluating the most common neural network compression techniques (weight\npruning, channel pruning, and quantisation) and optimising their parallel\nexecution with a range of programming approaches (OpenMP, OpenCL) and hardware\narchitectures (CPU, GPU). We provide comprehensive Pareto curves to instruct\ntrade-offs under constraints of accuracy, execution time, and memory space. \n\n"}
{"id": "1809.07575", "contents": "Title: Symbolic Music Genre Transfer with CycleGAN Abstract: Deep generative models such as Variational Autoencoders (VAEs) and Generative\nAdversarial Networks (GANs) have recently been applied to style and domain\ntransfer for images, and in the case of VAEs, music. GAN-based models employing\nseveral generators and some form of cycle consistency loss have been among the\nmost successful for image domain transfer. In this paper we apply such a model\nto symbolic music and show the feasibility of our approach for music genre\ntransfer. Evaluations using separate genre classifiers show that the style\ntransfer works well. In order to improve the fidelity of the transformed music,\nwe add additional discriminators that cause the generators to keep the\nstructure of the original music mostly intact, while still achieving strong\ngenre transfer. Visual and audible results further show the potential of our\napproach. To the best of our knowledge, this paper represents the first\napplication of GANs to symbolic music domain transfer. \n\n"}
{"id": "1809.08705", "contents": "Title: On the Behavior of the Expectation-Maximization Algorithm for Mixture\n  Models Abstract: Finite mixture models are among the most popular statistical models used in\ndifferent data science disciplines. Despite their broad applicability,\ninference under these models typically leads to computationally challenging\nnon-convex problems. While the Expectation-Maximization (EM) algorithm is the\nmost popular approach for solving these non-convex problems, the behavior of\nthis algorithm is not well understood. In this work, we focus on the case of\nmixture of Laplacian (or Gaussian) distribution. We start by analyzing a simple\nequally weighted mixture of two single dimensional Laplacian distributions and\nshow that every local optimum of the population maximum likelihood estimation\nproblem is globally optimal. Then, we prove that the EM algorithm converges to\nthe ground truth parameters almost surely with random initialization. Our\nresult extends the existing results for Gaussian distribution to Laplacian\ndistribution. Then we numerically study the behavior of mixture models with\nmore than two components. Motivated by our extensive numerical experiments, we\npropose a novel stochastic method for estimating the mean of components of a\nmixture model. Our numerical experiments show that our algorithm outperforms\nthe Naive EM algorithm in almost all scenarios. \n\n"}
{"id": "1809.08717", "contents": "Title: Unified recurrent neural network for many feature types Abstract: There are time series that are amenable to recurrent neural network (RNN)\nsolutions when treated as sequences, but some series, e.g. asynchronous time\nseries, provide a richer variation of feature types than current RNN cells take\ninto account. In order to address such situations, we introduce a unified RNN\nthat handles five different feature types, each in a different manner. Our RNN\nframework separates sequential features into two groups dependent on their\nfrequency, which we call sparse and dense features, and which affect cell\nupdates differently. Further, we also incorporate time features at the\nsequential level that relate to the time between specified events in the\nsequence and are used to modify the cell's memory state. We also include two\ntypes of static (whole sequence level) features, one related to time and one\nnot, which are combined with the encoder output. The experiments show that the\nmodeling framework proposed does increase performance compared to standard\ncells. \n\n"}
{"id": "1809.08820", "contents": "Title: Orthogonally Decoupled Variational Gaussian Processes Abstract: Gaussian processes (GPs) provide a powerful non-parametric framework for\nreasoning over functions. Despite appealing theory, its superlinear\ncomputational and memory complexities have presented a long-standing challenge.\nState-of-the-art sparse variational inference methods trade modeling accuracy\nagainst complexity. However, the complexities of these methods still scale\nsuperlinearly in the number of basis functions, implying that that sparse GP\nmethods are able to learn from large datasets only when a small model is used.\nRecently, a decoupled approach was proposed that removes the unnecessary\ncoupling between the complexities of modeling the mean and the covariance\nfunctions of a GP. It achieves a linear complexity in the number of mean\nparameters, so an expressive posterior mean function can be modeled. While\npromising, this approach suffers from optimization difficulties due to\nill-conditioning and non-convexity. In this work, we propose an alternative\ndecoupled parametrization. It adopts an orthogonal basis in the mean function\nto model the residues that cannot be learned by the standard coupled approach.\nTherefore, our method extends, rather than replaces, the coupled approach to\nachieve strictly better performance. This construction admits a straightforward\nnatural gradient update rule, so the structure of the information manifold that\nis lost during decoupling can be leveraged to speed up learning. Empirically,\nour algorithm demonstrates significantly faster convergence in multiple\nexperiments. \n\n"}
{"id": "1809.08899", "contents": "Title: Neural network approach to classifying alarming student responses to\n  online assessment Abstract: Automated scoring engines are increasingly being used to score the free-form\ntext responses that students give to questions. Such engines are not designed\nto appropriately deal with responses that a human reader would find alarming\nsuch as those that indicate an intention to self-harm or harm others, responses\nthat allude to drug abuse or sexual abuse or any response that would elicit\nconcern for the student writing the response. Our neural network models have\nbeen designed to help identify these anomalous responses from a large\ncollection of typical responses that students give. The responses identified by\nthe neural network can be assessed for urgency, severity, and validity more\nquickly by a team of reviewers than otherwise possible. Given the anomalous\nnature of these types of responses, our goal is to maximize the chance of\nflagging these responses for review given the constraint that only a fixed\npercentage of responses can viably be assessed by a team of reviewers. \n\n"}
{"id": "1809.10330", "contents": "Title: Variance reduction properties of the reparameterization trick Abstract: The reparameterization trick is widely used in variational inference as it\nyields more accurate estimates of the gradient of the variational objective\nthan alternative approaches such as the score function method. Although there\nis overwhelming empirical evidence in the literature showing its success, there\nis relatively little research exploring why the reparameterization trick is so\neffective. We explore this under the idealized assumptions that the variational\napproximation is a mean-field Gaussian density and that the log of the joint\ndensity of the model parameters and the data is a quadratic function that\ndepends on the variational mean. From this, we show that the marginal variances\nof the reparameterization gradient estimator are smaller than those of the\nscore function gradient estimator. We apply the result of our idealized\nanalysis to real-world examples. \n\n"}
{"id": "1809.10482", "contents": "Title: Budgeted Multi-Objective Optimization with a Focus on the Central Part\n  of the Pareto Front -- Extended Version Abstract: Optimizing nonlinear systems involving expensive computer experiments with\nregard to conflicting objectives is a common challenge. When the number of\nexperiments is severely restricted and/or when the number of objectives\nincreases, uncovering the whole set of Pareto optimal solutions is out of\nreach, even for surrogate-based approaches: the proposed solutions are\nsub-optimal or do not cover the front well. As non-compromising optimal\nsolutions have usually little point in applications, this work restricts the\nsearch to solutions that are close to the Pareto front center. The article\nstarts by characterizing this center, which is defined for any type of front.\nNext, a Bayesian multi-objective optimization method for directing the search\ntowards it is proposed. Targeting a subset of the Pareto front allows an\nimproved optimality of the solutions and a better coverage of this zone, which\nis our main concern. A criterion for detecting convergence to the center is\ndescribed. If the criterion is triggered, a widened central part of the Pareto\nfront is targeted such that sufficiently accurate convergence to it is\nforecasted within the remaining budget. Numerical experiments show how the\nresulting algorithm, C-EHI, better locates the central part of the Pareto front\nwhen compared to state-of-the-art Bayesian algorithms. \n\n"}
{"id": "1810.00383", "contents": "Title: Privacy-preserving Stochastic Gradual Learning Abstract: It is challenging for stochastic optimizations to handle large-scale\nsensitive data safely. Recently, Duchi et al. proposed private sampling\nstrategy to solve privacy leakage in stochastic optimizations. However, this\nstrategy leads to robustness degeneration, since this strategy is equal to the\nnoise injection on each gradient, which adversely affects updates of the primal\nvariable. To address this challenge, we introduce a robust stochastic\noptimization under the framework of local privacy, which is called\nPrivacy-pREserving StochasTIc Gradual lEarning (PRESTIGE). PRESTIGE bridges\nprivate updates of the primal variable (by private sampling) with the gradual\ncurriculum learning (CL). Specifically, the noise injection leads to the issue\nof label noise, but the robust learning process of CL can combat with label\nnoise. Thus, PRESTIGE yields \"private but robust\" updates of the primal\nvariable on the private curriculum, namely an reordered label sequence provided\nby CL. In theory, we reveal the convergence rate and maximum complexity of\nPRESTIGE. Empirical results on six datasets show that, PRESTIGE achieves a good\ntradeoff between privacy preservation and robustness over baselines. \n\n"}
{"id": "1810.00475", "contents": "Title: Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation Abstract: Left atrium shape has been shown to be an independent predictor of recurrence\nafter atrial fibrillation (AF) ablation. Shape-based representation is\nimperative to such an estimation process, where correspondence-based\nrepresentation offers the most flexibility and ease-of-computation for\npopulation-level shape statistics. Nonetheless, population-level shape\nrepresentations in the form of image segmentation and correspondence models\nderived from cardiac MRI require significant human resources with sufficient\nanatomy-specific expertise. In this paper, we propose a machine learning\napproach that uses deep networks to estimate AF recurrence by predicting shape\ndescriptors directly from MRI images, with NO image pre-processing involved. We\nalso propose a novel data augmentation scheme to effectively train a deep\nnetwork in a limited training data setting. We compare this new method of\nestimating shape descriptors from images with the state-of-the-art\ncorrespondence-based shape modeling that requires image segmentation and\ncorrespondence optimization. Results show that the proposed method and the\ncurrent state-of-the-art produce statistically similar outcomes on AF\nrecurrence, eliminating the need for expensive pre-processing pipelines and\nassociated human labor. \n\n"}
{"id": "1810.00664", "contents": "Title: Text Similarity in Vector Space Models: A Comparative Study Abstract: Automatic measurement of semantic text similarity is an important task in\nnatural language processing. In this paper, we evaluate the performance of\ndifferent vector space models to perform this task. We address the real-world\nproblem of modeling patent-to-patent similarity and compare TFIDF (and related\nextensions), topic models (e.g., latent semantic indexing), and neural models\n(e.g., paragraph vectors). Contrary to expectations, the added computational\ncost of text embedding methods is justified only when: 1) the target text is\ncondensed; and 2) the similarity comparison is trivial. Otherwise, TFIDF\nperforms surprisingly well in other cases: in particular for longer and more\ntechnical texts or for making finer-grained distinctions between nearest\nneighbors. Unexpectedly, extensions to the TFIDF method, such as adding noun\nphrases or calculating term weights incrementally, were not helpful in our\ncontext. \n\n"}
{"id": "1810.00787", "contents": "Title: On Theory for BART Abstract: Ensemble learning is a statistical paradigm built on the premise that many\nweak learners can perform exceptionally well when deployed collectively. The\nBART method of Chipman et al. (2010) is a prominent example of Bayesian\nensemble learning, where each learner is a tree. Due to its impressive\nperformance, BART has received a lot of attention from practitioners. Despite\nits wide popularity, however, theoretical studies of BART have begun emerging\nonly very recently. Laying the foundations for the theoretical analysis of\nBayesian forests, Rockova and van der Pas (2017) showed optimal posterior\nconcentration under conditionally uniform tree priors. These priors deviate\nfrom the actual priors implemented in BART. Here, we study the exact BART prior\nand propose a simple modification so that it also enjoys optimality properties.\nTo this end, we dive into branching process theory. We obtain tail bounds for\nthe distribution of total progeny under heterogeneous Galton-Watson (GW)\nprocesses exploiting their connection to random walks. We conclude with a\nresult stating the optimal rate of posterior convergence for BART. \n\n"}
{"id": "1810.00803", "contents": "Title: Large Scale Clustering with Variational EM for Gaussian Mixture Models Abstract: This paper represents a preliminary (pre-reviewing) version of a sublinear\nvariational algorithm for isotropic Gaussian mixture models (GMMs). Further\ndevelopments of the algorithm for GMMs with diagonal covariance matrices\n(instead of isotropic clusters) and their corresponding benchmarking results\nhave been published by TPAMI (doi:10.1109/TPAMI.2021.3133763) in the paper \"A\nVariational EM Acceleration for Efficient Clustering at Very Large Scales\". We\nkindly refer the reader to the TPAMI paper instead of this much earlier arXiv\nversion (the TPAMI paper is also open access). Publicly available source code\naccompanies the paper (see\nhttps://github.com/variational-sublinear-clustering). Please note that the\nTPAMI paper does not contain the benchmark on the 80 Million Tiny Images\ndataset anymore because we followed the call of the dataset creators to\ndiscontinue the use of that dataset.\n  The aim of the project (which resulted in this arXiv version and the later\nTPAMI paper) is the exploration of the current efficiency and large-scale\nlimits in fitting a parametric model for clustering to data distributions. To\nreduce computational complexity, we used a clustering objective based on\ntruncated variational EM (which reduces complexity for many clusters) in\ncombination with coreset objectives (which reduce complexity for many data\npoints). We used efficient coreset construction and efficient seeding to\ntranslate the theoretical sublinear complexity gains into an efficient\nalgorithm. In applications to standard large-scale benchmarks for clustering,\nwe then observed substantial wall-clock speedups compared to already highly\nefficient clustering approaches. To demonstrate that the observed efficiency\nenables applications previously considered unfeasible, we clustered the entire\nand unscaled 80 Million Tiny Images dataset into up to 32,000 clusters. \n\n"}
{"id": "1810.00826", "contents": "Title: How Powerful are Graph Neural Networks? Abstract: Graph Neural Networks (GNNs) are an effective framework for representation\nlearning of graphs. GNNs follow a neighborhood aggregation scheme, where the\nrepresentation vector of a node is computed by recursively aggregating and\ntransforming representation vectors of its neighboring nodes. Many GNN variants\nhave been proposed and have achieved state-of-the-art results on both node and\ngraph classification tasks. However, despite GNNs revolutionizing graph\nrepresentation learning, there is limited understanding of their\nrepresentational properties and limitations. Here, we present a theoretical\nframework for analyzing the expressive power of GNNs to capture different graph\nstructures. Our results characterize the discriminative power of popular GNN\nvariants, such as Graph Convolutional Networks and GraphSAGE, and show that\nthey cannot learn to distinguish certain simple graph structures. We then\ndevelop a simple architecture that is provably the most expressive among the\nclass of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism\ntest. We empirically validate our theoretical findings on a number of graph\nclassification benchmarks, and demonstrate that our model achieves\nstate-of-the-art performance. \n\n"}
{"id": "1810.01008", "contents": "Title: Learning Hash Codes via Hamming Distance Targets Abstract: We present a powerful new loss function and training scheme for learning\nbinary hash codes with any differentiable model and similarity function. Our\nloss function improves over prior methods by using log likelihood loss on top\nof an accurate approximation for the probability that two inputs fall within a\nHamming distance target. Our novel training scheme obtains a good estimate of\nthe true gradient by better sampling inputs and evaluating loss terms between\nall pairs of inputs in each minibatch. To fully leverage the resulting hashes,\nwe use multi-indexing. We demonstrate that these techniques provide large\nimprovements to a similarity search tasks. We report the best results to date\non competitive information retrieval tasks for ImageNet and SIFT 1M, improving\nMAP from 73% to 84% and reducing query cost by a factor of 2-8, respectively. \n\n"}
{"id": "1810.01367", "contents": "Title: FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative\n  Models Abstract: A promising class of generative models maps points from a simple distribution\nto a complex distribution through an invertible neural network.\nLikelihood-based training of these models requires restricting their\narchitectures to allow cheap computation of Jacobian determinants.\nAlternatively, the Jacobian trace can be used if the transformation is\nspecified by an ordinary differential equation. In this paper, we use\nHutchinson's trace estimator to give a scalable unbiased estimate of the\nlog-density. The result is a continuous-time invertible generative model with\nunbiased density estimation and one-pass sampling, while allowing unrestricted\nneural network architectures. We demonstrate our approach on high-dimensional\ndensity estimation, image generation, and variational inference, achieving the\nstate-of-the-art among exact likelihood methods with efficient sampling. \n\n"}
{"id": "1810.01480", "contents": "Title: Learning to Segment Inputs for NMT Favors Character-Level Processing Abstract: Most modern neural machine translation (NMT) systems rely on presegmented\ninputs. Segmentation granularity importantly determines the input and output\nsequence lengths, hence the modeling depth, and source and target vocabularies,\nwhich in turn determine model size, computational costs of softmax\nnormalization, and handling of out-of-vocabulary words. However, the current\npractice is to use static, heuristic-based segmentations that are fixed before\nNMT training. This begs the question whether the chosen segmentation is optimal\nfor the translation task. To overcome suboptimal segmentation choices, we\npresent an algorithm for dynamic segmentation based on the Adaptative\nComputation Time algorithm (Graves 2016), that is trainable end-to-end and\ndriven by the NMT objective. In an evaluation on four translation tasks we\nfound that, given the freedom to navigate between different segmentation\nlevels, the model prefers to operate on (almost) character level, providing\nsupport for purely character-level NMT models from a novel angle. \n\n"}
{"id": "1810.01778", "contents": "Title: A Bayesian model for sparse graphs with flexible degree distribution and\n  overlapping community structure Abstract: We consider a non-projective class of inhomogeneous random graph models with\ninterpretable parameters and a number of interesting asymptotic properties.\nUsing the results of Bollob\\'as et al. [2007], we show that i) the class of\nmodels is sparse and ii) depending on the choice of the parameters, the model\nis either scale-free, with power-law exponent greater than 2, or with an\nasymptotic degree distribution which is power-law with exponential cut-off. We\npropose an extension of the model that can accommodate an overlapping community\nstructure. Scalable posterior inference can be performed due to the specific\nchoice of the link probability. We present experiments on five different\nreal-world networks with up to 100,000 nodes and edges, showing that the model\ncan provide a good fit to the degree distribution and recovers well the latent\ncommunity structure. \n\n"}
{"id": "1810.01861", "contents": "Title: Inhibited Softmax for Uncertainty Estimation in Neural Networks Abstract: We present a new method for uncertainty estimation and out-of-distribution\ndetection in neural networks with softmax output. We extend softmax layer with\nan additional constant input. The corresponding additional output is able to\nrepresent the uncertainty of the network. The proposed method requires neither\nadditional parameters nor multiple forward passes nor input preprocessing nor\nout-of-distribution datasets. We show that our method performs comparably to\nmore computationally expensive methods and outperforms baselines on our\nexperiments from image recognition and sentiment analysis domains. \n\n"}
{"id": "1810.01873", "contents": "Title: Combining Natural Gradient with Hessian Free Methods for Sequence\n  Training Abstract: This paper presents a new optimisation approach to train Deep Neural Networks\n(DNNs) with discriminative sequence criteria. At each iteration, the method\ncombines information from the Natural Gradient (NG) direction with local\ncurvature information of the error surface that enables better paths on the\nparameter manifold to be traversed. The method is derived using an alternative\nderivation of Taylor's theorem using the concepts of manifolds, tangent vectors\nand directional derivatives from the perspective of Information Geometry. The\nefficacy of the method is shown within a Hessian Free (HF) style optimisation\nframework to sequence train both standard fully-connected DNNs and Time Delay\nNeural Networks as speech recognition acoustic models. It is shown that for the\nsame number of updates the proposed approach achieves larger reductions in the\nword error rate (WER) than both NG and HF, and also leads to a lower WER than\nstandard stochastic gradient descent. The paper also addresses the issue of\nover-fitting due to mismatch between training criterion and Word Error Rate\n(WER) that primarily arises during sequence training of ReLU-DNN models. \n\n"}
{"id": "1810.02068", "contents": "Title: Towards Fast and Energy-Efficient Binarized Neural Network Inference on\n  FPGA Abstract: Binarized Neural Network (BNN) removes bitwidth redundancy in classical CNN\nby using a single bit (-1/+1) for network parameters and intermediate\nrepresentations, which has greatly reduced the off-chip data transfer and\nstorage overhead. However, a large amount of computation redundancy still\nexists in BNN inference. By analyzing local properties of images and the\nlearned BNN kernel weights, we observe an average of $\\sim$78% input similarity\nand $\\sim$59% weight similarity among weight kernels, measured by our proposed\nmetric in common network architectures. Thus there does exist redundancy that\ncan be exploited to further reduce the amount of on-chip computations.\n  Motivated by the observation, in this paper, we proposed two types of fast\nand energy-efficient architectures for BNN inference. We also provide analysis\nand insights to pick the better strategy of these two for different datasets\nand network models. By reusing the results from previous computation, much\ncycles for data buffer access and computations can be skipped. By experiments,\nwe demonstrate that 80% of the computation and 40% of the buffer access can be\nskipped by exploiting BNN similarity. Thus, our design can achieve 17%\nreduction in total power consumption, 54% reduction in on-chip power\nconsumption and 2.4$\\times$ maximum speedup, compared to the baseline without\napplying our reuse technique. Our design also shows 1.9$\\times$ more\narea-efficiency compared to state-of-the-art BNN inference design. We believe\nour deployment of BNN on FPGA leads to a promising future of running deep\nlearning models on mobile devices. \n\n"}
{"id": "1810.02267", "contents": "Title: Turn-Key Diode-Pumped All-Fiber Broadband Polarization-Entangled Photon\n  Source Abstract: In this letter, we report a compact, low-power laser diode-pumped, all-fiber\npolarization-entangled photon pair source based on periodically-poled silica\nfiber technology. The all-fiber source offers room-temperature, alignment-free,\nturn-key operation, with low power consumption, and is packaged into a fanless,\nportable enclosure. It features a broad biphoton spectrum of more than 100nm\nwith a concurrence that is greater than 0.96 for polarization entanglement. The\nsource is stable over at least 10 hours of continuous operation, achieving\ncoincidence-to-accidental ratios of more than 2000 consistently over this time\nperiod. \n\n"}
{"id": "1810.02406", "contents": "Title: Projective Inference in High-dimensional Problems: Prediction and\n  Feature Selection Abstract: This paper discusses predictive inference and feature selection for\ngeneralized linear models with scarce but high-dimensional data. We argue that\nin many cases one can benefit from a decision theoretically justified two-stage\napproach: first, construct a possibly non-sparse model that predicts well, and\nthen find a minimal subset of features that characterize the predictions. The\nmodel built in the first step is referred to as the \\emph{reference model} and\nthe operation during the latter step as predictive \\emph{projection}. The key\ncharacteristic of this approach is that it finds an excellent tradeoff between\nsparsity and predictive accuracy, and the gain comes from utilizing all\navailable information including prior and that coming from the left out\nfeatures. We review several methods that follow this principle and provide\nnovel methodological contributions. We present a new projection technique that\nunifies two existing techniques and is both accurate and fast to compute. We\nalso propose a way of evaluating the feature selection process using fast\nleave-one-out cross-validation that allows for easy and intuitive model size\nselection. Furthermore, we prove a theorem that helps to understand the\nconditions under which the projective approach could be beneficial. The\nbenefits are illustrated via several simulated and real world examples. \n\n"}
{"id": "1810.02528", "contents": "Title: Local Stability and Performance of Simple Gradient Penalty\n  mu-Wasserstein GAN Abstract: Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance\nbetween a data distribution and sample distribution. Recent studies have\nproposed stabilizing the training process for the WGAN and implementing the\nLipschitz constraint. In this study, we prove the local stability of optimizing\nthe simple gradient penalty $\\mu$-WGAN(SGP $\\mu$-WGAN) under suitable\nassumptions regarding the equilibrium and penalty measure $\\mu$. The measure\nvalued differentiation concept is employed to deal with the derivative of the\npenalty terms, which is helpful for handling abstract singular measures with\nlower dimensional support. Based on this analysis, we claim that penalizing the\ndata manifold or sample manifold is the key to regularizing the original WGAN\nwith a gradient penalty. Experimental results obtained with unintuitive penalty\nmeasures that satisfy our assumptions are also provided to support our\ntheoretical results. \n\n"}
{"id": "1810.02797", "contents": "Title: RCCNet: An Efficient Convolutional Neural Network for Histological\n  Routine Colon Cancer Nuclei Classification Abstract: Efficient and precise classification of histological cell nuclei is of utmost\nimportance due to its potential applications in the field of medical image\nanalysis. It would facilitate the medical practitioners to better understand\nand explore various factors for cancer treatment. The classification of\nhistological cell nuclei is a challenging task due to the cellular\nheterogeneity. This paper proposes an efficient Convolutional Neural Network\n(CNN) based architecture for classification of histological routine colon\ncancer nuclei named as RCCNet. The main objective of this network is to keep\nthe CNN model as simple as possible. The proposed RCCNet model consists of only\n1,512,868 learnable parameters which are significantly less compared to the\npopular CNN models such as AlexNet, CIFARVGG, GoogLeNet, and WRN. The\nexperiments are conducted over publicly available routine colon cancer\nhistological dataset \"CRCHistoPhenotypes\". The results of the proposed RCCNet\nmodel are compared with five state-of-the-art CNN models in terms of the\naccuracy, weighted average F1 score and training time. The proposed method has\nachieved a classification accuracy of 80.61% and 0.7887 weighted average F1\nscore. The proposed RCCNet is more efficient and generalized terms of the\ntraining time and data over-fitting, respectively. \n\n"}
{"id": "1810.02912", "contents": "Title: Actor-Attention-Critic for Multi-Agent Reinforcement Learning Abstract: Reinforcement learning in multi-agent scenarios is important for real-world\napplications but presents challenges beyond those seen in single-agent\nsettings. We present an actor-critic algorithm that trains decentralized\npolicies in multi-agent settings, using centrally computed critics that share\nan attention mechanism which selects relevant information for each agent at\nevery timestep. This attention mechanism enables more effective and scalable\nlearning in complex multi-agent environments, when compared to recent\napproaches. Our approach is applicable not only to cooperative settings with\nshared rewards, but also individualized reward settings, including adversarial\nsettings, as well as settings that do not provide global states, and it makes\nno assumptions about the action spaces of the agents. As such, it is flexible\nenough to be applied to most multi-agent learning problems. \n\n"}
{"id": "1810.03256", "contents": "Title: Deep Diffeomorphic Normalizing Flows Abstract: The Normalizing Flow (NF) models a general probability density by estimating\nan invertible transformation applied on samples drawn from a known\ndistribution. We introduce a new type of NF, called Deep Diffeomorphic\nNormalizing Flow (DDNF). A diffeomorphic flow is an invertible function where\nboth the function and its inverse are smooth. We construct the flow using an\nordinary differential equation (ODE) governed by a time-varying smooth vector\nfield. We use a neural network to parametrize the smooth vector field and a\nrecursive neural network (RNN) for approximating the solution of the ODE. Each\ncell in the RNN is a residual network implementing one Euler integration step.\nThe architecture of our flow enables efficient likelihood evaluation,\nstraightforward flow inversion, and results in highly flexible density\nestimation. An end-to-end trained DDNF achieves competitive results with\nstate-of-the-art methods on a suite of density estimation and variational\ninference tasks. Finally, our method brings concepts from Riemannian geometry\nthat, we believe, can open a new research direction for neural density\nestimation. \n\n"}
{"id": "1810.03307", "contents": "Title: Local Explanation Methods for Deep Neural Networks Lack Sensitivity to\n  Parameter Values Abstract: Explaining the output of a complicated machine learning model like a deep\nneural network (DNN) is a central challenge in machine learning. Several\nproposed local explanation methods address this issue by identifying what\ndimensions of a single input are most responsible for a DNN's output. The goal\nof this work is to assess the sensitivity of local explanations to DNN\nparameter values. Somewhat surprisingly, we find that DNNs with\nrandomly-initialized weights produce explanations that are both visually and\nquantitatively similar to those produced by DNNs with learned weights. Our\nconjecture is that this phenomenon occurs because these explanations are\ndominated by the lower level features of a DNN, and that a DNN's architecture\nprovides a strong prior which significantly affects the representations learned\nat these lower layers. NOTE: This work is now subsumed by our recent\nmanuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we\nexpand on findings and address concerns raised in Sundararajan et. al. (2018). \n\n"}
{"id": "1810.03382", "contents": "Title: Deep learning cardiac motion analysis for human survival prediction Abstract: Motion analysis is used in computer vision to understand the behaviour of\nmoving objects in sequences of images. Optimising the interpretation of dynamic\nbiological systems requires accurate and precise motion tracking as well as\nefficient representations of high-dimensional motion trajectories so that these\ncan be used for prediction tasks. Here we use image sequences of the heart,\nacquired using cardiac magnetic resonance imaging, to create time-resolved\nthree-dimensional segmentations using a fully convolutional network trained on\nanatomical shape priors. This dense motion model formed the input to a\nsupervised denoising autoencoder (4Dsurvival), which is a hybrid network\nconsisting of an autoencoder that learns a task-specific latent code\nrepresentation trained on observed outcome data, yielding a latent\nrepresentation optimised for survival prediction. To handle right-censored\nsurvival outcomes, our network used a Cox partial likelihood loss function. In\na study of 302 patients the predictive accuracy (quantified by Harrell's\nC-index) was significantly higher (p < .0001) for our model C=0.73 (95$\\%$ CI:\n0.68 - 0.78) than the human benchmark of C=0.59 (95$\\%$ CI: 0.53 - 0.65). This\nwork demonstrates how a complex computer vision task using high-dimensional\nmedical image data can efficiently predict human survival. \n\n"}
{"id": "1810.03744", "contents": "Title: Neural Networks Models for Analyzing Magic: the Gathering Cards Abstract: Historically, games of all kinds have often been the subject of study in\nscientific works of Computer Science, including the field of machine learning.\nBy using machine learning techniques and applying them to a game with defined\nrules or a structured dataset, it's possible to learn and improve on the\nalready existing techniques and methods to tackle new challenges and solve\nproblems that are out of the ordinary. The already existing work on card games\ntends to focus on gameplay and card mechanics. This work aims to apply neural\nnetworks models, including Convolutional Neural Networks and Recurrent Neural\nNetworks, in order to analyze Magic: the Gathering cards, both in terms of card\ntext and illustrations; the card images and texts are used to train the\nnetworks in order to be able to classify them into multiple categories. The\nultimate goal was to develop a methodology that could generate card text\nmatching it to an input image, which was attained by relating the prediction\nvalues of the images and generated text across the different categories. \n\n"}
{"id": "1810.03773", "contents": "Title: Average Margin Regularization for Classifiers Abstract: Adversarial robustness has become an important research topic given empirical\ndemonstrations on the lack of robustness of deep neural networks.\nUnfortunately, recent theoretical results suggest that adversarial training\ninduces a strict tradeoff between classification accuracy and adversarial\nrobustness. In this paper, we propose and then study a new regularization for\nany margin classifier or deep neural network. We motivate this regularization\nby a novel generalization bound that shows a tradeoff in classifier accuracy\nbetween maximizing its margin and average margin. We thus call our approach an\naverage margin (AM) regularization, and it consists of a linear term added to\nthe objective. We theoretically show that for certain distributions AM\nregularization can both improve classifier accuracy and robustness to\nadversarial attacks. We conclude by using both synthetic and real data to\nempirically show that AM regularization can strictly improve both accuracy and\nrobustness for support vector machine's (SVM's), relative to unregularized\nclassifiers and adversarially trained classifiers. \n\n"}
{"id": "1810.03814", "contents": "Title: SNAP: A semismooth Newton algorithm for pathwise optimization with\n  optimal local convergence rate and oracle properties Abstract: We propose a semismooth Newton algorithm for pathwise optimization (SNAP) for\nthe LASSO and Enet in sparse, high-dimensional linear regression. SNAP is\nderived from a suitable formulation of the KKT conditions based on Newton\nderivatives. It solves the semismooth KKT equations efficiently by actively and\ncontinuously seeking the support of the regression coefficients along the\nsolution path with warm start. At each knot in the path, SNAP converges locally\nsuperlinearly for the Enet criterion and achieves an optimal local convergence\nrate for the LASSO criterion, i.e., SNAP converges in one step at the cost of\ntwo matrix-vector multiplication per iteration. Under certain regularity\nconditions on the design matrix and the minimum magnitude of the nonzero\nelements of the target regression coefficients, we show that SNAP hits a\nsolution with the same signs as the regression coefficients and achieves a\nsharp estimation error bound in finite steps with high probability. The\ncomputational complexity of SNAP is shown to be the same as that of LARS and\ncoordinate descent algorithms per iteration. Simulation studies and real data\nanalysis support our theoretical results and demonstrate that SNAP is faster\nand accurate than LARS and coordinate descent algorithms. \n\n"}
{"id": "1810.04021", "contents": "Title: Deep Geodesic Learning for Segmentation and Anatomical Landmarking Abstract: In this paper, we propose a novel deep learning framework for anatomy\nsegmentation and automatic landmark- ing. Specifically, we focus on the\nchallenging problem of mandible segmentation from cone-beam computed tomography\n(CBCT) scans and identification of 9 anatomical landmarks of the mandible on\nthe geodesic space. The overall approach employs three inter-related steps. In\nstep 1, we propose a deep neu- ral network architecture with carefully designed\nregularization, and network hyper-parameters to perform image segmentation\nwithout the need for data augmentation and complex post- processing refinement.\nIn step 2, we formulate the landmark localization problem directly on the\ngeodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose\nto use a long short-term memory (LSTM) network to identify closely- spaced\nlandmarks, which is rather difficult to obtain using other standard detection\nnetworks. The proposed fully automated method showed superior efficacy compared\nto the state-of-the- art mandible segmentation and landmarking approaches in\ncraniofacial anomalies and diseased states. We used a very challenging CBCT\ndataset of 50 patients with a high-degree of craniomaxillofacial (CMF)\nvariability that is realistic in clinical practice. Complementary to the\nquantitative analysis, the qualitative visual inspection was conducted for\ndistinct CBCT scans from 250 patients with high anatomical variability. We have\nalso shown feasibility of the proposed work in an independent dataset from\nMICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance.\nLastly, we present an in-depth analysis of the proposed deep networks with\nrespect to the choice of hyper-parameters such as pooling and activation\nfunctions. \n\n"}
{"id": "1810.04261", "contents": "Title: A Tale of Three Probabilistic Families: Discriminative, Descriptive and\n  Generative Models Abstract: The pattern theory of Grenander is a mathematical framework where patterns\nare represented by probability models on random variables of algebraic\nstructures. In this paper, we review three families of probability models,\nnamely, the discriminative models, the descriptive models, and the generative\nmodels. A discriminative model is in the form of a classifier. It specifies the\nconditional probability of the class label given the input signal. A\ndescriptive model specifies the probability distribution of the signal, based\non an energy function defined on the signal. A generative model assumes that\nthe signal is generated by some latent variables via a transformation. We shall\nreview these models within a common framework and explore their connections. We\nshall also review the recent developments that take advantage of the high\napproximation capacities of deep neural networks. \n\n"}
{"id": "1810.04632", "contents": "Title: Non-linear process convolutions for multi-output Gaussian processes Abstract: The paper introduces a non-linear version of the process convolution\nformalism for building covariance functions for multi-output Gaussian\nprocesses. The non-linearity is introduced via Volterra series, one series per\neach output. We provide closed-form expressions for the mean function and the\ncovariance function of the approximated Gaussian process at the output of the\nVolterra series. The mean function and covariance function for the joint\nGaussian process are derived using formulae for the product moments of Gaussian\nvariables. We compare the performance of the non-linear model against the\nclassical process convolution approach in one synthetic dataset and two real\ndatasets. \n\n"}
{"id": "1810.04642", "contents": "Title: Virtual Battery Parameter Identification using Transfer Learning based\n  Stacked Autoencoder Abstract: Recent studies have shown that the aggregated dynamic flexibility of an\nensemble of thermostatic loads can be modeled in the form of a virtual battery.\nThe existing methods for computing the virtual battery parameters require the\nknowledge of the first-principle models and parameter values of the loads in\nthe ensemble. In real-world applications, however, it is likely that the only\navailable information are end-use measurements such as power consumption, room\ntemperature, device on/off status, etc., while very little about the individual\nload models and parameters are known. We propose a transfer learning based deep\nnetwork framework for calculating virtual battery state of a given ensemble of\nflexible thermostatic loads, from the available end-use measurements. This\nproposed framework extracts first order virtual battery model parameters for\nthe given ensemble. We illustrate the effectiveness of this novel framework on\ndifferent ensembles of ACs and WHs. \n\n"}
{"id": "1810.05207", "contents": "Title: On Kernel Derivative Approximation with Random Fourier Features Abstract: Random Fourier features (RFF) represent one of the most popular and\nwide-spread techniques in machine learning to scale up kernel algorithms.\nDespite the numerous successful applications of RFFs, unfortunately, quite\nlittle is understood theoretically on their optimality and limitations of their\nperformance. Only recently, precise statistical-computational trade-offs have\nbeen established for RFFs in the approximation of kernel values, kernel ridge\nregression, kernel PCA and SVM classification. Our goal is to spark the\ninvestigation of optimality of RFF-based approximations in tasks involving not\nonly function values but derivatives, which naturally lead to optimization\nproblems with kernel derivatives. Particularly, in this paper, we focus on the\napproximation quality of RFFs for kernel derivatives and prove that the\nexisting finite-sample guarantees can be improved exponentially in terms of the\ndomain where they hold, using recent tools from unbounded empirical process\ntheory. Our result implies that the same approximation guarantee is attainable\nfor kernel derivatives using RFF as achieved for kernel values. \n\n"}
{"id": "1810.05247", "contents": "Title: Real-time Faulted Line Localization and PMU Placement in Power Systems\n  through Convolutional Neural Networks Abstract: Diverse fault types, fast re-closures, and complicated transient states after\na fault event make real-time fault location in power grids challenging.\nExisting localization techniques in this area rely on simplistic assumptions,\nsuch as static loads, or require much higher sampling rates or total\nmeasurement availability. This paper proposes a faulted line localization\nmethod based on a Convolutional Neural Network (CNN) classifier using bus\nvoltages. Unlike prior data-driven methods, the proposed classifier is based on\nfeatures with physical interpretations that improve the robustness of the\nlocation performance. The accuracy of our CNN based localization tool is\ndemonstrably superior to other machine learning classifiers in the literature.\nTo further improve the location performance, a joint phasor measurement units\n(PMU) placement strategy is proposed and validated against other methods. A\nsignificant aspect of our methodology is that under very low observability (7%\nof buses), the algorithm is still able to localize the faulted line to a small\nneighborhood with high probability. The performance of our scheme is validated\nthrough simulations of faults of various types in the IEEE 39-bus and 68-bus\npower systems under varying uncertain conditions, system observability, and\nmeasurement quality. \n\n"}
{"id": "1810.05270", "contents": "Title: Rethinking the Value of Network Pruning Abstract: Network pruning is widely used for reducing the heavy inference cost of deep\nmodels in low-resource settings. A typical pruning algorithm is a three-stage\npipeline, i.e., training (a large model), pruning and fine-tuning. During\npruning, according to a certain criterion, redundant weights are pruned and\nimportant weights are kept to best preserve the accuracy. In this work, we make\nseveral surprising observations which contradict common beliefs. For all\nstate-of-the-art structured pruning algorithms we examined, fine-tuning a\npruned model only gives comparable or worse performance than training that\nmodel with randomly initialized weights. For pruning algorithms which assume a\npredefined target network architecture, one can get rid of the full pipeline\nand directly train the target network from scratch. Our observations are\nconsistent for multiple network architectures, datasets, and tasks, which imply\nthat: 1) training a large, over-parameterized model is often not necessary to\nobtain an efficient final model, 2) learned \"important\" weights of the large\nmodel are typically not useful for the small pruned model, 3) the pruned\narchitecture itself, rather than a set of inherited \"important\" weights, is\nmore crucial to the efficiency in the final model, which suggests that in some\ncases pruning can be useful as an architecture search paradigm. Our results\nsuggest the need for more careful baseline evaluations in future research on\nstructured pruning methods. We also compare with the \"Lottery Ticket\nHypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate,\nthe \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not\nbring improvement over random initialization. \n\n"}
{"id": "1810.05471", "contents": "Title: Safe Grid Search with Optimal Complexity Abstract: Popular machine learning estimators involve regularization parameters that\ncan be challenging to tune, and standard strategies rely on grid search for\nthis task. In this paper, we revisit the techniques of approximating the\nregularization path up to predefined tolerance $\\epsilon$ in a unified\nframework and show that its complexity is $O(1/\\sqrt[d]{\\epsilon})$ for\nuniformly convex loss of order $d \\geq 2$ and $O(1/\\sqrt{\\epsilon})$ for\nGeneralized Self-Concordant functions. This framework encompasses least-squares\nbut also logistic regression, a case that as far as we know was not handled as\nprecisely in previous works. We leverage our technique to provide refined\nbounds on the validation error as well as a practical algorithm for\nhyperparameter tuning. The latter has global convergence guarantee when\ntargeting a prescribed accuracy on the validation set. Last but not least, our\napproach helps relieving the practitioner from the (often neglected) task of\nselecting a stopping criterion when optimizing over the training set: our\nmethod automatically calibrates this criterion based on the targeted accuracy\non the validation set. \n\n"}
{"id": "1810.05507", "contents": "Title: Dynamic Difficulty Awareness Training for Continuous Emotion Prediction Abstract: Time-continuous emotion prediction has become an increasingly compelling task\nin machine learning. Considerable efforts have been made to advance the\nperformance of these systems. Nonetheless, the main focus has been the\ndevelopment of more sophisticated models and the incorporation of different\nexpressive modalities (e. g., speech, face, and physiology). In this paper,\nmotivated by the benefit of difficulty awareness in a human learning procedure,\nwe propose a novel machine learning framework, namely, Dynamic Difficulty\nAwareness Training (DDAT), which sheds fresh light on the research -- directly\nexploiting the difficulties in learning to boost the machine learning process.\nThe DDAT framework consists of two stages: information retrieval and\ninformation exploitation. In the first stage, we make use of the reconstruction\nerror of input features or the annotation uncertainty to estimate the\ndifficulty of learning specific information. The obtained difficulty level is\nthen used in tandem with original features to update the model input in a\nsecond learning stage with the expectation that the model can learn to focus on\nhigh difficulty regions of the learning process. We perform extensive\nexperiments on a benchmark database (RECOLA) to evaluate the effectiveness of\nthe proposed framework. The experimental results show that our approach\noutperforms related baselines as well as other well-established time-continuous\nemotion prediction systems, which suggests that dynamically integrating the\ndifficulty information for neural networks can help enhance the learning\nprocess. \n\n"}
{"id": "1810.05633", "contents": "Title: Stochastic (Approximate) Proximal Point Methods: Convergence,\n  Optimality, and Adaptivity Abstract: We develop model-based methods for solving stochastic convex optimization\nproblems, introducing the approximate-proximal point, or aProx, family, which\nincludes stochastic subgradient, proximal point, and bundle methods. When the\nmodeling approaches we propose are appropriately accurate, the methods enjoy\nstronger convergence and robustness guarantees than classical approaches, even\nthough the model-based methods typically add little to no computational\noverhead over stochastic subgradient methods. For example, we show that\nimproved models converge with probability 1 and enjoy optimal asymptotic\nnormality results under weak assumptions; these methods are also adaptive to a\nnatural class of what we term easy optimization problems, achieving linear\nconvergence under appropriate strong growth conditions on the objective. Our\nsubstantial experimental investigation shows the advantages of more accurate\nmodeling over standard subgradient methods across many smooth and non-smooth\noptimization problems. \n\n"}
{"id": "1810.05728", "contents": "Title: Estimating Information Flow in Deep Neural Networks Abstract: We study the flow of information and the evolution of internal\nrepresentations during deep neural network (DNN) training, aiming to demystify\nthe compression aspect of the information bottleneck theory. The theory\nsuggests that DNN training comprises a rapid fitting phase followed by a slower\ncompression phase, in which the mutual information $I(X;T)$ between the input\n$X$ and internal representations $T$ decreases. Several papers observe\ncompression of estimated mutual information on different DNN models, but the\ntrue $I(X;T)$ over these networks is provably either constant (discrete $X$) or\ninfinite (continuous $X$). This work explains the discrepancy between theory\nand experiments, and clarifies what was actually measured by these past works.\nTo this end, we introduce an auxiliary (noisy) DNN framework for which $I(X;T)$\nis a meaningful quantity that depends on the network's parameters. This noisy\nframework is shown to be a good proxy for the original (deterministic) DNN both\nin terms of performance and the learned representations. We then develop a\nrigorous estimator for $I(X;T)$ in noisy DNNs and observe compression in\nvarious models. By relating $I(X;T)$ in the noisy DNN to an\ninformation-theoretic communication problem, we show that compression is driven\nby the progressive clustering of hidden representations of inputs from the same\nclass. Several methods to directly monitor clustering of hidden\nrepresentations, both in noisy and deterministic DNNs, are used to show that\nmeaningful clusters form in the $T$ space. Finally, we return to the estimator\nof $I(X;T)$ employed in past works, and demonstrate that while it fails to\ncapture the true (vacuous) mutual information, it does serve as a measure for\nclustering. This clarifies the past observations of compression and isolates\nthe geometric clustering of hidden representations as the true phenomenon of\ninterest. \n\n"}
{"id": "1810.05741", "contents": "Title: Explaining Black Boxes on Sequential Data using Weighted Automata Abstract: Understanding how a learned black box works is of crucial interest for the\nfuture of Machine Learning. In this paper, we pioneer the question of the\nglobal interpretability of learned black box models that assign numerical\nvalues to symbolic sequential data. To tackle that task, we propose a spectral\nalgorithm for the extraction of weighted automata (WA) from such black boxes.\nThis algorithm does not require the access to a dataset or to the inner\nrepresentation of the black box: the inferred model can be obtained solely by\nquerying the black box, feeding it with inputs and analyzing its outputs.\nExperiments using Recurrent Neural Networks (RNN) trained on a wide collection\nof 48 synthetic datasets and 2 real datasets show that the obtained\napproximation is of great quality. \n\n"}
{"id": "1810.06509", "contents": "Title: Predictor-Corrector Policy Optimization Abstract: We present a predictor-corrector framework, called PicCoLO, that can\ntransform a first-order model-free reinforcement or imitation learning\nalgorithm into a new hybrid method that leverages predictive models to\naccelerate policy learning. The new \"PicCoLOed\" algorithm optimizes a policy by\nrecursively repeating two steps: In the Prediction Step, the learner uses a\nmodel to predict the unseen future gradient and then applies the predicted\nestimate to update the policy; in the Correction Step, the learner runs the\nupdated policy in the environment, receives the true gradient, and then\ncorrects the policy using the gradient error. Unlike previous algorithms,\nPicCoLO corrects for the mistakes of using imperfect predicted gradients and\nhence does not suffer from model bias. The development of PicCoLO is made\npossible by a novel reduction from predictable online learning to adversarial\nonline learning, which provides a systematic way to modify existing first-order\nalgorithms to achieve the optimal regret with respect to predictable\ninformation. We show, in both theory and simulation, that the convergence rate\nof several first-order model-free algorithms can be improved by PicCoLO. \n\n"}
{"id": "1810.06695", "contents": "Title: Exploring the Use of Attention within an Neural Machine Translation\n  Decoder States to Translate Idioms Abstract: Idioms pose problems to almost all Machine Translation systems. This type of\nlanguage is very frequent in day-to-day language use and cannot be simply\nignored. The recent interest in memory augmented models in the field of\nLanguage Modelling has aided the systems to achieve good results by bridging\nlong-distance dependencies. In this paper we explore the use of such techniques\ninto a Neural Machine Translation system to help in translation of idiomatic\nlanguage. \n\n"}
{"id": "1810.07322", "contents": "Title: Functionality-Oriented Convolutional Filter Pruning Abstract: The sophisticated structure of Convolutional Neural Network (CNN) allows for\noutstanding performance, but at the cost of intensive computation. As\nsignificant redundancies inevitably present in such a structure, many works\nhave been proposed to prune the convolutional filters for computation cost\nreduction. Although extremely effective, most works are based only on\nquantitative characteristics of the convolutional filters, and highly overlook\nthe qualitative interpretation of individual filter's specific functionality.\nIn this work, we interpreted the functionality and redundancy of the\nconvolutional filters from different perspectives, and proposed a\nfunctionality-oriented filter pruning method. With extensive experiment\nresults, we proved the convolutional filters' qualitative significance\nregardless of magnitude, demonstrated significant neural network redundancy due\nto repetitive filter functions, and analyzed the filter functionality defection\nunder inappropriate retraining process. Such an interpretable pruning approach\nnot only offers outstanding computation cost optimization over previous filter\npruning methods, but also interprets filter pruning process. \n\n"}
{"id": "1810.07354", "contents": "Title: Fault Tolerance in Iterative-Convergent Machine Learning Abstract: Machine learning (ML) training algorithms often possess an inherent\nself-correcting behavior due to their iterative-convergent nature. Recent\nsystems exploit this property to achieve adaptability and efficiency in\nunreliable computing environments by relaxing the consistency of execution and\nallowing calculation errors to be self-corrected during training. However, the\nbehavior of such systems are only well understood for specific types of\ncalculation errors, such as those caused by staleness, reduced precision, or\nasynchronicity, and for specific types of training algorithms, such as\nstochastic gradient descent. In this paper, we develop a general framework to\nquantify the effects of calculation errors on iterative-convergent algorithms\nand use this framework to design new strategies for checkpoint-based fault\ntolerance. Our framework yields a worst-case upper bound on the iteration cost\nof arbitrary perturbations to model parameters during training. Our system,\nSCAR, employs strategies which reduce the iteration cost upper bound due to\nperturbations incurred when recovering from checkpoints. We show that SCAR can\nreduce the iteration cost of partial failures by 78% - 95% when compared with\ntraditional checkpoint-based fault tolerance across a variety of ML models and\ntraining algorithms. \n\n"}
{"id": "1810.07874", "contents": "Title: A Self-Organizing Tensor Architecture for Multi-View Clustering Abstract: In many real-world applications, data are often unlabeled and comprised of\ndifferent representations/views which often provide information complementary\nto each other. Although several multi-view clustering methods have been\nproposed, most of them routinely assume one weight for one view of features,\nand thus inter-view correlations are only considered at the view-level. These\napproaches, however, fail to explore the explicit correlations between features\nacross multiple views. In this paper, we introduce a tensor-based approach to\nincorporate the higher-order interactions among multiple views as a tensor\nstructure. Specifically, we propose a multi-linear multi-view clustering (MMC)\nmethod that can efficiently explore the full-order structural information among\nall views and reveal the underlying subspace structure embedded within the\ntensor. Extensive experiments on real-world datasets demonstrate that our\nproposed MMC algorithm clearly outperforms other related state-of-the-art\nmethods. \n\n"}
{"id": "1810.08164", "contents": "Title: A Unified Approach to Translate Classical Bandit Algorithms to the\n  Structured Bandit Setting Abstract: We consider a finite-armed structured bandit problem in which mean rewards of\ndifferent arms are known functions of a common hidden parameter $\\theta^*$.\nSince we do not place any restrictions of these functions, the problem setting\nsubsumes several previously studied frameworks that assume linear or invertible\nreward functions. We propose a novel approach to gradually estimate the hidden\n$\\theta^*$ and use the estimate together with the mean reward functions to\nsubstantially reduce exploration of sub-optimal arms. This approach enables us\nto fundamentally generalize any classic bandit algorithm including UCB and\nThompson Sampling to the structured bandit setting. We prove via regret\nanalysis that our proposed UCB-C algorithm (structured bandit versions of UCB)\npulls only a subset of the sub-optimal arms $O(\\log T)$ times while the other\nsub-optimal arms (referred to as non-competitive arms) are pulled $O(1)$ times.\nAs a result, in cases where all sub-optimal arms are non-competitive, which can\nhappen in many practical scenarios, the proposed algorithms achieve bounded\nregret. We also conduct simulations on the Movielens recommendations dataset to\ndemonstrate the improvement of the proposed algorithms over existing structured\nbandit algorithms. \n\n"}
{"id": "1810.08322", "contents": "Title: Sequenced-Replacement Sampling for Deep Learning Abstract: We propose sequenced-replacement sampling (SRS) for training deep neural\nnetworks. The basic idea is to assign a fixed sequence index to each sample in\nthe dataset. Once a mini-batch is randomly drawn in each training iteration, we\nrefill the original dataset by successively adding samples according to their\nsequence index. Thus we carry out replacement sampling but in a batched and\nsequenced way. In a sense, SRS could be viewed as a way of performing\n\"mini-batch augmentation\". It is particularly useful for a task where we have a\nrelatively small images-per-class such as CIFAR-100. Together with a longer\nperiod of initial large learning rate, it significantly improves the\nclassification accuracy in CIFAR-100 over the current state-of-the-art results.\nOur experiments indicate that training deeper networks with SRS is less prone\nto over-fitting. In the best case, we achieve an error rate as low as 10.10%. \n\n"}
{"id": "1810.08676", "contents": "Title: Subset Scanning Over Neural Network Activations Abstract: This work views neural networks as data generating systems and applies\nanomalous pattern detection techniques on that data in order to detect when a\nnetwork is processing an anomalous input. Detecting anomalies is a critical\ncomponent for multiple machine learning problems including detecting\nadversarial noise. More broadly, this work is a step towards giving neural\nnetworks the ability to recognize an out-of-distribution sample. This is the\nfirst work to introduce \"Subset Scanning\" methods from the anomalous pattern\ndetection domain to the task of detecting anomalous input of neural networks.\nSubset scanning treats the detection problem as a search for the most anomalous\nsubset of node activations (i.e., highest scoring subset according to\nnon-parametric scan statistics). Mathematical properties of these scoring\nfunctions allow the search to be completed in log-linear rather than\nexponential time while still guaranteeing the most anomalous subset of nodes in\nthe network is identified for a given input. Quantitative results for detecting\nand characterizing adversarial noise are provided for CIFAR-10 images on a\nsimple convolutional neural network. We observe an \"interference\" pattern where\nanomalous activations in shallow layers suppress the activation structure of\nthe original image in deeper layers. \n\n"}
{"id": "1810.08907", "contents": "Title: Understanding the Acceleration Phenomenon via High-Resolution\n  Differential Equations Abstract: Gradient-based optimization algorithms can be studied from the perspective of\nlimiting ordinary differential equations (ODEs). Motivated by the fact that\nexisting ODEs do not distinguish between two fundamentally different\nalgorithms---Nesterov's accelerated gradient method for strongly convex\nfunctions (NAG-SC) and Polyak's heavy-ball method---we study an alternative\nlimiting process that yields high-resolution ODEs. We show that these ODEs\npermit a general Lyapunov function framework for the analysis of convergence in\nboth continuous and discrete time. We also show that these ODEs are more\naccurate surrogates for the underlying algorithms; in particular, they not only\ndistinguish between NAG-SC and Polyak's heavy-ball method, but they allow the\nidentification of a term that we refer to as \"gradient correction\" that is\npresent in NAG-SC but not in the heavy-ball method and is responsible for the\nqualitative difference in convergence of the two methods. We also use the\nhigh-resolution ODE framework to study Nesterov's accelerated gradient method\nfor (non-strongly) convex functions, uncovering a hitherto unknown\nresult---that NAG-C minimizes the squared gradient norm at an inverse cubic\nrate. Finally, by modifying the high-resolution ODE of NAG-C, we obtain a\nfamily of new optimization methods that are shown to maintain the accelerated\nconvergence rates of NAG-C for smooth convex functions. \n\n"}
{"id": "1810.09391", "contents": "Title: A neuro-inspired architecture for unsupervised continual learning based\n  on online clustering and hierarchical predictive coding Abstract: We propose that the Continual Learning desiderata can be achieved through a\nneuro-inspired architecture, grounded on Mountcastle's cortical column\nhypothesis. The proposed architecture involves a single module, called\nSelf-Taught Associative Memory (STAM), which models the function of a cortical\ncolumn. STAMs are repeated in multi-level hierarchies involving feedforward,\nlateral and feedback connections. STAM networks learn in an unsupervised\nmanner, based on a combination of online clustering and hierarchical predictive\ncoding. This short paper only presents the architecture and its connections\nwith neuroscience. A mathematical formulation and experimental results will be\npresented in an extended version of this paper. \n\n"}
{"id": "1810.09440", "contents": "Title: Deep multi-survey classification of variable stars Abstract: During the last decade, a considerable amount of effort has been made to\nclassify variable stars using different machine learning techniques. Typically,\nlight curves are represented as vectors of statistical descriptors or features\nthat are used to train various algorithms. These features demand big\ncomputational powers that can last from hours to days, making impossible to\ncreate scalable and efficient ways of automatically classifying variable stars.\nAlso, light curves from different surveys cannot be integrated and analyzed\ntogether when using features, because of observational differences. For\nexample, having variations in cadence and filters, feature distributions become\nbiased and require expensive data-calibration models. The vast amount of data\nthat will be generated soon make necessary to develop scalable machine learning\narchitectures without expensive integration techniques. Convolutional Neural\nNetworks have shown impressing results in raw image classification and\nrepresentation within the machine learning literature. In this work, we present\na novel Deep Learning model for light curve classification, mainly based on\nconvolutional units. Our architecture receives as input the differences between\ntime and magnitude of light curves. It captures the essential classification\npatterns regardless of cadence and filter. In addition, we introduce a novel\ndata augmentation schema for unevenly sampled time series. We test our method\nusing three different surveys: OGLE-III; Corot; and VVV, which differ in\nfilters, cadence, and area of the sky. We show that besides the benefit of\nscalability, our model obtains state of the art levels accuracy in light curve\nclassification benchmarks. \n\n"}
{"id": "1810.09751", "contents": "Title: Analysis of Atomistic Representations Using Weighted Skip-Connections Abstract: In this work, we extend the SchNet architecture by using weighted skip\nconnections to assemble the final representation. This enables us to study the\nrelative importance of each interaction block for property prediction. We\ndemonstrate on both the QM9 and MD17 dataset that their relative weighting\ndepends strongly on the chemical composition and configurational degrees of\nfreedom of the molecules which opens the path towards a more detailed\nunderstanding of machine learning models for molecules. \n\n"}
{"id": "1810.09785", "contents": "Title: SING: Symbol-to-Instrument Neural Generator Abstract: Recent progress in deep learning for audio synthesis opens the way to models\nthat directly produce the waveform, shifting away from the traditional paradigm\nof relying on vocoders or MIDI synthesizers for speech or music generation.\nDespite their successes, current state-of-the-art neural audio synthesizers\nsuch as WaveNet and SampleRNN suffer from prohibitive training and inference\ntimes because they are based on autoregressive models that generate audio\nsamples one at a time at a rate of 16kHz. In this work, we study the more\ncomputationally efficient alternative of generating the waveform frame-by-frame\nwith large strides. We present SING, a lightweight neural audio synthesizer for\nthe original task of generating musical notes given desired instrument, pitch\nand velocity. Our model is trained end-to-end to generate notes from nearly\n1000 instruments with a single decoder, thanks to a new loss function that\nminimizes the distances between the log spectrograms of the generated and\ntarget waveforms. On the generalization task of synthesizing notes for pairs of\npitch and instrument not seen during training, SING produces audio with\nsignificantly improved perceptual quality compared to a state-of-the-art\nautoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is\nabout 32 times faster for training and 2, 500 times faster for inference. \n\n"}
{"id": "1810.09957", "contents": "Title: NSML: Meet the MLaaS platform with a real-world case study Abstract: The boom of deep learning induced many industries and academies to introduce\nmachine learning based approaches into their concern, competitively. However,\nexisting machine learning frameworks are limited to sufficiently fulfill the\ncollaboration and management for both data and models. We proposed NSML, a\nmachine learning as a service (MLaaS) platform, to meet these demands. NSML\nhelps machine learning work be easily launched on a NSML cluster and provides a\ncollaborative environment which can afford development at enterprise scale.\nFinally, NSML users can deploy their own commercial services with NSML cluster.\nIn addition, NSML furnishes convenient visualization tools which assist the\nusers in analyzing their work. To verify the usefulness and accessibility of\nNSML, we performed some experiments with common examples. Furthermore, we\nexamined the collaborative advantages of NSML through three competitions with\nreal-world use cases. \n\n"}
{"id": "1810.10176", "contents": "Title: Text Embeddings for Retrieval From a Large Knowledge Base Abstract: Text embedding representing natural language documents in a semantic vector\nspace can be used for document retrieval using nearest neighbor lookup. In\norder to study the feasibility of neural models specialized for retrieval in a\nsemantically meaningful way, we suggest the use of the Stanford Question\nAnswering Dataset (SQuAD) in an open-domain question answering context, where\nthe first task is to find paragraphs useful for answering a given question.\nFirst, we compare the quality of various text-embedding methods on the\nperformance of retrieval and give an extensive empirical comparison on the\nperformance of various non-augmented base embedding with, and without IDF\nweighting. Our main results are that by training deep residual neural models,\nspecifically for retrieval purposes, can yield significant gains when it is\nused to augment existing embeddings. We also establish that deeper models are\nsuperior to this task. The best base baseline embeddings augmented by our\nlearned neural approach improves the top-1 paragraph recall of the system by\n14%. \n\n"}
{"id": "1810.10612", "contents": "Title: Continual Classification Learning Using Generative Models Abstract: Continual learning is the ability to sequentially learn over time by\naccommodating knowledge while retaining previously learned experiences. Neural\nnetworks can learn multiple tasks when trained on them jointly, but cannot\nmaintain performance on previously learned tasks when tasks are presented one\nat a time. This problem is called catastrophic forgetting. In this work, we\npropose a classification model that learns continuously from sequentially\nobserved tasks, while preventing catastrophic forgetting. We build on the\nlifelong generative capabilities of [10] and extend it to the classification\nsetting by deriving a new variational bound on the joint log likelihood, $\\log\np(x; y)$. \n\n"}
{"id": "1810.10703", "contents": "Title: K for the Price of 1: Parameter-efficient Multi-task and Transfer\n  Learning Abstract: We introduce a novel method that enables parameter-efficient transfer and\nmulti-task learning with deep neural networks. The basic approach is to learn a\nmodel patch - a small set of parameters - that will specialize to each task,\ninstead of fine-tuning the last layer or the entire network. For instance, we\nshow that learning a set of scales and biases is sufficient to convert a\npretrained network to perform well on qualitatively different problems (e.g.\nconverting a Single Shot MultiBox Detection (SSD) model into a 1000-class image\nclassification model while reusing 98% of parameters of the SSD feature\nextractor). Similarly, we show that re-learning existing low-parameter layers\n(such as depth-wise convolutions) while keeping the rest of the network frozen\nalso improves transfer-learning accuracy significantly. Our approach allows\nboth simultaneous (multi-task) as well as sequential transfer learning. In\nseveral multi-task learning problems, despite using much fewer parameters than\ntraditional logits-only fine-tuning, we match single-task performance. \n\n"}
{"id": "1810.10939", "contents": "Title: Evading classifiers in discrete domains with provable optimality\n  guarantees Abstract: Machine-learning models for security-critical applications such as bot,\nmalware, or spam detection, operate in constrained discrete domains. These\napplications would benefit from having provable guarantees against adversarial\nexamples. The existing literature on provable adversarial robustness of models,\nhowever, exclusively focuses on robustness to gradient-based attacks in domains\nsuch as images. These attacks model the adversarial cost, e.g., amount of\ndistortion applied to an image, as a $p$-norm. We argue that this approach is\nnot well-suited to model adversarial costs in constrained domains where not all\nexamples are feasible.\n  We introduce a graphical framework that (1) generalizes existing attacks in\ndiscrete domains, (2) can accommodate complex cost functions beyond $p$-norms,\nincluding financial cost incurred when attacking a classifier, and (3)\nefficiently produces valid adversarial examples with guarantees of minimal\nadversarial cost. These guarantees directly translate into a notion of\nadversarial robustness that takes into account domain constraints and the\nadversary's capabilities. We show how our framework can be used to evaluate\nsecurity by crafting adversarial examples that evade a Twitter-bot detection\nclassifier with provably minimal number of changes; and to build privacy\ndefenses by crafting adversarial examples that evade a privacy-invasive\nwebsite-fingerprinting classifier. \n\n"}
{"id": "1810.11347", "contents": "Title: Generating equilibrium molecules with deep neural networks Abstract: Discovery of atomistic systems with desirable properties is a major challenge\nin chemistry and material science. Here we introduce a novel, autoregressive,\nconvolutional deep neural network architecture that generates molecular\nequilibrium structures by sequentially placing atoms in three-dimensional\nspace. The model estimates the joint probability over molecular configurations\nwith tractable conditional probabilities which only depend on distances between\natoms and their nuclear charges. It combines concepts from state-of-the-art\natomistic neural networks with auto-regressive generative models for images and\nspeech. We demonstrate that the architecture is capable of generating molecules\nclose to equilibrium for constitutional isomers of C$_7$O$_2$H$_{10}$. \n\n"}
{"id": "1810.11367", "contents": "Title: LAMVI-2: A Visual Tool for Comparing and Tuning Word Embedding Models Abstract: Tuning machine learning models, particularly deep learning architectures, is\na complex process. Automated hyperparameter tuning algorithms often depend on\nspecific optimization metrics. However, in many situations, a developer trades\none metric against another: accuracy versus overfitting, precision versus\nrecall, smaller models and accuracy, etc. With deep learning, not only are the\nmodel's representations opaque, the model's behavior when parameters \"knobs\"\nare changed may also be unpredictable. Thus, picking the \"best\" model often\nrequires time-consuming model comparison. In this work, we introduce LAMVI-2, a\nvisual analytics system to support a developer in comparing hyperparameter\nsettings and outcomes. By focusing on word-embedding models (\"deep learning for\ntext\") we integrate views to compare both high-level statistics as well as\ninternal model behaviors (e.g., comparing word 'distances'). We demonstrate how\ndevelopers can work with LAMVI-2 to more quickly and accurately narrow down an\nappropriate and effective application-specific model. \n\n"}
{"id": "1810.11509", "contents": "Title: Neural Network-Based Approach to Phase Space Integration Abstract: Monte Carlo methods are widely used in particle physics to integrate and\nsample probability distributions (differential cross sections or decay rates)\non multi-dimensional phase spaces. We present a Neural Network (NN) algorithm\noptimized to perform this task. The algorithm has been applied to several\nexamples of direct relevance for particle physics, including situations with\nnon-trivial features such as sharp resonances and soft/collinear enhancements.\nExcellent performance has been demonstrated in all examples, with the properly\ntrained NN achieving unweighting efficiencies of between 30% and 75%. In\ncontrast to traditional Monte Carlo algorithms such as VEGAS, the NN-based\napproach does not require that the phase space coordinates be aligned with\nresonant or other features in the cross section. \n\n"}
{"id": "1810.11693", "contents": "Title: Stein Variational Gradient Descent as Moment Matching Abstract: Stein variational gradient descent (SVGD) is a non-parametric inference\nalgorithm that evolves a set of particles to fit a given distribution of\ninterest. We analyze the non-asymptotic properties of SVGD, showing that there\nexists a set of functions, which we call the Stein matching set, whose\nexpectations are exactly estimated by any set of particles that satisfies the\nfixed point equation of SVGD. This set is the image of Stein operator applied\non the feature maps of the positive definite kernel used in SVGD. Our results\nprovide a theoretical framework for analyzing the properties of SVGD with\ndifferent kernels, shedding insight into optimal kernel choice. In particular,\nwe show that SVGD with linear kernels yields exact estimation of means and\nvariances on Gaussian distributions, while random Fourier features enable\nprobabilistic bounds for distributional approximation. Our results offer a\nrefreshing view of the classical inference problem as fitting Stein's identity\nor solving the Stein equation, which may motivate more efficient algorithms. \n\n"}
{"id": "1810.11755", "contents": "Title: Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo\n  Tree Search Abstract: Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many\nchallenging benchmarks (e.g., Computer Go). However, they generally require a\nlarge number of rollouts, making their applications costly. Furthermore, it is\nalso extremely challenging to parallelize MCTS due to its inherent sequential\nnature: each rollout heavily relies on the statistics (e.g., node visitation\ncounts) estimated from previous simulations to achieve an effective\nexploration-exploitation tradeoff. In spite of these difficulties, we develop\nan algorithm, WU-UCT, to effectively parallelize MCTS, which achieves linear\nspeedup and exhibits only limited performance loss with an increasing number of\nworkers. The key idea in WU-UCT is a set of statistics that we introduce to\ntrack the number of on-going yet incomplete simulation queries (named as\nunobserved samples). These statistics are used to modify the UCT tree policy in\nthe selection steps in a principled manner to retain effective\nexploration-exploitation tradeoff when we parallelize the most time-consuming\nexpansion and simulation steps. Experiments on a proprietary benchmark and the\nAtari Game benchmark demonstrate the linear speedup and the superior\nperformance of WU-UCT comparing to existing techniques. \n\n"}
{"id": "1810.11758", "contents": "Title: Distributive Dynamic Spectrum Access through Deep Reinforcement\n  Learning: A Reservoir Computing Based Approach Abstract: Dynamic spectrum access (DSA) is regarded as an effective and efficient\ntechnology to share radio spectrum among different networks. As a secondary\nuser (SU), a DSA device will face two critical problems: avoiding causing\nharmful interference to primary users (PUs), and conducting effective\ninterference coordination with other secondary users. These two problems become\neven more challenging for a distributed DSA network where there is no\ncentralized controllers for SUs. In this paper, we investigate communication\nstrategies of a distributive DSA network under the presence of spectrum sensing\nerrors. To be specific, we apply the powerful machine learning tool, deep\nreinforcement learning (DRL), for SUs to learn \"appropriate\" spectrum access\nstrategies in a distributed fashion assuming NO knowledge of the underlying\nsystem statistics. Furthermore, a special type of recurrent neural network\n(RNN), called the reservoir computing (RC), is utilized to realize DRL by\ntaking advantage of the underlying temporal correlation of the DSA network.\nUsing the introduced machine learning-based strategy, SUs could make spectrum\naccess decisions distributedly relying only on their own current and past\nspectrum sensing outcomes. Through extensive experiments, our results suggest\nthat the RC-based spectrum access strategy can help the SU to significantly\nreduce the chances of collision with PUs and other SUs. We also show that our\nscheme outperforms the myopic method which assumes the knowledge of system\nstatistics, and converges faster than the Q-learning method when the number of\nchannels is large. \n\n"}
{"id": "1810.11874", "contents": "Title: Learning with Bad Training Data via Iterative Trimmed Loss Minimization Abstract: In this paper, we study a simple and generic framework to tackle the problem\nof learning model parameters when a fraction of the training samples are\ncorrupted. We first make a simple observation: in a variety of such settings,\nthe evolution of training accuracy (as a function of training epochs) is\ndifferent for clean and bad samples. Based on this we propose to iteratively\nminimize the trimmed loss, by alternating between (a) selecting samples with\nlowest current loss, and (b) retraining a model on only these samples. We prove\nthat this process recovers the ground truth (with linear convergence rate) in\ngeneralized linear models with standard statistical assumptions.\nExperimentally, we demonstrate its effectiveness in three settings: (a) deep\nimage classifiers with errors only in labels, (b) generative adversarial\nnetworks with bad training images, and (c) deep image classifiers with\nadversarial (image, label) pairs (i.e., backdoor attacks). For the well-studied\nsetting of random label noise, our algorithm achieves state-of-the-art\nperformance without having access to any a-priori guaranteed clean samples. \n\n"}
{"id": "1810.12165", "contents": "Title: Median activation functions for graph neural networks Abstract: Graph neural networks (GNNs) have been shown to replicate convolutional\nneural networks' (CNNs) superior performance in many problems involving graphs.\nBy replacing regular convolutions with linear shift-invariant graph filters\n(LSI-GFs), GNNs take into account the (irregular) structure of the graph and\nprovide meaningful representations of network data. However, LSI-GFs fail to\nencode local nonlinear graph signal behavior, and so do regular activation\nfunctions, which are nonlinear but pointwise. To address this issue, we propose\nmedian activation functions with support on graph neighborhoods instead of\nindividual nodes. A GNN architecture with a trainable multirresolution version\nof this activation function is then tested on synthetic and real-word datasets,\nwhere we show that median activation functions can improve GNN capacity with\nmarginal increase in complexity. \n\n"}
{"id": "1810.12263", "contents": "Title: Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization\n  Bounds Abstract: Gaussian Processes (GPs) are a generic modelling tool for supervised\nlearning. While they have been successfully applied on large datasets, their\nuse in safety-critical applications is hindered by the lack of good performance\nguarantees. To this end, we propose a method to learn GPs and their sparse\napproximations by directly optimizing a PAC-Bayesian bound on their\ngeneralization performance, instead of maximizing the marginal likelihood.\nBesides its theoretical appeal, we find in our evaluation that our learning\nmethod is robust and yields significantly better generalization guarantees than\nother common GP approaches on several regression benchmark datasets. \n\n"}
{"id": "1810.12757", "contents": "Title: Scaling Speech Enhancement in Unseen Environments with Noise Embeddings Abstract: We address the problem of speech enhancement generalisation to unseen\nenvironments by performing two manipulations. First, we embed an additional\nrecording from the environment alone, and use this embedding to alter\nactivations in the main enhancement subnetwork. Second, we scale the number of\nnoise environments present at training time to 16,784 different environments.\nExperiment results show that both manipulations reduce word error rates of a\npretrained speech recognition system and improve enhancement quality according\nto a number of performance measures. Specifically, our best model reduces the\nword error rate from 34.04% on noisy speech to 15.46% on the enhanced speech.\nEnhanced audio samples can be found in\nhttps://speechenhancement.page.link/samples. \n\n"}
{"id": "1810.13044", "contents": "Title: Scalable Laplacian K-modes Abstract: We advocate Laplacian K-modes for joint clustering and density mode finding,\nand propose a concave-convex relaxation of the problem, which yields a parallel\nalgorithm that scales up to large datasets and high dimensions. We optimize a\ntight bound (auxiliary function) of our relaxation, which, at each iteration,\namounts to computing an independent update for each cluster-assignment\nvariable, with guaranteed convergence. Therefore, our bound optimizer can be\ntrivially distributed for large-scale data sets. Furthermore, we show that the\ndensity modes can be obtained as byproducts of the assignment variables via\nsimple maximum-value operations whose additional computational cost is linear\nin the number of data points. Our formulation does not need storing a full\naffinity matrix and computing its eigenvalue decomposition, neither does it\nperform expensive projection steps and Lagrangian-dual inner iterates for the\nsimplex constraints of each point. Furthermore, unlike mean-shift, our\ndensity-mode estimation does not require inner-loop gradient-ascent iterates.\nIt has a complexity independent of feature-space dimension, yields modes that\nare valid data points in the input set and is applicable to discrete domains as\nwell as arbitrary kernels. We report comprehensive experiments over various\ndata sets, which show that our algorithm yields very competitive performances\nin term of optimization quality (i.e., the value of the discrete-variable\nobjective at convergence) and clustering accuracy. \n\n"}
{"id": "1810.13317", "contents": "Title: Contrastive Multivariate Singular Spectrum Analysis Abstract: We introduce Contrastive Multivariate Singular Spectrum Analysis, a novel\nunsupervised method for dimensionality reduction and signal decomposition of\ntime series data. By utilizing an appropriate background dataset, the method\ntransforms a target time series dataset in a way that evinces the sub-signals\nthat are enhanced in the target dataset, as opposed to only those that account\nfor the greatest variance. This shifts the goal from finding signals that\nexplain the most variance to signals that matter the most to the analyst. We\ndemonstrate our method on an illustrative synthetic example, as well as show\nthe utility of our method in the downstream clustering of electrocardiogram\nsignals from the public MHEALTH dataset. \n\n"}
{"id": "1811.00075", "contents": "Title: The UEA multivariate time series classification archive, 2018 Abstract: In 2002, the UCR time series classification archive was first released with\nsixteen datasets. It gradually expanded, until 2015 when it increased in size\nfrom 45 datasets to 85 datasets. In October 2018 more datasets were added,\nbringing the total to 128. The new archive contains a wide range of problems,\nincluding variable length series, but it still only contains univariate time\nseries classification problems. One of the motivations for introducing the\narchive was to encourage researchers to perform a more rigorous evaluation of\nnewly proposed time series classification (TSC) algorithms. It has worked: most\nrecent research into TSC uses all 85 datasets to evaluate algorithmic advances.\nResearch into multivariate time series classification, where more than one\nseries are associated with each class label, is in a position where univariate\nTSC research was a decade ago. Algorithms are evaluated using very few datasets\nand claims of improvement are not based on statistical comparisons. We aim to\naddress this problem by forming the first iteration of the MTSC archive, to be\nhosted at the website www.timeseriesclassification.com. Like the univariate\narchive, this formulation was a collaborative effort between researchers at the\nUniversity of East Anglia (UEA) and the University of California, Riverside\n(UCR). The 2018 vintage consists of 30 datasets with a wide range of cases,\ndimensions and series lengths. For this first iteration of the archive we\nformat all data to be of equal length, include no series with missing data and\nprovide train/test splits. \n\n"}
{"id": "1811.00115", "contents": "Title: Dimensionality Reduction has Quantifiable Imperfections: Two Geometric\n  Bounds Abstract: In this paper, we investigate Dimensionality reduction (DR) maps in an\ninformation retrieval setting from a quantitative topology point of view. In\nparticular, we show that no DR maps can achieve perfect precision and perfect\nrecall simultaneously. Thus a continuous DR map must have imperfect precision.\nWe further prove an upper bound on the precision of Lipschitz continuous DR\nmaps. While precision is a natural measure in an information retrieval setting,\nit does not measure `how' wrong the retrieved data is. We therefore propose a\nnew measure based on Wasserstein distance that comes with similar theoretical\nguarantee. A key technical step in our proofs is a particular optimization\nproblem of the $L_2$-Wasserstein distance over a constrained set of\ndistributions. We provide a complete solution to this optimization problem,\nwhich can be of independent interest on the technical side. \n\n"}
{"id": "1811.00121", "contents": "Title: A Mixture Model Based Defense for Data Poisoning Attacks Against Naive\n  Bayes Spam Filters Abstract: Naive Bayes spam filters are highly susceptible to data poisoning attacks.\nHere, known spam sources/blacklisted IPs exploit the fact that their received\nemails will be treated as (ground truth) labeled spam examples, and used for\nclassifier training (or re-training). The attacking source thus generates\nemails that will skew the spam model, potentially resulting in great\ndegradation in classifier accuracy. Such attacks are successful mainly because\nof the poor representation power of the naive Bayes (NB) model, with only a\nsingle (component) density to represent spam (plus a possible attack). We\npropose a defense based on the use of a mixture of NB models. We demonstrate\nthat the learned mixture almost completely isolates the attack in a second NB\ncomponent, with the original spam component essentially unchanged by the\nattack. Our approach addresses both the scenario where the classifier is being\nre-trained in light of new data and, significantly, the more challenging\nscenario where the attack is embedded in the original spam training set. Even\nfor weak attack strengths, BIC-based model order selection chooses a\ntwo-component solution, which invokes the mixture-based defense. Promising\nresults are presented on the TREC 2005 spam corpus. \n\n"}
{"id": "1811.00539", "contents": "Title: Deep Structured Prediction with Nonlinear Output Transformations Abstract: Deep structured models are widely used for tasks like semantic segmentation,\nwhere explicit correlations between variables provide important prior\ninformation which generally helps to reduce the data needs of deep nets.\nHowever, current deep structured models are restricted by oftentimes very local\nneighborhood structure, which cannot be increased for computational complexity\nreasons, and by the fact that the output configuration, or a representation\nthereof, cannot be transformed further. Very recent approaches which address\nthose issues include graphical model inference inside deep nets so as to permit\nsubsequent non-linear output space transformations. However, optimization of\nthose formulations is challenging and not well understood. Here, we develop a\nnovel model which generalizes existing approaches, such as structured\nprediction energy networks, and discuss a formulation which maintains\napplicability of existing inference techniques. \n\n"}
{"id": "1811.01506", "contents": "Title: Theoretical and Experimental Analysis on the Generalizability of\n  Distribution Regression Network Abstract: There is emerging interest in performing regression between distributions. In\ncontrast to prediction on single instances, these machine learning methods can\nbe useful for population-based studies or on problems that are inherently\nstatistical in nature. The recently proposed distribution regression network\n(DRN) has shown superior performance for the distribution-to-distribution\nregression task compared to conventional neural networks. However, in Kou et\nal. (2018) and some other works on distribution regression, there is a lack of\ncomprehensive comparative study on both theoretical basis and generalization\nabilities of the methods. We derive some mathematical properties of DRN and\nqualitatively compare it to conventional neural networks. We also perform\ncomprehensive experiments to study the generalizability of distribution\nregression models, by studying their robustness to limited training data, data\nsampling noise and task difficulty. DRN consistently outperforms conventional\nneural networks, requiring fewer training data and maintaining robust\nperformance with noise. Furthermore, the theoretical properties of DRN can be\nused to provide some explanation on the ability of DRN to achieve better\ngeneralization performance than conventional neural networks. \n\n"}
{"id": "1811.01704", "contents": "Title: ReLeQ: A Reinforcement Learning Approach for Deep Quantization of Neural\n  Networks Abstract: Deep Neural Networks (DNNs) typically require massive amount of computation\nresource in inference tasks for computer vision applications. Quantization can\nsignificantly reduce DNN computation and storage by decreasing the bitwidth of\nnetwork encodings. Recent research affirms that carefully selecting the\nquantization levels for each layer can preserve the accuracy while pushing the\nbitwidth below eight bits. However, without arduous manual effort, this deep\nquantization can lead to significant accuracy loss, leaving it in a position of\nquestionable utility. As such, deep quantization opens a large hyper-parameter\nspace (bitwidth of the layers), the exploration of which is a major challenge.\nWe propose a systematic approach to tackle this problem, by automating the\nprocess of discovering the quantization levels through an end-to-end deep\nreinforcement learning framework (ReLeQ). We adapt policy optimization methods\nto the problem of quantization, and focus on finding the best design decisions\nin choosing the state and action spaces, network architecture and training\nframework, as well as the tuning of various hyperparamters. We show how ReLeQ\ncan balance speed and quality, and provide an asymmetric general solution for\nquantization of a large variety of deep networks (AlexNet, CIFAR-10, LeNet,\nMobileNet-V1, ResNet-20, SVHN, and VGG-11) that virtually preserves the\naccuracy (=< 0.3% loss) while minimizing the computation and storage cost. With\nthese DNNs, ReLeQ enables conventional hardware to achieve 2.2x speedup over\n8-bit execution. Similarly, a custom DNN accelerator achieves 2.0x speedup and\nenergy reduction compared to 8-bit runs. These encouraging results mark ReLeQ\nas the initial step towards automating the deep quantization of neural\nnetworks. \n\n"}
{"id": "1811.02198", "contents": "Title: Collaborative Filtering with Stability Abstract: Collaborative filtering (CF) is a popular technique in today's recommender\nsystems, and matrix approximation-based CF methods have achieved great success\nin both rating prediction and top-N recommendation tasks. However, real-world\nuser-item rating matrices are typically sparse, incomplete and noisy, which\nintroduce challenges to the algorithm stability of matrix approximation, i.e.,\nsmall changes in the training data may significantly change the models. As a\nresult, existing matrix approximation solutions yield low generalization\nperformance, exhibiting high error variance on the training data, and\nminimizing the training error may not guarantee error reduction on the test\ndata. This paper investigates the algorithm stability problem of matrix\napproximation methods and how to achieve stable collaborative filtering via\nstable matrix approximation. We present a new algorithm design framework, which\n(1) introduces new optimization objectives to guide stable matrix approximation\nalgorithm design, and (2) solves the optimization problem to obtain stable\napproximation solutions with good generalization performance. Experimental\nresults on real-world datasets demonstrate that the proposed method can achieve\nbetter accuracy compared with state-of-the-art matrix approximation methods and\nensemble methods in both rating prediction and top-N recommendation tasks. \n\n"}
{"id": "1811.02540", "contents": "Title: Regret Circuits: Composability of Regret Minimizers Abstract: Regret minimization is a powerful tool for solving large-scale problems; it\nwas recently used in breakthrough results for large-scale extensive-form game\nsolving. This was achieved by composing simplex regret minimizers into an\noverall regret-minimization framework for extensive-form game strategy spaces.\nIn this paper we study the general composability of regret minimizers. We\nderive a calculus for constructing regret minimizers for composite convex sets\nthat are obtained from convexity-preserving operations on simpler convex sets.\nWe show that local regret minimizers for the simpler sets can be combined with\nadditional regret minimizers into an aggregate regret minimizer for the\ncomposite set. As one application, we show that the CFR framework can be\nconstructed easily from our framework. We also show ways to include curtailing\n(constraining) operations into our framework. For one, they enables the\nconstruction of CFR generalization for extensive-form games with general convex\nstrategy constraints that can cut across decision points. \n\n"}
{"id": "1811.02756", "contents": "Title: Bayesian State Estimation for Unobservable Distribution Systems via Deep\n  Learning Abstract: The problem of state estimation for unobservable distribution systems is\nconsidered. A deep learning approach to Bayesian state estimation is proposed\nfor real-time applications. The proposed technique consists of distribution\nlearning of stochastic power injection, a Monte Carlo technique for the\ntraining of a deep neural network for state estimation, and a Bayesian bad-data\ndetection and filtering algorithm. Structural characteristics of the deep\nneural networks are investigated. Simulations illustrate the accuracy of\nBayesian state estimation for unobservable systems and demonstrate the benefit\nof employing a deep neural network. Numerical results show the robustness of\nBayesian state estimation against modeling and estimation errors and the\npresence of bad and missing data. Comparing with pseudo-measurement techniques,\ndirect Bayesian state estimation via deep learning neural network outperforms\nexisting benchmarks. \n\n"}
{"id": "1811.03233", "contents": "Title: Knowledge Transfer via Distillation of Activation Boundaries Formed by\n  Hidden Neurons Abstract: An activation boundary for a neuron refers to a separating hyperplane that\ndetermines whether the neuron is activated or deactivated. It has been long\nconsidered in neural networks that the activations of neurons, rather than\ntheir exact output values, play the most important role in forming\nclassification friendly partitions of the hidden feature space. However, as far\nas we know, this aspect of neural networks has not been considered in the\nliterature of knowledge transfer. In this paper, we propose a knowledge\ntransfer method via distillation of activation boundaries formed by hidden\nneurons. For the distillation, we propose an activation transfer loss that has\nthe minimum value when the boundaries generated by the student coincide with\nthose by the teacher. Since the activation transfer loss is not differentiable,\nwe design a piecewise differentiable loss approximating the activation transfer\nloss. By the proposed method, the student learns a separating boundary between\nactivation region and deactivation region formed by each neuron in the teacher.\nThrough the experiments in various aspects of knowledge transfer, it is\nverified that the proposed method outperforms the current state-of-the-art. \n\n"}
{"id": "1811.03392", "contents": "Title: Transformative Machine Learning Abstract: The key to success in machine learning (ML) is the use of effective data\nrepresentations. Traditionally, data representations were hand-crafted.\nRecently it has been demonstrated that, given sufficient data, deep neural\nnetworks can learn effective implicit representations from simple input\nrepresentations. However, for most scientific problems, the use of deep\nlearning is not appropriate as the amount of available data is limited, and/or\nthe output models must be explainable. Nevertheless, many scientific problems\ndo have significant amounts of data available on related tasks, which makes\nthem amenable to multi-task learning, i.e. learning many related problems\nsimultaneously. Here we propose a novel and general representation learning\napproach for multi-task learning that works successfully with small amounts of\ndata. The fundamental new idea is to transform an input intrinsic data\nrepresentation (i.e., handcrafted features), to an extrinsic representation\nbased on what a pre-trained set of models predict about the examples. This\ntransformation has the dual advantages of producing significantly more accurate\npredictions, and providing explainable models. To demonstrate the utility of\nthis transformative learning approach, we have applied it to three real-world\nscientific problems: drug-design (quantitative structure activity relationship\nlearning), predicting human gene expression (across different tissue types and\ndrug treatments), and meta-learning for machine learning (predicting which\nmachine learning methods work best for a given problem). In all three problems,\ntransformative machine learning significantly outperforms the best intrinsic\nrepresentation. \n\n"}
{"id": "1811.04911", "contents": "Title: Boosting Model Performance through Differentially Private Model\n  Aggregation Abstract: A key factor in developing high performing machine learning models is the\navailability of sufficiently large datasets. This work is motivated by\napplications arising in Software as a Service (SaaS) companies where there\nexist numerous similar yet disjoint datasets from multiple client companies. To\novercome the challenges of insufficient data without explicitly aggregating the\nclients' datasets due to privacy concerns, one solution is to collect more data\nfor each individual client, another is to privately aggregate information from\nmodels trained on each client's data. In this work, two approaches for private\nmodel aggregation are proposed that enable the transfer of knowledge from\nexisting models trained on other companies' datasets to a new company with\nlimited labeled data while protecting each client company's underlying\nindividual sensitive information. The two proposed approaches are based on\nstate-of-the-art private learning algorithms: Differentially Private\nPermutation-based Stochastic Gradient Descent and Approximate Minima\nPerturbation. We empirically show that by leveraging differentially private\ntechniques, we can enable private model aggregation and augment data utility\nwhile providing provable mathematical guarantees on privacy. The proposed\nmethods thus provide significant business value for SaaS companies and their\nclients, specifically as a solution for the cold-start problem. \n\n"}
{"id": "1811.05095", "contents": "Title: A Local Regret in Nonconvex Online Learning Abstract: We consider an online learning process to forecast a sequence of outcomes for\nnonconvex models. A typical measure to evaluate online learning algorithms is\nregret but such standard definition of regret is intractable for nonconvex\nmodels even in offline settings. Hence, gradient based definition of regrets\nare common for both offline and online nonconvex problems. Recently, a notion\nof local gradient based regret was introduced. Inspired by the concept of\ncalibration and a local gradient based regret, we introduce another definition\nof regret and we discuss why our definition is more interpretable for\nforecasting problems. We also provide bound analysis for our regret under\ncertain assumptions. \n\n"}
{"id": "1811.05544", "contents": "Title: An Introductory Survey on Attention Mechanisms in NLP Problems Abstract: First derived from human intuition, later adapted to machine translation for\nautomatic token alignment, attention mechanism, a simple method that can be\nused for encoding sequence data based on the importance score each element is\nassigned, has been widely applied to and attained significant improvement in\nvarious tasks in natural language processing, including sentiment\nclassification, text summarization, question answering, dependency parsing,\netc. In this paper, we survey through recent works and conduct an introductory\nsummary of the attention mechanism in different NLP problems, aiming to provide\nour readers with basic knowledge on this widely used method, discuss its\ndifferent variants for different tasks, explore its association with other\ntechniques in machine learning, and examine methods for evaluating its\nperformance. \n\n"}
{"id": "1811.05927", "contents": "Title: Improvements on SCORE, Especially for Weak Signals Abstract: A network may have weak signals and severe degree heterogeneity, and may be\nvery sparse in one occurrence but very dense in another. SCORE (Jin, 2015) is a\nrecent approach to network community detection. It accommodates severe degree\nheterogeneity and is adaptive to different levels of sparsity, but its\nperformance for networks with weak signals is unclear. In this paper, we show\nthat in a broad class of network settings where we allow for weak signals,\nsevere degree heterogeneity, and a wide range of network sparsity, SCORE\nachieves prefect clustering and has the so-called \"exponential rate\" in Hamming\nclustering errors. The proof uses the most recent advancement on entry-wise\nbounds for the leading eigenvectors of the network adjacency matrix.\n  The theoretical analysis assures us that SCORE continues to work well in the\nweak signal settings, but it does not rule out the possibility that SCORE may\nbe further improved to have better performance in real applications, especially\nfor networks with weak signals. As a second contribution of the paper, we\npropose SCORE+ as an improved version of SCORE. We investigate SCORE+ with 8\nnetwork data sets and found that it outperforms several representative\napproaches. In particular, for the 6 data sets with relatively strong signals,\nSCORE+ has similar performance as that of SCORE, but for the 2 data sets\n(Simmons, Caltech) with possibly weak signals, SCORE+ has much lower error\nrates. SCORE+ proposes several changes to SCORE. We carefully explain the\nrationale underlying each of these changes, using a mixture of theoretical and\nnumerical study. \n\n"}
{"id": "1811.06067", "contents": "Title: Interpretable deep learning for guided structure-property explorations\n  in photovoltaics Abstract: The performance of an organic photovoltaic device is intricately connected to\nits active layer morphology. This connection between the active layer and\ndevice performance is very expensive to evaluate, either experimentally or\ncomputationally. Hence, designing morphologies to achieve higher performances\nis non-trivial and often intractable. To solve this, we first introduce a deep\nconvolutional neural network (CNN) architecture that can serve as a fast and\nrobust surrogate for the complex structure-property map. Several tests were\nperformed to gain trust in this trained model. Then, we utilize this fast\nframework to perform robust microstructural design to enhance device\nperformance. \n\n"}
{"id": "1811.06094", "contents": "Title: Unsupervised learning with contrastive latent variable models Abstract: In unsupervised learning, dimensionality reduction is an important tool for\ndata exploration and visualization. Because these aims are typically\nopen-ended, it can be useful to frame the problem as looking for patterns that\nare enriched in one dataset relative to another. These pairs of datasets occur\ncommonly, for instance a population of interest vs. control or signal vs.\nsignal free recordings.However, there are few methods that work on sets of data\nas opposed to data points or sequences. Here, we present a probabilistic model\nfor dimensionality reduction to discover signal that is enriched in the target\ndataset relative to the background dataset. The data in these sets do not need\nto be paired or grouped beyond set membership. By using a probabilistic model\nwhere some structure is shared amongst the two datasets and some is unique to\nthe target dataset, we are able to recover interesting structure in the latent\nspace of the target dataset. The method also has the advantages of a\nprobabilistic model, namely that it allows for the incorporation of prior\ninformation, handles missing data, and can be generalized to different\ndistributional assumptions. We describe several possible variations of the\nmodel and demonstrate the application of the technique to de-noising, feature\nselection, and subgroup discovery settings. \n\n"}
{"id": "1811.07465", "contents": "Title: Bayesian Cycle-Consistent Generative Adversarial Networks via\n  Marginalizing Latent Sampling Abstract: Recent techniques built on Generative Adversarial Networks (GANs), such as\nCycle-Consistent GANs, are able to learn mappings among different domains built\nfrom unpaired datasets, through min-max optimization games between generators\nand discriminators. However, it remains challenging to stabilize the training\nprocess and thus cyclic models fall into mode collapse accompanied by the\nsuccess of discriminator. To address this problem, we propose an novel Bayesian\ncyclic model and an integrated cyclic framework for inter-domain mappings. The\nproposed method motivated by Bayesian GAN explores the full posteriors of\ncyclic model via sampling latent variables and optimizes the model with maximum\na posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In\naddition, original CycleGAN cannot generate diversified results. But it is\nfeasible for Bayesian framework to diversify generated images by replacing\nrestricted latent variables in inference process. We evaluate the proposed\nBayesian CycleGAN on multiple benchmark datasets, including Cityscapes, Maps,\nand Monet2photo. The proposed method improve the per-pixel accuracy by 15% for\nthe Cityscapes semantic segmentation task within origin framework and improve\n20% within the proposed integrated framework, showing better resilience to\nimbalance confrontation. The diversified results of Monet2Photo style transfer\nalso demonstrate its superiority over original cyclic model. We provide codes\nfor all of our experiments in https://github.com/ranery/Bayesian-CycleGAN. \n\n"}
{"id": "1811.08413", "contents": "Title: Sampling Can Be Faster Than Optimization Abstract: Optimization algorithms and Monte Carlo sampling algorithms have provided the\ncomputational foundations for the rapid growth in applications of statistical\nmachine learning in recent years. There is, however, limited theoretical\nunderstanding of the relationships between these two kinds of methodology, and\nlimited understanding of relative strengths and weaknesses. Moreover, existing\nresults have been obtained primarily in the setting of convex functions (for\noptimization) and log-concave functions (for sampling). In this setting, where\nlocal properties determine global properties, optimization algorithms are\nunsurprisingly more efficient computationally than sampling algorithms. We\ninstead examine a class of nonconvex objective functions that arise in mixture\nmodeling and multi-stable systems. In this nonconvex setting, we find that the\ncomputational complexity of sampling algorithms scales linearly with the model\ndimension while that of optimization algorithms scales exponentially. \n\n"}
{"id": "1811.08812", "contents": "Title: Adversarial Classifier for Imbalanced Problems Abstract: Adversarial approach has been widely used for data generation in the last few\nyears. However, this approach has not been extensively utilized for classifier\ntraining. In this paper, we propose an adversarial framework for classifier\ntraining that can also handle imbalanced data. Indeed, a network is trained via\nan adversarial approach to give weights to samples of the majority class such\nthat the obtained classification problem becomes more challenging for the\ndiscriminator and thus boosts its classification capability. In addition to the\ngeneral imbalanced classification problems, the proposed method can also be\nused for problems such as graph representation learning in which it is desired\nto discriminate similar nodes from dissimilar nodes. Experimental results on\nimbalanced data classification and on the tasks like graph link prediction show\nthe superiority of the proposed method compared to the state-of-the-art\nmethods. \n\n"}
{"id": "1811.08840", "contents": "Title: Integrating Reinforcement Learning to Self Training for Pulmonary Nodule\n  Segmentation in Chest X-rays Abstract: Machine learning applications in medical imaging are frequently limited by\nthe lack of quality labeled data. In this paper, we explore the self training\nmethod, a form of semi-supervised learning, to address the labeling burden. By\nintegrating reinforcement learning, we were able to expand the application of\nself training to complex segmentation networks without any further human\nannotation. The proposed approach, reinforced self training (ReST), fine tunes\na semantic segmentation networks by introducing a policy network that learns to\ngenerate pseudolabels. We incorporate an expert demonstration network, based on\ninverse reinforcement learning, to enhance clinical validity and convergence of\nthe policy network. The model was tested on a pulmonary nodule segmentation\ntask in chest X-rays and achieved the performance of a standard U-Net while\nusing only 50% of the labeled data, by exploiting unlabeled data. When the same\nnumber of labeled data was used, a moderate to significant cross validation\naccuracy improvement was achieved depending on the absolute number of labels\nused. \n\n"}
{"id": "1811.08963", "contents": "Title: Multivariate Forecasting of Crude Oil Spot Prices using Neural Networks Abstract: Crude oil is a major component in most advanced economies of the world.\nAccurately predicting and understanding the behavior of crude oil prices is\nimportant for economists, analysts, forecasters, and traders, to name a few.\nThe price of crude oil has declined in the past decade and is seeing a phase of\nstability; but will this stability last? This work is an empirical study on how\nmultivariate analysis may be employed to predict crude oil spot prices using\nneural networks. The concept of using neural networks showed promising\npotential. A very simple neural network model was able to perform on par with\nARIMA models - the state-of-the-art model in time-series forecasting. Advanced\nneural network models using larger datasets may be used in the future to extend\nthis proof-of-concept to a full scale framework. \n\n"}
{"id": "1811.09716", "contents": "Title: Robustness via curvature regularization, and vice versa Abstract: State-of-the-art classifiers have been shown to be largely vulnerable to\nadversarial perturbations. One of the most effective strategies to improve\nrobustness is adversarial training. In this paper, we investigate the effect of\nadversarial training on the geometry of the classification landscape and\ndecision boundaries. We show in particular that adversarial training leads to a\nsignificant decrease in the curvature of the loss surface with respect to\ninputs, leading to a drastically more \"linear\" behaviour of the network. Using\na locally quadratic approximation, we provide theoretical evidence on the\nexistence of a strong relation between large robustness and small curvature. To\nfurther show the importance of reduced curvature for improving the robustness,\nwe propose a new regularizer that directly minimizes curvature of the loss\nsurface, and leads to adversarial robustness that is on par with adversarial\ntraining. Besides being a more efficient and principled alternative to\nadversarial training, the proposed regularizer confirms our claims on the\nimportance of exhibiting quasi-linear behavior in the vicinity of data points\nin order to achieve robustness. \n\n"}
{"id": "1811.09747", "contents": "Title: Amortized Bayesian inference for clustering models Abstract: We develop methods for efficient amortized approximate Bayesian inference\nover posterior distributions of probabilistic clustering models, such as\nDirichlet process mixture models. The approach is based on mapping distributed,\nsymmetry-invariant representations of cluster arrangements into conditional\nprobabilities. The method parallelizes easily, yields iid samples from the\napproximate posterior of cluster assignments with the same computational cost\nof a single Gibbs sampler sweep, and can easily be applied to both conjugate\nand non-conjugate models, as training only requires samples from the generative\nmodel. \n\n"}
{"id": "1811.10581", "contents": "Title: HOGWILD!-Gibbs can be PanAccurate Abstract: Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an\naccurate method for estimating probabilities of events on a small number of\nvariables of a graphical model satisfying Dobrushin's\ncondition~\\cite{DeSaOR16}. We investigate whether it can be used to accurately\nestimate expectations of functions of {\\em all the variables} of the model.\nUnder the same condition, we show that the synchronous (sequential) and\nasynchronous Gibbs samplers can be coupled so that the expected Hamming\ndistance between their (multivariate) samples remains bounded by $O(\\tau \\log\nn),$ where $n$ is the number of variables in the graphical model, and $\\tau$ is\na measure of the asynchronicity. A similar bound holds for any constant power\nof the Hamming distance. Hence, the expectation of any function that is\nLipschitz with respect to a power of the Hamming distance, can be estimated\nwith a bias that grows logarithmically in $n$. Going beyond Lipschitz\nfunctions, we consider the bias arising from asynchronicity in estimating the\nexpectation of polynomial functions of all variables in the model. Using recent\nconcentration of measure results, we show that the bias introduced by the\nasynchronicity is of smaller order than the standard deviation of the function\nvalue already present in the true model. We perform experiments on a\nmulti-processor machine to empirically illustrate our theoretical findings. \n\n"}
{"id": "1811.10714", "contents": "Title: Learning Robust Representations for Automatic Target Recognition Abstract: Radio frequency (RF) sensors are used alongside other sensing modalities to\nprovide rich representations of the world. Given the high variability of\ncomplex-valued target responses, RF systems are susceptible to attacks masking\ntrue target characteristics from accurate identification. In this work, we\nevaluate different techniques for building robust classification architectures\nexploiting learned physical structure in received synthetic aperture radar\nsignals of simulated 3D targets. \n\n"}
{"id": "1811.10947", "contents": "Title: Reliable Semi-Supervised Learning when Labels are Missing at Random Abstract: Semi-supervised learning methods are motivated by the availability of large\ndatasets with unlabeled features in addition to labeled data. Unlabeled data\nis, however, not guaranteed to improve classification performance and has in\nfact been reported to impair the performance in certain cases. A fundamental\nsource of error arises from restrictive assumptions about the unlabeled\nfeatures, which result in unreliable classifiers that underestimate their\nprediction error probabilities. In this paper, we develop a semi-supervised\nlearning approach that relaxes such assumptions and is capable of providing\nclassifiers that reliably quantify the label uncertainty. The approach is\napplicable using any generative model with a supervised learning algorithm. We\nillustrate the approach using both handwritten digit and cloth classification\ndata where the labels are missing at random. \n\n"}
{"id": "1811.11212", "contents": "Title: Self-Supervised GANs via Auxiliary Rotation Loss Abstract: Conditional GANs are at the forefront of natural image synthesis. The main\ndrawback of such models is the necessity for labeled data. In this work we\nexploit two popular unsupervised learning techniques, adversarial training and\nself-supervision, and take a step towards bridging the gap between conditional\nand unconditional GANs. In particular, we allow the networks to collaborate on\nthe task of representation learning, while being adversarial with respect to\nthe classic GAN game. The role of self-supervision is to encourage the\ndiscriminator to learn meaningful feature representations which are not\nforgotten during training. We test empirically both the quality of the learned\nimage representations, and the quality of the synthesized images. Under the\nsame conditions, the self-supervised GAN attains a similar performance to\nstate-of-the-art conditional counterparts. Finally, we show that this approach\nto fully unsupervised learning can be scaled to attain an FID of 23.4 on\nunconditional ImageNet generation. \n\n"}
{"id": "1811.11269", "contents": "Title: Generalizing semi-supervised generative adversarial networks to\n  regression using feature contrasting Abstract: In this work, we generalize semi-supervised generative adversarial networks\n(GANs) from classification problems to regression problems. In the last few\nyears, the importance of improving the training of neural networks using\nsemi-supervised training has been demonstrated for classification problems. We\npresent a novel loss function, called feature contrasting, resulting in a\ndiscriminator which can distinguish between fake and real data based on feature\nstatistics. This method avoids potential biases and limitations of alternative\napproaches. The generalization of semi-supervised GANs to the regime of\nregression problems of opens their use to countless applications as well as\nproviding an avenue for a deeper understanding of how GANs function. We first\ndemonstrate the capabilities of semi-supervised regression GANs on a toy\ndataset which allows for a detailed understanding of how they operate in\nvarious circumstances. This toy dataset is used to provide a theoretical basis\nof the semi-supervised regression GAN. We then apply the semi-supervised\nregression GANs to a number of real-world computer vision applications: age\nestimation, driving steering angle prediction, and crowd counting from single\nimages. We perform extensive tests of what accuracy can be achieved with\nsignificantly reduced annotated data. Through the combination of the\ntheoretical example and real-world scenarios, we demonstrate how\nsemi-supervised GANs can be generalized to regression problems. \n\n"}
{"id": "1811.11310", "contents": "Title: Using Attribution to Decode Dataset Bias in Neural Network Models for\n  Chemistry Abstract: Deep neural networks have achieved state of the art accuracy at classifying\nmolecules with respect to whether they bind to specific protein targets. A key\nbreakthrough would occur if these models could reveal the fragment\npharmacophores that are causally involved in binding. Extracting chemical\ndetails of binding from the networks could potentially lead to scientific\ndiscoveries about the mechanisms of drug actions. But doing so requires shining\nlight into the black box that is the trained neural network model, a task that\nhas proved difficult across many domains. Here we show how the binding\nmechanism learned by deep neural network models can be interrogated, using a\nrecently described attribution method. We first work with carefully constructed\nsynthetic datasets, in which the 'fragment logic' of binding is fully known. We\nfind that networks that achieve perfect accuracy on held out test datasets\nstill learn spurious correlations due to biases in the datasets, and we are\nable to exploit this non-robustness to construct adversarial examples that fool\nthe model. The dataset bias makes these models unreliable for accurately\nrevealing information about the mechanisms of protein-ligand binding. In light\nof our findings, we prescribe a test that checks for dataset bias given a\nhypothesis. If the test fails, it indicates that either the model must be\nsimplified or regularized and/or that the training dataset requires\naugmentation. \n\n"}
{"id": "1811.11644", "contents": "Title: WaveletNet: Logarithmic Scale Efficient Convolutional Neural Networks\n  for Edge Devices Abstract: We present a logarithmic-scale efficient convolutional neural network\narchitecture for edge devices, named WaveletNet. Our model is based on the\nwell-known depthwise convolution, and on two new layers, which we introduce in\nthis work: a wavelet convolution and a depthwise fast wavelet transform. By\nbreaking the symmetry in channel dimensions and applying a fast algorithm,\nWaveletNet shrinks the complexity of convolutional blocks by an O(logD/D)\nfactor, where D is the number of channels. Experiments on CIFAR-10 and ImageNet\nclassification show superior and comparable performances of WaveletNet compared\nto state-of-the-art models such as MobileNetV2. \n\n"}
{"id": "1811.12234", "contents": "Title: Machine Learning on Electronic Health Records: Models and Features\n  Usages to predict Medication Non-Adherence Abstract: Adherence can be defined as \"the extent to which patients take their\nmedications as prescribed by their healthcare providers\"[Osterberg and\nBlaschke, 2005]. World Health Organization's reports point out that, in\ndeveloped countries, only about 50% of patients with chronic diseases correctly\nfollow their treatments. This severely compromises the efficiency of long-term\ntherapy and increases the cost of health services. We propose in this paper\ndifferent models of patient drug consumption in breast cancer treatments. The\naim of these different approaches is to predict medication non-adherence while\ngiving insights to doctors of the underlying reasons of these illegitimate\ndrop-outs. Working with oncologists, we show the interest of Machine- Learning\nalgorithms fined tune by the feedback of experts to estimate a risk score of a\npatient's non-adherence and thus improve support throughout their care path. \n\n"}
{"id": "1811.12273", "contents": "Title: On the Transferability of Representations in Neural Networks Between\n  Datasets and Tasks Abstract: Deep networks, composed of multiple layers of hierarchical distributed\nrepresentations, tend to learn low-level features in initial layers and\ntransition to high-level features towards final layers. Paradigms such as\ntransfer learning, multi-task learning, and continual learning leverage this\nnotion of generic hierarchical distributed representations to share knowledge\nacross datasets and tasks. Herein, we study the layer-wise transferability of\nrepresentations in deep networks across a few datasets and tasks and note some\ninteresting empirical observations. \n\n"}
{"id": "1811.12402", "contents": "Title: On the Implicit Assumptions of GANs Abstract: Generative adversarial nets (GANs) have generated a lot of excitement.\nDespite their popularity, they exhibit a number of well-documented issues in\npractice, which apparently contradict theoretical guarantees. A number of\nenlightening papers have pointed out that these issues arise from unjustified\nassumptions that are commonly made, but the message seems to have been lost\namid the optimism of recent years. We believe the identified problems deserve\nmore attention, and highlight the implications on both the properties of GANs\nand the trajectory of research on probabilistic models. We recently proposed an\nalternative method that sidesteps these problems. \n\n"}
{"id": "1811.12500", "contents": "Title: Sequential Embedding Induced Text Clustering, a Non-parametric Bayesian\n  Approach Abstract: Current state-of-the-art nonparametric Bayesian text clustering methods model\ndocuments through multinomial distribution on bags of words. Although these\nmethods can effectively utilize the word burstiness representation of documents\nand achieve decent performance, they do not explore the sequential information\nof text and relationships among synonyms. In this paper, the documents are\nmodeled as the joint of bags of words, sequential features and word embeddings.\nWe proposed Sequential Embedding induced Dirichlet Process Mixture Model\n(SiDPMM) to effectively exploit this joint document representation in text\nclustering. The sequential features are extracted by the encoder-decoder\ncomponent. Word embeddings produced by the continuous-bag-of-words (CBOW) model\nare introduced to handle synonyms. Experimental results demonstrate the\nbenefits of our model in two major aspects: 1) improved performance across\nmultiple diverse text datasets in terms of the normalized mutual information\n(NMI); 2) more accurate inference of ground truth cluster numbers with\nregularization effect on tiny outlier clusters. \n\n"}
{"id": "1811.12629", "contents": "Title: LoAdaBoost: loss-based AdaBoost federated machine learning with reduced\n  computational complexity on IID and non-IID intensive care data Abstract: Intensive care data are valuable for improvement of health care, policy\nmaking and many other purposes. Vast amount of such data are stored in\ndifferent locations, on many different devices and in different data silos.\nSharing data among different sources is a big challenge due to regulatory,\noperational and security reasons. One potential solution is federated machine\nlearning, which is a method that sends machine learning algorithms\nsimultaneously to all data sources, trains models in each source and aggregates\nthe learned models. This strategy allows utilization of valuable data without\nmoving them. One challenge in applying federated machine learning is the\npossibly different distributions of data from diverse sources. To tackle this\nproblem, we proposed an adaptive boosting method named LoAdaBoost that\nincreases the efficiency of federated machine learning. Using intensive care\nunit data from hospitals, we investigated the performance of learning in IID\nand non-IID data distribution scenarios, and showed that the proposed\nLoAdaBoost method achieved higher predictive accuracy with lower computational\ncomplexity than the baseline method. \n\n"}
{"id": "1811.12804", "contents": "Title: Asymmetry Helps: Eigenvalue and Eigenvector Analyses of Asymmetrically\n  Perturbed Low-Rank Matrices Abstract: This paper is concerned with the interplay between statistical asymmetry and\nspectral methods. Suppose we are interested in estimating a rank-1 and\nsymmetric matrix $\\mathbf{M}^{\\star}\\in \\mathbb{R}^{n\\times n}$, yet only a\nrandomly perturbed version $\\mathbf{M}$ is observed. The noise matrix\n$\\mathbf{M}-\\mathbf{M}^{\\star}$ is composed of zero-mean independent (but not\nnecessarily homoscedastic) entries and is, therefore, not symmetric in general.\nThis might arise, for example, when we have two independent samples for each\nentry of $\\mathbf{M}^{\\star}$ and arrange them into an {\\em asymmetric} data\nmatrix $\\mathbf{M}$. The aim is to estimate the leading eigenvalue and\neigenvector of $\\mathbf{M}^{\\star}$. We demonstrate that the leading eigenvalue\nof the data matrix $\\mathbf{M}$ can be $O(\\sqrt{n})$ times more accurate --- up\nto some log factor --- than its (unadjusted) leading singular value in\neigenvalue estimation. Further, the perturbation of any linear form of the\nleading eigenvector of $\\mathbf{M}$ --- say, entrywise eigenvector perturbation\n--- is provably well-controlled. This eigen-decomposition approach is fully\nadaptive to heteroscedasticity of noise without the need of careful bias\ncorrection or any prior knowledge about the noise variance. We also provide\npartial theory for the more general rank-$r$ case. The takeaway message is\nthis: arranging the data samples in an asymmetric manner and performing\neigen-decomposition could sometimes be beneficial. \n\n"}
{"id": "1812.00071", "contents": "Title: Stochastic Gradient MCMC with Repulsive Forces Abstract: We propose a unifying view of two different Bayesian inference algorithms,\nStochastic Gradient Markov Chain Monte Carlo (SG-MCMC) and Stein Variational\nGradient Descent (SVGD), leading to improved and efficient novel sampling\nschemes. We show that SVGD combined with a noise term can be framed as a\nmultiple chain SG-MCMC method. Instead of treating each parallel chain\nindependently from others, our proposed algorithm implements a repulsive force\nbetween particles, avoiding collapse and facilitating a better exploration of\nthe parameter space. We also show how the addition of this noise term is\nnecessary to obtain a valid SG-MCMC sampler, a significant difference with\nSVGD. Experiments with both synthetic distributions and real datasets\nillustrate the benefits of the proposed scheme. \n\n"}
{"id": "1812.00273", "contents": "Title: Cross-Modulation Networks for Few-Shot Learning Abstract: A family of recent successful approaches to few-shot learning relies on\nlearning an embedding space in which predictions are made by computing\nsimilarities between examples. This corresponds to combining information\nbetween support and query examples at a very late stage of the prediction\npipeline. Inspired by this observation, we hypothesize that there may be\nbenefits to combining the information at various levels of abstraction along\nthe pipeline. We present an architecture called Cross-Modulation Networks which\nallows support and query examples to interact throughout the feature extraction\nprocess via a feature-wise modulation mechanism. We adapt the Matching Networks\narchitecture to take advantage of these interactions and show encouraging\ninitial results on miniImageNet in the 5-way, 1-shot setting, where we close\nthe gap with state-of-the-art. \n\n"}
{"id": "1812.00415", "contents": "Title: Feature Selection Based on Unique Relevant Information for Health Data Abstract: Feature selection, which searches for the most representative features in\nobserved data, is critical for health data analysis. Unlike feature extraction,\nsuch as PCA and autoencoder based methods, feature selection preserves\ninterpretability, meaning that the selected features provide direct information\nabout certain health conditions (i.e., the label). Thus, feature selection\nallows domain experts, such as clinicians, to understand the predictions made\nby machine learning based systems, as well as improve their own diagnostic\nskills. Mutual information is often used as a basis for feature selection since\nit measures dependencies between features and labels. In this paper, we\nintroduce a novel mutual information based feature selection (MIBFS) method\ncalled SURI, which boosts features with high unique relevant information. We\ncompare SURI to existing MIBFS methods using 3 different classifiers on 6\npublicly available healthcare data sets. The results indicate that, in addition\nto preserving interpretability, SURI selects more relevant feature subsets\nwhich lead to higher classification performance. More importantly, we explore\nthe dynamics of mutual information on a public low-dimensional health data set\nvia exhaustive search. The results suggest the important role of unique\nrelevant information in feature selection and verify the principles behind\nSURI. \n\n"}
{"id": "1812.01664", "contents": "Title: A Stable Cardinality Distance for Topological Classification Abstract: This work incorporates topological features via persistence diagrams to\nclassify point cloud data arising from materials science. Persistence diagrams\nare multisets summarizing the connectedness and holes of given data. A new\ndistance on the space of persistence diagrams generates relevant input features\nfor a classification algorithm for materials science data. This distance\nmeasures the similarity of persistence diagrams using the cost of matching\npoints and a regularization term corresponding to cardinality differences\nbetween diagrams. Establishing stability properties of this distance provides\ntheoretical justification for the use of the distance in comparisons of such\ndiagrams. The classification scheme succeeds in determining the crystal\nstructure of materials on noisy and sparse data retrieved from synthetic atom\nprobe tomography experiments. \n\n"}
{"id": "1812.01821", "contents": "Title: Regularized Ensembles and Transferability in Adversarial Learning Abstract: Despite the considerable success of convolutional neural networks in a broad\narray of domains, recent research has shown these to be vulnerable to small\nadversarial perturbations, commonly known as adversarial examples. Moreover,\nsuch examples have shown to be remarkably portable, or transferable, from one\nmodel to another, enabling highly successful black-box attacks. We explore this\nissue of transferability and robustness from two dimensions: first, considering\nthe impact of conventional $l_p$ regularization as well as replacing the top\nlayer with a linear support vector machine (SVM), and second, the value of\ncombining regularized models into an ensemble. We show that models trained with\ndifferent regularizers present barriers to transferability, as does partial\ninformation about the models comprising the ensemble. \n\n"}
{"id": "1812.01967", "contents": "Title: Unsupervised Feature Learning Architecture with Multi-clustering\n  Integration RBM Abstract: In this paper, we present a novel unsupervised feature learning architecture,\nwhich consists of a multi-clustering integration module and a variant of RBM\ntermed multi-clustering integration RBM (MIRBM). In the multi-clustering\nintegration module, we apply three unsupervised K-means, affinity propagation\nand spectral clustering algorithms to obtain three different clustering\npartitions (CPs) without any background knowledge or label. Then, an unanimous\nvoting strategy is used to generate a local clustering partition (LCP). The\nnovel MIRBM model is a core feature encoding part of the proposed unsupervised\nfeature learning architecture. The novelty of it is that the LCP as an\nunsupervised guidance is integrated into one step contrastive divergence (CD1)\nlearning to guide the distribution of the hidden layer features. For the\ninstance in the same LCP cluster, the hidden and reconstructed hidden layer\nfeatures of the MIRBM model in the proposed architecture tend to constrict\ntogether in the training process. Meanwhile, each LCP center tends to disperse\nfrom each other as much as possible in the hidden and reconstructed hidden\nlayer during training. The experiments demonstrate that the proposed\nunsupervised feature learning architecture has more powerful feature\nrepresentation and generalization capability than the state-of-the-art graph\nregularized RBM (GraphRBM) for clustering tasks in the Microsoft Research Asia\nMultimedia (MSRA-MM)2.0 dataset. \n\n"}
{"id": "1812.02216", "contents": "Title: Composing Entropic Policies using Divergence Correction Abstract: Composing previously mastered skills to solve novel tasks promises dramatic\nimprovements in the data efficiency of reinforcement learning. Here, we analyze\ntwo recent works composing behaviors represented in the form of action-value\nfunctions and show that they perform poorly in some situations. As part of this\nanalysis, we extend an important generalization of policy improvement to the\nmaximum entropy framework and introduce an algorithm for the practical\nimplementation of successor features in continuous action spaces. Then we\npropose a novel approach which addresses the failure cases of prior work and,\nin principle, recovers the optimal policy during transfer. This method works by\nexplicitly learning the (discounted, future) divergence between base policies.\nWe study this approach in the tabular case and on non-trivial continuous\ncontrol problems with compositional structure and show that it outperforms or\nmatches existing methods across all tasks considered. \n\n"}
{"id": "1812.02262", "contents": "Title: Accurate detection of arbitrary photon statistics Abstract: We report a measurement workflow free of systematic errors consisting of a\nreconfigurable photon-number-resolving detector, custom electronic circuitry,\nand faithful data-processing algorithm. We achieve unprecedentedly accurate\nmeasurement of various photon-number distributions going beyond the number of\ndetection channels with average fidelity 0.998, where the error is contributed\nprimarily by the sources themselves. Mean numbers of photons cover values up to\n20 and faithful autocorrelation measurements range from g^(2) = 0.006 to 2. We\nsuccessfully detect chaotic, classical, non-classical, non-Gaussian, and\nnegative-Wigner-function light. Our results open new paths for optical\ntechnologies by providing full access to the photon-number information. \n\n"}
{"id": "1812.02341", "contents": "Title: Quantifying Generalization in Reinforcement Learning Abstract: In this paper, we investigate the problem of overfitting in deep\nreinforcement learning. Among the most common benchmarks in RL, it is customary\nto use the same environments for both training and testing. This practice\noffers relatively little insight into an agent's ability to generalize. We\naddress this issue by using procedurally generated environments to construct\ndistinct training and test sets. Most notably, we introduce a new environment\ncalled CoinRun, designed as a benchmark for generalization in RL. Using\nCoinRun, we find that agents overfit to surprisingly large training sets. We\nthen show that deeper convolutional architectures improve generalization, as do\nmethods traditionally found in supervised learning, including L2\nregularization, dropout, data augmentation and batch normalization. \n\n"}
{"id": "1812.03288", "contents": "Title: No Peek: A Survey of private distributed deep learning Abstract: We survey distributed deep learning models for training or inference without\naccessing raw data from clients. These methods aim to protect confidential\npatterns in data while still allowing servers to train models. The distributed\ndeep learning methods of federated learning, split learning and large batch\nstochastic gradient descent are compared in addition to private and secure\napproaches of differential privacy, homomorphic encryption, oblivious transfer\nand garbled circuits in the context of neural networks. We study their\nbenefits, limitations and trade-offs with regards to computational resources,\ndata leakage and communication efficiency and also share our anticipated future\ntrends. \n\n"}
{"id": "1812.04439", "contents": "Title: Synergy Effect between Convolutional Neural Networks and the\n  Multiplicity of SMILES for Improvement of Molecular Prediction Abstract: In our study, we demonstrate the synergy effect between convolutional neural\nnetworks and the multiplicity of SMILES. The model we propose, the so-called\nConvolutional Neural Fingerprint (CNF) model, reaches the accuracy of\ntraditional descriptors such as Dragon (Mauri et al. [22]), RDKit (Landrum\n[18]), CDK2 (Willighagen et al. [43]) and PyDescriptor (Masand and Rastija\n[20]). Moreover the CNF model generally performs better than highly fine-tuned\ntraditional descriptors, especially on small data sets, which is of great\ninterest for the chemical field where data sets are generally small due to\nexperimental costs, the availability of molecules or accessibility to private\ndatabases. We evaluate the CNF model along with SMILES augmentation during both\ntraining and testing. To the best of our knowledge, this is the first time that\nsuch a methodology is presented. We show that using the multiplicity of SMILES\nduring training acts as a regulariser and therefore avoids overfitting and can\nbe seen as ensemble learning when considered for testing. \n\n"}
{"id": "1812.05069", "contents": "Title: Recent Advances in Autoencoder-Based Representation Learning Abstract: Learning useful representations with little or no supervision is a key\nchallenge in artificial intelligence. We provide an in-depth review of recent\nadvances in representation learning with a focus on autoencoder-based models.\nTo organize these results we make use of meta-priors believed useful for\ndownstream tasks, such as disentanglement and hierarchical organization of\nfeatures. In particular, we uncover three main mechanisms to enforce such\nproperties, namely (i) regularizing the (approximate or aggregate) posterior\ndistribution, (ii) factorizing the encoding and decoding distribution, or (iii)\nintroducing a structured prior distribution. While there are some promising\nresults, implicit or explicit supervision remains a key enabler and all current\nmethods use strong inductive biases and modeling assumptions. Finally, we\nprovide an analysis of autoencoder-based representation learning through the\nlens of rate-distortion theory and identify a clear tradeoff between the amount\nof prior knowledge available about the downstream tasks, and how useful the\nrepresentation is for this task. \n\n"}
{"id": "1812.05676", "contents": "Title: A Probe Towards Understanding GAN and VAE Models Abstract: This project report compares some known GAN and VAE models proposed prior to\n2017. There has been significant progress after we finished this report. We\nupload this report as an introduction to generative models and provide some\npersonal interpretations supported by empirical evidence. Both generative\nadversarial network models and variational autoencoders have been widely used\nto approximate probability distributions of data sets. Although they both use\nparametrized distributions to approximate the underlying data distribution,\nwhose exact inference is intractable, their behaviors are very different. We\nsummarize our experiment results that compare these two categories of models in\nterms of fidelity and mode collapse. We provide a hypothesis to explain their\ndifferent behaviors and propose a new model based on this hypothesis. We\nfurther tested our proposed model on MNIST dataset and CelebA dataset. \n\n"}
{"id": "1812.05836", "contents": "Title: Rethinking Layer-wise Feature Amounts in Convolutional Neural Network\n  Architectures Abstract: We characterize convolutional neural networks with respect to the relative\namount of features per layer. Using a skew normal distribution as a\nparametrized framework, we investigate the common assumption of monotonously\nincreasing feature-counts with higher layers of architecture designs. Our\nevaluation on models with VGG-type layers on the MNIST, Fashion-MNIST and\nCIFAR-10 image classification benchmarks provides evidence that motivates\nrethinking of our common assumption: architectures that favor larger early\nlayers seem to yield better accuracy. \n\n"}
{"id": "1812.06158", "contents": "Title: Few-shot classification in Named Entity Recognition Task Abstract: For many natural language processing (NLP) tasks the amount of annotated data\nis limited. This urges a need to apply semi-supervised learning techniques,\nsuch as transfer learning or meta-learning. In this work we tackle Named Entity\nRecognition (NER) task using Prototypical Network - a metric learning\ntechnique. It learns intermediate representations of words which cluster well\ninto named entity classes. This property of the model allows classifying words\nwith extremely limited number of training examples, and can potentially be used\nas a zero-shot learning method. By coupling this technique with transfer\nlearning we achieve well-performing classifiers trained on only 20 instances of\na target class. \n\n"}
{"id": "1812.06597", "contents": "Title: Learning Student Networks via Feature Embedding Abstract: Deep convolutional neural networks have been widely used in numerous\napplications, but their demanding storage and computational resource\nrequirements prevent their applications on mobile devices. Knowledge\ndistillation aims to optimize a portable student network by taking the\nknowledge from a well-trained heavy teacher network. Traditional\nteacher-student based methods used to rely on additional fully-connected layers\nto bridge intermediate layers of teacher and student networks, which brings in\na large number of auxiliary parameters. In contrast, this paper aims to\npropagate information from teacher to student without introducing new variables\nwhich need to be optimized. We regard the teacher-student paradigm from a new\nperspective of feature embedding. By introducing the locality preserving loss,\nthe student network is encouraged to generate the low-dimensional features\nwhich could inherit intrinsic properties of their corresponding\nhigh-dimensional features from teacher network. The resulting portable network\nthus can naturally maintain the performance as that of the teacher network.\nTheoretical analysis is provided to justify the lower computation complexity of\nthe proposed method. Experiments on benchmark datasets and well-trained\nnetworks suggest that the proposed algorithm is superior to state-of-the-art\nteacher-student learning methods in terms of computational and storage\ncomplexity. \n\n"}
{"id": "1812.06669", "contents": "Title: Learning to Generate Music with BachProp Abstract: As deep learning advances, algorithms of music composition increase in\nperformance. However, most of the successful models are designed for specific\nmusical structures. Here, we present BachProp, an algorithmic composer that can\ngenerate music scores in many styles given sufficient training data. To adapt\nBachProp to a broad range of musical styles, we propose a novel representation\nof music and train a deep network to predict the note transition probabilities\nof a given music corpus. In this paper, new music scores generated by BachProp\nare compared with the original corpora as well as with different network\narchitectures and other related models. We show that BachProp captures\nimportant features of the original datasets better than other models and invite\nthe reader to a qualitative comparison on a large collection of generated\nsongs. \n\n"}
{"id": "1812.06775", "contents": "Title: Variational Autoencoders Pursue PCA Directions (by Accident) Abstract: The Variational Autoencoder (VAE) is a powerful architecture capable of\nrepresentation learning and generative modeling. When it comes to learning\ninterpretable (disentangled) representations, VAE and its variants show\nunparalleled performance. However, the reasons for this are unclear, since a\nvery particular alignment of the latent embedding is needed but the design of\nthe VAE does not encourage it in any explicit way. We address this matter and\noffer the following explanation: the diagonal approximation in the encoder\ntogether with the inherent stochasticity force local orthogonality of the\ndecoder. The local behavior of promoting both reconstruction and orthogonality\nmatches closely how the PCA embedding is chosen. Alongside providing an\nintuitive understanding, we justify the statement with full theoretical\nanalysis as well as with experiments. \n\n"}
{"id": "1812.07035", "contents": "Title: On the Continuity of Rotation Representations in Neural Networks Abstract: In neural networks, it is often desirable to work with various\nrepresentations of the same space. For example, 3D rotations can be represented\nwith quaternions or Euler angles. In this paper, we advance a definition of a\ncontinuous representation, which can be helpful for training deep neural\nnetworks. We relate this to topological concepts such as homeomorphism and\nembedding. We then investigate what are continuous and discontinuous\nrepresentations for 2D, 3D, and n-dimensional rotations. We demonstrate that\nfor 3D rotations, all representations are discontinuous in the real Euclidean\nspaces of four or fewer dimensions. Thus, widely used representations such as\nquaternions and Euler angles are discontinuous and difficult for neural\nnetworks to learn. We show that the 3D rotations have continuous\nrepresentations in 5D and 6D, which are more suitable for learning. We also\npresent continuous representations for the general case of the n-dimensional\nrotation group SO(n). While our main focus is on rotations, we also show that\nour constructions apply to other groups such as the orthogonal group and\nsimilarity transforms. We finally present empirical results, which show that\nour continuous rotation representations outperform discontinuous ones for\nseveral practical problems in graphics and vision, including a simple\nautoencoder sanity test, a rotation estimator for 3D point clouds, and an\ninverse kinematics solver for 3D human poses. \n\n"}
{"id": "1812.07488", "contents": "Title: Solving the Empirical Bayes Normal Means Problem with Correlated Noise Abstract: The Normal Means problem plays a fundamental role in many areas of modern\nhigh-dimensional statistics, both in theory and practice. And the Empirical\nBayes (EB) approach to solving this problem has been shown to be highly\neffective, again both in theory and practice. However, almost all EB treatments\nof the Normal Means problem assume that the observations are independent. In\npractice correlations are ubiquitous in real-world applications, and these\ncorrelations can grossly distort EB estimates. Here, exploiting theory from\nSchwartzman (2010), we develop new EB methods for solving the Normal Means\nproblem that take account of unknown correlations among observations. We\nprovide practical software implementations of these methods, and illustrate\nthem in the context of large-scale multiple testing problems and False\nDiscovery Rate (FDR) control. In realistic numerical experiments our methods\ncompare favorably with other commonly-used multiple testing methods. \n\n"}
{"id": "1812.07504", "contents": "Title: Towards Unsupervised Single-Channel Blind Source Separation using\n  Adversarial Pair Unmix-and-Remix Abstract: Blind single-channel source separation is a long standing signal processing\nchallenge. Many methods were proposed to solve this task utilizing multiple\nsignal priors such as low rank, sparsity, temporal continuity etc. The recent\nadvance of generative adversarial models presented new opportunities in signal\nregression tasks. The power of adversarial training however has not yet been\nrealized for blind source separation tasks. In this work, we propose a novel\nmethod for blind source separation (BSS) using adversarial methods. We rely on\nthe independence of sources for creating adversarial constraints on pairs of\napproximately separated sources, which ensure good separation. Experiments are\ncarried out on image sources validating the good performance of our approach,\nand presenting our method as a promising approach for solving BSS for general\nsignals. \n\n"}
{"id": "1812.08119", "contents": "Title: Adam Induces Implicit Weight Sparsity in Rectifier Neural Networks Abstract: In recent years, deep neural networks (DNNs) have been applied to various\nmachine leaning tasks, including image recognition, speech recognition, and\nmachine translation. However, large DNN models are needed to achieve\nstate-of-the-art performance, exceeding the capabilities of edge devices. Model\nreduction is thus needed for practical use. In this paper, we point out that\ndeep learning automatically induces group sparsity of weights, in which all\nweights connected to an output channel (node) are zero, when training DNNs\nunder the following three conditions: (1) rectified-linear-unit (ReLU)\nactivations, (2) an $L_2$-regularized objective function, and (3) the Adam\noptimizer. Next, we analyze this behavior both theoretically and\nexperimentally, and propose a simple model reduction method: eliminate the zero\nweights after training the DNN. In experiments on MNIST and CIFAR-10 datasets,\nwe demonstrate the sparsity with various training setups. Finally, we show that\nour method can efficiently reduce the model size and performs well relative to\nmethods that use a sparsity-inducing regularizer. \n\n"}
{"id": "1812.08625", "contents": "Title: Deep Theory of Functional Connections: A New Method for Estimating the\n  Solutions of PDEs Abstract: This article presents a new methodology called deep Theory of Functional\nConnections (TFC) that estimates the solutions of partial differential\nequations (PDEs) by combining neural networks with TFC. TFC is used to\ntransform PDEs with boundary conditions into unconstrained optimization\nproblems by embedding the boundary conditions into a \"constrained expression.\"\nIn this work, a neural network is chosen as the free function, and used to\nsolve the now unconstrained optimization problem. The loss function is taken as\nthe square of the residual of the PDE. Then, the neural network is trained in\nan unsupervised manner to solve the unconstrained optimization problem. This\nmethodology has two major differences when compared with popular methods used\nto estimate the solutions of PDEs. First, this methodology does not need to\ndiscretize the domain into a grid, rather, this methodology randomly samples\npoints from the domain during the training phase. Second, after training, this\nmethodology represents a closed form, analytical, differentiable approximation\nof the solution throughout the entire training domain. In contrast, other\npopular methods require interpolation if the estimated solution is desired at\npoints that do not lie on the discretized grid. The deep TFC method for\nestimating the solution of PDEs is demonstrated on four problems with a variety\nof boundary conditions. \n\n"}
{"id": "1812.09041", "contents": "Title: A Multi-task Neural Approach for Emotion Attribution, Classification and\n  Summarization Abstract: Emotional content is a crucial ingredient in user-generated videos. However,\nthe sparsity of emotional expressions in the videos poses an obstacle to visual\nemotion analysis. In this paper, we propose a new neural approach, Bi-stream\nEmotion Attribution-Classification Network (BEAC-Net), to solve three related\nemotion analysis tasks: emotion recognition, emotion attribution, and\nemotion-oriented summarization, in a single integrated framework. BEAC-Net has\ntwo major constituents, an attribution network and a classification network.\nThe attribution network extracts the main emotional segment that classification\nshould focus on in order to mitigate the sparsity issue. The classification\nnetwork utilizes both the extracted segment and the original video in a\nbi-stream architecture. We contribute a new dataset for the emotion attribution\ntask with human-annotated ground-truth labels for emotion segments. Experiments\non two video datasets demonstrate superior performance of the proposed\nframework and the complementary nature of the dual classification streams. \n\n"}
{"id": "1812.09645", "contents": "Title: Mixed Membership Recurrent Neural Networks Abstract: Models for sequential data such as the recurrent neural network (RNN) often\nimplicitly model a sequence as having a fixed time interval between\nobservations and do not account for group-level effects when multiple sequences\nare observed. We propose a model for grouped sequential data based on the RNN\nthat accounts for varying time intervals between observations in a sequence by\nlearning a group-level base parameter to which each sequence can revert. Our\napproach is motivated by the mixed membership framework, and we show how it can\nbe used for dynamic topic modeling in which the distribution on topics (not the\ntopics themselves) are evolving in time. We demonstrate our approach on a\ndataset of 3.4 million online grocery shopping orders made by 206K customers. \n\n"}
{"id": "1812.09720", "contents": "Title: State preparation and tomography of a nanomechanical resonator with fast\n  light pulses Abstract: Pulsed optomechanical measurements enable squeezing, non-classical state\ncreation and backaction-free sensing. We demonstrate pulsed measurement of a\ncryogenic nanomechanical resonator with record precision close to the quantum\nregime. We use these to prepare thermally squeezed and purified conditional\nmechanical states, and to perform full state tomography. These demonstrations\nexploit large photon-phonon coupling in a nanophotonic cavity to reach a\nsingle-pulse imprecision of 9 times the mechanical zero-point amplitude\n$x_\\mathrm{zpf}$. We study the effect of other mechanical modes which limit the\nconditional state width to 58 $x_\\mathrm{zpf}$, and show how decoherence causes\nthe state to grow in time. \n\n"}
{"id": "1812.09916", "contents": "Title: Improving MMD-GAN Training with Repulsive Loss Function Abstract: Generative adversarial nets (GANs) are widely used to learn the data sampling\nprocess and their performance may heavily depend on the loss functions, given a\nlimited computational budget. This study revisits MMD-GAN that uses the maximum\nmean discrepancy (MMD) as the loss function for GAN and makes two\ncontributions. First, we argue that the existing MMD loss function may\ndiscourage the learning of fine details in data as it attempts to contract the\ndiscriminator outputs of real data. To address this issue, we propose a\nrepulsive loss function to actively learn the difference among the real data by\nsimply rearranging the terms in MMD. Second, inspired by the hinge loss, we\npropose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the\nrepulsive loss function. The proposed methods are applied to the unsupervised\nimage generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets.\nResults show that the repulsive loss function significantly improves over the\nMMD loss at no additional computational cost and outperforms other\nrepresentative loss functions. The proposed methods achieve an FID score of\n16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral\nnormalization. \n\n"}
{"id": "1812.10048", "contents": "Title: Parallel Clustering of Single Cell Transcriptomic Data with Split-Merge\n  Sampling on Dirichlet Process Mixtures Abstract: Motivation: With the development of droplet based systems, massive single\ncell transcriptome data has become available, which enables analysis of\ncellular and molecular processes at single cell resolution and is instrumental\nto understanding many biological processes. While state-of-the-art clustering\nmethods have been applied to the data, they face challenges in the following\naspects: (1) the clustering quality still needs to be improved; (2) most models\nneed prior knowledge on number of clusters, which is not always available; (3)\nthere is a demand for faster computational speed. Results: We propose to tackle\nthese challenges with Parallel Split Merge Sampling on Dirichlet Process\nMixture Model (the Para-DPMM model). Unlike classic DPMM methods that perform\nsampling on each single data point, the split merge mechanism samples on the\ncluster level, which significantly improves convergence and optimality of the\nresult. The model is highly parallelized and can utilize the computing power of\nhigh performance computing (HPC) clusters, enabling massive clustering on huge\ndatasets. Experiment results show the model outperforms current widely used\nmodels in both clustering quality and computational speed. Availability: Source\ncode is publicly available on\nhttps://github.com/tiehangd/Para_DPMM/tree/master/Para_DPMM_package \n\n"}
{"id": "1812.11118", "contents": "Title: Reconciling modern machine learning practice and the bias-variance\n  trade-off Abstract: Breakthroughs in machine learning are rapidly changing science and society,\nyet our fundamental understanding of this technology has lagged far behind.\nIndeed, one of the central tenets of the field, the bias-variance trade-off,\nappears to be at odds with the observed behavior of methods used in the modern\nmachine learning practice. The bias-variance trade-off implies that a model\nshould balance under-fitting and over-fitting: rich enough to express\nunderlying structure in data, simple enough to avoid fitting spurious patterns.\nHowever, in the modern practice, very rich models such as neural networks are\ntrained to exactly fit (i.e., interpolate) the data. Classically, such models\nwould be considered over-fit, and yet they often obtain high accuracy on test\ndata. This apparent contradiction has raised questions about the mathematical\nfoundations of machine learning and their relevance to practitioners.\n  In this paper, we reconcile the classical understanding and the modern\npractice within a unified performance curve. This \"double descent\" curve\nsubsumes the textbook U-shaped bias-variance trade-off curve by showing how\nincreasing model capacity beyond the point of interpolation results in improved\nperformance. We provide evidence for the existence and ubiquity of double\ndescent for a wide spectrum of models and datasets, and we posit a mechanism\nfor its emergence. This connection between the performance and the structure of\nmachine learning models delineates the limits of classical analyses, and has\nimplications for both the theory and practice of machine learning. \n\n"}
{"id": "1812.11377", "contents": "Title: Hessian-Aware Zeroth-Order Optimization for Black-Box Adversarial Attack Abstract: Zeroth-order optimization is an important research topic in machine learning.\nIn recent years, it has become a key tool in black-box adversarial attack to\nneural network based image classifiers. However, existing zeroth-order\noptimization algorithms rarely extract second-order information of the model\nfunction. In this paper, we utilize the second-order information of the\nobjective function and propose a novel \\textit{Hessian-aware zeroth-order\nalgorithm} called \\texttt{ZO-HessAware}. Our theoretical result shows that\n\\texttt{ZO-HessAware} has an improved zeroth-order convergence rate and query\ncomplexity under structured Hessian approximation, where we propose a few\napproximation methods for estimating Hessian. Our empirical studies on the\nblack-box adversarial attack problem validate that our algorithm can achieve\nimproved success rates with a lower query complexity. \n\n"}
{"id": "1812.11446", "contents": "Title: Greedy Layerwise Learning Can Scale to ImageNet Abstract: Shallow supervised 1-hidden layer neural networks have a number of favorable\nproperties that make them easier to interpret, analyze, and optimize than their\ndeep counterparts, but lack their representational power. Here we use 1-hidden\nlayer learning problems to sequentially build deep networks layer by layer,\nwhich can inherit properties from shallow networks. Contrary to previous\napproaches using shallow networks, we focus on problems where deep learning is\nreported as critical for success. We thus study CNNs on image classification\ntasks using the large-scale ImageNet dataset and the CIFAR-10 dataset. Using a\nsimple set of ideas for architecture and training we find that solving\nsequential 1-hidden-layer auxiliary problems lead to a CNN that exceeds AlexNet\nperformance on ImageNet. Extending this training methodology to construct\nindividual layers by solving 2-and-3-hidden layer auxiliary problems, we obtain\nan 11-layer network that exceeds several members of the VGG model family on\nImageNet, and can train a VGG-11 model to the same accuracy as end-to-end\nlearning. To our knowledge, this is the first competitive alternative to\nend-to-end training of CNNs that can scale to ImageNet. We illustrate several\ninteresting properties of these models theoretically and conduct a range of\nexperiments to study the properties this training induces on the intermediate\nlayers. \n\n"}
{"id": "1812.11651", "contents": "Title: Multi-player Multi-armed Bandits for Stable Allocation in Heterogeneous\n  Ad-Hoc Networks Abstract: Next generation networks are expected to be ultradense and aim to explore\nspectrum sharing paradigm that allows users to communicate in licensed, shared\nas well as unlicensed spectrum. Such ultra-dense networks will incur\nsignificant signaling load at base stations leading to a negative effect on\nspectrum and energy efficiency. To minimize signaling overhead, an adhoc\napproach is being considered for users communicating in the unlicensed and\nshared spectrums. For such users, decisions need to be completely decentralized\nas: 1) No communication between users and signaling from the base station is\npossible which necessitates independent channel selection at each user. A\ncollision occurs when multiple users transmit simultaneously on the same\nchannel, 2) Channel qualities may be heterogeneous, i.e., they are not same\nacross all users, and moreover, are unknown, and 3) The network could be\ndynamic where users can enter or leave anytime. We develop a multi-armed bandit\nbased distributed algorithm for static networks and extend it for the dynamic\nnetworks. The algorithms aim to achieve stable orthogonal allocation (SOC) in\nfinite time and meet the above three constraints with two novel\ncharacteristics: 1) Low complexity narrowband radio compared to wideband radio\nin existing works, and 2) Epoch-less approach for dynamic networks. We\nestablish a convergence of our algorithms to SOC and validate via extensive\nsimulation experiments. \n\n"}
{"id": "1812.11690", "contents": "Title: Deep Residual Learning in the JPEG Transform Domain Abstract: We introduce a general method of performing Residual Network inference and\nlearning in the JPEG transform domain that allows the network to consume\ncompressed images as input. Our formulation leverages the linearity of the JPEG\ntransform to redefine convolution and batch normalization with a tune-able\nnumerical approximation for ReLu. The result is mathematically equivalent to\nthe spatial domain network up to the ReLu approximation accuracy. A formulation\nfor image classification and a model conversion algorithm for spatial domain\nnetworks are given as examples of the method. We show that the sparsity of the\nJPEG format allows for faster processing of images with little to no penalty in\nthe network accuracy. \n\n"}
{"id": "1812.11806", "contents": "Title: An introduction to domain adaptation and transfer learning Abstract: In machine learning, if the training data is an unbiased sample of an\nunderlying distribution, then the learned classification function will make\naccurate predictions for new samples. However, if the training data is not an\nunbiased sample, then there will be differences between how the training data\nis distributed and how the test data is distributed. Standard classifiers\ncannot cope with changes in data distributions between training and test\nphases, and will not perform well. Domain adaptation and transfer learning are\nsub-fields within machine learning that are concerned with accounting for these\ntypes of changes. Here, we present an introduction to these fields, guided by\nthe question: when and how can a classifier generalize from a source to a\ntarget domain? We will start with a brief introduction into risk minimization,\nand how transfer learning and domain adaptation expand upon this framework.\nFollowing that, we discuss three special cases of data set shift, namely prior,\ncovariate and concept shift. For more complex domain shifts, there are a wide\nvariety of approaches. These are categorized into: importance-weighting,\nsubspace mapping, domain-invariant spaces, feature augmentation, minimax\nestimators and robust algorithms. A number of points will arise, which we will\ndiscuss in the last section. We conclude with the remark that many open\nquestions will have to be addressed before transfer learners and\ndomain-adaptive classifiers become practical. \n\n"}
{"id": "1901.00398", "contents": "Title: Judge the Judges: A Large-Scale Evaluation Study of Neural Language\n  Models for Online Review Generation Abstract: We conduct a large-scale, systematic study to evaluate the existing\nevaluation methods for natural language generation in the context of generating\nonline product reviews. We compare human-based evaluators with a variety of\nautomated evaluation procedures, including discriminative evaluators that\nmeasure how well machine-generated text can be distinguished from human-written\ntext, as well as word overlap metrics that assess how similar the generated\ntext compares to human-written references. We determine to what extent these\ndifferent evaluators agree on the ranking of a dozen of state-of-the-art\ngenerators for online product reviews. We find that human evaluators do not\ncorrelate well with discriminative evaluators, leaving a bigger question of\nwhether adversarial accuracy is the correct objective for natural language\ngeneration. In general, distinguishing machine-generated text is challenging\neven for human evaluators, and human decisions correlate better with lexical\noverlaps. We find lexical diversity an intriguing metric that is indicative of\nthe assessments of different evaluators. A post-experiment survey of\nparticipants provides insights into how to evaluate and improve the quality of\nnatural language generation systems. \n\n"}
{"id": "1901.00861", "contents": "Title: Mapping Informal Settlements in Developing Countries using Machine\n  Learning and Low Resolution Multi-spectral Data Abstract: Informal settlements are home to the most socially and economically\nvulnerable people on the planet. In order to deliver effective economic and\nsocial aid, non-government organizations (NGOs), such as the United Nations\nChildren's Fund (UNICEF), require detailed maps of the locations of informal\nsettlements. However, data regarding informal and formal settlements is\nprimarily unavailable and if available is often incomplete. This is due, in\npart, to the cost and complexity of gathering data on a large scale. To address\nthese challenges, we, in this work, provide three contributions. 1) A brand new\nmachine learning data-set, purposely developed for informal settlement\ndetection. 2) We show that it is possible to detect informal settlements using\nfreely available low-resolution (LR) data, in contrast to previous studies that\nuse very-high resolution (VHR) satellite and aerial imagery, something that is\ncost-prohibitive for NGOs. 3) We demonstrate two effective classification\nschemes on our curated data set, one that is cost-efficient for NGOs and\nanother that is cost-prohibitive for NGOs, but has additional utility. We\nintegrate these schemes into a semi-automated pipeline that converts either a\nLR or VHR satellite image into a binary map that encodes the locations of\ninformal settlements. \n\n"}
{"id": "1901.00862", "contents": "Title: Learning Nonlinear State Space Models with Hamiltonian Sequential Monte\n  Carlo Sampler Abstract: State space models (SSM) have been widely applied for the analysis and\nvisualization of large sequential datasets. Sequential Monte Carlo (SMC) is a\nvery popular particle-based method to sample latent states from intractable\nposteriors. However, SSM is significantly influenced by the choice of the\nproposal. Recently Hamiltonian Monte Carlo (HMC) sampling has shown success in\nmany practical problems. In this paper, we propose an SMC augmented by HMC\n(HSMC) for inference and model learning of nonlinear SSM, which can exempt us\nfrom learning proposals and reduce the model complexity significantly. Based on\nthe measure preserving property of HMC, the particles directly generated by\ntransition function can approximate the posterior of latent states arbitrarily\nwell. In order to better adapt to the local geometry of latent space, the HMC\nis conducted on Riemannian manifold defined by a positive definite metric. In\naddition, we show that the proposed HSMC method can improve SSMs realized by\nboth Gaussian Processes (GP) and Neural Network (NN). \n\n"}
{"id": "1901.01499", "contents": "Title: Understanding the (un)interpretability of natural image distributions\n  using generative models Abstract: Probability density estimation is a classical and well studied problem, but\nstandard density estimation methods have historically lacked the power to model\ncomplex and high-dimensional image distributions. More recent generative models\nleverage the power of neural networks to implicitly learn and represent\nprobability models over complex images. We describe methods to extract explicit\nprobability density estimates from GANs, and explore the properties of these\nimage density functions. We perform sanity check experiments to provide\nevidence that these probabilities are reasonable. However, we also show that\ndensity functions of natural images are difficult to interpret and thus limited\nin use. We study reasons for this lack of interpretability, and show that we\ncan get interpretability back by doing density estimation on latent\nrepresentations of images. \n\n"}
{"id": "1901.01558", "contents": "Title: Self-Expressive Subspace Clustering to Recognize Motion Dynamics of a\n  Multi-Joint Coordination for Chronic Ankle Instability Abstract: Ankle sprains and instability are major public health concerns. Up to 70% of\nindividuals do not fully recover from a single ankle sprain and eventually\ndevelop chronic ankle instability (CAI). The diagnosis of CAI has been mainly\nbased on self-report rather than objective biomechanical measures. The goal of\nthis study is to quantitatively recognize the motion pattern of a multi-joint\ncoordination using biosensor data from bilateral hip, knee, and ankle joints,\nand further distinguish between CAI and healthy cohorts. We propose an analytic\nframework, where a nonlinear subspace clustering method is developed to learn\nthe motion dynamic patterns from an inter-connected network of multiply joints.\nA support vector machine model is trained with a leave-one-subject-out cross\nvalidation to validate the learned measures compared to traditional statistical\nmeasures. The computational results showed >70% classification accuracy on\naverage based on the dataset of 48 subjects (25 with CAI and 23 normal\ncontrols) examined in our designed experiment. It is found that CAI can be\nobserved from other joints (e.g., hips) significantly, which reflects the fact\nthat there are interactions in the multi-joint coordination system. The\ndeveloped method presents a potential to support the decisions with motion\npatterns during diagnosis, treatment, rehabilitation of gait abnormality caused\nby physical injury (e.g., ankle sprains in this study) or even central nervous\nsystem disorders. \n\n"}
{"id": "1901.01631", "contents": "Title: Sharp Restricted Isometry Bounds for the Inexistence of Spurious Local\n  Minima in Nonconvex Matrix Recovery Abstract: Nonconvex matrix recovery is known to contain no spurious local minima under\na restricted isometry property (RIP) with a sufficiently small RIP constant\n$\\delta$. If $\\delta$ is too large, however, then counterexamples containing\nspurious local minima are known to exist. In this paper, we introduce a proof\ntechnique that is capable of establishing sharp thresholds on $\\delta$ to\nguarantee the inexistence of spurious local minima. Using the technique, we\nprove that in the case of a rank-1 ground truth, an RIP constant of\n$\\delta<1/2$ is both necessary and sufficient for exact recovery from any\narbitrary initial point (such as a random point). We also prove a local\nrecovery result: given an initial point $x_{0}$ satisfying\n$f(x_{0})\\le(1-\\delta)^{2}f(0)$, any descent algorithm that converges to\nsecond-order optimality guarantees exact recovery. \n\n"}
{"id": "1901.03006", "contents": "Title: Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud\n  Classifiers Abstract: 3D object classification and segmentation using deep neural networks has been\nextremely successful. As the problem of identifying 3D objects has many\nsafety-critical applications, the neural networks have to be robust against\nadversarial changes to the input data set. There is a growing body of research\non generating human-imperceptible adversarial attacks and defenses against them\nin the 2D image classification domain. However, 3D objects have various\ndifferences with 2D images, and this specific domain has not been rigorously\nstudied so far.\n  We present a preliminary evaluation of adversarial attacks on deep 3D point\ncloud classifiers, namely PointNet and PointNet++, by evaluating both white-box\nand black-box adversarial attacks that were proposed for 2D images and\nextending those attacks to reduce the perceptibility of the perturbations in 3D\nspace. We also show the high effectiveness of simple defenses against those\nattacks by proposing new defenses that exploit the unique structure of 3D point\nclouds. Finally, we attempt to explain the effectiveness of the defenses\nthrough the intrinsic structures of both the point clouds and the neural\nnetwork architectures. Overall, we find that networks that process 3D point\ncloud data are weak to adversarial attacks, but they are also more easily\ndefensible compared to 2D image classifiers. Our investigation will provide the\ngroundwork for future studies on improving the robustness of deep neural\nnetworks that handle 3D data. \n\n"}
{"id": "1901.03678", "contents": "Title: Machine Learning Automation Toolbox (MLaut) Abstract: In this paper we present MLaut (Machine Learning AUtomation Toolbox) for the\npython data science ecosystem. MLaut automates large-scale evaluation and\nbenchmarking of machine learning algorithms on a large number of datasets.\nMLaut provides a high-level workflow interface to machine algorithm algorithms,\nimplements a local back-end to a database of dataset collections, trained\nalgorithms, and experimental results, and provides easy-to-use interfaces to\nthe scikit-learn and keras modelling libraries. Experiments are easy to set up\nwith default settings in a few lines of code, while remaining fully\ncustomizable to the level of hyper-parameter tuning, pipeline composition, or\ndeep learning architecture.\n  As a principal test case for MLaut, we conducted a large-scale supervised\nclassification study in order to benchmark the performance of a number of\nmachine learning algorithms - to our knowledge also the first larger-scale\nstudy on standard supervised learning data sets to include deep learning\nalgorithms. While corroborating a number of previous findings in literature, we\nfound (within the limitations of our study) that deep neural networks do not\nperform well on basic supervised learning, i.e., outside the more specialized,\nimage-, audio-, or text-based tasks. \n\n"}
{"id": "1901.03909", "contents": "Title: Eliminating all bad Local Minima from Loss Landscapes without even\n  adding an Extra Unit Abstract: Recent work has noted that all bad local minima can be removed from neural\nnetwork loss landscapes, by adding a single unit with a particular\nparameterization. We show that the core technique from these papers can be used\nto remove all bad local minima from any loss landscape, so long as the global\nminimum has a loss of zero. This procedure does not require the addition of\nauxiliary units, or even that the loss be associated with a neural network. The\nmethod of action involves all bad local minima being converted into bad\n(non-local) minima at infinity in terms of auxiliary parameters. \n\n"}
{"id": "1901.04195", "contents": "Title: Integrating Learning and Reasoning with Deep Logic Models Abstract: Deep learning is very effective at jointly learning feature representations\nand classification models, especially when dealing with high dimensional input\npatterns. Probabilistic logic reasoning, on the other hand, is capable to take\nconsistent and robust decisions in complex environments. The integration of\ndeep learning and logic reasoning is still an open-research problem and it is\nconsidered to be the key for the development of real intelligent agents. This\npaper presents Deep Logic Models, which are deep graphical models integrating\ndeep learning and logic reasoning both for learning and inference. Deep Logic\nModels create an end-to-end differentiable architecture, where deep learners\nare embedded into a network implementing a continuous relaxation of the logic\nknowledge. The learning process allows to jointly learn the weights of the deep\nlearners and the meta-parameters controlling the high-level reasoning. The\nexperimental results show that the proposed methodology overtakes the\nlimitations of the other approaches that have been proposed to bridge deep\nlearning and reasoning. \n\n"}
{"id": "1901.04420", "contents": "Title: Data Augmentation with Manifold Exploring Geometric Transformations for\n  Increased Performance and Robustness Abstract: In this paper we propose a novel augmentation technique that improves not\nonly the performance of deep neural networks on clean test data, but also\nsignificantly increases their robustness to random transformations, both affine\nand projective. Inspired by ManiFool, the augmentation is performed by a\nline-search manifold-exploration method that learns affine geometric\ntransformations that lead to the misclassification on an image, while ensuring\nthat it remains on the same manifold as the training data.\n  This augmentation method populates any training dataset with images that lie\non the border of the manifolds between two-classes and maximizes the variance\nthe network is exposed to during training. Our method was thoroughly evaluated\non the challenging tasks of fine-grained skin lesion classification from\nlimited data, and breast tumor classification of mammograms. Compared with\ntraditional augmentation methods, and with images synthesized by Generative\nAdversarial Networks our method not only achieves state-of-the-art performance\nbut also significantly improves the network's robustness. \n\n"}
{"id": "1901.04436", "contents": "Title: Bayesian Learning of Neural Network Architectures Abstract: In this paper we propose a Bayesian method for estimating architectural\nparameters of neural networks, namely layer size and network depth. We do this\nby learning concrete distributions over these parameters. Our results show that\nregular networks with a learnt structure can generalise better on small\ndatasets, while fully stochastic networks can be more robust to parameter\ninitialisation. The proposed method relies on standard neural variational\nlearning and, unlike randomised architecture search, does not require a\nretraining of the model, thus keeping the computational overhead at minimum. \n\n"}
{"id": "1901.04530", "contents": "Title: CrossNet: Latent Cross-Consistency for Unpaired Image Translation Abstract: Recent GAN-based architectures have been able to deliver impressive\nperformance on the general task of image-to-image translation. In particular,\nit was shown that a wide variety of image translation operators may be learned\nfrom two image sets, containing images from two different domains, without\nestablishing an explicit pairing between the images. This was made possible by\nintroducing clever regularizers to overcome the under-constrained nature of the\nunpaired translation problem. In this work, we introduce a novel architecture\nfor unpaired image translation, and explore several new regularizers enabled by\nit. Specifically, our architecture comprises a pair of GANs, as well as a pair\nof translators between their respective latent spaces. These cross-translators\nenable us to impose several regularizing constraints on the learnt image\ntranslation operator, collectively referred to as latent cross-consistency. Our\nresults show that our proposed architecture and latent cross-consistency\nconstraints are able to outperform the existing state-of-the-art on a variety\nof image translation tasks. \n\n"}
{"id": "1901.05101", "contents": "Title: ReNeg and Backseat Driver: Learning from Demonstration with Continuous\n  Human Feedback Abstract: In autonomous vehicle (AV) control, allowing mistakes can be quite dangerous\nand costly in the real world. For this reason we investigate methods of\ntraining an AV without allowing the agent to explore and instead having a human\nexplorer collect the data. Supervised learning has been explored for AV\ncontrol, but it encounters the issue of the covariate shift. That is, training\ndata collected from an optimal demonstration consists only of the states\ninduced by the optimal control policy, but at runtime, the trained agent may\nencounter a vastly different state distribution with little relevant training\ndata. To mitigate this issue, we have our human explorer make sub-optimal\ndecisions. In order to have our agent not replicate these sub-optimal\ndecisions, supervised learning requires that we either erase these actions, or\nreplace these action with the correct action. Erasing is wasteful and replacing\nis difficult, since it is not easy to know the correct action without driving.\nWe propose an alternate framework that includes continuous scalar feedback for\neach action, marking which actions we should replicate, which we should avoid,\nand how sure we are. Our framework learns continuous control from sub-optimal\ndemonstration and evaluative feedback collected before training. We find that a\nhuman demonstrator can explore sub-optimal states in a safe manner, while still\ngetting enough gradation to benefit learning. The collection method for data\nand feedback we call \"Backseat Driver.\" We call the more general learning\nframework ReNeg, since it learns a regression from states to actions given\nnegative as well as positive examples. We empirically validate several models\nin the ReNeg framework, testing on lane-following with limited data. We find\nthat the best solution is a generalization of mean-squared error and\noutperforms supervised learning on the positive examples alone. \n\n"}
{"id": "1901.06016", "contents": "Title: Learning formation energy of inorganic compounds using matrix variate\n  deep Gaussian process Abstract: Future advancement of engineering applications is dependent on design of\nnovel materials with desired properties. Enormous size of known chemical space\nnecessitates use of automated high throughput screening to search the desired\nmaterial. The high throughput screening uses quantum chemistry calculations to\npredict material properties, however, computational complexity of these\ncalculations often imposes prohibitively high cost on the search for desired\nmaterial. This critical bottleneck is resolved by using deep machine learning\nto emulate the quantum computations. However, the deep learning algorithms\nrequire a large training dataset to ensure an acceptable generalization, which\nis often unavailable a-priory. In this paper, we propose a deep Gaussian\nprocess based approach to develop an emulator for quantum calculations. We\nfurther propose a novel molecular descriptor that enables implementation of the\nproposed approach. As demonstrated in this paper, the proposed approach can be\nimplemented using a small dataset. We demonstrate efficacy of our approach for\nprediction of formation energy of inorganic molecules. \n\n"}
{"id": "1901.06086", "contents": "Title: WALL-E: An Efficient Reinforcement Learning Research Framework Abstract: There are two halves to RL systems: experience collection time and policy\nlearning time. For a large number of samples in rollouts, experience collection\ntime is the major bottleneck. Thus, it is necessary to speed up the rollout\ngeneration time with multi-process architecture support. Our work, dubbed\nWALL-E, utilizes multiple rollout samplers running in parallel to rapidly\ngenerate experience. Due to our parallel samplers, we experience not only\nfaster convergence times, but also higher average reward thresholds. For\nexample, on the MuJoCo HalfCheetah-v2 task, with $N = 10$ parallel sampler\nprocesses, we are able to achieve much higher average return than those from\nusing only a single process architecture. \n\n"}
{"id": "1901.06576", "contents": "Title: Towards Physically Safe Reinforcement Learning under Supervision Abstract: This paper addresses the question of how a previously available control\npolicy $\\pi_s$ can be used as a supervisor to more quickly and safely train a\nnew learned control policy $\\pi_L$ for a robot. A weighted average of the\nsupervisor and learned policies is used during trials, with a heavier weight\ninitially on the supervisor, in order to allow safe and useful physical trials\nwhile the learned policy is still ineffective. During the process, the weight\nis adjusted to favor the learned policy. As weights are adjusted, the learned\nnetwork must compensate so as to give safe and reasonable outputs under the\ndifferent weights. A pioneer network is introduced that pre-learns a policy\nthat performs similarly to the current learned policy under the planned next\nstep for new weights; this pioneer network then replaces the currently learned\nnetwork in the next set of trials. Experiments in OpenAI Gym demonstrate the\neffectiveness of the proposed method. \n\n"}
{"id": "1901.07017", "contents": "Title: Spatial Broadcast Decoder: A Simple Architecture for Learning\n  Disentangled Representations in VAEs Abstract: We present a simple neural rendering architecture that helps variational\nautoencoders (VAEs) learn disentangled representations. Instead of the\ndeconvolutional network typically used in the decoder of VAEs, we tile\n(broadcast) the latent vector across space, concatenate fixed X- and\nY-\"coordinate\" channels, and apply a fully convolutional network with 1x1\nstride. This provides an architectural prior for dissociating positional from\nnon-positional features in the latent distribution of VAEs, yet without\nproviding any explicit supervision to this effect. We show that this\narchitecture, which we term the Spatial Broadcast decoder, improves\ndisentangling, reconstruction accuracy, and generalization to held-out regions\nin data space. It provides a particularly dramatic benefit when applied to\ndatasets with small objects. We also emphasize a method for visualizing learned\nlatent spaces that helped us diagnose our models and may prove useful for\nothers aiming to assess data representations. Finally, we show the Spatial\nBroadcast Decoder is complementary to state-of-the-art (SOTA) disentangling\ntechniques and when incorporated improves their performance. \n\n"}
{"id": "1901.07132", "contents": "Title: Universal Rules for Fooling Deep Neural Networks based Text\n  Classification Abstract: Recently, deep learning based natural language processing techniques are\nbeing extensively used to deal with spam mail, censorship evaluation in social\nnetworks, among others. However, there is only a couple of works evaluating the\nvulnerabilities of such deep neural networks. Here, we go beyond attacks to\ninvestigate, for the first time, universal rules, i.e., rules that are sample\nagnostic and therefore could turn any text sample in an adversarial one. In\nfact, the universal rules do not use any information from the method itself (no\ninformation from the method, gradient information or training dataset\ninformation is used), making them black-box universal attacks. In other words,\nthe universal rules are sample and method agnostic. By proposing a\ncoevolutionary optimization algorithm we show that it is possible to create\nuniversal rules that can automatically craft imperceptible adversarial samples\n(only less than five perturbations which are close to misspelling are inserted\nin the text sample). A comparison with a random search algorithm further\njustifies the strength of the method. Thus, universal rules for fooling\nnetworks are here shown to exist. Hopefully, the results from this work will\nimpact the development of yet more sample and model agnostic attacks as well as\ntheir defenses, culminating in perhaps a new age for artificial intelligence. \n\n"}
{"id": "1901.07181", "contents": "Title: Time-bin and Polarization Superdense Teleportation for Space\n  Applications Abstract: To build a global quantum communication network, low-transmission,\nfiber-based communication channels can be supplemented by using a free-space\nchannel between a satellite and a ground station on Earth. We have constructed\na system that generates hyperentangled photonic \"ququarts\" and measures them to\nexecute multiple quantum communication protocols of interest. We have\nsuccessfully executed and characterized superdense teleportation, a modified\nremote-state preparation protocol that transfers more quantum information than\nstandard teleportation, for the same classical information cost, and moreover,\nis in principle deterministic. Our measurements show an average fidelity of\n$0.94\\pm0.02$, with a phase resolution of $\\sim7^{\\circ}$, allowing reliable\ntransmission of $>10^5$ distinguishable quantum states. Additionally, we have\ndemonstrated the ability to compensate for the Doppler shift, which would\notherwise prevent sending time-bin encoded states from a rapidly moving\nsatellite, thus allowing the low-error execution of phase-sensitive protocols\nduring an orbital pass. Finally, we show that the estimated number of received\ncoincidence counts in a realistic implementation is sufficient to enable\nfaithful reconstruction of the received state in a single pass. \n\n"}
{"id": "1901.07868", "contents": "Title: Constant Time Graph Neural Networks Abstract: The recent advancements in graph neural networks (GNNs) have led to\nstate-of-the-art performances in various applications, including\nchemo-informatics, question-answering systems, and recommender systems.\nHowever, scaling up these methods to huge graphs, such as social networks and\nWeb graphs, remains a challenge. In particular, the existing methods for\naccelerating GNNs either are not theoretically guaranteed in terms of the\napproximation error or incur at least a linear time computation cost. In this\nstudy, we reveal the query complexity of the uniform node sampling scheme for\nMessage Passing Neural Networks, including GraphSAGE, graph attention networks\n(GATs), and graph convolutional networks (GCNs). Surprisingly, our analysis\nreveals that the complexity of the node sampling method is completely\nindependent of the number of the nodes, edges, and neighbors of the input and\ndepends only on the error tolerance and confidence probability while providing\na theoretical guarantee for the approximation error. To the best of our\nknowledge, this is the first paper to provide a theoretical guarantee of\napproximation for GNNs within constant time. Through experiments with synthetic\nand real-world datasets, we investigated the speed and precision of the node\nsampling scheme and validated our theoretical results. \n\n"}
{"id": "1901.07984", "contents": "Title: Typed Graph Networks Abstract: Recently, the deep learning community has given growing attention to neural\narchitectures engineered to learn problems in relational domains. Convolutional\nNeural Networks employ parameter sharing over the image domain, tying the\nweights of neural connections on a grid topology and thus enforcing the\nlearning of a number of convolutional kernels. By instantiating trainable\nneural modules and assembling them in varied configurations (apart from grids),\none can enforce parameter sharing over graphs, yielding models which can\neffectively be fed with relational data. In this context, vertices in a graph\ncan be projected into a hyperdimensional real space and iteratively refined\nover many message-passing iterations in an end-to-end differentiable\narchitecture. Architectures of this family have been referred to with several\ndefinitions in the literature, such as Graph Neural Networks, Message-passing\nNeural Networks, Relational Networks and Graph Networks. In this paper, we\nrevisit the original Graph Neural Network model and show that it generalises\nmany of the recent models, which in turn benefit from the insight of thinking\nabout vertex \\textbf{types}. To illustrate the generality of the original\nmodel, we present a Graph Neural Network formalisation, which partitions the\nvertices of a graph into a number of types. Each type represents an entity in\nthe ontology of the problem one wants to learn. This allows - for instance -\none to assign embeddings to edges, hyperedges, and any number of global\nattributes of the graph. As a companion to this paper we provide a\nPython/Tensorflow library to facilitate the development of such architectures,\nwith which we instantiate the formalisation to reproduce a number of models\nproposed in the current literature. \n\n"}
{"id": "1901.07988", "contents": "Title: Backprop with Approximate Activations for Memory-efficient Network\n  Training Abstract: Training convolutional neural network models is memory intensive since\nback-propagation requires storing activations of all intermediate layers. This\npresents a practical concern when seeking to deploy very deep architectures in\nproduction, especially when models need to be frequently re-trained on updated\ndatasets. In this paper, we propose a new implementation for back-propagation\nthat significantly reduces memory usage, by enabling the use of approximations\nwith negligible computational cost and minimal effect on training performance.\nThe algorithm reuses common buffers to temporarily store full activations and\ncompute the forward pass exactly. It also stores approximate per-layer copies\nof activations, at significant memory savings, that are used in the backward\npass. Compared to simply approximating activations within standard\nback-propagation, our method limits accumulation of errors across layers. This\nallows the use of much lower-precision approximations without affecting\ntraining accuracy. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that\nour method yields performance close to exact training, while storing\nactivations compactly with as low as 4-bit precision. \n\n"}
{"id": "1901.08256", "contents": "Title: Large-Batch Training for LSTM and Beyond Abstract: Large-batch training approaches have enabled researchers to utilize\nlarge-scale distributed processing and greatly accelerate deep-neural net (DNN)\ntraining. For example, by scaling the batch size from 256 to 32K, researchers\nhave been able to reduce the training time of ResNet50 on ImageNet from 29\nhours to 2.2 minutes (Ying et al., 2018). In this paper, we propose a new\napproach called linear-epoch gradual-warmup (LEGW) for better large-batch\ntraining. With LEGW, we are able to conduct large-batch training for both CNNs\nand RNNs with the Sqrt Scaling scheme. LEGW enables Sqrt Scaling scheme to be\nuseful in practice and as a result we achieve much better results than the\nLinear Scaling learning rate scheme. For LSTM applications, we are able to\nscale the batch size by a factor of 64 without losing accuracy and without\ntuning the hyper-parameters. For CNN applications, LEGW is able to achieve the\nsame accuracy even as we scale the batch size to 32K. LEGW works better than\nprevious large-batch auto-tuning techniques. LEGW achieves a 5.3X average\nspeedup over the baselines for four LSTM-based applications on the same\nhardware. We also provide some theoretical explanations for LEGW. \n\n"}
{"id": "1901.08334", "contents": "Title: Overcomplete Independent Component Analysis via SDP Abstract: We present a novel algorithm for overcomplete independent components analysis\n(ICA), where the number of latent sources k exceeds the dimension p of observed\nvariables. Previous algorithms either suffer from high computational complexity\nor make strong assumptions about the form of the mixing matrix. Our algorithm\ndoes not make any sparsity assumption yet enjoys favorable computational and\ntheoretical properties. Our algorithm consists of two main steps: (a)\nestimation of the Hessians of the cumulant generating function (as opposed to\nthe fourth and higher order cumulants used by most algorithms) and (b) a novel\nsemi-definite programming (SDP) relaxation for recovering a mixing component.\nWe show that this relaxation can be efficiently solved with a projected\naccelerated gradient descent method, which makes the whole algorithm\ncomputationally practical. Moreover, we conjecture that the proposed program\nrecovers a mixing component at the rate k < p^2/4 and prove that a mixing\ncomponent can be recovered with high probability when k < (2 - epsilon) p log p\nwhen the original components are sampled uniformly at random on the hyper\nsphere. Experiments are provided on synthetic data and the CIFAR-10 dataset of\nreal images. \n\n"}
{"id": "1901.08396", "contents": "Title: Self-Supervised Deep Learning on Point Clouds by Reconstructing Space Abstract: Point clouds provide a flexible and natural representation usable in\ncountless applications such as robotics or self-driving cars. Recently, deep\nneural networks operating on raw point cloud data have shown promising results\non supervised learning tasks such as object classification and semantic\nsegmentation. While massive point cloud datasets can be captured using modern\nscanning technology, manually labelling such large 3D point clouds for\nsupervised learning tasks is a cumbersome process. This necessitates methods\nthat can learn from unlabelled data to significantly reduce the number of\nannotated samples needed in supervised learning. We propose a self-supervised\nlearning task for deep learning on raw point cloud data in which a neural\nnetwork is trained to reconstruct point clouds whose parts have been randomly\nrearranged. While solving this task, representations that capture semantic\nproperties of the point cloud are learned. Our method is agnostic of network\narchitecture and outperforms current unsupervised learning approaches in\ndownstream object classification tasks. We show experimentally, that\npre-training with our method before supervised training improves the\nperformance of state-of-the-art models and significantly improves sample\nefficiency. \n\n"}
{"id": "1901.08437", "contents": "Title: Learning to compress and search visual data in large-scale systems Abstract: The problem of high-dimensional and large-scale representation of visual data\nis addressed from an unsupervised learning perspective. The emphasis is put on\ndiscrete representations, where the description length can be measured in bits\nand hence the model capacity can be controlled. The algorithmic infrastructure\nis developed based on the synthesis and analysis prior models whose\nrate-distortion properties, as well as capacity vs. sample complexity\ntrade-offs are carefully optimized. These models are then extended to\nmulti-layers, namely the RRQ and the ML-STC frameworks, where the latter is\nfurther evolved as a powerful deep neural network architecture with fast and\nsample-efficient training and discrete representations. For the developed\nalgorithms, three important applications are developed. First, the problem of\nlarge-scale similarity search in retrieval systems is addressed, where a\ndouble-stage solution is proposed leading to faster query times and shorter\ndatabase storage. Second, the problem of learned image compression is targeted,\nwhere the proposed models can capture more redundancies from the training\nimages than the conventional compression codecs. Finally, the proposed\nalgorithms are used to solve ill-posed inverse problems. In particular, the\nproblems of image denoising and compressive sensing are addressed with\npromising results. \n\n"}
{"id": "1901.08651", "contents": "Title: Decoupling feature extraction from policy learning: assessing benefits\n  of state representation learning in goal based robotics Abstract: Scaling end-to-end reinforcement learning to control real robots from vision\npresents a series of challenges, in particular in terms of sample efficiency.\nAgainst end-to-end learning, state representation learning can help learn a\ncompact, efficient and relevant representation of states that speeds up policy\nlearning, reducing the number of samples needed, and that is easier to\ninterpret. We evaluate several state representation learning methods on goal\nbased robotics tasks and propose a new unsupervised model that stacks\nrepresentations and combines strengths of several of these approaches. This\nmethod encodes all the relevant features, performs on par or better than\nend-to-end learning with better sample efficiency, and is robust to\nhyper-parameters change. \n\n"}
{"id": "1901.08770", "contents": "Title: Robust estimation of tree structured Gaussian Graphical Model Abstract: Consider jointly Gaussian random variables whose conditional independence\nstructure is specified by a graphical model. If we observe realizations of the\nvariables, we can compute the covariance matrix, and it is well known that the\nsupport of the inverse covariance matrix corresponds to the edges of the\ngraphical model. Instead, suppose we only have noisy observations. If the noise\nat each node is independent, we can compute the sum of the covariance matrix\nand an unknown diagonal. The inverse of this sum is (in general) dense. We ask:\ncan the original independence structure be recovered? We address this question\nfor tree structured graphical models. We prove that this problem is\nunidentifiable, but show that this unidentifiability is limited to a small\nclass of candidate trees. We further present additional constraints under which\nthe problem is identifiable. Finally, we provide an O(n^3) algorithm to find\nthis equivalence class of trees. \n\n"}
{"id": "1901.08798", "contents": "Title: Spurious Vanishing Problem in Approximate Vanishing Ideal Abstract: Approximate vanishing ideal is a concept from computer algebra that studies\nthe algebraic varieties behind perturbed data points. To capture the nonlinear\nstructure of perturbed points, the introduction of approximation to exact\nvanishing ideals plays a critical role. However, such an approximation also\ngives rise to a theoretical problem---the spurious vanishing problem---in the\nbasis construction of approximate vanishing ideals; namely, obtained basis\npolynomials can be approximately vanishing simply because of the small\ncoefficients. In this paper, we propose a first general method that enables\nvarious basis construction algorithms to overcome the spurious vanishing\nproblem. In particular, we integrate coefficient normalization with\npolynomial-based basis constructions, which do not need the proper ordering of\nmonomials to process for basis constructions. We further propose a method that\ntakes advantage of the iterative nature of basis construction so that\ncomputationally costly operations for coefficient normalization can be\ncircumvented. Moreover, a coefficient truncation method is proposed for further\naccelerations. From the experiments, it can be shown that the proposed method\novercomes the spurious vanishing problem, resulting in shorter feature vectors\nwhile sustaining comparable or even lower classification error. \n\n"}
{"id": "1901.08933", "contents": "Title: Self-Supervised Generalisation with Meta Auxiliary Learning Abstract: Learning with auxiliary tasks can improve the ability of a primary task to\ngeneralise. However, this comes at the cost of manually labelling auxiliary\ndata. We propose a new method which automatically learns appropriate labels for\nan auxiliary task, such that any supervised learning task can be improved\nwithout requiring access to any further data. The approach is to train two\nneural networks: a label-generation network to predict the auxiliary labels,\nand a multi-task network to train the primary task alongside the auxiliary\ntask. The loss for the label-generation network incorporates the loss of the\nmulti-task network, and so this interaction between the two networks can be\nseen as a form of meta learning with a double gradient. We show that our\nproposed method, Meta AuXiliary Learning (MAXL), outperforms single-task\nlearning on 7 image datasets, without requiring any additional data. We also\nshow that MAXL outperforms several other baselines for generating auxiliary\nlabels, and is even competitive when compared with human-defined auxiliary\nlabels. The self-supervised nature of our method leads to a promising new\ndirection towards automated generalisation. Source code can be found at\nhttps://github.com/lorenmt/maxl. \n\n"}
{"id": "1901.09021", "contents": "Title: Complexity of Linear Regions in Deep Networks Abstract: It is well-known that the expressivity of a neural network depends on its\narchitecture, with deeper networks expressing more complex functions. In the\ncase of networks that compute piecewise linear functions, such as those with\nReLU activation, the number of distinct linear regions is a natural measure of\nexpressivity. It is possible to construct networks with merely a single region,\nor for which the number of linear regions grows exponentially with depth; it is\nnot clear where within this range most networks fall in practice, either before\nor after training. In this paper, we provide a mathematical framework to count\nthe number of linear regions of a piecewise linear network and measure the\nvolume of the boundaries between these regions. In particular, we prove that\nfor networks at initialization, the average number of regions along any\none-dimensional subspace grows linearly in the total number of neurons, far\nbelow the exponential upper bound. We also find that the average distance to\nthe nearest region boundary at initialization scales like the inverse of the\nnumber of neurons. Our theory suggests that, even after training, the number of\nlinear regions is far below exponential, an intuition that matches our\nempirical observations. We conclude that the practical expressivity of neural\nnetworks is likely far below that of the theoretical maximum, and that this gap\ncan be quantified. \n\n"}
{"id": "1901.09054", "contents": "Title: Deep Learning on Small Datasets without Pre-Training using Cosine Loss Abstract: Two things seem to be indisputable in the contemporary deep learning\ndiscourse: 1. The categorical cross-entropy loss after softmax activation is\nthe method of choice for classification. 2. Training a CNN classifier from\nscratch on small datasets does not work well. In contrast to this, we show that\nthe cosine loss function provides significantly better performance than\ncross-entropy on datasets with only a handful of samples per class. For\nexample, the accuracy achieved on the CUB-200-2011 dataset without pre-training\nis by 30% higher than with the cross-entropy loss. Further experiments on other\npopular datasets confirm our findings. Moreover, we demonstrate that\nintegrating prior knowledge in the form of class hierarchies is straightforward\nwith the cosine loss and improves classification performance further. \n\n"}
{"id": "1901.09078", "contents": "Title: Finding Archetypal Spaces Using Neural Networks Abstract: Archetypal analysis is a data decomposition method that describes each\nobservation in a dataset as a convex combination of \"pure types\" or archetypes.\nThese archetypes represent extrema of a data space in which there is a\ntrade-off between features, such as in biology where different combinations of\ntraits provide optimal fitness for different environments. Existing methods for\narchetypal analysis work well when a linear relationship exists between the\nfeature space and the archetypal space. However, such methods are not\napplicable to systems where the feature space is generated non-linearly from\nthe combination of archetypes, such as in biological systems or image\ntransformations. Here, we propose a reformulation of the problem such that the\ngoal is to learn a non-linear transformation of the data into a latent\narchetypal space. To solve this problem, we introduce Archetypal Analysis\nnetwork (AAnet), which is a deep neural network framework for learning and\ngenerating from a latent archetypal representation of data. We demonstrate\nstate-of-the-art recovery of ground-truth archetypes in non-linear data\ndomains, show AAnet can generate from data geometry rather than from data\ndensity, and use AAnet to identify biologically meaningful archetypes in\nsingle-cell gene expression data. \n\n"}
{"id": "1901.09087", "contents": "Title: Optimality Implies Kernel Sum Classifiers are Statistically Efficient Abstract: We propose a novel combination of optimization tools with learning theory\nbounds in order to analyze the sample complexity of optimal kernel sum\nclassifiers. This contrasts the typical learning theoretic results which hold\nfor all (potentially suboptimal) classifiers. Our work also justifies\nassumptions made in prior work on multiple kernel learning. As a byproduct of\nour analysis, we also provide a new form of Rademacher complexity for\nhypothesis classes containing only optimal classifiers. \n\n"}
{"id": "1901.09290", "contents": "Title: PruneTrain: Fast Neural Network Training by Dynamic Sparse Model\n  Reconfiguration Abstract: State-of-the-art convolutional neural networks (CNNs) used in vision\napplications have large models with numerous weights. Training these models is\nvery compute- and memory-resource intensive. Much research has been done on\npruning or compressing these models to reduce the cost of inference, but little\nwork has addressed the costs of training. We focus precisely on accelerating\ntraining. We propose PruneTrain, a cost-efficient mechanism that gradually\nreduces the training cost during training. PruneTrain uses a structured\ngroup-lasso regularization approach that drives the training optimization\ntoward both high accuracy and small weight values. Small weights can then be\nperiodically removed by reconfiguring the network model to a smaller one. By\nusing a structured-pruning approach and additional reconfiguration techniques\nwe introduce, the pruned model can still be efficiently processed on a GPU\naccelerator. Overall, PruneTrain achieves a reduction of 39% in the end-to-end\ntraining time of ResNet50 for ImageNet by reducing computation cost by 40% in\nFLOPs, memory accesses by 37% for memory bandwidth bound layers, and the\ninter-accelerator communication by 55%. \n\n"}
{"id": "1901.09321", "contents": "Title: Fixup Initialization: Residual Learning Without Normalization Abstract: Normalization layers are a staple in state-of-the-art deep neural network\narchitectures. They are widely believed to stabilize training, enable higher\nlearning rate, accelerate convergence and improve generalization, though the\nreason for their effectiveness is still an active research topic. In this work,\nwe challenge the commonly-held beliefs by showing that none of the perceived\nbenefits is unique to normalization. Specifically, we propose fixed-update\ninitialization (Fixup), an initialization motivated by solving the exploding\nand vanishing gradient problem at the beginning of training via properly\nrescaling a standard initialization. We find training residual networks with\nFixup to be as stable as training with normalization -- even for networks with\n10,000 layers. Furthermore, with proper regularization, Fixup enables residual\nnetworks without normalization to achieve state-of-the-art performance in image\nclassification and machine translation. \n\n"}
{"id": "1901.09421", "contents": "Title: Information-Theoretic Understanding of Population Risk Improvement with\n  Model Compression Abstract: We show that model compression can improve the population risk of a\npre-trained model, by studying the tradeoff between the decrease in the\ngeneralization error and the increase in the empirical risk with model\ncompression. We first prove that model compression reduces an\ninformation-theoretic bound on the generalization error; this allows for an\ninterpretation of model compression as a regularization technique to avoid\noverfitting. We then characterize the increase in empirical risk with model\ncompression using rate distortion theory. These results imply that the\npopulation risk could be improved by model compression if the decrease in\ngeneralization error exceeds the increase in empirical risk. We show through a\nlinear regression example that such a decrease in population risk due to model\ncompression is indeed possible. Our theoretical results further suggest that\nthe Hessian-weighted $K$-means clustering compression approach can be improved\nby regularizing the distance between the clustering centers. We provide\nexperiments with neural networks to support our theoretical assertions. \n\n"}
{"id": "1901.09451", "contents": "Title: Bias in Bios: A Case Study of Semantic Representation Bias in a\n  High-Stakes Setting Abstract: We present a large-scale study of gender bias in occupation classification, a\ntask where the use of machine learning may lead to negative outcomes on\npeoples' lives. We analyze the potential allocation harms that can result from\nsemantic representation bias. To do so, we study the impact on occupation\nclassification of including explicit gender indicators---such as first names\nand pronouns---in different semantic representations of online biographies.\nAdditionally, we quantify the bias that remains when these indicators are\n\"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit\ngender indicators. As we demonstrate, differences in true positive rates\nbetween genders are correlated with existing gender imbalances in occupations,\nwhich may compound these imbalances. \n\n"}
{"id": "1901.09715", "contents": "Title: Revisiting the Bethe-Hessian: Improved Community Detection in Sparse\n  Heterogeneous Graphs Abstract: Spectral clustering is one of the most popular, yet still incompletely\nunderstood, methods for community detection on graphs. This article studies\nspectral clustering based on the Bethe-Hessian matrix $H_r = (r^2-1)I_n + D-rA$\nfor sparse heterogeneous graphs (following the degree-corrected stochastic\nblock model) in a two-class setting. For a specific value $r = \\zeta$,\nclustering is shown to be insensitive to the degree heterogeneity. We then\nstudy the behavior of the informative eigenvector of $H_{\\zeta}$ and, as a\nresult, predict the clustering accuracy. The article concludes with an overview\nof the generalization to more than two classes along with extensive simulations\non synthetic and real networks corroborating our findings. \n\n"}
{"id": "1901.09892", "contents": "Title: A Black-box Attack on Neural Networks Based on Swarm Evolutionary\n  Algorithm Abstract: Neural networks play an increasingly important role in the field of machine\nlearning and are included in many applications in society. Unfortunately,\nneural networks suffer from adversarial samples generated to attack them.\nHowever, most of the generation approaches either assume that the attacker has\nfull knowledge of the neural network model or are limited by the type of\nattacked model. In this paper, we propose a new approach that generates a\nblack-box attack to neural networks based on the swarm evolutionary algorithm.\nBenefiting from the improvements in the technology and theoretical\ncharacteristics of evolutionary algorithms, our approach has the advantages of\neffectiveness, black-box attack, generality, and randomness. Our experimental\nresults show that both the MNIST images and the CIFAR-10 images can be\nperturbed to successful generate a black-box attack with 100\\% probability on\naverage. In addition, the proposed attack, which is successful on distilled\nneural networks with almost 100\\% probability, is resistant to defensive\ndistillation. The experimental results also indicate that the robustness of the\nartificial intelligence algorithm is related to the complexity of the model and\nthe data set. In addition, we find that the adversarial samples to some extent\nreproduce the characteristics of the sample data learned by the neural network\nmodel. \n\n"}
{"id": "1901.10159", "contents": "Title: An Investigation into Neural Net Optimization via Hessian Eigenvalue\n  Density Abstract: To understand the dynamics of optimization in deep neural networks, we\ndevelop a tool to study the evolution of the entire Hessian spectrum throughout\nthe optimization process. Using this, we study a number of hypotheses\nconcerning smoothness, curvature, and sharpness in the deep learning\nliterature. We then thoroughly analyze a crucial structural feature of the\nspectra: in non-batch normalized networks, we observe the rapid appearance of\nlarge isolated eigenvalues in the spectrum, along with a surprising\nconcentration of the gradient in the corresponding eigenspaces. In batch\nnormalized networks, these two effects are almost absent. We characterize these\neffects, and explain how they affect optimization speed through both theory and\nexperiments. As part of this work, we adapt advanced tools from numerical\nlinear algebra that allow scalable and accurate estimation of the entire\nHessian spectrum of ImageNet-scale neural networks; this technique may be of\nindependent interest in other applications. \n\n"}
{"id": "1901.10200", "contents": "Title: catch22: CAnonical Time-series CHaracteristics Abstract: Capturing the dynamical properties of time series concisely as interpretable\nfeature vectors can enable efficient clustering and classification for\ntime-series applications across science and industry. Selecting an appropriate\nfeature-based representation of time series for a given application can be\nachieved through systematic comparison across a comprehensive time-series\nfeature library, such as those in the hctsa toolbox. However, this approach is\ncomputationally expensive and involves evaluating many similar features,\nlimiting the widespread adoption of feature-based representations of time\nseries for real-world applications. In this work, we introduce a method to\ninfer small sets of time-series features that (i) exhibit strong classification\nperformance across a given collection of time-series problems, and (ii) are\nminimally redundant. Applying our method to a set of 93 time-series\nclassification datasets (containing over 147000 time series) and using a\nfiltered version of the hctsa feature library (4791 features), we introduce a\ngenerically useful set of 22 CAnonical Time-series CHaracteristics, catch22.\nThis dimensionality reduction, from 4791 to 22, is associated with an\napproximately 1000-fold reduction in computation time and near linear scaling\nwith time-series length, despite an average reduction in classification\naccuracy of just 7%. catch22 captures a diverse and interpretable signature of\ntime series in terms of their properties, including linear and non-linear\nautocorrelation, successive differences, value distributions and outliers, and\nfluctuation scaling properties. We provide an efficient implementation of\ncatch22, accessible from many programming environments, that facilitates\nfeature-based time-series analysis for scientific, industrial, financial and\nmedical applications using a common language of interpretable time-series\nproperties. \n\n"}
{"id": "1901.10655", "contents": "Title: On the Calibration of Multiclass Classification with Rejection Abstract: We investigate the problem of multiclass classification with rejection, where\na classifier can choose not to make a prediction to avoid critical\nmisclassification. First, we consider an approach based on simultaneous\ntraining of a classifier and a rejector, which achieves the state-of-the-art\nperformance in the binary case. We analyze this approach for the multiclass\ncase and derive a general condition for calibration to the Bayes-optimal\nsolution, which suggests that calibration is hard to achieve by general loss\nfunctions unlike the binary case. Next, we consider another traditional\napproach based on confidence scores, in which the existing work focuses on a\nspecific class of losses. We propose rejection criteria for more general losses\nfor this approach and guarantee calibration to the Bayes-optimal solution.\nFinally, we conduct experiments to validate the relevance of our theoretical\nfindings. \n\n"}
{"id": "1901.10824", "contents": "Title: Diversity Regularized Adversarial Learning Abstract: The two key players in Generative Adversarial Networks (GANs), the\ndiscriminator and generator, are usually parameterized as deep neural networks\n(DNNs). On many generative tasks, GANs achieve state-of-the-art performance but\nare often unstable to train and sometimes miss modes. A typical failure mode is\nthe collapse of the generator to a single parameter configuration where its\noutputs are identical. When this collapse occurs, the gradient of the\ndiscriminator may point in similar directions for many similar points. We\nhypothesize that some of these shortcomings are in part due to primitive and\nredundant features extracted by discriminator and this can easily make the\ntraining stuck. We present a novel approach for regularizing adversarial models\nby enforcing diverse feature learning. In order to do this, both generator and\ndiscriminator are regularized by penalizing both negatively and positively\ncorrelated features according to their differentiation and based on their\nrelative cosine distances. In addition to the gradient information from the\nadversarial loss made available by the discriminator, diversity regularization\nalso ensures that a more stable gradient is provided to update both the\ngenerator and discriminator. Results indicate our regularizer enforces diverse\nfeatures, stabilizes training, and improves image synthesis. \n\n"}
{"id": "1901.10912", "contents": "Title: A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms Abstract: We propose to meta-learn causal structures based on how fast a learner adapts\nto new distributions arising from sparse distributional changes, e.g. due to\ninterventions, actions of agents and other sources of non-stationarities. We\nshow that under this assumption, the correct causal structural choices lead to\nfaster adaptation to modified distributions because the changes are\nconcentrated in one or just a few mechanisms when the learned knowledge is\nmodularized appropriately. This leads to sparse expected gradients and a lower\neffective number of degrees of freedom needing to be relearned while adapting\nto the change. It motivates using the speed of adaptation to a modified\ndistribution as a meta-learning objective. We demonstrate how this can be used\nto determine the cause-effect relationship between two observed variables. The\ndistributional changes do not need to correspond to standard interventions\n(clamping a variable), and the learner has no direct knowledge of these\ninterventions. We show that causal structures can be parameterized via\ncontinuous variables and learned end-to-end. We then explore how these ideas\ncould be used to also learn an encoder that would map low-level observed\nvariables to unobserved causal variables leading to faster adaptation\nout-of-distribution, learning a representation space where one can satisfy the\nassumptions of independent mechanisms and of small and sparse changes in these\nmechanisms due to actions and non-stationarities. \n\n"}
{"id": "1901.10966", "contents": "Title: A Novel Bulk-Optics Scheme for Quantum Walk with High Phase Stability Abstract: A novel bulk optics scheme for quantum walks is presented. It consists of a\none-dimensional lattice built on two concatenated displaced Sagnac\ninterferometers that make it possible to reproduce all the possible\ntrajectories of an optical quantum walk. Because of the closed loop\nconfiguration, the interferometric structure is intrinsically stable in phase.\nMoreover, the lattice structure is highly configurable, as any phase component\nperceived by the walker is accessible, and finally, all output modes can be\nmeasured at any step of the quantum walk evolution. We report here on the\nexperimental implementation of ordered and disordered quantum walks. \n\n"}
{"id": "1901.11058", "contents": "Title: HyperGAN: A Generative Model for Diverse, Performant Neural Networks Abstract: Standard neural networks are often overconfident when presented with data\noutside the training distribution. We introduce HyperGAN, a new generative\nmodel for learning a distribution of neural network parameters. HyperGAN does\nnot require restrictive assumptions on priors, and networks sampled from it can\nbe used to quickly create very large and diverse ensembles. HyperGAN employs a\nnovel mixer to project prior samples to a latent space with correlated\ndimensions, and samples from the latent space are then used to generate weights\nfor each layer of a deep neural network. We show that HyperGAN can learn to\ngenerate parameters which label the MNIST and CIFAR-10 datasets with\ncompetitive performance to fully supervised learning, while learning a rich\ndistribution of effective parameters. We also show that HyperGAN can also\nprovide better uncertainty estimates than standard ensembles by evaluating on\nout of distribution data as well as adversarial examples. \n\n"}
{"id": "quant-ph/0507040", "contents": "Title: Popper's test of quantum mechanics and two-photon \"ghost\" diffraction Abstract: A test on quantum mechanics proposed long ago by Karl Popper is reconsidered\nwith further detail and new insight. An ambiguity in the proposal, which turns\nout to be essential in order to make the test conclusive, is identified and\ntaken into account for the first time. Its implications for recently performed\nphoton experiments [such as in D. V. Strekalov \\textit{et al.}, Phys. Rev.\\\nLett. {\\bf 74}, 3600 (1995)] are briefly analyzed. \n\n"}
{"id": "quant-ph/0510119", "contents": "Title: Upper Bound Imposed upon Responsivity of Optical Modulators Abstract: We study theoretically the responsivity of optical modulators. For the case\nof linear response we find using perturbation theory an upper bound imposed\nupon the responsivity. For the case of two mode modulator we find a lower bound\nimposed upon the optical path required for achieving full modulation when the\nmaximum birefringence strength is given. \n\n"}
{"id": "quant-ph/0603150", "contents": "Title: Analysis of a Quantum Nondemolition Measurement Scheme Based on Kerr\n  Nonlinearity in Photonic Crystal Waveguides Abstract: We discuss the feasibility of a quantum nondemolition measurement (QND) of\nphoton number based on cross phase modulation due to the Kerr effect in\nPhotonic Crystal Waveguides (PCWs). In particular, we derive the equations for\ntwo modes propagating in PCWs and their coupling by a third order nonlinearity.\nThe reduced group velocity and small cross-sectional area of the PCW lead to an\nenhancement of the interaction relative to bulk materials. We show that in\nprinciple, such experiments may be feasible with current photonic technologies,\nalthough they are limited by material properties. Our analysis of the\npropagation equations is sufficiently general to be applicable to the study of\nsoliton formation, all-optical switching and can be extended to processes\ninvolving other orders of the nonlinearity. \n\n"}
{"id": "quant-ph/0605084", "contents": "Title: Semiclassical Theory of Amplification and Lasing Abstract: In this article we present a systematic derivation of the Maxwell-Bloch\nequations describing amplification and laser action in a ring cavity. We derive\nthe Maxwell-Bloch equations for a two-level medium and discuss their\napplicability to standard three- and four-level systems. After discusing\namplification, we consider lasing and pay special attention to the obtention of\nthe laser equations in the uniform field approximation. Finally, the connection\nof the laser equations with the Lorenz model is considered. \n\n"}
{"id": "quant-ph/9711055", "contents": "Title: Photon counting sampling of phase space Abstract: The recently proposed scheme for direct sampling of the quantum phase space\nby photon counting is discussed within the Wigner function formalism. \n\n"}
