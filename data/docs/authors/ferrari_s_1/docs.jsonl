{"id": "0704.0857", "contents": "Title: Extrasolar scale change in Newton's Law from 5D `plain' R^2-gravity Abstract: Galactic rotation curves and lack of direct observations of Dark Matter may\nindicate that General Relativity is not valid (on galactic scale) and should be\nreplaced with another theory. There is the only variant of Absolute Parallelism\nwhich solutions are free of arising singularities, if D=5 (there is no room for\nchanges). This variant does not have a Lagrangian, nor match GR: an equation of\n`plain' R^2-gravity (ie without R-term) is in sight instead. Arranging an\nexpanding O_4-symmetrical solution as the basis of 5D cosmological model, and\nprobing a universal_function of mass distribution (along very-very long the\nextra dimension) to place into bi-Laplace equation (R^2 gravity), one can\nderive the Law of Gravitation: 1/r^2 transforms to 1/r with distance (not with\nacceleration). \n\n"}
{"id": "0704.2944", "contents": "Title: Statistics of neutrinos and the double beta decay Abstract: We assume that the Pauli exclusion principle is violated for neutrinos, and\nthus, neutrinos obey at least partly the Bose-Einstein statistics. The\nparameter sin^2 chi is introduced that characterizes the bosonic (symmetric)\nfraction of the neutrino wave function. Consequences of the violation of the\nexclusion principle for the two-neutrino double beta decays are considered.\nThis violation strongly changes the rates of the decays and modifies the energy\nand angular distributions of the emitted electrons. Pure bosonic neutrinos are\nexcluded by the present data. In the case of partly bosonic (or\nmixed-statistics) neutrinos the analysis of the existing data allows to put the\nconservative upper bound sin^2 chi < 0.6. The sensitivity of future\nmeasurements of the two-neutrino double beta decay to sin^2 chi is evaluated. \n\n"}
{"id": "0704.3532", "contents": "Title: Some Phenomenologies of Unparticle Physics Abstract: Fermionic unparticles are introduced and their basic properties are\ndiscussed. Some phenomenologies related are exploited, such as their effects on\ncharged Higgs boson decays and anomalous magnetic moments of leptons. Also, it\nhas been found that measurements of $B^0-\\bar B^0$ mixing could yield\ninteresting constraints on couplings between unparticle operators and standard\nmodel fields. \n\n"}
{"id": "0706.2630", "contents": "Title: Parton shower Monte Carlos vs resummed calculations for interjet energy\n  flow observables Abstract: Parton showers in Monte Carlo event generators reflect to a certain accuracy\nour understanding of QCD radiation at all orders. For observables sensitive to\ninterjet energy flow in well defined regions of phase space, it has been known\nfor some time that relevant all-order dynamics is substantially more\ncomplicated than that encoded via angular ordering in parton shower algorithms,\neven to leading logarithmic accuracy. Here we investigate the extent of the\nnumerical mismatch between leading logarithmic analytical estimates\n(resummation) and parton showers in an effort to better understand the accuracy\nof parton showers for such observables. \n\n"}
{"id": "0706.3435", "contents": "Title: Undercomplete Blind Subspace Deconvolution via Linear Prediction Abstract: We present a novel solution technique for the blind subspace deconvolution\n(BSSD) problem, where temporal convolution of multidimensional hidden\nindependent components is observed and the task is to uncover the hidden\ncomponents using the observation only. We carry out this task for the\nundercomplete case (uBSSD): we reduce the original uBSSD task via linear\nprediction to independent subspace analysis (ISA), which we can solve. As it\nhas been shown recently, applying temporal concatenation can also reduce uBSSD\nto ISA, but the associated ISA problem can easily become `high dimensional'\n[1]. The new reduction method circumvents this dimensionality problem. We\nperform detailed studies on the efficiency of the proposed technique by means\nof numerical simulations. We have found several advantages: our method can\nachieve high quality estimations for smaller number of samples and it can cope\nwith deeper temporal convolutions. \n\n"}
{"id": "0706.4071", "contents": "Title: Cosmology and Astrophysics of Minimal Dark Matter Abstract: We consider DM that only couples to SM gauge bosons and fills one gauge\nmultiplet, e.g. a fermion 5-plet (which is automatically stable), or a\nwino-like 3-plet. We revisit the computation of the cosmological relic\nabundance including non-perturbative corrections. The predicted mass of e.g.\nthe 5-plet increases from 4.4 TeV to 10 TeV, and indirect detection rates are\nenhanced by 2 orders of magnitude. Next, we show that due to the\nquasi-degeneracy among neutral and charged components of the DM multiplet, a\nsignificant fraction of DM with energy E > 10^17 eV (possibly present among\nultra-high energy cosmic rays) can cross the Earth exiting in the charged state\nand may in principle be detected in neutrino telescopes. \n\n"}
{"id": "0707.0721", "contents": "Title: Primordial antimatter in the contemporary universe Abstract: In some baryogenesis scenarios, the universe acquires a non-vanishing average\nbaryonic charge, but the baryon to photon ratio is not spatially constant and\ncan be even negative in some space regions. This allows for existence of lumps\nof antimatter in our neighborhood and the possibility that very compact\nantimatter objects make a part of cosmological dark matter. Here I discuss the\npeculiar signatures which may be observed in a near future. \n\n"}
{"id": "0707.2671", "contents": "Title: More about F-term uplifting Abstract: We study moduli stabilization and a realization of de Sitter vacua in\ngeneralized F-term uplifting scenarios of the KKLT-type anti-de Sitter vacuum,\nwhere the uplifting sector X directly couples to the light K\\\"ahler modulus T\nin the superpotential through, e.g., stringy instanton effects. F-term\nuplifting can be achieved by a spontaneous supersymmetry breaking sector, e.g.,\nthe Polonyi model, the O'Raifeartaigh model and the Intriligator-Seiberg-Shih\nmodel. Several models with the X-T mixing are examined and qualitative features\nin most models {\\it even with such mixing} are almost the same as those in the\nKKLT scenario. One of the quantitative changes, which are relevant to the\nphenomenology, is a larger hierarchy between the modulus mass m_T and the\ngravitino mass $m_{3/2}$, i.e., $m_T/m_{3/2} = {\\cal O}(a^2)$, where $a \\sim 4\n\\pi^2$. In spite of such a large mass, the modulus F-term is suppressed not\nlike $F^T = {\\cal O}(m_{3/2}/a^2)$, but like $F^T = {\\cal O}(m_{3/2}/a)$ for\n$\\ln (M_{Pl}/m_{3/2}) \\sim a$, because of an enhancement factor coming from the\nX-T mixing. Then we typically find a mirage-mediation pattern of gaugino masses\nof ${\\cal O}(m_{3/2}/a)$, while the scalar masses would be generically of\n${\\cal O}(m_{3/2})$. \n\n"}
{"id": "0708.1347", "contents": "Title: Quantized Non-Abelian Monopoles on S^3 Abstract: A possible electric-magnetic duality suggests that the confinement of\nnon-Abelian electric charges manifests itself as a perturbative quantum effect\nfor the dual magnetic charges. Motivated by this possibility, we study vacuum\nfluctuations around a non-Abelian monopole-antimonopole pair treated as point\nobjects with charges g=\\pm n/2 (n=1,2,...), and placed on the antipodes of a\nthree sphere of radius R. We explicitly find all the fluctuation modes by\nlinearizing and solving the Yang-Mills equations about this background field on\na three sphere. We recover, generalize and extend earlier results, including\nthose on the stability analysis of non-Abelian magnetic monopoles. We find that\nfor g \\ge 1 monopoles there is an unstable mode that tends to squeeze magnetic\nflux in the angular directions. We sum the vacuum energy contributions of the\nfluctuation modes for the g=1/2 case and find oscillatory dependence on the\ncutoff scale. Subject to certain assumptions, we find that the contribution of\nthe fluctuation modes to the quantum zero point energy behaves as -R^{-2/3} and\nhence decays more slowly than the classical -R^{-1} Coulomb potential for large\nR. However, this correction to the zero point energy does not agree with the\nlinear growth expected if the monopoles are confined. \n\n"}
{"id": "0708.2285", "contents": "Title: Mapping an Island in the Landscape Abstract: We provide a complete classification and statistical analysis of all type IIA\norientifold compactifications with intersecting D6-branes on the orbifold\nT^6/Z'_6. The total number of four dimensional N=1 supersymmetric models is\nfound to be O(10^23). After a statistical analysis of the gauge sector\nproperties of all possible solutions, we study three subsets of configurations\nwhich contain the chiral matter sector of the standard model, a Pati-Salam or\nSU(5) GUT model, respectively. We find O(10^15) compactifications with an MSSM\nand O(10^11) models with a Pati-Salam sector. Along the way we derive an\nexplicit algebraic formulation for the computation of the non-chiral matter\nspectrum for all Z_N orbifolds. \n\n"}
{"id": "0709.3535", "contents": "Title: Maximum Likelihood Estimation in Latent Class Models For Contingency\n  Table Data Abstract: Statistical models with latent structure have a history going back to the\n1950s and have seen widespread use in the social sciences and, more recently,\nin computational biology and in machine learning. Here we study the basic\nlatent class model proposed originally by the sociologist Paul F. Lazarfeld for\ncategorical variables, and we explain its geometric structure. We draw\nparallels between the statistical and geometric properties of latent class\nmodels and we illustrate geometrically the causes of many problems associated\nwith maximum likelihood estimation and related statistical inference. In\nparticular, we focus on issues of non-identifiability and determination of the\nmodel dimension, of maximization of the likelihood function and on the effect\nof symmetric data. We illustrate these phenomena with a variety of synthetic\nand real-life tables, of different dimension and complexity. Much of the\nmotivation for this work stems from the \"100 Swiss Francs\" problem, which we\nintroduce and describe in detail. \n\n"}
{"id": "0710.0581", "contents": "Title: Quark and lepton masses and mixing in SO(10) with a GUT-scale vector\n  matter Abstract: We explore in detail the effective matter fermion mass sum-rules in a class\nof renormalizable SUSY SO(10) grand unified models where the quark and lepton\nmass and mixing patterns originate from non-decoupling effects of an extra\nvector matter multiplet living around the unification scale. If the\nrenormalizable type-II contribution governed by the SU(2)_L-triplet in 54_H\ndominates the seesaw formula, we obtain an interesting correlation between the\nmaximality of the atmospheric neutrino mixing and the proximity of y_s/y_b to\nV_cb in the quark sector. \n\n"}
{"id": "0711.0198", "contents": "Title: A Geometric Approach to Confidence Sets for Ratios: Fieller's Theorem,\n  Generalizations, and Bootstrap Abstract: We present a geometric method to determine confidence sets for the ratio\nE(Y)/E(X) of the means of random variables X and Y. This method reduces the\nproblem of constructing confidence sets for the ratio of two random variables\nto the problem of constructing confidence sets for the means of one-dimensional\nrandom variables. It is valid in a large variety of circumstances. In the case\nof normally distributed random variables, the so constructed confidence sets\ncoincide with the standard Fieller confidence sets. Generalizations of our\nconstruction lead to definitions of exact and conservative confidence sets for\nvery general classes of distributions, provided the joint expectation of (X,Y)\nexists and the linear combinations of the form aX + bY are well-behaved.\nFinally, our geometric method allows to derive a very simple bootstrap approach\nfor constructing conservative confidence sets for ratios which perform\nfavorably in certain situations, in particular in the asymmetric heavy-tailed\nregime. \n\n"}
{"id": "0711.1238", "contents": "Title: Neutrinos and Lepton Flavour Violation in the Left-Right Twin Higgs\n  Model Abstract: We analyse the lepton sector of the Left-Right Twin Higgs Model. This model\noffers an alternative way to solve the \"little hierarchy\" problem of the\nStandard Model. We show that one can achieve an effective see-saw to explain\nthe origin of neutrino masses and that this model can accommodate the observed\nneutrino masses and mixings. We have also studied the lepton flavour violation\nprocess l_1 -> l_2 \\gamma and discussed how the experimental bound from these\nbranching ratios constrains the scale of symmetry breaking of this Twin Higgs\nmodel. \n\n"}
{"id": "0711.1612", "contents": "Title: Enhancing Sparsity by Reweighted L1 Minimization Abstract: It is now well understood that (1) it is possible to reconstruct sparse\nsignals exactly from what appear to be highly incomplete sets of linear\nmeasurements and (2) that this can be done by constrained L1 minimization. In\nthis paper, we study a novel method for sparse signal recovery that in many\nsituations outperforms L1 minimization in the sense that substantially fewer\nmeasurements are needed for exact recovery. The algorithm consists of solving a\nsequence of weighted L1-minimization problems where the weights used for the\nnext iteration are computed from the value of the current solution. We present\na series of experiments demonstrating the remarkable performance and broad\napplicability of this algorithm in the areas of sparse signal recovery,\nstatistical estimation, error correction and image processing. Interestingly,\nsuperior gains are also achieved when our method is applied to recover signals\nwith assumed near-sparsity in overcomplete representations--not by reweighting\nthe L1 norm of the coefficient sequence as is common, but by reweighting the L1\nnorm of the transformed object. An immediate consequence is the possibility of\nhighly efficient data acquisition protocols by improving on a technique known\nas compressed sensing. \n\n"}
{"id": "0801.1345", "contents": "Title: The Physics of Heavy Z' Gauge Bosons Abstract: The U(1)' symmetry associated with a possible heavy Z' would have profound\nimplications for particle physics and cosmology. The motivations for such\nparticles in various extensions of the standard model, possible ranges for\ntheir masses and couplings, and classes of anomaly-free models are discussed.\nPresent limits from electroweak and collider experiments are briefly surveyed,\nas are prospects for discovery and diagnostic study at future colliders.\nImplications of a Z' are discussed, including an extended Higgs sector,\nextended neutralino sector, and solution to the mu problem in supersymmetry;\nexotic fermions needed for anomaly cancellation; possible flavor changing\nneutral current effects; neutrino mass; possible Z' mediation of supersymmetry\nbreaking; and cosmological implications for cold dark matter and electroweak\nbaryogenesis. \n\n"}
{"id": "0801.2648", "contents": "Title: Modulated Inflation Abstract: We have studied modulated inflation that generates curvature perturbation\nfrom light-field fluctuation. As discussed in previous works, even if the\nfluctuation of the inflaton itself does not generate the curvature perturbation\nat the horizon crossing, fluctuation of a light field may induce fluctuation\nfor the end-line of inflation and this may lead to generation of cosmological\nperturbation ``at the end of the inflation''. Our scenario is different from\nthose that are based on the fluctuations of the boundary of the inflaton\ntrajectory, as clearly explained in this paper by using the $\\delta N$\nformalism. In this paper, we will consider the perturbation of the inflaton\nvelocity that can be induced by a light field other than the inflaton. We also\nexplain the crucial difference from the standard multi-field inflation model.\nWe show concrete examples of the modulated inflation scenario in which\nnon-gaussianity can be generated. We also discuss the running of the\nnon-gaussianity parameter. \n\n"}
{"id": "0802.0529", "contents": "Title: The distribution of the maximum of a first order moving average: the\n  discrete case Abstract: We give the distribution of $M_n$, the maximum of a sequence of $n$\nobservations from a moving average of order 1. Solutions are first given in\nterms of repeated integrals and then for the case where the underlying\nindependent random variables are discrete. When the correlation is positive, $$\nP(M_n \\max^n_{i=1} X_i \\leq x) = \\sum_{j=1}^\\infty \\beta_{jx} \\nu_{jx}^{n}\n\\approx B_{x} r{1x}^{n} $$ where $\\{\\nu_{jx}\\}$ are the eigenvalues of a\ncertain matrix, $r_{1x}$ is the maximum magnitude of the eigenvalues, and $I$\ndepends on the number of possible values of the underlying random variables.\nThe eigenvalues do not depend on $x$ only on its range. \n\n"}
{"id": "0802.2377", "contents": "Title: Higher-Order Properties of Analytic Wavelets Abstract: The influence of higher-order wavelet properties on the analytic wavelet\ntransform behavior is investigated, and wavelet functions offering advantageous\nperformance are identified. This is accomplished through detailed investigation\nof the generalized Morse wavelets, a two-parameter family of exactly analytic\ncontinuous wavelets. The degree of time/frequency localization, the existence\nof a mapping between scale and frequency, and the bias involved in estimating\nproperties of modulated oscillatory signals, are proposed as important\nconsiderations. Wavelet behavior is found to be strongly impacted by the degree\nof asymmetry of the wavelet in both the frequency and the time domain, as\nquantified by the third central moments. A particular subset of the generalized\nMorse wavelets, recognized as deriving from an inhomogeneous Airy function,\nemerge as having particularly desirable properties. These \"Airy wavelets\"\nsubstantially outperform the only approximately analytic Morlet wavelets for\nhigh time localization. Special cases of the generalized Morse wavelets are\nexamined, revealing a broad range of behaviors which can be matched to the\ncharacteristics of a signal. \n\n"}
{"id": "0803.0481", "contents": "Title: $\\mu \\to e \\gamma$ and $\\tau \\to l \\gamma$ decays in the fermion triplet\n  seesaw model Abstract: In the framework of the seesaw models with triplets of fermions, we evaluate\nthe decay rates of $\\mu \\to e \\gamma$ and $\\tau \\to l \\gamma$ transitions. We\nshow that although, due to neutrino mass constraints, those rates are in\ngeneral expected to be well under the present experimental limits, this is not\nnecessarily always the case. Interestingly enough, the observation of one of\nthose decays in planned experiments would nevertheless contradict bounds\nstemming from present experimental limits on the $\\mu \\to eee$ and $\\tau \\to 3\nl$ decay rates. Such detection of radiative decays would therefore imply that\nthere exist sources of lepton flavour violation not associated to triplet\nfermions. \n\n"}
{"id": "0804.3478", "contents": "Title: U_A(1) anomaly and eta' mass from an infrared singular quark-gluon\n  vertex Abstract: The $U_A(1)$ problem of QCD is inevitably tied to the infrared behaviour of\nquarks and gluons with its most visible effect being the $\\eta^\\prime$ mass. A\ndimensional argument of Kogut and Susskind showed that the mixing of the\npseudoscalar flavour-singlet mesons with gluons can provide a screening of the\nGoldstone pole in this channel if the full quark-quark interaction is strongly\ninfrared singular as $\\sim 1/k^4$. We investigate this idea using previously\nobtained results for the Landau gauge ghost and gluon propagator, together with\nrecent determinations for the singular behaviour of the quark-gluon vertex. We\nfind that, even with an infrared vanishing gluon propagator, the singular\nstructure of the quark-gluon vertex for certain kinematics is apposite for\nyielding a non-zero screening mass. \n\n"}
{"id": "0805.4799", "contents": "Title: Second order chiral restoration phase transition at low temperatures in\n  quarkyonic matter Abstract: In this Addendum to our recent paper, Phys. Rev. D 77, 054027 (2008), we\npoint out that a chiral restoration phase transition in a quarkyonic matter at\nlow temperatures is of second order within a manifestly confining and chirally\nsymmetric large $N_c$ model. This result is qualitatively different as compared\nto NJL and NJL-like models that are not confining and might have some\nimplications for the existence or nonexistence of the critical end point in the\nQCD phase diagram. \n\n"}
{"id": "0806.3769", "contents": "Title: Improved testing inference in mixed linear models Abstract: Mixed linear models are commonly used in repeated measures studies. They\naccount for the dependence amongst observations obtained from the same\nexperimental unit. Oftentimes, the number of observations is small, and it is\nthus important to use inference strategies that incorporate small sample\ncorrections. In this paper, we develop modified versions of the likelihood\nratio test for fixed effects inference in mixed linear models. In particular,\nwe derive a Bartlett correction to such a test and also to a test obtained from\na modified profile likelihood function. Our results generalize those in Zucker\net al. (Journal of the Royal Statistical Society B, 2000, 62, 827-838) by\nallowing the parameter of interest to be vector-valued. Additionally, our\nBartlett corrections allow for random effects nonlinear covariance matrix\nstructure. We report numerical evidence which shows that the proposed tests\ndisplay superior finite sample behavior relative to the standard likelihood\nratio test. An application is also presented and discussed. \n\n"}
{"id": "0807.0851", "contents": "Title: Calculation of neutron background for underground experiments Abstract: New generation dark matter experiments aim at exploring the 10e-9 - 10e-10 pb\ncross-section region for the WIMP-nucleon scalar interactions. Neutrons\nproduced in the detector components are one of the main factors that can limit\ndetector sensitivity. Estimation of the background from this source then\nbecomes a crucial task for designing future large-scale detectors. Energy\nspectra and production rates for neutrons coming from radioactive contamination\nare required for all materials in and around the detector. In order to estimate\nneutron yields and spectra, the cross-sections of (a,n) reactions and\nprobabilities of transitions to different excited states should be known.\nCross-sections and transition probabilities have been calculated using\nEmpire2.19 for several isotopes, and for some isotopes, a comparison with the\nexperimental data is shown. The results have been used to calculate the neutron\nspectra from materials using the code Sources4A. Neutron background event rates\nfrom some detector components in a hypothetical dark matter detector based on\nGe crystals have been estimated. Some requirements for the radiopurity of the\nmaterials have been deduced from the results of these simulations. \n\n"}
{"id": "0807.1106", "contents": "Title: Principal components analysis for sparsely observed correlated\n  functional data using a kernel smoothing approach Abstract: In this paper, we consider the problem of estimating the covariance kernel\nand its eigenvalues and eigenfunctions from sparse, irregularly observed, noise\ncorrupted and (possibly) correlated functional data. We present a method based\non pre-smoothing of individual sample curves through an appropriate kernel. We\nshow that the naive empirical covariance of the pre-smoothed sample curves\ngives highly biased estimator of the covariance kernel along its diagonal. We\nattend to this problem by estimating the diagonal and off-diagonal parts of the\ncovariance kernel separately. We then present a practical and efficient method\nfor choosing the bandwidth for the kernel by using an approximation to the\nleave-one-curve-out cross validation score. We prove that under standard\nregularity conditions on the covariance kernel and assuming i.i.d. samples, the\nrisk of our estimator, under $L^2$ loss, achieves the optimal nonparametric\nrate when the number of measurements per curve is bounded. We also show that\neven when the sample curves are correlated in such a way that the noiseless\ndata has a separable covariance structure, the proposed method is still\nconsistent and we quantify the role of this correlation in the risk of the\nestimator. \n\n"}
{"id": "0807.1574", "contents": "Title: Large-Sample Confidence Intervals for the Treatment Difference in a\n  Two-Period Crossover Trial, Utilizing Prior Information Abstract: Consider a two-treatment, two-period crossover trial, with responses that are\ncontinuous random variables. We find a large-sample frequentist 1-alpha\nconfidence interval for the treatment difference that utilizes the uncertain\nprior information that there is no differential carryover effect. \n\n"}
{"id": "0807.2022", "contents": "Title: NLO contributions to $B \\to K K^*$ Decays in the pQCD approach Abstract: We calculate the important next-to-leading-order (NLO) contributions to the\n$B \\to K K^*$ decays from the vertex corrections, the quark loops, and the\nmagnetic penguins in the perturbative QCD (pQCD) factorization approach. The\npQCD predictions for the CP-averaged branching ratios are $Br(B^+ \\to K^+\n\\bar{K}^{*0}) \\approx 3.2\\times 10^{-7}$, $Br(B^+ \\to \\bar{K}^0 {K}^{*+})\n\\approx 2.1\\times 10^{-7}$, $Br(B^0/\\ov{B}^0 \\to K^0\\bar{K}^{*0}+\\bar{K}^0\nK^{*0}) \\approx 8.5\\times 10^{-7}$, $Br(B^0/\\ov{B}^0 \\to K^+K^{*-} + K^-K^{*+})\n\\approx 1.3\\times 10^{-7}$, which agree well with both the experimental upper\nlimits and the predictions based on the QCD factorization approach.\nFurthermore, the CP-violating asymmetries of the considered decay modes are\nalso evaluated. The NLO pQCD predictions for $\\acp(B^+ \\to K^+\\bar{K}^{*0})$\nand $\\acp(B^+ \\to K^{*+}\\bar{K}^{0})$ are $\\acp^{dir}(K^+\\bar{K}^{*0})\\approx\n-6.9 %$ and $\\acp^{dir}(K^{*+}\\bar{K}^0)\\approx 6.5 %$. \n\n"}
{"id": "0807.2324", "contents": "Title: Single spin asymmetry and five-quark components of the proton Abstract: We examine the single-spin asymmetry (SSA) caused by the five-quark\ncomponents of the proton for semi-inclusive electroproduction of charged pions\nin deep-inelastic scattering on a transversely polarized hydrogen target. The\nlarge SSA is considered to have close relation with quark orbital motion in the\nproton and suggests that the quark orbital angular momentum is nonzero. For the\nfive-quark $qqqq\\bar{q}$ components of the proton, the lowest configurations\nwith $qqqq$ system orbitally excited and the $\\bar{q}$ in the ground state\nwould give spin-orbit correlations naturally for the quarks in a polarized\nproton. We show that based on the basic reaction $\\gamma q \\to \\pi q'$, the\norbital-spin coupling of the probed quarks in the five-quark configuration\nleads to the single-spin asymmetry consistent with recent experiment results. \n\n"}
{"id": "0808.3511", "contents": "Title: Conditional probability based significance tests for sequential patterns\n  in multi-neuronal spike trains Abstract: In this paper we consider the problem of detecting statistically significant\nsequential patterns in multi-neuronal spike trains. These patterns are\ncharacterized by ordered sequences of spikes from different neurons with\nspecific delays between spikes. We have previously proposed a data mining\nscheme to efficiently discover such patterns which are frequent in the sense\nthat the count of non-overlapping occurrences of the pattern in the data stream\nis above a threshold. Here we propose a method to determine the statistical\nsignificance of these repeating patterns and to set the thresholds\nautomatically. The novelty of our approach is that we use a compound null\nhypothesis that includes not only models of independent neurons but also models\nwhere neurons have weak dependencies. The strength of interaction among the\nneurons is represented in terms of certain pair-wise conditional probabilities.\nWe specify our null hypothesis by putting an upper bound on all such\nconditional probabilities. We construct a probabilistic model that captures the\ncounting process and use this to calculate the mean and variance of the count\nfor any pattern. Using this we derive a test of significance for rejecting such\na null hypothesis. This also allows us to rank-order different significant\npatterns. We illustrate the effectiveness of our approach using spike trains\ngenerated from a non-homogeneous Poisson model with embedded dependencies. \n\n"}
{"id": "0810.2259", "contents": "Title: Hadronic quarkonium decays at order v^7 Abstract: We compute the complete imaginary part of the NRQCD Lagrangian at order 1/M^4\nin the heavy-quark mass expansion, which includes center of mass operators, and\nat order alpha_s^2 in the matching coefficients. We also compute the imaginary\npart of the NRQCD Lagrangian at order 1/M^6 and at order alpha_s^2 that\ncontributes to the S-wave and P-wave inclusive decay widths of heavy quarkonium\ninto light hadrons at order v^7 in the heavy-quark velocity expansion. If we\ncount alpha_s(M) ~ v^2, the calculation provides the complete next-to-leading\norder corrections to the P-wave hadronic widths, and in the original NRQCD\npower counting, the complete next-to-leading order corrections to the vector\nS-wave widths, and part of the next-to-next-to leading order corrections to the\npseudoscalar S-wave widths. In the S-wave case, we confirm previous findings\nand add new terms in a more conservative power counting. In the P-wave case,\nour results are in disagreement with previous ones. Constraints induced by\nPoincare' invariance on the NRQCD four-fermion sector are studied for the first\ntime and provide an additional check of the calculation. Perspectives for\nphenomenological applications are discussed. \n\n"}
{"id": "0810.3624", "contents": "Title: Heavy-light meson's physics in Lattice QCD Abstract: The possibility of revealing new physics by studying the flavor sector of the\nStandard Model strongly depends upon the accuracy that will be achieved in\n(near) future lattice QCD calculations and, in particular, on heavy-light\nmeson's observables. In turn, handling with heavy-light mesons on the lattice\nis a challenging problem, because of the presence of two largely separated\nenergy scales, and at present it is impossible to extract matrix elements\ninvolving B mesons in external states without recurring to some approximation.\nIn this note I give a fast overview of some of the methods that have been\ndevised to handle such kind of problems, emphasizing those based on finite\nvolume techniques, and briefly discuss some recent results obtained by their\napplication. \n\n"}
{"id": "0810.5103", "contents": "Title: Magnetization of the QCD vacuum at large fields Abstract: The response of the QCD vacuum to very large static external magnetic fields\n(q B >> Lambda_QCD^2) is studied. In this regime, the magnetization of the QCD\nvacuum is naturally described via perturbative QCD. Combining pQCD and the\nSchwinger proper time formalism, we calculate the magnetization of the QCD\nvacuum due to a strong magnetic field at leading order (one-loop) to be\nproportional to B log B. We show that the leading perturbative correction\n(two-loop) vanishes. \n\n"}
{"id": "0811.0812", "contents": "Title: Common gauge origin of discrete symmetries in observable sector and\n  hidden sector Abstract: An extra Abelian gauge symmetry is motivated in many new physics models in\nboth supersymmetric and nonsupersymmetric cases. Such a new gauge symmetry may\ninteract with both the observable sector and the hidden sector. We\nsystematically investigate the most general residual discrete symmetries in\nboth sectors from a common Abelian gauge symmetry. Those discrete symmetries\ncan ensure the stability of the proton and the dark matter candidate. A hidden\nsector dark matter candidate (lightest U-parity particle or LUP) interacts with\nthe standard model fields through the gauge boson Z', which may selectively\ncouple to quarks or leptons only. We make a comment on the implications of the\ndiscrete symmetry and the leptonically coupling dark matter candidate, which\nhas been highlighted recently due to the possibility of the simultaneous\nexplanation of the DAMA and the PAMELA results. We also show how to construct\nthe most general U(1) charges for a given discrete symmetry, and discuss the\nrelation between the U(1) gauge symmetry and R-parity. \n\n"}
{"id": "0811.1446", "contents": "Title: The Interaction Rate in Holographic Models of Dark Energy Abstract: Observational data from supernovae type Ia, baryon acoustic oscillations, gas\nmass fraction in galaxy clusters, and the growth factor are used to reconstruct\nthe the interaction rate of the holographic dark energy model recently proposed\nby Zimdahl and Pav\\'{o}n [1] in the redshift interval $0 < z < 1.8$. It shows a\nreasonable behavior as it increases with expansion from a small or vanishing\nvalue in the far past and begins decreasing at recent times. This suggests that\nthe equation of state parameter of dark energy does not cross the phantom\ndivide line. \n\n"}
{"id": "0811.3355", "contents": "Title: Approximate Bayesian computation (ABC) gives exact results under the\n  assumption of model error Abstract: Approximate Bayesian computation (ABC) or likelihood-free inference\nalgorithms are used to find approximations to posterior distributions without\nmaking explicit use of the likelihood function, depending instead on simulation\nof sample data sets from the model. In this paper we show that under the\nassumption of the existence of a uniform additive model error term, ABC\nalgorithms give exact results when sufficient summaries are used. This\ninterpretation allows the approximation made in many previous application\npapers to be understood, and should guide the choice of metric and tolerance in\nfuture work. ABC algorithms can be generalized by replacing the 0-1 cut-off\nwith an acceptance probability that varies with the distance of the simulated\ndata from the observed data. The acceptance density gives the distribution of\nthe error term, enabling the uniform error usually used to be replaced by a\ngeneral distribution. This generalization can also be applied to approximate\nMarkov chain Monte Carlo algorithms. In light of this work, ABC algorithms can\nbe seen as calibration techniques for implicit stochastic models, inferring\nparameter values in light of the computer model, data, prior beliefs about the\nparameter values, and any measurement or model errors. \n\n"}
{"id": "0812.0637", "contents": "Title: A note on large N scalar QCD_2 Abstract: We review the features of the bound state equation in large N scalar QCD in\ntwo dimensions, the 't Hooft model, and compute the discrete hadron mass\nspectrum in this theory. We make the Ansatz that the scalar fields of this\nmodel represent spin zero diquarks and we estimate the minimum allowed mass for\nthe first radial excitation of the lowest diquark-antidiquark scalar meson. The\ndiscussion is extended to the case of spin one diquarks. \n\n"}
{"id": "0812.1853", "contents": "Title: Quantized meson fields in and out of equilibrium. II: Chiral condensate\n  and collective meson excitations Abstract: We develop a quantum kinetic theory of the chiral condensate and meson\nquasi-particle excitations using the O(N) linear sigma model which describe the\nchiral phase transition both in and out of equilibrium in a unified way. A mean\nfield approximation is formulated in the presence of mesonic quasi-particle\nexcitations which are described by generalized Wigner functions. It is shown\nthat in equilibrium our kinetic equations reduce to the gap equations which\ndetermine the equilibrium condensate amplitude and the effective masses of the\nquasi-particle excitations, while linearization of transport equations, near\nsuch equilibrium, determine the dispersion relations of the collective mesonic\nexcitations at finite temperatures. Although all mass parameters for the meson\nexcitations become at finite temperature, apparently violating the Goldstone\ntheorem, the missing Nambu-Goldstone modes are retrieved in the collective\nexcitations of the system as three degenerate phonon-like modes in the\nsymmetry-broken phase. We show that the temperature dependence of the pole\nmasses of the collective pion excitations has non-analytic kink behavior at the\nthreshold of the quasi-particle excitations in the presence of explicit\nsymmetry breaking interaction. \n\n"}
{"id": "0812.3671", "contents": "Title: Regularized Multivariate Regression for Identifying Master Predictors\n  with Application to Integrative Genomics Study of Breast Cancer Abstract: In this paper, we propose a new method remMap -- REgularized Multivariate\nregression for identifying MAster Predictors -- for fitting multivariate\nresponse regression models under the high-dimension-low-sample-size setting.\nremMap is motivated by investigating the regulatory relationships among\ndifferent biological molecules based on multiple types of high dimensional\ngenomic data. Particularly, we are interested in studying the influence of DNA\ncopy number alterations on RNA transcript levels. For this purpose, we model\nthe dependence of the RNA expression levels on DNA copy numbers through\nmultivariate linear regressions and utilize proper regularizations to deal with\nthe high dimensionality as well as to incorporate desired network structures.\nCriteria for selecting the tuning parameters are also discussed. The\nperformance of the proposed method is illustrated through extensive simulation\nstudies. Finally, remMap is applied to a breast cancer study, in which genome\nwide RNA transcript levels and DNA copy numbers were measured for 172 tumor\nsamples. We identify a tran-hub region in cytoband 17q12-q21, whose\namplification influences the RNA expression levels of more than 30 unlinked\ngenes. These findings may lead to a better understanding of breast cancer\npathology. \n\n"}
{"id": "0901.0260", "contents": "Title: New Physics Contribution to Neutral Trilinear Gauge Boson Couplings Abstract: We study the one loop new physics effects to the CP even triple neutral gauge\nboson vertices $\\gamma^\\star \\gamma Z$, $\\gamma^\\star Z Z$, $Z^\\star Z \\gamma$\nand $Z^\\star Z Z$ in the context of Little Higgs models. We compute the\ncontribution of the additional fermions in Littles Higgs model in the framework\nof direct product groups where $[SU(2)\\times U(1)]^2 $ gauge symmetry is\nembedded in SU(5) global symmetry and also in the framework of simple group\nwhere $SU(N)\\times U(1)$ gauge symmetry breaks down to $SU(2)_L\\times U(1)$. We\ncalculate the contribution of the fermions to these couplings when $T$ parity\nis invoked. In addition, we re-examine the MSSM contribution at the chosen\npoint of SPS1a' and compare with the SM and Little Higgs models. \n\n"}
{"id": "0901.1784", "contents": "Title: Exact and Approximate Formulas for Neutrino Mixing and Oscillations with\n  Non-Standard Interactions Abstract: We present, both exactly and approximately, a complete set of mappings\nbetween the vacuum (or fundamental) leptonic mixing parameters and the\neffective ones in matter with non-standard neutrino interaction (NSI) effects\nincluded. Within the three-flavor neutrino framework and a constant matter\ndensity profile, a full set of sum rules is established, which enables us to\nreconstruct the moduli of the effective leptonic mixing matrix elements, in\nterms of the vacuum mixing parameters in order to reproduce the neutrino\noscillation probabilities for future long-baseline experiments. Very compact,\nbut quite accurate, approximate mappings are obtained based on series\nexpansions in the neutrino mass hierarchy parameter \\eta \\equiv \\Delta\nm^2_{21}/\\Delta m^2_{31}, the vacuum leptonic mixing parameter s_{13} \\equiv\n\\sin\\theta_{13}, and the NSI parameters \\epsilon_{\\alpha\\beta}. A detailed\nnumerical analysis about how the NSIs affect the smallest leptonic mixing angle\n\\theta_{13}, the deviation of the leptonic mixing angle \\theta_{23} from its\nmaximal mixing value, and the transition probabilities useful for future\nexperiments are performed using our analytical results. \n\n"}
{"id": "0901.4449", "contents": "Title: Nuclear Force from String Theory Abstract: We compute nuclear force in a holographic model of QCD on the basis of a\nD4-D8 brane configuration in type IIA string theory. Repulsive core of nucleons\nis quite important in nuclear physics, but its origin has not been\nwell-understood in strongly-coupled QCD. We find that string theory via\ngauge/string duality deduces this repulsive core at short distance between\nnucleons. Since baryons in the model are realized as solitons given by\nYang-Mills instanton configuration on flavor D8-branes, ADHM construction of\ntwo instantons probes well the nucleon interaction at short scale, which\nprovides the nuclear force quantitatively. We obtain, as well as a tensor\nforce, a central force which is strongly repulsive as suggested in experiments\nand lattice results. In particular, the nucleon-nucleon potential V(r) (as a\nfunction of the distance) scales as 1/r^2, which is peculiar to the holographic\nmodel. We compare our results with one-boson exchange model using the\nnucleon-nucleon-meson coupling obtained in our previous paper\n(arXiv:0806.3122). \n\n"}
{"id": "0902.2856", "contents": "Title: Heavy Quark Thermalization in Classical Lattice Gauge Theory: Lessons\n  for Strongly-Coupled QCD Abstract: Thermalization of a heavy quark near rest is controlled by the correlator of\ntwo electric fields along a temporal Wilson line. We address this correlator\nwithin real-time, classical lattice Yang-Mills theory, and elaborate on the\nanalogies that exist with the dynamics of hot QCD. In the weak-coupling limit,\nit can be shown analytically that the dynamics on the two sides are closely\nrelated to each other. For intermediate couplings, we carry out\nnon-perturbative simulations within the classical theory, showing that the\nleading term in the weak-coupling expansion significantly underestimates the\nheavy quark thermalization rate. Our analytic and numerical results also yield\na general understanding concerning the overall shape of the spectral function\ncorresponding to the electric field correlator, which may be helpful in\nsubsequent efforts to reconstruct it from Euclidean lattice Monte Carlo\nsimulations. \n\n"}
{"id": "0902.3725", "contents": "Title: Statistical Inference of Functional Connectivity in Neuronal Networks\n  using Frequent Episodes Abstract: Identifying the spatio-temporal network structure of brain activity from\nmulti-neuronal data streams is one of the biggest challenges in neuroscience.\nRepeating patterns of precisely timed activity across a group of neurons is\npotentially indicative of a microcircuit in the underlying neural tissue.\nFrequent episode discovery, a temporal data mining framework, has recently been\nshown to be a computationally efficient method of counting the occurrences of\nsuch patterns. In this paper, we propose a framework to determine when the\ncounts are statistically significant by modeling the counting process. Our\nmodel allows direct estimation of the strengths of functional connections\nbetween neurons with improved resolution over previously published methods. It\ncan also be used to rank the patterns discovered in a network of neurons\naccording to their strengths and begin to reconstruct the graph structure of\nthe network that produced the spike data. We validate our methods on simulated\ndata and present analysis of patterns discovered in data from cultures of\ncortical neurons. \n\n"}
{"id": "0902.3820", "contents": "Title: Lepton flavour violating stau decays versus seesaw parameters:\n  correlations and expected number of events for both seesaw type-I and II Abstract: In minimal supergravity (mSugra), the neutrino sector is related to the\nslepton sector by means of the renormalization group equations. This opens a\ndoor to indirectly test the neutrino sector via measurements at the LHC.\nConcretely, for the simplest seesaw type-I, we present the correlations between\nseesaw parameters and ratio of stau lepton flavour violating (LFV) branching\nratios. We find some simple, extreme scenarios for the unknown right-handed\nparameters, where ratios of LFV rates correlate with neutrino oscillation\nparameters. On the other hand, we scan the mSugra parameter space, for both\nseesaw type-I and II, to find regions where LFV stau decays can be maximized,\nwhile respecting low-energy experimental bounds. We estimate the expected\nnumber of events at the LHC for a sample luminosity of L = 100 fb^{-1}. \n\n"}
{"id": "0903.0564", "contents": "Title: Reconstruction of Quark Mass Matrices with Weak Basis Texture Zeroes\n  from Experimental Input Abstract: All quark mass matrices with texture zeroes obtained through weak basis\ntransformations are confronted with the experimental data. The reconstruction\nof the quark mass matrices M_u and M_d at the electroweak scale is performed in\na weak basis where the matrices are Hermitian and have a maximum of three\nvanishing elements. The same procedure is also accomplished for the Yukawa\ncoupling matrices at the grand unification scale in the context of the Standard\nModel and its minimal supersymmetric extension as well as of the two Higgs\ndoublet model. The analysis of all viable power structures on the quark Yukawa\ncoupling matrices that could naturally appear from a Froggatt-Nielsen mechanism\nis also presented. \n\n"}
{"id": "0903.1310", "contents": "Title: Pulsars versus Dark Matter Interpretation of ATIC/PAMELA Abstract: In this paper, we study the flux of electrons and positrons injected by\npulsars and by annihilating or decaying dark matter in the context of recent\nATIC, PAMELA, Fermi, and HESS data. We review the flux from a single pulsar and\nderive the flux from a distribution of pulsars. We point out that the particle\nacceleration in the pulsar magnetosphere is insufficient to explain the\nobserved excess of electrons and positrons with energy E ~ 1 TeV and one has to\ntake into account an additional acceleration of electrons at the termination\nshock between the pulsar and its wind nebula. We show that at energies less\nthan a few hundred GeV, the flux from a continuous distribution of pulsars\nprovides a good approximation to the expected flux from pulsars in the\nAustralia Telescope National Facility (ATNF) catalog. At higher energies, we\ndemonstrate that the electron/positron flux measured at the Earth will be\ndominated by a few young nearby pulsars, and therefore the spectrum would\ncontain bumplike features. We argue that the presence of such features at high\nenergies would strongly suggest a pulsar origin of the anomalous contribution\nto electron and positron fluxes. The absence of features either points to a\ndark matter origin or constrains pulsar models in such a way that the\nfluctuations are suppressed. Also we derive that the features can be partially\nsmeared due to spatial variation of the energy losses during propagation. \n\n"}
{"id": "0903.1994", "contents": "Title: Pion Form Factor in the NLC QCD SR approach Abstract: We present results of a calculation of the electromagnetic pion form factor\nwithin a framework of QCD Sum Rules with nonlocal condensates and using a\nperturbative spectral density which includes \\mathcal{O}(\\alpha_s)\ncontributions. \n\n"}
{"id": "0903.2475", "contents": "Title: Matter parity as the origin of scalar Dark Matter Abstract: We extend the concept of matter parity $P_M=(-1)^{3(B-L)}$ to\nnon-supersymmetric theories and argue that $P_M$ is the natural explanation to\nthe existence of Dark Matter of the Universe. We show that the\nnon-supersymmetric Dark Matter must be contained in scalar 16 representation(s)\nof $SO(10),$ thus the unique low energy Dark Matter candidates are $P_M$-odd\ncomplex scalar singlet(s) $S$ and inert scalar doublet(s) $H_2.$ We have\ncalculated the thermal relic Dark Matter abundance of the model and shown that\nits minimal form may be testable at LHC via the SM Higgs boson decays $H_1\\to\nDM DM.$ The PAMELA anomaly can be explained with the decays $DM\\to \\nu l W$\ninduced via seesaw-like operator which is additionally suppressed by Planck\nscale. Because the SM fermions are odd under matter parity too, the DM sector\nis just our scalar relative. \n\n"}
{"id": "0903.3002", "contents": "Title: Learning with Structured Sparsity Abstract: This paper investigates a new learning formulation called structured\nsparsity, which is a natural extension of the standard sparsity concept in\nstatistical learning and compressive sensing. By allowing arbitrary structures\non the feature set, this concept generalizes the group sparsity idea that has\nbecome popular in recent years. A general theory is developed for learning with\nstructured sparsity, based on the notion of coding complexity associated with\nthe structure. It is shown that if the coding complexity of the target signal\nis small, then one can achieve improved performance by using coding complexity\nregularization methods, which generalize the standard sparse regularization.\nMoreover, a structured greedy algorithm is proposed to efficiently solve the\nstructured sparsity problem. It is shown that the greedy algorithm\napproximately solves the coding complexity optimization problem under\nappropriate conditions. Experiments are included to demonstrate the advantage\nof structured sparsity over standard sparsity on some real applications. \n\n"}
{"id": "0903.4010", "contents": "Title: Scalar Multiplet Dark Matter Abstract: We perform a systematic study of the phenomenology associated to models where\nthe dark matter consists in the neutral component of a scalar SU(2)_L n-uplet,\nup to n=7. If one includes only the pure gauge induced annihilation\ncross-sections it is known that such particles provide good dark matter\ncandidates, leading to the observed dark matter relic abundance for a\nparticular value of their mass around the TeV scale. We show that these values\nactually become ranges of values -which we determine- if one takes into account\nthe annihilations induced by the various scalar couplings appearing in these\nmodels. This leads to predictions for both direct and indirect detection\nsignatures as a function of the dark matter mass within these ranges. Both can\nbe largely enhanced by the quartic coupling contributions. We also explain how,\nif one adds right-handed neutrinos to the scalar doublet case, the results of\nthis analysis allow to have altogether a viable dark matter candidate,\nsuccessful generation of neutrino masses, and leptogenesis in a particularly\nminimal way with all new physics at the TeV scale. \n\n"}
{"id": "0903.5494", "contents": "Title: CP violation in the effective action of the Standard Model Abstract: Following a suggestion by Smit, the CP odd terms of the effective action of\nthe Standard Model, obtained by integration of quarks and leptons, are computed\nto sixth order within a strict covariant derivative expansion approach. No\nother approximations are made. The final result so derived includes all\nStandard Model gauge fields and Higgs. Remarkably, at the order considered in\nthis work, all parity violating contributions turn out to be zero. Non\nvanishing CP violating terms are obtained in the C-odd P-even sector. These are\nseveral orders of magnitude larger than perturbative estimates. Various\nunexpected regularities in the final result are noted. \n\n"}
{"id": "0904.0866", "contents": "Title: Neutral quark matter in a Nambu-Jona Lasinio model with vector\n  interaction Abstract: We investigate the three flavor Nambu-Jona Lasinio model of neutral quark\nmatter at zero temperature and finite density, keeping into account the scalar,\nthe pseudoscalar and the Kobayashi-Maskawa-'t Hooft interactions as well as the\nrepulsive vector plus axial-vector interaction terms (vector extended NJL,\nVENJL in the following). We focus on the effect of the vector interaction on\nthe chiral restoration at finite density in neutral matter. We also study the\nevolution of the charged pseudoscalar meson energies as a function of the quark\nchemical potential. \n\n"}
{"id": "0904.2207", "contents": "Title: Delayed rejection schemes for efficient Markov-Chain Monte-Carlo\n  sampling of multimodal distributions Abstract: A number of problems in a variety of fields are characterised by target\ndistributions with a multimodal structure in which the presence of several\nisolated local maxima dramatically reduces the efficiency of Markov Chain Monte\nCarlo sampling algorithms. Several solutions, such as simulated tempering or\nthe use of parallel chains, have been proposed to facilitate the exploration of\nthe relevant parameter space. They provide effective strategies in the cases in\nwhich the dimension of the parameter space is small and/or the computational\ncosts are not a limiting factor. These approaches fail however in the case of\nhigh-dimensional spaces where the multimodal structure is induced by\ndegeneracies between regions of the parameter space. In this paper we present a\nfully Markovian way to efficiently sample this kind of distribution based on\nthe general Delayed Rejection scheme with an arbitrary number of steps, and\nprovide details for an efficient numerical implementation of the algorithm. \n\n"}
{"id": "0905.1348", "contents": "Title: Note on renormalization of the spin-1 resonance propagator at one loop\n  order Abstract: We study various aspects of the renormalization of the Resonance Chiral\nTheory at the one-loop level using a spin-one resonance propagator as a\nconcrete example. We calculate explicitly the one-loop self-energy within the\nantisymmetric tensor field formalism, briefly discuss the general structure of\nthe corresponding propagator obtained by means of the Dyson re-summation and\ngive a classification of the propagating degrees of freedom. We find that\nadditional pathological poles (negative norm ghosts or tachyons) are\nunavoidably generated and various scenarios according to their position are\npossible. We also briefly comment on the eventual dynamical generation of the\nopposite parity resonances which are frozen at the tree level and discuss the\nrole of appropriate symmetry which could prevent such a scenario. \n\n"}
{"id": "0906.1997", "contents": "Title: Viability of $\\Delta m^2\\sim$ 1 eV$^2$ sterile neutrino mixing models in\n  light of MiniBooNE electron neutrino and antineutrino data from the Booster\n  and NuMI beamlines Abstract: This paper examines sterile neutrino oscillation models in light of recently\npublished results from the MiniBooNE Experiment. The new MiniBooNE data include\nthe updated neutrino results, including the low energy region, and the first\nantineutrino results, as well as first results from the off-axis NuMI beam\nobserved in the MiniBooNE detector. These new global fits also include data\nfrom LSND, KARMEN, NOMAD, Bugey, CHOOZ, CCFR84, and CDHS. Constraints from\natmospheric oscillation data have been imposed. \n\n"}
{"id": "0906.2898", "contents": "Title: Casimir Effect on the brane Abstract: We consider the Casimir effect between two parallel plates localized on a\nbrane. In order to properly compute the contribution to the Casimir energy due\nto any higher dimensional field, it is necessary to take into account the\nlocalization properties of the KK modes. When no massless mode appears in the\nspectrum, the correction to the Casimir energy is exponentially suppressed.\nWhen a massless mode is present in the spectrum, the correction to the Casimir\nenergy can be, in principle, sizeable. Here we illustrate a new method to\ncompute the correction to the Casimir energy between two parallel plates,\nlocalized on a brane. The Casimir energy is suppressed by two factors: at\nlowest order in $\\varepsilon$, the correction comes entirely from the massive\nmode and turns out to be exponentially suppressed; the next-to-leading order\ncorrection in $\\varepsilon$ follows, instead, a power-law suppression due to\nthe small wave function overlap of the zero-mode with matter confined on the\nvisible brane. Generic comments on the constraints on new physics that may\narise from Casimir force experiments are also made. \n\n"}
{"id": "0906.3501", "contents": "Title: Semiparametric modeling of autonomous nonlinear dynamical systems with\n  applications Abstract: In this paper, we propose a semi-parametric model for autonomous nonlinear\ndynamical systems and devise an estimation procedure for model fitting. This\nmodel incorporates subject-specific effects and can be viewed as a nonlinear\nsemi-parametric mixed effects model. We also propose a computationally\nefficient model selection procedure. We prove consistency of the proposed\nestimator under suitable regularity conditions. We show by simulation studies\nthat the proposed estimation as well as model selection procedures can\nefficiently handle sparse and noisy measurements. Finally, we apply the\nproposed method to a plant growth data used to study growth displacement rates\nwithin meristems of maize roots under two different experimental conditions. \n\n"}
{"id": "0906.4052", "contents": "Title: Matching NLO parton shower matrix element with exact phase space: case\n  of W -> l nu (gamma) and gamma^* -> pi^+pi^-(gamma) Abstract: The PHOTOS Monte Carlo is often used for simulation of QED effects in decay\nof intermediate particles and resonances. Momenta are generated in such a way\nthat samples of events cover the whole bremsstrahlung phase space. With the\nhelp of selection cuts, experimental acceptance can be then taken into account.\nThe program is based on an exact multiphoton phase space. Crude matrix element\nis obtained by iteration of a universal multidimensional kernel. It ensures\nexact distribution in the soft photon region. Algorithm is compatible with\nexclusive exponentiation. To evaluate the program's precision, it is necessary\nto control the kernel with the help of perturbative results. If available,\nkernel is constructed from the exact first order matrix element. This ensures\nthat all terms necessary for non-leading logarithms are taken into account. In\nthe present paper we will focus on the W -> l nu and gamma^* -> pi^+ pi^-\ndecays. The Born level cross sections for both processes approach zero in some\npoints of the phase space. A process dependent compensating weight is\nconstructed to incorporate the exact matrix element, but is recommended for use\nin tests only. In the hard photon region, where scalar QED is not expected to\nbe reliable, the compensating weight for gamma^* decay can be large. With\nrespect to the total rate, the effect remains at the permille level. It is\nnonetheless of interest. The terms leading to the effect are analogous to some\nterms appearing in QCD. The present paper can be understood either as a\ncontribution to discussion on how to match two collinear emission chains\nresulting from charged sources in a way compatible with the exact and complete\nphase space, exclusive exponentiation and the first order matrix element of QED\n(scalar QED), or as the practical study of predictions for accelerator\nexperiments. \n\n"}
{"id": "0906.4335", "contents": "Title: Testing the Scale Dependence of the Scale Factor in Double Dijet\n  Production at the LHC Abstract: The scale factor is the effective cross section used to characterize the\nmeasured rate of inclusive double dijet production in high energy hadron\ncollisions. It is sensitive to the two-parton distributions in the hadronic\nprojectile. In principle, the scale factor depends on the center of mass energy\nand on the minimal transverse energy of the jets contributing to the double\ndijet cross section. Here, we point out that proton-proton collisions at the\nLHC will provide for the first time experimental access to these scale\ndependences in a logarithmically wide, nominally perturbative kinematic range\nof minimal transverse energy between 10 GeV and 100 GeV. This constrains the\ndependence of two-parton distribution functions on parton momentum fractions\nand parton localization in impact parameter space. Novel information is to be\nexpected about the transverse growth of hadronic distribution functions in the\nrange of semi-hard Bjorken x (0.001 < x < 0.1) and high resolution Q^2. We\ndiscuss to what extent one can disentangle different pictures of the\n$x$-evolution of two-parton distributions in the transverse plane by measuring\ndouble-hard scattering events at the LHC. \n\n"}
{"id": "0906.5207", "contents": "Title: Electroweak two-loop contribution to the mass splitting within a new\n  heavy SU(2)$_L$ fermion multiplet Abstract: New heavy particles in an SU(2)_L multiplet, sometimes introduced in\nextensions of the standard model, have highly degenerate tree-level mass M if\ntheir couplings to the Higgs bosons are very small or forbidden. However, loop\ncorrections may generate the gauge-symmetry-breaking mass splitting within the\nmultiplet, which does not vanish in the large M limit due to the threshold\nsingularity. We calculate the electroweak contribution to the mass splitting\nfor a heavy fermion multiplet, to the two-loop order. Numerically, two-loop\nelectroweak contributions are typically O(MeV). \n\n"}
{"id": "0907.1324", "contents": "Title: QCD corrections to associated production of $t\\bar t\\gamma$ at hadron\n  colliders Abstract: We report on the next-to-leading order(NLO) QCD computation of top-quark pair\nproduction in association with a photon at the Fermilab Tevatron RUN II and\nCERN Large Hadron Collider(LHC). We describe the impact of the complete NLO QCD\nradiative corrections to this process, and provide the predictions of the\nleading order(LO) and NLO integrated cross sections, distributions of the\ntransverse momenta of top-quark and photon for the LHC and Tevatron, and the LO\nand NLO forward-backward top-quark charge asymmetries for the Tevatron. We\ninvestigate the dependence of the LO and NLO cross sections on the\nrenormalization/factorization scale, and find the scale dependence of the LO\ncross section is obviously improved by the NLO QCD corrections. The K-factor of\nthe NLO QCD correction is $0.977(1.524)$ for the Tevatron(LHC). \n\n"}
{"id": "0907.1894", "contents": "Title: Dark Matter as the signal of Grand Unification Abstract: We argue that the existence of Dark Matter (DM) is a possible consequence of\nGUT symmetry breaking. In GUTs like SO(10), discrete Z_2 matter parity\n(-1)^{3(B-L)} survives despite of broken B-L, and group theory uniquely\ndetermines that the only possible Z_2-odd matter multiplets belong to\nrepresentation 16. We construct the minimal non-SUSY SO(10) model containing\none scalar 16 for DM and study its predictions below M_{G}. We find that EWSB\noccurs radiatively due to DM couplings to the SM Higgs boson. For thermal relic\nDM the mass range M_{DM}\\sim (0.1-1) TeV is predicted by model perturbativity\nup to M_{G}. For M_{DM}\\sim (1) TeV to explain the observed cosmic ray\nanomalies with DM decays, there exists a lower bound on the spin-independent\ndirect detection cross section within the reach of planned experiments. \n\n"}
{"id": "0907.2715", "contents": "Title: Heterotic N=(0,2) CP(N-1) Model with Twisted Masses Abstract: We present a two-dimensional heterotic N=(0,2) CP(N-1) model with twisted\nmasses. It is supposed to describe internal dynamics of non-Abelian strings in\nmassive N=2 SQCD with N=1-preserving deformations. We present gauge and\ngeometric formulations of the world-sheet theory and check its N=(0,2)\nsupersymmetry. It turns out that the set of twisted masses in the heterotic\nmodel has N complex mass parameters, rather than N-1. In the general case, when\nall mass parameters are nonvanishing, N=(0,2) supersymmetry is spontaneously\nbroken already at the classical level. If at least one of the above mass\nparameters vanishes, then N=(0,2) is unbroken at the classical level. The\nspontaneous breaking of supersymmetry in this case occurs through\nnonperturbative effects. \n\n"}
{"id": "0907.3074", "contents": "Title: LARGE EXTRA DIMENSIONS: Becoming acquainted with an alternative paradigm Abstract: This is a colloquium style pedagogical introduction to the paradigm of large\nextra dimensions. To be published in the Proceedings of the Workshop \"Crossing\nthe boundaries: Gauge dynamics at strong coupling,\" (May 14 - 17, 2009,\nMinneapolis), Eds. M. Peloso and A. Vainshtein, (World Scientific, Singapore,\n2009), p. 3. \n\n"}
{"id": "0907.3931", "contents": "Title: Mass-Matching in Higgsless Abstract: Modern extra-dimensional Higgsless scenarios rely on a mass-matching between\nfermionic and bosonic KK resonances to evade constraints from precision\nelectroweak measurements. After analyzing all of the Tevatron and LEP bounds on\nthese so-called Cured Higgsless scenarios, we study their LHC signatures and\nexplore how to identify the mass-matching mechanism, the key to their\nviability. We find singly and pair produced fermionic resonances show up as\nclean signals with 2 or 4 leptons and 2 hard jets, while neutral and charged\nbosonic resonances are visible in the dilepton and leptonic WZ channels,\nrespectively. A measurement of the resonance masses from these channels shows\nthe matching necessary to achieve $S\\simeq 0$. Moreover, a large single\nproduction of KK-fermion resonances is a clear indication of compositeness of\nSM quarks. Discovery reach is below 10 fb$^{-1}$ of luminosity for resonances\nin the 700 GeV range. \n\n"}
{"id": "0907.3953", "contents": "Title: The Fermi gamma-ray spectrum of the inner galaxy: Implications for\n  annihilating dark matter Abstract: Recently, a preliminary spectrum from the Fermi Gamma-ray Space Telescope has\nbeen presented for the inner galaxy (-30 < l < 30, -5 < b < 5), as well as the\ngalactic center (-1 < l < 1, -1< b < 1). We consider the implications of these\ndata for dark matter annihilation models, especially models capable of\nproducing the cosmic-ray excesses previously observed by PAMELA and Fermi.\nThese local cosmic-ray excesses, when extrapolated to the inner galaxy, imply\ninverse Compton scattering (ICS) gamma-ray signals largely consistent with the\npreliminary Fermi gamma-ray spectrum. For specific halos and models,\nparticularly those with prompt photons, the data have begun to constrain the\nallowed parameter space. Although significant modeling and background\nuncertainties remain, we explore how large a signal is permitted by the current\ndata. Based upon this, we make predictions for what signal could be present in\nother regions of the sky where dark matter signals may be easier to isolate\nfrom the astrophysical backgrounds. \n\n"}
{"id": "0907.5487", "contents": "Title: Short-Baseline Electron Neutrino Disappearance at a Neutrino Factory Abstract: We discuss short-baseline and very-short-baseline electron neutrino\ndisappearance at a neutrino factory. We take into account geometric effects,\nsuch as from averaging over the decay straights, and the uncertainties of the\ncross sections. We follow an approach similar to reactor experiments with two\ndetectors: we use two sets of near detectors at different distances to cancel\nsystematics. We demonstrate that such a setup is very robust with respect to\nsystematics, and can have excellent sensitivities to the effective mixing angle\nand squared-mass splitting. In addition, we allow for CPT invariance violation,\nwhich can be tested (depending on the parameters) up to a 0.1% level. \n\n"}
{"id": "0907.5589", "contents": "Title: On the Sunyaev-Zel'dovich effect from dark matter annihilation or decay\n  in galaxy clusters Abstract: We revisit the prospects for detecting the Sunyaev Zel'dovich (SZ) effect\ninduced by dark matter (DM) annihilation or decay. We show that with standard\n(or even extreme) assumptions for DM properties, the optical depth associated\nwith relativistic electrons injected from DM annihilation or decay is much\nsmaller than that associated with thermal electrons, when averaged over the\nangular resolution of current and future experiments. For example, we find:\n$\\tau_{\\rm DM} \\sim 10^{-9}-10^{-5}$ (depending on the assumptions) for $\\mchi\n= 1$ GeV and a density profile $\\rho\\propto r^{-1}$ for a template cluster\nlocated at 50 Mpc and observed within an angular resolution of $10\"$, compared\nto $\\tau_{\\rm th}\\sim 10^{-3}-10^{-2}$. This, together with a full spectral\nanalysis, enables us to demonstrate that, for a template cluster with generic\nproperties, the SZ effect due to DM annihilation or decay is far below the\nsensitivity of the Planck satellite. This is at variance with previous claims\nregarding heavier annihilating DM particles. Should DM be made of lighter\nparticles, the current constraints from 511 keV observations on the\nannihilation cross section or decay rate still prevent a detectable SZ effect.\nFinally, we show that spatial diffusion sets a core of a few kpc in the\nelectron distribution, even for very cuspy DM profiles, such that improving the\nangular resolution of the instrument, e.g. with ALMA, does not necessarily\nimprove the detection potential. We provide useful analytical formulae\nparameterized in terms of the DM mass, decay rate or annihilation cross section\nand DM halo features, that allow quick estimates of the SZ effect induced by\nany given candidate and any DM halo profile. \n\n"}
{"id": "0908.2312", "contents": "Title: Holographic scalar mesons Abstract: A holographic description of scalar mesons is presented, in which two- and\nthree-point functions are holographically reconstructed. Mass spectrum, decay\nconstants, eigenfunctions and the coupling of the scalar states with two pseu-\ndoscalars are found. A comparison of the results with current phenomenology is\ndiscussed. \n\n"}
{"id": "0908.3163", "contents": "Title: Nonparametric estimation of the volatility function in a high-frequency\n  model corrupted by noise Abstract: We consider the models Y_{i,n}=\\int_0^{i/n}\n\\sigma(s)dW_s+\\tau(i/n)\\epsilon_{i,n}, and \\tilde\nY_{i,n}=\\sigma(i/n)W_{i/n}+\\tau(i/n)\\epsilon_{i,n}, i=1,...,n, where W_t\ndenotes a standard Brownian motion and \\epsilon_{i,n} are centered i.i.d.\nrandom variables with E(\\epsilon_{i,n}^2)=1 and finite fourth moment.\nFurthermore, \\sigma and \\tau are unknown deterministic functions and W_t and\n(\\epsilon_{1,n},...,\\epsilon_{n,n}) are assumed to be independent processes.\nBased on a spectral decomposition of the covariance structures we derive series\nestimators for \\sigma^2 and \\tau^2 and investigate their rate of convergence of\nthe MISE in dependence of their smoothness. To this end specific basis\nfunctions and their corresponding Sobolev ellipsoids are introduced and we show\nthat our estimators are optimal in minimax sense. Our work is motivated by\nmicrostructure noise models. Our major finding is that the microstructure noise\n\\epsilon_{i,n} introduces an additionally degree of ill-posedness of 1/2;\nirrespectively of the tail behavior of \\epsilon_{i,n}. The method is\nillustrated by a small numerical study. \n\n"}
{"id": "0909.3052", "contents": "Title: Cross-Validation for Unsupervised Learning Abstract: Cross-validation (CV) is a popular method for model-selection. Unfortunately,\nit is not immediately obvious how to apply CV to unsupervised or exploratory\ncontexts. This thesis discusses some extensions of cross-validation to\nunsupervised learning, specifically focusing on the problem of choosing how\nmany principal components to keep. We introduce the latent factor model, define\nan objective criterion, and show how CV can be used to estimate the intrinsic\ndimensionality of a data set. Through both simulation and theory, we\ndemonstrate that cross-validation is a valuable tool for unsupervised learning. \n\n"}
{"id": "0909.3974", "contents": "Title: Generalized Dalitz Plot analysis of the near threshold pp-->ppK+K-\n  reaction in view of the K+K- final state interaction Abstract: The excitation function for the $pp\\to ppK^+K^-$ reaction revealed a\nsignificant enhancement close to threshold which may plausibly be assigned to\nthe influence of the $pK^-$ and $K^+K^-$ final state interactions. In an\nimproved reanalysis of COSY-11 data for the $pp\\to ppK^+K^-$ reaction at excess\nenergies of Q = 10 MeV and 28 MeV including the proton-K- interaction the\nenhancement is confirmed. Invariant mass distributions for the two- and\nthree-particle subsystems allow to test at low excess energies the ansatz and\nparameters for the description of the interaction in the ppK+K- system as\nderived from the COSY-ANKE data. Finally, based for the first time on the low\nenergy K+K- invariant mass distributions and the generalized Dalitz plot\nanalysis, we estimate the scattering length for the K+K- interaction to be\n|Re(a_K^+K^-)| = 0.5 + 4.0 -0.5 fm and Im(a_K^+K^-) = 3.0 +- 3.0 fm. \n\n"}
{"id": "0910.2497", "contents": "Title: Maximum entropy Edgeworth estimates of the number of integer points in\n  polytopes Abstract: Abstract: The number of points $x=(x_1 ,x_2 ,...x_n)$ that lie in an integer\ncube $C$ in $R^n$ and satisfy the constraints $\\sum_j h_{ij}(x_j )=s_i ,1\\le\ni\\le d$ is approximated by an Edgeworth-corrected Gaussian formula based on the\nmaximum entropy density $p$ on $x \\in C$, that satisfies $E\\sum_j h_{ij}(x_j\n)=s_i ,1\\le i\\le d$. Under $p$, the variables $X_1 ,X_2 ,...X_n $ are\nindependent with densities of exponential form. Letting $S_i$ denote the random\nvariable $\\sum_j h_{ij}(X_j )$, conditional on $S=s, X$ is uniformly\ndistributed over the integers in $C$ that satisfy $S=s$. The number of points\nin $C$ satisfying $S=s$ is $p \\{S=s\\}\\exp (I(p))$ where $I(p)$ is the entropy\nof the density $p$. We estimate $p \\{S=s\\}$ by $p_Z(s)$, the density at $s$ of\nthe multivariate Gaussian $Z$ with the same first two moments as $S$; and when\n$d$ is large we use in addition an Edgeworth factor that requires the first\nfour moments of $S$ under $p$. The asymptotic validity of the\nEdgeworth-corrected estimate is proved and demonstrated for counting\ncontingency tables with given row and column sums as the number of rows and\ncolumns approaches infinity, and demonstrated for counting the number of graphs\nwith a given degree sequence, as the number of vertices approaches infinity. \n\n"}
{"id": "0910.4558", "contents": "Title: Effect of indirect dependencies on \"A mutual information minimization\n  approach for a class of nonlinear recurrent separating systems\" Abstract: In a recent paper [4], Duarte and Jutten investigated the Blind Source\nSeparation (BSS) problem, for the nonlinear mixing model that they introduced\nin that paper. They proposed to solve this problem by using\ninformation-theoretic tools, more precisely by minimizing the mutual\ninformation (MI) of the outputs of the separating structure. When applying the\nMI approach to BSS problems, one usually determines the analytical expressions\nof the derivatives of the MI with respect to the parameters of the considered\nseparating model. In the literature, these calculations were mainly reported\nfor linear mixtures up to now. They are more complex for nonlinear mixtures,\ndue to dependencies between the considered quantities. Moreover, the notations\ncommonly employed by the BSS community in such calculations may become\nmisleading when using them for nonlinear mixtures, due to the above-mentioned\ndependencies. We claim that the calculations reported in [4] contain an error,\nbecause they did not take into account all these dependencies. In this\ndocument, we therefore explain this phenomenon, by showing the effect of\nindirect dependencies on the application of the MI approach to the mixing and\nseparating models considered in [4]. We thus introduce a corrected expression\nof the gradient of the considered BSS criterion based on MI. This correct\ngradient may then e.g. be used to optimize the adaptive coefficients of the\nconsidered separating system by means of the well-known gradient descent\nalgorithm. As explained hereafter, this investigation has some similarities\nwith an analysis that we previously reported in another arXiv document [3].\nHowever, these two investigations concern different problems (mixture and\nseparating structure, mathematical tools: see paper). \n\n"}
{"id": "0911.0806", "contents": "Title: Fermion correction to the mass of the scalar glueball in QCD sum rule Abstract: Contributions of fermions to the mass of the scalar glueball $0^{++}$ are\ncalculated at two-loop level in the framework of QCD sum rules. It obviously\nchanges the coefficients in the operator product expansion (OPE) and shifts the\nmass of glueball. \n\n"}
{"id": "0911.1120", "contents": "Title: Freeze-In Production of FIMP Dark Matter Abstract: We propose an alternate, calculable mechanism of dark matter genesis,\n\"thermal freeze-in,\" involving a Feebly Interacting Massive Particle (FIMP)\ninteracting so feebly with the thermal bath that it never attains thermal\nequilibrium. As with the conventional \"thermal freeze-out\" production\nmechanism, the relic abundance reflects a combination of initial thermal\ndistributions together with particle masses and couplings that can be measured\nin the laboratory or astrophysically. The freeze-in yield is IR dominated by\nlow temperatures near the FIMP mass and is independent of unknown UV physics,\nsuch as the reheat temperature after inflation. Moduli and modulinos of string\ntheory compactifications that receive mass from weak-scale supersymmetry\nbreaking provide implementations of the freeze-in mechanism, as do models that\nemploy Dirac neutrino masses or GUT-scale-suppressed interactions. Experimental\nsignals of freeze-in and FIMPs can be spectacular, including the production of\nnew metastable coloured or charged particles at the LHC as well as the\nalteration of big bang nucleosynthesis. \n\n"}
{"id": "0912.1545", "contents": "Title: Dark matter stability and unification without supersymmetry Abstract: In the absence of low energy supersymmetry, we show that (a) the dark matter\nparticle alone at the TeV scale can improve gauge coupling unification, raising\nthe unification scale up to the lower bound imposed by proton decay, and (b)\nthe dark matter stability can automatically follow from the grand unification\nsymmetry. Within reasonably simple unified models, a unique candidate\nsatisfying these two properties is singled out: a fermion isotriplet with zero\nhypercharge, member of a 45 (or larger) representation of SO(10). We discuss\nthe phenomenological signatures of this TeV scale fermion, which can be tested\nin direct and indirect future dark matter searches. The proton decay rate into\ne^+ \\pi^0 is predicted close to the present bound. \n\n"}
{"id": "0912.1796", "contents": "Title: On the vacuum of the minimal nonsupersymmetric SO(10) unification Abstract: We study a class of nonsupersymmetric SO(10) grand unified scenarios where\nthe first stage of the symmetry breaking is driven by the vacuum expectation\nvalues of the 45-dimensional adjoint representation. Three decade old results\nclaim that such a Higgs setting may lead exclusively to the flipped SU(5) x\nU(1) intermediate stage. We show that this conclusion is actually an artifact\nof the tree level potential. The study of the accidental global symmetries\nemerging in various limits of the scalar potential offers a simple\nunderstanding of the tree level result and a rationale for the drastic impact\nof quantum corrections. We scrutinize in detail the simplest and paradigmatic\ncase of the 45_{H} + 16_{H} Higgs sector triggering the breaking of SO(10) to\nthe standard electroweak model. We show that the minimization of the one-loop\neffective potential allows for intermediate SU(4)_C x SU(2)_L x U(1)_R and\nSU(3)_c x SU(2)_L x SU(2)_R x U(1)_{B-L} symmetric stages as well. These are\nthe options favoured by gauge unification. Our results, that apply whenever the\nSO(10) breaking is triggered by <45_H>, open the path for hunting the simplest\nrealistic scenario of nonsupersymmetric SO(10) grand unification. \n\n"}
{"id": "1001.2185", "contents": "Title: Improved estimators for dispersion models with dispersion covariates Abstract: In this paper we discuss improved estimators for the regression and the\ndispersion parameters in an extended class of dispersion models (J{\\o}rgensen,\n1996). This class extends the regular dispersion models by letting the\ndispersion parameter vary throughout the observations, and contains the\ndispersion models as particular case. General formulae for the second-order\nbias are obtained explicitly in dispersion models with dispersion covariates,\nwhich generalize previous results by Botter and Cordeiro (1998), Cordeiro and\nMcCullagh (1991), Cordeiro and Vasconcellos (1999), and Paula (1992). The\npractical use of the formulae is that we can derive closed-form expressions for\nthe second-order biases of the maximum likelihood estimators of the regression\nand dispersion parameters when the information matrix has a closed-form.\nVarious expressions for the second-order biases are given for special models.\nThe formulae have advantages for numerical purposes because they require only a\nsupplementary weighted linear regression. We also compare these bias-corrected\nestimators with two different estimators which are also bias-free to the\nsecond-order that are based on bootstrap methods. These estimators are compared\nby simulation. \n\n"}
{"id": "1001.2187", "contents": "Title: Skewness of maximum likelihood estimators in dispersion models Abstract: We introduce the dispersion models with a regression structure to extend the\ngeneralized linear models, the exponential family nonlinear models (Cordeiro\nand Paula, 1989) and the proper dispersion models (J{\\o}rgensen, 1997a). We\nprovide a matrix expression for the skewness of the maximum likelihood\nestimators of the regression parameters in dispersion models. The formula is\nsuitable for computer implementation and can be applied for several important\nsubmodels discussed in the literature. Expressions for the skewness of the\nmaximum likelihood estimators of the precision and dispersion parameters are\nalso derived. In particular, our results extend previous formulas obtained by\nCordeiro and Cordeiro (2001) and Cavalcanti et al. (2009). A simulation study\nis perfomed to show the practice importance of our results. \n\n"}
{"id": "1001.4351", "contents": "Title: Analytical continuation of imaginary axis data using maximum entropy Abstract: We study the maximum entropy (MaxEnt) approach for analytical continuation of\nspectral data from imaginary times to real frequencies. The total error is\ndivided in a statistical error, due to the noise in the input data, and a\nsystematic error, due to deviations of the default function, used in the MaxEnt\napproach, from the exact spectrum. We find that the MaxEnt approach in its\nclassical formulation can lead to a nonoptimal balance between the two types of\nerrors, leading to an unnecessary large statistical error. The statistical\nerror can be reduced by splitting up the data in several batches, performing a\nMaxEnt calculation for each batch and averaging. This can outweigh an increase\nin the systematic error resulting from this approach. The output from the\nMaxEnt result can be used as a default function for a new MaxEnt calculation.\nSuch iterations often lead to worse results due to an increase in the\nstatistical error. By splitting up the data in batches, the statistical error\nis reduced and and the increase resulting from iterations can be outweighed by\na decrease in the systematic error. Finally we consider a linearized version to\nobtain a better understanding of the method. \n\n"}
{"id": "1001.4775", "contents": "Title: String Correction for Baryon Orbital Excitations Abstract: The correction to the string junction three-quark potential in a baryon due\nto the proper moment of inertia of the QCD string is calculated. The magnitudes\nof the string corrections in P-wave heavy baryons are estimated. \n\n"}
{"id": "1002.4658", "contents": "Title: Principal Component Analysis with Contaminated Data: The High\n  Dimensional Case Abstract: We consider the dimensionality-reduction problem (finding a subspace\napproximation of observed data) for contaminated data in the high dimensional\nregime, where the number of observations is of the same magnitude as the number\nof variables of each observation, and the data set contains some (arbitrarily)\ncorrupted observations. We propose a High-dimensional Robust Principal\nComponent Analysis (HR-PCA) algorithm that is tractable, robust to contaminated\npoints, and easily kernelizable. The resulting subspace has a bounded deviation\nfrom the desired one, achieves maximal robustness -- a breakdown point of 50%\nwhile all existing algorithms have a breakdown point of zero, and unlike\nordinary PCA algorithms, achieves optimality in the limit case where the\nproportion of corrupted points goes to zero. \n\n"}
{"id": "1003.0747", "contents": "Title: Asymptotic Results on Adaptive False Discovery Rate Controlling\n  Procedures Based on Kernel Estimators Abstract: The False Discovery Rate (FDR) is a commonly used type I error rate in\nmultiple testing problems. It is defined as the expected False Discovery\nProportion (FDP), that is, the expected fraction of false positives among\nrejected hypotheses. When the hypotheses are independent, the\nBenjamini-Hochberg procedure achieves FDR control at any pre-specified level.\nBy construction, FDR control offers no guarantee in terms of power, or type II\nerror. A number of alternative procedures have been developed, including\nplug-in procedures that aim at gaining power by incorporating an estimate of\nthe proportion of true null hypotheses. In this paper, we study the asymptotic\nbehavior of a class of plug-in procedures based on kernel estimators of the\ndensity of the $p$-values, as the number $m$ of tested hypotheses grows to\ninfinity. In a setting where the hypotheses tested are independent, we prove\nthat these procedures are asymptotically more powerful in two respects: (i) a\ntighter asymptotic FDR control for any target FDR level and (ii) a broader\nrange of target levels yielding positive asymptotic power. We also show that\nthis increased asymptotic power comes at the price of slower, non-parametric\nconvergence rates for the FDP. These rates are of the form $m^{-k/(2k+1)}$,\nwhere $k$ is determined by the regularity of the density of the $p$-value\ndistribution, or, equivalently, of the test statistics distribution. These\nresults are applied to one- and two-sided tests statistics for Gaussian and\nLaplace location models, and for the Student model. \n\n"}
{"id": "1003.4156", "contents": "Title: A longest run test for heteroscedasticity in univariate regression model Abstract: The scope of this paper is the presentation of a test that enables to detect\nheteroscedasticity in univariate regression model. The test is simple to\ncompute and very general since no hypothesis is made on the regularity of the\nresponse function or on the normality of errors. Simulations show that our test\nfairs well with respect to other less general nonparametric tests. \n\n"}
{"id": "1004.2684", "contents": "Title: Disformal Scalar Fields and the Dark Sector of the Universe Abstract: Disformal transformations have proven to be very useful to devise models of\nthe dark sector. In the present paper we apply such transformation to a single\nscalar field theory as a way to drive the field into a slow roll phase. The\ncanonical scalar field Lagrangian, when coupled to a disformal metric, turns\nout to have relations to bimetric dark matter theories and to describe many\nspecific dark energy models at various limits, thus providing a surprisingly\nsimple parametrisation of a wide variety of models including tachyon, Chaplygin\ngas, K-essence and dilatonic ghost condensate. We investigate the evolution of\nthe background and linear perturbations in disformal quintessence in order to\nperform a full comparison of the predictions with the cosmological data. The\ndynamics of the expansion, in particular the mechanism of the transition to\naccelerating phase, is described in detail. We then study the effects of\ndisformal quintessence on cosmic microwave background (CMB) anisotropies and\nlarge scale structures (LSS). A likelihood analysis using the latest data on\nwide-ranging SNIa, CMB and LSS observations is performed allowing variations in\nsix cosmological parameters and the two parameters specifying the model. We\nfind that while a large region of parameter space remains compatible with\nobservations, models featuring either too much early dark energy or too slow\ntransition to acceleration are ruled out. \n\n"}
{"id": "1004.3782", "contents": "Title: On Practical Algorithms for Entropy Estimation and the Improved Sample\n  Complexity of Compressed Counting Abstract: Estimating the p-th frequency moment of data stream is a very heavily studied\nproblem. The problem is actually trivial when p = 1, assuming the strict\nTurnstile model. The sample complexity of our proposed algorithm is essentially\nO(1) near p=1. This is a very large improvement over the previously believed\nO(1/eps^2) bound. The proposed algorithm makes the long-standing problem of\nentropy estimation an easy task, as verified by the experiments included in the\nappendix. \n\n"}
{"id": "1004.4391", "contents": "Title: Locally most powerful sequential tests of a simple hypothesis vs.\n  One-sided alternatives for independent observations Abstract: Let $X_1,X_2,..., X_n,...$ be a stochastic process with independent values\nwhose distribution $P_\\theta$ depends on an unknown parameter $\\theta$,\n$\\theta\\in\\Theta$, where $\\Theta$ is an open subset of the real line. The\nproblem of testing $H_0:$ $\\theta=\\theta_0$ vs. a composite alternative $H_1:$\n$\\theta>\\theta_0$ is considered, where $\\theta_0\\in\\Theta$ is a fixed value of\nthe parameter. The main objective of this work is the characterization of the\nstructure of the locally most powerful (in the sense of Berk) sequential tests\nin this problem. \n\n"}
{"id": "1004.5154", "contents": "Title: Determination of the Source Flavor Ratio of Ultrahigh Energy Neutrinos Abstract: We discuss the reconstruction of neutrino flavor neutrino at a distant source\nin the very high en- ergy regime. This reconstruction procedure is relevant to\nthe confirmation of detecting cosmogenic neutrinos, for example. To facilitate\nsuch a reconstruction, it is imperative to achieve effective flavor\ndiscriminations in terrestrial neutrino telescopes. We note that, for energies\nbeyond few tens of PeV, a tau-lepton behaves like a track similar to a muon.\nHence, while it is rather challenging to separate {\\nu}{\\mu} from {\\nu}{\\tau}\nin this case, one can expect to isolate {\\nu}e from the rest by a distinctive\nshower signature. We present the result of flavor ratio reconstruction given\nthe anticipated accuracies of flavor measurement in neutrino telescopes and\ncurrent uncertainties of neutrino mixing parame- ters. It is shown that the\nfurther separation between {\\nu}{\\mu} and {\\nu}{\\tau} events does not improve\nthe flavor reconstruction due to the approximate {\\nu}{\\mu} - {\\nu}{\\tau}\nsymmetry. \n\n"}
{"id": "1006.1572", "contents": "Title: Asymptotic Properties of Self-Normalized Linear Processes with Long\n  Memory Abstract: In this paper we study the convergence to fractional Brownian motion for long\nmemory time series having independent innovations with infinite second moment.\nFor the sake of applications we derive the self-normalized version of this\ntheorem. The study is motivated by models arising in economical applications\nwhere often the linear processes have long memory, and the innovations have\nheavy tails. \n\n"}
{"id": "1006.1710", "contents": "Title: Muon lifetime dependent effects in MiniBooNE and LSND Abstract: We argue that because the source-detector distance of 1.8 microseconds (in\nnatural units) for the MiniBooNE is comparable to the muon lifetime of 2.2\nmicroseconds, and because time dilation effects are wiped out in the beam stop,\nthe Goldman entanglement of the neutrinos leads to hitherto unsuspected\ninterpretational consequences. We show that a distinct possibility exists in\nwhich a LSND-like experiment sees no CP violation, whereas a MiniBooNE-like\nsetup reproduces the LSND results for the $\\bar\\nu_\\mu$ to $\\bar\\nu_e$\noscillations while seeing only a significantly suppressed signal for the\n$\\nu_\\mu$ to $\\nu_e$ oscillations. We also discuss an alternate scenario. This\nalso suggests that the LSND experiment and the MiniBooNE should not be compared\nwithout taking into account the Goldman entanglement. \n\n"}
{"id": "1006.2170", "contents": "Title: Measure Problem for Eternal and Non-Eternal Inflation Abstract: We study various probability measures for eternal inflation by applying their\nregularization prescriptions to models where inflation is not eternal. For\nsimplicity we work with a toy model describing inflation that can interpolate\nbetween eternal and non-eternal inflation by continuous variation of a\nparameter. We investigate whether the predictions of four different measures\n(proper time, scale factor cutoff, stationary and causal {diamond}) change\ncontinuously with the change of this parameter. We will show that {only} for\nthe stationary measure the predictions change continuously. For the proper-time\nand the scale factor cutoff, the predictions are strongly discontinuous. For\nthe causal diamond measure, the predictions are continuous only if the stage of\nthe slow-roll inflation is sufficiently long. \n\n"}
{"id": "1006.3756", "contents": "Title: On the Observability of Collective Flavor Oscillations in Diffuse\n  Supernova Neutrino Background Abstract: Collective flavor oscillations are known to bring multiple splits in the\nsupernova (SN) neutrino and antineutrino spectra. These spectral splits depend\nnot only on the mass hierarchy of the neutrinos but also on the initial\nrelative flux composition. Observation of spectral splits in a future galactic\nsupernova signal is expected to throw light on the mass hierarchy pattern of\nthe neutrinos. However, since the Diffuse Supernova Neutrino Background (DSNB)\ncomprises of a superposition of neutrino fluxes from all past supernovae, and\nsince different supernovae are expected to have slightly different initial\nfluxes, it is pertinent to check if the hierarchy dependent signature of\ncollective oscillations can survive this averaging of the flux spectra. Since\nthe actual distribution of SN with initial relative flux spectra of the\nneutrinos and antineutrinos is unknown, we assume a log-normal distribution for\nthem. We study the dependence of the hierarchy sensitivity to the mean and\nvariance of the log-normal distribution function. We find that the hierarchy\nsensitivity depends crucially on the mean value of the relative initial\nluminosity. The effect of the width is to reduce the hierarchy sensitivity for\nall values of the mean initial relative luminosity. We find that in the very\nsmall mixing angle ($\\theta_{13}$) limit considering only statistical errors\neven for very moderate values of variance, there is almost no detectable\nhierarchy sensitivity if the mean relative luminosities of $\\nu_e$ and\n$\\bar\\nu_e$ are greater than 1. \n\n"}
{"id": "1007.2933", "contents": "Title: Bounds and Decays of New Heavy Vector-like Top Partners Abstract: We study the phenomenology of new heavy vector-like fermions that couple to\nthe third generation quarks via Yukawa interactions, covering all the allowed\nrepresentations under the standard model gauge groups. We first review tree and\nloop level bounds on these states. We then discuss tree level decays and\nloop-induced decays to photon or gluon plus top. The main decays at tree level\nare to W b and/or Z and Higgs plus top via the new Yukawa couplings. The\nradiative loop decays turn out to be quite close to the naive estimate: in all\ncases, in the allowed perturbative parameter space, the branching ratios are\nmildly sensitive on the new Yukawa couplings and small. We therefore conclude\nthat the new states can be observed at the LHC and that the tree level decays\ncan allow to distinguish the different representations. Moreover, the\nobservation of the radiative decays at the LHC would suggest a large Yukawa\ncoupling in the non-perturbative regime. \n\n"}
{"id": "1007.4013", "contents": "Title: Quasi-concave density estimation Abstract: Maximum likelihood estimation of a log-concave probability density is\nformulated as a convex optimization problem and shown to have an equivalent\ndual formulation as a constrained maximum Shannon entropy problem. Closely\nrelated maximum Renyi entropy estimators that impose weaker concavity\nrestrictions on the fitted density are also considered, notably a minimum\nHellinger discrepancy estimator that constrains the reciprocal of the\nsquare-root of the density to be concave. A limiting form of these estimators\nconstrains solutions to the class of quasi-concave densities. \n\n"}
{"id": "1007.4318", "contents": "Title: Impact of dynamical chiral symmetry breaking on meson structure and\n  interactions Abstract: We provide a glimpse of recent progress in meson physics made via QCD's\nDyson-Schwinger equations with: a perspective on confinement and dynamical\nchiral symmetry breaking (DCSB); a pre'cis on the physics of in-hadron\ncondensates; results for the masses of the \\pi, \\sigma, \\rho, a_1 mesons and\ntheir first-radial excitations; and an illustration of the impact of DCSB on\nthe pion form factor. \n\n"}
{"id": "1007.4710", "contents": "Title: Recent results and perspectives on cosmic rays ground experiments Abstract: I summarize in this paper the results and perspectives of representative\nground experiments for the observation of very high energy cosmic rays. \n\n"}
{"id": "1007.5253", "contents": "Title: 10 GeV dark matter candidates and cosmic-ray antiprotons Abstract: Recent measurements performed with some direct dark matter detection\nexperiments, e.g. CDMS-II and CoGENT (after DAMA/LIBRA), have unveiled a few\nevents compatible with weakly interacting massive particles. The preferred mass\nrange is around 10 GeV, with a quite large spin-independent cross section of\n$10^{-43}$-$10^{-41}\\,{\\rm cm^2}$. In this paper, we recall that a light dark\nmatter particle with dominant couplings to quarks should also generate\ncosmic-ray antiprotons. Taking advantage of recent works constraining the\nGalactic dark matter mass profile on the one hand and on cosmic-ray propagation\non the other hand, we point out that considering a thermal annihilation cross\nsection for such low mass candidates very likely results in an antiproton flux\nin tension with the current data, which should be taken into account in\nsubsequent studies. \n\n"}
{"id": "1008.0149", "contents": "Title: Bayesian Cointegrated Vector Autoregression models incorporating\n  Alpha-stable noise for inter-day price movements via Approximate Bayesian\n  Computation Abstract: We consider a statistical model for pairs of traded assets, based on a\nCointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR\nmodels to incorporate estimation of model parameters in the presence of price\nseries level shifts which are not accurately modeled in the standard Gaussian\nerror correction model (ECM) framework. This involves developing a novel matrix\nvariate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and\nAlpha-stable errors inter-day in the ECM framework. To achieve this we derive a\nnovel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR)\nrepresentation of Alpha-stable inter-day innovations. These results are\ngeneralized to asymmetric models for the innovation noise at inter-day\nboundaries allowing for skewed Alpha-stable models.\n  Our proposed model and sampling methodology is general, incorporating the\ncurrent literature on Gaussian models as a special subclass and also allowing\nfor price series level shifts either at random estimated time points or known a\npriori time points. We focus analysis on regularly observed non-Gaussian level\nshifts that can have significant effect on estimation performance in\nstatistical models failing to account for such level shifts, such as at the\nclose and open of markets. We compare the estimation accuracy of our model and\nestimation approach to standard frequentist and Bayesian procedures for CVAR\nmodels when non-Gaussian price series level shifts are present in the\nindividual series, such as inter-day boundaries. We fit a bi-variate\nAlpha-stable model to the inter-day jumps and model the effect of such jumps on\nestimation of matrix-variate CVAR model parameters using the likelihood based\nJohansen procedure and a Bayesian estimation. We illustrate our model and the\ncorresponding estimation procedures we develop on both synthetic and actual\ndata. \n\n"}
{"id": "1008.3536", "contents": "Title: Spectroscopy as a test of Coulomb's law - A probe of the hidden sector Abstract: High precision spectroscopy can provide a sensitive tool to test Coulomb's\nlaw on atomic length scales. This can then be used to constrain particles such\nas extra \"hidden\" photons or minicharged particles that are predicted in many\nextensions of the standard model, and which cause small deviations from\nCoulomb's law. In this paper we use a variety of transitions in atomic\nhydrogen, hydrogenic ions, and exotic atoms to probe Coulomb's law. This\nextends the region of pure Coulomb's law tests to larger masses. For hidden\nphotons and minicharged particles this region is already tested by other\nastrophysical and laboratory probes. However, future tests of true muonium and\nmuonic atoms are likely to probe new parameter space and therefore have good\ndiscovery potential for new physics. Finally, we investigate whether the\ndiscrepancy between the theoretical calculation of the 2s_{1/2}^{F=1} -\n2p_{3/2}^{F=2} transition in muonic hydrogen and its recent experimental\nmeasurement at PSI can be explained by the existence of a hidden photon. This\nexplanation is ruled out by measurements of the Lamb shift in ordinary\nhydrogen. \n\n"}
{"id": "1009.1926", "contents": "Title: Robust Bayesian variable selection with sub-harmonic priors Abstract: This paper studies Bayesian variable selection in linear models with general\nspherically symmetric error distributions. We propose sub-harmonic priors which\narise as a class of mixtures of Zellner's g-priors for which the Bayes factors\nare independent of the underlying error distribution, as long as it is in the\nspherically symmetric class. Because of this invariance to spherically\nsymmetric error distribution, we refer to our method as a robust Bayesian\nvariable selection method. We demonstrate that our Bayes factors have model\nselection consistency and are coherent. We also develop Laplace approximations\nto Bayes factors for a number of recently studied mixtures of g-priors that\nhave recently appeared in the literature (including our own) for Gaussian\nerrors. These approximations, in each case, are given by the Gaussian Bayes\nfactor based on BIC times a simple rational function of the prior's\nhyper-parameters and the R^2's for the respective models. We also extend model\nselection consistency for several g-prior based Bayes factor methods for\nGaussian errors to the entire class of spherically symmetric error\ndistributions. Additionally we demonstrate that our class of sub-harmonic\npriors are the only ones within a large class of mixtures of g-priors studied\nin the literature which are robust in our sense. A simulation study and an\nanalysis of two real data sets indicates good performance of our robust Bayes\nfactors relative to BIC and to other mixture of g-prior based methods. \n\n"}
{"id": "1009.2392", "contents": "Title: Solving the A{FB}^b anomaly in natural composite models Abstract: The Standard Model with a light Higgs provides a very accurate description of\nthe electroweak precision observables. The largest deviation between the\nStandard Model predictions and the experimental measurements, the\nforward-backward asymmetry of the bottom quark A{FB}^b, can be interpreted as\nan indication of new physics at the TeV scale. The strong agreement between\ntheory and experiment in the branching fraction of the Z into b-quarks puts\nstrong constraints for new physics aiming to solve the A{FB}^b puzzle. We study\na class of natural composite Higgs models that can solve the A{FB}^b anomaly\nreproducing the observed Rb as well as the top and bottom masses. We find that\nthe subgroup of the custodial symmetry able to protect Zb_L anti-b_L from large\ncorrections generated by the top sector play an important role if we want to\nmaintain naturalness in the composite sector. We make a thorough study of the\ncomposite operators mixing with the b-quark, determine their embedding under\nthe global composite symmetry and the parameter space that lead to the correct\nZb anti-b couplings while keeping the top and bottom masses to their physical\nvalues. We study the predictions for the spectrum of light fermionic resonances\nand the corrections to Zt anti-t couplings in this scenario. \n\n"}
{"id": "1009.3203", "contents": "Title: Intrinsic Inference on the Mean Geodesic of Planar Shapes and Tree\n  Discrimination by Leaf Growth Abstract: For planar landmark based shapes, taking into account the non-Euclidean\ngeometry of the shape space, a statistical test for a common mean first\ngeodesic principal component (GPC) is devised. It rests on one of two\nasymptotic scenarios, both of which are identical in a Euclidean geometry. For\nboth scenarios, strong consistency and central limit theorems are established,\nalong with an algorithm for the computation of a Ziezold mean geodesic. In\napplication, this allows to verify the geodesic hypothesis for leaf growth of\nCanadian black poplars and to discriminate genetically different trees by\nobservations of leaf shape growth over brief time intervals. With a test based\non Procrustes tangent space coordinates, not involving the shape space's\ncurvature, neither can be achieved. \n\n"}
{"id": "1009.5689", "contents": "Title: Square-Root Lasso: Pivotal Recovery of Sparse Signals via Conic\n  Programming Abstract: We propose a pivotal method for estimating high-dimensional sparse linear\nregression models, where the overall number of regressors $p$ is large,\npossibly much larger than $n$, but only $s$ regressors are significant. The\nmethod is a modification of the lasso, called the square-root lasso. The method\nis pivotal in that it neither relies on the knowledge of the standard deviation\n$\\sigma$ or nor does it need to pre-estimate $\\sigma$. Moreover, the method\ndoes not rely on normality or sub-Gaussianity of noise. It achieves near-oracle\nperformance, attaining the convergence rate $\\sigma \\{(s/n)\\log p\\}^{1/2}$ in\nthe prediction norm, and thus matching the performance of the lasso with known\n$\\sigma$. These performance results are valid for both Gaussian and\nnon-Gaussian errors, under some mild moment restrictions. We formulate the\nsquare-root lasso as a solution to a convex conic programming problem, which\nallows us to implement the estimator using efficient algorithmic methods, such\nas interior-point and first-order methods. \n\n"}
{"id": "1010.0988", "contents": "Title: Laboratory constraints on chameleon dark energy and power-law fields Abstract: We report results from the GammeV Chameleon Afterglow Search---a search for\nchameleon particles created via photon/chameleon oscillations within a magnetic\nfield. This experiment is sensitive to a wide class of chameleon power-law\nmodels and dark energy models not previously explored. These results exclude\nfive orders of magnitude in the coupling of chameleons to photons covering a\nrange of four orders of magnitude in chameleon effective mass and, for\nindividual chameleon models, exclude between 4 and 12 orders of magnitude in\nchameleon couplings to matter. \n\n"}
{"id": "1010.1935", "contents": "Title: Testing for Parallelism Between Trends in Multiple Time Series Abstract: This paper considers the inference of trends in multiple, nonstationary time\nseries. To test whether trends are parallel to each other, we use a parallelism\nindex based on the L2-distances between nonparametric trend estimators and\ntheir average. A central limit theorem is obtained for the test statistic and\nthe test's consistency is established. We propose a simulation-based\napproximation to the distribution of the test statistic, which significantly\nimproves upon the normal approximation. The test is also applied to devise a\nclustering algorithm. Finally, the finite-sample properties of the test are\nassessed through simulations and the test methodology is illustrated with time\nseries from Motorola cell phone activity in the United States. \n\n"}
{"id": "1010.2147", "contents": "Title: Wave functions for dynamically generated resonances; the two\n  $\\Lambda(1405)$ and $\\Lambda(1670)$ Abstract: In this work we develop a formalism to evaluate wave functions in momentum\nand coordinate space for the resonant states dynamically generated in a unitary\ncoupled channel approach. The on shell approach for the scattering matrix,\ncommonly used, is also obtained in Quantum Mechanics with a separable\npotential, which allows one to write wave functions in a trivial way. We\ndevelop useful relationships among the couplings of the dynamically generated\nresonances to the different channels and the wave functions at the origin. The\nformalism provides an intuitive picture of the resonances in the coupled\nchannel approach, as bound states of one bound channel, which decays into open\nones. It also provides an insight and practical rules for evaluating couplings\nof the resonances to external sources and how to deal with final state\ninteraction in production processes. As an application of the formalism we\nevaluate the wave functions of the two $\\Lambda(1405)$ states in the $\\pi\n\\Sigma$, $\\bar{K} N$ and other coupled channels. It also offers a practical way\nto study three body systems when two of them cluster into a resonance. \n\n"}
{"id": "1010.3390", "contents": "Title: Local shrinkage rules, Levy processes, and regularized regression Abstract: We use Levy processes to generate joint prior distributions, and therefore\npenalty functions, for a location parameter as p grows large. This generalizes\nthe class of local-global shrinkage rules based on scale mixtures of normals,\nilluminates new connections among disparate methods, and leads to new results\nfor computing posterior means and modes under a wide class of priors. We extend\nthis framework to large-scale regularized regression problems where p>n, and\nprovide comparisons with other methodologies. \n\n"}
{"id": "1010.5223", "contents": "Title: Good, great, or lucky? Screening for firms with sustained superior\n  performance using heavy-tailed priors Abstract: This paper examines historical patterns of ROA (return on assets) for a\ncohort of 53,038 publicly traded firms across 93 countries, measured over the\npast 45 years. Our goal is to screen for firms whose ROA trajectories suggest\nthat they have systematically outperformed their peer groups over time. Such a\nproject faces at least three statistical difficulties: adjustment for relevant\ncovariates, massive multiplicity, and longitudinal dependence. We conclude\nthat, once these difficulties are taken into account, demonstrably superior\nperformance appears to be quite rare. We compare our findings with other recent\nmanagement studies on the same subject, and with the popular literature on\ncorporate success. Our methodological contribution is to propose a new class of\npriors for use in large-scale simultaneous testing. These priors are based on\nthe hypergeometric inverted-beta family, and have two main attractive features:\nheavy tails and computational tractability. The family is a four-parameter\ngeneralization of the normal/inverted-beta prior, and is the natural conjugate\nprior for shrinkage coefficients in a hierarchical normal model. Our results\nemphasize the usefulness of these heavy-tailed priors in large multiple-testing\nproblems, as they have a mild rate of tail decay in the marginal likelihood\n$m(y)$---a property long recognized to be important in testing. \n\n"}
{"id": "1011.1253", "contents": "Title: Coupling optional P\\'olya trees and the two sample problem Abstract: Testing and characterizing the difference between two data samples is of\nfundamental interest in statistics. Existing methods such as Kolmogorov-Smirnov\nand Cramer-von-Mises tests do not scale well as the dimensionality increases\nand provides no easy way to characterize the difference should it exist. In\nthis work, we propose a theoretical framework for inference that addresses\nthese challenges in the form of a prior for Bayesian nonparametric analysis.\nThe new prior is constructed based on a random-partition-and-assignment\nprocedure similar to the one that defines the standard optional P\\'olya tree\ndistribution, but has the ability to generate multiple random distributions\njointly. These random probability distributions are allowed to \"couple\", that\nis to have the same conditional distribution, on subsets of the sample space.\nWe show that this \"coupling optional P\\'olya tree\" prior provides a convenient\nand effective way for both the testing of two sample difference and the\nlearning of the underlying structure of the difference. In addition, we discuss\nsome practical issues in the computational implementation of this prior and\nprovide several numerical examples to demonstrate its work. \n\n"}
{"id": "1011.2360", "contents": "Title: Exes and why Z? Some charming and beautiful observations Abstract: Threshold enhancements like the X(4660) and depletion effects as the X(4260)\nare listed as c-cbar resonances in the Particle Data Group tables. We will\ndiscuss these observations, and present a list of further c-cbar enhancements,\nwhich are more likely to represent true vector charmonium excitations. We will\nfurthermore discuss the importance of the observed Z resonances, viz. Z(4050),\nZ(4250), and Z(4430), for the family of charm-strange mesons. Another piece of\nvery important information that can be extracted from the present data is the\nuniversal, flavor independent frequency of 190 MeV for mesons, due to the\nquark-antiquark oscillations within the glue environment. Finally, we will show\nhints from the data at a further flavor-independent quantity, having a value of\n76 +-2 MeV, the origin of which is not yet understood.\n  Talk available at http://cft.fis.uc.pt/eef/Frascati2010start.htm \n\n"}
{"id": "1011.2403", "contents": "Title: Lattice QCD at the physical point: light quark masses Abstract: Ordinary matter is described by six fundamental parameters: three couplings\n(gravitational, electromagnetic and strong) and three masses: the electron's\n(m_e) and those of the up (m_u) and down (m_d) quarks. An additional mass\nenters through quantum fluctuations: the strange quark mass (m_s). The three\ncouplings and m_e are known with an accuracy of better than a few per mil.\nDespite their importance, $m_u$, $m_d$ (their average m_{ud}) and m_s are\nrelatively poorly known: e.g. the Particle Data Group quotes them with\nconservative errors close to 25%. Here we determine these quantities with a\nprecision below 2% by performing ab initio lattice quantum chromodynamics (QCD)\ncalculations, in which all systematics are controlled. We use pion and quark\nmasses down to (and even below) their physical values, lattice sizes of up to 6\nfm, and five lattice spacings to extrapolate to continuum spacetime. All\nnecessary renormalizations are performed nonperturbatively. \n\n"}
{"id": "1011.4916", "contents": "Title: Fast Bivariate Penalized Splines: the Sandwich Smoother Abstract: We propose a fast penalized spline method for bivariate smoothing. Univariate\nP-spline smoothers (Eilers and Marx, 1996) are applied simultaneously along\nboth coordinates. The new smoother has a sandwich form which suggested the name\n\"sandwich smoother\" to a referee. The sandwich smoother has a tensor product\nstructure that simplifies an asymptotic analysis and it can be fast computed.\nWe derive a local central limit theorem for the sandwich smoother, with simple\nexpressions for the asymptotic bias and variance, by showing that the sandwich\nsmoother is asymptotically equivalent to a bivariate kernel regression\nestimator with a product kernel. As far as we are aware, this is the first\ncentral limit theorem for a bivariate spline estimator of any type. Our\nsimulation study shows that the sandwich smoother is orders of magnitude faster\nto compute than other bivariate spline smoothers, even when the latter are\ncomputed using a fast GLAM (Generalized Linear Array Model) algorithm, and\ncomparable to them in terms of mean squared integrated errors. We extend the\nsandwich smoother to array data of higher dimensions, where a GLAM algorithm\nimproves the computational speed of the sandwich smoother. One important\napplication of the sandwich smoother is to estimate covariance functions in\nfunctional data analysis. In this application, our numerical results show that\nthe sandwich smoother is orders of magnitude faster than local linear\nregression. The speed of the sandwich formula is important because functional\ndata sets are becoming quite large. \n\n"}
{"id": "1012.3395", "contents": "Title: Final state interactions and the Sivers function Abstract: The non-vanishing of naive T-odd parton distributions function can be\nexplained by the existence of the gauge link which emerges from the factorized\ndescription of the deep inelastic scattering cross section into perturbatively\ncalculable and non-perturbative factors. This path ordered exponential\ndescribes initial / final-state interactions of the active parton due to soft\ngluon exchanges with the target remnants. Although these interactions are\nnon-perturbative, studies of final state interactions have been approximated by\nperturbative one-gluon exchange in Abelian models. We include higher-order\ngluonic contributions from the gauge link by applying non-perturbative eikonal\nmethods, incorporating color degrees of freedom in a calculation of the Sivers\nfunction. In this context we study the effects of color by considering the FSIs\nwith Abelian and non-Abelian gluon interactions. We confirm the large $N_c$ QCD\nscaling behavior of Sivers functions and further uncover the deviations for\nfinite $N_c$. Within this framework of FSIs we perform a quantitative check of\napproximate relations between T-odd TMDs and GPD which goes beyond the\ndiscussion of overall signs. \n\n"}
{"id": "1012.3784", "contents": "Title: Thermal production of relativistic Majorana neutrinos: Strong\n  enhancement by multiple soft scattering Abstract: The production rate of heavy Majorana neutrinos is relevant for models of\nthermal leptogenesis in the early Universe. In the high temperature limit the\nproduction can proceed via the 1 <-> 2 (inverse) decays which are allowed by\nthe thermal masses. We consider new production mechanisms which are obtained by\nincluding additional soft gauge interactions with the plasma. We show that an\narbitrary number of such interactions gives leading order contributions, and we\nsum all of them. The rate turns out to be smooth in the region where the 1 <->\n2 processes are kinematically forbidden. At higher temperature it is enhanced\nby a factor 3 compared to the 1 <-> 2 rate. \n\n"}
{"id": "1012.4587", "contents": "Title: On the stability of particle dark matter Abstract: From the particle physics point of view, the most peculiar property of the\ndark matter particle is its stability on cosmological time scales. We briefly\nreview the possible origins of this characteristic feature for candidates whose\nrelic density results from the thermal freeze-out of their annihilation. We\nemphasize that each stabilization mechanism implies an all specific\nphenomenology. The models reviewed include supersymmetric and\nnon-supersymmetric models where the stability is a consequence of\ngrand-unification, models where stability is due to an unbroken gauge group and\nmodels where the DM stability is accidental. The latter possibility includes\nminimal dark matter, hidden vector dark matter and composite DM models. \n\n"}
{"id": "1012.5971", "contents": "Title: The bulk transition of many-flavour QCD and the search for a UVFP at\n  strong coupling Abstract: We explore the nature of the bulk transition observed at strong coupling in\nthe SU(3) gauge theory with Nf=12 fermions in the fundamental representation.\nThe transition separates a weak coupling chirally symmetric phase from a strong\ncoupling chirally broken phase and is compatible with the scenario where\nconformality is restored by increasing the flavour content of a non abelian\ngauge theory. We explore the intriguing possibility that the observed bulk\ntransition is associated with the occurrence of an ultraviolet fixed point\n(UVFP) at strong coupling, where a new theory emerges in the continuum. \n\n"}
{"id": "1101.1359", "contents": "Title: Exponential-Family Random Graph Models for Valued Networks Abstract: Exponential-family random graph models (ERGMs) provide a principled and\nflexible way to model and simulate features common in social networks, such as\npropensities for homophily, mutuality, and friend-of-a-friend triad closure,\nthrough choice of model terms (sufficient statistics). However, those ERGMs\nmodeling the more complex features have, to date, been limited to binary data:\npresence or absence of ties. Thus, analysis of valued networks, such as those\nwhere counts, measurements, or ranks are observed, has necessitated\ndichotomizing them, losing information and introducing biases.\n  In this work, we generalize ERGMs to valued networks. Focusing on modeling\ncounts, we formulate an ERGM for networks whose ties are counts and discuss\nissues that arise when moving beyond the binary case. We introduce model terms\nthat generalize and model common social network features for such data and\napply these methods to a network dataset whose values are counts of\ninteractions. \n\n"}
{"id": "1101.4368", "contents": "Title: Inferences in Bayesian variable selection problems with large model\n  spaces Abstract: An important aspect of Bayesian model selection is how to deal with huge\nmodel spaces, since exhaustive enumeration of all the models entertained is\nunfeasible and inferences have to be based on the very small proportion of\nmodels visited. This is the case for the variable selection problem, with a\nmoderate to large number of possible explanatory variables being considered in\nthis paper. We review some of the strategies proposed in the literature and\nargue that inferences based on empirical frequencies via Markov Chain Monte\nCarlo sampling of the posterior distribution outperforms recently proposed\nsearching methods. We give a plausible yet very simple explanation of this\neffect, showing that estimators based on frequencies are unbiased. The results\nobtained in two illustrative examples provide strong evidence in favor of our\narguments. \n\n"}
{"id": "1101.5295", "contents": "Title: Cold Nuclear Matter Effects on jpsi Production with Extrinsic pT at\n  $\\sqrt{s_{NN}}=5.5\\mathrm{~TeV}$ at the LHC Abstract: Taking into account the gluon shadowing and the $J/\\psi$ nuclear absorption,\nwe evaluate the Cold Nuclear Matter (CNM) effects on $J/\\psi$ production in\n$p$Pb and PbPb collisions at the design LHC energy for PbPb collisions,\n$\\sqrt{s_{NN}}=5.5$ TeV. In view of the good agreement between the yield\nprediction using the LO CSM and the ALICE data at 7 TeV, we employ the latter\nmodel to use a complete description of the kinematics of the underlying $2\n\\rightarrow 2$ partonic process, namely $g + g \\rightarrow J/\\psi + g$. We\nobserve a large $J/\\psi$ suppression in $p$Pb and PbPb collisions due to the\nstrong shadowing of the gluon distribution in the lead ion at the corresponding\ngluon momentum fractions. This suppression from CNM effects has a strong\nrapidity dependence which may compensate, partially or completely, that of of\nthe predicted charm recombination in PbPb collisions. \n\n"}
{"id": "1102.5650", "contents": "Title: Climbing NLO and NNLO Summits of Weak Decays: 1988-2023 Abstract: I describe the history of the calculations of NLO and NNLO QCD corrections to\nweak decays of mesons and particle-antiparticle mixing in the period 1988-2023.\nAlso existing calculations of electroweak and QED corrections to these\nprocesses are included in this presentation. These efforts bear some analogies\nto the climbing of Himalayas and various expeditions by several teams of\nstrongly motivated \"climbers\" allowed to move this field from the LO through\nthe NLO to the NNLO level. We also summarize the most recent calculations\nwithin the Standard Model Effective Field Theory. The material is meant to be\nan up to date review of this very advanced field in non-technical terms as much\nas possible and a guide to the rich literature on NLO and NNLO corrections in\nquestion. In particular we stress for which processes these calculations are\ncrucial for the tests of the Standard Model and to be able to differentiate\nbetween numerous New Physics models. It includes several anecdotes related to\nthe climbs that I was involved in. I hope that some of the comments made in the\ncourse of the presentation could turn out to be not only amusing but also\ninstructive. \n\n"}
{"id": "1103.0184", "contents": "Title: Conformal anomaly and the vector coupling in dense matter Abstract: We construct an effective chiral Lagrangian for hadrons implemented by the\nconformal invariance and discuss the properties of nuclear matter at high\ndensity. The model is formulated based on two alternative assignment, \"naive\"\nand mirror, of chirality to the nucleons. It is shown that taking the dilaton\nlimit, in which the mended symmetry of Weinberg is manifest, the vector-meson\nYukawa coupling becomes suppressed and the symmetry energy becomes softer as\none approaches the chiral phase transition. This leads to softer equations of\nstate (EoS) and could accommodate the EoS without any exotica consistent with\nthe recent measurement of a $1.97 \\pm 0.04\\,M_\\odot$ neutron star. \n\n"}
{"id": "1103.1984", "contents": "Title: Search for stable hadronising squarks and gluinos with the ATLAS\n  experiment at the LHC Abstract: Hitherto unobserved long-lived massive particles with electric and/or colour\ncharge are predicted by a range of theories which extend the Standard Model. In\nthis paper a search is performed at the ATLAS experiment for slow-moving\ncharged particles produced in proton-proton collisions at 7 TeV centre-of-mass\nenergy at the LHC, using a data-set corresponding to an integrated luminosity\nof 34 pb-1. No deviations from Standard Model expectations are found. This\nresult is interpreted in a framework of supersymmetry models in which coloured\nsparticles can hadronise into long-lived bound hadronic states, termed\nR-hadrons, and 95% CL limits are set on the production cross-sections of\nsquarks and gluinos. The influence of R-hadron interactions in matter was\nstudied using a number of different models, and lower mass limits for stable\nsbottoms and stops are found to be 294 and 309 GeV respectively. The lower mass\nlimit for a stable gluino lies in the range from 562 to 586 GeV depending on\nthe model assumed. Each of these constraints is the most stringent to date. \n\n"}
{"id": "1103.6178", "contents": "Title: Knitting neutrino mass textures with or without Tri-Bi maximal mixing Abstract: The solar and baseline neutrino oscillation data suggest bimaximal neutrino\nmixing among the first two generations, and trimaximal mixing between all three\nneutrino flavors. It has been conjectured that this indicates the existence of\nan underlying symmetry for the leptonic fermion mass textures. The\nexperimentally measured quantities however, are associated to the latter\nindirectly and in a rather complicated way through the mixing matrices of the\ncharged leptons and neutrinos. Motivated by these facts, we derive exact\nanalytical expressions which directly link the charged lepton and neutrino mass\nand mixing parameters to measured quantities and obtain constraints on the\nparameter space. We discuss deviations from Tri-Bi mixing matrices and present\nminimal extensions of the Harrison Perkins and Scott matrices capable of\ninterpreting all neutrino data. \n\n"}
{"id": "1104.0341", "contents": "Title: Small-scale inference: Empirical Bayes and confidence methods for as few\n  as a single comparison Abstract: By restricting the possible values of the proportion of null hypotheses that\nare true, the local false discovery rate (LFDR) can be estimated using as few\nas one comparison. The proportion of proteins with equivalent abundance was\nestimated to be about 20% for patient group I and about 90% for group II. The\nsimultaneously-estimated LFDRs give approximately the same inferences as\nindividual-protein confidence levels for group I but are much closer to\nindividual-protein LFDR estimates for group II. Simulations confirm that\nconfidence-based inference or LFDR-based inference performs markedly better for\nlow or high proportions of true null hypotheses, respectively. \n\n"}
{"id": "1104.0694", "contents": "Title: Heavy meson three body decay: Three decades of Dalitz plot amplitude\n  analysis Abstract: Three body heavy meson decays allows one to extract important quantities for\nflavour physic, however the phenomenological analysis has some open problems to\nbe solved in order to make possible to extract physical parameters with\nprecision. After many years and some experience accumulated, we have learned\nnew features and we expect to learn much more from data coming mainly from B\nthree body decays. In this paper we address these aspects and we present a\nproposal to modify the Dalitz fit amplitude method in order to allow further\nstudies. \n\n"}
{"id": "1104.4135", "contents": "Title: Posterior consistency in linear models under shrinkage priors Abstract: We investigate the asymptotic behavior of posterior distributions of\nregression coefficients in high-dimensional linear models as the number of\ndimensions grows with the number of observations. We show that the posterior\ndistribution concentrates in neighborhoods of the true parameter under simple\nsufficient conditions. These conditions hold under popular shrinkage priors\ngiven some sparsity assumptions. \n\n"}
{"id": "1105.0902", "contents": "Title: Modeling Network Evolution Using Graph Motifs Abstract: Network structures are extremely important to the study of political science.\nMuch of the data in its subfields are naturally represented as networks. This\nincludes trade, diplomatic and conflict relationships. The social structure of\nseveral organization is also of interest to many researchers, such as the\naffiliations of legislators or the relationships among terrorist. A key aspect\nof studying social networks is understanding the evolutionary dynamics and the\nmechanism by which these structures grow and change over time. While current\nmethods are well suited to describe static features of networks, they are less\ncapable of specifying models of change and simulating network evolution. In the\nfollowing paper I present a new method for modeling network growth and\nevolution. This method relies on graph motifs to generate simulated network\ndata with particular structural characteristic. This technique departs notably\nfrom current methods both in form and function. Rather than a closed-form\nmodel, or stochastic implementation from a single class of graphs, the proposed\n\"graph motif model\" provides a framework for building flexible and complex\nmodels of network evolution. The paper proceeds as follows: first a brief\nreview of the current literature on network modeling is provided to place the\ngraph motif model in context. Next, the graph motif model is introduced, and a\nsimple example is provided. As a proof of concept, three classic random graph\nmodels are recovered using the graph motif modeling method: the Erdos-Renyi\nbinomial random graph, the Watts-Strogatz \"small world\" model, and the\nBarabasi-Albert preferential attachment model. In the final section I discuss\nthe results of these simulations and subsequent advantage and disadvantages\npresented by using this technique to model social networks. \n\n"}
{"id": "1105.1061", "contents": "Title: Heavy Flavour in a Nutshell Abstract: Moriond QCD brings together particle physicists of varied interests. This\nreview and introduction to heavy flavour physics is aimed at those not in the\nheavy-flavour field to describe the motivation and methodology of precision\nflavour physics, and introduce the most tantalising searches for new physics.\nThe LHC experiments are expected to make great inroads into constraining the\nnew physics parameter space and discover the new physics which I will argue\n\\emph{must} be present to describe our observed universe. \n\n"}
{"id": "1105.3638", "contents": "Title: Corrected portmanteau tests for VAR models with time-varying variance Abstract: The problem of test of fit for Vector AutoRegressive (VAR) processes with\nunconditionally heteroscedastic errors is studied. The volatility structure is\ndeterministic but time-varying and allows for changes that are commonly\nobserved in economic or financial multivariate series. Our analysis is based on\nthe residual autocovariances and autocorrelations obtained from Ordinary Least\nSquares (OLS), Generalized Least Squares (GLS)and Adaptive Least Squares (ALS)\nestimation of the autoregressive parameters. The ALS approach is the GLS\napproach adapted to the unknown time-varying volatility that is then estimated\nby kernel smoothing. The properties of the three types of residual\nautocovariances and autocorrelations are derived. In particular it is shown\nthat the ALS and GLS residual autocorrelations are asymptotically equivalent.\nIt is also found that the asymptotic distribution of the OLS residual\nautocorrelations can be quite different from the standard chi-square asymptotic\ndistribution obtained in a correctly specified VAR model with iid innovations.\nAs a consequence the standard portmanteau tests are unreliable in our\nframework. The correct critical values of the standard portmanteau tests based\non the OLS residuals are derived. Moreover, modified portmanteau statistics\nbased on ALS residual autocorrelations are introduced. Portmanteau tests with\nmodified statistics based on OLS and ALS residuals and standard chi-square\nasymptotic distributions under the null hypothesis are also proposed. An\nextension of our portmanteau approaches to testing the lag length in a vector\nerror correction type model for co-integrating relations is briefly\ninvestigated. The finite sample properties of the goodness-of-fit tests we\nconsider are investigated by Monte Carlo experiments. The theoretical results\nare also illustrated using two U.S. economic data sets. \n\n"}
{"id": "1106.1916", "contents": "Title: Threshold estimation based on a p-value framework in dose-response and\n  regression settings Abstract: We use p-values to identify the threshold level at which a regression\nfunction takes off from its baseline value, a problem motivated by applications\nin toxicological and pharmacological dose-response studies and environmental\nstatistics. We study the problem in two sampling settings: one where multiple\nresponses can be obtained at a number of different covariate-levels and the\nother the standard regression setting involving limited number of response\nvalues at each covariate. Our procedure involves testing the hypothesis that\nthe regression function is at its baseline at each covariate value and then\ncomputing the potentially approximate p-value of the test. An estimate of the\nthreshold is obtained by fitting a piecewise constant function with a single\njump discontinuity, otherwise known as a stump, to these observed p-values, as\nthey behave in markedly different ways on the two sides of the threshold. The\nestimate is shown to be consistent and its finite sample properties are studied\nthrough simulations. Our approach is computationally simple and extends to the\nestimation of the baseline value of the regression function, heteroscedastic\nerrors and to time-series. It is illustrated on some real data applications. \n\n"}
{"id": "1106.2246", "contents": "Title: General bootstrap for dual phi-divergence estimates Abstract: A general notion of bootstrapped $\\phi$-divergence estimates constructed by\nexchangeably weighting sample is introduced. Asymptotic properties of these\ngeneralized bootstrapped $\\phi$-divergence estimates are obtained, by mean of\nthe empirical process theory, which are applied to construct the bootstrap\nconfidence set with asymptotically correct coverage probability. Some of\npractical problems are discussed, including in particular, the choice of escort\nparameter and several examples of divergences are investigated. Simulation\nresults are provided to illustrate the finite sample performance of the\nproposed estimators. \n\n"}
{"id": "1106.2343", "contents": "Title: Supersymmetric Nonlinear Sigma Model in AdS_5 Abstract: We construct the supersymmetric nonlinear sigma model in a fixed AdS_5\nbackground. We use component fields and find that the complex bosons must be\nthe coordinates of a hyper-Kahler manifold that admits a Killing vector\nsatisfying an inhomogeneous tri-holomorphic condition. We propose boundary\nconditions that map the on-shell bulk hypermultiplets into off-shell chiral\nmultiplets on 3-branes that foliate the bulk. The supersymmetric AdS_5\nisometries reduce to superconformal transformations on the brane fields. \n\n"}
{"id": "1106.2525", "contents": "Title: Uniform Stability of a Particle Approximation of the Optimal Filter\n  Derivative Abstract: Sequential Monte Carlo methods, also known as particle methods, are a widely\nused set of computational tools for inference in non-linear non-Gaussian\nstate-space models. In many applications it may be necessary to compute the\nsensitivity, or derivative, of the optimal filter with respect to the static\nparameters of the state-space model; for instance, in order to obtain maximum\nlikelihood model parameters of interest, or to compute the optimal controller\nin an optimal control problem. In Poyiadjis et al. [2011] an original particle\nalgorithm to compute the filter derivative was proposed and it was shown using\nnumerical examples that the particle estimate was numerically stable in the\nsense that it did not deteriorate over time. In this paper we substantiate this\nclaim with a detailed theoretical study. Lp bounds and a central limit theorem\nfor this particle approximation of the filter derivative are presented. It is\nfurther shown that under mixing conditions these Lp bounds and the asymptotic\nvariance characterized by the central limit theorem are uniformly bounded with\nrespect to the time index. We demon- strate the performance predicted by theory\nwith several numerical examples. We also use the particle approximation of the\nfilter derivative to perform online maximum likelihood parameter estimation for\na stochastic volatility model. \n\n"}
{"id": "1106.3244", "contents": "Title: The T2K Indication of Relatively Large theta_13 and a Natural\n  Perturbation to the Democratic Neutrino Mixing Pattern Abstract: The T2K Collaboration has recently reported a remarkable indication of the\n\\nu_\\mu -> \\nu_e oscillation which is consistent with a relatively large value\nof \\theta_{13} in the three-flavor neutrino mixing scheme. We show that it is\npossible to account for such a result of \\theta_{13} by introducing a natural\nperturbation to the democratic neutrino mixing pattern, without or with CP\nviolation. A testable correlation between \\theta_{13} and \\theta_{23} is\npredicted in this ansatz. We also discuss the Wolfenstein-like parametrization\nof neutrino mixing, and comment on other possibilities of generating\nsufficiently large \\theta_{13} at the electroweak scale. \n\n"}
{"id": "1106.5016", "contents": "Title: Calculating the renormalisation group equations of a SUSY model with\n  Susyno Abstract: Susyno is a Mathematica package dedicated to the computation of the 2-loop\nrenormalisation group equations of a supersymmetric model based on any gauge\ngroup (the only exception being multiple U(1) groups) and for any field\ncontent. \n\n"}
{"id": "1106.5714", "contents": "Title: Non-parametric change-point detection using string matching algorithms Abstract: Given the output of a data source taking values in a finite alphabet, we wish\nto detect change-points, that is times when the statistical properties of the\nsource change. Motivated by ideas of match lengths in information theory, we\nintroduce a novel non-parametric estimator which we call CRECHE (CRossings\nEnumeration CHange Estimator). We present simulation evidence that this\nestimator performs well, both for simulated sources and for real data formed by\nconcatenating text sources. For example, we show that we can accurately detect\nthe point at which a source changes from a Markov chain to an IID source with\nthe same stationary distribution. Our estimator requires no assumptions about\nthe form of the source distribution, and avoids the need to estimate its\nprobabilities. Further, we establish consistency of the CRECHE estimator under\na related toy model, by establishing a fluid limit and using martingale\narguments. \n\n"}
{"id": "1106.5837", "contents": "Title: Grouped Variable Selection via Nested Spike and Slab Priors Abstract: In this paper we study grouped variable selection problems by proposing a\nspecified prior, called the nested spike and slab prior, to model collective\nbehavior of regression coefficients. At the group level, the nested spike and\nslab prior puts positive mass on the event that the l2-norm of the grouped\ncoefficients is equal to zero. At the individual level, each coefficient is\nassumed to follow a spike and slab prior. We carry out maximum a posteriori\nestimation for the model by applying blockwise coordinate descent algorithms to\nsolve an optimization problem involving an approximate objective modified by\nmajorization-minimization techniques. Simulation studies show that the proposed\nestimator performs relatively well in the situations in which the true and\nredundant covariates are both covered by the same group. Asymptotic analysis\nunder a frequentist's framework further shows that the l2 estimation error of\nthe proposed estimator can have a better upper bound if the group that covers\nthe true covariates does not cover too many redundant covariates. In addition,\ngiven some regular conditions hold, the proposed estimator is asymptotically\ninvariant to group structures, and its model selection consistency can be\nestablished without imposing irrepresentable-type conditions. \n\n"}
{"id": "1107.2317", "contents": "Title: Recent progress in QCD calculations for e^+e^- annihilation and hadron\n  collisions Abstract: We provide a brief summary of recent developments in QCD calculations in and\nbeyond fixed-order perturbation theory, for observables in $e^{+}e^{-}$\nannihilation as well as hadron collisions. \n\n"}
{"id": "1107.2764", "contents": "Title: Yukawa unification in SO(10) with light sparticle spectrum Abstract: We investigate supersymmetric SO(10) GUT model with \\mu<0. The requirements\nof top-bottom-tau Yukawa unification, correct radiative electroweak symmetry\nbreaking and agreement with the present experimental data may be met when the\nsoft masses of scalars and gauginos are non-universal. We show how appropriate\nnon-universalities can easily be obtained in the SO(10) GUT broken to the\nStandard Model. We discuss how values of BR(b-->s \\gamma) and (g-2)_\\mu\nsimultaneously in a good agreement with the experimental data can be achieved\nin SO(10) model with \\mu<0. In the region of the parameter space preferred by\nour analysis there are two main mechanisms leading to the LSP relic abundance\nconsistent with the WMAP results. One is the co-annihilation with the stau and\nthe second is the resonant annihilation via exchange of the Z boson or the\nlight Higgs scalar. A very interesting feature of SO(10) models with negative\n\\mu is that they predict relatively light sparticle spectra. Even the heaviest\nsuperpartners may easily have masses below 1.5 TeV in contrast to multi-TeV\nparticles typical for models with positive \\mu. \n\n"}
{"id": "1107.5257", "contents": "Title: Review of new physics effects in t-tbar production Abstract: Both CDF and DO report a forward-backward asymmetry in t-tbar production that\nis above the standard model prediction. We review new physics models that can\ngive a large forward backward asymmetry in t-tbar production at the Tevatron\nand the constraints these models face from searches for dijet resonances and\ncontact interactions, from flavor physics and the t-tbar cross section.\nExpected signals at the LHC are also reviewed. \n\n"}
{"id": "1107.6009", "contents": "Title: Lepton flavor violation and non-unitary lepton mixing in low-scale\n  type-I seesaw Abstract: Within low-scale seesaw mechanisms, such as the inverse and linear seesaw,\none expects (i) potentially large lepton flavor violation (LFV) and (ii)\nsizeable non-standard neutrino interactions (NSI). We consider the interplay\nbetween the magnitude of non-unitarity effects in the lepton mixing matrix, and\nthe constraints that follow from LFV searches in the laboratory. We find that\nNSI parameters can be sizeable, up to percent level in some cases, while LFV\nrates, such as that for \\mu -> e \\gamma, lie within current limits, including\nthe recent one set by the MEG collaboration. As a result the upcoming long\nbaseline neutrino experiments offer a window of opportunity for complementary\nLFV and weak universality tests. \n\n"}
{"id": "1108.0483", "contents": "Title: Thermodynamics of the Lee-Wick partners: An alternative approach Abstract: It was pointed out some time ago that there can be two variations in which\nthe divergences of a quantum field theory can be tamed using the ideas\npresented by Lee and Wick. In one variation the Lee-Wick partners of the normal\nfields live in an indefinite metric Hilbert space but have positive energy and\nin the other variation the Lee-Wick partners can live in a normal Hilbert space\nbut carry negative energy. Quantum mechanically the two variations mainly\ndiffer in the way the fields are quantized. In this article the second\nvariation of Lee and Wick's idea is discussed. Using statistical mechanical\nmethods the energy density, pressure and entropy density of the negative energy\nLee-Wick fields have been calculated. The results exactly match with the\nthermodynamic results of the conventional, positive energy Lee-Wick fields. The\nresult sheds some light on the second variation of Lee-Wick's idea. The result\nseems to say that the thermodynamics of the theories do not care about the way\nthey are quantized. \n\n"}
{"id": "1108.2986", "contents": "Title: Tests for multivariate normality based on canonical correlations Abstract: We propose new affine invariant tests for multivariate normality, based on\nindependence characterizations of the sample moments of the normal\ndistribution. The test statistics are obtained using canonical correlations\nbetween sets of sample moments, generalizing the Lin-Mudholkar test for\nnormality. The tests are compared to some popular tests based on Mardia's\nskewness and kurtosis measures in an extensive simulation power study and are\nfound to offer higher power against many of the alternatives. \n\n"}
{"id": "1108.3071", "contents": "Title: Higgs Mass and Muon Anomalous Magnetic Moment in Supersymmetric Models\n  with Vector-Like Matters Abstract: We study the muon anomalous magnetic moment (muon g-2) and the Higgs boson\nmass in a simple extension of the minimal supersymmetric (SUSY) Standard Model\nwith extra vector-like matters, in the frameworks of gauge mediated SUSY\nbreaking (GMSB) models and gravity mediation (mSUGRA) models. It is shown that\nthe deviation of the muon g-2 and a relatively heavy Higgs boson can be\nsimultaneously explained in large tan-beta region. (i) In GMSB models, the\nHiggs mass can be more than 135 GeV (130 GeV) in the region where muon g-2 is\nconsistent with the experimental value at the 2 sigma (1 sigma) level, while\nmaintaining the perturbative coupling unification. (ii) In the case of mSUGRA\nmodels with universal soft masses, the Higgs mass can be as large as about 130\nGeV when muon g-2 is consistent with the experimental value at the 2 sigma\nlevel. In both cases, the Higgs mass can be above 140 GeV if the g-2 constraint\nis not imposed. \n\n"}
{"id": "1109.0442", "contents": "Title: A Student-t based filter for robust signal detection Abstract: The search for gravitational-wave signals in detector data is often hampered\nby the fact that many data analysis methods are based on the theory of\nstationary Gaussian noise, while actual measurement data frequently exhibit\nclear departures from these assumptions. Deriving methods from models more\nclosely reflecting the data's properties promises to yield more sensitive\nprocedures. The commonly used matched filter is such a detection method that\nmay be derived via a Gaussian model. In this paper we propose a generalized\nmatched-filtering technique based on a Student-t distribution that is able to\naccount for heavier-tailed noise and is robust against outliers in the data. On\nthe technical side, it generalizes the matched filter's least-squares method to\nan iterative, or adaptive, variation. In a simplified Monte Carlo study we show\nthat when applied to simulated signals buried in actual interferometer noise it\nleads to a higher detection rate than the usual (\"Gaussian\") matched filter. \n\n"}
{"id": "1109.6090", "contents": "Title: Robust Parametric Classification and Variable Selection by a Minimum\n  Distance Criterion Abstract: We investigate a robust penalized logistic regression algorithm based on a\nminimum distance criterion. Influential outliers are often associated with the\nexplosion of parameter vector estimates, but in the context of standard\nlogistic regression, the bias due to outliers always causes the parameter\nvector to implode, that is shrink towards the zero vector. Thus, using\nLASSO-like penalties to perform variable selection in the presence of outliers\ncan result in missed detections of relevant covariates. We show that by\nchoosing a minimum distance criterion together with an Elastic Net penalty, we\ncan simultaneously find a parsimonious model and avoid estimation implosion\neven in the presence of many outliers in the important small $n$ large $p$\nsituation. Implementation using an MM algorithm is described and performance\nevaluated. \n\n"}
{"id": "1111.1257", "contents": "Title: Model-independent constraints on new physics in b --> s transitions Abstract: We provide a comprehensive model-independent analysis of rare decays\ninvolving the b --> s transition to put constraints on dimension-six Delta(F)=1\neffective operators. The constraints are derived from all the available\nup-to-date experimental data from the B-factories, CDF and LHCb. The\nimplications and future prospects for observables in b --> s l+l- and b --> s\nnu nu transitions in view of improved measurements are also investigated. The\npresent work updates and generalises previous studies providing, at the same\ntime, a useful tool to test the flavour structure of any theory beyond the SM. \n\n"}
{"id": "1111.3647", "contents": "Title: Flavor SU(4) breaking between effective couplings Abstract: Using a framework in which all elements are constrained by Dyson-Schwinger\nequation studies in QCD, and therefore incorporates a consistent, direct and\nsimultaneous description of light- and heavy-quarks and the states they\nconstitute, we analyze the accuracy of SU(4)-flavor symmetry relations between\n{\\pi}{\\rho}{\\pi}, K{\\rho}K and D{\\rho}D couplings. Such relations are widely\nused in phenomenological analyses of the interactions between matter and\ncharmed mesons. We find that whilst SU(3)-flavor symmetry is accurate to 20%,\nSU(4) relations underestimate the D{\\rho}D coupling by a factor of five. \n\n"}
{"id": "1111.6078", "contents": "Title: Leading order infrared quantum chromodynamics in Coulomb gauge Abstract: A truncation scheme for the Dyson-Schwinger equations of quantum\nchromodynamics in Coulomb gauge within the first order formalism is presented.\nThe truncation is based on an Ansatz for the Coulomb kernel occurring in the\naction. Results at leading loop order and in the infrared are discussed for\nboth the Yang-Mills and quark sectors. It is found that the resulting equations\nfor the static gluon and quark propagators agree with those derived in a\nquasi-particle approximation to the canonical Hamiltonian approach. Moreover, a\nconnection to the heavy quark limit is established. The equations are analyzed\nnumerically and it is seen that in both the gluonic and quark sectors, a\nnontrivial dynamical infrared mass scale emerges. \n\n"}
{"id": "1112.1198", "contents": "Title: Excited meson spectroscopy with two chirally improved quarks Abstract: The excited isovector meson spectrum is explored using two chirally improved\ndynamical quarks. Seven ensembles, with pion masses down to \\approx 250 MeV are\ndiscussed and used for extrapolations to the physical point. Strange mesons are\ninvestigated using partially quenched s-quarks. Using the variational method,\nwe extract excited states in several channels and most of the results are in\ngood agreement with experiment. \n\n"}
{"id": "1112.5854", "contents": "Title: On Bayesian Estimation via Divergences Abstract: In this Note we introduce a new methodology for Bayesian inference through\nthe use of $\\phi$-divergences and the duality technique. The asymptotic laws of\nthe estimates are established. \n\n"}
{"id": "1112.5966", "contents": "Title: Calculation of the mean duration and age of onset of a chronic disease\n  and application to dementia in Germany Abstract: This paper descibes a new method of calculating the mean duration and mean\nage of onset of a chronic disease from incidence and mortality rates. It is\nbased on an ordinary differential equation resulting from a simple compartment\nmodel. Applicability of the method is demonstrated in data about dementia in\nGermany. \n\n"}
{"id": "1201.3951", "contents": "Title: Estimation of length scale of RS II-$p$ braneworld model through\n  perturbations in Helium's atom ground state energy Abstract: We put to the test an effective three-dimensional electrostatic potential,\nobtained effectively by considering an electrostatic source inside a\n(5+$p$)-dimensional braneworld scenario with $p$ compact and one infinite\nspacial extra dimensions in the RS II-$p$ model, for $p=1$ and $p=2$. This\npotential is regular at the source and matches the standard Coulomb potential\noutside a neighborhood. We use variational and perturbative approximation\nmethods to calculate corrections to the ground energy of the Helium atom\nmodified by this potential, by making use of a 6 and 39-parameter trial wave\nfunction of Hylleraas type for the ground state. These corrections to the\nground-state energy are compared with experimental data for Helium atom in\norder to set bounds for the extra dimensions length scale. We find that these\nbounds are less restrictive than the ones obtained by Morales et. al. through a\ncalculation using the Lamb shift in Hydrogen. \n\n"}
{"id": "1201.4382", "contents": "Title: Top-quark production at hadron colliders Abstract: The current theoretical predictions for the observables related to the\ntop-quark pair and the single-top productions at hadron colliders are briefly\nreviewed. The theoretical predictions are compared to the experimental\nmeasurements carried out at the Tevatron and the LHC. \n\n"}
{"id": "1201.4403", "contents": "Title: Locally Adaptive Bayes Nonparametric Regression via Nested Gaussian\n  Processes Abstract: We propose a nested Gaussian process (nGP) as a locally adaptive prior for\nBayesian nonparametric regression. Specified through a set of stochastic\ndifferential equations (SDEs), the nGP imposes a Gaussian process prior for the\nfunction's $m$th-order derivative. The nesting comes in through including a\nlocal instantaneous mean function, which is drawn from another Gaussian process\ninducing adaptivity to locally-varying smoothness. We discuss the support of\nthe nGP prior in terms of the closure of a reproducing kernel Hilbert space,\nand consider theoretical properties of the posterior. The posterior mean under\nthe nGP prior is shown to be equivalent to the minimizer of a nested penalized\nsum-of-squares involving penalties for both the global and local roughness of\nthe function. Using highly-efficient Markov chain Monte Carlo for posterior\ninference, the proposed method performs well in simulation studies compared to\nseveral alternatives, and is scalable to massive data, illustrated through a\nproteomics application. \n\n"}
{"id": "1201.4891", "contents": "Title: Radiative Corrections to Scalar Masses and Mixing in a Scale Invariant\n  Two Higgs Doublet Model Abstract: We study the Higgs-boson mass spectrum of a classical scale-invariant\nrealization of the two-Higgs-doublet model (SI-2HDM). The classical scale\nsymmetry of the theory is explicitly broken by quantum loop effects due to\ngauge interactions, Higgs self-couplings and top-quark Yukawa couplings. We\ndetermine the allowed parameter space compatible with perturbative unitarity\nand electroweak precision data. Taking into account the LEP and the recent LHC\nexclusion limits on a Standard-Model-like Higgs boson H_SM, we obtain rather\nstrict constraints on the mass spectrum of the heavy Higgs sector of the\nSI-2HDM. In particular, if M_{H_SM} \\sim 125 GeV, the SI-2HDM strongly favours\nscenarios, in which at least one of the non-standard neutral Higgs bosons has a\nmass close to 400 GeV and is generically degenerate with the charged Higgs\nboson, whilst the third neutral Higgs scalar is lighter than ~ 500 GeV. \n\n"}
{"id": "1201.5133", "contents": "Title: Nonparametric estimation of pair-copula constructions with the empirical\n  pair-copula Abstract: A pair-copula construction is a decomposition of a multivariate copula into a\nstructured system, called regular vine, of bivariate copulae or pair-copulae.\nThe standard practice is to model these pair-copulae parametrically, which\ncomes at the cost of a large model risk, with errors propagating throughout the\nvine structure. The empirical pair-copula proposed in the paper provides a\nnonparametric alternative still achieving the parametric convergence rate. It\ncan be used as a basis for inference on dependence measures, for selecting and\npruning the vine structure, and for hypothesis tests concerning the form of the\npair-copulae. \n\n"}
{"id": "1201.5300", "contents": "Title: Dynamical Symmetry Breaking in Supersymmetric Extensions of\n  Nambu-Jona-Lasinio Model Abstract: In this paper we discuss Nambu-Jona-Lasinio model as a classical model for\ndynamical mass generation and symmetry breaking. In addition we discuss the\npossible supersymmetric extensions of this model resulting from interaction\nterms with four chiral superfields that may be regarded as a supersymmetric\ngeneralization of the four-fermion interactions of the Nambu-Jona-Lasinio\nmodel. A four-superfield interaction terms can be constructed as either\ndimension 6 or dimension 5 operators. Through analyzing solutions to the gap\nequations, we discuss the dynamical generation of superfield Dirac mass,\nincluding a supersymmetry breaking part. A dynamical symmetry breaking\ngenerally goes along with the dynamical mass generation, for which a\nbi-superfield condensate is responsible. \n\n"}
{"id": "1201.5893", "contents": "Title: A new stochastic differential equation modelling incidence and\n  prevalence with an application to systemic lupus erythematosus in England and\n  Wales, 1995 Abstract: This article reformulates a common illness-death model in terms of a new\nsystem of stochastical differential equations (SDEs). The SDEs are used to\nestimate epidemiological characteristics and burden of systemic lupus\nerythematosus in England and Wales in 1995. \n\n"}
{"id": "1201.5962", "contents": "Title: How many statistics are needed to characterize the univariate extremes Abstract: Let $X_{1},X_{2},...$ be a sequence of independent random variables ($rv$)\nwith common distribution function ($df$) $F$ such that $F(1)=0$. We consider\nthe simple statistical problem : find a statistics family of size $m\\geq 1$\nwhose convergence, in probability or almost surely, to a point of some domain\n$\\mathcal{S} \\in \\mathbb{R}^{m}$ is equivalent that $F$ lies in the extremal\ndomain of attraction $\\Gamma$. Such a family, whenever it exists, is called an\nEmpirical Characterizing Statistics Family for the EXTtremes (ECSFEXT). The\ndeparture point of this theory goes back to Mason, who proved that the Hill\nestimator converges a.s. to a positive real number for some particular\nsequences if and only $F$ lies in the attaction domain of a Fr\\'echet's law.\nConsidered for the whole attraction domain, the question becomes more complex.\nWe provide here an ECSFEXT of nine (9) elements and also characterize the\nsubdomains of $\\Gamma$. The question of lowering m=9 to a minimum number is\nlaunched. \n\n"}
{"id": "1202.0507", "contents": "Title: Associated production of light gravitinos at future linear colliders Abstract: We study light gravitino productions in association with a neutralino at\nfuture linear colliders in a scenario in which the lightest SUSY particle is a\ngravitino and the produced neutralino promptly decays into a photon and a\ngravitino. Comparing with the multiple goldstino scenario, we show that energy\nand angular distributions of the photon provide valuable information on the\nSUSY masses as well as the SUSY breaking. \n\n"}
{"id": "1202.0807", "contents": "Title: Seesaw Scale in the Minimal Renormalizable SO(10) Grand Unification Abstract: Simple SO(10) Higgs models with the adjoint representation triggering the\ngrand-unified symmetry breaking, discarded a long ago due to inherent\ntree-level tachyonic instabilities in the physically interesting scenarios,\nhave been recently brought back to life by quantum effects. In this work we\nfocus on the variant with 45_H+126_H in the Higgs sector and show that there\nare several regions in the parameter space of this model that can support\nstable unifying configurations with the B-L breaking scale as high as 10^14\nGeV, well above the previous generic estimates based on the minimal survival\nhypothesis. This admits for a renormalizable implementation of the canonical\nseesaw and makes the simplest potentially realistic scenario of this kind a\ngood candidate for a minimal SO(10) grand unification. Last, but not least,\nthis setting is likely to be extensively testable at future large-volume\nfacilities such as Hyper-Kamiokande. \n\n"}
{"id": "1202.0949", "contents": "Title: Bayesian filtering for multi-object systems with independently generated\n  observations Abstract: A general approach for Bayesian filtering of multi-object systems is studied,\nwith particular emphasis on the model where each object generates observations\nindependently of other objects. The approach is based on variational calculus\napplied to generating functionals, using the general version of Faa di Bruno's\nformula for Gateaux differentials. This result enables us to determine some\ngeneral formulae for the updated generating functional after the application of\na multi-object analogue of Bayes' rule. \n\n"}
{"id": "1202.1377", "contents": "Title: Statistical significance in high-dimensional linear models Abstract: We propose a method for constructing p-values for general hypotheses in a\nhigh-dimensional linear model. The hypotheses can be local for testing a single\nregression parameter or they may be more global involving several up to all\nparameters. Furthermore, when considering many hypotheses, we show how to\nadjust for multiple testing taking dependence among the p-values into account.\nOur technique is based on Ridge estimation with an additional correction term\ndue to a substantial projection bias in high dimensions. We prove strong error\ncontrol for our p-values and provide sufficient conditions for detection: for\nthe former, we do not make any assumption on the size of the true underlying\nregression coefficients while regarding the latter, our procedure might not be\noptimal in terms of power. We demonstrate the method in simulated examples and\na real data application. \n\n"}
{"id": "1202.4769", "contents": "Title: Beyond the Minimal Supersymmetric Standard Model: from theory to\n  phenomenology Abstract: Thanks to the latest development in the field of Monte Carlo event generators\nand satellite programs allowing for a straightforward implementation of any\nbeyond the Standard Model theory in those tools, studying the property of any\nsoftly-broken supersymmetric theory is become an easy task. We illustrate this\nstatement in the context of two non-minimal supersymmetric theories, namely the\nMinimal Supersymmetric Standard Model with R-parity violation and the Minimal\nR-symmetric Supersymmetric Standard Model and choose to probe interaction\nvertices involving a non-standard color structure and the sector of the top\nquark. We show how to efficiently implement these theories in the Mathematica\npackage FeynRules and use its interfaces to Monte Carlo tools for\nphenomenological studies. For the latter, we employ the latest version of the\nMadGraph program. \n\n"}
{"id": "1203.0196", "contents": "Title: The third type of fermion mixing in the lepton and quark interactions\n  with leptoquarks Abstract: The low-energy manifestations of a minimal extension of the electroweak\nstandard model based on the quark-lepton symmetry $SU(4)_V \\otimes SU(2)_L\n\\otimes G_R$ of the Pati--Salam type are analyzed. Given this symmetry the\nthird type of mixing in the interactions of the $SU(4)_V$ leptoquarks with\nquarks and leptons is shown to be required. An additional arbitrariness of the\nmixing parameters could allow, in principle, to decrease noticeably the lower\nbound on the vector leptoquark mass originated from the low-energy rare\nprocesses, strongly suppressed in the standard model. \n\n"}
{"id": "1203.0733", "contents": "Title: A Confinement Potential for Leptons and Their Tunneling Effects into\n  Extra Dimensions Abstract: Considering the five-dimensional warped spacetime AdS_5 with the D3-brane, we\nderive a potential in the fifth dimension, according to which ordinary\nparticles are initially confined on the D3-brane. It is estimated, however,\nthat the lightest neutrino with mass m_1 is tunneling away into the extra\ndimension. Hence there is a possibility that no neutrinos with mass m_1 exist\nin cosmicbackground neutrinos, but surviving neutrinos are those with heavier\nmasses m_2 and m_3. The other possibilities are also discussed. \n\n"}
{"id": "1203.5343", "contents": "Title: Two-dimensional chiral crystals in the NJL model Abstract: We investigate the phase structure of the Nambu--Jona-Lasinio model at zero\ntemperature, allowing for a two-dimensional spatial dependence of the chiral\ncondensate. Applying the mean-field approximation, we consider various periodic\nstructures with rectangular and hexagonal geometries, and minimize the\ncorresponding free energy. We find that these two-dimensional chiral crystals\nare favored over homogeneous phases in a certain window in the region where the\nphase transition would take place when the analysis was restricted to\nhomogeneous condensates. It turns out, however, that in this regime they are\ndisfavored against a phase with a one-dimensional modulation of the chiral\ncondensate. On the other hand, we find that square and hexagonal lattices\neventually get favored at higher chemical potentials. Although stretching the\nlimits of the model to some extent, this would support predictions from\nquarkyonic-matter studies. \n\n"}
{"id": "1204.0316", "contents": "Title: Subsampling Extremes: From Block Maxima to Smooth Tail Estimation Abstract: We study a new estimator for the tail index of a distribution in the Frechet\ndomain of attraction that arises naturally by computing subsample maxima. This\nestimator is equivalent to taking a U-statistic over a Hill estimator with two\norder statistics. The estimator presents multiple advantages over the Hill\nestimator. In particular, it has asymptotically smooth sample paths as a\nfunction of the threshold k, making it considerably more stable than the Hill\nestimator. The estimator also admits a simple and intuitive threshold selection\nrule that does not require fitting a second-order model. Journal of\nMultivariate Analysis, 130, 2014 \n\n"}
{"id": "1204.1937", "contents": "Title: Identification of gene pathways implicated in Alzheimer's disease using\n  longitudinal imaging phenotypes with sparse regression Abstract: We present a new method for the detection of gene pathways associated with a\nmultivariate quantitative trait, and use it to identify causal pathways\nassociated with an imaging endophenotype characteristic of longitudinal\nstructural change in the brains of patients with Alzheimer's disease (AD). Our\nmethod, known as pathways sparse reduced-rank regression (PsRRR), uses group\nlasso penalised regression to jointly model the effects of genome-wide single\nnucleotide polymorphisms (SNPs), grouped into functional pathways using prior\nknowledge of gene-gene interactions. Pathways are ranked in order of importance\nusing a resampling strategy that exploits finite sample variability. Our\napplication study uses whole genome scans and MR images from 464 subjects in\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) database. 66,182 SNPs\nare mapped to 185 gene pathways from the KEGG pathways database. Voxel-wise\nimaging signatures characteristic of AD are obtained by analysing 3D patterns\nof structural change at 6, 12 and 24 months relative to baseline. High-ranking,\nAD endophenotype-associated pathways in our study include those describing\nchemokine, Jak-stat and insulin signalling pathways, and tight junction\ninteractions. All of these have been previously implicated in AD biology. In a\nsecondary analysis, we investigate SNPs and genes that may be driving pathway\nselection, and identify a number of previously validated AD genes including\nCR1, APOE and TOMM40. \n\n"}
{"id": "1204.2474", "contents": "Title: Holography of the Conformal Window Abstract: Inspired by the model of Jarvinen and Kiritsis, we present a simple\nholographic model for the on set of chiral symmetry breaking at the edge of the\nconformal window in QCD in the Veneziano limit. Our most naive model enforces\nthe QCD two loop running coupling on a D3/D7 holographic brane system. The mass\nof the holographic field, describing the chiral condensate in the model, is\ndriven below the BF bound when the running is sufficiently strong, triggering\nchiral symmetry breaking for N_f/N_c<2.9. This model though contains too great\na remnant of supersymmetry and does not correctly encode the perturbative\nanomalous dimensions of QCD. In a second model we impose the QCD anomalous\ndimension result and find chiral symmetry breaking sets in for N_f/N_c=4 at a\nBKT-type phase transition. In this case the transition is triggered when the\nanomalous dimension of the mass operator \\gamma_m=1. \n\n"}
{"id": "1204.4148", "contents": "Title: Algorithm for multivariate data standardization up to third moment Abstract: An algorithm for transforming multivariate data to a form with normalized\nfirst, second and third moments is presented. \n\n"}
{"id": "1204.4494", "contents": "Title: Rotation Sampling for Functional Data Abstract: This paper addresses the survey estimation of a population mean in continuous\ntime. For this purpose we extend the rotation sampling method to functional\ndata. In contrast to conventional rotation designs that select the sample\nbefore the survey, our approach randomizes each sample replacement and thus\nallows for adaptive sampling. Using Markov chain theory, we evaluate the\ncovariance structure and the integrated squared error [ISE] of the related\nHorvitz-Thompson estimator. Our sampling designs decrease the mean ISE by\nsuitably reallocating the sample across population strata during replacements.\nThey also reduce the variance of the ISE by increasing the frequency or the\nintensity of replacements. To investigate the benefits of using both current\nand past measurements in the estimation, we develop a new composite estimator.\nIn an application to electricity usage data, our rotation method outperforms\nfixed panels and conventional rotation samples. Because of the weak temporal\ndependence of the data, the composite estimator only slightly improves upon the\nHorvitz-Thompson estimator. \n\n"}
{"id": "1204.5782", "contents": "Title: On calculation of cross sections in Lorentz violating theories Abstract: We develop a systematic approach to the calculation of scattering cross\nsections in theories with violation of the Lorentz invariance taking into\naccount the whole information about the theory Lagrangian. As an illustration\nwe derive the Feynman rules and formulas for sums over polarizations in spinor\nelectrodynamics with Lorentz violating operators of dimensions four and six.\nThese rules are applied to compute the probabilities of several astrophysically\nrelevant processes. We calculate the rates of photon decay and vacuum Cherenkov\nradiation along with the cross sections of electron-positron pair production on\nbackground radiation and in the Coulomb field. The latter process is essential\nfor detection of photon-induced air showers in the atmosphere. \n\n"}
{"id": "1204.6516", "contents": "Title: Detection of additive outliers in Poisson INteger-valued AutoRegressive\n  time series Abstract: Outlying observations are commonly encountered in the analysis of time\nseries. In this paper the problem of detecting additive outliers in\ninteger-valued time series is considered. We show how Gibbs sampling can be\nused to detect outlying observations in INAR(1) processes. The methodology\nproposed is illustrated using examples as well as an observed data set. \n\n"}
{"id": "1205.0684", "contents": "Title: GSI anomaly and spin-rotation coupling Abstract: We propose a model in which a recently reported modulation in the decay of\nthe hydrogenlike ions ${}^{140}$Pr$^{\\, 58 +}$, ${}^{142}$Pm$^{\\, 60 +}$ and\n${}^{122}$I$^{\\, 52 +}$ arises from the coupling of rotation to the spin of\nelectron and nucleus. The model shows that the spin-spin coupling of electron\nand nucleus does not contribute to the modulation and predicts that the anomaly\ncannot be observed if the motion of the ions is rectilinear, or if the ions are\nstopped in a target. It also supports the notion that the modulation frequency\nis proportional to the inverse of the atomic mass. \n\n"}
{"id": "1205.2709", "contents": "Title: Muon Anomaly and Dark Parity Violation Abstract: The muon anomalous magnetic moment exhibits a 3.6 \\sigma discrepancy between\nexperiment and theory. One explanation requires the existence of a light vector\nboson, Z_d (the dark Z), with mass 10 - 500 MeV that couples weakly to the\nelectromagnetic current through kinetic mixing. Support for such a solution\nalso comes from astrophysics conjectures regarding the utility of a U(1)_d\ngauge symmetry in the dark matter sector. In that scenario, we show that mass\nmixing between the Z_d and ordinary Z boson introduces a new source of \"dark\"\nparity violation which is potentially observable in atomic and polarized\nelectron scattering experiments. Restrictive bounds on the mixing (m_{Z_d} /\nm_Z) \\delta are found from existing atomic parity violation results, \\delta^2 <\n2 x 10^{-5}. Combined with future planned and proposed polarized electron\nscattering experiments, a sensitivity of \\delta^2 ~ 10^{-6} is expected to be\nreached, thereby complementing direct searches for the Z_d boson. \n\n"}
{"id": "1206.0867", "contents": "Title: Testing linear hypotheses in high-dimensional regressions Abstract: For a multivariate linear model, Wilk's likelihood ratio test (LRT)\nconstitutes one of the cornerstone tools. However, the computation of its\nquantiles under the null or the alternative requires complex analytic\napproximations and more importantly, these distributional approximations are\nfeasible only for moderate dimension of the dependent variable, say $p\\le 20$.\nOn the other hand, assuming that the data dimension $p$ as well as the number\n$q$ of regression variables are fixed while the sample size $n$ grows, several\nasymptotic approximations are proposed in the literature for Wilk's $\\bLa$\nincluding the widely used chi-square approximation. In this paper, we consider\nnecessary modifications to Wilk's test in a high-dimensional context,\nspecifically assuming a high data dimension $p$ and a large sample size $n$.\nBased on recent random matrix theory, the correction we propose to Wilk's test\nis asymptotically Gaussian under the null and simulations demonstrate that the\ncorrected LRT has very satisfactory size and power, surely in the large $p$ and\nlarge $n$ context, but also for moderately large data dimensions like $p=30$ or\n$p=50$. As a byproduct, we give a reason explaining why the standard chi-square\napproximation fails for high-dimensional data. We also introduce a new\nprocedure for the classical multiple sample significance test in MANOVA which\nis valid for high-dimensional data. \n\n"}
{"id": "1206.2380", "contents": "Title: The Highest Dimensional Stochastic Blockmodel with a Regularized\n  Estimator Abstract: In the high dimensional Stochastic Blockmodel for a random network, the\nnumber of clusters (or blocks) K grows with the number of nodes N. Two previous\nstudies have examined the statistical estimation performance of spectral\nclustering and the maximum likelihood estimator under the high dimensional\nmodel; neither of these results allow K to grow faster than N^{1/2}. We study a\nmodel where, ignoring log terms, K can grow proportionally to N. Since the\nnumber of clusters must be smaller than the number of nodes, no reasonable\nmodel allows K to grow faster; thus, our asymptotic results are the \"highest\"\ndimensional. To push the asymptotic setting to this extreme, we make additional\nassumptions that are motivated by empirical observations in physical\nanthropology (Dunbar, 1992), and an in depth study of massive empirical\nnetworks (Leskovec et al 2008). Furthermore, we develop a regularized maximum\nlikelihood estimator that leverages these insights and we prove that, under\ncertain conditions, the proportion of nodes that the regularized estimator\nmisclusters converges to zero. This is the first paper to explicitly introduce\nand demonstrate the advantages of statistical regularization in a parametric\nform for network analysis. \n\n"}
{"id": "1206.4832", "contents": "Title: Smoothed Functional Algorithms for Stochastic Optimization using\n  q-Gaussian Distributions Abstract: Smoothed functional (SF) schemes for gradient estimation are known to be\nefficient in stochastic optimization algorithms, specially when the objective\nis to improve the performance of a stochastic system. However, the performance\nof these methods depends on several parameters, such as the choice of a\nsuitable smoothing kernel. Different kernels have been studied in literature,\nwhich include Gaussian, Cauchy and uniform distributions among others. This\npaper studies a new class of kernels based on the q-Gaussian distribution, that\nhas gained popularity in statistical physics over the last decade. Though the\nimportance of this family of distributions is attributed to its ability to\ngeneralize the Gaussian distribution, we observe that this class encompasses\nalmost all existing smoothing kernels. This motivates us to study SF schemes\nfor gradient estimation using the q-Gaussian distribution. Using the derived\ngradient estimates, we propose two-timescale algorithms for optimization of a\nstochastic objective function in a constrained setting with projected gradient\nsearch approach. We prove the convergence of our algorithms to the set of\nstationary points of an associated ODE. We also demonstrate their performance\nnumerically through simulations on a queuing model. \n\n"}
{"id": "1206.5070", "contents": "Title: A fluctuation test for constant Spearman's rho with nuisance-free limit\n  distribution Abstract: A CUSUM type test for constant correlation that goes beyond a previously\nsuggested correlation constancy test by considering Spearman's rho in arbitrary\ndimensions is proposed. Since the new test does not require the existence of\nany moments, the applicability on usually heavy-tailed financial data is\ngreatly improved. The asymptotic null distribution is calculated using an\ninvariance principle for the sequential empirical copula process. The limit\ndistribution is free of nuisance parameters and critical values can be obtained\nwithout bootstrap techniques. A local power result and an analysis of the\nbehavior of the test in small samples is provided. \n\n"}
{"id": "1206.5414", "contents": "Title: Eta-meson production in the resonance energy region Abstract: We perform an updated coupled-channel analysis of eta-meson production\nincluding all recent photoproduction data on the proton. The dip observed in\nthe differential cross sections at c.m. energies W=1.68 GeV is explained by\ndestructive interference between the $S_{11}(1535)$ and $S_{11}(1560)$ states.\n  The effect from $P_{11}(1710)$ is found to be small but still important to\nreproduce the correct shape of the differential cross section.\n  For the $\\pi^- N \\to \\eta N$ scattering we suggest a reaction mechanism in\nterms of the $S_{11}(1535)$, $S_{11}(1560)$, and $P_{11}(1710)$ states. Our\nconclusion on the importance of the $S_{11}(1535)$, $S_{11}(1560)$, and\n$P_{11}(1710)$ resonances in the eta-production reactions is in line with our\nprevious results. No strong indication for a narrow state with a width of 15\nMeV and the mass of 1680 MeV is found in the analysis. $\\eta N$ scattering\nlength is extracted and discussed. \n\n"}
{"id": "1206.6367", "contents": "Title: A comparison of the discrete Kolmogorov-Smirnov statistic and the\n  Euclidean distance Abstract: Goodness-of-fit tests gauge whether a given set of observations is consistent\n(up to expected random fluctuations) with arising as independent and\nidentically distributed (i.i.d.) draws from a user-specified probability\ndistribution known as the \"model.\" The standard gauges involve the discrepancy\nbetween the model and the empirical distribution of the observed draws. Some\nmeasures of discrepancy are cumulative; others are not. The most popular\ncumulative measure is the Kolmogorov-Smirnov statistic; when all probability\ndistributions under consideration are discrete, a natural noncumulative measure\nis the Euclidean distance between the model and the empirical distributions. In\nthe present paper, both mathematical analysis and its illustration via various\ndata sets indicate that the Kolmogorov-Smirnov statistic tends to be more\npowerful than the Euclidean distance when there is a natural ordering for the\nvalues that the draws can take -- that is, when the data is ordinal -- whereas\nthe Euclidean distance is more reliable and more easily understood than the\nKolmogorov-Smirnov statistic when there is no natural ordering (or partial\norder) -- that is, when the data is nominal. \n\n"}
{"id": "1206.6379", "contents": "Title: LieART -- A Mathematica Application for Lie Algebras and Representation\n  Theory Abstract: We present the Mathematica application LieART (Lie Algebras and\nRepresentation Theory) for computations frequently encountered in Lie Algebras\nand representation theory, such as tensor product decomposition and subalgebra\nbranching of irreducible representations. LieART can handle all classical and\nexceptional Lie algebras. It computes root systems of Lie algebras, weight\nsystems and several other properties of irreducible representations. LieART's\nuser interface has been created with a strong focus on usability and thus\nallows the input of irreducible representations via their dimensional name,\nwhile the output is in the textbook style used in most particle-physics\npublications. The unique Dynkin labels of irreducible representations are used\ninternally and can also be used for input and output. LieART exploits the Weyl\nreflection group for most of the calculations, resulting in fast computations\nand a low memory consumption. Extensive tables of properties, tensor products\nand branching rules of irreducible representations are included in the\nappendix. \n\n"}
{"id": "1206.6912", "contents": "Title: Fourth Generations with an Inert Doublet Higgs Abstract: We explore an extension of the fourth generation model with multi-Higgs\ndoublets and three fermion singlets. The Standard Model neutrinos acquire mass\nradiatively at one loop level while the fourth generation neutrinos acquire a\nheavy tree-level mass. The model also contains several Dark Matter candidate\nwhose stability is guaranteed by a $Z_2$ discrete symmetry. The possibility of\nCP violation in the scalar sector is also briefly discussed. \n\n"}
{"id": "1207.2791", "contents": "Title: Hydrodynamics at large baryon densities: Understanding proton vs.\n  anti-proton v_2 and other puzzles Abstract: We study the importance of the initial state, baryon stopping and baryon\nnumber transport for the dynamical evolution of a strongly interacting system\nproduced in heavy ion collisions. We employ a hybrid model, which combines the\nfluid dynamical evolution of the fireball with a transport treatment for the\ninitial state and the final hadronic phase. We present results for collisions\nat beam energies from sqrt{s_{NN}}=7.7 to 200 GeV. We study various observables\nsuch as the centrality dependent freeze out parameters, the non-monotonic\nbehavior of effective slope parameter parameter with particle mass as well as\nthe apparent difference in particle and anti-particle elliptic flow. Our\nresults are in reasonable agreement with the available data. We find that the\npropagation of the baryon-number current in the hydrodynamic evolution as well\nas the transport treatment of the hadronic phase are essential for reproducing\nthe experimental data. \n\n"}
{"id": "1207.3166", "contents": "Title: Axion cosmology with long-lived domain walls Abstract: We investigate the cosmological constraints on axion models where the domain\nwall number is greater than one. In these models, multiple domain walls\nattached to strings are formed, and they survive for a long time. Their\nannihilation occurs due to the effects of explicit symmetry breaking term which\nmight be raised by Planck-scale physics. We perform three-dimensional lattice\nsimulations and compute the spectra of axions and gravitational waves produced\nby long-lived domain walls. Using the numerical results, we estimated relic\ndensity of axions and gravitational waves. We find that the existence of\nlong-lived domain walls leads to the overproduction of cold dark matter axions,\nwhile the density of gravitational waves is too small to observe at the present\ntime. Combining the results with other observational constraints, we find that\nthe whole parameter region of models are excluded unless an unacceptable\nfine-tuning exists. \n\n"}
{"id": "1207.4609", "contents": "Title: Resumming soft and collinear contributions in deeply virtual Compton\n  scattering Abstract: We calculate the quark coefficient function Tq(x,xi) that enters the\nfactorized amplitude for deeply virtual Compton scattering (DVCS) at all order\nin a soft and collinear gluon approximation, focusing on the leading double\nlogarithmic behavior in x +/- xi, where x +/- xi is the light cone momentum\nfraction of the incoming/outgoing quarks. We show that the dominant part of the\nknown one loop result can be understood in an axial gauge as the result of a\nsemi-eikonal approximation to the box diagram. We then derive an all order\nresult for the leading contribution of the ladder diagrams and deduce a\nresummation formula valid in the vicinity of the boundaries of the regions\ndefining the energy flows of the incoming/outcoming quarks, i.e. x = +/- xi.\nThe resummed series results in a simple closed expression. \n\n"}
{"id": "1207.5808", "contents": "Title: Anomalous Transport from Kubo Formulae Abstract: Chiral anomalies have profound impact on the transport properties of\nrelativistic fluids. In four dimensions there are different types of anomalies,\npure gauge and mixed gauge-gravitational anomalies. They give rise to two new\nnon-dissipative transport coefficients, the chiral magnetic conductivity and\nthe chiral vortical conductivity. They can be calculated from the microscopic\ndegrees of freedom with the help of Kubo formulae. We review the calculation of\nthe anomalous transport coefficients via Kubo formulae with a particular\nemphasis on the contribution of the mixed gauge-gravitational anomaly. \n\n"}
{"id": "1207.6082", "contents": "Title: CalcHEP 3.4 for collider physics within and beyond the Standard Model Abstract: We present version 3.4 of the CalcHEP software package which is designed for\neffective evaluation and simulation of high energy physics collider processes\nat parton level.\n  The main features of CalcHEP are the computation of Feynman diagrams,\nintegration over multi-particle phase space and event simulation at parton\nlevel. The principle attractive key-points along these lines are that it has:\na) an easy startup even for those who are not familiar with CalcHEP; b) a\nfriendly and convenient graphical user interface; c) the option for a user to\neasily modify a model or introduce a new model by either using the graphical\ninterface or by using an external package with the possibility of cross\nchecking the results in different gauges; d) a batch interface which allows to\nperform very complicated and tedious calculations connecting production and\ndecay modes for processes with many particles in the final state.\n  With this features set, CalcHEP can efficiently perform calculations with a\nhigh level of automation from a theory in the form of a Lagrangian down to\nphenomenology in the form of cross sections, parton level event simulation and\nvarious kinematical distributions.\n  In this paper we report on the new features of CalcHEP 3.4 which improves the\npower of our package to be an effective tool for the study of modern collider\nphenomenology. \n\n"}
{"id": "1207.6606", "contents": "Title: Weighted sampling, Maximum Likelihood and minimum divergence estimators Abstract: This paper explores Maximum Likelihood in parametric models in the context of\nSanov type Large Deviation Probabilities. MLE in parametric models under\nweighted sampling is shown to be associated with the minimization of a specific\ndivergence criterion defined with respect to the distribution of the weights.\nSome properties of the resulting inferential procedure are presented; Bahadur\nefficiency of tests are also considered in this context. \n\n"}
{"id": "1208.1211", "contents": "Title: PAC-Bayesian Estimation and Prediction in Sparse Additive Models Abstract: The present paper is about estimation and prediction in high-dimensional\nadditive models under a sparsity assumption ($p\\gg n$ paradigm). A PAC-Bayesian\nstrategy is investigated, delivering oracle inequalities in probability. The\nimplementation is performed through recent outcomes in high-dimensional MCMC\nalgorithms, and the performance of our method is assessed on simulated data. \n\n"}
{"id": "1208.5467", "contents": "Title: Censored quantile regression processes under dependence and penalization Abstract: We consider quantile regression processes from censored data under dependent\ndata structures and derive a uniform Bahadur representation for those\nprocesses. We also consider cases where the dimension of the parameter in the\nquantile regression model is large. It is demonstrated that traditional\npenalized estimators such as the adaptive lasso yield sub-optimal rates if the\ncoefficients of the quantile regression cross zero. New penalization techniques\nare introduced which are able to deal with specific problems of censored data\nand yield estimates with an optimal rate. In contrast to most of the\nliterature, the asymptotic analysis does not require the assumption of\nindependent observations, but is based on rather weak assumptions, which are\nsatisfied for many kinds of dependent data. \n\n"}
{"id": "1208.5760", "contents": "Title: Sign problems, noise, and chiral symmetry breaking in a QCD-like theory Abstract: The Nambu-Jona-Lasinio model reduced to 2+1 dimensions has two different path\nintegral formulations: at finite chemical potential one formulation has a\nsevere sign problem similar to that found in QCD, while the other does not. At\nlarge N, where N is the number of flavors, one can compute the probability\ndistributions of fermion correlators analytically in both formulations. In the\nformer case one finds a broad distribution with small mean; in the latter one\nfinds a heavy tailed positive distribution amenable to the cumulant expansion\ntechniques developed in earlier work. We speculate on the implications of this\nmodel for QCD. \n\n"}
{"id": "1208.6547", "contents": "Title: Cosmological Ohm's law and dynamics of non-minimal electromagnetism Abstract: The origin of large-scale magnetic fields in cosmic structures and the\nintergalactic medium is still poorly understood. We explore the effects of\nnon-minimal couplings of electromagnetism on the cosmological evolution of\ncurrents and magnetic fields. In this context, we revisit the mildly non-linear\nplasma dynamics around recombination that are known to generate weak magnetic\nfields. We use the covariant approach to obtain a fully general and non-linear\nevolution equation for the plasma currents and derive a generalised Ohm law\nvalid on large scales as well as in the presence of non-minimal couplings to\ncosmological (pseudo-)scalar fields. Due to the sizeable conductivity of the\nplasma and the stringent observational bounds on such couplings, we conclude\nthat modifications of the standard (adiabatic) evolution of magnetic fields are\nseverely limited in these scenarios. Even at scales well beyond a Mpc, any\ndeparture from flux freezing behaviour is inhibited. \n\n"}
{"id": "1209.0012", "contents": "Title: Residual variance and the signal-to-noise ratio in high-dimensional\n  linear models Abstract: Residual variance and the signal-to-noise ratio are important quantities in\nmany statistical models and model fitting procedures. They play an important\nrole in regression diagnostics, in determining the performance limits in\nestimation and prediction problems, and in shrinkage parameter selection in\nmany popular regularized regression methods for high-dimensional data analysis.\nWe propose new estimators for the residual variance, the l2-signal strength,\nand the signal-to-noise ratio that are consistent and asymptotically normal in\nhigh-dimensional linear models with Gaussian predictors and errors, where the\nnumber of predictors d is proportional to the number of observations n.\nExisting results on residual variance estimation in high-dimensional linear\nmodels depend on sparsity in the underlying signal. Our results require no\nsparsity assumptions and imply that the residual variance may be consistently\nestimated even when d > n and the underlying signal itself is non-estimable.\nBasic numerical work suggests that some of the distributional assumptions made\nfor our theoretical results may be relaxed. \n\n"}
{"id": "1209.0451", "contents": "Title: Cosmology on Compact and Stable Supergravity Background Abstract: We propose a cosmological model of D3-brane universe on compact and stable\nsupergravity background of wrapped D7-branes in type IIB string theory\npreviously argued to be dual to pure N=1 SU(N) gauge theory in four dimensions.\nA model universe of order Planck size near the UV boundary dynamically flows\ntoward the IR with constant total energy density and accelerating expansion\nfollowed by smooth transition to decelerating expansion and collides with the\nwrapped D7-branes at the IR boundary. The model addresses the horizon and\nflatness problems with most of the expansion produced during the decelerating\nexpansion phase. The inflationary scenario is used to generate sources of\ninhomogeneities in the cosmic microwave background radiation and seeds for\nlarge scale structure formation from quantum fluctuations which exit the Hubble\nradius early during the accelerating expansion phase and the model addresses\nthe inhomogeneity problem with red tilt in the power spectrum. We propose that\nthe kinetic energy of the model universe is converted to matter and radiation\nby the collision followed by formation of baryons that stabilizes the model\nuniverse against gravitational force from the background at a finite distance\nfrom the IR boundary with the wrapped D7-branes serving as sources of color.\nFriedmann evolution then takes over with a positive cosmological constant term\ncoming from the remaining potential energy density which is interpreted as dark\nenergy. The magnitude of dark energy density is smaller than the total energy\ndensity during the flow by a ratio of the scale factor when the model universe\nappears in the UV to the scale factor at the moment of collision and stays\nconstant while the matter-radiation density falls during Friedmann expansion. \n\n"}
{"id": "1209.0949", "contents": "Title: A simple algorithm for automatic Feynman diagram generation Abstract: An algorithm for the automatic Feynman diagram (FD) generation is presented\nin this paper. The algorithm starts directly from the definition formula of FD,\nand is simple in concept and easy for coding. The symmetry factor for each FD\nis naturally generated. It is expected to bring convenience for the researchers\nwho are studying new calculation techniques or making new calculation tools and\nfor the researchers who are studying effective field theory. A C-program made\nfrom the algorithm is also presented, which is short, fast, yet very general\npurpose: it receives arbitrary user defined model and arbitrary process as\ninput and generates FD's at any order. \n\n"}
{"id": "1209.1371", "contents": "Title: On the age-, time- and migration dependent dynamics of diseases Abstract: This paper generalizes a previously published differential equation that\ndescribes the relation between the age-specific incidence, remission, and\nmortality of a disease with its prevalence. The underlying model is a simple\ncompartment model with three states (illness-death model). In contrast to the\nformer work, migration- and calendar time-effects are included. As an\napplication of the theoretical findings, a hypothetical example of an\nirreversible disease is treated. \n\n"}
{"id": "1209.1951", "contents": "Title: Big-bang nucleosynthesis with a long-lived CHAMP including He4\n  spallation process Abstract: We propose helium-4 spallation processes induced by long-lived stau in\nsupersymmetric standard models, and investigate an impact of the processes on\nlight elements abundances. We show that, as long as the phase space of helium-4\nspallation processes is open, they are more important than stau-catalyzed\nfusion and hence constrain the stau property. This talk is based on works\n(Jittoh et al., 2011). \n\n"}
{"id": "1209.2072", "contents": "Title: On the impossibility of constructing good population mean estimators in\n  a realistic Respondent Driven Sampling model Abstract: Current methods for population mean estimation from data collected by\nRespondent Driven Sampling (RDS) are based on the Horvitz-Thompson estimator\ntogether with a set of assumptions on the sampling model under which the\ninclusion probabilities can be determined from the information contained in the\ndata. In this paper, we argue that such set of assumptions are too simplistic\nto be realistic and that under realistic sampling models, the situation is far\nmore complicated. Specifically, we study a realistic RDS sampling model that is\nmotivated by a real world RDS dataset. We show that, for this model, the\ninclusion probabilities, which are necessary for the application of the\nHorvitz-Thompson estimator, can not be determined by the information in the\nsample alone. An implication is that, unless additional information about the\nunderlying population network is obtained, it is hopeless to conceive of a\ngeneral theory of population mean estimation from current RDS data. \n\n"}
{"id": "1209.5908", "contents": "Title: Correlated variables in regression: clustering and sparse estimation Abstract: We consider estimation in a high-dimensional linear model with strongly\ncorrelated variables. We propose to cluster the variables first and do\nsubsequent sparse estimation such as the Lasso for cluster-representatives or\nthe group Lasso based on the structure from the clusters. Regarding the first\nstep, we present a novel and bottom-up agglomerative clustering algorithm based\non canonical correlations, and we show that it finds an optimal solution and is\nstatistically consistent. We also present some theoretical arguments that\ncanonical correlation based clustering leads to a better-posed compatibility\nconstant for the design matrix which ensures identifiability and an oracle\ninequality for the group Lasso. Furthermore, we discuss circumstances where\ncluster-representatives and using the Lasso as subsequent estimator leads to\nimproved results for prediction and detection of variables. We complement the\ntheoretical analysis with various empirical results. \n\n"}
{"id": "1210.0493", "contents": "Title: Exponential-Family Random Graph Models for Rank-Order Relational Data Abstract: Rank-order relational data, in which each actor ranks the others according to\nsome criterion, often arise from sociometric measurements of judgment (e.g.,\nself-reported interpersonal interaction) or preference (e.g., relative liking).\nWe propose a class of exponential-family models for rank-order relational data\nand derive a new class of sufficient statistics for such data, which assume no\nmore than within-subject ordinal properties. Application of MCMC MLE to this\nfamily allows us to estimate effects for a variety of plausible mechanisms\ngoverning rank structure in cross-sectional context, and to model the evolution\nof such structures over time. We apply this framework to model the evolution of\nrelative liking judgments in an acquaintance process, and to model recall of\nrelative volume of interpersonal interaction among members of a technology\neducation program. \n\n"}
{"id": "1210.1968", "contents": "Title: SQM studied in the Field Correlator Method Abstract: By using the recent nonperturbative equation of state of the quark gluon\nplasma derived in the formalism of the Field Correlator Method, we investigate\nthe bulk properties of the strange quark matter in beta-equilibrium and with\ncharge neutrality at T=p=0. The results show that the stability of strange\nquark matter with respect to $^{56}Fe$ is strongly dependent on the model\nparameters, namely, the gluon condensate $G_2$ and the q$\\bar{\\rm q}$\ninteraction potential $V_1$. A remarkable result is that the width of the\nstability window decreases as $V_1$ increases, being maximum at $V_1=0$ and\nnearly zero at $V_1=0.5$ GeV. For $V_1$ in the range $0\\leq V_1\\leq0.5$ GeV,\nall values of $G_2$ are lower than $0.006-0.007\\;{\\rm GeV}^4$ obtained from\ncomparison with lattice results at $T_c\\;(\\mu=0)\\sim170$ MeV. These results do\nnot favor the possibilities for the existence of (either nonnegative or\nnegative) absolutely stable strange quark matter. \n\n"}
{"id": "1211.0634", "contents": "Title: Effective Field Theories and the Role of Consistency in Theory Choice Abstract: Promoting a theory with a finite number of terms into an effective field\ntheory with an infinite number of terms worsens simplicity, predictability,\nfalsifiability, and other attributes often favored in theory choice. However,\nthe importance of these attributes pales in comparison with consistency, both\nobservational and mathematical consistency, which propels the effective theory\nto be superior to its simpler truncated version of finite terms, whether that\ntheory be renormalizable (e.g., Standard Model of particle physics) or\nnonrenormalizable (e.g., gravity). Some implications for the Large Hadron\nCollider and beyond are discussed, including comments on how directly\nacknowledging the preeminence of consistency can affect future theory work. \n\n"}
{"id": "1211.2148", "contents": "Title: Understanding the property of $\\eta(1405/1475)$ in the $J/\\psi$\n  radiative decay Abstract: In this work we make a systematic analysis of the correlated processes\n$J/\\psi\\to \\gamma \\eta(1440)/f_1(1420)$ with $\\eta(1440)/f_1(1420)\\to\nK\\bar{K}\\pi$, $\\eta\\pi\\pi$ and $3\\pi$, where the role played by the so-called\n\"triangle singularity mechanism\" (TSM) is clarified. Our results agree well\nwith the experimental data and suggest a small fraction of $f_1(1420)$\ncontributions in these processes. This study confirms our conclusion in [Phys.\nRev. Lett. 108, 081803 (2012)] that the dynamic feature of the TSM can be\nrecognized by the strong narrow peak observed in the $\\pi\\pi$ invariant mass\nspectrum of $\\eta(1440)\\to 3\\pi$ with anomalously large isospin violations.\nNevertheless, we explicitly demonstrate that the TSM can produce obvious peak\nposition shifts for the same $\\eta(1440)$ or $f_1(1420)$ state in different\ndecay channels. This is a strong evidence that the $\\eta(1405)$ and\n$\\eta(1475)$ are actually the same state, i.e. $\\eta(1440)$. We also make an\nanalysis of the radiative decays of $\\eta(1440)\\to \\gamma V$ ($V=\\phi$,\n$\\rho^0$ or $\\omega$) which shows that such a one-state prescription seems not\nto have a conflict with the so-far existing experimental data. Our analysis may\nshed a light on the long-standing puzzling question on the nature of\n$\\eta(1405)$ and $\\eta(1475)$. \n\n"}
{"id": "1211.6714", "contents": "Title: Global variables and correlations: Summary of the results presented at\n  the Quark Matter 2012 conference Abstract: In these proceedings, we highlight recent developments from both theory and\nexperiment related to the global description of matter produced in\nultra-relativistic heavy-ion collisions as presented during the Quark Matter\n2012 conference. \n\n"}
{"id": "1212.3935", "contents": "Title: Gauge Mediation Models with Vectorlike Matters at the LHC Abstract: Gauge mediation model with vectorlike matters (V-GMSB) is one of the few\nviable SUSY models that explains the 126 GeV Higgs boson mass and the muon\nanomalous magnetic moment simultaneously. We explore exclusion bounds on V-GMSB\nmodel from latest LHC SUSY searches. \n\n"}
{"id": "1212.4693", "contents": "Title: A General Metric for Riemannian Manifold Hamiltonian Monte Carlo Abstract: Markov Chain Monte Carlo (MCMC) is an invaluable means of inference with\ncomplicated models, and Hamiltonian Monte Carlo, in particular Riemannian\nManifold Hamiltonian Monte Carlo (RMHMC), has demonstrated impressive success\nin many challenging problems. Current RMHMC implementations, however, rely on a\nRiemannian metric that limits their application to analytically-convenient\nmodels. In this paper I propose a new metric for RMHMC without these\nlimitations and verify its success on a distribution that emulates many\nhierarchical and latent models. \n\n"}
{"id": "1212.5948", "contents": "Title: Multiple Z' -> t-tbar signals in a 4D Composite Higgs Model Abstract: We study the production of top-antitop pairs at the Large Hadron Collider as\na testbed for discovering heavy Z' bosons belonging to a composite Higgs model,\nas, in this scenario, such new gauge interaction states are sizeably coupled to\nthe third generation quarks of the Standard Model. We study their possible\nappearance in cross section as well as (charge and spin) asymmetry\ndistributions. Our calculations are performed in the minimal four-dimensional\nformulation of such a scenario, namely the 4-Dimensional Composite Higgs Model\n(4DCHM), which embeds five new $Z'$s. We pay particular attention to the case\nof nearly degenerate resonances, highlighting the conditions under which these\nare separable in the aforementioned observables. We also discuss the impact of\nthe intrinsic width of the new resonances onto the event rates and various\ndistributions. We confirm that the 14 TeV stage of the LHC will enable one to\ndetect two such states, assuming standard detector performance and machine\nluminosity. A mapping of the discovery potential of the LHC of these new gauge\nbosons is given. Finally, from the latter, several benchmarks are extracted\nwhich are amenable to experimental investigation. \n\n"}
{"id": "1212.5989", "contents": "Title: Mass Splitting between Charged and Neutral Winos at Two-Loop Level Abstract: The recent result of the higgs search at the LHC experiment has lead to more\nattention to the supersymmetric standard models with heavy sfermions. Among\nthem, the models with the almost pure wino being the lightest supersymmetric\nparticle (LSP) have been widely discussed due to their success in providing a\nconsistent dark matter candidate. The notable phenomenological feature of the\nwino LSP is the degeneracy with its charged SU(2)_L partner (the charged wino)\nin mass. The tiny mass splitting makes the charged wino long-lived, which\nallows us to detect the wino production at the LHC experiment by searching for\nthe disappearing charged tracks inside the detectors. Since the reach of the\nexperiment is sensitive to the mass splitting, it is mandatory to estimate it\nvery precisely. We therefore perform a full calculation of the mass splitting\nat two-loop level, and find that the splitting is reduced by a few MeV compared\nto the one-loop calculation. This reduction leads to about a 30 % longer\nlifetime of the charged wino, with which the current constraint on the wino\nmass by the ATLAS experiment is improved by about 10 %. \n\n"}
{"id": "1301.6624", "contents": "Title: Markovian acyclic directed mixed graphs for discrete data Abstract: Acyclic directed mixed graphs (ADMGs) are graphs that contain directed\n($\\rightarrow$) and bidirected ($\\leftrightarrow$) edges, subject to the\nconstraint that there are no cycles of directed edges. Such graphs may be used\nto represent the conditional independence structure induced by a DAG model\ncontaining hidden variables on its observed margin. The Markovian model\nassociated with an ADMG is simply the set of distributions obeying the global\nMarkov property, given via a simple path criterion (m-separation). We first\npresent a factorization criterion characterizing the Markovian model that\ngeneralizes the well-known recursive factorization for DAGs. For the case of\nfinite discrete random variables, we also provide a parameterization of the\nmodel in terms of simple conditional probabilities, and characterize its\nvariation dependence. We show that the induced models are smooth. Consequently,\nMarkovian ADMG models for discrete variables are curved exponential families of\ndistributions. \n\n"}
{"id": "1302.0907", "contents": "Title: Bootstrap Methods for the Empirical Study of Decision-Making and\n  Information Flows in Social Systems Abstract: We characterize the statistical bootstrap for the estimation of\ninformation-theoretic quantities from data, with particular reference to its\nuse in the study of large-scale social phenomena. Our methods allow one to\npreserve, approximately, the underlying axiomatic relationships of information\ntheory---in particular, consistency under arbitrary coarse-graining---that\nmotivate use of these quantities in the first place, while providing\nreliability comparable to the state of the art for Bayesian estimators. We show\nhow information-theoretic quantities allow for rigorous empirical study of the\ndecision-making capacities of rational agents and the time-asymmetric flows of\ninformation in distributed systems. We provide illustrative examples by\nreference to ongoing collaborative work on the semantic structure of the\nBritish Criminal Court system and the conflict dynamics of the contemporary\nAfghanistan insurgency. \n\n"}
{"id": "1302.1805", "contents": "Title: Testing for Homogeneity in Mixture Models Abstract: Statistical models of unobserved heterogeneity are typically formalized as\nmixtures of simple parametric models and interest naturally focuses on testing\nfor homogeneity versus general mixture alternatives. Many tests of this type\ncan be interpreted as $C(\\alpha)$ tests, as in Neyman (1959), and shown to be\nlocally, asymptotically optimal. These $C(\\alpha)$ tests will be contrasted\nwith a new approach to likelihood ratio testing for general mixture models. The\nlatter tests are based on estimation of general nonparametric mixing\ndistribution with the Kiefer and Wolfowitz (1956) maximum likelihood estimator.\nRecent developments in convex optimization have dramatically improved upon\nearlier EM methods for computation of these estimators, and recent results on\nthe large sample behavior of likelihood ratios involving such estimators yield\na tractable form of asymptotic inference. Improvement in computation efficiency\nalso facilitates the use of a bootstrap methods to determine critical values\nthat are shown to work better than the asymptotic critical values in finite\nsamples. Consistency of the bootstrap procedure is also formally established.\nWe compare performance of the two approaches identifying circumstances in which\neach is preferred. \n\n"}
{"id": "1302.3065", "contents": "Title: Bayesian analysis of measurement error models using INLA Abstract: To account for measurement error (ME) in explanatory variables, Bayesian\napproaches provide a flexible framework, as expert knowledge about unobserved\ncovariates can be incorporated in the prior distributions. However, given the\nanalytic intractability of the posterior distribution, model inference so far\nhas to be performed via time-consuming and complex Markov chain Monte Carlo\nimplementations. In this paper we extend the Integrated nested Laplace\napproximations (INLA) approach to formulate Gaussian ME models in generalized\nlinear mixed models. We present three applications, and show how parameter\nestimates are obtained for common ME models, such as the classical and Berkson\nerror model including heteroscedastic variances. To illustrate the practical\nfeasibility, R-code is provided. \n\n"}
{"id": "1302.3276", "contents": "Title: Practical corollaries of transverse Ward-Green-Takahashi identities Abstract: The gauge principle is fundamental in formulating the Standard Model.\nFermion--gauge-boson couplings are the inescapable consequence and the primary\ndetermining factor for observable phenomena. Vertices describing such couplings\nare simple in perturbation theory and yet the existence of strong-interaction\nbound-states guarantees that many phenomena within the Model are\nnonperturbative. It is therefore crucial to understand how dynamics dresses the\nvertices and thereby fundamentally alters the appearance of\nfermion--gauge-boson interactions. We consider the coupling of a\ndressed-fermion to an Abelian gauge boson, and describe a unified treatment and\nsolution of the familiar longitudinal Ward-Green-Takahashi identity and its\nless well known transverse counterparts. Novel consequences for the\ndressed-fermion--gauge-boson vertex are exposed. \n\n"}
{"id": "1302.4438", "contents": "Title: Gauge Coupling Unification and Non-Equilibrium Thermal Dark Matter Abstract: We study a new mechanism for the production of dark matter in the universe\nwhich does not rely on thermal equilibrium. Dark matter is populated from the\nthermal bath subsequent to inflationary reheating via a massive mediator whose\nmass is above the reheating scale, T_R. To this end, we consider models with an\nextra U(1) gauge symmetry broken at some intermediate scale M, of the order of\n10^10 -- 10^12 GeV. We show that not only does the model allow for gauge\ncoupling unification (at a higher scale associated with grand unification) but\ncan naturally provide a dark matter candidate which is a Standard Model singlet\nbut charged under the extra U(1). The intermediate scale gauge boson(s) which\nare predicted in several E6/SO(10) constructions can be a natural mediator\nbetween dark matter and the thermal bath. We show that the dark matter\nabundance, while never having achieved thermal equilibrium, is fixed shortly\nafter the reheating epoch by the relation T_R^3/M^4. As a consequence, we show\nthat the unification of gauge couplings which determines M also fixes the\nreheating temperature T_R, which can be as high as 10^11 GeV. \n\n"}
{"id": "1302.5831", "contents": "Title: On Testing Independence and Goodness-of-fit in Linear Models Abstract: We consider a linear regression model and propose an omnibus test to\nsimultaneously check the assumption of independence between the error and the\npredictor variables, and the goodness-of-fit of the parametric model. Our\napproach is based on testing for independence between the residual obtained\nfrom the parametric fit and the predictor using the Hilbert--Schmidt\nindependence criterion (Gretton et al. (2008)). The proposed method requires no\nuser-defined regularization, is simple to compute, based merely on pairwise\ndistances between points in the sample, and is consistent against all\nalternatives. We develop distribution theory for the proposed test statistic,\nboth under the null and the alternative hypotheses, and devise a bootstrap\nscheme to approximate its null distribution. We prove the consistency of the\nbootstrap scheme. A simulation study shows that our method has better power\nthan its main competitors. Two real datasets are analyzed to demonstrate the\nscope and usefulness of our method. \n\n"}
{"id": "1303.2230", "contents": "Title: Higgs decays to gamma l+ l- in the standard model Abstract: The radiative Higgs decays h -> gamma l+l- with l=e,mu and tau are analyzed\nin the standard model using m_h=125 GeV. Both tree and one-loop diagrams for\nthe processes are evaluated. In addition to their decay rates and dilepton\ninvariant mass distributions, we focus on the forward-back asymmetries in these\nmodes. Our calculation shows that the forward-backward asymmetries in h ->\ngamma e+e- and h -> gamma mu+mu- could be up to 10^{-2} while in the tau+tau-\nfinal state, these asymmetries are below 1%. Thus the forward-backward\nasymmetries in h -> gamma l+l- might be interesting observables in the future\nprecise experiments both to test our understanding of Higgs physics in the\nstandard model and to probe the novel Higgs dynamics in new physics scenarios. \n\n"}
{"id": "1303.4983", "contents": "Title: Gauge- and point-invariant vertices of nucleon-to-resonance interactions Abstract: We construct interactions of nucleons N with higher-spin resonances R\ninvariant under point and gauge transformations of the Rarita-Schwinger field.\nIt is found for arbitrarily high spin of a resonance that the requirement of\npoint- and gauge-invariance uniquely determines a Lagrangian of NR interactions\nwith pions, photons, and vector mesons, which might reduce model ambiguity in\neffective-field calculations involving such vertices. Considering the NR\ninteractions with photons and vector mesons, the symmetry provides a\nclassification of three NR vertices in terms of their differential order. The\nQ^2 dependencies of the point and gauge invariant form factors are considered\nin a vector-meson-dominance model. The model is in good agreement with\nexperimental data. In addition, we point out some empirical patterns in the Q^2\ndependencies of the form factors: low-Q^2 scaling of the N-Delta(1232) form\nfactor ratios and relations between form factors for N-N(1520) and N-N(1680)\ntransitions. \n\n"}
{"id": "1304.0282", "contents": "Title: Uniform Post Selection Inference for LAD Regression and Other\n  Z-estimation problems Abstract: We develop uniformly valid confidence regions for regression coefficients in\na high-dimensional sparse median regression model with homoscedastic errors.\nOur methods are based on a moment equation that is immunized against\nnon-regular estimation of the nuisance part of the median regression function\nby using Neyman's orthogonalization. We establish that the resulting\ninstrumental median regression estimator of a target regression coefficient is\nasymptotically normally distributed uniformly with respect to the underlying\nsparse model and is semi-parametrically efficient. We also generalize our\nmethod to a general non-smooth Z-estimation framework with the number of target\nparameters $p_1$ being possibly much larger than the sample size $n$. We extend\nHuber's results on asymptotic normality to this setting, demonstrating uniform\nasymptotic normality of the proposed estimators over $p_1$-dimensional\nrectangles, constructing simultaneous confidence bands on all of the $p_1$\ntarget parameters, and establishing asymptotic validity of the bands uniformly\nover underlying approximately sparse models.\n  Keywords: Instrument; Post-selection inference; Sparsity; Neyman's Orthogonal\nScore test; Uniformly valid inference; Z-estimation. \n\n"}
{"id": "1304.3073", "contents": "Title: R-Estimation for Asymmetric Independent Component Analysis Abstract: Independent Component Analysis (ICA) recently has attracted attention in the\nstatistical literature as an alternative to elliptical models. Whereas\nk-dimensional elliptical densities depend on one single unspecified radial\ndensity, however, k-dimensional independent component distributions involve k\nunspecified component densities that for given sample size n and dimension k\nmaking statistical analysis harder. We focus here on estimating the model's\nmixing matrix. Traditional methods (FOBI, Kernel-ICA, FastICA) originating from\nthe engineering literature have consistency that requires moment conditions\nwithout achieving any type of asymptotic efficiency. When based on robust\nscatter matrices, the two-scatter methods developed by Oja, et al. (2006) and\nNordhausen, et al. (2008) enjoy better robustness features but have unclear\noptimality properties. The semiparametric approach by Chen and Bickel (2006)\nachieves semiparametric efficiency but requires estimating the k unobserved\nindependent component densities. As a reaction, an efficient\n(signed-)rank-based approach has been proposed by Ilmonen and Paindaveine\n(2011) for the case of symmetric component densities that fail to be root-n\nconsistent as soon as one of the component densities is asymmetric. In this\npaper, using ranks rather than signed ranks, we extend their approach to the\nasymmetric case and propose a one-step R-estimator for ICA mixing matrices. The\nfinite-sample performances of those estimators are investigated and compared to\nthose of existing methods under moderately large sample sizes. Particularly\ngood performances are obtained when using data-driven scores taking into\naccount the skewness and kurtosis of residuals. Finally, we show, by an\nempirical exercise, that our methods also may provide excellent results in a\ncontext such as image analysis, where the basic assumptions of ICA are quite\nunlikely to hold. \n\n"}
{"id": "1304.3206", "contents": "Title: Multivariate Generalized Gaussian Distribution: Convexity and Graphical\n  Models Abstract: We consider covariance estimation in the multivariate generalized Gaussian\ndistribution (MGGD) and elliptically symmetric (ES) distribution. The maximum\nlikelihood optimization associated with this problem is non-convex, yet it has\nbeen proved that its global solution can be often computed via simple fixed\npoint iterations. Our first contribution is a new analysis of this likelihood\nbased on geodesic convexity that requires weaker assumptions. Our second\ncontribution is a generalized framework for structured covariance estimation\nunder sparsity constraints. We show that the optimizations can be formulated as\nconvex minimization as long the MGGD shape parameter is larger than half and\nthe sparsity pattern is chordal. These include, for example, maximum likelihood\nestimation of banded inverse covariances in multivariate Laplace distributions,\nwhich are associated with time varying autoregressive processes. \n\n"}
{"id": "1304.6309", "contents": "Title: Sequential Tests of Multiple Hypotheses Controlling Type I and II\n  Familywise Error Rates Abstract: This paper addresses the following general scenario: A scientist wishes to\nperform a battery of experiments, each generating a sequential stream of data,\nto investigate some phenomenon. The scientist would like to control the overall\nerror rate in order to draw statistically-valid conclusions from each\nexperiment, while being as efficient as possible. The between-stream data may\ndiffer in distribution and dimension but also may be highly correlated, even\nduplicated exactly in some cases. Treating each experiment as a hypothesis test\nand adopting the familywise error rate (FWER) metric, we give a procedure that\nsequentially tests each hypothesis while controlling both the type I and II\nFWERs regardless of the between-stream correlation, and only requires arbitrary\nsequential test statistics that control the error rates for a given stream in\nisolation. The proposed procedure, which we call the sequential Holm procedure\nbecause of its inspiration from Holm's (1979) seminal fixed-sample procedure,\nshows simultaneous savings in expected sample size and less conservative error\ncontrol relative to fixed sample, sequential Bonferroni, and other recently\nproposed sequential procedures in a simulation study. \n\n"}
{"id": "1304.6715", "contents": "Title: Skewness and kurtosis unbiased by Gaussian uncertainties Abstract: Noise is an unavoidable part of most measurements which can hinder a correct\ninterpretation of the data. Uncertainties propagate in the data analysis and\ncan lead to biased results even in basic descriptive statistics such as the\ncentral moments and cumulants. Expressions of noise-unbiased estimates of\ncentral moments and cumulants up to the fourth order are presented under the\nassumption of independent Gaussian uncertainties, for weighted and unweighted\nstatistics. These results are expected to be relevant for applications of the\nskewness and kurtosis estimators such as outlier detections, normality tests\nand in automated classification procedures. The comparison of estimators\ncorrected and not corrected for noise biases is illustrated with simulations as\na function of signal-to-noise ratio, employing different sample sizes and\nweighting schemes. \n\n"}
{"id": "1304.7198", "contents": "Title: Evidential Value in ANOVA Results in Favor of Fabrication Abstract: Some scientific publications are under suspicion of fabrication of data.\nSince humans are bad random number generators, there might be some evidential\nvalue in favor of fabrication in the statistical results as presented in such\npapers. In line with Uri Simonsohn (2012, 2013) we study the evidential value\nof the results of an ANOVA study in favor of the hypothesis of a dependence\nstructure in the underlying data. \n\n"}
{"id": "1305.0491", "contents": "Title: Searches for long-lived charged particles in pp collisions at sqrt(s) =\n  7 and 8 TeV Abstract: Results of searches for heavy stable charged particles produced in pp\ncollisions at sqrt(s) = 7 and 8 TeV are presented corresponding to an\nintegrated luminosity of 5.0 inverse femtobarns and 18.8 inverse femtobarns,\nrespectively. Data collected with the CMS detector are used to study the\nmomentum, energy deposition, and time-of-flight of signal candidates. Leptons\nwith an electric charge between e/3 and 8e, as well as bound states that can\nundergo charge exchange with the detector material, are studied. Analysis\nresults are presented for various combinations of signatures in the inner\ntracker only, inner tracker and muon detector, and muon detector only. Detector\nsignatures utilized are long time-of-flight to the outer muon system and\nanomalously high (or low) energy deposition in the inner tracker. The data are\nconsistent with the expected background, and upper limits are set on the\nproduction cross section of long-lived gluinos, scalar top quarks, and scalar\ntau leptons, as well as pair produced long-lived leptons. Corresponding lower\nmass limits, ranging up to 1322 GeV for gluinos, are the most stringent to\ndate. \n\n"}
{"id": "1305.1001", "contents": "Title: A non Supersymmetric SO(10) Grand Unified Model for All the Physics\n  below $M_{GUT}$ Abstract: We present a renormalizable non supersymmetric Grand Unified SO(10) model\nwhich, at the price of a large fine tuning, is compatible with all compelling\nphenomenological requirements below the unification scale and thus realizes a\nminimal extension of the SM, unified in SO(10) and describing all known physics\nbelow $M_{GUT}$. These requirements include coupling unification at a large\nenough scale to be compatible with the bounds on proton decay; a Yukawa sector\nin agreement with all the data on quark and lepton masses and mixings and with\nleptogenesis as the origin of the baryon asymmetry of the Universe; an axion\narising from the Higgs sector of the model, suitable to solve the strong CP\nproblem and to account for the observed amount of Dark Matter. The above\nconstraints imposed by the data are very stringent and single out a particular\nbreaking chain with the Pati-Salam group at an intermediate scale\n$M_I\\sim10^{11}$ GeV. \n\n"}
{"id": "1305.1067", "contents": "Title: Two-Body Strong Decay of Z(3930) as the $\\chi_{c2} (2P)$ State Abstract: The new particle Z(3930) found by the Belle and BaBar Collaborations through\nthe $\\gamma\\gamma\\rightarrow D\\bar D$ process is identified to be the\n$\\chi_{c2}(2P)$ state. Since the mass of this particle is above the $D\\bar\nD^{(\\ast)}$ threshold, the OZI-allowed two-body strong decays are the main\ndecay modes. In this paper, these strong decay modes are studied with two\nmethods. One is the instantaneous Bethe-Salpeter method within Mandelstam\nformalism. The other is the combination of the $^3P_0$ model and the former\nformalism. The total decay widths are 26.3 and 27.3 MeV for the methods with or\nwithout the $^3P_0$ vertex, respectively. The ratio of $\\Gamma_{D\\bar D}$ over\n$\\Gamma_{D\\bar D^\\ast}$ which changes along with the mass of the initial meson\nis also presented. \n\n"}
{"id": "1305.1181", "contents": "Title: Reactions with pions and vector mesons in the sector of odd intrinsic\n  parity Abstract: The Wess-Zumino-Witten structure is supplemented by a simple vector-meson\nLagrangian where the vector mesons are described by antisymmetric tensor\nfields. With the \\rho-\\omega-\\pi{} coupling as the only parameter in the sector\nof odd intrinsic parity, i.e. without additional contact terms, one can achieve\na proper description of the decay of an \\omega-meson into three pions, the\nsingle- and double-virtual pion transition form factor and the three-pion\nproduction in electron-positron collisions. \n\n"}
{"id": "1305.2382", "contents": "Title: Impact of active-sterile neutrino mixing on supernova explosion and\n  nucleosynthesis Abstract: We show that for the active-sterile flavor mixing parameters suggested by the\nreactor neutrino anomaly, substantial \\\\nu_e-\\\\nu_s and \\bar \\\\nu_e-\\bar \\\\nu_s\nconversion occurs in regions with electron fraction of \\approx 1/3 near the\ncore of an 8.8 M_sun electron-capture supernova. Compared to the case without\nsuch conversion, the neutron-richness of the ejected material is enhanced to\nallow production of elements from Sr, Y, and Zr up to Cd in broad agreement\nwith observations of the metal-poor star HD 122563. Active-sterile flavor\nconversion also strongly suppresses neutrino heating at times when it is\nimportant for the revival of the shock. Our results suggest that simulations of\nsupernova explosion and nucleosynthesis may be used to constrain active-sterile\nmixing parameters in combination with neutrino experiments and cosmological\nconsiderations. \n\n"}
{"id": "1305.4283", "contents": "Title: Statistical modelling of summary values leads to accurate Approximate\n  Bayesian Computations Abstract: Approximate Bayesian Computation (ABC) methods rely on asymptotic arguments,\nimplying that parameter inference can be systematically biased even when\nsufficient statistics are available. We propose to construct the ABC\naccept/reject step from decision theoretic arguments on a suitable auxiliary\nspace. This framework, referred to as ABC*, fully specifies which test\nstatistics to use, how to combine them, how to set the tolerances and how long\nto simulate in order to obtain accuracy properties on the auxiliary space. Akin\nto maximum-likelihood indirect inference, regularity conditions establish when\nthe ABC* approximation to the posterior density is accurate on the original\nparameter space in terms of the Kullback-Leibler divergence and the maximum a\nposteriori point estimate. Fundamentally, escaping asymptotic arguments\nrequires knowledge of the distribution of test statistics, which we obtain\nthrough modelling the distribution of summary values, data points on a summary\nlevel. Synthetic examples and an application to time series data of influenza A\n(H3N2) infections in the Netherlands illustrate ABC* in action. \n\n"}
{"id": "1305.5363", "contents": "Title: Narrowing the gap on heritability of common disease by direct estimation\n  in case-control GWAS Abstract: One of the major developments in recent years in the search for missing\nheritability of human phenotypes is the adoption of linear mixed-effects models\n(LMMs) to estimate heritability due to genetic variants which are not\nsignificantly associated with the phenotype. A variant of the LMM approach has\nbeen adapted to case-control studies and applied to many major diseases by Lee\net al. (2011), successfully accounting for a considerable portion of the\nmissing heritability. For example, for Crohn's disease their estimated\nheritability was 22% compared to 50-60% from family studies. In this letter we\npropose to estimate heritability of disease directly by regression of phenotype\nsimilarities on genotype correlations, corrected to account for ascertainment.\nWe refer to this method as genetic correlation regression (GCR). Using GCR we\nestimate the heritability of Crohn's disease at 34% using the same data. We\ndemonstrate through extensive simulation that our method yields unbiased\nheritability estimates, which are consistently higher than LMM estimates.\nMoreover, we develop a heuristic correction to LMM estimates, which can be\napplied to published LMM results. Applying our heuristic correction increases\nthe estimated heritability of multiple sclerosis from 30% to 52.6%. \n\n"}
{"id": "1306.3380", "contents": "Title: Constraints on Electroweak Effective Operators at One Loop Abstract: We derive bounds on nine dimension-six operators involving electroweak gauge\nbosons and the Higgs boson from precision electroweak data. Four of these\noperators contribute at tree level, and five contribute only at one loop. Using\nthe full power of effective field theory, we show that the bounds on the five\nloop-level operators are much weaker than previously claimed, and thus much\nweaker than bounds from tree-level processes at high-energy colliders. \n\n"}
{"id": "1306.4468", "contents": "Title: Fits to SO(10) Grand Unified Models Abstract: We perform numerical fits of Grand Unified Models based on SO(10), using\nvarious combinations of 10-, 120- and 126-dimensional Higgs representations.\nBoth the supersymmetric and non-supersymmetric versions are fitted, as well as\nboth possible neutrino mass orderings. In contrast to most previous works, we\nperform the fits at the weak scale, i.e. we use RG evolution from the GUT\nscale, at which the GUT-relations between the various Yukawa coupling matrices\nhold, down to the weak scale. In addition, the right-handed neutrinos of the\nseesaw mechanism are integrated out one by one in the RG running. Other new\nfeatures are the inclusion of recent results on the reactor neutrino mixing\nangle and the Higgs mass (in the non-SUSY case). As expected from vacuum\nstability considerations, the low Higgs mass and the large top-quark Yukawa\ncoupling cause some pressure on the fits. A lower top-quark mass, as sometimes\nargued to be the result of a more consistent extraction from experimental\nresults, can relieve this pressure and improve the fits. We give predictions\nfor neutrino masses, including the effective one for neutrinoless double beta\ndecay, as well as the atmospheric neutrino mixing angle and the leptonic CP\nphase for neutrino oscillations. \n\n"}
{"id": "1306.5289", "contents": "Title: Analytic Solutions for D-optimal Factorial Designs under Generalized\n  Linear Models Abstract: We develop two analytic approaches to solve D-optimal approximate designs\nunder generalized linear models. The first approach provides analytic D-optimal\nallocations for generalized linear models with two factors, which include as a\nspecial case the $2^2$ main-effects model considered by Yang, Mandal and\nMajumdar (2012). The second approach leads to explicit solutions for a class of\ngeneralized linear models with more than two factors. With the aid of the\nanalytic solutions, we provide a necessary and sufficient condition under which\na D-optimal design with two quantitative factors could be constructed on the\nboundary points only. It bridges the gap between D-optimal factorial designs\nand D-optimal designs with continuous factors. \n\n"}
{"id": "1306.5341", "contents": "Title: Schematic baryon models, their tight binding description and their\n  microwave realization Abstract: A schematic model for baryon excitations is presented in terms of a symmetric\nDirac gyroscope, a relativistic model solvable in closed form, that reduces to\na rotor in the non-relativistic limit. The model is then mapped on a nearest\nneighbour tight binding model. In its simplest one-dimensional form this model\nyields a finite equidistant spectrum. This is experimentally implemented as a\nchain of dielectric resonators under conditions where their coupling is\nevanescent and good agreement with the prediction is achieved. \n\n"}
{"id": "1306.5652", "contents": "Title: Production of Z' and W' via Drell-Yan processes in the 4D Composite\n  Higgs Model at the LHC Abstract: We present an analysis of both the Neutral Current (NC) and Charged Current\n(CC) Drell-Yan processes at the LHC within a 4 Dimensional realization of a\nComposite Higgs model studying the cross sections and taking into account the\npossible impact of the extra fermions present in the spectrum. \n\n"}
{"id": "1306.5718", "contents": "Title: Fast Covariance Estimation for High-dimensional Functional Data Abstract: For smoothing covariance functions, we propose two fast algorithms that scale\nlinearly with the number of observations per function. Most available methods\nand software cannot smooth covariance matrices of dimension $J \\times J$ with\n$J>500$; the recently introduced sandwich smoother is an exception, but it is\nnot adapted to smooth covariance matrices of large dimensions such as $J \\ge\n10,000$. Covariance matrices of order $J=10,000$, and even $J=100,000$, are\nbecoming increasingly common, e.g., in 2- and 3-dimensional medical imaging and\nhigh-density wearable sensor data. We introduce two new algorithms that can\nhandle very large covariance matrices: 1) FACE: a fast implementation of the\nsandwich smoother and 2) SVDS: a two-step procedure that first applies singular\nvalue decomposition to the data matrix and then smoothes the eigenvectors.\nCompared to existing techniques, these new algorithms are at least an order of\nmagnitude faster in high dimensions and drastically reduce memory requirements.\nThe new algorithms provide instantaneous (few seconds) smoothing for matrices\nof dimension $J=10,000$ and very fast ($<$ 10 minutes) smoothing for\n$J=100,000$. Although SVDS is simpler than FACE, we provide ready to use,\nscalable R software for FACE. When incorporated into R package {\\it refund},\nFACE improves the speed of penalized functional regression by an order of\nmagnitude, even for data of normal size ($J <500$). We recommend that FACE be\nused in practice for the analysis of noisy and high-dimensional functional\ndata. \n\n"}
{"id": "1306.6430", "contents": "Title: A General Framework for Updating Belief Distributions Abstract: We propose a framework for general Bayesian inference. We argue that a valid\nupdate of a prior belief distribution to a posterior can be made for parameters\nwhich are connected to observations through a loss function rather than the\ntraditional likelihood function, which is recovered under the special case of\nusing self information loss. Modern application areas make it is increasingly\nchallenging for Bayesians to attempt to model the true data generating\nmechanism. Moreover, when the object of interest is low dimensional, such as a\nmean or median, it is cumbersome to have to achieve this via a complete model\nfor the whole data distribution. More importantly, there are settings where the\nparameter of interest does not directly index a family of density functions and\nthus the Bayesian approach to learning about such parameters is currently\nregarded as problematic. Our proposed framework uses loss-functions to connect\ninformation in the data to functionals of interest. The updating of beliefs\nthen follows from a decision theoretic approach involving cumulative loss\nfunctions. Importantly, the procedure coincides with Bayesian updating when a\ntrue likelihood is known, yet provides coherent subjective inference in much\nmore general settings. Connections to other inference frameworks are\nhighlighted. \n\n"}
{"id": "1307.4082", "contents": "Title: Wino Dark Matter Under Siege Abstract: A fermion triplet of SU(2)_L - a wino - is a well-motivated dark matter\ncandidate. This work shows that present-day wino annihilations are constrained\nby indirect detection experiments, with the strongest limits coming from\nH.E.S.S. and Fermi. The bounds on wino dark matter are presented as a function\nof mass for two scenarios: thermal (winos constitute a subdominant component of\nthe dark matter for masses less than 3.1 TeV) and non-thermal (winos comprise\nall the dark matter). Assuming the NFW halo model, the H.E.S.S. search for\ngamma-ray lines excludes the 3.1 TeV thermal wino; the combined H.E.S.S. and\nFermi results completely exclude the non-thermal scenario. Uncertainties in the\nexclusions are explored. Indirect detection may provide the only probe for\nmodels of anomaly plus gravity mediation where the wino is the lightest\nsuperpartner and scalars reside at the 100 TeV scale. \n\n"}
{"id": "1307.5683", "contents": "Title: Understanding the B->K*mu+mu- Anomaly Abstract: We present a global analysis of the B->K*(->K pi)mu+mu- decay using the\nrecent LHCb measurements of the primary observables P_{1,2} and P'_{4,5,6,8}.\nSome of them exhibit large deviations with respect to the SM predictions. We\nexplain the observed pattern of deviations through a large New Physics\ncontribution to the Wilson coefficient of the semileptonic operator O9. This\ncontribution has an opposite sign to the SM one, i.e., reduces the size of this\ncoefficient significantly. A good description of data is achieved by allowing\nfor New Physics contributions to the Wilson coefficients C7 and C9 only. We\nfind a 4.5 sigma deviation with respect to the SM prediction, combining the\nlarge-recoil B->K*(->K pi)mu+mu- observables with other radiative processes.\nOnce low-recoil observables are included the significance gets reduced to 3.9\nsigma. We have tested different sources of systematics, none of them modifying\nour conclusions significantly. Finally, we propose additional ways of measuring\nthe primary observables through new foldings. \n\n"}
{"id": "1307.7682", "contents": "Title: Probability-Matching Predictors for Extreme Extremes Abstract: A location- and scale-invariant predictor is constructed which exhibits good\nprobability matching for extreme predictions outside the span of data drawn\nfrom a variety of (stationary) general distributions. It is constructed via the\nthree-parameter {\\mu, \\sigma, \\xi} Generalized Pareto Distribution (GPD). The\npredictor is designed to provide matching probability exactly for the GPD in\nboth the extreme heavy-tailed limit and the extreme bounded-tail limit, whilst\ngiving a good approximation to probability matching at all intermediate values\nof the tail parameter \\xi. The predictor is valid even for small sample sizes\nN, even as small as N = 3.\n  The main purpose of this paper is to present the somewhat lengthy derivations\nwhich draw heavily on the theory of hypergeometric functions, particularly the\nLauricella functions. Whilst the construction is inspired by the Bayesian\napproach to the prediction problem, it considers the case of vague prior\ninformation about both parameters and model, and all derivations are undertaken\nusing sampling theory. \n\n"}
{"id": "1308.0641", "contents": "Title: United Statistical Algorithm, Small and Big Data: Future OF Statistician Abstract: This article provides the role of big idea statisticians in future of Big\nData Science. We describe the `United Statistical Algorithms' framework for\ncomprehensive unification of traditional and novel statistical methods for\nmodeling Small Data and Big Data, especially mixed data (discrete, continuous). \n\n"}
{"id": "1308.1333", "contents": "Title: A comparison of the Higgs sectors of the CMSSM and NMSSM for a 126 GeV\n  Higgs boson Abstract: The recent discovery of a Higgs-like boson at the LHC with a mass of 126 GeV\nhas revived the interest in supersymmetric models, which predicted a Higgs\nboson mass below 130 GeV long before its discovery. We compare systematically\nthe allowed parameter space in the constrained Minimal Supersymmetric Standard\nModel (CMSSM) and the Next-to-Minimal Supersymmetric Model (NMSSM) by\nminimizing the chi^2 function with respect to all known constraints from\naccelerators and cosmology using GUT scale parameters. For the CMSSM the Higgs\nboson mass at tree level is below the Z^0 boson mass and large radiative\ncorrections are needed to obtain a Higgs boson mass of 126 GeV, which requires\nstop squark masses in the multi-TeV range. In contrast, for the NMSSM light\nstop quarks are allowed, since in the NMSSM at tree level the Higgs boson mass\ncan be above the Z^0 boson mass from mixing with the additional singlet Higgs\nboson. Predictions for the scalar boson masses are given in both models with\nemphasis on the unique signatures of the NMSSM, where the heaviest scalar Higgs\nboson decays in the two lighter scalar Higgs bosons with a significant\nbranching ratio, in which case one should observe double Higgs boson production\nat the LHC. Such a signal is strongly suppressed in the CMSSM. In addition,\nsince the LSP is higgsino-like, Higgs boson decays into LSPs can be\nappreciable, thus leading to invisible Higgs decays. \n\n"}
{"id": "1308.2052", "contents": "Title: Signatures Of Scalar Photon Interaction In Astrophysical Situations Abstract: Dimension-5 photon ($\\gamma$) scalar ($\\phi$) interaction terms usually\nappear in the Lagrangian of bosonic sector of unified theories of\nelectromagnetism and gravity. This interaction makes the medium dichoric and\ninduces optical activity. We have modelled the propagation of photons with this\ninteraction in the environment of cold a magnetized compact star (White Dwarf\n(WD) or Neutron Star (NS)), assuming synchro-curvature process as the dominant\nmechanism of emission. We have tried to outline the polarimetric implications\nof photon-scalar coupling on the produced spectrum of these emissions in the\nstellar atmosphere. Further more assuming the 'emission-energy vs\nemission-altitude' relation, that is believed to hold in such (i.e. cold\nmagnetized WD or NS) environments, we have tried to point out the possible\nmodifications to the radiation spectrum when the same is incorporated along\nwith dim-5 photon scalar mixing operator. \n\n"}
{"id": "1308.3903", "contents": "Title: Sensitivity of an Upgraded LHC to R-Parity Violating Signatures of the\n  MSSM Abstract: We present a sensitivity study for the pair-production of supersymmetric\nparticles which decay through R-parity violating channels. As the scope of\npossible RPV signatures is very broad, the reach of several selected signatures\nspanning a representative variety of possible final states is considered.\nPreference in representation is given to spectra motivated by naturalness, i.e.\nlight higgsinos, stops and gluinos. The sensitivity studies are presented for\nproton-proton collisions at 14 TeV with an integrated luminosity of 300 and\n3000 fb^-1, as well as at 33 TeV with an integrated luminosity of 3000 fb^-1. \n\n"}
{"id": "1308.5390", "contents": "Title: The restricted consistency property of leave-$n_v$-out cross-validation\n  for high-dimensional variable selection Abstract: Cross-validation (CV) methods are popular for selecting the tuning parameter\nin the high-dimensional variable selection problem. We show the mis-alignment\nof the CV is one possible reason of its over-selection behavior. To fix this\nissue, we propose a version of leave-$n_v$-out cross-validation (CV($n_v$)),\nfor selecting the optimal model among the restricted candidate model set for\nhigh-dimensional generalized linear models. By using the same candidate model\nsequence and a proper order of construction sample size $n_c$ in each CV split,\nCV($n_v$) avoids the potential hurdles in developing theoretical properties.\nCV($n_v$) is shown to enjoy the restricted model selection consistency property\nunder mild conditions. Extensive simulations and real data analysis support the\ntheoretical results and demonstrate the performances of CV($n_v$) in terms of\nboth model selection and prediction. \n\n"}
{"id": "1309.2097", "contents": "Title: The Technicolor Higgs in the Light of LHC Data Abstract: We consider scenarios in which the 125 GeV resonance observed at the Large\nHadron Collider is a Technicolor (TC) isosinglet scalar, the TC Higgs. By\ncomparison with quantum chromodynamics, we argue that the couplings of the TC\nHiggs to the massive weak bosons are very close to the Standard Model (SM)\nvalues. The couplings to photons and gluons are model-dependent, but close to\nthe SM values in several TC theories. The couplings of the TC Higgs to SM\nfermions are due to interactions beyond TC, such as Extended Technicolor: if\nsuch interactions successfully generate mass for the SM fermions, we argue that\nthe couplings of the latter to the TC Higgs are also SM-like.\n  We suggest a generic parameterization of the TC Higgs interactions with SM\nparticles that accommodates a large class of TC models, and we perform a fit of\nthese parameters to the Higgs LHC data. The fit reveals regions of parameter\nspace where the form factors are of order unity and consistent with data at the\n95% CL, in agreement with expectations in TC theories. This indicates that the\ndiscovered Higgs boson is consistent with the TC Higgs hypothesis for several\nTC theories. \n\n"}
{"id": "1309.3069", "contents": "Title: Longevity Problem of Sterile Neutrino Dark Matter Abstract: Sterile neutrino dark matter of mass O(1-10) keV decays into an active\nneutrino and an X-ray photon, and the non-observation of the corresponding\nX-ray line requires the sterile neutrino to be more long-lived than estimated\nbased on the seesaw formula : the longevity problem. We show that, if one or\nmore of the B-L Higgs fields are charged under a flavor symmetry (or discrete R\nsymmetry), the split mass spectrum for the right-handed neutrinos as well as\nthe required longevity is naturally realized. We provide several examples in\nwhich the predicted the X-ray flux is just below the current bound. \n\n"}
{"id": "1309.5923", "contents": "Title: Asymptotically Normal and Efficient Estimation of Covariate-Adjusted\n  Gaussian Graphical Model Abstract: A tuning-free procedure is proposed to estimate the covariate-adjusted\nGaussian graphical model. For each finite subgraph, this estimator is\nasymptotically normal and efficient. As a consequence, a confidence interval\ncan be obtained for each edge. The procedure enjoys easy implementation and\nefficient computation through parallel estimation on subgraphs or edges. We\nfurther apply the asymptotic normality result to perform support recovery\nthrough edge-wise adaptive thresholding. This support recovery procedure is\ncalled ANTAC, standing for Asymptotically Normal estimation with Thresholding\nafter Adjusting Covariates. ANTAC outperforms other methodologies in the\nliterature in a range of simulation studies. We apply ANTAC to identify\ngene-gene interactions using an eQTL dataset. Our result achieves better\ninterpretability and accuracy in comparison with CAMPE. \n\n"}
{"id": "1309.6473", "contents": "Title: On nonnegative unbiased estimators Abstract: We study the existence of algorithms generating almost surely nonnegative\nunbiased estimators. We show that given a nonconstant real-valued function $f$\nand a sequence of unbiased estimators of $\\lambda\\in\\mathbb{R}$, there is no\nalgorithm yielding almost surely nonnegative unbiased estimators of\n$f(\\lambda)\\in\\mathbb{R}^+$. The study is motivated by pseudo-marginal Monte\nCarlo algorithms that rely on such nonnegative unbiased estimators. These\nmethods allow \"exact inference\" in intractable models, in the sense that\nintegrals with respect to a target distribution can be estimated without any\nsystematic error, even though the associated probability density function\ncannot be evaluated pointwise. We discuss the consequences of our results on\nthe applicability of pseudo-marginal algorithms and thus on the possibility of\nexact inference in intractable models. We illustrate our study with particular\nchoices of functions $f$ corresponding to known challenges in statistics, such\nas exact simulation of diffusions, inference in large datasets and doubly\nintractable distributions. \n\n"}
{"id": "1309.6895", "contents": "Title: Consistency, breakdown robustness, and algorithms for robust improper\n  maximum likelihood clustering Abstract: The robust improper maximum likelihood estimator (RIMLE) is a new method for\nrobust multivariate clustering finding approximately Gaussian clusters. It\nmaximizes a pseudo-likelihood defined by adding a component with improper\nconstant density for accommodating outliers to a Gaussian mixture. A special\ncase of the RIMLE is MLE for multivariate finite Gaussian mixture models. In\nthis paper we treat existence, consistency, and breakdown theory for the RIMLE\ncomprehensively. RIMLE's existence is proved under non-smooth covariance matrix\nconstraints. It is shown that these can be implemented via a computationally\nfeasible Expectation-Conditional Maximization algorithm. \n\n"}
{"id": "1310.0595", "contents": "Title: MCMC for Normalized Random Measure Mixture Models Abstract: This paper concerns the use of Markov chain Monte Carlo methods for posterior\nsampling in Bayesian nonparametric mixture models with normalized random\nmeasure priors. Making use of some recent posterior characterizations for the\nclass of normalized random measures, we propose novel Markov chain Monte Carlo\nmethods of both marginal type and conditional type. The proposed marginal\nsamplers are generalizations of Neal's well-regarded Algorithm 8 for Dirichlet\nprocess mixture models, whereas the conditional sampler is a variation of those\nrecently introduced in the literature. For both the marginal and conditional\nmethods, we consider as a running example a mixture model with an underlying\nnormalized generalized Gamma process prior, and describe comparative simulation\nresults demonstrating the efficacies of the proposed methods. \n\n"}
{"id": "1310.1533", "contents": "Title: CAM: Causal additive models, high-dimensional order search and penalized\n  regression Abstract: We develop estimation for potentially high-dimensional additive structural\nequation models. A key component of our approach is to decouple order search\namong the variables from feature or edge selection in a directed acyclic graph\nencoding the causal structure. We show that the former can be done with\nnonregularized (restricted) maximum likelihood estimation while the latter can\nbe efficiently addressed using sparse regression techniques. Thus, we\nsubstantially simplify the problem of structure search and estimation for an\nimportant class of causal models. We establish consistency of the (restricted)\nmaximum likelihood estimator for low- and high-dimensional scenarios, and we\nalso allow for misspecification of the error distribution. Furthermore, we\ndevelop an efficient computational algorithm which can deal with many\nvariables, and the new method's accuracy and performance is illustrated on\nsimulated and real data. \n\n"}
{"id": "1310.2926", "contents": "Title: Partial Distance Correlation with Methods for Dissimilarities Abstract: Distance covariance and distance correlation are scalar coefficients that\ncharacterize independence of random vectors in arbitrary dimension. Properties,\nextensions, and applications of distance correlation have been discussed in the\nrecent literature, but the problem of defining the partial distance correlation\nhas remained an open question of considerable interest. The problem of partial\ndistance correlation is more complex than partial correlation partly because\nthe squared distance covariance is not an inner product in the usual linear\nspace. For the definition of partial distance correlation we introduce a new\nHilbert space where the squared distance covariance is the inner product. We\ndefine the partial distance correlation statistics with the help of this\nHilbert space, and develop and implement a test for zero partial distance\ncorrelation. Our intermediate results provide an unbiased estimator of squared\ndistance covariance, and a neat solution to the problem of distance correlation\nfor dissimilarities rather than distances. \n\n"}
{"id": "1310.2989", "contents": "Title: Constraining gamma-ray propagation on cosmic distances Abstract: Studying the propagation of gamma rays on cosmological distances encompasses\na variety of scientific fields, focusing on diffuse radiation fields such as\nthe extragalactic background light, on the probe of the magnetism of the\nUniverse on large scales, and on physics beyond the standard models of\ncosmology and particle physics. The measurements, constraints and hints from\nobservations of gamma-ray blazars by airborne and ground-based instruments are\nbriefly reviewed. These observations point to gamma-ray cosmology as one of the\nmajor science cases of the Cherenkov Telescope Array, CTA. \n\n"}
{"id": "1310.6334", "contents": "Title: The Black Hole Interior in AdS/CFT and the Information Paradox Abstract: We show that, within the AdS/CFT correspondence, recent formulations of the\ninformation paradox can be reduced to a question about the existence of certain\nkinds of operators in the CFT. We describe a remarkably simple construction of\nthese operators on a given state of the CFT. Our construction leads to a smooth\nhorizon, addresses the strong subadditivity paradox, while preserving locality\nwithin effective field theory, and reconciles the existence of the interior\nwith the growth of states with energy in the CFT. We also extend our\nconstruction to non-equilibrium states. \n\n"}
{"id": "1311.3350", "contents": "Title: Sequential Tests of Multiple Hypotheses Controlling False Discovery and\n  Nondiscovery Rates Abstract: We propose a general and flexible procedure for testing multiple hypotheses\nabout sequential (or streaming) data that simultaneously controls both the\nfalse discovery rate (FDR) and false nondiscovery rate (FNR) under minimal\nassumptions about the data streams which may differ in distribution, dimension,\nand be dependent. All that is needed is a test statistic for each data stream\nthat controls the conventional type I and II error probabilities, and no\ninformation or assumptions are required about the joint distribution of the\nstatistics or data streams. The procedure can be used with sequential, group\nsequential, truncated, or other sampling schemes. The procedure is a natural\nextension of Benjamini and Hochberg's (1995) widely-used fixed sample size\nprocedure to the domain of sequential data, with the added benefit of\nsimultaneous FDR and FNR control that sequential sampling affords. We prove the\nprocedure's error control and give some tips for implementation in commonly\nencountered testing situations. \n\n"}
{"id": "1311.7455", "contents": "Title: Semi-Penalized Inference with Direct False Discovery Rate Control in\n  High-Dimensions Abstract: We propose a new method, semi-penalized inference with direct false discovery\nrate control (SPIDR), for variable selection and confidence interval\nconstruction in high-dimensional linear regression. SPIDR first uses a\nsemi-penalized approach to constructing estimators of the regression\ncoefficients. We show that the SPIDR estimator is ideal in the sense that it\nequals an ideal least squares estimator with high probability under a sparsity\nand other suitable conditions. Consequently, the SPIDR estimator is\nasymptotically normal. Based on this distributional result, SPIDR determines\nthe selection rule by directly controlling false discovery rate. This provides\nan explicit assessment of the selection error. This also naturally leads to\nconfidence intervals for the selected coefficients with a proper confidence\nstatement. We conduct simulation studies to evaluate its finite sample\nperformance and demonstrate its application on a breast cancer gene expression\ndata set. Our simulation studies and data example suggest that SPIDR is a\nuseful method for high-dimensional statistical inference in practice. \n\n"}
{"id": "1312.3516", "contents": "Title: Density Estimation in Infinite Dimensional Exponential Families Abstract: In this paper, we consider an infinite dimensional exponential family,\n$\\mathcal{P}$ of probability densities, which are parametrized by functions in\na reproducing kernel Hilbert space, $H$ and show it to be quite rich in the\nsense that a broad class of densities on $\\mathbb{R}^d$ can be approximated\narbitrarily well in Kullback-Leibler (KL) divergence by elements in\n$\\mathcal{P}$. The main goal of the paper is to estimate an unknown density,\n$p_0$ through an element in $\\mathcal{P}$. Standard techniques like maximum\nlikelihood estimation (MLE) or pseudo MLE (based on the method of sieves),\nwhich are based on minimizing the KL divergence between $p_0$ and\n$\\mathcal{P}$, do not yield practically useful estimators because of their\ninability to efficiently handle the log-partition function. Instead, we propose\nan estimator, $\\hat{p}_n$ based on minimizing the \\emph{Fisher divergence},\n$J(p_0\\Vert p)$ between $p_0$ and $p\\in \\mathcal{P}$, which involves solving a\nsimple finite-dimensional linear system. When $p_0\\in\\mathcal{P}$, we show that\nthe proposed estimator is consistent, and provide a convergence rate of\n$n^{-\\min\\left\\{\\frac{2}{3},\\frac{2\\beta+1}{2\\beta+2}\\right\\}}$ in Fisher\ndivergence under the smoothness assumption that $\\log\np_0\\in\\mathcal{R}(C^\\beta)$ for some $\\beta\\ge 0$, where $C$ is a certain\nHilbert-Schmidt operator on $H$ and $\\mathcal{R}(C^\\beta)$ denotes the image of\n$C^\\beta$. We also investigate the misspecified case of $p_0\\notin\\mathcal{P}$\nand show that $J(p_0\\Vert\\hat{p}_n)\\rightarrow \\inf_{p\\in\\mathcal{P}}J(p_0\\Vert\np)$ as $n\\rightarrow\\infty$, and provide a rate for this convergence under a\nsimilar smoothness condition as above. Through numerical simulations we\ndemonstrate that the proposed estimator outperforms the non-parametric kernel\ndensity estimator, and that the advantage with the proposed estimator grows as\n$d$ increases. \n\n"}
{"id": "1312.4924", "contents": "Title: A framework for dynamical generation of flavor mixing Abstract: We present a dynamical mechanism \\`a la Nambu--Jona-Lasinio for the\ngeneration of masses and mixing for two interacting fermion fields. The\nanalysis is carried out in the framework introduced long ago by Umezawa et al.,\nin which mass generation is achieved via inequivalent representations, and that\nwe generalize to the case of two generations. The method allows a clear\nidentification of the vacuum structure for each physical phase, confirming\nprevious results about the distinct physical nature of the vacuum for fields\nwith definite mass and fields with definite flavor. Implications for the\nleptonic sector of the Standard Model are briefly discussed. \n\n"}
{"id": "1401.0201", "contents": "Title: Sparse Recovery with Very Sparse Compressed Counting Abstract: Compressed sensing (sparse signal recovery) often encounters nonnegative data\n(e.g., images). Recently we developed the methodology of using (dense)\nCompressed Counting for recovering nonnegative K-sparse signals. In this paper,\nwe adopt very sparse Compressed Counting for nonnegative signal recovery. Our\ndesign matrix is sampled from a maximally-skewed p-stable distribution (0<p<1),\nand we sparsify the design matrix so that on average (1-g)-fraction of the\nentries become zero. The idea is related to very sparse stable random\nprojections (Li et al 2006 and Li 2007), the prior work for estimating summary\nstatistics of the data.\n  In our theoretical analysis, we show that, when p->0, it suffices to use M=\nK/(1-exp(-gK) log N measurements, so that all coordinates can be recovered in\none scan of the coordinates. If g = 1 (i.e., dense design), then M = K log N.\nIf g= 1/K or 2/K (i.e., very sparse design), then M = 1.58K log N or M = 1.16K\nlog N. This means the design matrix can be indeed very sparse at only a minor\ninflation of the sample complexity.\n  Interestingly, as p->1, the required number of measurements is essentially M\n= 2.7K log N, provided g= 1/K. It turns out that this result is a general\nworst-case bound. \n\n"}
{"id": "1401.8274", "contents": "Title: Empirical Bayes unfolding of elementary particle spectra at the Large\n  Hadron Collider Abstract: We consider the so-called unfolding problem in experimental high energy\nphysics, where the goal is to estimate the true spectrum of elementary\nparticles given observations distorted by measurement error due to the limited\nresolution of a particle detector. This an important statistical inverse\nproblem arising in the analysis of data at the Large Hadron Collider at CERN.\nMathematically, the problem is formalized as one of estimating the intensity\nfunction of an indirectly observed Poisson point process. Particle physicists\nare particularly keen on unfolding methods that feature a principled way of\nchoosing the regularization strength and allow for the quantification of the\nuncertainty inherent in the solution. Though there are many approaches that\nhave been considered by experimental physicists, it can be argued that few --\nif any -- of these deal with these two key issues in a satisfactory manner. In\nthis paper, we propose to attack the unfolding problem within the framework of\nempirical Bayes estimation: we consider Bayes estimators of the coefficients of\na basis expansion of the unknown intensity, using a regularizing prior; and\nemploy a Monte Carlo expectation-maximization algorithm to find the marginal\nmaximum likelihood estimate of the hyperparameter controlling the strength of\nthe regularization. Due to the data-driven choice of the hyperparameter,\ncredible intervals derived using the empirical Bayes posterior lose their\nsubjective Bayesian interpretation. Since the properties and meaning of such\nintervals are poorly understood, we explore instead the use of bootstrap\nresampling for constructing purely frequentist confidence bands for the true\nintensity. The performance of the proposed methodology is demonstrated using\nboth simulations and real data from the Large Hadron Collider. \n\n"}
{"id": "1402.6287", "contents": "Title: Benchmarks for Dark Matter Searches at the LHC Abstract: We propose some scenarios to pursue dark matter searches at the LHC in a\nfairly model-independent way. The first benchmark case is dark matter\nco-annihilations with coloured particles (gluinos or squarks being special\nexamples). We determine the masses that lead to the correct thermal relic\ndensity including, for the first time, strong Sommerfeld corrections taking\ninto account colour decomposition. In the second benchmark case we consider\ndark matter that couples to SM particles via the Z or the Higgs. We determine\nthe couplings allowed by present experiments and discuss future prospects.\nFinally we present the case of dark matter that freezes out via decays and\napply our results to invisible Z and Higgs decays. \n\n"}
{"id": "1403.1345", "contents": "Title: Minimax Optimal Bayesian Aggregation Abstract: It is generally believed that ensemble approaches, which combine multiple\nalgorithms or models, can outperform any single algorithm at machine learning\ntasks, such as prediction. In this paper, we propose Bayesian convex and linear\naggregation approaches motivated by regression applications. We show that the\nproposed approach is minimax optimal when the true data-generating model is a\nconvex or linear combination of models in the list. Moreover, the method can\nadapt to sparsity structure in which certain models should receive zero\nweights, and the method is tuning parameter free unlike competitors. More\ngenerally, under an M-open view when the truth falls outside the space of all\nconvex/linear combinations, our theory suggests that the posterior measure\ntends to concentrate on the best approximation of the truth at the minimax\nrate. We illustrate the method through simulation studies and several\napplications. \n\n"}
{"id": "1403.1371", "contents": "Title: Open bottom states and the anti-B meson propagation in hadronic matter Abstract: The interaction and propagation of anti-B mesons with light mesons, N and\nDelta baryons is studied within a unitarized approach based on effective models\nthat are compatible with chiral and heavy-quark symmetries. We find several\nheavy-quark spin doublets in the open-bottom sectors, where anti-B and anti-B*\nmesons are present. In the meson sector we find several resonant states, among\nthem, a B0 and a B1 with masses 5530 MeV and 5579 MeV as well as Bs0* and Bs1*\nnarrow states at 5748 MeV and 5799 MeV, respectively. They form two doublets\nwith no experimental identification yet, the first one being the bottom\ncounterpart of the D0(2400) and D1(2430) states, and the second bottom doublet\nassociated to the ubiquitous Ds0* (2317) and the Ds1 (2460). In the baryon\nsector, several Lambda_b and Sigma_b doublets are identified, among them the\none given by the experimental Lambda_b(5910) and Lambda*_b(5921). Moreover, one\nof our states, the Sigma_b*(5904), turns out to be the bottom counterpart of\nthe Sigma*(1670) and Sigma_c*(2549), which is a case for discovery. We finally\nanalyze different transport coefficients for the anti-B meson in hot matter,\nsuch as formed in heavy-ion collisions at RHIC and LHC. For RHIC/LHC energies,\nthe main contribution to the coefficients comes from the interaction of anti-B\nmesons with pions. However, we also include the effects of baryonic density\nwhich might be sizable at temperatures T < 100 MeV, as the chemical potential\nis expected to increase in the last stages of the expansion. We conclude that\nalthough the relaxation time decreases with larger baryonic densities, the\nanti-B meson does not thermalize at RHIC/LHC energies, representing an ideal\nprobe for the initial bottom distribution. \n\n"}
{"id": "1403.3500", "contents": "Title: R-vine Models for Spatial Time Series with an Application to Daily Mean\n  Temperature Abstract: We introduce an extension of R-vine copula models for the purpose of spatial\ndependency modeling and model based prediction at unobserved locations. The\nnewly derived spatial R-vine model combines the flexibility of vine copulas\nwith the classical geostatistical idea of modeling spatial dependencies by\nmeans of the distances between the variable locations. In particular the model\nis able to capture non-Gaussian spatial dependencies. For the purpose of model\ndevelopment and as an illustration we consider daily mean temperature data\nobserved at 54 monitoring stations in Germany. We identify a relationship\nbetween the vine copula parameters and the station distances and exploit it in\norder to reduce the huge number of parameters needed to parametrize a\n54-dimensional R-vine model needed to fit the data. The new distance based\nmodel parametrization results in a distinct reduction in the number of\nparameters and makes parameter estimation and prediction at unobserved\nlocations feasible. The prediction capabilities are validated using adequate\nscoring techniques, showing a better performance of the spatial R-vine copula\nmodel compared to a Gaussian spatial model. \n\n"}
{"id": "1403.6295", "contents": "Title: Asymptotic Properties of Minimum S-Divergence Estimator for Discrete\n  Models Abstract: Robust inference based on the minimization of statistical divergences has\nproved to be a useful alternative to the classical techniques based on maximum\nlikelihood and related methods. Recently Ghosh et al. (2013) proposed a general\nclass of divergence measures, namely the S-Divergence Family and discussed its\nusefulness in robust parametric estimation through some numerical\nillustrations. In this present paper, we develop the asymptotic properties of\nthe proposed minimum S-Divergence estimators under discrete models. \n\n"}
{"id": "1403.6573", "contents": "Title: Exact Inference for Gaussian Process Regression in case of Big Data with\n  the Cartesian Product Structure Abstract: Approximation algorithms are widely used in many engineering problems. To\nobtain a data set for approximation a factorial design of experiments is often\nused. In such case the size of the data set can be very large. Therefore, one\nof the most popular algorithms for approximation - Gaussian Process regression\n- can be hardly applied due to its computational complexity. In this paper a\nnew approach for Gaussian Process regression in case of factorial design of\nexperiments is proposed. It allows to efficiently compute exact inference and\nhandle large multidimensional data sets. The proposed algorithm provides fast\nand accurate approximation and also handles anisotropic data. \n\n"}
{"id": "1403.6610", "contents": "Title: Unitarity Bounds on Dark Matter Effective Interactions at LHC Abstract: The perturbative unitarity bound is studied in the monojet process at LHC.\nThe production of the dark matter is described by the low-energy effective\ntheory. The analysis of the dark matter signal is not validated, if the\nunitarity condition is violated. It is shown that the current LHC analysis the\neffective theory breaks down, at least, when the dark matter is lighter than\nO(100) GeV. Future prospects for $\\sqrt{s}$ = 14 TeV are also discussed. The\nresult is independent of physics in high energy scales. \n\n"}
{"id": "1403.6786", "contents": "Title: Electroweak Vacuum Stability in light of BICEP2 Abstract: We consider the effect of a period of inflation with a high energy density\nupon the stability of the Higgs potential in the early universe. The recent\nmeasurement of a large tensor-to-scalar ratio, $r_T \\sim 0.16$, by the BICEP-2\nexperiment possibly implies that the energy density during inflation was very\nhigh, comparable with the GUT scale. Given that the standard model Higgs\npotential is known to develop an instability at $\\Lambda \\sim 10^{10}$ GeV this\nmeans that the resulting large quantum fluctuations of the Higgs field could\ndestabilize the vacuum during inflation, even if the Higgs field starts at zero\nexpectation value. We estimate the probability of such a catastrophic\ndestabilisation given such an inflationary scenario and calculate that for a\nHiggs mass of $m_h=125.5$ GeV that the top mass must be less than $m_t\\sim 172$\nGeV. We present two possible cures: a direct coupling between the Higgs and the\ninflaton and a non-zero temperature from dissipation during inflation. \n\n"}
{"id": "1403.7112", "contents": "Title: Present and Future K and B Meson Mixing Constraints on TeV Scale\n  Left-Right Symmetry Abstract: We revisit the $\\Delta F=2$ transitions in the $K$ and $B_{d,s}$ neutral\nmeson systems in the context of the minimal Left-Right symmetric model. We take\ninto account, in addition to up-to-date phenomenological data, the\ncontributions related to the renormalization of the flavor-changing neutral\nHiggs tree-level amplitude. These contributions were neglected in recent\ndiscussions, albeit formally needed in order to obtain a gauge independent\nresult. Their impact on the minimal LR model is crucial and twofold. First, the\neffects are relevant in $B$ meson oscillations, for both CP conserving and CP\nviolating observables, so that for the first time these imply constraints on\nthe LR scenario which compete with those of the $K$ sector (plagued by\nlong-distance uncertainties). Second, they sizably contribute to the indirect\nkaon CP violation parameter $\\varepsilon$. We discuss the bounds from $B$ and\n$K$ mesons in both cases of LR symmetry: generalized parity ($\\mathcal P$) and\ncharge conjugation ($\\mathcal C$). In the case of $\\mathcal P$, the interplay\nbetween the CP-violation parameters $\\varepsilon$ and $\\varepsilon'$ leads us\nto rule out the regime of very hierarchical bidoublet vacuum expectation values\n$v_2/v_1<m_b/m_t\\simeq 0.02$. In general, by minimizing the scalar field\ncontribution up to the limit of the perturbative regime and by definite values\nof the relevant CP phases in the charged right-handed currents, we find that a\nright-handed gauge boson $W_R$ as light as 3 TeV is allowed at the 95% CL. This\nis well within the reach of direct detection at the next LHC run. If not\ndiscovered, within a decade the upgraded LHCb and Super B factories may reach\nan indirect sensitivity to a Left-Right scale of 8 TeV. \n\n"}
{"id": "1403.7589", "contents": "Title: Prior-free probabilistic prediction of future observations Abstract: Prediction of future observations is a fundamental problem in statistics.\nHere we present a general approach based on the recently developed inferential\nmodel (IM) framework. We employ an IM-based technique to marginalize out the\nunknown parameters, yielding prior-free probabilistic prediction of future\nobservables. Verifiable sufficient conditions are given for validity of our IM\nfor prediction, and a variety of examples demonstrate the proposed method's\nperformance. Thanks to its generality and ease of implementation, we expect\nthat our IM-based method for prediction will be a useful tool for\npractitioners. \n\n"}
{"id": "1403.8120", "contents": "Title: Detecting relevant changes in time series models Abstract: Most of the literature on change-point analysis by means of hypothesis\ntesting considers hypotheses of the form H0 : \\theta_1 = \\theta_2 vs. H1 :\n\\theta_1 != \\theta_2, where \\theta_1 and \\theta_2 denote parameters of the\nprocess before and after a change point. This paper takes a different\nperspective and investigates the null hypotheses of no relevant changes, i.e.\nH0 : ||\\theta_1 - \\theta_2|| ? \\leq \\Delta?, where || \\cdot || is an\nappropriate norm. This formulation of the testing problem is motivated by the\nfact that in many applications a modification of the statistical analysis might\nnot be necessary, if the difference between the parameters before and after the\nchange-point is small. A general approach to problems of this type is developed\nwhich is based on the CUSUM principle. For the asymptotic analysis weak\nconvergence of the sequential empirical process has to be established under the\nalternative of non-stationarity, and it is shown that the resulting test\nstatistic is asymptotically normal distributed. Several applications of the\nmethodology are given including tests for relevant changes in the mean,\nvariance, parameter in a linear regression model and distribution function\namong others. The finite sample properties of the new tests are investigated by\nmeans of a simulation study and illustrated by analyzing a data example from\neconomics. \n\n"}
{"id": "1404.0148", "contents": "Title: Results and prospects of dark matter searches with ANTARES Abstract: Dark matter is one of the most important scientific goals for neutrino\ntelescopes. These instruments have particular advantages with respect to other\nexperimental approaches. Compared to direct searches, the sensitivity of\nneutrino telescopes to probe the spin-dependent cross section of WIMP-proton is\nunsurpassed. On the other hand, neutrino telescopes can look for dark matter in\nthe Sun, so a potential signal would be a strong indication of dark matter,\ncontrary to the case of other indirect searches like gammas or cosmic rays,\nwhere more conventional astrophysical interpretations are very hard to rule\nout. We present here the results of a binned search for neutralino annihilation\nin the Sun using data gathered by the ANTARES neutrino telescope during\n2007-2008. These result include limits on the neutrino and muon flux and on the\nspin-dependent and spin-independent cross section of the WIMP-proton\nscattering. \n\n"}
{"id": "1404.2918", "contents": "Title: Approximating Cross-validatory Predictive Evaluation in Bayesian Latent\n  Variables Models with Integrated IS and WAIC Abstract: A natural method for approximating out-of-sample predictive evaluation is\nleave-one-out cross-validation (LOOCV) --- we alternately hold out each case\nfrom a full data set and then train a Bayesian model using Markov chain Monte\nCarlo (MCMC) without the held-out; at last we evaluate the posterior predictive\ndistribution of all cases with their actual observations. However, actual LOOCV\nis time-consuming. This paper introduces two methods, namely iIS and iWAIC, for\napproximating LOOCV with only Markov chain samples simulated from a posterior\nbased on a \\textit{full} data set. iIS and iWAIC aim at improving the\napproximations given by importance sampling (IS) and WAIC in Bayesian models\nwith possibly correlated latent variables. In iIS and iWAIC, we first integrate\nthe predictive density over the distribution of the latent variables associated\nwith the held-out without reference to its observation, then apply IS and WAIC\napproximations to the integrated predictive density. We compare iIS and iWAIC\nwith other approximation methods in three real data examples that respectively\nuse mixture models, models with correlated spatial effects, and a random effect\nlogistic model. Our empirical results show that iIS and iWAIC give\nsubstantially better approximates than non-integrated IS and WAIC and other\nmethods. \n\n"}
{"id": "1404.4414", "contents": "Title: Probit transformation for nonparametric kernel estimation of the copula\n  density Abstract: Copula modelling has become ubiquitous in modern statistics. Here, the\nproblem of nonparametrically estimating a copula density is addressed. Arguably\nthe most popular nonparametric density estimator, the kernel estimator is not\nsuitable for the unit-square-supported copula densities, mainly because it is\nheavily affected by boundary bias issues. In addition, most common copulas\nadmit unbounded densities, and kernel methods are not consistent in that case.\nIn this paper, a kernel-type copula density estimator is proposed. It is based\non the idea of transforming the uniform marginals of the copula density into\nnormal distributions via the probit function, estimating the density in the\ntransformed domain, which can be accomplished without boundary problems, and\nobtaining an estimate of the copula density through back-transformation.\nAlthough natural, a raw application of this procedure was, however, seen not to\nperform very well in the earlier literature. Here, it is shown that, if\ncombined with local likelihood density estimation methods, the idea yields very\ngood and easy to implement estimators, fixing boundary issues in a natural way\nand able to cope with unbounded copula densities. The asymptotic properties of\nthe suggested estimators are derived, and a practical way of selecting the\ncrucially important smoothing parameters is devised. Finally, extensive\nsimulation studies and a real data analysis evidence their excellent\nperformance compared to their main competitors. \n\n"}
{"id": "1404.5609", "contents": "Title: Controlling the false discovery rate via knockoffs Abstract: In many fields of science, we observe a response variable together with a\nlarge number of potential explanatory variables, and would like to be able to\ndiscover which variables are truly associated with the response. At the same\ntime, we need to know that the false discovery rate (FDR) - the expected\nfraction of false discoveries among all discoveries - is not too high, in order\nto assure the scientist that most of the discoveries are indeed true and\nreplicable. This paper introduces the knockoff filter, a new variable selection\nprocedure controlling the FDR in the statistical linear model whenever there\nare at least as many observations as variables. This method achieves exact FDR\ncontrol in finite sample settings no matter the design or covariates, the\nnumber of variables in the model, or the amplitudes of the unknown regression\ncoefficients, and does not require any knowledge of the noise level. As the\nname suggests, the method operates by manufacturing knockoff variables that are\ncheap - their construction does not require any new data - and are designed to\nmimic the correlation structure found within the existing variables, in a way\nthat allows for accurate FDR control, beyond what is possible with\npermutation-based methods. The method of knockoffs is very general and\nflexible, and can work with a broad class of test statistics. We test the\nmethod in combination with statistics from the Lasso for sparse regression, and\nobtain empirical results showing that the resulting method has far more power\nthan existing selection rules when the proportion of null variables is high. \n\n"}
{"id": "1404.7197", "contents": "Title: Bayesian Model Comparison in Genetic Association Analysis: Linear Mixed\n  Modeling and SNP Set Testing Abstract: We consider the problems of hypothesis testing and model comparison under a\nflexible Bayesian linear regression model whose formulation is closely\nconnected with the linear mixed effect model and the parametric models for SNP\nset analysis in genetic association studies. We derive a class of analytic\napproximate Bayes factors and illustrate their connections with a variety of\nfrequentist test statistics, including the Wald statistic and the variance\ncomponent score statistic. Taking advantage of Bayesian model averaging and\nhierarchical modeling, we demonstrate some distinct advantages and\nflexibilities in the approaches utilizing the derived Bayes factors in the\ncontext of genetic association studies. We demonstrate our proposed methods\nusing real or simulated numerical examples in applications of single SNP\nassociation testing, multi-locus fine-mapping and SNP set association testing. \n\n"}
{"id": "1405.2012", "contents": "Title: Wiggly Whipped Inflation Abstract: Motivated by BICEP2 results on the CMB polarization B-mode which imply\nprimordial gravitational waves are produced when the Universe has the expansion\nrate of about $H \\approx 10^{14}$ GeV, and by deviations from a smooth\npower-law behaviour for multipoles $\\ell <50$ in the CMB temperature anisotropy\npower spectrum found in the WMAP and Planck experiments, we have expanded our\nclass of large field inflationary models that fit both the BICEP2 and Planck\nCMB observations consistently. These best-fitted large field models are found\nto have a transition from a faster roll to the slow roll $V(\\phi)=m^2 \\phi^2/2$\ninflation at a field value around 14.6~${\\rm M_{Pl}}$ and thus a potential\nenergy of $V(\\phi) \\sim (10^{16}\\,{\\rm GeV})^4$. In general this transition\nwith sharp features in the inflaton potential produces not only suppression of\nscalars relative to tensor modes at small $k$ but also introduces wiggles in\nthe primordial perturbation spectrum. These wiggles are shown to be useful to\nexplain some localized features in the CMB angular power spectrum and can also\nhave other observational consequences. Thus, primordial GW can be used now to\nmake a tomography of inflation determining its fine structure. The resulting\nWiggly Whipped Inflation scenario is described in details and the anticipated\nperturbation power spectra, CMB power spectra, non-Gaussianity and other\nobservational consequences are calculated and compared to existing and\nforthcoming observations. \n\n"}
{"id": "1405.3607", "contents": "Title: Drell-Yan lepton pair production at NNLO QCD with parton showers Abstract: We present a simple approach to combine NNLO QCD calculations and parton\nshowers, based on the UNLOPS technique. We apply the method to the computation\nof Drell-Yan lepton-pair production at the Large Hadron Collider. We comment on\npossible improvements and intrinsic uncertainties. \n\n"}
{"id": "1405.3920", "contents": "Title: A significance test for forward stepwise model selection Abstract: We apply the methods developed by Lockhart et al. (2013) and Taylor et al.\n(2013) on significance tests for penalized regression to forward stepwise model\nselection. A general framework for selection procedures described by quadratic\ninequalities includes a variant of forward stepwise with grouped variables,\nallowing us to handle categorical variables and factor models. We provide an\nalgorithm to compute a new statistic with an exact null distribution\nconditional on the outcome of the model selection procedure. This new\nstatistic, which we denote $T\\chi$, has a truncated $\\chi$ distribution under\nthe global null. We apply this test in forward stepwise iteratively on the\nresidual after each step. The resulting method has the computational strengths\nof stepwise selection and addresses the problem of invalid test statistics due\nto model selection. We illustrate the flexibility of this method by applying it\nto several specialized applications of forward stepwise including a\nhierarchical interactions model and a recently described additive model that\nadaptively chooses between linear and nonlinear effects for each variable. \n\n"}
{"id": "1405.5495", "contents": "Title: The Higgs Boson is found: What is next? Abstract: The situation in particle physics after the discovery of the Higgs boson is\ndiscussed. Is the Standard Model complete? Are there still mysteries which have\nno answer? Answering these questions we consider the Higgs sector, the neutrino\nsector and the flavor sector of the Standard Model, and list the problems which\nare still far from understanding. Going beyond the Standard Model we consider\nthe Dark matter in the Universe and possible existence of new particles and\ninteractions. The main attention is paid to supersymmetry. The problems faced\nby elementary particle physics in the near perspective are formulated. \n\n"}
{"id": "1405.7091", "contents": "Title: Bayesian hierarchical modelling for inferring genetic interactions in\n  yeast Abstract: Identifying genetic interactions for a given microorganism such as yeast is\ndifficult. Quantitative Fitness Analysis (QFA) is a high-throughput\nexperimental and computational methodology for quantifying the fitness of\nmicrobial cultures. QFA can be used to compare between fitness observations for\ndifferent genotypes and thereby infer genetic interaction strengths. Current\n\"naive\" frequentist statistical approaches used in QFA do not model\nbetween-genotype variation or difference in genotype variation under different\nconditions. In this thesis, a Bayesian approach is introduced to evaluate\nhierarchical models that better reflect the structure or design of QFA\nexperiments. First, a two-stage approach is presented: a hierarchical logistic\nmodel is fitted to microbial culture growth curves and then a hierarchical\ninteraction model is fitted to fitness summaries inferred for each genotype.\nNext, a one-stage Bayesian approach is presented: a joint hierarchical model\nwhich does not require a univariate summary of fitness, used to pass\ninformation between models. The new hierarchical approaches are then compared\nusing a dataset examining the effect of telomere defects on yeast. By better\ndescribing the experimental structure, new evidence is found for genes and\ncomplexes which interact with the telomere cap. Various extensions of these\nmodels, including models for data transformation, batch effects, and\nintrinsically stochastic growth models are also considered. \n\n"}
{"id": "1405.7163", "contents": "Title: Invisible decays of low mass Higgs bosons in supersymmetric models Abstract: The discovery of a 126 GeV Higgs like scalar at the LHC along with the non\nobservation of the supersymmetric particles, has in turn lead to constraining\nvarious supersymmetric models through the Higgs data. We here consider the case\nof both MSSM, as well its extension containing an additional chiral singlet\nsuperfield, NMSSM. We concentrate on the case where we identify the second\nlightest Higgs boson as the 126 GeV state discovered at the CERN LHC and\nconsider the invisible decays of the low mass Higgs bosons in both MSSM and\nNMSSM. We find that in case of the MSSM with universal boundary conditions at\nthe GUT scale, it is not possible to have light neutralinos leading to the\ndecay channel $H\\rightarrow \\tilde{\\chi}_1^0 \\tilde{\\chi}_1^0$. The invisible\ndecay mode is allowed in case of certain $SO(10)$ and $E_6$ grand unified\nmodels with large representations and nonuniversal gaugino masses at the GUT\nscale. In case of the NMSSM, for the parameter space considered it is possible\nto have the invisible decay channel with universal gaugino masses at the GUT\nscale. We furthermore consider the most general case, with $M_1$ and $M_2$ as\nindependent parameters for both MSSM and NMSSM. We isolate the regions in\nparameter space in both cases, where the second lightest Higgs boson has a mass\nof 126 GeV and then concentrate on the invisible decay of Higgs to lighter\nneutralinos. The other non-standard decay mode of the Higgs is also considered\nin detail. The invisible Higgs branching ratio being constrained by the LHC\nresults, we find that in this case with the second lightest Higgs being the 126\nGeV state, more data from the LHC is required to constrain the neutralino\nparameter space, compared to the case when the lightest Higgs boson is the 126\nGeV state. \n\n"}
{"id": "1406.1172", "contents": "Title: Naturalness, b to s gamma, and SUSY Heavy Higgses Abstract: We explore naturalness constraints on the masses of the heavy Higgs bosons\nH^0, H^+/-, and A^0 in supersymmetric theories. We show that, in any extension\nof MSSM which accommodates the 125 GeV Higgs at the tree level, one can derive\nan upper bound on the SUSY Higgs masses from naturalness considerations. As is\nwell-known for the MSSM, these bounds become weak at large tan beta. However,\nwe show that measurements of b to s gamma together with naturalness arguments\nlead to an upper bound on tan beta, strengthening the naturalness case for\nheavy Higgs states near the TeV scale. The precise bound depends somewhat on\nthe SUSY mediation scale: allowing a factor of 10 tuning in the stop sector,\nthe measured rate of b to s gamma implies tan beta < 30 for running down from\n10 TeV but tan beta < 4 for mediation at or above 100 TeV, placing m_A near the\nTeV scale for natural EWSB. Because the signatures of heavy Higgs bosons at\ncolliders are less susceptible to being \"hidden\" than standard superpartner\nsignatures, there is a strong motivation to make heavy Higgs searches a key\npart of the LHC's search for naturalness. In an appendix we comment on how the\nGoldstone boson equivalence theorem links the rates for H to hh and H to ZZ\nsignatures. \n\n"}
{"id": "1406.1441", "contents": "Title: The Nightmare Scenario and the Origin of the Standard Model. \"We Got it\n  Wrong ...How did we misread the signals? ... What to Do?\" Abstract: It is thought that the emergence of the \"nightmare scenario\" at the LHC could\nbe a serious crisis for particle physics that could require radical new\nconcepts and even a major paradigm change. A root cause may have been\nexaggeration of the significance of asymptotic freedom, leading to the\nhistorically profound mistake of formulating new short-distance extensions of\nthe Standard Model while ignoring both serious infra-red problems and central\nelements of long-distance physics. In fact, pursuit of the uniquely unitary\nCritical Pomeron leads to a possible gauge theory origin for the Standard Model\nthat is both radical and paradigm changing, but also explains many mysteries. A\nbound-state S-Matrix embedded in a unique weak coupling massless SU(5) field\ntheory emerges. The states and interactions of the Standard Model are enhanced,\nand the underlying SU(5) unification suppressed, by a wee parton divergence\nphenomenon involving wee gauge bosons coupled to S-Matrix massless fermion\nanomalies. Confinement, chiral symmetry breaking, the parton model, electroweak\nsymmetry breaking, dark matter, and neutrino masses, all appear to be present.\nMost significantly, perhaps, there is a Higgs boson but, as seen experimentally\nat the LHC, there is no new short-distance physics. The only new physics is\nelectroweak-scale QCD interactions due to color sextet quarks. \n\n"}
{"id": "1406.1803", "contents": "Title: Generalized Mode and Ridge Estimation Abstract: The generalized density is a product of a density function and a weight\nfunction. For example, the average local brightness of an astronomical image is\nthe probability of finding a galaxy times the mean brightness of the galaxy. We\npropose a method for studying the geometric structure of generalized densities.\nIn particular, we show how to find the modes and ridges of a generalized\ndensity function using a modification of the mean shift algorithm and its\nvariant, subspace constrained mean shift. Our method can be used to perform\nclustering and to calculate a measure of connectivity between clusters. We\nestablish consistency and rates of convergence for our estimator and apply the\nmethods to data from two astronomical problems. \n\n"}
{"id": "1406.2462", "contents": "Title: Empirical risk minimization for heavy-tailed losses Abstract: The purpose of this paper is to discuss empirical risk minimization when the\nlosses are not necessarily bounded and may have a distribution with heavy\ntails. In such situations, usual empirical averages may fail to provide\nreliable estimates and empirical risk minimization may provide large excess\nrisk. However, some robust mean estimators proposed in the literature may be\nused to replace empirical means. In this paper, we investigate empirical risk\nminimization based on a robust estimate proposed by Catoni. We develop\nperformance bounds based on chaining arguments tailored to Catoni's mean\nestimator. \n\n"}
{"id": "1406.3521", "contents": "Title: Exact prior-free probabilistic inference on the heritability coefficient\n  in a linear mixed model Abstract: Linear mixed-effect models with two variance components are often used when\nvariability comes from two sources. In genetics applications, variation in\nobserved traits can be attributed to biological and environmental effects, and\nthe heritability coefficient is a fundamental quantity that measures the\nproportion of total variability due to the biological effect. We propose a new\ninferential model approach which yields exact prior-free probabilistic\ninference on the heritability coefficient. In particular we construct exact\nconfidence intervals and demonstrate numerically our method's efficiency\ncompared to that of existing methods. \n\n"}
{"id": "1406.5392", "contents": "Title: Rate optimality of Random walk Metropolis algorithm in high-dimension\n  with heavy-tailed target distribution Abstract: The choice of the increment distribution is crucial for the random-walk\nMetropolis-Hastings (RWM) algorithm. In this paper we study the optimal choice\nin high-dimension setting among all possible increment distributions. The\nconclusion is rather counter intuitive, but the optimal rate of convergence is\nattained by the usual choice, the normal distribution as the increment\ndistribution. In particular, no heavy-tailed increment distribution can improve\nthe rate. \n\n"}
{"id": "1406.5663", "contents": "Title: Asymptotic theory for density ridges Abstract: The large sample theory of estimators for density modes is well understood.\nIn this paper we consider density ridges, which are a higher-dimensional\nextension of modes. Modes correspond to zero-dimensional, local high-density\nregions in point clouds. Density ridges correspond to $s$-dimensional, local\nhigh-density regions in point clouds. We establish three main results. First we\nshow that under appropriate regularity conditions, the local variation of the\nestimated ridge can be approximated by an empirical process. Second, we show\nthat the distribution of the estimated ridge converges to a Gaussian process.\nThird, we establish that the bootstrap leads to valid confidence sets for\ndensity ridges. \n\n"}
{"id": "1407.1778", "contents": "Title: Robust Estimation of Bivariate Tail Dependence Coefficient Abstract: The problem of estimating the coefficient of bivariate tail dependence is\nconsidered here from the robustness point of view; it combines two apparently\ncontradictory theories of robust statistics and extreme value statistics. The\nusual maximum likelihood based or the moment type estimators of tail dependence\ncoefficient are highly sensitive to the presence of outlying observations in\ndata. This paper proposes some alternative robust estimators obtained by\nminimizing the density power divergence with suitable model assumptions; their\nrobustness properties are examined through the classical influence function\nanalysis. The performance of the proposed estimators is illustrated through an\nextensive empirical study considering several important bivariate extreme value\ndistributions. \n\n"}
{"id": "1407.2219", "contents": "Title: Bayesian adaptation Abstract: In the need for low assumption inferential methods in infinite-dimensional\nsettings, Bayesian adaptive estimation via a prior distribution that does not\ndepend on the regularity of the function to be estimated nor on the sample size\nis valuable. We elucidate relationships among the main approaches followed to\ndesign priors for minimax-optimal rate-adaptive estimation meanwhile shedding\nlight on the underlying ideas. \n\n"}
{"id": "1407.3414", "contents": "Title: Interactive Q-learning for Probabilities and Quantiles Abstract: A dynamic treatment regime is a sequence of decision rules in which each\ndecision rule recommends treatment based on features of patient medical history\nsuch as past treatments and outcomes. Existing methods for estimating optimal\ndynamic treatment regimes from data optimize the mean of a response variable.\nHowever, the mean may not always be the most appropriate summary of\nperformance. We derive estimators of decision rules for optimizing\nprobabilities and quantiles computed with respect to the response distribution\nfor two-stage, binary treatment settings. This enables estimation of dynamic\ntreatment regimes that optimize the cumulative distribution function of the\nresponse at a prespecified point or a prespecified quantile of the response\ndistribution such as the median. The proposed methods perform favorably in\nsimulation experiments. We illustrate our approach with data from a\nsequentially randomized trial where the primary outcome is remission of\ndepression symptoms. \n\n"}
{"id": "1407.3939", "contents": "Title: Analysis of purely random forests bias Abstract: Random forests are a very effective and commonly used statistical method, but\ntheir full theoretical analysis is still an open problem. As a first step,\nsimplified models such as purely random forests have been introduced, in order\nto shed light on the good performance of random forests. In this paper, we\nstudy the approximation error (the bias) of some purely random forest models in\na regression framework, focusing in particular on the influence of the number\nof trees in the forest. Under some regularity assumptions on the regression\nfunction, we show that the bias of an infinite forest decreases at a faster\nrate (with respect to the size of each tree) than a single tree. As a\nconsequence, infinite forests attain a strictly better risk rate (with respect\nto the sample size) than single trees. Furthermore, our results allow to derive\na minimum number of trees sufficient to reach the same rate as an infinite\nforest. As a by-product of our analysis, we also show a link between the bias\nof purely random forests and the bias of some kernel estimators. \n\n"}
{"id": "1407.4184", "contents": "Title: Inference for biased models: a quasi-instrumental variable approach Abstract: For linear regression models who are not exactly sparse in the sense that the\ncoefficients of the insignificant variables are not exactly zero, the working\nmodels obtained by a variable selection are often biased. Even in sparse cases,\nafter a variable selection, when some significant variables are missing, the\nworking models are biased as well. Thus, under such situations, root-n\nconsistent estimation and accurate prediction could not be expected. In this\npaper, a novel remodelling method is proposed to produce an unbiased model when\nquasi-instrumental variables are introduced. The root-n estimation consistency\nand the asymptotic normality can be achieved, and the prediction accuracy can\nbe promoted as well. The performance of the new method is examined through\nsimulation studies. \n\n"}
{"id": "1407.4916", "contents": "Title: Extensions of stability selection using subsamples of observations and\n  covariates Abstract: We introduce extensions of stability selection, a method to stabilise\nvariable selection methods introduced by Meinshausen and B\\\"uhlmann (J R Stat\nSoc 72:417-473, 2010). We propose to apply a base selection method repeatedly\nto random observation subsamples and covariate subsets under scrutiny, and to\nselect covariates based on their selection frequency. We analyse the effects\nand benefits of these extensions. Our analysis generalizes the theoretical\nresults of Meinshausen and B\\\"uhlmann (J R Stat Soc 72:417-473, 2010) from the\ncase of half-samples to subsamples of arbitrary size. We study, in a\ntheoretical manner, the effect of taking random covariate subsets using a\nsimplified score model. Finally we validate these extensions on numerical\nexperiments on both synthetic and real datasets, and compare the obtained\nresults in detail to the original stability selection method. \n\n"}
{"id": "1407.5241", "contents": "Title: Influential Feature PCA for high dimensional clustering Abstract: We consider a clustering problem where we observe feature vectors $X_i \\in\nR^p$, $i = 1, 2, \\ldots, n$, from $K$ possible classes. The class labels are\nunknown and the main interest is to estimate them. We are primarily interested\nin the modern regime of $p \\gg n$, where classical clustering methods face\nchallenges.\n  We propose Influential Features PCA (IF-PCA) as a new clustering procedure.\nIn IF-PCA, we select a small fraction of features with the largest\nKolmogorov-Smirnov (KS) scores, where the threshold is chosen by adapting the\nrecent notion of Higher Criticism, obtain the first $(K-1)$ left singular\nvectors of the post-selection normalized data matrix, and then estimate the\nlabels by applying the classical k-means to these singular vectors. It can be\nseen that IF-PCA is a tuning free clustering method.\n  We apply IF-PCA to $10$ gene microarray data sets. The method has competitive\nperformance in clustering. Especially, in three of the data sets, the error\nrates of IF-PCA are only $29\\%$ or less of the error rates by other methods. We\nhave also rediscovered a phenomenon on empirical null by \\cite{Efron} on\nmicroarray data.\n  With delicate analysis, especially post-selection eigen-analysis, we derive\ntight probability bounds on the Kolmogorov-Smirnov statistics and show that\nIF-PCA yields clustering consistency in a broad context. The clustering problem\nis connected to the problems of sparse PCA and low-rank matrix recovery, but it\nis different in important ways. We reveal an interesting phase transition\nphenomenon associated with these problems and identify the range of interest\nfor each. \n\n"}
{"id": "1407.6593", "contents": "Title: Violation of energy-momentum conservation in Mueller-Navelet jets\n  production Abstract: We study effects related to violation of energy-momentum conservation\ninherent to the BFKL approach, in the particular case of Mueller-Navelet jets\nproduction. We argue, based on the comparison of the lowest order non trivial\ncorrections $\\mathcal{O}(\\alpha_s^3)$ to the cross section with predictions of\nan exact calculation, that the inclusion of next-to-leading order BFKL\ncorrections to the jet production vertex significantly reduces the importance\nof these effects. \n\n"}
{"id": "1407.7058", "contents": "Title: Wino-like Minimal Dark Matter and future colliders Abstract: We extend the Standard Model with an EW fermion triplet, stable thanks to one\nof the accidental symmetries already present in the theory. On top of being a\npotential Dark Matter candidate, additional motivations for this new state are\nthe stability of the vacuum, the fact it does not introduce a large fine-tuning\nin the Higgs mass, and that it helps with gauge coupling unification. We\nperform an analysis of the reach for such a particle at the high-luminosity\nLHC, and at a futuristic 100 TeV pp collider. We do so for the monojet,\nmonophoton, vector boson fusion and disappearing tracks channels. At 100 TeV,\ndisappearing tracks will likely probe the mass region of 3 TeV, relevant for\nthermally produced Dark Matter. The reach of the other channels is found to\nextend up to ~ 1.3 (1.7) TeV for 3 (30) ab^-1 of integrated luminosity,\nprovided systematics are well under control. This model also constitutes a\nbenchmark of a typical WIMP Dark Matter candidate, and its phenomenology\nreproduces that of various models of Supersymmetry featuring a pure Wino as the\nlightest sparticle. \n\n"}
{"id": "1407.8219", "contents": "Title: Detecting duplicates in a homicide registry using a Bayesian\n  partitioning approach Abstract: Finding duplicates in homicide registries is an important step in keeping an\naccurate account of lethal violence. This task is not trivial when unique\nidentifiers of the individuals are not available, and it is especially\nchallenging when records are subject to errors and missing values. Traditional\napproaches to duplicate detection output independent decisions on the\ncoreference status of each pair of records, which often leads to nontransitive\ndecisions that have to be reconciled in some ad-hoc fashion. The task of\nfinding duplicate records in a data file can be alternatively posed as\npartitioning the data file into groups of coreferent records. We present an\napproach that targets this partition of the file as the parameter of interest,\nthereby ensuring transitive decisions. Our Bayesian implementation allows us to\nincorporate prior information on the reliability of the fields in the data\nfile, which is especially useful when no training data are available, and it\nalso provides a proper account of the uncertainty in the duplicate detection\ndecisions. We present a study to detect killings that were reported multiple\ntimes to the United Nations Truth Commission for El Salvador. \n\n"}
{"id": "1408.2923", "contents": "Title: Asymptotic and finite-sample properties of estimators based on\n  stochastic gradients Abstract: Stochastic gradient descent procedures have gained popularity for parameter\nestimation from large data sets. However, their statistical properties are not\nwell understood, in theory. And in practice, avoiding numerical instability\nrequires careful tuning of key parameters. Here, we introduce implicit\nstochastic gradient descent procedures, which involve parameter updates that\nare implicitly defined. Intuitively, implicit updates shrink standard\nstochastic gradient descent updates. The amount of shrinkage depends on the\nobserved Fisher information matrix, which does not need to be explicitly\ncomputed; thus, implicit procedures increase stability without increasing the\ncomputational burden. Our theoretical analysis provides the first full\ncharacterization of the asymptotic behavior of both standard and implicit\nstochastic gradient descent-based estimators, including finite-sample error\nbounds. Importantly, analytical expressions for the variances of these\nstochastic gradient-based estimators reveal their exact loss of efficiency. We\nalso develop new algorithms to compute implicit stochastic gradient\ndescent-based estimators for generalized linear models, Cox proportional\nhazards, M-estimators, in practice, and perform extensive experiments. Our\nresults suggest that implicit stochastic gradient descent procedures are poised\nto become a workhorse for approximate inference from large data sets \n\n"}
{"id": "1408.3386", "contents": "Title: Solution of linear ill-posed problems using overcomplete dictionaries Abstract: In the present paper we consider application of overcomplete dictionaries to\nsolution of general ill-posed linear inverse problems. Construction of an\nadaptive optimal solution for such problems usually relies either on a singular\nvalue decomposition or representation of the solution via an orthonormal basis.\nThe shortcoming of both approaches lies in the fact that, in many situations,\nneither the eigenbasis of the linear operator nor a standard orthonormal basis\nconstitutes an appropriate collection of functions for sparse representation of\nthe unknown function. In the context of regression problems, there have been an\nenormous amount of effort to recover an unknown function using an overcomplete\ndictionary. One of the most popular methods, Lasso, is based on minimizing the\nempirical likelihood and requires stringent assumptions on the dictionary, the,\nso called, compatibility conditions. While these conditions may be satisfied\nfor the original dictionary functions, they usually do not hold for their\nimages due to contraction imposed by the linear operator. In what follows, we\nbypass this difficulty by a novel approach which is based on inverting each of\nthe dictionary functions and matching the resulting expansion to the true\nfunction, thus, avoiding unrealistic assumptions on the dictionary and using\nLasso in a predictive setting. We examine both the white noise and the\nobservational model formulations and also discuss how exact inverse images of\nthe dictionary functions can be replaced by their approximate counterparts.\nFurthermore, we show how the suggested methodology can be extended to the\nproblem of estimation of a mixing density in a continuous mixture. For all the\nsituations listed above, we provide the oracle inequalities for the risk in a\nfinite sample setting. Simulation studies confirm good computational properties\nof the Lasso-based technique. \n\n"}
{"id": "1408.3783", "contents": "Title: Long-term causal effects of economic mechanisms on agent incentives Abstract: Economic mechanisms administer the allocation of resources to interested\nagents based on their self-reported types. One objective in mechanism design is\nto design a strategyproof process so that no agent will have an incentive to\nmisreport its type. However, typical analyses of the incentives properties of\nmechanisms operate under strong, usually untestable assumptions. Empirical,\ndata-oriented approaches are, at best, under-developed. Furthermore,\nmechanism/policy evaluation methods usually ignore the dynamic nature of a\nmulti-agent system and are thus inappropriate for estimating long-term effects.\nWe introduce the problem of estimating the causal effects of mechanisms on\nincentives and frame it under the Rubin causal framework \\citep{rubin74,\nrubin78}. This raises unique technical challenges since the outcome of interest\n(agent truthfulness) is confounded with strategic interactions and,\ninterestingly, is typically never observed under any mechanism. We develop a\nmethodology to estimate such causal effects that using a prior that is based on\na strategic equilibrium model. Working on the domain of kidney exchanges, we\nshow how to apply our methodology to estimate causal effects of kidney\nallocation mechanisms on hospitals' incentives. Our results demonstrate that\nthe use of game-theoretic prior captures the dynamic nature of the kidney\nexchange multiagent system and shrinks the estimates towards long-term effects,\nthus improving upon typical methods that completely ignore agents' strategic\nbehavior. \n\n"}
{"id": "1408.3979", "contents": "Title: Hypotheses tests in boundary regression models Abstract: Consider a nonparametric regression model with one-sided errors and\nregression function in a general H\\\"older class. We estimate the regression\nfunction via minimization of the local integral of a polynomial approximation.\nWe show uniform rates of convergence for the simple regression estimator as\nwell as for a smooth version. These rates carry over to mean regression models\nwith a symmetric and bounded error distribution. In such a setting, one obtains\nfaster rates for irregular error distributions concentrating sufficient mass\nnear the endpoints than for the usual regular distributions. The results are\napplied to prove asymptotic $\\sqrt{n}$-equivalence of a residual-based\n(sequential) empirical distribution function to the (sequential) empirical\ndistribution function of unobserved errors in the case of irregular error\ndistributions. This result is remarkably different from corresponding results\nin mean regression with regular errors. It can readily be applied to develop\ngoodness-of-fit tests for the error distribution. We present some examples and\ninvestigate the small sample performance in a simulation study. We further\ndiscuss asymptotically distribution-free hypotheses tests for independence of\nthe error distribution from the points of measurement and for monotonicity of\nthe boundary function as well. \n\n"}
{"id": "1408.4102", "contents": "Title: Estimation of Monotone Treatment Effects in Network Experiments Abstract: Randomized experiments on social networks pose statistical challenges, due to\nthe possibility of interference between units. We propose new methods for\nestimating attributable treatment effects in such settings. The methods do not\nrequire partial interference, but instead require an identifying assumption\nthat is similar to requiring nonnegative treatment effects. Network or spatial\ninformation can be used to customize the test statistic; in principle, this can\nincrease power without making assumptions on the data generating process. \n\n"}
{"id": "1408.6293", "contents": "Title: Turbulent meson condensation in quark deconfinement Abstract: In a QCD-like strongly coupled gauge theory at large N_c, using the AdS/CFT\ncorrespondence, we find that heavy quark deconfinement is accompanied by a\ncoherent condensation of higher meson resonances. This is revealed in\nnon-equilibrium deconfinement transitions triggered by static, as well as,\nquenched electric fields even below the Schwinger limit. There, we observe a\n\"turbulent\" energy flow to higher meson modes, which finally results in the\nquark deconfinement. Our observation is consistent with seeing deconfinement as\na condensation of long QCD strings. \n\n"}
{"id": "1408.6937", "contents": "Title: An Exact Formula for the Average Run Length to False Alarm of the\n  Generalized Shiryaev-Roberts Procedure for Change-Point Detection under\n  Exponential Observations Abstract: We derive analytically an exact closed-form formula for the standard minimax\nAverage Run Length (ARL) to false alarm delivered by the Generalized\nShiryaev-Roberts (GSR) change-point detection procedure devised to detect a\nshift in the baseline mean of a sequence of independent exponentially\ndistributed observations. Specifically, the formula is found through direct\nsolution of the respective integral (renewal) equation, and is a general result\nin that the GSR procedure's headstart is not restricted to a bounded range, nor\nis there a \"ceiling\" value for the detection threshold. Apart from the\ntheoretical significance (in change-point detection, exact closed-form\nperformance formulae are typically either difficult or impossible to get,\nespecially for the GSR procedure), the obtained formula is also useful to a\npractitioner: in cases of practical interest, the formula is a function linear\nin both the detection threshold and the headstart, and, therefore, the ARL to\nfalse alarm of the GSR procedure can be easily computed. \n\n"}
{"id": "1409.0693", "contents": "Title: Production of $\\chi_c$- and $\\chi_b$-mesons in high energy hadronic\n  collisions Abstract: This paper is devoted to phenomenological study of $\\chi_{c,b}$-mesons\nproduction in high energy hadronic collisions in the framework of NRQCD. We\nanalyze all available experimental data on $\\chi_c$-mesons production and\nextract non-perturbative NRQCD matrix elements from fitting the data. We show,\nthat measured $p_T$-spectrum of $\\chi_c$-mesons is mainly formed by color\nsinglet components, while $\\sigma(\\chi_{c2})/\\sigma(\\chi_{c1})$ ratio depends\nstrongly on color octet matrix elements; this ratio becomes a highly sensitive\ntool to study contribution of different terms from the NRQCD expansion.\nObtained using NRQCD scaling rules predictions for $\\chi_b$-mesons cross\nsections are also given. \n\n"}
{"id": "1409.0874", "contents": "Title: The $D_{sJ}^*(2860)$ Mesons as Excited D-wave $c\\bar{s}$ States Abstract: A new charm-strange meson, the $D_{s1}^*(2863)$, has recently been observed\nby the LHCb collaboration which also determined the $D_{sJ}^*(2860)$ to have\nspin 3. One of the speculations about the previously observed $D_{s1}^*(2710)$\nis that it is the $1^3D_1(c\\bar{s})$ state. In this paper we reexamine the\nquark model properties and assignments of these three states in light of these\nnew measurements. We conclude that the $D_{s1}^*(2863)$ and $D_{s3}^*(2863)$\nare the $1^3D_1 (c\\bar{s})$ and $1^3D_3 (c\\bar{s})$ states respectively and the\n$D_{s1}^*(2710)$ is the $2^3S_1(c\\bar{s})$ state. In addition to these three\nstates there are another three excited $D_s$ states in this mass region still\nto be found; the $2^1S_0$ and two $1D_2$ states. We calculate the properties of\nthese states and expect that LHCb has the capability of observing these states\nin the near future. \n\n"}
{"id": "1409.0922", "contents": "Title: Challenges for New Physics in the Flavour Sector Abstract: In these proceedings I present a personal perspective of the challenges for\nnew physics (NP) searches in the flavour sector. Since the CKM mechanism of\nflavour violation has been established to a very high precision, we know that\nphysics beyond the Standard Model can only contribute sub-dominantly.\nTherefore, any realistic model of physics beyond the Standard Model (SM) must\nrespect the stringent constrains from flavour observables like $b\\to s \\gamma$,\n$B_s\\to\\mu^+\\mu^-$, $\\Delta F=2$ processes etc., in a first step. In a second\nstep, it is interesting to ask the question if some deviations from the SM\npredictions (like the anomalous magnetic moment of the muon or recently\nobserved discrepancies in tauonic $B$ decays or $B\\to K^*\\mu^+\\mu^-$) can be\nexplained by a model of NP without violating bounds from other observables. \n\n"}
{"id": "1409.1966", "contents": "Title: Formal Developments for Lattice QCD with Applications to Hadronic\n  Systems Abstract: Lattice quantum chromodynamics (QCD) will soon become the primary theoretical\ntool in rigorous studies of single- and multi-hadron sectors of QCD. It is\ntruly ab initio meaning that its only parameters are those of standard model.\nThe result of a lattice QCD calculation corresponds to that of nature only in\nthe limit when the volume of spacetime is taken to infinity and the spacing\nbetween discretized points on the lattice is taken to zero. A better\nunderstanding of these discretization and volume effects not only provides the\nconnection to the infinite-volume continuum observables, but also leads to\noptimized calculations that can be performed with available computational\nresources. This thesis includes various formal developments in this direction,\nalong with proposals for improvements, to be applied to the upcoming lattice\nQCD studies of nuclear and hadronic systems. Among these developments are i) an\nanalytical investigation of the recovery of rotational symmetry with the use of\nsuitably-formed smeared operators toward the continuum limit, ii) an extension\nof the Luscher finite-volume method to two-nucleon systems with arbitrary\nangular momentum, spin, parity and center of mass momentum, iii) the\napplication of such formalism in extracting the scattering parameters of the\n3S1-3D1 coupled channels, iv) an investigation of twisted boundary conditions\nin the single- and two-hadron sectors, with proposals for improving the\nvolume-dependence of the deuteron binding energy upon proper choices of\nboundary conditions, and v) exploring the volume dependence of the masses of\nhadrons and light-nuclei due to quantum electrodynamic interactions, including\nthe effects arising from particles' compositeness. The required background as\nwell as a brief status report of the field pertinent to the discussions in this\nthesis are presented. \n\n"}
{"id": "1409.3031", "contents": "Title: Supersymmetry and R-symmetry Breaking in Meta-stable Vacua at Finite\n  Temperature and Density Abstract: We study a meta-stable supersymmetry-breaking vacuum in a generalized\nO'Raifeartaigh model at finite temperature and chemical potentials. Fields in\nthe generalized O'Raifeartaigh model possess different R-charges to realize\nR-symmetry breaking. Accordingly, at finite density and temperature, the\nchemical potentials have to be introduced in a non-uniform way. Based on the\nformulation elaborated in our previous work we study the one-loop thermal\neffective potential including the chemical potentials in the generalized\nO'Raifeartaigh model. We perform a numerical analysis and find that the\nR-symmetry breaking vacua, which exist at zero temperature and zero chemical\npotential, are destabilized for some parameter regions. In addition, we find\nthat there are parameter regions where new R-symmetry breaking vacua are\nrealized even at high temperature by the finite density effects. \n\n"}
{"id": "1409.3301", "contents": "Title: Partial Correlation Screening for Estimating Large Precision Matrices,\n  with Applications to Classification Abstract: We propose Partial Correlation Screening (PCS) as a new row-by-row approach\nto estimating a large precision matrix $\\Omega$. To estimate the $i$-th row of\n$\\Omega$, $1 \\leq i \\leq p$, PCS uses a Screen step and a Clean step. In the\nScreen step, PCS recruits a (small) subset of indices using a stage-wise\nalgorithm, where in each stage, the algorithm updates the set of recruited\nindices by adding the index $j$ that has the largest (in magnitude) empirical\npartial correlation with $i$. In the Clean step, PCS re-investigates all\nrecruited indices and use them to reconstruct the $i$-th row of $\\Omega$.\n  PCS is computationally efficient and modest in memory use: to estimate a row\nof $\\Omega$, it only needs a few rows (determined sequentially) of the\nempirical covariance matrix. This enables PCS to execute the estimation of a\nlarge precision matrix (e.g., $p=10K$) in a few minutes, and open doors to\nestimating much larger precision matrices.\n  We use PCS for classification. Higher Criticism Thresholding (HCT) is a\nrecent classifier that enjoys optimality, but to exploit its full potential in\npractice, one needs a good estimate of the precision matrix $\\Omega$. Combining\nHCT with any approach to estimating $\\Omega$ gives a new classifier: examples\ninclude HCT-PCS and HCT-glasso. We have applied HCT-PCS to two large microarray\ndata sets ($p = 8K$ and $10K$) for classification, where it not only\nsignificantly outperforms HCT-glasso, but also is competitive to the Support\nVector Machine (SVM) and Random Forest (RF). The results suggest that PCS gives\nmore useful estimates of $\\Omega$ than the glasso.\n  We set up a general theoretical framework and show that in a broad context,\nPCS fully recovers the support of $\\Omega$ and HCT-PCS yields optimal\nclassification behavior. Our proofs shed interesting light on the behavior of\nstage-wise procedures. \n\n"}
{"id": "1409.3348", "contents": "Title: Hard exclusive two photon processes in QCD Abstract: This is a short review of some hard two-photon processes: $\\\\ a)\n\\,\\,\\gamma\\gamma\\to {\\overline P}_1 P_2,\\,\\, {\\overline P}_1 P_2= \\{\\pi^+\\pi^-,\nK^+ K^-, K_S K_S, \\pi^o\\pi^o, \\pi^o\\eta\\}\\,, \\\\ b) \\,\\,\\gamma\\gamma\\to V_1\nV_2,\\,\\, V_1 V_2=\\{\\rho^o\\rho^o, \\phi\\phi, \\omega\\phi, \\omega\\omega \\},\\\\ c)\n\\,\\,\\gamma\\gamma\\to {\\rm baryon-antibaryon},\\\\ d) \\,\\,\\gamma^*\\gamma\\to\nP^o,\\,\\, P^o=\\{\\pi^o, \\eta, \\eta^\\prime, \\eta_c\\}$.\n  The available experimental data are presented. A number of theoretical\napproaches to calculation of these processes is described, both those based\nmainly on QCD and more phenomenological (the handbag model, the diquark model,\netc). Some theoretical questions tightly connected with this subject are\ndiscussed, in particular: the applications of various types of QCD sum rules,\nthe endpoint behavior of the leading twist meson wave functions, etc. \n\n"}
{"id": "1409.3795", "contents": "Title: On the correspondence from Bayesian log-linear modelling to logistic\n  regression modelling with $g$-priors Abstract: Consider a set of categorical variables where at least one of them is binary.\nThe log-linear model that describes the counts in the resulting contingency\ntable implies a specific logistic regression model, with the binary variable as\nthe outcome. Within the Bayesian framework, the $g$-prior and mixtures of\n$g$-priors are commonly assigned to the parameters of a generalized linear\nmodel. We prove that assigning a $g$-prior (or a mixture of $g$-priors) to the\nparameters of a certain log-linear model designates a $g$-prior (or a mixture\nof $g$-priors) on the parameters of the corresponding logistic regression. By\nderiving an asymptotic result, and with numerical illustrations, we demonstrate\nthat when a $g$-prior is adopted, this correspondence extends to the posterior\ndistribution of the model parameters. Thus, it is valid to translate inferences\nfrom fitting a log-linear model to inferences within the logistic regression\nframework, with regard to the presence of main effects and interaction terms. \n\n"}
{"id": "1409.3886", "contents": "Title: On a Nonparametric Notion of Residual and its Applications Abstract: Let $(X, \\mathbf{Z})$ be a continuous random vector in $\\mathbb{R} \\times\n\\mathbb{R}^d$, $d \\ge 1$. In this paper, we define the notion of a\nnonparametric residual of $X$ on $\\mathbf{Z}$ that is always independent of the\npredictor $\\mathbf{Z}$. We study its properties and show that the proposed\nnotion of residual matches with the usual residual (error) in a multivariate\nnormal regression model. Given a random vector $(X, Y, \\mathbf{Z})$ in\n$\\mathbb{R} \\times \\mathbb{R} \\times \\mathbb{R}^d$, we use this notion of\nresidual to show that the conditional independence between $X$ and $Y$, given\n$\\mathbf{Z}$, is equivalent to the mutual independence of the residuals (of $X$\non $\\mathbf{Z}$ and $Y$ on $\\mathbf{Z}$) and $\\mathbf{Z}$. This result is used\nto develop a test for conditional independence. We propose a bootstrap scheme\nto approximate the critical value of this test. We compare the proposed test,\nwhich is easily implementable, with some of the existing procedures through a\nsimulation study. \n\n"}
{"id": "1409.6219", "contents": "Title: Flexible modelling in statistics: past, present and future Abstract: In times where more and more data become available and where the data exhibit\nrather complex structures (significant departure from symmetry, heavy or light\ntails), flexible modelling has become an essential task for statisticians as\nwell as researchers and practitioners from domains such as economics, finance\nor environmental sciences. This is reflected by the wealth of existing\nproposals for flexible distributions; well-known examples are Azzalini's\nskew-normal, Tukey's $g$-and-$h$, mixture and two-piece distributions, to cite\nbut these. My aim in the present paper is to provide an introduction to this\nresearch field, intended to be useful both for novices and professionals of the\ndomain. After a description of the research stream itself, I will narrate the\ngripping history of flexible modelling, starring emblematic heroes from the\npast such as Edgeworth and Pearson, then depict three of the most used flexible\nfamilies of distributions, and finally provide an outlook on future flexible\nmodelling research by posing challenging open questions. \n\n"}
{"id": "1409.8565", "contents": "Title: Sparse CCA: Adaptive Estimation and Computational Barriers Abstract: Canonical correlation analysis is a classical technique for exploring the\nrelationship between two sets of variables. It has important applications in\nanalyzing high dimensional datasets originated from genomics, imaging and other\nfields. This paper considers adaptive minimax and computationally tractable\nestimation of leading sparse canonical coefficient vectors in high dimensions.\nFirst, we establish separate minimax estimation rates for canonical coefficient\nvectors of each set of random variables under no structural assumption on\nmarginal covariance matrices. Second, we propose a computationally feasible\nestimator to attain the optimal rates adaptively under an additional sample\nsize condition. Finally, we show that a sample size condition of this kind is\nneeded for any randomized polynomial-time estimator to be consistent, assuming\nhardness of certain instances of the Planted Clique detection problem. The\nresult is faithful to the Gaussian models used in the paper. As a byproduct, we\nobtain the first computational lower bounds for sparse PCA under the Gaussian\nsingle spiked covariance model. \n\n"}
{"id": "1410.2597", "contents": "Title: Optimal Inference After Model Selection Abstract: To perform inference after model selection, we propose controlling the\nselective type I error; i.e., the error rate of a test given that it was\nperformed. By doing so, we recover long-run frequency properties among selected\nhypotheses analogous to those that apply in the classical (non-adaptive)\ncontext. Our proposal is closely related to data splitting and has a similar\nintuitive justification, but is more powerful. Exploiting the classical theory\nof Lehmann and Scheff\\'e (1955), we derive most powerful unbiased selective\ntests and confidence intervals for inference in exponential family models after\narbitrary selection procedures. For linear regression, we derive new selective\nz-tests that generalize recent proposals for inference after model selection\nand improve on their power, and new selective t-tests that do not require\nknowledge of the error variance. \n\n"}
{"id": "1410.3343", "contents": "Title: Charm and strange quark masses and $f_{D_s}$ from overlap fermions Abstract: We use overlap fermions as valence quarks to calculate meson masses in a wide\nquark mass range on the $2+1$-flavor domain-wall fermion gauge configurations\ngenerated by the RBC and UKQCD Collaborations. The well-defined quark masses in\nthe overlap fermion formalism and the clear valence quark mass dependence of\nmeson masses observed from the calculation facilitate a direct derivation of\nphysical current quark masses through a global fit to the lattice data, which\nincorporates $O(a^2)$ and $O(m_c^4a^4)$ corrections, chiral extrapolation, and\nquark mass interpolation. Using the physical masses of $D_s$, $D_s^*$ and\n$J/\\psi$ as inputs, Sommer's scale parameter $r_0$ and the masses of charm\nquark and strange quark in the $\\overline{\\rm MS}$ scheme are determined to be\n$r_0=0.465(4)(9)$ fm, $m_c^{\\overline{\\rm MS}}(2\\,{\\rm GeV})=1.118(6)(24)$ GeV\n(or $m_c^{\\overline{\\rm MS}}(m_c)=1.304(5)(20)$ GeV), and $m_s^{\\overline{\\rm\nMS}}(2\\,{\\rm GeV})=0.101(3)(6)\\,{\\rm GeV}$, respectively. Furthermore, we\nobserve that the mass difference of the vector meson and the pseudoscalar meson\nwith the same valence quark content is proportional to the reciprocal of the\nsquare root of the valence quark masses. The hyperfine splitting of charmonium,\n$M_{J/\\psi}-M_{\\eta_c}$, is determined to be 119(2)(7) MeV, which is in good\nagreement with the experimental value. We also predict the decay constant of\n$D_s$ to be $f_{D_s}=254(2)(4)$ MeV. The masses of charmonium $P$-wave states\n$\\chi_{c0}, \\chi_{c1}$ and $h_c$ are also in good agreement with experiments. \n\n"}
{"id": "1410.3687", "contents": "Title: Identifying the number of factors from singular values of a large sample\n  auto-covariance matrix Abstract: Identifying the number of factors in a high-dimensional factor model has\nattracted much attention in recent years and a general solution to the problem\nis still lacking. A promising ratio estimator based on the singular values of\nthe lagged autocovariance matrix has been recently proposed in the literature\nand is shown to have a good performance under some specific assumption on the\nstrength of the factors. Inspired by this ratio estimator and as a first main\ncontribution, this paper proposes a complete theory of such sample singular\nvalues for both the factor part and the noise part under the large-dimensional\nscheme where the dimension and the sample size proportionally grow to infinity.\nIn particular, we provide the exact description of the phase transition\nphenomenon that determines whether a factor is strong enough to be detected\nwith the observed sample singular values. Based on these findings and as a\nsecond main contribution of the paper, we propose a new estimator of the number\nof factors which is strongly consistent for the detection of all significant\nfactors (which are the only theoretically detectable ones). In particular,\nfactors are assumed to have the minimum strength above the phase transition\nboundary which is of the order of a constant; they are thus not required to\ngrow to infinity together with the dimension (as assumed in most of the\nexisting papers on high-dimensional factor models). Empirical Monte-Carlo study\nas well as the analysis of stock returns data attest a very good performance of\nthe proposed estimator. In all the tested cases, the new estimator largely\noutperforms the existing estimator using the same ratios of singular values. \n\n"}
{"id": "1410.5014", "contents": "Title: Optimal Two-Step Prediction in Regression Abstract: High-dimensional prediction typically comprises two steps: variable selection\nand subsequent least-squares refitting on the selected variables. However, the\nstandard variable selection procedures, such as the lasso, hinge on tuning\nparameters that need to be calibrated. Cross-validation, the most popular\ncalibration scheme, is computationally costly and lacks finite sample\nguarantees. In this paper, we introduce an alternative scheme, easy to\nimplement and both computationally and theoretically efficient. \n\n"}
{"id": "1410.6157", "contents": "Title: UltraViolet Freeze-in Abstract: If dark matter is thermally decoupled from the visible sector, the observed\nrelic density can potentially be obtained via freeze-in production of dark\nmatter. Typically in such models it is assumed that the dark matter is\nconnected to the thermal bath through feeble renormalisable interactions. Here,\nrather, we consider the case in which the hidden and visible sectors are\ncoupled only via non-renormalisable operators. This is arguably a more generic\nrealisation of the dark matter freeze-in scenario, as it does not require the\nintroduction of diminutive renormalisable couplings. We examine general aspects\nof freeze-in via non-renormalisable operators in a number of toy models and\npresent several motivated implementations in the context of Beyond the Standard\nModel physics. Specifically, we study models related to the Peccei-Quinn\nmechanism and Z' portals. \n\n"}
{"id": "1410.7690", "contents": "Title: Trend Filtering on Graphs Abstract: We introduce a family of adaptive estimators on graphs, based on penalizing\nthe $\\ell_1$ norm of discrete graph differences. This generalizes the idea of\ntrend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate\nnonparametric regression, to graphs. Analogous to the univariate case, graph\ntrend filtering exhibits a level of local adaptivity unmatched by the usual\n$\\ell_2$-based graph smoothers. It is also defined by a convex minimization\nproblem that is readily solved (e.g., by fast ADMM or Newton algorithms). We\ndemonstrate the merits of graph trend filtering through examples and theory. \n\n"}
{"id": "1410.8845", "contents": "Title: The Effective Planck Mass and the Scale of Inflation Abstract: Observable quantities in cosmology are dimensionless, and therefore\nindependent of the units in which they are measured. This is true of all\nphysical quantities associated with the primordial perturbations that source\ncosmic microwave background anisotropies such as their amplitude and spectral\nproperties. However, if one were to try and infer an absolute energy scale for\ninflation-- a priori, one of the more immediate corollaries of detecting\nprimordial tensor modes-- one necessarily makes reference to a particular\nchoice of units, the natural choice for which is Planck units. In this note, we\ndiscuss various aspects of how inferring the energy scale of inflation is\ncomplicated by the fact that the effective strength of gravity as seen by\ninflationary quanta necessarily differs from that seen by gravitational\nexperiments at presently accessible scales. The uncertainty in the former\nrelative to the latter has to do with the unknown spectrum of universally\ncoupled particles between laboratory scales and the putative scale of\ninflation. These intermediate particles could be in hidden as well as visible\nsectors or could also be associated with Kaluza-Klein resonances associated\nwith a compactification scale below the scale of inflation. We discuss various\nimplications for cosmological observables. \n\n"}
{"id": "1411.0752", "contents": "Title: Electroweakly-Interacting Dirac Dark Matter Abstract: We consider a class of fermionic dark matter candidates that are charged\nunder both the SU(2)$_L$ and U(1)$_Y$ gauge interactions. In this case a\ncertain amount of dark matter-Higgs couplings, which can split the dark matter\ninto a pair of Majorana fermions, should be present to evade the constraints\nfrom the dark matter direct detection experiments. These effects may be probed\nby means of the dark matter-nucleus scattering via the Higgs-boson exchange\nprocess, as well as the electric dipole moments induced by the dark matter and\nits SU(2)$_L$ partner fields. In this article, we evaluate them with an\neffective field approach. It turns out that the constraints coming from the\nexperiments for the quantities have already restricted the dark matter with\nhypercharge $Y\\geq 3/2$. Future experiments have sensitivities to probe this\nclass of dark matter candidates, and may disfavor the $Y\\geq 1$ cases if no\nsignal is observed. In this case, only the $Y=0$ and $1/2$ cases may be the\nremaining possibilities for the SU(2)$_L$ charged fermionic dark matter\ncandidates. \n\n"}
{"id": "1411.3816", "contents": "Title: Monte Carlo error analyses of Spearman's rank test Abstract: Spearman's rank correlation test is commonly used in astronomy to discern\nwhether a set of two variables are correlated or not. Unlike most other\nquantities quoted in astronomical literature, the Spearman's rank correlation\ncoefficient is generally quoted with no attempt to estimate the errors on its\nvalue. This is a practice that would not be accepted for those other\nquantities, as it is often regarded that an estimate of a quantity without an\nestimate of its associated uncertainties is meaningless. This manuscript\ndescribes a number of easily implemented, Monte Carlo based methods to estimate\nthe uncertainty on the Spearman's rank correlation coefficient, or more\nprecisely to estimate its probability distribution. \n\n"}
{"id": "1411.4158", "contents": "Title: Bayesian Graphical Models for Multivariate Functional Data Abstract: Graphical models express conditional independence relationships among\nvariables. Although methods for vector-valued data are well established,\nfunctional data graphical models remain underdeveloped. We introduce a notion\nof conditional independence between random functions, and construct a framework\nfor Bayesian inference of undirected, decomposable graphs in the multivariate\nfunctional data context. This framework is based on extending Markov\ndistributions and hyper Markov laws from random variables to random processes,\nproviding a principled alternative to naive application of multivariate methods\nto discretized functional data. Markov properties facilitate the composition of\nlikelihoods and priors according to the decomposition of a graph. Our focus is\non Gaussian process graphical models using orthogonal basis expansions. We\npropose a hyper-inverse-Wishart-process prior for the covariance kernels of the\ninfinite coefficient sequences of the basis expansion, establish existence,\nuniqueness, strong hyper Markov property, and conjugacy. Stochastic search\nMarkov chain Monte Carlo algorithms are developed for posterior inference,\nassessed through simulations, and applied to a study of brain activity and\nalcoholism. \n\n"}
{"id": "1411.5876", "contents": "Title: Butterfly resampling: asymptotics for particle filters with constrained\n  interactions Abstract: We generalize the elementary mechanism of sampling with replacement $N$ times\nfrom a weighted population of size $N$, by introducing auxiliary variables and\nconstraints on conditional independence characterised by modular congruence\nrelations. Motivated by considerations of parallelism, a convergence study\nreveals how sparsity of the mechanism's conditional independence graph is\nrelated to fluctuation properties of particle filters which use it for\nresampling, in some cases exhibiting exotic scaling behaviour. The proofs\ninvolve detailed combinatorial analysis of conditional independence graphs. \n\n"}
{"id": "1412.0753", "contents": "Title: Convex clustering via $\\ell_1$ fusion penalization Abstract: We study the large sample behavior of a convex clustering framework, which\nminimizes the sample within cluster sum of squares under an~$\\ell_1$ fusion\nconstraint on the cluster centroids. This recently proposed approach has been\ngaining in popularity, however, its asymptotic properties have remained mostly\nunknown. Our analysis is based on a novel representation of the sample\nclustering procedure as a sequence of cluster splits determined by a sequence\nof maximization problems. We use this representation to provide a simple and\nintuitive formulation for the population clustering procedure. We then\ndemonstrate that the sample procedure consistently estimates its population\nanalog, and derive the corresponding rates of convergence. The proof conducts a\ncareful simultaneous analysis of a collection of M-estimation problems, whose\ncardinality grows together with the sample size. Based on the new perspectives\ngained from the asymptotic investigation, we propose a key post-processing\nmodification of the original clustering framework. We show, both theoretically\nand empirically, that the resulting approach can be successfully used to\nestimate the number of clusters in the population. Using simulated data, we\ncompare the proposed method with existing number of clusters and modality\nassessment approaches, and obtain encouraging results. We also demonstrate the\napplicability of our clustering method for the detection of cellular\nsubpopulations in a single-cell virology study. \n\n"}
{"id": "1412.1341", "contents": "Title: On the applicability of approximations used in calculation of spectrum\n  of Dark Matter particles produced in particle decays Abstract: For the Warm Dark Matter (WDM) candidates the momentum distribution of\nparticles becomes important, since it can be probed with observations of\nLyman-$\\alpha$ forest structures and confronted with coarse grained phase space\ndensity in galaxy clusters. We recall the calculation bt Kaplinghat (2005) of\nthe spectrum in case of dark matter non-thermal production in decays of heavy\nparticles emphasizing on the inherent applicability conditions, which are\nrather restrictive and sometimes ignored in literature. Cold part of the\nspectrum requires special care when WDM is considered. \n\n"}
{"id": "1412.4222", "contents": "Title: A prediction interval for a function-valued forecast model Abstract: Starting from the information contained in the shape of the load curves, we\nhave proposed a flexible nonparametric function-valued fore-cast model called\nKWF (Kernel+Wavelet+Functional) well suited to handle nonstationary series. The\npredictor can be seen as a weighted average of futures of past situations,\nwhere the weights increase with the similarity between the past situations and\nthe actual one. In addi-tion, this strategy provides with a simultaneous\nmultiple horizon pre-diction. These weights induce a probability distribution\nthat can be used to produce bootstrap pseudo predictions. Prediction intervals\nare constructed after obtaining the corresponding bootstrap pseudo pre-diction\nresiduals. We develop two propositions following directly the KWF strategy and\ncompare it to two alternative ways coming from proposals of econometricians.\nThey construct simultaneous prediction intervals using multiple comparison\ncorrections through the control of the family wise error (FWE) or the false\ndiscovery rate. Alternatively, such prediction intervals can be constructed\nbootstrapping joint prob-ability regions. In this work we propose to obtain\nprediction intervals for the KWF model that are simultaneously valid for the H\npredic-tion horizons that corresponds with the corresponding path forecast,\nmaking a connection between functional time series and the econome-tricians'\nframework. \n\n"}
{"id": "1412.4482", "contents": "Title: Sensing Short-Range Forces with a Nanosphere Matter-Wave Interferometer Abstract: We describe a method for sensing short range forces using matter wave\ninterference in dielectric nanospheres. When compared with atom\ninterferometers, the larger mass of the nanosphere results in reduced wave\npacket expansion, enabling investigations of forces nearer to surfaces in a\nfree-fall interferometer. By laser cooling a nanosphere to the ground state of\nan optical potential and releasing it by turning off the optical trap,\nacceleration sensing at the $10^{-8}$m/s$^2$ level is possible. The approach\ncan yield improved sensitivity to Yukawa-type deviations from Newtonian gravity\nat the $5$ $\\mu$m length scale by a factor of $10^4$ over current limits. \n\n"}
{"id": "1412.8714", "contents": "Title: Higgs Boson Spectra in Supersymmetric Left-Right Models Abstract: We present a comprehensive analysis of the Higgs boson spectra in several\nversions of the supersymmetric left--right model based on the gauge symmetry\n$SU(3)_c \\times SU(2)_L \\times SU(2)_R \\times U(1)_{B-L}$. A variety of\nsymmetry breaking sectors are studied, with a focus on the constraints placed\non model parameters by the lightest neutral CP even Higgs boson mass $M_h$. The\nbreaking of $SU(2)_R$ symmetry is achieved by Higgs fields transforming either\nas triplets or doublets, and the electroweak symmetry breaking is triggered by\neither bi--doublets or doublets. The Higgs potential is analyzed with or\nwithout a gauge singlet Higgs field present. Seesaw models of Type I and Type\nII, inverse seesaw models, universal seesaw models and an $E_6$ inspired\nalternate left--right model are included in our analysis. Several of these\nmodels lead to the tree--level relation $M_h \\leq \\sqrt{2}\\,m_W$ (rather than\n$M_h \\leq m_Z$ that arises in the MSSM), realized when the $SU(2)_R$ symmetry\nbreaking scale is of order TeV. With such an enhanced upper limit, it becomes\npossible to accommodate a Higgs boson of mass 126 GeV with relatively light\nstops that mix negligibly. In models with Higgs triplets, a doubly charged\nscalar remains light below a TeV with its mass arising entirely from radiative\ncorrections. We carry out the complete one--loop calculation for its mass\ninduced by the Majorana Yukawa couplings and show the consistency of the\nframework. We argue that these models prefer a low $SU(2)_R$ breaking scale.\nOther theoretical and phenomenological implications of these models are briefly\ndiscussed. \n\n"}
{"id": "1501.00049", "contents": "Title: Model Selection for High Dimensional Quadratic Regression via\n  Regularization Abstract: Quadratic regression (QR) models naturally extend linear models by\nconsidering interaction effects between the covariates. To conduct model\nselection in QR, it is important to maintain the hierarchical model structure\nbetween main effects and interaction effects. Existing regularization methods\ngenerally achieve this goal by solving complex optimization problems, which\nusually demands high computational cost and hence are not feasible for high\ndimensional data. This paper focuses on scalable regularization methods for\nmodel selection in high dimensional QR. We first consider two-stage\nregularization methods and establish theoretical properties of the two-stage\nLASSO. Then, a new regularization method, called Regularization Algorithm under\nMarginality Principle (RAMP), is proposed to compute a hierarchy-preserving\nregularization solution path efficiently. Both methods are further extended to\nsolve generalized QR models. Numerical results are also shown to demonstrate\nperformance of the methods. \n\n"}
{"id": "1501.00478", "contents": "Title: Uniform Inference in High-dimensional Dynamic Panel Data Models Abstract: We establish oracle inequalities for a version of the Lasso in\nhigh-dimensional fixed effects dynamic panel data models. The inequalities are\nvalid for the coefficients of the dynamic and exogenous regressors. Separate\noracle inequalities are derived for the fixed effects. Next, we show how one\ncan conduct uniformly valid simultaneous inference on the parameters of the\nmodel and construct a uniformly valid estimator of the asymptotic covariance\nmatrix which is robust to conditional heteroskedasticity in the error terms.\nAllowing for conditional heteroskedasticity is important in dynamic models as\nthe conditional error variance may be non-constant over time and depend on the\ncovariates. Furthermore, our procedure allows for inference on high-dimensional\nsubsets of the parameter vector of an increasing cardinality. We show that the\nconfidence bands resulting from our procedure are asymptotically honest and\ncontract at the optimal rate. This rate is different for the fixed effects than\nfor the remaining parts of the parameter vector. \n\n"}
{"id": "1501.01219", "contents": "Title: Robust high-dimensional precision matrix estimation Abstract: The dependency structure of multivariate data can be analyzed using the\ncovariance matrix $\\Sigma$. In many fields the precision matrix $\\Sigma^{-1}$\nis even more informative. As the sample covariance estimator is singular in\nhigh-dimensions, it cannot be used to obtain a precision matrix estimator. A\npopular high-dimensional estimator is the graphical lasso, but it lacks\nrobustness. We consider the high-dimensional independent contamination model.\nHere, even a small percentage of contaminated cells in the data matrix may lead\nto a high percentage of contaminated rows. Downweighting entire observations,\nwhich is done by traditional robust procedures, would then results in a loss of\ninformation. In this paper, we formally prove that replacing the sample\ncovariance matrix in the graphical lasso with an elementwise robust covariance\nmatrix leads to an elementwise robust, sparse precision matrix estimator\ncomputable in high-dimensions. Examples of such elementwise robust covariance\nestimators are given. The final precision matrix estimator is positive\ndefinite, has a high breakdown point under elementwise contamination and can be\ncomputed fast. \n\n"}
{"id": "1501.01316", "contents": "Title: Hall Scrambling on Black Hole Horizons Abstract: We explore the effect of the electrodynamics $\\theta$-angle on the\nmacroscopic properties of black hole horizons. Using only classical\nEinstein-Maxwell-Chern-Simons theory in (3+1)-dimensions, in the form of the\nmembrane paradigm, we show that in the presence of the $\\theta$-term, a black\nhole horizon behaves as a Hall conductor, for an observer hovering outside. We\nstudy how localized perturbations created on the stretched horizon scramble on\nthe horizon by dropping a charged particle. We show that the $\\theta$-angle\naffects the way perturbations scramble on the horizon, in particular, it\nintroduces vortices without changing the scrambling time. This Hall scrambling\nof information is also expected to occur on cosmological horizons. \n\n"}
{"id": "1501.01840", "contents": "Title: Gibbs posterior inference on the minimum clinically important difference Abstract: IIt is known that a statistically significant treatment may not be clinically\nsignificant. A quantity that can be used to assess clinical significance is\ncalled the minimum clinically important difference (MCID), and inference on the\nMCID is an important and challenging problem. Modeling for the purpose of\ninference on the MCID is non-trivial, and concerns about bias from a\nmisspecified parametric model or inefficiency from a nonparametric model\nmotivate an alternative approach to balance robustness and efficiency. In\nparticular, a recently proposed representation of the MCID as the minimizer of\na suitable risk function makes it possible to construct a Gibbs posterior\ndistribution for the MCID without specifying a model. We establish the\nposterior convergence rate and show, numerically, that an appropriately scaled\nversion of this Gibbs posterior yields interval estimates for the MCID which\nare both valid and efficient even for relatively small sample sizes. \n\n"}
{"id": "1501.02189", "contents": "Title: The nature of $Z_b$ states from a combined analysis of\n  $\\Upsilon(5S)\\rightarrow h_b(mP) \\pi^+ \\pi^-$ and $\\Upsilon(5S)\\rightarrow\n  B^{(\\ast)}\\bar B^{(\\ast)}\\pi$ Abstract: With a combined analysis of data on $\\Upsilon(5S)\\rightarrow\nh_b(1P,2P)\\pi^+\\pi^-$ and $\\Upsilon(5S)\\rightarrow B^{(\\ast)}\\bar\nB^{(\\ast)}\\pi$ in an effective field theory approach, we determine resonance\nparameters of $Z_b$ states in two scenarios. In one scenario we assume that\n$Z_b$ states are pure molecular states, while in the other one we assume that\n$Z_b$ states contain compact components. We find that the present data favor\nthat there should be some compact components inside $Z_b^{(\\prime)}$ associated\nwith the molecular components. By fitting the invariant mass spectra of\n$\\Upsilon(5S)\\rightarrow h_b(1P,2P)\\pi^+\\pi^-$ and $\\Upsilon(5S)\\rightarrow\nB^{(\\ast)}\\bar B^{\\ast}\\pi$, we determine that the probability of finding the\ncompact components in $Z_b$ states may be as large as about $40\\%$. \n\n"}
{"id": "1501.05959", "contents": "Title: $v_2$ of charged hadrons in a`soft + hard' model for PbPb collisions at\n  $\\sqrt s$ = 2.76 ATeV Abstract: We describe transverse spectra as well as azimuthal anisotropy ($v_2$) of\ncharged hadrons stemming from various centrality PbPb collisions at $\\sqrt s$ =\n2.76 ATeV analytically in a `soft + hard' model. In this model, we propose that\nhadron yields produced in heavy-ion collisions are simply the sum of yields\nstemming from jets (hard yields) and yields stemming from the Quark-Gluon\nPlasma (soft yields). The hadron spectra in both types of yields are\napproximated by the Tsallis distribution. It is found that the anisotropy\ndecreases for more central collisions. \n\n"}
{"id": "1501.07329", "contents": "Title: A Big Data Architecture Design for Smart Grids Based on Random Matrix\n  Theory Abstract: Model-based analysis tools, built on assumptions and simplifications, are\ndifficult to handle smart grids with data characterized by 4Vs data. This\npaper, using random matrix theory (RMT), motivates data-driven tools to\nperceive the complex grids in highdimension; meanwhile, an architecture with\ndetailed procedures is proposed. In algorithm perspective, the architecture\nperforms a high-dimensional analysis, and compares the findings with RMT\npredictions to conduct anomaly detections. Mean Spectral Radius (MSR), as a\nstatistical indicator, is defined to reflect the correlations of system data in\ndifferent dimensions. In management mode perspective, a group-work mode is\ndiscussed for smart grids operation. This mode breaks through regional\nlimitations for energy flows and data flows, and makes advanced big data\nanalyses possible. For a specific large-scale zone-dividing system with\nmultiple connected utilities, each site, operating under the group-work mode,\nis able to work out the regional MSR only with its own measured/simulated data.\nThe large-scale interconnected system, in this way, is naturally decoupled from\nstatistical parameters perspective, rather than from engineering models\nperspective. Furthermore, a comparative analysis of these distributed MSRs,\neven with imperceptible different raw data, will produce a contour line to\ndetect the event and locate the source. It demonstrates that the architecture\nis compatible with the block calculation only using the regional small\ndatabase; beyond that, this architecture, as a data-driven solution, is\nsensitive to system situation awareness, and practical for real large-scale\ninterconnected systems. Five case studies and their visualizations validate the\ndesigned architecture in various fields of power systems. To our best\nknowledge, this study is the first attempt to apply big data technology into\nsmart grids. \n\n"}
{"id": "1502.00587", "contents": "Title: Combining Functional Data Registration and Factor Analysis Abstract: We extend the definition of functional data registration to encompass a\nlarger class of registered functions. In contrast to traditional registration\nmodels, we allow for registered functions that have more than one primary\ndirection of variation. The proposed Bayesian hierarchical model simultaneously\nregisters the observed functions and estimates the two primary factors that\ncharacterize variation in the registered functions. Each registered function is\nassumed to be predominantly composed of a linear combination of these two\nprimary factors, and the function-specific weights for each observation are\nestimated within the registration model. We show how these estimated weights\ncan easily be used to classify functions after registration using both\nsimulated data and a juggling data set. \n\n"}
{"id": "1502.01115", "contents": "Title: Regression Adjustment for Noncrossing Bayesian Quantile Regression Abstract: A two-stage approach is proposed to overcome the problem in quantile\nregression, where separately fitted curves for several quantiles may cross. The\nstandard Bayesian quantile regression model is applied in the first stage,\nfollowed by a Gaussian process regression adjustment, which monotonizes the\nquantile function whilst borrowing strength from nearby quantiles. The two\nstage approach is computationally efficient, and more general than existing\ntechniques. The method is shown to be competitive with alternative approaches\nvia its performance in simulated examples. \n\n"}
{"id": "1502.02008", "contents": "Title: Stochastic Newton Sampler: R Package sns Abstract: The R package sns implements Stochastic Newton Sampler (SNS), a\nMetropolis-Hastings Monte Carlo Markov Chain algorithm where the proposal\ndensity function is a multivariate Gaussian based on a local, second-order\nTaylor series expansion of log-density. The mean of the proposal function is\nthe full Newton step in Newton-Raphson optimization algorithm. Taking advantage\nof the local, multivariate geometry captured in log-density Hessian allows SNS\nto be more efficient than univariate samplers, approaching independent sampling\nas the density function increasingly resembles a multivariate Gaussian. SNS\nrequires the log-density Hessian to be negative-definite everywhere in order to\nconstruct a valid proposal function. This property holds, or can be easily\nchecked, for many GLM-like models. When initial point is far from density peak,\nrunning SNS in non-stochastic mode by taking the Newton step, augmented with\nwith line search, allows the MCMC chain to converge to high-density areas\nfaster. For high-dimensional problems, partitioning of state space into\nlower-dimensional subsets, and applying SNS to the subsets within a Gibbs\nsampling framework can significantly improve the mixing of SNS chains. In\naddition to the above strategies for improving convergence and mixing, sns\noffers diagnostics and visualization capabilities, as well as a function for\nsample-based calculation of Bayesian predictive posterior distributions. \n\n"}
{"id": "1502.02501", "contents": "Title: A CLT for an improved subspace estimator with observations of increasing\n  dimensions Abstract: This paper deals with subspace estimation in the small sample size regime,\nwhere the number of samples is comparable in magnitude with the observation\ndimension. The traditional estimators, mostly based on the sample correlation\nmatrix, are known to perform well as long as the number of available samples is\nmuch larger than the observation dimension. However, in the small sample size\nregime, the performance degrades. Recently, based on random matrix theory\nresults, a new subspace estimator was introduced, which was shown to be\nconsistent in the asymptotic regime where the number of samples and the\nobservation dimension converge to infinity at the same rate. In practice, this\nestimator outperforms the traditional ones even for certain scenarios where the\nobservation dimension is small and of the same order of magnitude as the number\nof samples. In this paper, we address a performance analysis of this recent\nestimator, by proving a central limit theorem in the above asymptotic regime.\nWe propose an accurate approximation of the mean square error, which can be\nevaluated numerically. \n\n"}
{"id": "1502.02708", "contents": "Title: A comparative review of generalizations of the Gumbel extreme value\n  distribution with an application to wind speed data Abstract: The generalized extreme value distribution and its particular case, the\nGumbel extreme value distribution, are widely applied for extreme value\nanalysis. The Gumbel distribution has certain drawbacks because it is a\nnon-heavy-tailed distribution and is characterized by constant skewness and\nkurtosis. The generalized extreme value distribution is frequently used in this\ncontext because it encompasses the three possible limiting distributions for a\nnormalized maximum of infinite samples of independent and identically\ndistributed observations. However, the generalized extreme value distribution\nmight not be a suitable model when each observed maximum does not come from a\nlarge number of observations. Hence, other forms of generalizations of the\nGumbel distribution might be preferable. Our goal is to collect in the present\nliterature the distributions that contain the Gumbel distribution embedded in\nthem and to identify those that have flexible skewness and kurtosis, are\nheavy-tailed and could be competitive with the generalized extreme value\ndistribution. The generalizations of the Gumbel distribution are described and\ncompared using an application to a wind speed data set and Monte Carlo\nsimulations. We show that some distributions suffer from overparameterization\nand coincide with other generalized Gumbel distributions with a smaller number\nof parameters, i.e., are non-identifiable. Our study suggests that the\ngeneralized extreme value distribution and a mixture of two extreme value\ndistributions should be considered in practical applications. \n\n"}
{"id": "1502.03155", "contents": "Title: A lava attack on the recovery of sums of dense and sparse signals Abstract: Common high-dimensional methods for prediction rely on having either a sparse\nsignal model, a model in which most parameters are zero and there are a small\nnumber of non-zero parameters that are large in magnitude, or a dense signal\nmodel, a model with no large parameters and very many small non-zero\nparameters. We consider a generalization of these two basic models, termed here\na \"sparse+dense\" model, in which the signal is given by the sum of a sparse\nsignal and a dense signal. Such a structure poses problems for traditional\nsparse estimators, such as the lasso, and for traditional dense estimation\nmethods, such as ridge estimation. We propose a new penalization-based method,\ncalled lava, which is computationally efficient. With suitable choices of\npenalty parameters, the proposed method strictly dominates both lasso and\nridge. We derive analytic expressions for the finite-sample risk function of\nthe lava estimator in the Gaussian sequence model. We also provide an deviation\nbound for the prediction risk in the Gaussian regression model with fixed\ndesign. In both cases, we provide Stein's unbiased estimator for lava's\nprediction risk. A simulation example compares the performance of lava to\nlasso, ridge, and elastic net in a regression example using feasible,\ndata-dependent penalty parameters and illustrates lava's improved performance\nrelative to these benchmarks. \n\n"}
{"id": "1502.03278", "contents": "Title: Eta-nuclear interaction: optical vs. coupled channels Abstract: The existence of etamesic nuclei has been speculated for a long time without\nfirm experimental evidence. Much of the effort has taken place in final state\ninteractions on production. One crucial factor in seeing a quasibound state is\nits width, which should be related to the imaginary part of the scattering\nlength. Comparing two models for eta-N scattering giving the same elementary\nscattering length, a simple optical potential and an explicit coupling to the\npionic channel, it is seen that the latter yields a much smaller imaginary part\nfor eta-nucleus scattering. This decrease of absorption may also mean a\npossibility for narrow eta-nuclear states. \n\n"}
{"id": "1502.03853", "contents": "Title: Two Sample Inference for Populations of Graphical Models with\n  Applications to Functional Connectivity Abstract: Gaussian Graphical Models (GGM) are popularly used in neuroimaging studies\nbased on fMRI, EEG or MEG to estimate functional connectivity, or relationships\nbetween remote brain regions. In multi-subject studies, scientists seek to\nidentify the functional brain connections that are different between two groups\nof subjects, i.e. connections present in a diseased group but absent in\ncontrols or vice versa. This amounts to conducting two-sample large scale\ninference over network edges post graphical model selection, a novel problem we\ncall Population Post Selection Inference. Current approaches to this problem\ninclude estimating a network for each subject, and then assuming the subject\nnetworks are fixed, conducting two-sample inference for each edge. These\napproaches, however, fail to account for the variability associated with\nestimating each subject's graph, thus resulting in high numbers of false\npositives and low statistical power. By using resampling and random\npenalization to estimate the post selection variability together with proper\nrandom effects test statistics, we develop a new procedure we call $R^{3}$ that\nsolves these problems. Through simulation studies we show that $R^{3}$ offers\nmajor improvements over current approaches in terms of error control and\nstatistical power. We apply our method to identify functional connections\npresent or absent in autistic subjects using the ABIDE multi-subject fMRI\nstudy. \n\n"}
{"id": "1502.03939", "contents": "Title: Polynomial-Chaos-based Kriging Abstract: Computer simulation has become the standard tool in many engineering fields\nfor designing and optimizing systems, as well as for assessing their\nreliability. To cope with demanding analysis such as optimization and\nreliability, surrogate models (a.k.a meta-models) have been increasingly\ninvestigated in the last decade. Polynomial Chaos Expansions (PCE) and Kriging\nare two popular non-intrusive meta-modelling techniques. PCE surrogates the\ncomputational model with a series of orthonormal polynomials in the input\nvariables where polynomials are chosen in coherency with the probability\ndistributions of those input variables. On the other hand, Kriging assumes that\nthe computer model behaves as a realization of a Gaussian random process whose\nparameters are estimated from the available computer runs, i.e. input vectors\nand response values. These two techniques have been developed more or less in\nparallel so far with little interaction between the researchers in the two\nfields. In this paper, PC-Kriging is derived as a new non-intrusive\nmeta-modeling approach combining PCE and Kriging. A sparse set of orthonormal\npolynomials (PCE) approximates the global behavior of the computational model\nwhereas Kriging manages the local variability of the model output. An adaptive\nalgorithm similar to the least angle regression algorithm determines the\noptimal sparse set of polynomials. PC-Kriging is validated on various benchmark\nanalytical functions which are easy to sample for reference results. From the\nnumerical investigations it is concluded that PC-Kriging performs better than\nor at least as good as the two distinct meta-modeling techniques. A larger gain\nin accuracy is obtained when the experimental design has a limited size, which\nis an asset when dealing with demanding computational models. \n\n"}
{"id": "1502.05087", "contents": "Title: Computation of the O(p^6) order low-energy constants: an update Abstract: We update our original low-energy constants to the O(p6) order, including two\nand three flavours, the normal and anomalous ones. Following a comparative\nanalysis, the O(p4) results are considered better. In the O(p6) order, most of\nour results are consistent or better with those we have found in the\nliterature, although several are worse. \n\n"}
{"id": "1502.06254", "contents": "Title: The fundamental nature of the log loss function Abstract: The standard loss functions used in the literature on probabilistic\nprediction are the log loss function, the Brier loss function, and the\nspherical loss function; however, any computable proper loss function can be\nused for comparison of prediction algorithms. This note shows that the log loss\nfunction is most selective in that any prediction algorithm that is optimal for\na given data sequence (in the sense of the algorithmic theory of randomness)\nunder the log loss function will be optimal under any computable proper mixable\nloss function; on the other hand, there is a data sequence and a prediction\nalgorithm that is optimal for that sequence under either of the two other\nstandard loss functions but not under the log loss function. \n\n"}
{"id": "1502.06929", "contents": "Title: Dark Matter and Gauge Coupling Unification in Non-supersymmetric SO(10)\n  Grand Unified Models Abstract: Unlike minimal SU(5), SO(10) provides a straightforward path towards gauge\ncoupling unification by modifying the renormalization group evolution of the\ngauge couplings above some intermediate scale which may also be related to the\nseesaw mechanism for neutrino masses. Unification can be achieved for several\ndifferent choices of the intermediate gauge group below the SO(10) breaking\nscale. In this work, we consider in detail the possibility that SO(10)\nunification may also provide a natural dark matter candidate, stability being\nguaranteed by a leftover $Z_2$ symmetry. We systematically examine the possible\nintermediate gauge groups which allow a non-degenerate, fermionic, Standard\nModel singlet dark matter candidate while at the same time respecting gauge\ncoupling unification. Our analysis is done at the two-loop level. Surprisingly,\ndespite the richness of SO(10), we find that only two models survive the\nanalysis of phenomenological constraints, which include suitable neutrino\nmasses, proton decay, and reheating. \n\n"}
{"id": "1503.01401", "contents": "Title: Quantifying Uncertainty in Stochastic Models with Parametric Variability Abstract: We present a method to quantify uncertainty in the predictions made by\nsimulations of mathematical models that can be applied to a broad class of\nstochastic, discrete, and differential equation models. Quantifying uncertainty\nis crucial for determining how accurate the model predictions are and\nidentifying which input parameters affect the outputs of interest. Most of the\nexisting methods for uncertainty quantification require many samples to\ngenerate accurate results, are unable to differentiate where the uncertainty is\ncoming from (e.g., parameters or model assumptions), or require a lot of\ncomputational resources. Our approach addresses these challenges and\nopportunities by allowing different types of uncertainty, that is, uncertainty\nin input parameters as well as uncertainty created through stochastic model\ncomponents. This is done by combining the Karhunen-Loeve decomposition,\npolynomial chaos expansion, and Bayesian Gaussian process regression to create\na statistical surrogate for the stochastic model. The surrogate separates the\nanalysis of variation arising through stochastic simulation and variation\narising through uncertainty in the model parameterization. We illustrate our\napproach by quantifying the uncertainty in a stochastic ordinary differential\nequation epidemic model. Specifically, we estimate four quantities of interest\nfor the epidemic model and show agreement between the surrogate and the actual\nmodel results. \n\n"}
{"id": "1503.01789", "contents": "Title: Updated NNLO QCD predictions for the weak radiative B-meson decays Abstract: Weak radiative decays of the B mesons belong to the most important flavor\nchanging processes that provide constraints on physics at the TeV scale. In the\nderivation of such constraints, accurate standard model predictions for the\ninclusive branching ratios play a crucial role. In the current Letter we\npresent an update of these predictions, incorporating all our results for the\nO(alpha_s^2) and lower-order perturbative corrections that have been calculated\nafter 2006. New estimates of nonperturbative effects are taken into account,\ntoo. For the CP- and isospin-averaged branching ratios, we find B_{s gamma} =\n(3.36 +_ 0.23) * 10^-4 and B_{d gamma} = 1.73^{+0.12}_{-0.22} * 10^-5, for\nE_gamma > 1.6GeV. Both results remain in agreement with the current\nexperimental averages. Normalizing their sum to the inclusive semileptonic\nbranching ratio, we obtain R_gamma = ( B_{s gamma} + B_{d gamma})/B_{c l nu} =\n(3.31 +_ 0.22) * 10^-3. A new bound from B_{s gamma} on the charged Higgs boson\nmass in the two-Higgs-doublet-model II reads M_{H^+} > 480 GeV at 95%C.L. \n\n"}
{"id": "1503.03129", "contents": "Title: Mid-rapidity charged hadron transverse spherocity in pp collisions\n  simulated with Pythia Abstract: The pp collisions have been studied for a long time, however, there are still\nsome effects which are not completely understood, such as the long range\nangular correlations and the flow patterns in high multiplicity events, which\nwere recently discovered at the LHC. In a recent work it was demonstrated that\nin Pythia 8, multi-parton interactions and color reconnection can give some of\nthe observed effects similar to the collective flow well known from heavy-ion\ncollisions. Now using the same model, a study based on mid-rapidity charged\nhadron transverse spherocity is presented. The main purpose of this work is to\nshow that a differential study combining multiplicity and event shapes opens\nthe possibility to understand better the features of data, specially at high\nmultiplicity. \n\n"}
{"id": "1503.05041", "contents": "Title: Probing the substructure of $f_0(1370)$ Abstract: Within an effective nonlinear chiral Lagrangian framework the substructure of\n$f_0(1370)$ is studied. The investigation is conducted in the context of a\nglobal picture of scalar mesons in which the importance of the underlying\nconnections among scalar mesons below and above 1 GeV is recognized and\nimplemented. These connections are due to the mixings among various\nquark-antiquarks, four-quarks and glue components and play a central role in\nunderstanding the properties of scalar mesons. Iterative Monte Carlo\nsimulations are first performed on the 14-dimensional parameter space of the\nmodel and sets of points in this parameter space (the global sets) that give an\noverall agreement with all experimental data on mass spectrum, various decay\nwidths and decay ratios of all isosinglet scalar states below 2 GeV are\ndetermined. Then within each global set, subsets that give closest agreement\nfor the properties of $f_0(1370)$ are studied. Unlike the properties of other\nisosinglet states that show a range of variation within each global set, it is\nfound that there is a clear signal for $f_0(1370)$ to be predominantly a\nquark-antiquark state with a substantial $s {\\bar s}$ component, together with\nsmall remnants of four-quark and glue components. \n\n"}
{"id": "1503.08340", "contents": "Title: Statistical Properties of Convex Clustering Abstract: In this manuscript, we study the statistical properties of convex clustering.\nWe establish that convex clustering is closely related to single linkage\nhierarchical clustering and $k$-means clustering. In addition, we derive the\nrange of tuning parameter for convex clustering that yields a non-trivial\nsolution. We also provide an unbiased estimate of the degrees of freedom, and\nprovide a finite sample bound for the prediction error for convex clustering.\nWe compare convex clustering to some traditional clustering methods in\nsimulation studies. \n\n"}
{"id": "1503.08610", "contents": "Title: Change point analysis of second order characteristics in non-stationary\n  time series Abstract: An important assumption in the work on testing for structural breaks in time\nseries consists in the fact that the model is formulated such that the\nstochastic process under the null hypothesis of \"no change-point\" is\nstationary. This assumption is crucial to derive (asymptotic) critical values\nfor the corresponding testing procedures using an elegant and powerful\nmathematical theory, but it might be not very realistic from a practical point\nof view.\n  This paper develops change point analysis under less restrictive assumptions\nand deals with the problem of detecting change points in the marginal variance\nand correlation structures of a non-stationary time series. A CUSUM approach is\nproposed, which is used to test the \"classical\" hypothesis of the form $H_0:\n\\theta_1=\\theta_2$ vs. $H_1: \\theta_1 \\not =\\theta_2$, where $\\theta_1$ and\n$\\theta_2$ denote second order parameters of the process before and after a\nchange point. The asymptotic distribution of the CUSUM test statistic is\nderived under the null hypothesis. This distribution depends in a complicated\nway on the dependency structure of the nonlinear non-stationary time series and\na bootstrap approach is developed to generate critical values. The results are\nthen extended to test the hypothesis of a {\\it non relevant change point}, i.e.\n$H_0: | \\theta_1-\\theta_2 | \\leq \\delta$, which reflects the fact that\ninference should not be changed, if the difference between the parameters\nbefore and after the change-point is small.\n  In contrast to previous work, our approach does neither require the mean to\nbe constant nor - in the case of testing for lag $k$-correlation - that the\nmean, variance and fourth order joint cumulants are constant under the null\nhypothesis. In particular, we allow that the variance has a change point at a\ndifferent location than the auto-covariance. \n\n"}
{"id": "1504.01147", "contents": "Title: On the Population Size Estimation from Dual-record System:\n  Profile-Likelihood Approaches Abstract: Motivated by various applications, we consider the problem of homogeneous\nhuman population size (N) estimation from Dual-record system (DRS)\n(equivalently, two-sample capture-recapture experiment). The likelihood\nestimate from the independent capture-recapture model Mt is widely used in this\ncontext though appropriateness of the behavioral dependence model Mtb is\nunanimously acknowledged. Our primary aim is to investigate the use of several\nrelevant pseudo-likelihood methods profiling N, explicitly for model Mtb. An\nadjustment over profile likelihood is proposed. Simulation studies are carried\nout to evaluate the performance of the proposed method compared with Bayes\nestimate suggested for general capture-recapture experiment by Lee et al.\n(Statistica Sinica, 2003, vol. 13). We also analyse the effect of possible\nmodel mis-specification, due to the use of model Mt, in terms of efficiency and\nrobustness. Finally two real life examples with different characteristics are\npresented for illustration of the methodologies discussed. \n\n"}
{"id": "1504.02995", "contents": "Title: An Overview on the Estimation of Large Covariance and Precision Matrices Abstract: Estimating large covariance and precision matrices are fundamental in modern\nmultivariate analysis. The problems arise from statistical analysis of large\npanel economics and finance data. The covariance matrix reveals marginal\ncorrelations between variables, while the precision matrix encodes conditional\ncorrelations between pairs of variables given the remaining variables. In this\npaper, we provide a selective review of several recent developments on\nestimating large covariance and precision matrices. We focus on two general\napproaches: rank based method and factor model based method. Theories and\napplications of both approaches are presented. These methods are expected to be\nwidely applicable to analysis of economic and financial data. \n\n"}
{"id": "1504.04696", "contents": "Title: On estimation of the diagonal elements of a sparse precision matrix Abstract: In this paper, we present several estimators of the diagonal elements of the\ninverse of the covariance matrix, called precision matrix, of a sample of iid\nrandom vectors. The focus is on high dimensional vectors having a sparse\nprecision matrix. It is now well understood that when the underlying\ndistribution is Gaussian, the columns of the precision matrix can be estimated\nindependently form one another by solving linear regression problems under\nsparsity constraints. This approach leads to a computationally efficient\nstrategy for estimating the precision matrix that starts by estimating the\nregression vectors, then estimates the diagonal entries of the precision matrix\nand, in a final step, combines these estimators for getting estimators of the\noff-diagonal entries. While the step of estimating the regression vector has\nbeen intensively studied over the past decade, the problem of deriving\nstatistically accurate estimators of the diagonal entries has received much\nless attention. The goal of the present paper is to fill this gap by presenting\nfour estimators---that seem the most natural ones---of the diagonal entries of\nthe precision matrix and then performing a comprehensive empirical evaluation\nof these estimators. The estimators under consideration are the residual\nvariance, the relaxed maximum likelihood, the symmetry-enforced maximum\nlikelihood and the penalized maximum likelihood. We show, both theoretically\nand empirically, that when the aforementioned regression vectors are estimated\nwithout error, the symmetry-enforced maximum likelihood estimator has the\nsmallest estimation error. However, in a more realistic setting when the\nregression vector is estimated by a sparsity-favoring computationally efficient\nmethod, the qualities of the estimators become relatively comparable with a\nslight advantage for the residual variance estimator. \n\n"}
{"id": "1504.06155", "contents": "Title: Neutrino Masses and Flavor Oscillations Abstract: This essay is intended to provide a brief description of the peculiar\nproperties of neutrinos within and beyond the standard theory of weak\ninteractions. The focus is on the flavor oscillations of massive neutrinos,\nfrom which one has achieved some striking knowledge about their mass spectrum\nand flavor mixing pattern. The experimental prospects towards probing the\nabsolute neutrino mass scale, possible Majorana nature and CP-violating effects\nwill also be addressed. \n\n"}
{"id": "1504.06601", "contents": "Title: Quark and Glue Components of the Proton Spin from Lattice Calculation Abstract: The status of lattice calculations of the quark spin, the quark orbital\nangular momentum, the glue angular momentum and glue spin in the nucleon is\nsummarized. The quark spin calculation is recently carried out from the\nanomalous Ward identity with chiral fermions and is found to be small mainly\ndue to the large negative anomaly term which is believed to be the source of\nthe `proton spin crisis'. We also present the first calculation of the glue\nspin at finite nucleon momenta. \n\n"}
{"id": "1504.07532", "contents": "Title: Euler-Heisenberg-Weiss action for QCD+QED Abstract: We derive an analytic expression for one-loop effective action of QCD+QED at\nzero and finite temperatures by using the Schwinger's proper time method. The\nresult is a nonlinear effective action not only for electromagnetic and\nchromo-electromagnetic fields but also the Polyakov loop, and thus reproduces\nthe Euler-Heisenberg action in QED, QCD, and QED+QCD, and also the Weiss\npotential for the Polyakov loop at finite temperature. As applications of this\n\"Euler-Heisenberg-Weiss\" action in QCD+QED, we investigate quark pair\nproductions induced by QCD+QED fields at zero temperature and the Polyakov loop\nin the presence of strong electromagnetic fields. Quark one-loop contribution\nto the effective potential of the Polyakov loop explicitly breaks the center\nsymmetry, and is found to be enhanced by the magnetic field, which is\nconsistent with the inverse magnetic catalysis observed in lattice QCD\nsimulation. \n\n"}
{"id": "1504.07715", "contents": "Title: Using Decision Lists to Construct Interpretable and Parsimonious\n  Treatment Regimes Abstract: A treatment regime formalizes personalized medicine as a function from\nindividual patient characteristics to a recommended treatment. A high-quality\ntreatment regime can improve patient outcomes while reducing cost, resource\nconsumption, and treatment burden. Thus, there is tremendous interest in\nestimating treatment regimes from observational and randomized studies.\nHowever, the development of treatment regimes for application in clinical\npractice requires the long-term, joint effort of statisticians and clinical\nscientists. In this collaborative process, the statistician must integrate\nclinical science into the statistical models underlying a treatment regime and\nthe clinician must scrutinize the estimated treatment regime for scientific\nvalidity. To facilitate meaningful information exchange, it is important that\nestimated treatment regimes be interpretable in a subject-matter context. We\npropose a simple, yet flexible class of treatment regimes whose members are\nrepresentable as a short list of if-then statements. Regimes in this class are\nimmediately interpretable and are therefore an appealing choice for broad\napplication in practice. We derive a robust estimator of the optimal regime\nwithin this class and demonstrate its finite sample performance using\nsimulation experiments. The proposed method is illustrated with data from two\nclinical trials. \n\n"}
{"id": "1504.07945", "contents": "Title: hhh Coupling in SUSY models after LHC run I Abstract: We examine the Higgs triple coupling in MSSM and NMSSM under current\nconstraints, which include the LHC measurements, Higgs data, B physics,\nelectroweak precision observables, relic density and so on. The ratio\n$\\lambda^{\\rm MSSM}_{hhh}/\\lambda^{\\rm SM}_{hhh}$ is above 0.97, due to the\nhighly constrained parameter space. While the ratio $\\lambda^{\\rm\nNMSSM}_{hhh}/\\lambda^{\\rm SM}_{hhh}$ can reach 0.1 under current constraints.\nThe precise measurement in future collider will give a tighter constraint to\nthe Higgs triple coupling in MSSM and NMSSM. \n\n"}
{"id": "1504.08270", "contents": "Title: De-Confinement in small systems: Clustering of color sources in high\n  multiplicity $\\bar{p}$p collisions at $\\sqrt{s}$= 1.8 TeV Abstract: It is shown that de-confinement can be achieved in high multiplicity non jet\n$\\bar{p}$p collisions at $\\sqrt{s}$= 1.8 TeV Fermi National Accelerator\nLaboratory(FNAL- E735) experiment. Previously the evidence for de-confinement\nwas the demonstrated by the constant freeze out energy density in high\nmultiplicity events. In this paper we use the same data but analyze the\ntransverse momentum spectrum in the framework of the clustering of color\nsources. The charged particle pseudorapidities densities in the range 7.0 $\\leq\n\\langle dN_{c}/d\\eta \\rangle \\leq$26.0 are considered. Results are presented\nfor both thermodynamic and transport properties. The initial temperature and\nenergy density are obtained and compared with the Lattice Quantum Chromo\nDynamics(LQCD) simulations. The energy density ($\\varepsilon/T^{4}$) $\\sim$\n11.5 for $ \\langle dN_{c}/d\\eta \\rangle \\sim $ 25.0 is close to the value for\n0-10\\% central events in Au+Au collisions at $\\sqrt{s_{NN}}$= 200 GeV. The\nshear viscosity to entropy density ratio($\\eta/s$) is $\\sim$ 0.2 at the\ntransition temperature. The result for the trace anomaly $\\Delta$ is in\nexcellent agreement with LQCD simulations. These results confirm our earlier\nobservation that the de-confined state of matter was created in high\nmultiplicity events in $\\bar{p}$p collisions at $\\sqrt{s}$=1.8 TeV. \n\n"}
{"id": "1505.00404", "contents": "Title: Mild bounds on bigravity from primordial gravitational waves Abstract: If the amplitude of primordial gravitational waves is measured in the\nnear-future, what could it tell us about bigravity? To address this question,\nwe study massive bigravity theories by focusing on a region in parameter space\nwhich is safe from known instabilities. Similarly to investigations on late\ntime constraints, we implicitly assume there is a successful implementation of\nthe Vainshtein mechanism which guarantees that standard cosmological evolution\nis largely unaffected. We find that viable bigravity models are subject to far\nless stringent constraints than massive gravity, where there is only one set of\n(massive) tensor modes. In principle sensitive to the effective graviton mass\nat the time of recombination, we find that in our setup the primordial tensor\nspectrum is more responsive to the dynamics of the massless tensor sector\nrather than its massive counterpart. We further show there are intriguing\nwindows in the parameter space of the theory which could potentially induce\ndistinctive signatures in the B-modes spectrum. \n\n"}
{"id": "1505.01506", "contents": "Title: Same sign di-lepton candles of the composite gluons Abstract: Composite Higgs models, where the Higgs boson is identified with the\npseudo-Nambu-Goldstone-Boson (pNGB) of a strong sector, typically have light\ncomposite fermions (top partners) to account for a light Higgs. This type of\nmodels generically also predicts the existence of heavy vector fields\n(composite gluons) which appear as an octet of QCD. These composite gluons\ngenerically become very broad resonances once phase-space allows them to decay\ninto two composite fermions. This makes their traditional experimental\nsearches, which are designed to look for narrow resonances, quite ineffective.\nIn this paper we, as an alternative, propose to utilize the impact of composite\ngluons on the production of top partners to constrain their parameter space. We\nplace constraints on the parameters of the composite resonances using the 8 TeV\nLHC data and also assess the reach of the 14 TeV LHC. We find that the high\nluminosity LHC will be able to probe composite gluon masses up to $\\sim 6$ TeV,\neven in the broad resonance regime. \n\n"}
{"id": "1505.01793", "contents": "Title: Implications of the observation of dark matter self-interactions for\n  singlet scalar dark matter Abstract: Evidence for dark matter self-interactions has recently been reported based\non the observation of a spatial offset between the dark matter halo and the\nstars in a galaxy in the cluster Abell 3827. Interpreting the offset as due to\ndark matter self-interactions leads to a cross section measurement of\nsigma_DM/m ~ (1-1.5) cm^2/g, where m is the mass of the dark matter particle.\nWe use this observation to constrain singlet scalar dark matter coupled to the\nStandard Model and to two-Higgs-doublet models. We show that the most natural\nscenario in this class of models is very light dark matter, below about 0.1\nGeV, whose relic abundance is set by freeze-in, i.e., by slow production of\ndark matter in the early universe via extremely tiny interactions with the\nHiggs boson, never reaching thermal equilibrium. We also show that the dark\nmatter abundance can be established through the usual thermal freeze-out\nmechanism in the singlet scalar extension of the Yukawa-aligned\ntwo-Higgs-doublet model, but that it requires rather severe fine tuning of the\nsinglet scalar mass. \n\n"}
{"id": "1505.02023", "contents": "Title: Tests for separability in nonparametric covariance operators of random\n  surfaces Abstract: The assumption of separability of the covariance operator for a random image\nor hypersurface can be of substantial use in applications, especially in\nsituations where the accurate estimation of the full covariance structure is\nunfeasible, either for computational reasons, or due to a small sample size.\nHowever, inferential tools to verify this assumption are somewhat lacking in\nhigh-dimensional or functional {data analysis} settings, where this assumption\nis most relevant. We propose here to test separability by focusing on\n$K$-dimensional projections of the difference between the covariance operator\nand a nonparametric separable approximation. The subspace we project onto is\none generated by the eigenfunctions of the covariance operator estimated under\nthe separability hypothesis, negating the need to ever estimate the full\nnon-separable covariance. We show that the rescaled difference of the sample\ncovariance operator with its separable approximation is asymptotically\nGaussian. As a by-product of this result, we derive asymptotically pivotal\ntests under Gaussian assumptions, and propose bootstrap methods for\napproximating the distribution of the test statistics. We probe the finite\nsample performance through simulations studies, and present an application to\nlog-spectrogram images from a phonetic linguistics dataset. \n\n"}
{"id": "1505.04898", "contents": "Title: Heterogeneous Change Point Inference Abstract: We propose HSMUCE (heterogeneous simultaneous multiscale change-point\nestimator) for the detection of multiple change-points of the signal in a\nheterogeneous gaussian regression model. A piecewise constant function is\nestimated by minimizing the number of change-points over the acceptance region\nof a multiscale test which locally adapts to changes in the variance. The\nmultiscale test is a combination of local likelihood ratio tests which are\nproperly calibrated by scale dependent critical values in order to keep a\nglobal nominal level alpha, even for finite samples. We show that HSMUCE\ncontrols the error of over- and underestimation of the number of change-points.\nTo this end, new deviation bounds for F-type statistics are derived. Moreover,\nwe obtain confidence sets for the whole signal. All results are non-asymptotic\nand uniform over a large class of heterogeneous change-point models. HSMUCE is\nfast to compute, achieves the optimal detection rate and estimates the number\nof change-points at almost optimal accuracy for vanishing signals, while still\nbeing robust. We compare HSMUCE with several state of the art methods in\nsimulations and analyse current recordings of a transmembrane protein in the\nbacterial outer membrane with pronounced heterogeneity for its states. An\nR-package is available online. \n\n"}
{"id": "1506.02278", "contents": "Title: Optimal Ridge Detection using Coverage Risk Abstract: We introduce the concept of coverage risk as an error measure for density\nridge estimation. The coverage risk generalizes the mean integrated square\nerror to set estimation. We propose two risk estimators for the coverage risk\nand we show that we can select tuning parameters by minimizing the estimated\nrisk. We study the rate of convergence for coverage risk and prove consistency\nof the risk estimators. We apply our method to three simulated datasets and to\ncosmology data. In all the examples, the proposed method successfully recover\nthe underlying density structure. \n\n"}
{"id": "1506.03456", "contents": "Title: Stringent Dilepton Bounds on Left-Right Models using LHC data Abstract: In canonical left-right symmetric models the lower mass bounds on the charged\ngauge bosons are in the ballpark of $3-4$ TeV, resulting into much stronger\nlimits on the neutral gauge boson $Z_R$, making its production unreachable at\nthe LHC. However, if one evokes different patterns of left-right symmetry\nbreaking the $Z_R$ might be lighter than the $W_R^\\pm$ motivating an\nindependent $Z_R$ collider study. In this work, we use the 8 TeV ATLAS $20.3$\nfb$^{-1}$ luminosity data to derive robust bounds on the $Z_R$ mass using\ndilepton data. %because they provide the most restrictive limits due to the\nsizable $Z_R$-lepton couplings. We find strong lower bounds on the $Z_R$ mass\nfor different right-handed gauge couplings, excluding $Z_R$ masses up to $\\sim\n3.2$TeV. For the canonical LR model we place a lower mass bound of $\\sim\n2.5$TeV. Our findings are almost independent of the right-handed neutrino\nmasses ($\\sim 2\\,\\%$ effect) and applicable to general left-right models. \n\n"}
{"id": "1506.03481", "contents": "Title: On the Asymptotic Efficiency of Approximate Bayesian Computation\n  Estimators Abstract: Many statistical applications involve models for which it is difficult to\nevaluate the likelihood, but from which it is relatively easy to sample.\nApproximate Bayesian computation is a likelihood-free method for implementing\nBayesian inference in such cases. We present results on the asymptotic variance\nof estimators obtained using approximate Bayesian computation in a large-data\nlimit. Our key assumption is that the data are summarized by a\nfixed-dimensional summary statistic that obeys a central limit theorem. We\nprove asymptotic normality of the mean of the approximate Bayesian computation\nposterior. This result also shows that, in terms of asymptotic variance, we\nshould use a summary statistic that is the same dimension as the parameter\nvector, p; and that any summary statistic of higher dimension can be reduced,\nthrough a linear transformation, to dimension p in a way that can only reduce\nthe asymptotic variance of the posterior mean. We look at how the Monte Carlo\nerror of an importance sampling algorithm that samples from the approximate\nBayesian computation posterior affects the accuracy of estimators. We give\nconditions on the importance sampling proposal distribution such that the\nvariance of the estimator will be the same order as that of the maximum\nlikelihood estimator based on the summary statistics used. This suggests an\niterative importance sampling algorithm, which we evaluate empirically on a\nstochastic volatility model. \n\n"}
{"id": "1506.03812", "contents": "Title: Indirect Dark Matter Signatures in the Cosmic Dark Ages II. Ionization,\n  Heating and Photon Production from Arbitrary Energy Injections Abstract: Any injection of electromagnetically interacting particles during the cosmic\ndark ages will lead to increased ionization, heating, production of Lyman-alpha\nphotons and distortions to the energy spectrum of the cosmic microwave\nbackground, with potentially observable consequences. In this note we describe\nnumerical results for the low-energy electrons and photons produced by the\ncooling of particles injected at energies from keV to multi-TeV scales, at\narbitrary injection redshifts (but focusing on the post-recombination epoch).\nWe use these data, combined with existing calculations modeling the cooling of\nthese low-energy particles, to estimate the resulting contributions to\nionization, excitation and heating of the gas, and production of low-energy\nphotons below the threshold for excitation and ionization. We compute corrected\ndeposition-efficiency curves for annihilating dark matter, and demonstrate how\nto compute equivalent curves for arbitrary energy-injection histories. These\ncalculations provide the necessary inputs for the limits on dark matter\nannihilation presented in the accompanying Paper I, but also have potential\napplications in the context of dark matter decay or de-excitation, decay of\nother metastable species, or similar energy injections from new physics. We\nmake our full results publicly available at\nhttp://nebel.rc.fas.harvard.edu/epsilon, to facilitate further independent\nstudies. In particular, we provide the full low-energy electron and photon\nspectra, to allow matching onto more detailed codes that describe the cooling\nof such particles at low energies. \n\n"}
{"id": "1506.04017", "contents": "Title: A Likelihood-Free Reverse Sampler of the Posterior Distribution Abstract: This paper considers properties of an optimization based sampler for\ntargeting the posterior distribution when the likelihood is intractable and\nauxiliary statistics are used to summarize information in the data. Our reverse\nsampler approximates the likelihood-based posterior distribution by solving a\nsequence of simulated minimum distance problems. By a change of variable\nargument, these estimates are reweighted with a prior and the volume of the\njacobian matrix to serve as draws from the desired posterior distribution. The\nsampler provides a conceptual framework to understand the difference between\ntwo types of likelihood free estimation. Because simulated minimum distance\nestimation always results in acceptable draws, the reverse sampler is\npotentially an alternative to existing approximate Bayesian methods that are\ncomputationally demanding because of a low acceptance rate. \n\n"}
{"id": "1506.04430", "contents": "Title: Exact simulation of max-stable processes Abstract: Max-stable processes play an important role as models for spatial extreme\nevents. Their complex structure as the pointwise maximum over an infinite\nnumber of random functions makes simulation highly nontrivial. Algorithms based\non finite approximations that are used in practice are often not exact and\ncomputationally inefficient. We will present two algorithms for exact\nsimulation of a max-stable process at a finite number of locations. The first\nalgorithm generalizes the approach by \\citet{DM-2014} for Brown--Resnick\nprocesses and it is based on simulation from the spectral measure. The second\nalgorithm relies on the idea to simulate only the extremal functions, that is,\nthose functions in the construction of a max-stable process that effectively\ncontribute to the pointwise maximum. We study the complexity of both algorithms\nand prove that the second procedure is always more efficient. Moreover, we\nprovide closed expressions for their implementation that cover the most popular\nmodels for max-stable processes and extreme value copulas. For simulation on\ndense grids, an adaptive design of the second algorithm is proposed. \n\n"}
{"id": "1506.04915", "contents": "Title: Bayesian nonparametric inference for discovery probabilities: credible\n  intervals and large sample asymptotics Abstract: Given a sample of size $n$ from a population of individuals belonging to\ndifferent species with unknown proportions, a popular problem of practical\ninterest consists in making inference on the probability $D_{n}(l)$ that the\n$(n+1)$-th draw coincides with a species with frequency $l$ in the sample, for\nany $l=0,1,\\ldots,n$. This paper contributes to the methodology of Bayesian\nnonparametric inference for $D_{n}(l)$. Specifically, under the general\nframework of Gibbs-type priors we show how to derive credible intervals for a\nBayesian nonparametric estimation of $D_{n}(l)$, and we investigate the large\n$n$ asymptotic behaviour of such an estimator. Of particular interest are\nspecial cases of our results obtained under the specification of the two\nparameter Poisson--Dirichlet prior and the normalized generalized Gamma prior,\nwhich are two of the most commonly used Gibbs-type priors. With respect to\nthese two prior specifications, the proposed results are illustrated through a\nsimulation study and a benchmark Expressed Sequence Tags dataset. To the best\nour knowledge, this illustration provides the first comparative study between\nthe two parameter Poisson--Dirichlet prior and the normalized generalized Gamma\nprior in the context of Bayesian nonparemetric inference for $D_{n}(l)$. \n\n"}
{"id": "1506.04967", "contents": "Title: Parsimonious Mixed Models Abstract: The analysis of experimental data with mixed-effects models requires\ndecisions about the specification of the appropriate random-effects structure.\nRecently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal'\nmodels with all possible random effect components included. Estimation of\nmaximal models, however, may not converge. We show that failure to converge\ntypically is not due to a suboptimal estimation algorithm, but is a consequence\nof attempting to fit a model that is too complex to be properly supported by\nthe data, irrespective of whether estimation is based on maximum likelihood or\non Bayesian hierarchical modeling with uninformative or weakly informative\npriors. Importantly, even under convergence, overparameterization may lead to\nuninterpretable models. We provide diagnostic tools for detecting\noverparameterization and guiding model simplification. \n\n"}
{"id": "1506.06101", "contents": "Title: Robust Bayesian inference via coarsening Abstract: The standard approach to Bayesian inference is based on the assumption that\nthe distribution of the data belongs to the chosen model class. However, even a\nsmall violation of this assumption can have a large impact on the outcome of a\nBayesian procedure. We introduce a simple, coherent approach to Bayesian\ninference that improves robustness to perturbations from the model: rather than\ncondition on the data exactly, one conditions on a neighborhood of the\nempirical distribution. When using neighborhoods based on relative entropy\nestimates, the resulting \"coarsened\" posterior can be approximated by simply\ntempering the likelihood---that is, by raising it to a fractional power---thus,\ninference is often easily implemented with standard methods, and one can even\nobtain analytical solutions when using conjugate priors. Some theoretical\nproperties are derived, and we illustrate the approach with real and simulated\ndata, using mixture models, autoregressive models of unknown order, and\nvariable selection in linear regression. \n\n"}
{"id": "1506.06767", "contents": "Title: Dirac-Fermionic Dark Matter in $U(1)_X$ Models Abstract: We study a number of $U(1)_X$ models featuring a Dirac fermion dark matter\nparticle. We perform a comprehensive analysis which includes the study of\ncorrections to the muon magnetic moment, dilepton searches with LHC data, as\nwell as direct and indirect dark matter detection constraints. We consider four\ndifferent coupling structures, namely $U(1)_{B-L}, U(1)_{d-u},\nU(1)_{universal}$, and $U(1)_{10+\\bar{5}}$, all motivated by compelling\nextensions to the standard model. We outline the viable and excluded regions of\nparameter space using a large set of probes. Our key findings are that (i) the\ncombination of direct detection and collider constraints rule out dark matter\nparticle masses lighter than $\\sim 1$ TeV, unless rather suppressed Z'-fermion\ncouplings exist, and that (ii) for several of the models under consideration,\ncollider constraints rule out Z' masses up to $\\sim 3$ TeV. Lastly, we show\nthat we can accommodate the recent Diboson excess reported by ATLAS\ncollaboration within the $U(1)_{d-u}$ model. \n\n"}
{"id": "1506.07177", "contents": "Title: Detecting underabundant neutralinos Abstract: The electroweak sector may play a crucial role in discovering supersymmetry.\nWe systematically investigate the patterns of the MSSM-like electroweakinos,\nwhen the neutralino relic abundance $\\Omega_\\chi h^2\\leq 0.12$, that is, also\nadmitting for multi-component Dark Matter, in a broad range of the parameter\nspace. We find that for a very large range of parameters the Direct Detection\nexperiments are/will be sensitive to underabundant neutralinos, in spite of the\nstrong rescaling of the flux factor. The second general conclusion is that the\nbound $\\Omega_\\chi h^2\\leq 0.12$ together with the LUX (XENON1T) limits for the\nneutralino spin independent scattering cross sections constrain the\nelectroweakino spectrum so that the mass differences between the NLSP and the\nLSP are smaller than 40 (10) GeV, respectively, with important implications for\nthe collider searches. The future Direct Detection experiments and the high\nluminosity LHC run will probe almost the entire range of the LSP and NLSP mass\nspectrum that is consistent with the bound $\\Omega_\\chi h^2\\leq 0.12$. \n\n"}
{"id": "1506.08910", "contents": "Title: Learning Single Index Models in High Dimensions Abstract: Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nclassification and regression. Response variables are modeled as a nonlinear,\nmonotonic function of a linear combination of features. Estimation in this\ncontext requires learning both the feature weights, and the nonlinear function.\nWhile methods have been described to learn SIMs in the low dimensional regime,\na method that can efficiently learn SIMs in high dimensions has not been\nforthcoming. We propose three variants of a computationally and statistically\nefficient algorithm for SIM inference in high dimensions. We establish excess\nrisk bounds for the proposed algorithms and experimentally validate the\nadvantages that our SIM learning methods provide relative to Generalized Linear\nModel (GLM) and low dimensional SIM based learning methods. \n\n"}
{"id": "1507.00955", "contents": "Title: Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination Abstract: This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%. \n\n"}
{"id": "1507.01584", "contents": "Title: Minimal Left-Right Symmetric Dark Matter Abstract: We show that left-right symmetric models can easily accommodate stable\nTeV-scale dark matter particles without the need for an ad-hoc stabilizing\nsymmetry. The stability of a newly introduced multiplet arises either\naccidentally as in the Minimal Dark Matter framework or comes courtesy of the\nremaining unbroken $\\mathbb{Z}_2$ subgroup of $B-L$. Only one new parameter is\nintroduced: the mass of the new multiplet. As minimal examples we study\nleft-right fermion triplets and quintuplets and show that they can form viable\ntwo-component dark matter. This approach is in particular valid for\n$SU(2)\\times SU(2)\\times U(1)$ models that explain the recent diboson excess at\nATLAS in terms of a new charged gauge boson of mass 2 TeV. \n\n"}
{"id": "1507.02537", "contents": "Title: Modeling asymptotically independent spatial extremes based on Laplace\n  random fields Abstract: We tackle the modeling of threshold exceedances in asymptotically independent\nstochastic processes by constructions based on Laplace random fields. These are\ndefined as Gaussian random fields scaled with a stochastic variable following\nan exponential distribution. This framework yields useful asymptotic properties\nwhile remaining statistically convenient. Univariate distribution tails are of\nthe half exponential type and are part of the limiting generalized Pareto\ndistributions for threshold exceedances. After normalizing marginal tail\ndistributions in data, a standard Laplace field can be used to capture spatial\ndependence among extremes. Asymptotic properties of Laplace fields are explored\nand compared to the classical framework of asymptotic dependence. Multivariate\njoint tail decay rates for Laplace fields are slower than for Gaussian fields\nwith the same covariance structure; hence they provide more conservative\nestimates of very extreme joint risks while maintaining asymptotic\nindependence. Statistical inference is illustrated on extreme wind gusts in the\nNetherlands where a comparison to the Gaussian dependence model shows a better\ngoodness-of-fit in terms of Akaike's criterion. In this specific application we\nfit the well-adapted Weibull distribution as univariate tail model, such that\nthe normalization of univariate tail distributions can be done through a simple\npower transformation of data. \n\n"}
{"id": "1507.02646", "contents": "Title: Pareto Smoothed Importance Sampling Abstract: Importance weighting is a general way to adjust Monte Carlo integration to\naccount for draws from the wrong distribution, but the resulting estimate can\nbe highly variable when the importance ratios have a heavy right tail. This\nroutinely occurs when there are aspects of the target distribution that are not\nwell captured by the approximating distribution, in which case more stable\nestimates can be obtained by modifying extreme importance ratios. We present a\nnew method for stabilizing importance weights using a generalized Pareto\ndistribution fit to the upper tail of the distribution of the simulated\nimportance ratios. The method, which empirically performs better than existing\nmethods for stabilizing importance sampling estimates, includes stabilized\neffective sample size estimates, Monte Carlo error estimates, and convergence\ndiagnostics. The presented Pareto $\\hat{k}$ finite sample convergence rate\ndiagnostic is useful for any Monte Carlo estimator. \n\n"}
{"id": "1507.04398", "contents": "Title: On the use of reproducing kernel Hilbert spaces in functional\n  classification Abstract: The H\\'ajek-Feldman dichotomy establishes that two Gaussian measures are\neither mutually absolutely continuous with respect to each other (and hence\nthere is a Radon-Nikodym density for each measure with respect to the other\none) or mutually singular. Unlike the case of finite dimensional Gaussian\nmeasures, there are non-trivial examples of both situations when dealing with\nGaussian stochastic processes. This paper provides:\n  (a) Explicit expressions for the optimal (Bayes) rule and the minimal\nclassification error probability in several relevant problems of supervised\nbinary classification of mutually absolutely continuous Gaussian processes. The\napproach relies on some classical results in the theory of Reproducing Kernel\nHilbert Spaces (RKHS).\n  (b) An interpretation, in terms of mutual singularity, for the \"near perfect\nclassification\" phenomenon described by Delaigle and Hall (2012). We show that\nthe asymptotically optimal rule proposed by these authors can be identified\nwith the sequence of optimal rules for an approximating sequence of\nclassification problems in the absolutely continuous case.\n  (c) A new model-based method for variable selection in binary classification\nproblems, which arises in a very natural way from the explicit knowledge of the\nRN-derivatives and the underlying RKHS structure. Different classifiers might\nbe used from the selected variables. In particular, the classical, linear\nfinite-dimensional Fisher rule turns out to be consistent under some standard\nconditions on the underlying functional model. \n\n"}
{"id": "1507.05372", "contents": "Title: When interpolation-induced reflection artifact meets time-frequency\n  analysis Abstract: While extracting the temporal dynamical features based on the time-frequency\nanalyses, like the reassignment and synchrosqueezing transform, attracts more\nand more interest in bio-medical data analysis, we should be careful about\nartifacts generated by interpolation schemes, in particular when the sampling\nrate is not significantly higher than the frequency of the oscillatory\ncomponent we are interested in. In this study, we formulate the problem called\nthe reflection effect and provide a theoretical justification of the statement.\nWe also show examples in the anesthetic depth analysis with clear but\nundesirable artifacts. The results show that the artifact associated with the\nreflection effect exists not only theoretically but practically. Its influence\nis pronounced when we apply the time-frequency analyses to extract the\ntime-varying dynamics hidden inside the signal. In conclusion, we have to\ncarefully deal with the artifact associated with the reflection effect by\nchoosing a proper interpolation scheme. \n\n"}
{"id": "1507.08377", "contents": "Title: Large Covariance Estimation through Elliptical Factor Models Abstract: We proposed a general Principal Orthogonal complEment Thresholding (POET)\nframework for large-scale covariance matrix estimation based on an approximate\nfactor model. A set of high level sufficient conditions for the procedure to\nachieve optimal rates of convergence under different matrix norms were brought\nup to better understand how POET works. Such a framework allows us to recover\nthe results for sub-Gaussian in a more transparent way that only depends on the\nconcentration properties of the sample covariance matrix. As a new theoretical\ncontribution, for the first time, such a framework allows us to exploit\nconditional sparsity covariance structure for the heavy-tailed data. In\nparticular, for the elliptical data, we proposed a robust estimator based on\nmarginal and multivariate Kendall's tau to satisfy these conditions. In\naddition, conditional graphical model was also studied under the same\nframework. The technical tools developed in this paper are of general interest\nto high dimensional principal component analysis. Thorough numerical results\nwere also provided to back up the developed theory. \n\n"}
{"id": "1508.00615", "contents": "Title: Bayesian Nonparametric Functional Mixture Estimation for Time-Series\n  Data, With Application to Estimation of State Employment Totals Abstract: The U.S. Bureau of Labor Statistics use monthly, by-state employment totals\nfrom the Current Population Survey (CPS) as a key input to develop employment\nestimates for counties within the states. The monthly CPS by-state totals,\nhowever, express high levels of volatility that compromise the accuracy of\nresulting estimates composed for the counties. Typically-employed models for\nsmall area estimation produce de-noised, state-level employment estimates by\nborrowing information over the survey months, but assume independence among the\ncollection of by-state time series, which is typically violated due to\nsimilarities in their underlying economies. We construct Gaussian process and\nGaussian Markov random field alternative functional prior specifications, each\nin a mixture of multivariate Gaussian distributions with a Dirichlet process\n(DP) mixing measure over the parameters of their covariance or precision\nmatrices. Our DP mixture of functions models allow the data to simultaneously\nestimate a dependence among the months and between states. A feature of our\nmodels is that those functions assigned to the same cluster are drawn from a\ndistribution with the same covariance parameters, so that they are similar, but\ndon't have to be identical. We compare the performances of our two alternatives\non synthetic data and apply them to recover de-noised, by-state CPS employment\ntotals for data from $2000-2013$. \n\n"}
{"id": "1508.04178", "contents": "Title: Confounder Adjustment in Multiple Hypothesis Testing Abstract: We consider large-scale studies in which thousands of significance tests are\nperformed simultaneously. In some of these studies, the multiple testing\nprocedure can be severely biased by latent confounding factors such as batch\neffects and unmeasured covariates that correlate with both primary variable(s)\nof interest (e.g. treatment variable, phenotype) and the outcome. Over the past\ndecade, many statistical methods have been proposed to adjust for the\nconfounders in hypothesis testing. We unify these methods in the same\nframework, generalize them to include multiple primary variables and multiple\nnuisance variables, and analyze their statistical properties. In particular, we\nprovide theoretical guarantees for RUV-4 and LEAPP, which correspond to two\ndifferent identification conditions in the framework: the first requires a set\nof \"negative controls\" that are known a priori to follow the null distribution;\nthe second requires the true non-nulls to be sparse. Two different estimators\nwhich are based on RUV-4 and LEAPP are then applied to these two scenarios. We\nshow that if the confounding factors are strong, the resulting estimators can\nbe asymptotically as powerful as the oracle estimator which observes the latent\nconfounding factors. For hypothesis testing, we show the asymptotic z-tests\nbased on the estimators can control the type I error. Numerical experiments\nshow that the false discovery rate is also controlled by the Benjamini-Hochberg\nprocedure when the sample size is reasonably large. \n\n"}
{"id": "1508.04631", "contents": "Title: Forward jets and large rapidity gaps Abstract: Hadronic jets are extremely abundant at the LHC, and testing QCD in various\ncorners of phase-space is important to understand backgrounds and some specific\nsignatures of new physics. In this article, various measurements aiming at\nprobing QCD in configurations where the theory modeling become challenging are\npresented. Azimuthal angle de-correlations are sensitive to hard as well as\nsoft QCD emission, and in most of the events jets are produced in a\nback-to-back configuration. Events where jets have a large rapidity separation\nare also rare, and those without additional radiation between the jets are\nexponentially suppressed. The modeling of radiation between very forward and\nbackward jets is complicated, and may require theoretical tools different with\nrespect to those normally used for central, high-pt events. Observables can be\ncreated that are sensitive to all these effects, like the study of azimuthal\nangle de-correlations between events where the two leading jets have large\nrapidity separations. The two general-purpose detectors of the LHC have\nmeasured these observables, and for some of them interesting deviations with\nrespect to the most commonly used theoretical models are observed. \n\n"}
{"id": "1508.05880", "contents": "Title: Scalable Bayes via Barycenter in Wasserstein Space Abstract: Divide-and-conquer based methods for Bayesian inference provide a general\napproach for tractable posterior inference when the sample size is large. These\nmethods divide the data into smaller subsets, sample from the posterior\ndistribution of parameters in parallel on all the subsets, and combine\nposterior samples from all the subsets to approximate the full data posterior\ndistribution. The smaller size of any subset compared to the full data implies\nthat posterior sampling on any subset is computationally more efficient than\nsampling from the true posterior distribution. Since the combination step takes\nnegligible time relative to sampling, posterior computations can be scaled to\nmassive data by dividing the full data into a sufficiently large number of data\nsubsets. One such approach relies on the geometry of posterior distributions\nestimated across different subsets and combines them through their barycenter\nin a Wasserstein space of probability measures. We provide theoretical\nguarantees on the accuracy of approximation that are valid in many\napplications. We show that the geometric method approximates the full data\nposterior distribution better than its competitors across diverse simulations\nand reproduces known results when applied to a movie ratings database. \n\n"}
{"id": "1509.00809", "contents": "Title: Weakly-Interacting Massive Particles in Non-supersymmetric SO(10) Grand\n  Unified Models Abstract: Non-supersymmetric SO(10) grand unified theories provide a framework in which\nthe stability of dark matter is explained while gauge coupling unification is\nrealized. In this work, we systematically study this possibility by classifying\nweakly interacting DM candidates in terms of their quantum numbers of\n$\\text{SU}(2)_L \\otimes \\text{U}(1)_Y$, $B-L$, and $\\text{SU}(2)_R$. We\nconsider both scalar and fermion candidates. We show that the requirement of a\nsufficiently high unification scale to ensure a proton lifetime compatible with\nexperimental constraints plays a strong role in selecting viable candidates.\nAmong the scalar candidates originating from either a 16 or 144 of SO(10), only\nSU(2)$_L$ singlets with zero hypercharge or doublets with $Y=1/2$ satisfy all\nconstraints for $\\text{SU}(4)_C \\otimes \\text{SU}(2)_L \\otimes \\text{SU}(2)_R$\nand $\\text{SU}(3)_C \\otimes \\text{SU}(2)_L \\otimes \\text{SU}(2)_R \\otimes\n\\text{U}(1)_{B-L}$ intermediate scale gauge groups. Among fermion triplets with\nzero hypercharge, only a triplet in the 45 with intermediate group\n$\\text{SU}(4)_C \\otimes \\text{SU}(2)_L \\otimes \\text{SU}(2)_R$ leads to\nsolutions with $M_{\\rm GUT} > M_{\\rm int}$ and a long proton lifetime. We find\nthree models with weak doublets and $Y=1/2$ as dark matter candidates for the\n$\\text{SU}(4)_C \\otimes \\text{SU}(2)_L \\otimes \\text{SU}(2)_R$ and\n$\\text{SU}(4)_C \\otimes \\text{SU}(2)_L \\otimes \\text{U}(1)_R$ intermediate\nscale gauge groups assuming a minimal Higgs content. We also discuss how these\nmodels may be tested at accelerators and in dark matter detection experiments. \n\n"}
{"id": "1509.00899", "contents": "Title: A robust approach for estimating change-points in the mean of an AR(p)\n  process Abstract: We consider the problem of change-points estimation in the mean of an AR(p)\nprocess. Taking into account the dependence structure does not allow us to use\nthe approach of the independent case. Especially, the dynamic programming\nalgorithm giving the optimal solution in the independent case cannot be used\nanymore. We propose a two-step method, based on the preliminary robust (to the\nchange-points) estimation of the autoregression parameters. Then, we propose to\nfollow the classical approach, by plugging this estimator in the criterion used\nfor change-point estimation, which is equivalent to decorrelate the series\nusing the estimated autoregression parameters. We show that the asymptotic\nproperties of these change-point location and mean estimators are the same as\nthose of the classical estimators in the independent framework. The same\nplug-in approach is then used to approximate the modified BIC and choose the\nnumber of segments, and to derive a heuristic BIC criterion to select both the\nnumber of changes and the order of the autoregression. Finally, we show, in the\nsimulation section, that for finite sample size taking into account the\ndependence structure improves the statistical performance of the change-point\nestimators and of the selection criterion. \n\n"}
{"id": "1509.01688", "contents": "Title: AIC for Non-concave Penalized Likelihood Method Abstract: Non-concave penalized maximum likelihood methods, such as the Bridge, the\nSCAD, and the MCP, are widely used because they not only do parameter\nestimation and variable selection simultaneously but also have a high\nefficiency as compared to the Lasso. They include a tuning parameter which\ncontrols a penalty level, and several information criteria have been developed\nfor selecting it. While these criteria assure the model selection consistency\nand so have a high value, it is a severe problem that there are no appropriate\nrules to choose the one from a class of information criteria satisfying such a\npreferred asymptotic property. In this paper, we derive an information\ncriterion based on the original definition of the AIC by considering the\nminimization of the prediction error rather than the model selection\nconsistency. Concretely speaking, we derive a function of the score statistic\nwhich is asymptotically equivalent to the non-concave penalized maximum\nlikelihood estimator, and then we provide an asymptotically unbiased estimator\nof the Kullback-Leibler divergence between the true distribution and the\nestimated distribution based on the function. Furthermore, through simulation\nstudies, we check that the performance of the proposed information criterion\ngives almost the same as or better than that of the cross-validation. \n\n"}
{"id": "1509.02452", "contents": "Title: Light Higgs Bosons in NMSSM at the LHC Abstract: The next-to-minimal supersymmetric standard model (NMSSM) with an extended\nHiggs sector offers one of the Higgs boson as the Standard model (SM) like\nHiggs with a mass around 125 GeV along with other Higgs bosons with lighter and\nheavier masses and not excluded by any current experiments. At the LHC,\nphenomenology of these non SM like Higgs bosons is very rich and considerably\ndifferent from the other supersymmetric models. In this work, assuming one of\nthe Higgs bosons to be the SM like, we revisit the mass spectrum and couplings\nof non SM like Higgs bosons taking into consideration all existing constraints\nand identify the relevant region of parameter space. The discovery potential of\nthese non SM like Higgs bosons, apart from their masses, is guided by their\ncouplings with gauge bosons and fermions which are very much parameter space\nsensitive. We evaluate the rates of productions of these non SM like Higgs\nbosons at the LHC for a variety of decay channels in the allowed region of the\nparameter space. Although bb, {\\tau}{\\tau} decay modes appear to be the most\npromising, it is observed that for a substantial region of parameter space the\ntwo-photon decay mode has a remarkably large rate. In this work we emphasize\nthat this diphoton mode can be exploited to find the NMSSM Higgs signal and can\nalso be potential avenue to distinguish the NMSSM from the MSSM. In addition,\nwe discuss briefly the various detectable signals of these non SM Higgs bosons\nat the LHC. \n\n"}
{"id": "1509.04284", "contents": "Title: Towards a No-Lose Theorem for Naturalness Abstract: We derive a phenomenological no-lose theorem for naturalness up to the TeV\nscale, which applies when quantum corrections to the Higgs mass from top quarks\nare canceled by perturbative BSM particles (top partners) of similar\nmultiplicity due to to some symmetry. Null results from LHC searches already\nseem to disfavor such partners if they are colored. Any partners with SM\ncharges and ~TeV masses will be exhaustively probed by the LHC and a future 100\nTeV collider. Therefore, we focus on neutral top partners. While these arise in\nTwin Higgs theories, we analyze neutral top partners as model-independently as\npossible using EFT and Simplified Model methods. We classify all perturbative\nneutral top partner structures in order to compute their irreducible low-energy\nsignatures at proposed future lepton and hadron colliders, as well as the\nirreducible tunings suffered in each scenario. Central to our theorem is the\nassumption that SM-charged BSM states appear in the UV completion of neutral\nnaturalness, which is the case in all known examples. Direct production at the\n100 TeV collider then allows this scale to be probed at the ~10 TeV level. We\nfind that proposed future colliders probe any such scenario of naturalness with\ntuning of 10% or better. This provides very strong model-independent motivation\nfor both new lepton and hadron colliders, which in tandem act as discovery\nmachines for general naturalness. We put our results in context by discussing\nother possibilities for naturalness, including \"swarms\" of top partners,\ninherently non-perturbative or exotic physics, or theories without SM-charged\nstates in the UV completion. Realizing a concrete scenario which avoids our\narguments while still lacking experimental signatures remains an open\nmodel-building challenge. \n\n"}
{"id": "1509.04704", "contents": "Title: Central limit theorems for network driven sampling Abstract: Respondent-Driven Sampling is a popular technique for sampling hidden\npopulations. This paper models Respondent-Driven Sampling as a Markov process\nindexed by a tree. Our main results show that the Volz-Heckathorn estimator is\nasymptotically normal below a critical threshold. The key technical\ndifficulties stem from (i) the dependence between samples and (ii) the tree\nstructure which characterizes the dependence. The theorems allow the growth\nrate of the tree to exceed one and suggest that this growth rate should not be\ntoo large. To illustrate the usefulness of these results beyond their obvious\nuse, an example shows that in certain cases the sample average is preferable to\ninverse probability weighting. We provide a test statistic to distinguish\nbetween these two cases. \n\n"}
{"id": "1509.06313", "contents": "Title: Fermion dark matter from SO(10) Abstract: We construct and analyze nonsupersymmetric SO(10) standard model extensions\nwhich explain dark matter (DM) through the fermionic Higgs portal. In these\nSO(10)-based models the DM particle is naturally stable since a $Z_2$ discrete\nsymmetry, the matter parity, is left at the end of the symmetry breaking chain\nto the standard model. Potentially realistic models contain the $\\bf{10}$ and\n$\\bf{45}$ fermionic representations from which a neutralino-like mass matrix\nwith arbitrary mixings can be obtained. Two different SO(10) breaking chains\nwill be analyzed in light of gauge coupling unification: the standard path\n$\\text{SU}(5)\\times U(1)_{X}$ and the left-right symmetry intermediate chain.\nThe former opens the possibility of a split supersymmetric-like spectrum with\nan additional (inert) scalar doublet, while the later requires additional\nexotic scalar representations associated to the breaking of the left-right\nsymmetry. \n\n"}
{"id": "1509.06428", "contents": "Title: Large-Scale Mode Identification and Data-Driven Sciences Abstract: Bump-hunting or mode identification is a fundamental problem that arises in\nalmost every scientific field of data-driven discovery. Surprisingly, very few\ndata modeling tools are available for automatic (not requiring manual\ncase-by-base investigation), objective (not subjective), and nonparametric (not\nbased on restrictive parametric model assumptions) mode discovery, which can\nscale to large data sets. This article introduces LPMode--an algorithm based on\na new theory for detecting multimodality of a probability density. We apply\nLPMode to answer important research questions arising in various fields from\nenvironmental science, ecology, econometrics, analytical chemistry to astronomy\nand cancer genomics. \n\n"}
{"id": "1509.06866", "contents": "Title: Expectile Asymptotics Abstract: We discuss in detail the asymptotic distribution of sample expectiles. First,\nwe show uniform consistency under the assumption of a finite mean. In case of a\nfinite second moment, we show that for expectiles other then the mean, only the\nadditional assumption of continuity of the distribution function at the\nexpectile implies asymptotic normality, otherwise, the limit is non-normal. For\na continuous distribution function we show the uniform central limit theorem\nfor the expectile process. If, in contrast, the distribution is heavy-tailed,\nand contained in the domain of attraction of a stable law with $1 < \\alpha <\n2$, then we show that the expectile is also asymptotically stable distributed.\nOur findings are illustrated in a simulation section. \n\n"}
{"id": "1509.07894", "contents": "Title: Light Leptonic New Physics at the Precision Frontier Abstract: Precision probes of new physics are often interpreted through their indirect\nsensitivity to short-distance scales. In this proceedings contribution, we\nfocus on the question of which precision observables, at current sensitivity\nlevels, allow for an interpretation via either short-distance new physics or\nconsistent models of long-distance new physics, weakly coupled to the Standard\nModel. The electroweak scale is chosen to set the dividing line between these\nscenarios. In particular, we find that inverse see-saw models of neutrino mass\nallow for light new physics interpretations of most precision leptonic\nobservables, such as lepton universality, lepton flavor violation, but not for\nthe electron EDM. \n\n"}
{"id": "1509.08724", "contents": "Title: The Heterotic Superpotential and Moduli Abstract: We study the four-dimensional effective theory arising from ten-dimensional\nheterotic supergravity compactified on manifolds with torsion. In particular,\ngiven the heterotic superpotential appropriately corrected at\n$\\mathcal{O}(\\alpha')$ to account for the Green-Schwarz anomaly cancellation\nmechanism, we investigate properties of four-dimensional Minkowski vacua of\nthis theory. Considering the restrictions arising from F-terms and D-terms we\nidentify the infinitesimal massless moduli space of the theory. We show that it\nagrees with the results that have recently been obtained from a ten-dimensional\nperspective where supersymmetric Minkowski solutions including the Bianchi\nidentity correspond to an integrable holomorphic structure, with infinitesimal\nmoduli calculated by its first cohomology. As has recently been noted,\ninterplay of complex structure and bundle deformations through holomorphic and\nanomaly constraints can lead to fewer moduli than may have been expected. We\nderive a relation between the number of complex structure and bundle moduli\nremoved from the low energy theory in this way, and give conditions for there\nto be no complex structure moduli or bundle moduli remaining in the low energy\ntheory. The link between Yukawa couplings and obstruction theory is also\nbriefly discussed. \n\n"}
{"id": "1510.01675", "contents": "Title: What's in a ball? Constructing and characterizing uncertainty sets Abstract: In the presence of model risk, it is well-established to replace classical\nexpected values by worst-case expectations over all models within a fixed\nradius from a given reference model. This is the \"robustness\" approach. We show\nthat previous methods for measuring this radius, e.g. relative entropy or\npolynomial divergences, are inadequate for reference models which are\nmoderately heavy-tailed such as lognormal models. Worst cases are either\ninfinitely pessimistic, or they rule out the possibility of fat-tailed \"power\nlaw\" models as plausible alternatives. We introduce a new family of divergence\nmeasures which captures intermediate levels of pessimism. \n\n"}
{"id": "1510.02451", "contents": "Title: The Bouncy Particle Sampler: A Non-Reversible Rejection-Free Markov\n  Chain Monte Carlo Method Abstract: Markov chain Monte Carlo methods have become standard tools in statistics to\nsample from complex probability measures. Many available techniques rely on\ndiscrete-time reversible Markov chains whose transition kernels build up over\nthe Metropolis-Hastings algorithm. We explore and propose several original\nextensions of an alternative approach introduced recently in Peters and de With\n(2012) where the target distribution of interest is explored using a\ncontinuous-time Markov process. In the Metropolis-Hastings algorithm, a trial\nmove to a region of lower target density, equivalently \"higher energy\", than\nthe current state can be rejected with positive probability. In this\nalternative approach, a particle moves along straight lines continuously around\nthe space and, when facing a high energy barrier, it is not rejected but its\npath is modified by bouncing against this barrier. The resulting non-reversible\nMarkov process provides a rejection-free MCMC sampling scheme. We propose\nseveral original techniques to simulate this continuous-time process exactly in\na wide range of scenarios of interest to statisticians. When the target\ndistribution factorizes as a product of factors involving only subsets of\nvariables, such as the posterior distribution associated to a probabilistic\ngraphical model, it is possible to modify the original algorithm to exploit\nthis structure and update in parallel variables within each clique. We present\nseveral extensions by proposing methods to sample mixed discrete-continuous\ndistributions and distributions restricted to a connected smooth domain. We\nalso show that it is possible to move the particle using a general flow instead\nof straight lines. We demonstrate the efficiency of this methodology through\nsimulations on a variety of applications and show that it can outperform Hybrid\nMonte Carlo schemes in interesting scenarios. \n\n"}
{"id": "1510.03516", "contents": "Title: Default Bayesian analysis with global-local shrinkage priors Abstract: We provide a framework for assessing the default nature of a prior\ndistribution using the property of regular variation, which we study for\nglobal-local shrinkage priors. In particular, we demonstrate the horseshoe\npriors, originally designed to handle sparsity, also possess regular variation\nand thus are appropriate for default Bayesian analysis. To illustrate our\nmethodology, we solve a problem of non-informative priors due to Efron (1973),\nwho showed standard flat non-informative priors in high-dimensional normal\nmeans model can be highly informative for nonlinear parameters of interest. We\nconsider four such problems and show global-local shrinkage priors such as the\nhorseshoe and horseshoe+ perform as Efron (1973) requires in each case. We find\nthe reason for this lies in the ability of the global-local shrinkage priors to\nseparate a low-dimensional signal embedded in high-dimensional noise, even for\nnonlinear functions. \n\n"}
{"id": "1510.05248", "contents": "Title: Design of Experiments for Screening Abstract: The aim of this paper is to review methods of designing screening\nexperiments, ranging from designs originally developed for physical experiments\nto those especially tailored to experiments on numerical models. The strengths\nand weaknesses of the various designs for screening variables in numerical\nmodels are discussed. First, classes of factorial designs for experiments to\nestimate main effects and interactions through a linear statistical model are\ndescribed, specifically regular and nonregular fractional factorial designs,\nsupersaturated designs and systematic fractional replicate designs. Generic\nissues of aliasing, bias and cancellation of factorial effects are discussed.\nSecond, group screening experiments are considered including factorial group\nscreening and sequential bifurcation. Third, random sampling plans are\ndiscussed including Latin hypercube sampling and sampling plans to estimate\nelementary effects. Fourth, a variety of modelling methods commonly employed\nwith screening designs are briefly described. Finally, a novel study\ndemonstrates six screening methods on two frequently-used exemplars, and their\nperformances are compared. \n\n"}
{"id": "1510.06759", "contents": "Title: The Effective Strength of Gravity, the Scale of Inflation (and how KK\n  gravitons evade the Higuchi Bound) Abstract: For any given momentum transfer, gravitational interactions have a strength\nset by a characteristic scale $M_*$ inferred from amplitudes calculated in an\neffective theory with a strong coupling scale $M_{**}$. These are in general\ndifferent from each other and $M_{\\rm pl}$, the macroscopic strength of gravity\nas determined by (laboratory scale) Cavendish experiments. During single field\ninflation, $M_*$ can differ from $M_{\\rm pl}$ due to the presence of any number\nof (hidden) universally coupled species between laboratory and inflationary\nscales. Although this has no effect on dimensionless (i.e. observable)\nquantities measured at a fixed scale such as the amplitude and spectral\nproperties of the CMB anisotropies, it complicates the inference of an absolute\nscale of inflation given any detection of primordial tensors. In this note we\nreview and elaborate upon these facts and address concerns raised in a recent\npaper. \n\n"}
{"id": "1510.08283", "contents": "Title: Sobolev spaces with respect to a weighted Gaussian measures in infinite\n  dimensions Abstract: Let $X$ be a separable Banach space endowed with a non-degenerate centered\nGaussian measure $\\mu$ and let $w$ be a positive function on $X$ such that\n$w\\in W^{1,s}(X,\\mu)$ and $\\log w\\in W^{1,t}(X,\\mu)$ for some $s>1$ and $t>s'$.\nIn the present paper we introduce and study Sobolev spaces with respect to the\nweighted Gaussian measure $\\nu:=w\\mu$. We obtain results regarding the\ndivergence operator (i.e. the adjoint in $L^2$ of the gradient operator along\nthe Cameron--Martin space) and the trace of Sobolev functions on hypersurfaces\n$\\{x\\in X\\,|\\, G(x) = 0\\}$, where $G$ is a suitable version of a Sobolev\nfunction. \n\n"}
{"id": "1510.09072", "contents": "Title: Palindromic Bernoulli distributions Abstract: We introduce and study a subclass of joint Bernoulli distributions which has\nthe palindromic property. For such distributions the vector of joint\nprobabilities is unchanged when the order of the elements is reversed. We prove\nfor binary variables that the palindromic property is equivalent to zero\nconstraints on all odd-order interaction parameters, be it in parameterizations\nwhich are log-linear, linear or multivariate logistic. In particular, we derive\nthe one-to-one parametric transformations for these three types of model\nspecifications and give simple closed forms of maximum likelihood estimates.\nSome special cases and a case study are described. \n\n"}
{"id": "1511.00282", "contents": "Title: A New Reduced-Rank Linear Discriminant Analysis Method and Its\n  Applications Abstract: We consider multi-class classification problems for high dimensional data.\nFollowing the idea of reduced-rank linear discriminant analysis (LDA), we\nintroduce a new dimension reduction tool with a flavor of supervised principal\ncomponent analysis (PCA). The proposed method is computationally efficient and\ncan incorporate the correlation structure among the features. Besides the\ntheoretical insights, we show that our method is a competitive classification\ntool by simulated and real data examples. \n\n"}
{"id": "1511.01478", "contents": "Title: Selective inference in regression models with groups of variables Abstract: We provide a general mathematical framework for selective inference with\nsupervised model selection procedures characterized by quadratic forms in the\noutcome variable. Forward stepwise with groups of variables is an important\nspecial case as it allows models with categorical variables or factors. Models\ncan be chosen by AIC, BIC, or a fixed number of steps. We provide an exact\nsignificance test for each group of variables in the selected model based on an\nappropriately truncated $\\chi$ or $F$ distribution for the cases of known and\nunknown $\\sigma^2$ respectively. An efficient software implementation is\navailable as a package in the R statistical programming language. \n\n"}
{"id": "1511.02350", "contents": "Title: Probing the top-quark width through ratios of resonance contributions of\n  $e^+e^-\\rightarrow W^+W^-b\\bar{b}$ Abstract: We exploit offshell regions in the process $e^+e^-\\rightarrow W^+W^-b\\bar{b}$\nto gain access to the top-quark width. Working at next-to-leading order in QCD\nwe show that carefully selected ratios of offshell regions to onshell regions\nin the reconstructed top and antitop invariant mass spectra are,\n\\emph{independently} of the coupling $g_{tbW}$, sensitive to the top-quark\nwidth. We explore this approach for different centre of mass energies and\ninitial-state beam polarisations at $e^+e^-$ colliders and briefly comment on\nthe applicability of this method for a measurement of the top-quark width at\nthe LHC. \n\n"}
{"id": "1511.02524", "contents": "Title: Dark Matter from the vector of SO(10) Abstract: SO(10) grand unified theories can ensure the stability of new particles in\nterms of the gauge group structure itself, and in this respect are well suited\nto accommodate dark matter (DM) candidates in the form of new stable massive\nparticles. We introduce new fermions in two vector 10 representations. When\nSO(10) is broken to the standard model by a minimal 45 + 126 + 10 scalar sector\nwith $SU(3)_C \\otimes SU(2)_L \\otimes SU(2)_R\\otimes U(1)_{B-L}$ as\nintermediate symmetry group, the resulting lightest new states are two Dirac\nfermions corresponding to combinations of the neutral members of the $SU(2)_L$\ndoublets in the 10's, which get splitted in mass by loop corrections involving\n$W_R$. The resulting lighter mass eigenstate is stable, and has only\nnon-diagonal $Z_{L,R}$ neutral current couplings to the heavier neutral state.\nDirect detection searches are evaded if the mass splitting is sufficiently\nlarge to suppress kinematically inelastic light-to-heavy scatterings. By\nrequiring that this condition is satisfied, we obtain the upper limit $M_{W_R}\n\\lesssim 25$ TeV. \n\n"}
{"id": "1511.02524", "contents": "Title: Dark Matter from the vector of SO(10) Abstract: SO(10) grand unified theories can ensure the stability of new particles in\nterms of the gauge group structure itself, and in this respect are well suited\nto accommodate dark matter (DM) candidates in the form of new stable massive\nparticles. We introduce new fermions in two vector 10 representations. When\nSO(10) is broken to the standard model by a minimal 45 + 126 + 10 scalar sector\nwith $SU(3)_C \\otimes SU(2)_L \\otimes SU(2)_R\\otimes U(1)_{B-L}$ as\nintermediate symmetry group, the resulting lightest new states are two Dirac\nfermions corresponding to combinations of the neutral members of the $SU(2)_L$\ndoublets in the 10's, which get splitted in mass by loop corrections involving\n$W_R$. The resulting lighter mass eigenstate is stable, and has only\nnon-diagonal $Z_{L,R}$ neutral current couplings to the heavier neutral state.\nDirect detection searches are evaded if the mass splitting is sufficiently\nlarge to suppress kinematically inelastic light-to-heavy scatterings. By\nrequiring that this condition is satisfied, we obtain the upper limit $M_{W_R}\n\\lesssim 25$ TeV. \n\n"}
{"id": "1511.02552", "contents": "Title: Estimation for bivariate quantile varying coefficient model Abstract: We propose a bivariate quantile regression method for the bivariate varying\ncoefficient model through a directional approach. The varying coefficients are\napproximated by the B-spline basis and an $L_{2}$ type penalty is imposed to\nachieve desired smoothness. We develop a multistage estimation procedure based\nthe Propagation-Separation~(PS) approach to borrow information from nearby\ndirections. The PS method is capable of handling the computational complexity\nraised by simultaneously considering multiple directions to efficiently\nestimate varying coefficients while guaranteeing certain smoothness along\ndirections. We reformulate the optimization problem and solve it by the\nAlternating Direction Method of Multipliers~(ADMM), which is implemented using\nR while the core is written in C to speed it up. Simulation studies are\nconducted to confirm the finite sample performance of our proposed method. A\nreal data on Diffusion Tensor Imaging~(DTI) properties from a clinical study on\nneurodevelopment is analyzed. \n\n"}
{"id": "1511.03212", "contents": "Title: Will protons become gray at 13 TeV and 100 TeV? Abstract: It is shown that the regime of pp-interactions at 7 TeV is a critical one.\nThe LHC data about elastic pp-scattering at 7 and 8 TeV are used to get some\ninformation about both elastic and inelastic profiles of pp-collisions. They\nare discussed in the context of two phenomenological models which pretend to\ndescribe the high energy pp-data with high accuracy. Some predictions following\nfrom these models for LHC energy 13 TeV and for energy 95 TeV of the newly\nproposed collider are discussed. It is claimed that the center of the inelastic\ninteraction region will become less dark with increase of energy albeit very\nslowly. \n\n"}
{"id": "1511.03334", "contents": "Title: Goodness of fit tests for high-dimensional linear models Abstract: In this work we propose a framework for constructing goodness of fit tests in\nboth low and high-dimensional linear models. We advocate applying regression\nmethods to the scaled residuals following either an ordinary least squares or\nLasso fit to the data, and using some proxy for prediction error as the final\ntest statistic. We call this family Residual Prediction (RP) tests. We show\nthat simulation can be used to obtain the critical values for such tests in the\nlow-dimensional setting, and demonstrate using both theoretical results and\nextensive numerical studies that some form of the parametric bootstrap can do\nthe same when the high-dimensional linear model is under consideration. We show\nthat RP tests can be used to test for significance of groups or individual\nvariables as special cases, and here they compare favourably with state of the\nart methods, but we also argue that they can be designed to test for as diverse\nmodel misspecifications as heteroscedasticity and nonlinearity. \n\n"}
{"id": "1511.03722", "contents": "Title: Doubly Robust Off-policy Value Evaluation for Reinforcement Learning Abstract: We study the problem of off-policy value evaluation in reinforcement learning\n(RL), where one aims to estimate the value of a new policy based on data\ncollected by a different policy. This problem is often a critical step when\napplying RL in real-world problems. Despite its importance, existing general\nmethods either have uncontrolled bias or suffer high variance. In this work, we\nextend the doubly robust estimator for bandits to sequential decision-making\nproblems, which gets the best of both worlds: it is guaranteed to be unbiased\nand can have a much lower variance than the popular importance sampling\nestimators. We demonstrate the estimator's accuracy in several benchmark\nproblems, and illustrate its use as a subroutine in safe policy improvement. We\nalso provide theoretical results on the hardness of the problem, and show that\nour estimator can match the lower bound in certain scenarios. \n\n"}
{"id": "1511.04452", "contents": "Title: Signatures from Scalar Dark Matter with a Vector-like Quark Mediator Abstract: We present a comprehensive study of a model where the dark matter is composed\nof a singlet real scalar that couples to the Standard Model predominantly via a\nYukawa interaction with a light quark and a colored vector-like fermion. A\ndistinctive feature of this scenario is that thermal freeze-out in the early\nuniverse may be driven by annihilation both into gluon pairs at one-loop ($gg$)\nand by virtual internal Bremsstrahlung of a gluon ($q \\bar{q} g$). Such a dark\nmatter candidate may also be tested through direct and indirect detection and\nat the LHC; viable candidates have either a mass nearly degenerate with that of\nthe fermionic mediator or a mass above about 2 TeV. \n\n"}
{"id": "1511.08866", "contents": "Title: Selective inference after cross-validation Abstract: This paper describes a method for performing inference on models chosen by\ncross-validation. When the test error being minimized in cross-validation is a\nresidual sum of squares it can be written as a quadratic form. This allows us\nto apply the inference framework in Loftus et al. (2015) for models determined\nby quadratic constraints to the model that minimizes CV test error. Our only\nrequirement on the model training pro- cedure is that its selection events are\nregions satisfying linear or quadratic constraints. This includes both Lasso\nand forward stepwise, which serve as our main examples throughout. We do not\nrequire knowledge of the error variance $\\sigma^2$. The procedures described\nhere are computationally intensive methods of selecting models adaptively and\nperforming inference for the selected model. Implementations are available in\nan R package. \n\n"}
{"id": "1512.00209", "contents": "Title: Equivalence Classes of Staged Trees Abstract: In this paper we give a complete characterization of the statistical\nequivalence classes of CEGs and of staged trees. We are able to show that all\ngraphical representations of the same model share a common polynomial\ndescription. Then, simple transformations on that polynomial enable us to\ntraverse the corresponding class of graphs. We illustrate our results with a\nreal analysis of the implicit dependence relationships within a previously\nstudied dataset. \n\n"}
{"id": "1512.03332", "contents": "Title: Phenomenology of left-right symmetric dark matter Abstract: We present a detailed study of dark matter phenomenology in low-scale\nleft-right symmetric models. Stability of new fermion or scalar multiplets is\nensured by an accidental matter parity that survives the spontaneous symmetry\nbreaking of the gauge group by scalar triplets. The relic abundance of these\nparticles is set by gauge interactions and gives rise to dark matter candidates\nwith masses above the electroweak scale. Dark matter annihilations are thus\nmodified by the Sommerfeld effect, not only in the early Universe, but also\ntoday, for instance, in the Center of the Galaxy. Majorana candidates -\ntriplet, quintuplet, bi-doublet, and bi-triplet - bring only one new parameter\nto the model, their mass, and are hence highly testable at colliders and\nthrough astrophysical observations. Scalar candidates - doublet and 7-plet, the\nlatter being only stable at the renormalizable level - have additional\nscalar-scalar interactions that give rise to rich phenomenology. The particles\nunder discussion share many features with the well-known candidates wino,\nHiggsino, inert doublet scalar, sneutrino, and Minimal Dark Matter. In\nparticular, they all predict a large gamma-ray flux from dark matter\nannihilations, which can be searched for with Cherenkov telescopes. We\nfurthermore discuss models with unequal left-right gauge couplings, $g_R \\neq\ng_L$, taking the recent experimental hints for a charged gauge boson with 2 TeV\nmass as a benchmark point. In this case, the dark matter mass is determined by\nthe observed relic density. \n\n"}
{"id": "1512.04093", "contents": "Title: Multiple Change-point Detection: a Selective Overview Abstract: Very long and noisy sequence data arise from biological sciences to social\nscience including high throughput data in genomics and stock prices in\neconometrics. Often such data are collected in order to identify and understand\nshifts in trend, e.g., from a bull market to a bear market in finance or from a\nnormal number of chromosome copies to an excessive number of chromosome copies\nin genetics. Thus, identifying multiple change points in a long, possibly very\nlong, sequence is an important problem. In this article, we review both\nclassical and new multiple change-point detection strategies. Considering the\nlong history and the extensive literature on the change-point detection, we\nprovide an in-depth discussion on a normal mean change-point model from aspects\nof regression analysis, hypothesis testing, consistency and inference. In\nparticular, we present a strategy to gather and aggregate local information for\nchange-point detection that has become the cornerstone of several emerging\nmethods because of its attractiveness in both computational and theoretical\nproperties. \n\n"}
{"id": "1512.04831", "contents": "Title: Coupling stochastic EM and Approximate Bayesian Computation for\n  parameter inference in state-space models Abstract: We study the class of state-space models and perform maximum likelihood\nestimation for the model parameters. We consider a stochastic approximation\nexpectation-maximization (SAEM) algorithm to maximize the likelihood function\nwith the novelty of using approximate Bayesian computation (ABC) within SAEM.\nThe task is to provide each iteration of SAEM with a filtered state of the\nsystem, and this is achieved using an ABC sampler for the hidden state, based\non sequential Monte Carlo (SMC) methodology. It is shown that the resulting\nSAEM-ABC algorithm can be calibrated to return accurate inference, and in some\nsituations it can outperform a version of SAEM incorporating the bootstrap\nfilter. Two simulation studies are presented, first a nonlinear Gaussian\nstate-space model then a state-space model having dynamics expressed by a\nstochastic differential equation. Comparisons with iterated filtering for\nmaximum likelihood inference, and Gibbs sampling and particle marginal methods\nfor Bayesian inference are presented. \n\n"}
{"id": "1512.06171", "contents": "Title: Regularized Estimation of Piecewise Constant Gaussian Graphical Models:\n  The Group-Fused Graphical Lasso Abstract: The time-evolving precision matrix of a piecewise-constant Gaussian graphical\nmodel encodes the dynamic conditional dependency structure of a multivariate\ntime-series. Traditionally, graphical models are estimated under the assumption\nthat data is drawn identically from a generating distribution. Introducing\nsparsity and sparse-difference inducing priors we relax these assumptions and\npropose a novel regularized M-estimator to jointly estimate both the graph and\nchangepoint structure. The resulting estimator possesses the ability to\ntherefore favor sparse dependency structures and/or smoothly evolving graph\nstructures, as required. Moreover, our approach extends current methods to\nallow estimation of changepoints that are grouped across multiple dependencies\nin a system. An efficient algorithm for estimating structure is proposed. We\nstudy the empirical recovery properties in a synthetic setting. The qualitative\neffect of grouped changepoint estimation is then demonstrated by applying the\nmethod on two real-world data-sets. \n\n"}
{"id": "1512.06497", "contents": "Title: Conical singularities and the Vainshtein screening in full GLPV theories Abstract: In Gleyzes-Langlois-Piazza-Vernizzi (GLPV) theories, it is known that the\nconical singularity arises at the center of a spherically symmetric body\n($r=0$) in the case where the parameter $\\alpha_{{\\rm H}4}$ characterizing the\ndeviation from the Horndeski Lagrangian $L_4$ approaches a non-zero constant as\n$r \\to 0$. We derive spherically symmetric solutions around the center in full\nGLPV theories and show that the GLPV Lagrangian $L_5$ does not modify the\ndivergent property of the Ricci scalar $R$ induced by the non-zero\n$\\alpha_{{\\rm H}4}$. Provided that $\\alpha_{{\\rm H}4}=0$, curvature scalar\nquantities can remain finite at $r=0$ even in the presence of $L_5$ beyond the\nHorndeski domain. For the theories in which the scalar field $\\phi$ is directly\ncoupled to $R$, we also obtain spherically symmetric solutions inside/outside\nthe body to study whether the fifth force mediated by $\\phi$ can be screened by\nnon-linear field self-interactions. We find that there is one specific model of\nGLPV theories in which the effect of $L_5$ vanishes in the equations of motion.\nWe also show that, depending on the sign of a $L_5$-dependent term in the field\nequation, the model can be compatible with solar-system constraints under the\nVainshtein mechanism or it is plagued by the problem of a divergence of the\nfield derivative in high-density regions. \n\n"}
{"id": "1512.07619", "contents": "Title: Uniformly Valid Post-Regularization Confidence Regions for Many\n  Functional Parameters in Z-Estimation Framework Abstract: In this paper we develop procedures to construct simultaneous confidence\nbands for $\\tilde p$ potentially infinite-dimensional parameters after model\nselection for general moment condition models where $\\tilde p$ is potentially\nmuch larger than the sample size of available data, $n$. This allows us to\ncover settings with functional response data where each of the $\\tilde p$\nparameters is a function. The procedure is based on the construction of score\nfunctions that satisfy certain orthogonality condition. The proposed\nsimultaneous confidence bands rely on uniform central limit theorems for\nhigh-dimensional vectors (and not on Donsker arguments as we allow for $\\tilde\np \\gg n$). To construct the bands, we employ a multiplier bootstrap procedure\nwhich is computationally efficient as it only involves resampling the estimated\nscore functions (and does not require resolving the high-dimensional\noptimization problems). We formally apply the general theory to inference on\nregression coefficient process in the distribution regression model with a\nlogistic link, where two implementations are analyzed in detail. Simulations\nand an application to real data are provided to help illustrate the\napplicability of the results. \n\n"}
{"id": "1601.00927", "contents": "Title: Degeneracies in long-baseline neutrino experiments from nonstandard\n  interactions Abstract: We study parameter degeneracies that can occur in long-baseline neutrino\nappearance experiments due to nonstandard interactions (NSI) in neutrino\npropagation. For a single off-diagonal NSI parameter, and neutrino and\nantineutrino measurements at a single L/E, there exists a continuous four-fold\ndegeneracy (related to the mass hierarchy and $\\theta_{23}$ octant) that\nrenders the mass hierarchy, octant, and CP phase unknowable. Even with a\ncombination of NO$\\nu$A and T2K data, which in principle can resolve the\ndegeneracy, both NSI and the CP phase remain unconstrained because of\nexperimental uncertainties. A wide-band beam experiment like DUNE will resolve\nthis degeneracy if the nonzero off-diagonal NSI parameter is $\\epsilon_{e\\mu}$.\nIf $\\epsilon_{e\\tau}$ is nonzero, or the diagonal NSI parameter $\\epsilon_{ee}$\nis O(1), a wrong determination of the mass hierarchy and of CP violation can\noccur at DUNE. The octant degeneracy can be further complicated by\n$\\epsilon_{e\\tau}$, but is not affected by $\\epsilon_{ee}$. \n\n"}
{"id": "1601.01938", "contents": "Title: The gauge-invariant canonical energy-momentum tensor Abstract: The canonical energy-momentum tensor is often considered as a purely academic\nobject because of its gauge dependence. However, it has recently been realized\nthat canonical quantities can in fact be defined in a gauge-invariant way\nprovided that strict locality is abandoned, the non-local aspect being\ndictacted in high-energy physics by the factorization theorems. Using the\ngeneral techniques for the parametrization of non-local parton correlators, we\nprovide for the first time a complete parametrization of the energy-momentum\ntensor (generalizing the purely local parametrizations of Ji and\nBakker-Leader-Trueman used for the kinetic energy-momentum tensor) and identify\nexplicitly the parts accessible from measurable two-parton distribution\nfunctions (TMDs and GPDs). As by-products, we confirm the absence of\nmodel-independent relations between TMDs and parton orbital angular momentum,\nrecover in a much simpler way the Burkardt sum rule and derive three similar\nnew sum rules expressing the conservation of transverse momentum. \n\n"}
{"id": "1601.03704", "contents": "Title: Computationally efficient change point detection for high-dimensional\n  regression Abstract: Large-scale sequential data is often exposed to some degree of inhomogeneity\nin the form of sudden changes in the parameters of the data-generating process.\nWe consider the problem of detecting such structural changes in a\nhigh-dimensional regression setting. We propose a joint estimator of the number\nand the locations of the change points and of the parameters in the\ncorresponding segments. The estimator can be computed using dynamic programming\nor, as we emphasize here, it can be approximated using a binary search\nalgorithm with $O(n \\log(n) \\mathrm{Lasso}(n))$ computational operations while\nstill enjoying essentially the same theoretical properties; here\n$\\mathrm{Lasso}(n)$ denotes the computational cost of computing the Lasso for\nsample size $n$. We establish oracle inequalities for the estimator as well as\nfor its binary search approximation, covering also the case with a large\n(asymptotically growing) number of change points. We evaluate the performance\nof the proposed estimation algorithms on simulated data and apply the\nmethodology to real data. \n\n"}
{"id": "1601.03748", "contents": "Title: Four-loop relation between the $\\bar{\\rm MS}$ and on-shell quark mass Abstract: In this contribution we discuss the four-loop relation between the on-shell\nand $\\bar{\\rm MS}$ definition of heavy quark masses which is applied to the\ntop, bottom and charm case. We also present relations between the $\\bar{\\rm\nMS}$ quark mass and various threshold mass definitions and discuss the\nuncertainty at next-to-next-to-next-to-leading order. \n\n"}
{"id": "1601.04083", "contents": "Title: Observational studies with unknown time of treatment Abstract: Time plays a fundamental role in causal analyses, where the goal is to\nquantify the effect of a specific treatment on future outcomes. In a randomized\nexperiment, times of treatment, and when outcomes are observed, are typically\nwell defined. In an observational study, treatment time marks the point from\nwhich pre-treatment variables must be regarded as outcomes, and it is often\nstraightforward to establish. Motivated by a natural experiment in online\nmarketing, we consider a situation where useful conceptualizations of the\nexperiment behind an observational study of interest lead to uncertainty in the\ndetermination of times at which individual treatments take place. Of interest\nis the causal effect of heavy snowfall in several parts of the country on daily\nmeasures of online searches for batteries, and then purchases. The data\navailable give information on actual snowfall, whereas the natural treatment is\nthe anticipation of heavy snowfall, which is not observed. In this article, we\nintroduce formal assumptions and inference methodology centered around a novel\nnotion of plausible time of treatment. These methods allow us to explicitly\nbound the last plausible time of treatment in observational studies with\nunknown times of treatment, and ultimately yield valid causal estimates in such\nsituations. \n\n"}
{"id": "1601.05630", "contents": "Title: Significance-based community detection in weighted networks Abstract: Community detection is the process of grouping strongly connected nodes in a\nnetwork. Many community detection methods for un-weighted networks have a\ntheoretical basis in a null model. Communities discovered by these methods\ntherefore have interpretations in terms of statistical signficance. In this\npaper, we introduce a null for weighted networks called the continuous\nconfiguration model. We use the model both as a tool for community detection\nand for simulating weighted networks with null nodes. First, we propose a\ncommunity extraction algorithm for weighted networks which incorporates\niterative hypothesis testing under the null. We prove a central limit theorem\nfor edge-weight sums and asymptotic consistency of the algorithm under a\nweighted stochastic block model. We then incorporate the algorithm in a\ncommunity detection method called CCME. To benchmark the method, we provide a\nsimulation framework incorporating the null to plant \"background\" nodes in\nweighted networks with communities. We show that the empirical performance of\nCCME on these simulations is competitive with existing methods, particularly\nwhen overlapping communities and background nodes are present. To further\nvalidate the method, we present two real-world networks with potential\nbackground nodes and analyze them with CCME, yielding results that reveal\nmacro-features of the corresponding systems. \n\n"}
{"id": "1601.06525", "contents": "Title: The collinearly-improved Balitsky-Kovchegov equation Abstract: The high-energy evolution in perturbative QCD suffers from a severe\nlack-of-convergence problem, due to higher order corrections enhanced by double\nand single transverse logarithms. We resum double logarithms to all orders\nwithin the non-linear Balitsky-Kovchegov equation, by taking into account\nsuccessive soft gluon emissions strongly ordered in lifetime. We further resum\nsingle logarithms generated by the first non-singular part of the splitting\nfunctions and by the one-loop running of the coupling. The resummed BK equation\nadmits stable solutions, which are used to successfully fit the HERA data at\nsmall $x$ for physically acceptable initial conditions and reasonable values of\nthe fit parameters. \n\n"}
{"id": "1601.08063", "contents": "Title: Coxeter groups and the PMNS matrix Abstract: We discuss the symmetries of the Lagrangian of the leptonic sector. We\nconsider the case when this symmetry group is a Coxeter group. The number of\nelements of the PMNS matrix predicted by this group structure would depend on\nthe number of generators of this group. We analyze finite Coxeter groups with 2\nto 4 generators and even finite subgroups of infinite Coxeter groups with 4\ngenerators and show which of them can give results that are consistent with\nexperimental data. \n\n"}
{"id": "1602.00714", "contents": "Title: Hidden SU(N) Glueball Dark Matter Abstract: We investigate the possibility that the dark matter candidate is from a pure\nnon-abelian gauge theory of the hidden sector, motivated in large part by its\nelegance and simplicity. The dark matter is the lightest bound state made of\nthe confined gauge fields, the hidden glueball. We point out this simple setup\nis capable of providing rich and novel phenomena in the dark sector, especially\nin the parameter space of large N. They include self-interacting and warm dark\nmatter scenarios, Bose-Einstein condensation leading to massive dark stars\npossibly millions of times heavier than our sun giving rise to gravitational\nlensing effects, and indirect detections through higher dimensional operators\nas well as interesting collider signatures. \n\n"}
{"id": "1602.02466", "contents": "Title: Overfitting hidden Markov models with an unknown number of states Abstract: This paper presents new theory and methodology for the Bayesian estimation of\noverfitted hidden Markov models, with finite state space. The goal is then to\nachieve posterior emptying of extra states. A prior configuration is\nconstructed which favours configurations where the hidden Markov chain remains\nergodic although it empties out some of the states. Asymptotic posterior\nconvergence rates are proven theoretically, and demonstrated with a large\nsample simulation. The problem of overfitted HMMs is then considered in the\ncontext of smaller sample sizes, and due to computational and mixing issues two\nalternative prior structures are studied, one commonly used in practice, and a\nmixture of the two priors. The Prior Parallel Tempering approach of van Havre\n(2015) is also extended to HMMs to allow MCMC estimation of the complex\nposterior space. A replicate simulation study and an in-depth exploration is\nperformed to compare the three priors with hyperparameters chosen according to\nthe asymptotic constraints alongside less informative alternatives. \n\n"}
{"id": "1602.02889", "contents": "Title: Ergodicity of Markov chain Monte Carlo with reversible proposal Abstract: We describe ergodic properties of some Metropolis-Hastings (MH) algorithms\nfor heavy-tailed target distributions. The analysis usually falls into\nsub-geometric ergodicity framework but we prove that the mixed preconditioned\nCrank-Nicolson (MpCN) algorithm has geometric ergodicity even for heavy-tailed\ntarget distributions. This useful property comes from the fact that the MpCN\nalgorithm becomes a random-walk Metropolis algorithm under suitable\ntransformation. \n\n"}
{"id": "1602.04805", "contents": "Title: DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression Abstract: Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems. \n\n"}
{"id": "1602.05125", "contents": "Title: Locally Stationary Functional Time Series Abstract: The literature on time series of functional data has focused on processes of\nwhich the probabilistic law is either constant over time or constant up to its\nsecond-order structure. Especially for long stretches of data it is desirable\nto be able to weaken this assumption. This paper introduces a framework that\nwill enable meaningful statistical inference of functional data of which the\ndynamics change over time. We put forward the concept of local stationarity in\nthe functional setting and establish a class of processes that have a\nfunctional time-varying spectral representation. Subsequently, we derive\nconditions that allow for fundamental results from nonstationary multivariate\ntime series to carry over to the function space. In particular, time-varying\nfunctional ARMA processes are investigated and shown to be functional locally\nstationary according to the proposed definition. As a side-result, we establish\na Cram\\'er representation for an important class of weakly stationary\nfunctional processes. Important in our context is the notion of a time-varying\nspectral density operator of which the properties are studied and uniqueness is\nderived. Finally, we provide a consistent nonparametric estimator of this\noperator and show it is asymptotically Gaussian using a weaker tightness\ncriterion than what is usually deemed necessary. \n\n"}
{"id": "1602.05455", "contents": "Title: Heterogeneity Adjustment with Applications to Graphical Model Inference Abstract: Heterogeneity is an unwanted variation when analyzing aggregated datasets\nfrom multiple sources. Though different methods have been proposed for\nheterogeneity adjustment, no systematic theory exists to justify these methods.\nIn this work, we propose a generic framework named ALPHA (short for Adaptive\nLow-rank Principal Heterogeneity Adjustment) to model, estimate, and adjust\nheterogeneity from the original data. Once the heterogeneity is adjusted, we\nare able to remove the biases of batch effects and to enhance the inferential\npower by aggregating the homogeneous residuals from multiple sources. Under a\npervasive assumption that the latent heterogeneity factors simultaneously\naffect a large fraction of observed variables, we provide a rigorous theory to\njustify the proposed framework. Our framework also allows the incorporation of\ninformative covariates and appeals to the \"Bless of Dimensionality\". As an\nillustrative application of this generic framework, we consider a problem of\nestimating high-dimensional precision matrix for graphical model inference\nbased on multiple datasets. We also provide thorough numerical studies on both\nsynthetic datasets and a brain imaging dataset to demonstrate the efficacy of\nthe developed theory and methods. \n\n"}
{"id": "1602.06957", "contents": "Title: Fully-Automated Precision Predictions for Heavy Neutrino Production\n  Mechanisms at Hadron Colliders Abstract: Motivated by TeV-scale neutrino mass models, we propose a systematic\ntreatment of heavy neutrino $(N)$ production at hadron colliders. Our simple\nand efficient modeling of the vector boson fusion (VBF) $W^\\pm\\gamma\\rightarrow\nN\\ell^\\pm$ and $N\\ell^\\pm+nj$ signal definitions resolve collinear and soft\ndivergences that have plagued past studies, and is applicable to other\ncolor-singlet processes, e.g., associated Higgs $(W^\\pm h)$, sparticle\n$(\\tilde{\\ell}^\\pm\\tilde{\\nu_\\ell})$, and charged Higgs $(h^{\\pm\\pm}h^{\\mp})$\nproduction. We present, for the first time, a comparison of all leading $N$\nproduction modes, including both gluon fusion (GF) $gg\\rightarrow\nZ^*/h^*\\rightarrow N\\overset{(-)}{\\nu_\\ell}$ and VBF. We obtain fully\ndifferential results up to next-to-leading order (NLO) in QCD accuracy using a\nMonte Carlo tool chain linking FeynRules, N{\\small LO}CT, and\nMadGraph5\\_aMC@NLO. Associated model files are publicly available. At the 14\nTeV LHC, the leading order GF rate is small and comparable to the NLO\n$N\\ell^\\pm+1j$ rate; at a future 100 TeV Very Large Hadron Collider, GF\ndominates for $m_N={300-1500}$ GeV, beyond which VBF takes lead. \n\n"}
{"id": "1602.08062", "contents": "Title: Probabilistic community detection with unknown number of communities Abstract: A fundamental problem in network analysis is clustering the nodes into groups\nwhich share a similar connectivity pattern. Existing algorithms for community\ndetection assume the knowledge of the number of clusters or estimate it a\npriori using various selection criteria and subsequently estimate the community\nstructure. Ignoring the uncertainty in the first stage may lead to erroneous\nclustering, particularly when the community structure is vague. We instead\npropose a coherent probabilistic framework for simultaneous estimation of the\nnumber of communities and the community structure, adapting recently developed\nBayesian nonparametric techniques to network models. An efficient Markov chain\nMonte Carlo (MCMC) algorithm is proposed which obviates the need to perform\nreversible jump MCMC on the number of clusters. The methodology is shown to\noutperform recently developed community detection algorithms in a variety of\nsynthetic data examples and in benchmark real-datasets. Using an appropriate\nmetric on the space of all configurations, we develop non-asymptotic Bayes risk\nbounds even when the number of clusters is unknown. Enroute, we develop\nconcentration properties of non-linear functions of Bernoulli random variables,\nwhich may be of independent interest. \n\n"}
{"id": "1603.00444", "contents": "Title: On divergences tests for composite hypotheses under composite likelihood Abstract: It is well-known that in some situations it is not easy to compute the\nlikelihood function as the datasets might be large or the model is too complex.\nIn that contexts composite likelihood, derived by multiplying the likelihoods\nof subjects of the variables, may be useful. The extension of the classical\nlikelihood ratio test statistics to the framework of composite likelihoods is\nused as a procedure to solve the problem of testing in the context of composite\nlikelihood. In this paper we introduce and study a new family of test\nstatistics for composite likelihood: Composite {\\phi}-divergence test\nstatistics for solving the problem of testing a simple null hypothesis or a\ncomposite null hypothesis. To do that we introduce and study the asymptotic\ndistribution of the restricted maximum composite likelihood estimate. \n\n"}
{"id": "1603.01606", "contents": "Title: Exploring high-mass diphoton resonance without new colored states Abstract: A new heavy resonance may be observable at the LHC if it has a significant\ndecay branching fraction into a pair of photons. We entertain this possibility\nby looking at the modest excess in the diphoton invariant mass spectrum around\n750 GeV recently reported in the ATLAS and CMS experiments. Assuming that it is\na spinless boson, dubbed $\\tilde s$, we consider it within a model containing\ntwo weak scalar doublets having zero vacuum expectation values and a scalar\nsinglet in addition to the doublet responsible for breaking the electroweak\nsymmetry. The model also possesses three Dirac neutral singlet fermions, the\nlightest one of which can play the role of dark matter and which participate\nwith the new doublet scalars in generating light neutrino masses radiatively.\nWe show that the model is consistent with all phenomenological constraints and\ncan yield a production cross section\n$\\sigma(pp\\rightarrow\\tilde{s}\\rightarrow\\gamma\\gamma)$ of roughly the desired\nsize, mainly via the photon-fusion contribution, without involving extra\ncolored fermions or bosons. We also discuss other major decay modes of $\\tilde\ns$ which are potentially testable in upcoming LHC measurements. \n\n"}
{"id": "1603.01775", "contents": "Title: Combined Analysis of Amplitude and Phase Variations in Functional Data Abstract: When functional data manifest amplitude and phase variations, a\ncommonly-employed framework for analyzing them is to take away the phase\nvariation through a function alignment and then to apply standard tools to the\naligned functions. A downside of this approach is that the important variations\ncontained in the phases are completely ignored. To combine both of amplitude\nand phase variations, we propose a variant of principal component analysis\n(PCA) that captures non-linear components representing the amplitude, phase and\ntheir associations simultaneously. The proposed method, which we call\nfunctional combined PCA, is aimed to provide more efficient dimension reduction\nwith interpretable components, in particular when the amplitudes and phases are\nclearly associated. We model principal components by non-linearly combining\ntime-warping functions and aligned functions. A data-adaptive weighting\nprocedure helps our dimension reduction to attain a maximal explaining power of\nobserved functions. We also discuss an application of functional canonical\ncorrelation analysis in investigation of the correlation structure between the\ntwo variations. We show that for two sets of real data the proposed method\nprovides interpretable major non-linear components, which are not typically\nfound in the usual functional PCA. \n\n"}
{"id": "1603.03568", "contents": "Title: Threshold Corrections to Dimension-six Proton Decay Operators in\n  Non-minimal SUSY SU(5) GUTs Abstract: We calculate the high and low scale threshold corrections to the D=6 proton\ndecay mode in supersymmetric SU(5) grand unified theories with\nhigher-dimensional representation Higgs multiplets. In particular, we focus on\na missing-partner model in which the grand unified group is spontaneously\nbroken by the 75-dimensional Higgs multiplet and the doublet-triplet splitting\nproblem is solved. We find that in the missing-partner model the D=6 proton\ndecay rate gets suppressed by about 60%, mainly due to the threshold effect at\nthe GUT scale, while the SUSY-scale threshold corrections are found to be less\nprominent when sfermions are heavy. \n\n"}
{"id": "1603.06045", "contents": "Title: Non-standard conditionally specified models for non-ignorable missing\n  data Abstract: Data analyses typically rely upon assumptions about missingness mechanisms\nthat lead to observed versus missing data. When the data are missing not at\nrandom, direct assumptions about the missingness mechanism, and indirect\nassumptions about the distributions of observed and missing data, are typically\nuntestable. We explore an approach, where the joint distribution of observed\ndata and missing data is specified through non-standard conditional\ndistributions. In this formulation, which traces back to a factorization of the\njoint distribution, apparently proposed by J.W. Tukey, the modeling assumptions\nabout the conditional factors are either testable or are designed to allow the\nincorporation of substantive knowledge about the problem at hand, thereby\noffering a possibly realistic portrayal of the data, both missing and observed.\nWe apply Tukey's conditional representation to exponential family models, and\nwe propose a computationally tractable inferential strategy for this class of\nmodels. We illustrate the utility of this approach using high-throughput\nbiological data with missing data that are not missing at random. \n\n"}
{"id": "1603.08470", "contents": "Title: Dark matter and neutrino masses from a classically scale-invariant\n  multi-Higgs portal Abstract: We present a classically scale-invariant model where the dark matter,\nneutrino and electroweak mass scales are dynamically generated from\ndimensionless couplings. The Standard Model gauge sector is extended by a dark\n$SU(2)_X$ gauge symmetry that is completely broken through a complex scalar\ndoublet via the Coleman-Weinberg mechanism. The three resulting dark vector\nbosons of equal mass are stable and can play the role of dark matter. We also\nincorporate right-handed neutrinos which are coupled to a real singlet scalar\nthat communicates with the other scalars through portal interactions. The\nmulti-Higgs sector is analyzed by imposing theoretical and experimental\nconstraints. We compute the dark matter relic abundance and study the\npossibility of the direct detection of the dark matter candidate from XENON 1T. \n\n"}
{"id": "1604.00729", "contents": "Title: The neutrino floor at ultra-low threshold Abstract: By lowering their energy threshold direct dark matter searches can reach the\nneutrino floor with experimental technology now in development. The 7Be flux\ncan be detected with $\\sim 10$ eV nuclear recoil energy threshold and 50 kg-yr\nexposure. The pep flux can be detected with $\\sim 3$ ton-yr exposure, and the\nfirst detection of the CNO flux is possible with similar exposure. The pp flux\ncan be detected with threshold of $\\sim$ eV and only $\\sim$ kg-yr exposure.\nThese can be the first pure neutral current measurements of the low-energy\nsolar neutrino flux. Measuring this flux is important for low mass dark matter\nsearches and for understanding the solar interior. \n\n"}
{"id": "1604.01255", "contents": "Title: QCD Axion as a Bridge Between String Theory and Flavor Physics Abstract: We construct a string-inspired model, motivated by the flavored Peccei-Quinn\n(PQ) axions, as a useful bridge between flavor physics and string theory. The\nkey feature is two anomalous gauged $U(1)$ symmetries, responsible for both the\nfermion mass hierarchy problem of the standard model and the strong CP problem,\nthat combine string theory with flavor physics and severely constrain the form\nof the F- and D-term contributions to the potential. In the context of\nsupersymmetric moduli stabilization we stabilize the size moduli with positive\nmasses while leaving two axions massless and one axion massive. We demonstrate\nthat, while the massive gauge bosons eat the two axionic degrees of freedom,\ntwo axionic directions survive to low energies as the flavored PQ axions. \n\n"}
{"id": "1604.02652", "contents": "Title: Hypergraphs in the characterization of regular vine copula structures Abstract: Vine copulas constitute a flexible way for modeling of dependences using only\npair copulas as building blocks. The pair-copula constructions introduced by\nJoe (1997) are able to encode more types of dependences in the same time since\nthey can be expressed as a product of different types of bi-variate copulas.\nThe Regular-vine structures (R-vines), as pair copulas corresponding to a\nsequence of trees, have been introduced by Bedford and Cooke (2001, 2002) and\nfurther explored by Kurowicka and Cooke (2006). The complexity of these models\nstrongly increases in larger dimensions. Therefore the so called truncated\nR-vines were introduced in Brechmann et al. (2012). In this paper we express\nthe Regular-vines using a special type of hypergraphs, which encodes the\nconditional independences. \n\n"}
{"id": "1604.03192", "contents": "Title: Scalar-on-Image Regression via the Soft-Thresholded Gaussian Process Abstract: The focus of this work is on spatial variable selection for scalar-on-image\nregression. We propose a new class of Bayesian nonparametric models,\nsoft-thresholded Gaussian processes and develop the efficient posterior\ncomputation algorithms. Theoretically, soft-thresholded Gaussian processes\nprovide large prior support for the spatially varying coefficients that enjoy\npiecewise smoothness, sparsity and continuity, characterizing the important\nfeatures of imaging data. Also, under some mild regularity conditions, the\nsoft-thresholded Gaussian process leads to the posterior consistency for both\nparameter estimation and variable selection for scalar-on-image regression,\neven when the number of true predictors is larger than the sample size. The\nproposed method is illustrated via simulations, compared numerically with\nexisting alternatives and applied to Electroencephalography (EEG) study of\nalcoholism. \n\n"}
{"id": "1604.06310", "contents": "Title: Inference on covariance operators via concentration inequalities:\n  k-sample tests, classification, and clustering via Rademacher complexities Abstract: We propose a novel approach to the analysis of covariance operators making\nuse of concentration inequalities. First, non-asymptotic confidence sets are\nconstructed for such operators. Then, subsequent applications including a k\nsample test for equality of covariance, a functional data classifier, and an\nexpectation-maximization style clustering algorithm are derived and tested on\nboth simulated and phoneme data. \n\n"}
{"id": "1604.06467", "contents": "Title: Gauge theories of Partial Compositeness: Scenarios for Run-II of the LHC Abstract: We continue our investigation of gauge theories in which the Higgs boson\narises as a pseudo-Nambu-Goldstone boson (pNGB) and top-partners arise as bound\nstates of three hyperfermions. All models have additional pNGBs in their\nspectrum that should be accessible at LHC. We analyze the patterns of symmetry\nbreaking and present all relevant couplings of the pNGBs with the gauge fields.\nWe discuss how vacuum misalignment and a mass for the pNGBs is generated by a\nloop-induced potential. Finally, we paint a very broad, qualitative, picture of\nthe kind of experimental signatures these models give rise to, setting the\nstage for further analysis. \n\n"}
{"id": "1604.07031", "contents": "Title: Predictive Hierarchical Clustering: Learning clusters of CPT codes for\n  improving surgical outcomes Abstract: We develop a novel algorithm, Predictive Hierarchical Clustering (PHC), for\nagglomerative hierarchical clustering of current procedural terminology (CPT)\ncodes. Our predictive hierarchical clustering aims to cluster subgroups, not\nindividual observations, found within our data, such that the clusters\ndiscovered result in optimal performance of a classification model. Therefore,\nmerges are chosen based on a Bayesian hypothesis test, which chooses pairings\nof the subgroups that result in the best model fit, as measured by held out\npredictive likelihoods. We place a Dirichlet prior on the probability of\nmerging clusters, allowing us to adjust the size and sparsity of clusters. The\nmotivation is to predict patient-specific surgical outcomes using data from ACS\nNSQIP (American College of Surgeon's National Surgical Quality Improvement\nProgram). An important predictor of surgical outcomes is the actual surgical\nprocedure performed as described by a CPT code. We use PHC to cluster CPT\ncodes, represented as subgroups, together in a way that enables us to better\npredict patient-specific outcomes compared to currently used clusters based on\nclinical judgment. \n\n"}
{"id": "1604.07087", "contents": "Title: Optimal Estimation of Slope Vector in High-dimensional Linear\n  Transformation Model Abstract: In a linear transformation model, there exists an unknown monotone nonlinear\ntransformation function such that the transformed response variable and the\npredictor variables satisfy a linear regression model. In this paper, we\npresent CENet, a new method for estimating the slope vector and simultaneously\nperforming variable selection in the high-dimensional sparse linear\ntransformation model. CENet is the solution to a convex optimization problem\nand can be computed efficiently from an algorithm with guaranteed convergence\nto the global optimum. We show that under a pairwise elliptical distribution\nassumption on each predictor-transformed-response pair and some regularity\nconditions, CENet attains the same optimal rate of convergence as the best\nregression method in the high-dimensional sparse linear regression model. To\nthe best of our limited knowledge, this is the first such result in the\nliterature. We demonstrate the empirical performance of CENet on both simulated\nand real datasets. We also discuss the connection of CENet with some nonlinear\nregression/multivariate methods proposed in the literature. \n\n"}
{"id": "1604.07708", "contents": "Title: Angular analysis of B -> J/psi K1 : towards a model independent\n  determination of the photon polarization with B-> K1 gamma Abstract: We propose a model independent extraction of the hadronic information needed\nto determine the photon polarization of the b-> s gamma process by the method\nutilizing the B -> K1 gamma -> K pi pi gamma angular distribution. We show that\nexactly the same hadronic information can be obtained by using the B -> J/psi\nK1 -> J/psi K pi pi channel, which leads to a much higher precision. \n\n"}
{"id": "1604.08320", "contents": "Title: Sequential Bayesian optimal experimental design via approximate dynamic\n  programming Abstract: The design of multiple experiments is commonly undertaken via suboptimal\nstrategies, such as batch (open-loop) design that omits feedback or greedy\n(myopic) design that does not account for future effects. This paper introduces\nnew strategies for the optimal design of sequential experiments. First, we\nrigorously formulate the general sequential optimal experimental design (sOED)\nproblem as a dynamic program. Batch and greedy designs are shown to result from\nspecial cases of this formulation. We then focus on sOED for parameter\ninference, adopting a Bayesian formulation with an information theoretic design\nobjective. To make the problem tractable, we develop new numerical approaches\nfor nonlinear design with continuous parameter, design, and observation spaces.\nWe approximate the optimal policy by using backward induction with regression\nto construct and refine value function approximations in the dynamic program.\nThe proposed algorithm iteratively generates trajectories via exploration and\nexploitation to improve approximation accuracy in frequently visited regions of\nthe state space. Numerical results are verified against analytical solutions in\na linear-Gaussian setting. Advantages over batch and greedy design are then\ndemonstrated on a nonlinear source inversion problem where we seek an optimal\npolicy for sequential sensing. \n\n"}
{"id": "1605.00353", "contents": "Title: Rate-Optimal Perturbation Bounds for Singular Subspaces with\n  Applications to High-Dimensional Statistics Abstract: Perturbation bounds for singular spaces, in particular Wedin's $\\sin \\Theta$\ntheorem, are a fundamental tool in many fields including high-dimensional\nstatistics, machine learning, and applied mathematics. In this paper, we\nestablish separate perturbation bounds, measured in both spectral and Frobenius\n$\\sin \\Theta$ distances, for the left and right singular subspaces. Lower\nbounds, which show that the individual perturbation bounds are rate-optimal,\nare also given.\n  The new perturbation bounds are applicable to a wide range of problems. In\nthis paper, we consider in detail applications to low-rank matrix denoising and\nsingular space estimation, high-dimensional clustering, and canonical\ncorrelation analysis (CCA). In particular, separate matching upper and lower\nbounds are obtained for estimating the left and right singular spaces. To the\nbest of our knowledge, this is the first result that gives different optimal\nrates for the left and right singular spaces under the same perturbation. In\naddition to these problems, applications to other high-dimensional problems\nsuch as community detection in bipartite networks, multidimensional scaling,\nand cross-covariance matrix estimation are also discussed. \n\n"}
{"id": "1605.00660", "contents": "Title: Operator Calculus for Information Field Theory Abstract: Signal inference problems with non-Gaussian posteriors can be hard to tackle.\nThrough using the concept of Gibbs free energy these posteriors are rephrased\nas Gaussian posteriors for the price of computing various expectation values\nwith respect to a Gaussian distribution. We present a new way of translating\nthese expectation values to a language of operators which is similar to that in\nquantum mechanics. This simplifies many calculations, for instance such\ninvolving log-normal priors. The operator calculus is illustrated by deriving a\nnovel self-calibrating algorithm which is tested with mock data. \n\n"}
{"id": "1605.01103", "contents": "Title: Laser Interferometers as Dark Matter Detectors Abstract: While global cosmological and local galactic abundance of dark matter is well\nestablished, its identity, physical size and composition remain a mystery. In\nthis paper, we analyze an important question of dark matter detectability\nthrough its gravitational interaction, using current and next generation\ngravitational-wave observatories to look for macroscopic (kilogram-scale or\nlarger) objects. Keeping the size of the dark matter objects to be smaller than\nthe physical dimensions of the detectors, and keeping their mass as free\nparameters, we derive the expected event rates. For favorable choice of mass,\nwe find that dark matter interactions could be detected in space-based\ndetectors such as LISA at a rate of one per ten years. We then assume the\nexistence of an additional Yukawa force between dark matter and regular matter.\nBy choosing the range of the force to be comparable to the size of the\ndetectors, we derive the levels of sensitivity to such a new force, which\nexceeds the sensitivity of other probes in a wide range of parameters. For\nsufficiently large Yukawa coupling strength, the rate of dark matter events can\nthen exceed 10 per year for both ground- and space-based detectors. Thus,\ngravitational-wave observatories can make an important contribution to a global\neffort of searching for non-gravitational interactions of dark matter. \n\n"}
{"id": "1605.01214", "contents": "Title: Factor Models for Asset Returns Based on Transformed Factors Abstract: The Fama-French three factor models are commonly used in the description of\nasset returns in finance. Statistically speaking, the Fama-French three factor\nmodels imply that the return of an asset can be accounted for directly by the\nFama-French three factors, i.e. market, size and value factor, through a linear\nfunction. A natural question is: would some kind of transformed Fama-French\nthree factors work better than the three factors? If so, what kind of\ntransformation should be imposed on each factor in order to make the\ntransformed three factors better account for asset returns? In this paper, we\nare going to address these questions through nonparametric modelling. We\npropose a data driven approach to construct the transformation for each factor\nconcerned. A generalised maximum likelihood ratio based hypothesis test is also\nproposed to test whether transformations on the Fama-French three factors are\nneeded for a given data set. Asymptotic properties are established to justify\nthe proposed methods. Intensive simulation studies are conducted to show how\nthe proposed methods work when sample size is finite. Finally, we apply the\nproposed methods to a real data set, which leads to some interesting findings. \n\n"}
{"id": "1605.01485", "contents": "Title: Matrix-Variate Regressions and Envelope Models Abstract: Modern technology often generates data with complex structures in which both\nresponse and explanatory variables are matrix-valued. Existing methods in the\nliterature are able to tackle matrix-valued predictors but are rather limited\nfor matrix-valued responses. In this article, we study matrix-variate\nregressions for such data, where the response Y on each experimental unit is a\nrandom matrix and the predictor X can be either a scalar, a vector, or a\nmatrix, treated as non-stochastic in terms of the conditional distribution Y|X.\nWe propose models for matrix-variate regressions and then develop envelope\nextensions of these models. Under the envelope framework, redundant variation\ncan be eliminated in estimation and the number of parameters can be notably\nreduced when the matrix-variate dimension is large, possibly resulting in\nsignificant gains in efficiency. The proposed methods are applicable to high\ndimensional settings. \n\n"}
{"id": "1605.03010", "contents": "Title: Near-threshold production of heavy quarks with QQbar_threshold Abstract: We describe the QQbar_threshold library for computing the production cross\nsection of heavy quark-antiquark pairs near threshold at electron-positron\ncolliders. The prediction includes all presently known QCD, electroweak, Higgs,\nand nonresonant corrections in the combined nonrelativistic and weak-coupling\nexpansion. \n\n"}
{"id": "1605.04260", "contents": "Title: Unquenching the meson spectrum: a model study of excited $\\rho$\n  resonances Abstract: Quark models taking into account the dynamical effects of hadronic decay\noften produce very different predictions for mass shifts in the hadron\nspectrum. The consequences for meson spectroscopy can be dramatic and\ncompletely obscure the underlying confining force. Recent unquenched lattice\ncalculations of mesonic resonances that also include meson-meson interpolators\nprovide a touchstone for such models, despite the present limitations in\napplicability. On the experimental side, the $\\rho(770)$ meson and its several\nobserved radial recurrences are a fertile testing ground for both quark models\nand lattice computations. Here we apply a unitarised quark model that has been\nsuccessful in the description of many enigmatic mesons to these vector $\\rho$\nresonances and the corresponding $P$-wave $\\pi\\pi$ phase shifts. This work is\nin progress, with encouraging preliminary results. \n\n"}
{"id": "1605.04565", "contents": "Title: Hierarchical Models for Independence Structures of Networks Abstract: We introduce a new family of network models, called hierarchical network\nmodels, that allow us to represent in an explicit manner the stochastic\ndependence among the dyads (random ties) of the network. In particular, each\nmember of this family can be associated with a graphical model defining\nconditional independence clauses among the dyads of the network, called the\ndependency graph. Every network model with dyadic independence assumption can\nbe generalized to construct members of this new family. Using this new\nframework, we generalize the Erd\\\"os-R\\'enyi and beta-models to create\nhierarchical Erd\\\"os-R\\'enyi and beta-models. We describe various methods for\nparameter estimation as well as simulation studies for models with sparse\ndependency graphs. \n\n"}
{"id": "1605.07244", "contents": "Title: Optimal Estimation of Co-heritability in High-dimensional Linear Models Abstract: Co-heritability is an important concept that characterizes the genetic\nassociations within pairs of quantitative traits. There has been significant\nrecent interest in estimating the co-heritability based on data from the\ngenome-wide association studies (GWAS). This paper introduces two measures of\nco-heritability in the high-dimensional linear model framework, including the\ninner product of the two regression vectors and a normalized inner product by\ntheir lengths. Functional de-biased estimators (FDEs) are developed to estimate\nthese two co-heritability measures. In addition, estimators of quadratic\nfunctionals of the regression vectors are proposed. Both theoretical and\nnumerical properties of the estimators are investigated. In particular, minimax\nrates of convergence are established and the proposed estimators of the inner\nproduct, the quadratic functionals and the normalized inner product are shown\nto be rate-optimal. Simulation results show that the FDEs significantly\noutperform the naive plug-in estimates. The FDEs are also applied to analyze a\nyeast segregant data set with multiple traits to estimate heritability and\nco-heritability among the traits. \n\n"}
{"id": "1605.08933", "contents": "Title: Interaction Pursuit with Feature Screening and Selection Abstract: Understanding how features interact with each other is of paramount\nimportance in many scientific discoveries and contemporary applications. Yet\ninteraction identification becomes challenging even for a moderate number of\ncovariates. In this paper, we suggest an efficient and flexible procedure,\ncalled the interaction pursuit (IP), for interaction identification in\nultra-high dimensions. The suggested method first reduces the number of\ninteractions and main effects to a moderate scale by a new feature screening\napproach, and then selects important interactions and main effects in the\nreduced feature space using regularization methods. Compared to existing\napproaches, our method screens interactions separately from main effects and\nthus can be more effective in interaction screening. Under a fairly general\nframework, we establish that for both interactions and main effects, the method\nenjoys the sure screening property in screening and oracle inequalities in\nselection. Our method and theoretical results are supported by several\nsimulation and real data examples. \n\n"}
{"id": "1605.09045", "contents": "Title: Cosmic microwave background polarization in Noncommutative space-time Abstract: In the standard model of cosmology (SMC) the B-mode polarization of the CMB\ncan be explained by the gravitational effects in the inflation epoch. However,\nthis is not the only way to explain the B-mode polarization for the CMB. It can\nbe shown that the Compton scattering in presence of a background besides\ngenerating a circularly polarized microwave, can leads to a B-mode polarization\nfor the CMB. Here we consider the non-commutative (NC) space time as a\nbackground to explore the CMB polarization at the last scattering surface. We\nobtain the B-mode spectrum of the CMB radiation by scalar perturbation of\nmetric via a correction on the Compton scattering in NC-space-time in terms of\nthe circular polarization power spectrum and the non-commutative energy scale.\nIt can be shown that even for the NC-scale as large as $10TeV$ the NC-effects\non the CMB polarization and the r-parameter is significant. We show that the\nV-mode power spectrum can be obtained in terms of linearly polarized power\nspectrum in the range Micro to Nano-Kelvin squared for the NC-scale about\n$1TeV$ to $10TeV$, respectively. \n\n"}
{"id": "1606.00033", "contents": "Title: Generalized Pseudolikelihood Methods for Inverse Covariance Estimation Abstract: We introduce PseudoNet, a new pseudolikelihood-based estimator of the inverse\ncovariance matrix, that has a number of useful statistical and computational\nproperties. We show, through detailed experiments with synthetic and also\nreal-world finance as well as wind power data, that PseudoNet outperforms\nrelated methods in terms of estimation error and support recovery, making it\nwell-suited for use in a downstream application, where obtaining low estimation\nerror can be important. We also show, under regularity conditions, that\nPseudoNet is consistent. Our proof assumes the existence of accurate estimates\nof the diagonal entries of the underlying inverse covariance matrix; we\nadditionally provide a two-step method to obtain these estimates, even in a\nhigh-dimensional setting, going beyond the proofs for related methods. Unlike\nother pseudolikelihood-based methods, we also show that PseudoNet does not\nsaturate, i.e., in high dimensions, there is no hard limit on the number of\nnonzero entries in the PseudoNet estimate. We present a fast algorithm as well\nas screening rules that make computing the PseudoNet estimate over a range of\ntuning parameters tractable. \n\n"}
{"id": "1606.00709", "contents": "Title: f-GAN: Training Generative Neural Samplers using Variational Divergence\n  Minimization Abstract: Generative neural samplers are probabilistic models that implement sampling\nusing feedforward neural networks: they take a random input vector and produce\na sample from a probability distribution defined by the network weights. These\nmodels are expressive and allow efficient computation of samples and\nderivatives, but cannot be used for computing likelihoods or for\nmarginalization. The generative-adversarial training method allows to train\nsuch models through the use of an auxiliary discriminative neural network. We\nshow that the generative-adversarial approach is a special case of an existing\nmore general variational divergence estimation approach. We show that any\nf-divergence can be used for training generative neural samplers. We discuss\nthe benefits of various choices of divergence functions on training complexity\nand the quality of the obtained generative models. \n\n"}
{"id": "1606.02931", "contents": "Title: Bayesian Estimation and Comparison of Moment Condition Models Abstract: In this paper we consider the problem of inference in statistical models\ncharacterized by moment restrictions by casting the problem within the\nExponentially Tilted Empirical Likelihood (ETEL) framework. Because the ETEL\nfunction has a well defined probabilistic interpretation and plays the role of\na nonparametric likelihood, a fully Bayesian semiparametric framework can be\ndeveloped. We establish a number of powerful results surrounding the Bayesian\nETEL framework in such models. One major concern driving our work is the\npossibility of misspecification. To accommodate this possibility, we show how\nthe moment conditions can be reexpressed in terms of additional nuisance\nparameters and that, even under misspecification, the Bayesian ETEL posterior\ndistribution satisfies a Bernstein-von Mises result. A second key contribution\nof the paper is the development of a framework based on marginal likelihoods\nand Bayes factors to compare models defined by different moment conditions.\nComputation of the marginal likelihoods is by the method of Chib (1995) as\nextended to Metropolis-Hastings samplers in Chib and Jeliazkov (2001). We\nestablish the model selection consistency of the marginal likelihood and show\nthat the marginal likelihood favors the model with the minimum number of\nparameters and the maximum number of valid moment restrictions. When the models\nare misspecified, the marginal likelihood model selection procedure selects the\nmodel that is closer to the (unknown) true data generating process in terms of\nthe Kullback-Leibler divergence. The ideas and results in this paper provide a\nfurther broadening of the theoretical underpinning and value of the Bayesian\nETEL framework with likely far-reaching practical consequences. The discussion\nis illuminated through several examples. \n\n"}
{"id": "1606.03940", "contents": "Title: High-dimensional simultaneous inference with the bootstrap Abstract: We propose a residual and wild bootstrap methodology for individual and\nsimultaneous inference in high-dimensional linear models with possibly\nnon-Gaussian and heteroscedastic errors. We establish asymptotic consistency\nfor simultaneous inference for parameters in groups $G$, where $p \\gg n$, $s_0\n= o(n^{1/2}/\\{\\log(p) \\log(|G|)^{1/2}\\})$ and $\\log(|G|) = o(n^{1/7})$, with\n$p$ the number of variables, $n$ the sample size and $s_0$ denoting the\nsparsity. The theory is complemented by many empirical results. Our proposed\nprocedures are implemented in the R-package hdi. \n\n"}
{"id": "1606.05129", "contents": "Title: Search for heavy long-lived charged $R$-hadrons with the ATLAS detector\n  in 3.2 fb$^{-1}$ of proton--proton collision data at $\\sqrt{s} = 13$ TeV Abstract: A search for heavy long-lived charged $R$-hadrons is reported using a data\nsample corresponding to 3.2$^{-1}$ of proton--proton collisions at $\\sqrt{s} =\n13$ TeV collected by the ATLAS experiment at the Large Hadron Collider at CERN.\nThe search is based on observables related to large ionisation losses and slow\npropagation velocities, which are signatures of heavy charged particles\ntravelling significantly slower than the speed of light. No significant\ndeviations from the expected background are observed. Upper limits at 95%\nconfidence level are provided on the production cross section of long-lived\n$R$-hadrons in the mass range from 600 GeV to 2000 GeV and gluino, bottom and\ntop squark masses are excluded up to 1580 GeV, 805 GeV and 890 GeV,\nrespectively. \n\n"}
{"id": "1606.05892", "contents": "Title: Bayesian design of experiments for generalised linear models and\n  dimensional analysis with industrial and scientific application Abstract: The design of an experiment can be always be considered at least implicitly\nBayesian, with prior knowledge used informally to aid decisions such as the\nvariables to be studied and the choice of a plausible relationship between the\nexplanatory variables and measured responses. Bayesian methods allow\nuncertainty in these decisions to be incorporated into design selection through\nprior distributions that encapsulate information available from scientific\nknowledge or previous experimentation. Further, a design may be explicitly\ntailored to the aim of the experiment through a decision-theoretic approach\nusing an appropriate loss function. We review the area of decision-theoretic\nBayesian design, with particular emphasis on recent advances in computational\nmethods. For many problems arising in industry and science, experiments result\nin a discrete response that is well described by a member of the class of\ngeneralised linear models. We describe how Gaussian process emulation, commonly\nused in computer experiments, can play an important role in facilitating\nBayesian design for realistic problems. A main focus is the combination of\nGaussian process regression to approximate the expected loss with cyclic\ndescent (coordinate exchange) optimisation algorithms to allow optimal designs\nto be found for previously infeasible problems. We also present the first\noptimal design results for statistical models formed from dimensional analysis,\na methodology widely employed in the engineering and physical sciences to\nproduce parsimonious and interpretable models. Using the famous paper\nhelicopter experiment, we show the potential for the combination of Bayesian\ndesign, generalised linear models and dimensional analysis to produce small but\ninformative experiments. \n\n"}
{"id": "1606.06246", "contents": "Title: High-dimensional changepoint estimation via sparse projection Abstract: Changepoints are a very common feature of Big Data that arrive in the form of\na data stream. In this paper, we study high-dimensional time series in which,\nat certain time points, the mean structure changes in a sparse subset of the\ncoordinates. The challenge is to borrow strength across the coordinates in\norder to detect smaller changes than could be observed in any individual\ncomponent series. We propose a two-stage procedure called `inspect' for\nestimation of the changepoints: first, we argue that a good projection\ndirection can be obtained as the leading left singular vector of the matrix\nthat solves a convex optimisation problem derived from the CUSUM transformation\nof the time series. We then apply an existing univariate changepoint estimation\nalgorithm to the projected series. Our theory provides strong guarantees on\nboth the number of estimated changepoints and the rates of convergence of their\nlocations, and our numerical studies validate its highly competitive empirical\nperformance for a wide range of data generating mechanisms. Software\nimplementing the methodology is available in the R package\n`InspectChangepoint'. \n\n"}
{"id": "1606.06696", "contents": "Title: Resurgence of $Z'$ from the single electron-muon event at ATLAS Abstract: Inspired by the recent single $e^{\\pm}\\mu^{\\mp}$ event at 2.1 TeV invariant\nmass from the ATLAS at $\\sqrt{s}=13$ TeV with 3.2 fb$^{-1}$ luminosity, we\npropose an explanation using a $Z'$ gauge boson, which possesses\nlepton-flavor-changing neutral currents originated from non-universal couplings\nto charged leptons. We assume that the left-handed charged-lepton mixing matrix\nequals to the PMNS matrix and no mixing in the neutrino sector to make this\nphenomenological $Z'$ model more predictive. There are indeed some parameter\nregions, where the $Z'$ can generate a large enough $e^{\\pm}\\mu^{\\mp}$\nproduction cross section, while at the same time satisfies various observables\nfrom lepton-flavor violation and other constraints from the LHC. \n\n"}
{"id": "1606.06746", "contents": "Title: Approximate Recovery in Changepoint Problems, from $\\ell_2$ Estimation\n  Error Rates Abstract: In the 1-dimensional multiple changepoint detection problem, we prove that\nany procedure with a fast enough $\\ell_2$ error rate, in terms of its\nestimation of the underlying piecewise constant mean vector, automatically has\nan (approximate) changepoint screening property---specifically, each true jump\nin the underlying mean vector has an estimated jump nearby. We also show, again\nassuming only knowledge of the $\\ell_2$ error rate, that a simple\npost-processing step can be used to eliminate spurious estimated changepoints,\nand thus delivers an (approximate) changepoint recovery\nproperty---specifically, in addition to the screening property described above,\nwe are assured that each estimated jump has a true jump nearby. As a special\ncase, we focus on the application of these results to the 1-dimensional fused\nlasso, i.e., 1-dimensional total variation denoising, and compare the\nimplications with existing results from the literature. We also study\nextensions to related problems, such as changepoint detection over graphs. \n\n"}
{"id": "1606.08400", "contents": "Title: Robust and rate-optimal Gibbs posterior inference on the boundary of a\n  noisy image Abstract: Detection of an image boundary when the pixel intensities are measured with\nnoise is an important problem in image segmentation, with numerous applications\nin medical imaging and engineering. From a statistical point of view, the\nchallenge is that likelihood-based methods require modeling the pixel\nintensities inside and outside the image boundary, even though these are\ntypically of no practical interest. Since misspecification of the pixel\nintensity models can negatively affect inference on the image boundary, it\nwould be desirable to avoid this modeling step altogether. Towards this, we\ndevelop a robust Gibbs approach that constructs a posterior distribution for\nthe image boundary directly, without modeling the pixel intensities. We prove\nthat, for a suitable prior on the image boundary, the Gibbs posterior\nconcentrates asymptotically at the minimax optimal rate, adaptive to the\nboundary smoothness. Monte Carlo computation of the Gibbs posterior is\nstraightforward, and simulation experiments show that the corresponding\ninference is more accurate than that based on existing Bayesian methodology. \n\n"}
{"id": "1606.08912", "contents": "Title: Studying the P_c(4450) resonance in J/psi photoproduction off protons Abstract: A resonance-like structure, the P_c(4450), has recently been observed in the\nJ/psi p spectrum by the LHCb collaboration. We discuss the feasibility of\ndetecting this structure in J/psi photoproduction in the CLAS12 experiment at\nJLab. We present a first estimate of the upper limit for the branching ratio of\nthe P_c(4450) to J/psi p. Our estimates, which take into account the\nexperimental resolution effects, predict that it will be possible to observe a\nsizable cross section close to the J/psi production threshold and shed light on\nthe P_c(4450) resonance in the future photoproduction measurements. \n\n"}
{"id": "1606.09391", "contents": "Title: A study of the process $e^+ +e^- \\to e^+ +e^- p \\bar {p}$ by the\n  two-photon mechanism $\\gamma \\gamma \\to p \\bar {p}$ at high energies Abstract: In this paper we consider the exclusive production of proton-antiproton pairs\nin the interaction between two quasireal photons in $e^+e^-$ collision. The\ndifferential and total cross section of the process $\\gamma \\gamma \\to p\\bar\n{p}$ at a beam energy of photons from 2.1 GeV to 4.5 GeV in the center-of-mass\nand for different values of $|cos\\theta^{\\ast}|$ is calculated. At energy\n$<\\sqrt {s_{e^+e^-}}>$=197 \\,\\,GeV the total cross section process o0f the $e^+\n+e^- \\to e^+ +e^- +p +\\bar {p}$ is calculated by the two-photon mechanism. The\nresults are in satisfactory agreement with the experimental data. \n\n"}
{"id": "1607.00743", "contents": "Title: A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank\n  Designs Abstract: We study the residual bootstrap (RB) method in the context of\nhigh-dimensional linear regression. Specifically, we analyze the distributional\napproximation of linear contrasts $c^{\\top} (\\hat{\\beta}_{\\rho}-\\beta)$, where\n$\\hat{\\beta}_{\\rho}$ is a ridge-regression estimator. When regression\ncoefficients are estimated via least squares, classical results show that RB\nconsistently approximates the laws of contrasts, provided that $p\\ll n$, where\nthe design matrix is of size $n\\times p$. Up to now, relatively little work has\nconsidered how additional structure in the linear model may extend the validity\nof RB to the setting where $p/n\\asymp 1$. In this setting, we propose a version\nof RB that resamples residuals obtained from ridge regression. Our main\nstructural assumption on the design matrix is that it is nearly low rank --- in\nthe sense that its singular values decay according to a power-law profile.\nUnder a few extra technical assumptions, we derive a simple criterion for\nensuring that RB consistently approximates the law of a given contrast. We then\nspecialize this result to study confidence intervals for mean response values\n$X_i^{\\top} \\beta$, where $X_i^{\\top}$ is the $i$th row of the design. More\nprecisely, we show that conditionally on a Gaussian design with near low-rank\nstructure, RB simultaneously approximates all of the laws\n$X_i^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, $i=1,\\dots,n$. This result is also\nnotable as it imposes no sparsity assumptions on $\\beta$. Furthermore, since\nour consistency results are formulated in terms of the Mallows (Kantorovich)\nmetric, the existence of a limiting distribution is not required. \n\n"}
{"id": "1607.01192", "contents": "Title: Bounded Influence Propagation {\\tau}-Estimation: A New Robust Method for\n  ARMA Model Estimation Abstract: A new robust and statistically efficient estimator for ARMA models called the\nbounded influence propagation (BIP) {\\tau}-estimator is proposed. The estimator\nincorporates an auxiliary model, which prevents the propagation of outliers.\nStrong consistency and asymptotic normality of the estimator for ARMA models\nthat are driven by independently and identically distributed (iid) innovations\nwith symmetric distributions are established. To analyze the infinitesimal\neffect of outliers on the estimator, the influence function is derived and\ncomputed explicitly for an AR(1) model with additive outliers. To obtain\nestimates for the AR(p) model, a robust Durbin-Levinson type and a\nforward-backward algorithm are proposed. An iterative algorithm to robustly\nobtain ARMA(p,q) parameter estimates is also presented. The problem of finding\na robust initialization is addressed, which for orders p+q>2 is a non-trivial\nmatter. Numerical experiments are conducted to compare the finite sample\nperformance of the proposed estimator to existing robust methodologies for\ndifferent types of outliers both in terms of average and of worst-case\nperformance, as measured by the maximum bias curve. To illustrate the practical\napplicability of the proposed estimator, a real-data example of outlier\ncleaning for R-R interval plots derived from electrocardiographic (ECG) data is\nconsidered. The proposed estimator is not limited to biomedical applications,\nbut is also useful in any real-world problem whose observations can be modeled\nas an ARMA process disturbed by outliers or impulsive noise. \n\n"}
{"id": "1607.01631", "contents": "Title: Bayesian emulation for optimization in multi-step portfolio decisions Abstract: We discuss the Bayesian emulation approach to computational solution of\nmulti-step portfolio studies in financial time series. \"Bayesian emulation for\ndecisions\" involves mapping the technical structure of a decision analysis\nproblem to that of Bayesian inference in a purely synthetic \"emulating\"\nstatistical model. This provides access to standard posterior analytic,\nsimulation and optimization methods that yield indirect solutions of the\ndecision problem. We develop this in time series portfolio analysis using\nclasses of economically and psychologically relevant multi-step ahead portfolio\nutility functions. Studies with multivariate currency, commodity and stock\nindex time series illustrate the approach and show some of the practical\nutility and benefits of the Bayesian emulation methodology. \n\n"}
{"id": "1607.03698", "contents": "Title: Indirect Maximum Entropy Bandwidth Abstract: This paper proposes a new method of bandwidth selection in kernel estimation\nof density and distribution functions motivated by the connection between\nmaximisation of the entropy of probability integral transforms and maximum\nlikelihood in classical parametric models. The proposed estimators are designed\nto indirectly maximise the entropy of the leave-one-out kernel estimates of a\ndistribution function, which are the analogues of the parametric probability\nintegral transforms.\n  The estimators based on minimisation of the Cramer-von Mises discrepancy,\nnear-solution of the moment-based estimating equations, and inversion of the\nNeyman smooth test statistic are discussed and their performance compared in a\nsimulation study. The bandwidth minimising the Anderson-Darling statistic is\nfound to perform reliably for a variety of distribution shapes and can be\nrecommended in practice.\n  The results will also be of interest to anyone analysing the cross-validation\nbandwidths based on leave-one-out estimates or evaluation of nonparametric\ndensity forecasts. \n\n"}
{"id": "1607.06358", "contents": "Title: Bayesian uncertainty analysis for complex systems biology models:\n  emulation, global parameter searches and evaluation of gene functions Abstract: Background: Many mathematical models have now been employed across every area\nof systems biology. These models increasingly involve large numbers of unknown\nparameters, have complex structure which can result in substantial evaluation\ntime relative to the needs of the analysis, and need to be compared to observed\ndata. The correct analysis of such models usually requires a global parameter\nsearch, over a high dimensional parameter space, that incorporates and respects\nthe most important sources of uncertainty. This can be an extremely difficult\ntask, but it is essential for any meaningful inference or prediction to be made\nabout any biological system. It hence represents a fundamental challenge for\nthe whole of systems biology.\n  Results: Bayesian statistical methodology for the uncertainty analysis of\ncomplex models is introduced, which is designed to address the high dimensional\nglobal parameter search problem. Bayesian emulators that mimic the systems\nbiology model but which are extremely fast to evaluate are embedded within an\niterative history match: an efficient method to search high dimensional spaces\nwithin a more formal statistical setting, while incorporating major sources of\nuncertainty. The approach is demonstrated via application to two models of\nhormonal crosstalk in Arabidopsis root development, which have 32 rate\nparameters, for which we identify the sets of rate parameter values that lead\nto acceptable matches to observed trend data. The biological consequences of\nthe resulting comparison, including the evaluation of gene functions, are\ndescribed. \n\n"}
{"id": "1608.00264", "contents": "Title: Frequency of Frequencies Distributions and Size Dependent Exchangeable\n  Random Partitions Abstract: Motivated by the fundamental problem of modeling the frequency of frequencies\n(FoF) distribution, this paper introduces the concept of a cluster structure to\ndefine a probability function that governs the joint distribution of a random\ncount and its exchangeable random partitions. A cluster structure, naturally\narising from a completely random measure mixed Poisson process, allows the\nprobability distribution of the random partitions of a subset of a population\nto be dependent on the population size, a distinct and motivated feature that\nmakes it more flexible than a partition structure. This allows it to model an\nentire FoF distribution whose structural properties change as the population\nsize varies. A FoF vector can be simulated by drawing an infinite number of\nPoisson random variables, or by a stick-breaking construction with a finite\nrandom number of steps. A generalized negative binomial process model is\nproposed to generate a cluster structure, where in the prior the number of\nclusters is finite and Poisson distributed, and the cluster sizes follow a\ntruncated negative binomial distribution. We propose a simple Gibbs sampling\nalgorithm to extrapolate the FoF vector of a population given the FoF vector of\na sample taken without replacement from the population. We illustrate our\nresults and demonstrate the advantages of the proposed models through the\nanalysis of real text, genomic, and survey data. \n\n"}
{"id": "1608.00948", "contents": "Title: The bootstrap, covariance matrices and PCA in moderate and\n  high-dimensions Abstract: We consider the properties of the bootstrap as a tool for inference\nconcerning the eigenvalues of a sample covariance matrix computed from an\n$n\\times p$ data matrix $X$. We focus on the modern framework where $p/n$ is\nnot close to 0 but remains bounded as $n$ and $p$ tend to infinity.\n  Through a mix of numerical and theoretical considerations, we show that the\nbootstrap is not in general a reliable inferential tool in the setting we\nconsider. However, in the case where the population covariance matrix is\nwell-approximated by a finite rank matrix, the bootstrap performs as it does in\nfinite dimension. \n\n"}
{"id": "1608.01985", "contents": "Title: Split NMSSM with electroweak baryogenesis Abstract: In light of the Higgs boson discovery we reconsider generation of the baryon\nasymmetry in the non-minimal split Supersymmetry model with an additional\nsinglet superfield in the Higgs sector. We find that successful baryogenesis\nduring the first order electroweak phase transition is possible within\nphenomenologically viable part of the model parameter space. We discuss several\nphenomenological consequences of this scenario, namely, predictions for the\nelectric dipole moments of electron and neutron and collider signatures of\nlight charginos and neutralinos. \n\n"}
{"id": "1608.02386", "contents": "Title: An Analytic Approach to Sunset Diagrams in Chiral Perturbation Theory:\n  Theory and Practice Abstract: We demonstrate the use of several code implementations of the Mellin-Barnes\nmethod available in the public domain to derive analytic expressions for the\nsunset diagrams that arise in the two-loop contribution to the pion mass and\ndecay constant in three-flavoured chiral perturbation theory. We also provide\nresults for all possible two-mass configurations of the sunset integral, and\nderive a new one-dimensional integral representation for the one mass sunset\nintegral with arbitrary external momentum. Thoroughly annotated Mathematica\nnotebooks are provided as ancillary files, which may serve as pedagogical\nsupplements to the methods described in this paper. \n\n"}
{"id": "1608.04378", "contents": "Title: Hadronic matter at the edge: A survey of some theoretical approaches to\n  the physics of the QCD phase diagram Abstract: In the past few years a wealth of high quality data has made possible to test\ncurrent theoretical ideas about the properties of hadrons subject to extreme\nconditions of density and temperature. The relativistic heavy-ion program\ncarried out at the CERN-SPS and under development at the BNL-RHIC and CERN-LHC\nhas provided results that probe the evolution of collisions of hadronic matter\nat high energies from the initially large density to the late dilute stages. In\naddition, QCD on the lattice has produced results complementing these findings\nwith first principles calculations for observables in a regime where\nperturbative techniques cannot describe the nature of strongly coupled systems.\nThis work aims to review some recent developments that make use of field\ntheoretical methods to describe the physics of hadrons at finite temperature\nand density. I concentrate on two of the main topics that have been explored in\nthe last few years: (1) The search for the structure of the phase diagram and\n(2) analytical signals linked to the chiral symmetry restoration/deconfinement. \n\n"}
{"id": "1608.04478", "contents": "Title: A Geometrical Approach to Topic Model Estimation Abstract: In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and the Singular Value Decomposition (SVD) is a potentially\nuseful tool for learning such a low-rank matrix. However, the connection\nbetween this low-rank matrix and the singular vectors of the text corpus matrix\nare usually complicated and hard to spell out, so how to use SVD for learning\ntopic models faces challenges. In this paper, we overcome the challenge by\nrevealing a surprising insight: there is a low-dimensional simplex structure\nwhich can be viewed as a bridge between the low-rank matrix of interest and the\nSVD of the text corpus matrix, and allows us to conveniently reconstruct the\nformer using the latter. Such an insight motivates a new SVD approach to\nlearning topic models, which we analyze with delicate random matrix theory and\nderive the rate of convergence. We support our methods and theory numerically,\nusing both simulated data and real data. \n\n"}
{"id": "1609.00451", "contents": "Title: Least Ambiguous Set-Valued Classifiers with Bounded Error Levels Abstract: In most classification tasks there are observations that are ambiguous and\ntherefore difficult to correctly label. Set-valued classifiers output sets of\nplausible labels rather than a single label, thereby giving a more appropriate\nand informative treatment to the labeling of ambiguous instances. We introduce\na framework for multiclass set-valued classification, where the classifiers\nguarantee user-defined levels of coverage or confidence (the probability that\nthe true label is contained in the set) while minimizing the ambiguity (the\nexpected size of the output). We first derive oracle classifiers assuming the\ntrue distribution to be known. We show that the oracle classifiers are obtained\nfrom level sets of the functions that define the conditional probability of\neach class. Then we develop estimators with good asymptotic and finite sample\nproperties. The proposed estimators build on existing single-label classifiers.\nThe optimal classifier can sometimes output the empty set, but we provide two\nsolutions to fix this issue that are suitable for various practical needs. \n\n"}
{"id": "1609.01672", "contents": "Title: Connectome Smoothing via Low-rank Approximations Abstract: In statistical connectomics, the quantitative study of brain networks,\nestimating the mean of a population of graphs based on a sample is a core\nproblem. Often, this problem is especially difficult because the sample or\ncohort size is relatively small, sometimes even a single subject. While using\nthe element-wise sample mean of the adjacency matrices is a common approach,\nthis method does not exploit any underlying structural properties of the\ngraphs. We propose using a low-rank method which incorporates tools for\ndimension selection and diagonal augmentation to smooth the estimates and\nimprove performance over the naive methodology for small sample sizes.\nTheoretical results for the stochastic blockmodel show that this method offers\nmajor improvements when there are many vertices. Similarly, we demonstrate that\nthe low-rank methods outperform the standard sample mean for a variety of\nindependent edge distributions as well as human connectome data derived from\nmagnetic resonance imaging, especially when sample sizes are small. Moreover,\nthe low-rank methods yield \"eigen-connectomes\", which correlate with the\nlobe-structure of the human brain and superstructures of the mouse brain. These\nresults indicate that low-rank methods are an important part of the tool box\nfor researchers studying populations of graphs in general, and statistical\nconnectomics in particular. \n\n"}
{"id": "1609.03436", "contents": "Title: Quasi-stationary Monte Carlo and the ScaLE Algorithm Abstract: This paper introduces a class of Monte Carlo algorithms which are based upon\nthe simulation of a Markov process whose quasi-stationary distribution\ncoincides with a distribution of interest. This differs fundamentally from,\nsay, current Markov chain Monte Carlo methods which simulate a Markov chain\nwhose stationary distribution is the target. We show how to approximate\ndistributions of interest by carefully combining sequential Monte Carlo methods\nwith methodology for the exact simulation of diffusions. The methodology\nintroduced here is particularly promising in that it is applicable to the same\nclass of problems as gradient based Markov chain Monte Carlo algorithms but\nentirely circumvents the need to conduct Metropolis-Hastings type accept/reject\nsteps whilst retaining exactness: the paper gives theoretical guarantees\nensuring the algorithm has the correct limiting target distribution.\nFurthermore, this methodology is highly amenable to big data problems. By\nemploying a modification to existing na{\\\"\\i}ve sub-sampling and control\nvariate techniques it is possible to obtain an algorithm which is still exact\nbut has sub-linear iterative cost as a function of data size. \n\n"}
{"id": "1609.04464", "contents": "Title: Peer Encouragement Designs in Causal Inference with Partial Interference\n  and Identification of Local Average Network Effects Abstract: In non-network settings, encouragement designs have been widely used to\nanalyze causal effects of a treatment, policy, or intervention on an outcome of\ninterest when randomizing the treatment was considered impractical or when\ncompliance to treatment cannot be perfectly enforced. Unfortunately, such\nquestions related to treatment compliance have received less attention in\nnetwork settings and the most well-studied experimental design in networks, the\ntwo-stage randomization design, requires perfect compliance with treatment. The\npaper proposes a new experimental design called peer encouragement design to\nstudy network treatment effects when enforcing treatment randomization is not\nfeasible. The key idea in peer encouragement design is the idea of personalized\nencouragement, which allows point-identification of familiar estimands in the\nencouragement design literature. The paper also defines new causal estimands,\nlocal average network effects, that can be identified under the new design and\nanalyzes the effect of non-compliance behavior in randomized experiments on\nnetworks. \n\n"}
{"id": "1609.04523", "contents": "Title: STORE: Sparse Tensor Response Regression and Neuroimaging Analysis Abstract: Motivated by applications in neuroimaging analysis, we propose a new\nregression model, Sparse TensOr REsponse regression (STORE), with a tensor\nresponse and a vector predictor. STORE embeds two key sparse structures:\nelement-wise sparsity and low-rankness. It can handle both a non-symmetric and\na symmetric tensor response, and thus is applicable to both structural and\nfunctional neuroimaging data. We formulate the parameter estimation as a\nnon-convex optimization problem, and develop an efficient alternating updating\nalgorithm. We establish a non-asymptotic estimation error bound for the actual\nestimator obtained from the proposed algorithm. This error bound reveals an\ninteresting interaction between the computational efficiency and the\nstatistical rate of convergence. When the distribution of the error tensor is\nGaussian, we further obtain a fast estimation error rate which allows the\ntensor dimension to grow exponentially with the sample size. We illustrate the\nefficacy of our model through intensive simulations and an analysis of the\nAutism spectrum disorder neuroimaging data. \n\n"}
{"id": "1609.04558", "contents": "Title: Statistical Inference in a Directed Network Model with Covariates Abstract: Networks are often characterized by node heterogeneity for which nodes\nexhibit different degrees of interaction and link homophily for which nodes\nsharing common features tend to associate with each other. In this paper, we\npropose a new directed network model to capture the former via node-specific\nparametrization and the latter by incorporating covariates. In particular, this\nmodel quantifies the extent of heterogeneity in terms of outgoingness and\nincomingness of each node by different parameters, thus allowing the number of\nheterogeneity parameters to be twice the number of nodes. We study the maximum\nlikelihood estimation of the model and establish the uniform consistency and\nasymptotic normality of the resulting estimators. Numerical studies demonstrate\nour theoretical findings and a data analysis confirms the usefulness of our\nmodel. \n\n"}
{"id": "1609.04967", "contents": "Title: Semiparametric estimation for isotropic max-stable space-time processes Abstract: Regularly varying space-time processes have proved useful to study extremal\ndependence in space-time data. We propose a semiparametric estimation procedure\nbased on a closed form expression of the extremogram to estimate parametric\nmodels of extremal dependence functions. We establish the asymptotic properties\nof the resulting parameter estimates and propose subsampling procedures to\nobtain asymptotically correct confidence intervals. A simulation study shows\nthat the proposed procedure works well for moderate sample sizes and is robust\nto small departures from the underlying model. Finally, we apply this\nestimation procedure to fitting a max-stable process to radar rainfall\nmeasurements in a region in Florida. Complementary results and some proofs of\nkey results are presented together with the simulation study in the supplement. \n\n"}
{"id": "1609.08382", "contents": "Title: Search for long-lived charged particles in proton-proton collisions at\n  sqrt(s) = 13 TeV Abstract: Results are presented of a search for heavy stable charged particles produced\nin proton-proton collisions at sqrt(s) = 13 TeV using a data sample\ncorresponding to an integrated luminosity of 2.5 inverse femtobarns collected\nin 2015 with the CMS detector at the CERN LHC. The search is conducted using\nsignatures of anomalously high energy deposits in the silicon tracker and long\ntime of flight measurements by the muon system. The data are consistent with\nthe expected background, and upper limits are set on the cross sections for\nproduction of long-lived gluinos, top squarks, tau sleptons, and leptonlike\nlong-lived fermions. These upper limits are equivalently expressed as lower\nlimits on the masses of new states; the limits for gluinos, ranging up to 1610\nGeV, are the most stringent to date. Limits on the cross sections for direct\npair production of long-lived tau sleptons are also determined. \n\n"}
{"id": "1609.09272", "contents": "Title: A New Algorithm for Circulant Rational Covariance Extension and\n  Applications to Finite-interval Smoothing Abstract: The partial stochastic realization of periodic processes from finite\ncovariance data has recently been solved by Lindquist and Picci based on convex\noptimization of a generalized entropy functional. The meaning and the role of\nthis criterion have an unclear origin. In this paper we propose a solution\nbased on a nonlinear generalization of the classical Yule-Walker type equations\nand on a new iterative algorithm which is shown to converge to the same\n(unique) solution of the variational problem. This provides a conceptual link\nto the variational principles and at the same time yields a robust algorithm\nwhich can for example be successfully applied to finite-interval smoothing\nproblems providing a simpler procedure if compared with the classical\nRiccati-based calculations. \n\n"}
{"id": "1609.09380", "contents": "Title: Testing mutual independence in high dimension via distance covariance Abstract: In this paper, we introduce a ${\\mathcal L}_2$ type test for testing mutual\nindependence and banded dependence structure for high dimensional data. The\ntest is constructed based on the pairwise distance covariance and it accounts\nfor the non-linear and non-monotone dependences among the data, which cannot be\nfully captured by the existing tests based on either Pearson correlation or\nrank correlation. Our test can be conveniently implemented in practice as the\nlimiting null distribution of the test statistic is shown to be standard\nnormal. It exhibits excellent finite sample performance in our simulation\nstudies even when the sample size is small albeit dimension is high, and is\nshown to successfully identify nonlinear dependence in empirical data analysis.\nOn the theory side, asymptotic normality of our test statistic is shown under\nquite mild moment assumptions and with little restriction on the growth rate of\nthe dimension as a function of sample size. As a demonstration of good power\nproperties for our distance covariance based test, we further show that an\ninfeasible version of our test statistic has the rate optimality in the class\nof Gaussian distribution with equal correlation. \n\n"}
{"id": "1609.09386", "contents": "Title: Search for supersymmetry in events with one lepton and multiple jets in\n  proton-proton collisions at sqrt(s) = 13 TeV Abstract: A search for supersymmetry is performed in events with a single electron or\nmuon in proton-proton collisions at a center-of-mass energy of 13 TeV. The data\nwere recorded by the CMS experiment at the LHC and correspond to an integrated\nluminosity of 2.3 inverse femtobarns. Several exclusive search regions are\ndefined based on the number of jets and b-tagged jets, the scalar sum of the\njet transverse momenta, and the scalar sum of the missing transverse momentum\nand the transverse momentum of the lepton. The observed event yields in data\nare consistent with the expected backgrounds from standard model processes. The\nresults are interpreted using two simplified models of supersymmetric particle\nspectra, both of which describe gluino pair production. In the first model,\neach gluino decays via a three-body process to top quarks and a neutralino,\nwhich is associated with the observed missing transverse momentum in the event.\nGluinos with masses up to 1.6 TeV are excluded for neutralino masses below 600\nGeV. In the second model, each gluino decays via a three-body process to two\nlight quarks and a chargino, which subsequently decays to a W boson and a\nneutralino. The mass of the chargino is taken to be midway between the gluino\nand neutralino masses. In this model, gluinos with masses below 1.4 TeV are\nexcluded for neutralino masses below 700 GeV. \n\n"}
{"id": "1610.00069", "contents": "Title: The choice of effect measure for binary outcomes: Introducing\n  counterfactual outcome state transition parameters Abstract: Standard measures of effect, including the risk ratio, the odds ratio, and\nthe risk difference, are associated with a number of well-described\nshortcomings, and no consensus exists about the conditions under which\ninvestigators should choose one effect measure over another. In this paper, we\nintroduce a new framework for reasoning about choice of effect measure by\nlinking two separate versions of the risk ratio to a counterfactual causal\nmodel. In our approach, effects are defined in terms of \"counterfactual outcome\nstate transition parameters\", that is, the proportion of those individuals who\nwould not have been a case by the end of follow-up if untreated, who would have\nresponded to treatment by becoming a case; and the proportion of those\nindividuals who would have become a case by the end of follow-up if untreated\nwho would have responded to treatment by not becoming a case. Although\ncounterfactual outcome state transition parameters are generally not identified\nfrom the data without strong monotonicity assumptions, we show that when they\nstay constant between populations, there are important implications for model\nspecification, meta-analysis, and research generalization. \n\n"}
{"id": "1610.00168", "contents": "Title: Learning Optimized Risk Scores Abstract: Risk scores are simple classification models that let users make quick risk\npredictions by adding and subtracting a few small numbers. These models are\nwidely used in medicine and criminal justice, but are difficult to learn from\ndata because they need to be calibrated, sparse, use small integer\ncoefficients, and obey application-specific operational constraints. In this\npaper, we present a new machine learning approach to learn risk scores. We\nformulate the risk score problem as a mixed integer nonlinear program, and\npresent a cutting plane algorithm for non-convex settings to efficiently\nrecover its optimal solution. We improve our algorithm with specialized\ntechniques to generate feasible solutions, narrow the optimality gap, and\nreduce data-related computation. Our approach can fit risk scores in a way that\nscales linearly in the number of samples, provides a certificate of optimality,\nand obeys real-world constraints without parameter tuning or post-processing.\nWe benchmark the performance benefits of this approach through an extensive set\nof numerical experiments, comparing to risk scores built using heuristic\napproaches. We also discuss its practical benefits through a real-world\napplication where we build a customized risk score for ICU seizure prediction\nin collaboration with the Massachusetts General Hospital. \n\n"}
{"id": "1610.01353", "contents": "Title: Confidence regions for high-dimensional generalized linear models under\n  sparsity Abstract: We study asymptotically normal estimation and confidence regions for\nlow-dimensional parameters in high-dimensional sparse models. Our approach is\nbased on the $\\ell_1$-penalized M-estimator which is used for construction of a\nbias corrected estimator. We show that the proposed estimator is asymptotically\nnormal, under a sparsity assumption on the high-dimensional parameter,\nsmoothness conditions on the expected loss and an entropy condition. This leads\nto uniformly valid confidence regions and hypothesis testing for\nlow-dimensional parameters. The present approach is different in that it allows\nfor treatment of loss functions that we not sufficiently differentiable, such\nas quantile loss, Huber loss or hinge loss functions. We also provide new\nresults for estimation of the inverse Fisher information matrix, which is\nnecessary for the construction of the proposed estimator. We formulate our\nresults for general models under high-level conditions, but investigate these\nconditions in detail for generalized linear models and provide mild sufficient\nconditions. As particular examples, we investigate the case of quantile loss\nand Huber loss in linear regression and demonstrate the performance of the\nestimators in a simulation study and on real datasets from genome-wide\nassociation studies. We further investigate the case of logistic regression and\nillustrate the performance of the estimator on simulated and real data. \n\n"}
{"id": "1610.03711", "contents": "Title: Plasmon mass scale in classical nonequilibrium gauge theory Abstract: Classical lattice Yang-Mills calculations provide a good way to understand\ndifferent nonequilibrium phenomena in nonperturbatively overoccupied systems.\nAbove the Debye scale the classical theory can be matched smoothly to kinetic\ntheory. The aim of this work is to study the limits of this quasiparticle\npicture by determining the plasmon mass in classical real time Yang-Mills\ntheory on a lattice in 3 spatial dimensions. We compare three methods to\ndetermine the plasmon mass: a hard thermal loop expression in terms of the\nparticle distribution, an effective dispersion relation constructed from fields\nand their time derivatives, and by measuring oscillations between electric and\nmagnetic field modes after artificially introducing a homogeneous color\nelectric field. We find that a version of the dispersion relation that uses\nelectric fields and their time derivatives agrees with the other methods within\n50%. \n\n"}
{"id": "1610.04465", "contents": "Title: On the combination of omics data for prediction of binary outcomes Abstract: Enrichment of predictive models with new biomolecular markers is an important\ntask in high-dimensional omic applications. Increasingly, clinical studies\ninclude several sets of such omics markers available for each patient,\nmeasuring different levels of biological variation. As a result, one of the\nmain challenges in predictive research is the integration of different sources\nof omic biomarkers for the prediction of health traits. We review several\napproaches for the combination of omic markers in the context of binary outcome\nprediction, all based on double cross-validation and regularized regression\nmodels. We evaluate their performance in terms of calibration and\ndiscrimination and we compare their performance with respect to single-omic\nsource predictions. We illustrate the methods through the analysis of two real\ndatasets. On the one hand, we consider the combination of two fractions of\nproteomic mass spectrometry for the calibration of a diagnostic rule for the\ndetection of early-stage breast cancer. On the other hand, we consider\ntranscriptomics and metabolomics as predictors of obesity using data from the\nDietary, Lifestyle, and Genetic determinants of Obesity and Metabolic syndrome\n(DILGOM) study, a population-based cohort, from Finland. \n\n"}
{"id": "1610.04960", "contents": "Title: Group SLOPE - adaptive selection of groups of predictors Abstract: Sorted L-One Penalized Estimation (SLOPE) is a relatively new convex\noptimization procedure which allows for adaptive selection of regressors under\nsparse high dimensional designs. Here we extend the idea of SLOPE to deal with\nthe situation when one aims at selecting whole groups of explanatory variables\ninstead of single regressors. Such groups can be formed by clustering strongly\ncorrelated predictors or groups of dummy variables corresponding to different\nlevels of the same qualitative predictor. We formulate the respective convex\noptimization problem, gSLOPE (group SLOPE), and propose an efficient algorithm\nfor its solution. We also define a notion of the group false discovery rate\n(gFDR) and provide a choice of the sequence of tuning parameters for gSLOPE so\nthat gFDR is provably controlled at a prespecified level if the groups of\nvariables are orthogonal to each other. Moreover, we prove that the resulting\nprocedure adapts to unknown sparsity and is asymptotically minimax with respect\nto the estimation of the proportions of variance of the response variable\nexplained by regressors from different groups. We also provide a method for the\nchoice of the regularizing sequence when variables in different groups are not\northogonal but statistically independent and illustrate its good properties\nwith computer simulations. Finally, we illustrate the advantages of gSLOPE in\nthe context of Genome Wide Association Studies. R package grpSLOPE with\nimplementation of our method is available on CRAN. \n\n"}
{"id": "1610.05533", "contents": "Title: The next challenge for neutrinos: the mass ordering Abstract: Neutrino physics is nowadays receiving more and more attention as a possible\nsource of information for the long--standing investigation of new physics\nbeyond the Standard Model. This is also supported by the recent change of\nperspectives in neutrino researches since the discovery period is almost over\nand we are entering the phase of precise measurements. Despite the limited\nstatistics collected for some variables, the three--flavour neutrino framework\nseems well strengthening. However some relevant pieces of this framework are\nstill missing. The amount of a possible CP violation phase and the mass\nordering are among the most challenging and probably those that will be known\nin the near future. In this paper we will discuss these two correlated issues\nand a very recent new statistical method introduced to get reliable results on\nthe mass ordering. \n\n"}
{"id": "1610.05857", "contents": "Title: Graphical Models for Zero-Inflated Single Cell Gene Expression Abstract: Bulk gene expression experiments relied on aggregations of thousands of cells\nto measure the average expression in an organism. Advances in microfluidic and\ndroplet sequencing now permit expression profiling in single cells. This study\nof cell-to-cell variation reveals that individual cells lack detectable\nexpression of transcripts that appear abundant on a population level, giving\nrise to zero-inflated expression patterns. To infer gene co-regulatory networks\nfrom such data, we propose a multivariate Hurdle model. It is comprised of a\nmixture of singular Gaussian distributions. We employ neighborhood selection\nwith the pseudo-likelihood and a group lasso penalty to select and fit\nundirected graphical models that capture conditional independences between\ngenes. The proposed method is more sensitive than existing approaches in\nsimulations, even under departures from our Hurdle model. The method is applied\nto data for T follicular helper cells, and a high-dimensional profile of mouse\ndendritic cells. It infers network structure not revealed by other methods; or\nin bulk data sets. An R implementation is available at\nhttps://github.com/amcdavid/HurdleNormal . \n\n"}
{"id": "1610.08203", "contents": "Title: Arbres CART et For\\^ets al\\'eatoires, Importance et s\\'election de\n  variables Abstract: Two algorithms proposed by Leo Breiman : CART trees (Classification And\nRegression Trees for) introduced in the first half of the 80s and random\nforests emerged, meanwhile, in the early 2000s, are the subject of this\narticle. The goal is to provide each of the topics, a presentation, a\ntheoretical guarantee, an example and some variants and extensions. After a\npreamble, introduction recalls objectives of classification and regression\nproblems before retracing some predecessors of the Random Forests. Then, a\nsection is devoted to CART trees then random forests are presented. Then, a\nvariable selection procedure based on permutation variable importance is\nproposed. Finally the adaptation of random forests to the Big Data context is\nsketched. \n\n"}
{"id": "1610.08621", "contents": "Title: Estimator Augmentation with Applications in High-Dimensional Group\n  Inference Abstract: To make inference about a group of parameters on high-dimensional data, we\ndevelop the method of estimator augmentation for the block Lasso, which is\ndefined via the block norm. By augmenting a block Lasso estimator $\\hat{\\beta}$\nwith the subgradient $S$ of the block norm evaluated at $\\hat{\\beta}$, we\nderive a closed-form density for the joint distribution of $(\\hat{\\beta},S)$\nunder a high-dimensional setting. This allows us to draw from an estimated\nsampling distribution of $\\hat{\\beta}$, or more generally any function of\n$(\\hat{\\beta},S)$, by Monte Carlo algorithms. We demonstrate the application of\nestimator augmentation in group inference with the group Lasso and a de-biased\ngroup Lasso constructed as a function of $(\\hat{\\beta},S)$. Our numerical\nresults show that importance sampling via estimator augmentation can be orders\nof magnitude more efficient than parametric bootstrap in estimating tail\nprobabilities for significance tests. This work also brings new insights into\nthe geometry of the sample space and the solution uniqueness of the block\nLasso. \n\n"}
{"id": "1610.08663", "contents": "Title: Regularization parameter selection in indirect regression by residual\n  based bootstrap Abstract: Residual-based analysis is generally considered a cornerstone of statistical\nmethodology. For a special case of indirect regression, we investigate the\nresidual-based empirical distribution function and provide a uniform expansion\nof this estimator, which is also shown to be asymptotically most precise. This\ninvestigation naturally leads to a completely data-driven technique for\nselecting a regularization parameter used in our indirect regression function\nestimator. The resulting methodology is based on a smooth bootstrap of the\nmodel residuals. A simulation study demonstrates the effectiveness of our\napproach. \n\n"}
{"id": "1610.09735", "contents": "Title: Community detection with nodal information Abstract: Community detection is one of the fundamental problems in the study of\nnetwork data. Most existing community detection approaches only consider edge\ninformation as inputs, and the output could be suboptimal when nodal\ninformation is available. In such cases, it is desirable to leverage nodal\ninformation for the improvement of community detection accuracy. Towards this\ngoal, we propose a flexible network model incorporating nodal information, and\ndevelop likelihood-based inference methods. For the proposed methods, we\nestablish favorable asymptotic properties as well as efficient algorithms for\ncomputation. Numerical experiments show the effectiveness of our methods in\nutilizing nodal information across a variety of simulated and real network data\nsets. \n\n"}
{"id": "1611.01205", "contents": "Title: Posterior Graph Selection and Estimation Consistency for\n  High-dimensional Bayesian DAG Models Abstract: Covariance estimation and selection for high-dimensional multivariate\ndatasets is a fundamental problem in modern statistics. Gaussian directed\nacyclic graph (DAG) models are a popular class of models used for this purpose.\nGaussian DAG models introduce sparsity in the Cholesky factor of the inverse\ncovariance matrix, and the sparsity pattern in turn corresponds to specific\nconditional independence assumptions on the underlying variables. A variety of\npriors have been developed in recent years for Bayesian inference in DAG\nmodels, yet crucial convergence and sparsity selection properties for these\nmodels have not been thoroughly investigated. Most of these priors are\nadaptations or generalizations of the Wishart distribution in the DAG context.\nIn this paper, we consider a flexible and general class of these 'DAG-Wishart'\npriors with multiple shape parameters. Under mild regularity assumptions, we\nestablish strong graph selection consistency and establish posterior\nconvergence rates for estimation when the number of variables p is allowed to\ngrow at an appropriate sub-exponential rate with the sample size n. \n\n"}
{"id": "1611.01238", "contents": "Title: Corrected Bayesian information criterion for stochastic block models Abstract: Estimating the number of communities is one of the fundamental problems in\ncommunity detection. We re-examine the Bayesian paradigm for stochastic block\nmodels and propose a \"corrected Bayesian information criterion\",to determine\nthe number of communities and show that the proposed estimator is consistent\nunder mild conditions. The proposed criterion improves those used in Wang and\nBickel (2016) and Saldana et al. (2017) which tend to underestimate and\noverestimate the number of communities, respectively. Along the way, we\nestablish the Wilks theorem for stochastic block models. Moreover, we show\nthat, to obtain the consistency of model selection for stochastic block models,\nwe need a so-called \"consistency condition\". We also provide sufficient\nconditions for both homogenous networks and non-homogenous networks. The\nresults are further extended to degree corrected stochastic block models.\nNumerical studies demonstrate our theoretical results. \n\n"}
{"id": "1611.02583", "contents": "Title: A logistic regression analysis approach for sample survey data based on\n  phi-divergence measures Abstract: A new family of minimum distance estimators for binary logistic regression\nmodels based on $\\phi$-divergence measures is introduced. The so called \"pseudo\nminimum phi-divergence estimator\"(PM$\\phi$E) family is presented as an\nextension of \"minimum phi-divergence estimator\" (M$\\phi$E) for general sample\nsurvey designs and contains, as a particular case, the pseudo maximum\nlikelihood estimator (PMLE) considered in Roberts et al. \\cite{r}. Through a\nsimulation study it is shown that some PM$\\phi$Es have a better behaviour, in\nterms of efficiency, than the PMLE. \n\n"}
{"id": "1611.02609", "contents": "Title: A Continuous Threshold Expectile Model Abstract: Expectile regression is a useful tool for exploring the relation between the\nresponse and the explanatory variables beyond the conditional mean. This\narticle develops a continuous threshold expectile regression for modeling data\nin which the effect of a covariate on the response variable is linear but\nvaries below and above an unknown threshold in a continuous way. Based on a\ngrid search approach, we obtain estimators for the threshold and the regression\ncoefficients via an asymmetric least squares regression method. We derive the\nasymptotic properties for all the estimators and show that the estimator for\nthe threshold achieves root-n consistency. We also develop a weighted CUSUM\ntype test statistic for the existence of a threshold in a given expectile, and\nderive its asymptotic properties under both the null and the local alternative\nmodels. This test only requires fitting the model under the null hypothesis in\nthe absence of a threshold, thus it is computationally more efficient than the\nlikelihood-ratio type tests. Simulation studies show desirable finite sample\nperformance in both homoscedastic and heteroscedastic cases. The application of\nour methods on a Dutch growth data and a baseball pitcher salary data reveals\ninteresting insights. \n\n"}
{"id": "1611.02716", "contents": "Title: How the Self-Interacting Dark Matter Model Explains the Diverse Galactic\n  Rotation Curves Abstract: The rotation curves of spiral galaxies exhibit a diversity that has been\ndifficult to understand in the cold dark matter (CDM) paradigm. We show that\nthe self-interacting dark matter (SIDM) model provides excellent fits to the\nrotation curves of a sample of galaxies with asymptotic velocities in the 25 to\n300 km/s range that exemplify the full range of diversity. We only assume the\nhalo concentration-mass relation predicted by the CDM model and a fixed value\nof the self-interaction cross section.In dark matter dominated galaxies,\nthermalization due to self-interactions creates large cores and reduces dark\nmatter densities. In contrast, thermalization leads to denser and smaller cores\nin more luminous galaxies, and naturally explains the flat rotation curves of\nthe highly luminous galaxies. Our results demonstrate that the impact of the\nbaryons on the SIDM halo profile and the scatter from the assembly history of\nhalos as encoded in the concentration-mass relation can explain the diverse\nrotation curves of spiral galaxies. \n\n"}
{"id": "1611.03146", "contents": "Title: The Control of the False Discovery Rate in Fixed Sequence Multiple\n  Testing Abstract: Controlling the false discovery rate (FDR) is a powerful approach to multiple\ntesting. In many applications, the tested hypotheses have an inherent\nhierarchical structure. In this paper, we focus on the fixed sequence structure\nwhere the testing order of the hypotheses has been strictly specified in\nadvance. We are motivated to study such a structure, since it is the most basic\nof hierarchical structures, yet it is often seen in real applications such as\nstatistical process control and streaming data analysis. We first consider a\nconventional fixed sequence method that stops testing once an acceptance\noccurs, and develop such a method controlling the FDR under both arbitrary and\nnegative dependencies. The method under arbitrary dependency is shown to be\nunimprovable without losing control of the FDR and unlike existing FDR methods;\nit cannot be improved even by restricting to the usual positive regression\ndependence on subset (PRDS) condition. To account for any potential mistakes in\nthe ordering of the tests, we extend the conventional fixed sequence method to\none that allows more but a given number of acceptances. Simulation studies show\nthat the proposed procedures can be powerful alternatives to existing FDR\ncontrolling procedures. The proposed procedures are illustrated through a real\ndata set from a microarray experiment. \n\n"}
{"id": "1611.04557", "contents": "Title: Quark Matter Equation of State from Perturbative QCD Abstract: In this proceedings contribution, we discuss recent developments in the\nperturbative determination of the Equation of State of dense quark matter,\nrelevant for the microscopic description of neutron star cores. First, we\nintroduce the current state of the art in the problem, both at zero and small\ntemperatures, and then present results from two recent perturbative studies\nthat pave the way towards extending the EoS to higher orders in perturbation\ntheory. \n\n"}
{"id": "1611.05201", "contents": "Title: Multiscale inference for multivariate deconvolution Abstract: In this paper we provide new methodology for inference of the geometric\nfeatures of a multivariate density in deconvolution. Our approach is based on\nmultiscale tests to detect significant directional derivatives of the unknown\ndensity at arbitrary points in arbitrary directions. The multiscale method is\nused to identify regions of monotonicity and to construct a general procedure\nfor the detection of modes of the multivariate density. Moreover, as an\nimportant application a significance test for the presence of a local maximum\nat a pre-specified point is proposed. The performance of the new methods is\ninvestigated from a theoretical point of view and the finite sample properties\nare illustrated by means of a small simulation study. \n\n"}
{"id": "1611.05550", "contents": "Title: $e$PCA: High Dimensional Exponential Family PCA Abstract: Many applications, such as photon-limited imaging and genomics, involve large\ndatasets with noisy entries from exponential family distributions. It is of\ninterest to estimate the covariance structure and principal components of the\nnoiseless distribution. Principal Component Analysis (PCA), the standard method\nfor this setting, can be inefficient when the noise is non-Gaussian.\n  We develop $e$PCA (exponential family PCA), a new methodology for PCA on\nexponential family distributions. $e$PCA can be used for dimensionality\nreduction and denoising of large data matrices. $e$PCA involves the\neigendecomposition of a new covariance matrix estimator, constructed in a\nsimple and deterministic way using moment calculations, shrinkage, and random\nmatrix theory.\n  We provide several theoretical justifications for our estimator, including\nthe finite-sample convergence rate, and the Marchenko-Pastur law in high\ndimensions. $e$PCA compares favorably to PCA and various PCA alternatives for\nexponential families, in simulations as well as in XFEL and SNP data analysis.\nAn open-source implementation is available. \n\n"}
{"id": "1611.06208", "contents": "Title: Distributed Simultaneous Inference in Generalized Linear Models via\n  Confidence Distribution Abstract: We propose a distributed method for simultaneous inference for datasets with\nsample size much larger than the number of covariates, i.e., N >> p, in the\ngeneralized linear models framework. When such datasets are too big to be\nanalyzed entirely by a single centralized computer, or when datasets are\nalready stored in distributed database systems, the strategy of\ndivide-and-combine has been the method of choice for scalability. Due to\npartition, the sub-dataset sample sizes may be uneven and some possibly close\nto p, which calls for regularization techniques to improve numerical stability.\nHowever, there is a lack of clear theoretical justification and practical\nguidelines to combine results obtained from separate regularized estimators,\nespecially when the final objective is simultaneous inference for a group of\nregression parameters. In this paper, we develop a strategy to combine\nbias-corrected lasso-type estimates by using confidence distributions. We show\nthat the resulting combined estimator achieves the same estimation efficiency\nas that of the maximum likelihood estimator using the centralized data. As\ndemonstrated by simulated and real data examples, our divide-and-combine method\nyields nearly identical inference as the centralized benchmark. \n\n"}
{"id": "1612.00877", "contents": "Title: Bayesian sparse multiple regression for simultaneous rank reduction and\n  variable selection Abstract: We develop a Bayesian methodology aimed at simultaneously estimating low-rank\nand row-sparse matrices in a high-dimensional multiple-response linear\nregression model. We consider a carefully devised shrinkage prior on the matrix\nof regression coefficients which obviates the need to specify a prior on the\nrank, and shrinks the regression matrix towards low-rank and row-sparse\nstructures. We provide theoretical support to the proposed methodology by\nproving minimax optimality of the posterior mean under the prediction risk in\nultra-high dimensional settings where the number of predictors can grow\nsub-exponentially relative to the sample size. A one-step post-processing\nscheme induced by group lasso penalties on the rows of the estimated\ncoefficient matrix is proposed for variable selection, with default choices of\ntuning parameters. We additionally provide an estimate of the rank using a\nnovel optimization function achieving dimension reduction in the covariate\nspace. We exhibit the performance of the proposed methodology in an extensive\nsimulation study and a real data example. \n\n"}
{"id": "1612.04467", "contents": "Title: On Procedures Controlling the FDR for Testing Hierarchically Ordered\n  Hypotheses Abstract: Complex large-scale studies, such as those related to microarray data and\nfMRI studies, often involve testing multiple hierarchically ordered hypotheses.\nHowever, most existing false discovery rate (FDR) controlling procedures do not\nexploit the inherent hierarchical structure among the tested hypotheses. In\nthis paper, we first present a generalized stepwise procedure which generalizes\nthe usual stepwise procedure to the case where each hypothesis is tested with a\ndifferent set of critical constants. This procedure is helpful in creating a\ngeneral framework under which our hierarchical testing procedures are\ndeveloped. Then, we present several hierarchical testing procedures which\ncontrol the FDR under various forms of dependence such as positive dependence\nand block dependence. Our simulation studies show that these proposed methods\ncan be more powerful in some situations than alternative methods such as\nYekutieli's hierarchical testing procedure (Yekutieli, \\emph{JASA} \\textbf{103}\n(2008) 309-316). Finally, we apply our proposed procedures to a real data set\ninvolving abundances of microbes in different ecological environments. \n\n"}
{"id": "1612.05609", "contents": "Title: A $QQ\\to QQ$ planar doublebox in canonical form Abstract: We consider a planar doublebox with four massive external momenta and two\nmassive internal propagators. We derive the system of differential equations\nfor the relevant master integrals, cast it in canonical form, write it as a\n$d\\log$ form and solve it in terms of iterated integrals up to depth four. \n\n"}
{"id": "1612.06304", "contents": "Title: Double shrunken selection operator Abstract: The least absolute shrinkage and selection operator (LASSO) of Tibshirani\n(1996) is a prominent estimator which selects significant (under some sense)\nfeatures and kills insignificant ones. Indeed the LASSO shrinks features lager\nthan a noise level to zero. In this paper, we force LASSO to be shrunken more\nby proposing a Stein-type shrinkage estimator emanating from the LASSO, namely\nthe Stein-type LASSO. The newly proposed estimator proposes good performance in\nrisk sense numerically. Variants of this estimator have smaller relative MSE\nand prediction error, compared to the LASSO, in the analysis of prostate cancer\ndata set. \n\n"}
{"id": "1612.07252", "contents": "Title: The matrix element method at next-to-leading order for arbitrary jet\n  algorithms Abstract: The matrix element method usually employs leading-order matrix elements. We\ndiscuss the generalisation towards higher orders in perturbation theory and\nshow how the matrix element method can be used at next-to-leading order for\narbitrary infrared-safe jet algorithms. We discuss three variants at\nnext-to-leading order. The first two variants work at the level of the jet\nmomenta. The first variant adheres to strict fixed-order in perturbation\ntheory. We present a method for the required integration over the radiation\nphase space. The second variant is inspired by the POWHEG method and works as\nthe first variant at the level of the jet momenta. The third variant is a more\nexclusive POWHEG version. Here we resolve exactly one jet into two sub-jets. If\nthe two sub-jets are resolved above a scale $p_\\bot^{\\mathrm{min}}$, the\nlikelihood is computed from the POWHEG-modified real emission part, otherwise\nit is given by the POWHEG-modified virtual part. \n\n"}
{"id": "1701.01503", "contents": "Title: Log-Linear Bayesian Additive Regression Trees for Multinomial Logistic\n  and Count Regression Models Abstract: We introduce Bayesian additive regression trees (BART) for log-linear models\nincluding multinomial logistic regression and count regression with\nzero-inflation and overdispersion. BART has been applied to nonparametric mean\nregression and binary classification problems in a range of settings. However,\nexisting applications of BART have been limited to models for Gaussian \"data\",\neither observed or latent. This is primarily because efficient MCMC algorithms\nare available for Gaussian likelihoods. But while many useful models are\nnaturally cast in terms of latent Gaussian variables, many others are not --\nincluding models considered in this paper.\n  We develop new data augmentation strategies and carefully specified prior\ndistributions for these new models. Like the original BART prior, the new prior\ndistributions are carefully constructed and calibrated to be flexible while\nguarding against overfitting. Together the new priors and data augmentation\nschemes allow us to implement an efficient MCMC sampler outside the context of\nGaussian models. The utility of these new methods is illustrated with examples\nand an application to a previously published dataset. \n\n"}
{"id": "1701.03772", "contents": "Title: Additive Partially Linear Models for Massive Heterogeneous Data Abstract: We consider an additive partially linear framework for modelling massive\nheterogeneous data. The major goal is to extract multiple common features\nsimultaneously across all sub-populations while exploring heterogeneity of each\nsub-population. We propose an aggregation type of estimators for the\ncommonality parameters that possess the asymptotic optimal bounds and the\nasymptotic distributions as if there were no heterogeneity. This oracle result\nholds when the number of sub-populations does not grow too fast and the tuning\nparameters are selected carefully. A plug-in estimator for the heterogeneity\nparameter is further constructed, and shown to possess the asymptotic\ndistribution as if the commonality information were available. Furthermore, we\ndevelop a heterogeneity test for the linear components and a homogeneity test\nfor the non-linear components accordingly. The performance of the proposed\nmethods is evaluated via simulation studies and an application to the Medicare\nProvider Utilization and Payment data. \n\n"}
{"id": "1701.05656", "contents": "Title: A Two-Step Geometric Framework For Density Modeling Abstract: We introduce a novel two-step approach for estimating a probability density\nfunction (pdf) given its samples, with the second and important step coming\nfrom a geometric formulation. The procedure involves obtaining an initial\nestimate of the pdf and then transforming it via a warping function to reach\nthe final estimate. The initial estimate is intended to be computationally\nfast, albeit suboptimal, but its warping creates a larger, flexible class of\ndensity functions, resulting in substantially improved estimation. The search\nfor optimal warping is accomplished by mapping diffeomorphic functions to the\ntangent space of a Hilbert sphere, a vector space whose elements can be\nexpressed using an orthogonal basis. Using a truncated basis expansion, we\nestimate the optimal warping under a (penalized) likelihood criterion and,\nthus, the optimal density estimate. This framework is introduced for\nunivariate, unconditional pdf estimation and then extended to conditional pdf\nestimation. The approach avoids many of the computational pitfalls associated\nwith classical conditional-density estimation methods, without losing on\nestimation performance. We derive asymptotic convergence rates of the density\nestimator and demonstrate this approach using both synthetic datasets and real\ndata, the latter relating to the association of a toxic metabolite on preterm\nbirth. \n\n"}
{"id": "1702.01141", "contents": "Title: Cosmological Implications of Dark Matter Bound States Abstract: We present generic formulae for computing how Sommerfeld corrections together\nwith bound-state formation affects the thermal abundance of Dark Matter with\nnon-abelian gauge interactions. We consider DM as a fermion 3plet (wino) or\n5plet under SU(2)$_L$. In the latter case bound states raise to 14 TeV the DM\nmass required to reproduce the cosmological DM abundance and give indirect\ndetection signals such as (for this mass) a dominant $\\gamma$-line around 85\nGeV. Furthermore, we consider DM co-annihilating with a colored particle, such\nas a squark or a gluino, finding that bound state effects are especially\nrelevant in the latter case. \n\n"}
{"id": "1702.03628", "contents": "Title: Multilevel Monte Carlo in Approximate Bayesian Computation Abstract: In the following article we consider approximate Bayesian computation (ABC)\ninference. We introduce a method for numerically approximating ABC posteriors\nusing the multilevel Monte Carlo (MLMC). A sequential Monte Carlo version of\nthe approach is developed and it is shown under some assumptions that for a\ngiven level of mean square error, this method for ABC has a lower cost than\ni.i.d. sampling from the most accurate ABC approximation. Several numerical\nexamples are given. \n\n"}
{"id": "1702.03673", "contents": "Title: Bayesian Probabilistic Numerical Methods Abstract: The emergent field of probabilistic numerics has thus far lacked clear\nstatistical principals. This paper establishes Bayesian probabilistic numerical\nmethods as those which can be cast as solutions to certain inverse problems\nwithin the Bayesian framework. This allows us to establish general conditions\nunder which Bayesian probabilistic numerical methods are well-defined,\nencompassing both non-linear and non-Gaussian models. For general computation,\na numerical approximation scheme is proposed and its asymptotic convergence\nestablished. The theoretical development is then extended to pipelines of\ncomputation, wherein probabilistic numerical methods are composed to solve more\nchallenging numerical tasks. The contribution highlights an important research\nfrontier at the interface of numerical analysis and uncertainty quantification,\nwith a challenging industrial application presented. \n\n"}
{"id": "1702.03776", "contents": "Title: Building and testing models with extended Higgs sectors Abstract: Models with non-minimal Higgs sectors represent a mainstream direction in\ntheoretical exploration of physics opportunities beyond the Standard Model.\nExtended scalar sectors help alleviate difficulties of the Standard Model and\nlead to a rich spectrum of characteristic collider signatures and astroparticle\nconsequences. In this review, we introduce the reader to the world of extended\nHiggs sectors. Not pretending to exhaustively cover the entire body of\nliterature, we walk through a selection of the most popular examples: the two-\nand multi-Higgs-doublet models, as well as singlet and triplet extensions. We\nwill show how one typically builds models with extended Higgs sectors, describe\nthe main goals and the challenges which arise on the way, and mention some\nmethods to overcome them. We will also describe how such models can be tested,\nwhat are the key observables one focuses on, and illustrate the general\nstrategy with a subjective selection of results. \n\n"}
{"id": "1702.04031", "contents": "Title: Maximum likelihood estimation in Gaussian models under total positivity Abstract: We analyze the problem of maximum likelihood estimation for Gaussian\ndistributions that are multivariate totally positive of order two (MTP2). By\nexploiting connections to phylogenetics and single-linkage clustering, we give\na simple proof that the maximum likelihood estimator (MLE) for such\ndistributions exists based on at least 2 observations, irrespective of the\nunderlying dimension. Slawski and Hein, who first proved this result, also\nprovided empirical evidence showing that the MTP2 constraint serves as an\nimplicit regularizer and leads to sparsity in the estimated inverse covariance\nmatrix, determining what we name the ML graph. We show that we can find an\nupper bound for the ML graph by adding edges corresponding to correlations in\nexcess of those explained by the maximum weight spanning forest of the\ncorrelation matrix. Moreover, we provide globally convergent coordinate descent\nalgorithms for calculating the MLE under the MTP2 constraint which are\nstructurally similar to iterative proportional scaling. We conclude the paper\nwith a discussion of signed MTP2 distributions. \n\n"}
{"id": "1702.08797", "contents": "Title: A Fused Gaussian Process Model for Very Large Spatial Data Abstract: With the development of new remote sensing technology, large or even massive\nspatial datasets covering the globe become available. Statistical analysis of\nsuch data is challenging. This article proposes a semiparametric approach to\nmodel large or massive spatial datasets. In particular, a Gaussian process with\nadditive components is proposed, with its covariance structure consisting of\ntwo components: one component is flexible without assuming a specific\nparametric covariance function but is able to achieve dimension reduction; the\nother is parametric and simultaneously induces sparsity. The inference\nalgorithm for parameter estimation and spatial prediction is devised. The\nresulting spatial prediction methodology that we call fused Gaussian process\n(FGP), is applied to simulated data and a massive satellite dataset. The\nresults demonstrate the computational and inferential benefits of FGP over\ncompeting methods and show that FGP is robust against model misspecification\nand captures spatial nonstationarity. The supplemental materials are available\nonline. \n\n"}
{"id": "1703.02177", "contents": "Title: Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t\n  Distributions for Model-Based Clustering with Incomplete Data Abstract: Robust clustering from incomplete data is an important topic because, in many\npractical situations, real data sets are heavy-tailed, asymmetric, and/or have\narbitrary patterns of missing observations. Flexible methods and algorithms for\nmodel-based clustering are presented via mixture of the generalized hyperbolic\ndistributions and its limiting case, the mixture of multivariate skew-t\ndistributions. An analytically feasible EM algorithm is formulated for\nparameter estimation and imputation of missing values for mixture models\nemploying missing at random mechanisms. The proposed methodologies are\ninvestigated through a simulation study with varying proportions of synthetic\nmissing values and illustrated using a real dataset. Comparisons are made with\nthose obtained from the traditional mixture of generalized hyperbolic\ndistribution counterparts by filling in the missing data using the mean\nimputation method. \n\n"}
{"id": "1703.02834", "contents": "Title: Exact Dimensionality Selection for Bayesian PCA Abstract: We present a Bayesian model selection approach to estimate the intrinsic\ndimensionality of a high-dimensional dataset. To this end, we introduce a novel\nformulation of the probabilisitic principal component analysis model based on a\nnormal-gamma prior distribution. In this context, we exhibit a closed-form\nexpression of the marginal likelihood which allows to infer an optimal number\nof components. We also propose a heuristic based on the expected shape of the\nmarginal likelihood curve in order to choose the hyperparameters. In\nnon-asymptotic frameworks, we show on simulated data that this exact\ndimensionality selection approach is competitive with both Bayesian and\nfrequentist state-of-the-art methods. \n\n"}
{"id": "1703.03165", "contents": "Title: Perturbation Bootstrap in Adaptive Lasso Abstract: The Adaptive Lasso(Alasso) was proposed by Zou [\\textit{J. Amer. Statist.\nAssoc. \\textbf{101} (2006) 1418-1429}] as a modification of the Lasso for the\npurpose of simultaneous variable selection and estimation of the parameters in\na linear regression model. Zou (2006) established that the Alasso estimator is\nvariable-selection consistent as well as asymptotically Normal in the indices\ncorresponding to the nonzero regression coefficients in certain\nfixed-dimensional settings. In an influential paper, Minnier, Tian and Cai\n[\\textit{J. Amer. Statist. Assoc. \\textbf{106} (2011) 1371-1382}] proposed a\nperturbation bootstrap method and established its distributional consistency\nfor the Alasso estimator in the fixed-dimensional setting. In this paper,\nhowever, we show that this (naive) perturbation bootstrap fails to achieve\nsecond order correctness in approximating the distribution of the Alasso\nestimator. We propose a modification to the perturbation bootstrap objective\nfunction and show that a suitably studentized version of our modified\nperturbation bootstrap Alasso estimator achieves second-order correctness even\nwhen the dimension of the model is allowed to grow to infinity with the sample\nsize. As a consequence, inferences based on the modified perturbation bootstrap\nwill be more accurate than the inferences based on the oracle Normal\napproximation. We give simulation studies demonstrating good finite-sample\nproperties of our modified perturbation bootstrap method as well as an\nillustration of our method on a real data set. \n\n"}
{"id": "1703.05312", "contents": "Title: On optimal experimental designs for Sparse Polynomial Chaos Expansions Abstract: Uncertainty quantification (UQ) has received much attention in the literature\nin the past decade. In this context, Sparse Polynomial chaos expansions (PCE)\nhave been shown to be among the most promising methods because of their ability\nto model highly complex models at relatively low computational costs. A\nleast-square minimization technique may be used to determine the coefficients\nof the sparse PCE by relying on the so called experimental design (ED), i.e.\nthe sample points where the original computational model is evaluated. An\nefficient sampling strategy is then needed to generate an accurate PCE at low\ncomputational cost. This paper is concerned with the problem of identifying an\noptimal experimental design that maximizes the accuracy of the surrogate model\nover the whole input space within a given computational budget. A novel\nsequential adaptive strategy where the ED is enriched sequentially by\ncapitalizing on the sparsity of the underlying metamodel is introduced. A\ncomparative study between several state-of-the-art methods is performed on four\nnumerical models with varying input dimensionality and computational\ncomplexity. It is shown that the optimal sequential design based on the S-value\ncriterion yields accurate, stable and computationally efficient PCE. \n\n"}
{"id": "1703.06163", "contents": "Title: Transverse Momentum Dependent Parton Distributions at Small-x Abstract: We study the transverse momentum dependent (TMD) parton distributions at\nsmall-x in a consistent framework that takes into account the TMD evolution and\nsmall-x evolution simultaneously. The small-x evolution effects are included by\ncomputing the TMDs at appropriate scales in terms of the dipole scattering\namplitudes, which obey the relevant Balitsky-Kovchegov equation. Meanwhile, the\nTMD evolution is obtained by resumming the Collins-Soper type large logarithms\nemerged from the calculations in small-x formalism into Sudakov factors. \n\n"}
{"id": "1703.06336", "contents": "Title: Analysis of error control in large scale two-stage multiple hypothesis\n  testing Abstract: When dealing with the problem of simultaneously testing a large number of\nnull hypotheses, a natural testing strategy is to first reduce the number of\ntested hypotheses by some selection (screening or filtering) process, and then\nto simultaneously test the selected hypotheses. The main advantage of this\nstrategy is to greatly reduce the severe effect of high dimensions. However,\nthe first screening or selection stage must be properly accounted for in order\nto maintain some type of error control. In this paper, we will introduce a\nselection rule based on a selection statistic that is independent of the test\nstatistic when the tested hypothesis is true. Combining this selection rule and\nthe conventional Bonferroni procedure, we can develop a powerful and valid\ntwo-stage procedure. The introduced procedure has several nice properties: (i)\nit completely removes the selection effect; (ii) it reduces the multiplicity\neffect; (iii) it does not \"waste\" data while carrying out both selection and\ntesting. Asymptotic power analysis and simulation studies illustrate that this\nproposed method can provide higher power compared to usual multiple testing\nmethods while controlling the Type 1 error rate. Optimal selection thresholds\nare also derived based on our asymptotic analysis. \n\n"}
{"id": "1703.07174", "contents": "Title: Cut and compute: Quick cascades with multiple amplitudes Abstract: In an earlier paper we have proposed a novel method to compute the decay\nwidth for a general $1\\to n$ cascade decay where the propagators are off-shell\nand may be of different spins. Here, we extend our algorithm to accommodate\nthose decays that are mediated by more than one such cascades. This generalizes\nour prescription and widens its applicability. We compute the three- and\nfour-body toy decay chains where identical final states appear through\ndifferent cascades. Here, we also provide the algorithm to calculate the\ninterference terms. For four-body decays we discuss both symmetric and\nasymmetric cascades, providing the expressions for the detailed phase space\nstructure in each case. We find that the results obtained with this algorithm\nhave a very impressive agreement with those from standard softwares using a\nsophisticated Monte Carlo based phase space integration. \n\n"}
{"id": "1703.07198", "contents": "Title: Overcoming model simplifications when quantifying predictive uncertainty Abstract: It is generally accepted that all models are wrong -- the difficulty is\ndetermining which are useful. Here, a useful model is considered as one that is\ncapable of combining data and expert knowledge, through an inversion or\ncalibration process, to adequately characterize the uncertainty in predictions\nof interest. This paper derives conditions that specify which simplified models\nare useful and how they should be calibrated. To start, the notion of an\noptimal simplification is defined. This relates the model simplifications to\nthe nature of the data and predictions, and determines when a standard\nprobabilistic calibration scheme is capable of accurately characterizing\nuncertainty. Furthermore, two additional conditions are defined for suboptimal\nmodels that determine when the simplifications can be safely ignored. The first\nallows a suboptimally simplified model to be used in a way that replicates the\nperformance of an optimal model. This is achieved through the judicial\nselection of a prior term for the calibration process that explicitly includes\nthe nature of the data, predictions and modelling simplifications. The second\nconsiders the dependency structure between the predictions and the available\ndata to gain insights into when the simplifications can be overcome by using\nthe right calibration data. Furthermore, the derived conditions are related to\nthe commonly used calibration schemes based on Tikhonov and subspace\nregularization. To allow concrete insights to be obtained, the analysis is\nperformed under a linear expansion of the model equations and where the\npredictive uncertainty is characterized via second order moments only. \n\n"}
{"id": "1703.08596", "contents": "Title: The Inner Structure of Time-Dependent Signals Abstract: This paper shows how a time series of measurements of an evolving system can\nbe processed to create an inner time series that is unaffected by any\ninstantaneous invertible, possibly nonlinear transformation of the\nmeasurements. An inner time series contains information that does not depend on\nthe nature of the sensors, which the observer chose to monitor the system.\nInstead, it encodes information that is intrinsic to the evolution of the\nobserved system. Because of its sensor-independence, an inner time series may\nproduce fewer false negatives when it is used to detect events in the presence\nof sensor drift. Furthermore, if the observed physical system is comprised of\nnon-interacting subsystems, its inner time series is separable; i.e., it\nconsists of a collection of time series, each one being the inner time series\nof an isolated subsystem. Because of this property, an inner time series can be\nused to detect a specific behavior of one of the independent subsystems without\nusing blind source separation to disentangle that subsystem from the others.\nThe method is illustrated by applying it to: 1) an analytic example; 2) the\naudio waveform of one speaker; 3) video images from a moving camera; 4)\nmixtures of audio waveforms of two speakers. \n\n"}
{"id": "1704.01538", "contents": "Title: Doubly Robust Inference for Targeted Minimum Loss Based Estimation in\n  Randomized Trials with Missing Outcome Data Abstract: Missing outcome data is one of the principal threats to the validity of\ntreatment effect estimates from randomized trials. The outcome distributions of\nparticipants with missing and observed data are often different, which\nincreases the risk of bias. Causal inference methods may aid in reducing the\nbias and improving efficiency by incorporating baseline variables into the\nanalysis. In particular, doubly robust estimators incorporate estimates of two\nnuisance parameters: the outcome regression and the missingness mechanism, to\nadjust for differences in the observed and unobserved groups that can be\nexplained by observed covariates. Such nuisance parameters are traditionally\nestimated using parametric models, which generally preclude consistent\nestimation, particularly in moderate to high dimensions. Recent research on\nmissing data has focused on data-adaptive estimation of the nuisance parameters\nin order to achieve consistency, but the large sample properties of such\nestimators are poorly understood. In this article we discuss a doubly robust\nestimator that is consistent and asymptotically normal (CAN) under\ndata-adaptive consistent estimation of the outcome regression or the\nmissingness mechanism. We provide a formula for an asymptotically valid\nconfidence interval under minimal assumptions. We show that our proposed\nestimator has smaller finite-sample bias compared to standard doubly robust\nestimators. We present a simulation study demonstrating the enhanced\nperformance of our estimators in terms of bias, efficiency, and coverage of the\nconfidence intervals. We present the results of an illustrative example: a\nrandomized, double-blind phase II/III trial of antiretroviral therapy in\nHIV-infected persons, and provide R code implementing our proposed estimators. \n\n"}
{"id": "1704.02097", "contents": "Title: Multivariate Count Autoregression Abstract: We are studying the problems of modeling and inference for multivariate count\ntime series data with Poisson marginals. The focus is on linear and log-linear\nmodels. For studying the properties of such processes we develop a novel\nconceptual framework which is based on copulas. However, our approach does not\nimpose the copula on a vector of counts; instead the joint distribution is\ndetermined by imposing a copula function on a vector of associated continuous\nrandom variables. This specific construction avoids conceptual difficulties\nresulting from the joint distribution of discrete random variables yet it keeps\nthe properties of the Poisson process marginally. We employ Markov chain theory\nand the notion of weak dependence to study ergodicity and stationarity of the\nmodels we consider. We obtain easily verifiable conditions for both linear and\nlog-linear models under both theoretical frameworks. Suitable estimating\nequations are suggested for estimating unknown model parameters. The large\nsample properties of the resulting estimators are studied in detail. The work\nconcludes with some simulations and a real data example. \n\n"}
{"id": "1704.02328", "contents": "Title: GUT Models at Current and Future Hadron Colliders and Implications to\n  Dark Matter Searches Abstract: Grand Unified Theories (GUT) offer an elegant and unified description of\nelectromagnetic, weak and strong interactions at high energy scales. A\nphenomenological and exciting possibility to grasp GUT is to search for TeV\nscale observables arising from Abelian groups embedded in GUT constructions.\nThat said, we use dilepton data (ee and $\\mu\\mu$) that has been proven to be a\ngolden channel for a wide variety of new phenomena expected in theories beyond\nthe Standard Model to probe GUT-inspired models. Since heavy dilepton\nresonances feature high signal selection efficiencies and relatively\nwell-understood backgrounds, stringent and reliable bounds can be placed on the\nmass of the $Z^{\\prime}$ gauge boson arising in such theories. In this work, we\nobtain 95\\% C.L. limits on the $Z^{\\prime}$ mass for several GUT-models using\ncurrent and future proton-proton colliders with $\\sqrt{s}= 13~{\\rm TeV},\\,\n33~{\\rm TeV},\\,{\\rm and}\\, 100$~TeV, and put them into perspective with dark\nmatter searches in light of the next generation of direct detection\nexperiments. \n\n"}
{"id": "1704.02531", "contents": "Title: Three Skewed Matrix Variate Distributions Abstract: Three-way data can be conveniently modelled by using matrix variate\ndistributions. Although there has been a lot of work for the matrix variate\nnormal distribution, there is little work in the area of matrix skew\ndistributions. Three matrix variate distributions that incorporate skewness, as\nwell as other flexible properties such as concentration, are discussed.\nEquivalences to multivariate analogues are presented, and moment generating\nfunctions are derived. Maximum likelihood parameter estimation is discussed,\nand simulated data is used for illustration. \n\n"}
{"id": "1704.03924", "contents": "Title: A Tutorial on Kernel Density Estimation and Recent Advances Abstract: This tutorial provides a gentle introduction to kernel density estimation\n(KDE) and recent advances regarding confidence bands and geometric/topological\nfeatures. We begin with a discussion of basic properties of KDE: the\nconvergence rate under various metrics, density derivative estimation, and\nbandwidth selection. Then, we introduce common approaches to the construction\nof confidence intervals/bands, and we discuss how to handle bias. Next, we talk\nabout recent advances in the inference of geometric and topological features of\na density function using KDE. Finally, we illustrate how one can use KDE to\nestimate a cumulative distribution function and a receiver operating\ncharacteristic curve. We provide R implementations related to this tutorial at\nthe end. \n\n"}
{"id": "1704.08127", "contents": "Title: Natural Supersymmetry from the Yukawa Deflect Mediations Abstract: The natural supersymmetry (SUSY) requires light stop quarks, light sbottom\nquark, and gluino to be around one TeV or lighter. The first generation squarks\ncan be effectively large which does not introduce any hierarchy problem in\norder to escape the constraints from LHC. In this paper we consider a Yukawa\ndeflect medation to realize the effective natural supersymmetry where the\ninteraction between squarks and messenger are made natural under certain\nFrogget-Nelson $U(1)_X$ charge. The first generation squarks obtain large and\npostive contribution from the yukawa deflect mediation. The corresponding\nphenomenology as well as sparticle spectrum are discussed in detail. \n\n"}
{"id": "1705.01715", "contents": "Title: Directed Networks with a Differentially Private Bi-degree Sequence Abstract: Although a lot of approaches are developed to release network data with a\ndifferentially privacy guarantee, inference using noisy data in many network\nmodels is still unknown or not properly explored. In this paper, we release the\nbi-degree sequences of directed networks using the Laplace mechanism and use\nthe $p_0$ model for inferring the degree parameters. The $p_0$ model is an\nexponential random graph model with the bi-degree sequence as its exclusively\nsufficient statistic. We show that the estimator of the parameter without the\ndenoised process is asymptotically consistent and normally distributed. This is\ncontrast sharply with some known results that valid inference such as the\nexistence and consistency of the estimator needs the denoised process. Along\nthe way, a new phenomenon is revealed in which an additional variance factor\nappears in the asymptotic variance of the estimator when the noise becomes\nlarge. Further, we propose an efficient algorithm for finding the closet point\nlying in the set of all graphical bi-degree sequences under the global $L_1$\noptimization problem. Numerical studies demonstrate our theoretical findings. \n\n"}
{"id": "1705.01788", "contents": "Title: An optimal transportation approach for assessing almost stochastic order Abstract: When stochastic dominance $F\\leq_{st}G$ does not hold, we can improve\nagreement to stochastic order by suitably trimming both distributions. In this\nwork we consider the $L_2-$Wasserstein distance, $\\mathcal W_2$, to stochastic\norder of these trimmed versions. Our characterization for that distance\nnaturally leads to consider a $\\mathcal W_2$-based index of disagreement with\nstochastic order, $\\varepsilon_{\\mathcal W_2}(F,G)$. We provide asymptotic\nresults allowing to test $H_0: \\varepsilon_{\\mathcal W_2}(F,G)\\geq\n\\varepsilon_0$ vs $H_a: \\varepsilon_{\\mathcal W_2}(F,G)<\\varepsilon_0$, that,\nunder rejection, would give statistical guarantee of almost stochastic\ndominance. We include a simulation study showing a good performance of the\nindex under the normal model. \n\n"}
{"id": "1705.02037", "contents": "Title: Persistence Terrace for Topological Inference of Point Cloud Data Abstract: Topological data analysis (TDA) is a rapidly developing collection of methods\nfor studying the shape of point cloud and other data types. One popular\napproach, designed to be robust to noise and outliers, is to first use a\nsmoothing function to convert the point cloud into a manifold and then apply\npersistent homology to a Morse filtration. A significant challenge is that this\nsmoothing process involves the choice of a parameter and persistent homology is\nhighly sensitive to that choice; moreover, important scale information is lost.\nWe propose a novel topological summary plot, called a persistence terrace, that\nincorporates a wide range of smoothing parameters and is robust, multi-scale,\nand parameter-free. This plot allows one to isolate distinct topological\nsignals that may have merged for any fixed value of the smoothing parameter,\nand it also allows one to infer the size and point density of the topological\nfeatures. We illustrate our method in some simple settings where noise is a\nserious issue for existing frameworks and then we apply it to a real data set\nby counting muscle fibers in a cross-sectional image. \n\n"}
{"id": "1705.03566", "contents": "Title: Spatial Random Sampling: A Structure-Preserving Data Sketching Tool Abstract: Random column sampling is not guaranteed to yield data sketches that preserve\nthe underlying structures of the data and may not sample sufficiently from\nless-populated data clusters. Also, adaptive sampling can often provide\naccurate low rank approximations, yet may fall short of producing descriptive\ndata sketches, especially when the cluster centers are linearly dependent.\nMotivated by that, this paper introduces a novel randomized column sampling\ntool dubbed Spatial Random Sampling (SRS), in which data points are sampled\nbased on their proximity to randomly sampled points on the unit sphere. The\nmost compelling feature of SRS is that the corresponding probability of\nsampling from a given data cluster is proportional to the surface area the\ncluster occupies on the unit sphere, independently from the size of the cluster\npopulation. Although it is fully randomized, SRS is shown to provide\ndescriptive and balanced data representations. The proposed idea addresses a\npressing need in data science and holds potential to inspire many novel\napproaches for analysis of big data. \n\n"}
{"id": "1705.04843", "contents": "Title: Last Electroweak WIMP Standing: Pseudo-Dirac Higgsino Status and Compact\n  Stars as Future Probes Abstract: Electroweak WIMPs are under intense scrutiny from direct detection, indirect\ndetection, and collider experiments. Nonetheless the pure (pseudo-Dirac)\nhiggsino, one of the simplest such WIMPs, remains elusive. We present an\nup-to-date assessment of current experimental constraints on neutralino dark\nmatter. The strongest bound on pure higgsino dark matter currently may arise\nfrom AMS-02 measurements of antiprotons, though the interpretation of these\nresults has sizable uncertainty. We discuss whether future astrophysical\nobservations could offer novel ways to test higgsino dark matter, especially in\nthe challenging regime with order MeV mass splitting between the two neutral\nhiggsinos. We find that heating of white dwarfs by annihilation of higgsinos\ncaptured via inelastic scattering could be one useful probe, although it will\nrequire challenging observations of distant dwarf galaxies or a convincing case\nto be made for substantial dark matter content in Omega Centauri, a globular\ncluster that may be a remnant of a disrupted dwarf galaxy. White dwarfs and\nneutron stars give a target for astronomical observations that could eventually\nhelp to close the last, most difficult corner of parameter space for dark\nmatter with weak interactions. \n\n"}
{"id": "1705.05752", "contents": "Title: Limitations of design-based causal inference and A/B testing under\n  arbitrary and network interference Abstract: Randomized experiments on a network often involve interference between\nconnected units; i.e., a situation in which an individual's treatment can\naffect the response of another individual. Current approaches to deal with\ninterference, in theory and in practice, often make restrictive assumptions on\nits structure---for instance, assuming that interference is local---even when\nusing otherwise nonparametric inference strategies. This reliance on explicit\nrestrictions on the interference mechanism suggests a shared intuition that\ninference is impossible without any assumptions on the interference structure.\nIn this paper, we begin by formalizing this intuition in the context of a\nclassical nonparametric approach to inference, referred to as design-based\ninference of causal effects. Next, we show how, always in the context of\ndesign-based inference, even parametric structural assumptions that allow the\nexistence of unbiased estimators, cannot guarantee a decreasing variance even\nin the large sample limit. This lack of concentration in large samples is often\nobserved empirically, in randomized experiments in which interference of some\nform is expected to be present. This result has direct consequences for the\ndesign and analysis of large experiments---for instance, in online social\nplatforms---where the belief is that large sample sizes automatically guarantee\nsmall variance. More broadly, our results suggest that although strategies for\ncausal inference in the presence of interference borrow their formalism and\nmain concepts from the traditional causal inference literature, much of the\nintuition from the no-interference case do not easily transfer to the\ninterference setting. \n\n"}
{"id": "1705.06655", "contents": "Title: First Dark Matter Search Results from the XENON1T Experiment Abstract: We report the first dark matter search results from XENON1T, a\n$\\sim$2000-kg-target-mass dual-phase (liquid-gas) xenon time projection chamber\nin operation at the Laboratori Nazionali del Gran Sasso in Italy and the first\nton-scale detector of this kind. The blinded search used 34.2 live days of data\nacquired between November 2016 and January 2017. Inside the (1042$\\pm$12) kg\nfiducial mass and in the [5, 40] $\\mathrm{keV}_{\\mathrm{nr}}$ energy range of\ninterest for WIMP dark matter searches, the electronic recoil background was\n$(1.93 \\pm 0.25) \\times 10^{-4}$ events/(kg $\\times$ day $\\times\n\\mathrm{keV}_{\\mathrm{ee}}$), the lowest ever achieved in a dark matter\ndetector. A profile likelihood analysis shows that the data is consistent with\nthe background-only hypothesis. We derive the most stringent exclusion limits\non the spin-independent WIMP-nucleon interaction cross section for WIMP masses\nabove 10 GeV/c${}^2$, with a minimum of 7.7 $\\times 10^{-47}$ cm${}^2$ for\n35-GeV/c${}^2$ WIMPs at 90% confidence level. \n\n"}
{"id": "1705.08020", "contents": "Title: Selective inference for effect modification via the lasso Abstract: Effect modification occurs when the effect of the treatment on an outcome\nvaries according to the level of other covariates and often has important\nimplications in decision making. When there are tens or hundreds of covariates,\nit becomes necessary to use the observed data to select a simpler model for\neffect modification and then make valid statistical inference. We propose a two\nstage procedure to solve this problem. First, we use Robinson's transformation\nto decouple the nuisance parameters from the treatment effect of interest and\nuse machine learning algorithms to estimate the nuisance parameters. Next,\nafter plugging in the estimates of the nuisance parameters, we use the Lasso to\nchoose a low-complexity model for effect modification. Compared to a full model\nconsisting of all the covariates, the selected model is much more\ninterpretable. Compared to the univariate subgroup analyses, the selected model\ngreatly reduces the number of false discoveries. We show that the conditional\nselective inference for the selected model is asymptotically valid given the\nrate assumptions in classical semiparametric regression. Extensive simulation\nstudies are conducted to verify the asymptotic results and an epidemiological\napplication is used to demonstrate the method. \n\n"}
{"id": "1705.08527", "contents": "Title: Causal inference for social network data Abstract: We describe semiparametric estimation and inference for causal effects using\nobservational data from a single social network. Our asymptotic results are the\nfirst to allow for dependence of each observation on a growing number of other\nunits as sample size increases. In addition, while previous methods have\nimplicitly permitted only one of two possible sources of dependence among\nsocial network observations, we allow for both dependence due to transmission\nof information across network ties and for dependence due to latent\nsimilarities among nodes sharing ties. We propose new causal effects that are\nspecifically of interest in social network settings, such as interventions on\nnetwork ties and network structure. We use our methods to reanalyze an\ninfluential and controversial study that estimated causal peer effects of\nobesity using social network data from the Framingham Heart Study; after\naccounting for network structure we find no evidence for causal peer effects. \n\n"}
{"id": "1705.09292", "contents": "Title: Coannihilation without chemical equilibrium Abstract: Chemical equilibrium is a commonly made assumption in the freeze-out\ncalculation of coannihilating dark matter. We explore the possible failure of\nthis assumption and find a new conversion-driven freeze-out mechanism.\nConsidering a representative simplified model inspired by supersymmetry with a\nneutralino- and sbottom-like particle we find regions in parameter space with\nvery small couplings accommodating the measured relic density. In this region\nfreeze-out takes place out of chemical equilibrium and dark matter\nself-annihilation is thoroughly inefficient. The relic density is governed\nprimarily by the size of the conversion terms in the Boltzmann equations. Due\nto the small dark matter coupling the parameter region is immune to direct\ndetection but predicts an interesting signature of disappearing tracks or\ndisplaced vertices at the LHC. Unlike freeze-in or superWIMP scenarios,\nconversion-driven freeze-out is not sensitive to the initial conditions at the\nend of reheating. \n\n"}
{"id": "1705.09599", "contents": "Title: Nearly Semiparametric Efficient Estimation of Quantile Regression Abstract: As a competitive alternative to least squares regression, quantile regression\nis popular in analyzing heterogenous data. For quantile regression model\nspecified for one single quantile level $\\tau$, major difficulties of\nsemiparametric efficient estimation are the unavailability of a parametric\nefficient score and the conditional density estimation. In this paper, with the\nhelp of the least favorable submodel technique, we first derive the\nsemiparametric efficient scores for linear quantile regression models that are\nassumed for a single quantile level, multiple quantile levels and all the\nquantile levels in $(0,1)$ respectively. Our main discovery is a one-step\n(nearly) semiparametric efficient estimation for the regression coefficients of\nthe quantile regression models assumed for multiple quantile levels, which has\nseveral advantages: it could be regarded as an optimal way to pool information\nacross multiple/other quantiles for efficiency gain; it is computationally\nfeasible and easy to implement, as the initial estimator is easily available;\ndue to the nature of quantile regression models under investigation, the\nconditional density estimation is straightforward by plugging in an initial\nestimator. The resulting estimator is proved to achieve the corresponding\nsemiparametric efficiency lower bound under regularity conditions. Numerical\nstudies including simulations and an example of birth weight of children\nconfirms that the proposed estimator leads to higher efficiency compared with\nthe Koenker-Bassett quantile regression estimator for all quantiles of\ninterest. \n\n"}
{"id": "1706.00103", "contents": "Title: From patterned response dependency to structured covariate dependency:\n  categorical-pattern-matching Abstract: Data generated from a system of interest typically consists of measurements\nfrom an ensemble of subjects across multiple response and covariate features,\nand is naturally represented by one response-matrix against one\ncovariate-matrix. Likely each of these two matrices simultaneously embraces\nheterogeneous data types: continuous, discrete and categorical. Here a matrix\nis used as a practical platform to ideally keep hidden dependency among/between\nsubjects and features intact on its lattice. Response and covariate dependency\nis individually computed and expressed through mutliscale blocks via a newly\ndeveloped computing paradigm named Data Mechanics. We propose a categorical\npattern matching approach to establish causal linkages in a form of information\nflows from patterned response dependency to structured covariate dependency.\nThe strength of an information flow is evaluated by applying the combinatorial\ninformation theory. This unified platform for system knowledge discovery is\nillustrated through five data sets. In each illustrative case, an information\nflow is demonstrated as an organization of discovered knowledge loci via\nemergent visible and readable heterogeneity. This unified approach\nfundamentally resolves many long standing issues, including statistical\nmodeling, multiple response, renormalization and feature selections, in data\nanalysis, but without involving man-made structures and distribution\nassumptions. The results reported here enhance the idea that linking patterns\nof response dependency to structures of covariate dependency is the true\nphilosophical foundation underlying data-driven computing and learning in\nsciences. \n\n"}
{"id": "1706.01139", "contents": "Title: Lorentz-covariant coordinate-space representation of the leading\n  hadronic contribution to the anomalous magnetic moment of the muon Abstract: We present a Lorentz-covariant, Euclidean coordinate-space expression for the\nhadronic vacuum polarisation, the Adler function and the leading hadronic\ncontribution to the anomalous magnetic moment of the muon. The representation\noffers a lot of flexibility for an implementation in lattice QCD. We expect it\nto be particularly helpful for the quark-line disconnected contributions. \n\n"}
{"id": "1706.02336", "contents": "Title: The Semi-Hooperon: Gamma-ray and anti-proton excesses in the Galactic\n  Center Abstract: A puzzling excess in gamma-rays at GeV energies has been observed in the\ncenter of our galaxy using Fermi-LAT data. Its origin is still unknown, but it\nis well fitted by Weakly Interacting Massive Particles (WIMPs) annihilations\ninto quarks with a cross section around $10^{-26}{\\rm cm^3 s^{-1}}$ with masses\nof $20-50$~GeV, scenario which is promptly revisited. An excess favoring\nsimilar WIMP properties has also been seen in anti-protons with AMS-02 data\npotentially coming from the Galactic Center as well. In this work, we explore\nthe possibility of fitting these excesses in terms of semi-annihilating dark\nmatter, dubbed as semi-Hooperon, with the process ${\\rm WIMP\\, WIMP \\rightarrow\nWIMP\\, X}$ being responsible for the gamma-ray excess, where X=h,Z. An\ninteresting feature of semi-annihilations is the change in the relic density\nprediction compared to the standard case, and the possibility to alleviate\nstringent limits stemming from direct detection searches. Moreover, we discuss\nwhich models might give rise to a successful semi-Hooperon setup in the context\nof $\\mathcal{Z}_3$,$\\mathcal{Z}_4$ and extra \"dark\" gauge symmetries. \n\n"}
{"id": "1706.02563", "contents": "Title: Jeffreys priors for mixture estimation: properties and alternatives Abstract: While Jeffreys priors usually are well-defined for the parameters of mixtures\nof distributions, they are not available in closed form. Furthermore, they\noften are improper priors. Hence, they have never been used to draw inference\non the mixture parameters. The implementation and the properties of Jeffreys\npriors in several mixture settings are studied. It is shown that the associated\nposterior distributions most often are improper. Nevertheless, the Jeffreys\nprior for the mixture weights conditionally on the parameters of the mixture\ncomponents will be shown to have the property of conservativeness with respect\nto the number of components, in case of overfitted mixture and it can be\ntherefore used as a default priors in this context. \n\n"}
{"id": "1706.02702", "contents": "Title: Hamiltonian approach to QCD in Coulomb gauge - a survey of recent\n  results Abstract: I report on recent results obtained within the Hamiltonian approach to QCD in\nCoulomb gauge. Furthermore this approach is compared to recent lattice data,\nwhich were obtained by an alternative gauge fixing method and which show an\nimproved agreement with the continuum results. By relating the Gribov\nconfinement scenario to the center vortex picture of confinement it is shown\nthat the Coulomb string tension is tied to the spatial string tension. For the\nquark sector a vacuum wave functional is used which explicitly contains the\ncoupling of the quarks to the transverse gluons and which results in\nvariational equations which are free of ultraviolet divergences. The\nvariational approach is extended to finite temperatures by compactifying a\nspatial dimension. The effective potential of the Polyakov loop is evaluated\nfrom the zero-temperature variational solution. For pure Yang--Mills theory,\nthe deconfinement phase transition is found to be second order for SU(2) and\nfirst order for SU(3), in agreement with the lattice results. The corresponding\ncritical temperatures are found to be $275 \\, \\mathrm{MeV}$ and $280 \\,\n\\mathrm{MeV}$, respectively. When quarks are included, the deconfinement\ntransition turns into a cross-over. From the dual and chiral quark condensate\none finds pseudo-critical temperatures of $198 \\, \\mathrm{MeV}$ and $170 \\,\n\\mathrm{MeV}$, respectively, for the deconfinement and chiral transition. \n\n"}
{"id": "1706.03400", "contents": "Title: A Prototype Knockoff Filter for Group Selection with FDR Control Abstract: In many applications, we need to study a linear regression model that\nconsists of a response variable and a large number of potential explanatory\nvariables and determine which variables are truly associated with the response.\nIn 2015, Barber and Candes introduced a new variable selection procedure called\nthe knockoff filter to control the false discovery rate (FDR) and proved that\nthis method achieves exact FDR control. In this paper, we propose a prototype\nknockoff filter for group selection by extending the Reid-Tibshirani prototype\nmethod. Our prototype knockoff filter improves the computational efficiency and\nstatistical power of the Reid-Tibshirani prototype method when it is applied\nfor group selection. In some cases when the group features are spanned by one\nor a few hidden factors, we demonstrate that the PCA prototype knockoff filter\noutperforms the Dai-Barber group knockoff filter. We present several numerical\nexperiments to compare our prototype knockoff filter with the Reid-Tibshirani\nprototype method and the group knockoff filter. We have also conducted some\nanalysis of the knockoff filter. Our analysis reveals that some knockoff path\nmethod statistics, including the Lasso path statistic, may lead to loss of\npower for certain design matrices and a specially designed response even if\ntheir signal strengths are still relatively strong. \n\n"}
{"id": "1706.04152", "contents": "Title: Learning to Detect Sepsis with a Multitask Gaussian Process RNN\n  Classifier Abstract: We present a scalable end-to-end classifier that uses streaming physiological\nand medication data to accurately predict the onset of sepsis, a\nlife-threatening complication from infections that has high mortality and\nmorbidity. Our proposed framework models the multivariate trajectories of\ncontinuous-valued physiological time series using multitask Gaussian processes,\nseamlessly accounting for the high uncertainty, frequent missingness, and\nirregular sampling rates typically associated with real clinical data. The\nGaussian process is directly connected to a black-box classifier that predicts\nwhether a patient will become septic, chosen in our case to be a recurrent\nneural network to account for the extreme variability in the length of patient\nencounters. We show how to scale the computations associated with the Gaussian\nprocess in a manner so that the entire system can be discriminatively trained\nend-to-end using backpropagation. In a large cohort of heterogeneous inpatient\nencounters at our university health system we find that it outperforms several\nbaselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under\nthe Receiver Operating Characteristic and Precision Recall curves as compared\nto the NEWS score currently used by our hospital. \n\n"}
{"id": "1706.04226", "contents": "Title: On primordial black holes from an inflection point Abstract: Recently, it has been claimed that inflationary models with an inflection\npoint in the scalar potential can produce a large resonance in the power\nspectrum of curvature perturbation. In this paper however we show that the\nprevious analyses are incorrect. The reason is twofold: firstly, the inflaton\nis over-shot from a stage of standard inflation and so deviates from the\nslow-roll attractor before reaching the inflection. Secondly, on the (or close\nto) the inflection point, the ultra-slow-roll trajectory supersede the\nslow-roll one and thus, the slow-roll approximations used in the literature\ncannot be used. We then reconsider the model and provide a recipe for how to\nproduce nevertheless a large peak in the matter power spectrum via fine-tuning\nof parameters. \n\n"}
{"id": "1706.05260", "contents": "Title: On the domain of elliptic operators defined in subsets of Wiener spaces Abstract: Let $X$ be a separable Banach space endowed with a non-degenerate centered\nGaussian measure $\\mu$. The associated Cameron-Martin space is denoted by $H$.\nConsider two sufficiently regular convex functions $U:X\\rightarrow\\mathbb{R}$\nand $G:X\\rightarrow \\mathbb{R}$. We let $\\nu=e^{-U}\\mu$ and\n$\\Omega=G^{-1}(-\\infty,0]$. In this paper we are interested in the domain of\nthe the self-adjoint operator associated with the quadratic form \\begin{gather}\n(\\psi,\\varphi)\\mapsto\n\\int_\\Omega\\langle\\nabla_H\\psi,\\nabla_H\\varphi\\rangle_Hd\\nu\\qquad\\psi,\\varphi\\in\nW^{1,2}(\\Omega,\\nu).\\qquad\\qquad (\\star) \\end{gather} In particular we obtain a\ncomplete characterization of the Ornstein-Uhlenbeck operator on half-spaces,\nnamely if $U\\equiv 0$ and $G$ is an affine function, then the domain of the\noperator defined via $(\\star)$ is the space \\[\\{u\\in W^{2,2}(\\Omega,\\mu)\\,|\\,\n\\langle\\nabla_H u(x),\\nabla_H G(x)\\rangle_H=0\\text{ for }\\rho\\text{-a.e. }x\\in\nG^{-1}(0)\\},\\] where $\\rho$ is the Feyel-de La Pradelle Hausdorff-Gauss surface\nmeasure. \n\n"}
{"id": "1706.10179", "contents": "Title: Lasso Meets Horseshoe : A Survey Abstract: The goal of this paper is to contrast and survey the major advances in two of\nthe most commonly used high-dimensional techniques, namely, the Lasso and\nhorseshoe regularization. Lasso is a gold standard for predictor selection\nwhile horseshoe is a state-of-the-art Bayesian estimator for sparse signals.\nLasso is fast and scalable and uses convex optimization whilst the horseshoe is\nnon-convex. Our novel perspective focuses on three aspects: (i) theoretical\noptimality in high dimensional inference for the Gaussian sparse model and\nbeyond, (ii) efficiency and scalability of computation and (iii) methodological\ndevelopment and performance. \n\n"}
{"id": "1707.01254", "contents": "Title: Regression approaches for Approximate Bayesian Computation Abstract: This book chapter introduces regression approaches and regression adjustment\nfor Approximate Bayesian Computation (ABC). Regression adjustment adjusts\nparameter values after rejection sampling in order to account for the imperfect\nmatch between simulations and observations. Imperfect match between simulations\nand observations can be more pronounced when there are many summary statistics,\na phenomenon coined as the curse of dimensionality. Because of this imperfect\nmatch, credibility intervals obtained with regression approaches can be\ninflated compared to true credibility intervals. The chapter presents the main\nconcepts underlying regression adjustment. A theorem that compares theoretical\nproperties of posterior distributions obtained with and without regression\nadjustment is presented. Last, a practical application of regression adjustment\nin population genetics shows that regression adjustment shrinks posterior\ndistributions compared to rejection approaches, which is a solution to avoid\ninflated credibility intervals. \n\n"}
{"id": "1707.02333", "contents": "Title: Robust Wald-type tests for non-homogeneous observations based on minimum\n  density power divergence estimator Abstract: This paper considers the problem of robust hypothesis testing under\nnon-identically distributed data. We propose Wald-type tests for both simple\nand composite hypothesis for independent but non-homogeneous observations based\non the robust minimum density power divergence estimator of the common\nunderlying parameter. Asymptotic and theoretical robustness properties of the\nproposed tests have been discussed. Application to the problem of testing the\ngeneral linear hypothesis in a generalized linear model with fixed-design has\nbeen considered in detail with specific illustrations for its special cases\nunder normal and Poisson distributions. \n\n"}
{"id": "1707.05284", "contents": "Title: Strange Hadron Spectroscopy with a Secondary KL Beam at GlueX Abstract: We propose to create a secondary beam of neutral kaons in Hall D at Jefferson\nLab to be used with the GlueX experimental setup for strange hadron\nspectroscopy. A flux on the order of 3 x 10^4 KL/s will allow a broad range of\nmeasurements to be made by improving the statistics of previous data obtained\non hydrogen targets by three orders of magnitude. Use of a deuteron target will\nprovide first measurements on the neutron which is {\\it terra incognita}.\n  The experiment will measure both differential cross sections and\nself-analyzed polarizations of the produced {\\Lambda}, {\\Sigma}, {\\Xi}, and\n{\\Omega} hyperons using the GlueX detector at the Jefferson Lab Hall D. The\nmeasurements will span c.m. cos{\\theta} from -0.95 to 0.95 in the c.m. range\nabove W = 1490 MeV and up to 3500 MeV. These new GlueX data will greatly\nconstrain partial-wave analyses and reduce model-dependent uncertainties in the\nextraction of strange resonance properties (including pole positions), and\nprovide a new benchmark for comparisons with QCD-inspired models and lattice\nQCD calculations.\n  The proposed facility will also have an impact in the strange meson sector by\nproviding measurements of the final-state K{\\pi} system from threshold up to 2\nGeV invariant mass to establish and improve on the pole positions and widths of\nall K*(K{\\pi}) P-wave states as well as for the S-wave scalar meson\n{\\kappa}(800). \n\n"}
{"id": "1707.06587", "contents": "Title: Inflation and Dark Matter in the Inert Doublet Model Abstract: We discuss inflation and dark matter in the inert doublet model coupled\nnon-minimally to gravity where the inert doublet is the inflaton and the\nneutral scalar part of the doublet is the dark matter candidate. We calculate\nthe various inflationary parameters like $n_s$, $r$ and $P_s$ and then proceed\nto the reheating phase where the inflaton decays into the Higgs and other gauge\nbosons which are non-relativistic owing to high effective masses. These bosons\nfurther decay or annihilate to give relativistic fermions which are finally\nresponsible for reheating the universe. At the end of the reheating phase, the\ninert doublet which was the inflaton enters into thermal equilibrium with the\nrest of the plasma and its neutral component later freezes out as cold dark\nmatter with a mass of about 2 TeV. \n\n"}
{"id": "1707.06852", "contents": "Title: A Statistical Perspective on Inverse and Inverse Regression Problems Abstract: Inverse problems, where in broad sense the task is to learn from the noisy\nresponse about some unknown function, usually represented as the argument of\nsome known functional form, has received wide attention in the general\nscientific disciplines. How- ever, in mainstream statistics such inverse\nproblem paradigm does not seem to be as popular. In this article we provide a\nbrief overview of such problems from a statistical, particularly Bayesian,\nperspective.\n  We also compare and contrast the above class of problems with the perhaps\nmore statistically familiar inverse regression problems, arguing that this\nclass of problems contains the traditional class of inverse problems. In course\nof our review we point out that the statistical literature is very scarce with\nrespect to both the inverse paradigms, and substantial research work is still\nnecessary to develop the fields. \n\n"}
{"id": "1707.08298", "contents": "Title: Variable Selection for High-dimensional Generalized Linear Models using\n  an Iterated Conditional Modes/Medians Algorithm Abstract: High-dimensional linear and nonlinear models have been extensively used to\nidentify associations between response and explanatory variables. The variable\nselection problem is commonly of interest in the presence of massive and\ncomplex data. An empirical Bayes model for high-dimensional generalized linear\nmodels (GLMs) is considered in this paper. The extension of the Iterated\nConditional Modes/Medians (ICM/M) algorithm is proposed to build up a GLM. With\nthe construction of pseudodata and pseudovariances based on iteratively\nreweighted least squares (IRLS), conditional modes are employed to obtain\ndata-drive optimal values for hyperparameters and conditional medians are used\nto estimate regression coefficients. With a spike-and-slab prior for each\ncoefficient, a conditional median can enforce variable estimation and selection\nat the same time. The ICM/M algorithm can also incorporate more complicated\nprior by taking the network structural information into account through the\nIsing model prior. Here we focus on two extensively used models for genomic\ndata: binary logistic and Cox's proportional hazards models. The performance of\nthe proposed method is demonstrated through both simulation studies and real\ndata examples. The implementation of the ICM/M algorithm for both linear and\nnonlinear models can be found in the icmm R package which is freely available\non CRAN. \n\n"}
{"id": "1708.00205", "contents": "Title: Dynamic Linear Discriminant Analysis in High Dimensional Space Abstract: High-dimensional data that evolve dynamically feature predominantly in the\nmodern data era. As a partial response to this, recent years have seen\nincreasing emphasis to address the dimensionality challenge. However, the\nnon-static nature of these datasets is largely ignored. This paper addresses\nboth challenges by proposing a novel yet simple dynamic linear programming\ndiscriminant (DLPD) rule for binary classification. Different from the usual\nstatic linear discriminant analysis, the new method is able to capture the\nchanging distributions of the underlying populations by modeling their means\nand covariances as smooth functions of covariates of interest. Under an\napproximate sparse condition, we show that the conditional misclassification\nrate of the DLPD rule converges to the Bayes risk in probability uniformly over\nthe range of the variables used for modeling the dynamics, when the\ndimensionality is allowed to grow exponentially with the sample size. The\nminimax lower bound of the estimation of the Bayes risk is also established,\nimplying that the misclassification rate of our proposed rule is minimax-rate\noptimal. The promising performance of the DLPD rule is illustrated via\nextensive simulation studies and the analysis of a breast cancer dataset. \n\n"}
{"id": "1708.00430", "contents": "Title: Breaking the curse of dimensionality in regression Abstract: Models with many signals, high-dimensional models, often impose structures on\nthe signal strengths. The common assumption is that only a few signals are\nstrong and most of the signals are zero or close (collectively) to zero.\nHowever, such a requirement might not be valid in many real-life applications.\nIn this article, we are interested in conducting large-scale inference in\nmodels that might have signals of mixed strengths. The key challenge is that\nthe signals that are not under testing might be collectively non-negligible\n(although individually small) and cannot be accurately learned. This article\ndevelops a new class of tests that arise from a moment matching formulation. A\nvirtue of these moment-matching statistics is their ability to borrow strength\nacross features, adapt to the sparsity size and exert adjustment for testing\ngrowing number of hypothesis. GRoup-level Inference of Parameter, GRIP, test\nharvests effective sparsity structures with hypothesis formulation for an\nefficient multiple testing procedure. Simulated data showcase that GRIPs error\ncontrol is far better than the alternative methods. We develop a minimax\ntheory, demonstrating optimality of GRIP for a broad range of models, including\nthose where the model is a mixture of a sparse and high-dimensional dense\nsignals. \n\n"}
{"id": "1708.02647", "contents": "Title: A Review of Self-Exciting Spatio-Temporal Point Processes and Their\n  Applications Abstract: Self-exciting spatio-temporal point process models predict the rate of events\nas a function of space, time, and the previous history of events. These models\nnaturally capture triggering and clustering behavior, and have been widely used\nin fields where spatio-temporal clustering of events is observed, such as\nearthquake modeling, infectious disease, and crime. In the past several\ndecades, advances have been made in estimation, inference, simulation, and\ndiagnostic tools for self-exciting point process models. In this review, I\ndescribe the basic theory, survey related estimation and inference techniques\nfrom each field, highlight several key applications, and suggest directions for\nfuture research. \n\n"}
{"id": "1708.04887", "contents": "Title: Fixed effects testing in high-dimensional linear mixed models Abstract: Many scientific and engineering challenges -- ranging from pharmacokinetic\ndrug dosage allocation and personalized medicine to marketing mix (4Ps)\nrecommendations -- require an understanding of the unobserved heterogeneity in\norder to develop the best decision making-processes. In this paper, we develop\na hypothesis test and the corresponding p-value for testing for the\nsignificance of the homogeneous structure in linear mixed models. A robust\nmatching moment construction is used for creating a test that adapts to the\nsize of the model sparsity. When unobserved heterogeneity at a cluster level is\nconstant, we show that our test is both consistent and unbiased even when the\ndimension of the model is extremely high. Our theoretical results rely on a new\nfamily of adaptive sparse estimators of the fixed effects that do not require\nconsistent estimation of the random effects. Moreover, our inference results do\nnot require consistent model selection. We showcase that moment matching can be\nextended to nonlinear mixed effects models and to generalized linear mixed\neffects models. In numerical and real data experiments, we find that the\ndeveloped method is extremely accurate, that it adapts to the size of the\nunderlying model and is decidedly powerful in the presence of irrelevant\ncovariates. \n\n"}
{"id": "1708.06982", "contents": "Title: Level set Cox processes Abstract: The log-Gaussian Cox process (LGCP) is a popular point process for modeling\nnon-interacting spatial point patterns. This paper extends the LGCP model to\nhandle data exhibiting fundamentally different behaviors in different\nsubregions of the spatial domain. The aim of the analyst might be either to\nidentify and classify these regions, to perform kriging, or to derive some\nproperties of the parameters driving the random field in one or several of the\nsubregions. The extension is based on replacing the latent Gaussian random\nfield in the LGCP by a latent spatial mixture model. The mixture model is\nspecified using a latent, categorically valued, random field induced by level\nset operations on a Gaussian random field. Conditional on the classification,\nthe intensity surface for each class is modeled by a set of independent\nGaussian random fields. This allows for standard stationary covariance\nstructures, such as the Mat\\'{e}rn family, to be used to model Gaussian random\nfields with some degree of general smoothness but also occasional and\nstructured sharp discontinuities.\n  A computationally efficient MCMC method is proposed for Bayesian inference\nand we show consistency of finite dimensional approximations of the model.\nFinally, the model is fitted to point pattern data derived from a tropical\nrainforest on Barro Colorado island, Panama. We show that the proposed model is\nable to capture behavior for which inference based on the standard LGCP is\nbiased. \n\n"}
{"id": "1708.07675", "contents": "Title: Exotic glueball $0^{\\pm -}$ states in QCD Sum Rules Abstract: The lowest dimension three-gluon currents that couple to the exotic\n$0^{\\pm-}$ glueballs have been constructed using the helicity formalism. Based\non the constructed currents, we obtain new QCD SRs that have been used to\nextract the masses and the decay constants of the scalar exotic $0^{\\pm-}$\nglueballs. We estimate the masses for the scalar state and for the pseudoscalar\nstate to be ${m_+=9.8^{+1.3}_{-1.4}}$ GeV and $m_-=6.8^{+1.1}_{-1.2}$ GeV. \n\n"}
{"id": "1708.08157", "contents": "Title: Characteristic and Universal Tensor Product Kernels Abstract: Maximum mean discrepancy (MMD), also called energy distance or N-distance in\nstatistics and Hilbert-Schmidt independence criterion (HSIC), specifically\ndistance covariance in statistics, are among the most popular and successful\napproaches to quantify the difference and independence of random variables,\nrespectively. Thanks to their kernel-based foundations, MMD and HSIC are\napplicable on a wide variety of domains. Despite their tremendous success,\nquite little is known about when HSIC characterizes independence and when MMD\nwith tensor product kernel can discriminate probability distributions. In this\npaper, we answer these questions by studying various notions of characteristic\nproperty of the tensor product kernel. \n\n"}
{"id": "1708.08278", "contents": "Title: Why optional stopping can be a problem for Bayesians Abstract: Recently, optional stopping has been a subject of debate in the Bayesian\npsychology community. Rouder (2014) argues that optional stopping is no problem\nfor Bayesians, and even recommends the use of optional stopping in practice, as\ndo Wagenmakers et al. (2012). This article addresses the question whether\noptional stopping is problematic for Bayesian methods, and specifies under\nwhich circumstances and in which sense it is and is not. By slightly varying\nand extending Rouder's (2014) experiments, we illustrate that, as soon as the\nparameters of interest are equipped with default or pragmatic priors - which\nmeans, in most practical applications of Bayes factor hypothesis testing -\nresilience to optional stopping can break down. We distinguish between three\ntypes of default priors, each having their own specific issues with optional\nstopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type\nII priors). \n\n"}
{"id": "1708.09305", "contents": "Title: A Pseudo Knockoff Filter for Correlated Features Abstract: In 2015, Barber and Candes introduced a new variable selection procedure\ncalled the knockoff filter to control the false discovery rate (FDR) and prove\nthat this method achieves exact FDR control. Inspired by the work of Barber and\nCandes (2015), we propose and analyze a pseudo-knockoff filter that inherits\nsome advantages of the original knockoff filter and has more flexibility in\nconstructing its knockoff matrix. Moreover, we perform a number of numerical\nexperiments that seem to suggest that the pseudo knockoff filter with the half\nLasso statistic has FDR control and offers more power than the original\nknockoff filter with the Lasso Path or the half Lasso Statistic for the\nnumerical examples that we consider in this paper. Although we cannot establish\nrigorous FDR control for the pseudo knockoff filter, we provide some partial\nanalysis of the pseudo knockoff filter with the half Lasso statistic and\nestablish a uniform FDP bound and an expectation inequality. \n\n"}
{"id": "1709.00092", "contents": "Title: RANK: Large-Scale Inference with Graphical Nonlinear Knockoffs Abstract: Power and reproducibility are key to enabling refined scientific discoveries\nin contemporary big data applications with general high-dimensional nonlinear\nmodels. In this paper, we provide theoretical foundations on the power and\nrobustness for the model-free knockoffs procedure introduced recently in\nCand\\`{e}s, Fan, Janson and Lv (2016) in high-dimensional setting when the\ncovariate distribution is characterized by Gaussian graphical model. We\nestablish that under mild regularity conditions, the power of the oracle\nknockoffs procedure with known covariate distribution in high-dimensional\nlinear models is asymptotically one as sample size goes to infinity. When\nmoving away from the ideal case, we suggest the modified model-free knockoffs\nmethod called graphical nonlinear knockoffs (RANK) to accommodate the unknown\ncovariate distribution. We provide theoretical justifications on the robustness\nof our modified procedure by showing that the false discovery rate (FDR) is\nasymptotically controlled at the target level and the power is asymptotically\none with the estimated covariate distribution. To the best of our knowledge,\nthis is the first formal theoretical result on the power for the knockoffs\nprocedure. Simulation results demonstrate that compared to existing approaches,\nour method performs competitively in both FDR control and power. A real data\nset is analyzed to further assess the performance of the suggested knockoffs\nprocedure. \n\n"}
{"id": "1709.00232", "contents": "Title: Estimating functions for jump-diffusions Abstract: Asymptotic theory for approximate martingale estimating functions is\ngeneralised to diffusions with finite-activity jumps, when the sampling\nfrequency and terminal sampling time go to infinity. Rate optimality and\nefficiency are of particular concern. Under mild assumptions, it is shown that\nestimators of drift, diffusion, and jump parameters are consistent and\nasymptotically normal, as well as rate-optimal for the drift and jump\nparameters. Additional conditions are derived, which ensure rate-optimality for\nthe diffusion parameter as well as efficiency for all parameters. The findings\nindicate a potentially fruitful direction for the further development of\nestimation for jump-diffusions. \n\n"}
{"id": "1709.00640", "contents": "Title: When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests,\n  $\\ell_2$-consistency and Neuroscience Applications Abstract: Many studies in biomedical and health sciences involve small sample sizes due\nto logistic or financial constraints. Often, identifying weak (but\nscientifically interesting) associations between a set of predictors and a\nresponse necessitates pooling datasets from multiple diverse labs or groups.\nWhile there is a rich literature in statistical machine learning to address\ndistributional shifts and inference in multi-site datasets, it is less clear\n${\\it when}$ such pooling is guaranteed to help (and when it does not) --\nindependent of the inference algorithms we use. In this paper, we present a\nhypothesis test to answer this question, both for classical and high\ndimensional linear regression. We precisely identify regimes where pooling\ndatasets across multiple sites is sensible, and how such policy decisions can\nbe made via simple checks executable on each site before any data transfer ever\nhappens. With a focus on Alzheimer's disease studies, we present empirical\nresults showing that in regimes suggested by our analysis, pooling a local\ndataset with data from an international study improves power. \n\n"}
{"id": "1709.01781", "contents": "Title: Parameterizations for Ensemble Kalman Inversion Abstract: The use of ensemble methods to solve inverse problems is attractive because\nit is a derivative-free methodology which is also well-adapted to\nparallelization. In its basic iterative form the method produces an ensemble of\nsolutions which lie in the linear span of the initial ensemble. Choice of the\nparameterization of the unknown field is thus a key component of the success of\nthe method. We demonstrate how both geometric ideas and hierarchical ideas can\nbe used to design effective parameterizations for a number of applied inverse\nproblems arising in electrical impedance tomography, groundwater flow and\nsource inversion. In particular we show how geometric ideas, including the\nlevel set method, can be used to reconstruct piecewise continuous fields, and\nwe show how hierarchical methods can be used to learn key parameters in\ncontinuous fields, such as length-scales, resulting in improved\nreconstructions. Geometric and hierarchical ideas are combined in the level set\nmethod to find piecewise constant reconstructions with interfaces of unknown\ntopology. \n\n"}
{"id": "1709.02532", "contents": "Title: Generalizing Distance Covariance to Measure and Test Multivariate Mutual\n  Dependence Abstract: We propose three measures of mutual dependence between multiple random\nvectors. All the measures are zero if and only if the random vectors are\nmutually independent. The first measure generalizes distance covariance from\npairwise dependence to mutual dependence, while the other two measures are sums\nof squared distance covariance. All the measures share similar properties and\nasymptotic distributions to distance covariance, and capture non-linear and\nnon-monotone mutual dependence between the random vectors. Inspired by complete\nand incomplete V-statistics, we define the empirical measures and simplified\nempirical measures as a trade-off between the complexity and power when testing\nmutual independence. Implementation of the tests is demonstrated by both\nsimulation results and real data examples. \n\n"}
{"id": "1709.04342", "contents": "Title: Model Selection Confidence Sets by Likelihood Ratio Testing Abstract: The traditional activity of model selection aims at discovering a single\nmodel superior to other candidate models. In the presence of pronounced noise,\nhowever, multiple models are often found to explain the same data equally well.\nTo resolve this model selection ambiguity, we introduce the general approach of\nmodel selection confidence sets (MSCSs) based on likelihood ratio testing. A\nMSCS is defined as a list of models statistically indistinguishable from the\ntrue model at a user-specified level of confidence, which extends the familiar\nnotion of confidence intervals to the model-selection framework. Our approach\nguarantees asymptotically correct coverage probability of the true model when\nboth sample size and model dimension increase. We derive conditions under which\nthe MSCS contains all the relevant information about the true model structure.\nIn addition, we propose natural statistics based on the MSCS to measure\nimportance of variables in a principled way that accounts for the overall model\nuncertainty. When the space of feasible models is large, MSCS is implemented by\nan adaptive stochastic search algorithm which samples MSCS models with high\nprobability. The MSCS methodology is illustrated through numerical experiments\non synthetic data and real data examples. \n\n"}
{"id": "1709.04840", "contents": "Title: Semi-standard partial covariance variable selection when irrepresentable\n  conditions fail Abstract: Traditional variable selection methods could fail to be sign consistent when\nirrepresentable conditions are violated. This is especially critical in\nhigh-dimensional settings when the number of predictors exceeds the sample\nsize. In this paper, we propose a new semi-standard partial covariance (SPAC)\napproach which is capable of reducing correlation effects from other covariates\nwhile fully capturing the magnitude of coefficients. The proposed SPAC is\neffective in choosing covariates which have direct effects on the response\nvariable, while eliminating the predictors which are not directly associated\nwith the response but are highly correlated with the relevant predictors. We\nshow that the proposed SPAC method with the Lasso penalty or the smoothly\nclipped absolute deviation (SCAD) penalty possesses strong sign consistency in\nhigh-dimensional settings. Numerical studies and a post-traumatic stress\ndisorder data application also confirm that the proposed method outperforms the\nexisting Lasso, adaptive Lasso, SCAD, Peter-Clark-simple algorithm, and\nfactor-adjusted regularized model selection methods when the irrepresentable\nconditions fail. \n\n"}
{"id": "1709.07799", "contents": "Title: Reconstruction of top-quark mass effects in Higgs pair production and\n  other gluon-fusion processes Abstract: We propose a novel method for the treatment of top-quark mass effects in the\nproduction of $H^{(*)}$, $HH$, $HZ$ and $ZZ$ final states in gluon fusion. We\nshow that it is possible to reconstruct the full top-quark mass dependence of\nthe virtual amplitudes from the corresponding large-$m_t$ expansion and the\nnon-analytic part of the amplitude near the top-quark threshold\n$\\hat{s}=4m_t^2$ with a Pad\\'e ansatz. The reliability of our method is clearly\ndemonstrated by a comparison with the recent NLO result for Higgs pair\nproduction with full top-quark mass dependence. \n\n"}
{"id": "1709.08251", "contents": "Title: Bootstrapping spectral statistics in high dimensions Abstract: Statistics derived from the eigenvalues of sample covariance matrices are\ncalled spectral statistics, and they play a central role in multivariate\ntesting. Although bootstrap methods are an established approach to\napproximating the laws of spectral statistics in low-dimensional problems,\nthese methods are relatively unexplored in the high-dimensional setting. The\naim of this paper is to focus on linear spectral statistics as a class of\nprototypes for developing a new bootstrap in high-dimensions --- and we refer\nto this method as the Spectral Bootstrap. In essence, the method originates\nfrom the parametric bootstrap, and is motivated by the notion that, in high\ndimensions, it is difficult to obtain a non-parametric approximation to the\nfull data-generating distribution. From a practical standpoint, the method is\neasy to use, and allows the user to circumvent the difficulties of complex\nasymptotic formulas for linear spectral statistics. In addition to proving the\nconsistency of the proposed method, we provide encouraging empirical results in\na variety of settings. Lastly, and perhaps most interestingly, we show through\nsimulations that the method can be applied successfully to statistics outside\nthe class of linear spectral statistics, such as the largest sample eigenvalue\nand others. \n\n"}
{"id": "1709.10051", "contents": "Title: Prospects for Improved Understanding of Isotopic Reactor Antineutrino\n  Fluxes Abstract: Predictions of antineutrino fluxes produced by fission isotopes in a nuclear\nreactor have recently received increased scrutiny due to observed differences\nin predicted and measured inverse beta decay (IBD) yields, referred to as the\n'reactor antineutrino flux anomaly.' In this paper, global fits are applied to\nexisting IBD yield measurements to produce constraints on antineutrino\nproduction by individual plutonium and uranium fission isotopes. We find that\nfits including measurements from highly 235U-enriched cores and fits including\nDaya Bay's new fuel evolution result produce discrepant best-fit IBD yields for\n235U and 239Pu. This discrepancy can be alleviated in a global analysis of all\ndatasets through simultaneous fitting of 239Pu, 235U, and 238U yields. The\nmeasured IBD yield of U238 in this analysis is (7.02 +/- 1.65) x 10^-43\ncm2/fission, nearly two standard deviations below existing predictions. Future\nhypothetical IBD yield measurements by short-baseline reactor experiments are\nexamined to determine their possible impact on global understanding of isotopic\nIBD yields. It is found that future improved short-baseline IBD yield\nmeasurements at both high-enriched and low-enriched cores can significantly\nimprove constraints for 235U, 238U, and 239Pu, providing comparable or superior\nprecision to existing conversion- and summation-based antineutrino flux\npredictions. Systematic and experimental requirements for these future\nmeasurements are also investigated. \n\n"}
{"id": "1710.03410", "contents": "Title: A Decision Theoretic Approach to A/B Testing Abstract: A/B testing is ubiquitous within the machine learning and data science\noperations of internet companies. Generically, the idea is to perform a\nstatistical test of the hypothesis that a new feature is better than the\nexisting platform---for example, it results in higher revenue. If the p value\nfor the test is below some pre-defined threshold---often, 0.05---the new\nfeature is implemented. The difficulty of choosing an appropriate threshold has\nbeen noted before, particularly because dependent tests are often done\nsequentially, leading some to propose control of the false discovery rate (FDR)\nrather than use of a single, universal threshold. However, it is still\nnecessary to make an arbitrary choice of the level at which to control FDR.\nHere we suggest a decision-theoretic approach to determining whether to adopt a\nnew feature, which enables automated selection of an appropriate threshold. Our\nmethod has the basic ingredients of any decision-theory problem: a loss\nfunction, action space, and a notion of optimality, for which we choose Bayes\nrisk. However, the loss function and the action space differ from the typical\nchoices made in the literature, which has focused on the theory of point\nestimation. We give some basic results for Bayes-optimal thresholding rules for\nthe feature adoption decision, and give some examples using eBay data. The\nresults suggest that the 0.05 p-value threshold may be too conservative in some\nsettings, but that its widespread use may reflect an ad-hoc means of\ncontrolling multiplicity in the common case of repeatedly testing variants of\nan experiment when the threshold is not reached. \n\n"}
{"id": "1710.03892", "contents": "Title: Variable screening with multiple studies Abstract: Advancement in technology has generated abundant high-dimensional data that\nallows integration of multiple relevant studies. Due to their huge\ncomputational advantage, variable screening methods based on marginal\ncorrelation have become promising alternatives to the popular regularization\nmethods for variable selection. However, all these screening methods are\nlimited to single study so far. In this paper, we consider a general framework\nfor variable screening with multiple related studies, and further propose a\nnovel two-step screening procedure using a self-normalized estimator for\nhigh-dimensional regression analysis in this framework. Compared to the\none-step procedure and rank-based sure independence screening (SIS) procedure,\nour procedure greatly reduces false negative errors while keeping a low false\npositive rate. Theoretically, we show that our procedure possesses the sure\nscreening property with weaker assumptions on signal strengths and allows the\nnumber of features to grow at an exponential rate of the sample size. In\naddition, we relax the commonly used normality assumption and allow\nsub-Gaussian distributions. Simulations and a real transcriptomic application\nillustrate the advantage of our method as compared to the rank-based SIS\nmethod. \n\n"}
{"id": "1710.04093", "contents": "Title: Efficient MCMC for Gibbs Random Fields using pre-computation Abstract: Bayesian inference of Gibbs random fields (GRFs) is often referred to as a\ndoubly intractable problem, since the likelihood function is intractable. The\nexploration of the posterior distribution of such models is typically carried\nout with a sophisticated Markov chain Monte Carlo (MCMC) method, the exchange\nalgorithm (Murray et al., 2006), which requires simulations from the likelihood\nfunction at each iteration. The purpose of this paper is to consider an\napproach to dramatically reduce this computational overhead. To this end we\nintroduce a novel class of algorithms which use realizations of the GRF model,\nsimulated offline, at locations specified by a grid that spans the parameter\nspace. This strategy speeds up dramatically the posterior inference, as\nillustrated on several examples. However, using the pre-computed graphs\nintroduces a noise in the MCMC algorithm, which is no longer exact. We study\nthe theoretical behaviour of the resulting approximate MCMC algorithm and\nderive convergence bounds using a recent theoretical development on approximate\nMCMC methods. \n\n"}
{"id": "1710.06056", "contents": "Title: Asymptotically Optimal Sequential Design for Rank Aggregation Abstract: A sequential design problem for rank aggregation is commonly encountered in\npsychology, politics, marketing, sports, etc. In this problem, a decision maker\nis responsible for ranking $K$ items by sequentially collecting pairwise noisy\ncomparison from judges. The decision maker needs to choose a pair of items for\ncomparison in each step, decide when to stop data collection, and make a final\ndecision after stopping, based on a sequential flow of information. Due to the\ncomplex ranking structure, existing sequential analysis methods are not\nsuitable.\n  In this paper, we formulate the problem under a Bayesian decision framework\nand propose sequential procedures that are asymptotically optimal. These\nprocedures achieve asymptotic optimality by seeking for a balance between\nexploration (i.e. finding the most indistinguishable pair of items) and\nexploitation (i.e. comparing the most indistinguishable pair based on the\ncurrent information). New analytical tools are developed for proving the\nasymptotic results, combining advanced change of measure techniques for\nhandling the level crossing of likelihood ratios and classic large deviation\nresults for martingales, which are of separate theoretical interest in solving\ncomplex sequential design problems. A mirror-descent algorithm is developed for\nthe computation of the proposed sequential procedures. \n\n"}
{"id": "1710.07136", "contents": "Title: Effects of initial spatial phase in radiative neutrino pair emission Abstract: We study radiative neutrino pair emission in deexcitation process of atoms\ntaking into account coherence effect in a macroscopic target system. In the\ncourse of preparing the coherent initial state to enhance the rate, a spatial\nphase factor is imprinted in the macroscopic target. It is shown that this\ninitial spatial phase changes the kinematics of the radiative neutrino pair\nemission. We investigate effects of the initial spatial phase in the photon\nspectrum of the process. It turns out that the initial spatial phase provides\nus significant improvements in exploring neutrino physics such as the\nDirac-Majorana distinction and the cosmic neutrino background. \n\n"}
{"id": "1710.08388", "contents": "Title: A Test for Separability in Covariance Operators of Random Surfaces Abstract: The assumption of separability is a simplifying and very popular assumption\nin the analysis of spatio-temporal or hypersurface data structures. It is often\nmade in situations where the covariance structure cannot be easily estimated,\nfor example because of a small sample size or because of computational storage\nproblems. In this paper we propose a new and very simple test to validate this\nassumption. Our approach is based on a measure of separability which is zero in\nthe case of separability and positive otherwise. The measure can be estimated\nwithout calculating the full non-separable covariance operator. We prove\nasymptotic normality of the corresponding statistic with a limiting variance,\nwhich can easily be estimated from the available data. As a consequence\nquantiles of the standard normal distribution can be used to obtain critical\nvalues and the new test of separability is very easy to implement. In\nparticular, our approach does neither require projections on subspaces\ngenerated by the eigenfunctions of the covariance operator, nor resampling\nprocedures to obtain critical values nor distributional assumptions as used by\nother available methods of constructing tests for separability. We investigate\nthe finite sample performance by means of a simulation study and also provide a\ncomparison with the currently available methodology. Finally, the new procedure\nis illustrated analyzing wind speed and temperature data. \n\n"}
{"id": "1710.08994", "contents": "Title: Variable Partitioning for Distributed Optimization Abstract: This paper is about how to partition decision variables while decomposing a\nlarge-scale optimization problem for the best performance of distributed\nsolution methods. Solving a large-scale optimization problem sequen- tially can\nbe computationally challenging. One classic approach is to decompose the\nproblem into smaller sub-problems and solve them in a distributed fashion.\nHowever, there is little discussion in the literature on which variables should\nbe grouped together to form the sub-problems, especially when the optimization\nformulation involves complex constraints. We focus on one of the most popular\ndistributed approaches, dual decomposition and distributed sub-gradient\nmethods. Based on a theoretical guarantee on its convergence rate, we explain\nthat a partition of variables can critically affect the speed of convergence\nand highlight the importance of the number of dualized constraints. Then, we\nintroduce a novel approach to find a partition that reduces the number of\ndualized constraints by utilizing a community detection algorithm from physics\nliterature. Roughly speaking, the proposed method groups decision variables\nthat appear together in con- straints and solves the resulting sub-problems\nwith blocks of variables in parallel. Empirical experiments on a real\napplication show that the proposed method significantly accelerates the\nconvergence of the distributed sub-gradient method. The advantage of our\napproach becomes more significant as the size of the problem increases and each\nconstraint involves more variables. \n\n"}
{"id": "1710.09020", "contents": "Title: Taming heavy-tailed features by shrinkage Abstract: In this work, we focus on a variant of the generalized linear model (GLM)\ncalled corrupted GLM (CGLM) with heavy-tailed features and responses. To\nrobustify the statistical inference on this model, we propose to apply\n$\\ell_4$-norm shrinkage to the feature vectors in the low-dimensional regime\nand apply elementwise shrinkage to them in the high-dimensional regime. Under\nbounded fourth moment assumptions, we show that the maximum likelihood\nestimator (MLE) based on the shrunk data enjoys nearly the minimax optimal rate\nwith an exponential deviation bound. Our simulations demonstrate that the\nproposed feature shrinkage significantly enhances the statistical performance\nin linear regression and logistic regression on heavy-tailed data. Finally, we\napply our shrinkage principle to guard against mislabeling and image noise in\nthe human-written digit recognition problem. We add an $\\ell_4$-norm shrinkage\nlayer to the original neural net and reduce the testing misclassification rate\nby more than $30\\%$ relatively in the presence of mislabeling and image noise. \n\n"}
{"id": "1710.09774", "contents": "Title: Energy independent scaling of ridge and final state description of high\n  multiplicity p+p collisions at $\\sqrt{s}$ = 7 and 13 TeV Abstract: An energy independent scaling of the near-side ridge yield at a given\nmultiplicity has been observed by the ATLAS and the CMS collaborations in p+p\ncollisions at s = 7 and 13 TeV. Such a striking feature of the data can be\nsuccessfully explained by approaches based on initial state momentum space\ncorrelation generated due to gluon saturation. In this paper, we try to examine\nif such a scaling is also an inherent feature of the approaches that employ\nstrong final state interaction in p+p collisions. We find that hydrodynamical\nmodeling of p+p collisions using EPOS 3 shows a violation of such scaling. The\ncurrent study can, therefore, provide important new insights on the origin of\nlong range azimuthal correlations in high multiplicity p+p collisions at the\nLHC energies. \n\n"}
{"id": "1710.10936", "contents": "Title: Asymptotically efficient estimators for stochastic blockmodels: the\n  naive MLE, the rank-constrained MLE, and the spectral Abstract: We establish asymptotic normality results for estimation of the block\nprobability matrix $\\mathbf{B}$ in stochastic blockmodel graphs using spectral\nembedding when the average degrees grows at the rate of $\\omega(\\sqrt{n})$ in\n$n$, the number of vertices. As a corollary, we show that when $\\mathbf{B}$ is\nof full-rank, estimates of $\\mathbf{B}$ obtained from spectral embedding are\nasymptotically efficient. When $\\mathbf{B}$ is singular the estimates obtained\nfrom spectral embedding can have smaller mean square error than those obtained\nfrom maximizing the log-likelihood under no rank assumption, and furthermore,\ncan be almost as efficient as the true MLE that assume known\n$\\mathrm{rk}(\\mathbf{B})$. Our results indicate, in the context of stochastic\nblockmodel graphs, that spectral embedding is not just computationally\ntractable, but that the resulting estimates are also admissible, even when\ncompared to the purportedly optimal but computationally intractable maximum\nlikelihood estimation under no rank assumption. \n\n"}
{"id": "1711.00097", "contents": "Title: Bayesian Markov Switching Tensor Regression for Time-varying Networks Abstract: We propose a new Bayesian Markov switching regression model for\nmultidimensional arrays (tensors) of binary time series. We assume a\nzero-inflated logit regression with time-varying parameters and apply it to\nmultilayer temporal networks. The original contribution is threefold. First, to\navoid over-fitting we propose a parsimonious parametrization based on a\nlow-rank decomposition of the tensor of regression coefficients. Second, we\nassume the parameters are driven by a hidden Markov chain, thus allowing for\nstructural changes in the network topology. We follow a Bayesian approach to\ninference and provide an efficient Gibbs sampler for posterior approximation.\nWe apply the methodology to a real dataset of financial networks to study the\nimpact of several risk factors on the edge probability. Supplementary materials\nfor this article are available online. \n\n"}
{"id": "1711.00484", "contents": "Title: Spatial Statistical Downscaling for Constructing High-Resolution Nature\n  Runs in Global Observing System Simulation Experiments Abstract: Observing system simulation experiments (OSSEs) have been widely used as a\nrigorous and cost-effective way to guide development of new observing systems,\nand to evaluate the performance of new data assimilation algorithms. Nature\nruns (NRs), which are outputs from deterministic models, play an essential role\nin building OSSE systems for global atmospheric processes because they are used\nboth to create synthetic observations at high spatial resolution, and to\nrepresent the \"true\" atmosphere against which the forecasts are verified.\nHowever, most NRs are generated at resolutions coarser than actual\nobservations. Here, we propose a principled statistical downscaling framework\nto construct high-resolution NRs via conditional simulation from\ncoarse-resolution numerical model output. We use nonstationary spatial\ncovariance function models that have basis function representations. This\napproach not only explicitly addresses the change-of-support problem, but also\nallows fast computation with large volumes of numerical model output. We also\npropose a data-driven algorithm to select the required basis functions\nadaptively, in order to increase the flexibility of our nonstationary\ncovariance function models. In this article we demonstrate these techniques by\ndownscaling a coarse-resolution physical NR at a native resolution of\n$1^{\\circ} \\text{ latitude} \\times 1.25^{\\circ} \\text{ longitude}$ of global\nsurface $\\text{CO}_2$ concentrations to 655,362 equal-area hexagons. \n\n"}
{"id": "1711.00572", "contents": "Title: Consistent estimation of the spectrum of trace class data augmentation\n  algorithms Abstract: Markov chain Monte Carlo is widely used in a variety of scientific\napplications to generate approximate samples from intractable distributions. A\nthorough understanding of the convergence and mixing properties of these Markov\nchains can be obtained by studying the spectrum of the associated Markov\noperator. While several methods to bound/estimate the second largest eigenvalue\nare available in the literature, very few general techniques for consistent\nestimation of the entire spectrum have been proposed. Existing methods for this\npurpose require the Markov transition density to be available in closed form,\nwhich is often not true in practice, especially in modern statistical\napplications. In this paper, we propose a novel method to consistently estimate\nthe entire spectrum of a general class of Markov chains arising from a popular\nand widely used statistical approach known as Data Augmentation. The transition\ndensities of these Markov chains can often only be expressed as intractable\nintegrals. We illustrate the applicability of our method using real and\nsimulated data. \n\n"}
{"id": "1711.00813", "contents": "Title: Bootstrapping Exchangeable Random Graphs Abstract: We introduce two new bootstraps for exchangeable random graphs. One, the\n\"empirical graphon bootstrap\", is based purely on resampling, while the other,\nthe \"histogram bootstrap\", is a model-based \"sieve\" bootstrap. We show that\nboth of them accurately approximate the sampling distributions of motif\ndensities, i.e., of the normalized counts of the number of times fixed\nsubgraphs appear in the network. These densities characterize the distribution\nof (infinite) exchangeable networks. Our bootstraps therefore give a valid\nquantification of uncertainty in inferences about fundamental network\nstatistics, and so of parameters identifiable from them. \n\n"}
{"id": "1711.01598", "contents": "Title: Multilayer tensor factorization with applications to recommender systems Abstract: Recommender systems have been widely adopted by electronic commerce and\nentertainment industries for individualized prediction and recommendation,\nwhich benefit consumers and improve business intelligence. In this article, we\npropose an innovative method, namely the recommendation engine of multilayers\n(REM), for tensor recommender systems. The proposed method utilizes the\nstructure of a tensor response to integrate information from multiple modes,\nand creates an additional layer of nested latent factors to accommodate\nbetween-subjects dependency. One major advantage is that the proposed method is\nable to address the \"cold-start\" issue in the absence of information from new\ncustomers, new products or new contexts. Specifically, it provides more\neffective recommendations through sub-group information. To achieve scalable\ncomputation, we develop a new algorithm for the proposed method, which\nincorporates a maximum block improvement strategy into the cyclic\nblockwise-coordinate-descent algorithm. In theory, we investigate both\nalgorithmic properties for global and local convergence, along with the\nasymptotic consistency of estimated parameters. Finally, the proposed method is\napplied in simulations and IRI marketing data with 116 million observations of\nproduct sales. Numerical studies demonstrate that the proposed method\noutperforms existing competitors in the literature. \n\n"}
{"id": "1711.03071", "contents": "Title: Heavy quark complex potential in a strongly magnetized hot QGP medium Abstract: We study the effect of a strong constant magnetic field, generated in\nrelativistic heavy ion collisions, on the heavy quark complex potential. We\nwork in the strong magnetic field limit with the lowest Landau level\napproximation. We find that the screening of the real part of the potential\nincreases with the increase in the magnetic field. Therefore, we expect less\nbinding of the $ Q\\bar{Q} $ pair in the presence of a strong magnetic field.\nThe imaginary part of the potential increases in magnitude with the increase in\nthe magnetic field, leading to an increase of the width of the quarkonium state\nwith the magnetic field. All of these effects result in the early dissociation\nof $ Q\\bar{Q} $ states in a magnetized hot quark-gluon plasma medium. \n\n"}
{"id": "1711.03170", "contents": "Title: Penalized Orthogonal Iteration for Sparse Estimation of Generalized\n  Eigenvalue Problem Abstract: We propose a new algorithm for sparse estimation of eigenvectors in\ngeneralized eigenvalue problems (GEP). The GEP arises in a number of modern\ndata-analytic situations and statistical methods, including principal component\nanalysis (PCA), multiclass linear discriminant analysis (LDA), canonical\ncorrelation analysis (CCA), sufficient dimension reduction (SDR) and invariant\nco-ordinate selection. We propose to modify the standard generalized orthogonal\niteration with a sparsity-inducing penalty for the eigenvectors. To achieve\nthis goal, we generalize the equation-solving step of orthogonal iteration to a\npenalized convex optimization problem. The resulting algorithm, called\npenalized orthogonal iteration, provides accurate estimation of the true\neigenspace, when it is sparse. Also proposed is a computationally more\nefficient alternative, which works well for PCA and LDA problems. Numerical\nstudies reveal that the proposed algorithms are competitive, and that our\ntuning procedure works well. We demonstrate applications of the proposed\nalgorithm to obtain sparse estimates for PCA, multiclass LDA, CCA and SDR.\nSupplementary materials are available online. \n\n"}
{"id": "1711.07516", "contents": "Title: Non-Gaussian Autoregressive Processes with Tukey g-and-h Transformations Abstract: When performing a time series analysis of continuous data, for example from\nclimate or environmental problems, the assumption that the process is Gaussian\nis often violated. Therefore, we introduce two non-Gaussian autoregressive time\nseries models that are able to fit skewed and heavy-tailed time series data.\nOur two models are based on the Tukey g-and-h transformation. We discuss\nparameter estimation, order selection, and forecasting procedures for our\nmodels and examine their performances in a simulation study. We demonstrate the\nusefulness of our models by applying them to two sets of wind speed data. \n\n"}
{"id": "1711.07801", "contents": "Title: Why \"Redefining Statistical Significance\" Will Not Improve\n  Reproducibility and Could Make the Replication Crisis Worse Abstract: A recent proposal to \"redefine statistical significance\" (Benjamin, et al.\nNature Human Behaviour, 2017) claims that false positive rates \"would\nimmediately improve\" by factors greater than two and replication rates would\ndouble simply by changing the conventional cutoff for 'statistical\nsignificance' from P<0.05 to P<0.005. I analyze the veracity of these claims,\nfocusing especially on how Benjamin, et al neglect the effects of P-hacking in\nassessing the impact of their proposal. My analysis shows that once P-hacking\nis accounted for the perceived benefits of the lower threshold all but\ndisappear, prompting two main conclusions: (i) The claimed improvements to\nfalse positive rate and replication rate in Benjamin, et al (2017) are\nexaggerated and misleading. (ii) There are plausible scenarios under which the\nlower cutoff will make the replication crisis worse. \n\n"}
{"id": "1711.08265", "contents": "Title: Sparse Variable Selection on High Dimensional Heterogeneous Data with\n  Tree Structured Responses Abstract: We consider the problem of sparse variable selection on high dimension\nheterogeneous data sets, which has been taking on renewed interest recently due\nto the growth of biological and medical data sets with complex, non-i.i.d.\nstructures and huge quantities of response variables. The heterogeneity is\nlikely to confound the association between explanatory variables and responses,\nresulting in enormous false discoveries when Lasso or its variants are\nna\\\"ively applied. Therefore, developing effective confounder correction\nmethods is a growing heat point among researchers. However, ordinarily\nemploying recent confounder correction methods will result in undesirable\nperformance due to the ignorance of the convoluted interdependency among\nresponse variables. To fully improve current variable selection methods, we\nintroduce a model, the tree-guided sparse linear mixed model, that can utilize\nthe dependency information from multiple responses to explore how specifically\nclusters are and select the active variables from heterogeneous data. Through\nextensive experiments on synthetic and real data sets, we show that our\nproposed model outperforms the existing methods and achieves the highest ROC\narea. \n\n"}
{"id": "1711.09317", "contents": "Title: Noncrossing simultaneous Bayesian quantile curve fitting Abstract: Bayesian simultaneous estimation of nonparametric quantile curves is a\nchallenging problem, requiring a flexible and robust data model whilst\nsatisfying the monotonicity or noncrossing constraints on the quantiles. This\npaper presents the use of the pyramid quantile regression method in the spline\nregression setting. In high dimensional problems, the choice of the pyramid\nlocations becomes crucial for a robust parameter estimation. In this work we\nderive the optimal {pyramid locations which then allows us to propose an\nefficient} adaptive block-update MCMC scheme for posterior computation.\nSimulation studies show the proposed method provides estimates with\nsignificantly smaller errors and better empirical coverage probability when\ncompared to existing alternative approaches. We illustrate the method with\nthree real applications. \n\n"}
{"id": "1711.09773", "contents": "Title: COHERENT constraints to conventional and exotic neutrino physics Abstract: The process of neutral-current coherent elastic neutrino-nucleus scattering,\nconsistent with the Standard Model (SM) expectation, has been recently measured\nby the COHERENT experiment at the Spallation Neutron Source. On the basis of\nthe observed signal and our nuclear calculations for the relevant Cs and I\nisotopes, the extracted constraints on both conventional and exotic neutrino\nphysics are updated. The present study concentrates on various SM extensions\ninvolving vector and tensor nonstandard interactions as well as neutrino\nelectromagnetic properties, with an emphasis on the neutrino magnetic moment\nand the neutrino charge radius. Furthermore, models addressing a light sterile\nneutrino state and scenarios with new propagator fields---such as vector\n$Z^\\prime$ and scalar bosons---are examined, and the corresponding regions\nexcluded by the COHERENT experiment are presented. \n\n"}
{"id": "1711.10421", "contents": "Title: A Review of Dynamic Network Models with Latent Variables Abstract: We present a selective review of statistical modeling of dynamic networks. We\nfocus on models with latent variables, specifically, the latent space models\nand the latent class models (or stochastic blockmodels), which investigate both\nthe observed features and the unobserved structure of networks. We begin with\nan overview of the static models, and then we introduce the dynamic extensions.\nFor each dynamic model, we also discuss its applications that have been studied\nin the literature, with the data source listed in Appendix. Based on the\nreview, we summarize a list of open problems and challenges in dynamic network\nmodeling with latent variables. \n\n"}
{"id": "1712.00229", "contents": "Title: Efficient determination of optimised multi-arm multi-stage experimental\n  designs with control of generalised error-rates Abstract: Primarily motivated by the drug development process, several publications\nhave now presented methodology for the design of multi-arm multi-stage\nexperiments with normally distributed outcome variables of known variance.\nHere, we extend these past considerations to allow the design of what we refer\nto as an abcd multi-arm multi-stage experiment. We provide a proof of how\nstrong control of the a-generalised type-I familywise error-rate can be\nensured. We then describe how to attain the power to reject at least b out of c\nfalse hypotheses, which is related to controlling the b-generalised type-II\nfamilywise error-rate. Following this, we detail how a design can be optimised\nfor a scenario in which rejection of any d null hypotheses brings about\ntermination of the experiment. We achieve this by proposing a highly\ncomputationally efficient approach for evaluating the performance of a\ncandidate design. Finally, using a real clinical trial as a motivating example,\nwe explore the effect of the design's control parameters on the statistical\noperating characteristics. \n\n"}
{"id": "1712.00588", "contents": "Title: Development of heavy-flavour flow-harmonics in high-energy nuclear\n  collisions Abstract: We employ the POWLANG transport setup, developed over the last few years, to\nprovide new predictions for several heavy-flavour observables in relativistic\nheavy-ion collisions from RHIC to LHC center-of-mass energies. In particular,\nwe focus on the development of the flow-harmonics $v_2$ and $v_3$ arising from\nthe initial geometric asymmetry in the initial conditions and its associated\nevent-by-event fluctuations. Within the same transport framework, for the sake\nof consistency, we also compare the nuclear modification factor of the $p_T$\nspectra of charm and beauty quarks, heavy hadrons and their decay electrons. We\ncompare our findings to the most recent data from the experimental\ncollaborations. We also study in detail the contribution to the flow harmonics\nfrom the quarks decoupling from the fireball during the various stages of its\nevolution: although not directly accessible to the experiments, this\ninformation can shed light on the major sources of the final measured effect. \n\n"}
{"id": "1712.00968", "contents": "Title: Two-loop mass splittings in electroweak multiplets: winos and minimal\n  dark matter Abstract: The radiatively-induced splitting of masses in electroweak multiplets is\nrelevant for both collider phenomenology and dark matter. Precision two-loop\ncorrections of $\\mathcal{O}$(MeV) to the triplet mass splitting in the wino\nlimit of the minimal supersymmetric standard model can affect particle\nlifetimes by up to $40\\%$. We improve on previous two-loop self-energy\ncalculations for the wino model by obtaining consistent input parameters to the\ncalculation via two-loop renormalisation-group running, and including the\neffect of finite light quark masses. We also present the first two-loop\ncalculation of the mass splitting in an electroweak fermionic quintuplet,\ncorresponding to the viable form of minimal dark matter (MDM). We place\nsignificant constraints on the lifetimes of the charged and doubly-charged\nfermions in this model. We find that the two-loop mass splittings in the MDM\nquintuplet are not constant in the large-mass limit, as might naively be\nexpected from the triplet calculation. This is due to the influence of the\nadditional heavy fermions in loop corrections to the gauge boson propagators. \n\n"}
{"id": "1712.02118", "contents": "Title: Search for long-lived charginos based on a disappearing-track signature\n  in $pp$ collisions at $\\sqrt{s} = 13$ TeV with the ATLAS detector Abstract: This paper presents a search for direct electroweak gaugino or gluino pair\nproduction with a chargino nearly mass-degenerate with a stable neutralino. It\nis based on an integrated luminosity of 36.1 $\\mathrm{fb}^{-1}$ of $pp$\ncollisions at $\\sqrt{s} = 13$ TeV collected by the ATLAS experiment at the LHC.\nThe final state of interest is a disappearing track accompanied by at least one\njet with high transverse momentum from initial-state radiation or by four jets\nfrom the gluino decay chain. The use of short track segments reconstructed from\nthe innermost tracking layers significantly improves the sensitivity to short\nchargino lifetimes. The results are found to be consistent with Standard Model\npredictions. Exclusion limits are set at 95% confidence level on the mass of\ncharginos and gluinos for different chargino lifetimes. For a pure wino with a\nlifetime of about 0.2 ns, chargino masses up to 460 GeV are excluded. For the\nstrong production channel, gluino masses up to 1.65 TeV are excluded assuming a\nchargino mass of 460 GeV and lifetime of 0.2 ns. \n\n"}
{"id": "1712.03646", "contents": "Title: Dynamic Mixed Frequency Synthesis for Economic Nowcasting Abstract: We develop a novel Bayesian framework for dynamic modeling of mixed frequency\ndata to nowcast quarterly U.S. GDP growth. The introduced framework utilizes\nfoundational Bayesian theory and treats data sampled at different frequencies\nas latent factors that are later synthesized, allowing flexible methodological\nspecifications based on interests and utility. Time-varying inter-dependencies\nbetween the mixed frequency data are learnt and effectively mapped onto easily\ninterpretable parameters. A macroeconomic study of nowcasting quarterly U.S.\nGDP growth using a number of monthly economic variables demonstrates\nimprovements in terms of nowcast performance and interpretability compared to\nthe standard in the literature. The study further shows that incorporating\ninformation during a quarter markedly improves the performance in terms of both\npoint and density nowcasts. \n\n"}
{"id": "1712.05630", "contents": "Title: Sparse principal component analysis via axis-aligned random projections Abstract: We introduce a new method for sparse principal component analysis, based on\nthe aggregation of eigenvector information from carefully-selected axis-aligned\nrandom projections of the sample covariance matrix. Unlike most alternative\napproaches, our algorithm is non-iterative, so is not vulnerable to a bad\nchoice of initialisation. We provide theoretical guarantees under which our\nprincipal subspace estimator can attain the minimax optimal rate of convergence\nin polynomial time. In addition, our theory provides a more refined\nunderstanding of the statistical and computational trade-off in the problem of\nsparse principal component estimation, revealing a subtle interplay between the\neffective sample size and the number of random projections that are required to\nachieve the minimax optimal rate. Numerical studies provide further insight\ninto the procedure and confirm its highly competitive finite-sample\nperformance. \n\n"}
{"id": "1712.06230", "contents": "Title: Testing Sparsity-Inducing Penalties Abstract: Many penalized maximum likelihood estimators correspond to posterior mode\nestimators under specific prior distributions. Appropriateness of a particular\nclass of penalty functions can therefore be interpreted as the appropriateness\nof a prior for the parameters. For example, the appropriateness of a lasso\npenalty for regression coefficients depends on the extent to which the\nempirical distribution of the regression coefficients resembles a Laplace\ndistribution. We give a testing procedure of whether or not a Laplace prior is\nappropriate and accordingly, whether or not using a lasso penalized estimate is\nappropriate. This testing procedure is designed to have power against\nexponential power priors which correspond to $\\ell_q$ penalties. Via\nsimulations, we show that this testing procedure achieves the desired level and\nhas enough power to detect violations of the Laplace assumption when the\nnumbers of observations and unknown regression coefficients are large. We then\nintroduce an adaptive procedure that chooses a more appropriate prior and\ncorresponding penalty from the class of exponential power priors when the null\nhypothesis is rejected. We show that this can improve estimation of the\nregression coefficients both when they are drawn from an exponential power\ndistribution and when they are drawn from a spike-and-slab distribution. \n\n"}
{"id": "1712.07197", "contents": "Title: Optimal P-value Weighting with Independent Information Abstract: The large-scale multiple testing inherent to high throughput biological data\nnecessitates very high statistical stringency and thus true effects in data are\ndifficult to detect unless they have high effect sizes. One solution to this\nproblem is to use an independent information to prioritize the most promising\nfeatures of the data and thus increase the power to detect them. Weighted\np-values provide a general framework for doing this in a statistically rigorous\nfashion. However, calculating weights that incorporate the independent\ninformation and optimize statistical power remains a challenging problem\ndespite recent advances in this area. Existing methods tend to perform poorly\nin the common situation that true positive features are rare and of low effect\nsize. We introduce covariate based weighting methods for calculating optimal\nweights conditioned on the effect sizes of the tests. This approach uses the\nprobabilistic relationship between covariate and test effect size to calculate\nmore informative weights that are not diluted by null effects as is common with\ngroup-based methods. This relationship can be calculated theoretically for\nnormally distributed covariates or estimated empirically in other cases. We\nshowed via simulations and applications to data that this method outperforms\nexisting methods by a large margin in the rare/low effect size scenario and has\nat least comparable performance in all scenarios. \n\n"}
{"id": "1712.07237", "contents": "Title: Exposing Dark Sector with Future Z-Factories Abstract: We investigate the prospects of searching dark sector models via exotic\nZ-boson decay at future $e^+ e^-$ colliders with Giga Z and Tera Z options.\nFour general categories of dark sector models: Higgs portal dark matter, vector\nportal dark matter, inelastic dark matter and axion-like particles, are\nconsidered. Focusing on channels motivated by the dark sector models, we carry\nout a model independent study of the sensitivities of Z-factories in probing\nexotic decays. The limits on branching ratios of the exotic Z decay are\ntypically $\\mathcal{O} (10^{-6} - 10^{-8.5}) $ for the Giga Z and $\\mathcal{O}\n(10^{-7.5} - 10^{-11})$ for the Tera Z, and they are compared with the\nprojection for the high luminosity LHC. We demonstrate that future Z-factories\ncan provide its unique and leading sensitivity, and highlight the\ncomplementarity with other experiments, including the indirect and direct dark\nmatter search limits, and the existing collider limits. Future Z factories will\nplay a leading role to uncover the hidden sector of the universe in the future. \n\n"}
{"id": "1712.07519", "contents": "Title: Statistical Inference for the Population Landscape via Moment Adjusted\n  Stochastic Gradients Abstract: Modern statistical inference tasks often require iterative optimization\nmethods to compute the solution. Convergence analysis from an optimization\nviewpoint only informs us how well the solution is approximated numerically but\noverlooks the sampling nature of the data. In contrast, recognizing the\nrandomness in the data, statisticians are keen to provide uncertainty\nquantification, or confidence, for the solution obtained using iterative\noptimization methods. This paper makes progress along this direction by\nintroducing the moment-adjusted stochastic gradient descents, a new stochastic\noptimization method for statistical inference. We establish non-asymptotic\ntheory that characterizes the statistical distribution for certain iterative\nmethods with optimization guarantees. On the statistical front, the theory\nallows for model mis-specification, with very mild conditions on the data. For\noptimization, the theory is flexible for both convex and non-convex cases.\nRemarkably, the moment-adjusting idea motivated from \"error standardization\" in\nstatistics achieves a similar effect as acceleration in first-order\noptimization methods used to fit generalized linear models. We also demonstrate\nthis acceleration effect in the non-convex setting through numerical\nexperiments. \n\n"}
{"id": "1712.09562", "contents": "Title: Spatial point processes intensity estimation with a diverging number of\n  covariates Abstract: Feature selection procedures for spatial point processes parametric intensity\nestimation have been recently developed since more and more applications\ninvolve a large number of covariates. In this paper, we investigate the setting\nwhere the number of covariates diverges as the domain of observation increases.\nIn particular, we consider estimating equations based on Campbell theorems\nderived from Poisson and logistic regression likelihoods regularized by a\ngeneral penalty function. We prove that, under some conditions, the\nconsistency, the sparsity, and the asymptotic normality are valid for such a\nsetting. We support the theoretical results by numerical ones obtained from\nsimulation experiments and an application to forestry datasets. \n\n"}
{"id": "1801.00319", "contents": "Title: An Additive Approximate Gaussian Process Model for Large Spatio-Temporal\n  Data Abstract: Motivated by a large ground-level ozone dataset, we propose a new\ncomputationally efficient additive approximate Gaussian process. The proposed\nmethod incorporates a computational-complexity-reduction method and a separable\ncovariance function, which can flexibly capture various spatio-temporal\ndependence structure. The first component is able to capture nonseparable\nspatio-temporal variability while the second component captures the separable\nvariation. Based on a hierarchical formulation of the model, we are able to\nutilize the computational advantages of both components and perform efficient\nBayesian inference. To demonstrate the inferential and computational benefits\nof the proposed method, we carry out extensive simulation studies assuming\nvarious scenarios of underlying spatio-temporal covariance structure. The\nproposed method is also applied to analyze large spatio-temporal measurements\nof ground-level ozone in the Eastern United States. \n\n"}
{"id": "1801.00359", "contents": "Title: Search for decays of stopped exotic long-lived particles produced in\n  proton-proton collisions at $\\sqrt{s}=$ 13 TeV Abstract: A search is presented for the decays of heavy exotic long-lived particles\n(LLPs) that are produced in proton-proton collisions at a center-of-mass energy\nof 13 TeV at the CERN LHC and come to rest in the CMS detector. Their decays\nwould be visible during periods of time well separated from proton-proton\ncollisions. Two decay scenarios of stopped LLPs are explored: a hadronic decay\ndetected in the calorimeter and a decay into muons detected in the muon system.\nThe calorimeter (muon) search covers a period of sensitivity totaling 721 (744)\nhours in 38.6 (39.0) fb$^{-1}$ of data collected by the CMS detector in 2015\nand 2016. The results are interpreted in several scenarios that predict LLPs.\nProduction cross section limits are set as a function of the mean proper\nlifetime and the mass of the LLPs, for lifetimes between 100 ns and 10 days.\nThese are the most stringent limits to date on the mass of hadronically\ndecaying stopped LLPs, and this is the first search at the LHC for stopped LLPs\nthat decay to muons. \n\n"}
{"id": "1801.00585", "contents": "Title: A generalized one-loop neutrino mass model with charged particles Abstract: We propose a radiative neutrino-mass model by introducing 3 generations of\nfermion pairs $E^{-(N+1)/2} E^{+(N+1)/2}$ and a couple of multi-charged bosonic\ndoublet fields $\\Phi_{N/2}, \\Phi_{N/2+1}$, where $N=1,3,5,7,9$. We show that\nthe models can satisfy the neutrino masses and oscillation data, and are\nconsistent with lepton-flavor violations, the muon anomalous magnetic moment,\nthe oblique parameters, and the beta function of the $U(1)_Y$ hypercharge gauge\ncoupling. We also discuss the collider signals for various $N$, namely,\nmulti-charged leptons in the final state from the Drell-Yan production of\n$E^{-(N+1)/2} E^{+(N+1)/2}$. In general, the larger the $N$ the more charged\nleptons will appear in the final state. \n\n"}
{"id": "1801.00753", "contents": "Title: Probabilistic supervised learning Abstract: Predictive modelling and supervised learning are central to modern data\nscience. With predictions from an ever-expanding number of supervised black-box\nstrategies - e.g., kernel methods, random forests, deep learning aka neural\nnetworks - being employed as a basis for decision making processes, it is\ncrucial to understand the statistical uncertainty associated with these\npredictions.\n  As a general means to approach the issue, we present an overarching framework\nfor black-box prediction strategies that not only predict the target but also\ntheir own predictions' uncertainty. Moreover, the framework allows for fair\nassessment and comparison of disparate prediction strategies. For this, we\nformally consider strategies capable of predicting full distributions from\nfeature variables, so-called probabilistic supervised learning strategies.\n  Our work draws from prior work including Bayesian statistics, information\ntheory, and modern supervised machine learning, and in a novel synthesis leads\nto (a) new theoretical insights such as a probabilistic bias-variance\ndecomposition and an entropic formulation of prediction, as well as to (b) new\nalgorithms and meta-algorithms, such as composite prediction strategies,\nprobabilistic boosting and bagging, and a probabilistic predictive independence\ntest.\n  Our black-box formulation also leads (c) to a new modular interface view on\nprobabilistic supervised learning and a modelling workflow API design, which we\nhave implemented in the newly released skpro machine learning toolbox,\nextending the familiar modelling interface and meta-modelling functionality of\nsklearn. The skpro package provides interfaces for construction, composition,\nand tuning of probabilistic supervised learning strategies, together with\norchestration features for validation and comparison of any such strategy - be\nit frequentist, Bayesian, or other. \n\n"}
{"id": "1801.01135", "contents": "Title: Colored Dark Matter Abstract: We explore the possibility that Dark Matter is the lightest hadron made of\ntwo stable color octet Dirac fermions ${\\cal Q}$. The cosmological DM abundance\nis reproduced for $M_{\\cal Q}\\approx 12.5$ TeV, compatibly with direct searches\n(the Rayleigh cross section, suppressed by $1/M_{\\cal Q}^6$, is close to\npresent bounds), indirect searches (enhanced by ${\\cal Q}{\\cal Q}+\\bar{\\cal\nQ}\\bar{\\cal Q}\\to {\\cal Q}\\bar{\\cal Q}+{\\cal Q}\\bar{\\cal Q}$ recombination),\nand with collider searches (where ${\\cal Q}$ manifests as tracks, pair produced\nvia QCD). Hybrid hadrons, made of $\\cal Q$ and of SM quarks and gluons, have\nlarge QCD cross sections, and do not reach underground detectors. Their\ncosmological abundance is $10^5$ times smaller than DM, such that their unusual\nsignals seem compatible with bounds. Those in the Earth and stars sank to their\ncenters; the Earth crust and meteorites later accumulate a secondary abundance,\nalthough their present abundance depends on nuclear and geological properties\nthat we cannot compute from first principles. \n\n"}
{"id": "1801.01525", "contents": "Title: Bayesian Constraint Relaxation Abstract: Prior information often takes the form of parameter constraints. Bayesian\nmethods include such information through prior distributions having constrained\nsupport. By using posterior sampling algorithms, one can quantify uncertainty\nwithout relying on asymptotic approximations. However, sharply constrained\npriors are (a) not necessary in some settings; and (b) tend to limit modeling\nscope to a narrow set of distributions that are tractable computationally.\nInspired by the vast literature that replaces the slab-and-spike prior with a\ncontinuous approximation, we propose to replace the sharp indicator function of\nthe constraint with an exponential kernel, thereby creating a\nclose-to-constrained neighborhood within the Euclidean space in which the\nconstrained subspace is embedded. This kernel decays with distance from the\nconstrained space at a rate depending on a relaxation hyperparameter. By\navoiding the sharp constraint, we enable use of off-the-shelf posterior\nsampling algorithms, such as Hamiltonian Monte Carlo, facilitating automatic\ncomputation in broad models. We study the constrained and relaxed distributions\nunder multiple settings, and theoretically quantify their differences. We\nillustrate the method through multiple novel modeling examples. \n\n"}
{"id": "1801.01990", "contents": "Title: Procrustes Metrics on Covariance Operators and Optimal Transportation of\n  Gaussian Processes Abstract: Covariance operators are fundamental in functional data analysis, providing\nthe canonical means to analyse functional variation via the celebrated\nKarhunen--Lo\\`eve expansion. These operators may themselves be subject to\nvariation, for instance in contexts where multiple functional populations are\nto be compared. Statistical techniques to analyse such variation are intimately\nlinked with the choice of metric on covariance operators, and the intrinsic\ninfinite-dimensionality of these operators. In this paper, we describe the\nmanifold geometry of the space of trace-class infinite-dimensional covariance\noperators and associated key statistical properties, under the recently\nproposed infinite-dimensional version of the Procrustes metric. We identify\nthis space with that of centred Gaussian processes equipped with the\nWasserstein metric of optimal transportation. The identification allows us to\nprovide a complete description of those aspects of this manifold geometry that\nare important in terms of statistical inference, and establish key properties\nof the Fr\\'echet mean of a random sample of covariances, as well as generative\nmodels that are canonical for such metrics and link with the problem of\nregistration of functional data. \n\n"}
{"id": "1801.02895", "contents": "Title: Vector leptoquark mass limits and branching ratios of $ K_L^0, B^0, B_s\n  \\to l^+_i l^-_j $ decays with account of fermion mixing in leptoquark\n  currents Abstract: The contributions of the vector leptoquarks of Pati-Salam type to the\nbranching ratios of $K_L^0, B^0, B_s \\to l \\, l^{\\prime}$ decays are calculated\nwith account of the fermion mixing in the leptoguark currents of the general\ntype. Using the general parametrizations of the mixing matrices the lower\nvector leptoquark mass limit $ m_V > 86 \\,\\, TeV $ is found from the current\nexperimental data on these decays. The branching ratios of the decays $B^0, B_s\n\\to l \\, l^{\\prime}$ predicted at $m_V = 86 \\,\\, TeV$ are calculated. These\nbranching ratios for the decays $ B^0, B_s \\to \\mu^+ \\mu^-, e \\mu $ are close\nto the experimental data whereas those for the decays $B^0 \\to e^+ e^-, e \\tau,\n\\mu \\tau$, and $B_s \\to e^+ e^-$ are by order of $2\\div4$ less than their\ncurrent experimental limits. For the decays $B_s \\to e \\tau, \\mu \\tau$ these\nbranching ratios are of order $10^{-10}$ and $10^{-9}$ respectively. The\npredicted branching ratios will be usefull in the current and future\nexperimental searches for these decays. \n\n"}
{"id": "1801.05108", "contents": "Title: Factor graph fragmentization of expectation propagation Abstract: Expectation propagation is a general approach to fast approximate inference\nfor graphical models. The existing literature treats models separately when it\ncomes to deriving and coding expectation propagation inference algorithms. This\ncomes at the cost of similar, long-winded algebraic steps being repeated and\nslowing down algorithmic development. We demonstrate how factor graph\nfragmentization can overcome this impediment. This involves adoption of the\nmessage passing on a factor graph approach to expectation propagation and\nidentification of factor graph sub-graphs, which we call fragments, that are\ncommon to wide classes of models. Key fragments and their corresponding\nmessages are catalogued which means that their algebra does not need to be\nrepeated. This allows compartmentalization of coding and efficient software\ndevelopment. \n\n"}
{"id": "1801.05432", "contents": "Title: Charged Fermions Below 100 GeV Abstract: How light can a fermion be if it has unit electric charge? We revisit the\nlore that LEP robustly excludes charged fermions lighter than about 100 GeV. We\nreview LEP chargino searches, and find them to exclude charged fermions lighter\nthan 90 GeV, assuming a higgsino-like cross section. However, if the charged\nfermion couples to a new scalar, destructive interference among production\nchannels can lower the LEP cross section by a factor of 3. In this case, we\nfind that charged fermions as light as 75 GeV can evade LEP bounds, while\nremaining consistent with constraints from the LHC. As the LHC collects more\ndata, charged fermions in the 75-100 GeV mass range serve as a target for\nfuture monojet and disappearing track searches. \n\n"}
{"id": "1801.05859", "contents": "Title: A Kotel'nikov Representation for Wavelets Abstract: This paper presents a wavelet representation using baseband signals, by\nexploiting Kotel'nikov results. Details of how to obtain the processes of\nenvelope and phase at low frequency are shown. The archetypal interpretation of\nwavelets as an analysis with a filter bank of constant quality factor is\nrevisited on these bases. It is shown that if the wavelet spectral support is\nlimited into the band $[f_m,f_M]$, then an orthogonal analysis is guaranteed\nprovided that $f_M \\leq 3f_m$, a quite simple result, but that invokes some\nparallel with the Nyquist rate. Nevertheless, in cases of orthogonal wavelets\nwhose spectrum does not verify this condition, it is shown how to construct an\n\"equivalent\" filter bank with no spectral overlapping. \n\n"}
{"id": "1801.06532", "contents": "Title: Distribution-free runs-based control charts Abstract: We propose distribution-free runs-based control charts for detecting location\nshifts. Using the fact that given the number of total successes, the outcomes\nof a sequence of Bernoulli trials are random permutations, we are able to\ncontrol the conditional probability of a signal detected at current time given\nthat there is not alarm before at a pre-determined level. This leads to a\ndesired in-control average run length and data-dependent control limits. Two\ncommon runs statistics, the longest run statistic and the scan statitsic, are\nstudied in detail and their exact conditional distributions given the number of\ntotal successes are obtained using the finite Markov chain imbedding technique.\nNumerical results are given to evaluate the performance of the proposed control\ncharts. \n\n"}
{"id": "1801.06739", "contents": "Title: Missing at random: a stochastic process perspective Abstract: We offer a natural and extensible measure-theoretic treatment of missingness\nat random. Within the standard missing data framework, we give a novel\ncharacterisation of the observed data as a stopping-set sigma algebra. We\ndemonstrate that the usual missingness at random conditions are equivalent to\nrequiring particular stochastic processes to be adapted to a set-indexed\nfiltration of the complete data: measurability conditions that suffice to\nensure the likelihood factorisation necessary for ignorability. Our rigorous\nstatement of the missing at random conditions also clarifies a common\nconfusion: what is fixed, and what is random? \n\n"}
{"id": "1801.08120", "contents": "Title: Optimal Estimation of Simultaneous Signals Using Absolute Inner Product\n  with Applications to Integrative Genomics Abstract: Integrating the summary statistics from genome-wide association study\n(\\textsc{gwas}) and expression quantitative trait loci (e\\textsc{qtl}) data\nprovides a powerful way of identifying the genes whose expression levels are\npotentially associated with complex diseases. A parameter called $T$-score that\nquantifies the genetic overlap between a gene and the disease phenotype based\non the summary statistics is introduced based on the mean values of two\nGaussian sequences. Specifically, given two independent samples\n$\\mathbf{x}_n\\sim N(\\theta, \\Sigma_1)$ and $\\mathbf{y}_n\\sim N(\\mu, \\Sigma_2)$,\nthe $T$-score is defined as $\\sum_{i=1}^n |\\theta_i\\mu_i|$, a non-smooth\nfunctional, which characterizes the amount of shared signals between two\nabsolute normal mean vectors $|\\theta|$ and $|\\mu|$. Using approximation\ntheory, estimators are constructed and shown to be minimax rate-optimal and\nadaptive over various parameter spaces. Simulation studies demonstrate the\nsuperiority of the proposed estimators over existing methods. The method is\napplied to an integrative analysis of heart failure genomics datasets and we\nidentify several genes and biological pathways that are potentially causal to\nhuman heart failure. \n\n"}
{"id": "1801.09236", "contents": "Title: Structure and Sensitivity in Differential Privacy: Comparing K-Norm\n  Mechanisms Abstract: Differential privacy (DP), provides a framework for provable privacy\nprotection against arbitrary adversaries, while allowing the release of summary\nstatistics and synthetic data. We address the problem of releasing a noisy\nreal-valued statistic vector $T$, a function of sensitive data under DP, via\nthe class of $K$-norm mechanisms with the goal of minimizing the noise added to\nachieve privacy. First, we introduce the sensitivity space of $T$, which\nextends the concepts of sensitivity polytope and sensitivity hull to the\nsetting of arbitrary statistics $T$. We then propose a framework consisting of\nthree methods for comparing the $K$-norm mechanisms: 1) a multivariate\nextension of stochastic dominance, 2) the entropy of the mechanism, and 3) the\nconditional variance given a direction, to identify the optimal $K$-norm\nmechanism. In all of these criteria, the optimal $K$-norm mechanism is\ngenerated by the convex hull of the sensitivity space. Using our methodology,\nwe extend the objective perturbation and functional mechanisms and apply these\ntools to logistic and linear regression, allowing for private releases of\nstatistical results. Via simulations and an application to a housing price\ndataset, we demonstrate that our proposed methodology offers a substantial\nimprovement in utility for the same level of risk. \n\n"}
{"id": "1801.09874", "contents": "Title: Change point analysis in non-stationary processes - a mass excess\n  approach Abstract: This paper considers the problem of testing if a sequence of means\n$(\\mu_t)_{t =1,\\ldots ,n }$ of a non-stationary time series $(X_t)_{t =1,\\ldots\n,n }$ is stable in the sense that the difference of the means $\\mu_1$ and\n$\\mu_t$ between the initial time $t=1$ and any other time is smaller than a\ngiven level, that is $ | \\mu_1 - \\mu_t | \\leq c $ for all $t =1,\\ldots ,n $. A\ntest for hypotheses of this type is developed using a biascorrected monotone\nrearranged local linear estimator and asymptotic normality of the corresponding\ntest statistic is established. As the asymptotic variance depends on the\nlocation and order of the critical roots of the equation $| \\mu_1 - \\mu_t | =\nc$ a new bootstrap procedure is proposed to obtain critical values and its\nconsistency is established. As a consequence we are able to quantitatively\ndescribe relevant deviations of a non-stationary sequence from its initial\nvalue. The results are illustrated by means of a simulation study and by\nanalyzing data examples. \n\n"}
{"id": "1801.10567", "contents": "Title: De-biased sparse PCA: Inference and testing for eigenstructure of large\n  covariance matrices Abstract: Sparse principal component analysis (sPCA) has become one of the most widely\nused techniques for dimensionality reduction in high-dimensional datasets. The\nmain challenge underlying sPCA is to estimate the first vector of loadings of\nthe population covariance matrix, provided that only a certain number of\nloadings are non-zero. In this paper, we propose confidence intervals for\nindividual loadings and for the largest eigenvalue of the population covariance\nmatrix. Given an independent sample $X^i \\in\\mathbb R^p, i = 1,...,n,$\ngenerated from an unknown distribution with an unknown covariance matrix\n$\\Sigma_0$, our aim is to estimate the first vector of loadings and the largest\neigenvalue of $\\Sigma_0$ in a setting where $p\\gg n$. Next to the\nhigh-dimensionality, another challenge lies in the inherent non-convexity of\nthe problem. We base our methodology on a Lasso-penalized M-estimator which,\ndespite non-convexity, may be solved by a polynomial-time algorithm such as\ncoordinate or gradient descent. We show that our estimator achieves the minimax\noptimal rates in $\\ell_1$ and $\\ell_2$-norm. We identify the bias in the\nLasso-based estimator and propose a de-biased sparse PCA estimator for the\nvector of loadings and for the largest eigenvalue of the covariance matrix\n$\\Sigma_0$. Our main results provide theoretical guarantees for asymptotic\nnormality of the de-biased estimator. The major conditions we impose are\nsparsity in the first eigenvector of small order $\\sqrt{n}/\\log p$ and sparsity\nof the same order in the columns of the inverse Hessian matrix of the\npopulation risk. \n\n"}
{"id": "1802.05444", "contents": "Title: A Weighted Likelihood Approach Based on Statistical Data Depths Abstract: We propose a general approach to construct weighted likelihood estimating\nequations with the aim of obtain robust estimates. The weight, attached to each\nscore contribution, is evaluated by comparing the statistical data depth at the\nmodel with that of the sample in a given point. Observations are considered\nregular when the ratio of these two depths is close to one, whereas, when the\nratio is large the corresponding score contribution may be downweigthed.\nDetails and examples are provided for the robust estimation of the parameters\nin the multivariate normal model. Because of the form of the weights, we expect\nthat, there will be no downweighting under the true model leading to highly\nefficient estimators. Robustness is illustrated using two real data sets. \n\n"}
{"id": "1802.08798", "contents": "Title: Automatic adaptation of MCMC algorithms Abstract: Markov chain Monte Carlo (MCMC) methods are ubiquitous tools for\nsimulation-based inference in many fields but designing and identifying good\nMCMC samplers is still an open question. This paper introduces a novel MCMC\nalgorithm, namely, Auto Adapt MCMC. For sampling variables or blocks of\nvariables, we use two levels of adaptation where the inner adaptation optimizes\nthe MCMC performance within each sampler, while the outer adaptation explores\nthe valid space of kernels to find the optimal samplers. We provide a\ntheoretical foundation for our approach. To show the generality and usefulness\nof the approach, we describe a framework using only standard MCMC samplers as\ncandidate samplers and some adaptation schemes for both inner and outer\niterations. In several benchmark problems, we show that our proposed approach\nsubstantially outperforms other approaches, including an automatic blocking\nalgorithm, in terms of MCMC efficiency and computational time. \n\n"}
{"id": "1802.08895", "contents": "Title: A Semi-Smooth Newton Algorithm for High-Dimensional Nonconvex Sparse\n  Learning Abstract: The smoothly clipped absolute deviation (SCAD) and the minimax concave\npenalty (MCP) penalized regression models are two important and widely used\nnonconvex sparse learning tools that can handle variable selection and\nparameter estimation simultaneously, and thus have potential applications in\nvarious fields such as mining biological data in high-throughput biomedical\nstudies. Theoretically, these two models enjoy the oracle property even in the\nhigh-dimensional settings, where the number of predictors $p$ may be much\nlarger than the number of observations $n$. However, numerically, it is quite\nchallenging to develop fast and stable algorithms due to their non-convexity\nand non-smoothness. In this paper we develop a fast algorithm for SCAD and MCP\npenalized learning problems. First, we show that the global minimizers of both\nmodels are roots of the nonsmooth equations. Then, a semi-smooth Newton (SSN)\nalgorithm is employed to solve the equations. We prove that the SSN algorithm\nconverges locally and superlinearly to the Karush-Kuhn-Tucker (KKT) points.\nComputational complexity analysis shows that the cost of the SSN algorithm per\niteration is $O(np)$. Combined with the warm-start technique, the SSN algorithm\ncan be very efficient and accurate. Simulation studies and a real data example\nsuggest that our SSN algorithm, with comparable solution accuracy with the\ncoordinate descent (CD) and the difference of convex (DC) proximal Newton\nalgorithms, is more computationally efficient. \n\n"}
{"id": "1802.09117", "contents": "Title: Testability of high-dimensional linear models with non-sparse structures Abstract: Understanding statistical inference under possibly non-sparse\nhigh-dimensional models has gained much interest recently. For a given\ncomponent of the regression coefficient, we show that the difficulty of the\nproblem depends on the sparsity of the corresponding row of the precision\nmatrix of the covariates, not the sparsity of the regression coefficients. We\ndevelop new concepts of uniform and essentially uniform non-testability that\nallow the study of limitations of tests across a broad set of alternatives.\nUniform non-testability identifies a collection of alternatives such that the\npower of any test, against any alternative in the group, is asymptotically at\nmost equal to the nominal size. Implications of the new constructions include\nnew minimax testability results that, in sharp contrast to the current results,\ndo not depend on the sparsity of the regression parameters. We identify new\ntradeoffs between testability and feature correlation. In particular, we show\nthat, in models with weak feature correlations, minimax lower bound can be\nattained by a test whose power has the $\\sqrt{n}$ rate, regardless of the size\nof the model sparsity. \n\n"}
{"id": "1803.00889", "contents": "Title: Scalable Bayesian uncertainty quantification in imaging inverse problems\n  via convex optimization Abstract: We propose a Bayesian uncertainty quantification method for large-scale\nimaging inverse problems. Our method applies to all Bayesian models that are\nlog-concave, where maximum-a-posteriori (MAP) estimation is a convex\noptimization problem. The method is a framework to analyse the confidence in\nspecific structures observed in MAP estimates (e.g., lesions in medical\nimaging, celestial sources in astronomical imaging), to enable using them as\nevidence to inform decisions and conclusions. Precisely, following Bayesian\ndecision theory, we seek to assert the structures under scrutiny by performing\na Bayesian hypothesis test that proceeds as follows: firstly, it postulates\nthat the structures are not present in the true image, and then seeks to use\nthe data and prior knowledge to reject this null hypothesis with high\nprobability. Computing such tests for imaging problems is generally very\ndifficult because of the high dimensionality involved. A main feature of this\nwork is to leverage probability concentration phenomena and the underlying\nconvex geometry to formulate the Bayesian hypothesis test as a convex problem,\nthat we then efficiently solve by using scalable optimization algorithms. This\nallows scaling to high-resolution and high-sensitivity imaging problems that\nare computationally unaffordable for other Bayesian computation approaches. We\nillustrate our methodology, dubbed BUQO (Bayesian Uncertainty Quantification by\nOptimization), on a range of challenging Fourier imaging problems arising in\nastronomy and medicine. \n\n"}
{"id": "1803.01231", "contents": "Title: Bayesian Projected Calibration of Computer Models Abstract: We develop a Bayesian approach called Bayesian projected calibration to\naddress the problem of calibrating an imperfect computer model using\nobservational data from a complex physical system. The calibration parameter\nand the physical system are parametrized in an identifiable fashion via\n$L_2$-projection. The physical process is assigned a Gaussian process prior,\nwhich naturally induces a prior distribution on the calibration parameter\nthrough the $L_2$-projection constraint. The calibration parameter is estimated\nthrough its posterior distribution, which provides a natural and non-asymptotic\nway for the uncertainty quantification. We provide a rigorous large sample\njustification for the proposed approach by establishing the asymptotic\nnormality of the posterior of the calibration parameter with the efficient\ncovariance matrix. In addition, two efficient computational algorithms based on\nstochastic approximation are designed with theoretical guarantees. Through\nextensive simulation studies and two real-world datasets analyses, we show that\nthe Bayesian projected calibration can accurately estimate the calibration\nparameters, appropriately calibrate the computer models, and compare favorably\nto alternative approaches. \n\n"}
{"id": "1803.01639", "contents": "Title: When do we have the power to detect biological interactions in spatial\n  point patterns? Abstract: Determining the relative importance of environmental factors, biotic\ninteractions and stochasticity in assembling and maintaining species-rich\ncommunities remains a major challenge in ecology. In plant communities,\ninteractions between individuals of different species are expected to leave a\nspatial signature in the form of positive or negative spatial correlations over\ndistances relating to the spatial scale of interaction. Most studies using\nspatial point process tools have found relatively little evidence for\ninteractions between pairs of species. More interactions tend to be detected in\ncommunities with fewer species. However, there is currently no understanding of\nhow the power to detect spatial interactions may change with sample size, or\nthe scale and intensity of interactions.\n  We use a simple 2-species model where the scale and intensity of interactions\nare controlled to simulate point pattern data. In combination with an\napproximation to the variance of the spatial summary statistics that we sample,\nwe investigate the power of current spatial point pattern methods to correctly\nreject the null model of bivariate species independence.\n  We show that the power to detect interactions is positively related to the\nabundances of the species tested, and the intensity and scale of interactions.\nIncreasing imbalance in abundances has a negative effect on the power to detect\ninteractions. At population sizes typically found in currently available\ndatasets for species-rich plant communities we find only a very low power to\ndetect interactions. Differences in power may explain the increased frequency\nof interactions in communities with fewer species. Furthermore, the\ncommunity-wide frequency of detected interactions is very sensitive to a\nminimum abundance criterion for including species in the analyses. \n\n"}
{"id": "1803.02302", "contents": "Title: Randomization inference with general interference and censoring Abstract: Interference occurs between individuals when the treatment (or exposure) of\none individual affects the outcome of another individual. Previous work on\ncausal inference methods in the presence of interference has focused on the\nsetting where a priori it is assumed there is 'partial interference,' in the\nsense that individuals can be partitioned into groups wherein there is no\ninterference between individuals in different groups. Bowers, Fredrickson, and\nPanagopoulos (2012) and Bowers, Fredrickson, and Aronow (2016) consider\nrandomization-based inferential methods that allow for more general\ninterference structures in the context of randomized experiments. In this\npaper, extensions of Bowers et al. which allow for failure time outcomes\nsubject to right censoring are proposed. Permitting right censored outcomes is\nchallenging because standard randomization-based tests of the null hypothesis\nof no treatment effect assume that whether an individual is censored does not\ndepend on treatment. The proposed extension of Bowers et al. to allow for\ncensoring entails adapting the method of Wang, Lagakos, and Gray (2010) for two\nsample survival comparisons in the presence of unequal censoring. The methods\nare examined via simulation studies and utilized to assess the effects of\ncholera vaccination in an individually-randomized trial of 73,000 children and\nwomen in Matlab, Bangladesh. \n\n"}
{"id": "1803.02861", "contents": "Title: LHC Phenomenology of Dark Matter with a Color-Octet Partner Abstract: Colored dark sectors where the dark matter particle is accompanied by colored\npartners have recently attracted theoretical and phenomenological interest. We\nexplore the possibility that the dark sector consists of the dark matter\nparticle and a color-octet partner, where the interaction with the Standard\nModel is governed by an effective operator involving gluons. The resulting\ninteractions resemble the color analogues of electric and magnetic dipole\nmoments. Although many phenomenological features of this kind of model only\ndepend on the group representation of the partner under SU(3)$_c$, we point out\nthat interesting collider signatures such as $R$-hadrons are indeed controlled\nby the interaction operator between the dark and visible sector. We perform a\nstudy of the current constraints and future reach of LHC searches, where the\ncomplementarity between different possible signals is highlighted and\nexploited. \n\n"}
{"id": "1803.04397", "contents": "Title: An information-theoretic Phase I/II design for molecularly targeted\n  agents that does not require an assumption of monotonicity Abstract: For many years Phase I and Phase II clinical trials were conducted\nseparately, but there was a recent shift to combine these Phases. While a\nvariety of Phase~I/II model-based designs for cytotoxic agents were proposed in\nthe literature, methods for molecularly targeted agents (TA) are just starting\nto develop. The main challenge of the TA setting is the unknown dose-efficacy\nrelation that can have either an increasing, plateau or umbrella shape. To\ncapture these, approaches with more parameters are needed to model the\ndose-efficacy relationship or, alternatively, more orderings of the\ndose-efficacy relationship are required to account for the uncertainty in the\ncurve shape. As a result, designs for more complex clinical trials, for\nexample, trials looking at schedules of a combination treatment involving TA,\nhave not been extensively studied yet. We propose a novel regimen-finding\ndesign which is based on a derived efficacy-toxicity trade-off function. Due to\nits special properties, an accurate regimen selection can be achieved without\nany parametric or monotonicity assumptions. We illustrate how this design can\nbe applied in the context of a complex combination-schedule clinical trial. We\ndiscuss practical and ethical issues such as coherence, delayed and missing\nefficacy responses, safety and futility constraints. \n\n"}
{"id": "1803.04991", "contents": "Title: Inference on a Distribution from Noisy Draws Abstract: We consider a situation where the distribution of a random variable is being\nestimated by the empirical distribution of noisy measurements of that variable.\nThis is common practice in, for example, teacher value-added models and other\nfixed-effect models for panel data. We use an asymptotic embedding where the\nnoise shrinks with the sample size to calculate the leading bias in the\nempirical distribution arising from the presence of noise. The leading bias in\nthe empirical quantile function is equally obtained. These calculations are new\nin the literature, where only results on smooth functionals such as the mean\nand variance have been derived. We provide both analytical and jackknife\ncorrections that recenter the limit distribution and yield confidence intervals\nwith correct coverage in large samples. Our approach can be connected to\ncorrections for selection bias and shrinkage estimation and is to be contrasted\nwith deconvolution. Simulation results confirm the much-improved sampling\nbehavior of the corrected estimators. An empirical illustration on\nheterogeneity in deviations from the law of one price is equally provided. \n\n"}
{"id": "1803.06292", "contents": "Title: Search for high-mass resonances in dilepton final states in\n  proton-proton collisions at $\\sqrt{s}=$ 13 TeV Abstract: A search is presented for new high-mass resonances decaying into electron or\nmuon pairs. The search uses proton-proton collision data at a centre-of-mass\nenergy of 13 TeV collected by the CMS experiment at the LHC in 2016,\ncorresponding to an integrated luminosity of 36 fb$^{-1}$. Observations are in\nagreement with standard model expectations. Upper limits on the product of a\nnew resonance production cross section and branching fraction to dileptons are\ncalculated in a model-independent manner. This permits the interpretation of\nthe limits in models predicting a narrow dielectron or dimuon resonance. A scan\nof different intrinsic width hypotheses is performed. Limits are set on the\nmasses of various hypothetical particles. For the Z$'_\\mathrm{SSM}$\n(Z$'_{\\psi}$) particle, which arises in the sequential standard model\n(superstring-inspired model), a lower mass limit of 4.50 (3.90) TeV is set at\n95% confidence level. The lightest Kaluza-Klein graviton arising in the\nRandall-Sundrum model of extra dimensions, with coupling parameters\n$k/\\overline{M}_\\mathrm{Pl}$ of 0.01, 0.05, and 0.10, is excluded at 95%\nconfidence level below 2.10, 3.65, and 4.25 TeV, respectively. In a simplified\nmodel of dark matter production via a vector or axial vector mediator, limits\nat 95% confidence level are obtained on the masses of the dark matter particle\nand its mediator. \n\n"}
{"id": "1803.08000", "contents": "Title: Boosting Random Forests to Reduce Bias; One-Step Boosted Forest and its\n  Variance Estimate Abstract: In this paper we propose using the principle of boosting to reduce the bias\nof a random forest prediction in the regression setting. From the original\nrandom forest fit we extract the residuals and then fit another random forest\nto these residuals. We call the sum of these two random forests a\n\\textit{one-step boosted forest}. We show with simulated and real data that the\none-step boosted forest has a reduced bias compared to the original random\nforest. The paper also provides a variance estimate of the one-step boosted\nforest by an extension of the infinitesimal Jackknife estimator. Using this\nvariance estimate we can construct prediction intervals for the boosted forest\nand we show that they have good coverage probabilities. Combining the bias\nreduction and the variance estimate we show that the one-step boosted forest\nhas a significant reduction in predictive mean squared error and thus an\nimprovement in predictive performance. When applied on datasets from the UCI\ndatabase, one-step boosted forest performs better than random forest and\ngradient boosting machine algorithms. Theoretically we can also extend such a\nboosting process to more than one step and the same principles outlined in this\npaper can be used to find variance estimates for such predictors. Such boosting\nwill reduce bias even further but it risks over-fitting and also increases the\ncomputational burden. \n\n"}
{"id": "1803.10655", "contents": "Title: Bayesian Regression with Undirected Network Predictors with an\n  Application to Brain Connectome Data Abstract: This article proposes a Bayesian approach to regression with a continuous\nscalar response and an undirected network predictor. Undirected network\npredictors are often expressed in terms of symmetric adjacency matrices, with\nrows and columns of the matrix representing the nodes, and zero entries\nsignifying no association between two corresponding nodes. Network predictor\nmatrices are typically vectorized prior to any analysis, thus failing to\naccount for the important structural information in the network. This results\nin poor inferential and predictive performance in presence of small sample\nsizes. We propose a novel class of network shrinkage priors for the coefficient\ncorresponding to the undirected network predictor. The proposed framework is\ndevised to detect both nodes and edges in the network predictive of the\nresponse. Our framework is implemented using an efficient Markov Chain Monte\nCarlo algorithm. Empirical results in simulation studies illustrate strikingly\nsuperior inferential and predictive gains of the proposed framework in\ncomparison with the ordinary high dimensional Bayesian shrinkage priors and\npenalized optimization schemes. We apply our method to a brain connectome\ndataset that contains information on brain networks along with a measure of\ncreativity for multiple individuals. Here, interest lies in building a\nregression model of the creativity measure on the network predictor to identify\nimportant regions and connections in the brain strongly associated with\ncreativity. To the best of our knowledge, our approach is the first principled\nBayesian method that is able to detect scientifically interpretable regions and\nconnections in the brain actively impacting the continuous response\n(creativity) in the presence of a small sample size. \n\n"}
{"id": "1804.00102", "contents": "Title: Collaborative targeted inference from continuously indexed nuisance\n  parameter estimators Abstract: We wish to infer the value of a parameter at a law from which we sample\nindependent observations. The parameter is smooth and we can define two\nvariation-independent features of the law, its $Q$- and $G$-components, such\nthat estimating them consistently at a fast enough product of rates allows to\nbuild a confidence interval (CI) with a given asymptotic level from a plain\ntargeted minimum loss estimator (TMLE). Say that the above product is not fast\nenough and the algorithm for the $G$-component is fine-tuned by a real-valued\n$h$. A plain TMLE with an $h$ chosen by cross-validation would typically not\nyield a CI. We construct a collaborative TMLE (C-TMLE) and show under mild\nconditions that, if there exists an oracle $h$ that makes a bulky remainder\nterm asymptotically Gaussian, then the C-TMLE yields a CI. We illustrate our\nfindings with the inference of the average treatment effect. We conduct a\nsimulation study where the $G$-component is estimated by the LASSO and $h$ is\nthe bound on the coefficients' norms. It sheds light on small sample\nproperties, in the face of low- to high-dimensional baseline covariates, and\npossibly positivity violation. \n\n"}
{"id": "1804.00195", "contents": "Title: Robust and Efficient Semi-Supervised Estimation of Average Treatment\n  Effects with Application to Electronic Health Records Data Abstract: We consider the problem of estimating the average treatment effect (ATE) in a\nsemi-supervised learning setting, where a very small proportion of the entire\nset of observations are labeled with the true outcome but features predictive\nof the outcome are available among all observations. This problem arises, for\nexample, when estimating treatment effects in electronic health records (EHR)\ndata because gold-standard outcomes are often not directly observable from the\nrecords but are observed for a limited number of patients through small-scale\nmanual chart review. We develop an imputation-based approach for estimating the\nATE that is robust to misspecification of the imputation model. This\neffectively allows information from the predictive features to be safely\nleveraged to improve efficiency in estimating the ATE. The estimator is\nadditionally doubly-robust in that it is consistent under correct specification\nof either an initial propensity score model or a baseline outcome model. It is\nalso locally semiparametric efficient under an ideal semi-supervised model\nwhere the distribution of the unlabeled data is known. Simulations exhibit the\nefficiency and robustness of the proposed method compared to existing\napproaches in finite samples.We illustrate the method by comparing rates of\ntreatment response to two biologic agents for treatment inflammatory bowel\ndisease using EHR data from Partner's Healthcare. \n\n"}
{"id": "1804.01864", "contents": "Title: Adaptive test for ergodic diffusions plus noise Abstract: We propose some parametric tests for ergodic diffusion-plus-noise model,\nwhich is a version of state-space modelling in statistics for stochastic\ndiffusion equations. The test statistics are classified into three types:\nlikelihood-ratio-type test statistic; Wald-type one; and Rao-type one. All the\ntest statistics are constructed with quasi-likelihood-functions for local mean\nsequence of noised observation. We also simulate the behaviour of them for\nseveral practical hypothesis tests and check the convergence in law of test\nstatistics under null hypotheses and consistency of the test under alternative\nones. We apply the method for real data analysis of wind data, and examine some\nsets of the hypotheses mainly with respect to the structure of diffusion\ncoefficient. \n\n"}
{"id": "1804.03366", "contents": "Title: Testing equality of spectral density operators for functional linear\n  processes Abstract: The problem of testing equality of the entire second order structure of two\nindependent functional linear processes is considered. A fully functional\n$L^2$-type test is developed which evaluates, over all frequencies, the\nHilbert-Schmidt distance between the estimated spectral density operators of\nthe two processes. The asymptotic behavior of the test statistic is\ninvestigated and its limiting distribution under the null hypothesis is\nderived. Furthermore, a novel frequency domain bootstrap method is developed\nwhich approximates more accurately the distribution of the test statistic under\nthe null than the large sample Gaussian approximation obtained. Asymptotic\nvalidity of the bootstrap procedure is established and consistency of the\nbootstrap-based test under the alternative is proved. Numerical simulations\nshow that, even for small samples, the bootstrap-based test has very good size\nand power behavior. An application to meteorological functional time series is\nalso presented. \n\n"}
{"id": "1804.04299", "contents": "Title: Model identification for ARMA time series through convolutional neural\n  networks Abstract: In this paper, we use convolutional neural networks to address the problem of\nmodel identification for autoregressive moving average time series models. We\ncompare the performance of several neural network architectures, trained on\nsimulated time series, with likelihood based methods, in particular the Akaike\nand Bayesian information criteria. We find that our neural networks can\nsignificantly outperform these likelihood based methods in terms of accuracy\nand, by orders of magnitude, in terms of speed. \n\n"}
{"id": "1804.04541", "contents": "Title: A copula-based sensitivity analysis method and its application to a\n  North Sea sediment transport model Abstract: This paper describes a novel sensitivity analysis method, able to handle\ndependency relationships between model parameters. The starting point is the\npopular Morris (1991) algorithm, which was initially devised under the\nassumption of parameter independence. This important limitation is tackled by\nallowing the user to incorporate dependency information through a copula. The\nset of model runs obtained using latin hypercube sampling, are then used for\nderiving appropriate sensitivity measures.\n  Delft3D-WAQ (Deltares, 2010) is a sediment transport model with strong\ncorrelations between input parameters. Despite this, the parameter ranking\nobtained with the newly proposed method is in accordance with the knowledge\nobtained from expert judgment. However, under the same conditions, the classic\nMorris method elicits its results from model runs which break the assumptions\nof the underlying physical processes. This leads to the conclusion that the\nproposed extension is superior to the classic Morris algorithm and can\naccommodate a wide range of use cases. \n\n"}
{"id": "1804.04560", "contents": "Title: Running of Fermion Observables in Non-Supersymmetric SO(10) Models Abstract: We investigate the complete renormalization group running of fermion\nobservables in two different realistic non-supersymmetric models based on the\ngauge group $\\textrm{SO}(10)$ with intermediate symmetry breaking for both\nnormal and inverted neutrino mass orderings. Contrary to results of previous\nworks, we find that the model with the more minimal Yukawa sector of the\nLagrangian fails to reproduce the measured values of observables at the\nelectroweak scale, whereas the model with the more extended Yukawa sector can\ndo so if the neutrino masses have normal ordering. The difficulty in finding\nacceptable fits to measured data is a result of the added complexity from the\neffect of an intermediate symmetry breaking as well as tension in the value of\nthe leptonic mixing angle $\\theta^\\ell_{23}$. \n\n"}
{"id": "1804.04583", "contents": "Title: A New Generative Statistical Model for Graphs: The Latent Order Logistic\n  (LOLOG) Model Abstract: Full probability models are critical for the statistical modeling of complex\nnetworks, and yet there are few general, flexible and widely applicable\ngenerative methods. We propose a new family of probability models motivated by\nthe idea of network growth, which we call the Latent Order Logistic (LOLOG)\nmodel. LOLOG is a fully general framework capable of describing any probability\ndistribution over graph configurations, though not all distributions are easily\nexpressible or estimable as a LOLOG. We develop inferential procedures based on\nMonte Carlo Method of Moments, Generalized Method of Moments and variational\ninference. To show the flexibility of the model framework, we show how\nso-called scale-free networks can be modeled as LOLOGs via preferential\nattachment. The advantages of LOLOG in terms of avoidance of degeneracy, ease\nof sampling, and model flexibility are illustrated. Connections with the\npopular Exponential-family Random Graph model (ERGM) are also explored, and we\nfind that they are identical in the case of dyadic independence. Finally, we\napply the model to a social network of collaboration within a corporate law\nfirm, a friendship network among adolescent students, and the friendship\nrelations in an online social network. \n\n"}
{"id": "1804.05079", "contents": "Title: Robust Estimation of the Weighted Average Treatment Effect for A Target\n  Population Abstract: The weighted average treatment effect (WATE) is a causal measure for the\ncomparison of interventions in a specific target population, which may be\ndifferent from the population where data are sampled from. For instance, when\nthe goal is to introduce a new treatment to a target population, the question\nis what efficacy (or effectiveness) can be gained by switching patients from a\nstandard of care (control) to this new treatment, for which the average\ntreatment effect for the control (ATC) estimand can be applied. In this paper,\nwe propose two estimators based on augmented inverse probability weighting to\nestimate the WATE for a well defined target population (i.e., there exists a\ntarget function that describes the population of interest), using observational\ndata. The first proposed estimator is doubly robust if the target function is\nknown or can be correctly specified. The second proposed estimator is doubly\nrobust if the target function has a linear dependence on the propensity score,\nwhich can be used to estimate the average treatment effect for the treated\n(ATT) and ATC. We demonstrate the properties of the proposed estimators through\ntheoretical proof and simulation studies. We also apply our proposed methods in\na comparison of glucagon-like peptide-1 receptor agonists therapy and insulin\ntherapy among patients with type 2 diabetes, using the UK clinical practice\nresearch datalink data. \n\n"}
{"id": "1804.05863", "contents": "Title: Introduction to Effective Field Theories Abstract: Lecture notes from the 2017 Les Houches Summer School on Effective Field\nTheories. The lectures covered introductory material on EFTs as used in high\nenergy physics to compute experimentally observable quantities. Other lectures\nat the school covered a wide range of applications in greater depth. \n\n"}
{"id": "1804.05923", "contents": "Title: A stochastic second-order generalized estimating equations approach for\n  estimating intraclass correlation coefficient in the presence of informative\n  missing data Abstract: Design and analysis of cluster randomized trials must take into account\ncorrelation among outcomes from the same clusters. When applying standard\ngeneralized estimating equations (GEE), the first-order (e.g. treatment)\neffects can be estimated consistently even with a misspecified correlation\nstructure. In settings for which the correlation is of interest, one could\nestimate this quantity via second-order generalized estimating equations\n(GEE2). We build upon GEE2 in the setting of missing data, for which we\nincorporate a \"second-order\" inverse-probability weighting (IPW) scheme and\n\"second-order\" doubly robust (DR) estimating equations that guard against\npartial model misspecification. We highlight the need to model correlation\namong missing indicators in such settings. In addition, the computational\ndifficulties in solving these second-order equations have motivated our\ndevelopment of more computationally efficient algorithms for solving GEE2,\nwhich alleviates reliance on parameter starting values and provides\nsubstantially faster and higher convergence rates than the more widely used\ndeterministic root-solving methods. \n\n"}
{"id": "1804.08650", "contents": "Title: Bayesian Bandwidth Test and Selection for High-dimensional Banded\n  Precision Matrices Abstract: Assuming a banded structure is one of the common practice in the estimation\nof high-dimensional precision matrix. In this case, estimating the bandwidth of\nthe precision matrix is a crucial initial step for subsequent analysis.\nAlthough there exist some consistent frequentist tests for the bandwidth\nparameter, bandwidth selection consistency for precision matrices has not been\nestablished in a Bayesian framework. In this paper, we propose a prior\ndistribution tailored to the bandwidth estimation of high-dimensional precision\nmatrices. The banded structure is imposed via the Cholesky factor from the\nmodified Cholesky decomposition. We establish the strong model selection\nconsistency for the bandwidth as well as the consistency of the Bayes factor.\nThe convergence rates for Bayes factors under both the null and alternative\nhypotheses are derived which yield similar order of rates. As a by-product, we\nalso proposed an estimation procedure for the Cholesky factors yielding an\nalmost optimal order of convergence rates. Two-sample bandwidth test is also\nconsidered, and it turns out that our method is able to consistently detect the\nequality of bandwidths between two precision matrices. The simulation study\nconfirms that our method in general outperforms the existing frequentist and\nBayesian methods. \n\n"}
{"id": "1804.08962", "contents": "Title: Data-driven regularization of Wasserstein barycenters with an\n  application to multivariate density registration Abstract: We present a framework to simultaneously align and smooth data in the form of\nmultiple point clouds sampled from unknown densities with support in a\nd-dimensional Euclidean space. This work is motivated by applications in\nbioinformatics where researchers aim to automatically homogenize large datasets\nto compare and analyze characteristics within a same cell population.\nInconveniently, the information acquired is most certainly noisy due to\nmis-alignment caused by technical variations of the environment. To overcome\nthis problem, we propose to register multiple point clouds by using the notion\nof regularized barycenters (or Fr\\'{e}chet mean) of a set of probability\nmeasures with respect to the Wasserstein metric. A first approach consists in\npenalizing a Wasserstein barycenter with a convex functional as recently\nproposed in Bigot and al. (2018). A second strategy is to transform the\nWasserstein metric itself into an entropy regularized transportation cost\nbetween probability measures as introduced in Cuturi (2013). The main\ncontribution of this work is to propose data-driven choices for the\nregularization parameters involved in each approach using the\nGoldenshluger-Lepski's principle. Simulated data sampled from Gaussian mixtures\nare used to illustrate each method, and an application to the analysis of flow\ncytometry data is finally proposed. This way of choosing of the regularization\nparameter for the Sinkhorn barycenter is also analyzed through the prism of an\noracle inequality that relates the error made by such data-driven estimators to\nthe one of an ideal estimator. \n\n"}
{"id": "1804.09749", "contents": "Title: Corrections of two-photon interactions in the fine and hyperfine\n  structure of the P-energy levels of muonic hydrogen Abstract: In the framework of the quasipotential method in quantum electrodynamics we\ncalculate corrections to the nuclear structure proportional to $r_N^2$ from\ntwo-photon exchange amplitudes in the fine and hyperfine structure of P-states\nin muonic hydrogen, as well as the photon-photon interaction amplitudes,\nleading to the exchange of the axial vector meson. In constructing the\nquasipotential of the muon-nucleus interaction, we use the method of projection\noperators on states of two particles with a definite spin and total angular\nmomentum. Analytical calculation of the matrix elements is performed and\ncontributions to the fine and hyperfine structure of the $2P_{1/2}$ and\n$2P_{3/2}$ levels are obtained. \n\n"}
{"id": "1804.10527", "contents": "Title: Detecting and modeling worst-case dependence structures between random\n  inputs of computational reliability models Abstract: Uncertain information on input parameters of reliability models is usually\nmodeled by considering these parameters as random, and described by marginal\ndistributions and a dependence structure of these variables. In numerous\nreal-world applications, while information is mainly provided by marginal\ndistributions, typically from samples , little is really known on the\ndependence structure itself. Faced with this problem of incomplete or missing\ninformation, risk studies are often conducted by considering independence of\ninput variables, at the risk of including irrelevant situations. This approach\nis especially used when reliability functions are considered as black-box\ncomputational models. Such analyses remain weakened in absence of in-depth\nmodel exploration, at the possible price of a strong risk misestimation.\nConsidering the frequent case where the reliability output is a quantile, this\narticle provides a methodology to improve risk assessment, by exploring a set\nof pessimistic dependencies using a copula-based strategy. In dimension greater\nthan two, a greedy algorithm is provided to build input regular vine copulas\nreaching a minimum quantile to which a reliability admissible limit value can\nbe compared, by selecting pairwise components of sensitive influence on the\nresult. The strategy is tested over toy models and a real industrial\ncase-study. The results highlight that current approaches can provide\nnon-conservative results, and that a nontrivial dependence structure can be\nexhibited to define a worst-case scenario. \n\n"}
{"id": "1805.00015", "contents": "Title: Electroweak Dark Matter at Future Hadron Colliders Abstract: In a large class of scenarios, dark matter (DM) particles that belong to a\nmultiplet of the standard model (SM) weak interactions are challenging to probe\nin direct detection experiments due to loop-suppressed cross-sections. Direct\nproduction at colliders is thus crucial to look for such DM candidates, and\nunder current estimates, future runs of the 14-TeV LHC are projected to probe\nmasses of around 300 GeV for DM belonging to an SU(2) doublet (Higgsino-like),\nand 900 GeV for SU(2) triplet (wino-like). We examine how far this mass reach\ncan be extended at the proposed 27-TeV high-energy upgrade of the LHC (HE-LHC),\nand compare the results to the case for a 100-TeV hadron collider. Following a\ndetector setup similar to that of the ATLAS tracking system for the Run-2 LHC\nupgrade, with a new Insertable B-Layer (IBL), a disappearing charged track\nanalysis at the HE-LHC can probe Higgsino-like (wino-like) DM mass of up to 600\nGeV (2.1 TeV) at the 95% C.L. The monojet and missing transverse momentum\nsearch, on the otherhand, has a weaker reach of 490 GeV (700 GeV) at 95% C.L.\nfor the Higgsino-like (wino-like) states. The mass range accessible in the\ncollider searches can be complementary to the indirect detection probes using\ngamma rays from dwarf-spheroidal galaxies. \n\n"}
{"id": "1805.01500", "contents": "Title: Noisin: Unbiased Regularization for Recurrent Neural Networks Abstract: Recurrent neural networks (RNNs) are powerful models of sequential data. They\nhave been successfully used in domains such as text and speech. However, RNNs\nare susceptible to overfitting; regularization is important. In this paper we\ndevelop Noisin, a new method for regularizing RNNs. Noisin injects random noise\ninto the hidden states of the RNN and then maximizes the corresponding marginal\nlikelihood of the data. We show how Noisin applies to any RNN and we study many\ndifferent types of noise. Noisin is unbiased--it preserves the underlying RNN\non average. We characterize how Noisin regularizes its RNN both theoretically\nand empirically. On language modeling benchmarks, Noisin improves over dropout\nby as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We\nalso compared the state-of-the-art language model of Yang et al. 2017, both\nwith and without Noisin. On the Penn Treebank, the method with Noisin more\nquickly reaches state-of-the-art performance. \n\n"}
{"id": "1805.01904", "contents": "Title: Dark Matter in Anomaly-Free Gauge Extensions Abstract: A consistent model for vector mediators to dark matter needs to be\nanomaly-free and include a scalar mode from mass generation. For the leading\nU(1) extensions we review the structure and constraints, including kinetic\nmixing at loop level. The thermal relic density suggests that the vector and\nscalar masses are similar. For the LHC we combine a $Z'$ shape analysis with\nmono-jets. For the latter, we find that a shape analysis offers significant\nimprovement over existing cut-and-count approaches. Direct detection limits\nstrongly constrain the kinetic mixing angle and we propose a $\\ell^+\\ell^- E_T$\nsearch strategy based on the scalar mediator. \n\n"}
{"id": "1805.02826", "contents": "Title: Optimal Subspace Estimation Using Overidentifying Vectors via\n  Generalized Method of Moments Abstract: Many statistical models seek relationship between variables via subspaces of\nreduced dimensions. For instance, in factor models, variables are roughly\ndistributed around a low dimensional subspace determined by the loading matrix;\nin mixed linear regression models, the coefficient vectors for different\nmixtures form a subspace that captures all regression functions; in multiple\nindex models, the effect of covariates is summarized by the effective dimension\nreduction space.\n  Such subspaces are typically unknown, and good estimates are crucial for data\nvisualization, dimension reduction, diagnostics and estimation of unknown\nparameters. Usually, we can estimate these subspaces by computing moments from\ndata. Often, there are many ways to estimate a subspace, by using moments of\ndifferent orders, transformed moments, etc. A natural question is: how can we\ncombine all these moment conditions and achieve optimality for subspace\nestimation?\n  In this paper, we formulate our problem as estimation of an unknown subspace\n$\\mathcal{S}$ of dimension $r$, given a set of overidentifying vectors $\\{\n\\mathrm{\\bf v}_\\ell \\}_{\\ell=1}^m$ (namely $m \\ge r$) that satisfy $\\mathbb{E}\n\\mathrm{\\bf v}_{\\ell} \\in \\mathcal{S}$ and have the form $$ \\mathrm{\\bf v}_\\ell\n= \\frac{1}{n} \\sum_{i=1}^n \\mathrm{\\bf f}_\\ell(\\mathbf{x}_i, y_i), $$ where\ndata are i.i.d. and each function $\\mathrm{\\bf f}_\\ell$ is known. By exploiting\ncertain covariance information related to $\\mathrm{\\bf v}_\\ell$, our estimator\nof $\\mathcal{S}$ uses an optimal weighting matrix and achieves the smallest\nasymptotic error, in terms of canonical angles. The analysis is based on the\ngeneralized method of moments that is tailored to our problem. Our method is\napplied to aforementioned models and distributed estimation of heterogeneous\ndatasets, and may be potentially extended to analyze matrix completion, neural\nnets, among others. \n\n"}
{"id": "1805.04421", "contents": "Title: Covariate-Adjusted Tensor Classification in High-Dimensions Abstract: In contemporary scientific research, it is of great interest to predict a\ncategorical response based on a high-dimensional tensor (i.e. multi-dimensional\narray) and additional covariates. This mixture of different types of data leads\nto challenges in statistical analysis. Motivated by applications in science and\nengineering, we propose a comprehensive and interpretable discriminant analysis\nmodel, called CATCH model (in short for Covariate-Adjusted Tensor\nClassification in High-dimensions), which efficiently integrates the covariates\nand the tensor to predict the categorical outcome. The CATCH model jointly\nmodels the relationships among the covariates, the tensor predictor, and the\ncategorical response. More importantly, it preserves and utilizes the\nstructures of the data for maximum interpretability and optimal prediction. To\ntackle the new computational and statistical challenges arising from the\nintimidating tensor dimensions, we propose a penalized approach to select a\nsubset of tensor predictor entries that has direct discriminative effect after\nadjusting for covariates. We further develop an efficient algorithm that takes\nadvantage of the tensor structure. Theoretical results confirm that our method\nachieves variable selection consistency and optimal classification error, even\nwhen the tensor dimension is much larger than the sample size. The superior\nperformance of our method over existing methods is demonstrated in extensive\nsimulated and real data examples. \n\n"}
{"id": "1805.05109", "contents": "Title: Multi-loop techniques for massless Feynman diagram calculations Abstract: We review several multi-loop techniques for analytical massless Feynman\ndiagram calculations in relativistic quantum field theories: integration by\nparts, the method of uniqueness, functional equations and the Gegenbauer\npolynomial technique. A brief, historically oriented, overview of some of the\nresults obtained over the decades for the massless 2-loop propagator-type\ndiagram is given. Concrete examples of up to $5$-loop diagram calculations are\nalso provided. \n\n"}
{"id": "1805.05480", "contents": "Title: ABC-CDE: Towards Approximate Bayesian Computation with Complex\n  High-Dimensional Data and Limited Simulations Abstract: Approximate Bayesian Computation (ABC) is typically used when the likelihood\nis either unavailable or intractable but where data can be simulated under\ndifferent parameter settings using a forward model. Despite the recent interest\nin ABC, high-dimensional data and costly simulations still remain a bottleneck\nin some applications. There is also no consensus as to how to best assess the\nperformance of such methods without knowing the true posterior. We show how a\nnonparametric conditional density estimation (CDE) framework, which we refer to\nas ABC-CDE, help address three nontrivial challenges in ABC: (i) how to\nefficiently estimate the posterior distribution with limited simulations and\ndifferent types of data, (ii) how to tune and compare the performance of ABC\nand related methods in estimating the posterior itself, rather than just\ncertain properties of the density, and (iii) how to efficiently choose among a\nlarge set of summary statistics based on a CDE surrogate loss. We provide\ntheoretical and empirical evidence that justify ABC-CDE procedures that {\\em\ndirectly} estimate and assess the posterior based on an initial ABC sample, and\nwe describe settings where standard ABC and regression-based approaches are\ninadequate. \n\n"}
{"id": "1805.06225", "contents": "Title: Determination of quark masses from $\\mathbf{n_f=4}$ lattice QCD and the\n  RI-SMOM intermediate scheme Abstract: We determine the charm and strange quark masses in the $\\overline{\\text{MS}}$\nscheme, using $n_f=2+1+1$ lattice QCD calculations with highly improved\nstaggered quarks (HISQ) and the RI-SMOM intermediate scheme to connect the bare\nlattice quark masses to continuum renormalisation schemes. Our study covers\nanalysis of systematic uncertainties from this method, including\nnonperturbative artefacts and the impact of the non-zero physical sea quark\nmasses. We find $m_c^{\\overline{\\text{MS}}}(3 \\text{GeV}) = 0.9896(61)$ GeV and\n$m_s^{\\overline{\\text{MS}}}(3 \\text{GeV}) = 0.08536(85)$ GeV, where the\nuncertainties are dominated by the tuning of the bare lattice quark masses.\nThese results are consistent with, and of similar accuracy to, those using the\ncurrent-current correlator approach coupled to high-order continuum QCD\nperturbation theory, implemented in the same quark formalism and on the same\ngauge field configurations. This provides a strong test of the consistency of\nmethods for determining the quark masses to high precision from lattice QCD. We\nalso give updated lattice QCD world averages for $c$ and $s$ quark masses. \n\n"}
{"id": "1805.06970", "contents": "Title: Global and Simultaneous Hypothesis Testing for High-Dimensional Logistic\n  Regression Models Abstract: High-dimensional logistic regression is widely used in analyzing data with\nbinary outcomes. In this paper, global testing and large-scale multiple testing\nfor the regression coefficients are considered in both single- and\ntwo-regression settings. A test statistic for testing the global null\nhypothesis is constructed using a generalized low-dimensional projection for\nbias correction and its asymptotic null distribution is derived. A lower bound\nfor the global testing is established, which shows that the proposed test is\nasymptotically minimax optimal over some sparsity range. For testing the\nindividual coefficients simultaneously, multiple testing procedures are\nproposed and shown to control the false discovery rate (FDR) and falsely\ndiscovered variables (FDV) asymptotically. Simulation studies are carried out\nto examine the numerical performance of the proposed tests and their\nsuperiority over existing methods. The testing procedures are also illustrated\nby analyzing a data set of a metabolomics study that investigates the\nassociation between fecal metabolites and pediatric Crohn's disease and the\neffects of treatment on such associations. \n\n"}
{"id": "1805.08300", "contents": "Title: Lassoing Eigenvalues Abstract: The properties of penalized sample covariance matrices depend on the choice\nof the penalty function. In this paper, we introduce a class of non-smooth\npenalty functions for the sample covariance matrix, and demonstrate how this\nmethod results in a grouping of the estimated eigenvalues. We refer to this\nmethod as \"lassoing eigenvalues\" or as the \"elasso\". \n\n"}
{"id": "1805.08304", "contents": "Title: Anchored Bayesian Gaussian Mixture Models Abstract: Finite mixtures are a flexible modeling tool for irregularly shaped densities\nand samples from heterogeneous populations. When modeling with mixtures using\nan exchangeable prior on the component features, the component labels are\narbitrary and are indistinguishable in posterior analysis. This makes it\nimpossible to attribute any meaningful interpretation to the marginal posterior\ndistributions of the component features. We propose a model in which a small\nnumber of observations are assumed to arise from some of the labeled component\ndensities. The resulting model is not exchangeable, allowing inference on the\ncomponent features without post-processing. Our method assigns meaning to the\ncomponent labels at the modeling stage and can be justified as a data-dependent\ninformative prior on the labelings. We show that our method produces\ninterpretable results, often (but not always) similar to those resulting from\nrelabeling algorithms, with the added benefit that the marginal inferences\noriginate directly from a well specified probability model rather than a post\nhoc manipulation. We provide asymptotic results leading to practical guidelines\nfor model selection that are motivated by maximizing prior information about\nthe class labels and demonstrate our method on real and simulated data. \n\n"}
{"id": "1805.09182", "contents": "Title: Two-loop five-point massless QCD amplitudes within the IBP approach Abstract: We solve the integration-by-parts (IBP) identities needed for the computation\nof any planar two-loop five-point massless amplitude in QCD. We also derive\nsome new results for the most complicated non-planar topology with irreducible\nnumerators of power as high as six. We do this by applying a new strategy for\nsolving the IBP identities which scales better for problems with a large number\nof scales and/or master integrals. Our results are a proof of principle that\nthe remaining non-planar contributions for all two-loop five-point massless QCD\namplitudes can be computed in analytic form. \n\n"}
{"id": "1805.09736", "contents": "Title: Estimating Population Average Causal Effects in the Presence of\n  Non-Overlap: The Effect of Natural Gas Compressor Station Exposure on Cancer\n  Mortality Abstract: Most causal inference studies rely on the assumption of overlap to estimate\npopulation or sample average causal effects. When data exhibit non-overlap,\nestimation of these estimands requires reliance on model specifications, due to\npoor data support. All existing methods to address non-overlap, such as\ntrimming or down-weighting data in regions of poor support, change the\nestimand. In environmental health research, where study results are often\nintended to influence policy, changes in the estimand can diminish the study's\nimpact, because estimates may not be representative of effects in the\npopulation of interest to policymakers. Researchers may be willing to make\nadditional, minimal modeling assumptions in order to preserve the ability to\nestimate population average causal effects. We seek to make two contributions\non this topic. First, we propose a flexible, data-driven definition of\npropensity score overlap and non-overlap regions. Second, we develop a novel\nBayesian framework to estimate population average causal effects with minor\nmodel dependence and appropriately large uncertainties in the presence of\nnon-overlap. In this approach, the tasks of estimating causal effects in the\noverlap and non-overlap regions are delegated to two distinct models, suited to\nthe degree of data support in each region. Tree ensembles are used to\nnon-parametrically estimate individual causal effects in the overlap region,\nwhere the data can speak for themselves. In the non-overlap region, where\ninsufficient data support means reliance on model specification is necessary,\nindividual causal effects are estimated by extrapolating trends from the\noverlap region via a spline model. The promising performance of our method is\ndemonstrated in simulations. Finally, we utilize our method to perform a novel\ninvestigation of the causal effect of natural gas compressor station exposure\non cancer outcomes. \n\n"}
{"id": "1805.12562", "contents": "Title: Dark Matter Search Results from a One Tonne$\\times$Year Exposure of\n  XENON1T Abstract: We report on a search for Weakly Interacting Massive Particles (WIMPs) using\n278.8 days of data collected with the XENON1T experiment at LNGS. XENON1T\nutilizes a liquid xenon time projection chamber with a fiducial mass of $(1.30\n\\pm 0.01)$ t, resulting in a 1.0 t$\\times$yr exposure. The energy region of\ninterest, [1.4, 10.6] $\\mathrm{keV_{ee}}$ ([4.9, 40.9] $\\mathrm{keV_{nr}}$),\nexhibits an ultra-low electron recoil background rate of $(82\\substack{+5 \\\\\n-3}\\textrm{ (sys)}\\pm3\\textrm{ (stat)})$\nevents/$(\\mathrm{t}\\times\\mathrm{yr}\\times\\mathrm{keV_{ee}})$. No significant\nexcess over background is found and a profile likelihood analysis parameterized\nin spatial and energy dimensions excludes new parameter space for the\nWIMP-nucleon spin-independent elastic scatter cross-section for WIMP masses\nabove 6 GeV/c${}^2$, with a minimum of $4.1\\times10^{-47}$ cm$^2$ at 30\nGeV/c${}^2$ and 90% confidence level. \n\n"}
{"id": "1806.00657", "contents": "Title: Properties of solutions of the \"naive\" functional Schroedinger equation\n  for QCD Abstract: In this paper we consider the simplest functional Schroedinger equation of a\nquantum field theory (in particular QCD) and study its solutions. We observe\nthat the solutions to this equation must possess a number of properties. Its\nTaylor coefficients are multivalued functions with rational and logarithmic\nbranchings and essential singularities of exponential type. These singularities\noccur along a locus defined by polynomial equations. The conditions we find\ndefine a class of functions that generalizes to multiple dimensions meromorphic\nfunctions with finite Nevanlinna type. We note that in perturbation theory\nthese functions have local asymptotics that is given by multidimensional\nconfluent hypergeometric functions in the sense of Gelfand-Kapranov-Zelevinsky. \n\n"}
{"id": "1806.01615", "contents": "Title: merlin - a unified modelling framework for data analysis and methods\n  development in Stata Abstract: merlin can do a lot of things. From simple stuff, like fitting a linear\nregression or a Weibull survival model, to a three-level logistic mixed effects\nmodel, or a multivariate joint model of multiple longitudinal outcomes (of\ndifferent types) and a recurrent event and survival with non-linear\neffects...the list is rather endless. merlin can do things I haven't even\nthought of yet. I'll take a single dataset, and attempt to show you the full\nrange of capabilities of merlin, and discuss some future directions for the\nimplementation in Stata. \n\n"}
{"id": "1806.02304", "contents": "Title: Variable Selection with ABC Bayesian Forests Abstract: Few problems in statistics are as perplexing as variable selection in the\npresence of very many redundant covariates. The variable selection problem is\nmost familiar in parametric environments such as the linear model or additive\nvariants thereof. In this work, we abandon the linear model framework, which\ncan be quite detrimental when the covariates impact the outcome in a non-linear\nway, and turn to tree-based methods for variable selection. Such variable\nscreening is traditionally done by pruning down large trees or by ranking\nvariables based on some importance measure. Despite heavily used in practice,\nthese ad-hoc selection rules are not yet well understood from a theoretical\npoint of view. In this work, we devise a Bayesian tree-based probabilistic\nmethod and show that it is consistent for variable selection when the\nregression surface is a smooth mix of $p>n$ covariates. These results are the\nfirst model selection consistency results for Bayesian forest priors.\nProbabilistic assessment of variable importance is made feasible by a\nspike-and-slab wrapper around sum-of-trees priors. Sampling from posterior\ndistributions over trees is inherently very difficult. As an alternative to\nMCMC, we propose ABC Bayesian Forests, a new ABC sampling method based on\ndata-splitting that achieves higher ABC acceptance rate. We show that the\nmethod is robust and successful at finding variables with high marginal\ninclusion probabilities. Our ABC algorithm provides a new avenue towards\napproximating the median probability model in non-parametric setups where the\nmarginal likelihood is intractable. \n\n"}
{"id": "1806.02791", "contents": "Title: Hadronic and New Physics Contributions to $b \\to s$ Transitions Abstract: Assuming the source of the anomalies observed recently in $b \\to s$ data to\nbe new physics, there is a priori no reason to believe that - in the effective\nfield theory language - only one type of operator is responsible for the\ntensions. We thus perform for the first time a global fit where all the Wilson\ncoefficients which can effectively receive new physics contributions are\nconsidered, allowing for lepton flavour universality breaking effects as well\nas contributions from chirality flipped and scalar and pseudoscalar operators,\nand find the SM pull taking into account all effective parameters. As a result\nof the full fit to all available $b \\to s$ data including all relevant Wilson\ncoefficients, we obtain a total pull of 4.1$\\sigma$ with the SM hypothesis\nassuming 10% error for the power corrections. Moreover, we make a statistical\ncomparison to find whether the most favoured explanation of the anomalies is\nnew physics or underestimated hadronic effects using the most general\nparameterisation which is fully consistent with the analyticity structure of\nthe amplitudes. This Wilks' test will be a very useful tool to analyse the\nforthcoming $B\\to K^* \\mu^+ \\mu^-$ data. Because the significance of the\nobserved tensions in the angular observables in $B \\to K^* \\mu^+\\mu^-$ is\npresently dependent on the theory estimation of the hadronic contributions to\nthese decays, we briefly discuss the various available approaches for taking\ninto account the long-distance hadronic effects and examine how the different\nestimations of these contributions result in distinct significance of the new\nphysics interpretation of the observed anomalies. \n\n"}
{"id": "1806.03325", "contents": "Title: Cornering Colored Coannihilation Abstract: In thermal dark matter models, allowing the dark matter candidate to\ncoannihilate with another particle can considerably loosen the relic density\nconstraints on the dark matter mass. In particular, introducing a single\nstrongly interacting coannihilation partner in a dark matter model can bring\nthe upper bound on the dark sector energy scale from a few TeV up to about 10\nTeV. While these energies are outside the LHC reach, a large part of the\nparameter space for such coannihilating models can be explored by future hadron\ncolliders. In this context, it is essential to determine whether the current\nbounds on dark matter simplified models also hold in non-minimal scenarios. In\nthis paper, we study extended models that include multiple coannihilation\npartners. We show that the relic density bounds on the dark matter mass in\nthese scenarios are stronger than for the minimal models in most of the\nparameter space and that weakening these bounds requires sizable interactions\nbetween the different species of coannihilation partners. Furthermore, we\ndiscuss how these new interactions as well as the additional particles in the\nmodels can lead to stronger collider bounds, notably in jets plus missing\ntransverse energy searches. This study serves as a vital ingredient towards the\ndetermination of the highest possible energy scale for thermal dark matter\nmodels. \n\n"}
{"id": "1806.04119", "contents": "Title: Valid Post-selection Inference in Assumption-lean Linear Regression Abstract: Construction of valid statistical inference for estimators based on\ndata-driven selection has received a lot of attention in the recent times. Berk\net al. (2013) is possibly the first work to provide valid inference for\nGaussian homoscedastic linear regression with fixed covariates under arbitrary\ncovariate/variable selection. The setting is unrealistic and is extended by\nBachoc et al. (2016) by relaxing the distributional assumptions. A major\ndrawback of the aforementioned works is that the construction of valid\nconfidence regions is computationally intensive. In this paper, we first prove\nthat post-selection inference is equivalent to simultaneous inference and then\nconstruct valid post-selection confidence regions which are computationally\nsimple. Our construction is based on deterministic inequalities and apply to\nindependent as well as dependent random variables without the requirement of\ncorrect distributional assumptions. Finally, we compare the volume of our\nconfidence regions with the existing ones and show that under non-stochastic\ncovariates, our regions are much smaller. \n\n"}
{"id": "1806.04971", "contents": "Title: Simultaneous extractions of $|V_{ub}|$ and $|V_{cb}|$ with only the\n  exclusive $\\Lambda_b$ decays Abstract: We perform the simultaneous $|V_{ub}|$ and $|V_{cb}|$ extractions with only\nthe exclusive $\\Lambda_b$ decays of $\\Lambda_b\\to (p,\\Lambda_c^+)\\mu\\bar\n\\nu_\\mu$, $\\Lambda_b\\to p\\pi^-$ and $\\Lambda_b\\to \\Lambda_c^+ (\\pi^-, D^-)$. We\nobtain that $|V_{ub}|=(3.7\\pm 0.3)\\times 10^{-3}$ and $|V_{cb}|=(45.9\\pm\n2.7)\\times 10^{-3}$. Our value of $|V_{ub}|$ is larger than that of $(3.27\\pm\n0.15\\pm 0.16\\pm 0.06)\\times 10^{-3}$, previously extracted by the LHC\nCollaboration from the exclusive $\\Lambda_b$ decays also, but nearly identical\nto $(3.72\\pm 0.19)\\times 10^{-3}$ from the exclusive $B$ decays. On the other\nhand, our extracted result of $|V_{cb}|$ favors the value of $(42.2\\pm\n0.8)\\times 10^{-3}$ from the inclusive $B$ decays. \n\n"}
{"id": "1806.06028", "contents": "Title: A new characterization of the Gamma distribution and associated goodness\n  of fit tests Abstract: We propose a class of weighted $L_2$-type tests of fit to the Gamma\ndistribution. Our novel procedure is based on a fixed point property of a new\ntransformation connected to a Steinian characterization of the family of Gamma\ndistributions. We derive the weak limits of the statistic under the null\nhypothesis and under contiguous alternatives. Further, we establish the global\nconsistency of the tests and apply a parametric bootstrap technique in a Monte\nCarlo simulation study to show the competitiveness to existing procedures. \n\n"}
{"id": "1806.06179", "contents": "Title: Semi-supervised Inference for Explained Variance in High-dimensional\n  Linear Regression and Its Applications Abstract: This paper considers statistical inference for the explained variance\n$\\beta^{\\intercal}\\Sigma \\beta$ under the high-dimensional linear model\n$Y=X\\beta+\\epsilon$ in the semi-supervised setting, where $\\beta$ is the\nregression vector and $\\Sigma$ is the design covariance matrix. A calibrated\nestimator, which efficiently integrates both labelled and unlabelled data, is\nproposed. It is shown that the estimator achieves the minimax optimal rate of\nconvergence in the general semi-supervised framework. The optimality result\ncharacterizes how the unlabelled data contributes to the estimation accuracy.\nMoreover, the limiting distribution for the proposed estimator is established\nand the unlabelled data has also proven useful in reducing the length of the\nconfidence interval for the explained variance. The proposed method is extended\nto the semi-supervised inference for the unweighted quadratic functional,\n$\\|\\beta\\|_2^2$. The obtained inference results are then applied to a range of\nhigh-dimensional statistical problems, including signal detection and global\ntesting, prediction accuracy evaluation, and confidence ball construction. The\nnumerical improvement of incorporating the unlabelled data is demonstrated\nthrough simulation studies and an analysis of estimating heritability for a\nyeast segregant data set with multiple traits. \n\n"}
{"id": "1806.07274", "contents": "Title: Efficient data augmentation for multivariate probit models with panel\n  data: An application to general practitioner decision-making about\n  contraceptives Abstract: This article considers the problem of estimating a multivariate probit model\nin a panel data setting with emphasis on sampling a high-dimensional\ncorrelation matrix and improving the overall efficiency of the data\naugmentation approach. We reparameterise the correlation matrix in a principled\nway and then carry out efficient Bayesian inference using Hamiltonian Monte\nCarlo. We also propose a novel antithetic variable method to generate samples\nfrom the posterior distribution of the random effects and regression\ncoefficients, resulting in significant gains in efficiency. We apply the\nmethodology by analysing stated preference data obtained from Australian\ngeneral practitioners evaluating alternative contraceptive products. Our\nanalysis suggests that the joint probability of discussing combinations of\ncontraceptive products with a patient shows medical practice variation among\nthe general practitioners, which indicates some resistance to even discuss\nthese products, let alone recommend them. \n\n"}
{"id": "1806.08602", "contents": "Title: A framework for the chiral extrapolation of the charmed baryon\n  ground-state masses Abstract: We consider the chiral Lagrangian for charmed baryon fields with $J^P\n=\\frac{1}{2}^+$ or $J^P =\\frac{3}{2}^+$ quantum numbers. A chiral expansion\nframework for the baryon ground state masses is worked out to N$^3$LO as to\ncompute their dependence on the up, down and strange quark masses for finite\nbox QCD lattice simulations. It is formulated in terms of on-shell meson and\nbaryon masses. The convergence of such a scheme is illustrated with physical\nmasses as taken from the PDG. The counter terms relevant at N$^3$LO are\ncorrelated systematically by large-$N_c$ sum rules to leading and subleading\norder in a manner that keeps the renormalization scale invariance of the\napproach. \n\n"}
{"id": "1806.09762", "contents": "Title: Boulevard: Regularized Stochastic Gradient Boosted Trees and Their\n  Limiting Distribution Abstract: This paper examines a novel gradient boosting framework for regression. We\nregularize gradient boosted trees by introducing subsampling and employ a\nmodified shrinkage algorithm so that at every boosting stage the estimate is\ngiven by an average of trees. The resulting algorithm, titled Boulevard, is\nshown to converge as the number of trees grows. We also demonstrate a central\nlimit theorem for this limit, allowing a characterization of uncertainty for\npredictions. A simulation study and real world examples provide support for\nboth the predictive accuracy of the model and its limiting behavior. \n\n"}
{"id": "1806.10163", "contents": "Title: FACT: Fast closed testing for exchangeable local tests Abstract: Multiple hypothesis testing problems arise naturally in science. In this\npaper, we introduce the new Fast Closed Testing (FACT) method for multiple\ntesting, controlling the family-wise error rate. This error rate is state of\nthe art in many important application areas, and is preferred to false\ndiscovery rate control for many reasons, including that it leads to stronger\nreproducibility. The closure principle rejects an individual hypothesis if all\nglobal nulls of subsets containing it are rejected using some test statistics.\nIt takes exponential time in the worst case. When the tests are symmetric and\nmonotone, our method is an exact algorithm for computing the closure, quadratic\nin the number of tests, and linear in the number of discoveries. Our framework\ngeneralizes most examples of closed testing such as Holm's and the Bonferroni\nmethod. As a special case of our method, we propose the Simes-higher criticism\nfusion test, which is powerful for detecting both a few strong signals, and\nalso many moderate signals. \n\n"}
{"id": "1806.10622", "contents": "Title: Joint resummation of two angularities at next-to-next-to-leading\n  logarithmic order Abstract: Multivariate analyses are emerging as important tools to understand\nproperties of hadronic jets which play a key role in the LHC experimental\nprogram. We take a first step towards precise and differential theory\npredictions by calculating the cross section for $e^+ e^- \\to$ 2 jets\ndifferential in the angularities $e_\\alpha$ and $e_\\beta$. The logarithms of\n$e_\\alpha$ and $e_\\beta$ in the cross section are jointly resummed to\nnext-to-next-to-leading logarithmic accuracy, using the SCET+ framework we\ndeveloped, and are matched to the next-to-leading order cross section. We\nperform analytic one-loop calculations that serve as input for our numerical\nanalysis, provide controlled theory uncertainties, and compare our results to\nPythia. We also obtain predictions for the cross section differential in the\nratio $e_\\alpha/e_\\beta$, which cannot be determined from a fixed-order\ncalculation. The effect of nonperturbative corrections is also investigated.\nUsing Event2, we validate the logarithmic structure of the single angularity\ncross section predicted by factorization theorems at ${\\mathcal\nO}(\\alpha_s^2)$, demonstrating that for specific angularities recoil must be\ntaken into account when using the thrust axis, while it can be ignored if these\nare measured with respect to the winner-take-all axis. \n\n"}
{"id": "1806.11219", "contents": "Title: Using Exposure Mappings as Side Information in Experiments with\n  Interference Abstract: Exposure mappings are widely used to model potential outcomes in the presence\nof interference, where each unit's outcome may depend not only on its own\ntreatment, but also on the treatment of other units as well. However, in\npractice these models may be only a crude proxy for social dynamics. In this\nwork, we give estimands and estimators that are robust to the misspecification\nof an exposure model. In the first part, we require the treatment effect to be\nnonnegative (or \"monotone\") in both direct effects and spillovers. In the\nsecond part, we consider a weaker estimand (\"contrasts attributable to\ntreatment\") which makes no restrictions on the interference at all. \n\n"}
{"id": "1807.01346", "contents": "Title: Finite Sample $L_2$ Bounds for Sequential Monte Carlo and Adaptive Path\n  Selection Abstract: We prove a bound on the finite sample error of sequential Monte Carlo (SMC)\non static spaces using the $L_2$ distance between interpolating distributions\nand the mixing times of Markov kernels. This result is unique in that it is the\nfirst finite sample convergence result for SMC that does not require an upper\nbound on the importance weights. Using this bound we show that careful\nselection of the interpolating distributions can lead to substantial\nimprovements in the computational complexity of the algorithm. This result also\njustifies the adaptive selection of SMC distributions using the relative\neffective sample size commonly used in the literature and we establish\nconditions guaranteeing the approximation accuracy of the adaptive SMC\napproach. We then demonstrate empirically that this procedure provides\nnearly-optimal sequences of distributions in an automatic fashion for realistic\nexamples. \n\n"}
{"id": "1807.02161", "contents": "Title: Minimizing Sensitivity to Model Misspecification Abstract: We propose a framework for estimation and inference when the model may be\nmisspecified. We rely on a local asymptotic approach where the degree of\nmisspecification is indexed by the sample size. We construct estimators whose\nmean squared error is minimax in a neighborhood of the reference model, based\non one-step adjustments. In addition, we provide confidence intervals that\ncontain the true parameter under local misspecification. As a tool to interpret\nthe degree of misspecification, we map it to the local power of a specification\ntest of the reference model. Our approach allows for systematic sensitivity\nanalysis when the parameter of interest may be partially or irregularly\nidentified. As illustrations, we study three applications: an empirical\nanalysis of the impact of conditional cash transfers in Mexico where\nmisspecification stems from the presence of stigma effects of the program, a\ncross-sectional binary choice model where the error distribution is\nmisspecified, and a dynamic panel data binary choice model where the number of\ntime periods is small and the distribution of individual effects is\nmisspecified. \n\n"}
{"id": "1807.03419", "contents": "Title: On Causal Discovery with Equal Variance Assumption Abstract: Prior work has shown that causal structure can be uniquely identified from\nobservational data when these follow a structural equation model whose error\nterms have equal variances. We show that this fact is implied by an ordering\namong (conditional) variances. We demonstrate that ordering estimates of these\nvariances yields a simple yet state-of-the-art method for causal structure\nlearning that is readily extendable to high-dimensional problems. \n\n"}
{"id": "1807.04272", "contents": "Title: Towards a Complete Picture of Stationary Covariance Functions on Spheres\n  Cross Time Abstract: With the advent of wide-spread global and continental-scale spatiotemporal\ndatasets, increased attention has been given to covariance functions on spheres\nover time. This paper provides results for stationary covariance functions of\nrandom fields defined over $d$-dimensional spheres cross time. Specifically, we\nprovide a bridge between the characterization in \\cite{berg-porcu} for\ncovariance functions on spheres cross time and Gneiting's lemma\n\\citep{gneiting2002} that deals with planar surfaces.\n  We then prove that there is a valid class of covariance functions similar in\nform to the Gneiting class of space-time covariance functions\n\\citep{gneiting2002} that replaces the squared Euclidean distance with the\ngreat circle distance. Notably, the provided class is shown to be positive\ndefinite on every $d$-dimensional sphere cross time, while the Gneiting class\nis positive definite over $\\R^d \\times \\R$ for fixed $d$ only.\n  In this context, we illustrate the value of our adapted Gneiting class by\ncomparing examples from this class to currently established nonseparable\ncovariance classes using out-of-sample predictive criteria. These comparisons\nare carried out on two climate reanalysis datasets from the National Centers\nfor Environmental Prediction and National Center for Atmospheric Research. For\nthese datasets, we show that examples from our covariance class have better\npredictive performance than competing models. \n\n"}
{"id": "1807.04982", "contents": "Title: Generalized simultaneous component analysis of binary and quantitative\n  data Abstract: In the current era of systems biological research there is a need for the\nintegrative analysis of binary and quantitative genomics data sets measured on\nthe same objects. One standard tool of exploring the underlying dependence\nstructure present in multiple quantitative data sets is simultaneous component\nanalysis (SCA) model. However, it does not have any provisions when a part of\nthe data are binary. To this end, we propose the generalized SCA (GSCA) model,\nwhich takes into account the distinct mathematical properties of binary and\nquantitative measurements in the maximum likelihood framework. Like in the SCA\nmodel, a common low dimensional subspace is assumed to represent the shared\ninformation between these two distinct types of measurements. However, the GSCA\nmodel can easily be overfitted when a rank larger than one is used, leading to\nsome of the estimated parameters to become very large. To achieve a low rank\nsolution and combat overfitting, we propose to use a concave variant of the\nnuclear norm penalty. An efficient majorization algorithm is developed to fit\nthis model with different concave penalties. Realistic simulations (low\nsignal-to-noise ratio and highly imbalanced binary data) are used to evaluate\nthe performance of the proposed model in recovering the underlying structure.\nAlso, a missing value based cross validation procedure is implemented for model\nselection. We illustrate the usefulness of the GSCA model for exploratory data\nanalysis of quantitative gene expression and binary copy number aberration\n(CNA) measurements obtained from the GDSC1000 data sets. \n\n"}
{"id": "1807.05405", "contents": "Title: The conditional permutation test for independence while controlling for\n  confounders Abstract: We propose a general new method, the conditional permutation test, for\ntesting the conditional independence of variables $X$ and $Y$ given a\npotentially high-dimensional random vector $Z$ that may contain confounding\nfactors. The proposed test permutes entries of $X$ non-uniformly, so as to\nrespect the existing dependence between $X$ and $Z$ and thus account for the\npresence of these confounders. Like the conditional randomization test of\nCand\\`es et al. (2018), our test relies on the availability of an approximation\nto the distribution of $X \\mid Z$. While Cand\\`es et al. (2018)'s test uses\nthis estimate to draw new $X$ values, for our test we use this approximation to\ndesign an appropriate non-uniform distribution on permutations of the $X$\nvalues already seen in the true data. We provide an efficient Markov Chain\nMonte Carlo sampler for the implementation of our method, and establish bounds\non the Type I error in terms of the error in the approximation of the\nconditional distribution of $X\\mid Z$, finding that, for the worst case test\nstatistic, the inflation in Type I error of the conditional permutation test is\nno larger than that of the conditional randomization test. We validate these\ntheoretical results with experiments on simulated data and on the Capital\nBikeshare data set. \n\n"}
{"id": "1807.06504", "contents": "Title: Multimessenger Tests of Einstein's Weak Equivalence Principle and\n  Lorentz Invariance with a High-energy Neutrino from a Flaring Blazar Abstract: The detection of the high-energy ($\\sim290$ TeV) neutrino coincident with the\nflaring blazar TXS 0506+056, the first and only $3\\sigma$ neutrino-source\nassociation to date, provides new, multimessenger tests of the weak equivalence\nprinciple (WEP) and Lorentz invariance. Assuming that the flight time\ndifference between the TeV neutrino and gamma-ray photons from the blazar flare\nis mainly caused by the gravitational potential of the Laniakea supercluster of\ngalaxies, we show that the deviation from the WEP for neutrinos and photons is\nconservatively constrained to have an accuracy of $10^{-6}-10^{-7}$, which is\n3--4 orders of magnitude better than previous results placed by MeV neutrinos\nfrom supernova 1987A. In addition, we demonstrate that the association of the\nTeV neutrino with the blazar flare sets limits on the energy scales of quantum\ngravity for both linear and quadratic violations of Lorentz invariance (LIV) to\n$E_{\\rm QG, 1}>3.2\\times10^{15}-3.7\\times10^{16}$ GeV and $E_{\\rm QG,\n2}>4.0\\times10^{10}-1.4\\times10^{11}$ GeV. These improve previous limits on\nboth linear and quadratic LIV energy scales in neutrino propagation by 5--7\norders of magnitude. \n\n"}
{"id": "1807.08409", "contents": "Title: Subsampling MCMC - An introduction for the survey statistician Abstract: The rapid development of computing power and efficient Markov Chain Monte\nCarlo (MCMC) simulation algorithms have revolutionized Bayesian statistics,\nmaking it a highly practical inference method in applied work. However, MCMC\nalgorithms tend to be computationally demanding, and are particularly slow for\nlarge datasets. Data subsampling has recently been suggested as a way to make\nMCMC methods scalable on massively large data, utilizing efficient sampling\nschemes and estimators from the survey sampling literature. These developments\ntend to be unknown by many survey statisticians who traditionally work with\nnon-Bayesian methods, and rarely use MCMC. Our article explains the idea of\ndata subsampling in MCMC by reviewing one strand of work, Subsampling MCMC, a\nso called pseudo-marginal MCMC approach to speeding up MCMC through data\nsubsampling. The review is written for a survey statistician without previous\nknowledge of MCMC methods since our aim is to motivate survey sampling experts\nto contribute to the growing Subsampling MCMC literature. \n\n"}
{"id": "1807.09643", "contents": "Title: LHC Searches for Top-philic Kaluza-Klein Graviton Abstract: We study the phenomenology of a massive graviton $G$ with non-universal\ncouplings to the Standard Model (SM) particles. Such a particle can arise as a\nwarped Kaluza-Klein graviton from a framework of the Randall-Sundrum\nextra-dimension model. In particular, we consider a case in which $G$ is\ntop-philic, i.e., $G$ interacts strongly with the right-handed top quark,\nresulting in the large top-loop contributions to its production via the gluon\nfusion and its decays to the SM gauge bosons. We take into account the\nconstraints from the current 13 TeV LHC data on the channels of $t\\bar{t}$,\n$\\gamma\\gamma$, $jj (gg)$, $\\gamma Z$, and $ZZ$. Consequently, it is found that\nthe strongest limit for this spin-2 resonance $G$ comes from the $t\\bar{t}$\npair search, which constrains the cutoff scale to be of ${\\cal O}$(100 GeV) for\nthe right-top coupling of ${\\cal O}(1)$ and the massive graviton mass in the\nrange $m_G$=2-5 TeV, significantly relaxed compared with the universal $G$\ncoupling case. \n\n"}
{"id": "1807.10797", "contents": "Title: Estimating a change point in a sequence of very high-dimensional\n  covariance matrices Abstract: This paper considers the problem of estimating a change point in the\ncovariance matrix in a sequence of high-dimensional vectors, where the\ndimension is substantially larger than the sample size. A two-stage approach is\nproposed to efficiently estimate the location of the change point. The first\nstep consists of a reduction of the dimension to identify elements of the\ncovariance matrices corresponding to significant changes. In a second step we\nuse the components after dimension reduction to determine the position of the\nchange point. Theoretical properties are developed for both steps and numerical\nstudies are conducted to support the new methodology. \n\n"}
{"id": "1807.11530", "contents": "Title: Search for a Dark Photon in Electro-Produced $e^{+}e^{-}$ Pairs with the\n  Heavy Photon Search Experiment at JLab Abstract: The Heavy Photon Search experiment took its first data in a 2015 engineering\nrun at the Thomas Jefferson National Accelerator Facility, searching for a\nprompt, electro-produced dark photon with a mass between 19 and 81 MeV/$c^2$. A\nsearch for a resonance in the $e^{+}e^{-}$ invariant mass distribution, using\n1.7 days (1170 nb$^{-1}$) of data, showed no evidence of dark photon decays\nabove the large QED background, confirming earlier searches and demonstrating\nthe full functionality of the experiment. Upper limits on the square of the\ncoupling of the dark photon to the Standard Model photon are set at the level\nof 6$\\times$10$^{-6}$. Future runs with higher luminosity will explore new\nterritory. \n\n"}
{"id": "1807.11917", "contents": "Title: Direct $CP$ violation from isospin symmetry breaking effects in PQCD Abstract: We investigate the direct $CP$ violation for the decay process of\n$\\bar{B}_{s}\\rightarrow P(V)\\pi^{0}$ (P,V refer to the pseudoscalar meson and\nvector meson, respectively) via isospin symmetry breaking effects from the\n$\\pi^{0}-\\eta-\\eta'$ mixing mechanism in PQCD factorization approach. Isospin\nsymmetry breaking arises from the electroweak interaction and the u-d quark\nmass difference by the strong interaction which are known to be tiny. However,\nwe find that isospin symmetry breaking at the leading order shifts the $CP$\nviolation due to the new strong phases. \n\n"}
{"id": "1808.01749", "contents": "Title: Regularized matrix data clustering and its application to image analysis Abstract: In this paper, we propose a regularized mixture probabilistic model to\ncluster matrix data and apply it to brain signals. The approach is able to\ncapture the sparsity (low rank, small/zero values) of the original signals by\nintroducing regularization terms into the likelihood function. Through a\nmodified EM algorithm, our method achieves the optimal solution with low\ncomputational cost. Theoretical results are also provided to establish the\nconsistency of the proposed estimators. Simulations show the advantages of the\nproposed method over other existing methods. We also apply the approach to two\nreal datasets from different experiments. Promising results imply that the\nproposed method successfully characterizes signals with different patterns\nwhile yielding insightful scientific interpretation. \n\n"}
{"id": "1808.02430", "contents": "Title: Granger Causality Analysis Based on Quantized Minimum Error Entropy\n  Criterion Abstract: Linear regression model (LRM) based on mean square error (MSE) criterion is\nwidely used in Granger causality analysis (GCA), which is the most commonly\nused method to detect the causality between a pair of time series. However,\nwhen signals are seriously contaminated by non-Gaussian noises, the LRM\ncoefficients will be inaccurately identified. This may cause the GCA to detect\na wrong causal relationship. Minimum error entropy (MEE) criterion can be used\nto replace the MSE criterion to deal with the non-Gaussian noises. But its\ncalculation requires a double summation operation, which brings computational\nbottlenecks to GCA especially when sizes of the signals are large. To address\nthe aforementioned problems, in this study we propose a new method called GCA\nbased on the quantized MEE (QMEE) criterion (GCA-QMEE), in which the QMEE\ncriterion is applied to identify the LRM coefficients and the quantized error\nentropy is used to calculate the causality indexes. Compared with the\ntraditional GCA, the proposed GCA-QMEE not only makes the results more\ndiscriminative, but also more robust. Its computational complexity is also not\nhigh because of the quantization operation. Illustrative examples on synthetic\nand EEG datasets are provided to verify the desirable performance and the\navailability of the GCA-QMEE. \n\n"}
{"id": "1808.03692", "contents": "Title: Estimation of natural indirect effects robust to unmeasured confounding\n  and mediator measurement error Abstract: The use of causal mediation analysis to evaluate the pathways by which an\nexposure affects an outcome is widespread in the social and biomedical\nsciences. Recent advances in this area have established formal conditions for\nidentification and estimation of natural direct and indirect effects. However,\nthese conditions typically involve stringent no unmeasured confounding\nassumptions and that the mediator has been measured without error. These\nassumptions may fail to hold in practice where mediation methods are often\napplied. The goal of this paper is two-fold. First, we show that the natural\nindirect effect can in fact be identified in the presence of unmeasured\nexposure-outcome confounding provided there is no additive interaction between\nthe mediator and unmeasured confounder(s). Second, we introduce a new estimator\nof the natural indirect effect that is robust to both classical measurement\nerror of the mediator and unmeasured confounding of both exposure-outcome and\nmediator-outcome relations under certain no interaction assumptions. We provide\nformal proofs and a simulation study to demonstrate our results. \n\n"}
{"id": "1808.04401", "contents": "Title: Horseshoe-based Bayesian nonparametric estimation of effective\n  population size trajectories Abstract: Phylodynamics is an area of population genetics that uses genetic sequence\ndata to estimate past population dynamics. Modern state-of-the-art Bayesian\nnonparametric methods for recovering population size trajectories of unknown\nform use either change-point models or Gaussian process priors. Change-point\nmodels suffer from computational issues when the number of change-points is\nunknown and needs to be estimated. Gaussian process-based methods lack local\nadaptivity and cannot accurately recover trajectories that exhibit features\nsuch as abrupt changes in trend or varying levels of smoothness. We propose a\nnovel, locally-adaptive approach to Bayesian nonparametric phylodynamic\ninference that has the flexibility to accommodate a large class of functional\nbehaviors. Local adaptivity results from modeling the log-transformed effective\npopulation size a priori as a horseshoe Markov random field, a recently\nproposed statistical model that blends together the best properties of the\nchange-point and Gaussian process modeling paradigms. We use simulated data to\nassess model performance, and find that our proposed method results in reduced\nbias and increased precision when compared to contemporary methods. We also use\nour models to reconstruct past changes in genetic diversity of human hepatitis\nC virus in Egypt and to estimate population size changes of ancient and modern\nsteppe bison. These analyses show that our new method captures features of the\npopulation size trajectories that were missed by the state-of-the-art methods. \n\n"}
{"id": "1808.06689", "contents": "Title: Bayesian Function-on-Scalars Regression for High Dimensional Data Abstract: We develop a fully Bayesian framework for function-on-scalars regression with\nmany predictors. The functional data response is modeled nonparametrically\nusing unknown basis functions, which produces a flexible and data-adaptive\nfunctional basis. We incorporate shrinkage priors that effectively remove\nunimportant scalar covariates from the model and reduce sensitivity to the\nnumber of (unknown) basis functions. For variable selection in functional\nregression, we propose a decision theoretic posterior summarization technique,\nwhich identifies a subset of covariates that retains nearly the predictive\naccuracy of the full model. Our approach is broadly applicable for Bayesian\nfunctional regression models, and unlike existing methods provides joint rather\nthan marginal selection of important predictor variables. Computationally\nscalable posterior inference is achieved using a Gibbs sampler with linear time\ncomplexity in the number of predictors. The resulting algorithm is empirically\nfaster than existing frequentist and Bayesian techniques, and provides joint\nestimation of model parameters, prediction and imputation of functional\ntrajectories, and uncertainty quantification via the posterior distribution. A\nsimulation study demonstrates improvements in estimation accuracy, uncertainty\nquantification, and variable selection relative to existing alternatives. The\nmethodology is applied to actigraphy data to investigate the association\nbetween intraday physical activity and responses to a sleep questionnaire. \n\n"}
{"id": "1808.07433", "contents": "Title: Bayesian Estimation of Sparse Spiked Covariance Matrices in High\n  Dimensions Abstract: We propose a Bayesian methodology for estimating spiked covariance matrices\nwith jointly sparse structure in high dimensions. The spiked covariance matrix\nis reparametrized in terms of the latent factor model, where the loading matrix\nis equipped with a novel matrix spike-and-slab LASSO prior, which is a\ncontinuous shrinkage prior for modeling jointly sparse matrices. We establish\nthe rate-optimal posterior contraction for the covariance matrix with respect\nto the operator norm as well as that for the principal subspace with respect to\nthe projection operator norm loss. We also study the posterior contraction rate\nof the principal subspace with respect to the two-to-infinity norm loss, a\nnovel loss function measuring the distance between subspaces that is able to\ncapture element-wise eigenvector perturbations. We show that the posterior\ncontraction rate with respect to the two-to-infinity norm loss is tighter than\nthat with respect to the routinely used projection operator norm loss under\ncertain low-rank and bounded coherence conditions. In addition, a point\nestimator for the principal subspace is proposed with the rate-optimal risk\nbound with respect to the projection operator norm loss. These results are\nbased on a collection of concentration and large deviation inequalities for the\nmatrix spike-and-slab LASSO prior. The numerical performance of the proposed\nmethodology is assessed through synthetic examples and the analysis of a\nreal-world face data example. \n\n"}
{"id": "1808.07704", "contents": "Title: Data-adaptive trimming of the Hill estimator and detection of outliers\n  in the extremes of heavy-tailed data Abstract: We introduce a trimmed version of the Hill estimator for the index of a\nheavy-tailed distribution, which is robust to perturbations in the extreme\norder statistics. In the ideal Pareto setting, the estimator is essentially\nfinite-sample efficient among all unbiased estimators with a given strict upper\nbreak-down point. For general heavy-tailed models, we establish the asymptotic\nnormality of the estimator under second order regular variation conditions and\nalso show it is minimax rate-optimal in the Hall class of distributions. We\nalso develop an automatic, data-driven method for the choice of the trimming\nparameter which yields a new type of robust estimator that can adapt to the\nunknown level of contamination in the extremes. This adaptive robustness\nproperty makes our estimator particularly appealing and superior to other\nrobust estimators in the setting where the extremes of the data are\ncontaminated. As an important application of the data-driven selection of the\ntrimming parameters, we obtain a methodology for the principled identification\nof extreme outliers in heavy tailed data. Indeed, the method has been shown to\ncorrectly identify the number of outliers in the previously explored Condroz\ndata set. \n\n"}
{"id": "1808.08956", "contents": "Title: Precision Photon Spectra for Wino Annihilation Abstract: We provide precise predictions for the hard photon spectrum resulting from\nneutral SU$(2)_W$ triplet (wino) dark matter annihilation. Our calculation is\nperformed utilizing an effective field theory expansion around the endpoint\nregion where the photon energy is near the wino mass. This has direct relevance\nto line searches at indirect detection experiments. We compute the spectrum at\nnext-to-leading logarithmic (NLL) accuracy within the framework established by\na factorization formula derived previously by our collaboration. This allows\nsimultaneous resummation of large Sudakov logarithms (arising from a restricted\nfinal state) and Sommerfeld effects. Resummation at NLL accuracy shows good\nconvergence of the perturbative series due to the smallness of the electroweak\ncoupling constant - scale variation yields uncertainties on our NLL prediction\nat the level of $5\\%$. We highlight a number of interesting field theory\neffects that appear at NLL associated with the presence of electroweak symmetry\nbreaking, which should have more general applicability. We also study the\nimportance of using the full spectrum as compared with a single endpoint bin\napproximation when computing experimental limits. Our calculation provides a\nstate of the art prediction for the hard photon spectrum that can be easily\ngeneralized to other DM candidates, allowing for the robust interpretation of\ndata collected by current and future indirect detection experiments. \n\n"}
{"id": "1808.10506", "contents": "Title: Maximum Entropy Principle Analysis in Network Systems with Short-time\n  Recordings Abstract: In many realistic systems, maximum entropy principle (MEP) analysis provides\nan effective characterization of the probability distribution of network\nstates. However, to implement the MEP analysis, a sufficiently long-time data\nrecording in general is often required, e.g., hours of spiking recordings of\nneurons in neuronal networks. The issue of whether the MEP analysis can be\nsuccessfully applied to network systems with data from short recordings has yet\nto be fully addressed. In this work, we investigate relationships underlying\nthe probability distributions, moments, and effective interactions in the MEP\nanalysis and then show that, with short recordings of network dynamics, the MEP\nanalysis can be applied to reconstructing probability distributions of network\nstates under the condition of asynchronous activity of nodes in the network.\nUsing spike trains obtained from both Hodgkin-Huxley neuronal networks and\nelectrophysiological experiments, we verify our results and demonstrate that\nMEP analysis provides a tool to investigate the neuronal population coding\nproperties, even for short recordings. \n\n"}
{"id": "1809.03759", "contents": "Title: On the aberrations of mixed level Orthogonal Arrays with removed runs Abstract: Given an Orthogonal Array we analyze the aberrations of the sub-fractions\nwhich are obtained by the deletion of some of its points. We provide formulae\nto compute the Generalized Word-Length Pattern of any sub-fraction. In the case\nof the deletion of one single point, we provide a simple methodology to find\nwhich the best sub-fractions are according to the Generalized Minimum\nAberration criterion. We also study the effect of the deletion of 1, 2 or 3\npoints on some examples. The methodology does not put any restriction on the\nnumber of levels of each factor. It follows that any mixed level Orthogonal\nArray can be considered. \n\n"}
{"id": "1809.03974", "contents": "Title: $SO(10) \\to SU(5) \\times U(1)_\\chi$ as the Origin of Dark Matter Abstract: In the decomposition of $SO(10)$ grand unification to $SU(5) \\times\nU(1)_\\chi$, two desirable features are obtained with the addition of one\ncolored fermion octet $\\Omega$, one electroweak fermion triplet $\\Sigma$ and\none complex scalar triplet $S$ to the particle content of the standard model\nwith two Higgs doublets. They are (1) gauge coupling unification of $SU(3)_C\n\\times SU(2)_L \\times U(1)_Y$ to $SU(5)$, and (2) the automatic (predestined)\nemergence of dark matter, i.e. $\\Omega$, $\\Sigma$ and $S$, with dark parity\ngiven by $(-1)^{Q_\\chi + 2j}$. It suggests that $U(1)_\\chi$ may well be the\nunderlying symmetry of the dark sector. \n\n"}
{"id": "1809.04541", "contents": "Title: Lugsail lag windows for estimating time-average covariance matrices Abstract: Lag windows are commonly used in time series, econometrics, steady-state\nsimulation, and Markov chain Monte Carlo to estimate time-average covariance\nmatrices. In the presence of positive correlation of the underlying process,\nestimators of this matrix almost always exhibit significant negative bias,\nleading to undesirable finite-sample properties. We propose a new family of lag\nwindows specifically designed to improve finite-sample performance by\noffsetting this negative bias. Any existing lag window can be adapted into a\nlugsail equivalent with no additional assumptions. We use these lag windows\nwithin spectral variance estimators and demonstrate its advantages in a linear\nregression model with autocorrelated and heteroskedastic residuals. We further\nemploy the lugsail lag windows in weighted batch means estimators due to their\ncomputational efficiency on large simulation output. We obtain bias and\nvariance results for these multivariate estimators and significantly weaken the\nmixing condition on the process. Superior finite-sample properties are\nillustrated in a vector autoregressive process and a Bayesian logistic\nregression model. \n\n"}
{"id": "1809.06092", "contents": "Title: Testing relevant hypotheses in functional time series via\n  self-normalization Abstract: In this paper we develop methodology for testing relevant hypotheses about\nfunctional time series in a tuning-free way. Instead of testing for exact\nequality, for example for the equality of two mean functions from two\nindependent time series, we propose to test the null hypothesis of no relevant\ndeviation. In the two sample problem this means that an $L^2$-distance between\nthe two mean functions is smaller than a pre-specified threshold. For such\nhypotheses self-normalization, which was introduced by Shao (2010) and Shao and\nZhang (2010) and is commonly used to avoid the estimation of nuisance\nparameters, is not directly applicable. We develop new self-normalized\nprocedures for testing relevant hypotheses in the one sample, two sample and\nchange point problem and investigate their asymptotic properties. Finite sample\nproperties of the proposed tests are illustrated by means of a simulation study\nand data examples. Our main focus is on functional time series, but extensions\nto other settings are also briefly discussed. \n\n"}
{"id": "1809.06143", "contents": "Title: Contribution to the discussion of \"When should meta-analysis avoid\n  making hidden normality assumptions?\": A Bayesian perspective Abstract: Contribution to the discussion of \"When should meta-analysis avoid making\nhidden normality assumptions?\" by Dan Jackson and Ian R. White (2018;\nhttps://doi.org/10.1002/bimj.201800071). \n\n"}
{"id": "1809.06797", "contents": "Title: Revisiting RGEs for general gauge theories Abstract: We revisit the renormalisation group equations (RGE) for general\nrenormalisable gauge theories at one- and two-loop accuracy. We identify and\ncorrect various mistakes in the literature for the $\\beta$-functions of the\ndimensionful Lagrangian parameters (the fermion mass, the bilinear and\ntrilinear scalar couplings) as well as the dimensionless quartic scalar\ncouplings. There are two sources for these discrepancies. Firstly, the known\nexpressions for the scalar couplings assume a diagonal wave-function\nrenormalisation which is not appropriate for models with mixing in the scalar\nsector. Secondly, the dimensionful parameters have been derived in the\nliterature using a dummy field method which we critically re-examine, obtaining\nrevised expressions for the $\\beta$-function of the fermion mass. We perform an\nindependent cross-check using well-tested supersymmetric RGEs which confirms\nour results. The numerical impact of the changes in the $\\beta$-function for\nthe fermion mass terms is illustrated using a toy model with a heavy\nvector-like fermion pair coupled to a scalar gauge singlet. Unsurprisingly, the\ncorrection to the running of the fermion mass becomes sizeable for large Yukawa\ncouplings of the order of O(1). Furthermore, we demonstrate the importance of\nthe correction to the $\\beta$-functions of the scalar quartic couplings using a\ngeneral type-III Two-Higgs-Doublet-Model. All the corrected expressions have\nbeen implemented in updated versions of the Mathematica package SARAH and the\nPython package PyR@TE. \n\n"}
{"id": "1809.08771", "contents": "Title: Modeling longitudinal data using matrix completion Abstract: In clinical practice and biomedical research, measurements are often\ncollected sparsely and irregularly in time while the data acquisition is\nexpensive and inconvenient. Examples include measurements of spine bone mineral\ndensity, cancer growth through mammography or biopsy, a progression of\ndefective vision, or assessment of gait in patients with neurological\ndisorders. Since the data collection is often costly and inconvenient,\nestimation of progression from sparse observations is of great interest for\npractitioners.\n  From the statistical standpoint, such data is often analyzed in the context\nof a mixed-effect model where time is treated as both a fixed-effect\n(population progression curve) and a random-effect (individual variability).\nAlternatively, researchers analyze Gaussian processes or functional data where\nobservations are assumed to be drawn from a certain distribution of processes.\nThese models are flexible but rely on probabilistic assumptions, require very\ncareful implementation, specific to the given problem, and tend to be slow in\npractice.\n  In this study, we propose an alternative elementary framework for analyzing\nlongitudinal data, relying on matrix completion. Our method yields estimates of\nprogression curves by iterative application of the Singular Value\nDecomposition. Our framework covers multivariate longitudinal data, regression,\nand can be easily extended to other settings. As it relies on existing tools\nfor matrix algebra it is efficient and easy to implement.\n  We apply our methods to understand trends of progression of motor impairment\nin children with Cerebral Palsy. Our model approximates individual progression\ncurves and explains 30% of the variability. Low-rank representation of\nprogression trends enables identification of different progression trends in\nsubtypes of Cerebral Palsy. \n\n"}
{"id": "1809.09114", "contents": "Title: The phenomenology of electric dipole moments in models of scalar\n  leptoquarks Abstract: We study the phenomenology of electric dipole moments (EDMs) induced in\nvarious scalar leptoquark models. We consider generic leptoquark couplings to\nquarks and leptons and match to Standard Model effective field theory. After\nevolving the resulting operators to low energies, we connect to EDM experiments\nby using up-to-date hadronic, nuclear, and atomic matrix elements. We show that\ncurrent experimental limits set strong constraints on the possible CP-violating\nphases in leptoquark models. Depending on the quarks and leptons involved in\nthe interaction, the existing searches for EDMs of leptons, nucleons, atoms,\nand molecules all play a role in constraining the CP-violating couplings. We\ndiscuss the impact of hadronic and nuclear uncertainties as well as the\nsensitivities that can be achieved with future EDM experiments. Finally, we\nstudy the impact of EDM constraints on a specific leptoquark model that can\nexplain the recent $B$-physics anomalies. \n\n"}
{"id": "1810.00274", "contents": "Title: Bayesian network marker selection via the thresholded graph Laplacian\n  Gaussian prior Abstract: Selecting informative nodes over large-scale networks becomes increasingly\nimportant in many research areas. Most existing methods focus on the local\nnetwork structure and incur heavy computational costs for the large-scale\nproblem. In this work, we propose a novel prior model for Bayesian network\nmarker selection in the generalized linear model (GLM) framework: the\nThresholded Graph Laplacian Gaussian (TGLG) prior, which adopts the graph\nLaplacian matrix to characterize the conditional dependence between neighboring\nmarkers accounting for the global network structure. Under mild conditions, we\nshow the proposed model enjoys the posterior consistency with a diverging\nnumber of edges and nodes in the network. We also develop a Metropolis-adjusted\nLangevin algorithm (MALA) for efficient posterior computation, which is\nscalable to large-scale networks. We illustrate the superiorities of the\nproposed method compared with existing alternatives via extensive simulation\nstudies and an analysis of the breast cancer gene expression dataset in the\nCancer Genome Atlas (TCGA). \n\n"}
{"id": "1810.00739", "contents": "Title: Bayesian inference in high-dimensional linear models using an empirical\n  correlation-adaptive prior Abstract: In the context of a high-dimensional linear regression model, we propose the\nuse of an empirical correlation-adaptive prior that makes use of information in\nthe observed predictor variable matrix to adaptively address high collinearity,\ndetermining if parameters associated with correlated predictors should be\nshrunk together or kept apart. Under suitable conditions, we prove that this\nempirical Bayes posterior concentrates around the true sparse parameter at the\noptimal rate asymptotically. A simplified version of a shotgun stochastic\nsearch algorithm is employed to implement the variable selection procedure, and\nwe show, via simulation experiments across different settings and a real-data\napplication, the favorable performance of the proposed method compared to\nexisting methods. \n\n"}
{"id": "1810.01370", "contents": "Title: Covariate Distribution Balance via Propensity Scores Abstract: This paper proposes new estimators for the propensity score that aim to\nmaximize the covariate distribution balance among different treatment groups.\nHeuristically, our proposed procedure attempts to estimate a propensity score\nmodel by making the underlying covariate distribution of different treatment\ngroups as close to each other as possible. Our estimators are data-driven, do\nnot rely on tuning parameters such as bandwidths, admit an asymptotic linear\nrepresentation, and can be used to estimate different treatment effect\nparameters under different identifying assumptions, including unconfoundedness\nand local treatment effects. We derive the asymptotic properties of inverse\nprobability weighted estimators for the average, distributional, and quantile\ntreatment effects based on the proposed propensity score estimator and\nillustrate their finite sample performance via Monte Carlo simulations and two\nempirical applications. \n\n"}
{"id": "1810.03260", "contents": "Title: Visually Communicating and Teaching Intuition for Influence Functions Abstract: Estimators based on influence functions (IFs) have been shown to be effective\nin many settings, especially when combined with machine learning techniques. By\nfocusing on estimating a specific target of interest (e.g., the average effect\nof a treatment), rather than on estimating the full underlying data generating\ndistribution, IF-based estimators are often able to achieve asymptotically\noptimal mean-squared error. Still, many researchers find IF-based estimators to\nbe opaque or overly technical, which makes their use less prevalent and their\nbenefits less available. To help foster understanding and trust in IF-based\nestimators, we present tangible, visual illustrations of when and how IF-based\nestimators can outperform standard ``plug-in'' estimators. The figures we show\nare based on connections between IFs, gradients, linear approximations, and\nNewton-Raphson. \n\n"}
{"id": "1810.03296", "contents": "Title: Event History Analysis of Dynamic Communication Networks Abstract: Statistical analysis on networks has received growing attention due to demand\nfrom various emerging applications. In dynamic networks, one of the key\ninterests is to model the event history of time-stamped interactions amongst\nnodes. We propose to model dynamic directed communication networks via\nmultivariate counting processes. A pseudo partial likelihood approach is\nexploited to capture the network dependence structure. Asymptotic results of\nthe resulting estimation are established. Numerical results are performed to\ndemonstrate effectiveness of our proposal. \n\n"}
{"id": "1810.04806", "contents": "Title: Kaplan-Meier V- and U-statistics Abstract: In this paper, we study Kaplan-Meier V- and U-statistics respectively defined\nas $\\theta(\\widehat{F}_n)=\\sum_{i,j}K(X_{[i:n]},X_{[j:n]})W_iW_j$ and\n$\\theta_U(\\widehat{F}_n)=\\sum_{i\\neq j}K(X_{[i:n]},X_{[j:n]})W_iW_j/\\sum_{i\\neq\nj}W_iW_j$, where $\\widehat{F}_n$ is the Kaplan-Meier estimator,\n$\\{W_1,\\ldots,W_n\\}$ are the Kaplan-Meier weights and $K:(0,\\infty)^2\\to\\mathbb\nR$ is a symmetric kernel. As in the canonical setting of uncensored data, we\ndifferentiate between two asymptotic behaviours for $\\theta(\\widehat{F}_n)$ and\n$\\theta_U(\\widehat{F}_n)$. Additionally, we derive an asymptotic canonical\nV-statistic representation of the Kaplan-Meier V- and U-statistics. By using\nthis representation we study properties of the asymptotic distribution.\nApplications to hypothesis testing are given. \n\n"}
{"id": "1810.04842", "contents": "Title: On formulations of skew factor models: skew errors versus skew factors Abstract: In the past few years, there have been a number of proposals for generalizing\nthe factor analysis (FA) model and its mixture version (known as mixtures of\nfactor analyzers (MFA)) using non-normal and asymmetric distributions. These\nmodels adopt various types of skew densities for either the factors or the\nerrors. While the relationships between various choices of skew distributions\nhave been discussed in the literature, the differences between placing the\nassumption of skewness on the factors or on the errors have not been closely\nstudied. This paper examines these formulations and discusses the connections\nbetween these two types of formulations for skew factor models. In doing so, we\nintroduce a further formulation that unifies these two formulations; that is,\nplacing a skew distribution on both the factors and the errors. \n\n"}
{"id": "1810.06506", "contents": "Title: $U(1)_\\chi$, Seesaw Dark Matter, and Higgs Decay Abstract: It has recently been pointed out that the underlying symmetry of dark matter\nmay well be $U(1)_\\chi$ (coming from $SO(10) \\to SU(5) \\times U(1)_\\chi$) with\nthe dark parity of any given particle determined by $(-1)^{Q_\\chi+2j}$, where\n$Q_\\chi$ is its $U(1)_\\chi$ charge and $j$ its spin angular momentum. Armed\nwith this new insight, previous simple models of dark matter are reinterpreted,\nand a novel idea is proposed that light seesaw dark matter exists in analogy to\nlight neutrinos and is produced by the rare decay of the standard-model Higgs\nboson. \n\n"}
{"id": "1810.07349", "contents": "Title: Probing Electroweakly Interacting Massive Particles with Drell-Yan\n  Process at 100 TeV Hadron Colliders Abstract: There are many models beyond the standard model which include electroweakly\ninteracting massive particles (EWIMPs), often in the context of the dark\nmatter. In this paper, we study the indirect search of EWIMPs using a precise\nmeasurement of the Drell-Yan cross sections at future $100\\,{\\rm TeV}$ hadron\ncolliders. It is revealed that this search strategy is suitable in particular\nfor Higgsino and that the Higgsino mass up to about $1.3\\,{\\rm TeV}$ will be\ncovered at $95\\,\\%$ C.L. irrespective of the chargino and neutralino mass\ndifference. We also show that the study of the Drell-Yan process provides\nimportant and independent information about every kind of EWIMP in addition to\nHiggsino. \n\n"}
{"id": "1810.08140", "contents": "Title: Impact of model misspecification in shared frailty survival models Abstract: Survival models incorporating random effects to account for unmeasured\nheterogeneity are being increasingly used in biostatistical and applied\nresearch. Specifically, unmeasured covariates whose lack of inclusion in the\nmodel would lead to biased, inefficient results are commonly modelled by\nincluding a subject-specific (or cluster-specific) frailty term that follows a\ngiven distribution (e.g. Gamma or log-Normal). Despite that, in the context of\nparametric frailty models little is known about the impact of misspecifying the\nbaseline hazard, the frailty distribution, or both. Therefore, our aim is to\nquantify the impact of such misspecification in a wide variety of clinically\nplausible scenarios via Monte Carlo simulation, using open source software\nreadily available to applied researchers. We generate clustered survival data\nassuming various baseline hazard functions, including mixture distributions\nwith turning points, and assess the impact of sample size, variance of the\nfrailty, baseline hazard function, and frailty distribution. Models compared\ninclude standard parametric distributions and more flexible spline-based\napproaches; we also included semiparametric Cox models. The resulting bias can\nbe clinically relevant. In conclusion, we highlight the importance of fitting\nmodels that are flexible enough and the importance of assessing model fit. We\nillustrate our conclusions with two applications using data on diabetic\nretinopathy and bladder cancer. Our results show the importance of assessing\nmodel fit with respect to the baseline hazard function and the distribution of\nthe frailty: misspecifying the former leads to biased relative and absolute\nrisk estimates while misspecifying the latter affects absolute risk estimates\nand measures of heterogeneity. \n\n"}
{"id": "1810.08259", "contents": "Title: A systematic investigation of classical causal inference strategies\n  under mis-specification due to network interference Abstract: We systematically investigate issues due to mis-specification that arise in\nestimating causal effects when (treatment) interference is informed by a\nnetwork available pre-intervention, i.e., in situations where the outcome of a\nunit may depend on the treatment assigned to other units. We develop theory for\nseveral forms of interference through the concept of exposure neighborhood, and\ndevelop the corresponding semi-parametric representation for potential outcomes\nas a function of the exposure neighborhood. Using this representation, we\nextend the definition of two popular classes of causal estimands, marginal and\naverage causal effects, to the case of network interference. We characterize\nthe bias and variance one incurs when combining classical randomization\nstrategies (namely, Bernoulli, Completely Randomized, and Cluster Randomized\ndesigns) and estimators (namely, difference-in-means and Horvitz-Thompson) used\nto estimate average treatment effect and on the total treatment effect, under\nmisspecification due to interference. We illustrate how difference-in-means\nestimators can have arbitrarily large bias when estimating average causal\neffects, depending on the form and strength of interference, which is unknown\nat design stage. Horvitz-Thompson (HT) estimators are unbiased when the correct\nweights are specified. Here, we derive the HT weights for unbiased estimation\nof different estimands, and illustrate how they depend on the design, the form\nof interference, which is unknown at design stage, and the estimand. More\nimportantly, we show that HT estimators are in-admissible for a large class of\nrandomization strategies, in the presence of interference. We develop new\nmodel-assisted and model-dependent strategies to improve HT estimators, and we\ndevelop new randomization strategies for estimating the average treatment\neffect and total treatment effect. \n\n"}
{"id": "1810.08292", "contents": "Title: A similarity measure for second order properties of non-stationary\n  functional time series with applications to clustering and testing Abstract: Due to the surge of data storage techniques, the need for the development of\nappropriate techniques to identify patterns and to extract knowledge from the\nresulting enormous data sets, which can be viewed as collections of dependent\nfunctional data, is of increasing interest in many scientific areas. We develop\na similarity measure for spectral density operators of a collection of\nfunctional time series, which is based on the aggregation of Hilbert-Schmidt\ndifferences of the individual time-varying spectral density operators. Under\nfairly general conditions, the asymptotic properties of the corresponding\nestimator are derived and asymptotic normality is established. The introduced\nstatistic lends itself naturally to quantify (dis)-similarity between\nfunctional time series, which we subsequently exploit in order to build a\nspectral clustering algorithm. Our algorithm is the first of its kind in the\nanalysis of non-stationary (functional) time series and enables to discover\nparticular patterns by grouping together `similar' series into clusters,\nthereby reducing the complexity of the analysis considerably. The algorithm is\nsimple to implement and computationally feasible. As a further application we\nprovide a simple test for the hypothesis that the second order properties of\ntwo non-stationary functional time series coincide. \n\n"}
{"id": "1810.08361", "contents": "Title: AdaPtive Noisy Data Augmentation (PANDA) for Simultaneous Construction\n  of Multiple Graph Models Abstract: We extend the data augmentation technique PANDA by Li et al. (2018) that\nregularizes single graph estimation to jointly learning multiple graphical\nmodels with various node types in a unified framework. We design two types of\nnoise to augment the observed data: the first type regularizes the estimation\nof each graph while the second type promotes either the structural similarity,\nreferred as the \\joint group lasso regularization, or the numerical similarity,\nreferred as the joint fused ridge regularization, among the edges in the same\nposition across graphs. The computation in PANDA is straightforward and only\ninvolves obtaining maximum likelihood estimator in generalized linear models in\nan iterative manner. The simulation studies demonstrate PANDA is non-inferior\nto existing joint estimation approaches for Gaussian graphical models, and\nsignificantly improves over the naive differencing approach for non-Gaussian\ngraphical models. We apply PANDA to a real-life lung cancer microarray data to\nsimultaneously construct four protein networks. \n\n"}
{"id": "1811.00645", "contents": "Title: The Holdout Randomization Test for Feature Selection in Black Box Models Abstract: We propose the holdout randomization test (HRT), an approach to feature\nselection using black box predictive models. The HRT is a specialized version\nof the conditional randomization test (CRT; Candes et al., 2018) that uses data\nsplitting for feasible computation. The HRT works with any predictive model and\nproduces a valid $p$-value for each feature. To make the HRT more practical, we\npropose a set of extensions to maximize power and speed up computation. In\nsimulations, these extensions lead to greater power than a competing\nknockoffs-based approach, without sacrificing control of the error rate. We\napply the HRT to two case studies from the scientific literature where\nheuristics were originally used to select important features for predictive\nmodels. The results illustrate how such heuristics can be misleading relative\nto principled methods like the HRT. Code is available at\nhttps://github.com/tansey/hrt. \n\n"}
{"id": "1811.00809", "contents": "Title: Evolution of higher moments of multiplicity distribution Abstract: Evolution of a multiplicity distribution can be described with the help of\nmaster equation. We first look at 3rd and 4th factorial moments of multiplicity\ndistributions and derive their equilibrium values. From them central moments\nand other ratios can be calculated. We study the master equation for a fixed\ntemperature, because we want to know how fast different moments of the\nmultiplicity distribution approach their equilibrium value. Then we investigate\nthe situation in which the temperature of the system decreases. We find out\nthat in the non-equilibrium state, higher factorial moments differ more from\ntheir equilibrium values than the lower moments and that the behaviour of a\ncombination of the central moments depends on the combination we choose. \n\n"}
{"id": "1811.01280", "contents": "Title: Nonparametric Spectral Methods for Multivariate Spatial and\n  Spatial-Temporal Data Abstract: We propose computationally efficient methods for estimating stationary\nmultivariate spatial and spatial-temporal spectra from incomplete gridded data.\nThe methods are iterative and rely on successive imputation of data and\nupdating of model estimates. Imputations are done according to a periodic model\non an expanded domain. The periodicity of the imputations is a key feature that\nreduces edge effects in the periodogram and is facilitated by efficient\ncirculant embedding techniques. In addition, we describe efficient methods for\ndecomposing the estimated cross spectral density function into a linear model\nof coregionalization plus a residual process. The methods are applied to two\nstorm datasets, one of which is from Hurricane Florence, which struck the\nsouteastern United States in September 2018. The application demonstrates how\nfitted models from different datasets can be compared, and how the methods are\ncomputationally feasible on datasets with more than 200,000 total observations. \n\n"}
{"id": "1811.01520", "contents": "Title: User-Friendly Covariance Estimation for Heavy-Tailed Distributions Abstract: We offer a survey of recent results on covariance estimation for heavy-tailed\ndistributions. By unifying ideas scattered in the literature, we propose\nuser-friendly methods that facilitate practical implementation. Specifically,\nwe introduce element-wise and spectrum-wise truncation operators, as well as\ntheir $M$-estimator counterparts, to robustify the sample covariance matrix.\nDifferent from the classical notion of robustness that is characterized by the\nbreakdown property, we focus on the tail robustness which is evidenced by the\nconnection between nonasymptotic deviation and confidence level. The key\nobservation is that the estimators needs to adapt to the sample size,\ndimensionality of the data and the noise level to achieve optimal tradeoff\nbetween bias and robustness. Furthermore, to facilitate their practical use, we\npropose data-driven procedures that automatically calibrate the tuning\nparameters. We demonstrate their applications to a series of structured models\nin high dimensions, including the bandable and low-rank covariance matrices and\nsparse precision matrices. Numerical studies lend strong support to the\nproposed methods. \n\n"}
{"id": "1811.01821", "contents": "Title: Statistical reform and the replication crisis Abstract: The replication crisis has prompted many to call for statistical reform\nwithin the psychological sciences. Here we examine issues within Frequentist\nstatistics that may have led to the replication crisis, and we examine the\nalternative---Bayesian statistics---that many have suggested as a replacement.\nThe Frequentist approach and the Bayesian approach offer radically different\nperspectives on evidence and inference with the Frequentist approach\nprioritising error control and the Bayesian approach offering a formal method\nfor quantifying the relative strength of evidence for hypotheses. We suggest\nthat rather than mere statistical reform, what is needed is a better\nunderstanding of the different modes of statistical inference and a better\nunderstanding of how statistical inference relates to scientific inference. \n\n"}
{"id": "1811.02316", "contents": "Title: Stacked Penalized Logistic Regression for Selecting Views in Multi-View\n  Learning Abstract: In biomedical research, many different types of patient data can be\ncollected, such as various types of omics data and medical imaging modalities.\nApplying multi-view learning to these different sources of information can\nincrease the accuracy of medical classification models compared with\nsingle-view procedures. However, collecting biomedical data can be expensive\nand/or burdening for patients, so that it is important to reduce the amount of\nrequired data collection. It is therefore necessary to develop multi-view\nlearning methods which can accurately identify those views that are most\nimportant for prediction. In recent years, several biomedical studies have used\nan approach known as multi-view stacking (MVS), where a model is trained on\neach view separately and the resulting predictions are combined through\nstacking. In these studies, MVS has been shown to increase classification\naccuracy. However, the MVS framework can also be used for selecting a subset of\nimportant views. To study the view selection potential of MVS, we develop a\nspecial case called stacked penalized logistic regression (StaPLR). Compared\nwith existing view-selection methods, StaPLR can make use of faster\noptimization algorithms and is easily parallelized. We show that nonnegativity\nconstraints on the parameters of the function which combines the views play an\nimportant role in preventing unimportant views from entering the model. We\ninvestigate the performance of StaPLR through simulations, and consider two\nreal data examples. We compare the performance of StaPLR with an existing view\nselection method called the group lasso and observe that, in terms of view\nselection, StaPLR is often more conservative and has a consistently lower false\npositive rate. \n\n"}
{"id": "1811.06433", "contents": "Title: On a minimum distance procedure for threshold selection in tail analysis Abstract: Power-law distributions have been widely observed in different areas of\nscientific research. Practical estimation issues include how to select a\nthreshold above which observations follow a power-law distribution and then how\nto estimate the power-law tail index. A minimum distance selection procedure\n(MDSP) is proposed in Clauset et al. (2009) and has been widely adopted in\npractice, especially in the analyses of social networks. However, theoretical\njustifications for this selection procedure remain scant. In this paper, we\nstudy the asymptotic behavior of the selected threshold and the corresponding\npower-law index given by the MDSP. We find that the MDSP tends to choose too\nhigh a threshold level and leads to Hill estimates with large variances and\nroot mean squared errors for simulated data with Pareto-like tails. \n\n"}
{"id": "1811.08878", "contents": "Title: Complete One-Loop Matching for a Singlet Scalar in the Standard Model\n  EFT Abstract: We present the results of the first complete one-loop matching calculation\nbetween the real singlet scalar extension of the Standard Model and the\nStandard Model effective field theory (SMEFT) at dimension six. Beyond their\nimmediate relevance to the precision calculation of observables in singlet\nextensions of the Standard Model, our results illustrate a variety of general\nfeatures of one-loop matching. We explore the interplay between\nnon-supersymmetric non-renormalization theorems, the logarithmic dependence of\nWilson coefficients, and the relevance of mixed diagrams in theories with large\nscale separation. In addition, we highlight some of the subtleties involved in\ncomputing observables at next-to-leading order in SMEFT by mapping our results\nto the $T$ parameter at one loop. \n\n"}
{"id": "1811.10443", "contents": "Title: Sparse spectral estimation with missing and corrupted measurements Abstract: Supervised learning methods with missing data have been extensively studied\nnot just due to the techniques related to low-rank matrix completion. Also in\nunsupervised learning one often relies on imputation methods. As a matter of\nfact, missing values induce a bias in various estimators such as the sample\ncovariance matrix. In the present paper, a convex method for sparse subspace\nestimation is extended to the case of missing and corrupted measurements. This\nis done by correcting the bias instead of imputing the missing values. The\nestimator is then used as an initial value for a nonconvex procedure to improve\nthe overall statistical performance. The methodological as well as theoretical\nframeworks are applied to a wide range of statistical problems. These include\nsparse Principal Component Analysis with different types of randomly missing\ndata and the estimation of eigenvectors of low-rank matrices with missing\nvalues. Finally, the statistical performance is demonstrated on synthetic data. \n\n"}
{"id": "1811.10488", "contents": "Title: Identifying treatment effect heterogeneity in dose-finding trials using\n  Bayesian hierarchical models Abstract: An important task in drug development is to identify patients, which respond\nbetter or worse to an experimental treatment. Identifying predictive\ncovariates, which influence the treatment effect and can be used to define\nsubgroups of patients, is a key aspect of this task. Analyses of treatment\neffect heterogeneity are however known to be challenging, since the number of\npossible covariates or subgroups is often large, while samples sizes in earlier\nphases of drug development are often small. In addition, distinguishing\npredictive covariates from prognostic covariates, which influence the response\nindependent of the given treatment, can often be difficult. While many\napproaches for these types of problems have been proposed, most of them focus\non the two-arm clinical trial setting, where patients are given either the\ntreatment or a control. In this paper we consider parallel groups dose-finding\ntrials, in which patients are administered different doses of the same\ntreatment. To investigate treatment effect heterogeneity in this setting we\npropose a Bayesian hierarchical dose-response model with covariate effects on\ndose-response parameters. We make use of shrinkage priors to prevent\noverfitting, which can easily occur, when the number of considered covariates\nis large and sample sizes are small. We compare several such priors in\nsimulations and also investigate dependent modeling of prognostic and\npredictive effects to better distinguish these two types of effects. We\nillustrate the use of our proposed approach using a Phase II dose-finding trial\nand show how it can be used to identify predictive covariates and subgroups of\npatients with increased treatment effects. \n\n"}
{"id": "1811.11025", "contents": "Title: CVEK: Robust Estimation and Testing for Nonlinear Effects using Kernel\n  Machine Ensemble Abstract: The R package CVEK introduces a suite of flexible machine learning models and\nrobust hypothesis tests for learning the joint nonlinear effects of multiple\ncovariates in limited samples. It implements the Cross-validated Ensemble of\nKernels (CVEK)(Liu and Coull 2017), an ensemble-based kernel machine learning\nmethod that adaptively learns the joint nonlinear effect of multiple covariates\nfrom data, and provides powerful hypothesis tests for both main effects of\nfeatures and interactions among features. The R Package CVEK provides a\nflexible, easy-to-use implementation of CVEK, and offers a wide range of\nchoices for the kernel family (for instance, polynomial, radial basis\nfunctions, Mat\\'ern, neural network, and others), model selection criteria,\nensembling method (averaging, exponential weighting, cross-validated stacking),\nand the type of hypothesis test (asymptotic or parametric bootstrap). Through\nextensive simulations we demonstrate the validity and robustness of this\napproach, and provide practical guidelines on how to design an estimation\nstrategy for optimal performance in different data scenarios. \n\n"}
{"id": "1811.12682", "contents": "Title: Large Datasets, Bias and Model Oriented Optimal Design of Experiments Abstract: We review recent literature that proposes to adapt ideas from classical model\nbased optimal design of experiments to problems of data selection of large\ndatasets. Special attention is given to bias reduction and to protection\nagainst confounders. Some new results are presented. Theoretical and\ncomputational comparisons are made. \n\n"}
{"id": "1812.00258", "contents": "Title: A New Approach for Large Scale Multiple Testing with Application to FDR\n  Control for Graphically Structured Hypotheses Abstract: In many large scale multiple testing applications, the hypotheses often have\na known graphical structure, such as gene ontology in gene expression data.\nExploiting this graphical structure in multiple testing procedures can improve\npower as well as aid in interpretation. However, incorporating the structure\ninto large scale testing procedures and proving that an error rate, such as the\nfalse discovery rate (FDR), is controlled can be challenging. In this paper, we\nintroduce a new general approach for large scale multiple testing, which can\naid in developing new procedures under various settings with proven control of\ndesired error rates. This approach is particularly useful for developing FDR\ncontrolling procedures, which is simplified as the problem of developing\nper-family error rate (PFER) controlling procedures. Specifically, for testing\nhypotheses with a directed acyclic graph (DAG) structure, by using the general\napproach, under the assumption of independence, we first develop a specific\nPFER controlling procedure and based on this procedure, then develop a new FDR\ncontrolling procedure, which can preserve the desired DAG structure among the\nrejected hypotheses. Through a small simulation study and a real data analysis,\nwe illustrate nice performance of the proposed FDR controlling procedure for\nDAG-structured hypotheses. \n\n"}
{"id": "1812.02509", "contents": "Title: On separate chemical freeze-outs of hadrons and light (anti)nuclei in\n  high energy nuclear collisions Abstract: The multiplicities of light (anti)nuclei were measured recently by the ALICE\ncollaboration in Pb+Pb collisions at the center-of-mass collision energy\n$\\sqrt{s_{NN}} =2.76$ TeV. Surprisingly, the hadron resonance gas model is able\nto perfectly describe their multiplicities under various assumptions. For\ninstance, one can consider the (anti)nuclei with a vanishing hard-core radius\n(as the point-like particles) or with the hard-core radius of proton, but the\nfit quality is the same for these assumptions. In this paper we assume the\nhard-core radius of nuclei consisting of $A$ baryons or antibaryons to follow\nthe simple law $R(A) = R_b (A)^\\frac{1}{3}$, where $R_b$ is the hard-core\nradius of nucleon. To implement such a relation into the hadron resonance gas\nmodel we employ the induced surface tension concept and analyze the hadronic\nand (anti)nuclei multiplicities measured by the ALICE collaboration. The hadron\nresonance gas model with the induced surface tension allows us to verify\ndifferent scenarios of chemical freeze-out of (anti)nuclei. It is shown that\nthe most successful description of hadrons can be achieved at the chemical\nfreeze-out temperature $T_h=150$ MeV, while the one for all (anti)nuclei is\n$T_A=168.5$ MeV. Possible explanations of this high temperature of (anti)nuclei\nchemical freeze-out are discussed. \n\n"}
{"id": "1812.03001", "contents": "Title: Spin rotation effects in diffractive electroproduction of heavy\n  quarkonia Abstract: In this work we present for the first time the comprehensive study of the\nMelosh spin rotation effects in diffractive electroproduction of S-wave heavy\nquarkonia off a nucleon target. Such a study has been performed within the\ncolor dipole approach using, as an example and a reference point, two popular\nparametrizations of the dipole cross section and two potentials describing the\ninteraction between Q and bar{Q} and entering in the Schroedinger equation\nbased formalism for determination of the quarkonia wave functions. We find a\nstrong onset of spin rotation effects in 1S charmonium photoproduction which is\nobviously neglected in present calculations of corresponding cross sections.\nFor photoproduction of radially excited Psi'(2S) these effects are even\nstronger leading to an increase of the photoproduction cross section by a\nfactor of 2-3 depending on the photon energy. Even in production of radially\nexcited Y'(2S) and Y\"(3S) they can not be neglected and cause the 20-30%\nenhancement of the photoproduction cross section. Finally, we predict that the\nspin effects vanish gradually with photon virtuality Q^2 following universality\nproperties in production of different heavy quarkonia as a function of Q^2 +\nM_V^2. \n\n"}
{"id": "1812.03775", "contents": "Title: Sufficient Dimension Reduction for Classification Abstract: We propose a new sufficient dimension reduction approach designed\ndeliberately for high-dimensional classification. This novel method is named\nmaximal mean variance (MMV), inspired by the mean variance index first proposed\nby Cui, Li and Zhong (2015), which measures the dependence between a\ncategorical random variable with multiple classes and a continuous random\nvariable. Our method requires reasonably mild restrictions on the predicting\nvariables and keeps the model-free advantage without the need to estimate the\nlink function. The consistency of the MMV estimator is established under\nregularity conditions for both fixed and diverging dimension (p) cases and the\nnumber of the response classes can also be allowed to diverge with the sample\nsize n. We also construct the asymptotic normality for the estimator when the\ndimension of the predicting vector is fixed. Furthermore, our method works\npretty well when n < p. The surprising classification efficiency gain of the\nproposed method is demonstrated by simulation studies and real data analysis. \n\n"}
{"id": "1812.05170", "contents": "Title: Markov Decision Processes with Dynamic Transition Probabilities: An\n  Analysis of Shooting Strategies in Basketball Abstract: In this paper we model basketball plays as episodes from team-specific\nnon-stationary Markov decision processes (MDPs) with shot clock dependent\ntransition probabilities. Bayesian hierarchical models are employed in the\nmodeling and parametrization of the transition probabilities to borrow strength\nacross players and through time. To enable computational feasibility, we\ncombine lineup-specific MDPs into team-average MDPs using a novel transition\nweighting scheme. Specifically, we derive the dynamics of the team-average\nprocess such that the expected transition count for an arbitrary state-pair is\nequal to the weighted sum of the expected counts of the separate\nlineup-specific MDPs.\n  We then utilize these non-stationary MDPs in the creation of a basketball\nplay simulator with uncertainty propagated via posterior samples of the model\ncomponents. After calibration, we simulate seasons both on-policy and under\naltered policies and explore the net changes in efficiency and production under\nthe alternate policies. Additionally, we discuss the game-theoretic\nramifications of testing alternative decision policies. \n\n"}
{"id": "1812.05240", "contents": "Title: Seven largest couplings of the standard model as IR fixed points Abstract: We report on an intriguing observation that the values of all the couplings\nin the standard model except those related to first two generations can be\nunderstood from the IR fixed point structure of renormalization group equations\nin the minimal supersymmetric model extended by one complete vectorlike family\nwith the scale of new physics in a multi-TeV range. \n\n"}
{"id": "1812.05723", "contents": "Title: On the sign recovery by LASSO, thresholded LASSO and thresholded Basis\n  Pursuit Denoising Abstract: Basis Pursuit (BP), Basis Pursuit DeNoising (BPDN), and LASSO are popular\nmethods for identifying important predictors in the high-dimensional linear\nregression model, i.e. when the number of rows of the design matrix X is\nsmaller than the number of columns. By definition, BP uniquely recovers the\nvector of regression coefficients b if there is no noise and the vector b has\nthe smallest L1 norm among all vectors s such that Xb=Xs (identifiability\ncondition). Furthermore, LASSO can recover the sign of b only under a much\nstronger irrepresentability condition. Meanwhile, it is known that the model\nselection properties of LASSO can be improved by hard-thresholding its\nestimates. This article supports these findings by proving that thresholded\nLASSO, thresholded BPDN and thresholded BP recover the sign of b in both the\nnoisy and noiseless cases if and only if b is identifiable and large enough. In\nparticular, if X has iid Gaussian entries and the number of predictors grows\nlinearly with the sample size, then these thresholded estimators can recover\nthe sign of b when the signal sparsity is asymptotically below the\nDonoho-Tanner transition curve. This is in contrast to the regular LASSO, which\nasymptotically recovers the sign of b only when the signal sparsity tends to 0.\nNumerical experiments show that the identifiability condition, unlike the\nirrepresentability condition, does not seem to be affected by the structure of\nthe correlations in the $X$ matrix. \n\n"}
{"id": "1812.06406", "contents": "Title: Community Detection with Dependent Connectivity Abstract: In network analysis, within-community members are more likely to be connected\nthan between-community members, which is reflected in that the edges within a\ncommunity are intercorrelated. However, existing probabilistic models for\ncommunity detection such as the stochastic block model (SBM) are not designed\nto capture the dependence among edges. In this paper, we propose a new\ncommunity detection approach to incorporate within-community dependence of\nconnectivities through the Bahadur representation. The proposed method does not\nrequire specifying the likelihood function, which could be intractable for\ncorrelated binary connectivities. In addition, the proposed method allows for\nheterogeneity among edges between different communities. In theory, we show\nthat incorporating correlation information can lower estimation bias and\naccelerate algorithm convergence. Our simulation studies show that the proposed\nalgorithm outperforms the popular variational EM algorithm assuming conditional\nindependence among edges. We also demonstrate the application of the proposed\nmethod to agricultural product trading networks from different countries. \n\n"}
{"id": "1812.07153", "contents": "Title: Gaussian Process Mixtures for Estimating Heterogeneous Treatment Effects Abstract: We develop a Gaussian-process mixture model for heterogeneous treatment\neffect estimation that leverages the use of transformed outcomes. The approach\nwe will present attempts to improve point estimation and uncertainty\nquantification relative to past work that has used transformed variable related\nmethods as well as traditional outcome modeling. Earlier work on modeling\ntreatment effect heterogeneity using transformed outcomes has relied on tree\nbased methods such as single regression trees and random forests. Under the\numbrella of non-parametric models, outcome modeling has been performed using\nBayesian additive regression trees and various flavors of weighted single\ntrees. These approaches work well when large samples are available, but suffer\nin smaller samples where results are more sensitive to model misspecification -\nour method attempts to garner improvements in inference quality via a correctly\nspecified model rooted in Bayesian non-parametrics. Furthermore, while we begin\nwith a model that assumes that the treatment assignment mechanism is known, an\nextension where it is learnt from the data is presented for applications to\nobservational studies. Our approach is applied to simulated and real data to\ndemonstrate our theorized improvements in inference with respect to two causal\nestimands: the conditional average treatment effect and the average treatment\neffect. By leveraging our correctly specified model, we are able to more\naccurately estimate the treatment effects while reducing their variance. \n\n"}
{"id": "1812.07811", "contents": "Title: Exact Top Yukawa corrections to Higgs boson decay into bottom quarks Abstract: In this letter we present the results of the exact computation of\ncontributions to the Higgs boson decay into bottom quarks that are proportional\nto the top Yukawa coupling. Our computation demonstrates that approximate\nresults already available in the literature turn out to be particularly\naccurate for the three physical mass values of the Higgs boson, the bottom and\ntop quarks. Furthermore, contrary to expectations, the impact of these\ncorrections on differential distributions relevant for the searches of the\nHiggs boson decaying into bottom quarks at the Large Hadron Collider is rather\nsmall. \n\n"}
{"id": "1812.09100", "contents": "Title: Flatness without CMB - the Entanglement of Spatial Curvature and Dark\n  Energy Equation of State Abstract: The cosmic spatial curvature parameter $\\Omega_k$ is constrained, primarily\nby cosmic microwave background (CMB) data, to be very small. Observations of\nthe cosmic distance ladder and the large scale structure can provide\nindependent checks of the cosmic flatness. Such late-universe constraints on\n$\\Omega_k$, however, are sensitive to the assumptions of the nature of dark\nenergy. For minimally coupled scalar-field models of dark energy, the equation\nof state $w$ has nontrivial dependence on the cosmic spatial curvature\n$\\Omega_k$ (Miao and Huang, 2018). Such dependence has not been taken into\naccount in previous studies of future observational projects. In this paper we\nuse the $w$ parameterization proposed by Miao and Huang, where the dependence\nof $w$ on $\\Omega_k$ is encoded, and perform Fisher forecast on mock data of\nthree benchmark projects: a WFIRST-like Type Ia supernovae survey, an\nEUCLID-like spectroscopic redshift survey, and an LSST-like photometric\nredshift survey. We find that the correlation between $\\Omega_k$ and $w$ is\nprimarily determined by the data rather than by the theoretical prior. We thus\nvalidate the standard approaches of treating $\\Omega_k$ and $w$ as independent\nquantities. \n\n"}
{"id": "1812.10105", "contents": "Title: Nonlocal generalization of Galilean theories and gravity Abstract: In this paper we propose a wider class of symmetries including the Galilean\nshift symmetry as a subclass. We will show how to construct ghost-free nonlocal\nactions, consisting of infinite derivative operators, which are invariant under\nsuch symmetries, but whose functional form is not simply given by exponentials\nof entire functions. Motivated by this, we will consider the case of a scalar\nfield and discuss the pole structure of the propagator which has infinitely\nmany complex conjugate poles, but satisfies the tree-level unitarity. We will\nalso consider the possibility to construct UV complete Galilean theories by\nshowing how the ultraviolet behavior of loop integrals can be ameliorated.\nMoreover, we will consider kinetic operators respecting the same symmetries in\nthe context of linearized gravity. In such a scenario, the graviton propagator\nturns out to be ghost-free and the spacetime metric generated by a point-like\nsource is nonsingular. These new nonlocal models can be seen as an infinite\nderivative generalization of Lee-Wick theories and open a new branch of\nnonlocal theories. \n\n"}
{"id": "1812.11433", "contents": "Title: On the Construction of Knockoffs in Case-Control Studies Abstract: Consider a case-control study in which we have a random sample, constructed\nin such a way that the proportion of cases in our sample is different from that\nin the general population---for instance, the sample is constructed to achieve\na fixed ratio of cases to controls. Imagine that we wish to determine which of\nthe potentially many covariates under study truly influence the response by\napplying the new model-X knockoffs approach. This paper demonstrates that it\nsuffices to design knockoff variables using data that may have a different\nratio of cases to controls. For example, the knockoff variables can be\nconstructed using the distribution of the original variables under any of the\nfollowing scenarios: (1) a population of controls only; (2) a population of\ncases only; (3) a population of cases and controls mixed in an arbitrary\nproportion (irrespective of the fraction of cases in the sample at hand). The\nconsequence is that knockoff variables may be constructed using unlabeled data,\nwhich is often available more easily than labeled data, while maintaining\nType-I error guarantees. \n\n"}
{"id": "1812.11699", "contents": "Title: A semiparametric spatiotemporal Bayesian model for the bulk and extremes\n  of the Fosberg Fire Weather Index Abstract: Large wildfires pose a major environmental concern, and precise maps of fire\nrisk can improve disaster relief planning. Fosberg Fire Weather Index (FFWI) is\noften used to measure wildfire risk; FFWI exhibits non-Gaussian marginal\ndistributions as well as strong spatiotemporal extremal dependence and thus,\nmodeling FFWI using geostatistical models like Gaussian processes is\nquestionable. Extreme value theory (EVT)-driven models like max-stable\nprocesses are theoretically appealing but are computationally demanding and\napplicable only for threshold exceedances or block maxima. Disaster management\npolicies often consider moderate-to-extreme quantiles of climate parameters and\nhence, joint modeling of the bulk and the tail of the data is required. In this\npaper, we consider a Dirichlet process mixture of spatial skew-t processes that\ncan flexibly model the bulk as well as the tail. The proposed model has\nnonstationary mean and covariance structure, and also nonzero spatiotemporal\nextremal dependence. A simulation study demonstrates that the proposed model\nhas better spatial prediction performance compared to some competing models. We\ndevelop spatial maps of FFWI medians and extremes, and discuss the wildfire\nrisk throughout the Santa Ana region of California. \n\n"}
{"id": "1901.00663", "contents": "Title: Efficient augmentation and relaxation learning for individualized\n  treatment rules using observational data Abstract: Individualized treatment rules aim to identify if, when, which, and to whom\ntreatment should be applied. A globally aging population, rising healthcare\ncosts, and increased access to patient-level data have created an urgent need\nfor high-quality estimators of individualized treatment rules that can be\napplied to observational data. A recent and promising line of research for\nestimating individualized treatment rules recasts the problem of estimating an\noptimal treatment rule as a weighted classification problem. We consider a\nclass of estimators for optimal treatment rules that are analogous to convex\nlarge-margin classifiers. The proposed class applies to observational data and\nis doubly-robust in the sense that correct specification of either a propensity\nor outcome model leads to consistent estimation of the optimal individualized\ntreatment rule. Using techniques from semiparametric efficiency theory, we\nderive rates of convergence for the proposed estimators and use these rates to\ncharacterize the bias-variance trade-off for estimating individualized\ntreatment rules with classification-based methods. Simulation experiments\ninformed by these results demonstrate that it is possible to construct new\nestimators within the proposed framework that significantly outperform existing\nones. We illustrate the proposed methods using data from a labor training\nprogram and a study of inflammatory bowel syndrome. \n\n"}
{"id": "1901.00886", "contents": "Title: Nonparametric graphical model for counts Abstract: Although multivariate count data are routinely collected in many application\nareas, there is surprisingly little work developing flexible models for\ncharacterizing their dependence structure. This is particularly true when\ninterest focuses on inferring the conditional independence graph. In this\narticle, we propose a new class of pairwise Markov random field-type models for\nthe joint distribution of a multivariate count vector. By employing a novel\ntype of transformation, we avoid restricting to non-negative dependence\nstructures or inducing other restrictions through truncations. Taking a\nBayesian approach to inference, we choose a Dirichlet process prior for the\ndistribution of a random effect to induce great flexibility in the\nspecification. An efficient Markov chain Monte Carlo (MCMC) algorithm is\ndeveloped for posterior computation. We prove various theoretical properties,\nincluding posterior consistency, and show that our COunt Nonparametric\nGraphical Analysis (CONGA) approach has good performance relative to\ncompetitors in simulation studies. The methods are motivated by an application\nto neuron spike count data in mice. \n\n"}
{"id": "1901.01864", "contents": "Title: The Jensen Effect and Functional Single Index Models: Estimating the\n  Ecological Implications of Nonlinear Reaction Norms Abstract: This paper develops tools to characterize how species are affected by\nenvironmental variability, based on a functional single index model relating a\nresponse such as growth rate or survival to environmental conditions. In\necology, the curvature of such responses are used, via Jensen's inequality, to\ndetermine whether environmental variability is harmful or beneficial, and\ndiffering nonlinear responses to environmental variability can contribute to\nthe coexistence of competing species.\n  Here, we address estimation and inference for these models with observational\ndata on individual responses to environmental conditions. Because nonparametric\nestimation of the curvature (second derivative) in a nonparametric functional\nsingle index model requires unrealistic sample sizes, we instead focus on\ndirectly estimating the effect of the nonlinearity, by comparing the average\nresponse to a variable environment with the response at the expected\nenvironment, which we call the Jensen Effect. We develop a test statistic to\nassess whether this effect is significantly different from zero. In doing so we\nre-interpret the SiZer method of Chaudhuri and Marron (1995) by maximizing a\ntest statistic over smoothing parameters. We show that our proposed method\nworks well both in simulations and on real ecological data from the long-term\ndata set described in Drake (2005). \n\n"}
{"id": "1901.04885", "contents": "Title: Only Closed Testing Procedures are Admissible for Controlling False\n  Discovery Proportions Abstract: We consider the class of all multiple testing methods controlling tail\nprobabilities of the false discovery proportion, either for one random set or\nsimultaneously for many such sets. This class encompasses methods controlling\nfamilywise error rate, generalized familywise error rate, false discovery\nexceedance, joint error rate, simultaneous control of all false discovery\nproportions, and others, as well as seemingly unrelated methods such as gene\nset testing in genomics and cluster inference methods in neuroimaging. We show\nthat all such methods are either equivalent to a closed testing method, or are\nuniformly improved by one. Moreover, we show that a closed testing method is\nadmissible as a method controlling tail probabilities of false discovery\nproportions if and only if all its local tests are admissible. This implies\nthat, when designing such methods, it is sufficient to restrict attention to\nclosed testing methods only. We demonstrate the practical usefulness of this\ndesign principle by constructing a uniform improvement of a recently proposed\nmethod. \n\n"}
{"id": "1901.06750", "contents": "Title: A simple recipe for making accurate parametric inference in finite\n  sample Abstract: Constructing tests or confidence regions that control over the error rates in\nthe long-run is probably one of the most important problem in statistics. Yet,\nthe theoretical justification for most methods in statistics is asymptotic. The\nbootstrap for example, despite its simplicity and its widespread usage, is an\nasymptotic method. There are in general no claim about the exactness of\ninferential procedures in finite sample. In this paper, we propose an\nalternative to the parametric bootstrap. We setup general conditions to\ndemonstrate theoretically that accurate inference can be claimed in finite\nsample. \n\n"}
{"id": "1901.09982", "contents": "Title: Hierarchical network models for structured exchangeable interaction\n  processes Abstract: Network data often arises via a series of structured interactions among a\npopulation of constituent elements. E-mail exchanges, for example, have a\nsingle sender followed by potentially multiple receivers. Scientific articles,\non the other hand, may have multiple subject areas and multiple authors. We\nintroduce hierarchical edge exchangeable models for the study of these\nstructured interaction networks. In particular, we introduce the hierarchical\nvertex components model as a canonical example, which partially pools\ninformation via a latent, shared population-level distribution. Theoretical\nanalysis and supporting simulations provide clear model interpretation, and\nestablish global sparsity and power-law degree distribution. A computationally\ntractable Gibbs algorithm is derived. We demonstrate the model on both the\nEnron e-mail dataset and an ArXiv dataset, showing goodness of fit of the model\nvia posterior predictive validation. \n\n"}
{"id": "astro-ph/0107476", "contents": "Title: A nuclear many-body theory at finite temperature applied to protoneutron\n  star Abstract: Thermodynamical properties of nuclear matter are studied in the framework of\nan effective many-body field theory at finite temperature, considering the\nSommerfeld approximation. We perform the calculations by using the nonlinear\nBoguta and Bodmer model, extended by the inclusion of the fundamental baryon\noctet and leptonic degrees of freedom. Trapped neutrinos are included in order\nto describe protoneutron star properties through the integration of the\nTolman-Oppenheimer-Volkoff equations, from which we obtain, beyond the standard\nplots for the mass and radius of the protoneutron stars as functions of central\ndensity, new plots of these quantities as functions of temperature. Our\npredictions include the determination of an absolute value for the limiting\nprotoneutron star mass; new aspects on nuclear matter phase transition via the\nbehaviour of the specific heat and, through the inclusion of quark degrees of\nfreedom, the properties of a hadron-quark phase transition and of the hybrid\nprotoneutron stars. \n\n"}
{"id": "astro-ph/0302071", "contents": "Title: Supernova Observation Via Neutrino-Nucleus Elastic Scattering in the\n  CLEAN Detector Abstract: Development of large mass detectors for low-energy neutrinos and dark matter\nmay allow supernova detection via neutrino-nucleus elastic scattering. An\nelastic-scattering detector could observe a few, or more, events per ton for a\ngalactic supernova at 10 kpc ($3.1 \\times 10^{20}$ m). This large yield, a\nfactor of at least 20 greater than that for existing light-water detectors,\narises because of the very large coherent cross section and the sensitivity to\nall flavors of neutrinos and antineutrinos. An elastic scattering detector can\nprovide important information on the flux and spectrum of $\\nu_\\mu$ and\n$\\nu_\\tau$ from supernovae. We consider many detectors and a range of target\nmaterials from $^4$He to $^{208}$Pb. Monte Carlo simulations of low-energy\nbackgrounds are presented for the liquid-neon-based Cryogenic Low Energy\nAstrophysics with Noble gases (CLEAN) detector. The simulated background is\nmuch smaller than the expected signal from a galactic supernova. \n\n"}
{"id": "astro-ph/0303021", "contents": "Title: Constraints on Cardassian Expansion from Distant type Ia Supernovae Abstract: The distant type Ia supernovae data compiled by Perlmutter et al. (1999) are\nused to analyze the Cardassian expansion scenario, which was recently proposed\nby Freese and Lewis (2002) as an alternative to a cosmological constant (or\nmore generally a dark energy component) in explaining the currently\naccelerating universe. We show that the allowed intervals for $n$ and $z_{eq}$,\nthe two parameters of the Cardassian model, will give rise to a universe with a\nvery low matter density, which can hardly be reconciled with the current value\nderived from the measurements of the cosmic microwave background anisotropy and\ngalaxy clusters (cluster baryon fraction). As a result, this Cardassian\nexpansion proposal does not seem to survive the magnitude-redshift test for the\npresent type Ia supernovae data, unless the universe contains primarily\nbaryonic matter. \n\n"}
{"id": "astro-ph/0308248", "contents": "Title: Towards a complete theory of Gamma Ray Bursts Abstract: Gamma Ray Bursts (GRBs) are notorious for their diversity. Yet, they have a\nseries of common features. The typical energy of their $\\gamma$ rays is a\nfraction of an MeV. The energy distributions are well described by a ``Band\nspectrum'', with ``peak energies'' spanning a surprisingly narrow range. The\ntime structure of a GRB consists of pulses, superimposed or not, rising and\ndecreasing fast. The number of photons in a pulse, the pulses' widths and their\ntotal energy vary within broad but given ranges. Within a pulse, the energy\nspectrum softens with increasing time. The duration of a pulse decreases at\nhigher energies and its peak intensity shifts to earlier time. Many other\ncorrelations between pairs of GRB observables have been identified. Last (and\nbased on one measured event!) the $\\gamma$-ray polarization is very large. A\nsatisfactory theory of GRBs should naturally and very simply explain, among\nothers, all these facts. We show that the \"cannonball\" (CB) model does it. In\nthe CB model the process leading to the ejection of highly relativistic jetted\nCBs in core-collapse supernova (SN) explosions is akin to the one observed in\nquasars and microquasars. The prompt $\\gamma$-ray emission --the GRB-- is\nexplained extremely well by inverse Compton scattering of light in the near\nenvironment of the SN by the electrons in the CBs' plasma. We have previously\nshown that the CB-model's description of GRB afterglows as synchrotron\nradiation from ambient electrons --swept in and accelerated within the CBs-- is\nalso simple, universal and very successful. The only obstacle still separating\nthe CB model from a complete theory of GRBs is the theoretical understanding of\nthe CBs' ejection mechanism in SN explosions. \n\n"}
{"id": "astro-ph/0310667", "contents": "Title: High-Energy Cosmic Rays from Gamma-Ray Bursts Abstract: A model is proposed for the origin of cosmic rays (CRs) from ~10^14 eV to the\nhighest energies, >10^20 eV. Gamma-Ray Bursts (GRBs) are assumed to inject CR\nprotons and ions into the interstellar medium of star-forming\ngalaxies--including the Milky Way--with a power law spectrum extending to a\nmaximum energy ~10^20 eV. The CR spectrum near the knee is fit with CRs trapped\nin the Galactic halo that were accelerated and injected by an earlier Galactic\nGRB. These CRs diffuse in the disk and halo of the Galaxy due to gyroresonant\npitch-angle scattering with MHD turbulence in the Galaxy's magnetic field. The\npreliminary (2001) KASCADE data through the knee of the CR spectrum are fit by\na model with energy-dependent propagation of CR ions from a single Galactic\nGRB. Ultra-high energy CRs (UHECRs), with energies above the ankle are assumed\nto propagate rectilinearly with their spectrum modified by photo-pion,\nphoto-pair, and expansion losses. We fit the measured UHECR spectrum assuming\ncomoving luminosity densities of GRB sources consitent with possible star\nformation rate histories of the universe. For power-law CR proton injection p>2\nthis model implies that the nonthermal content in the GRB blast waves is\nhadronically dominated by a factor ~60-200. Calculations show that 100 TeV-100\nPeV neutrinos could be detected several times per year from all GRBs in\nkilometer-scale neutrino detectors such as IceCube, for GRB blast-wave Doppler\nfactors <~200. GLAST measurements of gamma-ray components and cutoffs will\nconstrain the product of nonthermal baryon loading and radiative efficiency,\nlimit the Doppler factor, and test this senario. \n\n"}
{"id": "astro-ph/0408426", "contents": "Title: Big-Bang Nucleosynthesis and Hadronic Decay of Long-Lived Massive\n  Particles Abstract: We study the big-bang nucleosynthesis (BBN) with the long-lived exotic\nparticle, called X. If the lifetime of X is longer than \\sim 0.1 sec, its decay\nmay cause non-thermal nuclear reactions during or after the BBN, altering the\npredictions of the standard BBN scenario. We pay particular attention to its\nhadronic decay modes and calculate the primordial abundances of the light\nelements. Using the result, we derive constraints on the primordial abundance\nof X. Compared to the previous studies, we have improved the following points\nin our analysis: The JETSET 7.4 Monte Carlo event generator is used to\ncalculate the spectrum of hadrons produced by the decay of X; The evolution of\nthe hadronic shower is studied taking account of the details of the energy-loss\nprocesses of the nuclei in the thermal bath; We have used the most recent\nobservational constraints on the primordial abundances of the light elements;\nIn order to estimate the uncertainties, we have performed the Monte Carlo\nsimulation which includes the experimental errors of the cross sections and\ntransfered energies. We will see that the non-thermal productions of D, He3,\nHe4 and Li6 provide stringent upper bounds on the primordial abundance of\nlate-decaying particle, in particular when the hadronic branching ratio of X is\nsizable. We apply our results to the gravitino problem, and obtain upper bound\non the reheating temperature after inflation. \n\n"}
{"id": "astro-ph/0411394", "contents": "Title: Non-Gaussianity from Preheating Abstract: We consider a two-field model for inflation where the second order metric\nperturbations can be amplified by a parametric resonance during preheating. We\ndemonstrate that there can arise a considerable enhancement of non-Gaussianity\nsourced by the local terms generated through the coupled perturbations. We\nargue that the non-Gaussianity parameter could be as large as f_{NL} ~ 50. Our\nresults may provide a useful test of preheating in future CMB experiments. \n\n"}
{"id": "astro-ph/0606350", "contents": "Title: $^3$He experimentum crucis for Dark Matter puzzles Abstract: The leading direct dark matter search experiments: CDMS, Edelweis and\nDAMA/NaI exhibit different results for different approaches to the problem.\nThis contradiction can reflect a nontrivial and probably a multi-component\nnature of the cosmological dark matter. WIMPs can possess dominantly a Spin\nDependent interaction with nucleons. They can be superheavy or represent\natom-like systems of superheavy charged particles. The Dark matter can contain\na component, which strongly interacts with the matter. We show that even a\nmoderate size superfluid $^3$He detector provides a crucial test for these\nhypotheses and that its existing laboratory prototype is already of interest\nfor the experimental dark matter search. \n\n"}
{"id": "astro-ph/9510089", "contents": "Title: The Nature of the Dark Matter Abstract: We review some recent determinations of the amount of dark matter on galactic\nand larger scales, with special attention to the dark matter in the Milky Way.\nWe then briefly review the motivation for and basic physics of several dark\nmatter candidates, and then go into more depth for two candidates, the\nneutralino from supersymmetry, and the baryonic Macho candidate. We give some\nmotivation for supersymmetry and review neutralino detection strategies. For\nMachos we give a description of the discovery of Machos via gravitational\nmicrolensing and the interpretation of the results with respect to the dark\nmatter problem. \n\n"}
{"id": "astro-ph/9712116", "contents": "Title: Recent developments in Vorton Theory Abstract: This article provides a concise overview of recent theoretical results\nconcerning the theory of vortons, which are defined to be (centrifugally\nsupported) equilibrium configurations of (current carrying) cosmic string\nloops. Following a presentation of the results of work on the dynamical\nevolution of small circular string loops, whose minimum energy states are the\nsimplest examples of vortons, recent order of magnitude estimates of the\ncosmological density of vortons produced in various kinds of theoretical\nscenario are briefly summarised. \n\n"}
{"id": "astro-ph/9801224", "contents": "Title: The fractal structure of the universe : a new field theory approach Abstract: While the universe becomes more and more homogeneous at large scales,\nstatistical analysis of galaxy catalogs have revealed a fractal structure at\nsmall-scales (\\lambda < 100 h^{-1} Mpc), with a fractal dimension D=1.5-2\n(Sylos Labini et al 1996). We study the thermodynamics of a self-gravitating\nsystem with the theory of critical phenomena and finite-size scaling and show\nthat gravity provides a dynamical mechanism to produce this fractal structure.\nWe develop a field theoretical approach to compute the galaxy distribution,\nassuming them to be in quasi-isothermal equilibrium. Only a limited, (although\nlarge), range of scales is involved, between a short-distance cut-off below\nwhich other physics intervene, and a large-distance cut-off, where the thermo-\ndynamic equilibrium is not satisfied. The galaxy ensemble can be considered at\ncritical conditions, with large density fluctuations developping at any scale.\n  From the theory of critical phenomena, we derive the two independent critical\nexponents nu and eta and predict the fractal dimension D = 1/nu to be either\n1.585 or 2, depending on whether the long-range behaviour is governed by the\nIsing or the mean field fixed points, respectively. Both set of values are\ncompatible with present observations. In addition, we predict the scaling\nbehaviour of the gravitational potential to be r^{-(1 + eta)/2}. That is,\nr^{-0.5} for mean field or r^{- 0.519} for the Ising fixed point. The theory\nallows to compute the three and higher density correlators without any\nassumption or Ansatz. We find that the N-points density scales as\nr_1^{(N-1)(D-3)}, when r_1 >> r_i, 2 leq i leq N . There are no free parameters\nin this theory. \n\n"}
{"id": "astro-ph/9804023", "contents": "Title: Can high energy neutrino annihilation on relic neutrinos generate the\n  observed highest energy cosmic-rays? Abstract: Annihilation of high energy, $\\sim 10^{21}$eV, neutrinos on big bang relic\nneutrinos of $\\sim 1$eV mass, clustered in the Galactic halo or in a nearby\ngalaxy cluster halo, has been suggested to generate, through hadronic Z decay,\nhigh energy nucleons and photons which may account for the detected flux of\n  >10^{20}eV cosmic-rays. We show that the flux of high energy nucleons and\nphotons produced by this process is dominated by annihilation on the uniform,\nnon-clustered, neutrino background, and that the energy generation rate of\n10^{21}eV neutrinos required to account for the detected flux of >10^{20}eV\nparticles is >10^{48} erg/Mpc^3 yr. This energy generation rate, comparable to\nthe total luminosity of the universe, is 4 orders of magnitude larger than the\nrate of production of high energy nucleons required to account for the flux of\n  >10^{19}eV cosmic-rays. Thus, in order for neutrino annihilation to\ncontribute significantly to the detected flux of >10^{20}eV cosmic-rays, the\nexistence of a new class of high-energy neutrino sources, likely unrelated to\nthe sources of >10^{19}eV cosmic-rays, must be invoked. \n\n"}
{"id": "astro-ph/9902024", "contents": "Title: Scattering of Ultrahigh Energy (UHE) Extragalactic Neutrinos onto Light\n  Relic Neutrinos in Galactic HDM Halo Overcoming the GZK Cut off Abstract: The rarest cosmic rays above the GZK cut'off $(E_{CR} \\tilde{>} 10^{19} \\div\n10^{20} eV)$ are probably born at cosmic distances ($\\tilde{>}$ tens Mpc) by\nAGNs (QSRs, BLac, Blazars...). Their puzzling survival over $2.75 K^o$ BBR\nradio waves opacities (the ``GZK cut off'') might find a natural explanation if\nthe traveling primordial cosmic rays were UHE neutrinos (born by UHE photopion\ndecay) which are transparent to $\\gamma$ or $\\nu$ BBR. These UHE$% \\nu$ might\nscatter onto those (light and cosmological) relic neutrinos clustered around\nour galactic halo.\n  The branched chain reactions from a primordial nucleon (via photoproduction\nof pions and decay to UHE neutrinos) toward the consequent beam dump scattering\non galactic relic neutrinos is at least three order of magnitude more efficient\nthan any known neutrino interactions with Earth atmosphere or direct nucleon\npropagation. Therefore the rarest cosmic rays (as the 320 EeV event) might be\noriginated at far $(\\tilde{>} 100 Mpc)$ distances (as Seyfert galaxy MCG\n8-11-11). The needed UHE radiation power is in rough agreement with the NCG\n8-11-11 observed in MeV gamma energy total output power. The final chain\nproducts observed on Earth by the Fly's Eye detector might be mainly neutron\nand antineutrons as well as, at later stages, protons and antiprotons. These\nhadronic products are most probably secondaries of $W^+ W^-$ or $ZZ$ pair\nproductions and might be consistent with the last AGASA discoveries of 6\ndoublet and one triplet event. \n\n"}
{"id": "cond-mat/0202422", "contents": "Title: Magnetic field driven metal-insulator phase transition in planar systems Abstract: A theory of the magnetic field driven (semi-)metal-insulator phase transition\nis developed for planar systems with a low density of carriers and a linear\n(i.e., relativistic like) dispersion relation for low energy quasiparticles.\nThe general structure of the phase diagram of the theory with respect to the\ncoupling constant, the chemical potential and temperature is derived in two\ncases, with and without an external magnetic field. The conductivity and\nresistivity as functions of temperature and magnetic field are studied in\ndetail. An exact relation for the value of the \"offset\" magnetic field $B_c$,\ndetermining the threshold for the realization of the phase transition at zero\ntemperature, is established. The theory is applied to the description of a\nrecently observed phase transition induced by a magnetic field in highly\noriented pyrolytic graphite. \n\n"}
{"id": "gr-qc/0212128", "contents": "Title: Operational indistinguishabilty of doubly special relativities from\n  special relativity Abstract: We argue that existing doubly special relativities may not be operationally\ndistinguishable from the special relativity. In the process we point out that\nsome of the phenomenologically motivated modifications of dispersion relations,\nand arrived conclusions, must be reconsidered. Finally, we reflect on the\npossible conceptual issues that arise in quest for a theory of spacetime with\ntwo invariant scales. \n\n"}
{"id": "hep-ex/9811038", "contents": "Title: New Limit on Muon and Electron Lepton Number Violation from $K^0_L \\to\n  \\mu^\\pm e^\\mp$ Decay Abstract: The most sensitive experiment to date to search for the muon and electron\nlepton number violating decay $K^0_L \\to \\mu^\\pm e^\\mp$ has detected no events\nconsistent with this process. Based on this result, the 90% confidence level\nupper limit on the branching fraction is $B(K^0_L \\to \\mu^\\pm e^\\mp) <\n4.7\\times10^{-12}$. \n\n"}
{"id": "hep-lat/0003005", "contents": "Title: A new approach to Ginsparg-Wilson fermions Abstract: We expand the most general lattice Dirac operator D in a basis of simple\noperators. The Ginsparg-Wilson equation turns into a system of coupled\nquadratic equations for the expansion coefficients. Our expansion of D allows\nfor a natural cutoff and the remaining quadratic equations can be solved\nnumerically. The procedure allows to find Dirac operators which obey the\nGinpsparg-Wilson equation with arbitrary precision. \n\n"}
{"id": "hep-lat/0012004", "contents": "Title: A derivation of Regge trajectories in large-N transverse lattice QCD Abstract: Large-N QCD is analysed in light-front coordinates with a transverse lattice\nat strong coupling. The general formalism can be looked up on as a d+n\nexpansion with a stack of d-dimensional hyperplanes uniformly spaced in n\ntransverse dimensions. It can arise by application of the renormalisation group\ntransformations only in the transverse directions. At leading order in strong\ncoupling, the gauge field dynamics reduces to the constraint that only colour\nsinglet states can jump between the hyperplanes. With d=2, n=2 and large-N, the\nleading order strong coupling results are simple renormalisations of those for\nthe 't Hooft model. The meson spectrum lies on a set of parallel trajectories\nlabeled by spin. This is the first derivation of the widely anticipated Regge\ntrajectories in a regulated systematic expansion in QCD. \n\n"}
{"id": "hep-lat/0409139", "contents": "Title: QCD Thermodynamics on lattice Abstract: I discuss recent developments in lattice QCD thermodynamics on the nature of\nthe transition at finite temperature and density, equation of state, screening\nof static charges and meson spectral functions at high temperatures. \n\n"}
{"id": "hep-ph/0001033", "contents": "Title: Light Stop: MSSM versus R-parity violation Abstract: We discuss the phenomenology of the lightest stops in models where R-parity\nis broken by bilinear terms. In this class of models we consider scenarios\nwhere the R-parity breaking two-body decay stop_1 -> tau + b competes with the\nleading three-body decays stop_1 -> W^+ + b + neutralino_1, H^+ + b +\nneutralino_1, b slepton^+_i neutrino_l, b sneutrino_l l^+ (l=e, mu, tau). We\ndemonstrate that the R-parity violating decay can be the dominant one. In\nparticular we focus on the implications for a future electron posistion Linear\nCollider. \n\n"}
{"id": "hep-ph/0001148", "contents": "Title: Event-by-Event Analysis and the Central Limit Theorem Abstract: Event-by-event analysis of heavy-ion collision events is an important tool\nfor the study of the QCD phase boundary and formation of a quark-gluon plasma.\nA universal feature of phase boundaries is the appearance of increased\nfluctuations of conserved measures as manifested by excess measure variance\ncompared to a reference. In this paper I consider a particular aspect of EbyE\nanalysis emphasizing global-variables variance comparisons and the central\nlimit theorem. I find that the central limit theorem is, in a broader\ninterpretation, a statement about the scale invariance of total variance for a\nmeasure distribution, which in turn relates to the scale-dependent symmetry\nproperties of the distribution.. I further generalize this concept to the\nrelationship between the scale dependence of a covariance matrix for all\nconserved measures defined on a dynamical system and a matrix of covariance\nintegrals defined on two-point measure spaces, which points the way to a\ndetailed description of the symmetry dynamics of a complex measure system.\nFinally, I relate this generalized description to several recently proposed or\ncompleted event-by-event analyses. \n\n"}
{"id": "hep-ph/0003100", "contents": "Title: Analytic Continuation of Mellin Transforms up to two-loop Order Abstract: The analytic continuation of the Mellin transforms to complex values of N for\nthe basic functions $g_i(x)$ of the momentum fraction x emerging in the\nquantities of massless QED and QCD up to two-loop order, as the unpolarized and\npolarized splitting functions, coefficient functions, and hard scattering cross\nsections for space- and time-like momentum transfer are evaluated. These Mellin\ntransforms provide the analytic continuations of all finite harmonic sums up to\nthe level of the threefold sums of transcendentality four, where the basis-set\n${g_i(x)}$ consists of products of {\\sc Nielsen}-integrals up to\ntranscendentality four. The computer code {\\tt ANCONT} is provided. \n\n"}
{"id": "hep-ph/0003139", "contents": "Title: Large CP Violation, Large Mixings of Neutrinos and the $Z_3$ Symmetry Abstract: We present neutrino mass matrices which predict the atmospheric neutrino\nmixing to be almost maximal, $\\sin^2 2\\theta_{atm}>0.999$, as well as the large\nsolar neutrino mixing, $8/9>\\sin^2 2\\theta_{sol}>0.87$, and the large CP\nviolation (the CP violation phase in the standard form is maximal\n$\\delta=\\pi/2$), based on the $Z_3$ symmetry. \n\n"}
{"id": "hep-ph/0004101", "contents": "Title: Fermion Electric Dipole Moments in Supersymmetric Models with R-parity\n  Violation Abstract: We analyze the electron and neutron electric dipole moments induced by\nR-parity violating interactions in supersymmetric models. It is pointed out\nthat dominant contributions can come from one-loop diagrams involving both the\nbilinear and trilinear R-parity odd couplings, leading to somewhat severe\nconstraints on the products of those couplings. \n\n"}
{"id": "hep-ph/0006258", "contents": "Title: A Calculation of Higgs Mass in the Standard Model Abstract: The assumption that the ratio of the Higgs self-coupling to the square of its\nyukawa coupling to the top is (almost) independent of the renormalization scale\nfixes the Higgs mass within narrow limits at m=160 GeV using only the values of\ngauge couplings and top mass. \n\n"}
{"id": "hep-ph/0007333", "contents": "Title: The ground state of bottomium to two loops and higher Abstract: We consider the properties of the ground state of bottomium. The $\\Upsilon$\nmass is evaluated to two loops, and including leading higher order\n[$O(\\alpha_s^5\\log\\alpha_s)$] and $m_c^2/m_b^2$ corrections. This allows us to\npresent updated values for the pole mass and $\\bar{MS}$ mass of the $b$ quark:\n$m_b=5022\\pm58$ MeV, for the pole mass, and $\\bar{m}_b(\\bar{m}_b)=4286\\pm36$\nMeV for the $\\bar{MS}$ one. The value for the \\msbar mass is accurate including\nand $O(\\alpha_s^3)$ corrections and leading orders in the ratio $m_c^2/m_b^2$.\nWe then consider the wave function for the ground state of $\\bar{b}b$, which is\ncalculated to two loops in the nonrelativistic approximation. Taking into\naccount the evaluation of the matching coefficients by Beneke and Signer one\ncan calculate, in principle, the width for the decay $\\Upsilon\\to e^+e^-$ to\norder $\\alpha_s^5$. Unfortunately, given the size of the corrections it is\nimpossible to produce reliable numbers. The situation is slightly better for\nthe ground state of toponium, where a decay width into $e^+e^-$ of 11 -- 14 keV\nis predicted. \n\n"}
{"id": "hep-ph/0008023", "contents": "Title: Do electroweak precision data and Higgs-mass constraints rule out a\n  scalar bottom quark with mass of O(5 GeV)? Abstract: We investigate the phenomenological implications of a light scalar bottom\nquark, with a mass of about the bottom quark mass, within the minimal\nsupersymmetric standard model. The study of such a scenario is of theoretical\ninterest, since, depending on their production and decay modes, light sbottoms\nmay have escaped experimental detection up to now and, in addition, may\nnaturally appear for large values of \\tan\\beta. In this article we show that\nsuch a light sbottom cannot be ruled out by the constraints from the\nelectroweak precision data and the present bound on the lightest CP-even Higgs\nboson mass at LEP. It is inferred that a light sbottom scenario requires in\ngeneral a relatively light scalar top quark whose mass is typically about the\ntop-quark mass. It is also shown that under these conditions the lightest\nCP-even Higgs boson decays predominantly into scalar bottom quarks in most of\nthe parameter space and that its mass is restricted to m_h ~< 123 GeV. \n\n"}
{"id": "hep-ph/0008024", "contents": "Title: Schematic Method for Estimation of CP Asymmetry in Neutrino Oscillation Abstract: Within the framework of three generations of leptons the schematical method\nfor estimating CP asymmetry in neutrino oscillations is considered. We\nintroduce a unitarity triangle corresponding to \\nu_e-\\nu_{\\mu} oscillation, in\naddition, we show that it is convenient for the estimation to define another\nnew triangle. CP asymmetry is determined by the difference between the shapes\nof a unitarity triangle and new triangle. As results, (i) we show that CP\nasymmetry becomes maximal if the shapes of these two triangles are the same.\n(ii) We can easily estimate L/E which leads almost 100% asymmetry using this.\n(iii) In \\nu_e-\\nu_{\\mu} oscillation with \\sin^2 2\\theta_{13} \\simeq 0.04, we\nobtain about 90% asymmetry for LMA MSW scenario and 3% asymmetry for SMA MSW\nscenario within long baseline neutrino expriments to be realized in near\nfuture. \n\n"}
{"id": "hep-ph/0008293", "contents": "Title: Light-Front QCD in Light-Cone Gauge Abstract: The light-front (LF) quantization of QCD in light-cone (l.c.) gauge is\ndiscussed. The Dirac method is employed to construct the LF Hamiltonian and\ntheory quantized canonically. The Dyson-Wick perturbation theory expansion\nbased on LF-time ordering is constructed. The framework incorporates in it\nsimultaneously the Lorentz gauge condition as an operator equation as well. The\npropagator of the dynamical $\\psi_+$ part of the free fermionic propagator is\nshown to be causal while the gauge field propagator is found to be transverse.\nThe interaction Hamiltonian is re-expressed in the form closely resembling the\none in covariant theory, except for additional instantaneous interactions,\nwhich can be treated systematically. Some explicit computations in QCD are\ngiven. \n\n"}
{"id": "hep-ph/0009121", "contents": "Title: Heavy Quark Production in $\\gamma\\gamma$ Collisions Abstract: New results on inclusive heavy quark production in gamma gamma collisions are\npresented. Charm and bottom production are investigated at LEP II energies by\nthe experiments ALEPH, DELPHI, L3, and OPAL. The total and differential cross\nsections for charm quarks are measured. The contributions from the direct and\nsingle-resolved processes are separated and their fractions quantified. More\ndetailed studies, such as the dependence of the cross section on the two-photon\ncentre-of-mass energy and the charm structure function F^2_gamma,c, are\nreported. The inclusive bottom cross section is presented. Measurements are\ncompared to next-to-leading order calculations. \n\n"}
{"id": "hep-ph/0009191", "contents": "Title: Many-Body Approach to Mesons, Hybrids and Glueballs Abstract: We represent QCD at the hadronic scale by means of an effective Hamiltonian,\nH, formulated in the Coulomb gauge. As in the Nambu-Jona-Lasinio model, chiral\nsymmetry is dynamically broken, however our approach is renormalizable and also\nincludes confinement through a linear potential with slope specified by lattice\ngauge theory. We perform a comparative study of alternative many-body\ntechniques for approximately diagonalizing H: BCS for the vacuum ground state;\nTDA and RPA for the excited hadron states. We adequately describe the\nexperimental meson and lattice glueball spectra and perform the first\nrelativistic, three quasiparticle calculation for hybrid mesons. In general\nagreement with alternative theoretical approaches, we predict the lightest\nhybrid states near but above 2 GeV, indicating the two recently observed\n$J^{PC} = 1^{-+}$ exotics at 1.4 and 1.6 GeV are of a different, perhaps four\nquark, structure. We also detail a new isospin dependent interaction from\n$q\\bar{q}$ color octet annihilation (analogous to ortho positronium) which\nsplits I = 0 and I = 1 states. \n\n"}
{"id": "hep-ph/0009307", "contents": "Title: Gravitational Origin of Quark Masses in an Extra-Dimensional Brane World Abstract: Using the warped extra dimension geometry of the many-brane extension of the\nRandall-Sundrum solution, we find a natural explanation for the observed quark\nmasses of the three Standard Model (SM) generations. Localizing massless SM\nmatter generations on neighboring 3-branes in an extra dimensional world leads\nto phenomenologically acceptable effective four dimensional masses arising from\nthe coupling of the fermion field with the background metric. Thus this\ngeometry can simultaneously address the gauge and quark mass hierarchy\nproblems. \n\n"}
{"id": "hep-ph/0010068", "contents": "Title: Can Precision Measurements of Slepton Masses Probe Right Handed\n  Neutrinos? Abstract: In a supersymmetric model, the presence of a right handed neutrino with a\nlarge Yukawa coupling $f_{\\nu}$ would affect slepton masses via its\ncontribution to the renormalization group evolution between the grand\nunification and weak scales. Assuming a hierarchichal pattern of neutrino\nmasses, these effects are large for only the third generation of sleptons. We\nconstruct mass combinations to isolate the effect of $f_{\\nu}$ from mass\ncorrections already expected from tau Yukawa couplings. We then analyze the\nsize of these effects, assuming that the Super-Kamiokande data constrain 0.033\neV $\\alt m_{\\nu_{\\tau}} \\alt 0.1$ eV and that neutrino masses arise via a\nsee-saw mechanism. We also explore whether these effects might be detectable in\nexperiments at future $e^+e^-$ linear colliders. We find that $m_{\\tnu_{\\tau}}$\nneeds to be measured with a precision of about 2-3% to measure the effect of\n$f_{\\nu}$ if the neutrino and top Yukawa couplings unify at the grand\nunification scale. In a simple case study, we find a precision of only 6-10%\nmight be attainable after several years of operation. If the neutrino Yukawa\ncoupling is larger, or in more complicated models of neutrino masses, a\ndetermination of $\\ttau_1$ and $\\tnu_{\\tau}$ masses might provide a signal of a\nYukawa interaction of neutrinos. \n\n"}
{"id": "hep-ph/0010134", "contents": "Title: Leading logarithmic contribution to the second-order Lamb shift induced\n  by the loop-after-loop diagram Abstract: Contribution of order \\alpha^2 (Z \\alpha)^6 \\ln^3(Z \\alpha)^{-2} to the\nground-state Lamb shift in hydrogen induced by the loop-after-loop diagram is\nevaluated analytically. An additional contribution of this order is found\ncompared to the previous calculation by Karshenboim [JETP 76, 541 (1993)]. As a\nresult, an agreement is achieved for this correction between different\nnumerical and analytical methods. \n\n"}
{"id": "hep-ph/0011163", "contents": "Title: Challenges in Hyperon Decays Abstract: We give an personal overview of some of the unsolved problems related to\nhyperon decays. We cover nonleptonic decays, radiative decays and magnetic\nmoments. Some of the theoretical issues are also touched upon. \n\n"}
{"id": "hep-ph/0011240", "contents": "Title: Probing the violation of equivalence principle at a muon storage ring\n  via neutrino oscillation Abstract: We examine the possible tests of violation of the gravitational equivalence\nprinciple (VEP) at a muon storage ring via neutrino oscillation experiments. If\nthe gravitational interactions of the neutrinos are not diagonal in the flavour\nbasis and the gravitational interaction eigenstates have different couplings to\nthe gravitational field, this leads to the neutrino oscillation. If one starts\nwith $\\mu ^+$ beam then appearance of $\\tau ^\\pm$, $e ^+$ and $\\mu ^-$ in the\nfinal state are the signals for neutrino oscillation. We have estimated the\nnumber of $\\mu ^-$ events in this scenario in $\\nu_\\mu -N$ deep inelastic\nscattering. Final state lepton energy distribution can be used to distinguish\nthe VEP scenario from the others. A large area of VEP parameter space can be\nexplored at a future muon storage ring facility with moderate beam energy. \n\n"}
{"id": "hep-ph/0101108", "contents": "Title: Dynamical Spontaneous Symmetry Breaking in Quantum Chromodynamics Abstract: This study proposes that the longstanding problems of quantum chromodynamics\n(QCD) as an SU(3)_C gauge theory, the confinement mechanism and \\Theta vacuum,\ncan be resolved by dynamical spontaneous symmetry breaking (DSSB) through the\ncondensation of singlet gluons and quantum nucleardynamics (QND) as an SU(2)_N\n\\times U(1)_Z gauge theory is produced. The confinement mechanism is the result\nof massive gluons and the Yukawa potential provides hadron formation. The\nevidences for the breaking of discrete symmetries (C, P, T, CP) during DSSB\nappear explicitly: baryons and mesons without their parity partners, the\nconservation of vector current and the partial conservation of the axial vector\ncurrent, the baryon asymmetry \\delta_B \\simeq 10^{-10}, and the neutron\nelectric dipole moment \\Theta < 10^{-9}. \n\n"}
{"id": "hep-ph/0101138", "contents": "Title: Inelastic Dark Matter Abstract: Many observations suggest that much of the matter of the universe is\nnon-baryonic. Recently, the DAMA NaI dark matter direct detection experiment\nreported an annual modulation in their event rate consistent with a WIMP relic.\nHowever, the Cryogenic Dark Matter Search (CDMS) Ge experiment excludes most of\nthe region preferred by DAMA. We demonstrate that if the dark matter can only\nscatter by making a transition to a slightly heavier state (Delta m ~ 100kev),\nthe experiments are no longer in conflict. Moreover, differences in the energy\nspectrum of nuclear recoil events could distinguish such a scenario from the\nstandard WIMP scenario. Finally, we discuss the sneutrino as a candidate for\ninelastic dark matter in supersymmetric theories. \n\n"}
{"id": "hep-ph/0104072", "contents": "Title: Kinetic equilibration in heavy ion collisions: The role of elastic\n  processes Abstract: We study the kinetic equilibration of gluons produced in the very early\nstages of a high energy heavy ion collision in a ``self-consistent'' relaxation\ntime approximation. We compare two scenarios describing the initial state of\nthe gluon system, namely the saturation and the minijet scenarios, both at RHIC\nand LHC energies. We argue that, in order to characterize kinetic\nequilibration, it is relevant to test the isotropy of various observables. As a\nconsequence, we find in particular that in both scenarios elastic processes are\nnot sufficient for the system to reach kinetic equilibrium at RHIC energies.\nMore generally, we show that, contrary to what is often assumed in the\nliterature, elastic collisions alone are not sufficient to rapidly achieve\nkinetic equilibration. Because of longitudinal expansion at early times, the\nactual equilibration time is at least of the order of a few fermis. \n\n"}
{"id": "hep-ph/0104120", "contents": "Title: CPT, T, and Lorentz Violation in Neutral-Meson Oscillations Abstract: Tests of CPT and Lorentz symmetry using neutral-meson oscillations are\nstudied within a formalism that allows for indirect CPT and T violation of\narbitrary size and is independent of phase conventions. The analysis is\nparticularly appropriate for studies of CPT and T violation in oscillations of\nthe heavy neutral mesons D, B_d, and B_s. The general Lorentz- and CPT-breaking\nstandard-model extension is used to derive an expression for the parameter for\nCPT violation. It varies in a prescribed way with the magnitude and orientation\nof the meson momentum and consequently also with sidereal time. Decay\nprobabilities are presented for both uncorrelated and correlated mesons, and\nsome implications for experiments are discussed. \n\n"}
{"id": "hep-ph/0108014", "contents": "Title: Structure of the Coulomb and unitarity corrections to the cross section\n  of $e^+e^-$ pair production in ultra-relativistic nuclear collisions Abstract: We analyze the structure of the Coulomb and unitarity corrections to the\nsingle pair production as well as the cross section for the multiple pair\nproduction. In the external field approximation we consider the probability of\n$e^+e^-$ pair production at fixed impact parameter $\\rho$ between colliding\nultra-relativistic heavy nuclei. We obtain the analytical result for this\nprobability at large $\\rho$ as compared to the electron Compton wavelength. We\nestimate also the unitary corrections to the total cross section of the\nprocess. \n\n"}
{"id": "hep-ph/0109288", "contents": "Title: Lower Limits on Soft Supersymmetry-Breaking Scalar Masses Abstract: Working in the context of the CMSSM, we argue that phenomenological\nconstraints now require the universal soft supersymmetry-breaking scalar mass\nm_0 be non-zero at the input GUT scale. This conclusion is primarily imposed by\nthe LEP lower limit on the Higgs mass and the requirement that the lightest\nsupersymmetric particle not be charged. We find that m_0 > 0 for all tan beta\nif mu < 0, and m_0 = 0 may be allowed for mu > 0 only when tan beta sim 8 and\none allows an uncertainty of 3+ GeV in the theoretical calculation of the Higgs\nmass. Upper limits on flavour-changing neutral interactions in the MSSM squark\nsector allow substantial violations of non-universality in the m_0 values, even\nif their magnitudes are comparable to the lower limit we find in the CMSSM.\nAlso, we show that our lower limit on m_0 at the GUT scale in the CMSSM is\ncompatible with the no-scale boundary condition m_0 = 0 at the Planck scale. \n\n"}
{"id": "hep-ph/0110150", "contents": "Title: Color superconducting quark matter in compact stars Abstract: When nuclear matter reaches a high enough density, we expect that the\nnucleons will overlap so much as to lose their separate identities, and merge\ninto quark matter. In this talk I will review some theoretical expectations and\nspeculations about quark matter, focusing on the phenomenon of quark pair\ncondensation (color superconductivity) and its application to compact stars. \n\n"}
{"id": "hep-ph/0110195", "contents": "Title: Large-Nc QCD and Low Energy Interactions Abstract: This talk reviews recent progress in formulating the dynamics of the\nelectroweak interactions of hadrons at low energies, within the framework of\nthe 1/Nc-expansion in QCD. The emphasis is put on the basic issues of the\napproach. \n\n"}
{"id": "hep-ph/0111418", "contents": "Title: Global analysis of Solar neutrino oscillation evidence including SNO and\n  implications for Borexino Abstract: An updated analysis of all available neutrino oscillation evidence in Solar\nexperiments including the latest $SNO$ data is presented. Predictions for total\nrates and day-night asymmetry in Borexino are calculated. Our analysis features\nthe use of exhaustive computation of the neutrino oscillation probabilities and\nthe use of an improved statistical $\\chi^2$ minimization.\n  In the framework of two neutrino oscillations we conclude that the best fit\nto the data is obtained in the LMA region with parameters $(\\Delta m^2,\n\\tan^2\\theta) = (5.2 \\times 10^{-5} \\eV^2, 0.47)$, ($\\chi^2_{min}/n=0.82$,\n$n=38$ degrees of freedom). Although less favored, solutions in the LOW and VAC\nregions are still possible with a reasonable statistical significance. The best\npossible solution in the SMA region gets as maximum a statistical significance\nas low as $\\sim 3%$.\n  We study the implications of these results for the prospects of Borexino and\nthe possibility of discriminating between the different solutions. The expected\nnormalized Borexino signal is 0.62 at the best fit LMA solution while the DN\nasymmetry is negligible (approximately $10^{-5}$). In the LOW region the signal\nis in the range $\\sim 0.6-0.7$ at 90% confidence level while the asymmetry is\n$\\simeq 1-20%$. As a consequence, the combined Borexino measurements of the\ntotal event rate with a error below $\\pm 5-10%$ and day-night total rate\nasymmetry with a precision comparable to the one of SuperKamiokande, will have\na strong chance of distinguishing or at least strongly favoring one of the\nSolar neutrino solutions provided by present data. \n\n"}
{"id": "hep-ph/0112314", "contents": "Title: Kinetic equation for gluons at the early stage Abstract: We derive the kinetic equation for pure gluon QCD plasma in a general way,\napplying the background field method. We show that the quantum kinetic equation\ncontains a term as in the classical case, that describes a color charge\nprecession of partons moving in the gauge field. We emphasize that this new\nterm is necessary for the gauge covariance of the resulting equation. \n\n"}
{"id": "hep-ph/0201048", "contents": "Title: Phases of QCD, Thermal Quasiparticles and Dilepton Radiation from a\n  Fireball Abstract: We calculate dilepton production rates from a fireball adapted to the\nkinematical conditions realized in ultrarelativistic heavy ion collisions over\na broad range of beam energies. The freeze-out state of the fireball is fixed\nby hadronic observables. We use this information combined with the initial\ngeometry of the collision region to follow the space-time evolution of the\nfireball. Assuming entropy conservation, its bulk thermodynamic properties can\nthen be uniquely obtained once the equation of state (EoS) is specified. The\nhigh-temperature (QGP) phase is modelled by a non-perturbative quasiparticle\nmodel that incorporates a phenomenological confinement description, adapted to\nlattice QCD results. For the hadronic phase, we interpolate the EoS into the\nregion where a resonance gas approach seems applicable, keeping track of a\npossible overpopulation of the pion phase space. In this way, the fireball\nevolution is specified without reference to dilepton data, thus eliminating it\nas an adjustable parameter in the rate calculations. Dilepton emission in the\nQGP phase is then calculated within the quasiparticle model. In the hadronic\nphase, both temperature and finite baryon density effects on the photon\nspectral function are incorporated. Existing dilepton data from CERES at 158\nand 40 AGeV Pb-Au collisions are well described, and a prediction for the\nPHENIX setup at RHIC for sqrt(s) = 200 AGeV is given. \n\n"}
{"id": "hep-ph/0201110", "contents": "Title: Topcolor Dynamics and The effective gluon-gluon-Higgs Operator Abstract: We discuss the production of the composite Higgs boson in topcolor models via\nthe gluon fusion process. We consider the contribution of color-octet massive\ngauge bosons (colorons) strongly interacting with the top quark, in addition to\nnonstandard contributions of the top-Yukawa coupling and heavy colored fermions\nother than the top quark. In order to estimate the contribution of colorons, we\nderive the low-energy effective theory by eliminating colorons by using the\nequation of motion for colorons. We replace the composite operator (\\bar{q}_L\nt_R)(\\bar{t}_R q_L) in the effective theory by the composite Higgs operator. We\nthen obtain the effective gluon-gluon-Higgs (ggH-) operator induced by colorons\nand find that its coefficient (A_{col}) is proportional to m_{dyn}^2/M^2, where\nM and m_{dyn} denote the coloron mass and the mass dynamically generated by\ncolorons, respectively. The contribution of colorons A_{col} becomes comparable\nto the top-loop effect A_{top} for M=O(1TeV) and m_{dyn}=O(0.6TeV). Such a\nlarge dynamical mass can be realized in top-seesaw (TSS) models consistently\nwith the experimental value of the top quark mass (m_t^{exp}), while the\ndynamical mass itself is adjusted to m_t^{exp} in topcolor assisted technicolor\nmodels (TC2). We find that the coloron contribution A_{col} can be sizable in a\ncertain class of TSS models: the contribution of colorons (the top-loop) is\ndominant in the real (imaginary) part of the H->gg amplitude for the Higgs\nboson mass m_H of the order of 1 TeV. On the other hand, enhancement of the\ntop-Yukawa coupling becomes important in TC2. We can observe signatures of the\nHiggs boson in TC2 with m_H \\sim 200 GeV even at the Tevatron Run II as well as\nat the LHC. We estimate S/\\sqrt{B}=3-6 for an integrated luminosity of 2\nfb^{-1} and m_H=190 GeV at the Tevatron Run II. \n\n"}
{"id": "hep-ph/0201245", "contents": "Title: Some aspects of collisional sources for electroweak baryogenesis Abstract: We consider the dynamics of fermions with a spatially varying mass which\ncouple to bosons through a Yukawa interaction term and perform a consistent\nweak coupling truncation of the relevant kinetic equations. We then use a\ngradient expansion and derive the CP-violating source in the collision term for\nfermions which appears at first order in gradients. The collisional sources\ntogether with the semiclassical force constitute the CP-violating sources\nrelevant for baryogenesis at the electroweak scale. We discuss also the absence\nof sources at first order in gradients in the scalar equation, and the\nlimitations of the relaxation time approximation. \n\n"}
{"id": "hep-ph/0206047", "contents": "Title: Fractality and geometry in ultra-relativistic nuclear collisions Abstract: Assuming fractality of hadronic constituents, we argue that asymmetry of\nspace-time can be induced in the ultra-relativistic interactions of hadrons and\nnuclei. The asymmetry is expressed in terms of the anomalous fractal dimensions\nof the colliding objects. Besides state of motion, the relativistic principle\nis applied to the state of asymmetry as well. Such realization of relativity\nconcerns scale dependence of physical laws emerging at small distances. We show\nthat induced asymmetries of space-time are a priori not excluded by the\nMichelson's experiment even at large scales. \n\n"}
{"id": "hep-ph/0206125", "contents": "Title: Chiral dynamics and B to 3 pi decay Abstract: I discuss our knowledge of the scalar sector of QCD and how it impacts the\ndetermination of the CKM angle \\alpha from the isospin analysis of B\\to \\rho\\pi\ndecay. \n\n"}
{"id": "hep-ph/0208055", "contents": "Title: b Physics Beyond the Standard Model Abstract: I review the signals for New Physics in CP-violating measurements in B and\nLambda_b decays. I also discuss ways of identifying this New Physics, should\nsuch a signal be found. \n\n"}
{"id": "hep-ph/0208204", "contents": "Title: Exclusive Radiative Decays of B Mesons Abstract: We present within the Standard Model the exclusive radiative decays B ->\nK*/rho gamma and B_(s/d) -> gamma gamma in QCD factorization based on the\nheavy-quark limit m_b >> Lambda_QCD. For the decays with a vector meson in the\nfinal state we give results complete to next-to-leading order in QCD. \n\n"}
{"id": "hep-ph/0210015", "contents": "Title: Weak Corrections to Three-Jet Production in Electron-Positron\n  Annihilations: 1) The Factorisable Contributions Abstract: We report on the calculation of the factorisable one-loop weak-interaction\ncorrections to the initial and final states for three-jet observables in\nelectron-positron annihilations. We show that such corrections are of a few\npercent at $\\sqrt s=M_Z$. Hence, while their impact is not dramatic in the\ncontext of LEP1 and SLC, where the total error on the measured value of\n$\\alpha_{\\mathrm{S}}$ is larger, at a future Linear Collider, running at the\n$Z$ mass peak (e.g., GigaZ), they ought to be taken into account in the\nexperimental fits, as here the uncertainty on the value of the strong coupling\nconstant is expected to be at the 0.1% level or even smaller. The calculation\nhas been performed using helicity amplitudes so that it can be applied to the\ncase of polarised beams. \n\n"}
{"id": "hep-ph/0210108", "contents": "Title: Decoupling behaviour of O(m_t^4) corrections to the h^0 self-couplings Abstract: The decoupling behaviour of the leading one-loop Yukawa-coupling\ncontributions of O(m_t^4) to the lightest MSSM Higgs boson self-couplings, when\nthe top-squarks are heavy as compared to the electroweak scale, are discussed.\nAs shown analytically and numerically, the large corrections can almost\ncompletely be absorbed into the h^0-boson mass and therefore, the h^0\nself-couplings remain similar to the coupling of the SM Higgs boson for a heavy\ntop-squark sector. \n\n"}
{"id": "hep-ph/0211071", "contents": "Title: Relic Neutralino Densities and Detection Rates with Nonuniversal Gaugino\n  Masses Abstract: We extend previous analyses on the interplay between nonuniversalities in the\ngaugino mass sector and the thermal relic densities of LSP neutralinos, in\nparticular to the case of moderate to large tan beta. We introduce a set of\nparameters that generalizes the standard unified scenario to cover the complete\nallowed parameter space in the gaugino mass sector. We discuss the physical\nsignificance of the cosmologically preferred degree of degeneracy between\ncharginos and the LSP and study the effect this degree of degeneracy has on the\nprospects for direct detection of relic neutralinos in the next round of dark\nmatter detection experiments. Lastly, we compare the fine tuning required to\nachieve a satisfactory relic density with the case of universal gaugino masses,\nas in minimal supergravity, and find it to be of a similar magnitude. The\nsensitivity of quantifiable measures of fine-tuning on such factors as the\ngluino mass and top and bottom masses is also examined. \n\n"}
{"id": "hep-ph/0211116", "contents": "Title: Flavor changing Z decay $Z\\to b\\bar{s}(\\bar{b}s)$ in topcolor-assisted\n  technicolor models Abstract: In the context of topcolor-assisted technicolor (TC2) models, we examine the\nflavor changing (FC) Z decay $Z\\to b\\bar{s}(\\bar{b}s)$ and calculate the\ncontributions of the new particles predicted by TC2 models to the branching\nratio Br($Z\\to b\\bar{s}+\\bar{b}s$). We find that the contributions mainly come\nfrom the top-pions. In most of the parameter space, the Br($Z\\to\nb\\bar{s}+\\bar{b}s$) can reach $10^{-5}$, which may be detected in near future\nexperiments such as Giga-Z version of the TESLA. Thus, the FC Z decay $Z\\to\nb\\bar{s}(\\bar{b}s)$ can be used to test TC2 models. \n\n"}
{"id": "hep-ph/0211232", "contents": "Title: Single-Spin Asymmetries and Transversity Abstract: A pedagogical introduction to single-spin asymmetries (SSA's) and\ntransversity is presented. Discussion in some detail is made of certain aspects\nof (SSA's) in lepton-nucleon and in hadron-hadron scattering and the role of\npQCD and evolution in the context of transversity. \n\n"}
{"id": "hep-ph/0212098", "contents": "Title: Consistent treatment of spin-1 mesons in the light-front formalism Abstract: We analyze the matrix element of the electroweak current between $q \\qb$\nvector meson states in the framework of a covariant extension of the\nlight-front formalism. The light-front matrix element of a one-body current is\nnaturally associated with zero modes, which affect some of the form factors\nthat are necessary to represent the Lorentz structure of the light-front\nintegral. The angular condition contains some information on zero modes, i.e.,\nonly if the effect of zero modes is accounted for correctly, is it satisfied.\nWith plausible assumptions we derive from the angular condition several\nconsistency conditions which can be used quite generally to determine the zero\nmode contribution of form factors. The correctness of this method is tested by\nthe phenomenological success of the derived form factors. We compare the\npredictions of our formalism with those of the standard light-front approach\nand with available data. As examples we discuss the magnetic moment of the\n$\\rho$, the coupling constant $g_{D^\\ast D \\pi}$, and the coupling constants of\nthe pseudoscalar density, $g_\\pi$ and $g_K$, which provide a phenomenological\nlink between constituent and current quark masses. \n\n"}
{"id": "hep-ph/0212171", "contents": "Title: Meson mass spectrum and OPE: matching to the large-N_c QCD Abstract: The relations between masses and decay constants of variety of meson\nresonances in the energy range 0--3 GeV are verified from the string-like,\nlinear mass spectrum for vector, axial-vector, scalar and pseudoscalar mesons\nwith a universal slope. The way to match the universality with the Operator\nProduct Expansion (OPE) is proposed. The necessity of small deviations from\nlinearity in parameterization of the meson mass spectrum and their decay\nconstants is proven from matching to OPE. \n\n"}
{"id": "hep-ph/0212248", "contents": "Title: Parton interactions in the Bjorken aymptotics Abstract: We demonstrate the effective action scheme for the leading parton\ninteractions and discuss the symmetry properties. The interaction kernels are\nparticular cases of conformal symmetric two-particle kernels. There is a direct\nrelation to the conformal symmetric rational solution of the Yang-Baxter\nequation. \n\n"}
{"id": "hep-ph/0301015", "contents": "Title: Revisiting Top-Bottom-Tau Yukawa Unification in Supersymmetric Grand\n  Unified Theories Abstract: Third family Yukawa unification, as suggested by minimal SO(10) unification,\nis revisited in light of recent experimental measurements and theoretical\nprogress. We characterize unification in a semi-model-independent fashion, and\nconclude that finite $b$ quark mass corrections from superpartners must be\nnonzero, but much smaller than naively would be expected. We show that a\nsolution that does not require cancellations of dangerously large tanbeta\neffects in observables implies that scalar superpartner masses should be\nsubstantially heavier than the Z scale, and perhaps inaccessible to all\ncurrently approved colliders. On the other hand, gauginos must be significantly\nlighter than the scalars. We demonstrate that a spectrum of anomaly-mediated\ngaugino masses and heavy scalars works well as a theory compatible with third\nfamily Yukawa unification and dark matter observations. \n\n"}
{"id": "hep-ph/0301063", "contents": "Title: The Tachyon Inflationary Models with Exact Mode Functions Abstract: We show two analytical solutions of the tachyon inflation for which the\nspectrum of curvature (density) perturbations can be calculated exactly to\nlinear order, ignoring both gravity and the self-interactions of the tachyon\nfield . The main feature of these solutions is that the spectral indices are\nindependent with scale. \n\n"}
{"id": "hep-ph/0302039", "contents": "Title: Probing the LSND scale and four neutrino scenarios with a neutrino\n  telescope Abstract: We show in this paper that the observation of the angular distribution of\nupward-going muons and cascade events induced by atmospheric neutrinos at the\nTeV energy scale, which can be performed by a kilometer-scale neutrino\ntelescope, such as the IceCube detector, can be used to probe a large neutrino\nmass splitting, $| \\Delta m^2 | \\sim (0.5-2.0)$ eV$^2$, implied by the LSND\nexperiment and discriminate among four neutrino mass schemes. This is due to\nthe fact that such a large mass scale can promote non negligible muon neutrino\nto electron/tau neutrino (and/or anti-muon neutrino to anti-electron/anti-tau\nneutrino) conversions at these energies by the MSW effect as well as vacuum\noscillation, unlike what is expected if all the neutrino mass splittings are\nsmall. \n\n"}
{"id": "hep-ph/0304025", "contents": "Title: Pseudoscalars Mesons in Hot, Dense Matter Abstract: Phase transitions in hot and dense matter and the in--medium behavior of\npseudoscalar mesons ($\\pi^{\\pm}, \\pi^0, K^{\\pm}, K^0 ,\\bar K^0,\\eta {and} \\eta'\n$) are investigated, in the framework of the three flavor Nambu--Jona-Lasinio\nmodel, including the 't Hooft interaction, which breaks the $U_A(1)$ symmetry.\nThree different scenarios are considered: zero density and finite temperature,\nzero temperature and finite density in quark matter with different degrees of\nstrangeness, and finite temperature and density. At T=0, the role of strange\nvalence quarks in the medium is discussed, in connection with the phase\ntransition and the mesonic behavior. It is found that the appearance of strange\nquarks, above certain densities, leads to meaningful changes in different\nobservables, especially in matter with \\beta$ --equilibrium. The behavior of\nmesons in the $T-\\rho$ plane is analyzed in connection with possible signatures\nof restoration of symmetries. \n\n"}
{"id": "hep-ph/0304247", "contents": "Title: The \\beta-term for D^* --> D \\gamma within a heavy-light chiral quark\n  model Abstract: We present a calculation of the \\beta-term for D^* --> D gamma within a\nheavy-light chiral quark model. Within the model, soft gluon effects in terms\nof the gluon condensate with lowest dimension are included. Also, calculations\nof 1/m_c corrections are performed. We find that the value of \\beta is rather\nsensitive to the constituent quark mass compared to other quantities calculated\nwithin the same model. Also, to obtain a value close to the experimental value,\none has to choose a constituent light quark mass larger than for other\nquantities studied in previous papers. For a light quark mass in the range 250\nto 300 MeV and a quark condensate in the range -(250-270 MeV)^3 we find the\nvalue (2.5 +- 0.6) GeV^-1. This value is in agreement with the value of \\beta\nextracted from experiment 2.7 +- 0.2 GeV^-1. \n\n"}
{"id": "hep-ph/0305241", "contents": "Title: Problems of the rotating-torsion-balance limit on the photon mass Abstract: We discuss the problems (and the promise) of the ingenious method introduced\nby Lakes, and recently improved on by Luo, to detect a possible small photon\nmass $\\mu$ by measuring the ambient magnetic vector potential from large scale\nmagnetic fields. We also point out how an improved ``indirect'' limit can be\nobtained using modern measurements of astrophysical magnetic fields and plasmas\nand that a good ``direct'' limit exists using properties of the solar wind. \n\n"}
{"id": "hep-ph/0308091", "contents": "Title: Yukawa Hierarchies From Extra Dimensions With Small FCNC Abstract: We investigate a class of extra dimensional models where all of the Standard\nModel fermions are localized to a single fixed point in an $S_1/Z_2$ orbifold,\nand each species is localized with an exponential wavefunction with a different\nwidth. We show that this naturally generates Yukawa hierarchies of the size\npresent in the Standard Model, and we find a set of model parameters that\nreproduces the observed masses and mixings to experimental accuracy. In\naddition, the dominant constraints, arising from flavor changing neutral\ncurrents, are shown to restrict the compactification scale to be $1/R \\ge 2-5\n\\tev$, which is a much less stringent constraint than in similar extra\ndimensional models of the Yukawa hierarchy. \n\n"}
{"id": "hep-ph/0309305", "contents": "Title: Is Theta+(1540) a Kaon--Skyrmion Resonance? Abstract: We reconsider the relationship between the bound state and the SU(3) rigid\nrotator approaches to strangeness in the Skyrme model. For non-exotic S=-1\nbaryons the bound state approach matches for small m_K onto the rigid rotator\napproach, and the bound state mode turns into the rotator zero-mode. However,\nfor small m_K, we find no S=+1 kaon bound states or resonances in the spectrum,\nconfirming previous work. This suggests that, at least for large N and small\nm_K, the exotic state may be an artifact of the rigid rotator approach to the\nSkyrme model. An S=+1 near-threshold state comes into existence only for\nsufficiently large SU(3) breaking. If such a state exists, then it has the\nexpected quantum numbers of Theta+: I=0, J=1/2 and positive parity. Other\nexotic states with (I=1, J^P=3/2+), (I=1,J^P=1/2+), (I=2, J^P=5/2+) and\n(I=2,J^P=3/2+) appear as its SU(2) rotator excitations. As a test of our\nmethods, we also identify a D-wave S=-1 near-threshold resonance that, upon\nSU(2) collective coordinate quantization, reproduces the mass splittings of the\nobserved states Lambda(1520), Sigma(1670) and Sigma(1775) with good accuracy. \n\n"}
{"id": "hep-ph/0310200", "contents": "Title: The isotensor pentaquark Abstract: Further consequences of the 1540 MeV Theta+ resonance as an isotensor\npentaquark beyond Capstick et al. are explored. It is argued that the SAPHIR\ndata may not currently exclude the existence of the charged partner Theta++.\nThe usual prediction of the dominance of non-resonant Theta+ K, and Theta+ K*,\nfinal states in photoproduction on the proton is argued not to obtain for an\nisotensor Theta+. This enhances the importance of excited baryon final states,\nwhere the excited baryon decays to Theta+ K or Theta+ K*; as well as the\nnon-resonant Theta+ K pi final state. The small width of the recently\ndiscovered Xi-- cascade resonance to Xi- pi- is easier to explain if Theta+ is\nan isotensor pentaquark than if it is in the 10bar representation, due to both\nan isospin and U-spin selection rule. A new production diagram for Theta+ in\nthe photoproduction on the deuteron is suggested. \n\n"}
{"id": "hep-ph/0311100", "contents": "Title: New nonrenormalization theorems for anomalous three point functions Abstract: Nonrenormalization theorems involving the transverse, i.e. non anomalous,\npart of the <VVA> correlator in perturbative QCD are proven. Some of their\nconsequences and questions they raise are discussed. \n\n"}
{"id": "hep-ph/0312270", "contents": "Title: Present status of our knowledge of |V_{cb}| Abstract: The Cabibbo-Kobayashi-Maskawa parameter $|V_{cb}|$ plays an important role\namong the experimental constraints of the Yukawa sector of the Standard Model.\nThe present status of our knowledge will be summarized with particular emphasis\nto the interplay between theoretical and experimental advances needed to\nimprove upon present uncertainties. \n\n"}
{"id": "hep-ph/0401148", "contents": "Title: Prospects for Higgs Searches via VBF at the LHC with the ATLAS Detector Abstract: We report on the potential for the discovery of a Standard Model Higgs boson\nwith the vector boson fusion mechanism in the mass range $115<M_H<500 \\gev$\nwith the ATLAS experiment at the LHC. Feasibility studies at hadron level\nfollowed by a fast detector simulation have been performed for $H\\to\nW^{(*)}W^{(*)}\\to l^+l^-\\sla{p_T}$, $H\\to\\gamma\\gamma$ and $H\\to ZZ\\to\nl^+l^-q\\bar{q}$. The results obtained show a large discovery potential in the\nrange $115<M_H<300 \\gev$. Results obtained with multivariate techniques are\nreported for a number of channels. \n\n"}
{"id": "hep-ph/0401210", "contents": "Title: Charmonium levels near threshold and the narrow state $X(3872) \\to\n  \\pi^{+}\\pi^{-}\\jpsi$ Abstract: We explore the influence of open-charm channels on charmonium properties, and\nprofile the 1:3D2, 1:3D3 and 2:1P1 charmonium candidates for X(3872). The\nfavored candidates, the 1:3D2 and 1:3D3 levels, both have prominent radiative\ndecays. The 1:3D2 might be visible in the $D^{0}\\bar{D}^{*0}$ channel, while\nthe dominant decay of the 1:3D3 state should be into $D\\bar{D}$. We propose\nthat additional discrete charmonium levels can be discovered as narrow\nresonances of charmed and anticharmed mesons. \n\n"}
{"id": "hep-ph/0402008", "contents": "Title: A Mass Inequality for the $\\Xi^*$ and $\\Theta^+$ Pentaquarks Abstract: We derive an upper bound on the mass difference between the $\\Xi^*$ and\n$\\Theta^+$ pentaquarks which are the manifestly exotic members of the $SU(3)_f$\nantidecuplet. The derivation is based on simple assumptions about $SU(3)_f$\nsymmetry breaking and uses the standard quantum mechanical variational method.\nThe resulting rather robust bound is more than 20 MeV below the experimentally\nreported $\\Xi^*-\\Theta^+$ mass difference, emphasizing the need for\nconfirmation of the experimental mass values and placing strong constraints on\nquark models of the pentaquark structure. \n\n"}
{"id": "hep-ph/0402264", "contents": "Title: Neutrinos: \"...annus mirabilis\" Abstract: Main results and achievements of 2002 - 2003 in neutrino physics are\nsummarized. The field moves quickly to new phase with clear experimental and\nphenomenological programs, and with new theoretical puzzle which may lead us to\ndiscoveries of the fundamental importance. One of the main results is amazing\npattern of the lepton mixing which emerges from the data. The key questions\nare: Does lepton mixing imply new symmetry of Nature? Is the large (maximal?)\nmixing related to degeneracy of the neutrino mass spectrum? In this connection\npriorities of the future studies are formulated. \n\n"}
{"id": "hep-ph/0402297", "contents": "Title: Superconvergence Relations and Parity Violating analogue of GDH sum rule Abstract: Sum rules of superconvergence type for parity violating amplitudes (p.v.\nanalogue of Gerasimov-Drell-Hearn sum rule) are considered. Elementary\nprocesses initiated by polarized photons in the lowest order of electroweak\ntheory are calculated as examples illustrating the validity of the p.v.sum\nrules. The parity violating polarized photon-induced processes for proton\ntarget are considered in the frame of effective low energy theories and\nphenomenological models based on p.v. nucleon-meson effective interactions.\nAssuming the saturation of p.v. sum rule the possibility to limit the range of\nthe parameters, poorly known from existing experimental data and used in these\nmodels is discussed. The asymmetries for p.v. $\\pi^0$ and $\\pi^+$ production,\nmeasurable in future high intensity polarized photon beams experiments, are\ngiven. \n\n"}
{"id": "hep-ph/0403065", "contents": "Title: Constructing 5d orbifold grand unified theories from heterotic strings Abstract: A three-generation Pati-Salam model is constructed by compactifying the\nheterotic string on a particular T^6/Z_6 Abelian symmetric orbifold with two\ndiscrete Wilson lines. The compactified space is taken to be the Lie algebra\nlattice G_2+SU(3)+SO(4). When one dimension of the SO(4) lattice is large\ncompared to the string scale, this model reproduces many features of a 5d\nSO(10) grand unified theory compactified on an S^1/Z_2 orbifold. (Of course,\nwith two large extra dimensions we can obtain a 6d SO(10) grand unified\ntheory.) We identify the orbifold parities and other ingredients of the\norbifold grand unified theories in the string model. Our construction provides\na UV completion of orbifold grand unified theories, and gives new insights into\nboth field theoretical and string theoretical constructions. \n\n"}
{"id": "hep-ph/0403083", "contents": "Title: MSbar Charm Mass from Charmonium Sum Rules with Contour Improvement Abstract: A detailed error analysis is carried out for the determination of the MSbar\ncharm quark mass $\\bar m_c(\\bar m_c)$ from moments at order alpha_s^2 of the\ncharm cross section in e^+e^- annihilation. To estimate the theoretical\nuncertainties the renormalization scale is implemented in various ways\nincluding energy-dependent functions, which lead to ``contour-improved''\npredictions. We obtain $\\bar m_c(\\bar m_c)=1.29\\pm 0.07$ GeV which contains a\nsubstantial theoretical uncertainty. \n\n"}
{"id": "hep-ph/0404078", "contents": "Title: The radiative return at phi- and B-factories: FSR for muon pair\n  production at next-to-leading order Abstract: Muon pair production through the radiative return is of importance for a\nmeasurement of the hadronic production cross section in two ways: it provides\nan independent calibration and it may give rise to an important background for\na measurement of the pion form factor. With this motivation the Monte Carlo\nevent generator PHOKHARA is extended to include next-to-leading order radiative\ncorrections to the reaction $e^+e^-\\to \\mu^+\\mu^-\\gamma$. Furthermore, virtual\nISR corrections to FSR from pions are introduced, which extends the\napplicability of the generator into a new kinematical regime. Finally, the\neffect of photon vacuum polarization is introduced into this new version of the\ngenerator. \n\n"}
{"id": "hep-ph/0405264", "contents": "Title: Higgs Bosons in the Two-Doublet Model with CP Violation Abstract: We consider the effective two-Higgs-doublet potential with complex\nparameters, when the CP invariance is broken both explicitly and spontaneously.\nDiagonal mass term in the local minimum of the potential is constructed for the\nphysical basis of Higgs fields, keeping explicitly the limiting case of\nCP-conservation if the parameters are taken real. For special case of the\ntwo-doublet Higgs sector of the minimal supersymmetric model, when CP\ninvariance is violated by the Higgs bosons interaction with scalar quarks of\nthe third generation, we calculate by means of the effective pothential method\nthe Higgs boson masses and evaluate the two-fermion Higgs boson decay widths\nand the widths of rare one-loop mediated decays H -> \\gamma \\gamma, H -> gg. \n\n"}
{"id": "hep-ph/0405300", "contents": "Title: SO(10) Group Theory for the Unified Model Building Abstract: The complete tables of Clebsh-Gordan (CG) coefficients for a wide class of\nSO(10) SUSY grand unified theories (GUTs) are given. Explicit expression of all\nstates and corresponding multiplets under standard model gauge group G_{321} =\nSU(3)_C x SU(2)_L x U(1)_Y, necessary for evaluation of the CG coefficients are\npresented. The SUSY SO(10) GUT model considered here incudes most of the Higgs\nirreducible representations usually used in the literature: 10, 45, 54, 120,\n126 126-bar and 210. Mass matrices of all G_{321} multiplets are found for the\nmost general superpotential. These results are indispensible for the precision\ncalculations of the gauge coupling unification and proton decay, etc. \n\n"}
{"id": "hep-ph/0406132", "contents": "Title: Study of the eightfold degeneracy with a standard $\\beta$-Beam and a\n  Super-Beam facility Abstract: The study of the eightfold degeneracy at a neutrino complex that includes a\nstandard $\\beta$-Beam and a Super-Beam facility is presented for the first time\nin this paper. The scenario where the neutrinos are sent toward a Megaton water\nCerenkov detector located at the Fr\\'{e}jus laboratory (baseline 130 Km) is\nexploited. The performance in terms of sensitivity for measuring the continuous\n($\\theta_{13}$ and $\\delta$) and discrete (${sign} [ \\Delta m^2_{23} ]$ and\n${sign} [\\tan (2\\theta_{23}) ]$) oscillation parameters for the $\\beta$-Beam\nand Super-Beam alone, and for their combination has been studied. A brief\nreview of the present uncertainties on the neutrino and antineutrino\ncross-sections is also reported and their impact on the discovery potential\ndiscussed. \n\n"}
{"id": "hep-ph/0406208", "contents": "Title: The effects of non-universal extra dimensions on the radiative lepton\n  flavor decays \\mu\\to e\\gamma and \\tau\\to \\mu\\gamma in the two Higgs doublet\n  model Abstract: We study the effect of non-universal extra dimensions on the branching ratios\nof the lepton flavor violating processes \\mu\\to e\\gamma and \\tau\\to \\mu\\gamma\nin the general two Higgs doublet model. We observe that these effects are small\nfor a single extra dimension, however, in the case of two extra dimensions\nthere is a considerable enhancement in the additional contributions. \n\n"}
{"id": "hep-ph/0407036", "contents": "Title: A Statistical Analysis of Supersymmetric Dark Matter in the MSSM after\n  WMAP Abstract: We study supersymmetric dark matter in the general flavor diagonal MSSM by\nmeans of an extensive random scan of its parameter space. We find that, in\ncontrast with the standard mSUGRA lore, the large majority of viable models\nfeatures either a higgsino or a wino-like lightest neutralino, and yields a\nrelic abundance well below the WMAP bound. Among the models with neutralino\nrelic density within the WMAP range, higgsino-like neutralinos are still\ndominant, though a sizeable fraction of binos is also present. In this latter\ncase, relic density suppression mechanisms are shown to be essential in order\nto obtain the correct neutralino abundance. We then carry out a statistical\nanalysis and a general discussion of neutralino dark matter direct detection\nand of indirect neutralino detection at neutrino telescopes and at antimatter\nsearch experiments. We point out that current data exclude only a marginal\nportion of the viable parameter space, and that models whose thermal relic\nabundance lies in the WMAP range will be significantly probed only at future\ndirect detection experiments. Finally, we emphasize the importance of relic\ndensity enhancement mechanisms for indirect detection perspectives, in\nparticular at future antimatter search experiments. \n\n"}
{"id": "hep-ph/0407089", "contents": "Title: Non-Singlet QCD Analysis of the Structure Function F_2 in 3-Loops Abstract: First results of a non--singlet QCD analysis of the structure function\n$F_2(x,Q^2)$ in 3--loop order based on the non--singlet world data are\npresented. Correlated errors are determined and their propagation through the\nevolution equations is performed analytically. The value for $\\alpha_s(M_Z)$ is\ndetermined to be $0.1135 +/- 0.0023/0.0026$ compatible with results from other\nQCD analyses. Low moments for $u_v(x)$, $d_v(x)$ and $u_v(x) - d_v(x)$ with\ncorrelated errors are calculated which may be compared with results from\nlattice simulations. \n\n"}
{"id": "hep-ph/0408098", "contents": "Title: Z-prime Gauge Bosons at the Tevatron Abstract: We study the discovery potential of the Tevatron for a Z-prime gauge boson.\nWe introduce a parametrization of the Z-prime signal which provides a\nconvenient bridge between collider searches and specific Z-prime models. The\ncross section for p pbar -> Z-prime X -> l^+ l^- X depends primarily on the\nZ-prime mass and the Z-prime decay branching fraction into leptons times the\naverage square coupling to up and down quarks. If the quark and lepton masses\nare generated as in the standard model, then the Z-prime bosons accessible at\nthe Tevatron must couple to fermions proportionally to a linear combination of\nbaryon and lepton numbers in order to avoid the limits on Z--Z-prime mixing.\nMore generally, we present several families of U(1) extensions of the standard\nmodel that include as special cases many of the Z-prime models discussed in the\nliterature. Typically, the CDF and D0 experiments are expected to probe\nZ-prime-fermion couplings down to 0.1 for Z-prime masses in the 500--800 GeV\nrange, which in various models would substantially improve the limits set by\nthe LEP experiments. \n\n"}
{"id": "hep-ph/0409323", "contents": "Title: Electrophobic Lorentz invariance violation for neutrinos and the see-saw\n  mechanism Abstract: In this talk we show how Lorentz invariance violation (LIV) can occur for\nMajorana neutrinos, without inducing LIV in the charged leptons via radiative\ncorrections. Such ``electrophobic'' LIV is due to the Majorana nature of the\nLIV operator together with electric charge conservation. Being free from the\nstrong constraints coming from the charged lepton sector, electrophobic LIV can\nin principle be as large as current neutrino experiments permit. On the other\nhand electrophobic LIV could be naturally small if it originates from LIV in\nsome singlet ``right-handed neutrino'' sector, and is felt in the physical\nleft-handed neutrinos via a see-saw mechanism. \n\n"}
{"id": "hep-ph/0410049", "contents": "Title: Uncertainties in the Prediction of the Relic Density of Supersymmetric\n  Dark Matter Abstract: We investigate how well the relic density of dark matter can be predicted in\nmSUGRA. We determine the parameters to which the relic density is most\nsensitive and quantify the collider accuracy needed to match the accuracy of\nWMAP and PLANCK. \n\n"}
{"id": "hep-ph/0410145", "contents": "Title: Effective Theory Approach to the Skyrme model and Application to\n  Pentaquarks Abstract: The Skyrme model is reconsidered from an effective theory point of view. From\nthe most general chiral Lagrangian up to including terms of order $p^4$, $N_c$\nand $\\delta m^2$ ($\\delta m\\equiv m_s-m$), new interactions, which have never\nbeen considered before, appear upon collective coordinate quantization. We\nobtain the parameter set best fitted to the observed low-lying baryon masses,\nby performing the second order perturbative calculations with respect to\n$\\delta m$. We calculate the masses and the decay widths of the other members\nof (mainly) anti-decuplet pentaquark states. The formula for the decay widths\nis reconsidered and its baryon mass dependence is clarified. \n\n"}
{"id": "hep-ph/0410171", "contents": "Title: V_{us} from hyperon semileptonic decays Abstract: A model-independent determination of the CKM matrix element V_{us} from five\nmeasured strangeness-changing hyperon semileptonic decays is performed. Flavor\nSU(3) symmetry breaking effects in the leading vector and axial-vector form\nfactors are analyzed in the framework of the 1/N_c expansion of QCD. A fit to\nexperimental data allows one to extract the value V_{us}=0.2199\\pm 0.0026,\nwhich is comparable to the one from K_{e3} decays. This reconciliation is\nachieved through second-order symmetry breaking effects of a few percent in the\nform factors f_1, which increase their magnitudes over their SU(3) predictions. \n\n"}
{"id": "hep-ph/0412107", "contents": "Title: Analysis of B_d^0 -> phi K^{*0} decay mode with supersymmetry Abstract: Motivated by the recent measurement of low longitudinal polarization fraction\nin the decay mode $\\B \\to \\phi K^{*0}$, which appears not to be in agreement\nwith the standard model expectation, we analyze this mode in the minimal\nsupersymmetric standard model with mass insertion approximation. Within the\nstandard model, with factorization approximation, the longitudinal polarization\nis expected to be $f_L \\sim 1-{\\cal O}(1/m_b^2)$. We find that this anomaly can\nbe explained in the minimal supersymmetric standard model with either $LR$ or\n$RL$ mass insertion approximation. \n\n"}
{"id": "hep-ph/0412181", "contents": "Title: Neutrino mass bounds from cosmology Abstract: Cosmology is at present one of the most powerful probes of neutrino\nproperties. The advent of precision data from the cosmic microwave background\nand large scale structure has allowed for a very strong bound on the neutrino\nmass. Here, I review the status of cosmological bounds on neutrino properties\nwith emphasis on mass bounds on light neutrinos. \n\n"}
{"id": "hep-ph/0412287", "contents": "Title: Role of the gluons in the color screening in a QCD plasma Abstract: The color screening in a QCD plasma, that was studied in a formulation making\nevident similarities and differences with the electric case, is continued by\ntaking into account the contributions of real gluons. The results, which\ninclude a numerical analysis not previously performed, show a damping of the\ncorrelation function which, if not exponential, does not differ very much from\nthat form.\n  The role of the temperature, which affect both the population and the\ndynamics of the quark-gluon system, is found to be relevant. \n\n"}
{"id": "hep-ph/0502069", "contents": "Title: Multiplicity and Pt Correlations in AA-interactions at High Energies Abstract: The theoretical description of the correlations between observables in two\nseparated rapidity intervals in relativistic nuclear collisions is presented.\nIt is shown, that the event-by-event pt-pt correlation defined as the\ncorrelation between event mean values of transverse momenta of all particles\nemitted in two different rapidity intervals does not decrease to zero with the\nincrease of the number of strings in contrast with two particle pt-pt\ncorrelation - the correlation between the transverse momenta of single\nparticles produced in these two rapidity windows.\n  In the idealized case with the homogeneous string distribution in the\ntransverse plane in the framework of the cellular analog of string fusion model\n(SFM) the asymptotic of pt-pt correlation coefficient is analytically\ncalculated and compared with the results of the Monte-Carlo (MC) calculations\nfulfilled both in the framework of the original SFM and in the framework of its\ncellular analog, which enables to control the MC algorithms.\n  In the case with the realistic nucleon distribution density of colliding\nnuclei the results of the MC calculations of the pt-pt correlation function for\nminimum bias nuclear collisions at SPS, RHIC and LHC energies are presented and\nanalysed. \n\n"}
{"id": "hep-ph/0503045", "contents": "Title: Renormalization-group improved evolution of the meson distribution\n  amplitude at the two-loop level Abstract: We discuss the two-loop evolution of the flavor-nonsinglet meson distribution\namplitude in perturbative QCD. After reviewing previous two-loop computations,\nwe outline the incompatibility of these solutions with the group property of\nthe renormalization-group transformations. To cure this deficiency, we compute\na correction factor for the non-diagonal part of the meson evolution equation\nand prove that with this modification the two-loop solution conforms with the\ngroup properties of the renormalization-group transformations. The special case\nof a fixed strong coupling (no Q^2 dependence) is also discussed and comparison\nis given to previously obtained results. \n\n"}
{"id": "hep-ph/0505225", "contents": "Title: Boson boson scattering at LHC Abstract: We analyse some features of WW scattering processes at LHC. The severe\ncancellations between fusion diagrams and the other contributions evidence the\nnecessity of complete calculations for studying the high WW invariant mass\nregion and disentangling the standard Higgs case from new physics. \n\n"}
{"id": "hep-ph/0507013", "contents": "Title: Antihyperon polarization in high-energy inclusive reactions Abstract: We propose a model for the antihyperon polarization in high-energy\nproton-nucleus inclusive reactions, based on the final-state interactions\nbetween the antihyperons and other produced particles (predominantly pions). To\nformulate this idea, we use the previously obtained low-energy\npion-(anti-)hyperon interaction using effective chiral Lagrangians, and a\nhydrodynamic parametrization of the background matter, which expands and\ndecouples at a certain freezeout temperature. \n\n"}
{"id": "hep-ph/0507299", "contents": "Title: Perturbative approach to U_A(1) breaking Abstract: The six-quark instanton induced 't Hooft interaction is considered in\ncombination with the Nambu-Jona-Lasinio (NJL) type U_L(3)X U_R(3) chiral\nsymmetric Lagrangian. We discuss the bosonization of this multi-quark\ninteraction, taking the U_A(1) breaking as a perturbation. We discuss its\nrelation with the usual approach. \n\n"}
{"id": "hep-ph/0509127", "contents": "Title: Next-to-leading order QCD jet production with parton showers and\n  hadronization Abstract: We report on a method for matching the next-to-leading order calculation of\nQCD jet production in e+e- annihilation with a Monte Carlo parton shower event\ngenerator (MC) to produce realistic final states. The final result is accurate\nto next-to-leading order (NLO) for infrared-safe one-scale quantities, such as\nthe Durham 3-jet fraction y_3, and agrees well with parton shower results for\nmulti-scale quantities, such as the jet mass distribution in 3-jet events. For\nour numerical results, the NLO calculation is matched to the event generator\nPythia, though the method is more general. We compare one scale and multi-scale\nquantities from pure NLO, pure MC, and matched NLO-MC calculations. \n\n"}
{"id": "hep-ph/0509315", "contents": "Title: Weak radiative corrections to the Drell-Yan process for large invariant\n  mass of a dilepton pair Abstract: The weak radiative corrections to the Drell-Yan process above the Z-peak have\nbeen studied. The compact asymptotic expression for the two heavy boson\nexchange - one of the significant contributions to the investigated process -\nhas been obtained, the results expand in the powers of the Sudakov electroweak\nlogarithms. At the quark level we compare the weak radiative corrections to the\ntotal cross section and forward-backward asymmetry with the existing results\nand achieve a rather good coincidence at \\sqrt{s}>= 0.5 TeV. The numerical\nanalysis has been performed in the high energy region corresponding to the\nfuture experiments at the CERN Large Hadron Collider (LHC). To simulate the\ndetector acceptance we used the standard CMS detector cuts. It was shown that\ndouble Sudakov logarithms of the WW boxes are the dominant contributions in\nhadronic cross section. The considered radiative corrections are significant at\nhigh dilepton mass M and change the dilepton mass distribution up to ~+3(-12)%\nat the LHC energies and M=1(5) TeV. \n\n"}
{"id": "hep-ph/0510175", "contents": "Title: Neutrino Masses, Mixing and Oscillations Abstract: Basics of neutrino oscillations is discussed. Importance of time-energy\nuncertainty relation is stressed. Neutrino oscillations in the leading\napproximation and evidence for neutrino oscillations are briefly summarized. \n\n"}
{"id": "hep-ph/0510377", "contents": "Title: Charm in cosmic rays (The long-flying component of EAS cores) Abstract: Experimental data on cosmic ray cascades with enlarged attenuation lengths\n(Tien-Shan effect) are presented and analyzed in terms of charm\nhadroproduction. The very first estimates of charm hadroproduction cross\nsections from experimental data at high energies are confirmed and compared\nwith recent accelerator results. \n\n"}
{"id": "hep-ph/0511339", "contents": "Title: Elements of Physics with a Photon Collider Abstract: After a brief description of the basic principle of a photon collider, we\nsummarize the physics potential of such a facility at high energies. Unique\nopportunities are provided in supersymmetric theories for the discovery of\nheavy scalar and pseudoscalar Higgs bosons as well as selectrons and\ne-sneutrinos. \n\n"}
{"id": "hep-ph/0512020", "contents": "Title: CPT violation in the top sector Abstract: We study the viability of observation of CPT violation in the top sector at\nfuture colliders. We show possible studies and different estimates for hadronic\nand linear colliders. In particular, we will present current constraints for\nTevatron and prospects for the LHC and the ILC. \n\n"}
{"id": "hep-ph/0512090", "contents": "Title: Minimal Dark Matter Abstract: A few multiplets that can be added to the SM contain a lightest neutral\ncomponent which is automatically stable and provides allowed DM candidates with\na non-standard phenomenology. Thanks to coannihilations, a successful thermal\nabundance is obtained for well defined DM masses. The best candidate seems to\nbe a SU(2)_L fermion quintuplet with mass 4.4 TeV, accompanied by a charged\npartner 166 MeV heavier with life-time 1.8 cm, that manifests at colliders as\ncharged tracks disappearing in pi^\\pm with 97.7% branching ratio. The cross\nsection for usual NC direct DM detection is sigma_SI = f^2 1.0 10^-43 cm^2\nwhere f ~ 1 is a nucleon matrix element. We study prospects for CC direct\ndetection and for indirect detection. \n\n"}
{"id": "hep-ph/0601023", "contents": "Title: Proton stability in grand unified theories, in strings, and in branes Abstract: A broad overview of the current status of proton stability in unified models\nof particle interactions is given which includes non - supersymmetric\nunification, SUSY and SUGRA unified models, unification based on extra\ndimensions, and string-M-theory models. The extra dimensional unification\nincludes 5D and 6D and universal extra dimensional (UED) models, and models\nbased on warped geometry. Proton stability in a wide array of string theory and\nM theory models is reviewed. These include Calabi-Yau models, grand unified\nmodels with Kac-Moody levels $k>1$, a new class of heterotic string models,\nmodels based on intersecting D branes, and string landscape models. The\ndestabilizing effect of quantum gravity on the proton is discussed. The\npossibility of testing grand unified models, models based on extra dimensions\nand string-M-theory models via their distinctive modes is investigated. The\nproposed next generation proton decay experiments, HyperK, UNO, MEMPHYS,\nICARUS, LANNDD (DUSEL), and LENA would shed significant light on the nature of\nunification complementary to the physics at the LHC. Mathematical tools for the\ncomputation of proton lifetime are given in the appendices. Prospects for the\nfuture are discussed. \n\n"}
{"id": "hep-ph/0601225", "contents": "Title: Verifiable Radiative Seesaw Mechanism of Neutrino Mass and Dark Matter Abstract: A minimal extension of the Standard Model is proposed, where the observed\nleft-handed neutrinos obtain naturally small Majorana masses from a one-loop\nradiative seesaw mechanism. This model has two candidates (one bosonic and one\nfermionic) for the dark matter of the Universe. It has a very simple structure\nand should be verifiable in forthcoming experiments at the Large Hadron\nCollider. \n\n"}
{"id": "hep-ph/0602048", "contents": "Title: Systematics of heavy quarkonia from Regge trajectories on $(n,M^2)$ and\n  $(M^2,J)$ planes Abstract: In this paper we show that heavy quarckonium states, similar to light mesons,\nform Regge trajectories in $(n,M^2)$ and $(M^2,J)$ planes and the slope of\nthese trajectories is independent on the quantum numbers of the mesons. This\nfact can be useful for the prediction of the masses of heavy quarkonia and the\ndetermination of the quantum numbers of the newly discovered states. \n\n"}
{"id": "hep-ph/0602209", "contents": "Title: Dynamical mass generation in strongly coupled Quantum Electrodynamics\n  with weak magnetic fields Abstract: We study the dynamical generation of masses for fundamental fermions in\nquenched quantum electrodynamics in the presence of weak magnetic fields using\nSchwinger-Dyson equations. Contrary to the case where the magnetic field is\nstrong, in the weak field limit the coupling should exceed certain critical\nvalue in order for the generation of masses to take place, just as in the case\nwhere no magnetic field is present. The weak field limit is defined as eB <<\nm(0)^2, where m(0) is the value of the dynamically generated mass in the\nabsence of the field. We carry out a numerical analysis to study the magnetic\nfield dependence of the mass function above critical coupling and show that in\nthis regime the dynamically generated mass and the chiral condensate for the\nlowest Landau level increase proportionally to (eB)^2. \n\n"}
{"id": "hep-ph/0603034", "contents": "Title: Simulation of QED Radiation in Particle decays using the YFS Formalism Abstract: In this paper we describe a program (SOPHTY) implementing QED corrections to\ndecays in the HERWIG++ event generator. In order to resum the dominant soft\nemissions to all orders, the program is based on the YFS formalism. In\naddition, universal large collinear logarithms are included and the approach\ncan be systematically extended to incorporate exact, process specific, higher\norder corrections to decays. Due to the large number of possible decay modes\nthe program is designed to operate, as far as possible, independently of the\ndecay matrix elements. \n\n"}
{"id": "hep-ph/0603090", "contents": "Title: Production of Triply Charmed $\\Omega_{ccc}$ Baryons in $e^+e^-$\n  Annihilation Abstract: The total and differential cross sections for the production of triply\ncharmed $\\Omega_{ccc}$ baryons in $e^{+}e^{-}$ annihilation are calculated at\nthe $Z$-boson pole. \n\n"}
{"id": "hep-ph/0603095", "contents": "Title: Natural Implementation of Neutralino Dark Matter Abstract: The prediction of neutralino dark matter is generally regarded as one of the\nsuccesses of the Minimal Supersymmetric Standard Model (MSSM). However the\nsuccessful regions of parameter space allowed by WMAP and collider constraints\nare quite restricted. We discuss fine-tuning with respect to both dark matter\nand Electroweak Symmetry Breaking (EWSB) and explore regions of MSSM parameter\nspace with non-universal gaugino and third family scalar masses in which\nneutralino dark matter may be implemented naturally. In particular allowing\nnon-universal gauginos opens up the bulk region that allows Bino annihilation\nvia t-channel slepton exchange, leading to ``supernatural dark matter''\ncorresponding to no fine-tuning at all with respect to dark matter. By contrast\nwe find that the recently proposed ``well tempered neutralino'' regions involve\nsubstantial fine-tuning of MSSM parameters in order to satisfy the dark matter\nconstraints, although the fine tuning may be ameliorated if several\nannihilation channels act simultaneously. Although we have identified regions\nof ``supernatural dark matter'' in which there is no fine tuning to achieve\nsuccessful dark matter, the usual MSSM fine tuning to achieve EWSB always\nremains. \n\n"}
{"id": "hep-ph/0603188", "contents": "Title: Improved Naturalness with a Heavy Higgs: An Alternative Road to LHC\n  Physics Abstract: The quadratic divergences of the Higgs mass may be cancelled either\naccidentally or by the exchange of some new particles. Alternatively its impact\non naturalness may be weakened by raising the Higgs mass, which requires\nchanging the Standard Model below its natural cut-off. We show in detail how\nthis can be achieved, while preserving perturbativity and consistency with the\nelectroweak precision tests, by extending the Standard Model to include a\nsecond Higgs doublet that has neither a vev nor couplings to quarks and\nleptons. This Inert Doublet Model yields a perturbative and completely natural\ndescription of electroweak physics at all energies up to 1.5 TeV. The discrete\nsymmetry that yields the Inert Doublet is unbroken, so that Dark Matter may be\ncomposed of neutral inert Higgs bosons, which may have escaped detection at\nLEP2. Predictions are given for multilepton events with missing transverse\nenergy at the Large Hadron Collider, and for the direct detection of dark\nmatter. \n\n"}
{"id": "hep-ph/0605148", "contents": "Title: Limiting fragmentation in heavy-ion collisions and percolation of\n  strings Abstract: The observed limiting fragmentation of charged particle distributions in\nheavy ion collisions is difficult to explain as it does not apply to the proton\nspectrum itself. On the other hand, string percolation provides a mechanism to\nregenerate fast particles, eventually compensating the rapidity shift (energy\nloss) of the nucleons. However a delicate energy-momentum compensation is\nrequired, and in our framework we see no reason for limiting fragmentation to\nbe exact. A prediction, based on percolation arguments, is given for the\ncharged particle density in the full rapidity interval at LHC energy $(\\sqrt s\n=5500 GeV)$. \n\n"}
{"id": "hep-ph/0606202", "contents": "Title: Mirror particles and mirror matter: 50 years of speculation and search Abstract: This review describes the history of discovery of violation of spatial parity\nP, charge conjugation parity C, combined parity CP. The hypothesis of existence\nof mirror particles was called upon by its authors to restore the symmetry\nbetween left and right. The review presents the emergence and evolution of the\nconcepts ``mirror particles'' and ``mirror matter''. It could serve as a\nconcise guide to the ``mirror-land''. An important part of the review is the\nlist of about 250 references with their titles. \n\n"}
{"id": "hep-ph/0607059", "contents": "Title: micrOMEGAs2.0: a program to calculate the relic density of dark matter\n  in a generic model Abstract: micrOMEGAs2.0 is a code which calculates the relic density of a stable\nmassive particle in an arbitrary model. The underlying assumption is that there\nis a conservation law like R-parity in supersymmetry which guarantees the\nstability of the lightest odd particle. The new physics model must be\nincorporated in the notation of CalcHEP, a package for the automatic generation\nof squared matrix elements. Once this is done, all annihilation and\ncoannihilation channels are included automatically in any model. Cross-sections\nat $v=0$, relevant for indirect detection of dark matter, are also computed\nautomatically. The package includes three sample models: the minimal\nsupersymmetric standard model (MSSM), the MSSM with complex phases and the\nNMSSM. Extension to other models, including non supersymmetric models, is\ndescribed. \n\n"}
{"id": "hep-ph/0608026", "contents": "Title: Refined gluino and squark pole masses beyond leading order Abstract: The physical pole and running masses of squarks and gluinos have recently\nbeen related at two-loop order in a mass-independent renormalization scheme. I\npropose a general method for improvement of such formulas, and argue that\nbetter accuracy results. The improved version gives an imaginary part of the\npole mass that agrees exactly with the direct calculation of the physical width\nat next-to-leading order. I also find the leading three-loop contributions to\nthe gluino pole mass in the case that squarks are heavier, using effective\nfield theory and renormalization group methods. The efficacy of these\nimprovements for the gluino and squarks is illustrated with numerical examples.\nSome necessary three-loop results for gauge coupling and fermion mass beta\nfunctions and pole masses in theories with more than one type of fermion\nrepresentation, which are not directly accessible from the published\nliterature, are presented in an Appendix. \n\n"}
{"id": "hep-ph/0610207", "contents": "Title: A Double Parton Scattering Background to Associate $WH$ and $ZH$\n  Production at the LHC Abstract: Higgs boson production in association with $W$ and $Z$ bosons at high\nluminosity CERN Large Hadron Collider (LHC,$\\sqrt{s}$=14 TeV), is one of the\nmost promising discovery channel for a SM Higgs particle with a mass below 135\nGeV, where the Higgs decays into $b\\bar{b}$ final states is dominant. The\nexperimental capability of recognizing the presence of $b$ quarks in a complex\nhadronic final state has brought attention towards the final states with pairs\nfor observing the production of the Higgs at the LHC. We point out that double\nparton scattering processes are going to represent a sizable background to the\nprocess. \n\n"}
{"id": "hep-ph/0610249", "contents": "Title: Non-perturbative Effect on Thermal Relic Abundance of Dark Matter Abstract: We point out that thermal relic abundance of the dark matter is strongly\naltered by a non-perturbative effect called the Sommerfeld enhancement, when\nconstituent particles of the dark matter are non-singlet under the SU(2)_L\ngauge interaction and much heavier than the weak gauge bosons. Typical\ncandidates for such dark matter particles are the heavy wino- and higgsino-like\nneutralinos. We investigate the non-perturbative effect on the relic abundance\nof dark matter for the wino-like neutralino as an example. We show that its\nthermal abundance is reduced by 50% compared to the perturbative result. The\nwino-like neutralino mass consistent with the observed dark matter abundance\nturns out to be 2.7 TeV < m < 3.0 TeV. \n\n"}
{"id": "hep-ph/0610442", "contents": "Title: A Peculiar Dynamically Warped Theory Space Abstract: We study a supersymmetric deconstructed gauge theory in which a warp factor\nemerges dynamically, driven by Fayet-Iliopoulos terms. The model is peculiar in\nthat it possesses a global supersymmetry that remains unbroken despite\nnonvanishing D-term vacuum expectation values. Inclusion of gravity and/or\nadditional messenger fields leads to the collective breaking of supersymmetry\nand to an unusual phenomenology. \n\n"}
{"id": "hep-ph/0611052", "contents": "Title: The SUSY Flavor Problem, Proton Decay and Discrete Family Symmetry Abstract: We consider a supersymmetric extension of the standard model, which possess a\nfamily symmetry based on a binary dihedral group Q6, and investigate the\nconsequences of the family symmetry on the mixing of fermions, FCNCs and the\nstability of proton. \n\n"}
{"id": "hep-ph/0611313", "contents": "Title: Phenomenological Implications of a Class of Neutrino Mass Matrices Abstract: The generic predictions of two-texture zero neutrino mass matrices of class A\nin the flavor basis have been reexamined especially in relation to the\ndegeneracy between mass matrices of types A_1 and A_2 and interesting\nconstraints on the neutrino parameters have been obtained. It is shown that the\noctant of $\\theta_{23}$ and the quadrant of the Dirac-type CP-violating phase\n$\\delta$ can be used to lift this degeneracy. \n\n"}
{"id": "hep-ph/0612140", "contents": "Title: Windows over a New Low Energy Axion Abstract: We outline some general features of possible extensions of the Standard Model\nthat include anomalous U(1) gauge symmetries, a certain number of axions and\ntheir mixings with the CP-odd Higgs sector. As previously shown, after the\nmixing one of the axions becomes a physical pseudoscalar (the axi-Higgs) that\ncan take the role of a modified QCD axion. It can be driven to be very light by\nthe same non-perturbative effects that are held responsible for the solution of\nthe strong CP-problem. At the same time the axi-Higgs has a sizeable gauge\ninteraction, which is not allowed to the Peccei-Quinn axion, possibly\nexplaining the PVLAS results. We point out that the Wess-Zumino term, typical\nof these models, can be both interpreted as an anomaly inflow from higher\ndimensional theories (second window) but also as a result of partial decoupling\nof an extra Higgs sector (and of a fermion) that leaves behind an effective\nanomalous abelian theory (first window) in a broken St\\\"{u}ckelberg phase. The\npossibility that the axi-Higgs can be heavy, of the order of the Higgs mass or\nlarger, however, can't be excluded. The potentialities for the discovery of\nthis particle and of anomaly effects in the neutral current sector at the LHC\nare briefly discussed in the context of a superstring inspired model (second\nwindow), but with results that remain valid also if any of the two\npossibilities is realized in Nature. \n\n"}
{"id": "hep-ph/0612154", "contents": "Title: |V_us| and m_s from hadronic tau decays Abstract: Recent progress in the determination of |V_us| employing strange hadronic\ntau-decay data are reported. This includes using the recent OPAL update of the\nstrange spectral function, as well as augmenting the dimension-two perturbative\ncontribution with the recently calculated order alpha_s^3 term on the theory\nside. These updates result in |V_us| = 0.2220 +- 0.0033, with the uncertainty\npresently being dominated by experiment, and already being competitive with the\nstandard extraction from K_e3 decays and other new proposals to determine\n|V_us|. In view of the ongoing work to analyse tau-decay data at the\nB-factories BaBar and Belle, as well as future results from BESIII, the error\non |V_us| from tau decays is expected to be much reduced in the near future. \n\n"}
{"id": "hep-ph/0612275", "contents": "Title: The Inert Doublet Model: an Archetype for Dark Matter Abstract: The Inert Doublet Model (IDM), a two Higgs extension of the Standard Model\nwith an unbroken $Z_2$ symmetry, is a simple and yet rich model of dark matter.\nWe present a systematic analysis of the dark matter abundance and investigate\nthe potentialities for direct and gamma indirect detection. We show that the\nmodel should be within the range of future experiments, like GLAST and ZEPLIN.\nThe lightest stable scalar in the IDM is a perfect example, or archetype of a\nweakly interacting massive particle. \n\n"}
{"id": "hep-ph/0612290", "contents": "Title: Branching fractions, polarisation and asymmetries of B -> VV decays Abstract: We calculate the hard-scattering kernels relevant to the negative-helicity\ndecay amplitude in B decays to two vector mesons in the framework of QCD\nfactorisation. We then perform a comprehensive analysis of the 34 B->VV decays,\nincluding B_s decays and the complete set of polarisation observables. We find\nconsiderable uncertainties from weak annihilation and the non-factorisation of\nspectator-scattering. Large longitudinal polarisation is expected with\ncertainty only for a few tree-dominated colour-allowed modes, which receive\nsmall penguin and spectator-scattering contributions. This allows for an\naccurate determination of the CKM angle alpha (or gamma) from S_L(rhorho)\nresulting in alpha=(85.6^{+7.4}_{-7.3}) degrees. We also emphasize that the rho\nK* system is ideal for an investigation of electroweak penguin effects. \n\n"}
{"id": "hep-ph/0701261", "contents": "Title: SM Precision Constraints at the LHC/ILC Abstract: The prospects for electroweak precision physics at the LHC and the ILC are\nreviewed. This includes projections for measurements of the effective Z pole\nweak mixing angle, sin^2 theta_W (eff.), as well as top quark, W boson, and\nHiggs scalar properties. The upcoming years may also see very precise\ndeterminations of sin^2 theta_W (eff.) from lower energies. \n\n"}
{"id": "hep-ph/0702052", "contents": "Title: Hard Photoproduction at HERA Abstract: In view of possible photoproduction studies in ultraperipheral heavy-ion\ncollisions at the LHC, we briefly review the present theoretical understanding\nof photons and hard photoproduction processes at HERA, discussing the\nproduction of jets, light and heavy hadrons, quarkonia, and prompt photons. We\naddress in particular the extraction of the strong coupling constant from\nphoton structure function and inclusive jet measurements, the infrared safety\nand computing time of jet definitions, the sensitivity of dijet cross sections\non the parton densities in the photon, factorization breaking in diffractive\ndijet production, the treatment of the heavy-quark mass in charm production,\nthe relevance of the color-octet mechanism for quarkonium production, and\nisolation criteria for prompt photons. \n\n"}
{"id": "hep-ph/9309335", "contents": "Title: Naturalness Versus Supersymmetric Non-renormalization Theorems Abstract: We give an intuitive proof of a new non-renormalization theorem in\nsupersymmetric field theories. It applies both perturbatively and\nnon-perturbatively. The superpotential is not renormalized in perturbation\ntheory but receives non-perturbative corrections. However, these\nnon-perturbative corrections are {\\it not} generic functions of the fields\nconsistent with the symmetries. Certain invariant terms are not generated. This\nviolation of naturalness has applications to dynamical supersymmetry breaking. \n\n"}
{"id": "hep-ph/9405349", "contents": "Title: Anomalous Vector-Boson Couplings in Majorana Neutrino Models Abstract: We examine the contributions of Majorana neutrinos to CP-violating WWZ and\nZZZ self-couplings, using a model in which sterile neutrinos couple to the W\nand Z by mixing with a fourth-generation heavy lepton. We find that the induced\nform factors can be as large as 0.5%. The model satisfies all phenomenological\nbounds in a natural way, including those due to the strong limits on the\nneutron and electron electric dipole moments. Anomalous CP-odd couplings of\nthis size are unlikely to be observed at LEP200, but might be detectable at\nNLC. \n\n"}
{"id": "hep-ph/9408211", "contents": "Title: Supersymmetry Breaking by Hidden Matter Condensation in Superstrings Abstract: We show that supersymmetry can be broken mainly by hidden matter condensates\nin the observable matter direction in generic superstring models. This happens\nonly when the fields whose VEVs give masses to hidden matter do not decouple at\nthe condensation scale. We find how the parameters of the string model and the\nvacuum determine whether supersymmetry is broken mainly by hidden matter or\ngaugino condensates and in the matter or moduli directions. \n\n"}
{"id": "hep-ph/9409438", "contents": "Title: Form Factor $A_0(q^2)$, Non Leptonic $D(B) \\to PV$ Transitions and Rare\n  $B \\to K^* \\gamma$ Decays Abstract: We use three-point function QCD sum rules to calculate the form factor\n$A_0(q^2)$ appearing in the matrix element of the flavour-changing axial\ncurrent between the $D (B)$ state and a vector meson state. We describe the\nrole of this form factor in nonleptonic $D (B) \\to P V$ decays and analyze the\nlight $SU(3)_F$ symmetry breaking effects. We also discuss a proposal to relate\nthe branching ratio of $B \\to K^* \\gamma$ to the spectrum of the semileptonic\n$B \\to \\rho \\ell \\nu$ decay. \n\n"}
{"id": "hep-ph/9504310", "contents": "Title: The Chiral Phase Transition in QCD: Critical Phenomena and Long\n  Wavelength Pion Oscillations Abstract: In QCD with two massless quarks, the chiral phase transition is plausibly in\nthe same universality class as the classical O(4) magnet. To test this\nhypothesis, critical exponents characterizing the behaviour of universal\nquantities near the 2nd order critical point can be calculated and compared to\nresults from lattice simulations. Present simulations already allow many\nqualitative tests; quantitative tests await future simulations with longer\ncorrelation lengths. In a heavy ion collision, a long correlation length would\nlead to large fluctuations in the number ratio of neutral to charged pions.\nUnfortunately, no equilibrium correlation length gets long enough for this to\noccur. Modelling the dynamics of the chiral order parameter in a far from\nequilibrium transition by quenching in the linear sigma model suggests that\nlong wavelength modes of the pion field can be amplified. This could have\ndramatic phenomenological consequences. Theoretical advances include attempts\nto relax the quench approximation and to include expansion and quantum effects.\nLong wavelength pion oscillations arise in a number of theoretical treatments;\nhowever, all involve idealizations and are at best qualitative guides. It is up\nto experimentalists to determine whether such phenomena occur; detection in a\nheavy ion collision would imply an out of equilibrium chiral transition. \n\n"}
{"id": "hep-ph/9511381", "contents": "Title: Particle Production and Gravitino Abundance after Inflation Abstract: Thermal history after inflation is studied in a chaotic inflation model with\nsupersymmetric couplings of the inflaton to matter fields. Time evolution\nequation is solved in a formalism that incorporates both the back reaction of\nparticle production and the cosmological expansion. The effect of the\nparametric resonance gives rise to a rapid initial phase of the inflaton decay\nfollowed by a slow stage of the Born term decay. Thermalization takes place\nimmediately after the first explosive stage for a medium strength of the\ncoupling among created particles. As an application we calculate time evolution\nof the gravitino abundance that is produced by ordinary particles directly\ncreated from the inflaton decay, which typically results in much more enhanced\nyield than what a naive estimate based on the Born term would suggest. \n\n"}
{"id": "hep-ph/9511410", "contents": "Title: Production of B_c Mesons in Photon-Photon and Hadron-Hadron Collisions Abstract: We discuss two-photon and hadronic production of $B_c$ mesons in\nnonrelativistic bound state approximation and to lowest order in the coupling\nconstants $\\alpha$ and $\\alpha_s$. It is shown that in photon-photon\ncollisions, heavy quark fragmentation is dominated by recombination of $\\bar b$\nand $c$ quarks up to the highest accessible transverse momenta. In contrast, in\nhadroproduction, which at high energies mainly involves gluon--gluon\ncollisions, the fragmentation mechanism dominates at transverse momenta $p_T >\nm_{B_c}$, providing a simple and satisfactory approximation of the complete\n$O(\\alpha_s^4)$ results in the high-$p_T$ regime. Contradictions in previous\npublications on hadroproduction of $B_c$ mesons are clarified. We also present\npredictions for cross sections and differential distributions at present and\nfuture accelerators. \n\n"}
{"id": "hep-ph/9601279", "contents": "Title: QCD Interference Effects of Heavy Particles Below Threshold Abstract: We consider how two classes of heavy particles: extra vector-like families,\nand strongly interacting superpartners, manifest themselves below threshold, by\ninterference of virtual loops with normal QCD processes. Quantitative estimates\nare presented. \n\n"}
{"id": "hep-ph/9603347", "contents": "Title: Search for New Neutral Bosons at Future Colliders Abstract: This is a short review of present and future limits on new neutral gauge\nbosons, in particular on hadrophilic or leptophobic $Z'$s recently proposed to\ninterpret the observed fluctuations of $\\Gamma _{c,b}$ at LEP. Light gauge\nbosons coupled to lepton number differences or to baryon number are also\nexamples of the model dependence of these bounds. The mixing between the $U(1)$\nfactors plays an important role in the phenomenology of these extended\nelectroweak models. Future improvements based on the analysis of precise\nelectroweak data are emphasized. \n\n"}
{"id": "hep-ph/9604303", "contents": "Title: Four Fermions Productions at a $\\gamma\\gamma$ Collider Abstract: Using the recently proposed ALPHA algorithm (and the resulting code) I\ncompute the rate (at tree level) for the process\n$\\gamma\\gamma\\rightarrow\\bar\\nu_e e^- u \\bar d$. The bulk of the contribution\nis due to W pair production and decay. However a non negligible ($\\sim10 \\%$)\ncontribution comes from other channels, mainly the production and decay of a W\nand a collinear charged fermion. Requiring that the reconstructed invariant $u\n\\bar d$ mass lies in the intervals $M_W\\pm 5 $ GeV and $M_W\\pm 20 $ GeV one\nobtains a rate which is lower, by 25 \\% and 4 \\% respectively, than the rate\nobtained in the $narrow$ $width$ approximation, thus demonstrating the\nrelevance of the finite W width. \n\n"}
{"id": "hep-ph/9604421", "contents": "Title: Hadronic production of heavy mesons in perturbative QCD Abstract: In the framework of perturbative QCD in the fourth order over the $\\alpha_s$\ncoupling constant for the production of two pairs of quarks and in the model of\nweak binding of the quarks into meson, the analysis of the hadronic production\nof mesons, containing $b$-quark, is performed, and a minimal transverse\nmomentum is determined, so that at the momenta greater than the found one, the\nmeson differential spectrum can be reliably described in the model of\nfactorization of the hard $b$-quark production and the subsequent fragmentation\ninto the mesons. At low transverse momenta of the meson, nonfragmentational\ncontributions are essential. They are determined by the complete set of QCD\ndiagrams in the given order over $\\alpha_s$, so that the latters result in the\neffect of destructive interference with the diagrams of fragmentation at\n$z=2E/\\sqrt{s}$ close to $1$. \n\n"}
{"id": "hep-ph/9606327", "contents": "Title: Comments on Recent Measurements of R_c and R_b Abstract: Discrepancies between Standard Model predictions and experimental\nmeasurements of the fractions R_c and R_b of hadronic Z decays to charm and\nbottom are investigated. We show that there exists a discrepancy in two\ncomplementary determinations of B(Bbar --> D X). Reducing the branching ratio\nB(D0 --> K- Pi+) by about 15% from currently accepted values to 3.50+/-0.21\nremoves the discrepancy. Since B(D0 --> K- Pi+) calibrates most charmed hadron\nyields, the reduced value also eliminates the discrepancy between the predicted\nand measured values of R_c and mitigates a problem in semileptonic B decays. A\nreduction in B(D0 --> K- Pi+) would also mean that roughly 15% of all D0 and D+\ndecays have not been properly taken into account. It is shown that if the\nmissing decay modes involve multiple charged particles, they would be more\nlikely to pass the requirements for lifetime B tagging at LEP and SLC. This\nwould mean that the charm tagging efficiency in (Z --> c cbar) has been\nunderestimated. As a consequence R_b would need to be revised downward,\npotentially bringing it in line with the Standard Model prediction. \n\n"}
{"id": "hep-ph/9606389", "contents": "Title: Parton Sum Rules and Improved Scaling Variable Abstract: The effect from quark masses and transversal motion on the Gottfried,\nBjorken, and Ellis-Jaffe sum rules is examined by using a quark-parton model of\nnucleon structure functions based on an improved scaling variable. Its use\nresults in corrections to the Gottfried, Bjorken, and Ellis-Jaffe sum rules. We\nuse the Brodsky-Huang-Lepage prescription of light-cone wavefunctions to\nestimate the size of the corrections. We constrain our choice of parameters by\nthe roughly known higher twist corrections to the Bjorken sum rule and find\nthat the resulting corrections to the Gottfried and Ellis-Jaffe sum rules are\nrelevant, though not large enough to explain the observed sum rule violations. \n\n"}
{"id": "hep-ph/9608421", "contents": "Title: Combined Squark and Gluino Mass Bounds from LEP Data Abstract: Under the assumption of gaugino mass unification at a high scale, chargino\nand neutralino masses depend on the value of the gluino mass, which itself\nbecomes a function of squark masses through self-energy corrections. We\ndemonstrate that this leads to combined bounds on squark and gluino masses from\nthe limits on chargino, neutralino and Higgs boson masses obtained in the CERN\nLEP-1 and LEP-1.5 runs. These bounds turn out to be comparable to those\nobtained from direct searches at the Fermilab Tevatron and may be expected to\nimprove as LEP energies go higher. \n\n"}
{"id": "hep-ph/9609463", "contents": "Title: Naturalness Lowers the Upper Bound on the Lightest Higgs Boson Mass in\n  Supersymmetry Abstract: We quantify the extent to which naturalness is lost as experimental lower\nbounds on the Higgs boson mass increase, and we compute the natural upper bound\non the lightest supersymmetric Higgs boson mass. We find that it would be\nunnatural for the mass of the lightest supersymmetric Higgs boson to saturate\nit's maximal upper bound. In the absence of significant fine-tuning, the\nlightest Higgs boson mass should lie below $120$ GeV, and in the most natural\ncases it should be lighter than $108$ GeV. For modest $tan \\beta$, these bounds\nare significantly lower. Our results imply that a failure to observe a light\nHiggs boson in pre-LHC experiments could provide a serious challenge to the\nprincipal motivation for weak-scale supersymmetry. \n\n"}
{"id": "hep-ph/9610348", "contents": "Title: Soft particle production and QCD coherence Abstract: We discuss the behaviour of the energy spectrum of particles in jets near the\nlimit of small momenta of a few hundred MeV. In QCD parton cascades the soft\ngluons are coherently emitted from all faster partons in the jet and their\nproduction rate is predicted to scale. The observed charged and identified\nparticle spectra follow this behaviour surprisingly well supporting of the\nhypothesis of the Local Parton Hadron Duality (LPHD) for this extreme limit.\nFurther tests of this perturbative approach are discussed. \n\n"}
{"id": "hep-ph/9611287", "contents": "Title: A grand unified model with $M_G\\sim M_{string}$ and $M_I\\sim 10^{12}$\n  GeV Abstract: We present a model based on the gauge group SU(2)_L\\times SU(2)_R \\times\nSU(4)_C with gauge couplings that are found to be unified at a scale near the\nstring unification scale. This model breaks to the MSSM at an intermediate\nscale which is instrumental in producing a neutrino in a mass range that can\nserve as hot dark matter and can also solve the strong CP problem via a\nharmless invisible axion. \n\n"}
{"id": "hep-ph/9611362", "contents": "Title: Constraints on doubly charged Higgs interactions at linear collider Abstract: Production of a single doubly charged Higgs boson $Delta^{--}$ in polarized\n$e^+e^-$ and $e^+\\gamma$ collision modes of the linear collider have been\ninvestigated. The mass range of $Delta^{--}$ to be probed extends up to the\ncollision energy. The diagonal lepton number violating Yukawa coupling $h_{ee}$\nwill be tested at least three orders of magnitude more strictly than in present\nexperiments. \n\n"}
{"id": "hep-ph/9704215", "contents": "Title: Strong WW Scattering Physics: A Comparative Study for the LHC, NLC and a\n  Muon Collider Abstract: We discuss the model independent parameterization for a strongly interacting\nelectroweak sector. Phenomenological studies are made to probe such a sector\nfor future colliders such as the LHC, $e^+e^-$ Linear collider and a muon\ncollider. \n\n"}
{"id": "hep-ph/9704361", "contents": "Title: Neutralino Relic Density including Coannihilations Abstract: We evaluate the relic density of the lightest neutralino, the lightest\nsupersymmetric particle, in the Minimal Supersymmetric extension of the\nStandard Model (MSSM). For the first time, we include all coannihilation\nprocesses between neutralinos and charginos for any neutralino mass and\ncomposition. We use the most sophisticated routines for integrating the cross\nsections and the Boltzmann equation. We properly treat (sub)threshold and\nresonant annihilations. We also include one-loop corrections to neutralino\nmasses. We find that coannihilation processes are important not only for light\nhiggsino-like neutralinos, as pointed out before, but also for heavy higgsinos\nand for mixed and gaugino-like neutralinos. Indeed, coannihilations should be\nincluded whenever $|\\mu| \\lsim 2 |M_1|$, independently of the neutralino\ncomposition. When $|\\mu| \\sim |M_1|$, coannihilations can increase or decrease\nthe relic density in and out of the cosmologically interesting region. We find\nthat there is still a window of light higgsino-like neutralinos that are viable\ndark matter candidates and that coannihilations shift the cosmological upper\nbound on the neutralino mass from 3 to 7 TeV. \n\n"}
{"id": "hep-ph/9705320", "contents": "Title: CP Violation in Quasi-inclusive $B\\to K^{(*)} X$ Decays Abstract: We consider the possibility of observing CP violation in quasi-inclusive\ndecays of the type $B^-\\to K^- X$, $B^-\\to K^{*-} X$, $\\bar B^0\\to K^- X$ and\n$\\bar B^0\\to K^{*-} X$, where $X$ does not contain strange quarks. We present\nestimates of rates and asymmetries for these decays in the Standard Model and\ncomment on the experimental feasibility of observing CP violation in these\ndecays at future $B$ factories. We find the rate asymmetries can be quite\nsizeable. \n\n"}
{"id": "hep-ph/9706238", "contents": "Title: Postmodern Technicolor Abstract: Using new insights into strongly coupled gauge theories arising from analytic\ncalculations and lattice simulations, we explore a framework for technicolor\nmodel building that relies on a non-trivial infrared fixed point, and an\nessential role for QCD. Interestingly, the models lead to a simple relation\nbetween the electroweak scale and the QCD confinement scale, and to the\npossible existence of exotic leptoquarks with masses of several hundred GeV. \n\n"}
{"id": "hep-ph/9706434", "contents": "Title: Non-standard t production at the NLC Abstract: The top-quark system as a probe for new physics is considered using a\nconsistent, gauge-invariant effective Lagrangian approach. The magnitude of new\neffects is estimated and the results are applied to top production through W\nfusion. Other processes are also briefly discussed. \n\n"}
{"id": "hep-ph/9706539", "contents": "Title: Weak Decay Process of $B \\to \\rho \\ell \\bar\\nu_\\ell$: A Varying External\n  Field Approach in QCD Sum Rules Abstract: A varying external field approach in QCD sum rules is formulated in a\nsystematic way to treat the weak decay form factors and their $q^2$ dependence\nin the process of $B \\to \\rho \\ell \\bar\\nu_\\ell$. From the form factor sum\nrules, we can also obtain the mass sum rules for B and $\\rho$ mesons, which can\nhelp us determine the reliable Borel windows in studying the relevant form\nfactor sum rules. In this way, we thus demonstrate that some QCD sum rule\ncalculations in the literature are less reliable. We also include induced\ncondensate contributions, which have been ignored, into the relevant sum rules.\nWe obtain the ratios $\\Gamma(\\bar B^0\\to\\rho^+ e^- \\bar\\nu_e)$/ $\\Gamma(\\bar\nB^0\\to\\pi^+ e^- \\bar\\nu_e)$$\\approx$ 0.94 and $\\Gamma(\\bar B^0\\to\\rho^+ \\tau^-\n\\bar\\nu_\\tau)$/ $\\Gamma(\\bar B^0\\to\\pi^+ \\tau^- \\bar\\nu_\\tau)$ $\\approx$ 1.15.\nWe apply this approach to re-examine the case of the $D$ meson decay. \n\n"}
{"id": "hep-ph/9708203", "contents": "Title: Model for two generations of fermions Abstract: In the model with the spontaneous breaking of chiral gauge symmetry, the\nvacuum structure for the pair of Higgs fields can provide the introduction of\ntwo generations of fermions. The mixing matrix of charged currents is\ndetermined. \n\n"}
{"id": "hep-ph/9708351", "contents": "Title: General constraints on light resonances in a strongly coupled symmetry\n  breaking sector Abstract: In this paper we consider the information that can be obtained about a\nstrongly-interacting symmetry breaking sector from precision measurements of\nfour-fermion processes at LEP2 or a (500 GeV) NLC . Using a ``Z-peak''\nsubtracted approach to describe four-fermion processes, we show that\nmeasurements of the cross section for muon production, the related\nforward-backward asymmetry, and the total cross section for the production of\nhadrons (except $t$'s) can place constraints on (or measure the effectsof) the\nlightest vector or axial resonances present in a strong symmetry breaking\nsector. We estimate that such effects will be visible at LEP2 for resonances of\nmasses up to approximately 350 GeV, and at a 500 GeV NLC for resonances of\nmasses up to approximately 800 GeV. Multiscale models, for example, predict the\npresence of light vector and axial mesons in this mass range and their effects\ncould be probed by these measurements. \n\n"}
{"id": "hep-ph/9708399", "contents": "Title: Electroweak Baryogenesis in the Minimal Supersymmetric Standard Model Abstract: I describe work done in collaboration with M. Joyce and K. Kainulainen on (1)\nthe strength of the electroweak phase transition in the MSSM and (2) the\nmechanism for producing the baryon asymmetry during the phase transition. In\nthe former we compare the effective potential and dimensional reduction methods\nfor describing the phase transition and search the parameter space of the MSSM\nfor those values where it is strong enough. In the latter we give a systematic\ncomputation of the baryon asymmetry due the CP-violating force acting on\ncharginos in the vicinity of the bubble wall. We find that a light right-handed\nstop, a light Higgs boson, and a large phase in the mu parameter, are the main\nnecessary ingredients for producing the baryon asymmetry. \n\n"}
{"id": "hep-ph/9711301", "contents": "Title: CP Violation in Selected B Decays Abstract: We summarize the results of two papers in which we have studied CP violation\nin inclusive and exclusive decays b -> d e^+e^-. Two CP-violating effects are\ncalculated: the partial rate asymmetry between b and \\bar{b} decay, and the\nasymmetry between e^- and e^+ spectra for an untagged B/\\bar{B} mixture. These\nasymmetries, combined with the branching ratio, can potentially determine the\nparameters (\\rho, \\eta) of the unitarity triangle. We also summarize a paper by\nBrowder et al. on a possible CP-violating asymmetry in the inclusive reaction B\n-> K^-(K^{*-})X. \n\n"}
{"id": "hep-ph/9801238", "contents": "Title: Bounds on Supersymmetry from Electroweak Precision Analysis Abstract: The Standard Model global fit to precision data is excellent. The Minimal\nSupersymmetric Standard Model can also fit the data well, though not as well as\nthe Standard Model. At best, supersymmetric contributions either decouple or\nonly slightly decrease the total chi^2, at the expense of decreasing the number\nof degrees of freedom. In general, regions of parameter space with large\nsupersymmetric corrections from light superpartners are associated with poor\nfits to the data. We contrast results of a simple (oblique) approximation with\nfull one-loop results, and show that for the most important observables the\nnon-oblique corrections can be larger than the oblique corrections, and must be\ntaken into account. We elucidate the regions of parameter space in both\ngravity- and gauge-mediated models which are excluded. Significant regions of\nparameter space are excluded, especially with positive supersymmetric mass\nparameter mu. We give a complete listing of the bounds on all the superpartner\nand Higgs boson masses. For either sign of mu, and for all supersymmetric\nmodels considered, we set a lower limit on the mass of the lightest CP-even\nHiggs scalar, mh > 78 GeV. Also, the first and second generation squark masses\nare constrained to be above 280 (325) GeV in the supergravity (gauge-mediated)\nmodel. \n\n"}
{"id": "hep-ph/9801285", "contents": "Title: An Investigation of Uncertainties in the QCD NLO Predictions of the\n  Inclusive Jet Cross Section in pbarp Collisions at sqrt(s) = 1.8 TeV and 630\n  GeV Abstract: Uncertainties in the NLO calculation of the inclusive jet cross section due\nto the choice of renormalization scale, parton distribution functions and\nclustering algorithm are explored. These are found to be similar in size to the\ncurrent experimental uncertainties of the measured inclusive jet cross section\nat DZero and CDF. \n\n"}
{"id": "hep-ph/9801367", "contents": "Title: Topological Defects: Fossils from the Early Universe Abstract: In the context of current particle physics theories, it is quite likely that\ntopological defects may be present in our universe. An observation of these\nfossils from the early universe would lead to invaluable insight into cosmology\nand particle physics, while their absence provides important constraints on\nparticle-cosmology model building. I describe recent efforts to address\ncosmological issues in condensed matter systems such as He-3 and a possible\nsolution to the magnetic monopole problem due to defect interactions. (Invited\ntalk at the 1997 RESCEU Symposium, University of Tokyo.) \n\n"}
{"id": "hep-ph/9803300", "contents": "Title: Self-consistent approximations in relativistic plasmas: Quasiparticle\n  analysis of the thermodynamic properties Abstract: We generalize the concept of conserving, Phi-derivable, approximations to\nrelativistic field theories. Treating the interaction field as a dynamical\ndegree of freedom, we derive the thermodynamic potential in terms of fully\ndressed propagators, an approach which allows us to resolve the entropy of a\nrelativistic plasma into contributions from its interacting elementary\nexcitations. We illustrate the derivation for a hot relativistic system\ngoverned by electromagnetic interactions. \n\n"}
{"id": "hep-ph/9804252", "contents": "Title: Electroweak radiative corrections to b --> s gamma Abstract: Two loop electroweak corrections to b --> s gamma decays are computed.\nFermion and photonic loop effects are found to reduce R = BR(b --> s\ngamma)/BR(b --> c e nu) by 8+-2% and lead to the standard model prediction BR(B\n--> X_s gamma)=(3.28 +- 0.30)x10^-4 for inclusive B meson decays. Comparison of\nR^{theory} =(3.04 +- 0.25)x10^-3(1+0.10rho), where rho is a Wolfenstein CKM\nparameter, with the current experimental average R^{exp}=(2.52+-0.52)x10^-3\ngives rho=-1.7+-1.9 which is consistent with -0.21<rho<0.27 obtained from other\nB and K physics constraints. \n\n"}
{"id": "hep-ph/9804417", "contents": "Title: Indirect CP violation in an electroweak SU(2)left x U(1) gauge theory of\n  chiral mesons Abstract: Indirect CP violation is analyzed in the framework of the electroweak gauge\ntheory of J=0 mesons proposed in ref.[1], in which they transform like\ncomposite fermion-antifermion operators by the chiral U(N)left x U(N)right\ngroup and by the SU(2)left x U(1) gauge group of the Glashow-Salam-Weinberg\nmodel. It is shown that, in this model where, in particular, mass terms can be\nintroduced for the mesons themselves, and unlike what happens in the standard\nmodel for fermions: - electroweak mass eigenstates can differ from CP\neigenstates even in the case of two generations; - the existence of a complex\nentry in the mixing matrix for the constituent fermions is no longer a\nsufficient condition for indirect CP violation to occur at the mesonic level. \n\n"}
{"id": "hep-ph/9805497", "contents": "Title: Renormalization of Supersymmetric Theories Abstract: We review the renormalization of the electroweak sector of the standard\nmodel. The derivation also applies to the minimal supersymmetric standard\nmodel. We discuss regularization and the relation between the threshold\ncorrections and the renormalization group equations. We consider the\ncorrections to many precision observables, including MW and sin^2 theta^eff. We\nshow that global fits to the data exclude regions of supersymmetric model\nparameter space and lead to lower bounds on superpartner masses. \n\n"}
{"id": "hep-ph/9807573", "contents": "Title: Why e+ e- to T Tbar is Different Abstract: Unlike other examples of fermion pair production in e+ e- collisions, we show\nthat top-quark pairs are produced in an essentially unique spin configuration\nin polarized e+ e- colliders at all energies. Since the directions of the\nelectroweak decay products of polarized top-quarks are strongly correlated to\nthe top-quark spin axis, this unique spin configuration leads to a distinctive\ntopology for top-quark pair events which can be used to constrain anomalous\ncouplings to the top-quark. A significant interference effect between the\nlongitudinal and transverse W-bosons in the decay of polarized top-quarks is\nalso discussed. \n\n"}
{"id": "hep-ph/9808465", "contents": "Title: Grand Unification and B & L Conservation Abstract: A review of baryon and lepton conservation in supersymmetric grand unified\ntheories is given. Proton stability is discussed in the minimal SU(5)\nsupergravity grand unification and in several non-minimal extensions such as\nthe $SU(3)^3$, SO(10) and some string based models. Effects of dark matter on\nproton stability are also discussed and it is shown that the combined\nconstraints of dark matter and proton stability constrain the sparticle\nspectrum. It is also shown that proton lifetime limits put severe constraints\non the event rates in dark matter detectors. Future prospects for the\nobservation of baryon and lepton number violation are also discussed. \n\n"}
{"id": "hep-ph/9810351", "contents": "Title: A useful approximate isospin equality for charmless strange B Decays Abstract: A useful inequality is obtained if charmless strange B decays are assumed to\nbe dominated by a $\\Delta I = 0$ transition like that from the gluonic penguin\ndiagram and the contributions of all other diagrams including the tree,\nelectroweak penguin and annihilation diagrams are small but not negligible. The\ninterference contributions which are linear in these other amplitudes are\nincluded but the direct contributions which are quadratic are neglected. \n\n"}
{"id": "hep-ph/9902439", "contents": "Title: Nucleosynthesis Constraints on a Scale-Dependent New Intermediate Range\n  Interaction Abstract: We derive constraints on the strength of a new intermediate range interaction\nthat couples to baryon number from primordial nucleosynthesis yields. The\nnucleosysnthesis limits here used arise from matching observations and\npredictions of standard and inhomogeneous primordial scenarios. We show that\nthe standard nucleosynthesis scenario is more restrictive ($\\alpha_5 \\lsim\n0.2$) when the range of the interaction is greater than about 1 m. We further\ndiscuss the implications of considering the scalar particle responsible for the\nnew interaction as the main component of the dark matter in the galactic halo\nsuch that its decay can account for the ionization of hydrogen in the\ninterstellar medium and the temperature of Lyman-$\\alpha$ clouds. \n\n"}
{"id": "hep-ph/9903532", "contents": "Title: The static $Q\\bar Q$ interaction at small distances and OPE violating\n  terms Abstract: Nonperturbative contribution to the one-gluon exchange produces a universal\nlinear term in the static potential at small distances $\\Delta V=\\frac{6N_c\n\\alpha_s \\sigma r}{2\\pi}$. Its role in the resolution of long--standing\ndiscrepancies in the fine splitting of heavy quarkonia and improved agreement\nwith lattice data for static potentials is discussed, as well as implications\nfor OPE violating terms in other processes. \n\n"}
{"id": "hep-ph/9904208", "contents": "Title: Leading/nonleading charm production asymmetry in $\\Sigma^-p$\n  interactions Abstract: The asymmetries between the spectra of leading and nonleading charmed mesons\nmeasured in $\\Sigma^-A$ interactions at $p_L$= 340 GeV/c in the WA89 experiment\nare described in the framework of Quark-Gluon String Model. There are two\nversions of the model under consideration: one of them includes the sea charm\nquark-antiquark pairs and the other one does not. It's shown that the\nasymmetries between $D^-$ and $D^+$-meson spectra and between $D_s^-$ and\n$D^+_s$- meson spectra can be fitted by QGSM curves obtained with the same\nparameters as charm asymmetry in $\\pi^-A$ experiments described in previous\nstudies. The QGSM results are compared with the calculations in the\nnext-to-leading approximation of perturbative QCD approach carried out by the\nother authors. \n\n"}
{"id": "hep-ph/9905326", "contents": "Title: The electroproduction of the $\\Delta$(1232) in the chiral quark-soliton\n  model Abstract: We calculate the ratios E2/M1 and C2/M1 for the electroproduction of the\n$\\Delta$(1232) in the region of photon virtuality $0<-q^2<1$ GeV$^2$. The\nmagnetic dipole amplitude M1 is also presented. The theory used is the chiral\nquark-soliton model, which is based in the instanton vaccum of the QCD. The\ncalculations are performed in flavor SU(2) and SU(3) taking rotational\n($1/N_c$) corrections into account. The results for the ratios agree\nqualitatively with the available data, although the magnitude of both ratios\nseems to underestimate the latest experimental results. \n\n"}
{"id": "hep-ph/9906258", "contents": "Title: Quasiparticle description of deconfined matter at finite mu and T Abstract: An effective quasiparticle description of deconfined QCD thermodynamics\ncompatible with both finite temperature nonperturbative lattice data and the\nasymptotic limit is generalized to finite chemical potential. Implications for\nthe N_f = 4 flavor lattice data extended to mu \\neq 0 as well as for deconfined\nmatter with realistic quark masses are considered. \n\n"}
{"id": "hep-ph/9906266", "contents": "Title: Bounds on Kaluza-Klein excitations of the SM vector bosons from\n  electroweak tests Abstract: Within a minimal extension of the SM in 4+1 dimensions, we study how Kaluza\nKlein excitations of the SM gauge bosons affect the electroweak precision\nobservables. Asymmetries in Z decays provide the dominant bound on the\ncompactification scale M of the extra dimension. If the higgs is so light that\nwill be discovered at LEP2, we find the following 95% CL bounds: M > 3.5 TeV\n(if the higgs lives in the extra dimension) and M > 4.3 TeV (if the higgs is\nconfined to our 4 dimensions). In the second case Kaluza Klein modes give\n\"universal\"corrections and a good fit of precision data can be obtained with an\nheavier higgs (up to 500 GeV) and with a smaller M > 3.4 TeV. \n\n"}
{"id": "hep-ph/9906439", "contents": "Title: Estimation of the particle-antiparticle correlation effect for pion\n  production in heavy ion collisions Abstract: Estimation of the back-to-back pi-pi correlations arising due to evolution of\nthe pionic field in the course of pion production process is given for central\nheavy nucleus collisions at moderate energies. \n\n"}
{"id": "hep-ph/9906501", "contents": "Title: Preheating with non-minimally coupled scalar fields in higher-curvature\n  inflation models Abstract: In higher-curvature inflation models ($R+\\alpha_n R^n$), we study a\nparametric preheating of a scalar field $\\chi$ coupled non-minimally to a\nspacetime curvature $R$ ($\\xi R \\chi^2$). In the case of $R^2$-inflation model,\nefficient preheating becomes possible for rather small values of $\\xi$, i.e.\n$|\\xi|< several. Although the maximal fluctuation $\\sqrt{< \\chi^2 >}_{max}\n\\approx 2 \\times10^{17}$ GeV for $\\xi \\approx -4$ is almost the same as the\nchaotic inflation model with a non-minimally coupled $\\chi$ field, the growth\nrate of the fluctuation becomes much larger and efficient preheating is\nrealized. We also investigate preheating for $R^4$ model and find that the\nmaximal fluctuation is $\\sqrt{< \\chi^2 >}_{max} \\approx 8 \\times 10^{16}$ GeV\nfor $\\xi \\approx -35$. \n\n"}
{"id": "hep-ph/9909476", "contents": "Title: Higgs-Mediated $B^0 -> \\mu^+ \\mu^-$ in Minimal Supersymmetry Abstract: In this letter we demonstrate a new source for large flavor-changing neutral\ncurrents within the minimal supersymmetric standard model. At moderate to large\ntan(beta), it is no longer possible to diagonalize the masses of the quarks in\nthe same basis as their Yukawa couplings. This generates large flavor-violating\ncouplings of the form $\\bar b_R d_L H$ and $\\bar b_R s_L H$ where H is any of\nthe three neutral, physical Higgs bosons. These new couplings lead to rare\nprocesses in the B system such as $B^0 -> \\mu^+ \\mu^-$ decay and B-Bbar mixing.\nWe show that the latter is anomalously suppressed, while the former is in the\nexperimentally interesting range. Current limits on $B^0 -> \\mu^+ \\mu^-$\nalready provide nontrivial constraints on models of moderate to large\ntan(beta), with an observable signal possible at Run II of the Tevatron if m_A\n< 400-600 GeV, extending to the TeV range if a proposed Run III of 30/fb were\nto occur. \n\n"}
{"id": "hep-ph/9909517", "contents": "Title: Massive-Evolution Effects on Charmonium Hadroproduction Abstract: The fragmentation functions D_{a -> H}(x,mu^2) of a heavy hadron H, with mass\nm_H, satisfy the phase-space constraint D_{a - > H}(x,mu^2)=0 for x <\nm_H^2/mu^2, which is violated by the naive mu^2 evolution equations. Using\nappropriately generalized mu^2 evolution equations, we reconsider the inclusive\nhadroproduction of prompt J/psi mesons with high transverse momenta in the\nframework of the factorization formalism of nonrelativistic quantum\nchromodynamics, and determine the resulting shifts in the values of the leading\ncolour-octet matrix elements, which are fitted to data from the Fermilab\nTevatron. \n\n"}
{"id": "hep-ph/9910235", "contents": "Title: Can Geodesics in Extra Dimensions Solve the Cosmological Horizon\n  Problem? Abstract: We demonstrate a non-inflationary solution to the cosmological horizon\nproblem in scenarios in which our observable universe is confined to three\nspatial dimensions (a three-brane) embedded in a higher dimensional space. A\nsignal traveling along an extra-dimensional null geodesic may leave our\nthree-brane, travel into the extra dimensions, and subsequently return to a\ndifferent place on our three-brane in a shorter time than the time a signal\nconfined to our three-brane would take. Hence, these geodesics may connect\ndistant points which would otherwise be ``outside'' the four dimensional\nhorizon (points not in causal contact with one another). \n\n"}
{"id": "hep-ph/9910512", "contents": "Title: On the lifetime of a cold dark matter particle and the cosmological\n  diffuse photon background Abstract: We show that a Majorana heavy neutrino with a mass O(100TeV) is a good\ncandidate particle for cold dark matter. It can be responsible for the majority\nof the cosmological diffuse photon background owing to lifetime of the order of\nO(10^(25)s), dominantly fixed by the radiative two-body decay. The lifetime is\nsuppressed by two mechanisms: the leptonic GIM cancellation and the see-saw\nweak coupling suppression. As a fermion cold dark matter particle, a heavy\nneutrino favours the average mass density of the Universe constrained by the\nEinstein-Cartan cosmology. \n\n"}
{"id": "hep-ph/9912231", "contents": "Title: Three-flavor MSW solutions of the solar neutrino problem Abstract: We perform an updated phenomenological analysis of the\nMikheyev-Smirnov-Wolfenstein (MSW) solutions of the solar neutrino problem,\nassuming oscillations between two and three neutrino families. The analysis\nincludes the total rates of the Homestake, SAGE, GALLEX, Kamiokande and\nSuper-Kamiokande experiments, as well as the day-night asymmetry and the 18-bin\nenergy spectrum of Super-Kamiokande. Solutions are found at several values of\nthe theta_{13} mixing angle. Among the most interesting features, we find that\nsolar neutrino data alone put the constraint theta_{13} < 55--59 deg at 95%\nC.L., and that a fraction of the MSW solutions extends at and beyond maximal\n(nu_1,nu_2) mixing (theta_{12} > pi/4), especially if the neutrino square mass\nsplitting is in its lower range (m^2_2-m^2_1 ~ 10^{-7} eV^2) and if theta_{13}\nis nonzero. In particular, bimaximal (or nearly bimaximal) mixing is possible\nfor atmospheric and MSW solar neutrino oscillations within the stringent\nreactor bounds on theta_{13}. \n\n"}
{"id": "hep-ph/9912266", "contents": "Title: Single-Brane Cosmological Solutions with a Stable Compact Extra\n  Dimension Abstract: We consider 5-dimensional cosmological solutions of a single brane. The\ncorrect cosmology on the brane, i.e., governed by the standard 4-dimensional\nFriedmann equation, and stable compactification of the extra dimension is\nguaranteed by the existence of a non-vanishing \\hat{T}^5_5 which is\nproportional to the 4-dimensional trace of the energy-momentum tensor. We show\nthat this component of the energy-momentum tensor arises from the backreaction\nof the dilaton coupling to the brane. The same positive features are exhibited\nin solutions found in the presence of non-vanishing cosmological constants both\non the brane (\\Lambda_{br}) and in the bulk (\\Lambda_B). Moreover, the\nrestoration of the Friedmann equation, with the correct sign, takes place for\nboth signs of $\\Lambda_B$ so long as the sign of $\\Lambda_{br}$ is opposite\n$\\Lambda_B$ in order to cancel the energy densities of the two cosmological\nconstants. We further extend our single-brane thin-wall solution to allow a\nbrane with finite thickness. \n\n"}
{"id": "hep-ph/9912442", "contents": "Title: Electromagnetic Corrections to Low-Energy pi-pi Scattering Abstract: Electromagnetic corrections to the low-energy pi+ pi- -> pi0 pi0 scattering\namplitude at next-to-leading order in the chiral expansion are reviewed. Their\neffects on the corresponding scattering lengths are estimated and compared to\nthe two-loop strong interaction contributions. Contribution to the proceedings\nof the Eighth International Symposium on Meson-Nucleon Physics and the\nStructure of the Nucleon, Zuoz, Engadine, Switzerland, August 15-21 1999. \n\n"}
{"id": "hep-th/0002078", "contents": "Title: On Perturbative Gravity and Gauge Theory Abstract: We review some applications of tree-level (classical) relations between\ngravity and gauge theory that follow from string theory. Together with\n$D$-dimensional unitarity, these relations can be used to perturbatively\nquantize gravity theories, i.e. they contain the necessary information for\nobtaining loop contributions. We also review recent applications of these ideas\nshowing that N=1 D=11 supergravity diverges, and review arguments that N=8 D=4\nsupergravity is less divergent than previously thought, though it does appear\nto diverge at five loops. Finally, we describe field variables for the\nEinstein-Hilbert Lagrangian that help clarify the perturbative relationship\nbetween gravity and gauge theory. \n\n"}
{"id": "hep-th/0006064", "contents": "Title: A gauge invariant exact renormalization group II Abstract: A manifestly gauge invariant and regularized renormalization group flow\nequation is constructed for pure SU(N) gauge theory in the large N limit. In\nthis way we make precise and concrete the notion of a non-perturbative gauge\ninvariant continuum Wilsonian effective action. Manifestly gauge invariant\ncalculations may be performed, without gauge fixing, and receive a natural\ninterpretation in terms of fluctuating Wilson loops. Regularization is achieved\nby covariant higher derivatives and by embedding in a spontaneously broken\nSU(N|N) supergauge theory; the resulting heavy fermionic vectors are\nPauli-Villars fields. We prove the finiteness of this method to one loop and\nany number of external gauge fields. A duality is uncovered that changes the\nsign of the squared coupling constant. As a test of the basic formalism we\ncompute the one loop beta function, for the first time without any gauge\nfixing, and prove its universality with respect to cutoff function. \n\n"}
{"id": "hep-th/0108195", "contents": "Title: Effective lagrangians for QCD at high density Abstract: We describe low energy physics in the CFL and LOFF phases by means of\neffective lagrangians. In the CFL case we present also how to derive\nexpressions for the parameters appearing in the lagrangian via weak coupling\ncalculations taking advantage of the dimensional reduction of fermion physics\naround the Fermi surface. The Goldstone boson of the LOFF phase turns out to be\na phonon satisfying an anisotropic dispersion relation. \n\n"}
{"id": "hep-th/0304237", "contents": "Title: D-brane Dynamics and Creations of Open and Closed Strings after\n  Recommbination Abstract: A quantum-mechanical technique is used within the framework of U(2)\nsuper-Yang-Mills theory to investigate what happens in the process after\nrecombination of two D-p-branes at one angle. Two types of initial conditions\nare considered, one of which with $p=4$ is a candidate of inflation mechanism.\nIt is observed that the branes' shapes come to have three extremes due to\nlocalization of tachyon condensation. Furthermore, open string pairs connecting\nthe decaying D-branes are shown to be created; most part of the released energy\nis used to create them. It also strongly suggests that creation of closed\nstrings happens afterward. Closed strings as gravitational radiation from the\nD-branes are also shown to be created. A few speculations are also given on\nimplications of the above phenomena for an inflation model. \n\n"}
{"id": "hep-th/0305220", "contents": "Title: Super Background Field Method for N=2 SYM Abstract: The implementation of the Background Field Method (BFM) for quantum field\ntheories is analysed within the Batalin-Vilkovisky (BV) formalism. We provide a\nsystematic way of constructing general splittings of the fields into classical\nand quantum parts, such that the background transformations of the quantum\nfields are linear in the quantum variables. This leads to linear Ward-Takahashi\nidentities for the background invariance and to great simplifications in\nmultiloop computations. In addition, the gauge fixing is obtained by means of\n(anti)canonical transformations generated by the gauge-fixing fermion. Within\nthis framework we derive the BFM for the N=2 Super-Yang-Mills theory in the\nWess-Zumino gauge viewed as the twisted version of Donaldson-Witten topological\ngauge theory. We obtain the background transformations for the full BRST\ndifferential of N=2 Super-Yang-Mills (including gauge transformations, SUSY\ntransformations and translations). The BFM permits all observables of the\nsupersymmetric theory to be identified easily by computing the equivariant\ncohomology of the topological theory. These results should be regarded as a\nstep towards the construction of a super BFM for the Minimal Supersymmetric\nStandard Model. \n\n"}
{"id": "hep-th/0407043", "contents": "Title: Low Energy Supersymmetry From the Landscape Abstract: There has been some debate as to whether the landscape does or does not\npredict low energy supersymmetry. We argue that under rather mild assumptions,\nthe landscape seems to favor such breaking, quite possibly at a very low scale.\nSome of the issues which must be addressed in order to settle these questions\nare the relative frequency with which tree level and non-perturbative effects\ngenerate expectation values for auxillary fields and the superpotential, as\nwell as the likelihood of both $R$- and non-$R$ discrete or accidental\nsymmetries. Alternate scenarios with warped compactifications or large extra\ndimensions are also discussed. \n\n"}
{"id": "hep-th/0609214", "contents": "Title: Universal Reconnection of Non-Abelian Cosmic Strings Abstract: We show that local/semilocal strings in Abelian/non-Abelian gauge theories\nwith critical couplings always reconnect classically in collision, by using\nmoduli space approximation. The moduli matrix formalism explicitly identifies a\nwell-defined set of the vortex moduli parameters. Our analysis of generic\ngeodesic motion in terms of those shows right-angle scattering in head-on\ncollision of two vortices, which is known to give the reconnection of the\nstrings. \n\n"}
{"id": "hep-th/9410242", "contents": "Title: A Scheme Independent Definition of $\\Lambda_{\\rm QCD}$ Abstract: Given a renormalization scheme of QCD, one can define a mass scale\n$\\Lambda_{\\rm QCD}$ in terms of the beta function. Under a change of the\nrenormalization scheme, however, $\\Lambda_{\\rm QCD}$ changes by a\nmultiplicative constant. We introduce a scheme independent $\\Lambda_{\\rm QCD}$\nusing a connection on the space of the coupling constant. \n\n"}
{"id": "hep-th/9801027", "contents": "Title: Quark Number Fractionalization in N=2 Supersymmetric $SU(2) \\times\n  U(1)^{N_f}$ Gauge Theories Abstract: Physical quark-number charges of dyons are determined, via a formula which\ngeneralizes that of Witten for the electric charge, in N=2 supersymmetric\ntheories with $SU(2) \\times U(1)^{N_f} $ gauge group. The quark numbers of the\nmassless monopole at a nondegenerate singularity of QMS turn out to vanish in\nall cases. A puzzle related to CP invariant cases is solved. Generalization of\nour results to $SU(N_c)\\times U(1)^{N_f}$ gauge theories is straightforward. \n\n"}
{"id": "hep-th/9801146", "contents": "Title: Duality in the Presence of Supersymmetry Breaking Abstract: We study Seiberg duality for N=1 supersymmetric QCD with soft\nsupersymmetry-breaking terms. We generate the soft terms through gauge\nmediation by coupling two theories related by Seiberg duality to the same\nsupersymmetry-breaking sector. In this way, we know what a\nsupersymmetry-breaking perturbation in one theory maps into in its ``dual''.\nAssuming a canonical Kahler potential we calculate the soft terms induced in\nthe magnetic theory and find that some of the scalars acquire negative masses\nsquared. If duality is still good for small supersymmetry breaking, this may\nimply some specific symmetry breaking patterns for supersymmetric QCD with\nsmall soft supersymmetry-breaking masses, in the case that its dual theory is\nweakly coupled in the infrared. In the limit of large supersymmetry breaking,\nthe electric theory becomes ordinary QCD. However, the resulting symmetry\nbreaking in the magnetic theory is incompatible with that expected for QCD. \n\n"}
{"id": "hep-th/9806171", "contents": "Title: Glueballs and Their Kaluza-Klein Cousins Abstract: Spectra of glueball masses in non-supersymmetric Yang-Mills theory in three\nand four dimensions have recently been computed using the conjectured duality\nbetween superstring theory and large N gauge theory. The Kaluza-Klein states of\nsupergravity do not correspond to any states in the Yang-Mills theory and\ntherefore should decouple in the continuum limit. On the other hand, in the\nsupergravity limit g_{YM}^2 N -> \\infty, we find that the masses of the\nKaluza-Klein states are comparable to those of the glueballs. We also show that\nthe leading (g_{YM}^2N)^{-1} corrections do not make these states heavier than\nthe glueballs. Therefore, the decoupling of the Kaluza-Klein states is not\nevident to this order. \n\n"}
{"id": "hep-th/9811172", "contents": "Title: Four-point functions in N=4 supersymmetric Yang-Mills theory at two\n  loops Abstract: Four-point functions of gauge-invariant operators in D=4, N=4 supersymmetric\nYang-Mills theory are studied using N=2 harmonic superspace perturbation\ntheory. The results are expressed in terms of differential operators acting on\na scalar two loop integral. The leading singular behaviour is obtained in the\nlimit that two of the points approach one another. We find logarithmic\nsingularities which do not cancel out in the sum of all diagrams. It is\nconfirmed that Green's functions of analytic operators are indeed analytic at\nthis order in perturbation theory. \n\n"}
{"id": "nucl-ex/0509011", "contents": "Title: Single Electron Transverse Momentum and Azimuthal Anisotropy\n  Distributions: Charm Hadron Production at RHIC Abstract: Quantum Chromodynamics (QCD) is a basic gauge field theory to describe strong\ninteractions. Lattice QCD calculations predict a phase transition from hadronic\nmatter to a deconfined, locally thermalized Quark-Gluon Plasma (QGP) state at\nhigh temperature and small baryon density. Plenty of exciting results from RHIC\nexperiments in the first three years have demonstrated that a hot dense matter\nwith strong collective motion which cannot be described with hadronic degrees\nof freedom was created at RHIC. Charm quarks are believed to be mostly created\nfrom initial gluon fusion in heavy ion collisions. Since they are massive,\ncharm hadrons are proposed to be ideal probes to study the early stage dynamics\nin heavy ion collisions.\n  We provide here an indirect measurement of charm semi-leptonic decay. Single\nelectron transverse momentum ($p_T$) distributions from 200 GeV \\dAu, \\pp\ncollisions and 62.4 GeV \\AuAu collisions, and single electron azimuthal\nanisotropy ($v_2$) from 62.4 GeV \\AuAu collisions are presented. (Abridged) \n\n"}
{"id": "nucl-th/0104023", "contents": "Title: Chemical freeze-out parameters at RHIC from microscopic model\n  calculations Abstract: The relaxation of hot nuclear matter to an equilibrated state in the central\nzone of heavy-ion collisions at energies from AGS to RHIC is studied within the\nmicroscopic UrQMD model. It is found that the system reaches the\n(quasi)equilibrium stage for the period of 10-15 fm/$c$. Within this time the\nmatter in the cell expands nearly isentropically with the entropy to baryon\nratio $S/A = 150 - 170$. Thermodynamic characteristics of the system at AGS and\nat SPS energies at the endpoints of this stage are very close to the parameters\nof chemical and thermal freeze-out extracted from the thermal fit to\nexperimental data. Predictions are made for the full RHIC energy $\\sqrt{s} =\n200$ AGeV. The formation of a resonance-rich state at RHIC energies is\ndiscussed. \n\n"}
{"id": "nucl-th/0204035", "contents": "Title: One-loop corrections to omega photoproduction near threshold Abstract: One-loop corrections to $\\omega$ photoproduction near threshold have been\ninvestigated by using the approximation that all relevant transition amplitudes\nare calculated from the tree diagrams of effective Lagrangians. With the\nparameters constrained by the data of $\\gamma N \\to \\pi N$, $\\gamma N \\to \\rho\nN$, and $\\pi N \\to \\omega N$ reactions, it is found that the one-loop effects\ndue to the intermediate $\\pi N$ and $\\rho N$ states can significantly change\nthe differential cross sections and spin observables. The results from this\nexploratory investigation suggest strongly that the coupled-channel effects\nshould be taken into account in extracting reliable resonance parameters from\nthe data of vector meson photoproduction in the resonance region. \n\n"}
{"id": "nucl-th/0301078", "contents": "Title: Bound Nucleon Form Factors, Quark-Hadron Duality, and Nuclear EMC Effect Abstract: We discuss the electromagnetic form factors, axial form factors, and\nstructure functions of a bound nucleon in the quark-meson coupling (QMC) model.\nFree space nucleon form factors are calculated using the improved cloudy bag\nmodel (ICBM). After describing finite nuclei and nuclear matter in the\nquark-based QMC model, we compute the in-medium modification of the bound\nnucleon form factors in the same framework. Finally, limits on the medium\nmodification of the bound nucleon $F_2$ structure function are obtained using\nthe calculated in-medium electromagnetic form factors and local quark-hadron\nduality. \n\n"}
{"id": "nucl-th/0304046", "contents": "Title: Higher-order calculations of electron-deuteron scattering in nuclear\n  effective theory Abstract: Motivated by recent advances in the application of effective field theory\ntechniques to light nuclei we revisit the problem of electron-deuteron\nscattering in these approaches. By sidestepping problems with the description\nof electron-nucleon scattering data in effective field theories, we show that\nthe effective theory expansion for deuteron physics converges well over a wide\nrange of momentum transfers. The resultant description of the physics of the\ntwo-nucleon system is good up to virtual photon momenta of order 700 MeV. \n\n"}
{"id": "nucl-th/0410070", "contents": "Title: Recent issues in hadron spectroscopy Abstract: A brief survey is presented of recently discovered hadrons, some of them\npresumably demonstrating a new kind of internal structure. This includes :\nspin-singlet quarkonium, mesons with unexpected mass or width, baryons with two\nheavy quarks, and pentaquark candidates. Flavour configurations with a\ncombination of light and heavy quarks appear as particularly promising. \n\n"}
{"id": "nucl-th/0504004", "contents": "Title: Strange form factors and Chiral Perturbation Theory Abstract: We review the contributions of Chiral Perturbation Theory to the theoretical\nunderstanding or not-quite-yet-understanding of the nucleon matrix elements of\nthe strange vector current. \n\n"}
{"id": "nucl-th/0512027", "contents": "Title: Deconfinement, naturalness and the nuclear-quark equation of state Abstract: Baryon-loops vacuum contribution in renormalized models like the Linear sigma\nmodel and the Walecka model give rise to large unnatural interaction\ncoefficients, indicating that the quantum vacuum is not adequately described by\nlong-range degrees of freedom. We extend such models into nonrenormalizable\nclass by introducing an ultraviolet cutoff into the model definition and treat\nthe Dirac-sea explicitly. In this way, one can avoid unnaturalness. We\ncalculate the equation of state for symmetric nuclear matter at zero\ntemperature in a modified $\\sigma-\\omega$ model. We show that the strong\nattraction originating from the Dirac-sea softens the nuclear matter equation\nof state and generates a vacuum with dynamically broken symmetry. In this model\nthe vector-meson is important for the description of normal nuclear matter, but\nit obstructs the chiral phase transition. We investigate the chiral phase\ntransition in this model by incorporating deconfinement at high density. A\nfirst-order quark deconfinement is simulated by changing the active degrees of\nfreedom from nucleons to quarks at high density. We show that the chiral phase\ntransition is first-order when quark decouples from the vector-meson and\ncoincides with the deconfinement critical density. \n\n"}
{"id": "nucl-th/0601071", "contents": "Title: Hadron Physics and Dyson-Schwinger Equations Abstract: Detailed investigations of the structure of hadrons are essential for\nunderstanding how matter is constructed from the quarks and gluons of QCD, and\namongst the questions posed to modern hadron physics, three stand out. What is\nthe rigorous, quantitative mechanism responsible for confinement? What is the\nconnection between confinement and dynamical chiral symmetry breaking? And are\nthese phenomena together sufficient to explain the origin of more than 98% of\nthe mass of the observable universe? Such questions may only be answered using\nthe full machinery of nonperturbative relativistic quantum field theory. These\nlecture notes provide an introduction to the application of Dyson-Schwinger\nequations in this context, and a perspective on progress toward answering these\nkey questions. \n\n"}
{"id": "nucl-th/0702013", "contents": "Title: Scaling of $v_2$ in heavy ion collisions Abstract: We interpret the scaling of the corrected elliptic flow parameter w.r.t. the\ncorrected multiplicity, observed to hold in heavy ion collisions for a wide\nvariety of energies and system sizes. We use dimensional analysis and\npower-counting arguments to place constraints on the changes of initial\nconditions in systems with different center of mass energy $\\sqrt{s}$.\nSpecifically, we show that a large class of changes in the (initial) equation\nof state, mean free path, and longitudinal geometry over the observed\n$\\sqrt{s}$ are likely to spoil the scaling in $v_2$ observed experimentally. We\ntherefore argue that the system produced at most Super Proton Synchrotron (SPS)\nand Relativistic Heavy Ion Collider (RHIC) energies is fundamentally the same\nas far as the soft and approximately thermalized degrees of freedom are\nconsidered. The ``sQGP'' (Strongly interacting Quark-Gluon Plasma) phase, if it\nis there, is therefore not exclusive to RHIC. We suggest, as a goal for further\nlow-energy heavy ion experiments, to search for a ``transition'' $\\sqrt{s}$\nwhere the observed scaling breaks. \n\n"}
{"id": "nucl-th/9807056", "contents": "Title: Infrared Behaviour of Propagators and Vertices Abstract: We elucidate constraints imposed by confinement and dynamical chiral symmetry\nbreaking on the infrared behaviour of the dressed-quark and -gluon propagators,\nand dressed-quark-gluon vertex. In covariant gauges the dressing of the gluon\npropagator is completely specified by P(k^2):= 1/[1+Pi(k^2)], where Pi(k^2) is\nthe vacuum polarisation. In the absence of particle-like singularities in the\ndressed-quark-gluon vertex, extant proposals for the dressed-gluon propagator\nthat manifest P(k^2=0)=0 and Max[P(k^2)]~10 neither confine quarks nor break\nchiral symmetry dynamically. This class includes all existing estimates of\nP(k^2) via numerical simulations. \n\n"}
{"id": "physics/9611016", "contents": "Title: A Theory of Measurement Uncertainty Based on Conditional Probability Abstract: A theory of measurement uncertainty is presented, which, since it is based\nexclusively on the Bayesian approach and on the subjective concept of\nconditional probability, is applicable in the most general cases.\n  The recent International Organization for Standardization (ISO)\nrecommendation on measurement uncertainty is reobtained as the limit case in\nwhich linearization is meaningful and one is interested only in the best\nestimates of the quantities and in their variances. \n\n"}
