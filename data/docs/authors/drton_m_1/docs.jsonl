{"id": "0704.0945", "contents": "Title: Gibbs fragmentation trees Abstract: We study fragmentation trees of Gibbs type. In the binary case, we identify\nthe most general Gibbs-type fragmentation tree with Aldous' beta-splitting\nmodel, which has an extended parameter range $\\beta>-2$ with respect to the\n${\\rm beta}(\\beta+1,\\beta+1)$ probability distributions on which it is based.\nIn the multifurcating case, we show that Gibbs fragmentation trees are\nassociated with the two-parameter Poisson--Dirichlet models for exchangeable\nrandom partitions of $\\mathbb {N}$, with an extended parameter range\n$0\\le\\alpha\\le1$, $\\theta\\ge-2\\alpha$ and $\\alpha<0$, $\\theta =-m\\alpha$, $m\\in\n\\mathbb {N}$. \n\n"}
{"id": "0705.1701", "contents": "Title: Universality results for largest eigenvalues of some sample covariance\n  matrix ensembles Abstract: For sample covariance matrices with iid entries with sub-Gaussian tails, when\nboth the number of samples and the number of variables become large and the\nratio approaches to one, it is a well-known result of A. Soshnikov that the\nlimiting distribution of the largest eigenvalue is same as the of Gaussian\nsamples. In this paper, we extend this result to two cases. The first case is\nwhen the ratio approaches to an arbitrary finite value. The second case is when\nthe ratio becomes infinity or arbitrarily small. \n\n"}
{"id": "0705.2605", "contents": "Title: Sample eigenvalue based detection of high dimensional signals in white\n  noise using relatively few samples Abstract: We present a mathematically justifiable, computationally simple, sample\neigenvalue based procedure for estimating the number of high-dimensional\nsignals in white noise using relatively few samples. The main motivation for\nconsidering a sample eigenvalue based scheme is the computational simplicity\nand the robustness to eigenvector modelling errors which are can adversely\nimpact the performance of estimators that exploit information in the sample\neigenvectors.\n  There is, however, a price we pay by discarding the information in the sample\neigenvectors; we highlight a fundamental asymptotic limit of sample eigenvalue\nbased detection of weak/closely spaced high-dimensional signals from a limited\nsample size. This motivates our heuristic definition of the effective number of\nidentifiable signals which is equal to the number of \"signal\" eigenvalues of\nthe population covariance matrix which exceed the noise variance by a factor\nstrictly greater than 1+sqrt(Dimensionality of the system/Sample size). The\nfundamental asymptotic limit brings into sharp focus why, when there are too\nfew samples available so that the effective number of signals is less than the\nactual number of signals, underestimation of the model order is unavoidable (in\nan asymptotic sense) when using any sample eigenvalue based detection scheme,\nincluding the one proposed herein. The analysis reveals why adding more sensors\ncan only exacerbate the situation. Numerical simulations are used to\ndemonstrate that the proposed estimator consistently estimates the true number\nof signals in the dimension fixed, large sample size limit and the effective\nnumber of identifiable signals in the large dimension, large sample size limit. \n\n"}
{"id": "0706.0791", "contents": "Title: A volume inequality for quantum Fisher information and the uncertainty\n  principle Abstract: Let $A_1,...,A_N$ be complex self-adjoint matrices and let $\\rho$ be a\ndensity matrix. The Robertson uncertainty principle $$ det(Cov_\\rho(A_h,A_j))\n\\geq det(- \\frac{i}{2} Tr(\\rho [A_h,A_j])) $$ gives a bound for the quantum\ngeneralized covariance in terms of the commutators $[A_h,A_j]$. The right side\nmatrix is antisymmetric and therefore the bound is trivial (equal to zero) in\nthe odd case $N=2m+1$.\n  Let $f$ be an arbitrary normalized symmetric operator monotone function and\nlet $<\\cdot, \\cdot >_{\\rho,f}$ be the associated quantum Fisher information. In\nthis paper we conjecture the inequality $$ det (Cov_\\rho(A_h,A_j)) \\geq det\n(\\frac{f(0)}{2} < i[\\rho, A_h],i[\\rho,A_j] >_{\\rho,f}) $$ that gives a\nnon-trivial bound for any natural number $N$ using the commutators $i[\\rho,\nA_h]$. The inequality has been proved in the cases $N=1,2$ by the joint efforts\nof many authors. In this paper we prove the case N=3 for real matrices. \n\n"}
{"id": "0707.1231", "contents": "Title: A Robertson-type Uncertainty Principle and Quantum Fisher Information Abstract: Let $A_1,...,A_N$ be complex selfadjoint matrices and let $\\rho$ be a density\nmatrix. The Robertson uncertainty principle $$ det (Cov_\\rho(A_h,A_j)) \\geq det\n(- \\frac{i}{2} Tr (\\rho [A_h,A_j])) $$ gives a bound for the quantum\ngeneralized covariance in terms of the commutators $ [A_h,A_j]$. The right side\nmatrix is antisymmetric and therefore the bound is trivial (equal to zero) in\nthe odd case $N=2m+1$.\n  Let $f$ be an arbitrary normalized symmetric operator monotone function and\nlet $<\\cdot, \\cdot >_{\\rho,f}$ be the associated quantum Fisher information. In\nthis paper we prove the inequality $$ det (Cov_\\rho (A_h,A_j)) \\geq det\n(\\frac{f(0)}{2} < i[\\rho, A_h],i[\\rho,A_j] >_{\\rho,f}) $$ that gives a\nnon-trivial bound for any $N \\in {\\mathbb N}$ using the commutators\n$[\\rho,A_h]$. \n\n"}
{"id": "0708.1580", "contents": "Title: Optimal Causal Inference: Estimating Stored Information and\n  Approximating Causal Architecture Abstract: We introduce an approach to inferring the causal architecture of stochastic\ndynamical systems that extends rate distortion theory to use causal\nshielding---a natural principle of learning. We study two distinct cases of\ncausal inference: optimal causal filtering and optimal causal estimation.\n  Filtering corresponds to the ideal case in which the probability distribution\nof measurement sequences is known, giving a principled method to approximate a\nsystem's causal structure at a desired level of representation. We show that,\nin the limit in which a model complexity constraint is relaxed, filtering finds\nthe exact causal architecture of a stochastic dynamical system, known as the\ncausal-state partition. From this, one can estimate the amount of historical\ninformation the process stores. More generally, causal filtering finds a graded\nmodel-complexity hierarchy of approximations to the causal architecture. Abrupt\nchanges in the hierarchy, as a function of approximation, capture distinct\nscales of structural organization.\n  For nonideal cases with finite data, we show how the correct number of\nunderlying causal states can be found by optimal causal estimation. A\npreviously derived model complexity control term allows us to correct for the\neffect of statistical fluctuations in probability estimates and thereby avoid\nover-fitting. \n\n"}
{"id": "0708.2321", "contents": "Title: Fast learning rates for plug-in classifiers Abstract: It has been recently shown that, under the margin (or low noise) assumption,\nthere exist classifiers attaining fast rates of convergence of the excess Bayes\nrisk, that is, rates faster than $n^{-1/2}$. The work on this subject has\nsuggested the following two conjectures: (i) the best achievable fast rate is\nof the order $n^{-1}$, and (ii) the plug-in classifiers generally converge more\nslowly than the classifiers based on empirical risk minimization. We show that\nboth conjectures are not correct. In particular, we construct plug-in\nclassifiers that can achieve not only fast, but also super-fast rates, that is,\nrates faster than $n^{-1}$. We establish minimax lower bounds showing that the\nobtained rates cannot be improved. \n\n"}
{"id": "0709.3413", "contents": "Title: Deconvolution for an atomic distribution Abstract: Let $X_1,...,X_n$ be i.i.d. observations, where $X_i=Y_i+\\sigma Z_i$ and\n$Y_i$ and $Z_i$ are independent. Assume that unobservable $Y$'s are distributed\nas a random variable $UV,$ where $U$ and $V$ are independent, $U$ has a\nBernoulli distribution with probability of zero equal to $p$ and $V$ has a\ndistribution function $F$ with density $f.$ Furthermore, let the random\nvariables $Z_i$ have the standard normal distribution and let $\\sigma>0.$ Based\non a sample $X_1,..., X_n,$ we consider the problem of estimation of the\ndensity $f$ and the probability $p.$ We propose a kernel type deconvolution\nestimator for $f$ and derive its asymptotic normality at a fixed point. A\nconsistent estimator for $p$ is given as well. Our results demonstrate that our\nestimator behaves very much like the kernel type deconvolution estimator in the\nclassical deconvolution problem. \n\n"}
{"id": "0709.4297", "contents": "Title: Modulated Branching Processes, Origins of Power Laws and Queueing\n  Duality Abstract: Power law distributions have been repeatedly observed in a wide variety of\nsocioeconomic, biological and technological areas. In many of the observations,\ne.g., city populations and sizes of living organisms, the objects of interest\nevolve due to the replication of their many independent components, e.g.,\nbirths-deaths of individuals and replications of cells. Furthermore, the rates\nof the replication are often controlled by exogenous parameters causing periods\nof expansion and contraction, e.g., baby booms and busts, economic booms and\nrecessions, etc. In addition, the sizes of these objects often have reflective\nlower boundaries, e.g., cities do not fall bellow a certain size, low income\nindividuals are subsidized by the government, companies are protected by\nbankruptcy laws, etc.\n  Hence, it is natural to propose reflected modulated branching processes as\ngeneric models for many of the preceding observations. Indeed, our main results\nshow that the proposed mathematical models result in power law distributions\nunder quite general polynomial Gartner-Ellis conditions, the generality of\nwhich could explain the ubiquitous nature of power law distributions. In\naddition, on a logarithmic scale, we establish an asymptotic equivalence\nbetween the reflected branching processes and the corresponding multiplicative\nones. The latter, as recognized by Goldie (1991), is known to be dual to\nqueueing/additive processes. We emphasize this duality further in the\ngenerality of stationary and ergodic processes. \n\n"}
{"id": "0710.5749", "contents": "Title: On the Laplace transform of some quadratic forms and the exact\n  distribution of the sample variance from a gamma or uniform parent\n  distribution Abstract: From a suitable integral representation of the Laplace transform of a\npositive semi-definite quadratic form of independent real random variables with\nnot necessarily identical densities a univariate integral representation is\nderived for the cumulative distribution function of the sample variance of\ni.i.d. random variables with a gamma density, supplementing former formulas of\nthe author. Furthermore, from the above Laplace transform Fourier series are\nobtained for the density and the distribution function of the sample variance\nof i.i.d. random variables with a uniform distribution. This distribution can\nbe applied e.g. to a statistical test for a scale parameter. \n\n"}
{"id": "0711.2119", "contents": "Title: Goodness-of-fit Tests for high-dimensional Gaussian linear models Abstract: Let $(Y,(X_i)_{i\\in\\mathcal{I}})$ be a zero mean Gaussian vector and $V$ be a\nsubset of $\\mathcal{I}$. Suppose we are given $n$ i.i.d. replications of the\nvector $(Y,X)$. We propose a new test for testing that $Y$ is independent of\n$(X_i)_{i\\in \\mathcal{I}\\backslash V}$ conditionally to $(X_i)_{i\\in V}$\nagainst the general alternative that it is not. This procedure does not depend\non any prior information on the covariance of $X$ or the variance of $Y$ and\napplies in a high-dimensional setting. It straightforwardly extends to test the\nneighbourhood of a Gaussian graphical model. The procedure is based on a model\nof Gaussian regression with random Gaussian covariates. We give non asymptotic\nproperties of the test and we prove that it is rate optimal (up to a possible\n$\\log(n)$ factor) over various classes of alternatives under some additional\nassumptions. Besides, it allows us to derive non asymptotic minimax rates of\ntesting in this setting. Finally, we carry out a simulation study in order to\nevaluate the performance of our procedure. \n\n"}
{"id": "0712.1922", "contents": "Title: Prediction of long memory processes on same-realisation Abstract: For the class of stationary Gaussian long memory processes, we study some\nproperties of the least-squares predictor of X_{n+1} based on (X_n, ..., X_1).\nThe predictor is obtained by projecting X_{n+1} onto the finite past and the\ncoefficients of the predictor are estimated on the same realisation. First we\nprove moment bounds for the inverse of the empirical covariance matrix. Then we\ndeduce an asymptotic expression of the mean-squared error. In particular we\ngive a relation between the number of terms used to estimate the coefficients\nand the number of past terms used for prediction, which ensures the L^2-sense\nconvergence of the predictor. Finally we prove a central limit theorem when our\npredictor converges to the best linear predictor based on all the past. \n\n"}
{"id": "0712.2881", "contents": "Title: From quasi-entropy to skew information Abstract: This paper gives an overview about particular quasi-entropies, generalized\nquantum covariances, quantum Fisher informations, skew-informations and their\nrelations. The point is the dependence on operator monotone functions. It is\nproven that a skew-information is the Hessian of a quasi-entropy. The\nskew-information and some inequalities are extended to a von Neumann algebra\nsetting. \n\n"}
{"id": "0802.1406", "contents": "Title: Two simple sufficient conditions for FDR control Abstract: We show that the control of the false discovery rate (FDR) for a multiple\ntesting procedure is implied by two coupled simple sufficient conditions. The\nfirst one, which we call ``self-consistency condition'', concerns the algorithm\nitself, and the second, called ``dependency control condition'' is related to\nthe dependency assumptions on the $p$-value family. Many standard multiple\ntesting procedures are self-consistent (e.g. step-up, step-down or step-up-down\nprocedures), and we prove that the dependency control condition can be\nfulfilled when choosing correspondingly appropriate rejection functions, in\nthree classical types of dependency: independence, positive dependency (PRDS)\nand unspecified dependency. As a consequence, we recover earlier results\nthrough simple and unifying proofs while extending their scope to several\nregards: weighted FDR, $p$-value reweighting, new family of step-up procedures\nunder unspecified $p$-value dependency and adaptive step-up procedures. We give\nadditional examples of other possible applications. This framework also allows\nfor defining and studying FDR control for multiple testing procedures over a\ncontinuous, uncountable space of hypotheses. \n\n"}
{"id": "0802.1517", "contents": "Title: On the $\\ell_1-\\ell_q$ Regularized Regression Abstract: In this paper we consider the problem of grouped variable selection in\nhigh-dimensional regression using $\\ell_1-\\ell_q$ regularization ($1\\leq q \\leq\n\\infty$), which can be viewed as a natural generalization of the\n$\\ell_1-\\ell_2$ regularization (the group Lasso). The key condition is that the\ndimensionality $p_n$ can increase much faster than the sample size $n$, i.e.\n$p_n \\gg n$ (in our case $p_n$ is the number of groups), but the number of\nrelevant groups is small. The main conclusion is that many good properties from\n$\\ell_1-$regularization (Lasso) naturally carry on to the $\\ell_1-\\ell_q$ cases\n($1 \\leq q \\leq \\infty$), even if the number of variables within each group\nalso increases with the sample size. With fixed design, we show that the whole\nfamily of estimators are both estimation consistent and variable selection\nconsistent under different conditions. We also show the persistency result with\nrandom design under a much weaker condition. These results provide a unified\ntreatment for the whole family of estimators ranging from $q=1$ (Lasso) to\n$q=\\infty$ (iCAP), with $q=2$ (group Lasso)as a special case. When there is no\ngroup structure available, all the analysis reduces to the current results of\nthe Lasso estimator ($q=1$). \n\n"}
{"id": "0803.2111", "contents": "Title: Asymptotic properties of false discovery rate controlling procedures\n  under independence Abstract: We investigate the performance of a family of multiple comparison procedures\nfor strong control of the False Discovery Rate ($\\mathsf{FDR}$). The\n$\\mathsf{FDR}$ is the expected False Discovery Proportion ($\\mathsf{FDP}$),\nthat is, the expected fraction of false rejections among all rejected\nhypotheses. A number of refinements to the original Benjamini-Hochberg\nprocedure [1] have been proposed, to increase power by estimating the\nproportion of true null hypotheses, either implicitly, leading to one-stage\nadaptive procedures [4, 7] or explicitly, leading to two-stage adaptive (or\nplug-in) procedures [2, 21]. We use a variant of the stochastic process\napproach proposed by Genovese and Wasserman [11] to study the fluctuations of\nthe $\\mathsf{FDP}$ achieved with each of these procedures around its\nexpectation, for independent tested hypotheses. We introduce a framework for\nthe derivation of generic Central Limit Theorems for the $\\mathsf{FDP}$ of\nthese procedures, characterizing the associated regularity conditions, and\ncomparing the asymptotic power of the various procedures. We interpret recently\nproposed one-stage adaptive procedures [4, 7] as fixed points in the iteration\nof well known two-stage adaptive procedures [2, 21]. \n\n"}
{"id": "0803.3134", "contents": "Title: Discussion: A tale of three cousins: Lasso, L2Boosting and Dantzig Abstract: Discussion of ``The Dantzig selector: Statistical estimation when $p$ is much\nlarger than $n$'' by Emmanuel Candes and Terence Tao [math/0506081] \n\n"}
{"id": "0803.3136", "contents": "Title: Rejoinder: The Dantzig selector: Statistical estimation when $p$ is much\n  larger than $n$ Abstract: Rejoinder to ``The Dantzig selector: Statistical estimation when $p$ is much\nlarger than $n$'' [math/0506081] \n\n"}
{"id": "0803.3913", "contents": "Title: The Reverse of The Law of Large Numbers Abstract: The Law of Large Numbers tells us that as the sample size (N) is increased,\nthe sample mean converges on the population mean, provided that the latter\nexists. In this paper, we investigate the opposite effect: keeping the sample\nsize fixed while increasing the number of outcomes (M) available to a discrete\nrandom variable. We establish sufficient conditions for the variance of the\nsample mean to increase monotonically with the number of outcomes, such that\nthe sample mean ``diverges'' from the population mean, acting like an\n``reverse'' to the law of large numbers. These results, we believe, are\nrelevant to many situations which require sampling of statistics of certain\nfinite discrete random variables. \n\n"}
{"id": "0804.2310", "contents": "Title: Bounds for the loss probability in large loss queueing systems Abstract: Let $\\mathcal{G}(\\frak{g}_1,\\frak{g}_2)$ be the class of all probability\ndistribution functions of positive random variables having the given first two\nmoments $\\frak{g}_1$ and $\\frak{g}_2$. Let $G_1(x)$ and $G_2(x)$ be two\nprobability distribution functions of this class satisfying the condition\n$|G_1(x)-G_2(x)|<\\epsilon$ for some small positive value $\\epsilon$ and let\n$\\widehat{G}_1(s)$ and, respectively, $\\widehat{G}_2(s)$ denote their\nLaplace-Stieltjes transforms. For real $\\mu$ satisfying $\\mu\\frak{g}_1>1$ let\nus denote by $\\gamma_{G_1}$ and $\\gamma_{G_2}$ the least positive roots of the\nequations $z=\\widehat{G}_1(\\mu-\\mu z)$ and $z=\\widehat{G}_2(\\mu-\\mu z)$\nrespectively. In the paper, the upper bound for $|\\gamma_{G_1}-\\gamma_{G_2}|$\nis derived. This upper bound is then used to find lower and upper bounds for\nthe loss probabilities in different large loss queueing systems. \n\n"}
{"id": "0806.2426", "contents": "Title: Estimation of conditional laws given an extreme component Abstract: Let $(X,Y)$ be a bivariate random vector. The estimation of a probability of\nthe form $P(Y\\leq y \\mid X >t) $ is challenging when $t$ is large, and a\nfruitful approach consists in studying, if it exists, the limiting conditional\ndistribution of the random vector $(X,Y)$, suitably normalized, given that $X$\nis large. There already exists a wide literature on bivariate models for which\nthis limiting distribution exists. In this paper, a statistical analysis of\nthis problem is done. Estimators of the limiting distribution (which is assumed\nto exist) and the normalizing functions are provided, as well as an estimator\nof the conditional quantile function when the conditioning event is extreme.\nConsistency of the estimators is proved and a functional central limit theorem\nfor the estimator of the limiting distribution is obtained. The small sample\nbehavior of the estimator of the conditional quantile function is illustrated\nthrough simulations. \n\n"}
{"id": "0806.4014", "contents": "Title: Compound real Wishart and q-Wishart matrices Abstract: We introduce a family of matrices with non-commutative entries that\ngeneralize the classical real Wishart matrices.\n  With the help of the Brauer product, we derive a non-asymptotic expression\nfor the moments of traces of monomials in such matrices; the expression is\nquite similar to the formula derived in our previous work for independent\ncomplex Wishart matrices. We then analyze the fluctuations about the\nMarchenko-Pastur law. We show that after centering by the mean, traces of real\nsymmetric polynomials in q-Wishart matrices converge in distribution, and we\nidentify the asymptotic law as the normal law when q=1, and as the semicircle\nlaw when q=0. \n\n"}
{"id": "0806.4048", "contents": "Title: About the maximal rank of 3-tensors over the real and the complex number\n  field Abstract: High dimensional array data, tensor data, is becoming important in recent\ndays. Then maximal rank of tensors is important in theory and applications. In\nthis paper we consider the maximal rank of 3 tensors. It can be attacked from\nvarious viewpoints, however, we trace the method of Atkinson-Stephens(1979) and\nAtkinson-Lloyd(1980). They treated the problem in the complex field, and we\nwill present various bounds over the real field by proving several lemmas and\npropositions, which is real counterparts of their results. \n\n"}
{"id": "0807.0528", "contents": "Title: Asymptotic analysis for bifurcating autoregressive processes via a\n  martingale approach Abstract: We study the asymptotic behavior of the least squares estimators of the\nunknown parameters of bifurcating autoregressive processes. Under very weak\nassumptions on the driven noise of the process, namely conditional pair-wise\nindependence and suitable moment conditions, we establish the almost sure\nconvergence of our estimators together with the quadratic strong law and the\ncentral limit theorem. All our analysis relies on non-standard asymptotic\nresults for martingales. \n\n"}
{"id": "0807.1106", "contents": "Title: Principal components analysis for sparsely observed correlated\n  functional data using a kernel smoothing approach Abstract: In this paper, we consider the problem of estimating the covariance kernel\nand its eigenvalues and eigenfunctions from sparse, irregularly observed, noise\ncorrupted and (possibly) correlated functional data. We present a method based\non pre-smoothing of individual sample curves through an appropriate kernel. We\nshow that the naive empirical covariance of the pre-smoothed sample curves\ngives highly biased estimator of the covariance kernel along its diagonal. We\nattend to this problem by estimating the diagonal and off-diagonal parts of the\ncovariance kernel separately. We then present a practical and efficient method\nfor choosing the bandwidth for the kernel by using an approximation to the\nleave-one-curve-out cross validation score. We prove that under standard\nregularity conditions on the covariance kernel and assuming i.i.d. samples, the\nrisk of our estimator, under $L^2$ loss, achieves the optimal nonparametric\nrate when the number of measurements per curve is bounded. We also show that\neven when the sample curves are correlated in such a way that the noiseless\ndata has a separable covariance structure, the proposed method is still\nconsistent and we quantify the role of this correlation in the risk of the\nestimator. \n\n"}
{"id": "0807.3469", "contents": "Title: Nonparametric estimation of the characteristic triplet of a discretely\n  observed L\\'evy process Abstract: Given a discrete time sample $X_1,... X_n$ from a L\\'evy process\n$X=(X_t)_{t\\geq 0}$ of a finite jump activity, we study the problem of\nnonparametric estimation of the characteristic triplet $(\\gamma,\\sigma^2,\\rho)$\ncorresponding to the process $X.$ Based on Fourier inversion and kernel\nsmoothing, we propose estimators of $\\gamma,\\sigma^2$ and $\\rho$ and study\ntheir asymptotic behaviour. The obtained results include derivation of upper\nbounds on the mean square error of the estimators of $\\gamma$ and $\\sigma^2$\nand an upper bound on the mean integrated square error of an estimator of\n$\\rho.$ \n\n"}
{"id": "0808.3194", "contents": "Title: Nonparametric goodness-of fit testing in quantum homodyne tomography\n  with noisy data Abstract: In the framework of quantum optics, we study the problem of goodness-of-fit\ntesting in a severely ill-posed inverse problem. A novel testing procedure is\nintroduced and its rates of convergence are investigated under various\nsmoothness assumptions. The procedure is derived from a projection-type\nestimator, where the projection is done in $\\mathbb{L}_2$ distance on some\nsuitably chosen pattern functions. The proposed methodology is illustrated with\nsimulated data sets. \n\n"}
{"id": "0811.2097", "contents": "Title: A Central Limit Theorem, and related results, for a two-color randomly\n  reinforced urn Abstract: We prove a Central Limit Theorem for the sequence of random compositions of a\ntwo-color randomly reinforced urn. As a consequence, we are able to show that\nthe distribution of the urn limit composition has no point masses. \n\n"}
{"id": "0811.3355", "contents": "Title: Approximate Bayesian computation (ABC) gives exact results under the\n  assumption of model error Abstract: Approximate Bayesian computation (ABC) or likelihood-free inference\nalgorithms are used to find approximations to posterior distributions without\nmaking explicit use of the likelihood function, depending instead on simulation\nof sample data sets from the model. In this paper we show that under the\nassumption of the existence of a uniform additive model error term, ABC\nalgorithms give exact results when sufficient summaries are used. This\ninterpretation allows the approximation made in many previous application\npapers to be understood, and should guide the choice of metric and tolerance in\nfuture work. ABC algorithms can be generalized by replacing the 0-1 cut-off\nwith an acceptance probability that varies with the distance of the simulated\ndata from the observed data. The acceptance density gives the distribution of\nthe error term, enabling the uniform error usually used to be replaced by a\ngeneral distribution. This generalization can also be applied to approximate\nMarkov chain Monte Carlo algorithms. In light of this work, ABC algorithms can\nbe seen as calibration techniques for implicit stochastic models, inferring\nparameter values in light of the computer model, data, prior beliefs about the\nparameter values, and any measurement or model errors. \n\n"}
{"id": "0902.0552", "contents": "Title: Corrections to LRT on Large Dimensional Covariance Matrix by RMT Abstract: In this paper, we give an explanation to the failure of two likelihood ratio\nprocedures for testing about covariance matrices from Gaussian populations when\nthe dimension is large compared to the sample size. Next, using recent central\nlimit theorems for linear spectral statistics of sample covariance matrices and\nof random F-matrices, we propose necessary corrections for these LR tests to\ncope with high-dimensional effects. The asymptotic distributions of these\ncorrected tests under the null are given. Simulations demonstrate that the\ncorrected LR tests yield a realized size close to nominal level for both\nmoderate p (around 20) and high dimension, while the traditional LR tests with\nchi-square approximation fails. Another contribution from the paper is that for\ntesting the equality between two covariance matrices, the proposed correction\napplies equally for non-Gaussian populations yielding a valid pseudo-likelihood\nratio test. \n\n"}
{"id": "0902.1495", "contents": "Title: The binomial ideal of the intersection axiom for conditional\n  probabilities Abstract: The binomial ideal associated with the intersection axiom of conditional\nprobability is shown to be radical and is expressed as intersection of toric\nprime ideals. This resolves a conjecture in algebraic statistics due to\nCartwright and Engstr\\\"om. \n\n"}
{"id": "0902.3130", "contents": "Title: Risk Bounds for CART Classifiers under a Margin Condition Abstract: Risk bounds for Classification and Regression Trees (CART, Breiman et. al.\n1984) classifiers are obtained under a margin condition in the binary\nsupervised classification framework. These risk bounds are obtained\nconditionally on the construction of the maximal deep binary tree and permit to\nprove that the linear penalty used in the CART pruning algorithm is valid under\na margin condition. It is also shown that, conditionally on the construction of\nthe maximal tree, the final selection by test sample does not alter\ndramatically the estimation accuracy of the Bayes classifier. In the two-class\nclassification framework, the risk bounds that are proved, obtained by using\npenalized model selection, validate the CART algorithm which is used in many\ndata mining applications such as Biology, Medicine or Image Coding. \n\n"}
{"id": "0902.4058", "contents": "Title: On the Bennett-Hoeffding inequality Abstract: The well-known Bennett-Hoeffding bound for sums of independent random\nvariables is refined, by taking into account truncated third moments, and at\nthat also improved by using, instead of the class of all increasing exponential\nfunctions, the much larger class of all generalized moment functions f such\nthat f and f\" are increasing and convex. It is shown that the resulting bounds\nhave certain optimality properties. Comparisons with related known bounds are\ngiven. The results can be extended in a standard manner to (the maximal\nfunctions of) (super)martingales. \n\n"}
{"id": "0903.0623", "contents": "Title: Some Diffusion Processes Associated With Two Parameter Poisson-Dirichlet\n  Distribution and Dirichlet Process Abstract: The two parameter Poisson-Dirichlet distribution $PD(\\alpha,\\theta)$ is the\ndistribution of an infinite dimensional random discrete probability. It is a\ngeneralization of Kingman's Poisson-Dirichlet distribution. The two parameter\nDirichlet process $\\Pi_{\\alpha,\\theta,\\nu_0}$ is the law of a pure atomic\nrandom measure with masses following the two parameter Poisson-Dirichlet\ndistribution. In this article we focus on the construction and the properties\nof the infinite dimensional symmetric diffusion processes with respective\nsymmetric measures $PD(\\alpha,\\theta)$ and $\\Pi_{\\alpha,\\theta,\\nu_0}$. The\nmethods used come from the theory of Dirichlet forms. \n\n"}
{"id": "0903.1223", "contents": "Title: Sparse Regression Learning by Aggregation and Langevin Monte-Carlo Abstract: We consider the problem of regression learning for deterministic design and\nindependent random errors. We start by proving a sharp PAC-Bayesian type bound\nfor the exponentially weighted aggregate (EWA) under the expected squared\nempirical loss. For a broad class of noise distributions the presented bound is\nvalid whenever the temperature parameter $\\beta$ of the EWA is larger than or\nequal to $4\\sigma^2$, where $\\sigma^2$ is the noise variance. A remarkable\nfeature of this result is that it is valid even for unbounded regression\nfunctions and the choice of the temperature parameter depends exclusively on\nthe noise level. Next, we apply this general bound to the problem of\naggregating the elements of a finite-dimensional linear space spanned by a\ndictionary of functions $\\phi_1,...,\\phi_M$. We allow $M$ to be much larger\nthan the sample size $n$ but we assume that the true regression function can be\nwell approximated by a sparse linear combination of functions $\\phi_j$. Under\nthis sparsity scenario, we propose an EWA with a heavy tailed prior and we show\nthat it satisfies a sparsity oracle inequality with leading constant one.\nFinally, we propose several Langevin Monte-Carlo algorithms to approximately\ncompute such an EWA when the number $M$ of aggregated functions can be large.\nWe discuss in some detail the convergence of these algorithms and present\nnumerical experiments that confirm our theoretical findings. \n\n"}
{"id": "0903.3002", "contents": "Title: Learning with Structured Sparsity Abstract: This paper investigates a new learning formulation called structured\nsparsity, which is a natural extension of the standard sparsity concept in\nstatistical learning and compressive sensing. By allowing arbitrary structures\non the feature set, this concept generalizes the group sparsity idea that has\nbecome popular in recent years. A general theory is developed for learning with\nstructured sparsity, based on the notion of coding complexity associated with\nthe structure. It is shown that if the coding complexity of the target signal\nis small, then one can achieve improved performance by using coding complexity\nregularization methods, which generalize the standard sparse regularization.\nMoreover, a structured greedy algorithm is proposed to efficiently solve the\nstructured sparsity problem. It is shown that the greedy algorithm\napproximately solves the coding complexity optimization problem under\nappropriate conditions. Experiments are included to demonstrate the advantage\nof structured sparsity over standard sparsity on some real applications. \n\n"}
{"id": "0904.0966", "contents": "Title: Exact Asymptotics of Bivariate Scale Mixture Distributions Abstract: Let (RU_1, R U_2) be a given bivariate scale mixture random vector, with R>0\nbeing independent of the bivariate random vector (U_1,U_2). In this paper we\nderive exact asymptotic expansions of the tail probability P{RU_1> x, RU_2>\nax}, a \\in (0,1] as x tends infintiy assuming that R has distribution function\nin the Gumbel max-domain of attraction and (U_1,U_2) has a specific tail\nbehaviour around some absorbing point. As a special case of our results we\nretrieve the exact asymptotic behaviour of bivariate polar random vectors. We\napply our results to investigate the asymptotic independence and the asymptotic\nbehaviour of conditional excess for bivariate scale mixture distributions. \n\n"}
{"id": "0904.1148", "contents": "Title: Calibration of thresholding rules for Poisson intensity estimation Abstract: In this paper, we deal with the problem of calibrating thresholding rules in\nthe setting of Poisson intensity estimation. By using sharp concentration\ninequalities, oracle inequalities are derived and we establish the optimality\nof our estimate up to a logarithmic term. This result is proved under mild\nassumptions and we do not impose any condition on the support of the signal to\nbe estimated. Our procedure is based on data-driven thresholds. As usual, they\ndepend on a threshold parameter $\\gamma$ whose optimal value is hard to\nestimate from the data. Our main concern is to provide some theoretical and\nnumerical results to handle this issue. In particular, we establish the\nexistence of a minimal threshold parameter from the theoretical point of view:\ntaking $\\gamma<1$ deteriorates oracle performances of our procedure. In the\nsame spirit, we establish the existence of a maximal threshold parameter and\nour theoretical results point out the optimal range $\\gamma\\in[1,12]$. Then, we\nlead a numerical study that shows that choosing $\\gamma$ larger than 1 but\nclose to 1 is a fairly good choice. Finally, we compare our procedure with\nclassical ones revealing the harmful role of the support of functions when\nestimated by classical procedures. \n\n"}
{"id": "0905.0989", "contents": "Title: Adaptive tests of homogeneity for a Poisson process Abstract: We propose to test the homogeneity of a Poisson process observed on a finite\ninterval. In this framework, we first provide lower bounds for the uniform\nseparation rates in $\\mathbb{L}^2$ norm over classical Besov bodies and weak\nBesov bodies. Surprisingly, the obtained lower bounds over weak Besov bodies\ncoincide with the minimax estimation rates over such classes. Then we construct\nnon asymptotic and nonparametric testing procedures that are adaptive in the\nsense that they achieve, up to a possible logarithmic factor, the optimal\nuniform separation rates over various Besov bodies simultaneously. These\nprocedures are based on model selection and thresholding methods. We finally\ncomplete our theoretical study with a Monte Carlo evaluation of the power of\nour tests under various alternatives. \n\n"}
{"id": "0906.0874", "contents": "Title: A Jacobian inequality for gradient maps on the sphere and its\n  application to directional statistics Abstract: In the field of optimal transport theory, an optimal map is known to be a\ngradient map of a potential function satisfying cost-convexity. In this paper,\nthe Jacobian determinant of a gradient map is shown to be log-concave with\nrespect to a convex combination of the potential functions when the underlying\nmanifold is the sphere and the cost function is the distance squared. The proof\nuses the non-negative cross-curvature property of the sphere recently\nestablished by Kim and McCann, and Figalli and Rifford. As an application to\nstatistics, a new family of probability densities on the sphere is defined in\nterms of cost-convex functions. The log-concave property of the likelihood\nfunction follows from the inequality. \n\n"}
{"id": "0906.3124", "contents": "Title: Model selection by resampling penalization Abstract: In this paper, a new family of resampling-based penalization procedures for\nmodel selection is defined in a general framework. It generalizes several\nmethods, including Efron's bootstrap penalization and the leave-one-out\npenalization recently proposed by Arlot (2008), to any exchangeable weighted\nbootstrap resampling scheme. In the heteroscedastic regression framework,\nassuming the models to have a particular structure, these resampling penalties\nare proved to satisfy a non-asymptotic oracle inequality with leading constant\nclose to 1. In particular, they are asympotically optimal. Resampling penalties\nare used for defining an estimator adapting simultaneously to the smoothness of\nthe regression function and to the heteroscedasticity of the noise. This is\nremarkable because resampling penalties are general-purpose devices, which have\nnot been built specifically to handle heteroscedastic data. Hence, resampling\npenalties naturally adapt to heteroscedasticity. A simulation study shows that\nresampling penalties improve on V-fold cross-validation in terms of final\nprediction error, in particular when the signal-to-noise ratio is not large. \n\n"}
{"id": "0907.0077", "contents": "Title: Stability for random measures, point processes and discrete semigroups Abstract: Discrete stability extends the classical notion of stability to random\nelements in discrete spaces by defining a scaling operation in a randomised\nway: an integer is transformed into the corresponding binomial distribution.\nSimilarly defining the scaling operation as thinning of counting measures we\ncharacterise the corresponding discrete stability property of point processes.\nIt is shown that these processes are exactly Cox (doubly stochastic Poisson)\nprocesses with strictly stable random intensity measures. We give spectral and\nLePage representations for general strictly stable random measures without\nassuming their independent scattering. As a consequence, spectral\nrepresentations are obtained for the probability generating functional and void\nprobabilities of discrete stable processes. An alternative cluster\nrepresentation for such processes is also derived using the so-called Sibuya\npoint processes, which constitute a new family of purely random point\nprocesses. The obtained results are then applied to explore stable random\nelements in discrete semigroups, where the scaling is defined by means of\nthinning of a point process on the basis of the semigroup. Particular examples\ninclude discrete stable vectors that generalise discrete stable random\nvariables and the family of natural numbers with the multiplication operation,\nwhere the primes form the basis. \n\n"}
{"id": "0907.0676", "contents": "Title: Central Limit Theorems for Multicolor Urns with Dominated Colors Abstract: An urn contains balls of d colors. At each time, a ball is drawn and then\nreplaced together with a random number of balls of the same color. Assuming\nthat some colors are dominated by others, we prove central limit theorems. Some\nstatistical applications are discussed. \n\n"}
{"id": "0907.3454", "contents": "Title: Generalized density clustering Abstract: We study generalized density-based clustering in which sharply defined\nclusters such as clusters on lower-dimensional manifolds are allowed. We show\nthat accurate clustering is possible even in high dimensions. We propose two\ndata-based methods for choosing the bandwidth and we study the stability\nproperties of density clusters. We show that a simple graph-based algorithm\nsuccessfully approximates the high density clusters. \n\n"}
{"id": "0908.1530", "contents": "Title: Equivariant Groebner bases and the Gaussian two-factor model Abstract: Exploiting symmetry in Groebner basis computations is difficult when the\nsymmetry takes the form of a group acting by automorphisms on monomials in\nfinitely many variables. This is largely due to the fact that the group\nelements, being invertible, cannot preserve a term order. By contrast, inspired\nby work of Aschenbrenner and Hillar, we introduce the concept of equivariant\nGroebner basis in a setting where a_monoid_ acts by_homomorphisms_ on monomials\nin potentially infinitely many variables. We require that the action be\ncompatible with a term order, and under some further assumptions derive a\nBuchberger-type algorithm for computing equivariant Groebner bases. Using this\nalgorithm and the monoid of strictly increasing functions N -> N we prove that\nthe kernel of the ring homomorphism R[y_{ij} | i,j in N, i > j] -> R[s_i,t_i |\ni in N], y_{ij} -> s_i s_j + t_i t_j is generated by two types of polynomials:\noff-diagonal 3x3-minors and pentads. This confirms a conjecture by Drton,\nSturmfels, and Sullivant on the Gaussian two-factor model from algebraic\nstatistics. \n\n"}
{"id": "0908.1736", "contents": "Title: Finiteness of small factor analysis models Abstract: We consider small factor analysis models with one or two factors. Fixing the\nnumber of factors, we prove a finiteness result about the covariance matrix\nparameter space when the size of the covariance matrix increases. According to\nthis result, there exists a distinguished matrix size starting at which one can\ndetermine whether a given covariance matrix belongs to the parameter space by\ndetermining whether all principal submatrices of the distinguished size belong\nto the corresponding parameter space. We show that the distinguished matrix\nsize is equal to four in the one-factor model and six with two factors. \n\n"}
{"id": "0908.1767", "contents": "Title: Power-enhanced multiple decision functions controlling family-wise error\n  and false discovery rates Abstract: Improved procedures, in terms of smaller missed discovery rates (MDR), for\nperforming multiple hypotheses testing with weak and strong control of the\nfamily-wise error rate (FWER) or the false discovery rate (FDR) are developed\nand studied. The improvement over existing procedures such as the \\v{S}id\\'ak\nprocedure for FWER control and the Benjamini--Hochberg (BH) procedure for FDR\ncontrol is achieved by exploiting possible differences in the powers of the\nindividual tests. Results signal the need to take into account the powers of\nthe individual tests and to have multiple hypotheses decision functions which\nare not limited to simply using the individual $p$-values, as is the case, for\nexample, with the \\v{S}id\\'ak, Bonferroni, or BH procedures. They also enhance\nunderstanding of the role of the powers of individual tests, or more precisely\nthe receiver operating characteristic (ROC) functions of decision processes, in\nthe search for better multiple hypotheses testing procedures. A\ndecision-theoretic framework is utilized, and through auxiliary randomizers the\nprocedures could be used with discrete or mixed-type data or with rank-based\nnonparametric tests. This is in contrast to existing $p$-value based procedures\nwhose theoretical validity is contingent on each of these $p$-value statistics\nbeing stochastically equal to or greater than a standard uniform variable under\nthe null hypothesis. Proposed procedures are relevant in the analysis of\nhigh-dimensional \"large $M$, small $n$\" data sets arising in the natural,\nphysical, medical, economic and social sciences, whose generation and creation\nis accelerated by advances in high-throughput technology, notably, but not\nlimited to, microarray technology. \n\n"}
{"id": "0908.1940", "contents": "Title: Asymptotic distribution and sparsistency for l1-penalized parametric\n  M-estimators with applications to linear SVM and logistic regression Abstract: Since its early use in least squares regression problems, the l1-penalization\nframework for variable selection has been employed in conjunction with a wide\nrange of loss functions encompassing regression, classification and survival\nanalysis. While a well developed theory exists for the l1-penalized least\nsquares estimates, few results concern the behavior of l1-penalized estimates\nfor general loss functions. In this paper, we derive two results concerning\npenalized estimates for a wide array of penalty and loss functions. Our first\nresult characterizes the asymptotic distribution of penalized parametric\nM-estimators under mild conditions on the loss and penalty functions in the\nclassical setting (fixed-p-large-n). Our second result explicits necessary and\nsufficient generalized irrepresentability (GI) conditions for l1-penalized\nparametric M-estimates to consistently select the components of a model\n(sparsistency) as well as their sign (sign consistency). In general, the GI\nconditions depend on the Hessian of the risk function at the true value of the\nunknown parameter. Under Gaussian predictors, we obtain a set of conditions\nunder which the GI conditions can be re-expressed solely in terms of the second\nmoment of the predictors. We apply our theory to contrast l1-penalized SVM and\nlogistic regression classifiers and find conditions under which they have the\nsame behavior in terms of their model selection consistency (sparsistency and\nsign consistency). Finally, we provide simulation evidence for the theory based\non these classification examples. \n\n"}
{"id": "0908.3767", "contents": "Title: Asymptotic expansion of the minimum covariance determinant estimators Abstract: In arXiv:0907.0079 by Cator and Lopuhaa, an asymptotic expansion for the MCD\nestimators is established in a very general framework. This expansion requires\nthe existence and non-singularity of the derivative in a first-order Taylor\nexpansion. In this paper, we prove the existence of this derivative for\nmultivariate distributions that have a density and provide an explicit\nexpression. Moreover, under suitable symmetry conditions on the density, we\nshow that this derivative is non-singular. These symmetry conditions include\nthe elliptically contoured multivariate location-scatter model, in which case\nwe show that the minimum covariance determinant (MCD) estimators of\nmultivariate location and covariance are asymptotically equivalent to a sum of\nindependent identically distributed vector and matrix valued random elements,\nrespectively. This provides a proof of asymptotic normality and a precise\ndescription of the limiting covariance structure for the MCD estimators. \n\n"}
{"id": "0908.4400", "contents": "Title: Theoretical properties of the log-concave maximum likelihood estimator\n  of a multidimensional density Abstract: We present theoretical properties of the log-concave maximum likelihood\nestimator of a density based on an independent and identically distributed\nsample in $\\mathbb{R}^d$. Our study covers both the case where the true\nunderlying density is log-concave, and where this model is misspecified. We\nbegin by showing that for a sequence of log-concave densities, convergence in\ndistribution implies much stronger types of convergence -- in particular, it\nimplies convergence in Hellinger distance and even in certain exponentially\nweighted total variation norms. In our main result, we prove the existence and\nuniqueness of a log-concave density that minimises the Kullback--Leibler\ndivergence from the true density over the class all log-concave densities, and\nalso show that the log-concave maximum likelihood estimator converges almost\nsurely in these exponentially weighted total variation norms to this minimiser.\nIn the case of a correctly specified model, this demonstrates a strong type of\nconsistency for the estimator; in a misspecified model, it shows that the\nestimator converges to the log-concave density that is closest in the\nKullback--Leibler sense to the true density. \n\n"}
{"id": "0909.5216", "contents": "Title: Learning Gaussian Tree Models: Analysis of Error Exponents and Extremal\n  Structures Abstract: The problem of learning tree-structured Gaussian graphical models from\nindependent and identically distributed (i.i.d.) samples is considered. The\ninfluence of the tree structure and the parameters of the Gaussian distribution\non the learning rate as the number of samples increases is discussed.\nSpecifically, the error exponent corresponding to the event that the estimated\ntree structure differs from the actual unknown tree structure of the\ndistribution is analyzed. Finding the error exponent reduces to a least-squares\nproblem in the very noisy learning regime. In this regime, it is shown that the\nextremal tree structure that minimizes the error exponent is the star for any\nfixed set of correlation coefficients on the edges of the tree. If the\nmagnitudes of all the correlation coefficients are less than 0.63, it is also\nshown that the tree structure that maximizes the error exponent is the Markov\nchain. In other words, the star and the chain graphs represent the hardest and\nthe easiest structures to learn in the class of tree-structured Gaussian\ngraphical models. This result can also be intuitively explained by correlation\ndecay: pairs of nodes which are far apart, in terms of graph distance, are\nunlikely to be mistaken as edges by the maximum-likelihood estimator in the\nasymptotic regime. \n\n"}
{"id": "0911.2502", "contents": "Title: Ergodicity and Gaussianity for Spherical Random Fields Abstract: We investigate the relationship between ergodicity and asymptotic Gaussianity\nof isotropic spherical random fields, in the high-resolution (or\nhigh-frequency) limit. In particular, our results suggest that under a wide\nvariety of circumstances the two conditions are equivalent, i.e. the sample\nangular power spectrum may converge to the population value if and only if the\nunderlying field is asymptotically Gaussian, in the high frequency sense. These\nfindings may shed some light on the role of Cosmic Variance in Cosmic Microwave\nBackground (CMB) radiation data analysis. \n\n"}
{"id": "0911.2919", "contents": "Title: Kullback-Leibler aggregation and misspecified generalized linear models Abstract: In a regression setup with deterministic design, we study the pure\naggregation problem and introduce a natural extension from the Gaussian\ndistribution to distributions in the exponential family. While this extension\nbears strong connections with generalized linear models, it does not require\nidentifiability of the parameter or even that the model on the systematic\ncomponent is true. It is shown that this problem can be solved by constrained\nand/or penalized likelihood maximization and we derive sharp oracle\ninequalities that hold both in expectation and with high probability. Finally\nall the bounds are proved to be optimal in a minimax sense. \n\n"}
{"id": "0912.3389", "contents": "Title: On spectral representations of tensor random fields on the sphere Abstract: We study the representations of tensor random fields on the sphere basing on\nthe theory of representations of the rotation group. Introducing specific\ncomponents of a tensor field and imposing the conditions of weak isotropy and\nmean square continuity, we derive their spectral decompositions in terms of\ngeneralized spherical functions. The properties of random coefficients of the\ndecompositions are characterized, including such an important question as\nconditions of Gaussianity. \n\n"}
{"id": "1001.1051", "contents": "Title: Perturbation expansions of signal subspaces for long signals Abstract: Singular Spectrum Analysis and many other subspace-based methods of signal\nprocessing are implicitly relying on the assumption of close proximity of\nunperturbed and perturbed signal subspaces extracted by the Singular Value\nDecomposition of special \"signal\" and \"perturbed signal\" matrices. In this\npaper, the analysis of the main principal angle between these subspaces is\nperformed in terms of the perturbation expansions of the corresponding\northogonal projectors. Applicable upper bounds are derived. The main attention\nis paid to the asymptotical case when the length of the time series tends to\ninfinity. Results concerning conditions for convergence, rate of convergence,\nand the main terms of proximity are presented. \n\n"}
{"id": "1001.2185", "contents": "Title: Improved estimators for dispersion models with dispersion covariates Abstract: In this paper we discuss improved estimators for the regression and the\ndispersion parameters in an extended class of dispersion models (J{\\o}rgensen,\n1996). This class extends the regular dispersion models by letting the\ndispersion parameter vary throughout the observations, and contains the\ndispersion models as particular case. General formulae for the second-order\nbias are obtained explicitly in dispersion models with dispersion covariates,\nwhich generalize previous results by Botter and Cordeiro (1998), Cordeiro and\nMcCullagh (1991), Cordeiro and Vasconcellos (1999), and Paula (1992). The\npractical use of the formulae is that we can derive closed-form expressions for\nthe second-order biases of the maximum likelihood estimators of the regression\nand dispersion parameters when the information matrix has a closed-form.\nVarious expressions for the second-order biases are given for special models.\nThe formulae have advantages for numerical purposes because they require only a\nsupplementary weighted linear regression. We also compare these bias-corrected\nestimators with two different estimators which are also bias-free to the\nsecond-order that are based on bootstrap methods. These estimators are compared\nby simulation. \n\n"}
{"id": "1001.4475", "contents": "Title: X-Armed Bandits Abstract: We consider a generalization of stochastic bandits where the set of arms,\n$\\cX$, is allowed to be a generic measurable space and the mean-payoff function\nis \"locally Lipschitz\" with respect to a dissimilarity function that is known\nto the decision maker. Under this condition we construct an arm selection\npolicy, called HOO (hierarchical optimistic optimization), with improved regret\nbounds compared to previous results for a large class of problems. In\nparticular, our results imply that if $\\cX$ is the unit hypercube in a\nEuclidean space and the mean-payoff function has a finite number of global\nmaxima around which the behavior of the function is locally continuous with a\nknown smoothness degree, then the expected regret of HOO is bounded up to a\nlogarithmic factor by $\\sqrt{n}$, i.e., the rate of growth of the regret is\nindependent of the dimension of the space. We also prove the minimax optimality\nof our algorithm when the dissimilarity is a metric. Our basic strategy has\nquadratic computational complexity as a function of the number of time steps\nand does not rely on the doubling trick. We also introduce a modified strategy,\nwhich relies on the doubling trick but runs in linearithmic time. Both results\nare improvements with respect to previous approaches. \n\n"}
{"id": "1001.5265", "contents": "Title: The distribution of the maximum of a second order autoregressive\n  process: the continuous case Abstract: We give the distribution function of $M_n$, the maximum of a sequence of $n$\nobservations from an autoregressive process of order 2. Solutions are first\ngiven in terms of repeated integrals and then for the case, where the\nunderlying random variables are absolutely continuous. When the correlations\nare positive, P(M_n \\leq x) =a_{n,x}, where a_{n,x}= \\sum_{j=1}^\\infty\n\\beta_{jx} \\nu_{jx}^{n} = O (\\nu_{1x}^{n}), where $\\{\\nu_{jx}\\}$ are the\neigenvalues of a non-symmetric Fredholm kernel, and $\\nu_{1x}$ is the\neigenvalue of maximum magnitude. The weights $\\beta_{jx}$ depend on the $j$th\nleft and right eigenfunctions of the kernel.\n  These results are large deviations expansions for estimates, since the\nmaximum need not be standardized to have a limit. In fact such a limit need not\nexist. \n\n"}
{"id": "1002.0224", "contents": "Title: Convergence of U-statistics for interacting particle systems Abstract: The convergence of U-statistics has been intensively studied for estimators\nbased on families of i.i.d. random variables and variants of them. In most\ncases, the independence assumption is crucial [Lee90, de99]. When dealing with\nFeynman-Kac and other interacting particle systems of Monte Carlo type, one\nfaces a new type of problem. Namely, in a sample of N particles obtained\nthrough the corresponding algorithms, the distributions of the particles are\ncorrelated -although any finite number of them is asymptotically independent\nwith respect to the total number N of particles. In the present article,\nexploiting the fine asymptotics of particle systems, we prove convergence\ntheorems for U-statistics in this framework. \n\n"}
{"id": "1002.0675", "contents": "Title: Small time Chung-type LIL for L\\'{e}vy processes Abstract: We prove Chung-type laws of the iterated logarithm for general L\\'{e}vy\nprocesses at zero. In particular, we provide tools to translate small deviation\nestimates directly into laws of the iterated logarithm. This reveals laws of\nthe iterated logarithm for L\\'{e}vy processes at small times in many concrete\nexamples. In some cases, exotic norming functions are derived. \n\n"}
{"id": "1002.0730", "contents": "Title: Divergences and Duality for Estimation and Test under Moment Condition\n  Models Abstract: We introduce estimation and test procedures through divergence minimiza- tion\nfor models satisfying linear constraints with unknown parameter. These\nprocedures extend the empirical likelihood (EL) method and share common\nfeatures with generalized empirical likelihood approach. We treat the problems\nof existence and characterization of the divergence projections of probability\ndistributions on sets of signed finite measures. We give a precise\ncharacterization of duality, for the proposed class of estimates and test\nstatistics, which is used to derive their limiting distributions (including the\nEL estimate and the EL ratio statistic) both under the null hypotheses and\nunder alterna- tives or misspecification. An approximation to the power\nfunction is deduced as well as the sample size which ensures a desired power\nfor a given alternative. \n\n"}
{"id": "1002.3911", "contents": "Title: Parameter estimations for SPDEs with multiplicative fractional noise Abstract: We study parameter estimation problem for diagonalizable stochastic partial\ndifferential equations driven by a multiplicative fractional noise with any\nHurst parameter $H\\in(0,1)$. Two classes of estimators are investigated:\ntraditional maximum likelihood type estimators, and a new class called\nclosed-form exact estimators. Finally the general results are applied to\nstochastic heat equation driven by a fractional Brownian motion. \n\n"}
{"id": "1002.4781", "contents": "Title: Optimal properties of centroid-based classifiers for very\n  high-dimensional data Abstract: We show that scale-adjusted versions of the centroid-based classifier enjoys\noptimal properties when used to discriminate between two very high-dimensional\npopulations where the principal differences are in location. The scale\nadjustment removes the tendency of scale differences to confound differences in\nmeans. Certain other distance-based methods, for example, those founded on\nnearest-neighbor distance, do not have optimal performance in the sense that we\npropose. Our results permit varying degrees of sparsity and signal strength to\nbe treated, and require only mild conditions on dependence of vector\ncomponents. Additionally, we permit the marginal distributions of vector\ncomponents to vary extensively. In addition to providing theory we explore\nnumerical properties of a centroid-based classifier, and show that these\nfeatures reflect theoretical accounts of performance. \n\n"}
{"id": "1003.0205", "contents": "Title: Detecting Weak but Hierarchically-Structured Patterns in Networks Abstract: The ability to detect weak distributed activation patterns in networks is\ncritical to several applications, such as identifying the onset of anomalous\nactivity or incipient congestion in the Internet, or faint traces of a\nbiochemical spread by a sensor network. This is a challenging problem since\nweak distributed patterns can be invisible in per node statistics as well as a\nglobal network-wide aggregate. Most prior work considers situations in which\nthe activation/non-activation of each node is statistically independent, but\nthis is unrealistic in many problems. In this paper, we consider structured\npatterns arising from statistical dependencies in the activation process. Our\ncontributions are three-fold. First, we propose a sparsifying transform that\nsuccinctly represents structured activation patterns that conform to a\nhierarchical dependency graph. Second, we establish that the proposed transform\nfacilitates detection of very weak activation patterns that cannot be detected\nwith existing methods. Third, we show that the structure of the hierarchical\ndependency graph governing the activation process, and hence the network\ntransform, can be learnt from very few (logarithmic in network size)\nindependent snapshots of network activity. \n\n"}
{"id": "1003.2804", "contents": "Title: An overview of latent Markov models for longitudinal categorical data Abstract: We provide a comprehensive overview of latent Markov (LM) models for the\nanalysis of longitudinal categorical data. The main assumption behind these\nmodels is that the response variables are conditionally independent given a\nlatent process which follows a first-order Markov chain. We first illustrate\nthe basic LM model in which the conditional distribution of each response\nvariable given the corresponding latent variable and the initial and transition\nprobabilities of the latent process are unconstrained. For this model we also\nillustrate in detail maximum likelihood estimation through the\nExpectation-Maximization algorithm, which may be efficiently implemented by\nrecursions known in the hidden Markov literature. We then illustrate several\nconstrained versions of the basic LM model, which make the model more\nparsimonious and allow us to include and test hypotheses of interest. These\nconstraints may be put on the conditional distribution of the response\nvariables given the latent process (measurement model) or on the distribution\nof the latent process (latent model). We also deal with extensions of LM model\nfor the inclusion of individual covariates and to multilevel data. Covariates\nmay affect the measurement or the latent model; we discuss the implications of\nthese two different approaches according to the context of application.\nFinally, we outline methods for obtaining standard errors for the parameter\nestimates, for selecting the number of states and for path prediction. Models\nand related inference are illustrated by the description of relevant\nsocio-economic applications available in the literature. \n\n"}
{"id": "1003.6039", "contents": "Title: Stein couplings for normal approximation Abstract: In this article we propose a general framework for normal approximation using\nStein's method. We introduce the new concept of Stein couplings and we show\nthat it lies at the heart of popular approaches such as the local approach,\nexchangeable pairs, size biasing and many other approaches. We prove several\ntheorems with which normal approximation for the Wasserstein and Kolmogorov\nmetrics becomes routine once a Stein coupling is found. To illustrate the\nversatility of our framework we give applications in Hoeffding's combinatorial\ncentral limit theorem, functionals in the classic occupancy scheme,\nneighbourhood statistics of point patterns with fixed number of points and\nfunctionals of the components of randomly chosen vertices of sub-critical\nErdos-Renyi random graphs. In all these cases, we use new, non-standard\ncouplings. \n\n"}
{"id": "1004.3794", "contents": "Title: Comparison of quantum statistical models: equivalent conditions for\n  sufficiency Abstract: A family of probability distributions (i.e. a statistical model) is said to\nbe sufficient for another, if there exists a transition matrix transforming the\nprobability distributions in the former to the probability distributions in the\nlatter. The Blackwell-Sherman-Stein (BSS) theorem provides necessary and\nsufficient conditions for one statistical model to be sufficient for another,\nby comparing their information values in statistical decision problems. In this\npaper we extend the BSS theorem to quantum statistical decision theory, where\nstatistical models are replaced by families of density matrices defined on\nfinite-dimensional Hilbert spaces, and transition matrices are replaced by\ncompletely positive, trace-preserving maps (i.e. coarse-grainings). The\nframework we propose is suitable for unifying results that previously were\nindependent, like the BSS theorem for classical statistical models and its\nanalogue for pairs of bipartite quantum states, recently proved by Shmaya. An\nimportant role in this paper is played by statistical morphisms, namely, affine\nmaps whose definition generalizes that of coarse-grainings given by Petz and\ninduces a corresponding criterion for statistical sufficiency that is weaker,\nand hence easier to be characterized, than Petz's. \n\n"}
{"id": "1004.5529", "contents": "Title: High-Rate Vector Quantization for the Neyman-Pearson Detection of\n  Correlated Processes Abstract: This paper investigates the effect of quantization on the performance of the\nNeyman-Pearson test. It is assumed that a sensing unit observes samples of a\ncorrelated stationary ergodic multivariate process. Each sample is passed\nthrough an N-point quantizer and transmitted to a decision device which\nperforms a binary hypothesis test. For any false alarm level, it is shown that\nthe miss probability of the Neyman-Pearson test converges to zero exponentially\nas the number of samples tends to infinity, assuming that the observed process\nsatisfies certain mixing conditions. The main contribution of this paper is to\nprovide a compact closed-form expression of the error exponent in the high-rate\nregime i.e., when the number N of quantization levels tends to infinity,\ngeneralizing previous results of Gupta and Hero to the case of non-independent\nobservations. If d represents the dimension of one sample, it is proved that\nthe error exponent converges at rate N^{2/d} to the one obtained in the absence\nof quantization. As an application, relevant high-rate quantization strategies\nwhich lead to a large error exponent are determined. Numerical results indicate\nthat the proposed quantization rule can yield better performance than existing\nones in terms of detection error. \n\n"}
{"id": "1005.0829", "contents": "Title: Transductive versions of the LASSO and the Dantzig Selector Abstract: Transductive methods are useful in prediction problems when the training\ndataset is composed of a large number of unlabeled observations and a smaller\nnumber of labeled observations. In this paper, we propose an approach for\ndeveloping transductive prediction procedures that are able to take advantage\nof the sparsity in the high dimensional linear regression. More precisely, we\ndefine transductive versions of the LASSO and the Dantzig Selector . These\nprocedures combine labeled and unlabeled observations of the training dataset\nto produce a prediction for the unlabeled observations. We propose an\nexperimental study of the transductive estimators, that shows that they improve\nthe LASSO and Dantzig Selector in many situations, and particularly in high\ndimensional problems when the predictors are correlated. We then provide\nnon-asymptotic theoretical guarantees for these estimation methods.\nInterestingly, our theoretical results show that the Transductive LASSO and\nDantzig Selector satisfy sparsity inequalities under weaker assumptions than\nthose required for the \"original\" LASSO. \n\n"}
{"id": "1005.1717", "contents": "Title: A Markov basis for two-state toric homogeneous Markov chain model\n  without initial parameters Abstract: We derive a Markov basis consisting of moves of degree at most three for\ntwo-state toric homogeneous Markov chain model of arbitrary length without\nparameters for initial states. Our basis consists of moves of degree three and\ndegree one, which alter the initial frequencies, in addition to moves of degree\ntwo and degree one for toric homogeneous Markov chain model with parameters for\ninitial states. \n\n"}
{"id": "1005.5620", "contents": "Title: Practical simulation and estimation for Gibbs Delaunay-Voronoi\n  tessellations with geometric hardcore interaction Abstract: General models of Gibbs Delaunay-Voronoi tessellations, which can be viewed\nas extensions of Ord's process, are considered. The interaction may occur on\neach cell of the tessellation and between neighbour cells. The tessellation may\nalso be subjected to a geometric hardcore interaction, forcing the cells not to\nbe too large, too small, or too flat. This setting, natural for applications,\nintroduces some theoretical difficulties since the interaction is not\nnecessarily hereditary. Mathematical results available for studying these\nmodels are reviewed and further outcomes are provided. They concern the\nexistence, the simulation and the estimation of such tessellations. Based on\nthese results, tools to handle these objects in practice are presented: how to\nsimulate them, estimate their parameters and validate the fitted model. Some\nexamples of simulated tessellations are studied in details. \n\n"}
{"id": "1006.2980", "contents": "Title: Risk bounds for purely uniformly random forests Abstract: Random forests, introduced by Leo Breiman in 2001, are a very effective\nstatistical method. The complex mechanism of the method makes theoretical\nanalysis difficult. Therefore, a simplified version of random forests, called\npurely random forests, which can be theoretically handled more easily, has been\nconsidered. In this paper we introduce a variant of this kind of random\nforests, that we call purely uniformly random forests. In the context of\nregression problems with a one-dimensional predictor space, we show that both\nrandom trees and random forests reach minimax rate of convergence. In addition,\nwe prove that compared to random trees, random forests improve accuracy by\nreducing the estimator variance by a factor of three fourths. \n\n"}
{"id": "1006.4524", "contents": "Title: Fundamental Rate-Reliability-Complexity Limits in Outage Limited MIMO\n  Communications Abstract: The work establishes fundamental limits with respect to rate, reliability and\ncomputational complexity, for a general setting of outage-limited MIMO\ncommunications. In the high-SNR regime, the limits are optimized over all\nencoders, all decoders, and all complexity regulating policies. The work then\nproceeds to explicitly identify encoder-decoder designs and policies, that meet\nthis optimal tradeoff. In practice, the limits aim to meaningfully quantify\ndifferent pertinent measures, such as the optimal rate-reliability capabilities\nper unit complexity and power, the optimal diversity gains per complexity\ncosts, or the optimal number of numerical operations (i.e., flops) per bit.\nFinally the tradeoff's simple nature, renders it useful for insightful\ncomparison of the rate-reliability-complexity capabilities for different\nencoders-decoders. \n\n"}
{"id": "1007.0549", "contents": "Title: Minimax Manifold Estimation Abstract: We find the minimax rate of convergence in Hausdorff distance for estimating\na manifold M of dimension d embedded in R^D given a noisy sample from the\nmanifold. We assume that the manifold satisfies a smoothness condition and that\nthe noise distribution has compact support. We show that the optimal rate of\nconvergence is n^{-2/(2+d)}. Thus, the minimax rate depends only on the\ndimension of the manifold, not on the dimension of the space in which M is\nembedded. \n\n"}
{"id": "1007.3351", "contents": "Title: Takacs Fiksel method for stationary marked Gibbs point processes Abstract: This paper studies a method to estimate the parameters governing the\ndistribution of a stationary marked Gibbs point process. This procedure, known\nas the Takacs-Fiksel method, is based on the estimation of the left and right\nhand sides of the Georgii-Nguyen-Zessin formula and leads to a family of\nestimators due to the possible choices of test functions. We propose several\nexamples illustrating the interest and flexibility of this procedure. We also\nprovide sufficient conditions based on the model and the test functions to\nderive asymptotic properties (consistency and asymptotic normality) of the\nresulting estimator. The different assumptions are discussed for exponential\nfamily models and for a large class of test functions. A short simulation study\nis proposed to assess the correctness of the methodology and the asymptotic\nresults. \n\n"}
{"id": "1007.4013", "contents": "Title: Quasi-concave density estimation Abstract: Maximum likelihood estimation of a log-concave probability density is\nformulated as a convex optimization problem and shown to have an equivalent\ndual formulation as a constrained maximum Shannon entropy problem. Closely\nrelated maximum Renyi entropy estimators that impose weaker concavity\nrestrictions on the fitted density are also considered, notably a minimum\nHellinger discrepancy estimator that constrains the reciprocal of the\nsquare-root of the density to be concave. A limiting form of these estimators\nconstrains solutions to the class of quasi-concave densities. \n\n"}
{"id": "1008.0526", "contents": "Title: Minimax risks for sparse regressions: Ultra-high-dimensional phenomenons Abstract: Consider the standard Gaussian linear regression model $Y=X\\theta+\\epsilon$,\nwhere $Y\\in R^n$ is a response vector and $ X\\in R^{n*p}$ is a design matrix.\nNumerous work have been devoted to building efficient estimators of $\\theta$\nwhen $p$ is much larger than $n$. In such a situation, a classical approach\namounts to assume that $\\theta_0$ is approximately sparse. This paper studies\nthe minimax risks of estimation and testing over classes of $k$-sparse vectors\n$\\theta$. These bounds shed light on the limitations due to\nhigh-dimensionality. The results encompass the problem of prediction\n(estimation of $X\\theta$), the inverse problem (estimation of $\\theta_0$) and\nlinear testing (testing $X\\theta=0$). Interestingly, an elbow effect occurs\nwhen the number of variables $k\\log(p/k)$ becomes large compared to $n$.\nIndeed, the minimax risks and hypothesis separation distances blow up in this\nultra-high dimensional setting. We also prove that even dimension reduction\ntechniques cannot provide satisfying results in an ultra-high dimensional\nsetting. Moreover, we compute the minimax risks when the variance of the noise\nis unknown. The knowledge of this variance is shown to play a significant role\nin the optimal rates of estimation and testing. All these minimax bounds\nprovide a characterization of statistical problems that are so difficult so\nthat no procedure can provide satisfying results. \n\n"}
{"id": "1008.1355", "contents": "Title: Control Variates for Reversible MCMC Samplers Abstract: A general methodology is introduced for the construction and effective\napplication of control variates to estimation problems involving data from\nreversible MCMC samplers. We propose the use of a specific class of functions\nas control variates, and we introduce a new, consistent estimator for the\nvalues of the coefficients of the optimal linear combination of these\nfunctions. The form and proposed construction of the control variates is\nderived from our solution of the Poisson equation associated with a specific\nMCMC scenario. The new estimator, which can be applied to the same MCMC sample,\nis derived from a novel, finite-dimensional, explicit representation for the\noptimal coefficients. The resulting variance-reduction methodology is primarily\napplicable when the simulated data are generated by a conjugate random-scan\nGibbs sampler. MCMC examples of Bayesian inference problems demonstrate that\nthe corresponding reduction in the estimation variance is significant, and that\nin some cases it can be quite dramatic. Extensions of this methodology in\nseveral directions are given, including certain families of Metropolis-Hastings\nsamplers and hybrid Metropolis-within-Gibbs algorithms. Corresponding\nsimulation examples are presented illustrating the utility of the proposed\nmethods. All methodological and asymptotic arguments are rigorously justified\nunder easily verifiable and essentially minimal conditions. \n\n"}
{"id": "1008.1393", "contents": "Title: Towards Nonstationary, Nonparametric Independent Process Analysis with\n  Unknown Source Component Dimensions Abstract: The goal of this paper is to extend independent subspace analysis (ISA) to\nthe case of (i) nonparametric, not strictly stationary source dynamics and (ii)\nunknown source component dimensions. We make use of functional autoregressive\n(fAR) processes to model the temporal evolution of the hidden sources. An\nextension of the ISA separation principle--which states that the ISA problem\ncan be solved by traditional independent component analysis (ICA) and\nclustering of the ICA elements--is derived for the solution of the defined fAR\nindependent process analysis task (fAR-IPA): applying fAR identification we\nreduce the problem to ISA. A local averaging approach, the Nadaraya-Watson\nkernel regression technique is adapted to obtain strongly consistent fAR\nestimation. We extend the Amari-index to different dimensional components and\nillustrate the efficiency of the fAR-IPA approach by numerical examples. \n\n"}
{"id": "1008.4059", "contents": "Title: The distribution of the square sum of Dirichlet random variables and a\n  table with quantiles of Greenwood's statistic Abstract: The exact distribution of the square sum of Dirichlet random variables is\ngiven by two different univariate integral representations. Alternatively,\nthree representations by orthogonal series with Jacobi or Legendre polynomials\nare derived. As a special case the distribution of the square sum of spacings -\nalso called Greenwood's statistic - is obtained. Nine quantiles of this\nstatistic are tabulated with eight digits where the number of squares ranges\nfrom 10 to 100. \n\n"}
{"id": "1009.0906", "contents": "Title: Near-Oracle Performance of Greedy Block-Sparse Estimation Techniques\n  from Noisy Measurements Abstract: This paper examines the ability of greedy algorithms to estimate a block\nsparse parameter vector from noisy measurements. In particular, block sparse\nversions of the orthogonal matching pursuit and thresholding algorithms are\nanalyzed under both adversarial and Gaussian noise models. In the adversarial\nsetting, it is shown that estimation accuracy comes within a constant factor of\nthe noise power. Under Gaussian noise, the Cramer-Rao bound is derived, and it\nis shown that the greedy techniques come close to this bound at high SNR. The\nguarantees are numerically compared with the actual performance of block and\nnon-block algorithms, highlighting the advantages of block sparse techniques. \n\n"}
{"id": "1009.3203", "contents": "Title: Intrinsic Inference on the Mean Geodesic of Planar Shapes and Tree\n  Discrimination by Leaf Growth Abstract: For planar landmark based shapes, taking into account the non-Euclidean\ngeometry of the shape space, a statistical test for a common mean first\ngeodesic principal component (GPC) is devised. It rests on one of two\nasymptotic scenarios, both of which are identical in a Euclidean geometry. For\nboth scenarios, strong consistency and central limit theorems are established,\nalong with an algorithm for the computation of a Ziezold mean geodesic. In\napplication, this allows to verify the geodesic hypothesis for leaf growth of\nCanadian black poplars and to discriminate genetically different trees by\nobservations of leaf shape growth over brief time intervals. With a test based\non Procrustes tangent space coordinates, not involving the shape space's\ncurvature, neither can be achieved. \n\n"}
{"id": "1009.4434", "contents": "Title: The universal Glivenko-Cantelli property Abstract: Let F be a separable uniformly bounded family of measurable functions on a\nstandard measurable space, and let N_{[]}(F,\\epsilon,\\mu) be the smallest\nnumber of \\epsilon-brackets in L^1(\\mu) needed to cover F. The following are\nequivalent:\n  1. F is a universal Glivenko-Cantelli class.\n  2. N_{[]}(F,\\epsilon,\\mu)<\\infty for every \\epsilon>0 and every probability\nmeasure \\mu.\n  3. F is totally bounded in L^1(\\mu) for every probability measure \\mu.\n  4. F does not contain a Boolean \\sigma-independent sequence.\n  It follows that universal Glivenko-Cantelli classes are uniformity classes\nfor general sequences of almost surely convergent random measures. \n\n"}
{"id": "1010.0324", "contents": "Title: An identity of Jack polynomials Abstract: In this work it is propose an alterative proof of one of basic properties of\nthe zonal polynomials. This identity is generalised for the Jack polynomials. \n\n"}
{"id": "1010.0745", "contents": "Title: On the Existence of the MLE for a Directed Random Graph Network Model\n  with Reciprocation Abstract: Holland and Leinhardt (1981) proposed a directed random graph model, the p1\nmodel, to describe dyadic interactions in a social network. In previous work\n(Petrovic et al., 2010), we studied the algebraic properties of the p1 model\nand showed that it is a toric model specified by a multi-homogeneous ideal. We\nconducted an extensive study of the Markov bases for p1 that incorporate\nexplicitly the constraint arising from multi-homogeneity. Here we consider the\nproperties of the corresponding toric variety and relate them to the conditions\nfor the existence of the maximum likelihood and extended maximum likelihood\nestimators or the model parameters. Our results are directly relevant to the\nestimation and conditional goodness-of-fit testing problems in p1 models. \n\n"}
{"id": "1010.3320", "contents": "Title: Exact block-wise optimization in group lasso and sparse group lasso for\n  linear regression Abstract: The group lasso is a penalized regression method, used in regression problems\nwhere the covariates are partitioned into groups to promote sparsity at the\ngroup level. Existing methods for finding the group lasso estimator either use\ngradient projection methods to update the entire coefficient vector\nsimultaneously at each step, or update one group of coefficients at a time\nusing an inexact line search to approximate the optimal value for the group of\ncoefficients when all other groups' coefficients are fixed. We present a new\nmethod of computation for the group lasso in the linear regression case, the\nSingle Line Search (SLS) algorithm, which operates by computing the exact\noptimal value for each group (when all other coefficients are fixed) with one\nunivariate line search. We perform simulations demonstrating that the SLS\nalgorithm is often more efficient than existing computational methods. We also\nextend the SLS algorithm to the sparse group lasso problem via the Signed\nSingle Line Search (SSLS) algorithm, and give theoretical results to support\nboth algorithms. \n\n"}
{"id": "1010.3566", "contents": "Title: Perturbation of matrices and non-negative rank with a view toward\n  statistical models Abstract: In this paper we study how perturbing a matrix changes its non-negative rank.\nWe prove that the non-negative rank is upper-semicontinuos and we describe some\nspecial families of perturbations. We show how our results relate to Statistics\nin terms of the study of Maximum Likelihood Estimation for mixture models. \n\n"}
{"id": "1010.4345", "contents": "Title: Sparse Models and Methods for Optimal Instruments with an Application to\n  Eminent Domain Abstract: We develop results for the use of Lasso and Post-Lasso methods to form\nfirst-stage predictions and estimate optimal instruments in linear instrumental\nvariables (IV) models with many instruments, $p$. Our results apply even when\n$p$ is much larger than the sample size, $n$. We show that the IV estimator\nbased on using Lasso or Post-Lasso in the first stage is root-n consistent and\nasymptotically normal when the first-stage is approximately sparse; i.e. when\nthe conditional expectation of the endogenous variables given the instruments\ncan be well-approximated by a relatively small set of variables whose\nidentities may be unknown. We also show the estimator is semi-parametrically\nefficient when the structural error is homoscedastic. Notably our results allow\nfor imperfect model selection, and do not rely upon the unrealistic \"beta-min\"\nconditions that are widely used to establish validity of inference following\nmodel selection. In simulation experiments, the Lasso-based IV estimator with a\ndata-driven penalty performs well compared to recently advocated\nmany-instrument-robust procedures. In an empirical example dealing with the\neffect of judicial eminent domain decisions on economic outcomes, the\nLasso-based IV estimator outperforms an intuitive benchmark.\n  In developing the IV results, we establish a series of new results for Lasso\nand Post-Lasso estimators of nonparametric conditional expectation functions\nwhich are of independent theoretical and practical interest. We construct a\nmodification of Lasso designed to deal with non-Gaussian, heteroscedastic\ndisturbances which uses a data-weighted $\\ell_1$-penalty function. Using\nmoderate deviation theory for self-normalized sums, we provide convergence\nrates for the resulting Lasso and Post-Lasso estimators that are as sharp as\nthe corresponding rates in the homoscedastic Gaussian case under the condition\nthat $\\log p = o(n^{1/3})$. \n\n"}
{"id": "1010.4504", "contents": "Title: Reading Dependencies from Covariance Graphs Abstract: The covariance graph (aka bi-directed graph) of a probability distribution\n$p$ is the undirected graph $G$ where two nodes are adjacent iff their\ncorresponding random variables are marginally dependent in $p$. In this paper,\nwe present a graphical criterion for reading dependencies from $G$, under the\nassumption that $p$ satisfies the graphoid properties as well as weak\ntransitivity and composition. We prove that the graphical criterion is sound\nand complete in certain sense. We argue that our assumptions are not too\nrestrictive. For instance, all the regular Gaussian probability distributions\nsatisfy them. \n\n"}
{"id": "1011.1067", "contents": "Title: Coupling property and gradient estimates of L\\'{e}vy processes via the\n  symbol Abstract: We derive explicitly the coupling property for the transition semigroup of a\nL\\'{e}vy process and gradient estimates for the associated semigroup of\ntransition operators. This is based on the asymptotic behaviour of the symbol\nor the characteristic exponent near zero and infinity, respectively. Our\nresults can be applied to a large class of L\\'{e}vy processes, including stable\nL\\'{e}vy processes, layered stable processes, tempered stable processes and\nrelativistic stable processes. \n\n"}
{"id": "1011.1253", "contents": "Title: Coupling optional P\\'olya trees and the two sample problem Abstract: Testing and characterizing the difference between two data samples is of\nfundamental interest in statistics. Existing methods such as Kolmogorov-Smirnov\nand Cramer-von-Mises tests do not scale well as the dimensionality increases\nand provides no easy way to characterize the difference should it exist. In\nthis work, we propose a theoretical framework for inference that addresses\nthese challenges in the form of a prior for Bayesian nonparametric analysis.\nThe new prior is constructed based on a random-partition-and-assignment\nprocedure similar to the one that defines the standard optional P\\'olya tree\ndistribution, but has the ability to generate multiple random distributions\njointly. These random probability distributions are allowed to \"couple\", that\nis to have the same conditional distribution, on subsets of the sample space.\nWe show that this \"coupling optional P\\'olya tree\" prior provides a convenient\nand effective way for both the testing of two sample difference and the\nlearning of the underlying structure of the difference. In addition, we discuss\nsome practical issues in the computational implementation of this prior and\nprovide several numerical examples to demonstrate its work. \n\n"}
{"id": "1012.0753", "contents": "Title: An asymptotic approximation of the marginal likelihood for general\n  Markov models Abstract: The standard Bayesian Information Criterion (BIC) is derived under regularity\nconditions which are not always satisfied by the graphical models with hidden\nvariables. In this paper we derive the BIC score for Bayesian networks in the\ncase of binary data and when the underlying graph is a rooted tree and all the\ninner nodes represent hidden variables. This provides a direct generalization\nof a similar formula given by Rusakov and Geiger for naive Bayes models. The\nmain tool used in this paper is a connection between asymptotic approximation\nof Laplace integrals and the real log-canonical threshold. \n\n"}
{"id": "1012.2063", "contents": "Title: Bounding Standard Gaussian Tail Probabilities Abstract: We review various inequalities for Mills' ratio (1 - \\Phi)/\\phi, where \\phi\nand \\Phi denote the standard Gaussian density and distribution function,\nrespectively. Elementary considerations involving finite continued fractions\nlead to a general approximation scheme which implies and refines several known\nbounds. \n\n"}
{"id": "1012.4183", "contents": "Title: Non-asymptotic deviation inequalities for smoothed additive functionals\n  in non-linear state-space models Abstract: The approximation of fixed-interval smoothing distributions is a key issue in\ninference for general state-space hidden Markov models (HMM). This contribution\nestablishes non-asymptotic bounds for the Forward Filtering Backward Smoothing\n(FFBS) and the Forward Filtering Backward Simulation (FFBSi) estimators of\nfixed-interval smoothing functionals. We show that the rate of convergence of\nthe Lq-mean errors of both methods depends on the number of observations T and\nthe number of particles N only through the ratio T/N for additive functionals.\nIn the case of the FFBS, this improves recent results providing bounds\ndepending on T and the square root of N. \n\n"}
{"id": "1101.1057", "contents": "Title: Sparsity regret bounds for individual sequences in online linear\n  regression Abstract: We consider the problem of online linear regression on arbitrary\ndeterministic sequences when the ambient dimension d can be much larger than\nthe number of time rounds T. We introduce the notion of sparsity regret bound,\nwhich is a deterministic online counterpart of recent risk bounds derived in\nthe stochastic setting under a sparsity scenario. We prove such regret bounds\nfor an online-learning algorithm called SeqSEW and based on exponential\nweighting and data-driven truncation. In a second part we apply a\nparameter-free version of this algorithm to the stochastic setting (regression\nmodel with random design). This yields risk bounds of the same flavor as in\nDalalyan and Tsybakov (2011) but which solve two questions left open therein.\nIn particular our risk bounds are adaptive (up to a logarithmic factor) to the\nunknown variance of the noise if the latter is Gaussian. We also address the\nregression model with fixed design. \n\n"}
{"id": "1102.2078", "contents": "Title: A goodness-of-fit test for bivariate extreme-value copulas Abstract: It is often reasonable to assume that the dependence structure of a bivariate\ncontinuous distribution belongs to the class of extreme-value copulas. The\nlatter are characterized by their Pickands dependence function. In this paper,\na procedure is proposed for testing whether this function belongs to a given\nparametric family. The test is based on a Cram\\'{e}r--von Mises statistic\nmeasuring the distance between an estimate of the parametric Pickands\ndependence function and either one of two nonparametric estimators thereof\nstudied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the\nlimiting distribution of the test statistic depends on unknown parameters, it\nmust be estimated via a parametric bootstrap procedure, the validity of which\nis established. Monte Carlo simulations are used to assess the power of the\ntest and an extension to dependence structures that are left-tail decreasing in\nboth variables is considered. \n\n"}
{"id": "1102.2713", "contents": "Title: One-sided L\\'{e}vy stable distributions Abstract: In this paper, we show new representations of one-sided L\\'{e}vy stable\ndistributions for irrational L\\'{e}vy indices of the type\n$\\left(\\frac{p}{q}\\right)^{\\frac{l_{2}}{l_{1}}}$ which are not covered in\n\\cite{pg1} : for rational L\\'{e}vy indices. Furthermore, other equivalent\nrepresentations for a distribution of a rational L\\'{e}vy index is described.\nWe also give a simplest proof for the formulae which cover the cases for\nrational L\\'{e}vy indices. Finally we introduce the concepts of L\\'{e}vy\nsmashing and L\\'{e}vy-smashed gamma stochastic processes. \n\n"}
{"id": "1103.0628", "contents": "Title: Bivariate least squares linear regression: towards a unified analytic\n  formalism Abstract: Concerning bivariate least squares linear regression, the classical approach\npursued for functional models in earlier attempts is reviewed using a new\nformalism in terms of deviation (matrix) traces. Within the framework of\nclassical error models, the dependent variable relates to the independent\nvariable according to the usual additive model. Linear models of regression\nlines are considered in the general case of correlated errors in X and in Y for\nheteroscedastic data. The special case of (C) generalized orthogonal regression\nis considered in detail together with well known subcases. In the limit of\nhomoscedastic data, the results determined for functional models are compared\nwith their counterparts related to extreme structural models. While regression\nline slope and intercept estimators for functional and structural models\nnecessarily coincide, the contrary holds for related variance estimators even\nif the residuals obey a Gaussian distribution, with a single exception. An\nexample of astronomical application is considered, concerning the [O/H]-[Fe/H]\nempirical relations deduced from five samples related to different stars and/or\ndifferent methods of oxygen abundance determination. For selected samples and\nassigned methods, different regression models yield consistent results within\nthe errors for both heteroscedastic and homoscedastic data. Conversely, samples\nrelated to different methods produce discrepant results, due to the presence of\n(still undetected) systematic errors, which implies no definitive statement can\nbe made at present. A comparison is also made between different expressions of\nregression line slope and intercept variance estimators, where fractional\ndiscrepancies are found to be not exceeding a few percent, which grows up to\nabout 20% in presence of large dispersion data. \n\n"}
{"id": "1103.3442", "contents": "Title: Minimax nonparametric testing in a problem related to the Radon\n  transform Abstract: We consider the detection problem of a two-dimensional function from noisy\nobservations of its integrals over lines. We study both rate and sharp\nasymptotics for the error probabilities in the minimax setup. By construction,\nthe derived tests are non-adaptive. We also construct a minimax rate-optimal\nadaptive test of rather simple structure. \n\n"}
{"id": "1103.5538", "contents": "Title: Online Learning as Stochastic Approximation of Regularization Paths Abstract: In this paper, an online learning algorithm is proposed as sequential\nstochastic approximation of a regularization path converging to the regression\nfunction in reproducing kernel Hilbert spaces (RKHSs). We show that it is\npossible to produce the best known strong (RKHS norm) convergence rate of batch\nlearning, through a careful choice of the gain or step size sequences,\ndepending on regularity assumptions on the regression function. The\ncorresponding weak (mean square distance) convergence rate is optimal in the\nsense that it reaches the minimax and individual lower rates in the literature.\nIn both cases we deduce almost sure convergence, using Bernstein-type\ninequalities for martingales in Hilbert spaces.\n  To achieve this we develop a bias-variance decomposition similar to the batch\nlearning setting; the bias consists in the approximation and drift errors along\nthe regularization path, which display the same rates of convergence, and the\nvariance arises from the sample error analysed as a reverse martingale\ndifference sequence. The rates above are obtained by an optimal trade-off\nbetween the bias and the variance. \n\n"}
{"id": "1104.2365", "contents": "Title: Instantaneous frequency and wave shape functions (I) Abstract: Although one can formulate an intuitive notion of instantaneous frequency,\ngeneralizing \"frequency\" as we understand it in e.g. the Fourier transform, a\nrigorous mathematical definition is lacking. In this paper, we consider a class\nof functions composed of waveforms that repeat nearly periodically, and for\nwhich the instantaneous frequency can be given a rigorous meaning. We show that\nSynchrosqueezing can be used to determine the instantaneous frequency of\nfunctions in this class, even if the waveform is not harmonic, thus\ngeneralizing earlier results for cosine wave functions. We also provide\nreal-life examples and discuss the advantages, for these examples, of\nconsidering such non-harmonic waveforms. \n\n"}
{"id": "1104.4135", "contents": "Title: Posterior consistency in linear models under shrinkage priors Abstract: We investigate the asymptotic behavior of posterior distributions of\nregression coefficients in high-dimensional linear models as the number of\ndimensions grows with the number of observations. We show that the posterior\ndistribution concentrates in neighborhoods of the true parameter under simple\nsufficient conditions. These conditions hold under popular shrinkage priors\ngiven some sparsity assumptions. \n\n"}
{"id": "1104.4595", "contents": "Title: Scaled Sparse Linear Regression Abstract: Scaled sparse linear regression jointly estimates the regression coefficients\nand noise level in a linear model. It chooses an equilibrium with a sparse\nregression method by iteratively estimating the noise level via the mean\nresidual square and scaling the penalty in proportion to the estimated noise\nlevel. The iterative algorithm costs little beyond the computation of a path or\ngrid of the sparse regression estimator for penalty levels above a proper\nthreshold. For the scaled lasso, the algorithm is a gradient descent in a\nconvex minimization of a penalized joint loss function for the regression\ncoefficients and noise level. Under mild regularity conditions, we prove that\nthe scaled lasso simultaneously yields an estimator for the noise level and an\nestimated coefficient vector satisfying certain oracle inequalities for\nprediction, the estimation of the noise level and the regression coefficients.\nThese inequalities provide sufficient conditions for the consistency and\nasymptotic normality of the noise level estimator, including certain cases\nwhere the number of variables is of greater order than the sample size.\nParallel results are provided for the least squares estimation after model\nselection by the scaled lasso. Numerical results demonstrate the superior\nperformance of the proposed methods over an earlier proposal of joint convex\nminimization. \n\n"}
{"id": "1105.1404", "contents": "Title: Geometric sensitivity of random matrix results: consequences for\n  shrinkage estimators of covariance and related statistical methods Abstract: Shrinkage estimators of covariance are an important tool in modern applied\nand theoretical statistics. They play a key role in regularized estimation\nproblems, such as ridge regression (aka Tykhonov regularization), regularized\ndiscriminant analysis and a variety of optimization problems.\n  In this paper, we bring to bear the tools of random matrix theory to\nunderstand their behavior, and in particular, that of quadratic forms involving\ninverses of those estimators, which are important in practice.\n  We use very mild assumptions compared to the usual assumptions made in random\nmatrix theory, requiring only mild conditions on the moments of linear and\nquadratic forms in our random vectors. In particular, we show that our results\napply for instance to log-normal data, which are of interest in financial\napplications.\n  Our study highlights the relative sensitivity of random matrix results (and\ntheir practical consequences) to geometric assumptions which are often\nimplicitly made by random matrix theorists and may not be relevant in data\nanalytic practice. \n\n"}
{"id": "1105.1475", "contents": "Title: Pivotal estimation via square-root Lasso in nonparametric regression Abstract: We propose a self-tuning $\\sqrt{\\mathrm {Lasso}}$ method that simultaneously\nresolves three important practical problems in high-dimensional regression\nanalysis, namely it handles the unknown scale, heteroscedasticity and (drastic)\nnon-Gaussianity of the noise. In addition, our analysis allows for badly\nbehaved designs, for example, perfectly collinear regressors, and generates\nsharp bounds even in extreme cases, such as the infinite variance case and the\nnoiseless case, in contrast to Lasso. We establish various nonasymptotic bounds\nfor $\\sqrt{\\mathrm {Lasso}}$ including prediction norm rate and sparsity. Our\nanalysis is based on new impact factors that are tailored for bounding\nprediction norm. In order to cover heteroscedastic non-Gaussian noise, we rely\non moderate deviation theory for self-normalized sums to achieve Gaussian-like\nresults under weak conditions. Moreover, we derive bounds on the performance of\nordinary least square (ols) applied to the model selected by $\\sqrt{\\mathrm\n{Lasso}}$ accounting for possible misspecification of the selected model. Under\nmild conditions, the rate of convergence of ols post $\\sqrt{\\mathrm {Lasso}}$\nis as good as $\\sqrt{\\mathrm {Lasso}}$'s rate. As an application, we consider\nthe use of $\\sqrt{\\mathrm {Lasso}}$ and ols post $\\sqrt{\\mathrm {Lasso}}$ as\nestimators of nuisance parameters in a generic semiparametric problem\n(nonlinear moment condition or $Z$-problem), resulting in a construction of\n$\\sqrt{n}$-consistent and asymptotically normal estimators of the main\nparameters. \n\n"}
{"id": "1105.2128", "contents": "Title: Asymptotic equivalence for inference on the volatility from noisy\n  observations Abstract: We consider discrete-time observations of a continuous martingale under\nmeasurement error. This serves as a fundamental model for high-frequency data\nin finance, where an efficient price process is observed under microstructure\nnoise. It is shown that this nonparametric model is in Le Cam's sense\nasymptotically equivalent to a Gaussian shift experiment in terms of the square\nroot of the volatility function $\\sigma$ and a nonstandard noise level. As an\napplication, new rate-optimal estimators of the volatility function and simple\nefficient estimators of the integrated volatility are constructed. \n\n"}
{"id": "1105.2165", "contents": "Title: Unbiased risk estimation and scoring rules Abstract: Stein unbiased risk estimation is generalized twice, from the Gaussian shift\nmodel to nonparametric families of smooth densities, and from the quadratic\nrisk to more general divergence type distances. The development relies on a\nconnection with local proper scoring rules. \n\n"}
{"id": "1105.6154", "contents": "Title: Conditional Quantile Processes based on Series or Many Regressors Abstract: Quantile regression (QR) is a principal regression method for analyzing the\nimpact of covariates on outcomes. The impact is described by the conditional\nquantile function and its functionals. In this paper we develop the\nnonparametric QR-series framework, covering many regressors as a special case,\nfor performing inference on the entire conditional quantile function and its\nlinear functionals. In this framework, we approximate the entire conditional\nquantile function by a linear combination of series terms with\nquantile-specific coefficients and estimate the function-valued coefficients\nfrom the data. We develop large sample theory for the QR-series coefficient\nprocess, namely we obtain uniform strong approximations to the QR-series\ncoefficient process by conditionally pivotal and Gaussian processes. Based on\nthese strong approximations, or couplings, we develop four resampling methods\n(pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be\nused for inference on the entire QR-series coefficient function.\n  We apply these results to obtain estimation and inference methods for linear\nfunctionals of the conditional quantile function, such as the conditional\nquantile function itself, its partial derivatives, average partial derivatives,\nand conditional average partial derivatives. Specifically, we obtain uniform\nrates of convergence and show how to use the four resampling methods mentioned\nabove for inference on the functionals. All of the above results are for\nfunction-valued parameters, holding uniformly in both the quantile index and\nthe covariate value, and covering the pointwise case as a by-product. We\ndemonstrate the practical utility of these results with an example, where we\nestimate the price elasticity function and test the Slutsky condition of the\nindividual demand for gasoline, as indexed by the individual unobserved\npropensity for gasoline consumption. \n\n"}
{"id": "1106.0773", "contents": "Title: Multivariate stratified sampling by stochastic multiobjective\n  optimisation Abstract: This work considers the allocation problem for multivariate stratified random\nsampling as a problem of integer non-linear stochastic multiobjective\nmathematical programming. With this goal in mind the asymptotic distribution of\nthe vector of sample variances is studied. Two alternative approaches are\nsuggested for solving the allocation problem for multivariate stratified random\nsampling. An example is presented by applying the different proposed\ntechniques. \n\n"}
{"id": "1106.3503", "contents": "Title: Adaptive estimation in the nonparametric random coefficients binary\n  choice model by needlet thresholding Abstract: In the random coefficients binary choice model, a binary variable equals 1\niff an index $X^\\top\\beta$ is positive.The vectors $X$ and $\\beta$ are\nindependent and belong to the sphere $\\mathbb{S}^{d-1}$ in $\\mathbb{R}^{d}$.We\nprove lower bounds on the minimax risk for estimation of the density\n$f\\_{\\beta}$ over Besov bodies where the loss is a power of the\n$L^p(\\mathbb{S}^{d-1})$ norm for $1\\le p\\le \\infty$. We show that a hard\nthresholding estimator based on a needlet expansion with data-driven thresholds\nachieves these lower bounds up to logarithmic factors. \n\n"}
{"id": "1106.4983", "contents": "Title: Parametric inference and forecasting in continuously invertible\n  volatility models Abstract: We introduce the notion of continuously invertible volatility models that\nrelies on some Lyapunov condition and some regularity condition. We show that\nit is almost equivalent to the ability of the volatilities forecasting using\nthe parametric inference approach based on the SRE given in [16]. Under very\nweak assumptions, we prove the strong consistency and the asymptotic normality\nof the parametric inference. Based on this parametric estimation, a natural\nstrongly consistent forecast of the volatility is given. We apply successfully\nthis approach to recover known results on univariate and multivariate GARCH\ntype models and to the EGARCH(1,1) model. We prove the strong consistency of\nthe forecasting as soon as the model is invertible and the asymptotic normality\nof the parametric inference as soon as the limiting variance exists. Finally,\nwe give some encouraging empirical results of our approach on simulations and\nreal data. \n\n"}
{"id": "1106.5599", "contents": "Title: A pseudo-RIP for multivariate regression Abstract: We give a suitable RI-Property under which recent results for trace\nregression translate into strong risk bounds for multivariate regression. This\npseudo-RIP is compatible with the setting $n < p$. \n\n"}
{"id": "1107.1971", "contents": "Title: Homogeneity and change-point detection tests for multivariate data using\n  rank statistics Abstract: Detecting and locating changes in highly multivariate data is a major concern\nin several current statistical applications. In this context, the first\ncontribution of the paper is a novel non-parametric two-sample homogeneity test\nfor multivariate data based on the well-known Wilcoxon rank statistic. The\nproposed two-sample homogeneity test statistic can be extended to deal with\nordinal or censored data as well as to test for the homogeneity of more than\ntwo samples. The second contribution of the paper concerns the use of the\nproposed test statistic to perform retrospective change-point analysis. It is\nfirst shown that the approach is computationally feasible even when looking for\na large number of change-points thanks to the use of dynamic programming.\nComputable asymptotic $p$-values for the test are then provided in the case\nwhere a single potential change-point is to be detected. Compared to available\nalternatives, the proposed approach appears to be very reliable and robust.\nThis is particularly true in situations where the data is contaminated by\noutliers or corrupted by noise and where the potential changes only affect\nsubsets of the coordinates of the data. \n\n"}
{"id": "1107.2353", "contents": "Title: Blending Bayesian and frequentist methods according to the precision of\n  prior information with an application to hypothesis testing Abstract: The following zero-sum game between nature and a statistician blends Bayesian\nmethods with frequentist methods such as p-values and confidence intervals.\nNature chooses a posterior distribution consistent with a set of possible\npriors. At the same time, the statistician selects a parameter distribution for\ninference with the goal of maximizing the minimum Kullback-Leibler information\ngained over a confidence distribution or other benchmark distribution. An\napplication to testing a simple null hypothesis leads the statistician to\nreport a posterior probability of the hypothesis that is informed by both\nBayesian and frequentist methodology, each weighted according how well the\nprior is known.\n  Since neither the Bayesian approach nor the frequentist approach is entirely\nsatisfactory in situations involving partial knowledge of the prior\ndistribution, the proposed procedure reduces to a Bayesian method given\ncomplete knowledge of the prior, to a frequentist method given complete\nignorance about the prior, and to a blend between the two methods given partial\nknowledge of the prior. The blended approach resembles the Bayesian method\nrather than the frequentist method to the precise extent that the prior is\nknown.\n  The problem of testing a point null hypothesis illustrates the proposed\nframework. The blended probability that the null hypothesis is true is equal to\nthe p-value or a lower bound of an unknown Bayesian posterior probability,\nwhichever is greater. Thus, given total ignorance represented by a lower bound\nof 0, the p-value is used instead of any Bayesian posterior probability. At the\nopposite extreme of a known prior, the p-value is ignored. In the intermediate\ncase, the possible Bayesian posterior probability that is closest to the\np-value is used for inference. Thus, both the Bayesian method and the\nfrequentist method influence the inferences made. \n\n"}
{"id": "1108.4130", "contents": "Title: Supplement paper to \"Online Expectation Maximization based algorithms\n  for inference in hidden Markov models\" Abstract: This is a supplementary material to the paper \"Online Expectation\nMaximization based algorithms for inference in hidden Markov models\". It\ncontains further technical derivations and additional simulation results. \n\n"}
{"id": "1108.5266", "contents": "Title: Fluctuations of an improved population eigenvalue estimator in sample\n  covariance matrix models Abstract: This article provides a central limit theorem for a consistent estimator of\npopulation eigenvalues with large multiplicities based on sample covariance\nmatrices. The focus is on limited sample size situations, whereby the number of\navailable observations is known and comparable in magnitude to the observation\ndimension. An exact expression as well as an empirical, asymptotically\naccurate, approximation of the limiting variance is derived. Simulations are\nperformed that corroborate the theoretical claims. A specific application to\nwireless sensor networks is developed. \n\n"}
{"id": "1109.0392", "contents": "Title: Context Tree Estimation in Variable Length Hidden Markov Models Abstract: We address the issue of context tree estimation in variable length hidden\nMarkov models. We propose an estimator of the context tree of the hidden Markov\nprocess which needs no prior upper bound on the depth of the context tree. We\nprove that the estimator is strongly consistent. This uses\ninformation-theoretic mixture inequalities in the spirit of Finesso and\nLorenzo(Consistent estimation of the order for Markov and hidden Markov\nchains(1990)) and E.Gassiat and S.Boucheron (Optimal error exponents in hidden\nMarkov model order estimation(2003)). We propose an algorithm to efficiently\ncompute the estimator and provide simulation studies to support our result. \n\n"}
{"id": "1109.0524", "contents": "Title: Simultaneous Inference of Covariances Abstract: We consider asymptotic distributions of maximum deviations of sample\ncovariance matrices, a fundamental problem in high-dimensional inference of\ncovariances. Under mild dependence conditions on the entries of the data\nmatrices, we establish the Gumbel convergence of the maximum deviations. Our\nresult substantially generalizes earlier ones where the entries are assumed to\nbe independent and identically distributed, and it provides a theoretical\nfoundation for high-dimensional simultaneous inference of covariances. \n\n"}
{"id": "1109.4533", "contents": "Title: Construction of an informative hierarchical prior for a small sample\n  with the help of historical data and application to electricity load\n  forecasting Abstract: We are interested in the estimation and prediction of a parametric model on a\nshort dataset upon which it is expected to overfit and perform badly. To\novercome the lack of data (relatively to the dimension of the model) we propose\nthe construction of an informative hierarchical Bayesian prior based upon\nanother longer dataset which is assumed to share some similarities with the\noriginal, short dataset. We illustrate the performance of our prior on\nsimulated dataset from three standard models. Then we apply the methodology to\na working model for the electricity load forecasting on real datasets, where it\nleads to a substantial improvement of the quality of the predictions. \n\n"}
{"id": "1109.5278", "contents": "Title: Controlling the degree of caution in statistical inference with the\n  Bayesian and frequentist approaches as opposite extremes Abstract: In statistical practice, whether a Bayesian or frequentist approach is used\nin inference depends not only on the availability of prior information but also\non the attitude taken toward partial prior information, with frequentists\ntending to be more cautious than Bayesians. The proposed framework defines that\nattitude in terms of a specified amount of caution, thereby enabling data\nanalysis at the level of caution desired and on the basis of any prior\ninformation. The caution parameter represents the attitude toward partial prior\ninformation in much the same way as a loss function represents the attitude\ntoward risk. When there is very little prior information and nonzero caution,\nthe resulting inferences correspond to those of the candidate confidence\nintervals and p-values that are most similar to the credible intervals and\nhypothesis probabilities of the specified Bayesian posterior. On the other\nhand, in the presence of a known physical distribution of the parameter,\ninferences are based only on the corresponding physical posterior. In those\nextremes of either negligible prior information or complete prior information,\ninferences do not depend on the degree of caution. Partial prior information\nbetween those two extremes leads to intermediate inferences that are more\nfrequentistic to the extent that the caution is high and more Bayesian to the\nextent that the caution is low. \n\n"}
{"id": "1109.5998", "contents": "Title: Estimating beta-mixing coefficients via histograms Abstract: The literature on statistical learning for time series often assumes\nasymptotic independence or \"mixing\" of the data-generating process. These\nmixing assumptions are never tested, nor are there methods for estimating\nmixing coefficients from data. Additionally, for many common classes of\nprocesses (Markov processes, ARMA processes, etc.) general functional forms for\nvarious mixing rates are known, but not specific coefficients. We present the\nfirst estimator for beta-mixing coefficients based on a single stationary\nsample path and show that it is risk consistent. Since mixing rates depend on\ninfinite-dimensional dependence, we use a Markov approximation based on only a\nfinite memory length $d$. We present convergence rates for the Markov\napproximation and show that as $d\\rightarrow\\infty$, the Markov approximation\nconverges to the true mixing coefficient. Our estimator is constructed using\n$d$-dimensional histogram density estimates. Allowing asymptotics in the\nbandwidth as well as the dimension, we prove $L^1$ concentration for the\nhistogram as an intermediate step. Simulations wherein the mixing rates are\ncalculable and a real-data example demonstrate our methodology. \n\n"}
{"id": "1109.6501", "contents": "Title: A test for Archimedeanity in bivariate copula models Abstract: We propose a new test for the hypothesis that a bivariate copula is an\nArchimedean copula. The test statistic is based on a combination of two\nmeasures resulting from the characterization of Archimedean copulas by the\nproperty of associativity and by a strict upper bound on the diagonal by the\nFr\\'echet-upper bound. We prove weak convergence of this statistic and show\nthat the critical values of the corresponding test can be determined by the\nmultiplier bootstrap method. The test is shown to be consistent against all\ndepartures from Archimedeanity if the copula satisfies weak smoothness\nassumptions. A simulation study is presented which illustrates the finite\nsample properties of the new test. \n\n"}
{"id": "1110.0108", "contents": "Title: Fast approach to the Tracy-Widom law at the edge of GOE and GUE Abstract: We study the rate of convergence for the largest eigenvalue distributions in\nthe Gaussian unitary and orthogonal ensembles to their Tracy-Widom limits. We\nshow that one can achieve an $O(N^{-2/3})$ rate with particular choices of the\ncentering and scaling constants. The arguments here also shed light on more\ncomplicated cases of Laguerre and Jacobi ensembles, in both unitary and\northogonal versions. Numerical work shows that the suggested constants yield\nreasonable approximations, even for surprisingly small values of N. \n\n"}
{"id": "1110.1265", "contents": "Title: Bayesian multivariate mixed-scale density estimation Abstract: Although continuous density estimation has received abundant attention in the\nBayesian nonparametrics literature, there is limited theory on multivariate\nmixed scale density estimation. In this note, we consider a general framework\nto jointly model continuous, count and categorical variables under a\nnonparametric prior, which is induced through rounding latent variables having\nan unknown density with respect to Lebesgue measure. For the proposed class of\npriors, we provide sufficient conditions for large support, strong consistency\nand rates of posterior contraction. These conditions allow one to convert\nsufficient conditions obtained in the setting of multivariate continuous\ndensity estimation to the mixed scale case. To illustrate the procedure a\nrounded multivariate nonparametric mixture of Gaussians is introduced and\napplied to a crime and communities dataset. \n\n"}
{"id": "1111.1044", "contents": "Title: Anisotropic function estimation using multi-bandwidth Gaussian processes Abstract: In nonparametric regression problems involving multiple predictors, there is\ntypically interest in estimating an anisotropic multivariate regression surface\nin the important predictors while discarding the unimportant ones. Our focus is\non defining a Bayesian procedure that leads to the minimax optimal rate of\nposterior contraction (up to a log factor) adapting to the unknown dimension\nand anisotropic smoothness of the true surface. We propose such an approach\nbased on a Gaussian process prior with dimension-specific scalings, which are\nassigned carefully-chosen hyperpriors. We additionally show that using a\nhomogenous Gaussian process with a single bandwidth leads to a sub-optimal rate\nin anisotropic cases. \n\n"}
{"id": "1111.2205", "contents": "Title: Parameter estimation in linear regression driven by a Gaussian sheet Abstract: The problem of estimating the parameters of a linear regression model\n$Z(s,t)=m_1g_1(s,t)+ \\cdots + m_pg_p(s,t)+U(s,t)$ based on observations of $Z$\non a spatial domain $G$ of special shape is considered, where the driving\nprocess $U$ is a Gaussian random field and $g_1, \\ldots, g_p$ are known\nfunctions. Explicit forms of the maximum likelihood estimators of the\nparameters are derived in the cases when $U$ is either a Wiener or a stationary\nor nonstationary Ornstein-Uhlenbeck sheet. Simulation results are also\npresented, where the driving random sheets are simulated with the help of their\nKarhunen-Lo\\`eve expansions. \n\n"}
{"id": "1111.2622", "contents": "Title: Optimal re-centering bounds, with applications to Rosenthal-type\n  concentration of measure inequalities Abstract: For any nonnegative Borel-measurable function f such that f(x)=0 if and only\nif x=0, the best constant c_f in the inequality E f(X-E X) \\leq c_f E f(X) for\nall random variables X with a finite mean is obtained. Properties of the\nconstant c_f in the case when f=|.|^p are studied. Applications to\nconcentration of measure in the form of Rosenthal-type bounds on the moments of\nseparately Lipschitz functions on product spaces are given. \n\n"}
{"id": "1112.0708", "contents": "Title: Information-Theoretically Optimal Compressed Sensing via Spatial\n  Coupling and Approximate Message Passing Abstract: We study the compressed sensing reconstruction problem for a broad class of\nrandom, band-diagonal sensing matrices. This construction is inspired by the\nidea of spatial coupling in coding theory. As demonstrated heuristically and\nnumerically by Krzakala et al. \\cite{KrzakalaEtAl}, message passing algorithms\ncan effectively solve the reconstruction problem for spatially coupled\nmeasurements with undersampling rates close to the fraction of non-zero\ncoordinates.\n  We use an approximate message passing (AMP) algorithm and analyze it through\nthe state evolution method. We give a rigorous proof that this approach is\nsuccessful as soon as the undersampling rate $\\delta$ exceeds the (upper)\nR\\'enyi information dimension of the signal, $\\uRenyi(p_X)$. More precisely,\nfor a sequence of signals of diverging dimension $n$ whose empirical\ndistribution converges to $p_X$, reconstruction is with high probability\nsuccessful from $\\uRenyi(p_X)\\, n+o(n)$ measurements taken according to a band\ndiagonal matrix.\n  For sparse signals, i.e., sequences of dimension $n$ and $k(n)$ non-zero\nentries, this implies reconstruction from $k(n)+o(n)$ measurements. For\n`discrete' signals, i.e., signals whose coordinates take a fixed finite set of\nvalues, this implies reconstruction from $o(n)$ measurements. The result is\nrobust with respect to noise, does not apply uniquely to random signals, but\nrequires the knowledge of the empirical distribution of the signal $p_X$. \n\n"}
{"id": "1112.2080", "contents": "Title: Asymptotic inference in system identification for the atom maser Abstract: System identification is an integrant part of control theory and plays an\nincreasing role in quantum engineering. In the quantum set-up, system\nidentification is usually equated to process tomography, i.e. estimating a\nchannel by probing it repeatedly with different input states. However for\nquantum dynamical systems like quantum Markov processes, it is more natural to\nconsider the estimation based on continuous measurements of the output, with a\ngiven input which may be stationary. We address this problem using asymptotic\nstatistics tools, for the specific example of estimating the Rabi frequency of\nan atom maser. We compute the Fisher information of different measurement\nprocesses as well as the quantum Fisher information of the atom maser, and\nestablish the local asymptotic normality of these statistical models. The\nstatistical notions can be expressed in terms of spectral properties of certain\ndeformed Markov generators and the connection to large deviations is briefly\ndiscussed. \n\n"}
{"id": "1112.2381", "contents": "Title: Edge universality of correlation matrices Abstract: Let $\\widetilde{X}_{M\\times N}$ be a rectangular data matrix with independent\nreal-valued entries $[\\widetilde{x}_{ij}]$ satisfying $\\mathbb\n{E}\\widetilde{x}_{ij}=0$ and $\\mathbb {E}\\widetilde{x}^2_{ij}=\\frac{1}{M}$,\n$N,M\\to\\infty$. These entries have a subexponential decay at the tails. We will\nbe working in the regime $N/M=d_N,\\lim_{N\\to\\infty}d_N\\neq0,1,\\infty$. In this\npaper we prove the edge universality of correlation matrices ${X}^{\\dagger}X$,\nwhere the rectangular matrix $X$ (called the standardized matrix) is obtained\nby normalizing each column of the data matrix $\\widetilde{X}$ by its Euclidean\nnorm. Our main result states that asymptotically the $k$-point ($k\\geq1$)\ncorrelation functions of the extreme eigenvalues (at both edges of the\nspectrum) of the correlation matrix ${X}^{\\dagger}X$ converge to those of the\nGaussian correlation matrix, that is, Tracy-Widom law, and, thus, in\nparticular, the largest and the smallest eigenvalues of ${X}^{\\dagger}X$ after\nappropriate centering and rescaling converge to the Tracy-Widom distribution.\nThe asymptotic distribution of extreme eigenvalues of the Gaussian correlation\nmatrix has been worked out only recently. As a corollary of the main result in\nthis paper, we also obtain that the extreme eigenvalues of Gaussian correlation\nmatrices are asymptotically distributed according to the Tracy-Widom law. The\nproof is based on the comparison of Green functions, but the key obstacle to be\nsurmounted is the strong dependence of the entries of the correlation matrix.\nWe achieve this via a novel argument which involves comparing the moments of\nproduct of the entries of the standardized data matrix to those of the raw data\nmatrix. Our proof strategy may be extended for proving the edge universality of\nother random matrix ensembles with dependent entries and hence is of\nindependent interest. \n\n"}
{"id": "1112.3450", "contents": "Title: The sparse Laplacian shrinkage estimator for high-dimensional regression Abstract: We propose a new penalized method for variable selection and estimation that\nexplicitly incorporates the correlation patterns among predictors. This method\nis based on a combination of the minimax concave penalty and Laplacian\nquadratic associated with a graph as the penalty function. We call it the\nsparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave\npenalty for encouraging sparsity and Laplacian quadratic penalty for promoting\nsmoothness among coefficients associated with the correlated predictors. The\nSLS has a generalized grouping property with respect to the graph represented\nby the Laplacian quadratic. We show that the SLS possesses an oracle property\nin the sense that it is selection consistent and equal to the oracle Laplacian\nshrinkage estimator with high probability. This result holds in sparse,\nhigh-dimensional settings with p >> n under reasonable conditions. We derive a\ncoordinate descent algorithm for computing the SLS estimates. Simulation\nstudies are conducted to evaluate the performance of the SLS method and a real\ndata example is used to illustrate its application. \n\n"}
{"id": "1112.3914", "contents": "Title: Robust empirical mean Estimators Abstract: We study robust estimators of the mean of a probability measure $P$, called\nrobust empirical mean estimators. This elementary construction is then used to\nrevisit a problem of aggregation and a problem of estimator selection,\nextending these methods to not necessarily bounded collections of previous\nestimators. We consider then the problem of robust $M$-estimation. We propose a\nslightly more complicated construction to handle this problem and, as examples\nof applications, we apply our general approach to least-squares density\nestimation, to density estimation with K\\\"ullback loss and to a non-Gaussian,\nunbounded, random design and heteroscedastic regression problem. Finally, we\nshow that our strategy can be used when the data are only assumed to be mixing. \n\n"}
{"id": "1112.5806", "contents": "Title: Complex-Valued Best Linear Unbiased Estimator of an Unknown Constant\n  Mean of White Noise Abstract: In this paper the complex-valued best linear unbiased estimator of an unknown\nconstant mean of white noise was derived the ordinary least-squares estimator\nof an unknown constant mean of random field (arithmetic mean) charged by an\nimaginary error. \n\n"}
{"id": "1201.3102", "contents": "Title: A note on Bayesian convergence rates under local prior support\n  conditions Abstract: Bounds on Bayesian posterior convergence rates, assuming the prior satisfies\nboth local and global support conditions, are now readily available. In this\npaper we explore, in the context of density estimation, Bayesian convergence\nrates assuming only local prior support conditions. Our results give optimal\nrates under minimal conditions using very simple arguments. \n\n"}
{"id": "1202.0391", "contents": "Title: Parametric or nonparametric? A parametricness index for model selection Abstract: In model selection literature, two classes of criteria perform well\nasymptotically in different situations: Bayesian information criterion (BIC)\n(as a representative) is consistent in selection when the true model is finite\ndimensional (parametric scenario); Akaike's information criterion (AIC)\nperforms well in an asymptotic efficiency when the true model is infinite\ndimensional (nonparametric scenario). But there is little work that addresses\nif it is possible and how to detect the situation that a specific model\nselection problem is in. In this work, we differentiate the two scenarios\ntheoretically under some conditions. We develop a measure, parametricness index\n(PI), to assess whether a model selected by a potentially consistent procedure\ncan be practically treated as the true model, which also hints on AIC or BIC is\nbetter suited for the data for the goal of estimating the regression function.\nA consequence is that by switching between AIC and BIC based on the PI, the\nresulting regression estimator is simultaneously asymptotically efficient for\nboth parametric and nonparametric scenarios. In addition, we systematically\ninvestigate the behaviors of PI in simulation and real data and show its\nusefulness. \n\n"}
{"id": "1202.1212", "contents": "Title: Robust 1-bit compressed sensing and sparse logistic regression: A convex\n  programming approach Abstract: This paper develops theoretical results regarding noisy 1-bit compressed\nsensing and sparse binomial regression. We show that a single convex program\ngives an accurate estimate of the signal, or coefficient vector, for both of\nthese models. We demonstrate that an s-sparse signal in R^n can be accurately\nestimated from m = O(slog(n/s)) single-bit measurements using a simple convex\nprogram. This remains true even if each measurement bit is flipped with\nprobability nearly 1/2. Worst-case (adversarial) noise can also be accounted\nfor, and uniform results that hold for all sparse inputs are derived as well.\nIn the terminology of sparse logistic regression, we show that O(slog(n/s))\nBernoulli trials are sufficient to estimate a coefficient vector in R^n which\nis approximately s-sparse. Moreover, the same convex program works for\nvirtually all generalized linear models, in which the link function may be\nunknown. To our knowledge, these are the first results that tie together the\ntheory of sparse logistic regression to 1-bit compressed sensing. Our results\napply to general signal structures aside from sparsity; one only needs to know\nthe size of the set K where signals reside. The size is given by the mean width\nof K, a computable quantity whose square serves as a robust extension of the\ndimension. \n\n"}
{"id": "1202.2211", "contents": "Title: Nonparametric estimation of the jump rate for non-homogeneous marked\n  renewal processes Abstract: This paper is devoted to the nonparametric estimation of the jump rate and\nthe cumulative rate for a general class of non-homogeneous marked renewal\nprocesses, defined on a separable metric space. In our framework, the\nestimation needs only one observation of the process within a long time. Our\napproach is based on a generalization of the multiplicative intensity model,\nintroduced by Aalen in the seventies. We provide consistent estimators of these\ntwo functions, under some assumptions related to the ergodicity of an embedded\nchain and the characteristics of the process. The paper is illustrated by a\nnumerical example. \n\n"}
{"id": "1202.2212", "contents": "Title: Nonparametric estimation of the conditional distribution of the\n  inter-jumping times for piecewise-deterministic Markov processes Abstract: This paper presents a nonparametric method for estimating the conditional\ndensity associated to the jump rate of a piecewise-deterministic Markov\nprocess. In our framework, the estimation needs only one observation of the\nprocess within a long time interval. Our method relies on a generalization of\nAalen's multiplicative intensity model. We prove the uniform consistency of our\nestimator, under some reasonable assumptions related to the primitive\ncharacteristics of the process. A simulation example illustrates the behavior\nof our estimator. \n\n"}
{"id": "1202.4294", "contents": "Title: Prediction of quantiles by statistical learning and application to GDP\n  forecasting Abstract: In this paper, we tackle the problem of prediction and confidence intervals\nfor time series using a statistical learning approach and quantile loss\nfunctions. In a first time, we show that the Gibbs estimator (also known as\nExponentially Weighted aggregate) is able to predict as well as the best\npredictor in a given family for a wide set of loss functions. In particular,\nusing the quantile loss function of Koenker and Bassett (1978), this allows to\nbuild confidence intervals. We apply these results to the problem of prediction\nand confidence regions for the French Gross Domestic Product (GDP) growth, with\npromising results. \n\n"}
{"id": "1202.6316", "contents": "Title: Block thresholding for wavelet-based estimation of function derivatives\n  from a heteroscedastic multichannel convolution model Abstract: We observe $n$ heteroscedastic stochastic processes $\\{Y_v(t)\\}_{v}$, where\nfor any $v\\in\\{1,\\ldots,n\\}$ and $t \\in [0,1]$, $Y_v(t)$ is the convolution\nproduct of an unknown function $f$ and a known blurring function $g_v$\ncorrupted by Gaussian noise. Under an ordinary smoothness assumption on\n$g_1,\\ldots,g_n$, our goal is to estimate the $d$-th derivatives (in weak\nsense) of $f$ from the observations. We propose an adaptive estimator based on\nwavelet block thresholding, namely the \"BlockJS estimator\". Taking the mean\nintegrated squared error (MISE), our main theoretical result investigates the\nminimax rates over Besov smoothness spaces, and shows that our block estimator\ncan achieve the optimal minimax rate, or is at least nearly-minimax in the\nleast favorable situation. We also report a comprehensive suite of numerical\nsimulations to support our theoretical findings. The practical performance of\nour block estimator compares very favorably to existing methods of the\nliterature on a large set of test functions. \n\n"}
{"id": "1203.0565", "contents": "Title: Fast learning rate of multiple kernel learning: Trade-off between\n  sparsity and smoothness Abstract: We investigate the learning rate of multiple kernel learning (MKL) with\n$\\ell_1$ and elastic-net regularizations. The elastic-net regularization is a\ncomposition of an $\\ell_1$-regularizer for inducing the sparsity and an\n$\\ell_2$-regularizer for controlling the smoothness. We focus on a sparse\nsetting where the total number of kernels is large, but the number of nonzero\ncomponents of the ground truth is relatively small, and show sharper\nconvergence rates than the learning rates have ever shown for both $\\ell_1$ and\nelastic-net regularizations. Our analysis reveals some relations between the\nchoice of a regularization function and the performance. If the ground truth is\nsmooth, we show a faster convergence rate for the elastic-net regularization\nwith less conditions than $\\ell_1$-regularization; otherwise, a faster\nconvergence rate for the $\\ell_1$-regularization is shown. \n\n"}
{"id": "1203.0839", "contents": "Title: Accuracy of the Tracy--Widom limits for the extreme eigenvalues in white\n  Wishart matrices Abstract: The distributions of the largest and the smallest eigenvalues of a\n$p$-variate sample covariance matrix $S$ are of great importance in statistics.\nFocusing on the null case where $nS$ follows the standard Wishart distribution\n$W_p(I,n)$, we study the accuracy of their scaling limits under the setting:\n$n/p\\rightarrow \\gamma\\in(0,\\infty)$ as $n\\rightarrow \\infty$. The limits here\nare the orthogonal Tracy--Widom law and its reflection about the origin. With\ncarefully chosen rescaling constants, the approximation to the rescaled largest\neigenvalue distribution by the limit attains accuracy of order ${\\mathrm\n{O}({\\min(n,p)^{-2/3}})}$. If $\\gamma>1$, the same order of accuracy is\nobtained for the smallest eigenvalue after incorporating an additional log\ntransform. Numerical results show that the relative error of approximation at\nconventional significance levels is reduced by over 50% in rectangular and over\n75% in `thin' data matrix settings, even with $\\min(n,p)$ as small as 2. \n\n"}
{"id": "1203.2675", "contents": "Title: Quantum Simpsons Paradox and High Order Bell-Tsirelson Inequalities Abstract: The well-known Simpson's Paradox, or Yule-Simpson Effect, in statistics is\noften illustrated by the following thought experiment: A drug may be found in a\ntrial to increase the survival rate for both men and women, but decrease the\nrate for all the subjects as a whole. This paradoxical reversal effect has been\nfound in numerous datasets across many disciplines, and is now included in most\nintroductory statistics textbooks. In the language of the drug trial, the\neffect is impossible, however, if both treatment groups' survival rates are\nhigher than both control groups'. Here we show that for quantum probabilities,\nsuch a reversal remains possible. In particular, a \"quantum drug\", so to speak,\ncould be life-saving for both men and women yet deadly for the whole\npopulation. We further identify a simple inequality on conditional\nprobabilities that must hold classically but is violated by our quantum\nscenarios, and completely characterize the maximum quantum violation. As\npolynomial inequalities on entries of the density operator, our inequalities\nare of degree 6. \n\n"}
{"id": "1203.6574", "contents": "Title: Tail asymptotics for cumulative processes sampled at heavy-tailed random\n  times with applications to queueing models in Markovian environments Abstract: This paper considers the tail asymptotics for a cumulative process $\\{B(t); t\n\\ge 0\\}$ sampled at a heavy-tailed random time $T$. The main contribution of\nthis paper is to establish several sufficient conditions for the asymptotic\nequality ${\\sf P}(B(T) > bx) \\sim {\\sf P}(M(T) > bx) \\sim {\\sf P}(T>x)$ as $x\n\\to \\infty$, where $M(t) = \\sup_{0 \\le u \\le t}B(u)$ and $b$ is a certain\npositive constant. The main results of this paper can be used to obtain the\nsubexponential asymptotics for various queueing models in Markovian\nenvironments. As an example, using the main results, we derive subexponential\nasymptotic formulas for the loss probability of a single-server finite-buffer\nqueue with an on/off arrival process in a Markovian environment. \n\n"}
{"id": "1204.1664", "contents": "Title: Optimally-Weighted Herding is Bayesian Quadrature Abstract: Herding and kernel herding are deterministic methods of choosing samples\nwhich summarise a probability distribution. A related task is choosing samples\nfor estimating integrals using Bayesian quadrature. We show that the criterion\nminimised when selecting samples in kernel herding is equivalent to the\nposterior variance in Bayesian quadrature. We then show that sequential\nBayesian quadrature can be viewed as a weighted version of kernel herding which\nachieves performance superior to any other weighted herding method. We\ndemonstrate empirically a rate of convergence faster than O(1/N). Our results\nalso imply an upper bound on the empirical error of the Bayesian quadrature\nestimate. \n\n"}
{"id": "1204.2109", "contents": "Title: On the asymptotic normality of finite population L-statistics Abstract: We give sufficient conditions for the asymptotic normality of linear\ncombinations of order statistics (L-statistics) in the case of simple random\nsamples without replacement. In the first case, restrictions are imposed on the\nweights of L-statistics. The second case is on trimmed means, where we\nintroduce a new finite population smoothness condition. \n\n"}
{"id": "1204.2296", "contents": "Title: Co-clustering for directed graphs: the Stochastic co-Blockmodel and\n  spectral algorithm Di-Sim Abstract: Directed graphs have asymmetric connections, yet the current graph clustering\nmethodologies cannot identify the potentially global structure of these\nasymmetries. We give a spectral algorithm called di-sim that builds on a dual\nmeasure of similarity that correspond to how a node (i) sends and (ii) receives\nedges. Using di-sim, we analyze the global asymmetries in the networks of Enron\nemails, political blogs, and the c elegans neural connectome. In each example,\na small subset of nodes have persistent asymmetries; these nodes send edges\nwith one cluster, but receive edges with another cluster. Previous approaches\nwould have assigned these asymmetric nodes to only one cluster, failing to\nidentify their sending/receiving asymmetries. Regularization and \"projection\"\nare two steps of di-sim that are essential for spectral clustering algorithms\nto work in practice. The theoretical results show that these steps make the\nalgorithm weakly consistent under the degree corrected Stochastic\nco-Blockmodel, a model that generalizes the Stochastic Blockmodel to allow for\nboth (i) degree heterogeneity and (ii) the global asymmetries that we intend to\ndetect. The theoretical results make no assumptions on the smallest degree\nnodes. Instead, the theorem requires that the average degree grows sufficiently\nfast and that the weak consistency only applies to the subset of the nodes with\nsufficiently large leverage scores. The results results also apply to bipartite\ngraphs. \n\n"}
{"id": "1204.4268", "contents": "Title: Deviation probability bounds for fractional martingales and related\n  remarks Abstract: In this paper we prove exponential inequalities (also called Bernstein's\ninequality) for fractional martingales. As an immediate corollary, we will\ndiscuss weak law of large numbers for fractional martingales under divergence\nassumption on the $\\beta-$variation of the fractional martingale. A non trivial\nexample of application of this convergence result is proposed. \n\n"}
{"id": "1204.5357", "contents": "Title: Learning AMP Chain Graphs under Faithfulness Abstract: This paper deals with chain graphs under the alternative\nAndersson-Madigan-Perlman (AMP) interpretation. In particular, we present a\nconstraint based algorithm for learning an AMP chain graph a given probability\ndistribution is faithful to. We also show that the extension of Meek's\nconjecture to AMP chain graphs does not hold, which compromises the development\nof efficient and correct score+search learning algorithms under assumptions\nweaker than faithfulness. \n\n"}
{"id": "1204.6382", "contents": "Title: Uniform convergence and asymptotic confidence bands for model-assisted\n  estimators of the mean of sampled functional data Abstract: When the study variable is functional and storage capacities are limited or\ntransmission costs are high, selecting with survey sampling techniques a small\nfraction of the observations is an interesting alternative to signal\ncompression techniques, particularly when the goal is the estimation of simple\nquantities such as means or totals. We extend, in this functional framework,\nmodel-assisted estimators with linear regression models that can take account\nof auxiliary variables whose totals over the population are known. We first\nshow, under weak hypotheses on the sampling design and the regularity of the\ntrajectories, that the estimator of the mean function as well as its variance\nestimator are uniformly consistent. Then, under additional assumptions, we\nprove a functional central limit theorem and we assess rigorously a fast\ntechnique based on simulations of Gaussian processes which is employed to build\nasymptotic confidence bands. The accuracy of the variance function estimator is\nevaluated on a real dataset of sampled electricity consumption curves measured\nevery half an hour over a period of one week. \n\n"}
{"id": "1205.3658", "contents": "Title: Random coefficients bifurcating autoregressive processes Abstract: This paper presents a model of asymmetric bifurcating autoregressive process\nwith random coefficients. We couple this model with a Galton Watson tree to\ntake into account possibly missing observations. We propose least-squares\nestimators for the various parameters of the model and prove their consistency\nwith a convergence rate, and their asymptotic normality. We use both the\nbifurcating Markov chain and martingale approaches and derive new important\ngeneral results in both these frameworks. \n\n"}
{"id": "1205.3703", "contents": "Title: Generic chaining and the l1-penalty Abstract: We address the choice of the tuning parameter $\\lambda$ in $\\ell_1$-penalized\nM-estimation. Our main concern is models which are highly nonlinear, such as\nthe Gaussian mixture model. The number of parameters $p$ is moreover large,\npossibly larger than the number of observations $n$. The generic chaining\ntechnique of Talagrand[2005] is tailored for this problem. It leads to the\nchoice $\\lambda \\asymp \\sqrt {\\log p / n}$, as in the standard Lasso procedure\n(which concerns the linear model and least squares loss). \n\n"}
{"id": "1205.4094", "contents": "Title: Bandit Theory meets Compressed Sensing for high dimensional Stochastic\n  Linear Bandit Abstract: We consider a linear stochastic bandit problem where the dimension $K$ of the\nunknown parameter $\\theta$ is larger than the sampling budget $n$. In such\ncases, it is in general impossible to derive sub-linear regret bounds since\nusual linear bandit algorithms have a regret in $O(K\\sqrt{n})$. In this paper\nwe assume that $\\theta$ is $S-$sparse, i.e. has at most $S-$non-zero\ncomponents, and that the space of arms is the unit ball for the $||.||_2$ norm.\nWe combine ideas from Compressed Sensing and Bandit Theory and derive\nalgorithms with regret bounds in $O(S\\sqrt{n})$. \n\n"}
{"id": "1205.5050", "contents": "Title: A lasso for hierarchical interactions Abstract: We add a set of convex constraints to the lasso to produce sparse interaction\nmodels that honor the hierarchy restriction that an interaction only be\nincluded in a model if one or both variables are marginally important. We give\na precise characterization of the effect of this hierarchy constraint, prove\nthat hierarchy holds with probability one and derive an unbiased estimate for\nthe degrees of freedom of our estimator. A bound on this estimate reveals the\namount of fitting \"saved\" by the hierarchy constraint. We distinguish between\nparameter sparsity - the number of nonzero coefficients - and practical\nsparsity - the number of raw variables one must measure to make a new\nprediction. Hierarchy focuses on the latter, which is more closely tied to\nimportant data collection concerns such as cost, time and effort. We develop an\nalgorithm, available in the R package hierNet, and perform an empirical study\nof our method. \n\n"}
{"id": "1205.5658", "contents": "Title: Bayesian computation via empirical likelihood Abstract: Approximate Bayesian computation (ABC) has become an essential tool for the\nanalysis of complex stochastic models when the likelihood function is\nnumerically unavailable. However, the well-established statistical method of\nempirical likelihood provides another route to such settings that bypasses\nsimulations from the model and the choices of the ABC parameters (summary\nstatistics, distance, tolerance), while being convergent in the number of\nobservations. Furthermore, bypassing model simulations may lead to significant\ntime savings in complex models, for instance those found in population\ngenetics. The BCel algorithm we develop in this paper also provides an\nevaluation of its own performance through an associated effective sample size.\nThe method is illustrated using several examples, including estimation of\nstandard distributions, time series, and population genetics models. \n\n"}
{"id": "1205.6248", "contents": "Title: Some Counterexamples Concerning Maximal Correlation and Linear\n  Regression Abstract: A class of examples concerning the relationship of linear regression and\nmaximal correlation is provided. More precisely, these examples show that if\ntwo random variables have (strictly) linear regression on each other, then\ntheir maximal correlation is not necessarily equal to their (absolute)\ncorrelation. \n\n"}
{"id": "1205.6644", "contents": "Title: Simultaneous confidence bands for Yule-Walker estimators and order\n  selection Abstract: Let $\\{X_k,k\\in{\\mathbb{Z}}\\}$ be an autoregressive process of order $q$.\nVarious estimators for the order $q$ and the parameters ${\\bolds\n\\Theta}_q=(\\theta_1,...,\\theta_q)^T$ are known; the order is usually determined\nwith Akaike's criterion or related modifications, whereas Yule-Walker, Burger\nor maximum likelihood estimators are used for the parameters\n${\\bolds\\Theta}_q$. In this paper, we establish simultaneous confidence bands\nfor the Yule--Walker estimators $\\hat{\\theta}_i$; more precisely, it is shown\nthat the limiting distribution of ${\\max_{1\\leq i\\leq\nd_n}}|\\hat{\\theta}_i-\\theta_i|$ is the Gumbel-type distribution $e^{-e^{-z}}$,\nwhere $q\\in\\{0,...,d_n\\}$ and $d_n=\\mathcal {O}(n^{\\delta})$, $\\delta >0$. This\nallows to modify some of the currently used criteria (AIC, BIC, HQC, SIC), but\nalso yields a new class of consistent estimators for the order $q$. These\nestimators seem to have some potential, since they outperform most of the\npreviously mentioned criteria in a small simulation study. In particular, if\nsome of the parameters $\\{\\theta_i\\}_{1\\leq i\\leq d_n}$ are zero or close to\nzero, a significant improvement can be observed. As a byproduct, it is shown\nthat BIC, HQC and SIC are consistent for $q\\in\\{0,...,d_n\\}$ where\n$d_n=\\mathcal {O}(n^{\\delta})$. \n\n"}
{"id": "1205.6659", "contents": "Title: Q-learning with censored data Abstract: We develop methodology for a multistage decision problem with flexible number\nof stages in which the rewards are survival times that are subject to\ncensoring. We present a novel Q-learning algorithm that is adjusted for\ncensored data and allows a flexible number of stages. We provide finite sample\nbounds on the generalization error of the policy learned by the algorithm, and\nshow that when the optimal Q-function belongs to the approximation space, the\nexpected survival time for policies obtained by the algorithm converges to that\nof the optimal policy. We simulate a multistage clinical trial with flexible\nnumber of stages and apply the proposed censored-Q-learning algorithm to find\nindividualized treatment regimens. The methodology presented in this paper has\nimplications in the design of personalized medicine trials in cancer and in\nother life-threatening diseases. \n\n"}
{"id": "1205.6687", "contents": "Title: Planification d'exp\\'eriences s\\'equentielle dans un contexte de\n  m\\'eta-mod\\'elisation multi-fid\\'elit\\'e Abstract: Large computer codes are widely used in engineering to study physical\nsystems. Nevertheless, simulations can sometimes be time-consuming. In this\ncase, an approximation of the code input/output relation is made using a\nmetamodel. Actually, a computer code can often be run at different levels of\ncomplexity and a hierarchy of levels of code can hence be obtained. For\nexample, it can be a finite element model with a more or less fine mesh. The\naim of our research is to study the use of several levels of a code to predict\nthe output of a costly computer code. The presented multi-stage metamodel is a\nparticular case of co-kriging which is a well-known geostatistical method. We\nfirst describe the construction of the co-kriging model and we focus then on a\nsequential experimental design strategy. Indeed, one of the strengths of\nco-kriging is that it provides through the predictive co-kriging variance an\nestimation of the model error at each point of the input space parameter.\nTherefore, to improve the surrogate model we can sequentially add points in the\ntraining set at locations where the predictive variances are the largest ones.\nNonetheless, in a multi-fidelity framework, we also have to choose which level\nof code we have to run. We present here different strategies to choose this\nlevel. They are based on an original result which gives the contribution of\neach code on the co-kriging variance. \n\n"}
{"id": "1206.0313", "contents": "Title: The Lasso Problem and Uniqueness Abstract: The lasso is a popular tool for sparse linear regression, especially for\nproblems in which the number of variables p exceeds the number of observations\nn. But when p>n, the lasso criterion is not strictly convex, and hence it may\nnot have a unique minimum. An important question is: when is the lasso solution\nwell-defined (unique)? We review results from the literature, which show that\nif the predictor variables are drawn from a continuous probability\ndistribution, then there is a unique lasso solution with probability one,\nregardless of the sizes of n and p. We also show that this result extends\neasily to $\\ell_1$ penalized minimization problems over a wide range of loss\nfunctions.\n  A second important question is: how can we deal with the case of\nnon-uniqueness in lasso solutions? In light of the aforementioned result, this\ncase really only arises when some of the predictor variables are discrete, or\nwhen some post-processing has been performed on continuous predictor\nmeasurements. Though we certainly cannot claim to provide a complete answer to\nsuch a broad question, we do present progress towards understanding some\naspects of non-uniqueness. First, we extend the LARS algorithm for computing\nthe lasso solution path to cover the non-unique case, so that this path\nalgorithm works for any predictor matrix. Next, we derive a simple method for\ncomputing the component-wise uncertainty in lasso solutions of any given\nproblem instance, based on linear programming. Finally, we review results from\nthe literature on some of the unifying properties of lasso solutions, and also\npoint out particular forms of solutions that have distinctive properties. \n\n"}
{"id": "1206.0333", "contents": "Title: Sparse Trace Norm Regularization Abstract: We study the problem of estimating multiple predictive functions from a\ndictionary of basis functions in the nonparametric regression setting. Our\nestimation scheme assumes that each predictive function can be estimated in the\nform of a linear combination of the basis functions. By assuming that the\ncoefficient matrix admits a sparse low-rank structure, we formulate the\nfunction estimation problem as a convex program regularized by the trace norm\nand the $\\ell_1$-norm simultaneously. We propose to solve the convex program\nusing the accelerated gradient (AG) method and the alternating direction method\nof multipliers (ADMM) respectively; we also develop efficient algorithms to\nsolve the key components in both AG and ADMM. In addition, we conduct\ntheoretical analysis on the proposed function estimation scheme: we derive a\nkey property of the optimal solution to the convex program; based on an\nassumption on the basis functions, we establish a performance bound of the\nproposed function estimation scheme (via the composite regularization).\nSimulation studies demonstrate the effectiveness and efficiency of the proposed\nalgorithms. \n\n"}
{"id": "1206.0457", "contents": "Title: Independent component analysis via nonparametric maximum likelihood\n  estimation Abstract: Independent Component Analysis (ICA) models are very popular semiparametric\nmodels in which we observe independent copies of a random vector $X = AS$,\nwhere $A$ is a non-singular matrix and $S$ has independent components. We\npropose a new way of estimating the unmixing matrix $W = A^{-1}$ and the\nmarginal distributions of the components of $S$ using nonparametric maximum\nlikelihood. Specifically, we study the projection of the empirical distribution\nonto the subset of ICA distributions having log-concave marginals. We show\nthat, from the point of view of estimating the unmixing matrix, it makes no\ndifference whether or not the log-concavity is correctly specified. The\napproach is further justified by both theoretical results and a simulation\nstudy. \n\n"}
{"id": "1206.1898", "contents": "Title: A Nonparametric Conjugate Prior Distribution for the Maximizing Argument\n  of a Noisy Function Abstract: We propose a novel Bayesian approach to solve stochastic optimization\nproblems that involve finding extrema of noisy, nonlinear functions. Previous\nwork has focused on representing possible functions explicitly, which leads to\na two-step procedure of first, doing inference over the function space and\nsecond, finding the extrema of these functions. Here we skip the representation\nstep and directly model the distribution over extrema. To this end, we devise a\nnon-parametric conjugate prior based on a kernel regressor. The resulting\nposterior distribution directly captures the uncertainty over the maximum of\nthe unknown function. We illustrate the effectiveness of our model by\noptimizing a noisy, high-dimensional, non-convex objective function. \n\n"}
{"id": "1206.3627", "contents": "Title: Posterior contraction in sparse Bayesian factor models for massive\n  covariance matrices Abstract: Sparse Bayesian factor models are routinely implemented for parsimonious\ndependence modeling and dimensionality reduction in high-dimensional\napplications. We provide theoretical understanding of such Bayesian procedures\nin terms of posterior convergence rates in inferring high-dimensional\ncovariance matrices where the dimension can be larger than the sample size.\nUnder relevant sparsity assumptions on the true covariance matrix, we show that\ncommonly-used point mass mixture priors on the factor loadings lead to\nconsistent estimation in the operator norm even when $p\\gg n$. One of our major\ncontributions is to develop a new class of continuous shrinkage priors and\nprovide insights into their concentration around sparse vectors. Using such\npriors for the factor loadings, we obtain similar rate of convergence as\nobtained with point mass mixture priors. To obtain the convergence rates, we\nconstruct test functions to separate points in the space of high-dimensional\ncovariance matrices using insights from random matrix theory; the tools\ndeveloped may be of independent interest. We also derive minimax rates and show\nthat the Bayesian posterior rates of convergence coincide with the minimax\nrates upto a $\\sqrt{\\log n}$ term. \n\n"}
{"id": "1206.3911", "contents": "Title: Saturated fractions of two-factor designs Abstract: In this paper we study saturated fractions of a two-factor design under the\nsimple effect model. In particular, we define a criterion to check whether a\ngiven fraction is saturated or not, and we compute the number of saturated\nfractions. All proofs are constructive and can be used as actual methods to\nbuild saturated fractions. Moreover, we show how the theory of Markov bases for\ncontingency tables can be applied to two-factor designs for moving between the\ndesigns with given margins. \n\n"}
{"id": "1206.4285", "contents": "Title: Exponential weighting and oracle inequalities for projection methods Abstract: We consider the problem of recovering an unknown vector from noisy data with\nthe help of projection estimates. The goal is to find a convex combination of\nthese estimates with the minimal risk. We study an aggregation method based on\nthe so-called exponential weighting and provide a new upper bound for the mean\nsquare risk of this method. \n\n"}
{"id": "1206.5637", "contents": "Title: What you can do with Coordinated Samples Abstract: Sample coordination, where similar instances have similar samples, was\nproposed by statisticians four decades ago as a way to maximize overlap in\nrepeated surveys. Coordinated sampling had been since used for summarizing\nmassive data sets.\n  The usefulness of a sampling scheme hinges on the scope and accuracy within\nwhich queries posed over the original data can be answered from the sample. We\naim here to gain a fundamental understanding of the limits and potential of\ncoordination. Our main result is a precise characterization, in terms of simple\nproperties of the estimated function, of queries for which estimators with\ndesirable properties exist. We consider unbiasedness, nonnegativity, finite\nvariance, and bounded estimates.\n  Since generally a single estimator can not be optimal (minimize variance\nsimultaneously) for all data, we propose {\\em variance competitiveness}, which\nmeans that the expectation of the square on any data is not too far from the\nminimum one possible for the data. Surprisingly perhaps, we show how to\nconstruct, for any function for which an unbiased nonnegative estimator exists,\na variance competitive estimator. \n\n"}
{"id": "1206.6128", "contents": "Title: Leave-one-out cross-validation is risk consistent for lasso Abstract: The lasso procedure is ubiquitous in the statistical and signal processing\nliterature, and as such, is the target of substantial theoretical and applied\nresearch. While much of this research focuses on the desirable properties that\nlasso possesses---predictive risk consistency, sign consistency, correct model\nselection---all of it has assumes that the tuning parameter is chosen in an\noracle fashion. Yet, this is impossible in practice. Instead, data analysts\nmust use the data twice, once to choose the tuning parameter and again to\nestimate the model. But only heuristics have ever justified such a procedure.\nTo this end, we give the first definitive answer about the risk consistency of\nlasso when the smoothing parameter is chosen via cross-validation. We show that\nunder some restrictions on the design matrix, the lasso estimator is still risk\nconsistent with an empirically chosen tuning parameter. \n\n"}
{"id": "1206.6367", "contents": "Title: A comparison of the discrete Kolmogorov-Smirnov statistic and the\n  Euclidean distance Abstract: Goodness-of-fit tests gauge whether a given set of observations is consistent\n(up to expected random fluctuations) with arising as independent and\nidentically distributed (i.i.d.) draws from a user-specified probability\ndistribution known as the \"model.\" The standard gauges involve the discrepancy\nbetween the model and the empirical distribution of the observed draws. Some\nmeasures of discrepancy are cumulative; others are not. The most popular\ncumulative measure is the Kolmogorov-Smirnov statistic; when all probability\ndistributions under consideration are discrete, a natural noncumulative measure\nis the Euclidean distance between the model and the empirical distributions. In\nthe present paper, both mathematical analysis and its illustration via various\ndata sets indicate that the Kolmogorov-Smirnov statistic tends to be more\npowerful than the Euclidean distance when there is a natural ordering for the\nvalues that the draws can take -- that is, when the data is ordinal -- whereas\nthe Euclidean distance is more reliable and more easily understood than the\nKolmogorov-Smirnov statistic when there is no natural ordering (or partial\norder) -- that is, when the data is nominal. \n\n"}
{"id": "1206.6927", "contents": "Title: Profile Likelihood Biclustering Abstract: Biclustering, the process of simultaneously clustering the rows and columns\nof a data matrix, is a popular and effective tool for finding structure in a\nhigh-dimensional dataset. Many biclustering procedures appear to work well in\npractice, but most do not have associated consistency guarantees. To address\nthis shortcoming, we propose a new biclustering procedure based on profile\nlikelihood. The procedure applies to a broad range of data modalities,\nincluding binary, count, and continuous observations. We prove that the\nprocedure recovers the true row and column classes when the dimensions of the\ndata matrix tend to infinity, even if the functional form of the data\ndistribution is misspecified. The procedure requires computing a combinatorial\nsearch, which can be expensive in practice. Rather than performing this search\ndirectly, we propose a new heuristic optimization procedure based on the\nKernighan-Lin heuristic, which has nice computational properties and performs\nwell in simulations. We demonstrate our procedure with applications to\ncongressional voting records, and microarray analysis. \n\n"}
{"id": "1207.0170", "contents": "Title: Single parameter galaxy classification: The Principal Curve through the\n  multi-dimensional space of galaxy properties Abstract: We propose to describe the variety of galaxies from SDSS by using only one\naffine parameter. To this aim, we build the Principal Curve (P-curve) passing\nthrough the spine of the data point cloud, considering the eigenspace derived\nfrom Principal Component Analysis of morphological, physical and photometric\ngalaxy properties. Thus, galaxies can be labeled, ranked and classified by a\nsingle arc length value of the curve, measured at the unique closest projection\nof the data points on the P-curve. We find that the P-curve has a \"W\" letter\nshape with 3 turning points, defining 4 branches that represent distinct galaxy\npopulations. This behavior is controlled mainly by 2 properties, namely u-r and\nSFR. We further present the variations of several galaxy properties as a\nfunction of arc length. Luminosity functions variate from steep Schechter fits\nat low arc length, to double power law and ending in Log-normal fits at high\narc length. Galaxy clustering shows increasing autocorrelation power at large\nscales as arc length increases. PCA analysis allowed to find peculiar galaxy\npopulations located apart from the main cloud of data points, such as small red\ngalaxies dominated by a disk, of relatively high stellar mass-to-light ratio\nand surface mass density. The P-curve allows not only dimensionality reduction,\nbut also provides supporting evidence for relevant physical models and\nscenarios in extragalactic astronomy: 1) Evidence for the hierarchical merging\nscenario in the formation of a selected group of red massive galaxies. These\ngalaxies present a log-normal r-band luminosity function, which might arise\nfrom multiplicative processes involved in this scenario. 2) Connection between\nthe onset of AGN activity and star formation quenching, which appears in green\ngalaxies when transitioning from blue to red populations. (Full abstract in\ndownloadable version) \n\n"}
{"id": "1207.0258", "contents": "Title: Control of probability flow in Markov chain Monte Carlo --\n  Nonreversibility and lifting Abstract: The Markov chain Monte Carlo (MCMC) method is widely used in various fields\nas a powerful numerical integration technique for systems with many degrees of\nfreedom. In MCMC methods, probabilistic state transitions can be considered as\na random walk in state space, and random walks allow for sampling from complex\ndistributions. However, paradoxically, it is necessary to carefully suppress\nthe randomness of the random walk to improve computational efficiency. By\nbreaking detailed balance, we can create a probability flow in the state space\nand perform more efficient sampling along this flow. Motivated by this idea,\npractical and efficient nonreversible MCMC methods have been developed over the\npast ten years. In particular, the lifting technique, which introduces\nprobability flows in an extended state space, has been applied to various\nsystems and has proven more efficient than conventional reversible updates. We\nreview and discuss several practical approaches to implementing nonreversible\nMCMC methods, including the shift method in the cumulative distribution and the\ndirected-worm algorithm. \n\n"}
{"id": "1207.0547", "contents": "Title: Geometry of the faithfulness assumption in causal inference Abstract: Many algorithms for inferring causality rely heavily on the faithfulness\nassumption. The main justification for imposing this assumption is that the set\nof unfaithful distributions has Lebesgue measure zero, since it can be seen as\na collection of hypersurfaces in a hypercube. However, due to sampling error\nthe faithfulness condition alone is not sufficient for statistical estimation,\nand strong-faithfulness has been proposed and assumed to achieve uniform or\nhigh-dimensional consistency. In contrast to the plain faithfulness assumption,\nthe set of distributions that is not strong-faithful has nonzero Lebesgue\nmeasure and in fact, can be surprisingly large as we show in this paper. We\nstudy the strong-faithfulness condition from a geometric and combinatorial\npoint of view and give upper and lower bounds on the Lebesgue measure of\nstrong-faithful distributions for various classes of directed acyclic graphs.\nOur results imply fundamental limitations for the PC-algorithm and potentially\nalso for other algorithms based on partial correlation testing in the Gaussian\ncase. \n\n"}
{"id": "1207.1805", "contents": "Title: A Novel Ergodic Capacity Analysis of Diversity Combining and Multihop\n  Transmission Systems over Generalized Composite Fading Channels Abstract: Ergodic capacity is an important performance measure associated with reliable\ncommunication at the highest rate at which information can be sent over the\nchannel with a negligible probability of error. In the shadow of this\ndefinition, diversity receivers (such as selection combining, equal-gain\ncombining and maximal-ratio combining) and transmission techniques (such as\ncascaded fading channels, amplify-and-forward multihop transmission) are\ndeployed in mitigating various performance impairing effects such as fading and\nshadowing in digital radio communication links. However, the exact analysis of\nergodic capacity is in general not always possible for all of these forms of\ndiversity receivers and transmission techniques over generalized composite\nfading environments due to it's mathematical intractability. In the literature,\npublished papers concerning the exact analysis of ergodic capacity have been\ntherefore scarce (i.e., only [1] and [2]) when compared to those concerning the\nexact analysis of average symbol error probability. In addition, they are\nessentially targeting to the ergodic capacity of the maximal ratio combining\ndiversity receivers and are not readily applicable to the capacity analysis of\nthe other diversity combiners / transmission techniques. In this paper, we\npropose a novel moment generating function-based approach for the exact ergodic\ncapacity analysis of both diversity receivers and transmission techniques over\ngeneralized composite fading environments. As such, we demonstrate how to\nsimultaneously treat the ergodic capacity analysis of all forms of both\ndiversity receivers and multihop transmission techniques. \n\n"}
{"id": "1207.2231", "contents": "Title: Laplace deconvolution and its application to Dynamic Contrast Enhanced\n  imaging Abstract: In the present paper we consider the problem of Laplace deconvolution with\nnoisy discrete observations. The study is motivated by Dynamic Contrast\nEnhanced imaging using a bolus of contrast agent, a procedure which allows\nconsiderable improvement in {evaluating} the quality of a vascular network and\nits permeability and is widely used in medical assessment of brain flows or\ncancerous tumors. Although the study is motivated by medical imaging\napplication, we obtain a solution of a general problem of Laplace deconvolution\nbased on noisy data which appears in many different contexts. We propose a new\nmethod for Laplace deconvolution which is based on expansions of the\nconvolution kernel, the unknown function and the observed signal over Laguerre\nfunctions basis. The expansion results in a small system of linear equations\nwith the matrix of the system being triangular and Toeplitz. The number $m$ of\nthe terms in the expansion of the estimator is controlled via complexity\npenalty. The advantage of this methodology is that it leads to very fast\ncomputations, does not require exact knowledge of the kernel and produces no\nboundary effects due to extension at zero and cut-off at $T$. The technique\nleads to an estimator with the risk within a logarithmic factor of $m$ of the\noracle risk under no assumptions on the model and within a constant factor of\nthe oracle risk under mild assumptions. The methodology is illustrated by a\nfinite sample simulation study which includes an example of the kernel obtained\nin the real life DCE experiments. Simulations confirm that the proposed\ntechnique is fast, efficient, accurate, usable from a practical point of view\nand competitive. \n\n"}
{"id": "1207.4447", "contents": "Title: A robust, adaptive M-estimator for pointwise estimation in\n  heteroscedastic regression Abstract: We introduce a robust and fully adaptive method for pointwise estimation in\nheteroscedastic regression. We allow for noise and design distributions that\nare unknown and fulfill very weak assumptions only. In particular, we do not\nimpose moment conditions on the noise distribution. Moreover, we do not require\na positive density for the design distribution. In a first step, we study the\nconsistency of locally polynomial M-estimators that consist of a contrast and a\nkernel. Afterwards, minimax results are established over unidimensional\nH\\\"older spaces for degenerate design. We then choose the contrast and the\nkernel that minimize an empirical variance term and demonstrate that the\ncorresponding M-estimator is adaptive with respect to the noise and design\ndistributions and adaptive (Huber) minimax for contamination models. In a\nsecond step, we additionally choose a data-driven bandwidth via Lepski's\nmethod. This leads to an M-estimator that is adaptive with respect to the noise\nand design distributions and, additionally, adaptive with respect to the\nsmoothness of an isotropic, multivariate, locally polynomial target function.\nThese results are also extended to anisotropic, locally constant target\nfunctions. Our data-driven approach provides, in particular, a level of\nrobustness that adapts to the noise, contamination, and outliers. \n\n"}
{"id": "1207.6606", "contents": "Title: Weighted sampling, Maximum Likelihood and minimum divergence estimators Abstract: This paper explores Maximum Likelihood in parametric models in the context of\nSanov type Large Deviation Probabilities. MLE in parametric models under\nweighted sampling is shown to be associated with the minimization of a specific\ndivergence criterion defined with respect to the distribution of the weights.\nSome properties of the resulting inferential procedure are presented; Bahadur\nefficiency of tests are also considered in this context. \n\n"}
{"id": "1207.6745", "contents": "Title: Universally Consistent Latent Position Estimation and Vertex\n  Classification for Random Dot Product Graphs Abstract: In this work we show that, using the eigen-decomposition of the adjacency\nmatrix, we can consistently estimate latent positions for random dot product\ngraphs provided the latent positions are i.i.d. from some distribution. If\nclass labels are observed for a number of vertices tending to infinity, then we\nshow that the remaining vertices can be classified with error converging to\nBayes optimal using the $k$-nearest-neighbors classification rule. We evaluate\nthe proposed methods on simulated data and a graph derived from Wikipedia. \n\n"}
{"id": "1208.1154", "contents": "Title: Identifying subtree perfectness in decision trees Abstract: In decision problems, often, utilities and probabilities are hard to\ndetermine. In such cases, one can resort to so-called choice functions. They\nprovide a means to determine which options in a particular set are optimal, and\nallow incomparability among any number of options. Applying choice functions in\nsequential decision problems can be highly non-trivial, as the usual properties\nof maximising expected utility may no longer be satisfied. In this paper, we\nstudy one of these properties: we revisit and reinterpret Selten's concept of\nsubgame perfectness in the context of decision trees, leading us to the concept\nof subtree perfectness, which basically says that the optimal solution of a\ndecision tree should not depend on any larger tree it may be embedded in. In\nother words, subtree perfectness excludes counterfactual reasoning, and\ntherefore may be desirable from some philosophical points of view. Subtree\nperfectness is also desirable from a practical point of view, because it admits\nefficient algorithms for solving decision trees, such as backward induction.\nThe main contribution of this paper is a very simple non-technical criterion\nfor determining whether any given choice function will satisfy subtree\nperfectness or not. We demonstrate the theorem and illustrate subtree\nperfectness, or the lack thereof, through numerous examples, for a wide variety\nof choice functions, where incomparability amongst strategies can be caused by\nimprecision in either probabilities or utilities. We find that almost no choice\nfunction, except for maximising expected utility, satisfies it in general. We\nalso find that choice functions other than maximising expected utility can\nsatisfy it, provided that we restrict either the structure of the tree, or the\nstructure of the choice function. \n\n"}
{"id": "1208.5600", "contents": "Title: A population Monte Carlo scheme with transformed weights and its\n  application to stochastic kinetic models Abstract: This paper addresses the problem of Monte Carlo approximation of posterior\nprobability distributions. In particular, we have considered a recently\nproposed technique known as population Monte Carlo (PMC), which is based on an\niterative importance sampling approach. An important drawback of this\nmethodology is the degeneracy of the importance weights when the dimension of\neither the observations or the variables of interest is high. To alleviate this\ndifficulty, we propose a novel method that performs a nonlinear transformation\non the importance weights. This operation reduces the weight variation, hence\nit avoids their degeneracy and increases the efficiency of the importance\nsampling scheme, specially when drawing from a proposal functions which are\npoorly adapted to the true posterior.\n  For the sake of illustration, we have applied the proposed algorithm to the\nestimation of the parameters of a Gaussian mixture model. This is a very simple\nproblem that enables us to clearly show and discuss the main features of the\nproposed technique. As a practical application, we have also considered the\npopular (and challenging) problem of estimating the rate parameters of\nstochastic kinetic models (SKM). SKMs are highly multivariate systems that\nmodel molecular interactions in biological and chemical problems. We introduce\na particularization of the proposed algorithm to SKMs and present numerical\nresults. \n\n"}
{"id": "1209.0899", "contents": "Title: Shrinkage estimators for prediction out-of-sample: Conditional\n  performance Abstract: We find that, in a linear model, the James-Stein estimator, which dominates\nthe maximum-likelihood estimator in terms of its in-sample prediction error,\ncan perform poorly compared to the maximum-likelihood estimator in\nout-of-sample prediction. We give a detailed analysis of this phenomenon and\ndiscuss its implications. When evaluating the predictive performance of\nestimators, we treat the regressor matrix in the training data as fixed, i.e.,\nwe condition on the design variables. Our findings contrast those obtained by\nBaranchik (1973, Ann. Stat. 1:312-321) and, more recently, by Dicker (2012,\narXiv:1102.2952) in an unconditional performance evaluation. \n\n"}
{"id": "1209.2434", "contents": "Title: Query Complexity of Derivative-Free Optimization Abstract: This paper provides lower bounds on the convergence rate of Derivative Free\nOptimization (DFO) with noisy function evaluations, exposing a fundamental and\nunavoidable gap between the performance of algorithms with access to gradients\nand those with access to only function evaluations. However, there are\nsituations in which DFO is unavoidable, and for such situations we propose a\nnew DFO algorithm that is proved to be near optimal for the class of strongly\nconvex objective functions. A distinctive feature of the algorithm is that it\nuses only Boolean-valued function comparisons, rather than function\nevaluations. This makes the algorithm useful in an even wider range of\napplications, such as optimization based on paired comparisons from human\nsubjects, for example. We also show that regardless of whether DFO is based on\nnoisy function evaluations or Boolean-valued function comparisons, the\nconvergence rate is the same. \n\n"}
{"id": "1209.3394", "contents": "Title: Distribution of the largest eigenvalue for real Wishart and Gaussian\n  random matrices and a simple approximation for the Tracy-Widom distribution Abstract: We derive efficient recursive formulas giving the exact distribution of the\nlargest eigenvalue for finite dimensional real Wishart matrices and for the\nGaussian Orthogonal Ensemble (GOE). In comparing the exact distribution with\nthe limiting distribution of large random matrices, we also found that the\nTracy-Widom law can be approximated by a properly scaled and shifted Gamma\ndistribution, with great accuracy for the values of common interest in\nstatistical applications. \n\n"}
{"id": "1209.3628", "contents": "Title: Bayes procedures for adaptive inference in inverse problems for the\n  white noise model Abstract: We study empirical and hierarchical Bayes approaches to the problem of\nestimating an infinite-dimensional parameter in mildly ill-posed inverse\nproblems. We consider a class of prior distributions indexed by a\nhyperparameter that quantifies regularity. We prove that both methods we\nconsider succeed in automatically selecting this parameter optimally, resulting\nin optimal convergence rates for truths with Sobolev or analytic \"smoothness\",\nwithout using knowledge about this regularity. Both methods are illustrated by\nsimulation examples. \n\n"}
{"id": "1209.5908", "contents": "Title: Correlated variables in regression: clustering and sparse estimation Abstract: We consider estimation in a high-dimensional linear model with strongly\ncorrelated variables. We propose to cluster the variables first and do\nsubsequent sparse estimation such as the Lasso for cluster-representatives or\nthe group Lasso based on the structure from the clusters. Regarding the first\nstep, we present a novel and bottom-up agglomerative clustering algorithm based\non canonical correlations, and we show that it finds an optimal solution and is\nstatistically consistent. We also present some theoretical arguments that\ncanonical correlation based clustering leads to a better-posed compatibility\nconstant for the design matrix which ensures identifiability and an oracle\ninequality for the group Lasso. Furthermore, we discuss circumstances where\ncluster-representatives and using the Lasso as subsequent estimator leads to\nimproved results for prediction and detection of variables. We complement the\ntheoretical analysis with various empirical results. \n\n"}
{"id": "1210.0100", "contents": "Title: On the Sum of Squared \\eta-\\mu Random Variates With Application to the\n  Performance of Wireless Communication Systems Abstract: The probability density function (PDF) and cumulative distribution function\nof the sum of L independent but not necessarily identically distributed squared\n\\eta-\\mu variates, applicable to the output statistics of maximal ratio\ncombining (MRC) receiver operating over \\eta-\\mu fading channels that includes\nthe Hoyt and the Nakagami-m models as special cases, is presented in\nclosed-form in terms of the Fox's H-bar function. Further analysis,\nparticularly on the bit error rate via PDF-based approach, is also represented\nin closed form in terms of the extended Fox's H-bar function (H-hat). The\nproposed new analytical results complement previous results and are illustrated\nby extensive numerical and Monte Carlo simulation results. \n\n"}
{"id": "1210.1715", "contents": "Title: On adaptive minimax density estimation on $R^d$ Abstract: We address the problem of adaptive minimax density estimation on $\\bR^d$ with\n$\\bL_p$--loss on the anisotropic Nikol'skii classes. We fully characterize\nbehavior of the minimax risk for different relationships between regularity\nparameters and norm indexes in definitions of the functional class and of the\nrisk. In particular, we show that there are four different regimes with respect\nto the behavior of the minimax risk. We develop a single estimator which is\n(nearly) optimal in orderover the complete scale of the anisotropic Nikol'skii\nclasses. Our estimation procedure is based on a data-driven selection of an\nestimator from a fixed family of kernel estimators. \n\n"}
{"id": "1210.2314", "contents": "Title: Clustering of Markov chain exceedances Abstract: The tail chain of a Markov chain can be used to model the dependence between\nextreme observations. For a positive recurrent Markov chain, the tail chain\naids in describing the limit of a sequence of point processes $\\{N_n,n\\geq1\\}$,\nconsisting of normalized observations plotted against scaled time points. Under\nfairly general conditions on extremal behaviour, $\\{N_n\\}$ converges to a\ncluster Poisson process. Our technique decomposes the sample path of the chain\ninto i.i.d. regenerative cycles rather than using blocking argument typically\nemployed in the context of stationarity with mixing. \n\n"}
{"id": "1210.2667", "contents": "Title: Estimating Population Size with Link-Tracing Sampling Abstract: We present a new design and inference method for estimating population size\nof a hidden population best reached through a link-tracing design. The strategy\ninvolves the Rao-Blackwell Theorem applied to a sufficient statistic markedly\ndifferent from the usual one that arises in sampling from a finite population.\nAn empirical application is described. The result demonstrates that the\nstrategy can efficiently incorporate adaptively selected members of the sample\ninto the inference procedure. \n\n"}
{"id": "1210.3060", "contents": "Title: Markov Kernels and the Conditional Extreme Value Model Abstract: The classical approach to multivariate extreme value modelling assumes that\nthe joint distribution belongs to a multivariate domain of attraction. This\nrequires each marginal distribution be individually attracted to a univariate\nextreme value distribution. An apparently more flexible extremal model for\nmultivariate data was proposed by Heffernan and Tawn under which not all the\ncomponents are required to belong to an extremal domain of attraction but\nassumes instead the existence of an asymptotic approximation to the conditional\ndistribution of the random vector given one of the components is extreme.\nCombined with the knowledge that the conditioning component belongs to a\nunivariate domain of attraction, this leads to an approximation of the\nprobability of certain risk regions. The original focus on conditional\ndistributions had technical drawbacks but is natural in several contexts. We\nplace this approach in the context of the more general approach using\nconvergence of measures and multivariate regular variation on cones. \n\n"}
{"id": "1211.0266", "contents": "Title: Non Asymptotic Performance of Some Markov Chains Order Estimators Abstract: In what follows we study non asymptotic behavior of different well known\nestimators AIC(\\cite{Tong}), BIC(\\cite{Schwarz}) and EDC(\\cite{Zhao,Dorea}) in\ncontrast with the Markov chain order estimator, named as Global Depency Level -\nGDL(\\cite{Baigorri}).\n  The estimator GDL, is based on a different principle which makes it behave in\na quite different form. It is strongly consistent and more efficient than\nAIC(inconsistent), outperforming the well established and consistent BIC and\nEDC, mainly on relatively small samples.\n  The estimators mentioned above mainly consist in the evaluation of the Markov\nchain's sample by different multivariate deterministic functions. The log\nlikelihood approach or the GDL approach, shall be analysed exhibiting different\nstructural properties. It will become clear the intimate differences existing\nbetween the variance of both estimators, which induce quite dissimilar\nperformance, mainly for samples of moderated sizes. \n\n"}
{"id": "1211.0811", "contents": "Title: Discussion: Latent variable graphical model selection via convex\n  optimization Abstract: Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290]. \n\n"}
{"id": "1211.1309", "contents": "Title: Sparse PCA: Optimal rates and adaptive estimation Abstract: Principal component analysis (PCA) is one of the most commonly used\nstatistical procedures with a wide range of applications. This paper considers\nboth minimax and adaptive estimation of the principal subspace in the high\ndimensional setting. Under mild technical conditions, we first establish the\noptimal rates of convergence for estimating the principal subspace which are\nsharp with respect to all the parameters, thus providing a complete\ncharacterization of the difficulty of the estimation problem in term of the\nconvergence rate. The lower bound is obtained by calculating the local metric\nentropy and an application of Fano's lemma. The rate optimal estimator is\nconstructed using aggregation, which, however, might not be computationally\nfeasible. We then introduce an adaptive procedure for estimating the principal\nsubspace which is fully data driven and can be computed efficiently. It is\nshown that the estimator attains the optimal rates of convergence\nsimultaneously over a large collection of the parameter spaces. A key idea in\nour construction is a reduction scheme which reduces the sparse PCA problem to\na high-dimensional multivariate regression problem. This method is potentially\nalso useful for other related problems. \n\n"}
{"id": "1211.2917", "contents": "Title: High-dimensionality effects in the Markowitz problem and other quadratic\n  programs with linear constraints: Risk underestimation Abstract: We first study the properties of solutions of quadratic programs with linear\nequality constraints whose parameters are estimated from data in the\nhigh-dimensional setting where p, the number of variables in the problem, is of\nthe same order of magnitude as n, the number of observations used to estimate\nthe parameters. The Markowitz problem in Finance is a subcase of our study.\nAssuming normality and independence of the observations we relate the efficient\nfrontier computed empirically to the \"true\" efficient frontier. Our\ncomputations show that there is a separation of the errors induced by\nestimating the mean of the observations and estimating the covariance matrix.\nIn particular, the price paid for estimating the covariance matrix is an\nunderestimation of the variance by a factor roughly equal to 1-p/n. Therefore\nthe risk of the optimal population solution is underestimated when we estimate\nit by solving a similar quadratic program with estimated parameters. We also\ncharacterize the statistical behavior of linear functionals of the empirical\noptimal vector and show that they are biased estimators of the corresponding\npopulation quantities. \n\n"}
{"id": "1211.3394", "contents": "Title: Non-asymptotic approach to varying coefficient model Abstract: In the present paper we consider the varying coefficient model which\nrepresents a useful tool for exploring dynamic patterns in many applications.\nExisting methods typically provide asymptotic evaluation of precision of\nestimation procedures under the assumption that the number of observations\ntends to infinity. In practical applications, however, only a finite number of\nmeasurements are available. In the present paper we focus on a non-asymptotic\napproach to the problem. We propose a novel estimation procedure which is based\non recent developments in matrix estimation. In particular, for our estimator,\nwe obtain upper bounds for the mean squared and the pointwise estimation\nerrors. The obtained oracle inequalities are non-asymptotic and hold for finite\nsample size. \n\n"}
{"id": "1211.5194", "contents": "Title: On pattern recovery of the fused Lasso Abstract: We study the property of the Fused Lasso Signal Approximator (FLSA) for\nestimating a blocky signal sequence with additive noise. We transform the FLSA\nto an ordinary Lasso problem. By studying the property of the design matrix in\nthe transformed Lasso problem, we find that the irrepresentable condition might\nnot hold, in which case we show that the FLSA might not be able to recover the\nsignal pattern. We then apply the newly developed preconditioning method --\nPuffer Transformation [Jia and Rohe, 2012] on the transformed Lasso problem. We\ncall the new method the preconditioned fused Lasso and we give non-asymptotic\nresults for this method. Results show that when the signal jump strength\n(signal difference between two neighboring groups) is big and the noise level\nis small, our preconditioned fused Lasso estimator gives the correct pattern\nwith high probability. Theoretical results give insight on what controls the\nsignal pattern recovery ability -- it is the noise level {instead of} the\nlength of the sequence. Simulations confirm our theorems and show significant\nimprovement of the preconditioned fused Lasso estimator over the vanilla FLSA. \n\n"}
{"id": "1211.5281", "contents": "Title: Distributions of exponential integrals of independent increment\n  processes related to generalized gamma convolutions Abstract: It is known that in many cases distributions of exponential integrals of Levy\nprocesses are infinitely divisible and in some cases they are also\nselfdecomposable. In this paper, we give some sufficient conditions under which\ndistributions of exponential integrals are not only selfdecomposable but\nfurthermore are generalized gamma convolution. We also study exponential\nintegrals of more general independent increment processes. Several examples are\ngiven for illustration. \n\n"}
{"id": "1212.1791", "contents": "Title: Generative Models for Functional Data using Phase and Amplitude\n  Separation Abstract: Constructing generative models for functional observations is an important\ntask in statistical functional analysis. In general, functional data contains\nboth phase (or x or horizontal) and amplitude (or y or vertical) variability.\nTradi- tional methods often ignore the phase variability and focus solely on\nthe amplitude variation, using cross-sectional techniques such as fPCA for\ndimensional reduction and data modeling. Ignoring phase variability leads to a\nloss of structure in the data and inefficiency in data models. This paper\npresents an approach that relies on separating the phase (x-axis) and amplitude\n(y-axis), then modeling these components using joint distributions. This\nseparation, in turn, is performed using a technique called elastic shape\nanalysis of curves that involves a new mathematical representation of\nfunctional data. Then, using individual fPCAs, one each for phase and amplitude\ncomponents, while respecting the nonlinear geometry of the phase representation\nspace; impose joint probability models on principal coefficients of these\ncomponents. These ideas are demonstrated using random sampling, for models\nestimated from simulated and real datasets, and show their superiority over\nmodels that ignore phase-amplitude separation. Furthermore, the generative\nmodels are applied to classification of functional data and achieve high\nperformance in applications involv- ing SONAR signals of underwater objects,\nhandwritten signatures, and periodic body movements recorded by smart phones. \n\n"}
{"id": "1212.4899", "contents": "Title: New inequalities of Mill's ratio and Its Application to The Inverse\n  Q-function Approximation Abstract: In this paper, we investigate the Mill's ratio estimation problem and get two\nnew inequalities. Compared to the well known results obtained by Gordon, they\nbecomes tighter. Furthermore, we also discuss the inverse Q-function\napproximation problem and present some useful results on the inverse solution.\nNumerical results confirm the validness of our theoretical analysis. In\naddition, we also present a conjecture on the bounds of inverse solution on\nQ-function. \n\n"}
{"id": "1212.6088", "contents": "Title: Bayesian shrinkage Abstract: Penalized regression methods, such as $L_1$ regularization, are routinely\nused in high-dimensional applications, and there is a rich literature on\noptimality properties under sparsity assumptions. In the Bayesian paradigm,\nsparsity is routinely induced through two-component mixture priors having a\nprobability mass at zero, but such priors encounter daunting computational\nproblems in high dimensions. This has motivated an amazing variety of\ncontinuous shrinkage priors, which can be expressed as global-local scale\nmixtures of Gaussians, facilitating computation. In sharp contrast to the\ncorresponding frequentist literature, very little is known about the properties\nof such priors. Focusing on a broad class of shrinkage priors, we provide\nprecise results on prior and posterior concentration. Interestingly, we\ndemonstrate that most commonly used shrinkage priors, including the Bayesian\nLasso, are suboptimal in high-dimensional settings. A new class of Dirichlet\nLaplace (DL) priors are proposed, which are optimal and lead to efficient\nposterior computation exploiting results from normalized random measure theory.\nFinite sample performance of Dirichlet Laplace priors relative to alternatives\nis assessed in simulations. \n\n"}
{"id": "1301.0901", "contents": "Title: Compressed Sensing under Matrix Uncertainty: Optimum Thresholds and\n  Robust Approximate Message Passing Abstract: In compressed sensing one measures sparse signals directly in a compressed\nform via a linear transform and then reconstructs the original signal. However,\nit is often the case that the linear transform itself is known only\napproximately, a situation called matrix uncertainty, and that the measurement\nprocess is noisy. Here we present two contributions to this problem: first, we\nuse the replica method to determine the mean-squared error of the Bayes-optimal\nreconstruction of sparse signals under matrix uncertainty. Second, we consider\na robust variant of the approximate message passing algorithm and demonstrate\nnumerically that in the limit of large systems, this algorithm matches the\noptimal performance in a large region of parameters. \n\n"}
{"id": "1301.1722", "contents": "Title: Linear Bandits in High Dimension and Recommendation Systems Abstract: A large number of online services provide automated recommendations to help\nusers to navigate through a large collection of items. New items (products,\nvideos, songs, advertisements) are suggested on the basis of the user's past\nhistory and --when available-- her demographic profile. Recommendations have to\nsatisfy the dual goal of helping the user to explore the space of available\nitems, while allowing the system to probe the user's preferences.\n  We model this trade-off using linearly parametrized multi-armed bandits,\npropose a policy and prove upper and lower bounds on the cumulative \"reward\"\nthat coincide up to constants in the data poor (high-dimensional) regime. Prior\nwork on linear bandits has focused on the data rich (low-dimensional) regime\nand used cumulative \"risk\" as the figure of merit. For this data rich regime,\nwe provide a simple modification for our policy that achieves near-optimal risk\nperformance under more restrictive assumptions on the geometry of the problem.\nWe test (a variation of) the scheme used for establishing achievability on the\nNetflix and MovieLens datasets and obtain good agreement with the qualitative\npredictions of the theory we develop. \n\n"}
{"id": "1301.4183", "contents": "Title: On Graphical Models via Univariate Exponential Family Distributions Abstract: Undirected graphical models, or Markov networks, are a popular class of\nstatistical models, used in a wide variety of applications. Popular instances\nof this class include Gaussian graphical models and Ising models. In many\nsettings, however, it might not be clear which subclass of graphical models to\nuse, particularly for non-Gaussian and non-categorical data. In this paper, we\nconsider a general sub-class of graphical models where the node-wise\nconditional distributions arise from exponential families. This allows us to\nderive multivariate graphical model distributions from univariate exponential\nfamily distributions, such as the Poisson, negative binomial, and exponential\ndistributions. Our key contributions include a class of M-estimators to fit\nthese graphical model distributions; and rigorous statistical analysis showing\nthat these M-estimators recover the true graphical model structure exactly,\nwith high probability. We provide examples of genomic and proteomic networks\nlearned via instances of our class of graphical models derived from Poisson and\nexponential distributions. \n\n"}
{"id": "1301.7212", "contents": "Title: Multiscale Change-Point Inference Abstract: We introduce a new estimator SMUCE (simultaneous multiscale change-point\nestimator) for the change-point problem in exponential family regression. An\nunknown step function is estimated by minimizing the number of change-points\nover the acceptance region of a multiscale test at a level \\alpha. The\nprobability of overestimating the true number of change-points K is controlled\nby the asymptotic null distribution of the multiscale test statistic. Further,\nwe derive exponential bounds for the probability of underestimating K. By\nbalancing these quantities, \\alpha will be chosen such that the probability of\ncorrectly estimating K is maximized. All results are even non-asymptotic for\nthe normal case. Based on the aforementioned bounds, we construct\nasymptotically honest confidence sets for the unknown step function and its\nchange-points. At the same time, we obtain exponential bounds for estimating\nthe change-point locations which for example yield the minimax rate O(1/n) up\nto a log term. Finally, SMUCE asymptotically achieves the optimal detection\nrate of vanishing signals. We illustrate how dynamic programming techniques can\nbe employed for efficient computation of estimators and confidence regions. The\nperformance of the proposed multiscale approach is illustrated by simulations\nand in two cutting-edge applications from genetic engineering and photoemission\nspectroscopy. \n\n"}
{"id": "1302.0406", "contents": "Title: Generalization Guarantees for a Binary Classification Framework for\n  Two-Stage Multiple Kernel Learning Abstract: We present generalization bounds for the TS-MKL framework for two stage\nmultiple kernel learning. We also present bounds for sparse kernel learning\nformulations within the TS-MKL framework. \n\n"}
{"id": "1302.1158", "contents": "Title: Functional calibration estimation by the maximum entropy on the mean\n  principle Abstract: We extend the problem of obtaining an estimator for the finite population\nmean parameter incorporating complete auxiliary information through calibration\nestimation in survey sampling but considering a functional data framework. The\nfunctional calibration sampling weights of the estimator are obtained by\nmatching the calibration estimation problem with the maximum entropy on the\nmean principle. In particular, the calibration estimation is viewed as an\ninfinite dimensional linear inverse problem following the structure of the\nmaximum entropy on the mean approach. We give a precise theoretical setting and\nestimate the functional calibration weights assuming, as prior measures, the\ncentered Gaussian and compound Poisson random measures. Additionally, through a\nsimple simulation study, we show that our functional calibration estimator\nimproves its accuracy compared with the Horvitz-Thompson estimator. \n\n"}
{"id": "1302.2882", "contents": "Title: Markov chain Monte Carlo methods for the regular two-level fractional\n  factorial designs and cut ideals Abstract: It is known that a Markov basis of the binary graph model of a graph $G$\ncorresponds to a set of binomial generators of cut ideals $I_{\\widehat{G}}$ of\nthe suspension $\\widehat{G}$ of $G$. In this paper, we give another application\nof cut ideals to statistics. We show that a set of binomial generators of cut\nideals is a Markov basis of some regular two-level fractional factorial design.\nAs application, we give a Markov basis of degree 2 for designs defined by at\nmost two relations. \n\n"}
{"id": "1302.3430", "contents": "Title: Bernstein - von Mises Theorem for growing parameter dimension Abstract: This paper revisits the prominent Fisher, Wilks, and Bernstein -- von Mises\n(BvM) results from different viewpoints. Particular issues to address are:\nnonasymptotic framework with just one finite sample, possible model\nmisspecification, and a large parameter dimension. In particular, in the case\nof an i.i.d. sample, the mentioned results can be stated for any smooth\nparametric family provided that the dimension \\(p \\) of the parameter space\nsatisfies the condition \"\\(p^{2}/n \\) is small\" for the Fisher expansion, while\nthe Wilks and the BvM results require \"\\(p^{3}/n \\) is small\". \n\n"}
{"id": "1302.4101", "contents": "Title: Posterior Consistency for Bayesian Inverse Problems through Stability\n  and Regression Results Abstract: In the Bayesian approach, the a priori knowledge about the input of a\nmathematical model is described via a probability measure. The joint\ndistribution of the unknown input and the data is then conditioned, using\nBayes' formula, giving rise to the posterior distribution on the unknown input.\nIn this setting we prove posterior consistency for nonlinear inverse problems:\na sequence of data is considered, with diminishing fluctuations around a single\ntruth and it is then of interest to show that the resulting sequence of\nposterior measures arising from this sequence of data concentrates around the\ntruth used to generate the data. Posterior consistency justifies the use of the\nBayesian approach very much in the same way as error bounds and convergence\nresults for regularisation techniques do. As a guiding example, we consider the\ninverse problem of reconstructing the diffusion coefficient from noisy\nobservations of the solution to an elliptic PDE in divergence form. This\nproblem is approached by splitting the forward operator into the underlying\ncontinuum model and a simpler observation operator based on the output of the\nmodel.\n  In general, these splittings allow us to conclude posterior consistency\nprovided a deterministic stability result for the underlying inverse problem\nand a posterior consistency result for the Bayesian regression problem with the\npush-forward prior.\n  Moreover, we prove posterior consistency for the Bayesian regression problem\nbased on the regularity, the tail behaviour and the small ball probabilities of\nthe prior. \n\n"}
{"id": "1303.0148", "contents": "Title: High-Frequency Tail Index Estimation by Nearly Tight Frames Abstract: This work develops the asymptotic properties (weak consistency and\nGaussianity), in the high-frequency limit, of approximate maximum likelihood\nestimators for the spectral parameters of Gaussian and isotropic spherical\nrandom fields. The procedure we used exploits the so-called mexican needlet\nconstruction by Geller and Mayeli in [Geller, Mayeli (2009)]. Furthermore, we\npropose a plug-in procedure to optimize the precision of the estimators in\nterms of asymptotic variance. \n\n"}
{"id": "1303.0159", "contents": "Title: Smoothing effect of Compound Poisson approximation to distribution of\n  weighted sums Abstract: The accuracy of compound Poisson approximation to the sum\n$S=w_1S_1+w_2S_2+...+w_NS_N$ is estimated.\n  Here $S_i$ are sums of independent or weakly dependent random variables, and\n$w_i$ denote weights. The overall smoothing effect of $S$ on $w_iS_i$ is\nestimated by L\\' evy concentration function. \n\n"}
{"id": "1303.0974", "contents": "Title: Block Thresholding on the Sphere Abstract: The aim of this paper is to study the nonparametric regression estimators on\nthe sphere built by the needlet block thresholding. The block thresholding\nprocedure proposed here follows the method introduced by Hall, Kerkyacharian\nand Picard in [Hall, Kerkyacharian, Picard, (1998), (1999)], modified to\nexploit the properties of the spherical standard needlets. Therefore, we will\ninvestigate on their convergence rates, attaining their adaptive properties\nover the Besov balls. This work is strongly motivated by issues arising in\nCosmology and Astrophysics, concerning in particular the analysis of cosmic\nrays. \n\n"}
{"id": "1303.2814", "contents": "Title: Convergence rate of Markov chain methods for genomic motif discovery Abstract: We analyze the convergence rate of a simplified version of a popular Gibbs\nsampling method used for statistical discovery of gene regulatory binding\nmotifs in DNA sequences. This sampler satisfies a very strong form of\nergodicity (uniform). However, we show that, due to multimodality of the\nposterior distribution, the rate of convergence often decreases exponentially\nas a function of the length of the DNA sequence. Specifically, we show that\nthis occurs whenever there is more than one true repeating pattern in the data.\nIn practice there are typically multiple such patterns in biological data, the\ngoal being to detect the most well-conserved and frequently-occurring of these.\nOur findings match empirical results, in which the motif-discovery Gibbs\nsampler has exhibited such poor convergence that it is used only for finding\nmodes of the posterior distribution (candidate motifs) rather than for\nobtaining samples from that distribution. Ours are some of the first meaningful\nbounds on the convergence rate of a Markov chain method for sampling from a\nmultimodal posterior distribution, as a function of statistical quantities like\nthe number of observations. \n\n"}
{"id": "1303.2910", "contents": "Title: Understanding Operational Risk Capital Approximations: First and Second\n  Orders Abstract: We set the context for capital approximation within the framework of the\nBasel II / III regulatory capital accords. This is particularly topical as the\nBasel III accord is shortly due to take effect. In this regard, we provide a\nsummary of the role of capital adequacy in the new accord, highlighting along\nthe way the significant loss events that have been attributed to the\nOperational Risk class that was introduced in the Basel II and III accords.\nThen we provide a semi-tutorial discussion on the modelling aspects of capital\nestimation under a Loss Distributional Approach (LDA). Our emphasis is to focus\non the important loss processes with regard to those that contribute most to\ncapital, the so called high consequence, low frequency loss processes. This\nleads us to provide a tutorial overview of heavy tailed loss process modelling\nin OpRisk under Basel III, with discussion on the implications of such tail\nassumptions for the severity model in an LDA structure. This provides\npractitioners with a clear understanding of the features that they may wish to\nconsider when developing OpRisk severity models in practice. From this\ndiscussion on heavy tailed severity models, we then develop an understanding of\nthe impact such models have on the right tail asymptotics of the compound loss\nprocess and we provide detailed presentation of what are known as first and\nsecond order tail approximations for the resulting heavy tailed loss process.\nFrom this we develop a tutorial on three key families of risk measures and\ntheir equivalent second order asymptotic approximations: Value-at-Risk (Basel\nIII industry standard); Expected Shortfall (ES) and the Spectral Risk Measure.\nThese then form the capital approximations. \n\n"}
{"id": "1303.5142", "contents": "Title: Moments of the Riesz distribution Abstract: This article derives the first two moments of the two versions of the Riesz\ndistribution in the terms of their characteristic functions. \n\n"}
{"id": "1303.7090", "contents": "Title: Gaussian process models for periodicity detection Abstract: We consider the problem of detecting and quantifying the periodic component\nof a function given noise-corrupted observations of a limited number of\ninput/output tuples. Our approach is based on Gaussian process regression which\nprovides a flexible non-parametric framework for modelling periodic data. We\nintroduce a novel decomposition of the covariance function as the sum of\nperiodic and aperiodic kernels. This decomposition allows for the creation of\nsub-models which capture the periodic nature of the signal and its complement.\nTo quantify the periodicity of the signal, we derive a periodicity ratio which\nreflects the uncertainty in the fitted sub-models. Although the method can be\napplied to many kernels, we give a special emphasis to the Mat\\'ern family,\nfrom the expression of the reproducing kernel Hilbert space inner product to\nthe implementation of the associated periodic kernels in a Gaussian process\ntoolkit. The proposed method is illustrated by considering the detection of\nperiodically expressed genes in the arabidopsis genome. \n\n"}
{"id": "1303.7286", "contents": "Title: On the symmetrical Kullback-Leibler Jeffreys centroids Abstract: Due to the success of the bag-of-word modeling paradigm, clustering\nhistograms has become an important ingredient of modern information processing.\nClustering histograms can be performed using the celebrated $k$-means\ncentroid-based algorithm. From the viewpoint of applications, it is usually\nrequired to deal with symmetric distances. In this letter, we consider the\nJeffreys divergence that symmetrizes the Kullback-Leibler divergence, and\ninvestigate the computation of Jeffreys centroids. We first prove that the\nJeffreys centroid can be expressed analytically using the Lambert $W$ function\nfor positive histograms. We then show how to obtain a fast guaranteed\napproximation when dealing with frequency histograms. Finally, we conclude with\nsome remarks on the $k$-means histogram clustering. \n\n"}
{"id": "1304.1598", "contents": "Title: A New Distribution-Random Limit Normal Distribution Abstract: This paper introduces a new distribution to improve tail risk modeling. Based\non the classical normal distribution, we define a new distribution by a series\nof heat equations. Then, we use market data to verify our model. \n\n"}
{"id": "1304.2810", "contents": "Title: High-dimensional Mixed Graphical Models Abstract: While graphical models for continuous data (Gaussian graphical models) and\ndiscrete data (Ising models) have been extensively studied, there is little\nwork on graphical models linking both continuous and discrete variables (mixed\ndata), which are common in many scientific applications. We propose a novel\ngraphical model for mixed data, which is simple enough to be suitable for\nhigh-dimensional data, yet flexible enough to represent all possible graph\nstructures. We develop a computationally efficient regression-based algorithm\nfor fitting the model by focusing on the conditional log-likelihood of each\nvariable given the rest. The parameters have a natural group structure, and\nsparsity in the fitted graph is attained by incorporating a group lasso\npenalty, approximated by a weighted $\\ell_1$ penalty for computational\nefficiency. We demonstrate the effectiveness of our method through an extensive\nsimulation study and apply it to a music annotation data set (CAL500),\nobtaining a sparse and interpretable graphical model relating the continuous\nfeatures of the audio signal to categorical variables such as genre, emotions,\nand usage associated with particular songs. While we focus on binary discrete\nvariables, we also show that the proposed methodology can be easily extended to\ngeneral discrete variables. \n\n"}
{"id": "1304.2995", "contents": "Title: Linear Multifractional Stable Motion: wavelet estimation of $H(\\cdot)$\n  and $\\al$ parameters Abstract: Linear Fractional Stable Motion (LFSM) of Hurst parameter $H$ and of\nstability parameter $\\al$, is one of the most classical extensions of the\nwell-known Gaussian Fractional Brownian Motion (FBM), to the setting of\nheavy-tailed stable distributions \\cite{SamTaq,EmMa}. In order to overcome some\nlimitations of its areas of application, coming from stationarity of its\nincrements as well as constancy over time of its self-similarity exponent,\nStoev and Taqqu introduced in \\cite{stoev2004stochastic} an extension of LFSM,\ncalled Linear Multifractional Stable Motion (LMSM), in which the Hurst\nparameter becomes a function $H(\\cdot)$ depending on the time variable $t$.\nSimilarly to LFSM, the tail heaviness of the marginal distributions of LMSM is\ndetermined by $\\al$; also, under some conditions, its self-similarity is\ngoverned by $H(\\cdot)$ and its path roughness is closely related to\n$H(\\cdot)-1/\\al$. Namely, it was shown in \\cite{stoev2004stochastic} that\n$H(t_0)$ is the self-similarity exponent of LMSM at a time $t_0\\neq 0$;\nmoreover, very recently, it was established in \\cite{hamonier2012lmsm}, that\nthe quantities $\\min_{t\\in I} H(t)-1/\\al$, and $H(t_0)-1/\\al$, are respectively\nthe uniform H\\\"older exponent of LMSM on a compact interval $I$, and its local\nH\\\"older exponent at $t_0$.\n  The main goal of our article, is to construct, using wavelet coefficients of\nLMSM, strongly consistent (i.e. almost surely convergent) statistical\nestimators of $\\min_{t\\in I} H(t)$, $H(t_0)$, and $\\al$; our estimation\nresults, are obtained when $\\al\\in (1,2)$, and, $H(\\cdot)$ is a H\\\"older\nfunction smooth enough, with values in a compact subinterval\n$[\\underline{H},\\bar{H}]$ of $(1/\\al,1)$. \n\n"}
{"id": "1304.4362", "contents": "Title: Elemental estimators for the Generalized Extreme Value tail Abstract: In a companion paper (McRobie(2013) arxiv:1304.3918), a simple set of\n`elemental' estimators was presented for the Generalized Pareto tail parameter.\nEach elemental estimator: involves only three log-spacings; is absolutely\nunbiased for all values of the tail parameter; is location- and\nscale-invariant; and is valid for all sample sizes $N$, even as small as $N=\n3$. It was suggested that linear combinations of such elementals could then be\nused to construct efficient unbiased estimators. In this paper, the analogous\nmathematical approach is taken to the Generalised Extreme Value (GEV)\ndistribution. The resulting elemental estimators, although not absolutely\nunbiased, are found to have very small bias, and may thus provide a useful\nbasis for the construction of efficient estimators. \n\n"}
{"id": "1304.5113", "contents": "Title: A note on weak convergence of the sequential multivariate empirical\n  process under strong mixing Abstract: This article investigates weak convergence of the sequential $d$-dimensional\nempirical process under strong mixing. Weak convergence is established for\nmixing rates $\\alpha_n = O(n^{-a})$, where $a>1$, which slightly improves upon\nexisting results in the literature that are based on mixing rates depending on\nthe dimension $d$. \n\n"}
{"id": "1304.5802", "contents": "Title: Nonlinear Basis Pursuit Abstract: In compressive sensing, the basis pursuit algorithm aims to find the sparsest\nsolution to an underdetermined linear equation system. In this paper, we\ngeneralize basis pursuit to finding the sparsest solution to higher order\nnonlinear systems of equations, called nonlinear basis pursuit. In contrast to\nthe existing nonlinear compressive sensing methods, the new algorithm that\nsolves the nonlinear basis pursuit problem is convex and not greedy. The novel\nalgorithm enables the compressive sensing approach to be used for a broader\nrange of applications where there are nonlinear relationships between the\nmeasurements and the unknowns. \n\n"}
{"id": "1304.8089", "contents": "Title: Distribution and Symmetric Distribution Regression Model for\n  Interval-Valued Variables Abstract: Symbolic Data Analysis works with variables for which each unit or class of\nunits takes a finite set of values/categories, an interval or a distribution\n(an histogram, for instance). When to each observation corresponds an empirical\ndistribution, we have a histogram-valued variable; it reduces to the case of an\ninterval-valued variable if each unit takes values on only one interval with\nprobability equal to one. Distribution and Symmetric Distribution is a linear\nregression model proposed for histogram-valued variables that may be\nparticularized to interval-valued variables. This model is defined for n\nexplicative variables and is based on the distributions considered within the\nintervals. In this paper we study the special case where the Uniform\ndistribution is assumed in each observed interval. As in the classical case, a\ngoodness-of-fit measure is deduced from the model. Some illustrative examples\nare presented. A simulation study allows discussing interpretations of the\nbehavior of the model for this variable type. \n\n"}
{"id": "1305.0617", "contents": "Title: Bayesian Manifold Regression Abstract: There is increasing interest in the problem of nonparametric regression with\nhigh-dimensional predictors. When the number of predictors $D$ is large, one\nencounters a daunting problem in attempting to estimate a $D$-dimensional\nsurface based on limited data. Fortunately, in many applications, the support\nof the data is concentrated on a $d$-dimensional subspace with $d \\ll D$.\nManifold learning attempts to estimate this subspace. Our focus is on\ndeveloping computationally tractable and theoretically supported Bayesian\nnonparametric regression methods in this context. When the subspace corresponds\nto a locally-Euclidean compact Riemannian manifold, we show that a Gaussian\nprocess regression approach can be applied that leads to the minimax optimal\nadaptive rate in estimating the regression function under some conditions. The\nproposed model bypasses the need to estimate the manifold, and can be\nimplemented using standard algorithms for posterior computation in Gaussian\nprocesses. Finite sample performance is illustrated in an example data\nanalysis. \n\n"}
{"id": "1305.1229", "contents": "Title: Central limit theorems for pre-averaging covariance estimators under\n  endogenous sampling times Abstract: We consider two continuous It\\^o semimartingales observed with noise and\nsampled at stopping times in a nonsynchronous manner. In this article we\nestablish a central limit theorem for the pre-averaged Hayashi-Yoshida\nestimator of their integrated covariance in a general endogenous time setting.\nIn particular, we show that the time endogeneity has no impact on the\nasymptotic distribution of the pre-averaged Hayashi-Yoshida estimator, which\ncontrasts the case for the realized volatility in a pure diffusion setting. We\nalso establish a central limit theorem for the modulated realized covariance,\nwhich is another pre-averaging based integrated covariance estimator, and\ndemonstrate the above property seems to be a special feature of the\npre-averaging technique. \n\n"}
{"id": "1305.4269", "contents": "Title: Casimir Friction Between Dense Polarizable Media Abstract: The present paper - a continuation of our recent series of papers on Casimir\nfriction for a pair of particles at low relative particle velocity - extends\nthe analysis so as to include dense media. The situation becomes in this case\nmore complex due to induced dipolar correlations, both within planes, and\nbetween planes. We show that the structure of the problem can be simplified by\nregarding the two half-planes as a generalized version of a pair of particles.\nIt turns out that macroscopic parameters such as permittivity suffice to\ndescribe the friction also in the finite density case. The expression for the\nfriction force per unit surface area becomes mathematically well-defined and\nfinite at finite temperature. We give numerical estimates, and compare them\nwith those obtained earlier by Pendry (1997) and by Volokitin and Persson\n(2007). We also show in an appendix how the statistical methods that we are\nusing, correspond to the field theoretical methods more commonly in use. \n\n"}
{"id": "1305.4836", "contents": "Title: Semiparametric posterior limits Abstract: We review the Bayesian theory of semiparametric inference following Bickel\nand Kleijn (2012) and Kleijn and Knapik (2013). After an overview of efficiency\nin parametric and semiparametric estimation problems, we consider the\nBernstein-von Mises theorem (see, e.g., Le Cam and Yang (1990)) and generalize\nit to (LAN) regular and (LAE) irregular semiparametric estimation problems. We\nformulate a version of the semiparametric Bernstein-von Mises theorem that does\nnot depend on least-favourable submodels, thus bypassing the most restrictive\ncondition in the presentation of Bickel and Kleijn (2012). The results are\napplied to the (regular) estimation of the linear coefficient in partial linear\nregression (with a Gaussian nuisance prior) and of the kernel bandwidth in a\nmodel of normal location mixtures (with a Dirichlet nuisance prior), as well as\nthe (irregular) estimation of the boundary of the support of a monotone family\nof densities (with a Gaussian nuisance prior). \n\n"}
{"id": "1305.7406", "contents": "Title: Asymptotic normality of a Sobol index estimator in Gaussian process\n  regression framework Abstract: Stochastic simulators such as Monte-Carlo estimators are widely used in\nscience and engineering to study physical systems through their probabilistic\nrepresentation. Global sensitivity analysis aims to identify the input\nparameters which have the most important impact on the output. A popular tool\nto perform global sensitivity analysis is the variance-based method which comes\nfrom the Hoeffding-Sobol decomposition. Nevertheless, this method requires an\nimportant number of simulations and is often unfeasible under reasonable time\nconstraint. Therefore, an approximation of the input/output relation of the\ncode is built with a Gaussian process regression model. This paper provides\nconditions which ensure the asymptotic normality of a Sobol's index estimator\nevaluated through this surrogate model. This result allows for building\nasymptotic confidence intervals for the considered Sobol index estimator. The\npresented method is successfully applied on an academic example on the heat\nequation. \n\n"}
{"id": "1306.0113", "contents": "Title: Trust, but verify: benefits and pitfalls of least-squares refitting in\n  high dimensions Abstract: Least-squares refitting is widely used in high dimensional regression to\nreduce the prediction bias of l1-penalized estimators (e.g., Lasso and\nSquare-Root Lasso). We present theoretical and numerical results that provide\nnew insights into the benefits and pitfalls of least-squares refitting. In\nparticular, we consider both prediction and estimation, and we pay close\nattention to the effects of correlations in the design matrices of linear\nregression models, since these correlations - although often neglected - are\ncrucial in the context of linear regression, especially in high dimensional\ncontexts. First, we demonstrate that the benefit of least-squares refitting\nstrongly depends on the setting and task under consideration: least-squares\nrefitting can be beneficial even for settings with highly correlated design\nmatrices but is not advisable for all settings, and least-squares refitting can\nbe beneficial for estimation but performs better for prediction. Finally, we\nintroduce a criterion that indicates whether least-squares refitting is\nadvisable for a specific setting and task under consideration, and we conduct a\nthorough simulation study involving the Lasso to show the usefulness of this\ncriterion. \n\n"}
{"id": "1306.0254", "contents": "Title: Central Limit Theorems for Classical Likelihood Ratio Tests for\n  High-Dimensional Normal Distributions Abstract: For random samples of size n obtained from p-variate normal distributions, we\nconsider the classical likelihood ratio tests (LRT) for their means and\ncovariance matrices in the high-dimensional setting. These test statistics have\nbeen extensively studied in multivariate analysis and their limiting\ndistributions under the null hypothesis were proved to be chi-square\ndistributions as n goes to infinity and p remains fixed. In this paper, we\nconsider the high-dimensional case where both p and n go to infinity with p=n/y\nin (0, 1]. We prove that the likelihood ratio test statistics under this\nassumption will converge in distribution to normal distributions with explicit\nmeans and variances. We perform the simulation study to show that the\nlikelihood ratio tests using our central limit theorems outperform those using\nthe traditional chi-square approximations for analyzing high-dimensional data. \n\n"}
{"id": "1306.0480", "contents": "Title: Nonparametric Information Geometry Abstract: The differential-geometric structure of the set of positive densities on a\ngiven measure space has raised the interest of many mathematicians after the\ndiscovery by C.R. Rao of the geometric meaning of the Fisher information. Most\nof the research is focused on parametric statistical models. In series of\npapers by author and coworkers a particular version of the nonparametric case\nhas been discussed. It consists of a minimalistic structure modeled according\nthe theory of exponential families: given a reference density other densities\nare represented by the centered log likelihood which is an element of an Orlicz\nspace. This mappings give a system of charts of a Banach manifold. It has been\nobserved that, while the construction is natural, the practical applicability\nis limited by the technical difficulty to deal with such a class of Banach\nspaces. It has been suggested recently to replace the exponential function with\nother functions with similar behavior but polynomial growth at infinity in\norder to obtain more tractable Banach spaces, e.g. Hilbert spaces. We give\nfirst a review of our theory with special emphasis on the specific issues of\nthe infinite dimensional setting. In a second part we discuss two specific\ntopics, differential equations and the metric connection. The position of this\nline of research with respect to other approaches is briefly discussed. \n\n"}
{"id": "1306.4041", "contents": "Title: Bayesian Monotone Regression using Gaussian Process Projection Abstract: Shape constrained regression analysis has applications in dose-response\nmodeling, environmental risk assessment, disease screening and many other\nareas. Incorporating the shape constraints can improve estimation efficiency\nand avoid implausible results. We propose two novel methods focusing on\nBayesian monotone curve and surface estimation using Gaussian process\nprojections. The first projects samples from an unconstrained prior, while the\nsecond projects samples from the Gaussian process posterior. Theory is\ndeveloped on continuity of the projection, posterior consistency and rates of\ncontraction. The second approach is shown to have an empirical Bayes\njustification and to lead to simple computation with good performance in finite\nsamples. Our projection approach can be applied in other constrained function\nestimation problems including in multivariate settings. \n\n"}
{"id": "1306.4158", "contents": "Title: Approximating dependent rare events Abstract: In this paper we give a historical account of the development of Poisson\napproximation using Stein's method and present some of the main results. We\ngive two recent applications, one on maximal arithmetic progressions and the\nother on bootstrap percolation. We also discuss generalisations to compound\nPoisson approximation, Poisson process approximation and multivariate Poisson\napproximation, and state a few open problems. \n\n"}
{"id": "1306.4408", "contents": "Title: Marginal empirical likelihood and sure independence feature screening Abstract: We study a marginal empirical likelihood approach in scenarios when the\nnumber of variables grows exponentially with the sample size. The marginal\nempirical likelihood ratios as functions of the parameters of interest are\nsystematically examined, and we find that the marginal empirical likelihood\nratio evaluated at zero can be used to differentiate whether an explanatory\nvariable is contributing to a response variable or not. Based on this finding,\nwe propose a unified feature screening procedure for linear models and the\ngeneralized linear models. Different from most existing feature screening\napproaches that rely on the magnitudes of some marginal estimators to identify\ntrue signals, the proposed screening approach is capable of further\nincorporating the level of uncertainties of such estimators. Such a merit\ninherits the self-studentization property of the empirical likelihood approach,\nand extends the insights of existing feature screening methods. Moreover, we\nshow that our screening approach is less restrictive to distributional\nassumptions, and can be conveniently adapted to be applied in a broad range of\nscenarios such as models specified using general moment conditions. Our\ntheoretical results and extensive numerical examples by simulations and data\nanalysis demonstrate the merits of the marginal empirical likelihood approach. \n\n"}
{"id": "1306.6295", "contents": "Title: Tight Lower Bound for Linear Sketches of Moments Abstract: The problem of estimating frequency moments of a data stream has attracted a\nlot of attention since the onset of streaming algorithms [AMS99]. While the\nspace complexity for approximately computing the $p^{\\rm th}$ moment, for\n$p\\in(0,2]$ has been settled [KNW10], for $p>2$ the exact complexity remains\nopen. For $p>2$ the current best algorithm uses $O(n^{1-2/p}\\log n)$ words of\nspace [AKO11,BO10], whereas the lower bound is of $\\Omega(n^{1-2/p})$ [BJKS04].\n  In this paper, we show a tight lower bound of $\\Omega(n^{1-2/p}\\log n)$ words\nfor the class of algorithms based on linear sketches, which store only a sketch\n$Ax$ of input vector $x$ and some (possibly randomized) matrix $A$. We note\nthat all known algorithms for this problem are linear sketches. \n\n"}
{"id": "1306.6566", "contents": "Title: Three Problems Related to the Eigenvalues of Complex Non-central Wishart\n  Matrices with a Rank-1 Mean Abstract: Recently, D. Wang has devised a new contour integral based method to simplify\ncertain matrix integrals. Capitalizing on that approach, we derive a new\nexpression for the probability density function (p.d.f.) of the joint\neigenvalues of a complex non-central Wishart matrix with a rank-1 mean. The\nresulting functional form in turn enables us to use powerful classical\northogonal polynomial techniques in solving three problems related to the\nnon-central Wishart matrix. To be specific, for an $n\\times n$ complex\nnon-central Wishart matrix $\\mathbf{W}$ with $m$ degrees of freedom ($m\\geq n$)\nand a rank-1 mean, we derive a new expression for the cumulative distribution\nfunction (c.d.f.) of the minimum eigenvalue ($\\lambda_{\\min}$). The c.d.f. is\nexpressed as the determinant of a square matrix, the size of which depends only\non the difference $m-n$. This further facilitates the analysis of the\nmicroscopic limit for the minimum eigenvalue which takes the form of the\ndeterminant of a square matrix of size $m-n$ with the Bessel kernel. We also\ndevelop a moment generating function based approach to derive the p.d.f. of the\nrandom variable $\\frac{\\text{tr}(\\mathbf{W})}{\\lambda_{\\min}}$, where\n$\\text{tr}(\\cdot)$ denotes the trace of a square matrix. This random quantity\nis of great importance in the so-called smoothed analysis of Demmel condition\nnumber. Finally, we find the average of the reciprocal of the characteristic\npolynomial $\\det[z\\mathbf{I}_n+\\mathbf{W}],\\; |\\arg z|<\\pi$, where\n$\\mathbf{I}_n$ and $\\det[\\cdot]$ denote the identity matrix of size $n$ and the\ndeterminant, respectively. \n\n"}
{"id": "1307.3719", "contents": "Title: Comparison of asymptotic variances of inhomogeneous Markov chains with\n  application to Markov chain Monte Carlo methods Abstract: In this paper, we study the asymptotic variance of sample path averages for\ninhomogeneous Markov chains that evolve alternatingly according to two\ndifferent $\\pi$-reversible Markov transition kernels $P$ and $Q$. More\nspecifically, our main result allows us to compare directly the asymptotic\nvariances of two inhomogeneous Markov chains associated with different kernels\n$P_i$ and $Q_i$, $i\\in\\{0,1\\}$, as soon as the kernels of each pair $(P_0,P_1)$\nand $(Q_0,Q_1)$ can be ordered in the sense of lag-one autocovariance. As an\nimportant application, we use this result for comparing different\ndata-augmentation-type Metropolis-Hastings algorithms. In particular, we\ncompare some pseudo-marginal algorithms and propose a novel exact algorithm,\nreferred to as the random refreshment algorithm, which is more efficient, in\nterms of asymptotic variance, than the Grouped Independence Metropolis-Hastings\nalgorithm and has a computational complexity that does not exceed that of the\nMonte Carlo Within Metropolis algorithm. \n\n"}
{"id": "1307.5339", "contents": "Title: The Cluster Graphical Lasso for improved estimation of Gaussian\n  graphical models Abstract: We consider the task of estimating a Gaussian graphical model in the\nhigh-dimensional setting. The graphical lasso, which involves maximizing the\nGaussian log likelihood subject to an l1 penalty, is a well-studied approach\nfor this task. We begin by introducing a surprising connection between the\ngraphical lasso and hierarchical clustering: the graphical lasso in effect\nperforms a two-step procedure, in which (1) single linkage hierarchical\nclustering is performed on the variables in order to identify connected\ncomponents, and then (2) an l1-penalized log likelihood is maximized on the\nsubset of variables within each connected component. In other words, the\ngraphical lasso determines the connected components of the estimated network\nvia single linkage clustering. Unfortunately, single linkage clustering is\nknown to perform poorly in certain settings. Therefore, we propose the cluster\ngraphical lasso, which involves clustering the features using an alternative to\nsingle linkage clustering, and then performing the graphical lasso on the\nsubset of variables within each cluster. We establish model selection\nconsistency for this technique, and demonstrate its improved performance\nrelative to the graphical lasso in a simulation study, as well as in\napplications to an equities data set, a university webpage data set, and a gene\nexpression data set. \n\n"}
{"id": "1307.5476", "contents": "Title: Bootstrapped Pivots for Sample and Population Means and Distribution\n  Functions Abstract: In this paper we introduce the concept of bootstrapped pivots for the sample\nand the population means. This is in contrast to the classical method of\nconstructing bootstrapped confidence intervals for the population mean via\nestimating the cutoff points via drawing a number of bootstrap sub-samples. We\nshow that this new method leads to constructing asymptotic confidence intervals\nwith significantly smaller error in comparison to both of the traditional\nt-intervals and the classical bootstrapped confidence intervals. The approach\ntaken in this paper relates naturally to super-population modeling, as well as\nto estimating empirical and theoretical distributions. \n\n"}
{"id": "1307.5870", "contents": "Title: Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery Abstract: Recovering a low-rank tensor from incomplete information is a recurring\nproblem in signal processing and machine learning. The most popular convex\nrelaxation of this problem minimizes the sum of the nuclear norms of the\nunfoldings of the tensor. We show that this approach can be substantially\nsuboptimal: reliably recovering a $K$-way tensor of length $n$ and Tucker rank\n$r$ from Gaussian measurements requires $\\Omega(r n^{K-1})$ observations. In\ncontrast, a certain (intractable) nonconvex formulation needs only $O(r^K +\nnrK)$ observations. We introduce a very simple, new convex relaxation, which\npartially bridges this gap. Our new formulation succeeds with $O(r^{\\lfloor K/2\n\\rfloor}n^{\\lceil K/2 \\rceil})$ observations. While these results pertain to\nGaussian measurements, simulations strongly suggest that the new norm also\noutperforms the sum of nuclear norms for tensor completion from a random subset\nof entries.\n  Our lower bound for the sum-of-nuclear-norms model follows from a new result\non recovering signals with multiple sparse structures (e.g. sparse, low rank),\nwhich perhaps surprisingly demonstrates the significant suboptimality of the\ncommonly used recovery approach via minimizing the sum of individual sparsity\ninducing norms (e.g. $l_1$, nuclear norm). Our new formulation for low-rank\ntensor recovery however opens the possibility in reducing the sample complexity\nby exploiting several structures jointly. \n\n"}
{"id": "1307.8136", "contents": "Title: DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering Abstract: The level set tree approach of Hartigan (1975) provides a probabilistically\nbased and highly interpretable encoding of the clustering behavior of a\ndataset. By representing the hierarchy of data modes as a dendrogram of the\nlevel sets of a density estimator, this approach offers many advantages for\nexploratory analysis and clustering, especially for complex and\nhigh-dimensional data. Several R packages exist for level set tree estimation,\nbut their practical usefulness is limited by computational inefficiency,\nabsence of interactive graphical capabilities and, from a theoretical\nperspective, reliance on asymptotic approximations. To make it easier for\npractitioners to capture the advantages of level set trees, we have written the\nPython package DeBaCl for DEnsity-BAsed CLustering. In this article we\nillustrate how DeBaCl's level set tree estimates can be used for difficult\nclustering tasks and interactive graphical data analysis. The package is\nintended to promote the practical use of level set trees through improvements\nin computational efficiency and a high degree of user customization. In\naddition, the flexible algorithms implemented in DeBaCl enjoy finite sample\naccuracy, as demonstrated in recent literature on density clustering. Finally,\nwe show the level set tree framework can be easily extended to deal with\nfunctional data. \n\n"}
{"id": "1308.3089", "contents": "Title: LAN property for families of distributions of solutions to Levy driven\n  SDE's Abstract: The LAN property is proved in the statistical model based on discrete-time\nobservations of a solution to a L\\'{e}vy driven SDE. The proof is based on a\ngeneral sufficient condition for a statistical model based on a discrete\nobservations of a Markov process to possess the LAN property, and involves\nsubstantially the Malliavin calculus-based integral representations for\nderivatives of log-likelihood of the model. \n\n"}
{"id": "1308.3728", "contents": "Title: On the causal interpretation of acyclic mixed graphs under multivariate\n  normality Abstract: In multivariate statistics, acyclic mixed graphs with directed and bidirected\nedges are widely used for compact representation of dependence structures that\ncan arise in the presence of hidden (i.e., latent or unobserved) variables.\nIndeed, under multivariate normality, every mixed graph corresponds to a set of\ncovariance matrices that contains as a full-dimensional subset the covariance\nmatrices associated with a causally interpretable acyclic digraph. This digraph\ngenerally has some of its nodes corresponding to hidden variables. We seek to\nclarify for which mixed graphs there exists an acyclic digraph whose hidden\nvariable model coincides with the mixed graph model. Restricting to the\ntractable setting of chain graphs and multivariate normality, we show that\ndecomposability of the bidirected part of the chain graph is necessary and\nsufficient for equality between the mixed graph model and some hidden variable\nmodel given by an acyclic digraph. \n\n"}
{"id": "1308.4718", "contents": "Title: Invertibility and Robustness of Phaseless Reconstruction Abstract: This paper is concerned with the question of reconstructing a vector in a\nfinite-dimensional real Hilbert space when only the magnitudes of the\ncoefficients of the vector under a redundant linear map are known. We analyze\nvarious Lipschitz bounds of the nonlinear analysis map and we establish\ntheoretical performance bounds of any reconstruction algorithm. We show that\nrobust and stable reconstruction requires additional redundancy than the\ncritical threshold. \n\n"}
{"id": "1308.6315", "contents": "Title: Clustering, Classification, Discriminant Analysis, and Dimension\n  Reduction via Generalized Hyperbolic Mixtures Abstract: A method for dimension reduction with clustering, classification, or\ndiscriminant analysis is introduced. This mixture model-based approach is based\non fitting generalized hyperbolic mixtures on a reduced subspace within the\nparadigm of model-based clustering, classification, or discriminant analysis. A\nreduced subspace of the data is derived by considering the extent to which\ngroup means and group covariances vary. The members of the subspace arise\nthrough linear combinations of the original data, and are ordered by importance\nvia the associated eigenvalues. The observations can be projected onto the\nsubspace, resulting in a set of variables that captures most of the clustering\ninformation available. The use of generalized hyperbolic mixtures gives a\nrobust framework capable of dealing with skewed clusters. Although dimension\nreduction is increasingly in demand across many application areas, the authors\nare most familiar with biological applications and so two of the five real data\nexamples are within that sphere. Simulated data are also used for illustration.\nThe approach introduced herein can be considered the most general such approach\navailable, and so we compare results to three special and limiting cases.\nComparisons with several well established techniques illustrate its promising\nperformance. \n\n"}
{"id": "1309.0787", "contents": "Title: Online Tensor Methods for Learning Latent Variable Models Abstract: We introduce an online tensor decomposition based approach for two latent\nvariable modeling problems namely, (1) community detection, in which we learn\nthe latent communities that the social actors in social networks belong to, and\n(2) topic modeling, in which we infer hidden topics of text articles. We\nconsider decomposition of moment tensors using stochastic gradient descent. We\nconduct optimization of multilinear operations in SGD and avoid directly\nforming the tensors, to save computational and storage costs. We present\noptimized algorithm in two platforms. Our GPU-based implementation exploits the\nparallelism of SIMD architectures to allow for maximum speed-up by a careful\noptimization of storage and data transfer, whereas our CPU-based implementation\nuses efficient sparse matrix computations and is suitable for large sparse\ndatasets. For the community detection problem, we demonstrate accuracy and\ncomputational efficiency on Facebook, Yelp and DBLP datasets, and for the topic\nmodeling problem, we also demonstrate good performance on the New York Times\ndataset. We compare our results to the state-of-the-art algorithms such as the\nvariational method, and report a gain of accuracy and a gain of several orders\nof magnitude in the execution time. \n\n"}
{"id": "1309.1913", "contents": "Title: Dynamic Team Theory of Stochastic Differential Decision Systems with\n  Decentralized Noisy Information Structures via Girsanov's Measure\n  Transformation Abstract: In this paper, we present two methods which generalize static team theory to\ndynamic team theory, in the context of continuous-time stochastic nonlinear\ndifferential decentralized decision systems, with relaxed strategies, which are\nmeasurable to different noisy information structures. For both methods we apply\nGirsanov's measure transformation to obtain an equivalent dynamic team problem\nunder a reference probability measure, so that the observations and information\nstructures available for decisions, are not affected by any of the team\ndecisions. The first method is based on function space integration with respect\nto products of Wiener measures, and generalizes Witsenhausen's [1] definition\nof equivalence between discrete-time static and dynamic team problems. The\nsecond method is based on stochastic Pontryagin's maximum principle. The team\noptimality conditions are given by a \"Hamiltonian System\" consisting of forward\nand backward stochastic differential equations, and a conditional variational\nHamiltonian with respect to the information structure of each team member,\nexpressed under the initial and a reference probability space via Girsanov's\nmeasure transformation. Under global convexity conditions, we show that that\nPbP optimality implies team optimality. In addition, we also show existence of\nteam and PbP optimal relaxed decentralized strategies (conditional\ndistributions), in the weak$^*$ sense, without imposing convexity on the action\nspaces of the team members. Moreover, using the embedding of regular strategies\ninto relaxed strategies, we also obtain team and PbP optimality conditions for\nregular team strategies, which are measurable functions of decentralized\ninformation structures, and we use the Krein-Millman theorem to show\nrealizability of relaxed strategies by regular strategies. \n\n"}
{"id": "1309.4111", "contents": "Title: Regularized Spectral Clustering under the Degree-Corrected Stochastic\n  Blockmodel Abstract: Spectral clustering is a fast and popular algorithm for finding clusters in\nnetworks. Recently, Chaudhuri et al. (2012) and Amini et al.(2012) proposed\ninspired variations on the algorithm that artificially inflate the node degrees\nfor improved statistical performance. The current paper extends the previous\nstatistical estimation results to the more canonical spectral clustering\nalgorithm in a way that removes any assumption on the minimum degree and\nprovides guidance on the choice of the tuning parameter. Moreover, our results\nshow how the \"star shape\" in the eigenvectors--a common feature of empirical\nnetworks--can be explained by the Degree-Corrected Stochastic Blockmodel and\nthe Extended Planted Partition model, two statistical models that allow for\nhighly heterogeneous degrees. Throughout, the paper characterizes and justifies\nseveral of the variations of the spectral clustering algorithm in terms of\nthese models. \n\n"}
{"id": "1309.6013", "contents": "Title: A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under\na general non-uniform sampling distribution using the max-norm as a convex\nrelaxation for the rank. A max-norm constrained maximum likelihood estimate is\nintroduced and studied. The rate of convergence for the estimate is obtained.\nInformation-theoretical methods are used to establish a minimax lower bound\nunder the general sampling model. The minimax upper and lower bounds together\nyield the optimal rate of convergence for the Frobenius norm loss.\nComputational algorithms and numerical performance are also discussed. \n\n"}
{"id": "1309.7837", "contents": "Title: The geometry of least squares in the 21st century Abstract: It has been over 200 years since Gauss's and Legendre's famous priority\ndispute on who discovered the method of least squares. Nevertheless, we argue\nthat the normal equations are still relevant in many facets of modern\nstatistics, particularly in the domain of high-dimensional inference. Even\ntoday, we are still learning new things about the law of large numbers, first\ndescribed in Bernoulli's Ars Conjectandi 300 years ago, as it applies to high\ndimensional inference. The other insight the normal equations provide is the\nasymptotic Gaussianity of the least squares estimators. The general form of the\nGaussian distribution, Gaussian processes, are another tool used in modern\nhigh-dimensional inference. The Gaussian distribution also arises via the\ncentral limit theorem in describing weak convergence of the usual least squares\nestimators. In terms of high-dimensional inference, we are still missing the\nright notion of weak convergence. In this mostly expository work, we try to\ndescribe how both the normal equations and the theory of Gaussian processes,\nwhat we refer to as the \"geometry of least squares,\" apply to many questions of\ncurrent interest. \n\n"}
{"id": "1310.1800", "contents": "Title: Generalized Negative Binomial Processes and the Representation of\n  Cluster Structures Abstract: The paper introduces the concept of a cluster structure to define a joint\ndistribution of the sample size and its exchangeable random partitions. The\ncluster structure allows the probability distribution of the random partitions\nof a subset of the sample to be dependent on the sample size, a feature not\npresented in a partition structure. A generalized negative binomial process\ncount-mixture model is proposed to generate a cluster structure, where in the\nprior the number of clusters is finite and Poisson distributed and the cluster\nsizes follow a truncated negative binomial distribution. The number and sizes\nof clusters can be controlled to exhibit distinct asymptotic behaviors. Unique\nmodel properties are illustrated with example clustering results using a\ngeneralized Polya urn sampling scheme. The paper provides new methods to\ngenerate exchangeable random partitions and to control both the cluster-number\nand cluster-size distributions. \n\n"}
{"id": "1310.3222", "contents": "Title: On the block maxima method in extreme value theory: PWM estimators Abstract: In extreme value theory, there are two fundamental approaches, both widely\nused: the block maxima (BM) method and the peaks-over-threshold (POT) method.\nWhereas much theoretical research has gone into the POT method, the BM method\nhas not been studied thoroughly. The present paper aims at providing conditions\nunder which the BM method can be justified. We also provide a theoretical\ncomparative study of the methods, which is in general consistent with the vast\nliterature on comparing the methods all based on simulated data and fully\nparametric models. The results indicate that the BM method is a rather\nefficient method under usual practical conditions. In this paper, we restrict\nattention to the i.i.d. case and focus on the probability weighted moment (PWM)\nestimators of Hosking, Wallis and Wood [Technometrics (1985) 27 251-261]. \n\n"}
{"id": "1310.4259", "contents": "Title: Range-Renewal Structure of I.I.D. Samplings Abstract: In this note the range-renewal structure of general independent and\nidentically distributed samplings is discussed, which is a natural extension of\nthe result in Chen et al (arXiv:1305.1829). \n\n"}
{"id": "1310.4794", "contents": "Title: The Gaussian Radon Transform and Machine Learning Abstract: There has been growing recent interest in probabilistic interpretations of\nkernel-based methods as well as learning in Banach spaces. The absence of a\nuseful Lebesgue measure on an infinite-dimensional reproducing kernel Hilbert\nspace is a serious obstacle for such stochastic models. We propose an\nestimation model for the ridge regression problem within the framework of\nabstract Wiener spaces and show how the support vector machine solution to such\nproblems can be interpreted in terms of the Gaussian Radon transform. \n\n"}
{"id": "1310.5304", "contents": "Title: Local asymptotic mixed normality property for nonsynchronously observed\n  diffusion processes Abstract: We prove the local asymptotic mixed normality (LAMN) property for a family of\nprobability measures defined by parametrized diffusion processes with\nnonsynchronous observations. We assume that observation times of processes are\nindependent of processes and we will study asymptotics when the maximum length\nof observation intervals goes to zero in probability. We also prove that the\nquasi-maximum likelihood estimator and the Bayes-type estimator proposed in\nOgihara and Yoshida (Stochastic Process. Appl. 124 (2014) 2954-3008) are\nasymptotically efficient. \n\n"}
{"id": "1310.5764", "contents": "Title: Minimax Optimal Convergence Rates for Estimating Ground Truth from\n  Crowdsourced Labels Abstract: Crowdsourcing has become a primary means for label collection in many\nreal-world machine learning applications. A classical method for inferring the\ntrue labels from the noisy labels provided by crowdsourcing workers is\nDawid-Skene estimator. In this paper, we prove convergence rates of a projected\nEM algorithm for the Dawid-Skene estimator. The revealed exponent in the rate\nof convergence is shown to be optimal via a lower bound argument. Our work\nresolves the long standing issue of whether Dawid-Skene estimator has sound\ntheoretical guarantees besides its good performance observed in practice. In\naddition, a comparative study with majority voting illustrates both advantages\nand pitfalls of the Dawid-Skene estimator. \n\n"}
{"id": "1310.7462", "contents": "Title: Asymptotic Properties of Bayes Risk of a General Class of Shrinkage\n  Priors in Multiple Hypothesis Testing Under Sparsity Abstract: Consider the problem of simultaneous testing for the means of independent\nnormal observations. In this paper, we study some asymptotic optimality\nproperties of certain multiple testing rules induced by a general class of\none-group shrinkage priors in a Bayesian decision theoretic framework, where\nthe overall loss is taken as the number of misclassified hypotheses. We assume\na two-groups normal mixture model for the data and consider the asymptotic\nframework adopted in Bogdan et al. (2011) who introduced the notion of\nasymptotic Bayes optimality under sparsity in the context of multiple testing.\nThe general class of one-group priors under study is rich enough to include,\namong others, the families of three parameter beta, generalized double Pareto\npriors, and in particular the horseshoe, the normal-exponential-gamma and the\nStrawderman-Berger priors. We establish that within our chosen asymptotic\nframework, the multiple testing rules under study asymptotically attain the\nrisk of the Bayes Oracle up to a multiplicative factor, with the constant in\nthe risk close to the constant in the Oracle risk. This is similar to a result\nobtained in Datta and Ghosh (2013) for the multiple testing rule based on the\nhorseshoe estimator introduced in Carvalho et al. (2009, 2010). We further show\nthat under very mild assumption on the underlying sparsity parameter, the\ninduced decision rules based on an empirical Bayes estimate of the\ncorresponding global shrinkage parameter proposed by van der Pas et al. (2014),\nattain the optimal Bayes risk up to the same multiplicative factor\nasymptotically. We provide a unifying argument applicable for the general class\nof priors under study. In the process, we settle a conjecture regarding\noptimality property of the generalized double Pareto priors made in Datta and\nGhosh (2013). Our work also shows that the result in Datta and Ghosh (2013) can\nbe improved further. \n\n"}
{"id": "1310.7838", "contents": "Title: A spectral mean for point sampled closed curves Abstract: We propose a spectral mean for closed curves described by sample points on\nits boundary subject to mis-alignment and noise. First, we ignore mis-alignment\nand derive maximum likelihood estimators of the model and noise parameters in\nthe Fourier domain. We estimate the unknown curve by back-transformation and\nderive the distribution of the integrated squared error. Then, we model\nmis-alignment by means of a shifted parametric diffeomorphism and minimise a\nsuitable objective function simultaneously over the unknown curve and the\nmis-alignment parameters. Finally, the method is illustrated on simulated data\nas well as on photographs of Lake Tana taken by astronauts during a Shuttle\nmission. \n\n"}
{"id": "1311.1154", "contents": "Title: Modeling of Volatility with Non-linear Time Series Model Abstract: In this paper, non-linear time series models are used to describe volatility\nin financial time series data. To describe volatility, two of the non-linear\ntime series are combined into form TAR (Threshold Auto-Regressive Model) with\nAARCH (Asymmetric Auto-Regressive Conditional Heteroskedasticity) error term\nand its parameter estimation is studied. \n\n"}
{"id": "1311.2234", "contents": "Title: FuSSO: Functional Shrinkage and Selection Operator Abstract: We present the FuSSO, a functional analogue to the LASSO, that efficiently\nfinds a sparse set of functional input covariates to regress a real-valued\nresponse against. The FuSSO does so in a semi-parametric fashion, making no\nparametric assumptions about the nature of input functional covariates and\nassuming a linear form to the mapping of functional covariates to the response.\nWe provide a statistical backing for use of the FuSSO via proof of asymptotic\nsparsistency under various conditions. Furthermore, we observe good results on\nboth synthetic and real-world data. \n\n"}
{"id": "1311.2657", "contents": "Title: Random perturbation of low rank matrices: Improving classical bounds Abstract: Matrix perturbation inequalities, such as Weyl's theorem (concerning the\nsingular values) and the Davis-Kahan theorem (concerning the singular vectors),\nplay essential roles in quantitative science; in particular, these bounds have\nfound application in data analysis as well as related areas of engineering and\ncomputer science. In many situations, the perturbation is assumed to be random,\nand the original matrix has certain structural properties (such as having low\nrank). We show that, in this scenario, classical perturbation results, such as\nWeyl and Davis-Kahan, can be improved significantly. We believe many of our new\nbounds are close to optimal and also discuss some applications. \n\n"}
{"id": "1311.3492", "contents": "Title: High-dimensional learning of linear causal networks via inverse\n  covariance estimation Abstract: We establish a new framework for statistical estimation of directed acyclic\ngraphs (DAGs) when data are generated from a linear, possibly non-Gaussian\nstructural equation model. Our framework consists of two parts: (1) inferring\nthe moralized graph from the support of the inverse covariance matrix; and (2)\nselecting the best-scoring graph amongst DAGs that are consistent with the\nmoralized graph. We show that when the error variances are known or estimated\nto close enough precision, the true DAG is the unique minimizer of the score\ncomputed using the reweighted squared l_2-loss. Our population-level results\nhave implications for the identifiability of linear SEMs when the error\ncovariances are specified up to a constant multiple. On the statistical side,\nwe establish rigorous conditions for high-dimensional consistency of our\ntwo-part algorithm, defined in terms of a \"gap\" between the true DAG and the\nnext best candidate. Finally, we demonstrate that dynamic programming may be\nused to select the optimal DAG in linear time when the treewidth of the\nmoralized graph is bounded. \n\n"}
{"id": "1311.5000", "contents": "Title: Convergence rates of eigenvector empirical spectral distribution of\n  large dimensional sample covariance matrix Abstract: The eigenvector Empirical Spectral Distribution (VESD) is adopted to\ninvestigate the limiting behavior of eigenvectors and eigenvalues of covariance\nmatrices. In this paper, we shall show that the Kolmogorov distance between the\nexpected VESD of sample covariance matrix and the Mar\\v{c}enko-Pastur\ndistribution function is of order $O(N^{-1/2})$. Given that data dimension $n$\nto sample size $N$ ratio is bounded between 0 and 1, this convergence rate is\nestablished under finite 10th moment condition of the underlying distribution.\nIt is also shown that, for any fixed $\\eta>0$, the convergence rates of VESD\nare $O(N^{-1/4})$ in probability and $O(N^{-1/4+\\eta})$ almost surely,\nrequiring finite 8th moment of the underlying distribution. \n\n"}
{"id": "1312.2315", "contents": "Title: Noisy Bayesian Active Learning Abstract: We consider the problem of noisy Bayesian active learning, where we are given\na finite set of functions $\\mathcal{H}$, a sample space $\\mathcal{X}$, and a\nlabel set $\\mathcal{L}$. One of the functions in $\\mathcal{H}$ assigns labels\nto samples in $\\mathcal{X}$. The goal is to identify the function that\ngenerates the labels even though the result of a label query on a sample is\ncorrupted by independent noise. More precisely, the objective is to declare one\nof the functions in $\\mathcal{H}$ as the true label generating function with\nhigh confidence using as few label queries as possible, by selecting the\nqueries adaptively and in a strategic manner.\n  Previous work in Bayesian active learning considers Generalized Binary\nSearch, and its variants for the noisy case, and analyzes the number of queries\nrequired by these sampling strategies. In this paper, we show that these\nschemes are, in general, suboptimal. Instead we propose and analyze an\nalternative strategy for sample collection. Our sampling strategy is motivated\nby a connection between Bayesian active learning and active hypothesis testing,\nand is based on querying the label of a sample which maximizes the Extrinsic\nJensen-Shannon divergence at each step. We provide upper and lower bounds on\nthe performance of this sampling strategy, and show that these bounds are\nbetter than previous bounds. \n\n"}
{"id": "1312.2574", "contents": "Title: Backing off from Infinity: Performance Bounds via Concentration of\n  Spectral Measure for Random MIMO Channels Abstract: The performance analysis of random vector channels, particularly\nmultiple-input-multiple-output (MIMO) channels, has largely been established in\nthe asymptotic regime of large channel dimensions, due to the analytical\nintractability of characterizing the exact distribution of the objective\nperformance metrics. This paper exposes a new non-asymptotic framework that\nallows the characterization of many canonical MIMO system performance metrics\nto within a narrow interval under moderate-to-large channel dimensionality,\nprovided that these metrics can be expressed as a separable function of the\nsingular values of the matrix. The effectiveness of our framework is\nillustrated through two canonical examples. Specifically, we characterize the\nmutual information and power offset of random MIMO channels, as well as the\nminimum mean squared estimation error of MIMO channel inputs from the channel\noutputs. Our results lead to simple, informative, and reasonably accurate\ncontrol of various performance metrics in the finite-dimensional regime, as\ncorroborated by the numerical simulations. Our analysis framework is\nestablished via the concentration of spectral measure phenomenon for random\nmatrices uncovered by Guionnet and Zeitouni, which arises in a variety of\nrandom matrix ensembles irrespective of the precise distributions of the matrix\nentries. \n\n"}
{"id": "1312.4349", "contents": "Title: Empirical risk minimization is optimal for the convex aggregation\n  problem Abstract: Let $F$ be a finite model of cardinality $M$ and denote by $\\operatorname\n{conv}(F)$ its convex hull. The problem of convex aggregation is to construct a\nprocedure having a risk as close as possible to the minimal risk over\n$\\operatorname {conv}(F)$. Consider the bounded regression model with respect\nto the squared risk denoted by $R(\\cdot)$. If\n${\\widehat{f}}_n^{\\mathit{ERM-C}}$ denotes the empirical risk minimization\nprocedure over $\\operatorname {conv}(F)$, then we prove that for any $x>0$,\nwith probability greater than $1-4\\exp(-x)$,\n\\[R({\\widehat{f}}_n^{\\mathit{ERM-C}})\\leq\\min_{f\\in \\operatorname\n{conv}(F)}R(f)+c_0\\max \\biggl(\\psi_n^{(C)}(M),\\frac{x}{n}\\biggr),\\] where\n$c_0>0$ is an absolute constant and $\\psi_n^{(C)}(M)$ is the optimal rate of\nconvex aggregation defined in (In Computational Learning Theory and Kernel\nMachines (COLT-2003) (2003) 303-313 Springer) by $\\psi_n^{(C)}(M)=M/n$ when\n$M\\leq \\sqrt{n}$ and $\\psi_n^{(C)}(M)=\\sqrt{\\log (\\mathrm{e}M/\\sqrt{n})/n}$\nwhen $M>\\sqrt{n}$. \n\n"}
{"id": "1312.4936", "contents": "Title: A Functional Hodrick Prescott Filter Abstract: We propose a functional version of the Hodrick-Prescott filter for functional\ndata which take values in an infinite dimensional separable Hilbert space. We\nfurther characterize the associated optimal smoothing parameter when the\nassociated linear operator is compact and the underlying distribution of the\ndata is Gaussian. \n\n"}
{"id": "1312.5594", "contents": "Title: M-Functionals of Multivariate Scatter Abstract: This survey provides a self-contained account of $M$-estimation of\nmultivariate scatter. In particular, we present new proofs for existence of the\nunderlying $M$-functionals and discuss their weak continuity and\ndifferentiability. This is done in a rather general framework with\nmatrix-valued random variables. By doing so we reveal a connection between\nTyler's (1987) $M$-functional of scatter and the estimation of proportional\ncovariance matrices. Moreover, this general framework allows us to treat a new\nclass of scatter estimators, based on symmetrizations of arbitrary order.\nFinally these results are applied to $M$-estimation of multivariate location\nand scatter via multivariate $t$-distributions. \n\n"}
{"id": "1312.5839", "contents": "Title: Model selection in a sparse heterogeneous framework Abstract: We consider a Gaussian sequence space model $X_{\\lambda}=f_{\\lambda} +\n\\xi_{\\lambda},$ where $\\xi $ has a diagonal covariance matrix\n$\\Sigma=\\diag(\\sigma_\\lambda ^2)$. We consider the situation where the\nparameter vector $(f_{\\lambda})$ is sparse. Our goal is to estimate the unknown\nparameter by a model selection approach. The heterogenous case is much more\ninvolved than the direct model. Indeed, there is no more symmetry inside the\nstochastic process that one needs to control since each empirical coefficient\nhas its own variance. The problem and the penalty do not only depend on the\nnumber of coefficients that one selects, but also on their position. This\nappears also in the minimax bounds where the worst coefficients will go to the\nlarger variances. However, with a careful and explicit choice of the penalty we\nare able to select the correct coefficients and get a sharp non-asymptotic\ncontrol of the risk of our procedure. Some simulation results are provided. \n\n"}
{"id": "1312.6026", "contents": "Title: How to Construct Deep Recurrent Neural Networks Abstract: In this paper, we explore different ways to extend a recurrent neural network\n(RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in\nan RNN is not as clear as it is in feedforward neural networks. By carefully\nanalyzing and understanding the architecture of an RNN, however, we find three\npoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)\nhidden-to-hidden transition and (3) hidden-to-output function. Based on this\nobservation, we propose two novel architectures of a deep RNN which are\northogonal to an earlier attempt of stacking multiple recurrent layers to build\na deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an\nalternative interpretation of these deep RNNs using a novel framework based on\nneural operators. The proposed deep RNNs are empirically evaluated on the tasks\nof polyphonic music prediction and language modeling. The experimental result\nsupports our claim that the proposed deep RNNs benefit from the depth and\noutperform the conventional, shallow RNNs. \n\n"}
{"id": "1312.7402", "contents": "Title: Adaptive pointwise estimation of conditional density function Abstract: In this paper we consider the problem of estimating $f$, the conditional\ndensity of $Y$ given $X$, by using an independent sample distributed as $(X,Y)$\nin the multivariate setting. We consider the estimation of $f(x,.)$ where $x$\nis a fixed point. We define two different procedures of estimation, the first\none using kernel rules, the second one inspired from projection methods. Both\nadapted estimators are tuned by using the Goldenshluger and Lepski methodology.\nAfter deriving lower bounds, we show that these procedures satisfy oracle\ninequalities and are optimal from the minimax point of view on anisotropic\nH{\\\"o}lder balls. Furthermore, our results allow us to measure precisely the\ninfluence of $\\mathrm{f}\\_X(x)$ on rates of convergence, where $\\mathrm{f}\\_X$\nis the density of $X$. Finally, some simulations illustrate the good behavior\nof our tuned estimates in practice. \n\n"}
{"id": "1312.7614", "contents": "Title: Inference on causal and structural parameters using many moment\n  inequalities Abstract: This paper considers the problem of testing many moment inequalities where\nthe number of moment inequalities, denoted by $p$, is possibly much larger than\nthe sample size $n$. There is a variety of economic applications where solving\nthis problem allows to carry out inference on causal and structural parameters,\na notable example is the market structure model of Ciliberto and Tamer (2009)\nwhere $p=2^{m+1}$ with $m$ being the number of firms that could possibly enter\nthe market. We consider the test statistic given by the maximum of $p$\nStudentized (or $t$-type) inequality-specific statistics, and analyze various\nways to compute critical values for the test statistic. Specifically, we\nconsider critical values based upon (i) the union bound combined with a\nmoderate deviation inequality for self-normalized sums, (ii) the multiplier and\nempirical bootstraps, and (iii) two-step and three-step variants of (i) and\n(ii) by incorporating the selection of uninformative inequalities that are far\nfrom being binding and a novel selection of weakly informative inequalities\nthat are potentially binding but do not provide first order information. We\nprove validity of these methods, showing that under mild conditions, they lead\nto tests with the error in size decreasing polynomially in $n$ while allowing\nfor $p$ being much larger than $n$, indeed $p$ can be of order $\\exp (n^{c})$\nfor some $c > 0$. Importantly, all these results hold without any restriction\non the correlation structure between $p$ Studentized statistics, and also hold\nuniformly with respect to suitably large classes of underlying distributions.\nMoreover, in the online supplement, we show validity of a test based on the\nblock multiplier bootstrap in the case of dependent data under some general\nmixing conditions. \n\n"}
{"id": "1401.2709", "contents": "Title: A quantum linguistic characterization of the reverse relation between\n  confidence interval and hypothesis testing Abstract: Although there are many ideas for the formulations of statistical hypothesis\ntesting, we consider that the likelihood ratio test is the most reasonable and\northodox. However, it is not handy, and thus, it is not usual in elementary\nbooks. That is, the statistical hypothesis testing written in elementary books\nis different from the likelihood ratio test. Thus, from the theoretical point\nof view, we have the following question: \"What is the statistical hypothesis\ntesting written in elementary books?\" For example, we consider that even the\ndifference between \"one sided test\" and \"two sided test\" is not clear yet. In\nthis paper, we give an answer to this question. That is, we propose a new\nformulation of statistical hypothesis testing, which is contrary to the\nconfidence interval methods. In other words, they are two sides of the same\ncoin. This will be done in quantum language (or, measurement theory), which is\ncharacterized as the linguistic turn of the Copenhagen interpretation of\nquantum mechanics, and also, a kind of system theory such that it is applicable\nto both classical and quantum systems. Since quantum language is suited for\ntheoretical arguments, we believe that our results are essentially final as a\ngeneral theory. \n\n"}
{"id": "1401.3020", "contents": "Title: Empirical geodesic graphs and CAT(k) metrics for data analysis Abstract: A methodology is developed for data analysis based on empirically constructed\ngeodesic metric spaces. For a probability distribution, the length along a path\nbetween two points can be defined as the amount of probability mass accumulated\nalong the path. The geodesic, then, is the shortest such path and defines a\ngeodesic metric. Such metrics are transformed in a number of ways to produce\nparametrised families of geodesic metric spaces, empirical versions of which\nallow computation of intrinsic means and associated measures of dispersion.\nThese reveal properties of the data, based on geometry, such as those that are\ndifficult to see from the raw Euclidean distances. Examples of application\ninclude clustering and classification. For certain parameter ranges, the spaces\nbecome CAT(0) spaces and the intrinsic means are unique. In one case, a minimal\nspanning tree of a graph based on the data becomes CAT(0). In another, a\nso-called \"metric cone\" construction allows extension to CAT($k$) spaces. It is\nshown how to empirically tune the parameters of the metrics, making it possible\nto apply them to a number of real cases. \n\n"}
{"id": "1401.3121", "contents": "Title: Law-invariant risk measures: extension properties and qualitative\n  robustness Abstract: We characterize when a convex risk measure associated to a law-invariant\nacceptance set in $L^\\infty$ can be extended to $L^p$, $1\\leq p<\\infty$,\npreserving finiteness and continuity. This problem is strongly connected to the\nstatistical robustness of the corresponding risk measures. Special attention is\npaid to concrete examples including risk measures based on expected utility,\nmax-correlation risk measures, and distortion risk measures. \n\n"}
{"id": "1401.3146", "contents": "Title: The Blackwell relation defines no lattice Abstract: Blackwell's theorem shows the equivalence of two preorders on the set of\ninformation channels. Here, we restate, and slightly generalize, his result in\nterms of random variables. Furthermore, we prove that the corresponding partial\norder is not a lattice; that is, least upper bounds and greatest lower bounds\ndo not exist. \n\n"}
{"id": "1401.3889", "contents": "Title: Exact Post-Selection Inference for Sequential Regression Procedures Abstract: We propose new inference tools for forward stepwise regression, least angle\nregression, and the lasso. Assuming a Gaussian model for the observation vector\ny, we first describe a general scheme to perform valid inference after any\nselection event that can be characterized as y falling into a polyhedral set.\nThis framework allows us to derive conditional (post-selection) hypothesis\ntests at any step of forward stepwise or least angle regression, or any step\nalong the lasso regularization path, because, as it turns out, selection events\nfor these procedures can be expressed as polyhedral constraints on y. The\np-values associated with these tests are exactly uniform under the null\ndistribution, in finite samples, yielding exact type I error control. The tests\ncan also be inverted to produce confidence intervals for appropriate underlying\nregression parameters. The R package \"selectiveInference\", freely available on\nthe CRAN repository, implements the new inference tools described in this\npaper. \n\n"}
{"id": "1401.5833", "contents": "Title: Multiscale Dictionary Learning: Non-Asymptotic Bounds and Robustness Abstract: High-dimensional datasets are well-approximated by low-dimensional\nstructures. Over the past decade, this empirical observation motivated the\ninvestigation of detection, measurement, and modeling techniques to exploit\nthese low-dimensional intrinsic structures, yielding numerous implications for\nhigh-dimensional statistics, machine learning, and signal processing. Manifold\nlearning (where the low-dimensional structure is a manifold) and dictionary\nlearning (where the low-dimensional structure is the set of sparse linear\ncombinations of vectors from a finite dictionary) are two prominent theoretical\nand computational frameworks in this area. Despite their ostensible\ndistinction, the recently-introduced Geometric Multi-Resolution Analysis (GMRA)\nprovides a robust, computationally efficient, multiscale procedure for\nsimultaneously learning manifolds and dictionaries.\n  In this work, we prove non-asymptotic probabilistic bounds on the\napproximation error of GMRA for a rich class of data-generating statistical\nmodels that includes \"noisy\" manifolds, thereby establishing the theoretical\nrobustness of the procedure and confirming empirical observations. In\nparticular, if a dataset aggregates near a low-dimensional manifold, our\nresults show that the approximation error of the GMRA is completely independent\nof the ambient dimension. Our work therefore establishes GMRA as a provably\nfast algorithm for dictionary learning with approximation and sparsity\nguarantees. We include several numerical experiments confirming these\ntheoretical results, and our theoretical framework provides new tools for\nassessing the behavior of manifold learning and dictionary learning procedures\non a large class of interesting models. \n\n"}
{"id": "1401.6145", "contents": "Title: On Stochastic Geometry Modeling of Cellular Uplink Transmission with\n  Truncated Channel Inversion Power Control Abstract: Using stochastic geometry, we develop a tractable uplink modeling paradigm\nfor outage probability and spectral efficiency in both single and multi-tier\ncellular wireless networks. The analysis accounts for per user equipment (UE)\npower control as well as the maximum power limitations for UEs. More\nspecifically, for interference mitigation and robust uplink communication, each\nUE is required to control its transmit power such that the average received\nsignal power at its serving base station (BS) is equal to a certain threshold\n$\\rho_o$. Due to the limited transmit power, the UEs employ a truncated channel\ninversion power control policy with a cutoff threshold of $\\rho_o$. We show\nthat there exists a transfer point in the uplink system performance that\ndepends on the tuple: BS intensity ($\\lambda$), maximum transmit power of UEs\n($P_u$), and $\\rho_o$. That is, when $P_u$ is a tight operational constraint\nwith respect to [w.r.t.] $\\lambda$ and $\\rho_o$, the uplink outage probability\nand spectral efficiency highly depend on the values of $\\lambda$ and $\\rho_o$.\nIn this case, there exists an optimal cutoff threshold $\\rho^*_o$, which\ndepends on the system parameters, that minimizes the outage probability. On the\nother hand, when $P_u$ is not a binding operational constraint w.r.t. $\\lambda$\nand $\\rho_o$, the uplink outage probability and spectral efficiency become\nindependent of $\\lambda$ and $\\rho_o$. We obtain approximate yet accurate\nsimple expressions for outage probability and spectral efficiency which reduce\nto closed-forms in some special cases. \n\n"}
{"id": "1401.6882", "contents": "Title: Bandwidth selection in kernel empirical risk minimization via the\n  gradient Abstract: In this paper, we deal with the data-driven selection of multidimensional and\npossibly anisotropic bandwidths in the general framework of kernel empirical\nrisk minimization. We propose a universal selection rule, which leads to\noptimal adaptive results in a large variety of statistical models such as\nnonparametric robust regression and statistical learning with errors in\nvariables. These results are stated in the context of smooth loss functions,\nwhere the gradient of the risk appears as a good criterion to measure the\nperformance of our estimators. The selection rule consists of a comparison of\ngradient empirical risks. It can be viewed as a nontrivial improvement of the\nso-called Goldenshluger-Lepski method to nonlinear estimators. Furthermore, one\nmain advantage of our selection rule is the nondependency on the Hessian matrix\nof the risk, usually involved in standard adaptive procedures. \n\n"}
{"id": "1402.1920", "contents": "Title: Degrees of Freedom and Model Search Abstract: Degrees of freedom is a fundamental concept in statistical modeling, as it\nprovides a quantitative description of the amount of fitting performed by a\ngiven procedure. But, despite this fundamental role in statistics, its behavior\nnot completely well-understood, even in some fairly basic settings. For\nexample, it may seem intuitively obvious that the best subset selection fit\nwith subset size k has degrees of freedom larger than k, but this has not been\nformally verified, nor has is been precisely studied. In large part, the\ncurrent paper is motivated by this particular problem, and we derive an exact\nexpression for the degrees of freedom of best subset selection in a restricted\nsetting (orthogonal predictor variables). Along the way, we develop a concept\nthat we name \"search degrees of freedom\"; intuitively, for adaptive regression\nprocedures that perform variable selection, this is a part of the (total)\ndegrees of freedom that we attribute entirely to the model selection mechanism.\nFinally, we establish a modest extension of Stein's formula to cover\ndiscontinuous functions, and discuss its potential role in degrees of freedom\nand search degrees of freedom calculations. \n\n"}
{"id": "1402.1958", "contents": "Title: Better Optimism By Bayes: Adaptive Planning with Rich Models Abstract: The computational costs of inference and planning have confined Bayesian\nmodel-based reinforcement learning to one of two dismal fates: powerful\nBayes-adaptive planning but only for simplistic models, or powerful, Bayesian\nnon-parametric models but using simple, myopic planning strategies such as\nThompson sampling. We ask whether it is feasible and truly beneficial to\ncombine rich probabilistic models with a closer approximation to fully Bayesian\nplanning. First, we use a collection of counterexamples to show formal problems\nwith the over-optimism inherent in Thompson sampling. Then we leverage\nstate-of-the-art techniques in efficient Bayes-adaptive planning and\nnon-parametric Bayesian methods to perform qualitatively better than both\nexisting conventional algorithms and Thompson sampling on two contextual\nbandit-like problems. \n\n"}
{"id": "1402.2043", "contents": "Title: Approachability in unknown games: Online learning meets multi-objective\n  optimization Abstract: In the standard setting of approachability there are two players and a target\nset. The players play repeatedly a known vector-valued game where the first\nplayer wants to have the average vector-valued payoff converge to the target\nset which the other player tries to exclude it from this set. We revisit this\nsetting in the spirit of online learning and do not assume that the first\nplayer knows the game structure: she receives an arbitrary vector-valued reward\nvector at every round. She wishes to approach the smallest (\"best\") possible\nset given the observed average payoffs in hindsight. This extension of the\nstandard setting has implications even when the original target set is not\napproachable and when it is not obvious which expansion of it should be\napproached instead. We show that it is impossible, in general, to approach the\nbest target set in hindsight and propose achievable though ambitious\nalternative goals. We further propose a concrete strategy to approach these\ngoals. Our method does not require projection onto a target set and amounts to\nswitching between scalar regret minimization algorithms that are performed in\nepisodes. Applications to global cost minimization and to approachability under\nsample path constraints are considered. \n\n"}
{"id": "1402.2209", "contents": "Title: Approximative Tests for the Equality of Two Cumulative Incidence\n  Functions of a Competing Risk Abstract: In the context of the widely used competing risks set-up we discuss different\ninference procedures for testing equality of two cumulative incidence\nfunctions, where the data may be subject to independent right-censoring or\nleft-truncation. To this end we compare two-sample Kolmogorov-Smirnov- and\nCramer-von Mises-type test statistics. Since, in general, their corresponding\nasymptotic limit distributions depend on unknown quantities, we utilize wild\nbootstrap resampling as well as approximation techniques to construct adequate\ntest decisions. Here the latter procedures are motivated from testing\nprocedures for heteroscedastic factorial designs but have not yet been proposed\nin the survival context. A simulation study shows the performance of all\nconsidered tests under various settings and finally a real data example about\nbloodstream infection during neutropenia is used to illustrate their\napplication. \n\n"}
{"id": "1402.2243", "contents": "Title: Semiparametric topographical mixture models with symmetric errors Abstract: Motivated by the analysis of a Positron Emission Tomography (PET) imaging\ndata considered in Bowen et al. (2012), we introduce a semiparametric\ntopographical mixture model able to capture the characteristics of dichotomous\nshifted response-type experiments. We propose a local estimation procedure,\nbased on the symmetry of the local noise, for the proportion and locations\nfunctions involved in the proposed model. We establish under mild conditions\nthe minimax properties and asymptotic normality of our estimators when Monte\nCarlo simulations are conducted to examine their finite sample performance.\nFinally a statistical analysis of the PET imaging data in Bowen et al. (2012)\nis illustrated for the proposed method. \n\n"}
{"id": "1402.2966", "contents": "Title: Nonparametric Estimation of Renyi Divergence and Friends Abstract: We consider nonparametric estimation of $L_2$, Renyi-$\\alpha$ and\nTsallis-$\\alpha$ divergences between continuous distributions. Our approach is\nto construct estimators for particular integral functionals of two densities\nand translate them into divergence estimators. For the integral functionals,\nour estimators are based on corrections of a preliminary plug-in estimator. We\nshow that these estimators achieve the parametric convergence rate of\n$n^{-1/2}$ when the densities' smoothness, $s$, are both at least $d/4$ where\n$d$ is the dimension. We also derive minimax lower bounds for this problem\nwhich confirm that $s > d/4$ is necessary to achieve the $n^{-1/2}$ rate of\nconvergence. We validate our theoretical guarantees with a number of\nsimulations. \n\n"}
{"id": "1402.2997", "contents": "Title: Degrees of freedom for nonlinear least squares estimation Abstract: We give a general result on the effective degrees of freedom for nonlinear\nleast squares estimation, which relates the degrees of freedom to the\ndivergence of the estimator. We show that in a general framework, the\ndivergence of the least squares estimator is a well defined but potentially\nnegatively biased estimate of the degrees of freedom, and we give an exact\nrepresentation of the bias. This implies that if we use the divergence as a\nplug-in estimate of the degrees of freedom in Stein's unbiased risk estimate\n(SURE), we generally underestimate the true risk. Our result applies, for\ninstance, to model searching problems, yielding a finite sample\ncharacterization of how much the search contributes to the degrees of freedom.\nMotivated by the problem of fitting ODE models in systems biology, the general\nresults are illustrated by the estimation of systems of linear ODEs. In this\nexample the divergence turns out to be a useful estimate of degrees of freedom\nfor $\\ell_1$-constrained models. \n\n"}
{"id": "1402.3695", "contents": "Title: About the non-asymptotic behaviour of Bayes estimators Abstract: This paper investigates the {\\em nonasymptotic} properties of Bayes\nprocedures for estimating an unknown distribution from $n$ i.i.d.\\\nobservations. We assume that the prior is supported by a model $(\\scr{S},h)$\n(where $h$ denotes the Hellinger distance) with suitable metric properties\ninvolving the number of small balls that are needed to cover larger ones. We\nalso require that the prior put enough probability on small balls.\n  We consider two different situations. The simplest case is the one of a\nparametric model containing the target density for which we show that the\nposterior concentrates around the true distribution at rate $1/\\sqrt{n}$. In\nthe general situation, we relax the parametric assumption and take into account\na possible mispecification of the model. Provided that the Kullback-Leibler\nInformation between the true distribution and $\\scr{S}$ is finite, we establish\nrisk bounds for the Bayes estimators. \n\n"}
{"id": "1402.3835", "contents": "Title: Testing probability distributions underlying aggregated data Abstract: In this paper, we analyze and study a hybrid model for testing and learning\nprobability distributions. Here, in addition to samples, the testing algorithm\nis provided with one of two different types of oracles to the unknown\ndistribution $D$ over $[n]$. More precisely, we define both the dual and\ncumulative dual access models, in which the algorithm $A$ can both sample from\n$D$ and respectively, for any $i\\in[n]$,\n  - query the probability mass $D(i)$ (query access); or\n  - get the total mass of $\\{1,\\dots,i\\}$, i.e. $\\sum_{j=1}^i D(j)$ (cumulative\naccess)\n  These two models, by generalizing the previously studied sampling and query\noracle models, allow us to bypass the strong lower bounds established for a\nnumber of problems in these settings, while capturing several interesting\naspects of these problems -- and providing new insight on the limitations of\nthe models. Finally, we show that while the testing algorithms can be in most\ncases strictly more efficient, some tasks remain hard even with this additional\npower. \n\n"}
{"id": "1402.4520", "contents": "Title: Generalised matrix multivariate $T$-distribution Abstract: Supposing Kotz-Riesz type I and II distributions and their corresponding\nindependent univariate Riesz distributions the associated generalised matrix\nmultivariate T distributions, termed matrix multivariate T-Riesz distributions\nare obtained. In addition, its various properties are studied. All these\nresults are obtained for real normed division algebras. \n\n"}
{"id": "1402.4773", "contents": "Title: Signal detection for inverse problems in a multidimensional framework Abstract: This paper is devoted to multi-dimensional inverse problems. In this setting,\nwe address a goodness-of-fit testing problem. We investigate the separation\nrates associated to different kinds of smoothness assumptions and different\ndegrees of ill-posedness. \n\n"}
{"id": "1402.5763", "contents": "Title: Performance of empirical risk minimization in linear aggregation Abstract: We study conditions under which, given a dictionary $F=\\{f_1,\\ldots ,f_M\\}$\nand an i.i.d. sample $(X_i,Y_i)_{i=1}^N$, the empirical minimizer in\n$\\operatorname {span}(F)$ relative to the squared loss, satisfies that with\nhigh probability\n\\[R\\bigl(\\tilde{f}^{\\mathrm{ERM}}\\bigr)\\leq\\inf_{f\\in\\operatorname\n{span}(F)}R(f)+r_N(M),\\] where $R(\\cdot)$ is the squared risk and $r_N(M)$ is\nof the order of $M/N$. Among other results, we prove that a uniform small-ball\nestimate for functions in $\\operatorname {span}(F)$ is enough to achieve that\ngoal when the noise is independent of the design. \n\n"}
{"id": "1402.6419", "contents": "Title: Asymptotic Linear Spectral Statistics for Spiked Hermitian Random Matrix\n  Models Abstract: Using the Coulomb Fluid method, this paper derives central limit theorems\n(CLTs) for linear spectral statistics of three \"spiked\" Hermitian random matrix\nensembles. These include Johnstone's spiked model (i.e., central Wishart with\nspiked correlation), non-central Wishart with rank-one non-centrality, and a\nrelated class of non-central $F$ matrices. For a generic linear statistic, we\nderive simple and explicit CLT expressions as the matrix dimensions grow large.\nFor all three ensembles under consideration, we find that the primary effect of\nthe spike is to introduce an $O(1)$ correction term to the asymptotic mean of\nthe linear spectral statistic, which we characterize with simple formulas. The\nutility of our proposed framework is demonstrated through application to three\ndifferent linear statistics problems: the classical likelihood ratio test for a\npopulation covariance, the capacity analysis of multi-antenna wireless\ncommunication systems with a line-of-sight transmission path, and a classical\nmultiple sample significance testing problem. \n\n"}
{"id": "1403.0217", "contents": "Title: High-frequency asymptotics for path-dependent functionals of Ito\n  semimartingales Abstract: The estimation of local characteristics of Ito semimartingales has received a\ngreat deal of attention in both academia and industry over the past decades. In\nvarious papers limit theorems were derived for functionals of increments and\nranges in the infill asymptotics setting. In this paper we establish the\nasymptotic theory for a wide class of statistics that are built from the\nincremental process of an Ito semimartingale. More specifically, we will show\nthe law of large numbers and the associated stable central limit theorem for\nthe path dependent functionals in the continuous and discontinuous framework.\nSome examples from economics and physics demonstrate the potential\napplicability of our theoretical results in practice. \n\n"}
{"id": "1403.3374", "contents": "Title: High-dimensional Ising model selection with Bayesian information\n  criteria Abstract: We consider the use of Bayesian information criteria for selection of the\ngraph underlying an Ising model. In an Ising model, the full conditional\ndistributions of each variable form logistic regression models, and variable\nselection techniques for regression allow one to identify the neighborhood of\neach node and, thus, the entire graph. We prove high-dimensional consistency\nresults for this pseudo-likelihood approach to graph selection when using\nBayesian information criteria for the variable selection problems in the\nlogistic regressions. The results pertain to scenarios of sparsity and\nfollowing related prior work the information criteria we consider incorporate\nan explicit prior that encourages sparsity. \n\n"}
{"id": "1403.5456", "contents": "Title: Levy processes with summable Levy measures, long time behavior Abstract: In our previous paper (ArXiv:1306.1492) we have proved that a representation\nof the infinitesimal generators $L$ for Levy processes $X_t$ can be written\ndown in a convolution type form. For the case of non-summable Levy measures we\nconstructed the quasi-potential operators $B$ and investigated the long time\nbehavior of $X_t$. In the present paper we consider Levy processes $X_t$ with\nsummable Levy measures. In this case the form of the quasi-potential operators\n$B$ essentially differs from the form in the case of non-summable Levy\nmeasures. We use this new form in order to study the long time behavior of\n$X_t$ for the case of summable Levy measures. \n\n"}
{"id": "1403.6532", "contents": "Title: Minimax Optimal Rates for Poisson Inverse Problems with Physical\n  Constraints Abstract: This paper considers fundamental limits for solving sparse inverse problems\nin the presence of Poisson noise with physical constraints. Such problems arise\nin a variety of applications, including photon-limited imaging systems based on\ncompressed sensing. Most prior theoretical results in compressed sensing and\nrelated inverse problems apply to idealized settings where the noise is i.i.d.,\nand do not account for signal-dependent noise and physical sensing constraints.\nPrior results on Poisson compressed sensing with signal-dependent noise and\nphysical constraints provided upper bounds on mean squared error performance\nfor a specific class of estimators. However, it was unknown whether those\nbounds were tight or if other estimators could achieve significantly better\nperformance. This work provides minimax lower bounds on mean-squared error for\nsparse Poisson inverse problems under physical constraints. Our lower bounds\nare complemented by minimax upper bounds. Our upper and lower bounds reveal\nthat due to the interplay between the Poisson noise model, the sparsity\nconstraint and the physical constraints: (i) the mean-squared error does not\ndepend on the sample size $n$ other than to ensure the sensing matrix satisfies\nRIP-like conditions and the intensity $T$ of the input signal plays a critical\nrole; and (ii) the mean-squared error has two distinct regimes, a low-intensity\nand a high-intensity regime and the transition point from the low-intensity to\nhigh-intensity regime depends on the input signal $f^*$. In the low-intensity\nregime the mean-squared error is independent of $T$ while in the high-intensity\nregime, the mean-squared error scales as $\\frac{s \\log p}{T}$, where $s$ is the\nsparsity level, $p$ is the number of pixels or parameters and $T$ is the signal\nintensity. \n\n"}
{"id": "1403.7023", "contents": "Title: Worst possible sub-directions in high-dimensional models Abstract: We examine the rate of convergence of the Lasso estimator of lower\ndimensional components of the high-dimensional parameter. Under bounds on the\n$\\ell_1$-norm on the worst possible sub-direction these rates are of order\n$\\sqrt {|J| \\log p / n }$ where $p$ is the total number of parameters, $J\n\\subset \\{ 1, \\ldots, p \\}$ represents a subset of the parameters and $n$ is\nthe number of observations. We also derive rates in sup-norm in terms of the\nrate of convergence in $\\ell_1$-norm. The irrepresentable condition on a set\n$J$ requires that the $\\ell_1$-norm of the worst possible sub-direction is\nsufficiently smaller than one. In that case sharp oracle results can be\nobtained. Moreover, if the coefficients in $J$ are small enough the Lasso will\nput these coefficients to zero. This extends known results which say that the\nirrepresentable condition on the inactive set (the set where coefficients are\nexactly zero) implies no false positives. We further show that by\nde-sparsifying one obtains fast rates in supremum norm without conditions on\nthe worst possible sub-direction. The main assumption here is that approximate\nsparsity is of order $o (\\sqrt n / \\log p )$. The results are extended to\nM-estimation with $\\ell_1$-penalty for generalized linear models and\nexponential families for example. For the graphical Lasso this leads to an\nextension of known results to the case where the precision matrix is only\napproximately sparse. The bounds we provide are non-asymptotic but we also\npresent asymptotic formulations for ease of interpretation. \n\n"}
{"id": "1404.0198", "contents": "Title: On the Fisher Metric of Conditional Probability Polytopes Abstract: We consider three different approaches to define natural Riemannian metrics\non polytopes of stochastic matrices. First, we define a natural class of\nstochastic maps between these polytopes and give a metric characterization of\nChentsov type in terms of invariance with respect to these maps. Second, we\nconsider the Fisher metric defined on arbitrary polytopes through their\nembeddings as exponential families in the probability simplex. We show that\nthese metrics can also be characterized by an invariance principle with respect\nto morphisms of exponential families. Third, we consider the Fisher metric\nresulting from embedding the polytope of stochastic matrices in a simplex of\njoint distributions by specifying a marginal distribution. All three approaches\nresult in slight variations of products of Fisher metrics. This is consistent\nwith the nature of polytopes of stochastic matrices, which are Cartesian\nproducts of probability simplices. The first approach yields a scaled product\nof Fisher metrics; the second, a product of Fisher metrics; and the third, a\nproduct of Fisher metrics scaled by the marginal distribution. \n\n"}
{"id": "1404.0202", "contents": "Title: The Horseshoe Estimator: Posterior Concentration around Nearly Black\n  Vectors Abstract: We consider the horseshoe estimator due to Carvalho, Polson and Scott (2010)\nfor the multivariate normal mean model in the situation that the mean vector is\nsparse in the nearly black sense. We assume the frequentist framework where the\ndata is generated according to a fixed mean vector. We show that if the number\nof nonzero parameters of the mean vector is known, the horseshoe estimator\nattains the minimax $\\ell_2$ risk, possibly up to a multiplicative constant. We\nprovide conditions under which the horseshoe estimator combined with an\nempirical Bayes estimate of the number of nonzero means still yields the\nminimax risk. We furthermore prove an upper bound on the rate of contraction of\nthe posterior distribution around the horseshoe estimator, and a lower bound on\nthe posterior variance. These bounds indicate that the posterior distribution\nof the horseshoe prior may be more informative than that of other one-component\npriors, including the Lasso. \n\n"}
{"id": "1404.1530", "contents": "Title: Provable Deterministic Leverage Score Sampling Abstract: We explain theoretically a curious empirical phenomenon: \"Approximating a\nmatrix by deterministically selecting a subset of its columns with the\ncorresponding largest leverage scores results in a good low-rank matrix\nsurrogate\". To obtain provable guarantees, previous work requires randomized\nsampling of the columns with probabilities proportional to their leverage\nscores.\n  In this work, we provide a novel theoretical analysis of deterministic\nleverage score sampling. We show that such deterministic sampling can be\nprovably as accurate as its randomized counterparts, if the leverage scores\nfollow a moderately steep power-law decay. We support this power-law assumption\nby providing empirical evidence that such decay laws are abundant in real-world\ndata sets. We then demonstrate empirically the performance of deterministic\nleverage score sampling, which many times matches or outperforms the\nstate-of-the-art techniques. \n\n"}
{"id": "1404.2355", "contents": "Title: High-dimensional genome-wide association study and misspecified mixed\n  model analysis Abstract: We study behavior of the restricted maximum likelihood (REML) estimator under\na misspecified linear mixed model (LMM) that has received much attention in\nrecent gnome-wide association studies. The asymptotic analysis establishes\nconsistency of the REML estimator of the variance of the errors in the LMM, and\nconvergence in probability of the REML estimator of the variance of the random\neffects in the LMM to a certain limit, which is equal to the true variance of\nthe random effects multiplied by the limiting proportion of the nonzero random\neffects present in the LMM. The aymptotic results also establish convergence\nrate (in probability) of the REML estimators as well as a result regarding\nconvergence of the asymptotic conditional variance of the REML estimator. The\nasymptotic results are fully supported by the results of empirical studies,\nwhich include extensive simulation studies that compare the performance of the\nREML estimator (under the misspecified LMM) with other existing methods. \n\n"}
{"id": "1404.2825", "contents": "Title: Asymptotics of Fingerprinting and Group Testing: Capacity-Achieving\n  Log-Likelihood Decoders Abstract: We study the large-coalition asymptotics of fingerprinting and group testing,\nand derive explicit decoders that provably achieve capacity for many of the\nconsidered models. We do this both for simple decoders (fast but suboptimal)\nand for joint decoders (slow but optimal), and both for informed and uninformed\nsettings.\n  For fingerprinting, we show that if the pirate strategy is known, the\nNeyman-Pearson-based log-likelihood decoders provably achieve capacity,\nregardless of the strategy. The decoder built against the interleaving attack\nis further shown to be a universal decoder, able to deal with arbitrary attacks\nand achieving the uninformed capacity. This universal decoder is shown to be\nclosely related to the Lagrange-optimized decoder of Oosterwijk et al. and the\nempirical mutual information decoder of Moulin. Joint decoders are also\nproposed, and we conjecture that these also achieve the corresponding joint\ncapacities.\n  For group testing, the simple decoder for the classical model is shown to be\nmore efficient than the one of Chan et al. and it provably achieves the simple\ngroup testing capacity. For generalizations of this model such as noisy group\ntesting, the resulting simple decoders also achieve the corresponding simple\ncapacities. \n\n"}
{"id": "1404.3203", "contents": "Title: Compressive classification and the rare eclipse problem Abstract: This paper addresses the fundamental question of when convex sets remain\ndisjoint after random projection. We provide an analysis using ideas from\nhigh-dimensional convex geometry. For ellipsoids, we provide a bound in terms\nof the distance between these ellipsoids and simple functions of their\npolynomial coefficients. As an application, this theorem provides bounds for\ncompressive classification of convex sets. Rather than assuming that the data\nto be classified is sparse, our results show that the data can be acquired via\nvery few measurements yet will remain linearly separable. We demonstrate the\nfeasibility of this approach in the context of hyperspectral imaging. \n\n"}
{"id": "1404.6298", "contents": "Title: The Use of a Single Pseudo-Sample in Approximate Bayesian Computation Abstract: We analyze the computational efficiency of approximate Bayesian computation\n(ABC), which approximates a likelihood function by drawing pseudo-samples from\nthe associated model. For the rejection sampling version of ABC, it is known\nthat multiple pseudo-samples cannot substantially increase (and can\nsubstantially decrease) the efficiency of the algorithm as compared to\nemploying a high-variance estimate based on a single pseudo-sample. We show\nthat this conclusion also holds for a Markov chain Monte Carlo version of ABC,\nimplying that it is unnecessary to tune the number of pseudo-samples used in\nABC-MCMC. This conclusion is in contrast to particle MCMC methods, for which\nincreasing the number of particles can provide large gains in computational\nefficiency. \n\n"}
{"id": "1404.6989", "contents": "Title: The Maximum Likelihood Threshold of a Graph Abstract: The maximum likelihood threshold of a graph is the smallest number of data\npoints that guarantees that maximum likelihood estimates exist almost surely in\nthe Gaussian graphical model associated to the graph. We show that this graph\nparameter is connected to the theory of combinatorial rigidity. In particular,\nif the edge set of a graph $G$ is an independent set in the $n-1$-dimensional\ngeneric rigidity matroid, then the maximum likelihood threshold of $G$ is less\nthan or equal to $n$. This connection allows us to prove many results about the\nmaximum likelihood threshold. \n\n"}
{"id": "1405.0480", "contents": "Title: Asymptotic equivalence for inhomogeneous jump diffusion processes and\n  white noise Abstract: We prove the global asymptotic equivalence between the experiments generated\nby the discrete (high frequency) or continuous observation of a path of a time\ninhomogeneous jump-diffusion process and a Gaussian white noise experiment.\nHere, the considered parameter is the drift function, and we suppose that the\nobservation time $T$ tends to $\\infty$. The approximation is given in the sense\nof the Le Cam $\\Delta$-distance, under smoothness conditions on the unknown\ndrift function. These asymptotic equivalences are established by constructing\nexplicit Markov kernels that can be used to reproduce one experiment from the\nother. \n\n"}
{"id": "1405.3319", "contents": "Title: Fully Bayesian Logistic Regression with Hyper-Lasso Priors for\n  High-dimensional Feature Selection Abstract: High-dimensional feature selection arises in many areas of modern science.\nFor example, in genomic research we want to find the genes that can be used to\nseparate tissues of different classes (e.g. cancer and normal) from tens of\nthousands of genes that are active (expressed) in certain tissue cells. To this\nend, we wish to fit regression and classification models with a large number of\nfeatures (also called variables, predictors). In the past decade, penalized\nlikelihood methods for fitting regression models based on hyper-LASSO\npenalization have received increasing attention in the literature. However,\nfully Bayesian methods that use Markov chain Monte Carlo (MCMC) are still in\nlack of development in the literature. In this paper we introduce an MCMC\n(fully Bayesian) method for learning severely multi-modal posteriors of\nlogistic regression models based on hyper-LASSO priors (non-convex penalties).\nOur MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling\nframework; we call our method Bayesian logistic regression with hyper-LASSO\n(BLRHL) priors. We have used simulation studies and real data analysis to\ndemonstrate the superior performance of hyper-LASSO priors, and to investigate\nthe issues of choosing heaviness and scale of hyper-LASSO priors. \n\n"}
{"id": "1405.3340", "contents": "Title: Post-selection point and interval estimation of signal sizes in Gaussian\n  samples Abstract: We tackle the problem of the estimation of a vector of means from a single\nvector-valued observation $y$. Whereas previous work reduces the size of the\nestimates for the largest (absolute) sample elements via shrinkage (like\nJames-Stein) or biases estimated via empirical Bayes methodology, we take a\nnovel approach. We adapt recent developments by Lee et al (2013) in post\nselection inference for the Lasso to the orthogonal setting, where sample\nelements have different underlying signal sizes. This is exactly the setup\nencountered when estimating many means. It is shown that other selection\nprocedures, like selecting the $K$ largest (absolute) sample elements and the\nBenjamini-Hochberg procedure, can be cast into their framework, allowing us to\nleverage their results. Point and interval estimates for signal sizes are\nproposed. These seem to perform quite well against competitors, both recent and\nmore tenured.\n  Furthermore, we prove an upper bound to the worst case risk of our estimator,\nwhen combined with the Benjamini-Hochberg procedure, and show that it is within\na constant multiple of the minimax risk over a rich set of parameter spaces\nmeant to evoke sparsity. \n\n"}
{"id": "1405.6792", "contents": "Title: Discussion: \"A significance test for the lasso\" Abstract: Discussion of \"A significance test for the lasso\" by Richard Lockhart,\nJonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161]. \n\n"}
{"id": "1405.6805", "contents": "Title: Rejoinder: \"A significance test for the lasso\" Abstract: Rejoinder of \"A significance test for the lasso\" by Richard Lockhart,\nJonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161]. \n\n"}
{"id": "1406.0052", "contents": "Title: Variable selection in high-dimensional additive models based on norms of\n  projections Abstract: We consider the problem of variable selection in high-dimensional sparse\nadditive models. We focus on the case that the components belong to\nnonparametric classes of functions. The proposed method is motivated by\ngeometric considerations in Hilbert spaces and consists of comparing the norms\nof the projections of the data onto various additive subspaces. Under minimal\ngeometric assumptions, we prove concentration inequalities which lead to new\nconditions under which consistent variable selection is possible. As an\napplication, we establish conditions under which a single component can be\nestimated with the rate of convergence corresponding to the situation in which\nthe other components are known. \n\n"}
{"id": "1406.3269", "contents": "Title: Scheduled denoising autoencoders Abstract: We present a representation learning method that learns features at multiple\ndifferent levels of scale. Working within the unsupervised framework of\ndenoising autoencoders, we observe that when the input is heavily corrupted\nduring training, the network tends to learn coarse-grained features, whereas\nwhen the input is only slightly corrupted, the network tends to learn\nfine-grained features. This motivates the scheduled denoising autoencoder,\nwhich starts with a high level of noise that lowers as training progresses. We\nfind that the resulting representation yields a significant boost on a later\nsupervised task compared to the original input, or to a standard denoising\nautoencoder trained at a single noise level. After supervised fine-tuning our\nbest model achieves the lowest ever reported error on the CIFAR-10 data set\namong permutation-invariant methods. \n\n"}
{"id": "1406.4765", "contents": "Title: Fourth Moments and Independent Component Analysis Abstract: In independent component analysis it is assumed that the components of the\nobserved random vector are linear combinations of latent independent random\nvariables, and the aim is then to find an estimate for a transformation matrix\nback to these independent components. In the engineering literature, there are\nseveral traditional estimation procedures based on the use of fourth moments,\nsuch as FOBI (fourth order blind identification), JADE (joint approximate\ndiagonalization of eigenmatrices), and FastICA, but the statistical properties\nof these estimates are not well known. In this paper various independent\ncomponent functionals based on the fourth moments are discussed in detail,\nstarting with the corresponding optimization problems, deriving the estimating\nequations and estimation algorithms, and finding asymptotic statistical\nproperties of the estimates. Comparisons of the asymptotic variances of the\nestimates in wide independent component models show that in most cases JADE and\nthe symmetric version of FastICA perform better than their competitors. \n\n"}
{"id": "1406.4775", "contents": "Title: Non-negative Principal Component Analysis: Message Passing Algorithms\n  and Sharp Asymptotics Abstract: Principal component analysis (PCA) aims at estimating the direction of\nmaximal variability of a high-dimensional dataset. A natural question is: does\nthis task become easier, and estimation more accurate, when we exploit\nadditional knowledge on the principal vector? We study the case in which the\nprincipal vector is known to lie in the positive orthant. Similar constraints\narise in a number of applications, ranging from analysis of gene expression\ndata to spike sorting in neural signal processing.\n  In the unconstrained case, the estimation performances of PCA has been\nprecisely characterized using random matrix theory, under a statistical model\nknown as the `spiked model.' It is known that the estimation error undergoes a\nphase transition as the signal-to-noise ratio crosses a certain threshold.\nUnfortunately, tools from random matrix theory have no bearing on the\nconstrained problem. Despite this challenge, we develop an analogous\ncharacterization in the constrained case, within a one-spike model.\n  In particular: $(i)$~We prove that the estimation error undergoes a similar\nphase transition, albeit at a different threshold in signal-to-noise ratio that\nwe determine exactly; $(ii)$~We prove that --unlike in the unconstrained case--\nestimation error depends on the spike vector, and characterize the least\nfavorable vectors; $(iii)$~We show that a non-negative principal component can\nbe approximately computed --under the spiked model-- in nearly linear time.\nThis despite the fact that the problem is non-convex and, in general, NP-hard\nto solve exactly. \n\n"}
{"id": "1406.5936", "contents": "Title: The Markov basis of $K_{3,N}$ Abstract: This document explains how to obtain a Markov basis of the graphical model of\nthe complete bipartite graph $K_{3,N}$ with binary nodes. The computations\nillustrate the theory developed in arXiv:1404.6392 that explains how to compute\nMarkov bases of toric fiber products. \n\n"}
{"id": "1406.5973", "contents": "Title: Dependence of maxima in space Abstract: We propose a coefficient that measures the dependence among large values for\nspatial processes of maxima. Its main properties are: a) $k$ locations can be\ntaken into account; b) it takes values in $[0,1]$ and higher values indicate\nstronger dependence; c) it is independent of the univariate marginal\ndistributions of the random field; d) it can be related with the tail\ndependence and the extremal coefficients; e) it agrees with the concordance\nproperty for multivariate distributions; f) it has as a particular case the\nvariogram from geostatistics; g) it can be easily estimated. \n\n"}
{"id": "1406.6959", "contents": "Title: Maximum Likelihood Estimation of Functionals of Discrete Distributions Abstract: We consider the problem of estimating functionals of discrete distributions,\nand focus on tight nonasymptotic analysis of the worst case squared error risk\nof widely used estimators. We apply concentration inequalities to analyze the\nrandom fluctuation of these estimators around their expectations, and the\ntheory of approximation using positive linear operators to analyze the\ndeviation of their expectations from the true functional, namely their\n\\emph{bias}.\n  We characterize the worst case squared error risk incurred by the Maximum\nLikelihood Estimator (MLE) in estimating the Shannon entropy $H(P) = \\sum_{i =\n1}^S -p_i \\ln p_i$, and $F_\\alpha(P) = \\sum_{i = 1}^S p_i^\\alpha,\\alpha>0$, up\nto multiplicative constants, for any alphabet size $S\\leq \\infty$ and sample\nsize $n$ for which the risk may vanish. As a corollary, for Shannon entropy\nestimation, we show that it is necessary and sufficient to have $n \\gg S$\nobservations for the MLE to be consistent. In addition, we establish that it is\nnecessary and sufficient to consider $n \\gg S^{1/\\alpha}$ samples for the MLE\nto consistently estimate $F_\\alpha(P), 0<\\alpha<1$. The minimax rate-optimal\nestimators for both problems require $S/\\ln S$ and $S^{1/\\alpha}/\\ln S$\nsamples, which implies that the MLE has a strictly sub-optimal sample\ncomplexity. When $1<\\alpha<3/2$, we show that the worst-case squared error rate\nof convergence for the MLE is $n^{-2(\\alpha-1)}$ for infinite alphabet size,\nwhile the minimax squared error rate is $(n\\ln n)^{-2(\\alpha-1)}$. When\n$\\alpha\\geq 3/2$, the MLE achieves the minimax optimal rate $n^{-1}$ regardless\nof the alphabet size.\n  As an application of the general theory, we analyze the Dirichlet prior\nsmoothing techniques for Shannon entropy estimation. We show that no matter how\nwe tune the parameters in the Dirichlet prior, this technique cannot achieve\nthe minimax rates in entropy estimation. \n\n"}
{"id": "1406.7711", "contents": "Title: A definition of qualitative robustness for general point estimators, and\n  examples Abstract: A definition of qualitative robustness for point estimators in general\nstatistical models is proposed. Some criteria for robustness are established\nand applied to estimators in parametric, semiparametric, and nonparametric\nmodels. In specific nonparametric models, the proposed definition boils down to\nHampel robustness. It is also explained how plug-in estimators in certain\nnonparametric models can be reasonably classified w.r.t. their degrees of\nrobustness. \n\n"}
{"id": "1407.0381", "contents": "Title: Minimax rates of entropy estimation on large alphabets via best\n  polynomial approximation Abstract: Consider the problem of estimating the Shannon entropy of a distribution over\n$k$ elements from $n$ independent samples. We show that the minimax mean-square\nerror is within universal multiplicative constant factors of $$\\Big(\\frac{k }{n\n\\log k}\\Big)^2 + \\frac{\\log^2 k}{n}$$ if $n$ exceeds a constant factor of\n$\\frac{k}{\\log k}$; otherwise there exists no consistent estimator. This\nrefines the recent result of Valiant-Valiant \\cite{VV11} that the minimal\nsample size for consistent entropy estimation scales according to\n$\\Theta(\\frac{k}{\\log k})$. The apparatus of best polynomial approximation\nplays a key role in both the construction of optimal estimators and, via a\nduality argument, the minimax lower bound. \n\n"}
{"id": "1407.3397", "contents": "Title: Adaptive Bernstein-von Mises theorems in Gaussian white noise Abstract: We investigate Bernstein-von Mises theorems for adaptive nonparametric\nBayesian procedures in the canonical Gaussian white noise model. We consider\nboth a Hilbert space and multiscale setting with applications in $L^2$ and\n$L^\\infty$ respectively. This provides a theoretical justification for plug-in\nprocedures, for example the use of certain credible sets for sufficiently\nsmooth linear functionals. We use this general approach to construct optimal\nfrequentist confidence sets based on the posterior distribution. We also\nprovide simulations to numerically illustrate our approach and obtain a visual\nrepresentation of the geometries involved. \n\n"}
{"id": "1407.4578", "contents": "Title: Maximal Autocorrelation Functions in Functional Data Analysis Abstract: This paper proposes a new factor rotation for the context of functional\nprincipal components analysis. This rotation seeks to re-represent a functional\nsubspace in terms of directions of decreasing smoothness as represented by a\ngeneralized smoothing metric. The rotation can be implemented simply and we\nshow on two examples that this rotation can improve the interpretability of the\nleading components. \n\n"}
{"id": "1407.5978", "contents": "Title: Sequential Changepoint Approach for Online Community Detection Abstract: We present new algorithms for detecting the emergence of a community in large\nnetworks from sequential observations. The networks are modeled using\nErdos-Renyi random graphs with edges forming between nodes in the community\nwith higher probability. Based on statistical changepoint detection\nmethodology, we develop three algorithms: the Exhaustive Search (ES), the\nmixture, and the Hierarchical Mixture (H-Mix) methods. Performance of these\nmethods is evaluated by the average run length (ARL), which captures the\nfrequency of false alarms, and the detection delay. Numerical comparisons show\nthat the ES method performs the best; however, it is exponentially complex. The\nmixture method is polynomially complex by exploiting the fact that the size of\nthe community is typically small in a large network. However, it may react to a\ngroup of active edges that do not form a community. This issue is resolved by\nthe H-Mix method, which is based on a dendrogram decomposition of the network.\nWe present an asymptotic analytical expression for ARL of the mixture method\nwhen the threshold is large. Numerical simulation verifies that our\napproximation is accurate even in the non-asymptotic regime. Hence, it can be\nused to determine a desired threshold efficiently. Finally, numerical examples\nshow that the mixture and the H-Mix methods can both detect a community quickly\nwith a lower complexity than the ES method. \n\n"}
{"id": "1407.6288", "contents": "Title: Subspace Learning From Bits Abstract: Networked sensing, where the goal is to perform complex inference using a\nlarge number of inexpensive and decentralized sensors, has become an\nincreasingly attractive research topic due to its applications in wireless\nsensor networks and internet-of-things. To reduce the communication, sensing\nand storage complexity, this paper proposes a simple sensing and estimation\nframework to faithfully recover the principal subspace of high-dimensional data\nstreams using a collection of binary measurements from distributed sensors,\nwithout transmitting the whole data. The binary measurements are designed to\nindicate comparison outcomes of aggregated energy projections of the data\nsamples over pairs of randomly selected directions. When the covariance matrix\nis a low-rank matrix, we propose a spectral estimator that recovers the\nprincipal subspace of the covariance matrix as the subspace spanned by the top\neigenvectors of a properly designed surrogate matrix, which is provably\naccurate as soon as the number of binary measurements is sufficiently large. An\nadaptive rank selection strategy based on soft thresholding is also presented.\nFurthermore, we propose a tailored spectral estimator when the covariance\nmatrix is additionally Toeplitz, and show reliable estimation can be obtained\nfrom a substantially smaller number of binary measurements. Our results hold\neven when a constant fraction of the binary measurements is randomly flipped.\nFinally, we develop a low-complexity online algorithm to track the principal\nsubspace when new measurements arrive sequentially. Numerical examples are\nprovided to validate the proposed approach. \n\n"}
{"id": "1407.7811", "contents": "Title: Dimension reduction for data of unknown cluster structure Abstract: For numerous reasons there raises a need for dimension reduction that\npreserves certain characteristics of data. In this work we focus on data coming\nfrom a mixture of Gaussian distributions and we propose a method that preserves\ndistinctness of clustering structure, although the structure is assumed to be\nyet unknown. The rationale behind the method is the following: (i) had one\nknown the clusters (classes) within the data, one could facilitate further\nanalysis and reduce space dimension by projecting the data to the Fisher's\nlinear subspace, which -- by definition -- preserves the structure of the given\nclasses best (ii) under some reasonable assumptions, this can be done, albeit\napproximately, without the prior knowledge of the clusters (classes). In the\npaper, we show how this approach works. We present a method of preliminary data\ntransformation that brings the directions of largest overall variability close\nto the directions of the best between-class separation. Hence, for the\ntransformed data, simple PCA provides an approximation to the Fisher's\nsubspace. We show that the transformation preserves distinctness of unknown\nstructure in the data to a great extent. \n\n"}
{"id": "1407.8246", "contents": "Title: Exponential decay of reconstruction error from binary measurements of\n  sparse signals Abstract: Binary measurements arise naturally in a variety of statistical and\nengineering applications. They may be inherent to the problem---e.g., in\ndetermining the relationship between genetics and the presence or absence of a\ndisease---or they may be a result of extreme quantization. In one-bit\ncompressed sensing it has recently been shown that the number of one-bit\nmeasurements required for signal estimation mirrors that of unquantized\ncompressed sensing. Indeed, $s$-sparse signals in $\\mathbb{R}^n$ can be\nestimated (up to normalization) from $\\Omega(s \\log (n/s))$ one-bit\nmeasurements. Nevertheless, controlling the precise accuracy of the error\nestimate remains an open challenge. In this paper, we focus on optimizing the\ndecay of the error as a function of the oversampling factor $\\lambda := m/(s\n\\log(n/s))$, where $m$ is the number of measurements. It is known that the\nerror in reconstructing sparse signals from standard one-bit measurements is\nbounded below by $\\Omega(\\lambda^{-1})$. Without adjusting the measurement\nprocedure, reducing this polynomial error decay rate is impossible. However, we\nshow that an adaptive choice of the thresholds used for quantization may lower\nthe error rate to $e^{-\\Omega(\\lambda)}$. This improves upon guarantees for\nother methods of adaptive thresholding as proposed in Sigma-Delta quantization.\nWe develop a general recursive strategy to achieve this exponential decay and\ntwo specific polynomial-time algorithms which fall into this framework, one\nbased on convex programming and one on hard thresholding. This work is inspired\nby the one-bit compressed sensing model, in which the engineer controls the\nmeasurement procedure. Nevertheless, the principle is extendable to signal\nreconstruction problems in a variety of binary statistical models as well as\nstatistical estimation problems like logistic regression. \n\n"}
{"id": "1407.8300", "contents": "Title: Optimization of relative arbitrage Abstract: In stochastic portfolio theory, a relative arbitrage is an equity portfolio\nwhich is guaranteed to outperform a benchmark portfolio over a finite horizon.\nWhen the market is diverse and sufficiently volatile, and the benchmark is the\nmarket or a buy-and-hold portfolio, functionally generated portfolios\nintroduced by Fernholz provide a systematic way of constructing relative\narbitrages. In this paper we show that if the market portfolio is replaced by\nthe equal or entropy weighted portfolio among many others, no relative\narbitrages can be constructed under the same conditions using functionally\ngenerated portfolios. We also introduce and study a shaped-constrained\noptimization problem for functionally generated portfolios in the spirit of\nmaximum likelihood estimation of a log-concave density. \n\n"}
{"id": "1408.1681", "contents": "Title: Super-resolution, Extremal Functions and the Condition Number of\n  Vandermonde Matrices Abstract: Super-resolution is a fundamental task in imaging, where the goal is to\nextract fine-grained structure from coarse-grained measurements. Here we are\ninterested in a popular mathematical abstraction of this problem that has been\nwidely studied in the statistics, signal processing and machine learning\ncommunities. We exactly resolve the threshold at which noisy super-resolution\nis possible. In particular, we establish a sharp phase transition for the\nrelationship between the cutoff frequency ($m$) and the separation ($\\Delta$).\nIf $m > 1/\\Delta + 1$, our estimator converges to the true values at an inverse\npolynomial rate in terms of the magnitude of the noise. And when $m <\n(1-\\epsilon) /\\Delta$ no estimator can distinguish between a particular pair of\n$\\Delta$-separated signals even if the magnitude of the noise is exponentially\nsmall.\n  Our results involve making novel connections between {\\em extremal functions}\nand the spectral properties of Vandermonde matrices. We establish a sharp phase\ntransition for their condition number which in turn allows us to give the first\nnoise tolerance bounds for the matrix pencil method. Moreover we show that our\nmethods can be interpreted as giving preconditioners for Vandermonde matrices,\nand we use this observation to design faster algorithms for super-resolution.\nWe believe that these ideas may have other applications in designing faster\nalgorithms for other basic tasks in signal processing. \n\n"}
{"id": "1408.4057", "contents": "Title: Adaptation to lowest density regions with application to support\n  recovery Abstract: A scheme for locally adaptive bandwidth selection is proposed which\nsensitively shrinks the bandwidth of a kernel estimator at lowest density\nregions such as the support boundary which are unknown to the statistician. In\ncase of a H\\\"{o}lder continuous density, this locally minimax-optimal bandwidth\nis shown to be smaller than the usual rate, even in case of homogeneous\nsmoothness. Some new type of risk bound with respect to a density-dependent\nstandardized loss of this estimator is established. This bound is fully\nnonasymptotic and allows to deduce convergence rates at lowest density regions\nthat can be substantially faster than $n^{-1/2}$. It is complemented by a\nweighted minimax lower bound which splits into two regimes depending on the\nvalue of the density. The new estimator adapts into the second regime, and it\nis shown that simultaneous adaptation into the fastest regime is not possible\nin principle as long as the H\\\"{o}lder exponent is unknown. Consequences on\nplug-in rules for support recovery are worked out in detail. In contrast to\nthose with classical density estimators, the plug-in rules based on the new\nconstruction are minimax-optimal, up to some logarithmic factor. \n\n"}
{"id": "1408.6937", "contents": "Title: An Exact Formula for the Average Run Length to False Alarm of the\n  Generalized Shiryaev-Roberts Procedure for Change-Point Detection under\n  Exponential Observations Abstract: We derive analytically an exact closed-form formula for the standard minimax\nAverage Run Length (ARL) to false alarm delivered by the Generalized\nShiryaev-Roberts (GSR) change-point detection procedure devised to detect a\nshift in the baseline mean of a sequence of independent exponentially\ndistributed observations. Specifically, the formula is found through direct\nsolution of the respective integral (renewal) equation, and is a general result\nin that the GSR procedure's headstart is not restricted to a bounded range, nor\nis there a \"ceiling\" value for the detection threshold. Apart from the\ntheoretical significance (in change-point detection, exact closed-form\nperformance formulae are typically either difficult or impossible to get,\nespecially for the GSR procedure), the obtained formula is also useful to a\npractitioner: in cases of practical interest, the formula is a function linear\nin both the detection threshold and the headstart, and, therefore, the ARL to\nfalse alarm of the GSR procedure can be easily computed. \n\n"}
{"id": "1409.0912", "contents": "Title: Letter to the Editor of Annals of Applied Statistics Abstract: This is \"Letter to the Editor\" of Annals of Applied Statistics, addressing\nthe paper by Goerg G. M. (2011) \"Lambert W random variables-a new family of\ngeneralized skewed distributions with applications to risk estimation\". \n\n"}
{"id": "1409.1333", "contents": "Title: Model-based regression clustering for high-dimensional data. Application\n  to functional data Abstract: Finite mixture regression models are useful for modeling the relationship\nbetween response and predictors, arising from different subpopulations. In this\narticle, we study high-dimensional predic- tors and high-dimensional response,\nand propose two procedures to deal with this issue. We propose to use the Lasso\nestimator to take into account the sparsity, and a penalty on the rank, to take\ninto account the matrix structure. Then, we extend these procedures to the\nfunctional case, where predictors and responses are functions. For this\npurpose, we use a wavelet-based approach. Finally, for each situation, we\nprovide algorithms, and apply and evaluate our methods both on simulations and\nreal datasets. \n\n"}
{"id": "1409.1419", "contents": "Title: Finite Sample Properties of Tests Based on Prewhitened Nonparametric\n  Covariance Estimators Abstract: We analytically investigate size and power properties of a popular family of\nprocedures for testing linear restrictions on the coefficient vector in a\nlinear regression model with temporally dependent errors. The tests considered\nare autocorrelation-corrected F-type tests based on prewhitened nonparametric\ncovariance estimators that possibly incorporate a data-dependent bandwidth\nparameter, e.g., estimators as considered in Andrews and Monahan (1992), Newey\nand West (1994), or Rho and Shao (2013). For design matrices that are generic\nin a measure theoretic sense we prove that these tests either suffer from\nextreme size distortions or from strong power deficiencies. Despite this\nnegative result we demonstrate that a simple adjustment procedure based on\nartificial regressors can often resolve this problem. \n\n"}
{"id": "1409.2344", "contents": "Title: A nonparametric two-sample hypothesis testing problem for random dot\n  product graphs Abstract: We consider the problem of testing whether two finite-dimensional random dot\nproduct graphs have generating latent positions that are independently drawn\nfrom the same distribution, or distributions that are related via scaling or\nprojection. We propose a test statistic that is a kernel-based function of the\nadjacency spectral embedding for each graph. We obtain a limiting distribution\nfor our test statistic under the null and we show that our test procedure is\nconsistent across a broad range of alternatives. \n\n"}
{"id": "1409.4317", "contents": "Title: Bootstrap-Based K-Sample Testing For Functional Data Abstract: We investigate properties of a bootstrap-based methodology for testing\nhypotheses about equality of certain characteristics of the distributions\nbetween different populations in the context of functional data. The suggested\ntesting methodology is simple and easy to implement. It resamples the original\ndataset in such a way that the null hypothesis of interest is satisfied and it\ncan be potentially applied to a wide range of testing problems and test\nstatistics of interest. Furthermore, it can be utilized to the case where more\nthan two populations of functional data are considered. We illustrate the\nbootstrap procedure by considering the important problems of testing the\nequality of mean functions or the equality of covariance functions (resp.\ncovariance operators) between two populations. Theoretical results that justify\nthe validity of the suggested bootstrap-based procedure are established.\nFurthermore, simulation results demonstrate very good size and power\nperformances in finite sample situations, including the case of testing\nproblems and/or sample sizes where asymptotic considerations do not lead to\nsatisfactory approximations. A real-life dataset analyzed in the literature is\nalso examined. \n\n"}
{"id": "1409.5928", "contents": "Title: Estimation for models defined by conditions on their L-moments Abstract: This paper extends the empirical minimum divergence approach for models which\nsatisfy linear constraints with respect to the probability measure of the\nunderlying variable (moment constraints) to the case where such constraints\npertain to its quantile measure (called here semi parametric quantile models).\nThe case when these constraints describe shape conditions as handled by the\nL-moments is considered and both the description of these models as well as the\nresulting non classical minimum divergence procedures are presented. These\nmodels describe neighborhoods of classical models used mainly for their tail\nbehavior, for example neighborhoods of Pareto or Weibull distributions, with\nwhich they may share the same first L-moments. A parallel is drawn with similar\nproblems held in elasticity theory and in optimal transport problems. The\nproperties of the resulting estimators are illustrated by simulated examples\ncomparing Maximum Likelihood estimators on Pareto and Weibull models to the\nminimum Chi-square empirical divergence approach on semi parametric quantile\nmodels, and others. \n\n"}
{"id": "1409.7548", "contents": "Title: Large complex correlated Wishart matrices: Fluctuations and asymptotic\n  independence at the edges Abstract: We study the asymptotic behavior of eigenvalues of large complex correlated\nWishart matrices at the edges of the limiting spectrum. In this setting, the\nsupport of the limiting eigenvalue distribution may have several connected\ncomponents. Under mild conditions for the population matrices, we show that for\nevery generic positive edge of that support, there exists an extremal\neigenvalue which converges almost surely toward that edge and fluctuates\naccording to the Tracy-Widom law at the scale $N^{2/3}$. Moreover, given\nseveral generic positive edges, we establish that the associated extremal\neigenvalue fluctuations are asymptotically independent. Finally, when the\nleftmost edge is the origin (hard edge), the fluctuations of the smallest\neigenvalue are described by mean of the Bessel kernel at the scale $N^2$. \n\n"}
{"id": "1409.7552", "contents": "Title: The Advantage of Cross Entropy over Entropy in Iterative Information\n  Gathering Abstract: Gathering the most information by picking the least amount of data is a\ncommon task in experimental design or when exploring an unknown environment in\nreinforcement learning and robotics. A widely used measure for quantifying the\ninformation contained in some distribution of interest is its entropy. Greedily\nminimizing the expected entropy is therefore a standard method for choosing\nsamples in order to gain strong beliefs about the underlying random variables.\nWe show that this approach is prone to temporally getting stuck in local optima\ncorresponding to wrongly biased beliefs. We suggest instead maximizing the\nexpected cross entropy between old and new belief, which aims at challenging\nrefutable beliefs and thereby avoids these local optima. We show that both\ncriteria are closely related and that their difference can be traced back to\nthe asymmetry of the Kullback-Leibler divergence. In illustrative examples as\nwell as simulated and real-world experiments we demonstrate the advantage of\ncross entropy over simple entropy for practical applications. \n\n"}
{"id": "1409.8565", "contents": "Title: Sparse CCA: Adaptive Estimation and Computational Barriers Abstract: Canonical correlation analysis is a classical technique for exploring the\nrelationship between two sets of variables. It has important applications in\nanalyzing high dimensional datasets originated from genomics, imaging and other\nfields. This paper considers adaptive minimax and computationally tractable\nestimation of leading sparse canonical coefficient vectors in high dimensions.\nFirst, we establish separate minimax estimation rates for canonical coefficient\nvectors of each set of random variables under no structural assumption on\nmarginal covariance matrices. Second, we propose a computationally feasible\nestimator to attain the optimal rates adaptively under an additional sample\nsize condition. Finally, we show that a sample size condition of this kind is\nneeded for any randomized polynomial-time estimator to be consistent, assuming\nhardness of certain instances of the Planted Clique detection problem. The\nresult is faithful to the Gaussian models used in the paper. As a byproduct, we\nobtain the first computational lower bounds for sparse PCA under the Gaussian\nsingle spiked covariance model. \n\n"}
{"id": "1410.0247", "contents": "Title: A Practical Scheme and Fast Algorithm to Tune the Lasso With Optimality\n  Guarantees Abstract: We introduce a novel scheme for choosing the regularization parameter in\nhigh-dimensional linear regression with Lasso. This scheme, inspired by\nLepski's method for bandwidth selection in non-parametric regression, is\nequipped with both optimal finite-sample guarantees and a fast algorithm. In\nparticular, for any design matrix such that the Lasso has low sup-norm error\nunder an \"oracle choice\" of the regularization parameter, we show that our\nmethod matches the oracle performance up to a small constant factor, and show\nthat it can be implemented by performing simple tests along a single Lasso\npath. By applying the Lasso to simulated and real data, we find that our novel\nscheme can be faster and more accurate than standard schemes such as\nCross-Validation. \n\n"}
{"id": "1410.0255", "contents": "Title: Variance reduction for irreversible Langevin samplers and diffusion on\n  graphs Abstract: In recent papers it has been demonstrated that sampling a Gibbs distribution\nfrom an appropriate time-irreversible Langevin process is, from several points\nof view, advantageous when compared to sampling from a time-reversible one.\nAdding an appropriate irreversible drift to the overdamped Langevin equation\nresults in a larger large deviations rate function for the empirical measure of\nthe process, a smaller variance for the long time average of observables of the\nprocess, as well as a larger spectral gap. In this work, we concentrate on\nirreversible Langevin samplers with a drift of increasing intensity. The\nasymptotic variance is monotonically decreasing with respect to the growth of\nthe drift and we characterize its limiting behavior. For a Gibbs measure whose\npotential has one or more critical points, adding a large irreversible drift\nresults in a decomposition of the process in a slow and fast component with\nfast motion along the level sets of the potential and slow motion in the\northogonal direction. This result helps understanding the variance reduction,\nwhich can be explained at the process level by the induced fast motion of the\nprocess along the level sets of the potential. The limit of the asymptotic\nvariance as the magnitude of the irreversible perturbation grows is the\nasymptotic variance associated to the limiting slow motion. The latter is a\ndiffusion process on a graph. \n\n"}
{"id": "1410.0503", "contents": "Title: On Bayes Risk Lower Bounds Abstract: This paper provides a general technique for lower bounding the Bayes risk of\nstatistical estimation, applicable to arbitrary loss functions and arbitrary\nprior distributions. A lower bound on the Bayes risk not only serves as a lower\nbound on the minimax risk, but also characterizes the fundamental limit of any\nestimator given the prior knowledge. Our bounds are based on the notion of\n$f$-informativity, which is a function of the underlying class of probability\nmeasures and the prior. Application of our bounds requires upper bounds on the\n$f$-informativity, thus we derive new upper bounds on $f$-informativity which\noften lead to tight Bayes risk lower bounds. Our technique leads to\ngeneralizations of a variety of classical minimax bounds (e.g., generalized\nFano's inequality). Our Bayes risk lower bounds can be directly applied to\nseveral concrete estimation problems, including Gaussian location models,\ngeneralized linear models, and principal component analysis for spiked\ncovariance models. To further demonstrate the applications of our Bayes risk\nlower bounds to machine learning problems, we present two new theoretical\nresults: (1) a precise characterization of the minimax risk of learning\nspherical Gaussian mixture models under the smoothed analysis framework, and\n(2) lower bounds for the Bayes risk under a natural prior for both the\nprediction and estimation errors for high-dimensional sparse linear regression\nunder an improper learning setting. \n\n"}
{"id": "1410.1094", "contents": "Title: A higher-order LQ decomposition for separable covariance models Abstract: We develop a higher order generalization of the LQ decomposition and show\nthat this decomposition plays an important role in likelihood-based estimation\nand testing for separable, or Kronecker structured, covariance models, such as\nthe multilinear normal model. This role is analogous to that of the LQ\ndecomposition in likelihood inference for the multivariate normal model.\nAdditionally, this higher order LQ decomposition can be used to construct an\nalternative version of the popular higher order singular value decomposition\nfor tensor-valued data. We also develop a novel generalization of the polar\ndecomposition to tensor-valued data. \n\n"}
{"id": "1410.1931", "contents": "Title: Coherence Motivated Sampling and Convergence Analysis of Least-Squares\n  Polynomial Chaos Regression Abstract: Independent sampling of orthogonal polynomial bases via Monte Carlo is of\ninterest for uncertainty quantification of models, using Polynomial Chaos (PC)\nexpansions. It is known that bounding the spectral radius of a random matrix\nconsisting of PC samples, yields a bound on the number of samples necessary to\nidentify coefficients in the PC expansion via solution to a least-squares\nregression problem. We present a related analysis which guarantees a mean\nsquare convergence using a coherence parameter of the sampled PC basis that may\nbe both analytically bounded and computationally estimated. Utilizing\nasymptotic results for orthogonal polynomials, we bound the coherence parameter\nfor polynomials of Hermite and Legendre type under each respective natural\nsampling distribution. In both polynomial bases we identify an importance\nsampling distribution which yields a bound with weaker dependence on the order\nof the PC basis. For more general orthonormal bases, we propose the\ncoherence-optimal sampling: a Markov Chain Monte Carlo sampling, which directly\nuses the basis functions under consideration to achieve a statistical\noptimality among all such sampling schemes with identical support, and which\nguarantees recovery with a number of samples that is, up to logarithmic\nfactors, linear in the number of basis functions considered. We demonstrate\nthese different sampling strategies numerically in both high-order and\nhigh-dimensional manufactured PC expansions. In addition, the quality of each\nsampling method is compared in the identification of solutions to two\ndifferential equations, one with a high-dimensional random input and the other\nwith a high-order PC expansion. In all observed cases the coherence-optimal\nsampling leads to similar or considerably improved accuracy over the other\nconsidered sampling distributions. \n\n"}
{"id": "1410.2597", "contents": "Title: Optimal Inference After Model Selection Abstract: To perform inference after model selection, we propose controlling the\nselective type I error; i.e., the error rate of a test given that it was\nperformed. By doing so, we recover long-run frequency properties among selected\nhypotheses analogous to those that apply in the classical (non-adaptive)\ncontext. Our proposal is closely related to data splitting and has a similar\nintuitive justification, but is more powerful. Exploiting the classical theory\nof Lehmann and Scheff\\'e (1955), we derive most powerful unbiased selective\ntests and confidence intervals for inference in exponential family models after\narbitrary selection procedures. For linear regression, we derive new selective\nz-tests that generalize recent proposals for inference after model selection\nand improve on their power, and new selective t-tests that do not require\nknowledge of the error variance. \n\n"}
{"id": "1410.3544", "contents": "Title: The space of ultrametric phylogenetic trees Abstract: The reliability of a phylogenetic inference method from genomic sequence data\nis ensured by its statistical consistency. Bayesian inference methods produce a\nsample of phylogenetic trees from the posterior distribution given sequence\ndata. Hence the question of statistical consistency of such methods is\nequivalent to the consistency of the summary of the sample. More generally,\nstatistical consistency is ensured by the tree space used to analyse the\nsample.\n  In this paper, we consider two standard parameterisations of phylogenetic\ntime-trees used in evolutionary models: inter-coalescent interval lengths and\nabsolute times of divergence events. For each of these parameterisations we\nintroduce a natural metric space on ultrametric phylogenetic trees. We compare\nthe introduced spaces with existing models of tree space and formulate several\nformal requirements that a metric space on phylogenetic trees must possess in\norder to be a satisfactory space for statistical analysis, and justify them. We\nshow that only a few known constructions of the space of phylogenetic trees\nsatisfy these requirements. However, our results suggest that these basic\nrequirements are not enough to distinguish between the two metric spaces we\nintroduce and that the choice between metric spaces requires additional\nproperties to be considered. Particularly, that the summary tree minimising the\nsquare distance to the trees from the sample might be different for different\nparameterisations. This suggests that further fundamental insight is needed\ninto the problem of statistical consistency of phylogenetic inference methods. \n\n"}
{"id": "1410.4179", "contents": "Title: Distribution-Free Tests of Independence in High Dimensions Abstract: We consider the testing of mutual independence among all entries in a\n$d$-dimensional random vector based on $n$ independent observations. We study\ntwo families of distribution-free test statistics, which include Kendall's tau\nand Spearman's rho as important examples. We show that under the null\nhypothesis the test statistics of these two families converge weakly to Gumbel\ndistributions, and propose tests that control the type I error in the\nhigh-dimensional setting where $d>n$. We further show that the two tests are\nrate-optimal in terms of power against sparse alternatives, and outperform\ncompetitors in simulations, especially when $d$ is large. \n\n"}
{"id": "1410.4719", "contents": "Title: Limiting Statistics of the Largest and Smallest Eigenvalues in the\n  Correlated Wishart Model Abstract: The correlated Wishart model provides a standard tool for the analysis of\ncorrelations in a rich variety of systems. Although much is known for complex\ncorrelation matrices, the empirically much more important real case still poses\nsubstantial challenges. We put forward a new approach, which maps arbitrary\nstatistical quantities, depending on invariants only, to invariant Hermitian\nmatrix models. For completeness we also include the quaternion case and deal\nwith all three cases in a unified way. As an important application, we study\nthe statistics of the largest eigenvalue and its limiting distributions in the\ncorrelated Wishart model, because they help to estimate the behavior of large\ncomplex systems. We show that even for fully correlated Wishart ensembles, the\nTracy-Widom distribution can be the limiting distribution of the largest as\nwell as the smallest eigenvalue, provided that a certain scaling of the\nempirical eigenvalues holds. \n\n"}
{"id": "1410.4743", "contents": "Title: Higher Criticism for Large-Scale Inference, Especially for Rare and Weak\n  Effects Abstract: In modern high-throughput data analysis, researchers perform a large number\nof statistical tests, expecting to find perhaps a small fraction of significant\neffects against a predominantly null background. Higher Criticism (HC) was\nintroduced to determine whether there are any nonzero effects; more recently,\nit was applied to feature selection, where it provides a method for selecting\nuseful predictive features from a large body of potentially useful features,\namong which only a rare few will prove truly useful. In this article, we review\nthe basics of HC in both the testing and feature selection settings. HC is a\nflexible idea, which adapts easily to new situations; we point out simple\nadaptions to clique detection and bivariate outlier detection. HC, although\nstill early in its development, is seeing increasing interest from\npractitioners; we illustrate this with worked examples. HC is computationally\neffective, which gives it a nice leverage in the increasingly more relevant\n\"Big Data\" settings we see today. We also review the underlying theoretical\n\"ideology\" behind HC. The Rare/Weak (RW) model is a theoretical framework\nsimultaneously controlling the size and prevalence of useful/significant items\namong the useless/null bulk. The RW model shows that HC has important\nadvantages over better known procedures such as False Discovery Rate (FDR)\ncontrol and Family-wise Error control (FwER), in particular, certain optimality\nproperties. We discuss the rare/weak phase diagram, a way to visualize clearly\nthe class of RW settings where the true signals are so rare or so weak that\ndetection and feature selection are simply impossible, and a way to understand\nthe known optimality properties of HC. \n\n"}
{"id": "1410.5014", "contents": "Title: Optimal Two-Step Prediction in Regression Abstract: High-dimensional prediction typically comprises two steps: variable selection\nand subsequent least-squares refitting on the selected variables. However, the\nstandard variable selection procedures, such as the lasso, hinge on tuning\nparameters that need to be calibrated. Cross-validation, the most popular\ncalibration scheme, is computationally costly and lacks finite sample\nguarantees. In this paper, we introduce an alternative scheme, easy to\nimplement and both computationally and theoretically efficient. \n\n"}
{"id": "1410.5550", "contents": "Title: Minimization Problems Based on Relative $\\alpha$-Entropy II: Reverse\n  Projection Abstract: In part I of this two-part work, certain minimization problems based on a\nparametric family of relative entropies (denoted $\\mathscr{I}_{\\alpha}$) were\nstudied. Such minimizers were called forward\n$\\mathscr{I}_{\\alpha}$-projections. Here, a complementary class of minimization\nproblems leading to the so-called reverse $\\mathscr{I}_{\\alpha}$-projections\nare studied. Reverse $\\mathscr{I}_{\\alpha}$-projections, particularly on\nlog-convex or power-law families, are of interest in robust estimation problems\n($\\alpha >1$) and in constrained compression settings ($\\alpha <1$).\nOrthogonality of the power-law family with an associated linear family is first\nestablished and is then exploited to turn a reverse\n$\\mathscr{I}_{\\alpha}$-projection into a forward\n$\\mathscr{I}_{\\alpha}$-projection. The transformed problem is a simpler\nquasiconvex minimization subject to linear constraints. \n\n"}
{"id": "1410.7165", "contents": "Title: Exact Inference on Gaussian Graphical Models of Arbitrary Topology using\n  Path-Sums Abstract: We present the path-sum formulation for exact statistical inference of\nmarginals on Gaussian graphical models of arbitrary topology. The path-sum\nformulation gives the covariance between each pair of variables as a branched\ncontinued fraction of finite depth and breadth. Our method originates from the\nclosed-form resummation of infinite families of terms of the walk-sum\nrepresentation of the covariance matrix. We prove that the path-sum formulation\nalways exists for models whose covariance matrix is positive definite: i.e.~it\nis valid for both walk-summable and non-walk-summable graphical models of\narbitrary topology. We show that for graphical models on trees the path-sum\nformulation is equivalent to Gaussian belief propagation. We also recover, as a\ncorollary, an existing result that uses determinants to calculate the\ncovariance matrix. We show that the path-sum formulation formulation is valid\nfor arbitrary partitions of the inverse covariance matrix. We give detailed\nexamples demonstrating our results. \n\n"}
{"id": "1410.7600", "contents": "Title: Discussion of \"Frequentist coverage of adaptive nonparametric Bayesian\n  credible sets\" Abstract: Discussion of \"Frequentist coverage of adaptive nonparametric Bayesian\ncredible sets\" by Szab\\'o, van der Vaart and van Zanten [arXiv:1310.4489v5]. \n\n"}
{"id": "1410.7855", "contents": "Title: A New Family of Fractional Renewal Processes Abstract: Fractional renewal processes as a generalization of Poisson process are\nalready in the literature. In this paper, by introducing a new concept of\ngeneralized density function, the authors construct new fractional renewal\nprocesses in the $\\alpha$-fractional space and show that it is another\ninteresting and useful generalization of Poisson process. \n\n"}
{"id": "1410.8570", "contents": "Title: A Partially Linear Framework for Massive Heterogeneous Data Abstract: We consider a partially linear framework for modelling massive heterogeneous\ndata. The major goal is to extract common features across all sub-populations\nwhile exploring heterogeneity of each sub-population. In particular, we propose\nan aggregation type estimator for the commonality parameter that possesses the\n(non-asymptotic) minimax optimal bound and asymptotic distribution as if there\nwere no heterogeneity. This oracular result holds when the number of\nsub-populations does not grow too fast. A plug-in estimator for the\nheterogeneity parameter is further constructed, and shown to possess the\nasymptotic distribution as if the commonality information were available. We\nalso test the heterogeneity among a large number of sub-populations. All the\nabove results require to regularize each sub-estimation as though it had the\nentire sample size. Our general theory applies to the divide-and-conquer\napproach that is often used to deal with massive homogeneous data. A technical\nby-product of this paper is the statistical inferences for the general kernel\nridge regression. Thorough numerical results are also provided to back up our\ntheory. \n\n"}
{"id": "1411.0900", "contents": "Title: Kernel Mean Estimation via Spectral Filtering Abstract: The problem of estimating the kernel mean in a reproducing kernel Hilbert\nspace (RKHS) is central to kernel methods in that it is used by classical\napproaches (e.g., when centering a kernel PCA matrix), and it also forms the\ncore inference step of modern kernel methods (e.g., kernel-based non-parametric\ntests) that rely on embedding probability distributions in RKHSs. Muandet et\nal. (2014) has shown that shrinkage can help in constructing \"better\"\nestimators of the kernel mean than the empirical estimator. The present paper\nstudies the consistency and admissibility of the estimators in Muandet et al.\n(2014), and proposes a wider class of shrinkage estimators that improve upon\nthe empirical estimator by considering appropriate basis functions. Using the\nkernel PCA basis, we show that some of these estimators can be constructed\nusing spectral filtering algorithms which are shown to be consistent under some\ntechnical assumptions. Our theoretical analysis also reveals a fundamental\nconnection to the kernel-based supervised learning framework. The proposed\nestimators are simple to implement and perform well in practice. \n\n"}
{"id": "1411.3800", "contents": "Title: A Sharp First Order Analysis of Feynman-Kac Particle Models Abstract: This article provides a new theory for the analysis of forward and backward\nparticle approximations of Feynman-Kac models. Such formulae are found in a\nwide variety of applications and their numerical (particle) approximation are\nrequired due to their intractability. Under mild assumptions, we provide sharp\nand non-asymptotic first order expansions of these particle methods,\npotentially on path space and for possibly unbounded functions. These\nexpansions allows one to consider upper and lower bound bias type estimates for\na given time horizon $n$ and particle number $N$; these non-asymptotic\nestimates are of order $\\mathcal{O}(n/N)$. Our approach is extended to tensor\nproducts of particle density profiles, leading to new sharp and non-asymptotic\npropagation of chaos estimates. The resulting upper and lower bound propagation\nof chaos estimates seems to be the first result of this kind for mean field\nparticle models. As a by-product of our results, we also provide some analysis\nof the particle Gibbs sampler, providing first order expansions of the kernel\nand minorization estimates. \n\n"}
{"id": "1411.3875", "contents": "Title: Local Asymptotic Normality of the spectrum of high-dimensional spiked\n  F-ratios Abstract: We consider two types of spiked multivariate F distributions: a scaled\ndistribution with the scale matrix equal to a rank-one perturbation of the\nidentity, and a distribution with trivial scale, but rank-one non-centrality.\nThe norm of the rank-one matrix (spike) parameterizes the joint distribution of\nthe eigenvalues of the corresponding F matrix. We show that, for a spike\nlocated above a phase transition threshold, the asymptotic behavior of the log\nratio of the joint density of the eigenvalues of the F matrix to their joint\ndensity under a local deviation from this value depends only on the largest\neigenvalue $\\lambda_{1}$. Furthermore, $\\lambda_{1}$ is asymptotically normal,\nand the statistical experiment of observing all the eigenvalues of the F matrix\nconverges in the Le Cam sense to a Gaussian shift experiment that depends on\nthe asymptotic mean and variance of $\\lambda_{1}$. In particular, the best\nstatistical inference about a sufficiently large spike in the local asymptotic\nregime is based on the largest eigenvalue only. As a by-product of our\nanalysis, we establish joint asymptotic normality of a few of the largest\neigenvalues of the multi-spiked F matrix when the corresponding spikes are\nabove the phase transition threshold. \n\n"}
{"id": "1411.7650", "contents": "Title: Nonparametric statistical inference for the context tree of a stationary\n  ergodic process Abstract: We consider the problem of estimating the context tree of a stationary\nergodic process with finite alphabet without imposing additional conditions on\nthe process. As a starting point we introduce a Hamming metric in the space of\nirreducible context trees and we use the properties of the weak topology in the\nspace of ergodic stationary processes to prove that if the Hamming metric is\nunbounded, there exist no consistent estimators for the context tree. Even in\nthe bounded case we show that there exist no two-sided confidence bounds.\nHowever we prove that one-sided inference is possible in this general setting\nand we construct a consistent estimator that is a lower bound for the context\ntree of the process with an explicit formula for the coverage probability. We\ndevelop an efficient algorithm to compute the lower bound and we apply the\nmethod to test a linguistic hypothesis about the context tree of codified\nwritten texts in European Portuguese. \n\n"}
{"id": "1412.0446", "contents": "Title: Sequential block bootstrap in a Hilbert space with application to change\n  point analysis Abstract: A new test for structural changes in functional data is investigated. It is\nbased on Hilbert space theory and critical values are deduced from bootstrap\niterations. Thus a new functional central limit theorem for the block bootstrap\nin a Hilbert space is required. The test can also be used to detect changes in\nthe marginal distribution of random vectors, which is supplemented by a\nsimulation study. Our methods are applied to hydrological data from Germany. \n\n"}
{"id": "1412.0838", "contents": "Title: Semi-parametric modeling of excesses above high multivariate thresholds\n  with censored data Abstract: How to include censored data in a statistical analysis is a recur-rent issue\nin statistics. In multivariate extremes, the dependence structure of large\nobservations can be characterized in terms of a non parametric angular measure,\nwhile marginal excesses above asymptotically large thresholds have a parametric\ndistribution. In this work, a flexible semi-parametric Dirichlet mix-ture model\nfor angular measures is adapted to the context of censored data and missing\ncomponents. One major issue is to take into account censoring intervals\noverlapping the extremal threshold, without knowing whether the correspond-ing\nhidden data is actually extreme. Further, the censored likelihood needed for\nBayesian inference has no analytic expression. The first issue is tackled using\na Poisson process model for extremes, whereas a data augmentation scheme avoids\nmultivariate integration of the Poisson process intensity over both the\ncensored intervals and the failure region above threshold. The implemented MCMC\nalgorithm allows simultaneous estimation of marginal and dependence parameters,\nso that all sources of uncertainty other than model bias are cap-tured by\nposterior credible intervals. The method is illustrated on simulated and real\ndata. \n\n"}
{"id": "1412.1716", "contents": "Title: Nonparametric modal regression Abstract: Modal regression estimates the local modes of the distribution of $Y$ given\n$X=x$, instead of the mean, as in the usual regression sense, and can hence\nreveal important structure missed by usual regression methods. We study a\nsimple nonparametric method for modal regression, based on a kernel density\nestimate (KDE) of the joint distribution of $Y$ and $X$. We derive asymptotic\nerror bounds for this method, and propose techniques for constructing\nconfidence sets and prediction sets. The latter is used to select the smoothing\nbandwidth of the underlying KDE. The idea behind modal regression is connected\nto many others, such as mixture regression and density ridge estimation, and we\ndiscuss these ties as well. \n\n"}
{"id": "1412.3092", "contents": "Title: Eigenvalue Density of the Doubly Correlated Wishart Model: Exact Results Abstract: Data sets collected at different times and different observing points can\npossess correlations at different times $and$ at different positions. The\ndoubly correlated Wishart model takes both into account. We calculate the\neigenvalue density of the Wishart correlation matrices using supersymmetry. In\nthe complex case we obtain a new closed form expression which we compare to\nprevious results in the literature. In the more relevant and much more\ncomplicated real case we derive an expression for the density in terms of a\nfourfold integral. Finally, we calculate the density in the limit of large\ncorrelation matrices. \n\n"}
{"id": "1412.4056", "contents": "Title: Blind system identification using kernel-based methods Abstract: We propose a new method for blind system identification. Resorting to a\nGaussian regression framework, we model the impulse response of the unknown\nlinear system as a realization of a Gaussian process. The structure of the\ncovariance matrix (or kernel) of such a process is given by the stable spline\nkernel, which has been recently introduced for system identification purposes\nand depends on an unknown hyperparameter. We assume that the input can be\nlinearly described by few parameters. We estimate these parameters, together\nwith the kernel hyperparameter and the noise variance, using an empirical Bayes\napproach. The related optimization problem is efficiently solved with a novel\niterative scheme based on the Expectation-Maximization method. In particular,\nwe show that each iteration consists of a set of simple update rules. We show,\nthrough some numerical experiments, very promising performance of the proposed\nmethod. \n\n"}
{"id": "1412.8285", "contents": "Title: Marginal likelihood and model selection for Gaussian latent tree and\n  forest models Abstract: Gaussian latent tree models, or more generally, Gaussian latent forest models\nhave Fisher-information matrices that become singular along interesting\nsubmodels, namely, models that correspond to subforests. For these\nsingularities, we compute the real log-canonical thresholds (also known as\nstochastic complexities or learning coefficients) that quantify the\nlarge-sample behavior of the marginal likelihood in Bayesian inference. This\nprovides the information needed for a recently introduced generalization of the\nBayesian information criterion. Our mathematical developments treat the general\nsetting of Laplace integrals whose phase functions are sums of squared\ndifferences between monomials and constants. We clarify how in this case real\nlog-canonical thresholds can be computed using polyhedral geometry, and we show\nhow to apply the general theory to the Laplace integrals associated with\nGaussian latent tree and forest models. In simulations and a data example, we\ndemonstrate how the mathematical knowledge can be applied in model selection. \n\n"}
{"id": "1412.8340", "contents": "Title: On the Smallest Eigenvalue of General correlated Gaussian Matrices Abstract: This paper investigates the behaviour of the spectrum of generally correlated\nGaussian random matrices whose columns are zero-mean independent vectors but\nhave different correlations, under the specific regime where the number of\ntheir columns and that of their rows grow at infinity with the same pace. This\nwork is, in particular, motivated by applications from statistical signal\nprocessing and wireless communications, where this kind of matrices naturally\narise. Following the approach proposed in [1], we prove that under some\nspecific conditions, the smallest singular value of generally correlated\nGaussian matrices is almost surely away from zero. \n\n"}
{"id": "1412.8690", "contents": "Title: Breaking the Curse of Dimensionality with Convex Neural Networks Abstract: We consider neural networks with a single hidden layer and non-decreasing\nhomogeneous activa-tion functions like the rectified linear units. By letting\nthe number of hidden units grow unbounded and using classical non-Euclidean\nregularization tools on the output weights, we provide a detailed theoretical\nanalysis of their generalization performance, with a study of both the\napproximation and the estimation errors. We show in particular that they are\nadaptive to unknown underlying linear structures, such as the dependence on the\nprojection of the input variables onto a low-dimensional subspace. Moreover,\nwhen using sparsity-inducing norms on the input weights, we show that\nhigh-dimensional non-linear variable selection may be achieved, without any\nstrong assumption regarding the data and with a total number of variables\npotentially exponential in the number of ob-servations. In addition, we provide\na simple geometric interpretation to the non-convex problem of addition of a\nnew unit, which is the core potentially hard computational element in the\nframework of learning from continuously many basis functions. We provide simple\nconditions for convex relaxations to achieve the same generalization error\nbounds, even when constant-factor approxi-mations cannot be found (e.g.,\nbecause it is NP-hard such as for the zero-homogeneous activation function). We\nwere not able to find strong enough convex relaxations and leave open the\nexistence or non-existence of polynomial-time algorithms. \n\n"}
{"id": "1412.8697", "contents": "Title: On Semiparametric Exponential Family Graphical Models Abstract: We propose a new class of semiparametric exponential family graphical models\nfor the analysis of high dimensional mixed data. Different from the existing\nmixed graphical models, we allow the nodewise conditional distributions to be\nsemiparametric generalized linear models with unspecified base measure\nfunctions. Thus, one advantage of our method is that it is unnecessary to\nspecify the type of each node and the method is more convenient to apply in\npractice. Under the proposed model, we consider both problems of parameter\nestimation and hypothesis testing in high dimensions. In particular, we propose\na symmetric pairwise score test for the presence of a single edge in the graph.\nCompared to the existing methods for hypothesis tests, our approach takes into\naccount of the symmetry of the parameters, such that the inferential results\nare invariant with respect to the different parametrizations of the same edge.\nThorough numerical simulations and a real data example are provided to back up\nour results. \n\n"}
{"id": "1501.00438", "contents": "Title: (Non-) asymptotic properties of Stochastic Gradient Langevin Dynamics Abstract: Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data\nsets is computationally infeasible. The recently proposed stochastic gradient\nLangevin dynamics (SGLD) method circumvents this problem in three ways: it\ngenerates proposed moves using only a subset of the data, it skips the\nMetropolis-Hastings accept-reject step, and it uses sequences of decreasing\nstep sizes. In \\cite{TehThierryVollmerSGLD2014}, we provided the mathematical\nfoundations for the decreasing step size SGLD, including consistency and a\ncentral limit theorem. However, in practice the SGLD is run for a relatively\nsmall number of iterations, and its step size is not decreased to zero. The\npresent article investigates the behaviour of the SGLD with fixed step size. In\nparticular we characterise the asymptotic bias explicitly, along with its\ndependence on the step size and the variance of the stochastic gradient. On\nthat basis a modified SGLD which removes the asymptotic bias due to the\nvariance of the stochastic gradients up to first order in the step size is\nderived. Moreover, we are able to obtain bounds on the finite-time bias,\nvariance and mean squared error (MSE). The theory is illustrated with a\nGaussian toy model for which the bias and the MSE for the estimation of moments\ncan be obtained explicitly. For this toy model we study the gain of the SGLD\nover the standard Euler method in the limit of large data sets. \n\n"}
{"id": "1501.01732", "contents": "Title: Testing independence in high dimensions with sums of rank correlations Abstract: We treat the problem of testing independence between m continuous variables\nwhen m can be larger than the available sample size n. We consider three types\nof test statistics that are constructed as sums or sums of squares of pairwise\nrank correlations. In the asymptotic regime where both m and n tend to\ninfinity, a martingale central limit theorem is applied to show that the null\ndistributions of these statistics converge to Gaussian limits, which are valid\nwith no specific distributional or moment assumptions on the data. Using the\nframework of U-statistics, our result covers a variety of rank correlations\nincluding Kendall's tau and a dominating term of Spearman's rank correlation\ncoefficient (rho), but also degenerate U-statistics such as Hoeffding's $D$, or\nthe $\\tau^*$ of Bergsma and Dassios (2014). As in the classical theory for\nU-statistics, the test statistics need to be scaled differently when the rank\ncorrelations used to construct them are degenerate U-statistics. The power of\nthe considered tests is explored in rate-optimality theory under Gaussian\nequicorrelation alternatives as well as in numerical experiments for specific\ncases of more general alternatives. \n\n"}
{"id": "1501.01732", "contents": "Title: Testing independence in high dimensions with sums of rank correlations Abstract: We treat the problem of testing independence between m continuous variables\nwhen m can be larger than the available sample size n. We consider three types\nof test statistics that are constructed as sums or sums of squares of pairwise\nrank correlations. In the asymptotic regime where both m and n tend to\ninfinity, a martingale central limit theorem is applied to show that the null\ndistributions of these statistics converge to Gaussian limits, which are valid\nwith no specific distributional or moment assumptions on the data. Using the\nframework of U-statistics, our result covers a variety of rank correlations\nincluding Kendall's tau and a dominating term of Spearman's rank correlation\ncoefficient (rho), but also degenerate U-statistics such as Hoeffding's $D$, or\nthe $\\tau^*$ of Bergsma and Dassios (2014). As in the classical theory for\nU-statistics, the test statistics need to be scaled differently when the rank\ncorrelations used to construct them are degenerate U-statistics. The power of\nthe considered tests is explored in rate-optimality theory under Gaussian\nequicorrelation alternatives as well as in numerical experiments for specific\ncases of more general alternatives. \n\n"}
{"id": "1501.01840", "contents": "Title: Gibbs posterior inference on the minimum clinically important difference Abstract: IIt is known that a statistically significant treatment may not be clinically\nsignificant. A quantity that can be used to assess clinical significance is\ncalled the minimum clinically important difference (MCID), and inference on the\nMCID is an important and challenging problem. Modeling for the purpose of\ninference on the MCID is non-trivial, and concerns about bias from a\nmisspecified parametric model or inefficiency from a nonparametric model\nmotivate an alternative approach to balance robustness and efficiency. In\nparticular, a recently proposed representation of the MCID as the minimizer of\na suitable risk function makes it possible to construct a Gibbs posterior\ndistribution for the MCID without specifying a model. We establish the\nposterior convergence rate and show, numerically, that an appropriately scaled\nversion of this Gibbs posterior yields interval estimates for the MCID which\nare both valid and efficient even for relatively small sample sizes. \n\n"}
{"id": "1501.02103", "contents": "Title: Margins of discrete Bayesian networks Abstract: Bayesian network models with latent variables are widely used in statistics\nand machine learning. In this paper we provide a complete algebraic\ncharacterization of Bayesian network models with latent variables when the\nobserved variables are discrete and no assumption is made about the state-space\nof the latent variables. We show that it is algebraically equivalent to the\nso-called nested Markov model, meaning that the two are the same up to\ninequality constraints on the joint probabilities. In particular these two\nmodels have the same dimension. The nested Markov model is therefore the best\npossible description of the latent variable model that avoids consideration of\ninequalities, which are extremely complicated in general. A consequence of this\nis that the constraint finding algorithm of Tian and Pearl (UAI 2002,\npp519-527) is complete for finding equality constraints.\n  Latent variable models suffer from difficulties of unidentifiable parameters\nand non-regular asymptotics; in contrast the nested Markov model is fully\nidentifiable, represents a curved exponential family of known dimension, and\ncan easily be fitted using an explicit parameterization. \n\n"}
{"id": "1501.03588", "contents": "Title: Asymptotics of selective inference Abstract: In this paper, we seek to establish asymptotic results for selective\ninference procedures removing the assumption of Gaussianity. The class of\nselection procedures we consider are determined by affine inequalities, which\nwe refer to as affine selection procedures. Examples of affine selection\nprocedures include post-selection inference along the solution path of the\nLASSO, as well as post-selection inference after fitting the LASSO at a fixed\nvalue of the regularization parameter. We also consider some tests in penalized\ngeneralized linear models. Our method of proof adapts a method of Chatterjee\n(2005). \n\n"}
{"id": "1501.04970", "contents": "Title: Parameter estimation for SDEs related to stationary Gaussian processes Abstract: In this paper, we study central and non-central limit theorems for partial\nsum of functionals of general stationary Gaussian fields. We apply our result\nto study drift parameter estimation problems for some stochastic differential\nequations related to stationary Gaussian processes. \n\n"}
{"id": "1501.04972", "contents": "Title: Parameter Estimation for a partially observed Ornstein-Uhlenbeck process\n  with long-memory noise Abstract: \\noindent \\textbf{Abstract}: We consider the parameter estimation problem for\nthe Ornstein-Uhlenbeck process $X$ driven by a fractional Ornstein-Uhlenbeck\nprocess $V$, i.e. the pair of processes defined by the non-Markovian\ncontinuous-time long-memory dynamics $dX_{t}=-\\theta X_{t}dt+dV_{t};\\ t\\geq 0$,\nwith $dV_{t}=-\\rho V_{t}dt+dB_{t}^{H};\\ t\\geq 0$, where $\\theta >0$ and $\\rho\n>0$ are unknown parameters, and $B^{H}$ is a fractional Brownian motion of\nHurst index $H\\in (\\frac{1}{2},1)$. We study the strong consistency as well as\nthe asymptotic normality of the joint least squares estimator\n$(\\hat{\\theta}_{T},\\widehat{\\rho }% _{T}) $ of the pair $( \\theta ,\\rho) $,\nbased either on continuous or discrete observations of $\\{X_{s};\\ s\\in \\lbrack\n0,T]\\}$ as the horizon $T$ increases to +$\\infty $. Both cases qualify formally\nas partial-hbobservation questions since $V$ is unobserved. In the latter case,\nseveral discretization options are considered. Our proofs of asymptotic\nnormality based on discrete data, rely on increasingly strict restrictions on\nthe sampling frequency as one reduces the extent of sources of observation. The\nstrategy for proving the asymptotic properties is to study the case of\ncontinuous-time observations using the Malliavin calculus, and then to exploit\nthe fact that each discrete-data estimator can be considered as a perturbation\nof the continuous one in a mathematically precise way, despite the fact that\nthe implementation of the discrete-time estimators is distant from the\ncontinuous estimator. In this sense, we contend that the continuous-time\nestimator cannot be implemented in practice in any na\\\"ive way, and serves only\nas a mathematical tool in the study of the discrete-time estimators'\nasymptotics. \n\n"}
{"id": "1501.06094", "contents": "Title: Wavelet estimation for operator fractional Brownian motion Abstract: Operator fractional Brownian motion (OFBM) is the natural vector-valued\nextension of the univariate fractional Brownian motion. Instead of a scalar\nparameter, the law of an OFBM scales according to a Hurst matrix that affects\nevery component of the process. In this paper, we develop the wavelet analysis\nof OFBM, as well as a new estimator for the Hurst matrix of bivariate OFBM. For\nOFBM, the univariate-inspired approach of analyzing the entry-wise behavior of\nthe wavelet spectrum as a function of the (wavelet) scales is fraught with\ndifficulties stemming from mixtures of power laws. The proposed approach\nconsists of considering the evolution along scales of the eigenstructure of the\nwavelet spectrum. This is shown to yield consistent and asymptotically normal\nestimators of the Hurst eigenvalues, and also of the coordinate system itself\nunder assumptions. A simulation study is included to demonstrate the good\nperformance of the estimators under finite sample sizes. \n\n"}
{"id": "1501.06202", "contents": "Title: Robust Subjective Visual Property Prediction from Crowdsourced Pairwise\n  Labels Abstract: The problem of estimating subjective visual properties from image and video\nhas attracted increasing interest. A subjective visual property is useful\neither on its own (e.g. image and video interestingness) or as an intermediate\nrepresentation for visual recognition (e.g. a relative attribute). Due to its\nambiguous nature, annotating the value of a subjective visual property for\nlearning a prediction model is challenging. To make the annotation more\nreliable, recent studies employ crowdsourcing tools to collect pairwise\ncomparison labels because human annotators are much better at ranking two\nimages/videos (e.g. which one is more interesting) than giving an absolute\nvalue to each of them separately. However, using crowdsourced data also\nintroduces outliers. Existing methods rely on majority voting to prune the\nannotation outliers/errors. They thus require large amount of pairwise labels\nto be collected. More importantly as a local outlier detection method, majority\nvoting is ineffective in identifying outliers that can cause global ranking\ninconsistencies. In this paper, we propose a more principled way to identify\nannotation outliers by formulating the subjective visual property prediction\ntask as a unified robust learning to rank problem, tackling both the outlier\ndetection and learning to rank jointly. Differing from existing methods, the\nproposed method integrates local pairwise comparison labels together to\nminimise a cost that corresponds to global inconsistency of ranking order. This\nnot only leads to better detection of annotation outliers but also enables\nlearning with extremely sparse annotations. Extensive experiments on various\nbenchmark datasets demonstrate that our new approach significantly outperforms\nstate-of-the-arts alternatives. \n\n"}
{"id": "1502.00095", "contents": "Title: A nonlinear model for long memory conditional heteroscedasticity Abstract: We discuss a class of conditionally heteroscedastic time series models\nsatisfying the equation $r_t= \\zeta_t \\sigma_t$, where $\\zeta_t$ are\nstandardized i.i.d. r.v.'s and the conditional standard deviation $\\sigma_t$ is\na nonlinear function $Q$ of inhomogeneous linear combination of past values\n$r_s, s<t$ with coefficients $b_j$. The existence of stationary solution $r_t$\nwith finite $p$th moment, $0< p < \\infty $ is obtained under some conditions on\n$Q, b_j$ and $p$th moment of $\\zeta_0$. Weak dependence properties of $r_t$ are\nstudied, including the invariance principle for partial sums of Lipschitz\nfunctions of $r_t$. In the case of quadratic $Q^2$, we prove that $r_t$ can\nexhibit a leverage effect and long memory, in the sense that the squared\nprocess $r^2_t$ has long memory autocorrelation and its normalized partial sums\nprocess converges to a fractional Brownian motion. \n\n"}
{"id": "1502.00140", "contents": "Title: Regression version of the Matsumoto-Yor type characterization of the\n  gamma and Kummer distributions Abstract: In this paper we study a Matsumoto-Yor type property for the gamma and Kummer\ninde- pendent variables discovered in Koudou and Vallois (2012). We prove that\nconstancy of regressions of U = (1 + 1/(X + Y ))=(1 + 1/X) given V = X + Y and\nof 1/U given V , where X and Y are indepen- dent and positive random variables,\ncharacterizes the gamma and Kummer distributions. This result completes\ncharacterizations by independence of U and V obtained, under smoothness\nassumptions for densities, in Koudou and Vallois (2011, 2012). Since we work\nwith differential equations for the Laplace transforms, no density assumptions\nare needed. \n\n"}
{"id": "1502.00235", "contents": "Title: Exact sampling algorithms for Latin squares and Sudoku matrices via\n  probabilistic divide-and-conquer Abstract: We provide several algorithms for the exact, uniform random sampling of Latin\nsquares and Sudoku matrices via probabilistic divide-and-conquer (PDC). Our\napproach divides the sample space into smaller pieces, samples each separately,\nand combines them in a manner which yields an exact sample from the target\ndistribution. We demonstrate, in particular, a version of PDC in which one of\nthe pieces is sampled using a brute force approach, which we dub\n$\\textit{almost deterministic second half}$, as it is a generalization to a\nprevious application of PDC for which one of the pieces is uniquely determined\ngiven the others. \n\n"}
{"id": "1502.00352", "contents": "Title: Empirical and multiplier bootstraps for suprema of empirical processes\n  of increasing complexity, and related Gaussian couplings Abstract: We derive strong approximations to the supremum of the non-centered empirical\nprocess indexed by a possibly unbounded VC-type class of functions by the\nsuprema of the Gaussian and bootstrap processes. The bounds of these\napproximations are non-asymptotic, which allows us to work with classes of\nfunctions whose complexity increases with the sample size. The construction of\ncouplings is not of the Hungarian type and is instead based on the\nSlepian-Stein methods and Gaussian comparison inequalities. The increasing\ncomplexity of classes of functions and non-centrality of the processes make the\nresults useful for applications in modern nonparametric statistics (Gin\\'{e}\nand Nickl, 2015), in particular allowing us to study the power properties of\nnonparametric tests using Gaussian and bootstrap approximations. \n\n"}
{"id": "1502.02323", "contents": "Title: Markov chain Monte Carlo methods for the Box-Behnken designs and\n  centrally symmetric configurations Abstract: We consider Markov chain Monte Carlo methods for calculating conditional p\nvalues of statistical models for count data arising in Box-Behnken designs. The\nstatistical model we consider is a discrete version of the first-order model in\nthe response surface methodology. For our models, the Markov basis, a key\nnotion to construct a connected Markov chain on a given sample space, is\ncharacterized as generators of the toric ideals for the centrally symmetric\nconfigurations of root system D_n. We show the structure of the Groebner bases\nfor these cases. A numerical example for an imaginary data set is given. \n\n"}
{"id": "1502.02336", "contents": "Title: Posterior contraction in Gaussian process regression using Wasserstein\n  approximations Abstract: We study posterior rates of contraction in Gaussian process regression with\nunbounded covariate domain. Our argument relies on developing a Gaussian\napproximation to the posterior of the leading coefficients of a\nKarhunen--Lo\\'{e}ve expansion of the Gaussian process. The salient feature of\nour result is deriving such an approximation in the $L^2$ Wasserstein distance\nand relating the speed of the approximation to the posterior contraction rate\nusing a coupling argument. Specific illustrations are provided for the Gaussian\nor squared-exponential covariance kernel. \n\n"}
{"id": "1502.02501", "contents": "Title: A CLT for an improved subspace estimator with observations of increasing\n  dimensions Abstract: This paper deals with subspace estimation in the small sample size regime,\nwhere the number of samples is comparable in magnitude with the observation\ndimension. The traditional estimators, mostly based on the sample correlation\nmatrix, are known to perform well as long as the number of available samples is\nmuch larger than the observation dimension. However, in the small sample size\nregime, the performance degrades. Recently, based on random matrix theory\nresults, a new subspace estimator was introduced, which was shown to be\nconsistent in the asymptotic regime where the number of samples and the\nobservation dimension converge to infinity at the same rate. In practice, this\nestimator outperforms the traditional ones even for certain scenarios where the\nobservation dimension is small and of the same order of magnitude as the number\nof samples. In this paper, we address a performance analysis of this recent\nestimator, by proving a central limit theorem in the above asymptotic regime.\nWe propose an accurate approximation of the mean square error, which can be\nevaluated numerically. \n\n"}
{"id": "1502.03300", "contents": "Title: A sequential rejection testing method for high-dimensional regression\n  with correlated variables Abstract: We propose a general, modular method for significance testing of groups (or\nclusters) of variables in a high-dimensional linear model. In presence of high\ncorrelations among the covariables, due to serious problems of identifiability,\nit is indispensable to focus on detecting groups of variables rather than\nsingletons. We propose an inference method which allows to build in\nhierarchical structures. It relies on repeated sample splitting and sequential\nrejection, and we prove that it asymptotically controls the familywise error\nrate. It can be implemented on any collection of clusters and leads to improved\npower in comparison to more standard non-sequential rejection methods. We\ncomplete the theoretical analysis with empirical results for simulated and real\ndata. \n\n"}
{"id": "1502.06718", "contents": "Title: The gradient flow of the polarization measure. With an appendix Abstract: The polarization measure is the probability that among 3 individuals chosen\nat random from a finite population exactly 2 come from the same class. This\nindex is maximum at the midpoints of the edges of the probability simplex. We\ncompute the gradient flow of this index that is the differential equation whose\nsolutions are the curves of steepest ascent. Tools from Information Geometry\nare extensively used. In a time series, a comparison of the estimated velocity\nof variation with the direction of the gradient field should be a better index\nthan the simple variation of the index. \n\n"}
{"id": "1502.07989", "contents": "Title: Statistical Methods and Computing for Big Data Abstract: Big data are data on a massive scale in terms of volume, intensity, and\ncomplexity that exceed the capacity of standard software tools. They present\nopportunities as well as challenges to statisticians. The role of computational\nstatisticians in scientific discovery from big data analyses has been\nunder-recognized even by peer statisticians. This article reviews recent\nmethodological and software developments in statistics that address the big\ndata challenges. Methodologies are grouped into three classes:\nsubsampling-based, divide and conquer, and sequential updating for stream data.\nSoftware review focuses on the open source R and R packages, covering recent\ntools that help break the barriers of computer memory and computing power. Some\nof the tools are illustrated in a case study with a logistic regression for the\nchance of airline delay. \n\n"}
{"id": "1503.00489", "contents": "Title: Approximation and estimation of very small probabilities of multivariate\n  extreme events Abstract: This article discusses modelling of the tail of a multivariate distribution\nfunction by means of a large deviation principle (LDP), and its application to\nthe estimation of the probability of a multivariate extreme event from a sample\nof n iid random vectors, with the probability bounded by powers of sample size\nwith exponents below -1. One way to view classical tail limits is as limits of\nprobability ratios. In contrast, the tail LDP provides asymptotic bounds or\nlimits for log-probability ratios. After standardising the marginals to\nstandard exponential, dependence is represented by a homogeneous rate function.\nFurthermore, the tail LDP can be extended to represent both dependence and\nmarginals, the latter implying marginal log-GW tail limits. A connection is\nestablished between the tail LDP and residual tail dependence (or hidden\nregular variation) and a recent extension of it. Under a smoothness assumption,\nthey are implied by the tail LDP. Based on the tail LDP, a simple estimator for\nvery small probabilities of extreme events is formulated. It avoids estimation\nof the rate function by making use of its homogeneity. Strong consistency in\nthe sense of convergence of log-probability ratios is proven. Simulations and\nan application illustrate the difference between the classical approach and the\nLDP-based approach. \n\n"}
{"id": "1503.02045", "contents": "Title: Estimation after Parameter Selection: Performance Analysis and\n  Estimation Methods Abstract: In many practical parameter estimation problems, prescreening and parameter\nselection are performed prior to estimation. In this paper, we consider the\nproblem of estimating a preselected unknown deterministic parameter chosen from\na parameter set based on observations according to a predetermined selection\nrule, $\\Psi$. The data-based parameter selection process may impact the\nsubsequent estimation by introducing a selection bias and creating coupling\nbetween decoupled parameters. This paper introduces a post-selection mean\nsquared error (PSMSE) criterion as a performance measure. A corresponding\nCram\\'er-Rao-type bound on the PSMSE of any $\\Psi$-unbiased estimator is\nderived, where the $\\Psi$-unbiasedness is in the Lehmann-unbiasedness sense.\nThe post-selection maximum-likelihood (PSML) estimator is presented .It is\nproved that if there exists an $\\Psi$-unbiased estimator that achieves the\n$\\Psi$-Cram\\'er-Rao bound (CRB), i.e. an $\\Psi$-efficient estimator, then it is\nproduced by the PSML estimator. In addition, iterative methods are developed\nfor the practical implementation of the PSML estimator. Finally, the proposed\n$\\Psi$-CRB and PSML estimator are examined in estimation after parameter\nselection with different distributions. \n\n"}
{"id": "1503.03305", "contents": "Title: Evading the curse of dimensionality in nonparametric density estimation\n  with simplified vine copulas Abstract: Practical applications of nonparametric density estimators in more than three\ndimensions suffer a great deal from the well-known curse of dimensionality:\nconvergence slows down as dimension increases. We show that one can evade the\ncurse of dimensionality by assuming a simplified vine copula model for the\ndependence between variables. We formulate a general nonparametric estimator\nfor such a model and show under high-level assumptions that the speed of\nconvergence is independent of dimension. We further discuss a particular\nimplementation for which we validate the high-level assumptions and establish\nits asymptotic normality. Simulation experiments illustrate a large gain in\nfinite sample performance when the simplifying assumption is at least\napproximately true. But even when it is severely violated, the vine copula\nbased approach proves advantageous as soon as more than a few variables are\ninvolved. Lastly, we give an application of the estimator to a classification\nproblem from astrophysics. \n\n"}
{"id": "1503.04098", "contents": "Title: On the Mathematics of the Jeffreys-Lindley Paradox Abstract: This paper is concerned with the well known Jeffreys-Lindley paradox. In a\nBayesian set up, the so-called paradox arises when a point null hypothesis is\ntested and an objective prior is sought for the alternative hypothesis. In\nparticular, the posterior for the null hypothesis tends to one when the\nuncertainty, i.e. the variance, for the parameter value goes to infinity. We\nargue that the appropriate way to deal with the paradox is to use simple\nmathematics, and that any philosophical argument is to be regarded as\nirrelevant. \n\n"}
{"id": "1503.04427", "contents": "Title: Rates of convergence of rho-estimators for sets of densities satisfying\n  shape constraints Abstract: The purpose of this paper is to pursue our study of rho-estimators built from\ni.i.d. observations that we defined in Baraud et al. (2014). For a\n\\rho-estimator based on some model S (which means that the estimator belongs to\nS) and a true distribution of the observations that also belongs to S, the risk\n(with squared Hellinger loss) is bounded by a quantity which can be viewed as a\ndimension function of the model and is often related to the \"metric dimension\"\nof this model, as defined in Birg\\'e (2006). This is a minimax point of view\nand it is well-known that it is pessimistic. Typically, the bound is accurate\nfor most points in the model but may be very pessimistic when the true\ndistribution belongs to some specific part of it. This is the situation that we\nwant to investigate here. For some models, like the set of decreasing densities\non [0,1], there exist specific points in the model that we shall call\n\"extremal\" and for which the risk is substantially smaller than the typical\nrisk. Moreover, the risk at a non-extremal point of the model can be bounded by\nthe sum of the risk bound at a well-chosen extremal point plus the square of\nits distance to this point. This implies that if the true density is close\nenough to an extremal point, the risk at this point may be smaller than the\nminimax risk on the model and this actually remains true even if the true\ndensity does not belong to the model. The result is based on some refined\nbounds on the suprema of empirical processes that are established in Baraud\n(2016). \n\n"}
{"id": "1503.05033", "contents": "Title: Asymptotics for in-sample density forecasting Abstract: This paper generalizes recent proposals of density forecasting models and it\ndevelops theory for this class of models. In density forecasting, the density\nof observations is estimated in regions where the density is not observed.\nIdentification of the density in such regions is guaranteed by structural\nassumptions on the density that allows exact extrapolation. In this paper, the\nstructural assumption is made that the density is a product of one-dimensional\nfunctions. The theory is quite general in assuming the shape of the region\nwhere the density is observed. Such models naturally arise when the time point\nof an observation can be written as the sum of two terms (e.g., onset and\nincubation period of a disease). The developed theory also allows for a\nmultiplicative factor of seasonal effects. Seasonal effects are present in many\nactuarial, biostatistical, econometric and statistical studies. Smoothing\nestimators are proposed that are based on backfitting. Full asymptotic theory\nis derived for them. A practical example from the insurance business is given\nproducing a within year budget of reported insurance claims. A small sample\nstudy supports the theoretical results. \n\n"}
{"id": "1503.08340", "contents": "Title: Statistical Properties of Convex Clustering Abstract: In this manuscript, we study the statistical properties of convex clustering.\nWe establish that convex clustering is closely related to single linkage\nhierarchical clustering and $k$-means clustering. In addition, we derive the\nrange of tuning parameter for convex clustering that yields a non-trivial\nsolution. We also provide an unbiased estimate of the degrees of freedom, and\nprovide a finite sample bound for the prediction error for convex clustering.\nWe compare convex clustering to some traditional clustering methods in\nsimulation studies. \n\n"}
{"id": "1504.01823", "contents": "Title: Structured Matrix Completion with Applications to Genomic Data\n  Integration Abstract: Matrix completion has attracted significant recent attention in many fields\nincluding statistics, applied mathematics and electrical engineering. Current\nliterature on matrix completion focuses primarily on independent sampling\nmodels under which the individual observed entries are sampled independently.\nMotivated by applications in genomic data integration, we propose a new\nframework of structured matrix completion (SMC) to treat structured missingness\nby design. Specifically, our proposed method aims at efficient matrix recovery\nwhen a subset of the rows and columns of an approximately low-rank matrix are\nobserved. We provide theoretical justification for the proposed SMC method and\nderive lower bound for the estimation errors, which together establish the\noptimal rate of recovery over certain classes of approximately low-rank\nmatrices. Simulation studies show that the method performs well in finite\nsample under a variety of configurations. The method is applied to integrate\nseveral ovarian cancer genomic studies with different extent of genomic\nmeasurements, which enables us to construct more accurate prediction rules for\novarian cancer survival. \n\n"}
{"id": "1504.02852", "contents": "Title: Fast Estimation of the Median Covariation Matrix with Application to\n  Online Robust Principal Components Analysis Abstract: The geometric median covariation matrix is a robust multivariate indicator of\ndispersion which can be extended without any difficulty to functional data. We\ndefine estimators, based on recursive algorithms, that can be simply updated at\neach new observation and are able to deal rapidly with large samples of high\ndimensional data without being obliged to store all the data in memory.\nAsymptotic convergence properties of the recursive algorithms are studied under\nweak conditions. The computation of the principal components can also be\nperformed online and this approach can be useful for online outlier detection.\nA simulation study clearly shows that this robust indicator is a competitive\nalternative to minimum covariance determinant when the dimension of the data is\nsmall and robust principal components analysis based on projection pursuit and\nspherical projections for high dimension data. An illustration on a large\nsample and high dimensional dataset consisting of individual TV audiences\nmeasured at a minute scale over a period of 24 hours confirms the interest of\nconsidering the robust principal components analysis based on the median\ncovariation matrix. All studied algorithms are available in the R package\nGmedian on CRAN. \n\n"}
{"id": "1504.03144", "contents": "Title: Precise tail asymptotics of fixed points of the smoothing transform with\n  general weights Abstract: We consider solutions of the stochastic equation $R=_d\\sum_{i=1}^NA_iR_i+B$,\nwhere $N>1$ is a fixed constant, $A_i$ are independent, identically distributed\nrandom variables and $R_i$ are independent copies of $R$, which are independent\nboth from $A_i$'s and $B$. The hypotheses ensuring existence of solutions are\nwell known. Moreover under a number of assumptions the main being\n$\\mathbb{E}|A_1|^{\\alpha}=1/N$ and $\\mathbb{E}|A_1|^{\\alpha}\\log|A_1|>0$, the\nlimit $\\lim_{t\\to\\infty}t^{\\alpha}\\mathbb{P}[|R|>t]=K$ exists. In the present\npaper, we prove positivity of $K$. \n\n"}
{"id": "1504.04580", "contents": "Title: Robust estimation of U-statistics Abstract: An important part of the legacy of Evarist Gin\\'e is his fundamental\ncontributions to our understanding of $U$-statistics and $U$-processes. In this\npaper we discuss the estimation of the mean of multivariate functions in case\nof possibly heavy-tailed distributions. In such situations, reliable estimates\nof the mean cannot be obtained by usual $U$-statistics. We introduce a new\nestimator, based on the so-called median-of-means technique. We develop\nperformance bounds for this new estimator that generalizes an estimate of\nArcones and Gin\\'e (1993), showing that the new estimator performs, under\nminimal moment conditions, as well as classical $U$-statistics for bounded\nrandom variables. We discuss an application of this estimator to clustering. \n\n"}
{"id": "1504.04636", "contents": "Title: Consistent Learning by Composite Proximal Thresholding Abstract: We investigate the modeling and the numerical solution of machine learning\nproblems with prediction functions which are linear combinations of elements of\na possibly infinite-dimensional dictionary. We propose a novel flexible\ncomposite regularization model, which makes it possible to incorporate various\npriors on the coefficients of the prediction function, including sparsity and\nhard constraints. We show that the estimators obtained by minimizing the\nregularized empirical risk are consistent in a statistical sense, and we design\nan error-tolerant composite proximal thresholding algorithm for computing such\nestimators. New results on the asymptotic behavior of the proximal\nforward-backward splitting method are derived and exploited to establish the\nconvergence properties of the proposed algorithm. In particular, our method\nfeatures a $o(1/m)$ convergence rate in objective values. \n\n"}
{"id": "1504.04814", "contents": "Title: Asymptotic behaviour of the empirical Bayes posteriors associated to\n  maximum marginal likelihood estimator Abstract: We consider the asymptotic behaviour of the marginal maximum likelihood\nempirical Bayes posterior distribution in general setting. First we\ncharacterize the set where the maximum marginal likelihood estimator is located\nwith high probability. Then we provide oracle type of upper and lower bounds\nfor the contraction rates of the empirical Bayes posterior. We also show that\nthe hierarchical Bayes posterior achieves the same contraction rate as the\nmaximum marginal likelihood empirical Bayes posterior. We demonstrate the\napplicability of our general results for various models and prior distributions\nby deriving upper and lower bounds for the contraction rates of the\ncorresponding empirical and hierarchical Bayes posterior distributions. \n\n"}
{"id": "1504.06384", "contents": "Title: Multiple Testing of Local Extrema for Detection of Change Points Abstract: A new approach to detect change points based on differential smoothing and\nmultiple testing is presented for long data sequences modeled as piecewise\nconstant functions plus stationary ergodic Gaussian noise. As an application of\nthe STEM algorithm for peak detection developed in\n\\citet{schwartzman2011multiple} and \\citet{cheng2017multiple}, the method\ndetects change points as significant local maxima and minima after smoothing\nand differentiating the observed sequence. The algorithm, combined with the\nBenjamini-Hochberg procedure for thresholding p-values, provides asymptotic\nstrong control of the False Discovery Rate (FDR) and power consistency, as the\nlength of the sequence and the size of the jumps get large. Simulations show\nthat FDR levels are maintained in non-asymptotic conditions and guide the\nchoice of smoothing bandwidth. The methods are illustrated in magnetometer\nsensor data and genomic array-CGH data. An R package named \"dSTEM\" is available\nin R cran. \n\n"}
{"id": "1505.00261", "contents": "Title: Strongly Intensive Cumulants: Fluctuation Measures for Systems With\n  Incompletely Constrained Volumes Abstract: The cumulants of thermal variables are of general interest in physics due to\ntheir extensivity and their correspondence with susceptibilities. They become\nespecially significant near critical points of phase transitions where they\ndiverge along with the correlation length. Cumulant measurements have been used\nextensively within the field of heavy-ion physics, principally as tools in the\nsearch for a hypothetical QCD critical point along the transition between\nhadronic matter and QGP. The volume of individual heavy-ion collisions can be\nonly partially constrained and, as a result, cumulant measurements are\nsignificantly biased by the limited volume resolution. We propose a class of\nmoments called strongly intensive cumulants which can be accurately measured in\nthe presence of unconstrained volume fluctuations. Additionally, they share the\nsame direct relationship with susceptibilities as cumulants in many cases. \n\n"}
{"id": "1505.00579", "contents": "Title: Comparison of hit-and-run, slice sampling and random walk Metropolis Abstract: Different Markov chains can be used for approximate sampling of a\ndistribution given by an unnormalized density function with respect to the\nLebesgue measure. The hit-and-run, (hybrid) slice sampler and random walk\nMetropolis algorithm are popular tools to simulate such Markov chains. We\ndevelop a general approach to compare the efficiency of these sampling\nprocedures by the use of a partial ordering of their Markov operators, the\ncovariance ordering. In particular, we show that the hit-and-run and the simple\nslice sampler are more efficient than a hybrid slice sampler based on\nhit-and-run which, itself, is more efficient than a (lazy) random walk\nMetropolis algorithm. \n\n"}
{"id": "1505.01371", "contents": "Title: Re-scale boosting for regression and classification Abstract: Boosting is a learning scheme that combines weak prediction rules to produce\na strong composite estimator, with the underlying intuition that one can obtain\naccurate prediction rules by combining \"rough\" ones. Although boosting is\nproved to be consistent and overfitting-resistant, its numerical convergence\nrate is relatively slow. The aim of this paper is to develop a new boosting\nstrategy, called the re-scale boosting (RBoosting), to accelerate the numerical\nconvergence rate and, consequently, improve the learning performance of\nboosting. Our studies show that RBoosting possesses the almost optimal\nnumerical convergence rate in the sense that, up to a logarithmic factor, it\ncan reach the minimax nonlinear approximation rate. We then use RBoosting to\ntackle both the classification and regression problems, and deduce a tight\ngeneralization error estimate. The theoretical and experimental results show\nthat RBoosting outperforms boosting in terms of generalization. \n\n"}
{"id": "1505.01787", "contents": "Title: Uniform convergence rates over maximal domains in structural\n  nonparametric cointegrating regression Abstract: This paper presents uniform convergence rates for kernel regression\nestimators, in the setting of a structural nonlinear cointegrating regression\nmodel. We generalise the existing literature in three ways. First, the domain\nto which these rates apply is much wider than has been previously considered,\nand can be chosen so as to contain as large a fraction of the sample as desired\nin the limit. Second, our results allow the regression disturbance to be\nserially correlated, and cross-correlated with the regressor; previous work on\nthis problem (of obtaining uniform rates) having been confined entirely to the\nsetting of an exogenous regressor. Third, we permit the bandwidth to be\ndata-dependent, requiring only that it satisfy certain weak asymptotic\nshrinkage conditions. Our assumptions on the regressor process are consistent\nwith a very broad range of departures from the standard unit root\nautoregressive model, allowing the regressor to be fractionally integrated, and\nto have an infinite variance (and even infinite lower-order moments). \n\n"}
{"id": "1505.02023", "contents": "Title: Tests for separability in nonparametric covariance operators of random\n  surfaces Abstract: The assumption of separability of the covariance operator for a random image\nor hypersurface can be of substantial use in applications, especially in\nsituations where the accurate estimation of the full covariance structure is\nunfeasible, either for computational reasons, or due to a small sample size.\nHowever, inferential tools to verify this assumption are somewhat lacking in\nhigh-dimensional or functional {data analysis} settings, where this assumption\nis most relevant. We propose here to test separability by focusing on\n$K$-dimensional projections of the difference between the covariance operator\nand a nonparametric separable approximation. The subspace we project onto is\none generated by the eigenfunctions of the covariance operator estimated under\nthe separability hypothesis, negating the need to ever estimate the full\nnon-separable covariance. We show that the rescaled difference of the sample\ncovariance operator with its separable approximation is asymptotically\nGaussian. As a by-product of this result, we derive asymptotically pivotal\ntests under Gaussian assumptions, and propose bootstrap methods for\napproximating the distribution of the test statistics. We probe the finite\nsample performance through simulations studies, and present an application to\nlog-spectrogram images from a phonetic linguistics dataset. \n\n"}
{"id": "1505.04647", "contents": "Title: Estimation of the directional parameter of the offset exponential and\n  normal distributions in three-dimensional space using the sample mean Abstract: The directional precision of the sample mean estimator was calculated\nanalytically for the offset exponential and normal distributions in\nthree-dimensional space both for a finite sample and for limiting cases. It was\nshown that the spherical projection of the sample mean of the shifted\nexponential distribution has connections with modified Bessel functions and\nwith hypergeometric functions. It was shown explicitly how the distribution of\nthe sample mean of the exponential pdf converges near the mode to the normal\ndistribution. Approximation formulae for the distribution of the sample mean of\nthe shifted exponential distribution and for its directional precision and for\nthe precision of the estimation of the direction of shift of the normal\ndistribution were obtained. \n\n"}
{"id": "1505.08049", "contents": "Title: Sensing tensors with Gaussian filters Abstract: Sparse recovery from linear Gaussian measurements has been the subject of\nmuch investigation since the breaktrough papers \\cite{CRT:IEEEIT06} and\n\\cite{donoho2006compressed} on Compressed Sensing. Application to sparse\nvectors and sparse matrices via least squares penalized with sparsity promoting\nnorms is now well understood using tools such as Gaussian mean width,\nstatistical dimension and the notion of descent cones \\cite{tropp2014convex}\n\\cite{Vershynin:ArXivEstimation14}. Extention of these ideas to low rank tensor\nrecovery is starting to enjoy considerable interest due to its many potential\napplications to Independent Component Analysis, Hidden Markov Models and\nGaussian Mixture Models \\cite{AnandkumarEtAl:JMLR14}, hyperspectral image\nanalysis \\cite{zhang2008tensor}, to name a few. In this paper, we demonstrate\nthat the recent approach of \\cite{Vershynin:ArXivEstimation14} provides very\nuseful error bounds in the tensor setting using the nuclear norm or the\nRomera-Paredes--Pontil \\cite{RomeraParedesPontil:NIPS13} penalization. \n\n"}
{"id": "1506.00671", "contents": "Title: Sample-Optimal Density Estimation in Nearly-Linear Time Abstract: We design a new, fast algorithm for agnostically learning univariate\nprobability distributions whose densities are well approximated by piecewise\npolynomial functions. Let $f$ be the density function of an arbitrary\nunivariate distribution, and suppose that $f$ is $\\mathrm{OPT}$-close in\n$L_1$-distance to an unknown piecewise polynomial function with $t$ interval\npieces and degree $d$. Our algorithm draws $n = O(t(d+1)/\\epsilon^2)$ samples\nfrom $f$, runs in time $\\tilde{O}(n \\cdot \\mathrm{poly}(d))$, and with\nprobability at least $9/10$ outputs an $O(t)$-piecewise degree-$d$ hypothesis\n$h$ that is $4 \\cdot \\mathrm{OPT} +\\epsilon$ close to $f$.\n  Our general algorithm yields (nearly) sample-optimal and nearly-linear time\nestimators for a wide range of structured distribution families over both\ncontinuous and discrete domains in a unified way. For most of our applications,\nthese are the first sample-optimal and nearly-linear time estimators in the\nliterature. As a consequence, our work resolves the sample and computational\ncomplexities of a broad class of inference tasks via a single \"meta-algorithm\".\nMoreover, we experimentally demonstrate that our algorithm performs very well\nin practice.\n  Our algorithm consists of three \"levels\": (i) At the top level, we employ an\niterative greedy algorithm for finding a good partition of the real line into\nthe pieces of a piecewise polynomial. (ii) For each piece, we show that the\nsub-problem of finding a good polynomial fit on the current interval can be\nsolved efficiently with a separation oracle method. (iii) We reduce the task of\nfinding a separating hyperplane to a combinatorial problem and give an\nefficient algorithm for this problem. Combining these three procedures gives a\ndensity estimation algorithm with the claimed guarantees. \n\n"}
{"id": "1506.01782", "contents": "Title: High-dimensional Ordinary Least-squares Projection for Screening\n  Variables Abstract: Variable selection is a challenging issue in statistical applications when\nthe number of predictors $p$ far exceeds the number of observations $n$. In\nthis ultra-high dimensional setting, the sure independence screening (SIS)\nprocedure was introduced to significantly reduce the dimensionality by\npreserving the true model with overwhelming probability, before a refined\nsecond stage analysis. However, the aforementioned sure screening property\nstrongly relies on the assumption that the important variables in the model\nhave large marginal correlations with the response, which rarely holds in\nreality. To overcome this, we propose a novel and simple screening technique\ncalled the high-dimensional ordinary least-squares projection (HOLP). We show\nthat HOLP possesses the sure screening property and gives consistent variable\nselection without the strong correlation assumption, and has a low\ncomputational complexity. A ridge type HOLP procedure is also discussed.\nSimulation study shows that HOLP performs competitively compared to many other\nmarginal correlation based methods. An application to a mammalian eye disease\ndata illustrates the attractiveness of HOLP. \n\n"}
{"id": "1506.01833", "contents": "Title: Asymptotic properties of multivariate tapering for estimation and\n  prediction Abstract: Parameter estimation for and prediction of spatially or spatio--temporally\ncorrelated random processes are used in many areas and often require the\nsolution of a large linear system based on the covariance matrix of the\nobservations. In recent years, the dataset sizes to which these methods are\napplied have steadily increased such that straightforward statistical tools are\ncomputationally too expensive to be used. In the univariate context, tapering,\ni.e., creating sparse approximate linear systems, has been shown to be an\nefficient tool in both the estimation and prediction settings. The asymptotic\nproperties are derived under an infill asymptotic setting. In this paper we use\na domain increasing framework for estimation and prediction using multivariate\ntapering. Under this asymptotic regime we prove that tapering (one-tapered\nform) preserves the consistency of the untapered maximum likelihood estimator\nand show that tapering has asymptotically the same mean squared prediction\nerror as using the corresponding untapered predictor. The theoretical results\nare illustrated with simulations. \n\n"}
{"id": "1506.01842", "contents": "Title: Autoregressive Functions Estimation in Nonlinear Bifurcating\n  Autoregressive Models Abstract: Bifurcating autoregressive processes, which can be seen as an adaptation of\nau-toregressive processes for a binary tree structure, have been extensively\nstudied during the last decade in a parametric context. In this work we do not\nspecify any a priori form for the two autoregressive functions and we use\nnonparametric techniques. We investigate both nonasymp-totic and asymptotic\nbehavior of the Nadaraya-Watson type estimators of the autoregressive\nfunctions. We build our estimators observing the process on a finite subtree\ndenoted by Tn, up to the depth n. Estimators achieve the classical rate |Tn|\n--$\\beta$/(2$\\beta$+1) in quadratic loss over H{\\\"o}lder classes of smoothness.\nWe prove almost sure convergence, asymptotic normality giving the bias\nexpression when choosing the optimal bandwidth and a moderate deviations\nprinciple. Our proofs rely on specific techniques used to study bifurcating\nMarkov chains. Finally, we address the question of asymmetry and develop an\nasymptotic test for the equality of the two autoregressive functions. \n\n"}
{"id": "1506.02108", "contents": "Title: Deeply Learning the Messages in Message Passing Inference Abstract: Deep structured output learning shows great promise in tasks like semantic\nimage segmentation. We proffer a new, efficient deep structured model learning\nscheme, in which we show how deep Convolutional Neural Networks (CNNs) can be\nused to estimate the messages in message passing inference for structured\nprediction with Conditional Random Fields (CRFs). With such CNN message\nestimators, we obviate the need to learn or evaluate potential functions for\nmessage calculation. This confers significant efficiency for learning, since\notherwise when performing structured learning for a CRF with CNN potentials it\nis necessary to undertake expensive inference for every stochastic gradient\niteration. The network output dimension for message estimation is the same as\nthe number of classes, in contrast to the network output for general CNN\npotential functions in CRFs, which is exponential in the order of the\npotentials. Hence CNN message learning has fewer network parameters and is more\nscalable for cases that a large number of classes are involved. We apply our\nmethod to semantic image segmentation on the PASCAL VOC 2012 dataset. We\nachieve an intersection-over-union score of 73.4 on its test set, which is the\nbest reported result for methods using the VOC training images alone. This\nimpressive performance demonstrates the effectiveness and usefulness of our CNN\nmessage learning method. \n\n"}
{"id": "1506.03481", "contents": "Title: On the Asymptotic Efficiency of Approximate Bayesian Computation\n  Estimators Abstract: Many statistical applications involve models for which it is difficult to\nevaluate the likelihood, but from which it is relatively easy to sample.\nApproximate Bayesian computation is a likelihood-free method for implementing\nBayesian inference in such cases. We present results on the asymptotic variance\nof estimators obtained using approximate Bayesian computation in a large-data\nlimit. Our key assumption is that the data are summarized by a\nfixed-dimensional summary statistic that obeys a central limit theorem. We\nprove asymptotic normality of the mean of the approximate Bayesian computation\nposterior. This result also shows that, in terms of asymptotic variance, we\nshould use a summary statistic that is the same dimension as the parameter\nvector, p; and that any summary statistic of higher dimension can be reduced,\nthrough a linear transformation, to dimension p in a way that can only reduce\nthe asymptotic variance of the posterior mean. We look at how the Monte Carlo\nerror of an importance sampling algorithm that samples from the approximate\nBayesian computation posterior affects the accuracy of estimators. We give\nconditions on the importance sampling proposal distribution such that the\nvariance of the estimator will be the same order as that of the maximum\nlikelihood estimator based on the summary statistics used. This suggests an\niterative importance sampling algorithm, which we evaluate empirically on a\nstochastic volatility model. \n\n"}
{"id": "1506.03537", "contents": "Title: Exponential Series Approaches for Nonparametric Graphical Models Abstract: This thesis studies high-dimensional, continuous-valued pairwise Markov\nRandom Fields. We are particularly interested in approximating pairwise\ndensities whose logarithm belongs to a Sobolev space. For this problem we\npropose the method of exponential series [Crain, 1974; Barron and Sheu, 1991],\nwhich approximates the log density by a finite- dimensional exponential family\nwith the number of sufficient statistics increasing with the sample size. We\nconsider two approaches to estimating these models. The first is regularized\nmaximum likelihood. This involves optimizing the sum of the log-likelihood of\nthe data and a sparsity-inducing regularizer. We provide consistency and edge\nselection guarantees for this method. We then propose a variational\napproximation to the likelihood based on tree- reweighted, nonparametric\nmessage passing.\n  We then consider estimation using regularized score matching. This approach\nuses an alternative scoring rule to the log-likelihood, which obviates the need\nto compute the normalizing constant of the distribution. For general\ncontinuous-valued exponential families, we provide parameter and edge\nconsistency results. We then describe results for model selection in the\nnonparametric pairwise model using exponential series. The regularized score\nmatching problem is shown to be a convex program; we provide scalable\nalgorithms based on consensus Alternating Direction Method of Multipliers\n(ADMM, [Boyd et al., 2011]) and Coordinate-wise Descent. We compare our method\nto others in the literature as well as the aforementioned TRW estimator using\nsimulated data. \n\n"}
{"id": "1506.03537", "contents": "Title: Exponential Series Approaches for Nonparametric Graphical Models Abstract: This thesis studies high-dimensional, continuous-valued pairwise Markov\nRandom Fields. We are particularly interested in approximating pairwise\ndensities whose logarithm belongs to a Sobolev space. For this problem we\npropose the method of exponential series [Crain, 1974; Barron and Sheu, 1991],\nwhich approximates the log density by a finite- dimensional exponential family\nwith the number of sufficient statistics increasing with the sample size. We\nconsider two approaches to estimating these models. The first is regularized\nmaximum likelihood. This involves optimizing the sum of the log-likelihood of\nthe data and a sparsity-inducing regularizer. We provide consistency and edge\nselection guarantees for this method. We then propose a variational\napproximation to the likelihood based on tree- reweighted, nonparametric\nmessage passing.\n  We then consider estimation using regularized score matching. This approach\nuses an alternative scoring rule to the log-likelihood, which obviates the need\nto compute the normalizing constant of the distribution. For general\ncontinuous-valued exponential families, we provide parameter and edge\nconsistency results. We then describe results for model selection in the\nnonparametric pairwise model using exponential series. The regularized score\nmatching problem is shown to be a convex program; we provide scalable\nalgorithms based on consensus Alternating Direction Method of Multipliers\n(ADMM, [Boyd et al., 2011]) and Coordinate-wise Descent. We compare our method\nto others in the literature as well as the aforementioned TRW estimator using\nsimulated data. \n\n"}
{"id": "1506.03541", "contents": "Title: A Linear Model for Interval-valued Data Abstract: Interval-valued linear regression has been investigated for some time. One of\nthe critical issues is optimizing the balance between model flexibility and\ninterpretability. This paper proposes a linear model for interval-valued data\nbased on the affine operators in the cone $\\mathcal{C} = \\{ (x, y) \\in\n\\mathbb{R}^2 | x \\leq y\\}$. The resulting new model is shown to have improved\nflexibility over typical models in the literature, while maintaining a good\ninterpretability. The least squares (LS) estimators of the model parameters are\nprovided in a simple explicit form, which possesses a series of nice\nproperties. Further investigations into the LS estimators shed light on the\npositive restrictions of a subset of the parameters and their implications on\nthe model validity. A simulation study is presented that supports the\ntheoretical findings. An application to a real data set is also provided to\ndemonstrate the applicability of our model. \n\n"}
{"id": "1506.03621", "contents": "Title: Convergence of Estimated Option Price in a Regime switching Market Abstract: In an observed generalized semi-Markov regime, estimation of transition rate\nof regime switching leads towards calculation of locally risk minimizing option\nprice. Despite the uniform convergence of estimated step function of transition\nrate, to meet the existence of classical solution of the modified price\nequation, the estimator is approximated in the class of smooth functions and\nfurthermore, the convergence is established. Later, the existence of the\nsolution of the modified price equation is verified and the point-wise\nconvergence of such approximation of option price is proved to answer the\ntractability of its application in Finance. To demonstrate the consistency in\nresult a numerical experiment has been reported. \n\n"}
{"id": "1506.03740", "contents": "Title: Sharp bounds for cumulative distribution functions Abstract: Ratios of integrals can be bounded in terms of ratios of integrands under\ncertain monotonicity conditions. This result, related with L'H\\^{o}pital's\nmonotone rule, can be used to obtain sharp bounds for cumulative distribution\nfunctions. We consider the case of noncentral cumulative gamma and beta\ndistributions. Three different types of sharp bounds for the noncentral gamma\ndistributions (also called Marcum functions) are obtained in terms of modified\nBessel functions and one additional type of function: a second modified Bessel\nfunction, two error functions or one incomplete gamma function. For the\nnoncentral beta case the bounds are expressed in terms of Kummer functions and\none additional Kummer function or an incomplete beta function. These bounds\nimprove previous results with respect to their range of application and/or its\nsharpness. \n\n"}
{"id": "1506.04525", "contents": "Title: Unified Systems of FB-SPDEs/FB-SDEs with Jumps/Skew Reflections and\n  Stochastic Differential Games Abstract: We study four systems and their interactions. First, we formulate a unified\nsystem of coupled forward-backward stochastic partial differential equations\n(FB-SPDEs) with Levy jumps, whose drift, diffusion, and jump coefficients may\ninvolve partial differential operators. A solution to the FB-SPDEs is defined\nby a 4-tuple general dimensional random vector-field process evolving in time\ntogether with position parameters over a domain (e.g., a hyperbox or a\nmanifold). Under an infinite sequence of generalized local linear growth and\nLipschitz conditions, the well-posedness of an adapted 4-tuple strong solution\nis proved over a suitably constructed topological space. Second, we consider a\nunified system of FB-SDEs, a special form of the FB-SPDEs, however, with skew\nboundary reflections. Under randomized linear growth and Lipschitz conditions\ntogether with a general completely-S condition on reflections, we prove the\nwell-posedness of an adapted 6-tuple weak solution with boundary regulators to\nthe FB-SDEs by the Skorohod problem and an oscillation inequality.\nParticularly, if the spectral radii in some sense for reflection matrices are\nstrictly less than the unity, an adapted 6-tuple strong solution is concerned.\nThird, we formulate a stochastic differential game (SDG) with general number of\nplayers based on the FB-SDEs. By a solution to the FB-SPDEs, we get a solution\nto the FB-SDEs under a given control rule and then obtain a Pareto optimal Nash\nequilibrium policy process to the SDG. Fourth, we study the applications of the\nFB-SPDEs/FB-SDEs in queueing systems and quantum statistics while we use them\nto motivate the SDG. \n\n"}
{"id": "1506.04692", "contents": "Title: A new V-fold type procedure based on robust tests Abstract: We define a general V-fold cross-validation type method based on robust\ntests, which is an extension of the hold-out defined by Birg{\\'e} [7, Section\n9]. We give some theoretical results showing that, under some weak assumptions\non the considered statistical procedures, our selected estimator satisfies an\noracle type inequality. We also introduce a fast algorithm that implements our\nmethod. Moreover we show in our simulations that this V-fold performs generally\nwell for estimating a density for different sample sizes, and can handle\nwell-known problems, such as binwidth selection for histograms or bandwidth\nselection for kernels. We finally provide a comparison with other classical\nV-fold methods and study empirically the influence of the value of V on the\nrisk. \n\n"}
{"id": "1506.04775", "contents": "Title: A stochastic density matrix approach to approximation of probability\n  distributions and its application to nonlinear systems Abstract: This paper outlines an approach to the approximation of probability density\nfunctions by quadratic forms of weighted orthonormal basis functions with\npositive semi-definite Hermitian matrices of unit trace. Such matrices are\ncalled stochastic density matrices in order to reflect an analogy with the\nquantum mechanical density matrices. The SDM approximation of a PDF satisfies\nthe normalization condition and is nonnegative everywhere in contrast to the\ntruncated Gram-Charlier and Edgeworth expansions. For bases with an algebraic\nstructure, such as the Hermite polynomial and Fourier bases, the SDM\napproximation can be chosen so as to satisfy given moment specifications and\ncan be optimized using a quadratic proximity criterion. We apply the SDM\napproach to the Fokker-Planck-Kolmogorov PDF dynamics of Markov diffusion\nprocesses governed by nonlinear stochastic differential equations. This leads\nto an ordinary differential equation for the SDM dynamics of the approximating\nPDF. As an example, we consider the Smoluchowski SDE on a multidimensional\ntorus. \n\n"}
{"id": "1506.04854", "contents": "Title: A Correlation Analysis Method for Power Systems Based on Random Matrix\n  Theory Abstract: The operating status of power systems is influenced by growing varieties of\nfactors, resulting from the developing sizes and complexity of power systems;\nin this situation, the modelbased methods need be revisited. A data-driven\nmethod, as the novel alternative, on the other hand, is proposed in this paper:\nit reveals the correlations between the factors and the system status through\nstatistical properties of data. An augmented matrix, as the data source, is the\nkey trick for this method; it is formulated by two parts: 1) status data as the\nbasic part, and 2) factor data as the augmented part. The random matrix theory\n(RMT) is applied as the mathematical framework. The linear eigenvalue\nstatistics (LESs), such as the mean spectral radius (MSR), are defined to study\ndata correlations through large random matrices. Compared with model-based\nmethods, the proposed method is inspired by a pure statistical approach,\nwithout a prior knowledge of operation and interaction mechanism models for\npower systems and factors. In general, this method is direct in analysis,\nrobust against bad data, universal to various factors, and applicable for\nreal-time analysis. A case study, based on the standard IEEE 118-bus system,\nvalidates the proposed method. \n\n"}
{"id": "1506.04915", "contents": "Title: Bayesian nonparametric inference for discovery probabilities: credible\n  intervals and large sample asymptotics Abstract: Given a sample of size $n$ from a population of individuals belonging to\ndifferent species with unknown proportions, a popular problem of practical\ninterest consists in making inference on the probability $D_{n}(l)$ that the\n$(n+1)$-th draw coincides with a species with frequency $l$ in the sample, for\nany $l=0,1,\\ldots,n$. This paper contributes to the methodology of Bayesian\nnonparametric inference for $D_{n}(l)$. Specifically, under the general\nframework of Gibbs-type priors we show how to derive credible intervals for a\nBayesian nonparametric estimation of $D_{n}(l)$, and we investigate the large\n$n$ asymptotic behaviour of such an estimator. Of particular interest are\nspecial cases of our results obtained under the specification of the two\nparameter Poisson--Dirichlet prior and the normalized generalized Gamma prior,\nwhich are two of the most commonly used Gibbs-type priors. With respect to\nthese two prior specifications, the proposed results are illustrated through a\nsimulation study and a benchmark Expressed Sequence Tags dataset. To the best\nour knowledge, this illustration provides the first comparative study between\nthe two parameter Poisson--Dirichlet prior and the normalized generalized Gamma\nprior in the context of Bayesian nonparemetric inference for $D_{n}(l)$. \n\n"}
{"id": "1506.05337", "contents": "Title: Uniform Asymptotics for Nonparametric Quantile Regression with an\n  Application to Testing Monotonicity Abstract: In this paper, we establish a uniform error rate of a Bahadur representation\nfor local polynomial estimators of quantile regression functions. The error\nrate is uniform over a range of quantiles, a range of evaluation points in the\nregressors, and over a wide class of probabilities for observed random\nvariables. Most of the existing results on Bahadur representations for local\npolynomial quantile regression estimators apply to the fixed data generating\nprocess. In the context of testing monotonicity where the null hypothesis is of\na complex composite hypothesis, it is particularly relevant to establish\nBahadur expansions that hold uniformly over a large class of data generating\nprocesses. In addition, we establish the same error rate for bootstrap local\npolynomial estimators which can be useful for various bootstrap inference. As\nan illustration, we apply to testing monotonicity of quantile regression and\npresent Monte Carlo experiments based on this example. \n\n"}
{"id": "1506.05539", "contents": "Title: Confidence Intervals for High-Dimensional Linear Regression: Minimax\n  Rates and Adaptivity Abstract: Confidence sets play a fundamental role in statistical inference. In this\npaper, we consider confidence intervals for high dimensional linear regression\nwith random design. We first establish the convergence rates of the minimax\nexpected length for confidence intervals in the oracle setting where the\nsparsity parameter is given. The focus is then on the problem of adaptation to\nsparsity for the construction of confidence intervals. Ideally, an adaptive\nconfidence interval should have its length automatically adjusted to the\nsparsity of the unknown regression vector, while maintaining a prespecified\ncoverage probability. It is shown that such a goal is in general not\nattainable, except when the sparsity parameter is restricted to a small region\nover which the confidence intervals have the optimal length of the usual\nparametric rate. It is further demonstrated that the lack of adaptivity is not\ndue to the conservativeness of the minimax framework, but is fundamentally\ncaused by the difficulty of learning the bias accurately. \n\n"}
{"id": "1506.05555", "contents": "Title: Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with\n  Random Bases Abstract: For big data analysis, high computational cost for Bayesian methods often\nlimits their applications in practice. In recent years, there have been many\nattempts to improve computational efficiency of Bayesian inference. Here we\npropose an efficient and scalable computational technique for a\nstate-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian\nMonte Carlo (HMC). The key idea is to explore and exploit the structure and\nregularity in parameter space for the underlying probabilistic model to\nconstruct an effective approximation of its geometric properties. To this end,\nwe build a surrogate function to approximate the target distribution using\nproperly chosen random bases and an efficient optimization process. The\nresulting method provides a flexible, scalable, and efficient sampling\nalgorithm, which converges to the correct target distribution. We show that by\nchoosing the basis functions and optimization process differently, our method\ncan be related to other approaches for the construction of surrogate functions\nsuch as generalized additive models or Gaussian process models. Experiments\nbased on simulated and real data show that our approach leads to substantially\nmore efficient sampling algorithms compared to existing state-of-the art\nmethods. \n\n"}
{"id": "1506.06179", "contents": "Title: Detectability thresholds and optimal algorithms for community structure\n  in dynamic networks Abstract: We study the fundamental limits on learning latent community structure in\ndynamic networks. Specifically, we study dynamic stochastic block models where\nnodes change their community membership over time, but where edges are\ngenerated independently at each time step. In this setting (which is a special\ncase of several existing models), we are able to derive the detectability\nthreshold exactly, as a function of the rate of change and the strength of the\ncommunities. Below this threshold, we claim that no algorithm can identify the\ncommunities better than chance. We then give two algorithms that are optimal in\nthe sense that they succeed all the way down to this limit. The first uses\nbelief propagation (BP), which gives asymptotically optimal accuracy, and the\nsecond is a fast spectral clustering algorithm, based on linearizing the BP\nequations. We verify our analytic and algorithmic results via numerical\nsimulation, and close with a brief discussion of extensions and open questions. \n\n"}
{"id": "1506.06266", "contents": "Title: Uniform Asymptotic Inference and the Bootstrap After Model Selection Abstract: Recently, Tibshirani et al. (2016) proposed a method for making inferences\nabout parameters defined by model selection, in a typical regression setting\nwith normally distributed errors. Here, we study the large sample properties of\nthis method, without assuming normality. We prove that the test statistic of\nTibshirani et al. (2016) is asymptotically valid, as the number of samples n\ngrows and the dimension d of the regression problem stays fixed. Our asymptotic\nresult holds uniformly over a wide class of nonnormal error distributions. We\nalso propose an efficient bootstrap version of this test that is provably\n(asymptotically) conservative, and in practice, often delivers shorter\nintervals than those from the original normality-based approach. Finally, we\nprove that the test statistic of Tibshirani et al. (2016) does not enjoy\nuniform validity in a high-dimensional setting, when the dimension d is allowed\ngrow. \n\n"}
{"id": "1506.08163", "contents": "Title: A Geometric View on Constrained M-Estimators Abstract: We study the estimation error of constrained M-estimators, and derive\nexplicit upper bounds on the expected estimation error determined by the\nGaussian width of the constraint set. Both of the cases where the true\nparameter is on the boundary of the constraint set (matched constraint), and\nwhere the true parameter is strictly in the constraint set (mismatched\nconstraint) are considered. For both cases, we derive novel universal\nestimation error bounds for regression in a generalized linear model with the\ncanonical link function. Our error bound for the mismatched constraint case is\nminimax optimal in terms of its dependence on the sample size, for Gaussian\nlinear regression by the Lasso. \n\n"}
{"id": "1506.08253", "contents": "Title: Bayesian Inference for Latent Biologic Structure with Determinantal\n  Point Processes (DPP) Abstract: We discuss the use of the determinantal point process (DPP) as a prior for\nlatent structure in biomedical applications, where inference often centers on\nthe interpretation of latent features as biologically or clinically meaningful\nstructure. Typical examples include mixture models, when the terms of the\nmixture are meant to represent clinically meaningful subpopulations (of\npatients, genes, etc.). Another class of examples are feature allocation\nmodels. We propose the DPP prior as a repulsive prior on latent mixture\ncomponents in the first example, and as prior on feature-specific parameters in\nthe second case. We argue that the DPP is in general an attractive prior model\nfor latent structure when biologically relevant interpretation of such\nstructure is desired. We illustrate the advantages of DPP prior in three case\nstudies, including inference in mixture models for magnetic resonance images\n(MRI) and for protein expression, and a feature allocation model for gene\nexpression using data from The Cancer Genome Atlas. An important part of our\nargument are efficient and straightforward posterior simulation methods. We\nimplement a variation of reversible jump Markov chain Monte Carlo simulation\nfor inference under the DPP prior, using a density with respect to the unit\nrate Poisson process. \n\n"}
{"id": "1507.00210", "contents": "Title: Natural Neural Networks Abstract: We introduce Natural Neural Networks, a novel family of algorithms that speed\nup convergence by adapting their internal representation during training to\nimprove conditioning of the Fisher matrix. In particular, we show a specific\nexample that employs a simple and efficient reparametrization of the neural\nnetwork weights by implicitly whitening the representation obtained at each\nlayer, while preserving the feed-forward computation of the network. Such\nnetworks can be trained efficiently via the proposed Projected Natural Gradient\nDescent algorithm (PRONG), which amortizes the cost of these reparametrizations\nover many parameter updates and is closely related to the Mirror Descent online\nlearning algorithm. We highlight the benefits of our method on both\nunsupervised and supervised learning tasks, and showcase its scalability by\ntraining on the large-scale ImageNet Challenge dataset. \n\n"}
{"id": "1507.02608", "contents": "Title: High-dimensional consistency in score-based and hybrid structure\n  learning Abstract: Main approaches for learning Bayesian networks can be classified as\nconstraint-based, score-based or hybrid methods. Although high-dimensional\nconsistency results are available for constraint-based methods like the PC\nalgorithm, such results have not been proved for score-based or hybrid methods,\nand most of the hybrid methods have not even shown to be consistent in the\nclassical setting where the number of variables remains fixed and the sample\nsize tends to infinity. In this paper, we show that consistency of hybrid\nmethods based on greedy equivalence search (GES) can be achieved in the\nclassical setting with adaptive restrictions on the search space that depend on\nthe current state of the algorithm. Moreover, we prove consistency of GES and\nadaptively restricted GES (ARGES) in several sparse high-dimensional settings.\nARGES scales well to sparse graphs with thousands of variables and our\nsimulation study indicates that both GES and ARGES generally outperform the PC\nalgorithm. \n\n"}
{"id": "1507.02838", "contents": "Title: Non-strange Weird Resampling for Complex Survival Data Abstract: This paper introduces the new data-dependent multiplier bootstrap for\nnon-parametric analysis of survival data, possibly subject to competing risks.\nThe new resampling procedure includes both the general wild bootstrap and the\nweird bootstrap as special cases. The data may be subject to independent\nright-censoring and left-truncation. We rigorously prove asymptotic correctness\nwhich has in particular been pending for the weird bootstrap. As a consequence,\npointwise as well as time-simultaneous inference procedures for, amongst\nothers, the classical survival setting are deduced. We report simulation\nresults and a real data analysis of the cumulative cardiovascular event\nprobability. The simulation results suggest that both the weird bootstrap and\nuse of non-standard multipliers in the wild bootstrap may perform preferably. \n\n"}
{"id": "1507.03895", "contents": "Title: On consistency and sparsity for sliced inverse regression in high\n  dimensions Abstract: We provide here a framework to analyze the phase transition phenomenon of\nslice inverse regression (SIR), a supervised dimension reduction technique\nintroduced by \\cite{Li:1991}. Under mild conditions, the asymptotic ratio\n$\\rho= \\lim p/n$ is the phase transition parameter and the SIR estimator is\nconsistent if and only if $\\rho= 0$. When dimension $p$ is greater than $n$, we\npropose a diagonal thresholding screening SIR (DT-SIR) algorithm. This method\nprovides us with an estimate of the eigen-space of the covariance matrix of the\nconditional expectation $var(\\mathbf{E}[\\boldsymbol{x}|y])$. The desired\ndimension reduction space is then obtained by multiplying the inverse of the\ncovariance matrix on the eigen-space. Under certain sparsity assumptions on\nboth the covariance matrix of predictors and the loadings of the directions, we\nprove the consistency of DT-SIR in estimating the dimension reduction space in\nhigh dimensional data analysis. Extensive numerical experiments demonstrate\nsuperior performances of the proposed method in comparison to its competitors. \n\n"}
{"id": "1507.04398", "contents": "Title: On the use of reproducing kernel Hilbert spaces in functional\n  classification Abstract: The H\\'ajek-Feldman dichotomy establishes that two Gaussian measures are\neither mutually absolutely continuous with respect to each other (and hence\nthere is a Radon-Nikodym density for each measure with respect to the other\none) or mutually singular. Unlike the case of finite dimensional Gaussian\nmeasures, there are non-trivial examples of both situations when dealing with\nGaussian stochastic processes. This paper provides:\n  (a) Explicit expressions for the optimal (Bayes) rule and the minimal\nclassification error probability in several relevant problems of supervised\nbinary classification of mutually absolutely continuous Gaussian processes. The\napproach relies on some classical results in the theory of Reproducing Kernel\nHilbert Spaces (RKHS).\n  (b) An interpretation, in terms of mutual singularity, for the \"near perfect\nclassification\" phenomenon described by Delaigle and Hall (2012). We show that\nthe asymptotically optimal rule proposed by these authors can be identified\nwith the sequence of optimal rules for an approximating sequence of\nclassification problems in the absolutely continuous case.\n  (c) A new model-based method for variable selection in binary classification\nproblems, which arises in a very natural way from the explicit knowledge of the\nRN-derivatives and the underlying RKHS structure. Different classifiers might\nbe used from the selected variables. In particular, the classical, linear\nfinite-dimensional Fisher rule turns out to be consistent under some standard\nconditions on the underlying functional model. \n\n"}
{"id": "1507.04777", "contents": "Title: Sparse Probit Linear Mixed Model Abstract: Linear Mixed Models (LMMs) are important tools in statistical genetics. When\nused for feature selection, they allow to find a sparse set of genetic traits\nthat best predict a continuous phenotype of interest, while simultaneously\ncorrecting for various confounding factors such as age, ethnicity and\npopulation structure. Formulated as models for linear regression, LMMs have\nbeen restricted to continuous phenotypes. We introduce the Sparse Probit Linear\nMixed Model (Probit-LMM), where we generalize the LMM modeling paradigm to\nbinary phenotypes. As a technical challenge, the model no longer possesses a\nclosed-form likelihood function. In this paper, we present a scalable\napproximate inference algorithm that lets us fit the model to high-dimensional\ndata sets. We show on three real-world examples from different domains that in\nthe setup of binary labels, our algorithm leads to better prediction accuracies\nand also selects features which show less correlation with the confounding\nfactors. \n\n"}
{"id": "1507.05442", "contents": "Title: Large deviations of the maximum of independent and identically\n  distributed random variables Abstract: A pedagogical account of some aspects of Extreme Value Statistics (EVS) is\npresented from the somewhat non-standard viewpoint of Large Deviation Theory.\nWe address the following problem: given a set of $N$ i.i.d. random variables\n$\\{X_1,\\ldots,X_N\\}$ drawn from a parent probability density function (pdf)\n$p(x)$, what is the probability that the maximum value of the set\n$X_{\\mathrm{max}}=\\max_i X_i$ is \"atypically larger\" than expected? The cases\nof exponential and Gaussian distributed variables are worked out in detail, and\nthe right rate function for a general pdf in the Gumbel basin of attraction is\nderived. The Gaussian case convincingly demonstrates that the full rate\nfunction cannot be determined from the knowledge of the limiting distribution\n(Gumbel) alone, thus implying that it indeed carries additional information.\nGiven the simplicity and richness of the result and its derivation, its absence\nfrom textbooks, tutorials and lecture notes on EVS for physicists appears\ninexplicable. \n\n"}
{"id": "1507.06506", "contents": "Title: Brillinger mixing of determinantal point processes and statistical\n  applications Abstract: Stationary determinantal point processes are proved to be Brillinger mixing.\nThis property is an important step towards asymptotic statistics for these\nprocesses. As an important example, a central limit theorem for a wide class of\nfunctionals of determinantal point processes is established. This result yields\nin particular the asymptotic normality of the estimator of the intensity of a\nstationary determinantal point process and of the kernel estimator of its pair\ncorrelation. \n\n"}
{"id": "1507.07412", "contents": "Title: Posterior contraction rates for deconvolution of Dirichlet-Laplace\n  mixtures Abstract: We study nonparametric Bayesian inference with location mixtures of the\nLaplace density and a Dirichlet process prior on the mixing distribution. We\nderive a contraction rate of the corresponding posterior distribution, both for\nthe mixing distribution relative to the Wasserstein metric and for the mixed\ndensity relative to the Hellinger and $L_q$ metrics. \n\n"}
{"id": "1507.07495", "contents": "Title: Estimating an Activity Driven Hidden Markov Model Abstract: We define a Hidden Markov Model (HMM) in which each hidden state has\ntime-dependent $\\textit{activity levels}$ that drive transitions and emissions,\nand show how to estimate its parameters. Our construction is motivated by the\nproblem of inferring human mobility on sub-daily time scales from, for example,\nmobile phone records. \n\n"}
{"id": "1507.08266", "contents": "Title: Strong Consistency of Multivariate Spectral Variance Estimators Abstract: Markov chain Monte Carlo (MCMC) algorithms are used to estimate features of\ninterest of a distribution. The Monte Carlo error in estimation has an\nasymptotic normal distribution whose multivariate nature has so far been\nignored in the MCMC community. We present a class of multivariate spectral\nvariance estimators for the asymptotic covariance matrix in the Markov chain\ncentral limit theorem and provide conditions for strong consistency. We examine\nthe finite sample properties of the multivariate spectral variance estimators\nand its eigenvalues in the context of a vector autoregressive process of order\n1. \n\n"}
{"id": "1507.08689", "contents": "Title: Multiple Outlier Detection in Samples with Exponential & Pareto Tails:\n  Redeeming the Inward Approach & Detecting Dragon Kings Abstract: We introduce two ratio-based robust test statistics, max-robust-sum (MRS) and\nsum-robust-sum (SRS), designed to enhance the robustness of outlier detection\nin samples with exponential or Pareto tails. We also reintroduce the inward\nsequential testing method -- formerly relegated since the introduction of\noutward testing -- and show that MRS and SRS tests reduce susceptibility of the\ninward approach to masking, making the inward test as powerful as, and\npotentially less error-prone than, outward tests. Moreover, inward testing does\nnot require the complicated type I error control of outward tests. A\ncomprehensive comparison of the test statistics is done, considering\nperformance of the proposed tests in both block and sequential tests, and\ncontrasting their performance with classical test statistics across various\ndata scenarios. In five case studies -- financial crashes, nuclear power\ngeneration accidents, stock market returns, epidemic fatalities, and city sizes\n-- significant outliers are detected and related to the concept of `Dragon\nKing' events, defined as meaningful outliers that arise from a unique\ngenerating mechanism. \n\n"}
{"id": "1508.00249", "contents": "Title: Lepski's Method and Adaptive Estimation of Nonlinear Integral\n  Functionals of Density Abstract: We study the adaptive minimax estimation of non-linear integral functionals\nof a density and extend the results obtained for linear and quadratic\nfunctionals to general functionals. The typical rate optimal non-adaptive\nminimax estimators of \"smooth\" non-linear functionals are higher order\nU-statistics. Since Lepski's method requires tight control of tails of such\nestimators, we bypass such calculations by a modification of Lepski's method\nwhich is applicable in such situations. As a necessary ingredient, we also\nprovide a method to control higher order moments of minimax estimator of cubic\nintegral functionals. Following a standard constrained risk inequality method,\nwe also show the optimality of our adaptation rates. \n\n"}
{"id": "1508.01928", "contents": "Title: A variational approach to the consistency of spectral clustering Abstract: This paper establishes the consistency of spectral approaches to data\nclustering. We consider clustering of point clouds obtained as samples of a\nground-truth measure. A graph representing the point cloud is obtained by\nassigning weights to edges based on the distance between the points they\nconnect. We investigate the spectral convergence of both unnormalized and\nnormalized graph Laplacians towards the appropriate operators in the continuum\ndomain. We obtain sharp conditions on how the connectivity radius can be scaled\nwith respect to the number of sample points for the spectral convergence to\nhold.\n  We also show that the discrete clusters obtained via spectral clustering\nconverge towards a continuum partition of the ground truth measure. Such\ncontinuum partition minimizes a functional describing the continuum analogue of\nthe graph-based spectral partitioning. Our approach, based on variational\nconvergence, is general and flexible. \n\n"}
{"id": "1508.02201", "contents": "Title: Extrinsic local regression on manifold-valued data Abstract: We propose an extrinsic regression framework for modeling data with manifold\nvalued responses and Euclidean predictors. Regression with manifold responses\nhas wide applications in shape analysis, neuroscience, medical imaging and many\nother areas. Our approach embeds the manifold where the responses lie onto a\nhigher dimensional Euclidean space, obtains a local regression estimate in that\nspace, and then projects this estimate back onto the image of the manifold.\nOutside the regression setting both intrinsic and extrinsic approaches have\nbeen proposed for modeling i.i.d manifold-valued data. However, to our\nknowledge our work is the first to take an extrinsic approach to the regression\nproblem. The proposed extrinsic regression framework is general,\ncomputationally efficient and theoretically appealing. Asymptotic distributions\nand convergence rates of the extrinsic regression estimates are derived and a\nlarge class of examples are considered indicating the wide applicability of our\napproach. \n\n"}
{"id": "1508.03387", "contents": "Title: Optimal approximating Markov chains for Bayesian inference Abstract: The Markov Chain Monte Carlo method is the dominant paradigm for posterior\ncomputation in Bayesian analysis. It is common to control computation time by\nmaking approximations to the Markov transition kernel. Comparatively little\nattention has been paid to computational optimality in these approximating\nMarkov Chains, or when such approximations are justified relative to obtaining\nshorter paths from the exact kernel. We give simple, sharp bounds for uniform\napproximations of uniformly mixing Markov chains. We then suggest a notion of\noptimality that incorporates computation time and approximation error, and use\nour bounds to make generalizations about properties of good approximations in\nthe uniformly mixing setting. The relevance of these properties is demonstrated\nin applications to a minibatching-based approximate MCMC algorithm for large\n$n$ logistic regression and low-rank approximations for Gaussian processes. \n\n"}
{"id": "1508.03758", "contents": "Title: Bayesian Mixture Models With Focused Clustering for Mixed Ordinal and\n  Nominal Data Abstract: In some contexts, mixture models can fit certain variables well at the\nexpense of others in ways beyond the analyst's control. For example, when the\ndata include some variables with non-trivial amounts of missing values, the\nmixture model may fit the marginal distributions of the nearly and fully\ncomplete variables at the expense of the variables with high fractions of\nmissing data. Motivated by this setting, we present a mixture model for mixed\nordinal and nominal data that splits variables into two groups, focus variables\nand remainder variables. The model allows the analyst to specify a rich\nsub-model for the focus variables and a simpler sub-model for remainder\nvariables, yet still capture associations among the variables. Using\nsimulations, we illustrate advantages and limitations of focused clustering\ncompared to mixture models that do not distinguish variables. We apply the\nmodel to handle missing values in an analysis of the 2012 American National\nElection Study, estimating relationships among voting behavior, ideology, and\npolitical party affiliation. \n\n"}
{"id": "1508.04812", "contents": "Title: Multivariate Density Estimation via Adaptive Partitioning (II):\n  Posterior Concentration Abstract: In this paper, we study a class of non-parametric density estimators under\nBayesian settings. The estimators are piecewise constant functions on binary\npartitions. We analyze the concentration rate of the posterior distribution\nunder a suitable prior, and demonstrate that the rate does not directly depend\non the dimension of the problem. This paper can be viewed as an extension of a\nparallel work where the convergence rate of a related sieve MLE was\nestablished. Compared to the sieve MLE, the main advantage of the Bayesian\nmethod is that it can adapt to the unknown complexity of the true density\nfunction, thus achieving the optimal convergence rate without artificial\nconditions on the density. \n\n"}
{"id": "1508.06025", "contents": "Title: Strong data-processing inequalities for channels and Bayesian networks Abstract: The data-processing inequality, that is, $I(U;Y) \\le I(U;X)$ for a Markov\nchain $U \\to X \\to Y$, has been the method of choice for proving impossibility\n(converse) results in information theory and many other disciplines. Various\nchannel-dependent improvements (called strong data-processing inequalities, or\nSDPIs) of this inequality have been proposed both classically and more\nrecently. In this note we first survey known results relating various notions\nof contraction for a single channel. Then we consider the basic extension:\ngiven SDPI for each constituent channel in a Bayesian network, how to produce\nan end-to-end SDPI?\n  Our approach is based on the (extract of the) Evans-Schulman method, which is\ndemonstrated for three different kinds of SDPIs, namely, the usual\nAhslwede-G\\'acs type contraction coefficients (mutual information), Dobrushin's\ncontraction coefficients (total variation), and finally the $F_I$-curve (the\nbest possible non-linear SDPI for a given channel). Resulting bounds on the\ncontraction coefficients are interpreted as probability of site percolation. As\nan example, we demonstrate how to obtain SDPI for an $n$-letter memoryless\nchannel with feedback given an SDPI for $n=1$.\n  Finally, we discuss a simple observation on the equivalence of a linear SDPI\nand comparison to an erasure channel (in the sense of \"less noisy\" order). This\nleads to a simple proof of a curious inequality of Samorodnitsky (2015), and\nsheds light on how information spreads in the subsets of inputs of a memoryless\nchannel. \n\n"}
{"id": "1508.07929", "contents": "Title: On the contraction properties of some high-dimensional quasi-posterior\n  distributions Abstract: We study the contraction properties of a quasi-posterior distribution\n$\\check\\Pi_{n,d}$ obtained by combining a quasi-likelihood function and a\nsparsity inducing prior distribution on $\\rset^d$, as both $n$ (the sample\nsize), and $d$ (the dimension of the parameter) increase. We derive some\ngeneral results that highlight a set of sufficient conditions under which\n$\\check\\Pi_{n,d}$ puts increasingly high probability on sparse subsets of\n$\\rset^d$, and contracts towards the true value of the parameter. We apply\nthese results to the analysis of logistic regression models, and binary\ngraphical models, in high-dimensional settings. For the logistic regression\nmodel, we shows that for well-behaved design matrices, the posterior\ndistribution contracts at the rate $O(\\sqrt{s_\\star\\log(d)/n})$, where\n$s_\\star$ is the number of non-zero components of the parameter. For the binary\ngraphical model, under some regularity conditions, we show that a\nquasi-posterior analog of the neighborhood selection of \\cite{meinshausen06}\ncontracts in the Frobenius norm at the rate $O(\\sqrt{(p+S)\\log(p)/n})$, where\n$p$ is the number of nodes, and $S$ the number of edges of the true graph. \n\n"}
{"id": "1509.01877", "contents": "Title: On Degrees of Freedom of Projection Estimators with Applications to\n  Multivariate Nonparametric Regression Abstract: In this paper, we consider the nonparametric regression problem with\nmultivariate predictors. We provide a characterization of the degrees of\nfreedom and divergence for estimators of the unknown regression function, which\nare obtained as outputs of linearly constrained quadratic optimization\nprocedures, namely, minimizers of the least squares criterion with linear\nconstraints and/or quadratic penalties. As special cases of our results, we\nderive explicit expressions for the degrees of freedom in many nonparametric\nregression problems, e.g., bounded isotonic regression, multivariate\n(penalized) convex regression, and additive total variation regularization. Our\ntheory also yields, as special cases, known results on the degrees of freedom\nof many well-studied estimators in the statistics literature, such as ridge\nregression, Lasso and generalized Lasso. Our results can be readily used to\nchoose the tuning parameter(s) involved in the estimation procedure by\nminimizing the Stein's unbiased risk estimate. As a by-product of our analysis\nwe derive an interesting connection between bounded isotonic regression and\nisotonic regression on a general partially ordered set, which is of independent\ninterest. \n\n"}
{"id": "1509.05017", "contents": "Title: Asymptotic Theory for Kernel Estimators under Moderate Deviations from a\n  Unit Root, with an Application to the Asymptotic Size of Nonparametric Tests Abstract: We provide new asymptotic theory for kernel density estimators, when these\nare applied to autoregressive processes exhibiting moderate deviations from a\nunit root. This fills a gap in the existing literature, which has to date\nconsidered only nearly integrated and stationary autoregressive processes.\nThese results have applications to nonparametric predictive regression models.\nIn particular, we show that the null rejection probability of a nonparametric t\ntest is controlled uniformly in the degree of persistence of the regressor.\nThis provides a rigorous justification for the validity of the usual\nnonparametric inferential procedures, even in cases where regressors may be\nhighly persistent. \n\n"}
{"id": "1509.05453", "contents": "Title: Graph Estimation for Matrix-variate Gaussian Data Abstract: Matrix-variate Gaussian graphical models (GGM) have been widely used for\nmodeling matrix-variate data. Since the support of sparse precision matrix\nrepresents the conditional independence graph among matrix entries, conducting\nsupport recovery yields valuable information. A commonly used approach is the\npenalized log-likelihood method. However, due to the complicated structure of\nprecision matrices in the form of Kronecker product, the log-likelihood is\nnon-convex, which presents challenges for both computation and theoretical\nanalysis. In this paper, we propose an alternative approach by formulating the\nsupport recovery problem into a multiple testing problem. A new test statistic\nis developed and based on that, we use the popular Benjamini and Hochberg's\nprocedure to control false discovery rate (FDR) asymptotically. Our method\ninvolves only convex optimization, making it computationally attractive.\nTheoretically, our method allows very weak conditions, i.e., even when the\nsample size is finite and the dimensions go to infinity, the asymptotic\nnormality of the test statistics and FDR control can still be guaranteed. We\nfurther provide the power analysis result. The finite sample performance of the\nproposed method is illustrated by both simulated and real data analysis. \n\n"}
{"id": "1509.06422", "contents": "Title: Quasi-MLE for quadratic ARCH model with long memory Abstract: We discuss parametric quasi-maximum likelihood estimation for quadratic ARCH\nprocess with long memory introduced in Doukhan et al. (2015) and Grublyt\\.e and\n\\v{S}karnulis (2015) with conditional variance given by a strictly positive\nquadratic form of observable stationary sequence. We prove consistency and\nasymptotic normality of the corresponding QMLE estimates, including the\nestimate of long memory parameter $0< d < 1/2$. A simulation study of empirical\nMSE is included. \n\n"}
{"id": "1509.07229", "contents": "Title: High-dimensional robust precision matrix estimation: Cellwise corruption\n  under $\\epsilon$-contamination Abstract: We analyze the statistical consistency of robust estimators for precision\nmatrices in high dimensions. We focus on a contamination mechanism acting\ncellwise on the data matrix. The estimators we analyze are formed by plugging\nappropriately chosen robust covariance matrix estimators into the graphical\nLasso and CLIME. Such estimators were recently proposed in the robust\nstatistics literature, but only analyzed mathematically from the point of view\nof the breakdown point. This paper provides complementary high-dimensional\nerror bounds for the precision matrix estimators that reveal the interplay\nbetween the dimensionality of the problem and the degree of contamination\npermitted in the observed distribution. We also show that although the\ngraphical Lasso and CLIME estimators perform equally well from the point of\nview of statistical consistency, the breakdown property of the graphical Lasso\nis superior to that of CLIME. We discuss implications of our work for problems\ninvolving graphical model estimation when the uncontaminated data follow a\nmultivariate normal distribution, and the goal is to estimate the support of\nthe population-level precision matrix. Our error bounds do not make any\nassumptions about the the contaminating distribution and allow for a\nnonvanishing fraction of cellwise contamination. \n\n"}
{"id": "1509.08083", "contents": "Title: Non-asymptotic Analysis of $\\ell_1$-norm Support Vector Machines Abstract: Support Vector Machines (SVM) with $\\ell_1$ penalty became a standard tool in\nanalysis of highdimensional classification problems with sparsity constraints\nin many applications including bioinformatics and signal processing. Although\nSVM have been studied intensively in the literature, this paper has to our\nknowledge first non-asymptotic results on the performance of $\\ell_1$-SVM in\nidentification of sparse classifiers. We show that a $d$-dimensional $s$-sparse\nclassification vector can be (with high probability) well approximated from\nonly $O(s\\log(d))$ Gaussian trials. The methods used in the proof include\nconcentration of measure and probability in Banach spaces. \n\n"}
{"id": "1510.01948", "contents": "Title: An Optimal Transport Formulation of the Linear Feedback Particle Filter Abstract: Feedback particle filter (FPF) is an algorithm to numerically approximate the\nsolution of the nonlinear filtering problem in continuous time. The algorithm\nimplements a feedback control law for a system of particles such that the\nempirical distribution of particles approximates the posterior distribution.\nHowever, it has been noted in the literature that the feedback control law is\nnot unique. To find a unique control law, the filtering task is formulated here\nas an optimal transportation problem between the prior and the posterior\ndistributions. Based on this formulation, a time stepping optimization\nprocedure is proposed for the optimal control design. A key difference between\nthe optimal control law and the one in the original FPF, is the replacement of\nnoise term with a deterministic term. This difference serves to decreases the\nsimulation variance, as illustrated with a simple numerical example. \n\n"}
{"id": "1510.02451", "contents": "Title: The Bouncy Particle Sampler: A Non-Reversible Rejection-Free Markov\n  Chain Monte Carlo Method Abstract: Markov chain Monte Carlo methods have become standard tools in statistics to\nsample from complex probability measures. Many available techniques rely on\ndiscrete-time reversible Markov chains whose transition kernels build up over\nthe Metropolis-Hastings algorithm. We explore and propose several original\nextensions of an alternative approach introduced recently in Peters and de With\n(2012) where the target distribution of interest is explored using a\ncontinuous-time Markov process. In the Metropolis-Hastings algorithm, a trial\nmove to a region of lower target density, equivalently \"higher energy\", than\nthe current state can be rejected with positive probability. In this\nalternative approach, a particle moves along straight lines continuously around\nthe space and, when facing a high energy barrier, it is not rejected but its\npath is modified by bouncing against this barrier. The resulting non-reversible\nMarkov process provides a rejection-free MCMC sampling scheme. We propose\nseveral original techniques to simulate this continuous-time process exactly in\na wide range of scenarios of interest to statisticians. When the target\ndistribution factorizes as a product of factors involving only subsets of\nvariables, such as the posterior distribution associated to a probabilistic\ngraphical model, it is possible to modify the original algorithm to exploit\nthis structure and update in parallel variables within each clique. We present\nseveral extensions by proposing methods to sample mixed discrete-continuous\ndistributions and distributions restricted to a connected smooth domain. We\nalso show that it is possible to move the particle using a general flow instead\nof straight lines. We demonstrate the efficiency of this methodology through\nsimulations on a variety of applications and show that it can outperform Hybrid\nMonte Carlo schemes in interesting scenarios. \n\n"}
{"id": "1510.03542", "contents": "Title: Heteroscedasticity Testing for Regression Models: A Dimension\n  Reduction-based Model Adaptive Abstract: Heteroscedasticity testing is of importance in regression analysis. Existing\nlocal smoothing tests suffer severely from curse of dimensionality even when\nthe number of covariates is moderate because of use of nonparametric\nestimation. In this paper, a dimension reduction-based model adaptive test is\nproposed which behaves like a local smoothing test as if the number of\ncovariates were equal to the number of their linear combinations in the mean\nregression function, in particular, equal to 1 when the mean function contains\na single index. The test statistic is asymptotically normal under the null\nhypothesis such that critical values are easily determined. The finite sample\nperformances of the test are examined by simulations and a real data analysis. \n\n"}
{"id": "1510.04342", "contents": "Title: Estimation and Inference of Heterogeneous Treatment Effects using Random\n  Forests Abstract: Many scientific and engineering challenges -- ranging from personalized\nmedicine to customized marketing recommendations -- require an understanding of\ntreatment effect heterogeneity. In this paper, we develop a non-parametric\ncausal forest for estimating heterogeneous treatment effects that extends\nBreiman's widely used random forest algorithm. In the potential outcomes\nframework with unconfoundedness, we show that causal forests are pointwise\nconsistent for the true treatment effect, and have an asymptotically Gaussian\nand centered sampling distribution. We also discuss a practical method for\nconstructing asymptotic confidence intervals for the true treatment effect that\nare centered at the causal forest estimates. Our theoretical results rely on a\ngeneric Gaussian theory for a large family of random forest algorithms. To our\nknowledge, this is the first set of results that allows any type of random\nforest, including classification and regression forests, to be used for\nprovably valid statistical inference. In experiments, we find causal forests to\nbe substantially more powerful than classical methods based on nearest-neighbor\nmatching, especially in the presence of irrelevant covariates. \n\n"}
{"id": "1510.04514", "contents": "Title: Mixture Models: Building a Parameter Space Abstract: Despite the flexibility and popularity of mixture models, their associated\nparameter spaces are often difficult to represent due to fundamental\nidentification problems. This paper looks at a novel way of representing such a\nspace for general mixtures of exponential families, where the parameters are\nidentifiable, interpretable, and, due to a tractable geometric structure, the\nspace allows fast computational algorithms to be constructed. \n\n"}
{"id": "1510.04654", "contents": "Title: Moment Varieties of Gaussian Mixtures Abstract: The points of a moment variety are the vectors of all moments up to some\norder of a family of probability distributions. We study this variety for\nmixtures of Gaussians. Following up on Pearson's classical work from 1894, we\napply current tools from computational algebra to recover the parameters from\nthe moments. Our moment varieties extend objects familiar to algebraic\ngeometers. For instance, the secant varieties of Veronese varieties are the\nloci obtained by setting all covariance matrices to zero. We compute the ideals\nof the 5-dimensional moment varieties representing mixtures of two univariate\nGaussians, and we offer a comparison to the maximum likelihood approach. \n\n"}
{"id": "1510.07123", "contents": "Title: CoCoLasso for High-dimensional Error-in-variables Regression Abstract: Much theoretical and applied work has been devoted to high-dimensional\nregression with clean data. However, we often face corrupted data in many\napplications where missing data and measurement errors cannot be ignored. Loh\nand Wainwright (2012) proposed a non-convex modification of the Lasso for doing\nhigh-dimensional regression with noisy and missing data. It is generally agreed\nthat the virtues of convexity contribute fundamentally the success and\npopularity of the Lasso. In light of this, we propose a new method named\nCoCoLasso that is convex and can handle a general class of corrupted datasets\nincluding the cases of additive measurement error and random missing data. We\nestablish the estimation error bounds of CoCoLasso and its asymptotic\nsign-consistent selection property. We further elucidate how the standard cross\nvalidation techniques can be misleading in presence of measurement error and\ndevelop a novel corrected cross-validation technique by using the basic idea in\nCoCoLasso. The corrected cross-validation has its own importance. We\ndemonstrate the superior performance of our method over the non-convex approach\nby simulation studies. \n\n"}
{"id": "1511.01975", "contents": "Title: Persistence of centrality in random growing trees Abstract: We investigate properties of node centrality in random growing tree models.\nWe focus on a measure of centrality that computes the maximum subtree size of\nthe tree rooted at each node, with the most central node being the tree\ncentroid. For random trees grown according to a preferential attachment model,\na uniform attachment model, or a diffusion processes over a regular tree, we\nprove that a single node persists as the tree centroid after a finite number of\nsteps, with probability 1. Furthermore, this persistence property generalizes\nto the top $K \\ge 1$ nodes with respect to the same centrality measure. We also\nestablish necessary and sufficient conditions for the size of an initial seed\ngraph required to ensure persistence of a particular node with probability\n$1-\\epsilon$, as a function of $\\epsilon$: In the case of preferential and\nuniform attachment models, we derive bounds for the size of an initial hub\nconstructed around the special node. In the case of a diffusion process over a\nregular tree, we derive bounds for the radius of an initial ball centered\naround the special node. Our necessary and sufficient conditions match up to\nconstant factors for preferential attachment and diffusion tree models. \n\n"}
{"id": "1511.06259", "contents": "Title: Robust dimension-free Gram operator estimates Abstract: In this paper we investigate the question of estimating the Gram operator by\na robust estimator from an i.i.d. sample in a separable Hilbert space and we\npresent uniform bounds that hold under weak moment assumptions. The approach\nconsists in first obtaining non-asymptotic dimension-free bounds in\nfinite-dimensional spaces using some PAC-Bayesian inequalities related to\nGaussian perturbations of the parameter and then in generalizing the results in\na separable Hilbert space. We show both from a theoretical point of view and\nwith the help of some simulations that such a robust estimator improves the\nbehavior of the classical empirical one in the case of heavy tail data\ndistributions. \n\n"}
{"id": "1511.06813", "contents": "Title: Smooth, identifiable supermodels of discrete DAG models with latent\n  variables Abstract: We provide a parameterization of the discrete nested Markov model, which is a\nsupermodel that approximates DAG models (Bayesian network models) with latent\nvariables. Such models are widely used in causal inference and machine\nlearning. We explicitly evaluate their dimension, show that they are curved\nexponential families of distributions, and fit them to data. The\nparameterization avoids the irregularities and unidentifiability of latent\nvariable models. The parameters used are all fully identifiable and\ncausally-interpretable quantities. \n\n"}
{"id": "1511.07098", "contents": "Title: Metric Entropy estimation using o-minimality Theory Abstract: It is shown how tools from the area of Model Theory, specifically from the\nTheory of o-minimality, can be used to prove that a class of functions is\nVC-subgraph (in the sense of Dudley, 1987), and therefore satisfies a uniform\npolynomial metric entropy bound. We give examples where the use of these\nmethods significantly improves the existing metric entropy bounds. The methods\nproposed here can be applied to finite dimensional parametric families of\nfunctions without the need for the parameters to live in a compact set, as is\nsometimes required in theorems that produce similar entropy bounds (for\ninstance Theorem 19.7 of van der Vaart, 1998). \n\n"}
{"id": "1511.08327", "contents": "Title: Random Forests for Big Data Abstract: Big Data is one of the major challenges of statistical science and has\nnumerous consequences from algorithmic and theoretical viewpoints. Big Data\nalways involve massive data but they also often include online data and data\nheterogeneity. Recently some statistical methods have been adapted to process\nBig Data, like linear regression models, clustering methods and bootstrapping\nschemes. Based on decision trees combined with aggregation and bootstrap ideas,\nrandom forests were introduced by Breiman in 2001. They are a powerful\nnonparametric statistical method allowing to consider in a single and versatile\nframework regression problems, as well as two-class and multi-class\nclassification problems. Focusing on classification problems, this paper\nproposes a selective review of available proposals that deal with scaling\nrandom forests to Big Data problems. These proposals rely on parallel\nenvironments or on online adaptations of random forests. We also describe how\nrelated quantities -- such as out-of-bag error and variable importance -- are\naddressed in these methods. Then, we formulate various remarks for random\nforests in the Big Data context. Finally, we experiment five variants on two\nmassive datasets (15 and 120 millions of observations), a simulated one as well\nas real world data. One variant relies on subsampling while three others are\nrelated to parallel implementations of random forests and involve either\nvarious adaptations of bootstrap to Big Data or to \"divide-and-conquer\"\napproaches. The fifth variant relates on online learning of random forests.\nThese numerical experiments lead to highlight the relative performance of the\ndifferent variants, as well as some of their limitations. \n\n"}
{"id": "1511.08893", "contents": "Title: Degradable channels, less noisy channels, and quantum statistical\n  morphisms: an equivalence relation Abstract: Two partial orderings among communication channels, namely, `being degradable\ninto' and `being less noisy than,' are reconsidered in the light of recent\nresults about statistical comparisons of quantum channels. Though our analysis\ncovers at once both classical and quantum channels, we also provide a separate\ntreatment of classical noisy channels, and show how, in this case, an\nalternative self-contained proof can be constructed, with its own particular\nmerits with respect to the general result. \n\n"}
{"id": "1511.09078", "contents": "Title: Group SLOPE - adaptive selection of groups of predictors Abstract: Sorted L-One Penalized Estimation is a relatively new convex optimization\nprocedure which allows for adaptive selection of regressors under sparse high\ndimensional designs. Here we extend the idea of SLOPE to deal with the\nsituation when one aims at selecting whole groups of explanatory variables\ninstead of single regressors. This approach is particularly useful when\nvariables in the same group are strongly correlated and thus true predictors\nare difficult to distinguish from their correlated \"neighbors\"'. We formulate\nthe respective convex optimization problem, gSLOPE (group SLOPE), and propose\nan efficient algorithm for its solution. We also define a notion of the group\nfalse discovery rate (gFDR) and provide a choice of the sequence of tuning\nparameters for gSLOPE so that gFDR is provably controlled at a prespecified\nlevel if the groups of variables are orthogonal to each other. Moreover, we\nprove that the resulting procedure adapts to unknown sparsity and is\nasymptotically minimax with respect to the estimation of the proportions of\nvariance of the response variable explained by regressors from different\ngroups. We also provide a method for the choice of the regularizing sequence\nwhen variables in different groups are not orthogonal but statistically\nindependent and illustrate its good properties with computer simulations. \n\n"}
{"id": "1512.02303", "contents": "Title: The $f$-Sensitivity Index Abstract: This article presents a general multivariate $f$-sensitivity index, rooted in\nthe $f$-divergence between the unconditional and conditional probability\nmeasures of a stochastic response, for global sensitivity analysis. Unlike the\nvariance-based Sobol index, the $f$-sensitivity index is applicable to random\ninput following dependent as well as independent probability distributions.\nSince the class of $f$-divergences supports a wide variety of divergence or\ndistance measures, a plethora of $f$-sensitivity indices are possible,\naffording diverse choices to sensitivity analysis. Commonly used sensitivity\nindices or measures, such as mutual information, squared-loss mutual\ninformation, and Borgonovo's importance measure, are shown to be special cases\nof the proposed sensitivity index. New theoretical results, revealing\nfundamental properties of the $f$-sensitivity index and establishing important\ninequalities, are presented. Three new approximate methods, depending on how\nthe probability densities of a stochastic response are determined, are proposed\nto estimate the sensitivity index. Four numerical examples, including a\ncomputationally intensive stochastic boundary-value problem, illustrate these\nmethods and explain when one method is more relevant than the others. \n\n"}
{"id": "1512.03479", "contents": "Title: Optimal Adaptive Inference in Random Design Binary Regression Abstract: We construct confidence sets for the regression function in nonparametric\nbinary regression with an unknown design density. These confidence sets are\nadaptive in $L^2$ loss over a continuous class of Sobolev type spaces.\nAdaptation holds in the smoothness of the regression function, over the maximal\nparameter spaces where adaptation is possible, provided the design density is\nsmooth enough. We identify two key regimes --- one where adaptation is\npossible, and one where some critical regions must be removed. We address\nrelated questions about goodness of fit testing and adaptive estimation of\nrelevant parameters. \n\n"}
{"id": "1512.04093", "contents": "Title: Multiple Change-point Detection: a Selective Overview Abstract: Very long and noisy sequence data arise from biological sciences to social\nscience including high throughput data in genomics and stock prices in\neconometrics. Often such data are collected in order to identify and understand\nshifts in trend, e.g., from a bull market to a bear market in finance or from a\nnormal number of chromosome copies to an excessive number of chromosome copies\nin genetics. Thus, identifying multiple change points in a long, possibly very\nlong, sequence is an important problem. In this article, we review both\nclassical and new multiple change-point detection strategies. Considering the\nlong history and the extensive literature on the change-point detection, we\nprovide an in-depth discussion on a normal mean change-point model from aspects\nof regression analysis, hypothesis testing, consistency and inference. In\nparticular, we present a strategy to gather and aggregate local information for\nchange-point detection that has become the cornerstone of several emerging\nmethods because of its attractiveness in both computational and theoretical\nproperties. \n\n"}
{"id": "1512.07797", "contents": "Title: The Lov\\'asz Hinge: A Novel Convex Surrogate for Submodular Losses Abstract: Learning with non-modular losses is an important problem when sets of\npredictions are made simultaneously. The main tools for constructing convex\nsurrogate loss functions for set prediction are margin rescaling and slack\nrescaling. In this work, we show that these strategies lead to tight convex\nsurrogates iff the underlying loss function is increasing in the number of\nincorrect predictions. However, gradient or cutting-plane computation for these\nfunctions is NP-hard for non-supermodular loss functions. We propose instead a\nnovel surrogate loss function for submodular losses, the Lov\\'asz hinge, which\nleads to O(p log p) complexity with O(p) oracle accesses to the loss function\nto compute a gradient or cutting-plane. We prove that the Lov\\'asz hinge is\nconvex and yields an extension. As a result, we have developed the first\ntractable convex surrogates in the literature for submodular losses. We\ndemonstrate the utility of this novel convex surrogate through several set\nprediction tasks, including on the PASCAL VOC and Microsoft COCO datasets. \n\n"}
{"id": "1512.09052", "contents": "Title: Model-based testing for space-time interaction using point processes: An\n  application to psychiatric hospital admissions in an urban area Abstract: Spatio-temporal interaction is inherent to cases of infectious diseases and\noccurrences of earthquakes, whereas the spread of other events, such as cancer\nor crime, is less evident. Statistical significance tests of space-time\nclustering usually assess the correlation between the spatial and temporal\n(transformed) distances of the events. Although appealing through simplicity,\nthese classical tests do not adjust for the underlying population nor can they\naccount for a distance decay of interaction. We propose to use the framework of\nan endemic-epidemic point process model to jointly estimate a background event\nrate explained by seasonal and areal characteristics, as well as a superposed\nepidemic component representing the hypothesis of interest. We illustrate this\nnew model-based test for space-time interaction by analysing psychiatric\ninpatient admissions in Zurich, Switzerland (2007-2012). Several socio-economic\nfactors were found to be associated with the admission rate, but there was no\nevidence of general clustering of the cases. \n\n"}
{"id": "1512.09300", "contents": "Title: Autoencoding beyond pixels using a learned similarity metric Abstract: We present an autoencoder that leverages learned representations to better\nmeasure similarities in data space. By combining a variational autoencoder with\na generative adversarial network we can use learned feature representations in\nthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,\nwe replace element-wise errors with feature-wise errors to better capture the\ndata distribution while offering invariance towards e.g. translation. We apply\nour method to images of faces and show that it outperforms VAEs with\nelement-wise similarity measures in terms of visual fidelity. Moreover, we show\nthat the method learns an embedding in which high-level abstract visual\nfeatures (e.g. wearing glasses) can be modified using simple arithmetic. \n\n"}
{"id": "1601.00142", "contents": "Title: Joint Estimation of Precision Matrices in Heterogeneous Populations Abstract: We introduce a general framework for estimation of inverse covariance, or\nprecision, matrices from heterogeneous populations. The proposed framework uses\na Laplacian shrinkage penalty to encourage similarity among estimates from\ndisparate, but related, subpopulations, while allowing for differences among\nmatrices. We propose an efficient alternating direction method of multipliers\n(ADMM) algorithm for parameter estimation, as well as its extension for faster\ncomputation in high dimensions by thresholding the empirical covariance matrix\nto identify the joint block diagonal structure in the estimated precision\nmatrices. We establish both variable selection and norm consistency of the\nproposed estimator for distributions with exponential or polynomial tails.\nFurther, to extend the applicability of the method to the settings with unknown\npopulations structure, we propose a Laplacian penalty based on hierarchical\nclustering, and discuss conditions under which this data-driven choice results\nin consistent estimation of precision matrices in heterogenous populations.\nExtensive numerical studies and applications to gene expression data from\nsubtypes of cancer with distinct clinical outcomes indicate the potential\nadvantages of the proposed method over existing approaches. \n\n"}
{"id": "1601.01082", "contents": "Title: Model comparison for generalized linear models with dependent\n  observations Abstract: The stochastic expansion of the marginal quasi-likelihood function associated\nwith a class of generalized linear models is shown. Based on the expansion, a\nquasi-Bayesian information criterion is proposed that is able to deal with\nmisspecified models and dependent data, resulting in a theoretical extension of\nthe classical Schwarz's Bayesian information criterion. It is also proved that\nthe proposed criterion has model selection consistency with respect to the\noptimal model. Some illustrative numerical examples and a real data example are\npresented. \n\n"}
{"id": "1601.01452", "contents": "Title: Bayes meet Riemann- Bayesian Characterization of Infinite Series with\n  Application to Riemann Hypothesis Abstract: In the classical literature on infinite series there are various tests to\ndetermine if a given infinite series converges, diverges, or oscillates. But\nunfortunately, for very many infinite series all the existing tests can fail to\nprovide definitive answers. In this article we propose a novel Bayesian theory\nfor assessment of convergence properties of any given infinite series.\nRemarkably, this theory attempts to provide conclusive answers to the question\nof convergence even where all the existing tests of convergence fail. We apply\nour ideas to seven different examples, obtaining very encouraging results.\nImportantly, we also apply our ideas to investigate the Riemann Hypothesis, and\nobtain results that do not completely support the conjecture.\n  We also extend our ideas to develop a Bayesian theory on oscillating series,\nwhere we allow even infinite number of limit points. Analysis of Riemann\nHypothesis using Bayesian multiple limit points theory yielded almost identical\nresults as the Bayesian theory of convergence assessment. \n\n"}
{"id": "1601.02177", "contents": "Title: Optimal-order bounds on the rate of convergence to normality for maximum\n  likelihood estimators Abstract: It is well known that under general regularity conditions the distribution of\nthe maximum likelihood estimator (MLE) is asymptotically normal. Very recently,\nbounds of the optimal order $O(1/\\sqrt n)$ on the closeness of the distribution\nof the MLE to normality in the so-called bounded Wasserstein distance were\nobtained, where $n$ is the sample size. However, the corresponding bounds on\nthe Kolmogorov distance were only of the order $O(1/n^{1/4})$. In this note,\nbounds of the optimal order $O(1/\\sqrt n)$ on the closeness of the distribution\nof the MLE to normality in the Kolmogorov distance are given, as well as their\nnonuniform counterparts, which work better for large deviations of the MLE.\nThese results are based on previously obtained general optimal-order bounds on\nthe rate of convergence to normality in the multivariate delta method. The\ncrucial observation is that, under natural conditions, the MLE can be tightly\nenough bracketed between two smooth enough functions of the sum of independent\nrandom vectors, which makes the delta method applicable. \n\n"}
{"id": "1601.02844", "contents": "Title: Adaptive global thresholding on the sphere Abstract: This work is concerned with the study of the adaptivity properties of\nnonparametric regression estimators over the $d$-dimensional sphere within the\nglobal thresholding framework. The estimators are constructed by means of a\nform of spherical wavelets, the so-called needlets, which enjoy strong\nconcentration properties in both harmonic and real domains. The author\nestablishes the convergence rates of the $L^p$-risks of these estimators,\nfocussing on their minimax properties and proving their optimality over a scale\nof nonparametric regularity function spaces, namely, the Besov spaces. \n\n"}
{"id": "1601.03524", "contents": "Title: Degrees of Freedom for Piecewise Lipschitz Estimators Abstract: A representation of the degrees of freedom akin to Stein's lemma is given for\na class of estimators of a mean value parameter in $\\mathbb{R}^n$. Contrary to\nprevious results our representation holds for a range of discontinues\nestimators. It shows that even though the discontinuities form a Lebesgue null\nset, they cannot be ignored when computing degrees of freedom. Estimators with\ndiscontinuities arise naturally in regression if data driven variable selection\nis used. Two such examples, namely best subset selection and lasso-OLS, are\nconsidered in detail in this paper. For lasso-OLS the general representation\nleads to an estimate of the degrees of freedom based on the lasso solution\npath, which in turn can be used for estimating the risk of lasso-OLS. A similar\nestimate is proposed for best subset selection. The usefulness of the risk\nestimates for selecting the number of variables is demonstrated via simulations\nwith a particular focus on lasso-OLS. \n\n"}
{"id": "1601.05261", "contents": "Title: Statistical inference for expectile-based risk measures Abstract: Expectiles were introduced by Newey and Powell (1987) in the context of\nlinear regression models. Recently, Bellini et al. (2014) revealed that\nexpectiles can also be seen as reasonable law-invariant risk measures. In this\narticle, we show that the corresponding statistical functionals are continuous\nw.r.t. the $1$-weak topology and suitably functionally differentiable. By means\nof these regularity results we can derive several properties as consistency,\nasymptotic normality, bootstrap consistency, and qualitative robustness of the\ncorresponding estimators in nonparametric and parametric statistical models. \n\n"}
{"id": "1601.05388", "contents": "Title: Bayesian inference of natural selection from allele frequency time\n  series Abstract: The advent of accessible ancient DNA technology now allows the direct\nascertainment of allele frequencies in ancestral populations, thereby enabling\nthe use of allele frequency time series to detect and estimate natural\nselection. Such direct observations of allele frequency dynamics are expected\nto be more powerful than inferences made using patterns of linked neutral\nvariation obtained from modern individuals. We develop a Bayesian method to\nmake use of allele frequency time series data and infer the parameters of\ngeneral diploid selection, along with allele age, in non-equilibrium\npopulations. We introduce a novel path augmentation approach, in which we use\nMarkov chain Monte Carlo to integrate over the space of allele frequency\ntrajectories consistent with the observed data. Using simulations, we show that\nthis approach has good power to estimate selection coefficients and allele age.\nMoreover, when applying our approach to data on horse coat color, we find that\nignoring a relevant demographic history can significantly bias the results of\ninference. Our approach is made available in a C++ software package. \n\n"}
{"id": "1601.05686", "contents": "Title: Optimal exponential bounds for aggregation of estimators for the\n  Kullback-Leibler loss Abstract: We study the problem of model selection type aggregation with respect to the\nKullback-Leibler divergence for various probabilistic models. Rather than\nconsidering a convex combination of the initial estimators $f_1, \\ldots, f_N$,\nour aggregation procedures rely on the convex combination of the logarithms of\nthese functions. The first method is designed for probability density\nestimation as it gives an aggregate estimator that is also a proper density\nfunction, whereas the second method concerns spectral density estimation and\nhas no such mass-conserving feature. We select the aggregation weights based on\na penalized maximum likelihood criterion. We give sharp oracle inequalities\nthat hold with high probability, with a remainder term that is decomposed into\na bias and a variance part. We also show the optimality of the remainder terms\nby providing the corresponding lower bound results. \n\n"}
{"id": "1601.05766", "contents": "Title: Adaptive confidence sets in shape restricted regression Abstract: A simple construction of adaptive confidence sets is proposed in isotonic,\nconvex and unimodal regression. In univariate isotonic regression, the proposed\nconfidence set enjoys uniform coverage over all non-decreasing regression\nfunctions. Furthermore, the diameter of the proposed confidence set\nautomatically adapts to the unknown number of pieces of the true parameter, in\nthe sense that the diameter is bounded from above by the minimax risk over the\nclass of $k$-piecewise constant functions. The diameter of the confidence set\nis a simple increasing function of the number of jumps of the isotonic\nleast-squares estimate.\n  A similar construction is proposed in convex regression where the true\nregression function is convex and piecewise affine. Here, the confidence set\nenjoys uniform coverage and its diameter automatically adapt to the number of\naffine pieces of the true regression function. The diameter of the confidence\nset is an increasing function of the number of affine pieces of the convex\nleast-squares estimate.\n  We explain how to extend this technique to a non-convex set by proposing a\nsimilar adaptive confidence set in unimodal regression. The confidence set\nautomatically adapts to the number of jumps of the true unimodal regression\nfunction and its diameter is an increasing function of the number of jumps of\nthe unimodal least-squares estimate. \n\n"}
{"id": "1601.05835", "contents": "Title: Demystifying the Bias from Selective Inference: a Revisit to Dawid's\n  Treatment Selection Problem Abstract: We extend the heuristic discussion in Senn (2008) on the bias from selective\ninference for the treatment selection problem (Dawid 1994), by deriving the\nclosed-form expression for the selection bias. We illustrate the advantages of\nour theoretical results through numerical and simulated examples. \n\n"}
{"id": "1601.06212", "contents": "Title: Nonparametric Heterogeneity Testing For Massive Data Abstract: A massive dataset often consists of a growing number of (potentially)\nheterogeneous sub-populations. This paper is concerned about testing various\nforms of heterogeneity arising from massive data. In a general nonparametric\nframework, a set of testing procedures are designed to accommodate a growing\nnumber of sub-populations, denoted as $s$, with computational feasibility. In\ntheory, their null limit distributions are derived as being nearly Chi-square\nwith diverging degrees of freedom as long as $s$ does not grow too fast.\nInterestingly, we find that a lower bound on $s$ needs to be set for obtaining\na sufficiently powerful testing result, so-called \"blessing of aggregation.\" As\na by-produc, a type of homogeneity testing is also proposed with a test\nstatistic being aggregated over all sub-populations. Numerical results are\npresented to support our theory. \n\n"}
{"id": "1601.06233", "contents": "Title: Precise Error Analysis of Regularized M-estimators in High-dimensions Abstract: A popular approach for estimating an unknown signal from noisy, linear\nmeasurements is via solving a so called \\emph{regularized M-estimator}, which\nminimizes a weighted combination of a convex loss function and of a convex\n(typically, non-smooth) regularizer. We accurately predict the squared error\nperformance of such estimators in the high-dimensional proportional regime. The\nrandom measurement matrix is assumed to have entries iid Gaussian, only minimal\nand rather mild regularity conditions are imposed on the loss function, the\nregularizer, and on the noise and signal distributions. We show that the error\nconverges in probability to a nontrivial limit that is given as the solution to\na minimax convex-concave optimization problem on four scalar optimization\nvariables. We identify a new summary parameter, termed the Expected Moreau\nenvelope to play a central role in the error characterization. The\n\\emph{precise} nature of the results permits an accurate performance comparison\nbetween different instances of regularized M-estimators and allows to optimally\ntune the involved parameters (e.g. regularizer parameter, number of\nmeasurements). The key ingredient of our proof is the \\emph{Convex Gaussian\nMin-max Theorem} (CGMT) which is a tight and strengthened version of a\nclassical Gaussian comparison inequality that was proved by Gordon in 1988. \n\n"}
{"id": "1602.00199", "contents": "Title: Gaussian approximation for the sup-norm of high-dimensional\n  matrix-variate U-statistics and its applications Abstract: This paper studies the Gaussian approximation of high-dimensional and\nnon-degenerate U-statistics of order two under the supremum norm. We propose a\ntwo-step Gaussian approximation procedure that does not impose structural\nassumptions on the data distribution. Specifically, subject to mild moment\nconditions on the kernel, we establish the explicit rate of convergence that\ndecays polynomially in sample size for a high-dimensional scaling limit, where\nthe dimension can be much larger than the sample size. We also supplement a\npractical Gaussian wild bootstrap method to approximate the quantiles of the\nmaxima of centered U-statistics and prove its asymptotic validity. The wild\nbootstrap is demonstrated on statistical applications for high-dimensional\nnon-Gaussian data including: (i) principled and data-dependent tuning parameter\nselection for regularized estimation of the covariance matrix and its related\nfunctionals; (ii) simultaneous inference for the covariance and rank\ncorrelation matrices. In particular, for the thresholded covariance matrix\nestimator with the bootstrap selected tuning parameter, we show that the\nGaussian-like convergence rates can be achieved for heavy-tailed data, which\nare less conservative than those obtained by the Bonferroni technique that\nignores the dependency in the underlying data distribution. In addition, we\nalso show that even for subgaussian distributions, error bounds of the\nbootstrapped thresholded covariance matrix estimator can be much tighter than\nthose of the minimax estimator with a universal threshold. \n\n"}
{"id": "1602.00223", "contents": "Title: A Proximal Stochastic Quasi-Newton Algorithm Abstract: In this paper, we discuss the problem of minimizing the sum of two convex\nfunctions: a smooth function plus a non-smooth function. Further, the smooth\npart can be expressed by the average of a large number of smooth component\nfunctions, and the non-smooth part is equipped with a simple proximal mapping.\nWe propose a proximal stochastic second-order method, which is efficient and\nscalable. It incorporates the Hessian in the smooth part of the function and\nexploits multistage scheme to reduce the variance of the stochastic gradient.\nWe prove that our method can achieve linear rate of convergence. \n\n"}
{"id": "1602.00354", "contents": "Title: Active Learning Algorithms for Graphical Model Selection Abstract: The problem of learning the structure of a high dimensional graphical model\nfrom data has received considerable attention in recent years. In many\napplications such as sensor networks and proteomics it is often expensive to\nobtain samples from all the variables involved simultaneously. For instance,\nthis might involve the synchronization of a large number of sensors or the\ntagging of a large number of proteins. To address this important issue, we\ninitiate the study of a novel graphical model selection problem, where the goal\nis to optimize the total number of scalar samples obtained by allowing the\ncollection of samples from only subsets of the variables. We propose a general\nparadigm for graphical model selection where feedback is used to guide the\nsampling to high degree vertices, while obtaining only few samples from the\nones with the low degrees. We instantiate this framework with two specific\nactive learning algorithms, one of which makes mild assumptions but is\ncomputationally expensive, while the other is more computationally efficient\nbut requires stronger (nevertheless standard) assumptions. Whereas the sample\ncomplexity of passive algorithms is typically a function of the maximum degree\nof the graph, we show that the sample complexity of our algorithms is provable\nsmaller and that it depends on a novel local complexity measure that is akin to\nthe average degree of the graph. We finally demonstrate the efficacy of our\nframework via simulations. \n\n"}
{"id": "1602.00382", "contents": "Title: Distributed Constrained Recursive Nonlinear Least-Squares Estimation:\n  Algorithms and Asymptotics Abstract: This paper focuses on the problem of recursive nonlinear least squares\nparameter estimation in multi-agent networks, in which the individual agents\nobserve sequentially over time an independent and identically distributed\n(i.i.d.) time-series consisting of a nonlinear function of the true but unknown\nparameter corrupted by noise. A distributed recursive estimator of the\n\\emph{consensus} + \\emph{innovations} type, namely $\\mathcal{CIWNLS}$, is\nproposed, in which the agents update their parameter estimates at each\nobservation sampling epoch in a collaborative way by simultaneously processing\nthe latest locally sensed information~(\\emph{innovations}) and the parameter\nestimates from other agents~(\\emph{consensus}) in the local neighborhood\nconforming to a pre-specified inter-agent communication topology. Under rather\nweak conditions on the connectivity of the inter-agent communication and a\n\\emph{global observability} criterion, it is shown that at every network agent,\nthe proposed algorithm leads to consistent parameter estimates. Furthermore,\nunder standard smoothness assumptions on the local observation functions, the\ndistributed estimator is shown to yield order-optimal convergence rates, i.e.,\nas far as the order of pathwise convergence is concerned, the local parameter\nestimates at each agent are as good as the optimal centralized nonlinear least\nsquares estimator which would require access to all the observations across all\nthe agents at all times. In order to benchmark the performance of the proposed\ndistributed $\\mathcal{CIWNLS}$ estimator with that of the centralized nonlinear\nleast squares estimator, the asymptotic normality of the estimate sequence is\nestablished and the asymptotic covariance of the distributed estimator is\nevaluated. Finally, simulation results are presented which illustrate and\nverify the analytical findings. \n\n"}
{"id": "1602.01132", "contents": "Title: Interactive algorithms: from pool to stream Abstract: We consider interactive algorithms in the pool-based setting, and in the\nstream-based setting. Interactive algorithms observe suggested elements\n(representing actions or queries), and interactively select some of them and\nreceive responses. Pool-based algorithms can select elements at any order,\nwhile stream-based algorithms observe elements in sequence, and can only select\nelements immediately after observing them. We assume that the suggested\nelements are generated independently from some source distribution, and ask\nwhat is the stream size required for emulating a pool algorithm with a given\npool size. We provide algorithms and matching lower bounds for general pool\nalgorithms, and for utility-based pool algorithms. We further show that a\nmaximal gap between the two settings exists also in the special case of active\nlearning for binary classification. \n\n"}
{"id": "1602.02466", "contents": "Title: Overfitting hidden Markov models with an unknown number of states Abstract: This paper presents new theory and methodology for the Bayesian estimation of\noverfitted hidden Markov models, with finite state space. The goal is then to\nachieve posterior emptying of extra states. A prior configuration is\nconstructed which favours configurations where the hidden Markov chain remains\nergodic although it empties out some of the states. Asymptotic posterior\nconvergence rates are proven theoretically, and demonstrated with a large\nsample simulation. The problem of overfitted HMMs is then considered in the\ncontext of smaller sample sizes, and due to computational and mixing issues two\nalternative prior structures are studied, one commonly used in practice, and a\nmixture of the two priors. The Prior Parallel Tempering approach of van Havre\n(2015) is also extended to HMMs to allow MCMC estimation of the complex\nposterior space. A replicate simulation study and an in-depth exploration is\nperformed to compare the three priors with hyperparameters chosen according to\nthe asymptotic constraints alongside less informative alternatives. \n\n"}
{"id": "1602.02575", "contents": "Title: DECOrrelated feature space partitioning for distributed sparse\n  regression Abstract: Fitting statistical models is computationally challenging when the sample\nsize or the dimension of the dataset is huge. An attractive approach for\ndown-scaling the problem size is to first partition the dataset into subsets\nand then fit using distributed algorithms. The dataset can be partitioned\neither horizontally (in the sample space) or vertically (in the feature space).\nWhile the majority of the literature focuses on sample space partitioning,\nfeature space partitioning is more effective when $p\\gg n$. Existing methods\nfor partitioning features, however, are either vulnerable to high correlations\nor inefficient in reducing the model dimension. In this paper, we solve these\nproblems through a new embarrassingly parallel framework named DECO for\ndistributed variable selection and parameter estimation. In DECO, variables are\nfirst partitioned and allocated to $m$ distributed workers. The decorrelated\nsubset data within each worker are then fitted via any algorithm designed for\nhigh-dimensional problems. We show that by incorporating the decorrelation\nstep, DECO can achieve consistent variable selection and parameter estimation\non each subset with (almost) no assumptions. In addition, the convergence rate\nis nearly minimax optimal for both sparse and weakly sparse models and does NOT\ndepend on the partition number $m$. Extensive numerical experiments are\nprovided to illustrate the performance of the new framework. \n\n"}
{"id": "1602.04435", "contents": "Title: Random Forest Based Approach for Concept Drift Handling Abstract: Concept drift has potential in smart grid analysis because the socio-economic\nbehaviour of consumers is not governed by the laws of physics. Likewise there\nare also applications in wind power forecasting. In this paper we present\ndecision tree ensemble classification method based on the Random Forest\nalgorithm for concept drift. The weighted majority voting ensemble aggregation\nrule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method.\nBase learner weight in our case is computed for each sample evaluation using\nbase learners accuracy and intrinsic proximity measure of Random Forest. Our\nalgorithm exploits both temporal weighting of samples and ensemble pruning as a\nforgetting strategy. We present results of empirical comparison of our method\nwith original random forest with incorporated \"replace-the-looser\" forgetting\nandother state-of-the-art concept-drfit classifiers like AWE2. \n\n"}
{"id": "1602.04976", "contents": "Title: Stochastic Process Bandits: Upper Confidence Bounds Algorithms via\n  Generic Chaining Abstract: The paper considers the problem of global optimization in the setup of\nstochastic process bandits. We introduce an UCB algorithm which builds a\ncascade of discretization trees based on generic chaining in order to render\npossible his operability over a continuous domain. The theoretical framework\napplies to functions under weak probabilistic smoothness assumptions and also\nextends significantly the spectrum of application of UCB strategies. Moreover\ngeneric regret bounds are derived which are then specialized to Gaussian\nprocesses indexed on infinite-dimensional spaces as well as to quadratic forms\nof Gaussian processes. Lower bounds are also proved in the case of Gaussian\nprocesses to assess the optimality of the proposed algorithm. \n\n"}
{"id": "1602.05125", "contents": "Title: Locally Stationary Functional Time Series Abstract: The literature on time series of functional data has focused on processes of\nwhich the probabilistic law is either constant over time or constant up to its\nsecond-order structure. Especially for long stretches of data it is desirable\nto be able to weaken this assumption. This paper introduces a framework that\nwill enable meaningful statistical inference of functional data of which the\ndynamics change over time. We put forward the concept of local stationarity in\nthe functional setting and establish a class of processes that have a\nfunctional time-varying spectral representation. Subsequently, we derive\nconditions that allow for fundamental results from nonstationary multivariate\ntime series to carry over to the function space. In particular, time-varying\nfunctional ARMA processes are investigated and shown to be functional locally\nstationary according to the proposed definition. As a side-result, we establish\na Cram\\'er representation for an important class of weakly stationary\nfunctional processes. Important in our context is the notion of a time-varying\nspectral density operator of which the properties are studied and uniqueness is\nderived. Finally, we provide a consistent nonparametric estimator of this\noperator and show it is asymptotically Gaussian using a weaker tightness\ncriterion than what is usually deemed necessary. \n\n"}
{"id": "1602.05455", "contents": "Title: Heterogeneity Adjustment with Applications to Graphical Model Inference Abstract: Heterogeneity is an unwanted variation when analyzing aggregated datasets\nfrom multiple sources. Though different methods have been proposed for\nheterogeneity adjustment, no systematic theory exists to justify these methods.\nIn this work, we propose a generic framework named ALPHA (short for Adaptive\nLow-rank Principal Heterogeneity Adjustment) to model, estimate, and adjust\nheterogeneity from the original data. Once the heterogeneity is adjusted, we\nare able to remove the biases of batch effects and to enhance the inferential\npower by aggregating the homogeneous residuals from multiple sources. Under a\npervasive assumption that the latent heterogeneity factors simultaneously\naffect a large fraction of observed variables, we provide a rigorous theory to\njustify the proposed framework. Our framework also allows the incorporation of\ninformative covariates and appeals to the \"Bless of Dimensionality\". As an\nillustrative application of this generic framework, we consider a problem of\nestimating high-dimensional precision matrix for graphical model inference\nbased on multiple datasets. We also provide thorough numerical studies on both\nsynthetic datasets and a brain imaging dataset to demonstrate the efficacy of\nthe developed theory and methods. \n\n"}
{"id": "1602.05801", "contents": "Title: Leave-one-out prediction intervals in linear regression models with many\n  variables Abstract: We study prediction intervals based on leave-one-out residuals in a linear\nregression model where the number of explanatory variables can be large\ncompared to sample size. We establish uniform asymptotic validity (conditional\non the training sample) of the proposed interval under minimal assumptions on\nthe unknown error distribution and the high dimensional design. Our intervals\nare generic in the sense that they are valid for a large class of linear\npredictors used to obtain a point forecast, such as robust M-estimators,\nJames-Stein type estimators and penalized estimators like the LASSO. These\nresults show that despite the serious problems of resampling procedures for\ninference on the unknown parameters, leave-one-out methods can be successfully\napplied to obtain reliable predictive inference even in high dimensions. \n\n"}
{"id": "1602.06028", "contents": "Title: Generalized Gaussian Mechanism for Differential Privacy Abstract: Assessment of disclosure risk is of paramount importance in the research and\napplications of data privacy techniques. The concept of differential privacy\n(DP) formalizes privacy in probabilistic terms and provides a robust concept\nfor privacy protection without making assumptions about the background\nknowledge of adversaries. Practical applications of DP involve development of\nDP mechanisms to release results at a pre-specified privacy budget. In this\npaper, we generalize the widely used Laplace mechanism to the family of\ngeneralized Gaussian (GG) mechanism based on the $l_p$ global sensitivity of\nstatistical queries. We explore the theoretical requirement for the GG\nmechanism to reach DP at prespecified privacy parameters, and investigate the\nconnections and differences between the GG mechanism and the Exponential\nmechanism based on the GG distribution We also present a lower bound on the\nscale parameter of the Gaussian mechanism of $(\\epsilon,\\delta)$-probabilistic\nDP as a special case of the GG mechanism, and compare the statistical utility\nof the sanitized results in the tail probability and dispersion in the Gaussian\nand Laplace mechanisms. Lastly, we apply the GG mechanism in 3 experiments (the\nmildew, Czech, adult data), and compare the accuracy of sanitized results via\nthe $l_1$ distance and Kullback-Leibler divergence and examine how sanitization\naffects the prediction power of a classifier constructed with the sanitized\ndata in the adult experiment. \n\n"}
{"id": "1602.06896", "contents": "Title: Sharp detection in PCA under correlations: all eigenvalues matter Abstract: Principal component analysis (PCA) is a widely used method for dimension\nreduction. In high dimensional data, the \"signal\" eigenvalues corresponding to\nweak principal components (PCs) do not necessarily separate from the bulk of\nthe \"noise\" eigenvalues. Therefore, popular tests based on the largest\neigenvalue have little power to detect weak PCs. In the special case of the\nspiked model, certain tests asymptotically equivalent to linear spectral\nstatistics (LSS)---averaging effects over all eigenvalues---were recently shown\nto achieve some power.\n  We consider a nonparametric, non-Gaussian generalization of the spiked model\nto the setting of Marchenko and Pastur (1967). This allows a general bulk of\nthe noise eigenvalues, accomodating correlated variables even under the null\nhypothesis of no significant PCs.\n  We develop new tests based on LSS to detect weak PCs in this model. We show\nusing the CLT for LSS that the optimal LSS satisfy a Fredholm integral equation\nof the first kind. We develop algorithms to solve it, building on our recent\nmethod for computing the limit empirical spectrum. In contrast to the standard\nspiked model, we find that under \"widely spread\" null eigenvalue distributions,\nthe new tests have a lot of power. \n\n"}
{"id": "1602.07863", "contents": "Title: Learning Gaussian Graphical Models With Fractional Marginal\n  Pseudo-likelihood Abstract: We propose a Bayesian approximate inference method for learning the\ndependence structure of a Gaussian graphical model. Using pseudo-likelihood, we\nderive an analytical expression to approximate the marginal likelihood for an\narbitrary graph structure without invoking any assumptions about\ndecomposability. The majority of the existing methods for learning Gaussian\ngraphical models are either restricted to decomposable graphs or require\nspecification of a tuning parameter that may have a substantial impact on\nlearned structures. By combining a simple sparsity inducing prior for the graph\nstructures with a default reference prior for the model parameters, we obtain a\nfast and easily applicable scoring function that works well for even\nhigh-dimensional data. We demonstrate the favourable performance of our\napproach by large-scale comparisons against the leading methods for learning\nnon-decomposable Gaussian graphical models. A theoretical justification for our\nmethod is provided by showing that it yields a consistent estimator of the\ngraph structure. \n\n"}
{"id": "1602.08062", "contents": "Title: Probabilistic community detection with unknown number of communities Abstract: A fundamental problem in network analysis is clustering the nodes into groups\nwhich share a similar connectivity pattern. Existing algorithms for community\ndetection assume the knowledge of the number of clusters or estimate it a\npriori using various selection criteria and subsequently estimate the community\nstructure. Ignoring the uncertainty in the first stage may lead to erroneous\nclustering, particularly when the community structure is vague. We instead\npropose a coherent probabilistic framework for simultaneous estimation of the\nnumber of communities and the community structure, adapting recently developed\nBayesian nonparametric techniques to network models. An efficient Markov chain\nMonte Carlo (MCMC) algorithm is proposed which obviates the need to perform\nreversible jump MCMC on the number of clusters. The methodology is shown to\noutperform recently developed community detection algorithms in a variety of\nsynthetic data examples and in benchmark real-datasets. Using an appropriate\nmetric on the space of all configurations, we develop non-asymptotic Bayes risk\nbounds even when the number of clusters is unknown. Enroute, we develop\nconcentration properties of non-linear functions of Bernoulli random variables,\nwhich may be of independent interest. \n\n"}
{"id": "1602.08654", "contents": "Title: Testing for parameter change in general integer-valued time series Abstract: We consider the structural change in a class of discrete valued time series\nthat the conditional distribution follows a one-parameter exponential family.\nWe propose a change-point test based on the maximum likelihood estimator of the\nparameter of the model. Under the null hypothesis (of no change), the test\nstatistics converges to a well known distribution, allowing for the calculation\nof the critical values of the test. The test statistic diverges to infinity\nunder the alternative, that is, the test asymptotically has power one. Some\nsimulation results and real data applications are reported to show the\napplicability of the test procedure. \n\n"}
{"id": "1603.00389", "contents": "Title: Multi-Information Source Optimization Abstract: We consider Bayesian optimization of an expensive-to-evaluate black-box\nobjective function, where we also have access to cheaper approximations of the\nobjective. In general, such approximations arise in applications such as\nreinforcement learning, engineering, and the natural sciences, and are subject\nto an inherent, unknown bias. This model discrepancy is caused by an inadequate\ninternal model that deviates from reality and can vary over the domain, making\nthe utilization of these approximations a non-trivial task.\n  We present a novel algorithm that provides a rigorous mathematical treatment\nof the uncertainties arising from model discrepancies and noisy observations.\nIts optimization decisions rely on a value of information analysis that extends\nthe Knowledge Gradient factor to the setting of multiple information sources\nthat vary in cost: each sampling decision maximizes the predicted benefit per\nunit cost.\n  We conduct an experimental evaluation that demonstrates that the method\nconsistently outperforms other state-of-the-art techniques: it finds designs of\nconsiderably higher objective value and additionally inflicts less cost in the\nexploration process. \n\n"}
{"id": "1603.00596", "contents": "Title: Randomly Weighted Averages: A Multivariate Case Abstract: Stochastic linear combinations of some random vectors are studied where the\ndistribution of the random vectors and the joint distribution of their\ncoefficients are Dirichlet. A method is provided for calculating the\ndistribution of these combinations which has been studied before by some\nauthors. Our main result is a generalization of some existing results with a\nsimpler proof. \n\n"}
{"id": "1603.00929", "contents": "Title: A Kernel Test for Three-Variable Interactions with Random Processes Abstract: We apply a wild bootstrap method to the Lancaster three-variable interaction\nmeasure in order to detect factorisation of the joint distribution on three\nvariables forming a stationary random process, for which the existing\npermutation bootstrap method fails. As in the i.i.d. case, the Lancaster test\nis found to outperform existing tests in cases for which two independent\nvariables individually have a weak influence on a third, but that when\nconsidered jointly the influence is strong. The main contributions of this\npaper are twofold: first, we prove that the Lancaster statistic satisfies the\nconditions required to estimate the quantiles of the null distribution using\nthe wild bootstrap; second, the manner in which this is proved is novel,\nsimpler than existing methods, and can further be applied to other statistics. \n\n"}
{"id": "1603.01003", "contents": "Title: A review of 20 years of naive tests of significance for high-dimensional\n  mean vectors and covariance matrices Abstract: In this paper, we will introduce the so called naive tests and give a brief\nreview on the newly development. Naive testing methods are easy to understand\nand performs robust especially when the dimension is large. In this paper, we\nmainly focus on reviewing some naive testing methods for the mean vectors and\ncovariance matrices of high dimensional populations and believe this naive test\nidea can be wildly used in many other testing problems. \n\n"}
{"id": "1603.01295", "contents": "Title: Simultaneous Inference for High-dimensional Linear Models Abstract: This paper proposes a bootstrap-assisted procedure to conduct simultaneous\ninference for high dimensional sparse linear models based on the recent\nde-sparsifying Lasso estimator (van de Geer et al. 2014). Our procedure allows\nthe dimension of the parameter vector of interest to be exponentially larger\nthan sample size, and it automatically accounts for the dependence within the\nde-sparsifying Lasso estimator. Moreover, our simultaneous testing method can\nbe naturally coupled with the margin screening (Fan and Lv 2008) to enhance its\npower in sparse testing with a reduced computational cost, or with the\nstep-down method (Romano and Wolf 2005) to provide a strong control for the\nfamily-wise error rate. In theory, we prove that our simultaneous testing\nprocedure asymptotically achieves the pre-specified significance level, and\nenjoys certain optimality in terms of its power even when the model errors are\nnon-Gaussian. Our general theory is also useful in studying the support\nrecovery problem. To broaden the applicability, we further extend our main\nresults to generalized linear models with convex loss functions. The\neffectiveness of our methods is demonstrated via simulation studies. \n\n"}
{"id": "1603.03830", "contents": "Title: Homoscedasticity tests for both low and high-dimensional fixed design\n  regressions Abstract: This paper is to prove the asymptotic normality of a statistic for detecting\nthe existence of heteroscedasticity for linear regression models without\nassuming randomness of covariates when the sample size $n$ tends to infinity\nand the number of covariates $p$ is either fixed or tends to infinity. Moreover\nour approach indicates that its asymptotic normality holds even without\nhomoscedasticity. \n\n"}
{"id": "1603.03934", "contents": "Title: Some new ideas in nonparametric estimation Abstract: In the framework of an abstract statistical model we discuss how to use the\nsolution of one estimation problem ({\\it Problem A}) in order to construct an\nestimator in another, completely different, {\\it Problem B}. As a solution of\n{\\it Problem A} we understand a data-driven selection from a given family of\nestimators $\\mathbf{A}(\\mH)=\\big\\{\\widehat{A}_\\mh, \\mh\\in\\mH\\big\\}$ and\nestablishing for the selected estimator so-called oracle inequality.\n%parameterized by some se t$\\mH$. If $\\hat{\\mh}\\in\\mH$ is the selected\nparameter and $\\mathbf{B}(\\mH)=\\big\\{\\widehat{B}_\\mh, \\mh\\in\\mH\\big\\}$ is an\nestimator's collection built in {\\it Problem B} we suggest to use the estimator\n$\\widehat{B}_{\\hat{\\mh}}$. We present very general selection rule led to\nselector $\\hat{\\mh}$ and find conditions under which the estimator\n$\\widehat{B}_{\\hat{\\mh}}$ is reasonable. Our approach is illustrated by several\nexamples related to adaptive estimation. \n\n"}
{"id": "1603.05947", "contents": "Title: Noisy Hypotheses in the Age of Discovery Science Abstract: We draw attention to one specific issue raised by Ioannidis (2005), that of\nvery many hypotheses being tested in a given field of investigation. To better\nisolate the problem that arises in this (massive) multiple testing scenario, we\nconsider a utopian setting where the hypotheses are tested with no additional\nbias. We show that, as the number of hypotheses being tested becomes much\nlarger than the discoveries to be made, it becomes impossible to reliably\nidentify true discoveries. This phenomenon, well-known to statisticians working\nin the field of multiple testing, puts in jeopardy any naive pursuit in (pure)\ndiscovery science. \n\n"}
{"id": "1603.06358", "contents": "Title: Bayesian inference for multiple Gaussian graphical models with\n  application to metabolic association networks Abstract: We investigate the effect of cadmium (a toxic environmental pollutant) on the\ncorrelation structure of a number of urinary metabolites using Gaussian\ngraphical models (GGMs). The inferred metabolic associations can provide\nimportant information on the physiological state of a metabolic system and\ninsights on complex metabolic relationships. Using the fitted GGMs, we\nconstruct differential networks, which highlight significant changes in\nmetabolite interactions under different experimental conditions. The analysis\nof such metabolic association networks can reveal differences in the underlying\nbiological reactions caused by cadmium exposure. We consider Bayesian inference\nand propose using the multiplicative (or Chung-Lu random graph) model as a\nprior on the graphical space. In the multiplicative model, each edge is chosen\nindependently with probability equal to the product of the connectivities of\nthe end nodes. This class of prior is parsimonious yet highly flexible; it can\nbe used to encourage sparsity or graphs with a pre-specified degree\ndistribution when such prior knowledge is available. We extend the\nmultiplicative model to multiple GGMs linking the probability of edge inclusion\nthrough logistic regression and demonstrate how this leads to joint inference\nfor multiple GGMs. A sequential Monte Carlo (SMC) algorithm is developed for\nestimating the posterior distribution of the graphs. \n\n"}
{"id": "1603.06743", "contents": "Title: Localized Lasso for High-Dimensional Regression Abstract: We introduce the localized Lasso, which is suited for learning models that\nare both interpretable and have a high predictive power in problems with high\ndimensionality $d$ and small sample size $n$. More specifically, we consider a\nfunction defined by local sparse models, one at each data point. We introduce\nsample-wise network regularization to borrow strength across the models, and\nsample-wise exclusive group sparsity (a.k.a., $\\ell_{1,2}$ norm) to introduce\ndiversity into the choice of feature sets in the local models. The local models\nare interpretable in terms of similarity of their sparsity patterns. The cost\nfunction is convex, and thus has a globally optimal solution. Moreover, we\npropose a simple yet efficient iterative least-squares based optimization\nprocedure for the localized Lasso, which does not need a tuning parameter, and\nis guaranteed to converge to a globally optimal solution. The solution is\nempirically shown to outperform alternatives for both simulated and genomic\npersonalized medicine data. \n\n"}
{"id": "1603.07365", "contents": "Title: Positive-part moments via the characteristic functions, and more general\n  expressions Abstract: A unifying and generalizing approach to representations of the positive-part\nand absolute moments $\\mathsf{E} X_+^p$ and $\\mathsf{E}|X|^p$ of a random\nvariable $X$ for real $p$ in terms of the characteristic function (c.f.) of\n$X$, as well as to related representations of the c.f.\\ of $X_+$, generalized\nmoments $\\mathsf{E} X_+^p e^{iuX}$, truncated moments, and the distribution\nfunction is provided. Existing and new representations of these kinds are all\nshown to stem from a single basic representation. Computational aspects of\nthese representations are addressed. \n\n"}
{"id": "1603.08460", "contents": "Title: On boundary detection Abstract: Given a sample of a random variable supported by a smooth compact manifold\n$M\\subset \\mathbb{R}^d$, we propose a test to decide whether the boundary of\n$M$ is empty or not with no preliminary support estimation. The test statistic\nis based on the maximal distance between a sample point and the average of its\n$k_n$-nearest neighbors. We prove that the level of the test can be estimated,\nthat, with probability one, its power is one for $n$ large enough, and that\nthere exists a consistent decision rule. Heuristics for choosing a convenient\nvalue for the $k_n$ parameter and identifying observations close to the\nboundary are also given. We provide a simulation study of the test. \n\n"}
{"id": "1603.08691", "contents": "Title: Amplitude and phase variation of point processes Abstract: We develop a canonical framework for the study of the problem of registration\nof multiple point processes subjected to warping, known as the problem of\nseparation of amplitude and phase variation. The amplitude variation of a real\nrandom function $\\{Y(x):x\\in[0,1]\\}$ corresponds to its random oscillations in\nthe $y$-axis, typically encapsulated by its (co)variation around a mean level.\nIn contrast, its phase variation refers to fluctuations in the $x$-axis, often\ncaused by random time changes. We formalise similar notions for a point\nprocess, and nonparametrically separate them based on realisations of i.i.d.\ncopies $\\{\\Pi_i\\}$ of the phase-varying point process. A key element in our\napproach is to demonstrate that when the classical phase variation assumptions\nof Functional Data Analysis (FDA) are applied to the point process case, they\nbecome equivalent to conditions interpretable through the prism of the theory\nof optimal transportation of measure. We demonstrate that these induce a\nnatural Wasserstein geometry tailored to the warping problem, including a\nformal notion of bias expressing over-registration. Within this framework, we\nconstruct nonparametric estimators that tend to avoid over-registration in\nfinite samples. We show that they consistently estimate the warp maps,\nconsistently estimate the structural mean, and consistently register the warped\npoint processes, even in a sparse sampling regime. We also establish\nconvergence rates, and derive $\\sqrt{n}$-consistency and a central limit\ntheorem in the Cox process case under dense sampling, showing rate optimality\nof our structural mean estimator in that case. \n\n"}
{"id": "1604.01446", "contents": "Title: Quantifying Distributional Model Risk via Optimal Transport Abstract: This paper deals with the problem of quantifying the impact of model\nmisspecification when computing general expected values of interest. The\nmethodology that we propose is applicable in great generality, in particular,\nwe provide examples involving path dependent expectations of stochastic\nprocesses. Our approach consists in computing bounds for the expectation of\ninterest regardless of the probability measure used, as long as the measure\nlies within a prescribed tolerance measured in terms of a flexible class of\ndistances from a suitable baseline model. These distances, based on optimal\ntransportation between probability measures, include Wasserstein's distances as\nparticular cases. The proposed methodology is well-suited for risk analysis, as\nwe demonstrate with a number of applications. We also discuss how to estimate\nthe tolerance region non-parametrically using Skorokhod-type embeddings in some\nof these applications. \n\n"}
{"id": "1604.01733", "contents": "Title: A U-statistic Approach to Hypothesis Testing for Structure Discovery in\n  Undirected Graphical Models Abstract: Structure discovery in graphical models is the determination of the topology\nof a graph that encodes conditional independence properties of the joint\ndistribution of all variables in the model. For some class of probability\ndistributions, an edge between two variables is present if and only if the\ncorresponding entry in the precision matrix is non-zero. For a finite sample\nestimate of the precision matrix, entries close to zero may be due to low\nsample effects, or due to an actual association between variables; these two\ncases are not readily distinguishable. %Fisher provided a hypothesis test based\non a parametric approximation to the distribution of an entry in the precision\nmatrix of a Gaussian distribution, but this may not provide valid upper bounds\non $p$-values for non-Gaussian distributions. Many related works on this topic\nconsider potentially restrictive distributional or sparsity assumptions that\nmay not apply to a data sample of interest, and direct estimation of the\nuncertainty of an estimate of the precision matrix for general distributions\nremains challenging. Consequently, we make use of results for $U$-statistics\nand apply them to the covariance matrix. By probabilistically bounding the\ndistortion of the covariance matrix, we can apply Weyl's theorem to bound the\ndistortion of the precision matrix, yielding a conservative, but sound test\nthreshold for a much wider class of distributions than considered in previous\nworks. The resulting test enables one to answer with statistical significance\nwhether an edge is present in the graph, and convergence results are known for\na wide range of distributions. The computational complexities is linear in the\nsample size enabling the application of the test to large data samples for\nwhich computation time becomes a limiting factor. We experimentally validate\nthe correctness and scalability of the test on multivariate distributions for\nwhich the distributional assumptions of competing tests result in\nunderestimates of the false positive ratio. By contrast, the proposed test\nremains sound, promising to be a useful tool for hypothesis testing for diverse\nreal-world problems. \n\n"}
{"id": "1604.01785", "contents": "Title: Safe Probability Abstract: We formalize the idea of probability distributions that lead to reliable\npredictions about some, but not all aspects of a domain. The resulting notion\nof `safety' provides a fresh perspective on foundational issues in statistics,\nproviding a middle ground between imprecise probability and multiple-prior\nmodels on the one hand and strictly Bayesian approaches on the other. It also\nallows us to formalize fiducial distributions in terms of the set of random\nvariables that they can safely predict, thus taking some of the sting out of\nthe fiducial idea. By restricting probabilistic inference to safe uses, one\nalso automatically avoids paradoxes such as the Monty Hall problem. Safety\ncomes in a variety of degrees, such as \"validity\" (the strongest notion),\n\"calibration\", \"confidence safety\" and \"unbiasedness\" (almost the weakest\nnotion). \n\n"}
{"id": "1604.03192", "contents": "Title: Scalar-on-Image Regression via the Soft-Thresholded Gaussian Process Abstract: The focus of this work is on spatial variable selection for scalar-on-image\nregression. We propose a new class of Bayesian nonparametric models,\nsoft-thresholded Gaussian processes and develop the efficient posterior\ncomputation algorithms. Theoretically, soft-thresholded Gaussian processes\nprovide large prior support for the spatially varying coefficients that enjoy\npiecewise smoothness, sparsity and continuity, characterizing the important\nfeatures of imaging data. Also, under some mild regularity conditions, the\nsoft-thresholded Gaussian process leads to the posterior consistency for both\nparameter estimation and variable selection for scalar-on-image regression,\neven when the number of true predictors is larger than the sample size. The\nproposed method is illustrated via simulations, compared numerically with\nexisting alternatives and applied to Electroencephalography (EEG) study of\nalcoholism. \n\n"}
{"id": "1604.03227", "contents": "Title: Recurrent Attentional Networks for Saliency Detection Abstract: Convolutional-deconvolution networks can be adopted to perform end-to-end\nsaliency detection. But, they do not work well with objects of multiple scales.\nTo overcome such a limitation, in this work, we propose a recurrent attentional\nconvolutional-deconvolution network (RACDNN). Using spatial transformer and\nrecurrent network units, RACDNN is able to iteratively attend to selected image\nsub-regions to perform saliency refinement progressively. Besides tackling the\nscale problem, RACDNN can also learn context-aware features from past\niterations to enhance saliency refinement in future iterations. Experiments on\nseveral challenging saliency detection datasets validate the effectiveness of\nRACDNN, and show that RACDNN outperforms state-of-the-art saliency detection\nmethods. \n\n"}
{"id": "1604.04173", "contents": "Title: Distribution-Free Predictive Inference For Regression Abstract: We develop a general framework for distribution-free predictive inference in\nregression, using conformal inference. The proposed methodology allows for the\nconstruction of a prediction band for the response variable using any estimator\nof the regression function. The resulting prediction band preserves the\nconsistency properties of the original estimator under standard assumptions,\nwhile guaranteeing finite-sample marginal coverage even when these assumptions\ndo not hold. We analyze and compare, both empirically and theoretically, the\ntwo major variants of our conformal framework: full conformal inference and\nsplit conformal inference, along with a related jackknife method. These methods\noffer different tradeoffs between statistical accuracy (length of resulting\nprediction intervals) and computational efficiency. As extensions, we develop a\nmethod for constructing valid in-sample prediction intervals called {\\it\nrank-one-out} conformal inference, which has essentially the same computational\nefficiency as split conformal inference. We also describe an extension of our\nprocedures for producing prediction bands with locally varying length, in order\nto adapt to heteroskedascity in the data. Finally, we propose a model-free\nnotion of variable importance, called {\\it leave-one-covariate-out} or LOCO\ninference. Accompanying this paper is an R package {\\tt conformalInference}\nthat implements all of the proposals we have introduced. In the spirit of\nreproducibility, all of our empirical results can also be easily (re)generated\nusing this package. \n\n"}
{"id": "1604.07104", "contents": "Title: The limit of finite sample breakdown point of Tukey's halfspace median\n  for general data Abstract: Under special conditions on data set and underlying distribution, the limit\nof finite sample breakdown point of Tukey's halfspace median ($\\frac{1} {3}$)\nhas been obtained in literature. In this paper, we establish the result under\n\\emph{weaker assumption} imposed on underlying distribution (halfspace\nsymmetry) and on data set (not necessary in general position). The\nrepresentation of Tukey's sample depth regions for data set \\emph{not necessary\nin general position} is also obtained, as a by-product of our derivation. \n\n"}
{"id": "1604.07143", "contents": "Title: Neural Random Forests Abstract: Given an ensemble of randomized regression trees, it is possible to\nrestructure them as a collection of multilayered neural networks with\nparticular connection weights. Following this principle, we reformulate the\nrandom forest method of Breiman (2001) into a neural network setting, and in\nturn propose two new hybrid procedures that we call neural random forests. Both\npredictors exploit prior knowledge of regression trees for their architecture,\nhave less parameters to tune than standard networks, and less restrictions on\nthe geometry of the decision boundaries than trees. Consistency results are\nproved, and substantial numerical evidence is provided on both synthetic and\nreal data sets to assess the excellent performance of our methods in a large\nvariety of prediction problems. \n\n"}
{"id": "1605.00414", "contents": "Title: On sampling theorem with sparse decimated samples: exploring branching\n  spectrum degeneracy Abstract: The paper investigates possibility of recovery of sequences from their\ndecimated subsequences. It is shown that this recoverability is associated with\ncertain spectrum degeneracy of a new kind, and that a sequences of a general\nkind can be approximated by sequences featuring this degeneracy. This is\napplied to sparse sampling of continuous time band-limited functions. The paper\nshows that these functions allow an arbitrarily close approximation by\nfunctions that can be recovered from sparse equidistant samples with sampling\ndistance larger than the distance defined by the critical Nyquist rate for the\nunderlying function. This allows to bypass, in a certain sense, the restriction\non the sampling rate defined by the Nyquist rate. \n\n"}
{"id": "1605.01185", "contents": "Title: Linear Bandit algorithms using the Bootstrap Abstract: This study presents two new algorithms for solving linear stochastic bandit\nproblems. The proposed methods use an approach from non-parametric statistics\ncalled bootstrapping to create confidence bounds. This is achieved without\nmaking any assumptions about the distribution of noise in the underlying\nsystem. We present the X-Random and X-Fixed bootstrap bandits which correspond\nto the two well-known approaches for conducting bootstraps on models, in the\nliterature. The proposed methods are compared to other popular solutions for\nlinear stochastic bandit problems, namely, OFUL, LinUCB and Thompson Sampling.\nThe comparisons are carried out using a simulation study on a hierarchical\nprobability meta-model, built from published data of experiments, which are run\non real systems. The model representing the response surfaces is conceptualized\nas a Bayesian Network which is presented with varying degrees of noise for the\nsimulations. One of the proposed methods, X-Random bootstrap, performs better\nthan the baselines in-terms of cumulative regret across various degrees of\nnoise and different number of trials. In certain settings the cumulative regret\nof this method is less than half of the best baseline. The X-Fixed bootstrap\nperforms comparably in most situations and particularly well when the number of\ntrials is low. The study concludes that these algorithms could be a preferred\nalternative for solving linear bandit problems, especially when the\ndistribution of the noise in the system is unknown. \n\n"}
{"id": "1605.03330", "contents": "Title: On Asymptotic Inference in Stochastic Differential Equations with\n  Time-Varying Covariates Abstract: In this article, we introduce a system of stochastic differential equations\n(SDEs) consisting of time-dependent covariates and consider both fixed and\nrandom effects set-ups. We also allow the functional part associated with the\ndrift function to depend upon unknown parameters. In this general set-up of SDE\nsystem we establish consistency and asymptotic normality of the M LE through\nverification of the regularity conditions required by existing relevant\ntheorems. Besides, we consider the Bayesian approach to learning about the\npopulation parameters, and prove consistency and asymptotic normality of the\ncorresponding posterior distribution. We supplement our theoretical\ninvestigation with simulated and real data analyses, obtaining encouraging\nresults in each case. \n\n"}
{"id": "1605.03433", "contents": "Title: On optimality of empirical risk minimization in linear aggregation Abstract: In the first part of this paper, we show that the small-ball condition,\nrecently introduced by Mendelson (2015), may behave poorly for important\nclasses of localized functions such as wavelets, piecewise polynomials or\ntrigonometric polynomials, in particular leading to suboptimal estimates of the\nrate of convergence of ERM for the linear aggregation problem. In a second\npart, we recover optimal rates of covergence for the excess risk of ERM when\nthe dictionary is made of trigonometric functions. Considering the bounded\ncase, we derive the concentration of the excess risk around a single point,\nwhich is an information far more precise than the rate of convergence. In the\ngeneral setting of a L2 noise, we finally refine the small ball argument by\nrightly selecting the directions we are looking at, in such a way that we\nobtain optimal rates of aggregation for the Fourier dictionary. \n\n"}
{"id": "1605.03896", "contents": "Title: Characterization of the Riesz Exponential Family on homogeneous cones Abstract: In the paper we present a characterization theorem of the Riesz measure and a\nWishart exponential family on homogeneous cones through the invariance property\nof a natural exponential family under the action of the triangular group. \n\n"}
{"id": "1605.04796", "contents": "Title: Prediction risk for the horseshoe regression Abstract: We show that prediction performance for global-local shrinkage regression can\novercome two major difficulties of global shrinkage regression: (i) the amount\nof relative shrinkage is monotone in the singular values of the design matrix\nand (ii) the shrinkage is determined by a single tuning parameter.\nSpecifically, we show that the horseshoe regression, with heavy-tailed\ncomponent-specific local shrinkage parameters, in conjunction with a global\nparameter providing shrinkage towards zero, alleviates both these difficulties\nand consequently, results in an improved risk for prediction. Numerical\ndemonstrations of improved prediction over competing approaches in simulations\nand in a pharmacogenomics data set confirm our theoretical findings. \n\n"}
{"id": "1605.07520", "contents": "Title: Nonparametric estimation of a regression function using the gamma kernel\n  method in ergodic processes Abstract: In this paper we consider the nonparametric estimation of density and\nregression functions with non-negative support using a gamma kernel procedure\nintroduced by Chen (2000). Strong uniform consistency and asymptotic normality\nof the corresponding estimators are established under a general ergodic\nassumption on the data generation process. Our results generalize those of Shi\nand Song (2016), obtained in the classic i.i.d. framework, and the works of\nBouezmarni and Rombouts (2008, 2010b) and Gospodinov and Hirukawa (2007) for\nmixing time series. \n\n"}
{"id": "1605.08386", "contents": "Title: Heat-bath random walks with Markov bases Abstract: Graphs on lattice points are studied whose edges come from a finite set of\nallowed moves of arbitrary length. We show that the diameter of these graphs on\nfibers of a fixed integer matrix can be bounded from above by a constant. We\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\nalso state explicit conditions on the set of moves so that the heat-bath random\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\ndimension. \n\n"}
{"id": "1605.08824", "contents": "Title: Integrative Methods for Post-Selection Inference Under Convex\n  Constraints Abstract: Inference after model selection has been an active research topic in the past\nfew years, with numerous works offering different approaches to addressing the\nperils of the reuse of data. In particular, major progress has been made\nrecently on large and useful classes of problems by harnessing general theory\nof hypothesis testing in exponential families, but these methods have their\nlimitations. Perhaps most immediate is the gap between theory and practice:\nimplementing the exact theoretical prescription in realistic situations---for\nexample, when new data arrives and inference needs to be adjusted\naccordingly---may be a prohibitive task. In this paper we propose a Bayesian\nframework for carrying out inference after model selection in the linear model.\nOur framework is very flexible in the sense that it naturally accommodates\ndifferent models for the data, instead of requiring a case-by-case treatment.\nAt the core of our methods is a new approximation to the exact likelihood\nconditional on selection, the latter being generally intractable. We prove\nthat, under appropriate conditions, our approximation is asymptotically\nconsistent with the exact truncated likelihood. The advantages of our methods\nin practical data analysis are demonstrated in simulations and in application\nto HIV drug-resistance data. \n\n"}
{"id": "1605.09457", "contents": "Title: Asymptotic properties of the maximum likelihood estimator for nonlinear\n  AR processes with markov-switching Abstract: In this note, we propose a new approach for the proof of the consistency and\nnormality of the maximum likelihood estimator for nonlinear AR processes with\nmarkov-switching under the assumptions of uniform exponential forgetting of the\nprediction filter and $\\alpha$-mixing property. We show that in the linear and\nGaussian case our assumptions are fully satisfied. \n\n"}
{"id": "1606.00187", "contents": "Title: Robust Principal Component Analysis in Hilbert spaces Abstract: We propose a stable version of Principal Component Analysis (PCA) in the\ngeneral framework of a separable Hilbert space. It consists in interpreting the\nprojection on the first eigenvectors as a step function applied to the spectrum\nof the covariance operator and in replacing it with a smooth cut-off of the\neigenvalues. We study the problem from a statistical point of view, so that we\nassume that we do not have direct access to the covariance operator but we have\nto estimate it from an i.i.d. sample. We provide some results on the quality of\nthe approximation of our spectral cut-off in terms of the quality of the\napproximation of the eigenvalues of the covariance operator. \n\n"}
{"id": "1606.00318", "contents": "Title: Discovering Phase Transitions with Unsupervised Learning Abstract: Unsupervised learning is a discipline of machine learning which aims at\ndiscovering patterns in big data sets or classifying the data into several\ncategories without being trained explicitly. We show that unsupervised learning\ntechniques can be readily used to identify phases and phases transitions of\nmany body systems. Starting with raw spin configurations of a prototypical\nIsing model, we use principal component analysis to extract relevant low\ndimensional representations the original data and use clustering analysis to\nidentify distinct phases in the feature space. This approach successfully finds\nout physical concepts such as order parameter and structure factor to be\nindicators of the phase transition. We discuss future prospects of discovering\nmore complex phases and phase transitions using unsupervised learning\ntechniques. \n\n"}
{"id": "1606.00622", "contents": "Title: Consistent order estimation for nonparametric Hidden Markov Models Abstract: We consider the problem of estimating the number of hidden states (the order)\nof a nonparametric hidden Markov model (HMM). We propose two different methods\nand prove their almost sure consistency without any prior assumption, be it on\nthe order or on the emission distributions. This is the first time a\nconsistency result is proved in such a general setting without using\nrestrictive assumptions such as a priori upper bounds on the order or\nparametric restrictions on the emission distributions. Our main method relies\non the minimization of a penalized least squares criterion. In addition to the\nconsistency of the order estimation, we also prove that this method yields rate\nminimax adaptive estimators of the parameters of the HMM - up to a logarithmic\nfactor. Our second method relies on estimating the rank of a matrix obtained\nfrom the distribution of two consecutive observations. Finally, numerical\nexperiments are used to compare both methods and study their ability to select\nthe right order in several situations. \n\n"}
{"id": "1606.03059", "contents": "Title: Consistency and convergence rate of phylogenetic inference via\n  regularization Abstract: It is common in phylogenetics to have some, perhaps partial, information\nabout the overall evolutionary tree of a group of organisms and wish to find an\nevolutionary tree of a specific gene for those organisms. There may not be\nenough information in the gene sequences alone to accurately reconstruct the\ncorrect \"gene tree.\" Although the gene tree may deviate from the \"species tree\"\ndue to a variety of genetic processes, in the absence of evidence to the\ncontrary it is parsimonious to assume that they agree. A common statistical\napproach in these situations is to develop a likelihood penalty to incorporate\nsuch additional information. Recent studies using simulation and empirical data\nsuggest that a likelihood penalty quantifying concordance with a species tree\ncan significantly improve the accuracy of gene tree reconstruction compared to\nusing sequence data alone. However, the consistency of such an approach has not\nyet been established, nor have convergence rates been bounded. Because\nphylogenetics is a non-standard inference problem, the standard theory does not\napply. In this paper, we propose a penalized maximum likelihood estimator for\ngene tree reconstruction, where the penalty is the square of the\nBillera-Holmes-Vogtmann geodesic distance from the gene tree to the species\ntree. We prove that this method is consistent, and derive its convergence rate\nfor estimating the discrete gene tree structure and continuous edge lengths\n(representing the amount of evolution that has occurred on that branch)\nsimultaneously. We find that the regularized estimator is \"adaptive fast\nconverging,\" meaning that it can reconstruct all edges of length greater than\nany given threshold from gene sequences of polynomial length. Our method does\nnot require the species tree to be known exactly; in fact, our asymptotic\ntheory holds for any such guide tree. \n\n"}
{"id": "1606.03803", "contents": "Title: Tuning-Free Heterogeneity Pursuit in Massive Networks Abstract: Heterogeneity is often natural in many contemporary applications involving\nmassive data. While posing new challenges to effective learning, it can play a\ncrucial role in powering meaningful scientific discoveries through the\nunderstanding of important differences among subpopulations of interest. In\nthis paper, we exploit multiple networks with Gaussian graphs to encode the\nconnectivity patterns of a large number of features on the subpopulations. To\nuncover the heterogeneity of these structures across subpopulations, we suggest\na new framework of tuning-free heterogeneity pursuit (THP) via large-scale\ninference, where the number of networks is allowed to diverge. In particular,\ntwo new tests, the chi-based test and the linear functional-based test, are\nintroduced and their asymptotic null distributions are established. Under mild\nregularity conditions, we establish that both tests are optimal in achieving\nthe testable region boundary and the sample size requirement for the latter\ntest is minimal. Both theoretical guarantees and the tuning-free feature stem\nfrom efficient multiple-network estimation by our newly suggested approach of\nheterogeneous group square-root Lasso (HGSL) for high-dimensional\nmulti-response regression with heterogeneous noises. To solve this convex\nprogram, we further introduce a tuning-free algorithm that is scalable and\nenjoys provable convergence to the global optimum. Both computational and\ntheoretical advantages of our procedure are elucidated through simulation and\nreal data examples. \n\n"}
{"id": "1606.04276", "contents": "Title: An averaged projected Robbins-Monro algorithm for estimating the\n  parameters of a truncated spherical distribution Abstract: The objective of this work is to propose a new algorithm to fit a sphere on a\nnoisy 3D point cloud distributed around a complete or a truncated sphere. More\nprecisely, we introduce a projected Robbins-Monro algorithm and its averaged\nversion for estimating the center and the radius of the sphere. We give\nasymptotic results such as the almost sure convergence of these algorithms as\nwell as the asymptotic normality of the averaged algorithm. Furthermore, some\nnon-asymptotic results will be given, such as the rates of convergence in\nquadratic mean. Some numerical experiments show the efficiency of the proposed\nalgorithm on simulated data for small to moderate sample sizes. \n\n"}
{"id": "1606.05100", "contents": "Title: PECOK: a convex optimization approach to variable clustering Abstract: The problem of variable clustering is that of grouping similar components of\na $p$-dimensional vector $X=(X_{1},\\ldots,X_{p})$, and estimating these groups\nfrom $n$ independent copies of $X$. When cluster similarity is defined via\n$G$-latent models, in which groups of $X$-variables have a common latent\ngenerator, and groups are relative to a partition $G$ of the index set $\\{1,\n\\ldots, p\\}$, the most natural clustering strategy is $K$-means. We explain why\nthis strategy cannot lead to perfect cluster recovery and offer a correction,\nbased on semi-definite programing, that can be viewed as a penalized convex\nrelaxation of $K$-means (PECOK). We introduce a cluster separation measure\ntailored to $G$-latent models, and derive its minimax lower bound for perfect\ncluster recovery. The clusters estimated by PECOK are shown to recover $G$ at a\nnear minimax optimal cluster separation rate, a result that holds true even if\n$K$, the number of clusters, is estimated adaptively from the data. We compare\nPECOK with appropriate corrections of spectral clustering-type procedures, and\nshow that the former outperforms the latter for perfect cluster recovery of\nminimally separated clusters. \n\n"}
{"id": "1606.06352", "contents": "Title: Visualizing textual models with in-text and word-as-pixel highlighting Abstract: We explore two techniques which use color to make sense of statistical text\nmodels. One method uses in-text annotations to illustrate a model's view of\nparticular tokens in particular documents. Another uses a high-level,\n\"words-as-pixels\" graphic to display an entire corpus. Together, these methods\noffer both zoomed-in and zoomed-out perspectives into a model's understanding\nof text. We show how these interconnected methods help diagnose a classifier's\npoor performance on Twitter slang, and make sense of a topic model on\nhistorical political texts. \n\n"}
{"id": "1606.06519", "contents": "Title: Kernel Spectral Clustering Abstract: We investigate the question of studying spectral clustering in a Hilbert\nspace where the set of points to cluster are drawn i.i.d. according to an\nunknown probability distribution whose support is a union of compact connected\ncomponents. We modify the algorithm proposed by Ng, Jordan and Weiss in order\nto propose a new algorithm that automatically estimates the number of clusters\nand we characterize the convergence of this new algorithm in terms of\nconvergence of Gram operators. We also give a hint of how this approach may\nlead to learn transformation-invariant representations in the context of image\nclassification. \n\n"}
{"id": "1606.07268", "contents": "Title: Semi-supervised Inference: General Theory and Estimation of Means Abstract: We propose a general semi-supervised inference framework focused on the\nestimation of the population mean. As usual in semi-supervised settings, there\nexists an unlabeled sample of covariate vectors and a labeled sample consisting\nof covariate vectors along with real-valued responses (\"labels\"). Otherwise,\nthe formulation is \"assumption-lean\" in that no major conditions are imposed on\nthe statistical or functional form of the data. We consider both the ideal\nsemi-supervised setting where infinitely many unlabeled samples are available,\nas well as the ordinary semi-supervised setting in which only a finite number\nof unlabeled samples is available.\n  Estimators are proposed along with corresponding confidence intervals for the\npopulation mean. Theoretical analysis on both the asymptotic distribution and\n$\\ell_2$-risk for the proposed procedures are given. Surprisingly, the proposed\nestimators, based on a simple form of the least squares method, outperform the\nordinary sample mean. The simple, transparent form of the estimator lends\nconfidence to the perception that its asymptotic improvement over the ordinary\nsample mean also nearly holds even for moderate size samples. The method is\nfurther extended to a nonparametric setting, in which the oracle rate can be\nachieved asymptotically. The proposed estimators are further illustrated by\nsimulation studies and a real data example involving estimation of the homeless\npopulation. \n\n"}
{"id": "1606.07312", "contents": "Title: Unsupervised preprocessing for Tactile Data Abstract: Tactile information is important for gripping, stable grasp, and in-hand\nmanipulation, yet the complexity of tactile data prevents widespread use of\nsuch sensors. We make use of an unsupervised learning algorithm that transforms\nthe complex tactile data into a compact, latent representation without the need\nto record ground truth reference data. These compact representations can either\nbe used directly in a reinforcement learning based controller or can be used to\ncalibrate the tactile sensor to physical quantities with only a few datapoints.\nWe show the quality of our latent representation by predicting important\nfeatures and with a simple control task. \n\n"}
{"id": "1606.07384", "contents": "Title: Robust Learning of Fixed-Structure Bayesian Networks Abstract: We investigate the problem of learning Bayesian networks in a robust model\nwhere an $\\epsilon$-fraction of the samples are adversarially corrupted. In\nthis work, we study the fully observable discrete case where the structure of\nthe network is given. Even in this basic setting, previous learning algorithms\neither run in exponential time or lose dimension-dependent factors in their\nerror guarantees. We provide the first computationally efficient robust\nlearning algorithm for this problem with dimension-independent error\nguarantees. Our algorithm has near-optimal sample complexity, runs in\npolynomial time, and achieves error that scales nearly-linearly with the\nfraction of adversarially corrupted samples. Finally, we show on both synthetic\nand semi-synthetic data that our algorithm performs well in practice. \n\n"}
{"id": "1606.07702", "contents": "Title: Optimal adaptation for early stopping in statistical inverse problems Abstract: For linear inverse problems $Y=\\mathsf{A}\\mu+\\xi$, it is classical to recover\nthe unknown signal $\\mu$ by iterative regularisation methods $(\\widehat\n\\mu^{(m)}, m=0,1,\\ldots)$ and halt at a data-dependent iteration $\\tau$ using\nsome stopping rule, typically based on a discrepancy principle, so that the\nweak (or prediction) squared-error $\\|\\mathsf{A}(\\widehat\n\\mu^{(\\tau)}-\\mu)\\|^2$ is controlled. In the context of statistical estimation\nwith stochastic noise $\\xi$, we study oracle adaptation (that is, compared to\nthe best possible stopping iteration) in strong squared-error $E[\\|\\hat\n\\mu^{(\\tau)}-\\mu\\|^2]$.\n  For a residual-based stopping rule oracle adaptation bounds are established\nfor general spectral regularisation methods. The proofs use bias and variance\ntransfer techniques from weak prediction error to strong $L^2$-error, as well\nas convexity arguments and concentration bounds for the stochastic part.\nAdaptive early stopping for the Landweber method is studied in further detail\nand illustrated numerically. \n\n"}
{"id": "1606.07798", "contents": "Title: Which causal structures might support a quantum-classical gap? Abstract: A causal scenario is a graph that describes the cause and effect\nrelationships between all relevant variables in an experiment. A scenario is\ndeemed `not interesting' if there is no device-independent way to distinguish\nthe predictions of classical physics from any generalised probabilistic theory\n(including quantum mechanics). Conversely, an interesting scenario is one in\nwhich there exists a gap between the predictions of different operational\nprobabilistic theories, as occurs for example in Bell-type experiments. Henson,\nLal and Pusey (HLP) recently proposed a sufficient condition for a causal\nscenario to not be interesting. In this paper we supplement their analysis with\nsome new techniques and results. We first show that existing graphical\ntechniques due to Evans can be used to confirm by inspection that many graphs\nare interesting without having to explicitly search for inequality violations.\nFor three exceptional cases -- the graphs numbered 15,16,20 in HLP -- we show\nthat there exist non-Shannon type entropic inequalities that imply these graphs\nare interesting. In doing so, we find that existing methods of entropic\ninequalities can be greatly enhanced by conditioning on the specific values of\ncertain variables. \n\n"}
{"id": "1606.09288", "contents": "Title: Parameter estimation based on cumulative Kullback-Leibler divergence Abstract: In this paper, we propose some estimators for the parameters of a statistical\nmodel based on Kullback-Leibler divergence of the survival function in\ncontinuous setting. We prove that the proposed estimators are subclass of\n\"generalized estimating equations\" estimators. The asymptotic properties of the\nestimators such as consistency, asymptotic normality, asymptotic confidence\ninterval and asymptotic hypothesis testing are investigated. \n\n"}
{"id": "1606.09321", "contents": "Title: Performance of Ensemble Kalman filters in large dimensions Abstract: Contemporary data assimilation often involves more than a million prediction\nvariables. Ensemble Kalman filters (EnKF) have been developed by geoscientists.\nThey are successful indispensable tools in science and engineering, because\nthey allow for computationally cheap low ensemble state approximation for\nextremely large dimensional turbulent dynamical systems. The practical finite\nensemble filter like EnKF necessarily involve modifications such as covariance\ninflation and localization, and it is a genuine mystery why they perform so\nwell with small ensemble sizes in large dimensions. This paper provides the\nfirst rigorous stochastic analysis of the accuracy and covariance fidelity of\nEnKF in the practical regime where the ensemble size is much smaller than the\nlarge ambient dimension for EnKFs with random coefficients. A challenging issue\novercome here is that EnKF in huge dimensions introduces unavoidable bias and\nmodel errors which need to be controlled and estimated. \n\n"}
{"id": "1606.09424", "contents": "Title: Variance Allocation and Shapley Value Abstract: Motivated by the problem of utility allocation in a portfolio under a\nMarkowitz mean-variance choice paradigm, we propose an allocation criterion for\nthe variance of the sum of $n$ possibly dependent random variables. This\ncriterion, the Shapley value, requires to translate the problem into a\ncooperative game. The Shapley value has nice properties, but, in general, is\ncomputationally demanding. The main result of this paper shows that in our\nparticular case the Shapley value has a very simple form that can be easily\ncomputed. The same criterion is used also to allocate the standard deviation of\nthe sum of $n$ random variables and a conjecture about the relation of the\nvalues in the two games is formulated. \n\n"}
{"id": "1607.00360", "contents": "Title: A scaled Bregman theorem with applications Abstract: Bregman divergences play a central role in the design and analysis of a range\nof machine learning algorithms. This paper explores the use of Bregman\ndivergences to establish reductions between such algorithms and their analyses.\nWe present a new scaled isodistortion theorem involving Bregman divergences\n(scaled Bregman theorem for short) which shows that certain \"Bregman\ndistortions'\" (employing a potentially non-convex generator) may be exactly\nre-written as a scaled Bregman divergence computed over transformed data.\nAdmissible distortions include geodesic distances on curved manifolds and\nprojections or gauge-normalisation, while admissible data include scalars,\nvectors and matrices.\n  Our theorem allows one to leverage to the wealth and convenience of Bregman\ndivergences when analysing algorithms relying on the aforementioned Bregman\ndistortions. We illustrate this with three novel applications of our theorem: a\nreduction from multi-class density ratio to class-probability estimation, a new\nadaptive projection free yet norm-enforcing dual norm mirror descent algorithm,\nand a reduction from clustering on flat manifolds to clustering on curved\nmanifolds. Experiments on each of these domains validate the analyses and\nsuggest that the scaled Bregman theorem might be a worthy addition to the\npopular handful of Bregman divergence properties that have been pervasive in\nmachine learning. \n\n"}
{"id": "1607.01892", "contents": "Title: Uncertainty quantification for the horseshoe Abstract: We investigate the credible sets and marginal credible intervals resulting\nfrom the horseshoe prior in the sparse multivariate normal means model. We do\nso in an adaptive setting without assuming knowledge of the sparsity level\n(number of signals). We consider both the hierarchical Bayes method of putting\na prior on the unknown sparsity level and the empirical Bayes method with the\nsparsity level estimated by maximum marginal likelihood. We show that credible\nballs and marginal credible intervals have good frequentist coverage and\noptimal size if the sparsity level of the prior is set correctly. By general\ntheory honest confidence sets cannot adapt in size to an unknown sparsity\nlevel. Accordingly the hierarchical and empirical Bayes credible sets based on\nthe horseshoe prior are not honest over the full parameter space. We show that\nthis is due to over-shrinkage for certain parameters and characterise the set\nof parameters for which credible balls and marginal credible intervals do give\ncorrect uncertainty quantification. In particular we show that the fraction of\nfalse discoveries by the marginal Bayesian procedure is controlled by a correct\nchoice of cut-off. \n\n"}
{"id": "1607.03569", "contents": "Title: Partition structure and the A-hypergeometric distribution associated\n  with the rational normal curve Abstract: A distribution whose normalization constant is an A-hypergeometric polynomial\nis called an A-hypergeometric distribution. Such a distribution is in turn a\ngeneralization of the generalized hypergeometric distribution on the\ncontingency tables with fixed marginal sums. In this paper, we will see that an\nA-hypergeometric distribution with a homogeneous matrix of two rows,\nespecially, that associated with the rational normal curve, appears in\ninferences involving exchangeable partition structures. An exact sampling\nalgorithm is presented for the general (any number of rows) A-hypergeometric\ndistributions. Then, the maximum likelihood estimation of the A-hypergeometric\ndistribution associated with the rational normal curve, which is an algebraic\nexponential family, is discussed. The information geometry of the Newton\npolytope is useful for analyzing the full and the curved exponential family.\nAlgebraic methods are provided for evaluating the A-hypergeometric polynomials. \n\n"}
{"id": "1607.03990", "contents": "Title: Fast Algorithms for Segmented Regression Abstract: We study the fixed design segmented regression problem: Given noisy samples\nfrom a piecewise linear function $f$, we want to recover $f$ up to a desired\naccuracy in mean-squared error.\n  Previous rigorous approaches for this problem rely on dynamic programming\n(DP) and, while sample efficient, have running time quadratic in the sample\nsize. As our main contribution, we provide new sample near-linear time\nalgorithms for the problem that -- while not being minimax optimal -- achieve a\nsignificantly better sample-time tradeoff on large datasets compared to the DP\napproach. Our experimental evaluation shows that, compared with the DP\napproach, our algorithms provide a convergence rate that is only off by a\nfactor of $2$ to $4$, while achieving speedups of three orders of magnitude. \n\n"}
{"id": "1607.05735", "contents": "Title: Quantum Relative Lorenz Curves Abstract: The theory of majorization and its variants, including thermomajorization,\nhave been found to play a central role in the formulation of many physical\nresource theories, ranging from entanglement theory to quantum thermodynamics.\nHere we formulate the framework of quantum relative Lorenz curves, and show how\nit is able to unify majorization, thermomajorization, and their noncommutative\nanalogues. In doing so, we define the family of Hilbert $\\alpha$-divergences\nand show how it relates with other divergences used in quantum information\ntheory. We then apply these tools to the problem of deciding the existence of a\nsuitable transformation from an initial pair of quantum states to a final one,\nfocusing in particular on applications to the resource theory of athermality, a\nprecursor of quantum thermodynamics. \n\n"}
{"id": "1607.05980", "contents": "Title: Causal inference in partially linear structural equation models Abstract: We consider identifiability of partially linear additive structural equation\nmodels with Gaussian noise (PLSEMs) and estimation of distributionally\nequivalent models to a given PLSEM. Thereby, we also include robustness results\nfor errors in the neighborhood of Gaussian distributions. Existing\nidentifiability results in the framework of additive SEMs with Gaussian noise\nare limited to linear and nonlinear SEMs, which can be considered as special\ncases of PLSEMs with vanishing nonparametric or parametric part, respectively.\nWe close the wide gap between these two special cases by providing a\ncomprehensive theory of the identifiability of PLSEMs by means of (A) a\ngraphical, (B) a transformational, (C) a functional and (D) a causal ordering\ncharacterization of PLSEMs that generate a given distribution P. In particular,\nthe characterizations (C) and (D) answer the fundamental question to which\nextent nonlinear functions in additive SEMs with Gaussian noise restrict the\nset of potential causal models and hence influence the identifiability. On the\nbasis of the transformational characterization (B) we provide a score-based\nestimation procedure that outputs the graphical representation (A) of the\ndistribution equivalence class of a given PLSEM. We derive its\n(high-dimensional) consistency and demonstrate its performance on simulated\ndatasets. \n\n"}
{"id": "1607.07343", "contents": "Title: Gaussian processes and Bayesian moment estimation Abstract: Given a set of moment restrictions (MRs) that overidentify a parameter\n$\\theta$, we investigate a semiparametric Bayesian approach for inference on\n$\\theta$ that does not restrict the data distribution $F$ apart from the MRs.\nAs main contribution, we construct a degenerate Gaussian process prior that,\nconditionally on $\\theta$, restricts the $F$ generated by this prior to satisfy\nthe MRs with probability one. Our prior works even in the more involved case\nwhere the number of MRs is larger than the dimension of $\\theta$. We\ndemonstrate that the corresponding posterior for $\\theta$ is computationally\nconvenient. Moreover, we show that there exists a link between our procedure,\nthe Generalized Empirical Likelihood with quadratic criterion and the limited\ninformation likelihood-based procedures. We provide a frequentist validation of\nour procedure by showing consistency and asymptotic normality of the posterior\ndistribution of $\\theta$. The finite sample properties of our method are\nillustrated through Monte Carlo experiments and we provide an application to\ndemand estimation in the airline market. \n\n"}
{"id": "1607.08077", "contents": "Title: Algorithmic statistics: forty years later Abstract: Algorithmic statistics has two different (and almost orthogonal) motivations.\nFrom the philosophical point of view, it tries to formalize how the statistics\nworks and why some statistical models are better than others. After this notion\nof a \"good model\" is introduced, a natural question arises: it is possible that\nfor some piece of data there is no good model? If yes, how often these bad\n(\"non-stochastic\") data appear \"in real life\"?\n  Another, more technical motivation comes from algorithmic information theory.\nIn this theory a notion of complexity of a finite object (=amount of\ninformation in this object) is introduced; it assigns to every object some\nnumber, called its algorithmic complexity (or Kolmogorov complexity).\nAlgorithmic statistic provides a more fine-grained classification: for each\nfinite object some curve is defined that characterizes its behavior. It turns\nout that several different definitions give (approximately) the same curve.\n  In this survey we try to provide an exposition of the main results in the\nfield (including full proofs for the most important ones), as well as some\nhistorical comments. We assume that the reader is familiar with the main\nnotions of algorithmic information (Kolmogorov complexity) theory. \n\n"}
{"id": "1607.08554", "contents": "Title: Statistical Properties of Sanitized Results from Differentially Private\n  Laplace Mechanism with Univariate Bounding Constraints Abstract: Protection of individual privacy is a common concern when releasing and\nsharing data and information. Differential privacy (DP) formalizes privacy in\nprobabilistic terms without making assumptions about the background knowledge\nof data intruders, and thus provides a robust concept for privacy protection.\nPractical applications of DP involve development of differentially private\nmechanisms to generate sanitized results at a pre-specified privacy budget. For\nthe sanitization of statistics with publicly known bounds such as proportions\nand correlation coefficients, the bounding constraints will need to be\nincorporated in the differentially private mechanisms. There has been little\nwork on examining the consequences of the bounding constraints on the accuracy\nof sanitized results and the statistical inferences of the population\nparameters based on the sanitized results. In this paper, we formalize the\ndifferentially private truncated and boundary inflated truncated (BIT)\nprocedures for releasing statistics with publicly known bounding constraints.\nThe impacts of the truncated and BIT Laplace procedures on the statistical\naccuracy and validity of sanitized statistics are evaluated both theoretically\nand empirically via simulation studies. \n\n"}
{"id": "1608.00696", "contents": "Title: Can we trust the bootstrap in high-dimension? Abstract: We consider the performance of the bootstrap in high-dimensions for the\nsetting of linear regression, where $p<n$ but $p/n$ is not close to zero. We\nconsider ordinary least-squares as well as robust regression methods and adopt\na minimalist performance requirement: can the bootstrap give us good confidence\nintervals for a single coordinate of $\\beta$? (where $\\beta$ is the true\nregression vector).\n  We show through a mix of numerical and theoretical work that the bootstrap is\nfraught with problems. Both of the most commonly used methods of bootstrapping\nfor regression -- residual bootstrap and pairs bootstrap -- give very poor\ninference on $\\beta$ as the ratio $p/n$ grows. We find that the residuals\nbootstrap tend to give anti-conservative estimates (inflated Type I error),\nwhile the pairs bootstrap gives very conservative estimates (severe loss of\npower) as the ratio $p/n$ grows. We also show that the jackknife resampling\ntechnique for estimating the variance of $\\hat{\\beta}$ severely overestimates\nthe variance in high dimensions.\n  We contribute alternative bootstrap procedures based on our theoretical\nresults that mitigate these problems. However, the corrections depend on\nassumptions regarding the underlying data-generation model, suggesting that in\nhigh-dimensions it may be difficult to have universal, robust bootstrapping\ntechniques. \n\n"}
{"id": "1608.00874", "contents": "Title: Modelling and computation using NCoRM mixtures for density regression Abstract: Normalized compound random measures are flexible nonparametric priors for\nrelated distributions. We consider building general nonparametric regression\nmodels using normalized compound random measure mixture models. Posterior\ninference is made using a novel pseudo-marginal Metropolis-Hastings sampler for\nnormalized compound random measure mixture models. The algorithm makes use of a\nnew general approach to the unbiased estimation of Laplace functionals of\ncompound random measures (which includes completely random measures as a\nspecial case). The approach is illustrated on problems of density regression. \n\n"}
{"id": "1608.01824", "contents": "Title: The Le Cam distance between density estimation, Poisson processes and\n  Gaussian white noise Abstract: It is well-known that density estimation on the unit interval is\nasymptotically equivalent to a Gaussian white noise experiment, provided the\ndensities have H\\\"older smoothness larger than $1/2$ and are uniformly bounded\naway from zero. We derive matching lower and constructive upper bounds for the\nLe Cam deficiencies between these experiments, with explicit dependence on both\nthe sample size and the size of the densities in the parameter space. As a\nconsequence, we derive sharp conditions on how small the densities can be for\nasymptotic equivalence to hold. The related case of Poisson intensity\nestimation is also treated. \n\n"}
{"id": "1608.03487", "contents": "Title: A Richer Theory of Convex Constrained Optimization with Reduced\n  Projections and Improved Rates Abstract: This paper focuses on convex constrained optimization problems, where the\nsolution is subject to a convex inequality constraint. In particular, we aim at\nchallenging problems for which both projection into the constrained domain and\na linear optimization under the inequality constraint are time-consuming, which\nrender both projected gradient methods and conditional gradient methods (a.k.a.\nthe Frank-Wolfe algorithm) expensive. In this paper, we develop projection\nreduced optimization algorithms for both smooth and non-smooth optimization\nwith improved convergence rates under a certain regularity condition of the\nconstraint function. We first present a general theory of optimization with\nonly one projection. Its application to smooth optimization with only one\nprojection yields $O(1/\\epsilon)$ iteration complexity, which improves over the\n$O(1/\\epsilon^2)$ iteration complexity established before for non-smooth\noptimization and can be further reduced under strong convexity. Then we\nintroduce a local error bound condition and develop faster algorithms for\nnon-strongly convex optimization at the price of a logarithmic number of\nprojections. In particular, we achieve an iteration complexity of $\\widetilde\nO(1/\\epsilon^{2(1-\\theta)})$ for non-smooth optimization and $\\widetilde\nO(1/\\epsilon^{1-\\theta})$ for smooth optimization, where $\\theta\\in(0,1]$\nappearing the local error bound condition characterizes the functional local\ngrowth rate around the optimal solutions. Novel applications in solving the\nconstrained $\\ell_1$ minimization problem and a positive semi-definite\nconstrained distance metric learning problem demonstrate that the proposed\nalgorithms achieve significant speed-up compared with previous algorithms. \n\n"}
{"id": "1608.04550", "contents": "Title: Fast Calculation of the Knowledge Gradient for Optimization of\n  Deterministic Engineering Simulations Abstract: A novel efficient method for computing the Knowledge-Gradient policy for\nContinuous Parameters (KGCP) for deterministic optimization is derived. The\ndifferences with Expected Improvement (EI), a popular choice for Bayesian\noptimization of deterministic engineering simulations, are explored. Both\npolicies and the Upper Confidence Bound (UCB) policy are compared on a number\nof benchmark functions including a problem from structural dynamics. It is\nempirically shown that KGCP has similar performance as the EI policy for many\nproblems, but has better convergence properties for complex (multi-modal)\noptimization problems as it emphasizes more on exploration when the model is\nconfident about the shape of optimal regions. In addition, the relationship\nbetween Maximum Likelihood Estimation (MLE) and slice sampling for estimation\nof the hyperparameters of the underlying models, and the complexity of the\nproblem at hand, is studied. \n\n"}
{"id": "1608.04861", "contents": "Title: Adaptive confidence sets for matrix completion Abstract: In the present paper we study the problem of existence of honest and adaptive\nconfidence sets for matrix completion. We consider two statistical models: the\ntrace regression model and the Bernoulli model. In the trace regression model,\nwe show that honest confidence sets that adapt to the unknown rank of the\nmatrix exist even when the error variance is unknown. Contrary to this, we\nprove that in the Bernoulli model, honest and adaptive confidence sets exist\nonly when the error variance is known a priori. In the course of our proofs we\nobtain bounds for the minimax rates of certain composite hypothesis testing\nproblems arising in low rank inference. \n\n"}
{"id": "1608.08367", "contents": "Title: Pattern Coding Meets Censoring: (almost) Adaptive Coding on Countable\n  Alphabets Abstract: Adaptive coding faces the following problem: given a collection of source\nclasses such that each class in the collection has non-trivial minimax\nredundancy rate, can we design a single code which is asymptotically minimax\nover each class in the collection? In particular, adaptive coding makes sense\nwhen there is no universal code on the union of classes in the collection. In\nthis paper, we deal with classes of sources over an infinite alphabet, that are\ncharacterized by a dominating envelope. We provide asymptotic equivalents for\nthe redundancy of envelope classes enjoying a regular variation property. We\nfinally construct a computationally efficient online prefix code, which\ninterleaves the encoding of the so-called pattern of the message and the\nencoding of the dictionary of discovered symbols. This code is shown to be\nadaptive, within a $\\log\\log n$ factor, over the collection of regularly\nvarying envelope classes. The code is both simpler and less redundant than\npreviously described contenders. In contrast with previous attempts, it also\ncovers the full range of slowly varying envelope classes. \n\n"}
{"id": "1609.01025", "contents": "Title: Structured signal recovery from non-linear and heavy-tailed measurements Abstract: We study high-dimensional signal recovery from non-linear measurements with\ndesign vectors having elliptically symmetric distribution. Special attention is\ndevoted to the situation when the unknown signal belongs to a set of low\nstatistical complexity, while both the measurements and the design vectors are\nheavy-tailed. We propose and analyze a new estimator that adapts to the\nstructure of the problem, while being robust both to the possible model\nmisspecification characterized by arbitrary non-linearity of the measurements\nas well as to data corruption modeled by the heavy-tailed distributions.\nMoreover, this estimator has low computational complexity. Our results are\nexpressed in the form of exponential concentration inequalities for the error\nof the proposed estimator. On the technical side, our proofs rely on the\ngeneric chaining methods, and illustrate the power of this approach for\nstatistical applications. Theory is supported by numerical experiments\ndemonstrating that our estimator outperforms existing alternatives when data is\nheavy-tailed. \n\n"}
{"id": "1609.03958", "contents": "Title: Noisy Inductive Matrix Completion Under Sparse Factor Models Abstract: Inductive Matrix Completion (IMC) is an important class of matrix completion\nproblems that allows direct inclusion of available features to enhance\nestimation capabilities. These models have found applications in personalized\nrecommendation systems, multilabel learning, dictionary learning, etc. This\npaper examines a general class of noisy matrix completion tasks where the\nunderlying matrix is following an IMC model i.e., it is formed by a mixing\nmatrix (a priori unknown) sandwiched between two known feature matrices. The\nmixing matrix here is assumed to be well approximated by the product of two\nsparse matrices---referred here to as \"sparse factor models.\" We leverage the\nmain theorem of Soni:2016:NMC and extend it to provide theoretical error bounds\nfor the sparsity-regularized maximum likelihood estimators for the class of\nproblems discussed in this paper. The main result is general in the sense that\nit can be used to derive error bounds for various noise models. In this paper,\nwe instantiate our main result for the case of Gaussian noise and provide\ncorresponding error bounds in terms of squared loss. \n\n"}
{"id": "1609.05067", "contents": "Title: Asymptotic frequentist coverage properties of Bayesian credible sets for\n  sieve priors Abstract: We investigate the frequentist coverage properties of Bayesian credible sets\nin a general, adaptive, nonparametric framework. It is well known that the\nconstruction of adaptive and honest confidence sets is not possible in general.\nTo overcome this problem we introduce an extra assumption on the functional\nparameters, the so called \"general polished tail\" condition. We then show that\nunder standard assumptions both the hierarchical and empirical Bayes methods\nresults in honest confidence sets for sieve type of priors in general settings\nand we characterize their size. We apply the derived abstract results to\nvarious examples, including the nonparametric regression model, density\nestimation using exponential families of priors, density estimation using\nhistogram priors and nonparametric classification model, for which we show that\ntheir size is near minimax adaptive with respect to the considered specific\nsemi-metrics. \n\n"}
{"id": "1609.06789", "contents": "Title: Krigings Over Space and Time Based on Latent Low-Dimensional Structures Abstract: We propose a new approach to represent nonparametrically the linear\ndependence structure of a spatio-temporal process in terms of latent common\nfactors. Though it is formally similar to the existing reduced rank\napproximation methods (Section 7.1.3 of Cressie and Wikle, 2011), the\nfundamental difference is that the low-dimensional structure is completely\nunknown in our setting, which is learned from the data collected irregularly\nover space but regularly over time. Furthermore a graph Laplacian is\nincorporated in the learning in order to take the advantage of the continuity\nover space, and a new aggregation method via randomly partitioning space is\nintroduced to improve the efficiency. We do not impose any stationarity\nconditions over space either, as the learning is facilitated by the\nstationarity in time. Krigings over space and time are carried out based on the\nlearned low-dimensional structure, which is scalable to the cases when the data\nare taken over a large number of locations and/or over a long time period.\nAsymptotic properties of the proposed methods are established. Illustration\nwith both simulated and real data sets is also reported. \n\n"}
{"id": "1609.08199", "contents": "Title: Least squares estimator of fractional Ornstein Uhlenbeck processes with\n  periodic mean Abstract: We first study the drift parameter estimation of the fractional\nOrnstein-Uhlenbeck process (fOU) with periodic mean for every\n$\\frac{1}{2}<H<1$. More precisely, we extend the consistency proved in\n\\cite{DFW} for $\\frac{1}{2}<H<\\frac{3}{4}$ to the strong consistency for any\n$\\frac{1}{2}<H<1$ on the one hand, and on the other, we also discuss the\nasymptotic normality given in \\cite{DFW}. In the second main part of the paper,\nwe study the strong consistency and the asymptotic normality of the fOU of the\nsecond kind with periodic mean for any $\\frac{1}{2}<H<1$. \n\n"}
{"id": "1609.09272", "contents": "Title: A New Algorithm for Circulant Rational Covariance Extension and\n  Applications to Finite-interval Smoothing Abstract: The partial stochastic realization of periodic processes from finite\ncovariance data has recently been solved by Lindquist and Picci based on convex\noptimization of a generalized entropy functional. The meaning and the role of\nthis criterion have an unclear origin. In this paper we propose a solution\nbased on a nonlinear generalization of the classical Yule-Walker type equations\nand on a new iterative algorithm which is shown to converge to the same\n(unique) solution of the variational problem. This provides a conceptual link\nto the variational principles and at the same time yields a robust algorithm\nwhich can for example be successfully applied to finite-interval smoothing\nproblems providing a simpler procedure if compared with the classical\nRiccati-based calculations. \n\n"}
{"id": "1609.09287", "contents": "Title: Two-time-scale stochastic partial differential equations driven by\n  $\\alpha$-stable noises: Averaging principles Abstract: This paper focuses on stochastic partial differential equations (SPDEs) under\ntwo-time-scale formulation. Distinct from the work in the existing literature,\nthe systems are driven by $\\alpha$-stable processes with $\\alpha \\in(1,2)$. In\naddition, the SPDEs are either modulated by a continuous-time Markov chain with\na finite state space or have an addition fast jump component. The inclusion of\nthe Markov chain is for the needs of treating random environment, whereas the\naddition of the fast jump process enables the consideration of discontinuity in\nthe sample paths of the fast processes. Assuming either a fast changing Markov\nswitching or an additional fast-varying jump process, this work aims to obtain\nthe averaging principles for such systems. There are several distinct\ndifficulties. First, the noise is not square integrable. Second, in our setup,\nfor the underlying SPDE, there is only a unique mild solution and as a result,\nthere is only mild It\\^{o}'s formula that can be used. Moreover, another new\naspect is the addition of the fast regime switching and the addition of the\nfast varying jump processes in the formulation, which enlarges the\napplicability of the underlying systems. To overcome these difficulties, a\nsemigroup approach is taken. Under suitable conditions, it is proved that the\n$p$th moment convergence takes place with $p\\in(1,\\alpha )$, which is stronger\nthan the usual weak convergence approaches. \n\n"}
{"id": "1610.00207", "contents": "Title: Tuning parameter calibration for $\\ell_1$-regularized logistic\n  regression Abstract: Feature selection is a standard approach to understanding and modeling\nhigh-dimensional classification data, but the corresponding statistical methods\nhinge on tuning parameters that are difficult to calibrate. In particular,\nexisting calibration schemes in the logistic regression framework lack any\nfinite sample guarantees. In this paper, we introduce a novel calibration\nscheme for $\\ell_1$-penalized logistic regression. It is based on simple tests\nalong the tuning parameter path and is equipped with optimal guarantees for\nfeature selection. It is also amenable to easy and efficient implementations,\nand it rivals or outmatches existing methods in simulations and real data\napplications. \n\n"}
{"id": "1610.00690", "contents": "Title: How the instability of ranks under long memory affects large-sample\n  inference Abstract: Under long memory, the limit theorems for normalized sums of random variables\ntypically involve a positive integer called \"Hermite rank\". There is a\ndifferent limit for each Hermite rank. From a statistical point of view,\nhowever, we argue that a rank other than one is unstable, whereas, a rank equal\nto one is stable. We provide empirical evidence supporting this argument. This\nhas important consequences. Assuming a higher-order rank when it is not really\nthere usually results in underestimating the order of the fluctuations of the\nstatistic of interest. We illustrate this through various examples involving\nthe sample variance, the empirical processes and the Whittle estimator. \n\n"}
{"id": "1610.05683", "contents": "Title: Reparameterization Gradients through Acceptance-Rejection Sampling\n  Algorithms Abstract: Variational inference using the reparameterization trick has enabled\nlarge-scale approximate Bayesian inference in complex probabilistic models,\nleveraging stochastic optimization to sidestep intractable expectations. The\nreparameterization trick is applicable when we can simulate a random variable\nby applying a differentiable deterministic function on an auxiliary random\nvariable whose distribution is fixed. For many distributions of interest (such\nas the gamma or Dirichlet), simulation of random variables relies on\nacceptance-rejection sampling. The discontinuity introduced by the\naccept-reject step means that standard reparameterization tricks are not\napplicable. We propose a new method that lets us leverage reparameterization\ngradients even when variables are outputs of a acceptance-rejection sampling\nalgorithm. Our approach enables reparameterization on a larger class of\nvariational distributions. In several studies of real and synthetic data, we\nshow that the variance of the estimator of the gradient is significantly lower\nthan other state-of-the-art methods. This leads to faster convergence of\nstochastic gradient variational inference. \n\n"}
{"id": "1610.05792", "contents": "Title: Big Batch SGD: Automated Inference using Adaptive Batch Sizes Abstract: Classical stochastic gradient methods for optimization rely on noisy gradient\napproximations that become progressively less accurate as iterates approach a\nsolution. The large noise and small signal in the resulting gradients makes it\ndifficult to use them for adaptive stepsize selection and automatic stopping.\nWe propose alternative \"big batch\" SGD schemes that adaptively grow the batch\nsize over time to maintain a nearly constant signal-to-noise ratio in the\ngradient approximation. The resulting methods have similar convergence rates to\nclassical SGD, and do not require convexity of the objective. The high fidelity\ngradients enable automated learning rate selection and do not require stepsize\ndecay. Big batch methods are thus easily automated and can run with little or\nno oversight. \n\n"}
{"id": "1610.06107", "contents": "Title: Prediction error after model search Abstract: Estimation of the prediction error of a linear estimation rule is difficult\nif the data analyst also use data to select a set of variables and construct\nthe estimation rule using only the selected variables. In this work, we propose\nan asymptotically unbiased estimator for the prediction error after model\nsearch. Under some additional mild assumptions, we show that our estimator\nconverges to the true prediction error in $L^2$ at the rate of $O(n^{-1/2})$,\nwith $n$ being the number of data points. Our estimator applies to general\nselection procedures, not requiring analytical forms for the selection. The\nnumber of variables to select from can grow as an exponential factor of $n$,\nallowing applications in high-dimensional data. It also allows model\nmisspecifications, not requiring linear underlying models. One application of\nour method is that it provides an estimator for the degrees of freedom for many\ndiscontinuous estimation rules like best subset selection or relaxed Lasso.\nConnection to Stein's Unbiased Risk Estimator is discussed. We consider\nin-sample prediction errors in this work, with some extension to out-of-sample\nerrors in low dimensional, linear models. Examples such as best subset\nselection and relaxed Lasso are considered in simulations, where our estimator\noutperforms both $C_p$ and cross validation in various settings. \n\n"}
{"id": "1610.06960", "contents": "Title: Permutation tests in the two-sample problem for functional data Abstract: Three different permutation test schemes are discussed and compared in the\ncontext of the two-sample problem for functional data. One of the procedures\nwas essentially introduced by Lopez-Pintado and Romo (2009), using notions of\nfunctional data depth to adapt the ideas originally proposed by Liu and Singh\n(1993) for multivariate data. Of the new methods introduced here, one is also\nbased on functional data depths, but uses a different way (inspired by\nMeta-Analysis) to assess the significance of the depth differences. The second\nnew method presented here adapts, to the functional data setting, the\nk-nearest-neighbors statistic of Schilling (1986). The three methods are\ncompared among them and against the test of Horvath and Kokoszka (2012) in\nsimulated examples and real data. The comparison considers the performance of\nthe statistics in terms of statistical power and in terms of computational\ncost. \n\n"}
{"id": "1610.07507", "contents": "Title: High-Dimensional Adaptive Function-on-Scalar Regression Abstract: Applications of functional data with large numbers of predictors have grown\nprecipitously in recent years, driven, in part, by rapid advances in genotyping\ntechnologies. Given the large numbers of genetic mutations encountered in\ngenetic association studies, statistical methods which more fully exploit the\nunderlying structure of the data are imperative for maximizing statistical\npower. However, there is currently very limited work in functional data with\nlarge numbers of predictors. Tools are presented for simultaneous variable\nselection and parameter estimation in a functional linear model with a\nfunctional outcome and a large number of scalar predictors; the technique is\ncalled AFSL for $\\textit{Adaptive Function-on-Scalar Lasso}.$ It is\ndemonstrated how techniques from convex analysis over Hilbert spaces can be\nused to establish a functional version of the oracle property for AFSL over any\nreal separable Hilbert space, even when the number of predictors, $I$, is\nexponentially large compared to the sample size, $N$. AFSL is illustrated via a\nsimulation study and data from the Childhood Asthma Management Program, CAMP,\nselecting those genetic mutations which are important for lung growth. \n\n"}
{"id": "1610.07524", "contents": "Title: Fair prediction with disparate impact: A study of bias in recidivism\n  prediction instruments Abstract: Recidivism prediction instruments provide decision makers with an assessment\nof the likelihood that a criminal defendant will reoffend at a future point in\ntime. While such instruments are gaining increasing popularity across the\ncountry, their use is attracting tremendous controversy. Much of the\ncontroversy concerns potential discriminatory bias in the risk assessments that\nare produced. This paper discusses a fairness criterion originating in the\nfield of educational and psychological testing that has recently been applied\nto assess the fairness of recidivism prediction instruments. We demonstrate how\nadherence to the criterion may lead to considerable disparate impact when\nrecidivism prevalence differs across groups. \n\n"}
{"id": "1610.08148", "contents": "Title: Online Bayesian phylogenetic inference: theoretical foundations via\n  Sequential Monte Carlo Abstract: Phylogenetics, the inference of evolutionary trees from molecular sequence\ndata such as DNA, is an enterprise that yields valuable evolutionary\nunderstanding of many biological systems. Bayesian phylogenetic algorithms,\nwhich approximate a posterior distribution on trees, have become a popular if\ncomputationally expensive means of doing phylogenetics. Modern data collection\ntechnologies are quickly adding new sequences to already substantial databases.\nWith all current techniques for Bayesian phylogenetics, computation must start\nanew each time a sequence becomes available, making it costly to maintain an\nup-to-date estimate of a phylogenetic posterior. These considerations highlight\nthe need for an \\emph{online} Bayesian phylogenetic method which can update an\nexisting posterior with new sequences.\n  Here we provide theoretical results on the consistency and stability of\nmethods for online Bayesian phylogenetic inference based on Sequential Monte\nCarlo (SMC) and Markov chain Monte Carlo (MCMC). We first show a consistency\nresult, demonstrating that the method samples from the correct distribution in\nthe limit of a large number of particles. Next we derive the first reported set\nof bounds on how phylogenetic likelihood surfaces change when new sequences are\nadded. These bounds enable us to characterize the theoretical performance of\nsampling algorithms by bounding the effective sample size (ESS) with a given\nnumber of particles from below. We show that the ESS is guaranteed to grow\nlinearly as the number of particles in an SMC sampler grows. Surprisingly, this\nresult holds even though the dimensions of the phylogenetic model grow with\neach new added sequence. \n\n"}
{"id": "1610.08203", "contents": "Title: Arbres CART et For\\^ets al\\'eatoires, Importance et s\\'election de\n  variables Abstract: Two algorithms proposed by Leo Breiman : CART trees (Classification And\nRegression Trees for) introduced in the first half of the 80s and random\nforests emerged, meanwhile, in the early 2000s, are the subject of this\narticle. The goal is to provide each of the topics, a presentation, a\ntheoretical guarantee, an example and some variants and extensions. After a\npreamble, introduction recalls objectives of classification and regression\nproblems before retracing some predecessors of the Random Forests. Then, a\nsection is devoted to CART trees then random forests are presented. Then, a\nvariable selection procedure based on permutation variable importance is\nproposed. Finally the adaptation of random forests to the Big Data context is\nsketched. \n\n"}
{"id": "1610.09735", "contents": "Title: Community detection with nodal information Abstract: Community detection is one of the fundamental problems in the study of\nnetwork data. Most existing community detection approaches only consider edge\ninformation as inputs, and the output could be suboptimal when nodal\ninformation is available. In such cases, it is desirable to leverage nodal\ninformation for the improvement of community detection accuracy. Towards this\ngoal, we propose a flexible network model incorporating nodal information, and\ndevelop likelihood-based inference methods. For the proposed methods, we\nestablish favorable asymptotic properties as well as efficient algorithms for\ncomputation. Numerical experiments show the effectiveness of our methods in\nutilizing nodal information across a variety of simulated and real network data\nsets. \n\n"}
{"id": "1611.00630", "contents": "Title: The accumulated persistence function, a new useful functional summary\n  statistic for topological data analysis, with a view to brain artery trees\n  and spatial point process applications Abstract: We start with a simple introduction to topological data analysis where the\nmost popular tool is called a persistent diagram. Briefly, a persistent diagram\nis a multiset of points in the plane describing the persistence of topological\nfeatures of a compact set when a scale parameter varies. Since statistical\nmethods are difficult to apply directly on persistence diagrams, various\nalternative functional summary statistics have been suggested, but either they\ndo not contain the full information of the persistence diagram or they are\ntwo-dimensional functions. We suggest a new functional summary statistic that\nis one-dimensional and hence easier to handle, and which under mild conditions\ncontains the full information of the persistence diagram. Its usefulness is\nillustrated in statistical settings concerned with point clouds and brain\nartery trees. The appendix includes additional methods and examples, together\nwith technical details. The R-code used for all examples is available at\nhttp://people.math.aau.dk/~christophe/Rcode.zip. \n\n"}
{"id": "1611.01043", "contents": "Title: Uniformly valid confidence intervals post-model-selection Abstract: We suggest general methods to construct asymptotically uniformly valid\nconfidence intervals post-model-selection. The constructions are based on\nprinciples recently proposed by Berk et al. (2013). In particular the candidate\nmodels used can be misspecified, the target of inference is model-specific, and\ncoverage is guaranteed for any data-driven model selection procedure. After\ndeveloping a general theory we apply our methods to practically important\nsituations where the candidate set of models, from which a working model is\nselected, consists of fixed design homoskedastic or heteroskedastic linear\nmodels, or of binary regression models with general link functions. In an\nextensive simulation study, we find that the proposed confidence intervals\nperform remarkably well, even when compared to existing methods that are\ntailored only for specific model selection procedures. \n\n"}
{"id": "1611.01046", "contents": "Title: Learning to Pivot with Adversarial Networks Abstract: Several techniques for domain adaptation have been proposed to account for\ndifferences in the distribution of the data used for training and testing. The\nmajority of this work focuses on a binary domain label. Similar problems occur\nin a scientific context where there may be a continuous family of plausible\ndata generation processes associated to the presence of systematic\nuncertainties. Robust inference is possible if it is based on a pivot -- a\nquantity whose distribution does not depend on the unknown values of the\nnuisance parameters that parametrize this family of data generation processes.\nIn this work, we introduce and derive theoretical results for a training\nprocedure based on adversarial networks for enforcing the pivotal property (or,\nequivalently, fairness with respect to continuous attributes) on a predictive\nmodel. The method includes a hyperparameter to control the trade-off between\naccuracy and robustness. We demonstrate the effectiveness of this approach with\na toy example and examples from particle physics. \n\n"}
{"id": "1611.01230", "contents": "Title: Bayesian Optical Flow with Uncertainty Quantification Abstract: Optical flow refers to the visual motion observed between two consecutive\nimages. Since the degree of freedom is typically much larger than the\nconstraints imposed by the image observations, the straightforward formulation\nof optical flow as an inverse problem is ill-posed. Standard approaches to\ndetermine optical flow rely on formulating and solving an optimization problem\nthat contains both a data fidelity term and a regularization term, the latter\neffectively resolves the otherwise ill-posedness of the inverse problem. In\nthis work, we depart from the deterministic formalism, and instead treat\noptical flow as a statistical inverse problem. We discuss how a classical\noptical flow solution can be interpreted as a point estimate in this more\ngeneral framework. The statistical approach, whose \"solution\" is a distribution\nof flow fields, which we refer to as Bayesian optical flow, allows not only\n\"point\" estimates (e.g., the computation of average flow field), but also\nstatistical estimates (e.g., quantification of uncertainty) that are beyond any\nstandard method for optical flow. As application, we benchmark Bayesian optical\nflow together with uncertainty quantification using several types of prescribed\nground-truth flow fields and images. \n\n"}
{"id": "1611.02675", "contents": "Title: $k$-connectivity of inhomogeneous random key graphs with unreliable\n  links Abstract: We consider secure and reliable connectivity in wireless sensor networks that\nutilize a heterogeneous random key predistribution scheme. We model the\nunreliability of wireless links by an on-off channel model that induces an\nErd\\H{o}s-R\\'enyi graph, while the heterogeneous scheme induces an\ninhomogeneous random key graph. The overall network can thus be modeled by the\nintersection of both graphs. We present conditions (in the form of zero-one\nlaws) on how to scale the parameters of the intersection model so that with\nhigh probability i) all of its nodes are connected to at least $k$ other nodes;\ni.e., the minimum node degree of the graph is no less than $k$ and ii) the\ngraph is $k$-connected, i.e., the graph remains connected even if any $k-1$\nnodes leave the network. We also present numerical results to support these\nconditions in the finite-node regime. Our results are shown to complement and\ngeneralize several previous work in the literature. \n\n"}
{"id": "1611.06353", "contents": "Title: Cone distribution functions and quantiles for multivariate random\n  variables Abstract: Set-valued quantiles for multivariate distributions with respect to a general\nconvex cone are introduced which are based on a family of (univariate)\ndistribution functions rather than on the joint distribution function. It is\nshown that these quantiles enjoy basically all the properties of univariate\nquantile functions. Relationships to families of univariate quantile functions\nand to depth functions are discussed. Finally, a corresponding Value at Risk\nfor multivariate random variables as well as stochastic orders are introduced\nvia the set-valued approach. \n\n"}
{"id": "1611.07093", "contents": "Title: Using Empirical Covariance Matrix in Enhancing Prediction Accuracy of\n  Linear Models with Missing Information Abstract: Inference and Estimation in Missing Information (MI) scenarios are important\ntopics in Statistical Learning Theory and Machine Learning (ML). In ML\nliterature, attempts have been made to enhance prediction through precise\nfeature selection methods. In sparse linear models, LASSO is well-known in\nextracting the desired support of the signal and resisting against noisy\nsystems. When sparse models are also suffering from MI, the sparse recovery and\ninference of the missing models are taken into account simultaneously. In this\npaper, we will introduce an approach which enjoys sparse regression and\ncovariance matrix estimation to improve matrix completion accuracy, and as a\nresult enhancing feature selection preciseness which leads to reduction in\nprediction Mean Squared Error (MSE). We will compare the effect of employing\ncovariance matrix in enhancing estimation accuracy to the case it is not used\nin feature selection. Simulations show the improvement in the performance as\ncompared to the case where the covariance matrix estimation is not used. \n\n"}
{"id": "1611.07256", "contents": "Title: Adaptive Design of Experiments for Conservative Estimation of Excursion\n  Sets Abstract: We consider the problem of estimating the set of all inputs that leads a\nsystem to some particular behavior. The system is modeled by an\nexpensive-to-evaluate function, such as a computer experiment, and we are\ninterested in its excursion set, i.e. the set of points where the function\ntakes values above or below some prescribed threshold. The objective function\nis emulated with a Gaussian Process (GP) model based on an initial design of\nexperiments enriched with evaluation results at (batch-)sequentially determined\ninput points. The GP model provides conservative estimates for the excursion\nset, which control false positives while minimizing false negatives. We\nintroduce adaptive strategies that sequentially select new evaluations of the\nfunction by reducing the uncertainty on conservative estimates. Following the\nStepwise Uncertainty Reduction approach we obtain new evaluations by minimizing\nadapted criteria. Tractable formulae for the conservative criteria are derived,\nwhich allow more convenient optimization. The method is benchmarked on random\nfunctions generated under the model assumptions in different scenarios of noise\nand batch size. We then apply it to a reliability engineering test case.\nOverall, the proposed strategy of minimizing false negatives in conservative\nestimation achieves competitive performance both in terms of model-based and\nmodel-free indicators. \n\n"}
{"id": "1611.07800", "contents": "Title: Infinite Variational Autoencoder for Semi-Supervised Learning Abstract: This paper presents an infinite variational autoencoder (VAE) whose capacity\nadapts to suit the input data. This is achieved using a mixture model where the\nmixing coefficients are modeled by a Dirichlet process, allowing us to\nintegrate over the coefficients when performing inference. Critically, this\nthen allows us to automatically vary the number of autoencoders in the mixture\nbased on the data. Experiments show the flexibility of our method, particularly\nfor semi-supervised learning, where only a small number of training samples are\navailable. \n\n"}
{"id": "1611.08293", "contents": "Title: Global Testing Against Sparse Alternatives under Ising Models Abstract: In this paper, we study the effect of dependence on detecting sparse signals.\nIn particular, we focus on global testing against sparse alternatives for the\nmeans of binary outcomes following an Ising model, and establish how the\ninterplay between the strength and sparsity of a signal determines its\ndetectability under various notions of dependence. The profound impact of\ndependence is best illustrated under the Curie-Weiss model where we observe the\neffect of a \"thermodynamic\" phase transition. In particular, the critical state\nexhibits a subtle \"blessing of dependence\" phenomenon in that one can detect\nmuch weaker signals at criticality than otherwise. Furthermore, we develop a\ntesting procedure that is broadly applicable to account for dependence and show\nthat it is asymptotically minimax optimal under fairly general regularity\nconditions. \n\n"}
{"id": "1611.08761", "contents": "Title: Ergodicity and Accuracy of Optimal Particle Filters for Bayesian Data\n  Assimilation Abstract: For particle filters and ensemble Kalman filters it is of practical\nimportance to understand how and why data assimilation methods can be effective\nwhen used with a fixed small number of particles, since for many large-scale\napplications it is not practical to deploy algorithms close to the large\nparticle limit asymptotic. In this paper we address this question for particle\nfilters and, in particular, study their accuracy (in the small noise limit) and\nergodicity (for noisy signal and observation) without appealing to the large\nparticle number limit. We first overview the accuracy and minorization\nproperties for the true filtering distribution, working in the setting of\nconditional Gaussianity for the dynamics-observation model. We then show that\nthese properties are inherited by optimal particle filters for any fixed number\nof particles, and use the minorization to establish ergodicity of the filters.\nFor completeness we also prove large particle number consistency results for\nthe optimal particle filters, by writing the update equations for the\nunderlying distributions as recursions. In addition to looking at the optimal\nparticle filter with standard resampling, we derive all the above results for\n(what we term) the Gaussianized optimal particle filter and show that the\ntheoretical properties are favorable for this method, when compared to the\nstandard optimal particle filter. \n\n"}
{"id": "1611.09391", "contents": "Title: Simultaneous Clustering and Estimation of Heterogeneous Graphical Models Abstract: We consider joint estimation of multiple graphical models arising from\nheterogeneous and high-dimensional observations. Unlike most previous\napproaches which assume that the cluster structure is given in advance, an\nappealing feature of our method is to learn cluster structure while estimating\nheterogeneous graphical models. This is achieved via a high dimensional version\nof Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993).\nA joint graphical lasso penalty is imposed on the conditional maximization step\nto extract both homogeneity and heterogeneity components across all clusters.\nOur algorithm is computationally efficient due to fast sparse learning routines\nand can be implemented without unsupervised learning knowledge. The superior\nperformance of our method is demonstrated by extensive experiments and its\napplication to a Glioblastoma cancer dataset reveals some new insights in\nunderstanding the Glioblastoma cancer. In theory, a non-asymptotic error bound\nis established for the output directly from our high dimensional ECM algorithm,\nand it consists of two quantities: statistical error (statistical accuracy) and\noptimization error (computational complexity). Such a result gives a\ntheoretical guideline in terminating our ECM iterations. \n\n"}
{"id": "1611.09972", "contents": "Title: Nonparametric Regression with Adaptive Truncation via a Convex\n  Hierarchical Penalty Abstract: We consider the problem of non-parametric regression with a potentially large\nnumber of covariates. We propose a convex, penalized estimation framework that\nis particularly well-suited for high-dimensional sparse additive models. The\nproposed approach combines appealing features of finite basis representation\nand smoothing penalties for non-parametric estimation. In particular, in the\ncase of additive models, a finite basis representation provides a parsimonious\nrepresentation for fitted functions but is not adaptive when component\nfunctions posses different levels of complexity. On the other hand, a smoothing\nspline type penalty on the component functions is adaptive but does not offer a\nparsimonious representation of the estimated function. The proposed approach\nsimultaneously achieves parsimony and adaptivity in a computationally efficient\nframework. We demonstrate these properties through empirical studies on both\nreal and simulated datasets. We show that our estimator converges at the\nminimax rate for functions within a hierarchical class. We further establish\nminimax rates for a large class of sparse additive models. The proposed method\nis implemented using an efficient algorithm that scales similarly to the Lasso\nwith the number of covariates and samples size. \n\n"}
{"id": "1612.00877", "contents": "Title: Bayesian sparse multiple regression for simultaneous rank reduction and\n  variable selection Abstract: We develop a Bayesian methodology aimed at simultaneously estimating low-rank\nand row-sparse matrices in a high-dimensional multiple-response linear\nregression model. We consider a carefully devised shrinkage prior on the matrix\nof regression coefficients which obviates the need to specify a prior on the\nrank, and shrinks the regression matrix towards low-rank and row-sparse\nstructures. We provide theoretical support to the proposed methodology by\nproving minimax optimality of the posterior mean under the prediction risk in\nultra-high dimensional settings where the number of predictors can grow\nsub-exponentially relative to the sample size. A one-step post-processing\nscheme induced by group lasso penalties on the rows of the estimated\ncoefficient matrix is proposed for variable selection, with default choices of\ntuning parameters. We additionally provide an estimate of the rank using a\nnovel optimization function achieving dimension reduction in the covariate\nspace. We exhibit the performance of the proposed methodology in an extensive\nsimulation study and a real data example. \n\n"}
{"id": "1612.01508", "contents": "Title: Estimating Linear and Quadratic forms via Indirect Observations Abstract: In this paper, we further develop the approach, originating in [14\n(arXiv:1311.6765),20 (arXiv:1604.02576)], to \"computation-friendly\" hypothesis\ntesting and statistical estimation via Convex Programming. Specifically, we\nfocus on estimating a linear or quadratic form of an unknown \"signal,\" known to\nbelong to a given convex compact set, via noisy indirect observations of the\nsignal. Most of the existing theoretical results on the subject deal with\nprecisely stated statistical models and aim at designing statistical inferences\nand quantifying their performance in a closed analytic form. In contrast to\nthis descriptive (and highly instructive) traditional framework, the approach\nwe promote here can be qualified as operational -- the estimation routines and\ntheir risks are yielded by an efficient computation. All we know in advance is\nthat under favorable circumstances to be specified below, the risk of the\nresulting estimate, whether high or low, is provably near-optimal under the\ncircumstances. As a compensation for the lack of \"explanatory power,\" this\napproach is applicable to a much wider family of observation schemes than those\nwhere \"closed form descriptive analysis\" is possible.\n  The paper is a follow-up to our paper [20 (arXiv:1604.02576)] dealing with\nhypothesis testing, in what follows, we apply the machinery developed in this\nreference to estimating linear and quadratic forms. \n\n"}
{"id": "1612.04600", "contents": "Title: Predicting Process Behaviour using Deep Learning Abstract: Predicting business process behaviour is an important aspect of business\nprocess management. Motivated by research in natural language processing, this\npaper describes an application of deep learning with recurrent neural networks\nto the problem of predicting the next event in a business process. This is both\na novel method in process prediction, which has largely relied on explicit\nprocess models, and also a novel application of deep learning methods. The\napproach is evaluated on two real datasets and our results surpass the\nstate-of-the-art in prediction precision. \n\n"}
{"id": "1612.05445", "contents": "Title: Projection Pursuit for non-Gaussian Independent Components Abstract: In independent component analysis it is assumed that the observed random\nvariables are linear combinations of latent, mutually independent random\nvariables called the independent components. Our model further assumes that\nonly the non-Gaussian independent components are of interest, the Gaussian\ncomponents being treated as noise. In this paper projection pursuit is used to\nextract the non-Gaussian components and to separate the corresponding signal\nand noise subspaces. Our choice for the projection index is a convex\ncombination of squared third and fourth cumulants and we estimate the\nnon-Gaussian components either one-by-one (deflation-based approach) or\nsimultaneously (symmetric approach). The properties of both estimates are\nconsidered in detail through the corresponding optimization problems,\nestimating equations, algorithms and asymptotic properties. Various comparisons\nof the estimates show that the two approaches separate the signal and noise\nsubspaces equally well but the symmetric one is generally better in extracting\nthe individual non-Gaussian components. \n\n"}
{"id": "1612.05994", "contents": "Title: Algebraic Problems in Structural Equation Modeling Abstract: The paper gives an overview of recent advances in structural equation\nmodeling. A structural equation model is a multivariate statistical model that\nis determined by a mixed graph, also known as a path diagram. Our focus is on\nthe covariance matrices of linear structural equation models. In the linear\ncase, each covariance is a rational function of parameters that are associated\nto the edges and nodes of the graph. We statistically motivate algebraic\nproblems concerning the rational map that parametrizes the covariance matrix.\nWe review combinatorial tools such as the trek rule, projection to ancestral\nsets, and a graph decomposition due to Jin Tian. Building on these tools, we\ndiscuss advances in parameter identification, i.e., the study of (generic)\ninjectivity of the parametrization, and explain recent results on determinantal\nrelations among the covariances. The paper is based on lectures given at the\n8th Mathematical Society of Japan Seasonal Institute. \n\n"}
{"id": "1612.07490", "contents": "Title: A simple method to construct confidence bands in functional linear\n  regression Abstract: This paper develops a simple method to construct confidence bands, centered\nat a principal component analysis (PCA) based estimator, for the slope function\nin a functional linear regression model with a scalar response variable and a\nfunctional predictor variable. The PCA-based estimator is a series estimator\nwith estimated basis functions, and so construction of valid confidence bands\nfor it is a non-trivial challenge. We propose a confidence band that aims at\ncovering the slope function at \"most\" of points with a prespecified probability\n(level), and prove its asymptotic validity under suitable regularity\nconditions. Importantly, this is the first paper that derives confidence bands\nhaving theoretical justifications for the PCA-based estimator. We also propose\na practical method to choose the cut-off level used in PCA-based estimation,\nand conduct numerical studies to verify the finite sample performance of the\nproposed confidence band. Finally, we apply our methodology to spectrometric\ndata, and discuss extensions of our methodology to cases where additional\nvector-valued regressors are present. \n\n"}
{"id": "1612.08261", "contents": "Title: Change point estimation based on Wilcoxon tests in the presence of\n  long-range dependence Abstract: We consider an estimator for the location of a shift in the mean of\nlong-range dependent sequences. The estimation is based on the two-sample\nWilcoxon statistic. Consistency and the rate of convergence for the estimated\nchange point are established. In the case of a constant shift height, the $1/n$\nconvergence rate (with $n$ denoting the number of observations), which is\ntypical under the assumption of independent observations, is also achieved for\nlong memory sequences. It is proved that if the change point height decreases\nto $0$ with a certain rate, the suitably standardized estimator converges in\ndistribution to a functional of a fractional Brownian motion. The estimator is\ntested on two well-known data sets. Finite sample behaviors are investigated in\na Monte Carlo simulation study. \n\n"}
{"id": "1612.09265", "contents": "Title: Outliers, the Law of Large Numbers, Index of Stability and Heavy Tails Abstract: We are trying to give a mathematically correct definition of outliers. Our\napproach is based on the distance between two last order statistics and appears\nto be connected to the law of large numbers. Key words: outliers, law of large\nnumbers, heavy tails, stability index. \n\n"}
{"id": "1701.01961", "contents": "Title: Learning from MOM's principles: Le Cam's approach Abstract: We obtain estimation error rates for estimators obtained by aggregation of\nregularized median-of-means tests, following a construction of Le Cam. The\nresults hold with exponentially large probability -- as in the gaussian\nframework with independent noise- under only weak moments assumptions on data\nand without assuming independence between noise and design. Any norm may be\nused for regularization. When it has some sparsity inducing power we recover\nsparse rates of convergence.\n  The procedure is robust since a large part of data may be corrupted, these\noutliers have nothing to do with the oracle we want to reconstruct. Our general\nrisk bound is of order \\begin{equation*} \\max\\left(\\mbox{minimax rate in the\ni.i.d. setup}, \\frac{\\text{number of outliers}}{\\text{number of\nobservations}}\\right) \\enspace. \\end{equation*}In particular, the number of\noutliers may be as large as (number of data) $\\times$(minimax rate) without\naffecting this rate. The other data do not have to be identically distributed\nbut should only have equivalent $L^1$ and $L^2$ moments.\n  For example, the minimax rate $s \\log(ed/s)/N$ of recovery of a $s$-sparse\nvector in $\\mathbb{R}^d$ is achieved with exponentially large probability by a\nmedian-of-means version of the LASSO when the noise has $q_0$ moments for some\n$q_0>2$, the entries of the design matrix should have $C_0\\log(ed)$ moments and\nthe dataset can be corrupted up to $C_1 s \\log(ed/s)$ outliers. \n\n"}
{"id": "1701.01974", "contents": "Title: Arimoto-R\\'enyi Conditional Entropy and Bayesian $M$-ary Hypothesis\n  Testing Abstract: This paper gives upper and lower bounds on the minimum error probability of\nBayesian $M$-ary hypothesis testing in terms of the Arimoto-R\\'enyi conditional\nentropy of an arbitrary order $\\alpha$. The improved tightness of these bounds\nover their specialized versions with the Shannon conditional entropy\n($\\alpha=1$) is demonstrated. In particular, in the case where $M$ is finite,\nwe show how to generalize Fano's inequality under both the conventional and\nlist-decision settings. As a counterpart to the generalized Fano's inequality,\nallowing $M$ to be infinite, a lower bound on the Arimoto-R\\'enyi conditional\nentropy is derived as a function of the minimum error probability. Explicit\nupper and lower bounds on the minimum error probability are obtained as a\nfunction of the Arimoto-R\\'enyi conditional entropy for both positive and\nnegative $\\alpha$. Furthermore, we give upper bounds on the minimum error\nprobability as functions of the R\\'enyi divergence. In the setup of discrete\nmemoryless channels, we analyze the exponentially vanishing decay of the\nArimoto-R\\'enyi conditional entropy of the transmitted codeword given the\nchannel output when averaged over a random coding ensemble. \n\n"}
{"id": "1701.04398", "contents": "Title: Automatic sleep monitoring using ear-EEG Abstract: The monitoring of sleep patterns without patient's inconvenience or\ninvolvement of a medical specialist is a clinical question of significant\nimportance. To this end, we propose an automatic sleep stage monitoring system\nbased on an affordable, unobtrusive, discreet, and long-term wearable in-ear\nsensor for recording the Electroencephalogram (ear-EEG). The selected features\nfor sleep pattern classification from a single ear-EEG channel include the\nspectral edge frequency (SEF) and multi- scale fuzzy entropy (MSFE), a\nstructural complexity feature. In this preliminary study, the manually scored\nhypnograms from simultaneous scalp-EEG and ear-EEG recordings of four subjects\nare used as labels for two analysis scenarios: 1) classification of ear-EEG\nhypnogram labels from ear-EEG recordings and 2) prediction of scalp-EEG\nhypnogram labels from ear-EEG recordings. We consider both 2-class and 4-class\nsleep scoring, with the achieved accuracies ranging from 78.5 % to 95.2 % for\near-EEG labels predicted from ear-EEG, and 76.8 % to 91.8 % for scalp-EEG\nlabels predicted from ear-EEG. The corresponding kappa coefficients, which\nrange from 0.64 to 0.83 for Scenario 1 and from 0.65 to 0.80 for Scenario 2,\nindicate a Substantial to Almost Perfect agreement, thus proving the\nfeasibility of in-ear sensing for sleep monitoring in the community. \n\n"}
{"id": "1701.05009", "contents": "Title: Optimal Kullback-Leibler Aggregation in Mixture Density Estimation by\n  Maximum Likelihood Abstract: We study the maximum likelihood estimator of density of $n$ independent\nobservations, under the assumption that it is well approximated by a mixture\nwith a large number of components. The main focus is on statistical properties\nwith respect to the Kullback-Leibler loss. We establish risk bounds taking the\nform of sharp oracle inequalities both in deviation and in expectation. A\nsimple consequence of these bounds is that the maximum likelihood estimator\nattains the optimal rate $((\\log K)/n)^{1/2}$, up to a possible logarithmic\ncorrection, in the problem of convex aggregation when the number $K$ of\ncomponents is larger than $n^{1/2}$. More importantly, under the additional\nassumption that the Gram matrix of the components satisfies the compatibility\ncondition, the obtained oracle inequalities yield the optimal rate in the\nsparsity scenario. That is, if the weight vector is (nearly) $D$-sparse, we get\nthe rate $(D\\log K)/n$. As a natural complement to our oracle inequalities, we\nintroduce the notion of nearly-$D$-sparse aggregation and establish matching\nlower bounds for this type of aggregation. \n\n"}
{"id": "1701.05230", "contents": "Title: Surrogate Aided Unsupervised Recovery of Sparse Signals in Single Index\n  Models for Binary Outcomes Abstract: We consider the recovery of regression coefficients, denoted by\n$\\boldsymbol{\\beta}_0$, for a single index model (SIM) relating a binary\noutcome $Y$ to a set of possibly high dimensional covariates $\\boldsymbol{X}$,\nbased on a large but 'unlabeled' dataset $\\mathcal{U}$, with $Y$ never\nobserved. On $\\mathcal{U}$, we fully observe $\\boldsymbol{X}$ and additionally,\na surrogate $S$ which, while not being strongly predictive of $Y$ throughout\nthe entirety of its support, can forecast it with high accuracy when it assumes\nextreme values. Such datasets arise naturally in modern studies involving large\ndatabases such as electronic medical records (EMR) where $Y$, unlike\n$(\\boldsymbol{X}, S)$, is difficult and/or expensive to obtain. In EMR studies,\nan example of $Y$ and $S$ would be the true disease phenotype and the count of\nthe associated diagnostic codes respectively. Assuming another SIM for $S$\ngiven $\\boldsymbol{X}$, we show that under sparsity assumptions, we can recover\n$\\boldsymbol{\\beta}_0$ proportionally by simply fitting a least squares LASSO\nestimator to the subset of the observed data on $(\\boldsymbol{X}, S)$\nrestricted to the extreme sets of $S$, with $Y$ imputed using the surrogacy of\n$S$. We obtain sharp finite sample performance bounds for our estimator,\nincluding deterministic deviation bounds and probabilistic guarantees. We\ndemonstrate the effectiveness of our approach through multiple simulation\nstudies, as well as by application to real data from an EMR study conducted at\nthe Partners HealthCare Systems. \n\n"}
{"id": "1701.06140", "contents": "Title: Markovian Statistics on Evolving Systems Abstract: A novel framework for the analysis of observation statistics on time discrete\nlinear evolutions in Banach space is presented. The model differs from\ntraditional models for stochastic processes and, in particular, clearly\ndistinguishes between the deterministic evolution of a system and the\nstochastic nature of observations on the evolving system. General Markov chains\nare defined in this context and it is shown how typical traditional models of\nclassical or quantum random walks and Markov processes fit into the framework\nand how a theory of quantum statistics ({\\it sensu} Barndorff-Nielsen, Gill and\nJupp) may be developed from it. The framework permits a general theory of joint\nobservability of two or more observation variables which may be viewed as an\nextension of the Heisenberg uncertainty principle and, in particular, offers a\nnovel mathematical perspective on the violation of Bell's inequalities in\nquantum models. Main results include a general sampling theorem relative to\nRiesz evolution operators in the spirit of von Neumann's mean ergodic theorem\nfor normal operators in Hilbert space. \n\n"}
{"id": "1701.06501", "contents": "Title: Maximum likelihood estimation of determinantal point processes Abstract: Determinantal point processes (DPPs) have wide-ranging applications in\nmachine learning, where they are used to enforce the notion of diversity in\nsubset selection problems. Many estimators have been proposed, but surprisingly\nthe basic properties of the maximum likelihood estimator (MLE) have received\nlittle attention. The difficulty is that it is a non-concave maximization\nproblem, and such functions are notoriously difficult to understand in high\ndimensions, despite their importance in modern machine learning. Here we study\nboth the local and global geometry of the expected log-likelihood function. We\nprove several rates of convergence for the MLE and give a complete\ncharacterization of the case where these are parametric. We also exhibit a\npotential curse of dimensionality where the asymptotic variance of the MLE\nscales exponentially with the dimension of the problem. Moreover, we exhibit an\nexponential number of saddle points, and give evidence that these may be the\nonly critical points. \n\n"}
{"id": "1701.08701", "contents": "Title: Signal Recovery from Unlabeled Samples Abstract: In this paper, we study the recovery of a signal from a set of noisy linear\nprojections (measurements), when such projections are unlabeled, that is, the\ncorrespondence between the measurements and the set of projection vectors\n(i.e., the rows of the measurement matrix) is not known a priori. We consider a\nspecial case of unlabeled sensing referred to as Unlabeled Ordered Sampling\n(UOS) where the ordering of the measurements is preserved. We identify a\nnatural duality between this problem and classical Compressed Sensing (CS),\nwhere we show that the unknown support (location of nonzero elements) of a\nsparse signal in CS corresponds to the unknown indices of the measurements in\nUOS. While in CS it is possible to recover a sparse signal from an\nunder-determined set of linear equations (less equations than the signal\ndimension), successful recovery in UOS requires taking more samples than the\ndimension of the signal. Motivated by this duality, we develop a Restricted\nIsometry Property (RIP) similar to that in CS. We also design a low-complexity\nAlternating Minimization algorithm that achieves a stable signal recovery under\nthe established RIP. We analyze our proposed algorithm for different signal\ndimensions and number of measurements theoretically and investigate its\nperformance empirically via simulations. The results are reminiscent of\nphase-transition similar to that occurring in CS. \n\n"}
{"id": "1702.00908", "contents": "Title: Statistical inference for misspecified ergodic L\\'evy driven stochastic\n  differential equation models Abstract: This paper deals with the estimation problem of misspecified ergodic L\\'evy\ndriven stochastic differential equation models based on high-frequency samples.\nWe utilize the widely applicable and tractable Gaussian quasi-likelihood\napproach which focuses on (conditional) mean and variance structure. It is\nshown that the corresponding Gaussian quasi-likelihood estimators of drift and\nscale parameters satisfy tail probability estimates and asymptotic normality at\nthe same rate as correctly specified case. In this process, extended Poisson\nequation for time-homogeneous Feller Markov processes plays an important role\nto handle misspecification effect. Our result confirms the practical usefulness\nof the Gaussian quasi-likelihood approach for SDE models, more firmly. \n\n"}
{"id": "1702.00931", "contents": "Title: Online estimation of the asymptotic variance for averaged stochastic\n  gradient algorithms Abstract: Stochastic gradient algorithms are more and more studied since they can deal\nefficiently and online with large samples in high dimensional spaces. In this\npaper, we first establish a Central Limit Theorem for these estimates as well\nas for their averaged version in general Hilbert spaces. Moreover, since having\nthe asymptotic normality of estimates is often unusable without an estimation\nof the asymptotic variance, we introduce a new recursive algorithm for\nestimating this last one, and we establish its almost sure rate of convergence\nas well as its rate of convergence in quadratic mean. Finally, two examples\nconsisting in estimating the parameters of the logistic regression and\nestimating geometric quantiles are given. \n\n"}
{"id": "1702.01906", "contents": "Title: Affiliation networks with an increasing degree sequence Abstract: Affiliation network is one kind of two-mode social network with two different\nsets of nodes (namely, a set of actors and a set of social events) and edges\nrepresenting the affiliation of the actors with the social events. Although a\nnumber of statistical models are proposed to analyze affiliation networks, the\nasymptotic behaviors of the estimator are still unknown or have not been\nproperly explored. In this paper, we study an affiliation model with the degree\nsequence as the exclusively natural sufficient statistic in the exponential\nfamily distributions. We establish the uniform consistency and asymptotic\nnormality of the maximum likelihood estimator when the numbers of actors and\nevents both go to infinity. Simulation studies and a real data example\ndemonstrate our theoretical results. \n\n"}
{"id": "1702.02502", "contents": "Title: A Note on Prediction Markets Abstract: In a prediction market, individuals can sequentially place bets on the\noutcome of a future event. This leaves a trail of personal probabilities for\nthe event, each being conditional on the current individual's private\nbackground knowledge and on the previously announced probabilities of other\nindividuals, which give partial information about their private knowledge. By\nmeans of theory and examples, we revisit some results in this area. In\nparticular, we consider the case of two individuals, who start with the same\noverall probability distribution but different private information, and then\ntake turns in updating their probabilities. We note convergence of the\nannounced probabilities to a limiting value, which may or may not be the same\nas that based on pooling their private information. \n\n"}
{"id": "1702.02838", "contents": "Title: The DTM-signature for a geometric comparison of metric-measure spaces\n  from samples Abstract: In this paper, we introduce the notion of DTM-signature, a measure on R +\nthat can be associated to any metric-measure space. This signature is based on\nthe distance to a measure (DTM) introduced by Chazal, Cohen-Steiner and\nM\\'erigot. It leads to a pseudo-metric between metric-measure spaces,\nupper-bounded by the Gromov-Wasserstein distance. Under some geometric\nassumptions, we derive lower bounds for this pseudo-metric. Given two\nN-samples, we also build an asymptotic statistical test based on the\nDTM-signature, to reject the hypothesis of equality of the two underlying\nmetric-measure spaces, up to a measure-preserving isometry. We give strong\ntheoretical justifications for this test and propose an algorithm for its\nimplementation. \n\n"}
{"id": "1702.03884", "contents": "Title: Determinantal Generalizations of Instrumental Variables Abstract: Linear structural equation models relate the components of a random vector\nusing linear interdependencies and Gaussian noise. Each such model can be\nnaturally associated with a mixed graph whose vertices correspond to the\ncomponents of the random vector. The graph contains directed edges that\nrepresent the linear relationships between components, and bidirected edges\nthat encode unobserved confounding. We study the problem of generic\nidentifiability, that is, whether a generic choice of linear and confounding\neffects can be uniquely recovered from the joint covariance matrix of the\nobserved random vector. An existing combinatorial criterion for establishing\ngeneric identifiability is the half-trek criterion (HTC), which uses the\nexistence of trek systems in the mixed graph to iteratively discover\ngenerically invertible linear equation systems in polynomial time. By focusing\non edges one at a time, we establish new sufficient and necessary conditions\nfor generic identifiability of edge effects extending those of the HTC. In\nparticular, we show how edge coefficients can be recovered as quotients of\nsubdeterminants of the covariance matrix, which constitutes a determinantal\ngeneralization of formulas obtained when using instrumental variables for\nidentification. \n\n"}
{"id": "1702.04031", "contents": "Title: Maximum likelihood estimation in Gaussian models under total positivity Abstract: We analyze the problem of maximum likelihood estimation for Gaussian\ndistributions that are multivariate totally positive of order two (MTP2). By\nexploiting connections to phylogenetics and single-linkage clustering, we give\na simple proof that the maximum likelihood estimator (MLE) for such\ndistributions exists based on at least 2 observations, irrespective of the\nunderlying dimension. Slawski and Hein, who first proved this result, also\nprovided empirical evidence showing that the MTP2 constraint serves as an\nimplicit regularizer and leads to sparsity in the estimated inverse covariance\nmatrix, determining what we name the ML graph. We show that we can find an\nupper bound for the ML graph by adding edges corresponding to correlations in\nexcess of those explained by the maximum weight spanning forest of the\ncorrelation matrix. Moreover, we provide globally convergent coordinate descent\nalgorithms for calculating the MLE under the MTP2 constraint which are\nstructurally similar to iterative proportional scaling. We conclude the paper\nwith a discussion of signed MTP2 distributions. \n\n"}
{"id": "1702.04477", "contents": "Title: The Multiple Roots Phenomenon in Maximum Likelihood Estimation for\n  Factor Analysis Abstract: Multiple root estimation problems in statistical inference arise in many\ncontexts in the literature. In the context of maximum likelihood estimation,\nthe existence of multiple roots causes uncertainty in the computation of\nmaximum likelihood estimators using hill-climbing algorithms, and consequent\ndifficulties in the resulting statistical inference.\n  In this paper, we study the multiple roots phenomenon in maximum likelihood\nestimation for factor analysis. We prove that the corresponding likelihood\nequations have uncountably many feasible solutions even in the simplest cases.\nFor the case in which the observed data are two-dimensional and the unobserved\nfactor scores are one-dimensional, we prove that the solutions to the\nlikelihood equations form a one-dimensional real curve. \n\n"}
{"id": "1702.04672", "contents": "Title: Factor Analysis for Spectral Estimation Abstract: Power spectrum estimation is an important tool in many applications, such as\nthe whitening of noise. The popular multitaper method enjoys significant\nsuccess, but fails for short signals with few samples. We propose a statistical\nmodel where a signal is given by a random linear combination of fixed, yet\nunknown, stochastic sources. Given multiple such signals, we estimate the\nsubspace spanned by the power spectra of these fixed sources. Projecting\nindividual power spectrum estimates onto this subspace increases estimation\naccuracy. We provide accuracy guarantees for this method and demonstrate it on\nsimulated and experimental data from cryo-electron microscopy. \n\n"}
{"id": "1702.06488", "contents": "Title: Distributed Estimation of Principal Eigenspaces Abstract: Principal component analysis (PCA) is fundamental to statistical machine\nlearning. It extracts latent principal factors that contribute to the most\nvariation of the data. When data are stored across multiple machines, however,\ncommunication cost can prohibit the computation of PCA in a central location\nand distributed algorithms for PCA are thus needed. This paper proposes and\nstudies a distributed PCA algorithm: each node machine computes the top $K$\neigenvectors and transmits them to the central server; the central server then\naggregates the information from all the node machines and conducts a PCA based\non the aggregated information. We investigate the bias and variance for the\nresulting distributed estimator of the top $K$ eigenvectors. In particular, we\nshow that for distributions with symmetric innovation, the empirical top\neigenspaces are unbiased and hence the distributed PCA is \"unbiased\". We derive\nthe rate of convergence for distributed PCA estimators, which depends\nexplicitly on the effective rank of covariance, eigen-gap, and the number of\nmachines. We show that when the number of machines is not unreasonably large,\nthe distributed PCA performs as well as the whole sample PCA, even without full\naccess of whole data. The theoretical results are verified by an extensive\nsimulation study. We also extend our analysis to the heterogeneous case where\nthe population covariance matrices are different across local machines but\nshare similar top eigen-structures. \n\n"}
{"id": "1702.06832", "contents": "Title: Adversarial examples for generative models Abstract: We explore methods of producing adversarial examples on deep generative\nmodels such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning\narchitectures are known to be vulnerable to adversarial examples, but previous\nwork has focused on the application of adversarial examples to classification\ntasks. Deep generative models have recently become popular due to their ability\nto model input data distributions and generate realistic examples from those\ndistributions. We present three classes of attacks on the VAE and VAE-GAN\narchitectures and demonstrate them against networks trained on MNIST, SVHN and\nCelebA. Our first attack leverages classification-based adversaries by\nattaching a classifier to the trained encoder of the target generative model,\nwhich can then be used to indirectly manipulate the latent representation. Our\nsecond attack directly uses the VAE loss function to generate a target\nreconstruction image from the adversarial example. Our third attack moves\nbeyond relying on classification or the standard loss for the gradient and\ndirectly optimizes against differences in source and target latent\nrepresentations. We also motivate why an attacker might be interested in\ndeploying such techniques against a target generative network. \n\n"}
{"id": "1702.07027", "contents": "Title: Nonparametric Inference via Bootstrapping the Debiased Estimator Abstract: In this paper, we propose to construct confidence bands by bootstrapping the\ndebiased kernel density estimator (for density estimation) and the debiased\nlocal polynomial regression estimator (for regression analysis). The idea of\nusing a debiased estimator was recently employed by Calonico et al. (2018b) to\nconstruct a confidence interval of the density function (and regression\nfunction) at a given point by explicitly estimating stochastic variations. We\nextend their ideas of using the debiased estimator and further propose a\nbootstrap approach for constructing simultaneous confidence bands. This\nmodified method has an advantage that we can easily choose the smoothing\nbandwidth from conventional bandwidth selectors and the confidence band will be\nasymptotically valid. We prove the validity of the bootstrap confidence band\nand generalize it to density level sets and inverse regression problems.\nSimulation studies confirm the validity of the proposed confidence bands/sets.\nWe apply our approach to an Astronomy dataset to show its applicability \n\n"}
{"id": "1702.08536", "contents": "Title: Fast Threshold Tests for Detecting Discrimination Abstract: Threshold tests have recently been proposed as a useful method for detecting\nbias in lending, hiring, and policing decisions. For example, in the case of\ncredit extensions, these tests aim to estimate the bar for granting loans to\nwhite and minority applicants, with a higher inferred threshold for minorities\nindicative of discrimination. This technique, however, requires fitting a\ncomplex Bayesian latent variable model for which inference is often\ncomputationally challenging. Here we develop a method for fitting threshold\ntests that is two orders of magnitude faster than the existing approach,\nreducing computation from hours to minutes. To achieve these performance gains,\nwe introduce and analyze a flexible family of probability distributions on the\ninterval [0, 1] -- which we call discriminant distributions -- that is\ncomputationally efficient to work with. We demonstrate our technique by\nanalyzing 2.7 million police stops of pedestrians in New York City. \n\n"}
{"id": "1702.08546", "contents": "Title: Optimal rates of estimation for multi-reference alignment Abstract: In this paper, we establish optimal rates of adaptive estimation of a vector\nin the multi-reference alignment model, a problem with important applications\nin fields such as signal processing, image processing, and computer vision,\namong others. We describe how this model can be viewed as a multivariate\nGaussian mixture model under the constraint that the centers belong to the\norbit of a group. This enables us to derive matching upper and lower bounds\nthat feature an interesting dependence on the signal-to-noise ratio of the\nmodel. Both upper and lower bounds are articulated around a tight local control\nof Kullback-Leibler divergences that showcases the central role of moment\ntensors in this problem. \n\n"}
{"id": "1703.00167", "contents": "Title: Adaptive estimation of the sparsity in the Gaussian vector model Abstract: Consider the Gaussian vector model with mean value {\\theta}. We study the\ntwin problems of estimating the number |{\\theta}|_0 of non-zero components of\n{\\theta} and testing whether |{\\theta}|_0 is smaller than some value. For\ntesting, we establish the minimax separation distances for this model and\nintroduce a minimax adaptive test. Extensions to the case of unknown variance\nare also discussed. Rewriting the estimation of |{\\theta}|_0 as a multiple\ntesting problem of all hypotheses {|{\\theta}|_0 <= q}, we both derive a new way\nof assessing the optimality of a sparsity estimator and we exhibit such an\noptimal procedure. This general approach provides a roadmap for estimating the\ncomplexity of the signal in various statistical models. \n\n"}
{"id": "1703.00469", "contents": "Title: Confidence Bands for Coefficients in High Dimensional Linear Models with\n  Error-in-variables Abstract: We study high-dimensional linear models with error-in-variables. Such models\nare motivated by various applications in econometrics, finance and genetics.\nThese models are challenging because of the need to account for measurement\nerrors to avoid non-vanishing biases in addition to handle the high\ndimensionality of the parameters. A recent growing literature has proposed\nvarious estimators that achieve good rates of convergence. Our main\ncontribution complements this literature with the construction of simultaneous\nconfidence regions for the parameters of interest in such high-dimensional\nlinear models with error-in-variables.\n  These confidence regions are based on the construction of moment conditions\nthat have an additional orthogonal property with respect to nuisance\nparameters. We provide a construction that requires us to estimate an\nadditional high-dimensional linear model with error-in-variables for each\ncomponent of interest. We use a multiplier bootstrap to compute critical values\nfor simultaneous confidence intervals for a subset $S$ of the components. We\nshow its validity despite of possible model selection mistakes, and allowing\nfor the cardinality of $S$ to be larger than the sample size.\n  We apply and discuss the implications of our results to two examples and\nconduct Monte Carlo simulations to illustrate the performance of the proposed\nprocedure. \n\n"}
{"id": "1703.00884", "contents": "Title: A Dichotomy for Sampling Barrier-Crossing Events of Random Walks with\n  Regularly Varying Tails Abstract: We study how to sample paths of a random walk up to the first time it crosses\na fixed barrier, in the setting where the step sizes are iid with negative mean\nand have a regularly varying right tail. We introduce a desirable property for\na change of measure to be suitable for exact simulation. We study whether the\nchange of measure of Blanchet and Glynn (2008) satisfies this property and show\nthat it does so if and only if the tail index $\\alpha$ of the right tail lies\nin the interval $(1, \\, 3/2)$. \n\n"}
{"id": "1703.01232", "contents": "Title: Inconsistency of Template Estimation with the Fr{\\'e}chet mean in\n  Quotient Space Abstract: We tackle the problem of template estimation when data have been randomly\ntransformed under an isometric group action in the presence of noise. In order\nto estimate the template, one often minimizes the variance when the influence\nof the transformations have been removed (computation of the Fr{\\'e}chet mean\nin quotient space). The consistency bias is defined as the distance (possibly\nzero) between the orbit of the template and the orbit of one element which\nminimizes the variance. In this article we establish an asymptotic behavior of\nthe consistency bias with respect to the noise level. This behavior is linear\nwith respect to the noise level. As a result the inconsistency is unavoidable\nas soon as the noise is large enough. In practice, the template estimation with\na finite sample is often done with an algorithm called max-max. We show the\nconvergence of this algorithm to an empirical Karcher mean. Finally, our\nnumerical experiments show that the bias observed in practice cannot be\nattributed to the small sample size or to a convergence problem but is indeed\ndue to the previously studied inconsistency. \n\n"}
{"id": "1703.01536", "contents": "Title: A Statistical Machine Learning Approach to Yield Curve Forecasting Abstract: Yield curve forecasting is an important problem in finance. In this work we\nexplore the use of Gaussian Processes in conjunction with a dynamic modeling\nstrategy, much like the Kalman Filter, to model the yield curve. Gaussian\nProcesses have been successfully applied to model functional data in a variety\nof applications. A Gaussian Process is used to model the yield curve. The\nhyper-parameters of the Gaussian Process model are updated as the algorithm\nreceives yield curve data. Yield curve data is typically available as a time\nseries with a frequency of one day. We compare existing methods to forecast the\nyield curve with the proposed method. The results of this study showed that\nwhile a competing method (a multivariate time series method) performed well in\nforecasting the yields at the short term structure region of the yield curve,\nGaussian Processes perform well in the medium and long term structure regions\nof the yield curve. Accuracy in the long term structure region of the yield\ncurve has important practical implications. The Gaussian Process framework\nyields uncertainty and probability estimates directly in contrast to other\ncompeting methods. Analysts are frequently interested in this information. In\nthis study the proposed method has been applied to yield curve forecasting,\nhowever it can be applied to model high frequency time series data or data\nstreams in other domains. \n\n"}
{"id": "1703.02059", "contents": "Title: Cheshire: An Online Algorithm for Activity Maximization in Social\n  Networks Abstract: User engagement in social networks depends critically on the number of online\nactions their users take in the network. Can we design an algorithm that finds\nwhen to incentivize users to take actions to maximize the overall activity in a\nsocial network? In this paper, we model the number of online actions over time\nusing multidimensional Hawkes processes, derive an alternate representation of\nthese processes based on stochastic differential equations (SDEs) with jumps\nand, exploiting this alternate representation, address the above question from\nthe perspective of stochastic optimal control of SDEs with jumps. We find that\nthe optimal level of incentivized actions depends linearly on the current level\nof overall actions. Moreover, the coefficients of this linear relationship can\nbe found by solving a matrix Riccati differential equation, which can be solved\nefficiently, and a first order differential equation, which has a closed form\nsolution. As a result, we are able to design an efficient online algorithm,\nCheshire, to sample the optimal times of the users' incentivized actions.\nExperiments on both synthetic and real data gathered from Twitter show that our\nalgorithm is able to consistently maximize the number of online actions more\neffectively than the state of the art. \n\n"}
{"id": "1703.02834", "contents": "Title: Exact Dimensionality Selection for Bayesian PCA Abstract: We present a Bayesian model selection approach to estimate the intrinsic\ndimensionality of a high-dimensional dataset. To this end, we introduce a novel\nformulation of the probabilisitic principal component analysis model based on a\nnormal-gamma prior distribution. In this context, we exhibit a closed-form\nexpression of the marginal likelihood which allows to infer an optimal number\nof components. We also propose a heuristic based on the expected shape of the\nmarginal likelihood curve in order to choose the hyperparameters. In\nnon-asymptotic frameworks, we show on simulated data that this exact\ndimensionality selection approach is competitive with both Bayesian and\nfrequentist state-of-the-art methods. \n\n"}
{"id": "1703.04886", "contents": "Title: Information Theoretic Optimal Learning of Gaussian Graphical Models Abstract: What is the optimal number of independent observations from which a sparse\nGaussian Graphical Model can be correctly recovered? Information-theoretic\narguments provide a lower bound on the minimum number of samples necessary to\nperfectly identify the support of any multivariate normal distribution as a\nfunction of model parameters. For a model defined on a sparse graph with $p$\nnodes, a maximum degree $d$ and minimum normalized edge strength $\\kappa$, this\nnecessary number of samples scales at least as $d \\log p/\\kappa^2$. The sample\ncomplexity requirements of existing methods for perfect graph reconstruction\nexhibit dependency on additional parameters that do not enter in the lower\nbound. The question of whether the lower bound is tight and achievable by a\npolynomial time algorithm remains open. In this paper, we constructively answer\nthis question and propose an algorithm, termed DICE, whose sample complexity\nmatches the information-theoretic lower bound up to a universal constant\nfactor. We also propose a related algorithm SLICE that has a slightly higher\nsample complexity, but can be implemented as a mixed integer quadratic program\nwhich makes it attractive in practice. Importantly, SLICE retains a critical\nadvantage of DICE in that its sample complexity only depends on quantities\npresent in the information theoretic lower bound. We anticipate that this\nresult will stimulate future search of computationally efficient sample-optimal\nalgorithms. \n\n"}
{"id": "1703.06222", "contents": "Title: A unified treatment of multiple testing with prior knowledge using the\n  p-filter Abstract: There is a significant literature on methods for incorporating knowledge into\nmultiple testing procedures so as to improve their power and precision. Some\ncommon forms of prior knowledge include (a) beliefs about which hypotheses are\nnull, modeled by non-uniform prior weights; (b) differing importances of\nhypotheses, modeled by differing penalties for false discoveries; (c) multiple\narbitrary partitions of the hypotheses into (possibly overlapping) groups; and\n(d) knowledge of independence, positive or arbitrary dependence between\nhypotheses or groups, suggesting the use of more aggressive or conservative\nprocedures. We present a unified algorithmic framework called p-filter for\nglobal null testing and false discovery rate (FDR) control that allows the\nscientist to incorporate all four types of prior knowledge (a)-(d)\nsimultaneously, recovering a variety of known algorithms as special cases. \n\n"}
{"id": "1703.06419", "contents": "Title: Multivariate Functional Data Visualization and Outlier Detection Abstract: This article proposes a new graphical tool, the magnitude-shape (MS) plot,\nfor visualizing both the magnitude and shape outlyingness of multivariate\nfunctional data. The proposed tool builds on the recent notion of functional\ndirectional outlyingness, which measures the centrality of functional data by\nsimultaneously considering the level and the direction of their deviation from\nthe central region. The MS-plot intuitively presents not only levels but also\ndirections of magnitude outlyingness on the horizontal axis or plane, and\ndemonstrates shape outlyingness on the vertical axis. A dividing curve or\nsurface is provided to separate non-outlying data from the outliers. Both the\nsimulated data and the practical examples confirm that the MS-plot is superior\nto existing tools for visualizing centrality and detecting outliers for\nfunctional data. \n\n"}
{"id": "1703.06692", "contents": "Title: QMDP-Net: Deep Learning for Planning under Partial Observability Abstract: This paper introduces the QMDP-net, a neural network architecture for\nplanning under partial observability. The QMDP-net combines the strengths of\nmodel-free learning and model-based planning. It is a recurrent policy network,\nbut it represents a policy for a parameterized set of tasks by connecting a\nmodel with a planning algorithm that solves the model, thus embedding the\nsolution structure of planning in a network learning architecture. The QMDP-net\nis fully differentiable and allows for end-to-end training. We train a QMDP-net\non different tasks so that it can generalize to new ones in the parameterized\ntask set and \"transfer\" to other similar tasks beyond the set. In preliminary\nexperiments, QMDP-net showed strong performance on several robotic tasks in\nsimulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it\nsometimes outperforms the QMDP algorithm in the experiments, as a result of\nend-to-end learning. \n\n"}
{"id": "1703.07948", "contents": "Title: Fast Stochastic Variance Reduced Gradient Method with Momentum\n  Acceleration for Machine Learning Abstract: Recently, research on accelerated stochastic gradient descent methods (e.g.,\nSVRG) has made exciting progress (e.g., linear convergence for strongly convex\nproblems). However, the best-known methods (e.g., Katyusha) requires at least\ntwo auxiliary variables and two momentum parameters. In this paper, we propose\na fast stochastic variance reduction gradient (FSVRG) method, in which we\ndesign a novel update rule with the Nesterov's momentum and incorporate the\ntechnique of growing epoch size. FSVRG has only one auxiliary variable and one\nmomentum weight, and thus it is much simpler and has much lower per-iteration\ncomplexity. We prove that FSVRG achieves linear convergence for strongly convex\nproblems and the optimal $\\mathcal{O}(1/T^2)$ convergence rate for non-strongly\nconvex problems, where $T$ is the number of outer-iterations. We also extend\nFSVRG to directly solve the problems with non-smooth component functions, such\nas SVM. Finally, we empirically study the performance of FSVRG for solving\nvarious machine learning problems such as logistic regression, ridge\nregression, Lasso and SVM. Our results show that FSVRG outperforms the\nstate-of-the-art stochastic methods, including Katyusha. \n\n"}
{"id": "1704.02146", "contents": "Title: Quantum ensembles of quantum classifiers Abstract: Quantum machine learning witnesses an increasing amount of quantum algorithms\nfor data-driven decision making, a problem with potential applications ranging\nfrom automated image recognition to medical diagnosis. Many of those algorithms\nare implementations of quantum classifiers, or models for the classification of\ndata inputs with a quantum computer. Following the success of collective\ndecision making with ensembles in classical machine learning, this paper\nintroduces the concept of quantum ensembles of quantum classifiers. Creating\nthe ensemble corresponds to a state preparation routine, after which the\nquantum classifiers are evaluated in parallel and their combined decision is\naccessed by a single-qubit measurement. This framework naturally allows for\nexponentially large ensembles in which -- similar to Bayesian learning -- the\nindividual classifiers do not have to be trained. As an example, we analyse an\nexponentially large quantum ensemble in which each classifier is weighed\naccording to its performance in classifying the training data, leading to new\nresults for quantum as well as classical machine learning. \n\n"}
{"id": "1704.02531", "contents": "Title: Three Skewed Matrix Variate Distributions Abstract: Three-way data can be conveniently modelled by using matrix variate\ndistributions. Although there has been a lot of work for the matrix variate\nnormal distribution, there is little work in the area of matrix skew\ndistributions. Three matrix variate distributions that incorporate skewness, as\nwell as other flexible properties such as concentration, are discussed.\nEquivalences to multivariate analogues are presented, and moment generating\nfunctions are derived. Maximum likelihood parameter estimation is discussed,\nand simulated data is used for illustration. \n\n"}
{"id": "1704.04806", "contents": "Title: Simultaneous Inference for High Dimensional Mean Vectors Abstract: Let $X_1, \\ldots, X_n\\in\\mathbb{R}^p$ be i.i.d. random vectors. We aim to\nperform simultaneous inference for the mean vector $\\mathbb{E} (X_i)$ with\nfinite polynomial moments and an ultra high dimension. Our approach is based on\nthe truncated sample mean vector. A Gaussian approximation result is derived\nfor the latter under the very mild finite polynomial ($(2+\\theta)$-th) moment\ncondition and the dimension $p$ can be allowed to grow exponentially with the\nsample size $n$. Based on this result, we propose an innovative resampling\nmethod to construct simultaneous confidence intervals for mean vectors. \n\n"}
{"id": "1704.05466", "contents": "Title: A geometric approach to non-linear correlations with intrinsic scatter Abstract: We propose a new mathematical model for $n-k$-dimensional non-linear\ncorrelations with intrinsic scatter in $n$-dimensional data. The model is based\non Riemannian geometry, and is naturally symmetric with respect to the measured\nvariables and invariant under coordinate transformations. We combine the model\nwith a Bayesian approach for estimating the parameters of the correlation\nrelation and the intrinsic scatter. A side benefit of the approach is that\ncensored and truncated datasets and independent, arbitrary measurement errors\ncan be incorporated. We also derive analytic likelihoods for the typical\nastrophysical use case of linear relations in $n$-dimensional Euclidean space.\nWe pay particular attention to the case of linear regression in two dimensions,\nand compare our results to existing methods. Finally, we apply our methodology\nto the well-known $M_\\text{BH}$-$\\sigma$ correlation between the mass of a\nsupermassive black hole in the centre of a galactic bulge and the corresponding\nbulge velocity dispersion. The main result of our analysis is that the most\nlikely slope of this correlation is $\\sim 6$ for the datasets used, rather than\nthe values in the range $\\sim 4$-$5$ typically quoted in the literature for\nthese data. \n\n"}
{"id": "1704.05991", "contents": "Title: Thresholds For Detecting An Anomalous Path From Noisy Environments Abstract: We consider the \"searching for a trail in a maze\" composite hypothesis\ntesting problem, in which one attempts to detect an anomalous directed path in\na lattice 2D box of side n based on observations on the nodes of the box. Under\nthe signal hypothesis, one observes independent Gaussian variables of unit\nvariance at all nodes, with zero, mean off the anomalous path and mean \\mu_n on\nit. Under the null hypothesis, one observes i.i.d. standard Gaussians on all\nnodes. Arias-Castro et al. (2008) showed that if the unknown directed path\nunder the signal hypothesis has known the initial location, then detection is\npossible (in the minimax sense) if \\mu_n >> 1/\\sqrt log n, while it is not\npossible if \\mu_n << 1/ log n\\sqrt log log n. In this paper, we show that this\nresult continues to hold even when the initial location of the unknown path is\nnot known. As is the case with Arias-Castro et al. (2008), the upper bound here\nalso applies when the path is undirected. The improvement is achieved by\nreplacing the linear detection statistic used in Arias-Castro et al. (2008)\nwith a polynomial statistic, which is obtained by employing a multi-scale\nanalysis on a quadratic statistic to bootstrap its performance. Our analysis is\nmotivated by ideas developed in the context of the analysis of random polymers\nin Lacoin (2010). \n\n"}
{"id": "1704.06160", "contents": "Title: Halfspace depths for scatter, concentration and shape matrices Abstract: We propose halfspace depth concepts for scatter, concentration and shape\nmatrices. For scatter matrices, our concept is similar to those from Chen, Gao\nand Ren (2017) and Zhang (2002). Rather than focusing, as in these earlier\nworks, on deepest scatter matrices, we thoroughly investigate the properties of\nthe proposed depth and of the corresponding depth regions. We do so under\nminimal assumptions and, in particular, we do not restrict to elliptical\ndistributions nor to absolutely continuous distributions. Interestingly, fully\nunderstanding scatter halfspace depth requires considering different\ngeometries/topologies on the space of scatter matrices. We also discuss, in the\nspirit of Zuo and Serfling (2000), the structural properties a scatter depth\nshould satisfy, and investigate whether or not these are met by scatter\nhalfspace depth. Companion concepts of depth for concentration matrices and\nshape matrices are also proposed and studied. We show the practical relevance\nof the depth concepts considered in a real-data example from finance. \n\n"}
{"id": "1704.06977", "contents": "Title: Adaptive Estimation in Structured Factor Models with Applications to\n  Overlapping Clustering Abstract: This work introduces a novel estimation method, called LOVE, of the entries\nand structure of a loading matrix A in a sparse latent factor model X = AZ + E,\nfor an observable random vector X in Rp, with correlated unobservable factors Z\n\\in RK, with K unknown, and independent noise E. Each row of A is scaled and\nsparse. In order to identify the loading matrix A, we require the existence of\npure variables, which are components of X that are associated, via A, with one\nand only one latent factor. Despite the fact that the number of factors K, the\nnumber of the pure variables, and their location are all unknown, we only\nrequire a mild condition on the covariance matrix of Z, and a minimum of only\ntwo pure variables per latent factor to show that A is uniquely defined, up to\nsigned permutations. Our proofs for model identifiability are constructive, and\nlead to our novel estimation method of the number of factors and of the set of\npure variables, from a sample of size n of observations on X. This is the first\nstep of our LOVE algorithm, which is optimization-free, and has low\ncomputational complexity of order p2. The second step of LOVE is an easily\nimplementable linear program that estimates A. We prove that the resulting\nestimator is minimax rate optimal up to logarithmic factors in p. The model\nstructure is motivated by the problem of overlapping variable clustering,\nubiquitous in data science. We define the population level clusters as groups\nof those components of X that are associated, via the sparse matrix A, with the\nsame unobservable latent factor, and multi-factor association is allowed.\nClusters are respectively anchored by the pure variables, and form overlapping\nsub-groups of the p-dimensional random vector X. The Latent model approach to\nOVErlapping clustering is reflected in the name of our algorithm, LOVE. \n\n"}
{"id": "1704.07513", "contents": "Title: Oracle posterior contraction rates under hierarchical priors Abstract: We offer a general Bayes theoretic framework to derive posterior contraction\nrates under a hierarchical prior design: the first-step prior serves to assess\nthe model selection uncertainty, and the second-step prior quantifies the prior\nbelief on the strength of the signals within the model chosen from the first\nstep. In particular, we establish non-asymptotic oracle posterior contraction\nrates under (i) a local Gaussianity condition on the log likelihood ratio of\nthe statistical experiment, (ii) a local entropy condition on the\ndimensionality of the models, and (iii) a sufficient mass condition on the\nsecond-step prior near the best approximating signal for each model. The\nfirst-step prior can be designed generically. The posterior distribution enjoys\nGaussian tail behavior and therefore the resulting posterior mean also\nsatisfies an oracle inequality, automatically serving as an adaptive point\nestimator in a frequentist sense. Model mis-specification is allowed in these\noracle rates.\n  The local Gaussianity condition serves as a unified attempt of non-asymptotic\nGaussian quantification of the experiments, and can be easily verified in\nvarious experiments considered in [GvdV07a] and beyond. The general results are\napplied in various problems including: (i) trace regression, (ii)\nshape-restricted isotonic/convex regression, (iii) high-dimensional partially\nlinear regression, (iv) covariance matrix estimation in the sparse factor\nmodel, (v) detection of non-smooth polytopal image boundary, and (vi) intensity\nestimation in a Poisson point process model. These new results serve either as\ntheoretical justification of practical prior proposals in the literature, or as\nan illustration of the generic construction scheme of a (nearly) minimax\nadaptive estimator for a complicated experiment. \n\n"}
{"id": "1704.07531", "contents": "Title: Sufficient Markov Decision Processes with Alternating Deep Neural\n  Networks Abstract: Advances in mobile computing technologies have made it possible to monitor\nand apply data-driven interventions across complex systems in real time. Markov\ndecision processes (MDPs) are the primary model for sequential decision\nproblems with a large or indefinite time horizon. Choosing a representation of\nthe underlying decision process that is both Markov and low-dimensional is\nnon-trivial. We propose a method for constructing a low-dimensional\nrepresentation of the original decision process for which: 1. the MDP model\nholds; 2. a decision strategy that maximizes mean utility when applied to the\nlow-dimensional representation also maximizes mean utility when applied to the\noriginal process. We use a deep neural network to define a class of potential\nprocess representations and estimate the process of lowest dimension within\nthis class. The method is illustrated using data from a mobile study on heavy\ndrinking and smoking among college students. \n\n"}
{"id": "1704.08095", "contents": "Title: Converting High-Dimensional Regression to High-Dimensional Conditional\n  Density Estimation Abstract: There is a growing demand for nonparametric conditional density estimators\n(CDEs) in fields such as astronomy and economics. In astronomy, for example,\none can dramatically improve estimates of the parameters that dictate the\nevolution of the Universe by working with full conditional densities instead of\nregression (i.e., conditional mean) estimates. More generally, standard\nregression falls short in any prediction problem where the distribution of the\nresponse is more complex with multi-modality, asymmetry or heteroscedastic\nnoise. Nevertheless, much of the work on high-dimensional inference concerns\nregression and classification only, whereas research on density estimation has\nlagged behind. Here we propose FlexCode, a fully nonparametric approach to\nconditional density estimation that reformulates CDE as a non-parametric\northogonal series problem where the expansion coefficients are estimated by\nregression. By taking such an approach, one can efficiently estimate\nconditional densities and not just expectations in high dimensions by drawing\nupon the success in high-dimensional regression. Depending on the choice of\nregression procedure, our method can adapt to a variety of challenging\nhigh-dimensional settings with different structures in the data (e.g., a large\nnumber of irrelevant components and nonlinear manifold structure) as well as\ndifferent data types (e.g., functional data, mixed data types and sample sets).\nWe study the theoretical and empirical performance of our proposed method, and\nwe compare our approach with traditional conditional density estimators on\nsimulated as well as real-world data, such as photometric galaxy data, Twitter\ndata, and line-of-sight velocities in a galaxy cluster. \n\n"}
{"id": "1704.08227", "contents": "Title: Accelerating Stochastic Gradient Descent For Least Squares Regression Abstract: There is widespread sentiment that it is not possible to effectively utilize\nfast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy\nball) for the purposes of stochastic optimization due to their instability and\nerror accumulation, a notion made precise in d'Aspremont 2008 and Devolder,\nGlineur, and Nesterov 2014. This work considers these issues for the special\ncase of stochastic approximation for the least squares regression problem, and\nour main result refutes the conventional wisdom by showing that acceleration\ncan be made robust to statistical errors. In particular, this work introduces\nan accelerated stochastic gradient method that provably achieves the minimax\noptimal statistical risk faster than stochastic gradient descent. Critical to\nthe analysis is a sharp characterization of accelerated stochastic gradient\ndescent as a stochastic process. We hope this characterization gives insights\ntowards the broader question of designing simple and effective accelerated\nstochastic methods for more general convex and non-convex optimization\nproblems. \n\n"}
{"id": "1704.08672", "contents": "Title: Local Marchenko-Pastur Law for Random Bipartite Graphs Abstract: This paper is the first chapter of three of the author's undergraduate\nthesis. We study the random matrix ensemble of covariance matrices arising from\nrandom $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and\n$N$ white vertices, for $d_b \\gg \\log^4 N$. We simultaneously prove that the\nGreen's functions of these covariance matrices and the adjacency matrices of\nthe underlying graphs agree with the corresponding limiting law (e.g.\nMarchenko-Pastur law for covariance matrices) down to the optimal scale. This\nis an improvement from the previously known mesoscopic results. We obtain\neigenvector delocalization for the covariance matrix ensemble as consequence,\nas well as a weak rigidity estimate. \n\n"}
{"id": "1705.00586", "contents": "Title: Bootstrap confidence bands for spectral estimation of L\\'evy densities\n  under high-frequency observations Abstract: This paper develops bootstrap methods to construct uniform confidence bands\nfor nonparametric spectral estimation of L\\'{e}vy densities under\nhigh-frequency observations. We assume that we observe $n$ discrete\nobservations at frequency $1/\\Delta > 0$, and work with the high-frequency\nsetup where $\\Delta = \\Delta_{n} \\to 0$ and $n\\Delta \\to \\infty$ as $n \\to\n\\infty$. We employ a spectral (or Fourier-based) estimator of the L\\'{e}vy\ndensity, and develop novel implementations of Gaussian multiplier (or wild) and\nempirical (or Efron's) bootstraps to construct confidence bands for the\nspectral estimator on a compact set that does not intersect the origin. We\nprovide conditions under which the proposed confidence bands are asymptotically\nvalid. Our confidence bands are shown to be asymptotically valid for a wide\nclass of L\\'{e}vy processes. We also develop a practical method for bandwidth\nselection, and conduct simulation studies to investigate the finite sample\nperformance of the proposed confidence bands. \n\n"}
{"id": "1705.01715", "contents": "Title: Directed Networks with a Differentially Private Bi-degree Sequence Abstract: Although a lot of approaches are developed to release network data with a\ndifferentially privacy guarantee, inference using noisy data in many network\nmodels is still unknown or not properly explored. In this paper, we release the\nbi-degree sequences of directed networks using the Laplace mechanism and use\nthe $p_0$ model for inferring the degree parameters. The $p_0$ model is an\nexponential random graph model with the bi-degree sequence as its exclusively\nsufficient statistic. We show that the estimator of the parameter without the\ndenoised process is asymptotically consistent and normally distributed. This is\ncontrast sharply with some known results that valid inference such as the\nexistence and consistency of the estimator needs the denoised process. Along\nthe way, a new phenomenon is revealed in which an additional variance factor\nappears in the asymptotic variance of the estimator when the noise becomes\nlarge. Further, we propose an efficient algorithm for finding the closet point\nlying in the set of all graphical bi-degree sequences under the global $L_1$\noptimization problem. Numerical studies demonstrate our theoretical findings. \n\n"}
{"id": "1705.02212", "contents": "Title: Group invariance principles for causal generative models Abstract: The postulate of independence of cause and mechanism (ICM) has recently led\nto several new causal discovery algorithms. The interpretation of independence\nand the way it is utilized, however, varies across these methods. Our aim in\nthis paper is to propose a group theoretic framework for ICM to unify and\ngeneralize these approaches. In our setting, the cause-mechanism relationship\nis assessed by comparing it against a null hypothesis through the application\nof random generic group transformations. We show that the group theoretic view\nprovides a very general tool to study the structure of data generating\nmechanisms with direct applications to machine learning. \n\n"}
{"id": "1705.02276", "contents": "Title: Mixing properties and central limit theorem for associated point\n  processes Abstract: Positively (resp. negatively) associated point processes are a class of point\nprocesses that induce attraction (resp. inhibition) between the points. As an\nimportant example, determinantal point processes (DPPs) are negatively\nassociated. We prove $\\alpha$-mixing properties for associated spatial point\nprocesses by controlling their $\\alpha$-coefficients in terms of the first two\nintensity functions. A central limit theorem for functionals of associated\npoint processes is deduced, using both the association and the $\\alpha$-mixing\nproperties. We discuss in detail the case of DPPs, for which we obtain the\nlimiting distribution of sums, over subsets of close enough points of the\nprocess, of any bounded function of the DPP. As an application, we get the\nasymptotic properties of the parametric two-step estimator of some\ninhomogeneous DPPs. \n\n"}
{"id": "1705.02973", "contents": "Title: Community Detection in Hypergraphs, Spiked Tensor Models, and\n  Sum-of-Squares Abstract: We study the problem of community detection in hypergraphs under a stochastic\nblock model. Similarly to how the stochastic block model in graphs suggests\nstudying spiked random matrices, our model motivates investigating statistical\nand computational limits of exact recovery in a certain spiked tensor model. In\ncontrast with the matrix case, the spiked model naturally arising from\ncommunity detection in hypergraphs is different from the one arising in the\nso-called tensor Principal Component Analysis model. We investigate the\neffectiveness of algorithms in the Sum-of-Squares hierarchy on these models.\nInterestingly, our results suggest that these two apparently similar models\nexhibit significantly different computational to statistical gaps. \n\n"}
{"id": "1705.03439", "contents": "Title: Frequentist Consistency of Variational Bayes Abstract: A key challenge for modern Bayesian statistics is how to perform scalable\ninference of posterior distributions. To address this challenge, variational\nBayes (VB) methods have emerged as a popular alternative to the classical\nMarkov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while\nachieving comparable predictive performance. However, there are few theoretical\nresults around VB. In this paper, we establish frequentist consistency and\nasymptotic normality of VB methods. Specifically, we connect VB methods to\npoint estimates based on variational approximations, called frequentist\nvariational approximations, and we use the connection to prove a variational\nBernstein-von Mises theorem. The theorem leverages the theoretical\ncharacterizations of frequentist variational approximations to understand\nasymptotic properties of VB. In summary, we prove that (1) the VB posterior\nconverges to the Kullback-Leibler (KL) minimizer of a normal distribution,\ncentered at the truth and (2) the corresponding variational expectation of the\nparameter is consistent and asymptotically normal. As applications of the\ntheorem, we derive asymptotic properties of VB posteriors in Bayesian mixture\nmodels, Bayesian generalized linear mixed models, and Bayesian stochastic block\nmodels. We conduct a simulation study to illustrate these theoretical results. \n\n"}
{"id": "1705.03830", "contents": "Title: Nonparametric inference for continuous-time event counting and\n  link-based dynamic network models Abstract: A flexible approach for modeling both dynamic event counting and dynamic\nlink-based networks based on counting processes is proposed, and estimation in\nthese models is studied. We consider nonparametric likelihood based estimation\nof parameter functions via kernel smoothing. The asymptotic behavior of these\nestimators is rigorously analyzed by allowing the number of nodes to tend to\ninfinity. The finite sample performance of the estimators is illustrated\nthrough an empirical analysis of bike share data. \n\n"}
{"id": "1705.04974", "contents": "Title: On the maximal halfspace depth of permutation-invariant distributions on\n  the simplex Abstract: We compute the maximal halfspace depth for a class of permutation-invariant\ndistributions on the probability simplex. The derivations are based on\nstochastic ordering results that so far were only showed to be relevant for the\nBehrens-Fisher problem. \n\n"}
{"id": "1705.05312", "contents": "Title: Single-cluster PHD filter methods for joint multi-object filtering and\n  parameter estimation Abstract: Many multi-object estimation problems require additional estimation of model\nor sensor parameters that are either common to all objects or related to\nunknown characterisation of one or more sensors. Important examples of these\ninclude registration of multiple sensors, estimating clutter profiles, and\nrobot localisation. Often these parameters are estimated separately to the\nmulti-object estimation process, which can lead to systematic errors or\noverconfidence in the estimates. These parameters can be estimated jointly with\nthe multi-object process based only on the sensor data using a single-cluster\npoint process model. This paper presents novel results for joint parameter\nestimation and multi-object filtering based on a single-cluster second-order\nProbability Hypothesis Density (PHD) and Cardinalised PHD (CPHD) filter.\nExperiments provide a comparison between the discussed approaches using\ndifferent likelihood functions. \n\n"}
{"id": "1705.05543", "contents": "Title: In Defense of the Indefensible: A Very Naive Approach to\n  High-Dimensional Inference Abstract: A great deal of interest has recently focused on conducting inference on the\nparameters in a high-dimensional linear model.\n  In this paper, we consider a simple and very na\\\"{i}ve two-step procedure for\nthis task, in which we (i) fit a lasso model in order to obtain a subset of the\nvariables, and (ii) fit a least squares model on the lasso-selected set.\nConventional statistical wisdom tells us that we cannot make use of the\nstandard statistical inference tools for the resulting least squares model\n(such as confidence intervals and $p$-values), since we peeked at the data\ntwice: once in running the lasso, and again in fitting the least squares model.\nHowever, in this paper, we show that under a certain set of assumptions, with\nhigh probability, the set of variables selected by the lasso is identical to\nthe one selected by the noiseless lasso and is hence deterministic.\nConsequently, the na\\\"{i}ve two-step approach can yield asymptotically valid\ninference. We utilize this finding to develop the \\emph{na\\\"ive confidence\ninterval}, which can be used to draw inference on the regression coefficients\nof the model selected by the lasso, as well as the \\emph{na\\\"ive score test},\nwhich can be used to test the hypotheses regarding the full-model regression\ncoefficients. \n\n"}
{"id": "1705.06615", "contents": "Title: Adaptive Clustering through Semidefinite Programming Abstract: We analyze the clustering problem through a flexible probabilistic model that\naims to identify an optimal partition on the sample X 1 , ..., X n. We perform\nexact clustering with high probability using a convex semidefinite estimator\nthat interprets as a corrected, relaxed version of K-means. The estimator is\nanalyzed through a non-asymptotic framework and showed to be optimal or\nnear-optimal in recovering the partition. Furthermore, its performances are\nshown to be adaptive to the problem's effective dimension, as well as to K the\nunknown number of groups in this partition. We illustrate the method's\nperformances in comparison to other classical clustering algorithms with\nnumerical experiments on simulated data. \n\n"}
{"id": "1705.07196", "contents": "Title: Hypothesis Testing via Euclidean Separation Abstract: We discuss an \"operational\" approach to testing convex composite hypotheses\nwhen the underlying distributions are heavy-tailed. It relies upon Euclidean\nseparation of convex sets and can be seen as an extension of the approach to\ntesting by convex optimization developed in [8, 12]. In particular, we show how\none can construct quasi-optimal testing procedures for families of\ndistributions which are majorated, in a certain precise sense, by a\nsub-spherical symmetric one and study the relationship between tests based on\nEuclidean separation and \"potential-based tests.\" We apply the promoted\nmethodology in the problem of sequential detection and illustrate its practical\nimplementation in an application to sequential detection of changes in the\ninput of a dynamic system.\n  [8] Goldenshluger, Alexander and Juditsky, Anatoli and Nemirovski, Arkadi,\nHypothesis testing by convex optimization, Electronic Journal of Statistics,9\n(2):1645-1712, 2015. [12] Juditsky, Anatoli and Nemirovski, Arkadi, Hypothesis\ntesting via affine detectors, Electronic Journal of Statistics, 10:2204--2242,\n2016. \n\n"}
{"id": "1705.07411", "contents": "Title: Algebraic Aspects of Conditional Independence and Graphical Models Abstract: This chapter of the forthcoming Handbook of Graphical Models contains an\noverview of basic theorems and techniques from algebraic geometry and how they\ncan be applied to the study of conditional independence and graphical models.\nIt also introduces binomial ideals and some ideas from real algebraic geometry.\nWhen random variables are discrete or Gaussian, tools from computational\nalgebraic geometry can be used to understand implications between conditional\nindependence statements. This is accomplished by computing primary\ndecompositions of conditional independence ideals. As examples the chapter\npresents in detail the graphical model of a four cycle and the intersection\naxiom, a certain implication of conditional independence statements. Another\nimportant problem in the area is to determine all constraints on a graphical\nmodel, for example, equations determined by trek separation. The full set of\nequality constraints can be determined by computing the model's vanishing\nideal. The chapter illustrates these techniques and ideas with examples from\nthe literature and provides references for further reading. \n\n"}
{"id": "1705.07880", "contents": "Title: Reducing Reparameterization Gradient Variance Abstract: Optimization with noisy gradients has become ubiquitous in statistics and\nmachine learning. Reparameterization gradients, or gradient estimates computed\nvia the \"reparameterization trick,\" represent a class of noisy gradients often\nused in Monte Carlo variational inference (MCVI). However, when these gradient\nestimators are too noisy, the optimization procedure can be slow or fail to\nconverge. One way to reduce noise is to use more samples for the gradient\nestimate, but this can be computationally expensive. Instead, we view the noisy\ngradient as a random variable, and form an inexpensive approximation of the\ngenerating procedure for the gradient sample. This approximation has high\ncorrelation with the noisy gradient by construction, making it a useful control\nvariate for variance reduction. We demonstrate our approach on non-conjugate\nmulti-level hierarchical models and a Bayesian neural net where we observed\ngradient variance reductions of multiple orders of magnitude (20-2,000x). \n\n"}
{"id": "1705.08524", "contents": "Title: Designs for estimating the treatment effect in networks with\n  interference Abstract: In this paper we introduce new, easily implementable designs for drawing\ncausal inference from randomized experiments on networks with interference.\nInspired by the idea of matching in observational studies, we introduce the\nnotion of considering a treatment assignment as a quasi-coloring\" on a graph.\nOur idea of a perfect quasi-coloring strives to match every treated unit on a\ngiven network with a distinct control unit that has identical number of treated\nand control neighbors. For a wide range of interference functions encountered\nin applications, we show both by theory and simulations that the classical\nNeymanian estimator for the direct effect has desirable properties for our\ndesigns. This further extends to settings where homophily is present in\naddition to interference. \n\n"}
{"id": "1705.09056", "contents": "Title: Can Decentralized Algorithms Outperform Centralized Algorithms? A Case\n  Study for Decentralized Parallel Stochastic Gradient Descent Abstract: Most distributed machine learning systems nowadays, including TensorFlow and\nCNTK, are built in a centralized fashion. One bottleneck of centralized\nalgorithms lies on high communication cost on the central node. Motivated by\nthis, we ask, can decentralized algorithms be faster than its centralized\ncounterpart?\n  Although decentralized PSGD (D-PSGD) algorithms have been studied by the\ncontrol community, existing analysis and theory do not show any advantage over\ncentralized PSGD (C-PSGD) algorithms, simply assuming the application scenario\nwhere only the decentralized network is available. In this paper, we study a\nD-PSGD algorithm and provide the first theoretical analysis that indicates a\nregime in which decentralized algorithms might outperform centralized\nalgorithms for distributed stochastic gradient descent. This is because D-PSGD\nhas comparable total computational complexities to C-PSGD but requires much\nless communication cost on the busiest node. We further conduct an empirical\nstudy to validate our theoretical analysis across multiple frameworks (CNTK and\nTorch), different network configurations, and computation platforms up to 112\nGPUs. On network configurations with low bandwidth or high latency, D-PSGD can\nbe up to one order of magnitude faster than its well-optimized centralized\ncounterparts. \n\n"}
{"id": "1705.09494", "contents": "Title: Quantile function expansion using regularly varying functions Abstract: We present a simple result that allows us to evaluate the asymptotic order of\nthe remainder of a partial asymptotic expansion of the quantile function $h(u)$\nas $u\\to 0^+$ or $1^-$. This is focussed on important univariate distributions\nwhen $h(\\cdot)$ has no simple closed form, with a view to assessing asymptotic\nrate of decay to zero of tail dependence in the context of bivariate copulas.\nThe Introduction motivates the study in terms of the standard Normal. The\nNormal, Skew-Normal and Gamma are used as initial examples. Finally, we discuss\napproximation to the lower quantile of the Variance-Gamma and Skew-Slash\ndistributions. \n\n"}
{"id": "1705.09693", "contents": "Title: Multiplicative component models for replicated point processes Abstract: We propose a multiplicative semiparametric model for the intensity function\nof replicated point processes. Two examples of applications are given: a\ntemporal one, about the dynamics of Internet auctions, and a spatial one, about\nthe spatial distribution of street robberies in Chicago. \n\n"}
{"id": "1705.09874", "contents": "Title: Targeted Learning with Daily EHR Data Abstract: Electronic health records (EHR) data provide a cost and time-effective\nopportunity to conduct cohort studies of the effects of multiple time-point\ninterventions in the diverse patient population found in real-world clinical\nsettings. Because the computational cost of analyzing EHR data at daily (or\nmore granular) scale can be quite high, a pragmatic approach has been to\npartition the follow-up into coarser intervals of pre-specified length. Current\nguidelines suggest employing a 'small' interval, but the feasibility and\npractical impact of this recommendation has not been evaluated and no formal\nmethodology to inform this choice has been developed. We start filling these\ngaps by leveraging large-scale EHR data from a diabetes study to develop and\nillustrate a fast and scalable targeted learning approach that allows to follow\nthe current recommendation and study its practical impact on inference. More\nspecifically, we map daily EHR data into four analytic datasets using 90, 30,\n15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation\napproach, the longitudinal TMLE, to estimate the causal effects of four dynamic\ntreatment rules with each dataset, and compare the resulting inferences. To\novercome the computational challenges presented by the size of these data, we\npropose a novel TMLE implementation, the 'long-format TMLE', and rely on the\nlatest advances in scalable data-adaptive machine-learning software, xgboost\nand h2o, for estimation of the TMLE nuisance parameters. \n\n"}
{"id": "1705.09898", "contents": "Title: Projection Theorems of Divergences and Likelihood Maximization Methods Abstract: Projection theorems of divergences enable us to find reverse projection of a\ndivergence on a specific statistical model as a forward projection of the\ndivergence on a different but rather \"simpler\" statistical model, which, in\nturn, results in solving a system of linear equations. Reverse projection of\ndivergences are closely related to various estimation methods such as the\nmaximum likelihood estimation or its variants in robust statistics. We consider\nprojection theorems of three parametric families of divergences that are widely\nused in robust statistics, namely the R\\'enyi divergences (or the Cressie-Reed\npower divergences), density power divergences, and the relative\n$\\alpha$-entropy (or the logarithmic density power divergences). We explore\nthese projection theorems from the usual likelihood maximization approach and\nfrom the principle of sufficiency. In particular, we show the equivalence of\nsolving the estimation problems by the projection theorems of the respective\ndivergences and by directly solving the corresponding estimating equations. We\nalso derive the projection theorem for the density power divergences. \n\n"}
{"id": "1705.10190", "contents": "Title: Sequential Multiple Testing Abstract: We study an online multiple testing problem where the hypotheses arrive\nsequentially in a stream. The test statistics are independent and assumed to\nhave the same distribution under their respective null hypotheses. We\ninvestigate two procedures LORD and LOND, proposed by (Javanmard and Montanari,\n2015), which are proved to control the FDR in an online manner. In some\n(static) model, we show that LORD is optimal in some asymptotic sense, in\nparticular as powerful as the (static) Benjamini-Hochberg procedure to first\nasymptotic order. We also quantify the performance of LOND. Some numerical\nexperiments complement our theory. \n\n"}
{"id": "1705.10598", "contents": "Title: Performance analysis of local ensemble Kalman filter Abstract: Ensemble Kalman filter (EnKF) is an important data assimilation method for\nhigh dimensional geophysical systems. Efficient implementation of EnKF in\npractice often involves the localization technique, which updates each\ncomponent using only information within a local radius. This paper rigorously\nanalyzes the local EnKF (LEnKF) for linear systems, and shows that the filter\nerror can be dominated by the ensemble covariance, as long as 1) the sample\nsize exceeds the logarithmic of state dimension and a constant that depends\nonly on the local radius; 2) the forecast covariance matrix admits a stable\nlocalized structure. In particular, this indicates that with small system and\nobservation noises, the filter error will be accurate in long time even if the\ninitialization is not. The analysis also reveals an intrinsic inconsistency\ncaused by the localization technique, and a stable localized structure is\nnecessary to control this inconsistency. While this structure is usually taken\nfor granted for the operation of LEnKF, it can also be rigorously proved for\nlinear systems with sparse local observations and weak local interactions.\nThese theoretical results are also validated by numerical implementation of\nLEnKF on a simple stochastic turbulence in two dynamical regimes. \n\n"}
{"id": "1705.11014", "contents": "Title: Congruent families and invariant tensors Abstract: Classical results of Chentsov and Campbell state that -- up to constant\nmultiples -- the only $2$-tensor field of a statistical model which is\ninvariant under congruent Markov morphisms is the Fisher metric and the only\ninvariant $3$-tensor field is the Amari-Chentsov tensor. We generalize this\nresult for arbitrary degree $n$, showing that any family of $n$-tensors which\nis invariant under congruent Markov morphisms is algebraically generated by the\ncanonical tensor fields defined in an earlier paper. \n\n"}
{"id": "1706.00292", "contents": "Title: Learning Generative Models with Sinkhorn Divergences Abstract: The ability to compare two degenerate probability distributions (i.e. two\nprobability distributions supported on two distinct low-dimensional manifolds\nliving in a much higher-dimensional space) is a crucial problem arising in the\nestimation of generative models for high-dimensional observations such as those\narising in computer vision or natural language. It is known that optimal\ntransport metrics can represent a cure for this problem, since they were\nspecifically designed as an alternative to information divergences to handle\nsuch problematic scenarios. Unfortunately, training generative machines using\nOT raises formidable computational and statistical challenges, because of (i)\nthe computational burden of evaluating OT losses, (ii) the instability and lack\nof smoothness of these losses, (iii) the difficulty to estimate robustly these\nlosses and their gradients in high dimension. This paper presents the first\ntractable computational method to train large scale generative models using an\noptimal transport loss, and tackles these three issues by relying on two key\nideas: (a) entropic smoothing, which turns the original OT loss into one that\ncan be computed using Sinkhorn fixed point iterations; (b) algorithmic\n(automatic) differentiation of these iterations. These two approximations\nresult in a robust and differentiable approximation of the OT loss with\nstreamlined GPU execution. Entropic smoothing generates a family of losses\ninterpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus\nallowing to find a sweet spot leveraging the geometry of OT and the favorable\nhigh-dimensional sample complexity of MMD which comes with unbiased gradient\nestimates. The resulting computational architecture complements nicely standard\ndeep network generative models by a stack of extra layers implementing the loss\nfunction. \n\n"}
{"id": "1706.00961", "contents": "Title: Rates of estimation for determinantal point processes Abstract: Determinantal point processes (DPPs) have wide-ranging applications in\nmachine learning, where they are used to enforce the notion of diversity in\nsubset selection problems. Many estimators have been proposed, but surprisingly\nthe basic properties of the maximum likelihood estimator (MLE) have received\nlittle attention. In this paper, we study the local geometry of the expected\nlog-likelihood function to prove several rates of convergence for the MLE. We\nalso give a complete characterization of the case where the MLE converges at a\nparametric rate. Even in the latter case, we also exhibit a potential curse of\ndimensionality where the asymptotic variance of the MLE is exponentially large\nin the dimension of the problem. \n\n"}
{"id": "1706.01175", "contents": "Title: Optimal Rates for Community Estimation in the Weighted Stochastic Block\n  Model Abstract: Community identification in a network is an important problem in fields such\nas social science, neuroscience, and genetics. Over the past decade, stochastic\nblock models (SBMs) have emerged as a popular statistical framework for this\nproblem. However, SBMs have an important limitation in that they are suited\nonly for networks with unweighted edges; in various scientific applications,\ndisregarding the edge weights may result in a loss of valuable information. We\nstudy a weighted generalization of the SBM, in which observations are collected\nin the form of a weighted adjacency matrix and the weight of each edge is\ngenerated independently from an unknown probability density determined by the\ncommunity membership of its endpoints. We characterize the optimal rate of\nmisclustering error of the weighted SBM in terms of the Renyi divergence of\norder 1/2 between the weight distributions of within-community and\nbetween-community edges, substantially generalizing existing results for\nunweighted SBMs. Furthermore, we present a computationally tractable algorithm\nbased on discretization that achieves the optimal error rate. Our method is\nadaptive in the sense that the algorithm, without assuming knowledge of the\nweight densities, performs as well as the best algorithm that knows the weight\ndensities. \n\n"}
{"id": "1706.01825", "contents": "Title: Parallel and Distributed Thompson Sampling for Large-scale Accelerated\n  Exploration of Chemical Space Abstract: Chemical space is so large that brute force searches for new interesting\nmolecules are infeasible. High-throughput virtual screening via computer\ncluster simulations can speed up the discovery process by collecting very large\namounts of data in parallel, e.g., up to hundreds or thousands of parallel\nmeasurements. Bayesian optimization (BO) can produce additional acceleration by\nsequentially identifying the most useful simulations or experiments to be\nperformed next. However, current BO methods cannot scale to the large numbers\nof parallel measurements and the massive libraries of molecules currently used\nin high-throughput screening. Here, we propose a scalable solution based on a\nparallel and distributed implementation of Thompson sampling (PDTS). We show\nthat, in small scale problems, PDTS performs similarly as parallel expected\nimprovement (EI), a batch version of the most widely used BO heuristic.\nAdditionally, in settings where parallel EI does not scale, PDTS outperforms\nother scalable baselines such as a greedy search, $\\epsilon$-greedy approaches\nand a random search method. These results show that PDTS is a successful\nsolution for large-scale parallel BO. \n\n"}
{"id": "1706.02410", "contents": "Title: Convergence rates of least squares regression estimators with\n  heavy-tailed errors Abstract: We study the performance of the Least Squares Estimator (LSE) in a general\nnonparametric regression model, when the errors are independent of the\ncovariates but may only have a $p$-th moment ($p\\geq 1$). In such a\nheavy-tailed regression setting, we show that if the model satisfies a standard\n`entropy condition' with exponent $\\alpha \\in (0,2)$, then the $L_2$ loss of\nthe LSE converges at a rate \\begin{align*}\n\\mathcal{O}_{\\mathbf{P}}\\big(n^{-\\frac{1}{2+\\alpha}} \\vee\nn^{-\\frac{1}{2}+\\frac{1}{2p}}\\big). \\end{align*} Such a rate cannot be improved\nunder the entropy condition alone.\n  This rate quantifies both some positive and negative aspects of the LSE in a\nheavy-tailed regression setting. On the positive side, as long as the errors\nhave $p\\geq 1+2/\\alpha$ moments, the $L_2$ loss of the LSE converges at the\nsame rate as if the errors are Gaussian. On the negative side, if\n$p<1+2/\\alpha$, there are (many) hard models at any entropy level $\\alpha$ for\nwhich the $L_2$ loss of the LSE converges at a strictly slower rate than other\nrobust estimators.\n  The validity of the above rate relies crucially on the independence of the\ncovariates and the errors. In fact, the $L_2$ loss of the LSE can converge\narbitrarily slowly when the independence fails.\n  The key technical ingredient is a new multiplier inequality that gives sharp\nbounds for the `multiplier empirical process' associated with the LSE. We\nfurther give an application to the sparse linear regression model with\nheavy-tailed covariates and errors to demonstrate the scope of this new\ninequality. \n\n"}
{"id": "1706.02420", "contents": "Title: Berry-Ess\\'een bounds for parameter estimation of general Gaussian\n  processes Abstract: We study rates of convergence in central limit theorems for the partial sum\nof squares of general Gaussian sequences, using tools from analysis on Wiener\nspace. No assumption of stationarity, asymptotically or otherwise, is made. The\nmain theoretical tool is the so-called Optimal Fourth Moment Theorem\n\\cite{NP2015}, which provides a sharp quantitative estimate of the total\nvariation distance on Wiener chaos to the normal law. The only assumptions made\non the sequence are the existence of an asymptotic variance, that a\nleast-squares-type estimator for this variance parameter has a bias and a\nvariance which can be controlled, and that the sequence's auto-correlation\nfunction, which may exhibit long memory, has a no-worse memory than that of\nfractional Brownian motion with Hurst parameter }$H<3/4$.{\\ \\ Our main result\nis explicit, exhibiting the trade-off between bias, variance, and memory. We\napply our result to study drift parameter estimation problems for subfractional\nOrnstein-Uhlenbeck and bifractional Ornstein-Uhlenbeck processes with\nfixed-time-step observations. These are processes which fail to be stationary\nor self-similar, but for which detailed calculations result in explicit\nformulas for the estimators' asymptotic normality. \n\n"}
{"id": "1706.02858", "contents": "Title: On rumour propagation among sceptics Abstract: Junior, Machado and Zuluaga (2011) studied a model to understand the spread\nof a rumour. Their model consists of individuals situated at the integer points\nof the line $\\N$. An individual at the origin $0$ starts a rumour and passes it\nto all individuals in the interval $[0,R_0]$, where $R_0$ is a non-negative\nrandom variable. An individual located at $i$ in this interval receives the\nrumour and transmits it further among individuals in $[i, i+R_i]$ where $R_0$\nand $R_i$ are i.i.d. random variables. The rumour spreads in this manner. An\nalternate model considers individuals seeking to find the rumour from\nindividuals who have already heard it. For this s/he asks individuals to the\nleft of her/him and lying in an interval of a random size. We study these two\nmodels, when the individuals are more sceptical and they transmit or accept the\nrumour only if they receive it from at least two different sources.\n  In stochastic geometry the equivalent of this rumour process is the study of\ncoverage of the space $\\N^d$ by random sets. Our study here extends the study\nof coverage of space and considers the case when each vertex of $\\N^d$ is\ncovered by at least two distinct random sets. \n\n"}
{"id": "1706.04276", "contents": "Title: On the risk of convex-constrained least squares estimators under\n  misspecification Abstract: We consider the problem of estimating the mean of a noisy vector. When the\nmean lies in a convex constraint set, the least squares projection of the\nrandom vector onto the set is a natural estimator. Properties of the risk of\nthis estimator, such as its asymptotic behavior as the noise tends to zero,\nhave been well studied. We instead study the behavior of this estimator under\nmisspecification, that is, without the assumption that the mean lies in the\nconstraint set. For appropriately defined notions of risk in the misspecified\nsetting, we prove a generalization of a low noise characterization of the risk\ndue to Oymak and Hassibi in the case of a polyhedral constraint set. An\ninteresting consequence of our results is that the risk can be much smaller in\nthe misspecified setting than in the well-specified setting. We also discuss\nconsequences of our result for isotonic regression. \n\n"}
{"id": "1706.04983", "contents": "Title: FreezeOut: Accelerate Training by Progressively Freezing Layers Abstract: The early layers of a deep neural net have the fewest parameters, but take up\nthe most computation. In this extended abstract, we propose to only train the\nhidden layers for a set portion of the training run, freezing them out\none-by-one and excluding them from the backward pass. Through experiments on\nCIFAR, we empirically demonstrate that FreezeOut yields savings of up to 20%\nwall-clock time during training with 3% loss in accuracy for DenseNets, a 20%\nspeedup without loss of accuracy for ResNets, and no improvement for VGG\nnetworks. Our code is publicly available at\nhttps://github.com/ajbrock/FreezeOut \n\n"}
{"id": "1706.05394", "contents": "Title: A Closer Look at Memorization in Deep Networks Abstract: We examine the role of memorization in deep learning, drawing connections to\ncapacity, generalization, and adversarial robustness. While deep networks are\ncapable of memorizing noise data, our results suggest that they tend to\nprioritize learning simple patterns first. In our experiments, we expose\nqualitative differences in gradient-based optimization of deep neural networks\n(DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned\nexplicit regularization (e.g., dropout) we can degrade DNN training performance\non noise datasets without compromising generalization on real data. Our\nanalysis suggests that the notions of effective capacity which are dataset\nindependent are unlikely to explain the generalization performance of deep\nnetworks when trained with gradient based methods because training data itself\nplays an important role in determining the degree of memorization. \n\n"}
{"id": "1706.06182", "contents": "Title: Bernoulli Correlations and Cut Polytopes Abstract: Given $n$ symmetric Bernoulli variables, what can be said about their\ncorrelation matrix viewed as a vector? We show that the set of those vectors\n$R(\\mathcal{B}_n)$ is a polytope and identify its vertices. Those extreme\npoints correspond to correlation vectors associated to the discrete uniform\ndistributions on diagonals of the cube $[0,1]^n$. We also show that the\npolytope is affinely isomorphic to a well-known cut polytope ${\\rm CUT}(n)$\nwhich is defined as a convex hull of the cut vectors in a complete graph with\nvertex set $\\{1,\\ldots,n\\}$. The isomorphism is obtained explicitly as\n$R(\\mathcal{B}_n)= {\\mathbf{1}}-2~{\\rm CUT}(n)$. As a corollary of this work,\nit is straightforward using linear programming to determine if a particular\ncorrelation matrix is realizable or not. Furthermore, a sampling method for\nmultivariate symmetric Bernoullis with given correlation is obtained. In some\ncases the method can also be used for general, not exclusively Bernoulli,\nmarginals. \n\n"}
{"id": "1706.06274", "contents": "Title: Learning Graphical Models Using Multiplicative Weights Abstract: We give a simple, multiplicative-weight update algorithm for learning\nundirected graphical models or Markov random fields (MRFs). The approach is\nnew, and for the well-studied case of Ising models or Boltzmann machines, we\nobtain an algorithm that uses a nearly optimal number of samples and has\nquadratic running time (up to logarithmic factors), subsuming and improving on\nall prior work. Additionally, we give the first efficient algorithm for\nlearning Ising models over general alphabets.\n  Our main application is an algorithm for learning the structure of t-wise\nMRFs with nearly-optimal sample complexity (up to polynomial losses in\nnecessary terms that depend on the weights) and running time that is\n$n^{O(t)}$. In addition, given $n^{O(t)}$ samples, we can also learn the\nparameters of the model and generate a hypothesis that is close in statistical\ndistance to the true MRF. All prior work runs in time $n^{\\Omega(d)}$ for\ngraphs of bounded degree d and does not generate a hypothesis close in\nstatistical distance even for t=3. We observe that our runtime has the correct\ndependence on n and t assuming the hardness of learning sparse parities with\nnoise.\n  Our algorithm--the Sparsitron-- is easy to implement (has only one parameter)\nand holds in the on-line setting. Its analysis applies a regret bound from\nFreund and Schapire's classic Hedge algorithm. It also gives the first solution\nto the problem of learning sparse Generalized Linear Models (GLMs). \n\n"}
{"id": "1706.07510", "contents": "Title: Clustering with Noisy Queries Abstract: In this paper, we initiate a rigorous theoretical study of clustering with\nnoisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is to\nrecover the true clustering by asking minimum number of pairwise queries to an\noracle. Oracle can answer queries of the form : \"do elements $u$ and $v$ belong\nto the same cluster?\" -- the queries can be asked interactively (adaptive\nqueries), or non-adaptively up-front, but its answer can be erroneous with\nprobability $p$. In this paper, we provide the first information theoretic\nlower bound on the number of queries for clustering with noisy oracle in both\nsituations. We design novel algorithms that closely match this query complexity\nlower bound, even when the number of clusters is unknown. Moreover, we design\ncomputationally efficient algorithms both for the adaptive and non-adaptive\nsettings. The problem captures/generalizes multiple application scenarios. It\nis directly motivated by the growing body of work that use crowdsourcing for\n{\\em entity resolution}, a fundamental and challenging data mining task aimed\nto identify all records in a database referring to the same entity. Here crowd\nrepresents the noisy oracle, and the number of queries directly relates to the\ncost of crowdsourcing. Another application comes from the problem of {\\em sign\nedge prediction} in social network, where social interactions can be both\npositive and negative, and one must identify the sign of all pair-wise\ninteractions by querying a few pairs. Furthermore, clustering with noisy oracle\nis intimately connected to correlation clustering, leading to improvement\ntherein. Finally, it introduces a new direction of study in the popular {\\em\nstochastic block model} where one has an incomplete stochastic block model\nmatrix to recover the clusters. \n\n"}
{"id": "1706.07581", "contents": "Title: Cross-validation failure: small sample sizes lead to large error bars Abstract: Predictive models ground many state-of-the-art developments in statistical\nbrain image analysis: decoding, MVPA, searchlight, or extraction of biomarkers.\nThe principled approach to establish their validity and usefulness is\ncross-validation, testing prediction on unseen data. Here, I would like to\nraise awareness on error bars of cross-validation, which are often\nunderestimated. Simple experiments show that sample sizes of many neuroimaging\nstudies inherently lead to large error bars, eg $\\pm$10% for 100 samples. The\nstandard error across folds strongly underestimates them. These large error\nbars compromise the reliability of conclusions drawn with predictive models,\nsuch as biomarkers or methods developments where, unlike with cognitive\nneuroimaging MVPA approaches, more samples cannot be acquired by repeating the\nexperiment across many subjects. Solutions to increase sample size must be\ninvestigated, tackling possible increases in heterogeneity of the data. \n\n"}
{"id": "1706.07766", "contents": "Title: Asymmetric Matrix-Valued Covariances for Multivariate Random Fields on\n  Spheres Abstract: Matrix-valued covariance functions are crucial to geostatistical modeling of\nmultivariate spatial data. The classical assumption of symmetry of a\nmultivariate covariance function is overlay restrictive and has been considered\nas unrealistic for most of real data applications. Despite of that, the\nliterature on asymmetric covariance functions has been very sparse. In\nparticular, there is some work related to asymmetric covariances on Euclidean\nspaces, depending on the Euclidean distance. However, for data collected over\nlarge portions of planet Earth, the most natural spatial domain is a sphere,\nwith the corresponding geodesic distance being the natural metric. In this\nwork, we propose a strategy based on spatial rotations to generate asymmetric\ncovariances for multivariate random fields on the $d$-dimensional unit sphere.\nWe illustrate through simulations as well as real data analysis that our\nproposal allows to achieve improvements in the predictive performance in\ncomparison to the symmetric counterpart. \n\n"}
{"id": "1706.08058", "contents": "Title: Invariant Causal Prediction for Sequential Data Abstract: We investigate the problem of inferring the causal predictors of a response\n$Y$ from a set of $d$ explanatory variables $(X^1,\\dots,X^d)$. Classical\nordinary least squares regression includes all predictors that reduce the\nvariance of $Y$. Using only the causal predictors instead leads to models that\nhave the advantage of remaining invariant under interventions, loosely speaking\nthey lead to invariance across different \"environments\" or \"heterogeneity\npatterns\". More precisely, the conditional distribution of $Y$ given its causal\npredictors remains invariant for all observations. Recent work exploits such a\nstability to infer causal relations from data with different but known\nenvironments. We show that even without having knowledge of the environments or\nheterogeneity pattern, inferring causal relations is possible for time-ordered\n(or any other type of sequentially ordered) data. In particular, this allows\ndetecting instantaneous causal relations in multivariate linear time series\nwhich is usually not the case for Granger causality. Besides novel methodology,\nwe provide statistical confidence bounds and asymptotic detection results for\ninferring causal predictors, and present an application to monetary policy in\nmacroeconomics. \n\n"}
{"id": "1706.08289", "contents": "Title: Intrinsic data depth for Hermitian positive definite matrices Abstract: Nondegenerate covariance, correlation and spectral density matrices are\nnecessarily symmetric or Hermitian and positive definite. The main contribution\nof this paper is the development of statistical data depths for collections of\nHermitian positive definite matrices by exploiting the geometric structure of\nthe space as a Riemannian manifold. The depth functions allow one to naturally\ncharacterize most central or outlying matrices, but also provide a practical\nframework for inference in the context of samples of positive definite\nmatrices. First, the desired properties of an intrinsic data depth function\nacting on the space of Hermitian positive definite matrices are presented.\nSecond, we propose two computationally fast pointwise and integrated data depth\nfunctions that satisfy each of these requirements and investigate several\nrobustness and efficiency aspects. As an application, we construct depth-based\nconfidence regions for the intrinsic mean of a sample of positive definite\nmatrices, which is applied to the exploratory analysis of a collection of\ncovariance matrices associated to a multicenter research trial. \n\n"}
{"id": "1707.01227", "contents": "Title: Exponential random graphs behave like mixtures of stochastic block\n  models Abstract: We study the behavior of exponential random graphs in both the sparse and the\ndense regime. We show that exponential random graphs are approximate mixtures\nof graphs with independent edges whose probability matrices are critical points\nof an associated functional, thereby satisfying a certain matrix equation. In\nthe dense regime, every solution to this equation is close to a block matrix,\nconcluding that the exponential random graph behaves roughly like a mixture of\nstochastic block models. We also show existence and uniqueness of solutions to\nthis equation for several families of exponential random graphs, including the\ncase where the subgraphs are counted with positive weights and the case where\nall weights are small in absolute value. In particular, this generalizes some\nof the results in a paper by Chatterjee and Diaconis from the dense regime to\nthe sparse regime and strengthens their bounds from the cut-metric to the\none-metric. \n\n"}
{"id": "1707.01334", "contents": "Title: Shapley effects for sensitivity analysis with correlated inputs:\n  comparisons with Sobol' indices, numerical estimation and applications Abstract: The global sensitivity analysis of a numerical model aims to quantify, by\nmeans of sensitivity indices estimate, the contributions of each uncertain\ninput variable to the model output uncertainty. The so-called Sobol' indices,\nwhich are based on the functional variance analysis, present a difficult\ninterpretation in the presence of statistical dependence between inputs. The\nShapley effect was recently introduced to overcome this problem as they\nallocate the mutual contribution (due to correlation and interaction) of a\ngroup of inputs to each individual input within the group.In this paper, using\nseveral new analytical results, we study the effects of linear correlation\nbetween some Gaussian input variables on Shapley effects, and compare these\neffects to classical first-order and total Sobol' indices.This illustrates the\ninterest, in terms of sensitivity analysis setting and interpretation, of the\nShapley effects in the case of dependent inputs. For the practical issue of\ncomputationally demanding computer models, we show that the substitution of the\noriginal model by a metamodel (here, kriging) makes it possible to estimate\nthese indices with precision at a reasonable computational cost. \n\n"}
{"id": "1707.01350", "contents": "Title: Consistent Estimation of Mixed Memberships with Successive Projections Abstract: This paper considers the parameter estimation problem in Mixed Membership\nStochastic Block Model (MMSB), which is a quite general instance of random\ngraph model allowing for overlapping community structure. We present the new\nalgorithm successive projection overlapping clustering (SPOC) which combines\nthe ideas of spectral clustering and geometric approach for separable\nnon-negative matrix factorization. The proposed algorithm is provably\nconsistent under MMSB with general conditions on the parameters of the model.\nSPOC is also shown to perform well experimentally in comparison to other\nalgorithms. \n\n"}
{"id": "1707.02090", "contents": "Title: Structured Matrix Estimation and Completion Abstract: We study the problem of matrix estimation and matrix completion under a\ngeneral framework. This framework includes several important models as special\ncases such as the gaussian mixture model, mixed membership model, bi-clustering\nmodel and dictionary learning. We consider the optimal convergence rates in a\nminimax sense for estimation of the signal matrix under the Frobenius norm and\nunder the spectral norm. As a consequence of our general result we obtain\nminimax optimal rates of convergence for various special models. \n\n"}
{"id": "1707.06467", "contents": "Title: Explicit minimisation of a convex quadratic under a general quadratic\n  constraint: a global, analytic approach Abstract: A novel approach is introduced to a very widely occurring problem, providing\na complete, explicit resolution of it: minimisation of a convex quadratic under\na general quadratic, equality or inequality, constraint. Completeness comes via\nidentification of a set of mutually exclusive and exhaustive special cases.\nExplicitness, via algebraic expressions for each solution set. Throughout,\nunderlying geometry illuminates and informs algebraic development. In\nparticular, centrally to this new approach, affine equivalence is exploited to\nre-express the same problem in simpler coordinate systems. Overall, the\nanalysis presented provides insight into the diverse forms taken both by the\nproblem itself and its solution set, showing how each may be intrinsically\nunstable. Comparisons of this global, analytic approach with the, intrinsically\ncomplementary, local, computational approach of (generalised) trust region\nmethods point to potential synergies between them. Points of contact with\nsimultaneous diagonalisation results are noted. \n\n"}
{"id": "1707.06768", "contents": "Title: Integrability conditions for Compound Random Measures Abstract: Compound random measures (CoRM's) are a flexible and tractable framework for\nvectors of completely random measure. In this paper, we provide conditions to\nguarantee the existence of a CoRM. Furthermore, we prove some interesting\nproperties of CoRM's when exponential scores and regularly varying L\\'evy\nintensities are considered. \n\n"}
{"id": "1707.06852", "contents": "Title: A Statistical Perspective on Inverse and Inverse Regression Problems Abstract: Inverse problems, where in broad sense the task is to learn from the noisy\nresponse about some unknown function, usually represented as the argument of\nsome known functional form, has received wide attention in the general\nscientific disciplines. How- ever, in mainstream statistics such inverse\nproblem paradigm does not seem to be as popular. In this article we provide a\nbrief overview of such problems from a statistical, particularly Bayesian,\nperspective.\n  We also compare and contrast the above class of problems with the perhaps\nmore statistically familiar inverse regression problems, arguing that this\nclass of problems contains the traditional class of inverse problems. In course\nof our review we point out that the statistical literature is very scarce with\nrespect to both the inverse paradigms, and substantial research work is still\nnecessary to develop the fields. \n\n"}
{"id": "1707.07163", "contents": "Title: Warped Riemannian metrics for location-scale models Abstract: The present paper shows that warped Riemannian metrics, a class of Riemannian\nmetrics which play a prominent role in Riemannian geometry, are also of\nfundamental importance in information geometry. Precisely, the paper features a\nnew theorem, which states that the Rao-Fisher information metric of any\nlocation-scale model, defined on a Riemannian manifold, is a warped Riemannian\nmetric, whenever this model is invariant under the action of some Lie group.\nThis theorem is a valuable tool in finding the expression of the Rao-Fisher\ninformation metric of location-scale models defined on high-dimensional\nRiemannian manifolds. Indeed, a warped Riemannian metric is fully determined by\nonly two functions of a single variable, irrespective of the dimension of the\nunderlying Riemannian manifold. Starting from this theorem, several original\ncontributions are made. The expression of the Rao-Fisher information metric of\nthe Riemannian Gaussian model is provided, for the first time in the\nliterature. A generalised definition of the Mahalanobis distance is introduced,\nwhich is applicable to any location-scale model defined on a Riemannian\nmanifold. The solution of the geodesic equation is obtained, for any Rao-Fisher\ninformation metric defined in terms of warped Riemannian metrics. Finally,\nusing a mixture of analytical and numerical computations, it is shown that the\nparameter space of the von Mises-Fisher model of $n$-dimensional directional\ndata, when equipped with its Rao-Fisher information metric, becomes a Hadamard\nmanifold, a simply-connected complete Riemannian manifold of negative sectional\ncurvature, for $n = 2,\\ldots,8$. Hopefully, in upcoming work, this will be\nproved for any value of $n$. \n\n"}
{"id": "1707.08788", "contents": "Title: Bayesian inference for Stable Levy driven Stochastic Differential\n  Equations with high-frequency data Abstract: In this article we consider parametric Bayesian inference for stochastic\ndifferential equations (SDE) driven by a pure-jump stable Levy process, which\nis observed at high frequency. In most cases of practical interest, the\nlikelihood function is not available, so we use a quasi-likelihood and place an\nassociated prior on the unknown parameters. It is shown under regularity\nconditions that there is a Bernstein-von Mises theorem associated to the\nposterior. We then develop a Markov chain Monte Carlo (MCMC) algorithm for\nBayesian inference and assisted by our theoretical results, we show how to\nscale Metropolis-Hastings proposals when the frequency of the data grows, in\norder to prevent the acceptance ratio going to zero in the large data limit.\nOur algorithm is presented on numerical examples that help to verify our\ntheoretical findings. \n\n"}
{"id": "1708.00078", "contents": "Title: Bayesian Dyadic Trees and Histograms for Regression Abstract: Many machine learning tools for regression are based on recursive\npartitioning of the covariate space into smaller regions, where the regression\nfunction can be estimated locally. Among these, regression trees and their\nensembles have demonstrated impressive empirical performance. In this work, we\nshed light on the machinery behind Bayesian variants of these methods. In\nparticular, we study Bayesian regression histograms, such as Bayesian dyadic\ntrees, in the simple regression case with just one predictor. We focus on the\nreconstruction of regression surfaces that are piecewise constant, where the\nnumber of jumps is unknown. We show that with suitably designed priors,\nposterior distributions concentrate around the true step regression function at\na near-minimax rate. These results do not require the knowledge of the true\nnumber of steps, nor the width of the true partitioning cells. Thus, Bayesian\ndyadic regression trees are fully adaptive and can recover the true piecewise\nregression function nearly as well as if we knew the exact number and location\nof jumps. Our results constitute the first step towards understanding why\nBayesian trees and their ensembles have worked so well in practice. As an\naside, we discuss prior distributions on balanced interval partitions and how\nthey relate to an old problem in geometric probability. Namely, we relate the\nprobability of covering the circumference of a circle with random arcs whose\nendpoints are confined to a grid, a new variant of the original problem. \n\n"}
{"id": "1708.00185", "contents": "Title: Tensorial Recurrent Neural Networks for Longitudinal Data Analysis Abstract: Traditional Recurrent Neural Networks assume vectorized data as inputs.\nHowever many data from modern science and technology come in certain structures\nsuch as tensorial time series data. To apply the recurrent neural networks for\nthis type of data, a vectorisation process is necessary, while such a\nvectorisation leads to the loss of the precise information of the spatial or\nlongitudinal dimensions. In addition, such a vectorized data is not an optimum\nsolution for learning the representation of the longitudinal data. In this\npaper, we propose a new variant of tensorial neural networks which directly\ntake tensorial time series data as inputs. We call this new variant as\nTensorial Recurrent Neural Network (TRNN). The proposed TRNN is based on tensor\nTucker decomposition. \n\n"}
{"id": "1708.00294", "contents": "Title: Mean and Variance of Phylogenetic Trees Abstract: We describe the use of the Frechet mean and variance in the\nBillera-Holmes-Vogtmann (BHV) treespace to summarize and explore the diversity\nof a set of phylogenetic trees. We show that the Frechet mean is comparable to\nother summary methods, and, despite its stickiness property, is more likely to\nbe binary than the majority-rules consensus tree. We show that the Frechet\nvariance is faster and more precise than commonly used variance measures. The\nFrechet mean and variance are more theoretically justified, and more robust,\nthan previous estimates of this type, and can be estimated reasonably\nefficiently, providing a foundation for building more advanced statistical\nmethods and leading to applications such as mean hypothesis testing. \n\n"}
{"id": "1708.01012", "contents": "Title: On the convergence properties of a $K$-step averaging stochastic\n  gradient descent algorithm for nonconvex optimization Abstract: Despite their popularity, the practical performance of asynchronous\nstochastic gradient descent methods (ASGD) for solving large scale machine\nlearning problems are not as good as theoretical results indicate. We adopt and\nanalyze a synchronous K-step averaging stochastic gradient descent algorithm\nwhich we call K-AVG. We establish the convergence results of K-AVG for\nnonconvex objectives and explain why the K-step delay is necessary and leads to\nbetter performance than traditional parallel stochastic gradient descent which\nis a special case of K-AVG with $K=1$. We also show that K-AVG scales better\nthan ASGD. Another advantage of K-AVG over ASGD is that it allows larger\nstepsizes. On a cluster of $128$ GPUs, K-AVG is faster than ASGD\nimplementations and achieves better accuracies and faster convergence for\n\\cifar dataset. \n\n"}
{"id": "1708.04696", "contents": "Title: Generalized Uniformity Testing Abstract: In this work, we revisit the problem of uniformity testing of discrete\nprobability distributions. A fundamental problem in distribution testing,\ntesting uniformity over a known domain has been addressed over a significant\nline of works, and is by now fully understood.\n  The complexity of deciding whether an unknown distribution is uniform over\nits unknown (and arbitrary) support, however, is much less clear. Yet, this\ntask arises as soon as no prior knowledge on the domain is available, or\nwhenever the samples originate from an unknown and unstructured universe. In\nthis work, we introduce and study this generalized uniformity testing question,\nand establish nearly tight upper and lower bound showing that -- quite\nsurprisingly -- its sample complexity significantly differs from the\nknown-domain case. Moreover, our algorithm is intrinsically adaptive, in\ncontrast to the overwhelming majority of known distribution testing algorithms. \n\n"}
{"id": "1708.04985", "contents": "Title: On maxispaces of nonparametric tests Abstract: For the problems of nonparametric hypothesis testing we introduce the notion\nof maxisets and maxispace. We point out the maxisets of $\\chi^2-$tests,\nCramer-von Mises tests, tests generated $\\mathbb{L}_2$- norms of kernel\nestimators and tests generated quadratic forms of estimators of Fourier\ncoefficients. For these tests we show that, if sequence of alternatives having\ngiven rates of convergence to hypothesis is consistent, then each altehrnative\ncan be broken down into the sum of two parts: a function belonging to maxiset\nand orthogonal function. Sequence of functions belonging to maxiset is\nconsistent sequence of alternatives.\n  We point out asymptotically minimax tests if sets of alternatives are maxiset\nwith deleted \"small\" $\\mathbb{L}_2$-balls. \n\n"}
{"id": "1708.05343", "contents": "Title: Cauchy-Stieltjes families with polynomial variance functions and\n  generalized orthogonality Abstract: This paper studies variance functions of Cauchy-Stieltjes Kernel families\ngenerated by compactly supported centered probability measures. We describe\nseveral operations that allow us to construct additional variance functions\nfrom known ones. We construct a class of examples which exhausts all cubic\nvariance functions, and provide examples of polynomial variance functions of\narbitrary degree. We also relate Cauchy-Stieltjes Kernel families with\npolynomial variance functions to generalized orthogonality.\n  Our main results are stated solely in terms of classical probability; some\nproofs rely on analytic machinery of free probability. \n\n"}
{"id": "1708.07259", "contents": "Title: Dynamic Tensor Clustering Abstract: Dynamic tensor data are becoming prevalent in numerous applications. Existing\ntensor clustering methods either fail to account for the dynamic nature of the\ndata, or are inapplicable to a general-order tensor. Also there is often a gap\nbetween statistical guarantee and computational efficiency for existing tensor\nclustering solutions. In this article, we aim to bridge this gap by proposing a\nnew dynamic tensor clustering method, which takes into account both sparsity\nand fusion structures, and enjoys strong statistical guarantees as well as high\ncomputational efficiency. Our proposal is based upon a new structured tensor\nfactorization that encourages both sparsity and smoothness in parameters along\nthe specified tensor modes. Computationally, we develop a highly efficient\noptimization algorithm that benefits from substantial dimension reduction. In\ntheory, we first establish a non-asymptotic error bound for the estimator from\nthe structured tensor factorization. Built upon this error bound, we then\nderive the rate of convergence of the estimated cluster centers, and show that\nthe estimated clusters recover the true cluster structures with a high\nprobability. Moreover, our proposed method can be naturally extended to\nco-clustering of multiple modes of the tensor data. The efficacy of our\napproach is illustrated via simulations and a brain dynamic functional\nconnectivity analysis from an Autism spectrum disorder study. \n\n"}
{"id": "1708.08734", "contents": "Title: Posterior Concentration for Bayesian Regression Trees and Forests Abstract: Since their inception in the 1980's, regression trees have been one of the\nmore widely used non-parametric prediction methods. Tree-structured methods\nyield a histogram reconstruction of the regression surface, where the bins\ncorrespond to terminal nodes of recursive partitioning. Trees are powerful, yet\nsusceptible to over-fitting. Strategies against overfitting have traditionally\nrelied on pruning greedily grown trees. The Bayesian framework offers an\nalternative remedy against overfitting through priors. Roughly speaking, a good\nprior charges smaller trees where overfitting does not occur. While the\nconsistency of random histograms, trees and their ensembles has been studied\nquite extensively, the theoretical understanding of the Bayesian counterparts\nhas been missing. In this paper, we take a step towards understanding why/when\ndo Bayesian trees and their ensembles not overfit. To address this question, we\nstudy the speed at which the posterior concentrates around the true smooth\nregression function. We propose a spike-and-tree variant of the popular\nBayesian CART prior and establish new theoretical results showing that\nregression trees (and their ensembles) (a) are capable of recovering smooth\nregression surfaces, achieving optimal rates up to a log factor, (b) can adapt\nto the unknown level of smoothness and (c) can perform effective dimension\nreduction when p>n. These results provide a piece of missing theoretical\nevidence explaining why Bayesian trees (and additive variants thereof) have\nworked so well in practice. \n\n"}
{"id": "1708.09022", "contents": "Title: Deep Convolutional Neural Networks for Raman Spectrum Recognition: A\n  Unified Solution Abstract: Machine learning methods have found many applications in Raman spectroscopy,\nespecially for the identification of chemical species. However, almost all of\nthese methods require non-trivial preprocessing such as baseline correction\nand/or PCA as an essential step. Here we describe our unified solution for the\nidentification of chemical species in which a convolutional neural network is\ntrained to automatically identify substances according to their Raman spectrum\nwithout the need of ad-hoc preprocessing steps. We evaluated our approach using\nthe RRUFF spectral database, comprising mineral sample data. Superior\nclassification performance is demonstrated compared with other frequently used\nmachine learning algorithms including the popular support vector machine. \n\n"}
{"id": "1708.09211", "contents": "Title: On the consistency of the spacings test for multivariate uniformity Abstract: We give a simple conceptual proof of the consistency of a test for\nmultivariate uniformity in a bounded set $K \\subset \\mathbb{R}^d$ that is based\non the maximal spacing generated by i.i.d. points $X_1, \\ldots,X_n$ in $K$,\ni.e., the volume of the largest convex set of a given shape that is contained\nin $K$ and avoids each of these points. Since asymptotic results for the case\n$d > 1$ are only availabe under uniformity, a key element of the proof is a\nsuitable coupling. \n\n"}
{"id": "1708.09305", "contents": "Title: A Pseudo Knockoff Filter for Correlated Features Abstract: In 2015, Barber and Candes introduced a new variable selection procedure\ncalled the knockoff filter to control the false discovery rate (FDR) and prove\nthat this method achieves exact FDR control. Inspired by the work of Barber and\nCandes (2015), we propose and analyze a pseudo-knockoff filter that inherits\nsome advantages of the original knockoff filter and has more flexibility in\nconstructing its knockoff matrix. Moreover, we perform a number of numerical\nexperiments that seem to suggest that the pseudo knockoff filter with the half\nLasso statistic has FDR control and offers more power than the original\nknockoff filter with the Lasso Path or the half Lasso Statistic for the\nnumerical examples that we consider in this paper. Although we cannot establish\nrigorous FDR control for the pseudo knockoff filter, we provide some partial\nanalysis of the pseudo knockoff filter with the half Lasso statistic and\nestablish a uniform FDP bound and an expectation inequality. \n\n"}
{"id": "1709.00081", "contents": "Title: Two-sample instrumental variable analyses using heterogeneous samples Abstract: Instrumental variable analysis is a widely used method to estimate causal\neffects in the presence of unmeasured confounding. When the instruments,\nexposure and outcome are not measured in the same sample, Angrist and Krueger\n(1992) suggested to use two-sample instrumental variable (TSIV) estimators that\nuse sample moments from an instrument-exposure sample and an instrument-outcome\nsample. However, this method is biased if the two samples are from\nheterogeneous populations so that the distributions of the instruments are\ndifferent. In linear structural equation models, we derive a new class of TSIV\nestimators that are robust to heterogeneous samples under the key assumption\nthat the structural relations in the two samples are the same. The widely used\ntwo-sample two-stage least squares estimator belongs to this class. It is\ngenerally not asymptotically efficient, although we find that it performs\nsimilarly to the optimal TSIV estimator in most practical situations. We then\nattempt to relax the linearity assumption. We find that, unlike one-sample\nanalyses, the TSIV estimator is not robust to misspecified exposure model.\nAdditionally, to nonparametrically identify the magnitude of the causal effect,\nthe noise in the exposure must have the same distributions in the two samples.\nHowever, this assumption is in general untestable because the exposure is not\nobserved in one sample. Nonetheless, we may still identify the sign of the\ncausal effect in the absence of homogeneity of the noise. \n\n"}
{"id": "1709.02244", "contents": "Title: Improved Quantile Regression Estimators when the Errors are\n  Independently and Non-identically Distributed Abstract: In a classical regression model, it is usually assumed that the explanatory\nvariables are independent of each other and error terms are normally\ndistributed. But when these assumptions are not met, situations like the error\nterms are not independent or they are not identically distributed or both of\nthese, LSE will not be robust. Hence, quantile regression has been used to\ncomplement this deficiency of classical regression analysis and to improve the\nleast square estimation (LSE). In this study, we consider preliminary test and\nshrinkage estimation strategies for quantile regression models with\nindependently and non-identically distributed (i.ni.d.) errors. A Monte Carlo\nsimulation study is conducted to assess the relative performance of the\nestimators. Also, we numerically compare their performance with Ridge, Lasso,\nElastic Net penalty estimation strategies. A real data example is presented to\nillustrate the usefulness of the suggested methods. Finally, we obtain the\nasymptotic results of suggested estimators \n\n"}
{"id": "1709.03137", "contents": "Title: Methods for Estimation of Convex Sets Abstract: In the framework of shape constrained estimation, we review methods and works\ndone in convex set estimation. These methods mostly build on stochastic and\nconvex geometry, empirical process theory, functional analysis, linear\nprogramming, extreme value theory, etc. The statistical problems that we review\ninclude density support estimation, estimation of the level sets of densities\nor depth functions, nonparametric regression, etc. We focus on the estimation\nof convex sets under the Nikodym and Hausdorff metrics, which require different\ntechniques and, quite surprisingly, lead to very different results, in\nparticular in density support estimation. Finally, we discuss computational\nissues in high dimensions. \n\n"}
{"id": "1709.03393", "contents": "Title: Optimal prediction in the linearly transformed spiked model Abstract: We consider the linearly transformed spiked model, where observations $Y_i$\nare noisy linear transforms of unobserved signals of interest $X_i$:\n\\begin{align*}\n  Y_i = A_i X_i + \\varepsilon_i, \\end{align*} for $i=1,\\ldots,n$. The transform\nmatrices $A_i$ are also observed. We model $X_i$ as random vectors lying on an\nunknown low-dimensional space. How should we predict the unobserved signals\n(regression coefficients) $X_i$?\n  The naive approach of performing regression for each observation separately\nis inaccurate due to the large noise. Instead, we develop optimal linear\nempirical Bayes methods for predicting $X_i$ by \"borrowing strength\" across the\ndifferent samples. Our methods are applicable to large datasets and rely on\nweak moment assumptions. The analysis is based on random matrix theory.\n  We discuss applications to signal processing, deconvolution, cryo-electron\nmicroscopy, and missing data in the high-noise regime. For missing data, we\nshow in simulations that our methods are faster, more robust to noise and to\nunequal sampling than well-known matrix completion methods. \n\n"}
{"id": "1709.04342", "contents": "Title: Model Selection Confidence Sets by Likelihood Ratio Testing Abstract: The traditional activity of model selection aims at discovering a single\nmodel superior to other candidate models. In the presence of pronounced noise,\nhowever, multiple models are often found to explain the same data equally well.\nTo resolve this model selection ambiguity, we introduce the general approach of\nmodel selection confidence sets (MSCSs) based on likelihood ratio testing. A\nMSCS is defined as a list of models statistically indistinguishable from the\ntrue model at a user-specified level of confidence, which extends the familiar\nnotion of confidence intervals to the model-selection framework. Our approach\nguarantees asymptotically correct coverage probability of the true model when\nboth sample size and model dimension increase. We derive conditions under which\nthe MSCS contains all the relevant information about the true model structure.\nIn addition, we propose natural statistics based on the MSCS to measure\nimportance of variables in a principled way that accounts for the overall model\nuncertainty. When the space of feasible models is large, MSCS is implemented by\nan adaptive stochastic search algorithm which samples MSCS models with high\nprobability. The MSCS methodology is illustrated through numerical experiments\non synthetic data and real data examples. \n\n"}
{"id": "1709.05454", "contents": "Title: Statistical inference on random dot product graphs: a survey Abstract: The random dot product graph (RDPG) is an independent-edge random graph that\nis analytically tractable and, simultaneously, either encompasses or can\nsuccessfully approximate a wide range of random graphs, from relatively simple\nstochastic block models to complex latent position graphs. In this survey\npaper, we describe a comprehensive paradigm for statistical inference on random\ndot product graphs, a paradigm centered on spectral embeddings of adjacency and\nLaplacian matrices. We examine the analogues, in graph inference, of several\ncanonical tenets of classical Euclidean inference: in particular, we summarize\na body of existing results on the consistency and asymptotic normality of the\nadjacency and Laplacian spectral embeddings, and the role these spectral\nembeddings can play in the construction of single- and multi-sample hypothesis\ntests for graph data. We investigate several real-world applications, including\ncommunity detection and classification in large social networks and the\ndetermination of functional and biologically relevant network properties from\nan exploratory data analysis of the Drosophila connectome. We outline requisite\nbackground and current open problems in spectral graph inference. \n\n"}
{"id": "1709.06653", "contents": "Title: Unique Information via Dependency Constraints Abstract: The partial information decomposition (PID) is perhaps the leading proposal\nfor resolving information shared between a set of sources and a target into\nredundant, synergistic, and unique constituents. Unfortunately, the PID\nframework has been hindered by a lack of a generally agreed-upon, multivariate\nmethod of quantifying the constituents. Here, we take a step toward rectifying\nthis by developing a decomposition based on a new method that quantifies unique\ninformation. We first develop a broadly applicable method---the dependency\ndecomposition---that delineates how statistical dependencies influence the\nstructure of a joint distribution. The dependency decomposition then allows us\nto define a measure of the information about a target that can be uniquely\nattributed to a particular source as the least amount which the source-target\nstatistical dependency can influence the information shared between the sources\nand the target. The result is the first measure that satisfies the core axioms\nof the PID framework while not satisfying the Blackwell relation, which depends\non a particular interpretation of how the variables are related. This makes a\nkey step forward to a practical PID. \n\n"}
{"id": "1709.07036", "contents": "Title: Inter-Subject Analysis: Inferring Sparse Interactions with Dense\n  Intra-Graphs Abstract: We develop a new modeling framework for Inter-Subject Analysis (ISA). The\ngoal of ISA is to explore the dependency structure between different subjects\nwith the intra-subject dependency as nuisance. It has important applications in\nneuroscience to explore the functional connectivity between brain regions under\nnatural stimuli. Our framework is based on the Gaussian graphical models, under\nwhich ISA can be converted to the problem of estimation and inference of the\ninter-subject precision matrix. The main statistical challenge is that we do\nnot impose sparsity constraint on the whole precision matrix and we only assume\nthe inter-subject part is sparse. For estimation, we propose to estimate an\nalternative parameter to get around the non-sparse issue and it can achieve\nasymptotic consistency even if the intra-subject dependency is dense. For\ninference, we propose an \"untangle and chord\" procedure to de-bias our\nestimator. It is valid without the sparsity assumption on the inverse Hessian\nof the log-likelihood function. This inferential method is general and can be\napplied to many other statistical problems, thus it is of independent\ntheoretical interest. Numerical experiments on both simulated and brain imaging\ndata validate our methods and theory. \n\n"}
{"id": "1709.07097", "contents": "Title: Persistence Flamelets: multiscale Persistent Homology for kernel density\n  exploration Abstract: In recent years there has been noticeable interest in the study of the \"shape\nof data\". Among the many ways a \"shape\" could be defined, topology is the most\ngeneral one, as it describes an object in terms of its connectivity structure:\nconnected components (topological features of dimension 0), cycles (features of\ndimension 1) and so on. There is a growing number of techniques, generally\ndenoted as Topological Data Analysis, aimed at estimating topological\ninvariants of a fixed object; when we allow this object to change, however,\nlittle has been done to investigate the evolution in its topology. In this work\nwe define the Persistence Flamelets, a multiscale version of one of the most\npopular tool in TDA, the Persistence Landscape. We examine its theoretical\nproperties and we show how it could be used to gain insights on KDEs bandwidth\nparameter. \n\n"}
{"id": "1709.07842", "contents": "Title: Bayesian Optimization for Parameter Tuning of the XOR Neural Network Abstract: When applying Machine Learning techniques to problems, one must select model\nparameters to ensure that the system converges but also does not become stuck\nat the objective function's local minimum. Tuning these parameters becomes a\nnon-trivial task for large models and it is not always apparent if the user has\nfound the optimal parameters. We aim to automate the process of tuning a Neural\nNetwork, (where only a limited number of parameter search attempts are\navailable) by implementing Bayesian Optimization. In particular, by assigning\nGaussian Process Priors to the parameter space, we utilize Bayesian\nOptimization to tune an Artificial Neural Network used to learn the XOR\nfunction, with the result of achieving higher prediction accuracy. \n\n"}
{"id": "1709.09702", "contents": "Title: Projective, Sparse, and Learnable Latent Position Network Models Abstract: When modeling network data using a latent position model, it is typical to\nassume that the nodes' positions are independently and identically distributed.\nHowever, this assumption implies the average node degree grows linearly with\nthe number of nodes, which is inappropriate when the graph is thought to be\nsparse. We propose an alternative assumption -- that the latent positions are\ngenerated according to a Poisson point process -- and show that it is\ncompatible with various levels of sparsity. Unlike other notions of sparse\nlatent position models in the literature, our framework also defines a\nprojective sequence of probability models, thus ensuring consistency of\nstatistical inference across networks of different sizes. We establish\nconditions for consistent estimation of the latent positions, and compare our\nresults to existing frameworks for modeling sparse networks. \n\n"}
{"id": "1709.10280", "contents": "Title: Non-parametric Message Important Measure: Storage Code Design and\n  Transmission Planning for Big Data Abstract: Storage and transmission in big data are discussed in this paper, where\nmessage importance is taken into account. Similar to Shannon Entropy and Renyi\nEntropy, we define non-parametric message important measure (NMIM) as a measure\nfor the message importance in the scenario of big data, which can characterize\nthe uncertainty of random events. It is proved that the proposed NMIM can\nsufficiently describe two key characters of big data: rare events finding and\nlarge diversities of events. Based on NMIM, we first propose an effective\ncompressed encoding mode for data storage, and then discuss the channel\ntransmission over some typical channel models. Numerical simulation results\nshow that using our proposed strategy occupies less storage space without\nlosing too much message importance, and there are growth region and saturation\nregion for the maximum transmission, which contributes to designing of better\npractical communication system. \n\n"}
{"id": "1709.10367", "contents": "Title: Structured Embedding Models for Grouped Data Abstract: Word embeddings are a powerful approach for analyzing language, and\nexponential family embeddings (EFE) extend them to other types of data. Here we\ndevelop structured exponential family embeddings (S-EFE), a method for\ndiscovering embeddings that vary across related groups of data. We study how\nthe word usage of U.S. Congressional speeches varies across states and party\naffiliation, how words are used differently across sections of the ArXiv, and\nhow the co-purchase patterns of groceries can vary across seasons. Key to the\nsuccess of our method is that the groups share statistical information. We\ndevelop two sharing strategies: hierarchical modeling and amortization. We\ndemonstrate the benefits of this approach in empirical studies of speeches,\nabstracts, and shopping baskets. We show how S-EFE enables group-specific\ninterpretation of word usage, and outperforms EFE in predicting held-out data. \n\n"}
{"id": "1710.00769", "contents": "Title: Stochastic Comparisons of Lifetimes of Two Series and Parallel Systems\n  with Location-Scale Family Distributed Components having Archimedean Copulas Abstract: In this paper, we compare the lifetimes of two series and two parallel\nsystems stochastically where the lifetime of each component follows\nlocation-scale (LS) family of distributions. The comparison is carried out\nunder two scenarios: one, that the components of the systems have a dependent\nstructure sharing Archimedean copula and two, that the components are\nindependently distributed. It is shown that the systems with components in\nseries or parallel sharing Archimedean copula with more dispersion in the\nlocation or scale parameters results in better performance in the sense of the\nusual stochastic order. It is also shown that if the components are\nindependently distributed, it is possible to obtain more generalized results as\ncompared to the dependent set-up. The results in this paper generalizes similar\nresults in both independent and dependent set up for exponential and Weibull\ndistributed components. \n\n"}
{"id": "1710.00959", "contents": "Title: Bayesian Inference under Cluster Sampling with Probability Proportional\n  to Size Abstract: Cluster sampling is common in survey practice, and the corresponding\ninference has been predominantly design-based. We develop a Bayesian framework\nfor cluster sampling and account for the design effect in the outcome modeling.\nWe consider a two-stage cluster sampling design where the clusters are first\nselected with probability proportional to cluster size, and then units are\nrandomly sampled inside selected clusters. Challenges arise when the sizes of\nnonsampled cluster are unknown. We propose nonparametric and parametric\nBayesian approaches for predicting the unknown cluster sizes, with this\ninference performed simultaneously with the model for survey outcome.\nSimulation studies show that the integrated Bayesian approach outperforms\nclassical methods with efficiency gains. We use Stan for computing and apply\nthe proposal to the Fragile Families and Child Wellbeing study as an\nillustration of complex survey inference in health surveys. \n\n"}
{"id": "1710.01552", "contents": "Title: Bayesian inference for stationary data on finite state spaces Abstract: In this work the issue of Bayesian inference for stationary data is\naddressed. Therefor a parametrization of a statistically suitable subspace of\nthe the shift-ergodic probability measures on a Cartesian product of some\nfinite state space is given using an inverse limit construction. Moreover, an\nexplicit model for the prior is given by taking into account an additional step\nin the usual stepwise sampling scheme of data. An update to the posterior is\ndefined by exploiting this augmented sample scheme. Thereby, its model-step is\nupdated using a measurement of the empirical distances between the model\nclasses. \n\n"}
{"id": "1710.02704", "contents": "Title: Nonsparse learning with latent variables Abstract: As a popular tool for producing meaningful and interpretable models,\nlarge-scale sparse learning works efficiently when the underlying structures\nare indeed or close to sparse. However, naively applying the existing\nregularization methods can result in misleading outcomes due to model\nmisspecification. In particular, the direct sparsity assumption on coefficient\nvectors has been questioned in real applications. Therefore, we consider\nnonsparse learning with the conditional sparsity structure that the coefficient\nvector becomes sparse after taking out the impacts of certain unobservable\nlatent variables. A new methodology of nonsparse learning with latent variables\n(NSL) is proposed to simultaneously recover the significant observable\npredictors and latent factors as well as their effects. We explore a common\nlatent family incorporating population principal components and derive the\nconvergence rates of both sample principal components and their score vectors\nthat hold for a wide class of distributions. With the properly estimated latent\nvariables, properties including model selection consistency and oracle\ninequalities under various prediction and estimation losses are established for\nthe proposed methodology. Our new methodology and results are evidenced by\nsimulation and real data examples. \n\n"}
{"id": "1710.02715", "contents": "Title: Wasserstein and total variation distance between marginals of L\\'evy\n  processes Abstract: We present upper bounds for the Wasserstein distance of order $p$ between the\nmarginals of L\\'evy processes, including Gaussian approximations for jumps of\ninfinite activity. Using the convolution structure, we further derive upper\nbounds for the total variation distance between the marginals of L\\'evy\nprocesses. Connections to other metrics like Zolotarev and Toscani-Fourier\ndistances are established. The theory is illustrated by concrete examples and\nan application to statistical lower bounds. \n\n"}
{"id": "1710.04580", "contents": "Title: Additivity of Information in Multilayer Networks via Additive Gaussian\n  Noise Transforms Abstract: Multilayer (or deep) networks are powerful probabilistic models based on\nmultiple stages of a linear transform followed by a non-linear (possibly\nrandom) function. In general, the linear transforms are defined by matrices and\nthe non-linear functions are defined by information channels. These models have\ngained great popularity due to their ability to characterize complex\nprobabilistic relationships arising in a wide variety of inference problems.\nThe contribution of this paper is a new method for analyzing the fundamental\nlimits of statistical inference in settings where the model is known. The\nvalidity of our method can be established in a number of settings and is\nconjectured to hold more generally. A key assumption made throughout is that\nthe matrices are drawn randomly from orthogonally invariant distributions.\n  Our method yields explicit formulas for 1) the mutual information; 2) the\nminimum mean-squared error (MMSE); 3) the existence and locations of certain\nphase-transitions with respect to the problem parameters; and 4) the stationary\npoints for the state evolution of approximate message passing algorithms. When\napplied to the special case of models with multivariate Gaussian channels our\nmethod is rigorous and has close connections to free probability theory for\nrandom matrices. When applied to the general case of non-Gaussian channels, our\nmethod provides a simple alternative to the replica method from statistical\nphysics. A key observation is that the combined effects of the individual\ncomponents in the model (namely the matrices and the channels) are additive\nwhen viewed in a certain transform domain. \n\n"}
{"id": "1710.04584", "contents": "Title: Towards Scalable Spectral Clustering via Spectrum-Preserving\n  Sparsification Abstract: The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is\nthe main computational bottleneck in spectral clustering. In this work, we\nintroduce a highly-scalable, spectrum-preserving graph sparsification algorithm\nthat enables to build ultra-sparse NN (u-NN) graphs with guaranteed\npreservation of the original graph spectrums, such as the first few\neigenvectors of the original graph Laplacian. Our approach can immediately lead\nto scalable spectral clustering of large data networks without sacrificing\nsolution quality. The proposed method starts from constructing low-stretch\nspanning trees (LSSTs) from the original graphs, which is followed by\niteratively recovering small portions of \"spectrally critical\" off-tree edges\nto the LSSTs by leveraging a spectral off-tree embedding scheme. To determine\nthe suitable amount of off-tree edges to be recovered to the LSSTs, an\neigenvalue stability checking scheme is proposed, which enables to robustly\npreserve the first few Laplacian eigenvectors within the sparsified graph.\nAdditionally, an incremental graph densification scheme is proposed for\nidentifying extra edges that have been missing in the original NN graphs but\ncan still play important roles in spectral clustering tasks. Our experimental\nresults for a variety of well-known data sets show that the proposed method can\ndramatically reduce the complexity of NN graphs, leading to significant\nspeedups in spectral clustering. \n\n"}
{"id": "1710.05248", "contents": "Title: A Nonparametric Method for Producing Isolines of Bivariate Exceedance\n  Probabilities Abstract: We present a method for drawing isolines indicating regions of equal joint\nexceedance probability for bivariate data. The method relies on bivariate\nregular variation, a dependence framework widely used for extremes. This\nframework enables drawing isolines corresponding to very low exceedance\nprobabilities and these lines may lie beyond the range of the data. The method\nwe utilize for characterizing dependence in the tail is largely nonparametric.\nFurthermore, we extend this method to the case of asymptotic independence and\npropose a procedure which smooths the transition from asymptotic independence\nin the interior to the first-order behavior on the axes. We propose a\ndiagnostic plot for assessing isoline estimate and choice of smoothing, and a\nbootstrap procedure to visually assess uncertainty. \n\n"}
{"id": "1710.05895", "contents": "Title: Spectral Algorithms for Computing Fair Support Vector Machines Abstract: Classifiers and rating scores are prone to implicitly codifying biases, which\nmay be present in the training data, against protected classes (i.e., age,\ngender, or race). So it is important to understand how to design classifiers\nand scores that prevent discrimination in predictions. This paper develops\ncomputationally tractable algorithms for designing accurate but fair support\nvector machines (SVM's). Our approach imposes a constraint on the covariance\nmatrices conditioned on each protected class, which leads to a nonconvex\nquadratic constraint in the SVM formulation. We develop iterative algorithms to\ncompute fair linear and kernel SVM's, which solve a sequence of relaxations\nconstructed using a spectral decomposition of the nonconvex constraint. Its\neffectiveness in achieving high prediction accuracy while ensuring fairness is\nshown through numerical experiments on several data sets. \n\n"}
{"id": "1710.06592", "contents": "Title: Eigenvalue fluctuations for lattice Anderson Hamiltonians: Unbounded\n  potentials Abstract: We consider random Schr\\\"odinger operators with Dirichlet boundary conditions\noutside lattice approximations of a smooth Euclidean domain and study the\nbehavior of its lowest-lying eigenvalues in the limit when the lattice spacing\ntends to zero. Under a suitable moment assumption on the random potential and\nregularity of the spatial dependence of its mean, we prove that the eigenvalues\nof the random operator converge to those of a deterministic Schr\\\"odinger\noperator. Assuming also regularity of the variance, the fluctuation of the\nrandom eigenvalues around their mean are shown to obey a multivariate central\nlimit theorem. This extends the authors' recent work where similar conclusions\nhave been obtained for bounded random potentials. \n\n"}
{"id": "1710.07792", "contents": "Title: Cointegrated Density-Valued Linear Processes Abstract: In data rich environments we may sometimes deal with time series that are\nprobability density-function valued, such as observations of cross-sectional\nincome distributions over time. To apply the methods of functional time series\nanalysis to such observations, we should first embed them in a linear space in\nwhich the essential properties of densities are preserved under addition and\nscalar multiplication. Bayes Hilbert spaces provide one way to achieve this\nembedding. In this paper we investigate the use of Bayes Hilbert spaces to\nmodel cointegrated density-valued linear processes. We develop an I(1)\nrepresentation theory for cointegrated linear processes in a Bayes Hilbert\nspace, and adapt existing statistical procedures for estimating the\ncorresponding attractor space to a Bayes Hilbert space setting. We revisit\nempirical applications involving earnings and wage densities to illustrate the\nutility of our approach. \n\n"}
{"id": "1710.07797", "contents": "Title: Optimal Rates for Learning with Nystr\\\"om Stochastic Gradient Methods Abstract: In the setting of nonparametric regression, we propose and study a\ncombination of stochastic gradient methods with Nystr\\\"om subsampling, allowing\nmultiple passes over the data and mini-batches. Generalization error bounds for\nthe studied algorithm are provided. Particularly, optimal learning rates are\nderived considering different possible choices of the step-size, the mini-batch\nsize, the number of iterations/passes, and the subsampling level. In comparison\nwith state-of-the-art algorithms such as the classic stochastic gradient\nmethods and kernel ridge regression with Nystr\\\"om, the studied algorithm has\nadvantages on the computational complexity, while achieving the same optimal\nlearning rates. Moreover, our results indicate that using mini-batches can\nreduce the total computational cost while achieving the same optimal\nstatistical results. \n\n"}
{"id": "1710.08017", "contents": "Title: Adaptive Bayesian nonparametric regression using kernel mixture of\n  polynomials with application to partial linear model Abstract: We propose a kernel mixture of polynomials prior for Bayesian nonparametric\nregression. The regression function is modeled by local averages of polynomials\nwith kernel mixture weights. We obtain the minimax-optimal rate of contraction\nof the full posterior distribution up to a logarithmic factor that adapts to\nthe smoothness level of the true function by estimating metric entropies of\ncertain function classes. We also provide a frequentist sieve maximum\nlikelihood estimator with a near-optimal convergence rate. We further\ninvestigate the application of the kernel mixture of polynomials to the partial\nlinear model and obtain both the near-optimal rate of contraction for the\nnonparametric component and the Bernstein-von Mises limit (i.e., asymptotic\nnormality) of the parametric component. The proposed method is illustrated with\nnumerical examples and shows superior performance in terms of computational\nefficiency, accuracy, and uncertainty quantification compared to the local\npolynomial regression, DiceKriging, and the robust Gaussian stochastic process. \n\n"}
{"id": "1710.08388", "contents": "Title: A Test for Separability in Covariance Operators of Random Surfaces Abstract: The assumption of separability is a simplifying and very popular assumption\nin the analysis of spatio-temporal or hypersurface data structures. It is often\nmade in situations where the covariance structure cannot be easily estimated,\nfor example because of a small sample size or because of computational storage\nproblems. In this paper we propose a new and very simple test to validate this\nassumption. Our approach is based on a measure of separability which is zero in\nthe case of separability and positive otherwise. The measure can be estimated\nwithout calculating the full non-separable covariance operator. We prove\nasymptotic normality of the corresponding statistic with a limiting variance,\nwhich can easily be estimated from the available data. As a consequence\nquantiles of the standard normal distribution can be used to obtain critical\nvalues and the new test of separability is very easy to implement. In\nparticular, our approach does neither require projections on subspaces\ngenerated by the eigenfunctions of the covariance operator, nor resampling\nprocedures to obtain critical values nor distributional assumptions as used by\nother available methods of constructing tests for separability. We investigate\nthe finite sample performance by means of a simulation study and also provide a\ncomparison with the currently available methodology. Finally, the new procedure\nis illustrated analyzing wind speed and temperature data. \n\n"}
{"id": "1710.08607", "contents": "Title: Provable and practical approximations for the degree distribution using\n  sublinear graph samples Abstract: The degree distribution is one of the most fundamental properties used in the\nanalysis of massive graphs. There is a large literature on graph sampling,\nwhere the goal is to estimate properties (especially the degree distribution)\nof a large graph through a small, random sample. The degree distribution\nestimation poses a significant challenge, due to its heavy-tailed nature and\nthe large variance in degrees.\n  We design a new algorithm, SADDLES, for this problem, using recent\nmathematical techniques from the field of sublinear algorithms. The SADDLES\nalgorithm gives provably accurate outputs for all values of the degree\ndistribution. For the analysis, we define two fatness measures of the degree\ndistribution, called the $h$-index and the $z$-index. We prove that SADDLES is\nsublinear in the graph size when these indices are large. A corollary of this\nresult is a provably sublinear algorithm for any degree distribution bounded\nbelow by a power law.\n  We deploy our new algorithm on a variety of real datasets and demonstrate its\nexcellent empirical behavior. In all instances, we get extremely accurate\napproximations for all values in the degree distribution by observing at most\n$1\\%$ of the vertices. This is a major improvement over the state-of-the-art\nsampling algorithms, which typically sample more than $10\\%$ of the vertices to\ngive comparable results. We also observe that the $h$ and $z$-indices of real\ngraphs are large, validating our theoretical analysis. \n\n"}
{"id": "1710.09146", "contents": "Title: Bayesian hypothesis tests with diffuse priors: Can we have our cake and\n  eat it too? Abstract: We introduce a new class of priors for Bayesian hypothesis testing, which we\nname \"cake priors\". These priors circumvent Bartlett's paradox (also called the\nJeffreys-Lindley paradox); the problem associated with the use of diffuse\npriors leading to nonsensical statistical inferences. Cake priors allow the use\nof diffuse priors (having one's cake) while achieving theoretically justified\ninferences (eating it too). We demonstrate this methodology for Bayesian\nhypotheses tests for scenarios under which the one and two sample t-tests, and\nlinear models are typically derived. The resulting Bayesian test statistic\ntakes the form of a penalized likelihood ratio test statistic. By considering\nthe sampling distribution under the null and alternative hypotheses we show for\nindependent identically distributed regular parametric models that Bayesian\nhypothesis tests using cake priors are Chernoff-consistent, i.e., achieve zero\ntype I and II errors asymptotically. Lindley's paradox is also discussed. We\nargue that a true Lindley's paradox will only occur with small probability for\nlarge sample sizes. \n\n"}
{"id": "1710.10099", "contents": "Title: On the Optimal Reconstruction of Partially Observed Functional Data Abstract: We propose a new reconstruction operator that aims to recover the missing\nparts of a function given the observed parts. This new operator belongs to a\nnew, very large class of functional operators which includes the classical\nregression operators as a special case. We show the optimality of our\nreconstruction operator and demonstrate that the usually considered regression\noperators generally cannot be optimal reconstruction operators. Our estimation\ntheory allows for autocorrelated functional data and considers the practically\nrelevant situation in which each of the $n$ functions is observed at $m_i$,\n$i=1,\\dots,n$, discretization points. We derive rates of consistency for our\nnonparametric estimation procedures using a double asymptotic. For data\nsituations, as in our real data application where $m_i$ is considerably smaller\nthan $n$, we show that our functional principal components based estimator can\nprovide better rates of convergence than conventional nonparametric smoothing\nmethods. \n\n"}
{"id": "1710.10653", "contents": "Title: Robust adaptive efficient estimation for a semi-Markov continuous time\n  regression from discrete data Abstract: In this article we consider the nonparametric robust estimation problem for\nregression models in continuous time with semi-Markov noises observed in\ndiscrete time moments. An adaptive model selection procedure is proposed. A\nsharp non-asymptotic oracle inequality for the robust risks is obtained. We\nobtain sufficient conditions on the frequency observations under which the\nrobust efficiency is shown. It turns out that for the semi-Markov models the\nrobust minimax convergence rate may be faster or slower than the classical one. \n\n"}
{"id": "1710.10720", "contents": "Title: Globally Optimal Symbolic Regression Abstract: In this study we introduce a new technique for symbolic regression that\nguarantees global optimality. This is achieved by formulating a mixed integer\nnon-linear program (MINLP) whose solution is a symbolic mathematical expression\nof minimum complexity that explains the observations. We demonstrate our\napproach by rediscovering Kepler's law on planetary motion using exoplanet data\nand Galileo's pendulum periodicity equation using experimental data. \n\n"}
{"id": "1710.11592", "contents": "Title: On Learning Mixtures of Well-Separated Gaussians Abstract: We consider the problem of efficiently learning mixtures of a large number of\nspherical Gaussians, when the components of the mixture are well separated. In\nthe most basic form of this problem, we are given samples from a uniform\nmixture of $k$ standard spherical Gaussians, and the goal is to estimate the\nmeans up to accuracy $\\delta$ using $poly(k,d, 1/\\delta)$ samples.\n  In this work, we study the following question: what is the minimum separation\nneeded between the means for solving this task? The best known algorithm due to\nVempala and Wang [JCSS 2004] requires a separation of roughly\n$\\min\\{k,d\\}^{1/4}$. On the other hand, Moitra and Valiant [FOCS 2010] showed\nthat with separation $o(1)$, exponentially many samples are required. We\naddress the significant gap between these two bounds, by showing the following\nresults.\n  1. We show that with separation $o(\\sqrt{\\log k})$, super-polynomially many\nsamples are required. In fact, this holds even when the $k$ means of the\nGaussians are picked at random in $d=O(\\log k)$ dimensions.\n  2. We show that with separation $\\Omega(\\sqrt{\\log k})$, $poly(k,d,1/\\delta)$\nsamples suffice. Note that the bound on the separation is independent of\n$\\delta$. This result is based on a new and efficient \"accuracy boosting\"\nalgorithm that takes as input coarse estimates of the true means and in time\n$poly(k,d, 1/\\delta)$ outputs estimates of the means up to arbitrary accuracy\n$\\delta$ assuming the separation between the means is $\\Omega(\\min\\{\\sqrt{\\log\nk},\\sqrt{d}\\})$ (independently of $\\delta$).\n  We also present a computationally efficient algorithm in $d=O(1)$ dimensions\nwith only $\\Omega(\\sqrt{d})$ separation. These results together essentially\ncharacterize the optimal order of separation between components that is needed\nto learn a mixture of $k$ spherical Gaussians with polynomial samples. \n\n"}
{"id": "1711.00497", "contents": "Title: Post-selection estimation and testing following aggregated association\n  tests Abstract: The practice of pooling several individual test statistics to form aggregate\ntests is common in many statistical application where individual tests may be\nunderpowered. While selection by aggregate tests can serve to increase power,\nthe selection process invalidates the individual test-statistics, making it\ndifficult to identify the ones that drive the signal in follow-up inference.\nHere, we develop a general approach for valid inference following selection by\naggregate testing. We present novel powerful post-selection tests for the\nindividual null hypotheses which are exact for the normal model and\nasymptotically justified otherwise. Our approach relies on the ability to\ncharacterize the distribution of the individual test statistics after\nconditioning on the event of selection. We provide efficient algorithms for\nestimation of the post-selection maximum-likelihood estimates and suggest\nconfidence intervals which rely on a novel switching regime for good coverage\nguarantees. We validate our methods via comprehensive simulation studies and\napply them to data from the Dallas Heart Study, demonstrating that single\nvariant association discovery following selection by an aggregated test is\nindeed possible in practice. \n\n"}
{"id": "1711.02876", "contents": "Title: Dimension Estimation Using Random Connection Models Abstract: Information about intrinsic dimension is crucial to perform dimensionality\nreduction, compress information, design efficient algorithms, and do\nstatistical adaptation. In this paper we propose an estimator for the intrinsic\ndimension of a data set. The estimator is based on binary neighbourhood\ninformation about the observations in the form of two adjacency matrices, and\ndoes not require any explicit distance information. The underlying graph is\nmodelled according to a subset of a specific random connection model, sometimes\nreferred to as the Poisson blob model. Computationally the estimator scales\nlike n log n, and we specify its asymptotic distribution and rate of\nconvergence. A simulation study on both real and simulated data shows that our\napproach compares favourably with some competing methods from the literature,\nincluding approaches that rely on distance information. \n\n"}
{"id": "1711.03361", "contents": "Title: Multi-Relevance Transfer Learning Abstract: Transfer learning aims to faciliate learning tasks in a label-scarce target\ndomain by leveraging knowledge from a related source domain with plenty of\nlabeled data. Often times we may have multiple domains with little or no\nlabeled data as targets waiting to be solved. Most existing efforts tackle\ntarget domains separately by modeling the `source-target' pairs without\nexploring the relatedness between them, which would cause loss of crucial\ninformation, thus failing to achieve optimal capability of knowledge transfer.\nIn this paper, we propose a novel and effective approach called Multi-Relevance\nTransfer Learning (MRTL) for this purpose, which can simultaneously transfer\ndifferent knowledge from the source and exploits the shared common latent\nfactors between target domains. Specifically, we formulate the problem as an\noptimization task based on a collective nonnegative matrix tri-factorization\nframework. The proposed approach achieves both source-target transfer and\ntarget-target leveraging by sharing multiple decomposed latent subspaces.\nFurther, an alternative minimization learning algorithm is developed with\nconvergence guarantee. Empirical study validates the performance and\neffectiveness of MRTL compared to the state-of-the-art methods. \n\n"}
{"id": "1711.03459", "contents": "Title: Extreme matrices or how an exponential map links classical and free\n  extreme laws Abstract: Using the proposed by us thinning approach to describe extreme matrices, we\nfind an explicit exponentiation formula linking classical extreme laws of\nFr\\'echet, Gumbel and Weibull given by Fisher-Tippet-Gnedenko classification\nand free extreme laws of free Fr\\'echet, free Gumbel and free Weibull by Ben\nArous and Voiculescu [1]. We also develop an extreme random matrix formalism,\nin which refined questions about extreme matrices can be answered. In\nparticular, we demonstrate explicit calculations for several more or less known\nrandom matrix ensembles, providing examples of all three free extreme laws.\nFinally, we present an exact mapping, showing the equivalence of free extreme\nlaws to the Peak-Over-Threshold method in classical probability. \n\n"}
{"id": "1711.03613", "contents": "Title: Debiasing the Debiased Lasso with Bootstrap Abstract: We consider statistical inference for a single coordinate of regression\ncoefficients in high-dimensional linear models. Recently, the debiased\nestimators are popularly used for constructing confidence intervals and\nhypothesis testing in high-dimensional models. However, some representative\nnumerical experiments show that they tend to be biased for large coefficients,\nespecially when the number of large coefficients dominates the number of small\ncoefficients. In this paper, we propose a modified debiased Lasso estimator\nbased on bootstrap. Let us denote the proposed estimator BS-DB for short. We\nshow that, under the irrepresentable condition and other mild technical\nconditions, the BS-DB has smaller order of bias than the debiased Lasso in\nexistence of a large proportion of strong signals. If the irrepresentable\ncondition does not hold, the BS-DB is guaranteed to perform no worse than the\ndebiased Lasso asymptotically. Confidence intervals based on the BS-DB are\nproposed and proved to be asymptotically valid under mild conditions. Our study\non the inference problems integrates the properties of the Lasso on variable\nselection and estimation novelly. The superior performance of the BS-DB over\nthe debiased Lasso is demonstrated via extensive numerical studies. \n\n"}
{"id": "1711.03740", "contents": "Title: Estimation of Cusp Location of Stochastic Processes: a Survey Abstract: We present a review of some recent results on estimation of location\nparameter for several models of observations with cusp-type singularity at the\nchange point. We suppose that the cusp-type models fit better to the real\nphenomena described usually by change point models. The list of models includes\nGaussian, inhomogeneous Poisson, ergodic diffusion processes, time series and\nthe classical case of i.i.d. observations. We describe the properties of the\nmaximum likelihood and Bayes estimators under some asymptotic assumptions. The\nasymptotic efficiency of estimators are discussed as well and the results of\nsome numerical simulations are presented. We provide some heuristic arguments\nwhich demonstrate the convergence of log-likelihood ratios in the models under\nconsideration to the fractional Brownian motion. \n\n"}
{"id": "1711.03783", "contents": "Title: A Theoretical Analysis of Sparse Recovery Stability of Dantzig Selector\n  and LASSO Abstract: Dantzig selector (DS) and LASSO problems have attracted plenty of attention\nin statistical learning, sparse data recovery and mathematical optimization. In\nthis paper, we provide a theoretical analysis of the sparse recovery stability\nof these optimization problems in more general settings and from a new\nperspective. We establish recovery error bounds for these optimization problems\nunder a mild assumption called weak range space property of a transposed design\nmatrix. This assumption is less restrictive than the well known sparse recovery\nconditions such as restricted isometry property (RIP), null space property\n(NSP) or mutual coherence. In fact, our analysis indicates that this assumption\nis tight and cannot be relaxed for the standard DS problems in order to\nmaintain their sparse recovery stability. As a result, a series of new\nstability results for DS and LASSO have been established under various matrix\nproperties, including the RIP with constant $\\delta_{2k}< 1/\\sqrt{2}$ and the\n(constant-free) standard NSP of order $k.$ We prove that these matrix\nproperties can yield an identical recovery error bound for DS and LASSO with\nstability coefficients being measured by the so-called Robinson's constant,\ninstead of the conventional RIP or NSP constant. To our knowledge, this is the\nfirst time that the stability results with such a unified feature are\nestablished for DS and LASSO problems. Different from the standard analysis in\nthis area of research, our analysis is carried out deterministically, and the\nkey analytic tools used in our analysis include the error bound of linear\nsystems due to Hoffman and Robinson and polytope approximation of symmetric\nconvex bodies due to Barvinok. \n\n"}
{"id": "1711.03838", "contents": "Title: Modeling Asymmetric Relationships from Symmetric Networks Abstract: Many relationships requiring mutual agreement between pairs of actors produce\nobservable networks that are symmetric and undirected. Nevertheless the\nunobserved, asymmetric network is often of primary scientific interest. We\npropose a method that probabilistically reconstructs the unobserved, asymmetric\nnetwork from the observed, symmetric graph using a regression-based framework\nthat allows for inference on predictors of actors' decisions. We apply this\nmodel to the bilateral investment treaty network. Our approach extracts\npolitically relevant information about the network structure that is\ninaccessible to alternative approaches and has superior predictive performance. \n\n"}
{"id": "1711.05085", "contents": "Title: The mixability of elliptical distributions and log-elliptical\n  distributions Abstract: The concept of $\\phi$-complete mixability and $\\phi$-joint mixability was\nfirst introduced in Bignozzi and Puccetti (2015), which is an extension of\ncomplete and joint mixability. Following Bignozzi and Puccetti (2015), we\nconsider two more general cases of $\\phi$ and investigate the $\\phi$-joint\nmixability for elliptical distributions and logarithmic elliptical\ndistributions. Some sufficient conditions for the $\\phi$-joint mixability of\nsome distributions are investigated. In addition, a conjecture on the\nuniqueness of the center of $\\phi$-joint mixability and the forms of the\ndensities for some elliptical distributions are given. \n\n"}
{"id": "1711.05420", "contents": "Title: Accelerating Cross-Validation in Multinomial Logistic Regression with\n  $\\ell_1$-Regularization Abstract: We develop an approximate formula for evaluating a cross-validation estimator\nof predictive likelihood for multinomial logistic regression regularized by an\n$\\ell_1$-norm. This allows us to avoid repeated optimizations required for\nliterally conducting cross-validation; hence, the computational time can be\nsignificantly reduced. The formula is derived through a perturbative approach\nemploying the largeness of the data size and the model dimensionality. An\nextension to the elastic net regularization is also addressed. The usefulness\nof the approximate formula is demonstrated on simulated data and the ISOLET\ndataset from the UCI machine learning repository. \n\n"}
{"id": "1711.06821", "contents": "Title: Acquiring Common Sense Spatial Knowledge through Implicit Spatial\n  Templates Abstract: Spatial understanding is a fundamental problem with wide-reaching real-world\napplications. The representation of spatial knowledge is often modeled with\nspatial templates, i.e., regions of acceptability of two objects under an\nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with\nprior work that restricts spatial templates to explicit spatial prepositions\n(e.g., \"glass on table\"), here we extend this concept to implicit spatial\nlanguage, i.e., those relationships (generally actions) for which the spatial\narrangement of the objects is only implicitly implied (e.g., \"man riding\nhorse\"). In contrast with explicit relationships, predicting spatial\narrangements from implicit spatial language requires significant common sense\nspatial understanding. Here, we introduce the task of predicting spatial\ntemplates for two objects under a relationship, which can be seen as a spatial\nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t.\na horse when the man is walking the horse?\"). We present two simple\nneural-based models that leverage annotated images and structured text to learn\nthis task. The good performance of these models reveals that spatial locations\nare to a large extent predictable from implicit spatial language. Crucially,\nthe models attain similar performance in a challenging generalized setting,\nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have\nnever been seen before. Next, we go one step further by presenting the models\nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging\nword embeddings enables the models to output accurate spatial predictions,\nproving that the models acquire solid common sense spatial knowledge allowing\nfor such generalization. \n\n"}
{"id": "1711.07199", "contents": "Title: A new class of tests for multinormality with i.i.d. and Garch data based\n  on the empirical moment generating function Abstract: We generalize a recent class of tests for univariate normality that are based\non the empirical moment generating function to the multivariate setting, thus\nobtaining a class of affine invariant, consistent and easy-to-use\ngoodness-of-fit tests for multinormality. The test statistics are suitably\nweighted $L^2$-statistics, and we provide their asymptotic behavior both for\ni.i.d. observations as well as in the context of testing that the innovation\ndistribution of a multivariate GARCH model is Gaussian. We study the\nfinite-sample behavior of the new tests, compare the criteria with alternative\nexisting procedures, and apply the new procedure to a data set of monthly log\nreturns. \n\n"}
{"id": "1711.08328", "contents": "Title: Robust Bayes-Like Estimation: Rho-Bayes estimation Abstract: We consider the problem of estimating the joint distribution $P$ of $n$\nindependent random variables within the Bayes paradigm from a non-asymptotic\npoint of view. Assuming that $P$ admits some density $s$ with respect to a\ngiven reference measure, we consider a density model $\\overline S$ for $s$ that\nwe endow with a prior distribution $\\pi$ (with support $\\overline S$) and we\nbuild a robust alternative to the classical Bayes posterior distribution which\npossesses similar concentration properties around $s$ whenever it belongs to\nthe model $\\overline S$. Furthermore, in density estimation, the Hellinger\ndistance between the classical and the robust posterior distributions tends to\n0, as the number of observations tends to infinity, under suitable assumptions\non the model and the prior, provided that the model $\\overline S$ contains the\ntrue density $s$. However, unlike what happens with the classical Bayes\nposterior distribution, we show that the concentration properties of this new\nposterior distribution are still preserved in the case of a misspecification of\nthe model, that is when $s$ does not belong to $\\overline S$ but is close\nenough to it with respect to the Hellinger distance. \n\n"}
{"id": "1711.08947", "contents": "Title: Central limit theorems for entropy-regularized optimal transport on\n  finite spaces and statistical applications Abstract: The notion of entropy-regularized optimal transport, also known as Sinkhorn\ndivergence, has recently gained popularity in machine learning and statistics,\nas it makes feasible the use of smoothed optimal transportation distances for\ndata analysis. The Sinkhorn divergence allows the fast computation of an\nentropically regularized Wasserstein distance between two probability\ndistributions supported on a finite metric space of (possibly) high-dimension.\nFor data sampled from one or two unknown probability distributions, we derive\nthe distributional limits of the empirical Sinkhorn divergence and its centered\nversion (Sinkhorn loss). We also propose a bootstrap procedure which allows to\nobtain new test statistics for measuring the discrepancies between multivariate\nprobability distributions. Our work is inspired by the results of Sommerfeld\nand Munk (2016) on the asymptotic distribution of empirical Wasserstein\ndistance on finite space using unregularized transportation costs. Incidentally\nwe also analyze the asymptotic distribution of entropy-regularized Wasserstein\ndistances when the regularization parameter tends to zero. Simulated and real\ndatasets are used to illustrate our approach. \n\n"}
{"id": "1711.09279", "contents": "Title: A Big Data Analysis Framework Using Apache Spark and Deep Learning Abstract: With the spreading prevalence of Big Data, many advances have recently been\nmade in this field. Frameworks such as Apache Hadoop and Apache Spark have\ngained a lot of traction over the past decades and have become massively\npopular, especially in industries. It is becoming increasingly evident that\neffective big data analysis is key to solving artificial intelligence problems.\nThus, a multi-algorithm library was implemented in the Spark framework, called\nMLlib. While this library supports multiple machine learning algorithms, there\nis still scope to use the Spark setup efficiently for highly time-intensive and\ncomputationally expensive procedures like deep learning. In this paper, we\npropose a novel framework that combines the distributive computational\nabilities of Apache Spark and the advanced machine learning architecture of a\ndeep multi-layer perceptron (MLP), using the popular concept of Cascade\nLearning. We conduct empirical analysis of our framework on two real world\ndatasets. The results are encouraging and corroborate our proposed framework,\nin turn proving that it is an improvement over traditional big data analysis\nmethods that use either Spark or Deep learning as individual elements. \n\n"}
{"id": "1711.09628", "contents": "Title: Order-Sensitivity and Equivariance of Scoring Functions Abstract: The relative performance of competing point forecasts is usually measured in\nterms of loss or scoring functions. It is widely accepted that these scoring\nfunction should be strictly consistent in the sense that the expected score is\nminimized by the correctly specified forecast for a certain statistical\nfunctional such as the mean, median, or a certain risk measure. Thus, strict\nconsistency opens the way to meaningful forecast comparison, but is also\nimportant in regression and M-estimation. Usually strictly consistent scoring\nfunctions for an elicitable functional are not unique. To give guidance on the\nchoice of a scoring function, this paper introduces two additional quality\ncriteria. Order-sensitivity opens the possibility to compare two deliberately\nmisspecified forecasts given that the forecasts are ordered in a certain sense.\nOn the other hand, equivariant scoring functions obey similar equivariance\nproperties as the functional at hand - such as translation invariance or\npositive homogeneity. In our study, we consider scoring functions for popular\nfunctionals, putting special emphasis on vector-valued functionals, e.g. the\npair (mean, variance) or (Value at Risk, Expected Shortfall). \n\n"}
{"id": "1712.01793", "contents": "Title: Posterior Integration on a Riemannian Manifold Abstract: The geodesic Markov chain Monte Carlo method and its variants enable\ncomputation of integrals with respect to a posterior supported on a manifold.\nHowever, for regular integrals, the convergence rate of the ergodic average\nwill be sub-optimal. To fill this gap, this paper extends the efficient\nposterior integration method of Oates et al. (2017) to the case of a Riemannian\nmanifold. In contrast to the original Euclidean case, no non-trivial boundary\nconditions are needed for a closed manifold. The method is assessed through\nsimulation and deployed to compute posterior integrals for an Australian\nMesozoic paleomagnetic pole model, whose parameters are constrained to lie on\nthe manifold $M = \\mathbb{S}^2 \\times \\mathbb{R}_+$. \n\n"}
{"id": "1712.01934", "contents": "Title: Concentration of weakly dependent Banach-valued sums and applications to\n  statistical learning methods Abstract: We obtain a Bernstein-type inequality for sums of Banach-valued random\nvariables satisfying a weak dependence assumption of general type and under\ncertain smoothness assumptions of the underlying Banach norm. We use this\ninequality in order to investigate in the asymptotical regime the error upper\nbounds for the broad family of spectral regularization methods for reproducing\nkernel decision rules, when trained on a sample coming from a $\\tau-$mixing\nprocess. \n\n"}
{"id": "1712.02488", "contents": "Title: Cost-sensitive detection with variational autoencoders for environmental\n  acoustic sensing Abstract: Environmental acoustic sensing involves the retrieval and processing of audio\nsignals to better understand our surroundings. While large-scale acoustic data\nmake manual analysis infeasible, they provide a suitable playground for machine\nlearning approaches. Most existing machine learning techniques developed for\nenvironmental acoustic sensing do not provide flexible control of the trade-off\nbetween the false positive rate and the false negative rate. This paper\npresents a cost-sensitive classification paradigm, in which the\nhyper-parameters of classifiers and the structure of variational autoencoders\nare selected in a principled Neyman-Pearson framework. We examine the\nperformance of the proposed approach using a dataset from the HumBug project\nwhich aims to detect the presence of mosquitoes using sound collected by simple\nembedded devices. \n\n"}
{"id": "1712.05630", "contents": "Title: Sparse principal component analysis via axis-aligned random projections Abstract: We introduce a new method for sparse principal component analysis, based on\nthe aggregation of eigenvector information from carefully-selected axis-aligned\nrandom projections of the sample covariance matrix. Unlike most alternative\napproaches, our algorithm is non-iterative, so is not vulnerable to a bad\nchoice of initialisation. We provide theoretical guarantees under which our\nprincipal subspace estimator can attain the minimax optimal rate of convergence\nin polynomial time. In addition, our theory provides a more refined\nunderstanding of the statistical and computational trade-off in the problem of\nsparse principal component estimation, revealing a subtle interplay between the\neffective sample size and the number of random projections that are required to\nachieve the minimax optimal rate. Numerical studies provide further insight\ninto the procedure and confirm its highly competitive finite-sample\nperformance. \n\n"}
{"id": "1712.08773", "contents": "Title: An Approximate Bayesian Long Short-Term Memory Algorithm for Outlier\n  Detection Abstract: Long Short-Term Memory networks trained with gradient descent and\nback-propagation have received great success in various applications. However,\npoint estimation of the weights of the networks is prone to over-fitting\nproblems and lacks important uncertainty information associated with the\nestimation. However, exact Bayesian neural network methods are intractable and\nnon-applicable for real-world applications. In this study, we propose an\napproximate estimation of the weights uncertainty using Ensemble Kalman Filter,\nwhich is easily scalable to a large number of weights. Furthermore, we optimize\nthe covariance of the noise distribution in the ensemble update step using\nmaximum likelihood estimation. To assess the proposed algorithm, we apply it to\noutlier detection in five real-world events retrieved from the Twitter\nplatform. \n\n"}
{"id": "1712.09117", "contents": "Title: Overcomplete Frame Thresholding for Acoustic Scene Analysis Abstract: In this work, we derive a generic overcomplete frame thresholding scheme\nbased on risk minimization. Overcomplete frames being favored for analysis\ntasks such as classification, regression or anomaly detection, we provide a way\nto leverage those optimal representations in real-world applications through\nthe use of thresholding. We validate the method on a large scale bird activity\ndetection task via the scattering network architecture performed by means of\ncontinuous wavelets, known for being an adequate dictionary in audio\nenvironments. \n\n"}
{"id": "1712.09912", "contents": "Title: Optimal Covariance Change Point Localization in High Dimension Abstract: We study the problem of change point detection for covariance matrices in\nhigh dimensions. We assume that we observe a sequence {X_i}_{i=1,...,n} of\nindependent and centered p-dimensional sub-Gaussian random vectors whose\ncovariance matrices are piecewise constant. Our task is to recover with high\naccuracy the number and locations of the change points, which are assumed\nunknown. Our generic model setting allows for all the model parameters to\nchange with n, including the dimension p, the minimal spacing between\nconsecutive change points, the magnitude of smallest change size and the\nmaximal Orlicz- 2 norm of the covariance matrices of the sample points. Without\nassuming any additional structural assumption, such as low rank matrices or\nhaving sparse principle components, we set up a general framework and a\nbenchmark result for the covariance change point detection problem. We\nintroduce two procedures, one based on the binary segmentation algorithm (e.g.\nVostrikova, 1981) and the other on its extension known as wild binary\nsegmentation of Fryzlewicz (2014), and demonstrate that, under suitable\nconditions, both procedures are able to consistently es- timate the number and\nlocations of change points. Our second algorithm, called Wild Binary\nSegmentation through Independent Projection (WBSIP), is shown to be optimal in\nthe sense of allowing for the minimax scaling in all the relevant parameters.\nOur minimax analysis reveals a phase transition effect based on the problem of\nchange point localization. To the best of our knowledge, this type of results\nhas not been established elsewhere in the high-dimensional change point\ndetection literature. \n\n"}
{"id": "1712.09941", "contents": "Title: Sorted Concave Penalized Regression Abstract: The Lasso is biased. Concave penalized least squares estimation (PLSE) takes\nadvantage of signal strength to reduce this bias, leading to sharper error\nbounds in prediction, coefficient estimation and variable selection. For\nprediction and estimation, the bias of the Lasso can be also reduced by taking\na smaller penalty level than what selection consistency requires, but such\nsmaller penalty level depends on the sparsity of the true coefficient vector.\nThe sorted L1 penalized estimation (Slope) was proposed for adaptation to such\nsmaller penalty levels. However, the advantages of concave PLSE and Slope do\nnot subsume each other. We propose sorted concave penalized estimation to\ncombine the advantages of concave and sorted penalizations. We prove that\nsorted concave penalties adaptively choose the smaller penalty level and at the\nsame time benefits from signal strength, especially when a significant\nproportion of signals are stronger than the corresponding adaptively selected\npenalty levels. A local convex approximation, which extends the local linear\nand quadratic approximations to sorted concave penalties, is developed to\nfacilitate the computation of sorted concave PLSE and proven to possess desired\nprediction and estimation error bounds. We carry out a unified treatment of\npenalty functions in a general optimization setting, including the penalty\nlevels and concavity of the above mentioned sorted penalties and mixed\npenalties motivated by Bayesian considerations. Our analysis of prediction and\nestimation errors requires the restricted eigenvalue condition on the design,\nnot beyond, and provides selection consistency under a required minimum signal\nstrength condition in addition. Thus, our results also sharpens existing\nresults on concave PLSE by removing the upper sparse eigenvalue component of\nthe sparse Riesz condition. \n\n"}
{"id": "1801.00038", "contents": "Title: Identifiability of two-component skew normal mixtures with one known\n  component Abstract: We give sufficient identifiability conditions for estimating mixing\nproportions in two-component mixtures of skew normal distributions with one\nknown component. We consider the univariate case as well as two multivariate\nextensions: a multivariate skew normal distribution (MSN) by Azzalini and Dalla\nValle (1996) and the canonical fundamental skew normal distribution (CFUSN) by\nArellano-Valle and Genton (2005). The characteristic function of the CFUSN\ndistribution is additionally derived. \n\n"}
{"id": "1801.01007", "contents": "Title: A Comprehensive Bayesian Treatment of the Universal Kriging model with\n  Mat\\'ern correlation kernels Abstract: The Gibbs reference posterior distribution provides an objective\nfull-Bayesian solution to the problem of prediction of a stationary Gaussian\nprocess with Mat\\'ern anisotropic kernel. A full-Bayesian approach is possible,\nbecause the posterior distribution is expressed as the invariant distribution\nof a uniformly ergodic Markovian kernel for which we give an explicit\nexpression. In this paper, we show that it is appropriate for the Universal\nKriging framework, that is when an unknown function is added to the stationary\nGaussian process. We give sufficient conditions for the existence and propriety\nof the Gibbs reference posterior that apply to a wide variety of practical\ncases and illustrate the method with several examples. Finally, simulations of\nGaussian processes suggest that the Gibbs reference posterior has good\nfrequentist properties in terms of coverage of prediction intervals. \n\n"}
{"id": "1801.01170", "contents": "Title: Optimization-based AMP for Phase Retrieval: The Impact of Initialization\n  and $\\ell_2$-regularization Abstract: We consider an $\\ell_2$-regularized non-convex optimization problem for\nrecovering signals from their noisy phaseless observations. We design and study\nthe performance of a message passing algorithm that aims to solve this\noptimization problem. We consider the asymptotic setting $m,n \\rightarrow\n\\infty$, $m/n \\rightarrow \\delta$ and obtain sharp performance bounds, where\n$m$ is the number of measurements and $n$ is the signal dimension. We show that\nfor complex signals the algorithm can perform accurate recovery with only $m=\n\\left(\\frac{64}{\\pi^2}-4\\right)n \\approx 2.5n$ measurements. Also, we provide\nsharp analysis on the sensitivity of the algorithm to noise. We highlight the\nfollowing facts about our message passing algorithm: (i) Adding $\\ell_2$\nregularization to the non-convex loss function can be beneficial. (ii) Spectral\ninitialization has marginal impact on the performance of the algorithm. The\nsharp analyses in this paper, not only enable us to compare the performance of\nour method with other phase recovery schemes, but also shed light on designing\nbetter iterative algorithms for other non-convex optimization problems. \n\n"}
{"id": "1801.01394", "contents": "Title: Prediction Error Bounds for Linear Regression With the TREX Abstract: The TREX is a recently introduced approach to sparse linear regression. In\ncontrast to most well-known approaches to penalized regression, the TREX can be\nformulated without the use of tuning parameters. In this paper, we establish\nthe first known prediction error bounds for the TREX. Additionally, we\nintroduce extensions of the TREX to a more general class of penalties, and we\nprovide a bound on the prediction error in this generalized setting. These\nresults deepen the understanding of TREX from a theoretical perspective and\nprovide new insights into penalized regression in general. \n\n"}
{"id": "1801.03184", "contents": "Title: Known Boundary Emulation of Complex Computer Models Abstract: Computer models are now widely used across a range of scientific disciplines\nto describe various complex physical systems, however to perform full\nuncertainty quantification we often need to employ emulators. An emulator is a\nfast statistical construct that mimics the complex computer model, and greatly\naids the vastly more computationally intensive uncertainty quantification\ncalculations that a serious scientific analysis often requires. In some cases,\nthe complex model can be solved far more efficiently for certain parameter\nsettings, leading to boundaries or hyperplanes in the input parameter space\nwhere the model is essentially known. We show that for a large class of\nGaussian process style emulators, multiple boundaries can be formally\nincorporated into the emulation process, by Bayesian updating of the emulators\nwith respect to the boundaries, for trivial computational cost. The resulting\nupdated emulator equations are given analytically. This leads to emulators that\npossess increased accuracy across large portions of the input parameter space.\nWe also describe how a user can incorporate such boundaries within standard\nblack box GP emulation packages that are currently available, without altering\nthe core code. Appropriate designs of model runs in the presence of known\nboundaries are then analysed, with two kinds of general purpose designs\nproposed. We then apply the improved emulation and design methodology to an\nimportant systems biology model of hormonal crosstalk in Arabidopsis Thaliana. \n\n"}
{"id": "1801.03592", "contents": "Title: Estimation of the Robin coefficient field in a Poisson problem with\n  uncertain conductivity field Abstract: We consider the reconstruction of a heterogeneous coefficient field in a\nRobin boundary condition on an inaccessible part of the boundary in a Poisson\nproblem with an uncertain (or unknown) inhomogeneous conductivity field in the\ninterior of the domain. To account for model errors that stem from the\nuncertainty in the conductivity coefficient, we treat the unknown conductivity\nas a nuisance parameter and carry out approximative premarginalization over it,\nand invert for the Robin coefficient field only. We approximate the related\nmodelling errors via the Bayesian approximation error (BAE) approach. The\nuncertainty analysis presented here relies on a local linearization of the\nparameter-to-observable map at the maximum a posteriori (MAP) estimates, which\nleads to a normal (Gaussian) approximation of the parameter posterior density.\nTo compute the MAP point we apply an inexact Newton conjugate gradient approach\nbased on the adjoint methodology. The construction of the covariance is made\ntractable by invoking a low-rank approximation of the data misfit component of\nthe Hessian. Two numerical experiments are considered: one where the prior\ncovariance on the conductivity is isotropic, and one where the prior covariance\non the conductivity is anisotropic. Results are compared to those based on\nstandard error models, with particular emphasis on the feasibility of the\nposterior uncertainty estimates. We show that the BAE approach is a feasible\none in the sense that the predicted posterior uncertainty is consistent with\nthe actual estimation errors, while neglecting the related modelling error\nyields infeasible estimates for the Robin coefficient. In addition, we\ndemonstrate that the BAE approach is approximately as computationally expensive\n(measured in the number of PDE solves) as the conventional error approach. \n\n"}
{"id": "1801.04063", "contents": "Title: How Many Samples Required in Big Data Collection: A Differential Message\n  Importance Measure Abstract: Information collection is a fundamental problem in big data, where the size\nof sampling sets plays a very important role. This work considers the\ninformation collection process by taking message importance into account.\nSimilar to differential entropy, we define differential message importance\nmeasure (DMIM) as a measure of message importance for continuous random\nvariable. It is proved that the change of DMIM can describe the gap between the\ndistribution of a set of sample values and a theoretical distribution. In fact,\nthe deviation of DMIM is equivalent to Kolmogorov-Smirnov statistic, but it\noffers a new way to characterize the distribution goodness-of-fit. Numerical\nresults show some basic properties of DMIM and the accuracy of the proposed\napproximate values. Furthermore, it is also obtained that the empirical\ndistribution approaches the real distribution with decreasing of the DMIM\ndeviation, which contributes to the selection of suitable sampling points in\nactual system. \n\n"}
{"id": "1801.05466", "contents": "Title: Testing Separability of Functional Time Series Abstract: We derive and study a significance test for determining if a panel of\nfunctional time series is separable. In the context of this paper, separability\nmeans that the covariance structure factors into the product of two functions,\none depending only on time and the other depending only on the coordinates of\nthe panel. Separability is a property which can dramatically improve\ncomputational efficiency by substantially reducing model complexity. It is\nespecially useful for functional data as it implies that the functional\nprincipal components are the same for each member of the panel. However such an\nassumption must be verified before proceeding with further inference. Our\napproach is based on functional norm differences and provides a test with well\ncontrolled size and high power. We establish our procedure quite generally,\nallowing one to test separability of autocovariances as well. In addition to an\nasymptotic justification, our methodology is validated by a simulation study.\nIt is applied to functional panels of particulate pollution and stock market\ndata. \n\n"}
{"id": "1801.05695", "contents": "Title: Panel data analysis via mechanistic models Abstract: Panel data, also known as longitudinal data, consist of a collection of time\nseries. Each time series, which could itself be multivariate, comprises a\nsequence of measurements taken on a distinct unit. Mechanistic modeling\ninvolves writing down scientifically motivated equations describing the\ncollection of dynamic systems giving rise to the observations on each unit. A\ndefining characteristic of panel systems is that the dynamic interaction\nbetween units should be negligible. Panel models therefore consist of a\ncollection of independent stochastic processes, generally linked through shared\nparameters while also having unit-specific parameters. To give the scientist\nflexibility in model specification, we are motivated to develop a framework for\ninference on panel data permitting the consideration of arbitrary nonlinear,\npartially observed panel models. We build on iterated filtering techniques that\nprovide likelihood-based inference on nonlinear partially observed Markov\nprocess models for time series data. Our methodology depends on the latent\nMarkov process only through simulation; this plug-and-play property ensures\napplicability to a large class of models. We demonstrate our methodology on a\ntoy example and two epidemiological case studies. We address inferential and\ncomputational issues arising due to the combination of model complexity and\ndataset size. \n\n"}
{"id": "1801.08120", "contents": "Title: Optimal Estimation of Simultaneous Signals Using Absolute Inner Product\n  with Applications to Integrative Genomics Abstract: Integrating the summary statistics from genome-wide association study\n(\\textsc{gwas}) and expression quantitative trait loci (e\\textsc{qtl}) data\nprovides a powerful way of identifying the genes whose expression levels are\npotentially associated with complex diseases. A parameter called $T$-score that\nquantifies the genetic overlap between a gene and the disease phenotype based\non the summary statistics is introduced based on the mean values of two\nGaussian sequences. Specifically, given two independent samples\n$\\mathbf{x}_n\\sim N(\\theta, \\Sigma_1)$ and $\\mathbf{y}_n\\sim N(\\mu, \\Sigma_2)$,\nthe $T$-score is defined as $\\sum_{i=1}^n |\\theta_i\\mu_i|$, a non-smooth\nfunctional, which characterizes the amount of shared signals between two\nabsolute normal mean vectors $|\\theta|$ and $|\\mu|$. Using approximation\ntheory, estimators are constructed and shown to be minimax rate-optimal and\nadaptive over various parameter spaces. Simulation studies demonstrate the\nsuperiority of the proposed estimators over existing methods. The method is\napplied to an integrative analysis of heart failure genomics datasets and we\nidentify several genes and biological pathways that are potentially causal to\nhuman heart failure. \n\n"}
{"id": "1801.08724", "contents": "Title: Concentration of random graphs and application to community detection Abstract: Random matrix theory has played an important role in recent work on\nstatistical network analysis. In this paper, we review recent results on\nregimes of concentration of random graphs around their expectation, showing\nthat dense graphs concentrate and sparse graphs concentrate after\nregularization. We also review relevant network models that may be of interest\nto probabilists considering directions for new random matrix theory\ndevelopments, and random matrix theory tools that may be of interest to\nstatisticians looking to prove properties of network algorithms. Applications\nof concentration results to the problem of community detection in networks are\ndiscussed in detail. \n\n"}
{"id": "1801.10378", "contents": "Title: Data driven time scale in Gaussian quasi-likelihood inference Abstract: We study parametric estimation of ergodic diffusions observed at high\nfrequency. Different from the previous studies, we suppose that sampling\nstepsize is unknown, thereby making the conventional Gaussian quasi-likelihood\nnot directly applicable. In this situation, we construct estimators of both\nmodel parameters and sampling stepsize in a fully explicit way, and prove that\nthey are jointly asymptotically normally distributed. The $L^{q}$-boundedness\nof the obtained estimator is also derived. Further, we propose the Schwarz\n(BIC) type statistics for model selection and show its model-selection\nconsistency. We conducted some numerical experiments and found that the\nobserved finite-sample performance well supports our theoretical findings. Also\nprovided is a real data example. \n\n"}
{"id": "1801.10478", "contents": "Title: Change Point Analysis of Correlation in Non-stationary Time Series Abstract: A restrictive assumption in change point analysis is \"stationarity under the\nnull hypothesis of no change-point\", which is crucial for asymptotic theory but\nnot very realistic from a practical point of view. For example, if change point\nanalysis for correlations is performed, it is not necessarily clear that the\nmean, marginal variance or higher order moments are constant, even if there is\nno change in the correlation. This paper develops change point analysis for the\ncorrelation structures under less restrictive assumptions. In contrast to\nprevious work, our approach does not require that the mean, variance and fourth\norder joint cumulants are constant under the null hypothesis. Moreover, we also\naddress the problem of detecting relevant change points. \n\n"}
{"id": "1801.10567", "contents": "Title: De-biased sparse PCA: Inference and testing for eigenstructure of large\n  covariance matrices Abstract: Sparse principal component analysis (sPCA) has become one of the most widely\nused techniques for dimensionality reduction in high-dimensional datasets. The\nmain challenge underlying sPCA is to estimate the first vector of loadings of\nthe population covariance matrix, provided that only a certain number of\nloadings are non-zero. In this paper, we propose confidence intervals for\nindividual loadings and for the largest eigenvalue of the population covariance\nmatrix. Given an independent sample $X^i \\in\\mathbb R^p, i = 1,...,n,$\ngenerated from an unknown distribution with an unknown covariance matrix\n$\\Sigma_0$, our aim is to estimate the first vector of loadings and the largest\neigenvalue of $\\Sigma_0$ in a setting where $p\\gg n$. Next to the\nhigh-dimensionality, another challenge lies in the inherent non-convexity of\nthe problem. We base our methodology on a Lasso-penalized M-estimator which,\ndespite non-convexity, may be solved by a polynomial-time algorithm such as\ncoordinate or gradient descent. We show that our estimator achieves the minimax\noptimal rates in $\\ell_1$ and $\\ell_2$-norm. We identify the bias in the\nLasso-based estimator and propose a de-biased sparse PCA estimator for the\nvector of loadings and for the largest eigenvalue of the covariance matrix\n$\\Sigma_0$. Our main results provide theoretical guarantees for asymptotic\nnormality of the de-biased estimator. The major conditions we impose are\nsparsity in the first eigenvector of small order $\\sqrt{n}/\\log p$ and sparsity\nof the same order in the columns of the inverse Hessian matrix of the\npopulation risk. \n\n"}
{"id": "1802.00381", "contents": "Title: Signal-plus-noise matrix models: eigenvector deviations and fluctuations Abstract: Estimating eigenvectors and low-dimensional subspaces is of central\nimportance for numerous problems in statistics, computer science, and applied\nmathematics. This paper characterizes the behavior of perturbed eigenvectors\nfor a range of signal-plus-noise matrix models encountered in both statistical\nand random matrix theoretic settings. We prove both first-order approximation\nresults (i.e. sharp deviations) as well as second-order distributional limit\ntheory (i.e. fluctuations). The concise methodology considered in this paper\nsynthesizes tools rooted in two core concepts, namely (i) deterministic\ndecompositions of matrix perturbations and (ii) probabilistic matrix\nconcentration phenomena. We illustrate our theoretical results via simulation\nexamples involving stochastic block model random graphs. \n\n"}
{"id": "1802.00474", "contents": "Title: Bayesian Modeling via Goodness-of-fit Abstract: The two key issues of modern Bayesian statistics are: (i) establishing\nprincipled approach for distilling statistical prior that is consistent with\nthe given data from an initial believable scientific prior; and (ii)\ndevelopment of a Bayes-frequentist consolidated data analysis workflow that is\nmore effective than either of the two separately. In this paper, we propose the\nidea of \"Bayes via goodness of fit\" as a framework for exploring these\nfundamental questions, in a way that is general enough to embrace almost all of\nthe familiar probability models. Several illustrative examples show the benefit\nof this new point of view as a practical data analysis tool. Relationship with\nother Bayesian cultures is also discussed. \n\n"}
{"id": "1802.00495", "contents": "Title: Practical Bayesian Modeling and Inference for Massive Spatial Datasets\n  On Modest Computing Environments Abstract: With continued advances in Geographic Information Systems and related\ncomputational technologies, statisticians are often required to analyze very\nlarge spatial datasets. This has generated substantial interest over the last\ndecade, already too vast to be summarized here, in scalable methodologies for\nanalyzing large spatial datasets. Scalable spatial process models have been\nfound especially attractive due to their richness and flexibility and,\nparticularly so in the Bayesian paradigm, due to their presence in hierarchical\nmodel settings. However, the vast majority of research articles present in this\ndomain have been geared toward innovative theory or more complex model\ndevelopment. Very limited attention has been accorded to approaches for easily\nimplementable scalable hierarchical models for the practicing scientist or\nspatial analyst. This article is submitted to the Practice section of the\njournal with the aim of developing massively scalable Bayesian approaches that\ncan rapidly deliver Bayesian inference on spatial process that are practically\nindistinguishable from inference obtained using more expensive alternatives. A\nkey emphasis is on implementation within very standard (modest) computing\nenvironments (e.g., a standard desktop or laptop) using easily available\nstatistical software packages without requiring message-parsing interfaces or\nparallel programming paradigms. Key insights are offered regarding assumptions\nand approximations concerning practical efficiency. \n\n"}
{"id": "1802.04472", "contents": "Title: Community Detection through Likelihood Optimization: In Search of a\n  Sound Model Abstract: Community detection is one of the most important problems in network\nanalysis. Among many algorithms proposed for this task, methods based on\nstatistical inference are of particular interest: they are mathematically sound\nand were shown to provide partitions of good quality. Statistical inference\nmethods are based on fitting some random graph model (a.k.a. null model) to the\nobserved network by maximizing the likelihood. The choice of this model is\nextremely important and is the main focus of the current study. We provide an\nextensive theoretical and empirical analysis to compare several models: the\nwidely used planted partition model, recently proposed degree-corrected\nmodification of this model, and a new null model having some desirable\nstatistical properties. We also develop and compare two likelihood optimization\nalgorithms suitable for the models under consideration. An extensive empirical\nanalysis on a variety of datasets shows, in particular, that the new model is\nthe best one for describing most of the considered real-world complex networks\naccording to the likelihood of observed graph structures. \n\n"}
{"id": "1802.04838", "contents": "Title: Network Estimation from Point Process Data Abstract: Consider observing a collection of discrete events within a network that\nreflect how network nodes influence one another. Such data are common in spike\ntrains recorded from biological neural networks, interactions within a social\nnetwork, and a variety of other settings. Data of this form may be modeled as\nself-exciting point processes, in which the likelihood of future events depends\non the past events. This paper addresses the problem of estimating\nself-excitation parameters and inferring the underlying functional network\nstructure from self-exciting point process data. Past work in this area was\nlimited by strong assumptions which are addressed by the novel approach here.\nSpecifically, in this paper we (1) incorporate saturation in a point process\nmodel which both ensures stability and models non-linear thresholding effects;\n(2) impose general low-dimensional structural assumptions that include\nsparsity, group sparsity and low-rankness that allows bounds to be developed in\nthe high-dimensional setting; and (3) incorporate long-range memory effects\nthrough moving average and higher-order auto-regressive components. Using our\ngeneral framework, we provide a number of novel theoretical guarantees for\nhigh-dimensional self-exciting point processes that reflect the role played by\nthe underlying network structure and long-term memory. We also provide\nsimulations and real data examples to support our methodology and main results. \n\n"}
{"id": "1802.05821", "contents": "Title: Learning Latent Features with Pairwise Penalties in Low-Rank Matrix\n  Completion Abstract: Low-rank matrix completion has achieved great success in many real-world data\napplications. A matrix factorization model that learns latent features is\nusually employed and, to improve prediction performance, the similarities\nbetween latent variables can be exploited by pairwise learning using the graph\nregularized matrix factorization (GRMF) method. However, existing GRMF\napproaches often use the squared loss to measure the pairwise differences,\nwhich may be overly influenced by dissimilar pairs and lead to inferior\nprediction. To fully empower pairwise learning for matrix completion, we\npropose a general optimization framework that allows a rich class of\n(non-)convex pairwise penalty functions. A new and efficient algorithm is\ndeveloped to solve the proposed optimization problem, with a theoretical\nconvergence guarantee under mild assumptions. In an important situation where\nthe latent variables form a small number of subgroups, its statistical\nguarantee is also fully considered. In particular, we theoretically\ncharacterize the performance of the complexity-regularized maximum likelihood\nestimator, as a special case of our framework, which is shown to have smaller\nerrors when compared to the standard matrix completion framework without\npairwise penalties. We conduct extensive experiments on both synthetic and real\ndatasets to demonstrate the superior performance of this general framework. \n\n"}
{"id": "1802.05917", "contents": "Title: Robust estimation in controlled branching processes: Bayesian estimators\n  via disparities Abstract: This paper is concerned with Bayesian inferential methods for data from\ncontrolled branching processes that account for model robustness through the\nuse of disparities. Under regularity conditions, we establish that estimators\nbuilt on disparity-based posterior, such as expectation and maximum a\nposteriori estimates, are consistent and efficient under the posited model.\nAdditionally, we show that the estimates are robust to model misspecification\nand presence of aberrant outliers. To this end, we develop several fundamental\nideas relating minimum disparity estimators to Bayesian estimators built on the\ndisparity-based posterior, for dependent tree-structured data. We illustrate\nthe methodology through a simulated example and apply our methods to a real\ndata set from cell kinetics. \n\n"}
{"id": "1802.08513", "contents": "Title: Fast and Sample Near-Optimal Algorithms for Learning Multidimensional\n  Histograms Abstract: We study the problem of robustly learning multi-dimensional histograms. A\n$d$-dimensional function $h: D \\rightarrow \\mathbb{R}$ is called a\n$k$-histogram if there exists a partition of the domain $D \\subseteq\n\\mathbb{R}^d$ into $k$ axis-aligned rectangles such that $h$ is constant within\neach such rectangle. Let $f: D \\rightarrow \\mathbb{R}$ be a $d$-dimensional\nprobability density function and suppose that $f$ is $\\mathrm{OPT}$-close, in\n$L_1$-distance, to an unknown $k$-histogram (with unknown partition). Our goal\nis to output a hypothesis that is $O(\\mathrm{OPT}) + \\epsilon$ close to $f$, in\n$L_1$-distance. We give an algorithm for this learning problem that uses $n =\n\\tilde{O}_d(k/\\epsilon^2)$ samples and runs in time $\\tilde{O}_d(n)$. For any\nfixed dimension, our algorithm has optimal sample complexity, up to logarithmic\nfactors, and runs in near-linear time. Prior to our work, the time complexity\nof the $d=1$ case was well-understood, but significant gaps in our\nunderstanding remained even for $d=2$. \n\n"}
{"id": "1802.08658", "contents": "Title: On detecting changes in the jumps of arbitrary size of a time-continuous\n  stochastic process Abstract: This paper introduces test and estimation procedures for abrupt and gradual\nchanges in the entire jump behaviour of a discretely observed Ito\nsemimartingale. In contrast to existing work we analyse jumps of arbitrary size\nwhich are not restricted to a minimum height. Our methods are based on weak\nconvergence of a truncated sequential empirical distribution function of the\njump characteristic of the underlying Ito semimartingale. Critical values for\nthe new tests are obtained by a multiplier bootstrap approach and we\ninvestigate the performance of the tests also under local alternatives. An\nextensive simulation study shows the finite-sample properties of the new\nprocedures. \n\n"}
{"id": "1802.08946", "contents": "Title: Teacher Improves Learning by Selecting a Training Subset Abstract: We call a learner super-teachable if a teacher can trim down an iid training\nset while making the learner learn even better. We provide sharp super-teaching\nguarantees on two learners: the maximum likelihood estimator for the mean of a\nGaussian, and the large margin classifier in 1D. For general learners, we\nprovide a mixed-integer nonlinear programming-based algorithm to find a super\nteaching set. Empirical experiments show that our algorithm is able to find\ngood super-teaching sets for both regression and classification problems. \n\n"}
{"id": "1802.09411", "contents": "Title: Principles of Bayesian Inference using General Divergence Criteria Abstract: When it is acknowledged that all candidate parameterised statistical models\nare misspecified relative to the data generating process, the decision maker\n(DM) must currently concern themselves with inference for the parameter value\nminimising the KL-divergence between the model and the process (Walker, 2013).\nHowever, it has long been known that minimising the KL-divergence places a\nlarge weight on correctly capturing the tails of the sample distribution. As a\nresult the DM is required to worry about the robustness of their model to tail\nmisspecifications if they want to conduct principled inference. In this paper\nwe alleviate these concerns for the DM. We advance recent methodological\ndevelopments in general Bayesian updating (Bissiri, Holmes and Walker, 2016) to\npropose a statistically well principled Bayesian updating of beliefs targeting\nthe minimisation of more general divergence criteria. We improve both the\nmotivation and the statistical foundations of existing Bayesian minimum\ndivergence estimation (Hooker and Vidyashankar, 2014; Ghosh and Basu, 2016),\nallowing the well principled Bayesian to target predictions from the model that\nare close to the genuine model in terms of some alternative divergence measure\nto the KL-divergence. Our principled formulation allows us to consider a\nbroader range of divergences than have previously been considered. In fact we\nargue defining the divergence measure forms an important, subjective part of\nany statistical analysis, and aim to provide some decision theoretic rational\nfor this selection. We illustrate how targeting alternative divergence measures\ncan impact the conclusions of simple inference tasks, and discuss then how our\nmethods might apply to more complicated, high dimensional models. \n\n"}
{"id": "1803.02525", "contents": "Title: Fast Robust Methods for Singular State-Space Models Abstract: State-space models are used in a wide range of time series analysis\nformulations. Kalman filtering and smoothing are work-horse algorithms in these\nsettings. While classic algorithms assume Gaussian errors to simplify\nestimation, recent advances use a broader range of optimization formulations to\nallow outlier-robust estimation, as well as constraints to capture prior\ninformation.\n  Here we develop methods on state-space models where either innovations or\nerror covariances may be singular. These models frequently arise in navigation\n(e.g. for `colored noise' models or deterministic integrals) and are ubiquitous\nin auto-correlated time series models such as ARMA. We reformulate all\nstate-space models (singular as well as nonsinguar) as constrained convex\noptimization problems, and develop an efficient algorithm for this\nreformulation. The convergence rate is {\\it locally linear}, with constants\nthat do not depend on the conditioning of the problem.\n  Numerical comparisons show that the new approach outperforms competing\napproaches for {\\it nonsingular} models, including state of the art interior\npoint (IP) methods. IP methods converge at superlinear rates; we expect them to\ndominate. However, the steep rate of the proposed approach (independent of\nproblem conditioning) combined with cheap iterations wins against IP in a\nrun-time comparison. We therefore suggest that the proposed approach be the\n{\\it default choice} for estimating state space models outside of the Gaussian\ncontext, regardless of whether the error covariances are singular or not. \n\n"}
{"id": "1803.02734", "contents": "Title: Sklar's Omega: A Gaussian Copula-Based Framework for Assessing Agreement Abstract: The statistical measurement of agreement is important in a number of fields,\ne.g., content analysis, education, computational linguistics, biomedical\nimaging. We propose Sklar's Omega, a Gaussian copula-based framework for\nmeasuring intra-coder, inter-coder, and inter-method agreement as well as\nagreement relative to a gold standard. We demonstrate the efficacy and\nadvantages of our approach by applying it to both simulated and experimentally\nobserved datasets, including data from two medical imaging studies. Application\nof our proposed methodology is supported by our open-source R package,\nsklarsomega, which is available for download from the Comprehensive R Archive\nNetwork. \n\n"}
{"id": "1803.03677", "contents": "Title: Nonparametric Risk Assessment and Density Estimation for Persistence\n  Landscapes Abstract: This paper presents approximate confidence intervals for each function of\nparameters in a Banach space based on a bootstrap algorithm. We apply kernel\ndensity approach to estimate the persistence landscape. In addition, we\nevaluate the quality distribution function estimator of random variables using\nintegrated mean square error (IMSE). The results of simulation studies show a\nsignificant improvement achieved by our approach compared to the standard\nversion of confidence intervals algorithm. In the next step, we provide several\nalgorithms to solve our model. Finally, real data analysis shows that the\naccuracy of our method compared to that of previous works for computing the\nconfidence interval. \n\n"}
{"id": "1803.04859", "contents": "Title: On moments of integral exponential functionals of additive processes Abstract: For real-valued additive process $(X\\_t)\\_{t\\geq 0}$ a recursive equation is\nderived for the entire positive moments of functionals $$I\\_{s,t}= \\int\n\\_s^t\\exp(-X\\_u)du, \\quad 0\\leq s<t\\leq\\infty, $$ in case the Laplace exponent\nof $X\\_t$ exists for positive values of the parameter. From the equation\nemergesan easy-to-apply sufficient condition for the finiteness of the moments.\nAs an application we study first hitprocesses of diffusions. \n\n"}
{"id": "1803.08586", "contents": "Title: Optimization of Smooth Functions with Noisy Observations: Local Minimax\n  Rates Abstract: We consider the problem of global optimization of an unknown non-convex\nsmooth function with zeroth-order feedback. In this setup, an algorithm is\nallowed to adaptively query the underlying function at different locations and\nreceives noisy evaluations of function values at the queried points (i.e. the\nalgorithm has access to zeroth-order information). Optimization performance is\nevaluated by the expected difference of function values at the estimated\noptimum and the true optimum. In contrast to the classical optimization setup,\nfirst-order information like gradients are not directly accessible to the\noptimization algorithm. We show that the classical minimax framework of\nanalysis, which roughly characterizes the worst-case query complexity of an\noptimization algorithm in this setting, leads to excessively pessimistic\nresults. We propose a local minimax framework to study the fundamental\ndifficulty of optimizing smooth functions with adaptive function evaluations,\nwhich provides a refined picture of the intrinsic difficulty of zeroth-order\noptimization. We show that for functions with fast level set growth around the\nglobal minimum, carefully designed optimization algorithms can identify a near\nglobal minimizer with many fewer queries. For the special case of strongly\nconvex and smooth functions, our implied convergence rates match the ones\ndeveloped for zeroth-order convex optimization problems. At the other end of\nthe spectrum, for worst-case smooth functions no algorithm can converge faster\nthan the minimax rate of estimating the entire unknown function in the\n$\\ell_\\infty$-norm. We provide an intuitive and efficient algorithm that\nattains the derived upper error bounds. \n\n"}
{"id": "1803.09689", "contents": "Title: Flow From Motion: A Deep Learning Approach Abstract: Wearable devices have the potential to enhance sports performance, yet they\nare not fulfilling this promise. Our previous studies with 6 professional\ntennis coaches and 20 players indicate that this could be due the lack of\npsychological or mental state feedback, which the coaches claim to provide.\nTowards this end, we propose to detect the flow state, mental state of optimal\nperformance, using wearables data to be later used in training. We performed a\nstudy with a professional tennis coach and two players. The coach provided\nlabels about the players' flow state while each player had a wearable device on\ntheir racket holding wrist. We trained multiple models using the wearables data\nand the coach labels. Our deep neural network models achieved around 98%\ntesting accuracy for a variety of conditions. This suggests that the flow state\nor what coaches recognize as flow, can be detected using wearables data in\ntennis which is a novel result. The implication for the HCI community is that\nhaving access to such information would allow for design of novel hardware and\ninteraction paradigms that would be helpful in professional athlete training. \n\n"}
{"id": "1803.09730", "contents": "Title: Resilient Active Information Gathering with Mobile Robots Abstract: Applications of safety, security, and rescue in robotics, such as multi-robot\ntarget tracking, involve the execution of information acquisition tasks by\nteams of mobile robots. However, in failure-prone or adversarial environments,\nrobots get attacked, their communication channels get jammed, and their sensors\nmay fail, resulting in the withdrawal of robots from the collective task, and\nconsequently the inability of the remaining active robots to coordinate with\neach other. As a result, traditional design paradigms become insufficient and,\nin contrast, resilient designs against system-wide failures and attacks become\nimportant. In general, resilient design problems are hard, and even though they\noften involve objective functions that are monotone or submodular, scalable\napproximation algorithms for their solution have been hitherto unknown. In this\npaper, we provide the first algorithm, enabling the following capabilities:\nminimal communication, i.e., the algorithm is executed by the robots based only\non minimal communication between them; system-wide resiliency, i.e., the\nalgorithm is valid for any number of denial-of-service attacks and failures;\nand provable approximation performance, i.e., the algorithm ensures for all\nmonotone (and not necessarily submodular) objective functions a solution that\nis finitely close to the optimal. We quantify our algorithm's approximation\nperformance using a notion of curvature for monotone set functions. We support\nour theoretical analyses with simulated and real-world experiments, by\nconsidering an active information gathering scenario, namely, multi-robot\ntarget tracking. \n\n"}
{"id": "1803.09849", "contents": "Title: Adaptive nonparametric estimation for compound Poisson processes robust\n  to the discrete-observation scheme Abstract: A compound Poisson process whose jump measure and intensity are unknown is\nobserved at finitely many equispaced times. We construct a purely data-driven\nestimator of the L\\'evy density $\\nu$ through the spectral approach using\ngeneral Calderon--Zygmund integral operators, which include convolution and\nprojection kernels. Assuming minimal tail assumptions, it is shown to estimate\n$\\nu$ at the minimax rate of estimation over Besov balls under the losses\n$L^p(\\mathbb{R})$, $p\\in[1,\\infty]$, and robustly to the observation regime\n(high- and low-frequency). To achieve adaptation in a minimax sense, we use\nLepski\\u{i}'s method as it is particularly well-suited for our generality.\nThus, novel exponential-concentration inequalities are proved including one for\nthe uniform fluctuations of the empirical characteristic function. These are of\nindependent interest, as are the proof-strategies employed to deal with general\nCalderon--Zygmund operators, to depart from the ubiquitous quadratic structure\nand to show robustness without polynomial-tail conditions. Part of the\nmotivation for such generality is a new insight we include here too that,\nfurthermore, allows us to unify the main two approaches to construct estimators\nused in related literature. \n\n"}
{"id": "1803.11039", "contents": "Title: L\\'evy Area Analysis and Parameter Estimation for fOU Processes via\n  Non-Geometric Rough Path Theory Abstract: This paper addresses the estimation problem of an unknown drift parameter\nmatrix for a fractional Ornstein-Uhlenbeck process in a multi-dimensional\nsetting. To tackle this problem, we propose a novel approach based on rough\npath theory that allows us to construct pathwise rough path estimators from\nboth continuous and discrete observations of a single path. Our approach is\nparticularly suitable for high-frequency data. To formulate the parameter\nestimators, we introduce a theory of pathwise It\\^o integrals with respect to\nfractional Brownian motion. By establishing the regularity of fractional\nOrnstein-Uhlenbeck processes and analyzing the long-term behavior of the\nassociated L\\'evy area processes, we demonstrate that our estimators are\nstrongly consistent and pathwise stable. Our findings offer a new perspective\non estimating the drift parameter matrix for fractional Ornstein-Uhlenbeck\nprocesses in multi-dimensional settings, and may have practical implications\nfor fields including finance, economics, and engineering. \n\n"}
{"id": "1804.00102", "contents": "Title: Collaborative targeted inference from continuously indexed nuisance\n  parameter estimators Abstract: We wish to infer the value of a parameter at a law from which we sample\nindependent observations. The parameter is smooth and we can define two\nvariation-independent features of the law, its $Q$- and $G$-components, such\nthat estimating them consistently at a fast enough product of rates allows to\nbuild a confidence interval (CI) with a given asymptotic level from a plain\ntargeted minimum loss estimator (TMLE). Say that the above product is not fast\nenough and the algorithm for the $G$-component is fine-tuned by a real-valued\n$h$. A plain TMLE with an $h$ chosen by cross-validation would typically not\nyield a CI. We construct a collaborative TMLE (C-TMLE) and show under mild\nconditions that, if there exists an oracle $h$ that makes a bulky remainder\nterm asymptotically Gaussian, then the C-TMLE yields a CI. We illustrate our\nfindings with the inference of the average treatment effect. We conduct a\nsimulation study where the $G$-component is estimated by the LASSO and $h$ is\nthe bound on the coefficients' norms. It sheds light on small sample\nproperties, in the face of low- to high-dimensional baseline covariates, and\npossibly positivity violation. \n\n"}
{"id": "1804.00355", "contents": "Title: Near-Optimal Recovery of Linear and N-Convex Functions on Unions of\n  Convex Sets Abstract: In this paper we build provably near-optimal, in the minimax sense, estimates\nof linear forms and, more generally, \"$N$-convex functionals\" (the simplest\nexample being the maximum of several fractional-linear functions) of unknown\n\"signal\" known to belong to the union of finitely many convex compact sets from\nindirect noisy observations of the signal. Our main assumption is that the\nobservation scheme in question is good in the sense of A. Goldenshluger, A.\nJuditsky, A. Nemirovski, Electr. J. Stat. 9(2) (2015), arXiv:1311.6765, the\nsimplest example being the Gaussian scheme where the observation is the sum of\nlinear image of the signal and the standard Gaussian noise. The proposed\nestimates, same as upper bounds on their worst-case risks, stem from solutions\nto explicit convex optimization problems, making the estimates\n\"computation-friendly.\" \n\n"}
{"id": "1804.03911", "contents": "Title: Structural causal models for macro-variables in time-series Abstract: We consider a bivariate time series $(X_t,Y_t)$ that is given by a simple\nlinear autoregressive model. Assuming that the equations describing each\nvariable as a linear combination of past values are considered structural\nequations, there is a clear meaning of how intervening on one particular $X_t$\ninfluences $Y_{t'}$ at later times $t'>t$. In the present work, we describe\nconditions under which one can define a causal model between variables that are\ncoarse-grained in time, thus admitting statements like `setting $X$ to $x$\nchanges $Y$ in a certain way' without referring to specific time instances. We\nshow that particularly simple statements follow in the frequency domain, thus\nproviding meaning to interventions on frequencies. \n\n"}
{"id": "1804.03926", "contents": "Title: Weighted Poincar\\'e inequalities, concentration inequalities and tail\n  bounds related to the Stein kernels in dimension one Abstract: We investigate links between the so-called Stein's density approach in\ndimension one and some functional and concentration inequalities. We show that\nmeasures having a finite first moment and a density with connected support\nsatisfy a weighted Poincar\\'e inequality with the weight being the Stein\nkernel, that indeed exists and is unique in this case. Furthermore, we prove\nweighted log-Sobolev and asymmetric Brascamp-Lieb type inequalities related to\nStein kernels. We also show that existence of a uniformly bounded Stein kernel\nis sufficient to ensure a positive Cheeger isoperimetric constant. Then we\nderive new concentration inequalities. In particular, we prove generalized\nMills' type inequalities when a Stein kernel is uniformly bounded and sub-gamma\nconcentration for Lipschitz functions of a variable with a sub-linear Stein\nkernel. When some exponential moments are finite, a general concentration\ninequality is then expressed in terms of Legendre-Fenchel transform of the\nLaplace transform of the Stein kernel. Along the way, we prove a general lemma\nfor bounding the Laplace transform of a random variable, that should be useful\nin many other contexts when deriving concentration inequalities. Finally, we\nprovide density and tail formulas as well as tail bounds, generalizing previous\nresults that where obtained in the context of Malliavin calculus. \n\n"}
{"id": "1804.04583", "contents": "Title: A New Generative Statistical Model for Graphs: The Latent Order Logistic\n  (LOLOG) Model Abstract: Full probability models are critical for the statistical modeling of complex\nnetworks, and yet there are few general, flexible and widely applicable\ngenerative methods. We propose a new family of probability models motivated by\nthe idea of network growth, which we call the Latent Order Logistic (LOLOG)\nmodel. LOLOG is a fully general framework capable of describing any probability\ndistribution over graph configurations, though not all distributions are easily\nexpressible or estimable as a LOLOG. We develop inferential procedures based on\nMonte Carlo Method of Moments, Generalized Method of Moments and variational\ninference. To show the flexibility of the model framework, we show how\nso-called scale-free networks can be modeled as LOLOGs via preferential\nattachment. The advantages of LOLOG in terms of avoidance of degeneracy, ease\nof sampling, and model flexibility are illustrated. Connections with the\npopular Exponential-family Random Graph model (ERGM) are also explored, and we\nfind that they are identical in the case of dyadic independence. Finally, we\napply the model to a social network of collaboration within a corporate law\nfirm, a friendship network among adolescent students, and the friendship\nrelations in an online social network. \n\n"}
{"id": "1804.04950", "contents": "Title: DeepFM: An End-to-End Wide & Deep Learning Framework for CTR Prediction Abstract: Learning sophisticated feature interactions behind user behaviors is critical\nin maximizing CTR for recommender systems. Despite great progress, existing\nmethods have a strong bias towards low- or high-order interactions, or rely on\nexpertise feature engineering. In this paper, we show that it is possible to\nderive an end-to-end learning model that emphasizes both low- and high-order\nfeature interactions. The proposed framework, DeepFM, combines the power of\nfactorization machines for recommendation and deep learning for feature\nlearning in a new neural network architecture. Compared to the latest Wide &\nDeep model from Google, DeepFM has a shared raw feature input to both its\n\"wide\" and \"deep\" components, with no need of feature engineering besides raw\nfeatures. DeepFM, as a general learning framework, can incorporate various\nnetwork architectures in its deep component. In this paper, we study two\ninstances of DeepFM where its \"deep\" component is DNN and PNN respectively, for\nwhich we denote as DeepFM-D and DeepFM-P. Comprehensive experiments are\nconducted to demonstrate the effectiveness of DeepFM-D and DeepFM-P over the\nexisting models for CTR prediction, on both benchmark data and commercial data.\nWe conduct online A/B test in Huawei App Market, which reveals that DeepFM-D\nleads to more than 10% improvement of click-through rate in the production\nenvironment, compared to a well-engineered LR model. We also covered related\npractice in deploying our framework in Huawei App Market. \n\n"}
{"id": "1804.06378", "contents": "Title: Graph-based Selective Outlier Ensembles Abstract: An ensemble technique is characterized by the mechanism that generates the\ncomponents and by the mechanism that combines them. A common way to achieve the\nconsensus is to enable each component to equally participate in the aggregation\nprocess. A problem with this approach is that poor components are likely to\nnegatively affect the quality of the consensus result. To address this issue,\nalternatives have been explored in the literature to build selective classifier\nand cluster ensembles, where only a subset of the components contributes to the\ncomputation of the consensus. Of the family of ensemble methods, outlier\nensembles are the least studied. Only recently, the selection problem for\noutlier ensembles has been discussed. In this work we define a new graph-based\nclass of ranking selection methods. A method in this class is characterized by\ntwo main steps: (1) Mapping the rankings onto a graph structure; and (2) Mining\nthe resulting graph to identify a subset of rankings. We define a specific\ninstance of the graph-based ranking selection class. Specifically, we map the\nproblem of selecting ensemble components onto a mining problem in a graph. An\nextensive evaluation was conducted on a variety of heterogeneous data and\nmethods. Our empirical results show that our approach outperforms\nstate-of-the-art selective outlier ensemble techniques. \n\n"}
{"id": "1804.06872", "contents": "Title: Co-teaching: Robust Training of Deep Neural Networks with Extremely\n  Noisy Labels Abstract: Deep learning with noisy labels is practically challenging, as the capacity\nof deep models is so high that they can totally memorize these noisy labels\nsooner or later during training. Nonetheless, recent studies on the\nmemorization effects of deep neural networks show that they would first\nmemorize training data of clean labels and then those of noisy labels.\nTherefore in this paper, we propose a new deep learning paradigm called\nCo-teaching for combating with noisy labels. Namely, we train two deep neural\nnetworks simultaneously, and let them teach each other given every mini-batch:\nfirstly, each network feeds forward all data and selects some data of possibly\nclean labels; secondly, two networks communicate with each other what data in\nthis mini-batch should be used for training; finally, each network back\npropagates the data selected by its peer network and updates itself. Empirical\nresults on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that\nCo-teaching is much superior to the state-of-the-art methods in the robustness\nof trained deep models. \n\n"}
{"id": "1804.08650", "contents": "Title: Bayesian Bandwidth Test and Selection for High-dimensional Banded\n  Precision Matrices Abstract: Assuming a banded structure is one of the common practice in the estimation\nof high-dimensional precision matrix. In this case, estimating the bandwidth of\nthe precision matrix is a crucial initial step for subsequent analysis.\nAlthough there exist some consistent frequentist tests for the bandwidth\nparameter, bandwidth selection consistency for precision matrices has not been\nestablished in a Bayesian framework. In this paper, we propose a prior\ndistribution tailored to the bandwidth estimation of high-dimensional precision\nmatrices. The banded structure is imposed via the Cholesky factor from the\nmodified Cholesky decomposition. We establish the strong model selection\nconsistency for the bandwidth as well as the consistency of the Bayes factor.\nThe convergence rates for Bayes factors under both the null and alternative\nhypotheses are derived which yield similar order of rates. As a by-product, we\nalso proposed an estimation procedure for the Cholesky factors yielding an\nalmost optimal order of convergence rates. Two-sample bandwidth test is also\nconsidered, and it turns out that our method is able to consistently detect the\nequality of bandwidths between two precision matrices. The simulation study\nconfirms that our method in general outperforms the existing frequentist and\nBayesian methods. \n\n"}
{"id": "1804.08741", "contents": "Title: Statistical Estimation of Conditional Shannon Entropy Abstract: The new estimates of the conditional Shannon entropy are introduced in the\nframework of the model describing a discrete response variable depending on a\nvector of d factors having a density w.r.t. the Lebesgue measure in R^d.\nNamely, the mixed-pair model (X,Y) is considered where X and Y take values in\nR^d and an arbitrary finite set, respectively. Such models include, for\ninstance, the famous logistic regression. In contrast to the well-known\nKozachenko -- Leonenko estimates of unconditional entropy the proposed\nestimates are constructed by means of the certain spacial order statistics (or\nk-nearest neighbor statistics where k=k_n depends on amount of observations n)\nand a random number of i.i.d. observations contained in the balls of specified\nrandom radii. The asymptotic unbiasedness and L^2-consistency of the new\nestimates are established under simple conditions. The obtained results can be\napplied to the feature selection problem which is important, e.g., for medical\nand biological investigations. \n\n"}
{"id": "1804.09217", "contents": "Title: On Learning Sparsely Used Dictionaries from Incomplete Samples Abstract: Most existing algorithms for dictionary learning assume that all entries of\nthe (high-dimensional) input data are fully observed. However, in several\npractical applications (such as hyper-spectral imaging or blood glucose\nmonitoring), only an incomplete fraction of the data entries may be available.\nFor incomplete settings, no provably correct and polynomial-time algorithm has\nbeen reported in the dictionary learning literature. In this paper, we provide\nprovable approaches for learning - from incomplete samples - a family of\ndictionaries whose atoms have sufficiently \"spread-out\" mass. First, we propose\na descent-style iterative algorithm that linearly converges to the true\ndictionary when provided a sufficiently coarse initial estimate. Second, we\npropose an initialization algorithm that utilizes a small number of extra fully\nobserved samples to produce such a coarse initial estimate. Finally, we\ntheoretically analyze their performance and provide asymptotic statistical and\ncomputational guarantees. \n\n"}
{"id": "1804.09879", "contents": "Title: Estimation of convex supports from noisy measurements Abstract: A popular class of problem in statistics deals with estimating the support of\na density from $n$ observations drawn at random from a $d$-dimensional\ndistribution. The one-dimensional case reduces to estimating the end points of\na univariate density. In practice, an experimenter may only have access to a\nnoisy version of the original data. Therefore, a more realistic model allows\nfor the observations to be contaminated with additive noise.\n  In this paper, we consider estimation of convex bodies when the additive\nnoise is distributed according to a multivariate Gaussian distribution, even\nthough our techniques could easily be adapted to other noise distributions.\nUnlike standard methods in deconvolution that are implemented by thresholding a\nkernel density estimate, our method avoids tuning parameters and Fourier\ntransforms altogether. We show that our estimator, computable in $(O(\\ln\nn))^{(d-1)/2}$ time, converges at a rate of $ O_d(\\log\\log n/\\sqrt{\\log n}) $\nin Hausdorff distance, in accordance with the polylogarithmic rates encountered\nin Gaussian deconvolution problems. Part of our analysis also involves the\noptimality of the proposed estimator. We provide a lower bound for the minimax\nrate of estimation in Hausdorff distance that is $\\Omega_d(1/\\log^2 n)$. \n\n"}
{"id": "1805.00928", "contents": "Title: Lidar Cloud Detection with Fully Convolutional Networks Abstract: In this contribution, we present a novel approach for segmenting laser radar\n(lidar) imagery into geometric time-height cloud locations with a fully\nconvolutional network (FCN). We describe a semi-supervised learning method to\ntrain the FCN by: pre-training the classification layers of the FCN with\nimage-level annotations, pre-training the entire FCN with the cloud locations\nof the MPLCMASK cloud mask algorithm, and fully supervised learning with\nhand-labeled cloud locations. We show the model achieves higher levels of cloud\nidentification compared to the cloud mask algorithm implementation. \n\n"}
{"id": "1805.02474", "contents": "Title: Sentence-State LSTM for Text Representation Abstract: Bi-directional LSTMs are a powerful tool for text representation. On the\nother hand, they have been shown to suffer various limitations due to their\nsequential nature. We investigate an alternative LSTM structure for encoding\ntext, which consists of a parallel state for each word. Recurrent steps are\nused to perform local and global information exchange between words\nsimultaneously, rather than incremental reading of a sequence of words. Results\non various classification and sequence labelling benchmarks show that the\nproposed model has strong representation power, giving highly competitive\nperformances compared to stacked BiLSTM models with similar parameter numbers. \n\n"}
{"id": "1805.03825", "contents": "Title: On asymptotic normality in estimation after a group sequential trial Abstract: We prove that in many realistic cases, the ordinary sample mean after a group\nsequential trial is asymptotically normal if the maximal number of observations\nincreases. We derive that it is often safe to use naive confidence intervals\nfor the mean of the collected observations, based on the ordinary sample mean.\nOur theoretical findings are confirmed by a simulation study. \n\n"}
{"id": "1805.05199", "contents": "Title: Bivariate Discrete Exponentiated Weibull Distribution: Properties and\n  Applications Abstract: In this paper, a new bivariate discrete distribution is introduced which\ncalled bivariate discrete exponentiated Weibull (BDEW) distribution. Several of\nits mathematical statistical properties are derived such as the joint\ncumulative distribution function, the joint joint hazard rate function,\nprobability mass function, joint moment generating function, mathematical\nexpectation and reliability function for stress-strength model. Further, the\nparameters of the BDEW distribution are estimated by the maximum likelihood\nmethod. Two real data sets are analyzed, and it was found that the BDEW\ndistribution provides better fit than other discrete distributions. \n\n"}
{"id": "1805.06640", "contents": "Title: Testing for Conditional Mean Independence with Covariates through\n  Martingale Difference Divergence Abstract: As a crucial problem in statistics is to decide whether additional variables\nare needed in a regression model. We propose a new multivariate test to\ninvestigate the conditional mean independence of Y given X conditioning on some\nknown effect Z, i.e., E(Y|X, Z) = E(Y|Z). Assuming that E(Y|Z) and Z are\nlinearly related, we reformulate an equivalent notion of conditional mean\nindependence through transformation, which is approximated in practice. We\napply the martingale difference divergence (Shao and Zhang, 2014) to measure\nconditional mean dependence, and show that the estimation error from\napproximation is negligible, as it has no impact on the asymptotic distribution\nof the test statistic under some regularity assumptions. The implementation of\nour test is demonstrated by both simulations and a financial data example. \n\n"}
{"id": "1805.06811", "contents": "Title: Fast, asymptotically efficient, recursive estimation in a Riemannian\n  manifold Abstract: Stochastic optimisation in Riemannian manifolds, especially the Riemannian\nstochastic gradient method, has attracted much recent attention. The present\nwork applies stochastic optimisation to the task of recursive estimation of a\nstatistical parameter which belongs to a Riemannian manifold. Roughly, this\ntask amounts to stochastic minimisation of a statistical divergence function.\nThe following problem is considered : how to obtain fast, asymptotically\nefficient, recursive estimates, using a Riemannian stochastic optimisation\nalgorithm with decreasing step sizes? In solving this problem, several original\nresults are introduced. First, without any convexity assumptions on the\ndivergence function, it is proved that, with an adequate choice of step sizes,\nthe algorithm computes recursive estimates which achieve a fast non-asymptotic\nrate of convergence. Second, the asymptotic normality of these recursive\nestimates is proved, by employing a novel linearisation technique. Third, it is\nproved that, when the Fisher information metric is used to guide the algorithm,\nthese recursive estimates achieve an optimal asymptotic rate of convergence, in\nthe sense that they become asymptotically efficient. These results, while\nrelatively familiar in the Euclidean context, are here formulated and proved\nfor the first time, in the Riemannian context. In addition, they are\nillustrated with a numerical application to the recursive estimation of\nelliptically contoured distributions. \n\n"}
{"id": "1805.06970", "contents": "Title: Global and Simultaneous Hypothesis Testing for High-Dimensional Logistic\n  Regression Models Abstract: High-dimensional logistic regression is widely used in analyzing data with\nbinary outcomes. In this paper, global testing and large-scale multiple testing\nfor the regression coefficients are considered in both single- and\ntwo-regression settings. A test statistic for testing the global null\nhypothesis is constructed using a generalized low-dimensional projection for\nbias correction and its asymptotic null distribution is derived. A lower bound\nfor the global testing is established, which shows that the proposed test is\nasymptotically minimax optimal over some sparsity range. For testing the\nindividual coefficients simultaneously, multiple testing procedures are\nproposed and shown to control the false discovery rate (FDR) and falsely\ndiscovered variables (FDV) asymptotically. Simulation studies are carried out\nto examine the numerical performance of the proposed tests and their\nsuperiority over existing methods. The testing procedures are also illustrated\nby analyzing a data set of a metabolomics study that investigates the\nassociation between fecal metabolites and pediatric Crohn's disease and the\neffects of treatment on such associations. \n\n"}
{"id": "1805.07051", "contents": "Title: Bayesian Joint Spike-and-Slab Graphical Lasso Abstract: In this article, we propose a new class of priors for Bayesian inference with\nmultiple Gaussian graphical models. We introduce fully Bayesian treatments of\ntwo popular procedures, the group graphical lasso and the fused graphical\nlasso, and extend them to a continuous spike-and-slab framework to allow\nself-adaptive shrinkage and model selection simultaneously. We develop an EM\nalgorithm that performs fast and dynamic explorations of posterior modes. Our\napproach selects sparse models efficiently with substantially smaller bias than\nwould be induced by alternative regularization procedures. The performance of\nthe proposed methods are demonstrated through simulation and two real data\nexamples. \n\n"}
{"id": "1805.07272", "contents": "Title: Fast Multivariate Log-Concave Density Estimation Abstract: A novel computational approach to log-concave density estimation is proposed.\nPrevious approaches utilize the piecewise-affine parametrization of the density\ninduced by the given sample set. The number of parameters as well as non-smooth\nsubgradient-based convex optimization for determining the maximum likelihood\ndensity estimate cause long runtimes for dimensions $d \\geq 2$ and large sample\nsets. The presented approach is based on mildly non-convex smooth\napproximations of the objective function and \\textit{sparse}, adaptive\npiecewise-affine density parametrization. Established memory-efficient\nnumerical optimization techniques enable to process larger data sets for\ndimensions $d \\geq 2$. While there is no guarantee that the algorithm returns\nthe maximum likelihood estimate for every problem instance, we provide\ncomprehensive numerical evidence that it does yield near-optimal results after\nsignificantly shorter runtimes. For example, 10000 samples in $\\mathbb{R}^2$\nare processed in two seconds, rather than in $\\approx 14$ hours required by the\nprevious approach to terminate. For higher dimensions, density estimation\nbecomes tractable as well: Processing $10000$ samples in $\\mathbb{R}^6$\nrequires 35 minutes. The software is publicly available as CRAN R package\nfmlogcondens. \n\n"}
{"id": "1805.08020", "contents": "Title: Restricted eigenvalue property for corrupted Gaussian designs Abstract: Motivated by the construction of tractable robust estimators via convex\nrelaxations, we present conditions on the sample size which guarantee an\naugmented notion of Restricted Eigenvalue-type condition for Gaussian designs.\nSuch a notion is suitable for high-dimensional robust inference in a Gaussian\nlinear model and a multivariate Gaussian model when samples are corrupted by\noutliers either in the response variable or in the design matrix. Our proof\ntechnique relies on simultaneous lower and upper bounds of two random bilinear\nforms with very different behaviors. Such simultaneous bounds are used for\nbalancing the interaction between the parameter vector and the estimated\ncorruption vector as well as for controlling the presence of corruption in the\ndesign. Our technique has the advantage of not relying on known bounds of the\nextreme singular values of the associated Gaussian ensemble nor on the use of\nmutual incoherence arguments. A relevant consequence of our analysis, compared\nto prior work, is that a significantly sharper restricted eigenvalue constant\ncan be obtained under weaker assumptions. In particular, the sparsity of the\nunknown parameter and the number of outliers are allowed to be completely\nindependent of each other. \n\n"}
{"id": "1805.08926", "contents": "Title: Efficient estimation of stable Levy process with symmetric jumps Abstract: Efficient estimation of a non-Gaussian stable Levy process with drift and\nsymmetric jumps observed at high frequency is considered. For this statistical\nexperiment, the local asymptotic normality of the likelihood is proved with a\nnon-singular Fisher information matrix through the use of a non-diagonal\nnorming matrix. The asymptotic normality and efficiency of a sequence of roots\nof the associated likelihood equation are shown as well. Moreover, we show that\na simple preliminary method of moments can be used as an initial estimator of a\nscoring procedure, thereby conveniently enabling us to bypass numerically\ndemanding likelihood optimization. Our simulation results show that the\none-step estimator can exhibit quite similar finite-sample performance as the\nmaximum likelihood estimator. \n\n"}
{"id": "1805.09045", "contents": "Title: When Simple Exploration is Sample Efficient: Identifying Sufficient\n  Conditions for Random Exploration to Yield PAC RL Algorithms Abstract: Efficient exploration is one of the key challenges for reinforcement learning\n(RL) algorithms. Most traditional sample efficiency bounds require strategic\nexploration. Recently many deep RL algorithms with simple heuristic exploration\nstrategies that have few formal guarantees, achieve surprising success in many\ndomains. These results pose an important question about understanding these\nexploration strategies such as $e$-greedy, as well as understanding what\ncharacterize the difficulty of exploration in MDPs. In this work we propose\nproblem specific sample complexity bounds of $Q$ learning with random walk\nexploration that rely on several structural properties. We also link our\ntheoretical results to some empirical benchmark domains, to illustrate if our\nbound gives polynomial sample complexity in these domains and how that is\nrelated with the empirical performance. \n\n"}
{"id": "1805.09697", "contents": "Title: Learning and Testing Causal Models with Interventions Abstract: We consider testing and learning problems on causal Bayesian networks as\ndefined by Pearl (Pearl, 2009). Given a causal Bayesian network $\\mathcal{M}$\non a graph with $n$ discrete variables and bounded in-degree and bounded\n`confounded components', we show that $O(\\log n)$ interventions on an unknown\ncausal Bayesian network $\\mathcal{X}$ on the same graph, and\n$\\tilde{O}(n/\\epsilon^2)$ samples per intervention, suffice to efficiently\ndistinguish whether $\\mathcal{X}=\\mathcal{M}$ or whether there exists some\nintervention under which $\\mathcal{X}$ and $\\mathcal{M}$ are farther than\n$\\epsilon$ in total variation distance. We also obtain sample/time/intervention\nefficient algorithms for: (i) testing the identity of two unknown causal\nBayesian networks on the same graph; and (ii) learning a causal Bayesian\nnetwork on a given graph. Although our algorithms are non-adaptive, we show\nthat adaptivity does not help in general: $\\Omega(\\log n)$ interventions are\nnecessary for testing the identity of two unknown causal Bayesian networks on\nthe same graph, even adaptively. Our algorithms are enabled by a new\nsubadditivity inequality for the squared Hellinger distance between two causal\nBayesian networks. \n\n"}
{"id": "1805.10721", "contents": "Title: Bernstein's inequalities for general Markov chains Abstract: We establish Bernstein's inequalities for functions of general\n(general-state-space and possibly non-reversible) Markov chains. These\ninequalities achieve sharp variance proxies and encompass the classical\nBernstein inequality for independent random variables as special cases. The key\nanalysis lies in bounding the operator norm of a perturbed Markov transition\nkernel by the exponential of sum of two convex functions. One coincides with\nwhat delivers the classical Bernstein inequality, and the other reflects the\ninfluence of the Markov dependence. A convex analysis on these two functions\nthen derives our Bernstein inequalities. As applications, we apply our\nBernstein inequalities to the Markov chain Monte Carlo integral estimation\nproblem and the robust mean estimation problem with Markov-dependent samples,\nand achieve tight deviation bounds that previous inequalities can not. \n\n"}
{"id": "1805.11088", "contents": "Title: Deep Reinforcement Learning in Ice Hockey for Context-Aware Player\n  Evaluation Abstract: A variety of machine learning models have been proposed to assess the\nperformance of players in professional sports. However, they have only a\nlimited ability to model how player performance depends on the game context.\nThis paper proposes a new approach to capturing game context: we apply Deep\nReinforcement Learning (DRL) to learn an action-value Q function from 3M\nplay-by-play events in the National Hockey League (NHL). The neural network\nrepresentation integrates both continuous context signals and game history,\nusing a possession-based LSTM. The learned Q-function is used to value players'\nactions under different game contexts. To assess a player's overall\nperformance, we introduce a novel Game Impact Metric (GIM) that aggregates the\nvalues of the player's actions. Empirical Evaluation shows GIM is consistent\nthroughout a play season, and correlates highly with standard success measures\nand future salary. \n\n"}
{"id": "1805.11214", "contents": "Title: Distributed Statistical Inference for Massive Data Abstract: This paper considers distributed statistical inference for general symmetric\nstatistics %that encompasses the U-statistics and the M-estimators in the\ncontext of massive data where the data can be stored at multiple platforms in\ndifferent locations. In order to facilitate effective computation and to avoid\nexpensive communication among different platforms, we formulate distributed\nstatistics which can be conducted over smaller data blocks. The statistical\nproperties of the distributed statistics are investigated in terms of the mean\nsquare error of estimation and asymptotic distributions with respect to the\nnumber of data blocks. In addition, we propose two distributed bootstrap\nalgorithms which are computationally effective and are able to capture the\nunderlying distribution of the distributed statistics. Numerical simulation and\nreal data applications of the proposed approaches are provided to demonstrate\nthe empirical performance. \n\n"}
{"id": "1806.00176", "contents": "Title: Reparameterization Gradient for Non-differentiable Models Abstract: We present a new algorithm for stochastic variational inference that targets\nat models with non-differentiable densities. One of the key challenges in\nstochastic variational inference is to come up with a low-variance estimator of\nthe gradient of a variational objective. We tackle the challenge by\ngeneralizing the reparameterization trick, one of the most effective techniques\nfor addressing the variance issue for differentiable models, so that the trick\nworks for non-differentiable models as well. Our algorithm splits the space of\nlatent variables into regions where the density of the variables is\ndifferentiable, and their boundaries where the density may fail to be\ndifferentiable. For each differentiable region, the algorithm applies the\nstandard reparameterization trick and estimates the gradient restricted to the\nregion. For each potentially non-differentiable boundary, it uses a form of\nmanifold sampling and computes the direction for variational parameters that,\nif followed, would increase the boundary's contribution to the variational\nobjective. The sum of all the estimates becomes the gradient estimate of our\nalgorithm. Our estimator enjoys the reduced variance of the reparameterization\ngradient while remaining unbiased even for non-differentiable models. The\nexperiments with our preliminary implementation confirm the benefit of reduced\nvariance and unbiasedness. \n\n"}
{"id": "1806.00381", "contents": "Title: Persistence paths and signature features in topological data analysis Abstract: We introduce a new feature map for barcodes that arise in persistent homology\ncomputation. The main idea is to first realize each barcode as a path in a\nconvenient vector space, and to then compute its path signature which takes\nvalues in the tensor algebra of that vector space. The composition of these two\noperations - barcode to path, path to tensor series - results in a feature map\nthat has several desirable properties for statistical learning, such as\nuniversality and characteristicness, and achieves state-of-the-art results on\ncommon classification benchmarks. \n\n"}
{"id": "1806.02659", "contents": "Title: Scalable Multi-Class Bayesian Support Vector Machines for Structured and\n  Unstructured Data Abstract: We introduce a new Bayesian multi-class support vector machine by formulating\na pseudo-likelihood for a multi-class hinge loss in the form of a\nlocation-scale mixture of Gaussians. We derive a variational-inference-based\ntraining objective for gradient-based learning. Additionally, we employ an\ninducing point approximation which scales inference to large data sets.\nFurthermore, we develop hybrid Bayesian neural networks that combine standard\ndeep learning components with the proposed model to enable learning for\nunstructured data. We provide empirical evidence that our model outperforms the\ncompetitor methods with respect to both training time and accuracy in\nclassification experiments on 68 structured and two unstructured data sets.\nFinally, we highlight the key capability of our model in yielding prediction\nuncertainty for classification by demonstrating its effectiveness in the tasks\nof large-scale active learning and detection of adversarial images. \n\n"}
{"id": "1806.02682", "contents": "Title: Transfer Learning for Illustration Classification Abstract: The field of image classification has shown an outstanding success thanks to\nthe development of deep learning techniques. Despite the great performance\nobtained, most of the work has focused on natural images ignoring other domains\nlike artistic depictions. In this paper, we use transfer learning techniques to\npropose a new classification network with better performance in illustration\nimages. Starting from the deep convolutional network VGG19, pre-trained with\nnatural images, we propose two novel models which learn object representations\nin the new domain. Our optimized network will learn new low-level features of\nthe images (colours, edges, textures) while keeping the knowledge of the\nobjects and shapes that it already learned from the ImageNet dataset. Thus,\nrequiring much less data for the training. We propose a novel dataset of\nillustration images labelled by content where our optimized architecture\nachieves $\\textbf{86.61\\%}$ of top-1 and $\\textbf{97.21\\%}$ of top-5 precision.\nWe additionally demonstrate that our model is still able to recognize objects\nin photographs. \n\n"}
{"id": "1806.03195", "contents": "Title: Obtaining fairness using optimal transport theory Abstract: Statistical algorithms are usually helping in making decisions in many\naspects of our lives. But, how do we know if these algorithms are biased and\ncommit unfair discrimination of a particular group of people, typically a\nminority? \\textit{Fairness} is generally studied in a probabilistic framework\nwhere it is assumed that there exists a protected variable, whose use as an\ninput of the algorithm may imply discrimination. There are different\ndefinitions of Fairness in the literature. In this paper we focus on two of\nthem which are called Disparate Impact (DI) and Balanced Error Rate (BER). Both\nare based on the outcome of the algorithm across the different groups\ndetermined by the protected variable. The relationship between these two\nnotions is also studied. The goals of this paper are to detect when a binary\nclassification rule lacks fairness and to try to fight against the potential\ndiscrimination attributable to it. This can be done by modifying either the\nclassifiers or the data itself. Our work falls into the second category and\nmodifies the input data using optimal transport theory. \n\n"}
{"id": "1806.03227", "contents": "Title: An Information-Percolation Bound for Spin Synchronization on General\n  Graphs Abstract: This paper considers the problem of reconstructing $n$ independent uniform\nspins $X_1,\\dots,X_n$ living on the vertices of an $n$-vertex graph $G$, by\nobserving their interactions on the edges of the graph. This captures instances\nof models such as (i) broadcasting on trees, (ii) block models, (iii)\nsynchronization on grids, (iv) spiked Wigner models. The paper gives an\nupper-bound on the mutual information between two vertices in terms of a bond\npercolation estimate. Namely, the information between two vertices' spins is\nbounded by the probability that these vertices are connected in a bond\npercolation model, where edges are opened with a probability that \"emulates\"\nthe edge-information. Both the information and the open-probability are based\non the Chi-squared mutual information. The main results allow us to re-derive\nknown results for information-theoretic non-reconstruction in models (i)-(iv),\nwith more direct or improved bounds in some cases, and to obtain new results,\nsuch as for a spiked Wigner model on grids. The main result also implies a new\nsubadditivity property for the Chi-squared mutual information for symmetric\nchannels and general graphs, extending the subadditivity property obtained by\nEvans-Kenyon-Peres-Schulman [EKPS00] for trees. \n\n"}
{"id": "1806.04071", "contents": "Title: Concentration of posterior probabilities and normalized L0 criteria Abstract: We study frequentist properties of Bayesian and $L_0$ model selection, with a\nfocus on (potentially non-linear) high-dimensional regression. We propose a\nconstruction to study how posterior probabilities and normalized $L_0$ criteria\nconcentrate on the (Kullback-Leibler) optimal model and other subsets of the\nmodel space. When such concentration occurs, one also bounds the frequentist\nprobabilities of selecting the correct model, type I and type II errors. These\nresults hold generally, and help validate the use of posterior probabilities\nand $L_0$ criteria to control frequentist error probabilities associated to\nmodel selection and hypothesis tests. Regarding regression, we help understand\nthe effect of the sparsity imposed by the prior or the $L_0$ penalty, and of\nproblem characteristics such as the sample size, signal-to-noise, dimension and\ntrue sparsity. A particular finding is that one may use less sparse\nformulations than would be asymptotically optimal, but still attain consistency\nand often also significantly better finite-sample performance. We also prove\nnew results related to misspecifying the mean or covariance structures, and\ngive tighter rates for certain non-local priors than currently available. \n\n"}
{"id": "1806.04195", "contents": "Title: Application of information-percolation method to reconstruction problems\n  on graphs Abstract: In this paper we propose a method of proving impossibility results based on\napplying strong data-processing inequalities to estimate mutual information\nbetween sets of variables forming certain Markov random fields. The end result\nis that mutual information between two \"far away\" (as measured by the graph\ndistance) variables is bounded by the probability of the existence of an open\npath in a bond-percolation problem on the same graph. Furthermore, stronger\nbounds can be obtained by establishing mutual information comparison results\nwith an erasure model on the same graph, with erasure probabilities given by\nthe contraction coefficients.\n  As applications, we show that our method gives sharp threshold for partially\nrecovering a rank-one perturbation of a random Gaussian matrix (spiked Wigner\nmodel), yields the best known upper bound on the noise level for group\nsynchronization (obtained concurrently by Abbe and Boix), and establishes new\nimpossibility result for community detection on the stochastic block model with\n$k$ communities. \n\n"}
{"id": "1806.04715", "contents": "Title: CID Models on Real-world Social Networks and Goodness of Fit\n  Measurements Abstract: Assessing the model fit quality of statistical models for network data is an\nongoing and under-examined topic in statistical network analysis. Traditional\nmetrics for evaluating model fit on tabular data such as the Bayesian\nInformation Criterion are not suitable for models specialized for network data.\nWe propose a novel self-developed goodness of fit (GOF) measure, the\n`stratified-sampling cross-validation' (SCV) metric, that uses a procedure\nsimilar to traditional cross-validation via stratified-sampling to select dyads\nin the network's adjacency matrix to be removed. SCV is capable of intuitively\nexpressing different models' ability to predict on missing dyads. Using SCV on\nreal-world social networks, we identify the appropriate statistical models for\ndifferent network structures and generalize such patterns. In particular, we\nfocus on conditionally independent dyad (CID) models such as the Erdos Renyi\nmodel, the stochastic block model, the sender-receiver model, and the latent\nspace model. \n\n"}
{"id": "1806.05419", "contents": "Title: Ranking Recovery from Limited Comparisons using Low-Rank Matrix\n  Completion Abstract: This paper proposes a new method for solving the well-known rank aggregation\nproblem from pairwise comparisons using the method of low-rank matrix\ncompletion. The partial and noisy data of pairwise comparisons is transformed\ninto a matrix form. We then use tools from matrix completion, which has served\nas a major component in the low-rank completion solution of the Netflix\nchallenge, to construct the preference of the different objects. In our\napproach, the data of multiple comparisons is used to create an estimate of the\nprobability of object i to win (or be chosen) over object j, where only a\npartial set of comparisons between N objects is known. The data is then\ntransformed into a matrix form for which the noiseless solution has a known\nrank of one. An alternating minimization algorithm, in which the target matrix\ntakes a bilinear form, is then used in combination with maximum likelihood\nestimation for both factors. The reconstructed matrix is used to obtain the\ntrue underlying preference intensity. This work demonstrates the improvement of\nour proposed algorithm over the current state-of-the-art in both simulated\nscenarios and real data. \n\n"}
{"id": "1806.05421", "contents": "Title: Selfless Sequential Learning Abstract: Sequential learning, also called lifelong learning, studies the problem of\nlearning tasks in a sequence with access restricted to only the data of the\ncurrent task. In this paper we look at a scenario with fixed model capacity,\nand postulate that the learning process should not be selfish, i.e. it should\naccount for future tasks to be added and thus leave enough capacity for them.\nTo achieve Selfless Sequential Learning we study different regularization\nstrategies and activation functions. We find that imposing sparsity at the\nlevel of the representation (i.e.~neuron activations) is more beneficial for\nsequential learning than encouraging parameter sparsity. In particular, we\npropose a novel regularizer, that encourages representation sparsity by means\nof neural inhibition. It results in few active neurons which in turn leaves\nmore free neurons to be utilized by upcoming tasks. As neural inhibition over\nan entire layer can be too drastic, especially for complex tasks requiring\nstrong representations, our regularizer only inhibits other neurons in a local\nneighbourhood, inspired by lateral inhibition processes in the brain. We\ncombine our novel regularizer, with state-of-the-art lifelong learning methods\nthat penalize changes to important previously learned parts of the network. We\nshow that our new regularizer leads to increased sparsity which translates in\nconsistent performance improvement %over alternative regularizers we studied on\ndiverse datasets. \n\n"}
{"id": "1806.06121", "contents": "Title: Machine learning for prediction of extreme statistics in modulation\n  instability Abstract: A central area of research in nonlinear science is the study of instabilities\nthat drive the emergence of extreme events. Unfortunately, experimental\ntechniques for measuring such phenomena often provide only partial\ncharacterization. For example, real-time studies of instabilities in nonlinear\nfibre optics frequently use only spectral data, precluding detailed predictions\nabout the associated temporal properties. Here, we show how Machine Learning\ncan overcome this limitation by predicting statistics for the maximum intensity\nof temporal peaks in modulation instability based only on spectral\nmeasurements. Specifically, we train a neural network based Machine Learning\nmodel to correlate spectral and temporal properties of optical fibre modulation\ninstability using data from numerical simulations, and we then use this model\nto predict the temporal probability distribution based on high-dynamic range\nspectral data from experiments. These results open novel perspectives in all\nsystems exhibiting chaos and instability where direct time-domain observations\nare difficult. \n\n"}
{"id": "1806.06231", "contents": "Title: Adaptive estimating function inference for non-stationary determinantal\n  point processes Abstract: Estimating function inference is indispensable for many common point process\nmodels where the joint intensities are tractable while the likelihood function\nis not. In this paper we establish asymptotic normality of estimating function\nestimators in a very general setting of non-stationary point processes. We then\nadapt this result to the case of non-stationary determinantal point processes\nwhich are an important class of models for repulsive point patterns. In\npractice often first and second order estimating functions are used. For the\nlatter it is common practice to omit contributions for pairs of points\nseparated by a distance larger than some truncation distance which is usually\nspecified in an ad hoc manner. We suggest instead a data-driven approach where\nthe truncation distance is adapted automatically to the point process being\nfitted and where the approach integrates seamlessly with our asymptotic\nframework. The good performance of the adaptive approach is illustrated via\nsimulation studies for non-stationary determinantal point processes and by an\napplication to a real dataset. \n\n"}
{"id": "1806.06945", "contents": "Title: Overlapping Clustering Models, and One (class) SVM to Bind Them All Abstract: People belong to multiple communities, words belong to multiple topics, and\nbooks cover multiple genres; overlapping clusters are commonplace. Many\nexisting overlapping clustering methods model each person (or word, or book) as\na non-negative weighted combination of \"exemplars\" who belong solely to one\ncommunity, with some small noise. Geometrically, each person is a point on a\ncone whose corners are these exemplars. This basic form encompasses the widely\nused Mixed Membership Stochastic Blockmodel of networks (Airoldi et al., 2008)\nand its degree-corrected variants (Jin et al., 2017), as well as topic models\nsuch as LDA (Blei et al., 2003). We show that a simple one-class SVM yields\nprovably consistent parameter inference for all such models, and scales to\nlarge datasets. Experimental results on several simulated and real datasets\nshow our algorithm (called SVM-cone) is both accurate and scalable. \n\n"}
{"id": "1806.07508", "contents": "Title: Reducibility and Computational Lower Bounds for Problems with Planted\n  Sparse Structure Abstract: The prototypical high-dimensional statistics problem entails finding a\nstructured signal in noise. Many of these problems exhibit an intriguing\nphenomenon: the amount of data needed by all known computationally efficient\nalgorithms far exceeds what is needed for inefficient algorithms that search\nover all possible structures. A line of work initiated by Berthet and Rigollet\nin 2013 has aimed to explain these statistical-computational gaps by reducing\nfrom conjecturally hard average-case problems in computer science. However, the\ndelicate nature of average-case reductions has limited the applicability of\nthis approach. In this work we introduce several new techniques to give a web\nof average-case reductions showing strong computational lower bounds based on\nthe planted clique conjecture using natural problems as intermediates. These\ninclude tight lower bounds for Planted Independent Set, Planted Dense Subgraph,\nSparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block\nModel and a biased variant of Sparse PCA. We also give algorithms matching our\nlower bounds and identify the information-theoretic limits of the models we\nconsider. \n\n"}
{"id": "1806.08542", "contents": "Title: Removing the Curse of Superefficiency: an Effective Strategy For\n  Distributed Computing in Isotonic Regression Abstract: We propose a strategy for computing the isotonic least-squares estimate of a\nmonotone function in a general regression setting where the data are\ndistributed across different servers and the observations across servers,\nthough independent, can come from heterogeneous sub-populations, thereby\nviolating the identically distributed assumption. Our strategy fixes the\nsuper-efficiency phenomenon observed in prior work on distributed computing in\nthe isotonic regression framework, where averaging several isotonic estimates\n(each computed at a local server) on a central server produces super-efficient\nestimates that do not replicate the properties of the global isotonic\nestimator, i.e. the isotonic estimate that would be constructed by transferring\nall the data to a single server. The new estimator proposed in this paper works\nby smoothing the data on each local server, communicating the smoothed\nsummaries to the central server, and then computing an isotonic estimate at the\ncentral server, and is shown to replicate the asymptotic properties of the\nglobal estimator, and also overcome the super-efficiency phenomenon exhibited\nby earlier estimators. For data on $N$ observations, the new estimator can be\nconstructed by transferring data just over order $N^{1/3}$ across servers [as\ncompared to transferring data of order $N$ to compute the global isotonic\nestimator], and requires the same order of computing time as the global\nestimator. \n\n"}
{"id": "1806.09141", "contents": "Title: Constructing Deep Neural Networks by Bayesian Network Structure Learning Abstract: We introduce a principled approach for unsupervised structure learning of\ndeep neural networks. We propose a new interpretation for depth and inter-layer\nconnectivity where conditional independencies in the input distribution are\nencoded hierarchically in the network structure. Thus, the depth of the network\nis determined inherently. The proposed method casts the problem of neural\nnetwork structure learning as a problem of Bayesian network structure learning.\nThen, instead of directly learning the discriminative structure, it learns a\ngenerative graph, constructs its stochastic inverse, and then constructs a\ndiscriminative graph. We prove that conditional-dependency relations among the\nlatent variables in the generative graph are preserved in the class-conditional\ndiscriminative graph. We demonstrate on image classification benchmarks that\nthe deepest layers (convolutional and dense) of common networks can be replaced\nby significantly smaller learned structures, while maintaining classification\naccuracy---state-of-the-art on tested benchmarks. Our structure learning\nalgorithm requires a small computational cost and runs efficiently on a\nstandard desktop CPU. \n\n"}
{"id": "1806.09277", "contents": "Title: Towards Optimal Transport with Global Invariances Abstract: Many problems in machine learning involve calculating correspondences between\nsets of objects, such as point clouds or images. Discrete optimal transport\nprovides a natural and successful approach to such tasks whenever the two sets\nof objects can be represented in the same space, or at least distances between\nthem can be directly evaluated. Unfortunately neither requirement is likely to\nhold when object representations are learned from data. Indeed, automatically\nderived representations such as word embeddings are typically fixed only up to\nsome global transformations, for example, reflection or rotation. As a result,\npairwise distances across two such instances are ill-defined without specifying\ntheir relative transformation. In this work, we propose a general framework for\noptimal transport in the presence of latent global transformations. We cast the\nproblem as a joint optimization over transport couplings and transformations\nchosen from a flexible class of invariances, propose algorithms to solve it,\nand show promising results in various tasks, including a popular unsupervised\nword translation benchmark. \n\n"}
{"id": "1806.09471", "contents": "Title: Does data interpolation contradict statistical optimality? Abstract: We show that learning methods interpolating the training data can achieve\noptimal rates for the problems of nonparametric regression and prediction with\nsquare loss. \n\n"}
{"id": "1806.09762", "contents": "Title: Boulevard: Regularized Stochastic Gradient Boosted Trees and Their\n  Limiting Distribution Abstract: This paper examines a novel gradient boosting framework for regression. We\nregularize gradient boosted trees by introducing subsampling and employ a\nmodified shrinkage algorithm so that at every boosting stage the estimate is\ngiven by an average of trees. The resulting algorithm, titled Boulevard, is\nshown to converge as the number of trees grows. We also demonstrate a central\nlimit theorem for this limit, allowing a characterization of uncertainty for\npredictions. A simulation study and real world examples provide support for\nboth the predictive accuracy of the model and its limiting behavior. \n\n"}
{"id": "1806.10648", "contents": "Title: Uncoupled isotonic regression via minimum Wasserstein deconvolution Abstract: Isotonic regression is a standard problem in shape-constrained estimation\nwhere the goal is to estimate an unknown nondecreasing regression function $f$\nfrom independent pairs $(x_i, y_i)$ where $\\mathbb{E}[y_i]=f(x_i), i=1, \\ldots\nn$. While this problem is well understood both statistically and\ncomputationally, much less is known about its uncoupled counterpart where one\nis given only the unordered sets $\\{x_1, \\ldots, x_n\\}$ and $\\{y_1, \\ldots,\ny_n\\}$. In this work, we leverage tools from optimal transport theory to derive\nminimax rates under weak moments conditions on $y_i$ and to give an efficient\nalgorithm achieving optimal rates. Both upper and lower bounds employ\nmoment-matching arguments that are also pertinent to learning mixtures of\ndistributions and deconvolution. \n\n"}
{"id": "1806.11237", "contents": "Title: Nonparametric competing risks analysis using Bayesian Additive\n  Regression Trees (BART) Abstract: Many time-to-event studies are complicated by the presence of competing\nrisks. Such data are often analyzed using Cox models for the cause specific\nhazard function or Fine-Gray models for the subdistribution hazard. In practice\nregression relationships in competing risks data with either strategy are often\ncomplex and may include nonlinear functions of covariates, interactions,\nhigh-dimensional parameter spaces and nonproportional cause specific or\nsubdistribution hazards. Model misspecification can lead to poor predictive\nperformance. To address these issues, we propose a novel approach to flexible\nprediction modeling of competing risks data using Bayesian Additive Regression\nTrees (BART). We study the simulation performance in two-sample scenarios as\nwell as a complex regression setting, and benchmark its performance against\nstandard regression techniques as well as random survival forests. We\nillustrate the use of the proposed method on a recently published study of\npatients undergoing hematopoietic stem cell transplantation. \n\n"}
{"id": "1807.00359", "contents": "Title: On null hypotheses in survival analysis Abstract: The conventional nonparametric tests in survival analysis, such as the\nlog-rank test, assess the null hypothesis that the hazards are equal at all\ntimes. However, hazards are hard to interpret causally, and other null\nhypotheses are more relevant in many scenarios with survival outcomes. To allow\nfor a wider range of null hypotheses, we present a generic approach to define\ntest statistics. This approach utilizes the fact that a wide range of common\nparameters in survival analysis can be expressed as solutions of differential\nequations. Thereby we can test hypotheses based on survival parameters that\nsolve differential equations driven by cumulative hazards, and it is easy to\nimplement the tests on a computer. We present simulations, suggesting that our\ntests perform well for several hypotheses in a range of scenarios. Finally, we\nuse our tests to evaluate the effect of adjuvant chemotherapies in patients\nwith colon cancer, using data from a randomised controlled trial. \n\n"}
{"id": "1807.01470", "contents": "Title: Post hoc false positive control for spatially structured hypotheses Abstract: In a high dimensional multiple testing framework, we present new confidence\nbounds on the false positives contained in subsets S of selected null\nhypotheses. The coverage probability holds simultaneously over all subsets S,\nwhich means that the obtained confidence bounds are post hoc. Therefore, S can\nbe chosen arbitrarily, possibly by using the data set several times. We focus\nin this paper specifically on the case where the null hypotheses are spatially\nstructured. Our method is based on recent advances in post hoc inference and\nparticularly on the general methodology of Blanchard et al. (2017); we build\nconfidence bounds for some pre-specified forest-structured subsets {R k , k\n$\\in$ K}, called the reference family, and then we deduce a bound for any\nsubset S by interpolation. The proposed bounds are shown to improve\nsubstantially previous ones when the signal is locally structured. Our findings\nare supported both by theoretical results and numerical experiments. Moreover,\nwe show that our bound can be obtained by a low-complexity algorithm, which\nmakes our approach completely operational for a practical use. The proposed\nbounds are implemented in the open-source R package sansSouci. \n\n"}
{"id": "1807.04272", "contents": "Title: Towards a Complete Picture of Stationary Covariance Functions on Spheres\n  Cross Time Abstract: With the advent of wide-spread global and continental-scale spatiotemporal\ndatasets, increased attention has been given to covariance functions on spheres\nover time. This paper provides results for stationary covariance functions of\nrandom fields defined over $d$-dimensional spheres cross time. Specifically, we\nprovide a bridge between the characterization in \\cite{berg-porcu} for\ncovariance functions on spheres cross time and Gneiting's lemma\n\\citep{gneiting2002} that deals with planar surfaces.\n  We then prove that there is a valid class of covariance functions similar in\nform to the Gneiting class of space-time covariance functions\n\\citep{gneiting2002} that replaces the squared Euclidean distance with the\ngreat circle distance. Notably, the provided class is shown to be positive\ndefinite on every $d$-dimensional sphere cross time, while the Gneiting class\nis positive definite over $\\R^d \\times \\R$ for fixed $d$ only.\n  In this context, we illustrate the value of our adapted Gneiting class by\ncomparing examples from this class to currently established nonseparable\ncovariance classes using out-of-sample predictive criteria. These comparisons\nare carried out on two climate reanalysis datasets from the National Centers\nfor Environmental Prediction and National Center for Atmospheric Research. For\nthese datasets, we show that examples from our covariance class have better\npredictive performance than competing models. \n\n"}
{"id": "1807.05801", "contents": "Title: Weak dependence and GMM estimation of supOU and mixed moving average\n  processes Abstract: We consider a mixed moving average (MMA) process X driven by a L\\'evy basis\nand prove that it is weakly dependent with rates computable in terms of the\nmoving average kernel and the characteristic quadruple of the L\\'evy basis.\nUsing this property, we show conditions ensuring that sample mean and\nautocovariances of X have a limiting normal distribution. We extend these\nresults to stochastic volatility models and then investigate a Generalized\nMethod of Moments estimator for the supOU process and the supOU stochastic\nvolatility model after choosing a suitable distribution for the mean reversion\nparameter. For these estimators, we analyze the asymptotic behavior in detail. \n\n"}
{"id": "1807.06133", "contents": "Title: Density estimation by Randomized Quasi-Monte Carlo Abstract: We consider the problem of estimating the density of a random variable $X$\nthat can be sampled exactly by Monte Carlo (MC). We investigate the\neffectiveness of replacing MC by randomized quasi Monte Carlo (RQMC) or by\nstratified sampling over the unit cube, to reduce the integrated variance (IV)\nand the mean integrated square error (MISE) for kernel density estimators. We\nshow theoretically and empirically that the RQMC and stratified estimators can\nachieve substantial reductions of the IV and the MISE, and even faster\nconvergence rates than MC in some situations, while leaving the bias unchanged.\nWe also show that the variance bounds obtained via a traditional\nKoksma-Hlawka-type inequality for RQMC are much too loose to be useful when the\ndimension of the problem exceeds a few units. We describe an alternative way to\nestimate the IV, a good bandwidth, and the MISE, under RQMC or stratification,\nand we show empirically that in some situations, the MISE can be reduced\nsignificantly even in high-dimensional settings. \n\n"}
{"id": "1807.06217", "contents": "Title: An exposition of the false confidence theorem Abstract: A recent paper presents the \"false confidence theorem\" (FCT) which has\npotentially broad implications for statistical inference using Bayesian\nposterior uncertainty. This theorem says that with arbitrarily large\n(sampling/frequentist) probability, there exists a set which does \\textit{not}\ncontain the true parameter value, but which has arbitrarily large posterior\nprobability. Since the use of Bayesian methods has become increasingly popular\nin applications of science, engineering, and business, it is critically\nimportant to understand when Bayesian procedures lead to problematic\nstatistical inferences or interpretations. In this paper, we consider a number\nof examples demonstrating the paradoxical nature of false confidence to begin\nto understand the contexts in which the FCT does (and does not) play a\nmeaningful role in statistical inference. Our examples illustrate that models\ninvolving marginalization to non-linear, not one-to-one functions of multiple\nparameters play a key role in more extreme manifestations of false confidence. \n\n"}
{"id": "1807.06362", "contents": "Title: Confidence Intervals for Testing Disparate Impact in Fair Learning Abstract: We provide the asymptotic distribution of the major indexes used in the\nstatistical literature to quantify disparate treatment in machine learning. We\naim at promoting the use of confidence intervals when testing the so-called\ngroup disparate impact. We illustrate on some examples the importance of using\nconfidence intervals and not a single value. \n\n"}
{"id": "1807.09902", "contents": "Title: General-purpose Tagging of Freesound Audio with AudioSet Labels: Task\n  Description, Dataset, and Baseline Abstract: This paper describes Task 2 of the DCASE 2018 Challenge, titled\n\"General-purpose audio tagging of Freesound content with AudioSet labels\". This\ntask was hosted on the Kaggle platform as \"Freesound General-Purpose Audio\nTagging Challenge\". The goal of the task is to build an audio tagging system\nthat can recognize the category of an audio clip from a subset of 41 diverse\ncategories drawn from the AudioSet Ontology. We present the task, the dataset\nprepared for the competition, and a baseline system. \n\n"}
{"id": "1807.10478", "contents": "Title: Interpreting recurrent neural networks behaviour via excitable network\n  attractors Abstract: Introduction: Machine learning provides fundamental tools both for scientific\nresearch and for the development of technologies with significant impact on\nsociety. It provides methods that facilitate the discovery of regularities in\ndata and that give predictions without explicit knowledge of the rules\ngoverning a system. However, a price is paid for exploiting such flexibility:\nmachine learning methods are typically black-boxes where it is difficult to\nfully understand what the machine is doing or how it is operating. This poses\nconstraints on the applicability and explainability of such methods. Methods:\nOur research aims to open the black-box of recurrent neural networks, an\nimportant family of neural networks used for processing sequential data. We\npropose a novel methodology that provides a mechanistic interpretation of\nbehaviour when solving a computational task. Our methodology uses mathematical\nconstructs called excitable network attractors, which are invariant sets in\nphase space composed of stable attractors and excitable connections between\nthem. Results and Discussion: As the behaviour of recurrent neural networks\ndepends both on training and on inputs to the system, we introduce an algorithm\nto extract network attractors directly from the trajectory of a neural network\nwhile solving tasks. Simulations conducted on a controlled benchmark task\nconfirm the relevance of these attractors for interpreting the behaviour of\nrecurrent neural networks, at least for tasks that involve learning a finite\nnumber of stable states and transitions between them. \n\n"}
{"id": "1807.10511", "contents": "Title: Global and local evaluation of link prediction tasks with neural\n  embeddings Abstract: We focus our attention on the link prediction problem for knowledge graphs,\nwhich is treated herein as a binary classification task on neural embeddings of\nthe entities. By comparing, combining and extending different methodologies for\nlink prediction on graph-based data coming from different domains, we formalize\na unified methodology for the quality evaluation benchmark of neural embeddings\nfor knowledge graphs. This benchmark is then used to empirically investigate\nthe potential of training neural embeddings globally for the entire graph, as\nopposed to the usual way of training embeddings locally for a specific\nrelation. This new way of testing the quality of the embeddings evaluates the\nperformance of binary classifiers for scalable link prediction with limited\ndata. Our evaluation pipeline is made open source, and with this we aim to draw\nmore attention of the community towards an important issue of transparency and\nreproducibility of the neural embeddings evaluations. \n\n"}
{"id": "1807.10801", "contents": "Title: On the expected runtime of multiple testing algorithms with bounded\n  error Abstract: Consider testing multiple hypotheses in the setting where the p-values of all\nhypotheses are unknown and thus have to be approximated using Monte Carlo\nsimulations. One class of algorithms published in the literature for this\nscenario provides guarantees on the correctness of their testing result through\nthe computation of confidence statements on all approximated p-values. This\narticle focuses on the expected runtime of such algorithms and derives a\nvariety of finite and infinite expected runtime results. \n\n"}
{"id": "1807.11863", "contents": "Title: On the Unbiased Asymptotic Normality of Quantile Regression with Fixed\n  Effects Abstract: Nonlinear panel data models with fixed individual effects provide an\nimportant set of tools for describing microeconometric data. In a large class\nof such models (including probit, proportional hazard and quantile regression\nto name just a few) it is impossible to difference out individual effects, and\ninference is usually justified in a `large n large T' asymptotic framework.\nHowever, there is a considerable gap in the type of assumptions that are\ncurrently imposed in models with smooth score functions (such as probit, and\nproportional hazard) and quantile regression. In the present paper we show that\nthis gap can be bridged and establish asymptotic unbiased normality for\nquantile regression panels under conditions on n,T that are very close to what\nis typically assumed in standard nonlinear panels. Our results considerably\nimprove upon existing theory and show that quantile regression is applicable to\nthe same type of panel data (in terms of n,T) as other commonly used nonlinear\npanel data models. Thorough numerical experiments confirm our theoretical\nfindings. \n\n"}
{"id": "1808.00419", "contents": "Title: Mixed effects models for healthcare longitudinal data with an\n  informative visiting process: a Monte Carlo simulation study Abstract: Electronic health records are being increasingly used in medical research to\nanswer more relevant and detailed clinical questions; however, they pose new\nand significant methodological challenges. For instance, observation times are\nlikely correlated with the underlying disease severity: patients with worse\nconditions utilise health care more and may have worse biomarker values\nrecorded. Traditional methods for analysing longitudinal data assume\nindependence between observation times and disease severity; yet, with\nhealthcare data such assumptions unlikely holds. Through Monte Carlo\nsimulation, we compare different analytical approaches proposed to account for\nan informative visiting process to assess whether they lead to unbiased\nresults. Furthermore, we formalise a joint model for the observation process\nand the longitudinal outcome within an extended joint modelling framework. We\nillustrate our results using data from a pragmatic trial on enhanced care for\nindividuals with chronic kidney disease, and we introduce user-friendly\nsoftware that can be used to fit the joint model for the observation process\nand a longitudinal outcome. \n\n"}
{"id": "1808.00590", "contents": "Title: MLCapsule: Guarded Offline Deployment of Machine Learning as a Service Abstract: With the widespread use of machine learning (ML) techniques, ML as a service\nhas become increasingly popular. In this setting, an ML model resides on a\nserver and users can query it with their data via an API. However, if the\nuser's input is sensitive, sending it to the server is undesirable and\nsometimes even legally not possible. Equally, the service provider does not\nwant to share the model by sending it to the client for protecting its\nintellectual property and pay-per-query business model.\n  In this paper, we propose MLCapsule, a guarded offline deployment of machine\nlearning as a service. MLCapsule executes the model locally on the user's side\nand therefore the data never leaves the client. Meanwhile, MLCapsule offers the\nservice provider the same level of control and security of its model as the\ncommonly used server-side execution. In addition, MLCapsule is applicable to\noffline applications that require local execution. Beyond protecting against\ndirect model access, we couple the secure offline deployment with defenses\nagainst advanced attacks on machine learning models such as model stealing,\nreverse engineering, and membership inference. \n\n"}
{"id": "1808.01535", "contents": "Title: Triplet Network with Attention for Speaker Diarization Abstract: In automatic speech processing systems, speaker diarization is a crucial\nfront-end component to separate segments from different speakers. Inspired by\nthe recent success of deep neural networks (DNNs) in semantic inferencing,\ntriplet loss-based architectures have been successfully used for this problem.\nHowever, existing work utilizes conventional i-vectors as the input\nrepresentation and builds simple fully connected networks for metric learning,\nthus not fully leveraging the modeling power of DNN architectures. This paper\ninvestigates the importance of learning effective representations from the\nsequences directly in metric learning pipelines for speaker diarization. More\nspecifically, we propose to employ attention models to learn embeddings and the\nmetric jointly in an end-to-end fashion. Experiments are conducted on the\nCALLHOME conversational speech corpus. The diarization results demonstrate\nthat, besides providing a unified model, the proposed approach achieves\nimproved performance when compared against existing approaches. \n\n"}
{"id": "1808.01691", "contents": "Title: Sampling-based randomized designs for causal inference under the\n  potential outcomes framework Abstract: We establish the inferential properties of the mean-difference estimator for\nthe average treatment effect in randomized experiments where each unit in a\npopulation is randomized to one of two treatments and then units within\ntreatment groups are randomly sampled. The properties of this estimator are\nwell-understood in the experimental design scenario where first units are\nrandomly sampled and then treatment is randomly assigned, but not for the\naforementioned scenario where the sampling and treatment assignment stages are\nreversed. We find that the inferential properties of the mean-difference\nestimator under this experimental design scenario are identical to those under\nthe more common sample-first-randomize-second design. This finding will bring\nsome clarifications about sampling-based randomized designs for causal\ninference, particularly for settings where there is a finite super-population.\nFinally, we explore to what extent pre-treatment measurements can be used to\nimprove upon the mean-difference estimator for this\nrandomize-first-sample-second design. Unfortunately, we find that pre-treatment\nmeasurements are often unhelpful in improving the precision of average\ntreatment effect estimators under this design, unless a large number of\npre-treatment measurements that are highly associative with the post-treatment\nmeasurements can be obtained. We confirm these results using a simulation study\nbased on a real experiment in nanomaterials. \n\n"}
{"id": "1808.01905", "contents": "Title: Nuisance Parameters Free Changepoint Detection in Non-stationary Series Abstract: Detecting abrupt changes in the mean of a time series, so-called\nchangepoints, is important for many applications. However, many procedures rely\non the estimation of nuisance parameters (like long-run variance). Under the\nalternative (a change in mean), estimators might be biased and data-adaptive\nrules for the choice of tuning parameters might not work as expected. If the\ndata is not stationary, but heteroscedastic, this becomes more challenging. The\naim of this paper is to present and investigate two changepoint tests, which\ninvolve neither nuisance nor tuning parameters. This is achieved by combing\nself-normalization and wild bootstrap. We study the asymptotic behavior and\nshow the consistency of the bootstrap under the hypothesis as well as under the\nalternative, assuming mild conditions on the weak dependence of the time series\nand allowing the variance to change over time. As a by-product of the proposed\ntests, a changepoint estimator is introduced and its consistency is proved. The\nresults are illustrated through a simulation study, which demonstrates\ncomputational efficiency of the developed methods. The new tests will also be\napplied to real data examples from finance and hydrology. \n\n"}
{"id": "1808.01990", "contents": "Title: Hashing with Binary Matrix Pursuit Abstract: We propose theoretical and empirical improvements for two-stage hashing\nmethods. We first provide a theoretical analysis on the quality of the binary\ncodes and show that, under mild assumptions, a residual learning scheme can\nconstruct binary codes that fit any neighborhood structure with arbitrary\naccuracy. Secondly, we show that with high-capacity hash functions such as\nCNNs, binary code inference can be greatly simplified for many standard\nneighborhood definitions, yielding smaller optimization problems and more\nrobust codes. Incorporating our findings, we propose a novel two-stage hashing\nmethod that significantly outperforms previous hashing studies on widely used\nimage retrieval benchmarks. \n\n"}
{"id": "1808.02439", "contents": "Title: Fluctuation Bounds for Continuous Time Branching Processes and Evolution\n  of Growing Trees With a Change Point Abstract: We consider dynamic random trees constructed using an attachment function $f\n: \\mathbb{N} \\to \\mathbb{R}_+$ where, at each step of the evolution, a new\nvertex attaches to an existing vertex $v$ in the current tree with probability\nproportional to $f$(degree(v)). We explore the effect of a change point in the\nsystem; the dynamics are initially driven by a function f until the tree\nreaches size $\\tau(n) \\in (0,n)$, at which point the attachment function\nswitches to another function, $g$, until the tree reaches size $n$. Two change\npoint time scales are considered, namely the standard model where $\\tau(n) =\n\\gamma n$, and the quick big bang model where $\\tau(n) = n^\\gamma$, for some $0\n< \\gamma < 1$. In the former case, we obtain deterministic approximations for\nthe evolution of the empirical degree distribution (EDF) in sup-norm and use\nthese to devise a provably consistent non-parametric estimator for the change\npoint $\\gamma$. In the latter case, we show that the effect of pre-change point\ndynamics asymptotically vanishes in the EDF, although this effect persists in\nfunctionals such as the maximal degree. Our proofs rely on embedding the\ndiscrete time tree dynamics in an associated (time) inhomogeneous continuous\ntime branching process (CTBP). In the course of proving the above results, we\ndevelop novel mathematical techniques to analyze both homogeneous and\ninhomogeneous CTBPs and obtain rates of convergence for functionals of such\nprocesses, which are of independent interest. \n\n"}
{"id": "1808.03591", "contents": "Title: How Complex is your classification problem? A survey on measuring\n  classification complexity Abstract: Characteristics extracted from the training datasets of classification\nproblems have proven to be effective predictors in a number of meta-analyses.\nAmong them, measures of classification complexity can be used to estimate the\ndifficulty in separating the data points into their expected classes.\nDescriptors of the spatial distribution of the data and estimates of the shape\nand size of the decision boundary are among the known measures for this\ncharacterization. This information can support the formulation of new\ndata-driven pre-processing and pattern recognition techniques, which can in\nturn be focused on challenges highlighted by such characteristics of the\nproblems. This paper surveys and analyzes measures which can be extracted from\nthe training datasets in order to characterize the complexity of the respective\nclassification problems. Their use in recent literature is also reviewed and\ndiscussed, allowing to prospect opportunities for future work in the area.\nFinally, descriptions are given on an R package named Extended Complexity\nLibrary (ECoL) that implements a set of complexity measures and is made\npublicly available. \n\n"}
{"id": "1808.03889", "contents": "Title: Robust high dimensional factor models with applications to statistical\n  machine learning Abstract: Factor models are a class of powerful statistical models that have been\nwidely used to deal with dependent measurements that arise frequently from\nvarious applications from genomics and neuroscience to economics and finance.\nAs data are collected at an ever-growing scale, statistical machine learning\nfaces some new challenges: high dimensionality, strong dependence among\nobserved variables, heavy-tailed variables and heterogeneity. High-dimensional\nrobust factor analysis serves as a powerful toolkit to conquer these\nchallenges.\n  This paper gives a selective overview on recent advance on high-dimensional\nfactor models and their applications to statistics including Factor-Adjusted\nRobust Model selection (FarmSelect) and Factor-Adjusted Robust Multiple testing\n(FarmTest). We show that classical methods, especially principal component\nanalysis (PCA), can be tailored to many new problems and provide powerful tools\nfor statistical estimation and inference. We highlight PCA and its connections\nto matrix perturbation theory, robust statistics, random projection, false\ndiscovery rate, etc., and illustrate through several applications how insights\nfrom these fields yield solutions to modern challenges. We also present\nfar-reaching connections between factor models and popular statistical learning\nproblems, including network analysis and low-rank matrix recovery. \n\n"}
{"id": "1808.04308", "contents": "Title: Explaining the Unique Nature of Individual Gait Patterns with Deep\n  Learning Abstract: Machine learning (ML) techniques such as (deep) artificial neural networks\n(DNN) are solving very successfully a plethora of tasks and provide new\npredictive models for complex physical, chemical, biological and social\nsystems. However, in most cases this comes with the disadvantage of acting as a\nblack box, rarely providing information about what made them arrive at a\nparticular prediction. This black box aspect of ML techniques can be\nproblematic especially in medical diagnoses, so far hampering a clinical\nacceptance. The present paper studies the uniqueness of individual gait\npatterns in clinical biomechanics using DNNs. By attributing portions of the\nmodel predictions back to the input variables (ground reaction forces and\nfull-body joint angles), the Layer-Wise Relevance Propagation (LRP) technique\nreliably demonstrates which variables at what time windows of the gait cycle\nare most relevant for the characterisation of gait patterns from a certain\nindividual. By measuring the time-resolved contribution of each input variable\nto the prediction of ML techniques such as DNNs, our method describes the first\ngeneral framework that enables to understand and interpret non-linear ML\nmethods in (biomechanical) gait analysis and thereby supplies a powerful tool\nfor analysis, diagnosis and treatment of human gait. \n\n"}
{"id": "1808.04878", "contents": "Title: Latent Agents in Networks: Estimation and Targeting Abstract: We consider a network of agents. Associated with each agent are her covariate\nand outcome. Agents influence each other's outcomes according to a certain\nconnection/influence structure. A subset of the agents participate on a\nplatform, and hence, are observable to it. The rest are not observable to the\nplatform and are called the latent agents. The platform does not know the\ninfluence structure of the observable or the latent parts of the network. It\nonly observes the data on past covariates and decisions of the observable\nagents. Observable agents influence each other both directly and indirectly\nthrough the influence they exert on the latent agents.\n  We investigate how the platform can estimate the dependence of the observable\nagents' outcomes on their covariates, taking the latent agents into account.\nFirst, we show that this relationship can be succinctly captured by a matrix\nand provide an algorithm for estimating it under a suitable approximate\nsparsity condition using historical data of covariates and outcomes for the\nobservable agents. We also obtain convergence rates for the proposed estimator\ndespite the high dimensionality that allows more agents than observations.\nSecond, we show that the approximate sparsity condition holds under the\nstandard conditions used in the literature. Hence, our results apply to a large\nclass of networks. Finally, we apply our results to two practical settings:\ntargeted advertising and promotional pricing. We show that by using the\navailable historical data with our estimator, it is possible to obtain\nasymptotically optimal advertising/pricing decisions, despite the presence of\nlatent agents. \n\n"}
{"id": "1808.05293", "contents": "Title: Design-based Analysis in Difference-In-Differences Settings with\n  Staggered Adoption Abstract: In this paper we study estimation of and inference for average treatment\neffects in a setting with panel data. We focus on the setting where units,\ne.g., individuals, firms, or states, adopt the policy or treatment of interest\nat a particular point in time, and then remain exposed to this treatment at all\ntimes afterwards. We take a design perspective where we investigate the\nproperties of estimators and procedures given assumptions on the assignment\nprocess. We show that under random assignment of the adoption date the standard\nDifference-In-Differences estimator is is an unbiased estimator of a particular\nweighted average causal effect. We characterize the proeperties of this\nestimand, and show that the standard variance estimator is conservative. \n\n"}
{"id": "1808.05781", "contents": "Title: Inconsistency of diagonal scaling under high-dimensional limit: a\n  replica approach Abstract: In this note, we claim that diagonal scaling of a sample covariance matrix is\nasymptotically inconsistent if the ratio of the dimension to the sample size\nconverges to a positive constant, where population is assumed to be Gaussian\nwith a spike covariance model. Our non-rigorous proof relies on the replica\nmethod developed in statistical physics. In contrast to similar results known\nin literature on principal component analysis, the strong inconsistency is not\nobserved. Numerical experiments support the derived formulas. \n\n"}
{"id": "1808.06418", "contents": "Title: Spillover Effects in Cluster Randomized Trials with Noncompliance Abstract: Cluster randomized trials (CRTs) are popular in public health and in the\nsocial sciences to evaluate a new treatment or policy where the new policy is\nrandomly allocated to clusters of units rather than individual units. CRTs\noften feature both noncompliance, when individuals within a cluster are not\nexposed to the intervention, and individuals within a cluster may influence\neach other through treatment spillovers where those who comply with the new\npolicy may affect the outcomes of those who do not. Here, we study the\nidentification of causal effects in CRTs when both noncompliance and treatment\nspillovers are present. We prove that the standard analysis of CRT data with\nnoncompliance using instrumental variables does not identify the usual complier\naverage causal effect when treatment spillovers are present. We extend this\nresult and show that no analysis of CRT data can unbiasedly estimate local\nnetwork causal effects. Finally, we develop bounds for these causal effects\nunder the assumption that the treatment is not harmful compared to the control.\nWe demonstrate these results with an empirical study of a deworming\nintervention in Kenya. \n\n"}
{"id": "1808.06910", "contents": "Title: Scalable Population Synthesis with Deep Generative Modeling Abstract: Population synthesis is concerned with the generation of synthetic yet\nrealistic representations of populations. It is a fundamental problem in the\nmodeling of transport where the synthetic populations of micro-agents represent\na key input to most agent-based models. In this paper, a new methodological\nframework for how to 'grow' pools of micro-agents is presented. The model\nframework adopts a deep generative modeling approach from machine learning\nbased on a Variational Autoencoder (VAE). Compared to the previous population\nsynthesis approaches, including Iterative Proportional Fitting (IPF), Gibbs\nsampling and traditional generative models such as Bayesian Networks or Hidden\nMarkov Models, the proposed method allows fitting the full joint distribution\nfor high dimensions. The proposed methodology is compared with a conventional\nGibbs sampler and a Bayesian Network by using a large-scale Danish trip diary.\nIt is shown that, while these two methods outperform the VAE in the\nlow-dimensional case, they both suffer from scalability issues when the number\nof modeled attributes increases. It is also shown that the Gibbs sampler\nessentially replicates the agents from the original sample when the required\nconditional distributions are estimated as frequency tables. In contrast, the\nVAE allows addressing the problem of sampling zeros by generating agents that\nare virtually different from those in the original data but have similar\nstatistical properties. The presented approach can support agent-based modeling\nat all levels by enabling richer synthetic populations with smaller zones and\nmore detailed individual characteristics. \n\n"}
{"id": "1808.07319", "contents": "Title: The Scaled Uniform Model Revisited Abstract: Sufficiency, Conditionality and Invariance are basic principles of\nstatistical inference. Current mathematical statistics courses do not devote\nmuch teaching time to these classical principles, and even ignore the latter\ntwo, in order to teach modern methods. However, being the philosophical\ncornerstones of statistical inference, a minimal understanding of these\nprinciples should be part of any curriculum in statistics. The scaled uniform\nmodel is used here to demonstrate the importance and usefulness of the\nprinciples. The main focus is on the conditionality principle that is probably\nthe most basic and less familiar among the three. The appendix discusses the\ninvariance principle and the conditionality principle in the case of sampling\nfrom a finite population. \n\n"}
{"id": "1808.07433", "contents": "Title: Bayesian Estimation of Sparse Spiked Covariance Matrices in High\n  Dimensions Abstract: We propose a Bayesian methodology for estimating spiked covariance matrices\nwith jointly sparse structure in high dimensions. The spiked covariance matrix\nis reparametrized in terms of the latent factor model, where the loading matrix\nis equipped with a novel matrix spike-and-slab LASSO prior, which is a\ncontinuous shrinkage prior for modeling jointly sparse matrices. We establish\nthe rate-optimal posterior contraction for the covariance matrix with respect\nto the operator norm as well as that for the principal subspace with respect to\nthe projection operator norm loss. We also study the posterior contraction rate\nof the principal subspace with respect to the two-to-infinity norm loss, a\nnovel loss function measuring the distance between subspaces that is able to\ncapture element-wise eigenvector perturbations. We show that the posterior\ncontraction rate with respect to the two-to-infinity norm loss is tighter than\nthat with respect to the routinely used projection operator norm loss under\ncertain low-rank and bounded coherence conditions. In addition, a point\nestimator for the principal subspace is proposed with the rate-optimal risk\nbound with respect to the projection operator norm loss. These results are\nbased on a collection of concentration and large deviation inequalities for the\nmatrix spike-and-slab LASSO prior. The numerical performance of the proposed\nmethodology is assessed through synthetic examples and the analysis of a\nreal-world face data example. \n\n"}
{"id": "1808.07915", "contents": "Title: On Efficiency of the Plug-in Principle for Estimating Smooth Integrated\n  Functionals of a Nonincreasing Density Abstract: We consider the problem of estimating smooth integrated functionals of a\nmonotone nonincreasing density $f$ on $[0,\\infty)$ using the nonparametric\nmaximum likelihood based plug-in estimator. We find the exact asymptotic\ndistribution of this natural (tuning parameter-free) plug-in estimator,\nproperly normalized. In particular, we show that the simple plug-in estimator\nis always $\\sqrt{n}$-consistent, and is additionally asymptotically normal with\nzero mean and the semiparametric efficient variance for estimating a subclass\nof integrated functionals. Compared to the previous results on this topic (see\ne.g., Nickl (2007), Gine and Nickl (2008), Jankowski (2014), and Sohl (2015))\nour results hold for a much larger class of functionals (which include linear\nand non-linear functionals) under less restrictive assumptions on the\nunderlying $f$ --- we do not require $f$ to be (i) smooth, (ii) bounded away\nfrom $0$, or (iii) compactly supported. Further, when $f$ is the uniform\ndistribution on a compact interval we explicitly characterize the asymptotic\ndistribution of the plug-in estimator --- which now converges at a non-standard\nrate --- thereby extending the results in Groeneboom and Pyke (1983) for the\ncase of the quadratic functional. \n\n"}
{"id": "1808.08195", "contents": "Title: GoT-WAVE: Temporal network alignment using graphlet-orbit transitions Abstract: Global pairwise network alignment (GPNA) aims to find a one-to-one node\nmapping between two networks that identifies conserved network regions. GPNA\nalgorithms optimize node conservation (NC) and edge conservation (EC). NC\nquantifies topological similarity between nodes. Graphlet-based degree vectors\n(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were\nused as a dynamic NC measure within the first-ever algorithms for GPNA of\ntemporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger\nnetworks. We recently developed a different graphlet-based measure of temporal\nnode similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead\nof DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new\napproach, GoT-WAVE.\n  On synthetic networks, GoT-WAVE improves DynaWAVE's accuracy by 25% and speed\nby 64%. On real networks, when optimizing only dynamic NC, each method is\nsuperior ~50% of the time. While DynaWAVE benefits more from also optimizing\ndynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a\npromising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.\nFuture work on better incorporating dynamic EC may yield further improvements. \n\n"}
{"id": "1808.08766", "contents": "Title: Learning behavioral context recognition with multi-stream temporal\n  convolutional networks Abstract: Smart devices of everyday use (such as smartphones and wearables) are\nincreasingly integrated with sensors that provide immense amounts of\ninformation about a person's daily life such as behavior and context. The\nautomatic and unobtrusive sensing of behavioral context can help develop\nsolutions for assisted living, fitness tracking, sleep monitoring, and several\nother fields. Towards addressing this issue, we raise the question: can a\nmachine learn to recognize a diverse set of contexts and activities in a\nreal-life through joint learning from raw multi-modal signals (e.g.\naccelerometer, gyroscope and audio etc.)? In this paper, we propose a\nmulti-stream temporal convolutional network to address the problem of\nmulti-label behavioral context recognition. A four-stream network architecture\nhandles learning from each modality with a contextualization module which\nincorporates extracted representations to infer a user's context. Our empirical\nevaluation suggests that a deep convolutional network trained end-to-end\nachieves an optimal recognition rate. Furthermore, the presented architecture\ncan be extended to include similar sensors for performance improvements and\nhandles missing modalities through multi-task learning without any manual\nfeature engineering on highly imbalanced and sparsely labeled dataset. \n\n"}
{"id": "1808.08811", "contents": "Title: Exponential inequalities for nonstationary Markov Chains Abstract: Exponential inequalities are main tools in machine learning theory. To prove\nexponential inequalities for non i.i.d random variables allows to extend many\nlearning techniques to these variables. Indeed, much work has been done both on\ninequalities and learning theory for time series, in the past 15 years.\nHowever, for the non independent case, almost all the results concern\nstationary time series. This excludes many important applications: for example\nany series with a periodic behavior is non-stationary. In this paper, we extend\nthe basic tools of Dedecker and Fan (2015) to nonstationary Markov chains. As\nan application, we provide a Bernstein-type inequality, and we deduce risk\nbounds for the prediction of periodic autoregressive processes with an unknown\nperiod. \n\n"}
{"id": "1808.08871", "contents": "Title: B\\'ezierGAN: Automatic Generation of Smooth Curves from Interpretable\n  Low-Dimensional Parameters Abstract: Many real-world objects are designed by smooth curves, especially in the\ndomain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and\nhydrodynamic shapes (e.g., hulls) are designed. To facilitate the design\nprocess of those objects, we propose a deep learning based generative model\nthat can synthesize smooth curves. The model maps a low-dimensional latent\nrepresentation to a sequence of discrete points sampled from a rational\nB\\'ezier curve. We demonstrate the performance of our method in completing both\nsynthetic and real-world generative tasks. Results show that our method can\ngenerate diverse and realistic curves, while preserving consistent shape\nvariation in the latent space, which is favorable for latent space design\noptimization or design space exploration. \n\n"}
{"id": "1808.09748", "contents": "Title: On spike and slab empirical Bayes multiple testing Abstract: This paper explores a connection between empirical Bayes posterior\ndistributions and false discovery rate (FDR) control. In the Gaussian sequence\nmodel, this work shows that empirical Bayes-calibrated spike and slab posterior\ndistributions allow a correct FDR control under sparsity. Doing so, it offers a\nfrequentist theoretical validation of empirical Bayes methods in the context of\nmultiple testing. Our theoretical results are illustrated with numerical\nexperiments. \n\n"}
{"id": "1808.10660", "contents": "Title: Sup-norm adaptive simultaneous drift estimation for ergodic diffusions Abstract: We consider the question of estimating the drift and the invariant density\nfor a large class of scalar ergodic diffusion processes, based on continuous\nobservations, in $\\sup$-norm loss. The unknown drift $b$ is supposed to belong\nto a nonparametric class of smooth functions of unknown order. We suggest an\nadaptive approach which allows to construct drift estimators attaining minimax\noptimal $\\sup$-norm rates of convergence. In addition, we prove a Donsker\ntheorem for the classical kernel estimator of the invariant density and\nestablish its semiparametric efficiency. Finally, we combine both results and\npropose a fully data-driven bandwidth selection procedure which simultaneously\nyields both a rate-optimal drift estimator and an asymptotically efficient\nestimator of the invariant density of the diffusion. Crucial tool for our\ninvestigation are uniform exponential inequalities for empirical processes of\ndiffusions. \n\n"}
{"id": "1808.10669", "contents": "Title: Determining the signal dimension in second order source separation Abstract: While an important topic in practice, the estimation of the number of\nnon-noise components in blind source separation has received little attention\nin the literature. Recently, two bootstrap-based techniques for estimating the\ndimension were proposed, and although very efficient, they suffer from the long\ncomputation times caused by the resampling. We approach the problem from a\nlarge sample viewpoint and develop an asymptotic test for the true dimension.\nOur test statistic based on second-order temporal information has a very simple\nlimiting distribution under the null hypothesis and requires no parameters to\nestimate. Comparisons to the resampling-based estimates show that the\nasymptotic test provides comparable error rates with significantly faster\ncomputation time. An application to sound recording data is used to illustrate\nthe method in practice. \n\n"}
{"id": "1808.10828", "contents": "Title: On Second Order Conditions in the Multivariate Block Maxima and Peak\n  over Threshold Method Abstract: Second order conditions provide a natural framework for establishing\nasymptotic results about estimators for tail related quantities. Such\nconditions are typically tailored to the estimation principle at hand, and may\nbe vastly different for estimators based on the block maxima (BM) method or the\npeak-over-threshold (POT) approach. In this paper we provide details on the\nrelationship between typical second order conditions for BM and POT methods in\nthe multivariate case. We show that the two conditions typically imply each\nother, but with a possibly different second order parameter. The latter implies\nthat, depending on the data generating process, one of the two methods can\nattain faster convergence rates than the other. The class of multivariate\nArchimax copulas is examined in detail; we find that this class contains models\nfor which the second order parameter is smaller for the BM method and vice\nversa. The theory is illustrated by a small simulation study. \n\n"}
{"id": "1809.01606", "contents": "Title: Determining the Dependence Structure of Multivariate Extremes Abstract: In multivariate extreme value analysis, the nature of the extremal dependence\nbetween variables should be considered when selecting appropriate statistical\nmodels. Interest often lies with determining which subsets of variables can\ntake their largest values simultaneously, while the others are of smaller\norder. Our approach to this problem exploits hidden regular variation\nproperties on a collection of non-standard cones and provides a new set of\nindices that reveal aspects of the extremal dependence structure not available\nthrough existing measures of dependence. We derive theoretical properties of\nthese indices, demonstrate their value through a series of examples, and\ndevelop methods of inference that also estimate the proportion of extremal mass\nassociated with each cone. We apply the methods to UK river flows, estimating\nthe probabilities of different subsets of sites being large simultaneously. \n\n"}
{"id": "1809.02686", "contents": "Title: Multiresolution analysis and adaptive estimation on a sphere using\n  stereographic wavelets Abstract: We construct an adaptive estimator of a density function on $d$ dimensional\nunit sphere $S^d$ ($d \\geq 2 $), using a new type of spherical frames. The\nframes, or as we call them, stereografic wavelets are obtained by transforming\na wavelet system, namely Daubechies, using some stereographic operators. We\nprove that our estimator achieves an optimal rate of convergence on some Besov\ntype class of functions by adapting to unknown smoothness. Our new construction\nof stereografic wavelet system gives us a multiresolution approximation of\n$L^2(S^d)$ which can be used in many approximation and estimation problems. In\nthis paper we also demonstrate how to implement the density estimator in $S^2$\nand we present a finite sample behavior of that estimator in a numerical\nexperiment. \n\n"}
{"id": "1809.03145", "contents": "Title: Optimal variable selection and adaptive noisy Compressed Sensing Abstract: In the context of high-dimensional linear regression models, we propose an\nalgorithm of exact support recovery in the setting of noisy compressed sensing\nwhere all entries of the design matrix are independent and identically\ndistributed standard Gaussian. This algorithm achieves the same conditions of\nexact recovery as the exhaustive search (maximal likelihood) decoder, and has\nan advantage over the latter of being adaptive to all parameters of the problem\nand computable in polynomial time. The core of our analysis consists in the\nstudy of the non-asymptotic minimax Hamming risk of variable selection. This\nallows us to derive a procedure, which is nearly optimal in a non-asymptotic\nminimax sense. Then, we develop its adaptive version, and propose a robust\nvariant of the method to handle datasets with outliers and heavy-tailed\ndistributions of observations. The resulting polynomial time procedure is near\noptimal, adaptive to all parameters of the problem and also robust. \n\n"}
{"id": "1809.03550", "contents": "Title: Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and\n  Measurement Noise Abstract: In tracking of time-varying low-rank models of time-varying matrices, we\npresent a method robust to both uniformly-distributed measurement noise and\narbitrarily-distributed ``sparse'' noise. In theory, we bound the tracking\nerror. In practice, our use of randomised coordinate descent is scalable and\nallows for encouraging results on changedetection net, a benchmark. \n\n"}
{"id": "1809.03754", "contents": "Title: The reproducing kernel Hilbert space approach in nonparametric\n  regression problems with correlated observations Abstract: In this paper we investigate the problem of estimating the regression\nfunction in models with correlated observations. The data is obtained from\nseveral experimental units each of them forms a time series. We propose a new\nestimator based on the inverse of the autocovariance matrix of the\nobservations, assumed known and invertible. Using the properties of the\nReproducing Kernel Hilbert spaces, we give the asymptotic expressions of its\nbias and its variance. In addition, we give a theoretical comparison, by\ncalculating the IMSE, between this new estimator and the classical one proposed\nby Gasser and Muller. Finally, we conduct a simulation study to investigate the\nperformance of the proposed estimator and to compare it to the Gasser and\nMuller's estimator in a finite sample set. \n\n"}
{"id": "1809.05262", "contents": "Title: Network Recasting: A Universal Method for Network Architecture\n  Transformation Abstract: This paper proposes network recasting as a general method for network\narchitecture transformation. The primary goal of this method is to accelerate\nthe inference process through the transformation, but there can be many other\npractical applications. The method is based on block-wise recasting; it recasts\neach source block in a pre-trained teacher network to a target block in a\nstudent network. For the recasting, a target block is trained such that its\noutput activation approximates that of the source block. Such a block-by-block\nrecasting in a sequential manner transforms the network architecture while\npreserving the accuracy. This method can be used to transform an arbitrary\nteacher network type to an arbitrary student network type. It can even generate\na mixed-architecture network that consists of two or more types of block. The\nnetwork recasting can generate a network with fewer parameters and/or\nactivations, which reduce the inference time significantly. Naturally, it can\nbe used for network compression by recasting a trained network into a smaller\nnetwork of the same type. Our experiments show that it outperforms previous\ncompression approaches in terms of actual speedup on a GPU. \n\n"}
{"id": "1809.05596", "contents": "Title: The Generic Holdout: Preventing False-Discoveries in Adaptive Data\n  Science Abstract: Adaptive data analysis has posed a challenge to science due to its ability to\ngenerate false hypotheses on moderately large data sets. In general, with\nnon-adaptive data analyses (where queries to the data are generated without\nbeing influenced by answers to previous queries) a data set containing $n$\nsamples may support exponentially many queries in $n$. This number reduces to\nlinearly many under naive adaptive data analysis, and even sophisticated\nremedies such as the Reusable Holdout (Dwork et. al 2015) only allow\nquadratically many queries in $n$.\n  In this work, we propose a new framework for adaptive science which\nexponentially improves on this number of queries under a restricted yet\nscientifically relevant setting, where the goal of the scientist is to find a\nsingle (or a few) true hypotheses about the universe based on the samples. Such\na setting may describe the search for predictive factors of some disease based\non medical data, where the analyst may wish to try a number of predictive\nmodels until a satisfactory one is found.\n  Our solution, the Generic Holdout, involves two simple ingredients: (1) a\npartitioning of the data into a exploration set and a holdout set and (2) a\nlimited exposure strategy for the holdout set. An analyst is free to use the\nexploration set arbitrarily, but when testing hypotheses against the holdout\nset, the analyst only learns the answer to the question: \"Is the given\nhypothesis true (empirically) on the holdout set?\" -- and no more information,\nsuch as \"how well\" the hypothesis fits the holdout set. The resulting scheme is\nimmediate to analyze, but despite its simplicity we do not believe our method\nis obvious, as evidenced by the many violations in practice.\n  Our proposal can be seen as an alternative to pre-registration, and allows\nresearchers to get the benefits of adaptive data analysis without the problems\nof adaptivity. \n\n"}
{"id": "1809.06045", "contents": "Title: Building Prior Knowledge: A Markov Based Pedestrian Prediction Model\n  Using Urban Environmental Data Abstract: Autonomous Vehicles navigating in urban areas have a need to understand and\npredict future pedestrian behavior for safer navigation. This high level of\nsituational awareness requires observing pedestrian behavior and extrapolating\ntheir positions to know future positions. While some work has been done in this\nfield using Hidden Markov Models (HMMs), one of the few observed drawbacks of\nthe method is the need for informed priors for learning behavior. In this work,\nan extension to the Growing Hidden Markov Model (GHMM) method is proposed to\nsolve some of these drawbacks. This is achieved by building on existing work\nusing potential cost maps and the principle of Natural Vision. As a\nconsequence, the proposed model is able to predict pedestrian positions more\nprecisely over a longer horizon compared to the state of the art. The method is\ntested over \"legal\" and \"illegal\" behavior of pedestrians, having trained the\nmodel with sparse observations and partial trajectories. The method, with no\ntraining data, is compared against a trained state of the art model. It is\nobserved that the proposed method is robust even in new, previously unseen\nareas. \n\n"}
{"id": "1809.06092", "contents": "Title: Testing relevant hypotheses in functional time series via\n  self-normalization Abstract: In this paper we develop methodology for testing relevant hypotheses about\nfunctional time series in a tuning-free way. Instead of testing for exact\nequality, for example for the equality of two mean functions from two\nindependent time series, we propose to test the null hypothesis of no relevant\ndeviation. In the two sample problem this means that an $L^2$-distance between\nthe two mean functions is smaller than a pre-specified threshold. For such\nhypotheses self-normalization, which was introduced by Shao (2010) and Shao and\nZhang (2010) and is commonly used to avoid the estimation of nuisance\nparameters, is not directly applicable. We develop new self-normalized\nprocedures for testing relevant hypotheses in the one sample, two sample and\nchange point problem and investigate their asymptotic properties. Finite sample\nproperties of the proposed tests are illustrated by means of a simulation study\nand data examples. Our main focus is on functional time series, but extensions\nto other settings are also briefly discussed. \n\n"}
{"id": "1809.06222", "contents": "Title: GANs for Medical Image Analysis Abstract: Generative Adversarial Networks (GANs) and their extensions have carved open\nmany exciting ways to tackle well known and challenging medical image analysis\nproblems such as medical image de-noising, reconstruction, segmentation, data\nsimulation, detection or classification. Furthermore, their ability to\nsynthesize images at unprecedented levels of realism also gives hope that the\nchronic scarcity of labeled data in the medical field can be resolved with the\nhelp of these generative models. In this review paper, a broad overview of\nrecent literature on GANs for medical applications is given, the shortcomings\nand opportunities of the proposed methods are thoroughly discussed and\npotential future work is elaborated. We review the most relevant papers\npublished until the submission date. For quick access, important details such\nas the underlying method, datasets and performance are tabulated. An\ninteractive visualization which categorizes all papers to keep the review\nalive, is available at\nhttp://livingreview.in.tum.de/GANs_for_Medical_Applications. \n\n"}
{"id": "1809.07541", "contents": "Title: Admissibility of the usual confidence set for the mean of a univariate\n  or bivariate normal population: The unknown-variance case Abstract: In the Gaussian linear regression model (with unknown mean and variance), we\nshow that the standard confidence set for one or two regression coefficients is\nadmissible in the sense of Joshi (1969). This solves a long-standing open\nproblem in mathematical statistics, and this has important implications on the\nperformance of modern inference procedures post-model-selection or\npost-shrinkage, particularly in situations where the number of parameters is\nlarger than the sample size. As a technical contribution of independent\ninterest, we introduce a new class of conjugate priors for the Gaussian\nlocation-scale model. \n\n"}
{"id": "1809.08327", "contents": "Title: Quantifying total uncertainty in physics-informed neural networks for\n  solving forward and inverse stochastic problems Abstract: Physics-informed neural networks (PINNs) have recently emerged as an\nalternative way of solving partial differential equations (PDEs) without the\nneed of building elaborate grids, instead, using a straightforward\nimplementation. In particular, in addition to the deep neural network (DNN) for\nthe solution, a second DNN is considered that represents the residual of the\nPDE. The residual is then combined with the mismatch in the given data of the\nsolution in order to formulate the loss function. This framework is effective\nbut is lacking uncertainty quantification of the solution due to the inherent\nrandomness in the data or due to the approximation limitations of the DNN\narchitecture. Here, we propose a new method with the objective of endowing the\nDNN with uncertainty quantification for both sources of uncertainty, i.e., the\nparametric uncertainty and the approximation uncertainty. We first account for\nthe parametric uncertainty when the parameter in the differential equation is\nrepresented as a stochastic process. Multiple DNNs are designed to learn the\nmodal functions of the arbitrary polynomial chaos (aPC) expansion of its\nsolution by using stochastic data from sparse sensors. We can then make\npredictions from new sensor measurements very efficiently with the trained\nDNNs. Moreover, we employ dropout to correct the over-fitting and also to\nquantify the uncertainty of DNNs in approximating the modal functions. We then\ndesign an active learning strategy based on the dropout uncertainty to place\nnew sensors in the domain to improve the predictions of DNNs. Several numerical\ntests are conducted for both the forward and the inverse problems to quantify\nthe effectiveness of PINNs combined with uncertainty quantification. This\nNN-aPC new paradigm of physics-informed deep learning with uncertainty\nquantification can be readily applied to other types of stochastic PDEs in\nmulti-dimensions. \n\n"}
{"id": "1809.09879", "contents": "Title: Learning random points from geometric graphs or orderings Abstract: Suppose that there is a family of $n$ random points $X_v$ for $v \\in V$,\nindependently and uniformly distributed in the square\n$\\left[-\\sqrt{n}/2,\\sqrt{n}/2\\right]^2$ of area $n$. We do not see these\npoints, but learn about them in one of the following two ways.\n  Suppose first that we are given the corresponding random geometric graph $G$,\nwhere distinct vertices $u$ and $v$ are adjacent when the Euclidean distance\n$d_E(X_u,X_v)$ is at most $r$. If the threshold distance $r$ satisfies\n$n^{3/14} \\ll r \\ll n^{1/2}$, then the following holds with high probability.\nGiven the graph $G$ (without any geometric information), in polynomial time we\ncan approximately reconstruct the hidden embedding, in the sense that, `up to\nsymmetries', for each vertex $v$ we find a point within distance about $r$ of\n$X_v$; that is, we find an embedding with `displacement' at most about $r$.\n  Now suppose that, instead of being given the graph $G$, we are given, for\neach vertex $v$, the ordering of the other vertices by increasing Euclidean\ndistance from $v$. Then, with high probability, in polynomial time we can find\nan embedding with the much smaller displacement error $O(\\sqrt{\\log n})$. \n\n"}
{"id": "1809.09881", "contents": "Title: Boosting Functional Response Models for Location, Scale and Shape with\n  an Application to Bacterial Competition Abstract: We extend Generalized Additive Models for Location, Scale, and Shape (GAMLSS)\nto regression with functional response. This allows us to simultaneously model\npoint-wise mean curves, variances and other distributional parameters of the\nresponse in dependence of various scalar and functional covariate effects. In\naddition, the scope of distributions is extended beyond exponential families.\nThe model is fitted via gradient boosting, which offers inherent model\nselection and is shown to be suitable for both complex model structures and\nhighly auto-correlated response curves. This enables us to analyze bacterial\ngrowth in \\textit{Escherichia coli} in a complex interaction scenario,\nfruitfully extending usual growth models. \n\n"}
{"id": "1809.09934", "contents": "Title: Moment ideals of local Dirac mixtures Abstract: In this paper we study ideals arising from moments of local Dirac measures\nand their mixtures. We provide generators for the case of first order local\nDiracs and explain how to obtain the moment ideal of the Pareto distribution\nfrom them. We then use elimination theory and Prony's method for parameter\nestimation of finite mixtures. Our results are showcased with applications in\nsignal processing and statistics. We highlight the natural connections to\nalgebraic statistics, combinatorics and applications in analysis throughout the\npaper. \n\n"}
{"id": "1809.10455", "contents": "Title: Statistical dependence: Beyond Pearson's $\\rho$ Abstract: Pearson's $\\rho$ is the most used measure of statistical dependence. It gives\na complete characterization of dependence in the Gaussian case, and it also\nworks well in some non-Gaussian situations. It is well known, however, that it\nhas a number of shortcomings; in particular for heavy tailed distributions and\nin nonlinear situations, where it may produce misleading, and even disastrous\nresults. In recent years a number of alternatives have been proposed. In this\npaper, we will survey these developments, especially results obtained in the\nlast couple of decades. Among measures discussed are the copula,\ndistribution-based measures, the distance covariance, the HSIC measure popular\nin machine learning, and finally the local Gaussian correlation, which is a\nlocal version of Pearson's $\\rho$. Throughout we put the emphasis on conceptual\ndevelopments and a comparison of these. We point out relevant references to\ntechnical details as well as comparative empirical and simulated experiments.\nThere is a broad selection of references under each topic treated. \n\n"}
{"id": "1809.10652", "contents": "Title: Inference for Individual Mediation Effects and Interventional Effects in\n  Sparse High-Dimensional Causal Graphical Models Abstract: We consider the problem of identifying intermediate variables (or mediators)\nthat regulate the effect of a treatment on a response variable. While there has\nbeen significant research on this classical topic, little work has been done\nwhen the set of potential mediators is high-dimensional (HD). A further\ncomplication arises when these mediators are interrelated (with unknown\ndependencies). In particular, we assume that the causal structure of the\ntreatment, the confounders, the potential mediators and the response is a\n(possibly unknown) directed acyclic graph (DAG). HD DAG models have previously\nbeen used for the estimation of causal effects from observational data. In\nparticular, methods called IDA and joint-IDA have been developed for estimating\nthe effects of single and multiple simultaneous interventions, respectively. In\nthis paper, we propose an IDA-type method called MIDA for estimating so-called\nindividual mediation effects from HD observational data. Although IDA and\njoint-IDA estimators have been shown to be consistent in certain sparse HD\nsettings, their asymptotic properties such as convergence in distribution and\ninferential tools in such settings have remained unknown. In this paper, we\nprove HD consistency of MIDA for linear structural equation models with\nsub-Gaussian errors. More importantly, we derive distributional convergence\nresults for MIDA in similar HD settings, which are applicable to IDA and\njoint-IDA estimators as well. To our knowledge, these are the first such\ndistributional convergence results facilitating inference for IDA-type\nestimators. These are built on our novel theoretical results regarding uniform\nbounds for linear regression estimators over varying subsets of HD covariates\nwhich may be of independent interest. Finally, we empirically validate our\nasymptotic theory for MIDA and demonstrate its usefulness via simulations and a\nreal data application. \n\n"}
{"id": "1809.10851", "contents": "Title: Encoding Robust Representation for Graph Generation Abstract: Generative networks have made it possible to generate meaningful signals such\nas images and texts from simple noise. Recently, generative methods based on\nGAN and VAE were developed for graphs and graph signals. However, the\nmathematical properties of these methods are unclear, and training good\ngenerative models is difficult. This work proposes a graph generation model\nthat uses a recent adaptation of Mallat's scattering transform to graphs. The\nproposed model is naturally composed of an encoder and a decoder. The encoder\nis a Gaussianized graph scattering transform, which is robust to signal and\ngraph manipulation. The decoder is a simple fully connected network that is\nadapted to specific tasks, such as link prediction, signal generation on graphs\nand full graph and signal generation. The training of our proposed system is\nefficient since it is only applied to the decoder and the hardware requirements\nare moderate. Numerical results demonstrate state-of-the-art performance of the\nproposed system for both link prediction and graph and signal generation. \n\n"}
{"id": "1810.00297", "contents": "Title: Spectral gaps and error estimates for infinite-dimensional\n  Metropolis-Hastings with non-Gaussian priors Abstract: We study a class of Metropolis-Hastings algorithms for target measures that\nare absolutely continuous with respect to a large class of non-Gaussian prior\nmeasures on Banach spaces. The algorithm is shown to have a spectral gap in a\nWasserstein-like semimetric weighted by a Lyapunov function. A number of error\nbounds are given for computationally tractable approximations of the algorithm\nincluding bounds on the closeness of Ces\\'aro averages and other pathwise\nquantities via perturbation theory. Several applications illustrate the breadth\nof problems to which the results apply such as various likelihood\napproximations and perturbations of prior measures. \n\n"}
{"id": "1810.00363", "contents": "Title: A Kernel Perspective for Regularizing Deep Neural Networks Abstract: We propose a new point of view for regularizing deep neural networks by using\nthe norm of a reproducing kernel Hilbert space (RKHS). Even though this norm\ncannot be computed, it admits upper and lower approximations leading to various\npractical strategies. Specifically, this perspective (i) provides a common\numbrella for many existing regularization principles, including spectral norm\nand gradient penalties, or adversarial training, (ii) leads to new effective\nregularization penalties, and (iii) suggests hybrid strategies combining lower\nand upper bounds to get better approximations of the RKHS norm. We\nexperimentally show this approach to be effective when learning on small\ndatasets, or to obtain adversarially robust models. \n\n"}
{"id": "1810.01509", "contents": "Title: Hierarchical community detection by recursive partitioning Abstract: The problem of community detection in networks is usually formulated as\nfinding a single partition of the network into some \"correct\" number of\ncommunities. We argue that it is more interpretable and in some regimes more\naccurate to construct a hierarchical tree of communities instead. This can be\ndone with a simple top-down recursive partitioning algorithm, starting with a\nsingle community and separating the nodes into two communities by spectral\nclustering repeatedly, until a stopping rule suggests there are no further\ncommunities. This class of algorithms is model-free, computationally efficient,\nand requires no tuning other than selecting a stopping rule. We show that there\nare regimes where this approach outperforms K-way spectral clustering, and\npropose a natural framework for analyzing the algorithm's theoretical\nperformance, the binary tree stochastic block model. Under this model, we prove\nthat the algorithm correctly recovers the entire community tree under\nrelatively mild assumptions. We apply the algorithm to a gene network based on\ngene co-occurrence in 1580 research papers on anemia, and identify six clusters\nof genes in a meaningful hierarchy. We also illustrate the algorithm on a\ndataset of statistics papers. \n\n"}
{"id": "1810.01720", "contents": "Title: Sum decomposition of divergence into three divergences Abstract: Divergence functions play a key role as to measure the discrepancy between\ntwo points in the field of machine learning, statistics and signal processing.\nWell-known divergences are the Bregman divergences, the Jensen divergences and\nthe f-divergences. In this paper, we show that the symmetric Bregman divergence\ncan be decomposed into the sum of two types of Jensen divergences and the\nBregman divergence. Furthermore, applying this result, we show another sum\ndecomposition of divergence is possible which includes f-divergences\nexplicitly. \n\n"}
{"id": "1810.01724", "contents": "Title: A Nonparametric Approach to High-dimensional k-sample Comparison\n  Problems Abstract: High-dimensional k-sample comparison is a common applied problem. We\nconstruct a class of easy-to-implement nonparametric distribution-free tests\nbased on new tools and unexplored connections with spectral graph theory. The\ntest is shown to possess various desirable properties along with a\ncharacteristic exploratory flavor that has practical consequences. The\nnumerical examples show that our method works surprisingly well under a broad\nrange of realistic situations. \n\n"}
{"id": "1810.02016", "contents": "Title: The Four Point Permutation Test for Latent Block Structure in Incidence\n  Matrices Abstract: Transactional data may be represented as a bipartite graph $G:=(L \\cup R,\nE)$, where $L$ denotes agents, $R$ denotes objects visible to many agents, and\nan edge in $E$ denotes an interaction between an agent and an object.\nUnsupervised learning seeks to detect block structures in the adjacency matrix\n$Z$ between $L$ and $R$, thus grouping together sets of agents with similar\nobject interactions. New results on quasirandom permutations suggest a\nnon-parametric \\textbf{four point test} to measure the amount of block\nstructure in $G$, with respect to vertex orderings on $L$ and $R$. Take\ndisjoint 4-edge random samples, order these four edges by left endpoint, and\ncount the relative frequencies of the $4!$ possible orderings of the right\nendpoint. When these orderings are equiprobable, the edge set $E$ corresponds\nto a quasirandom permutation $\\pi$ of $|E|$ symbols. Total variation distance\nof the relative frequency vector away from the uniform distribution on 24\npermutations measures the amount of block structure. Such a test statistic,\nbased on $\\lfloor |E|/4 \\rfloor$ samples, is computable in $O(|E|/p)$ time on\n$p$ processors. Possibly block structure may be enhanced by precomputing\n\\textbf{natural orders} on $L$ and $R$, related to the second eigenvector of\ngraph Laplacians. In practice this takes $O(d |E|)$ time, where $d$ is the\ngraph diameter. Five open problems are described. \n\n"}
{"id": "1810.02991", "contents": "Title: Robust and Efficient Estimation in the Parametric Cox Regression Model\n  under Random Censoring Abstract: Cox proportional hazard regression model is a popular tool to analyze the\nrelationship between a censored lifetime variable with other relevant factors.\nThe semi-parametric Cox model is widely used to study different types of data\narising from applied disciplines like medical science, biology, reliability\nstudies and many more. A fully parametric version of the Cox regression model,\nif properly specified, can yield more efficient parameter estimates leading to\nbetter insight generation. However, the existing maximum likelihood approach of\ngenerating inference under the fully parametric Cox regression model is highly\nnon-robust against data-contamination which restricts its practical usage. In\nthis paper we develop a robust estimation procedure for the parametric Cox\nregression model based on the minimum density power divergence approach. The\nproposed minimum density power divergence estimator is seen to produce highly\nrobust estimates under data contamination with only a slight loss in efficiency\nunder pure data. Further, they are always seen to generate more precise\ninference than the likelihood based estimates under the semi-parametric Cox\nmodels or their existing robust versions. We also sketch the derivation of the\nasymptotic properties of the proposed estimator using the martingale approach\nand justify their robustness theoretically through the influence function\nanalysis. The practical applicability and usefulness of the proposal are\nillustrated through simulations and a real data example. \n\n"}
{"id": "1810.03260", "contents": "Title: Visually Communicating and Teaching Intuition for Influence Functions Abstract: Estimators based on influence functions (IFs) have been shown to be effective\nin many settings, especially when combined with machine learning techniques. By\nfocusing on estimating a specific target of interest (e.g., the average effect\nof a treatment), rather than on estimating the full underlying data generating\ndistribution, IF-based estimators are often able to achieve asymptotically\noptimal mean-squared error. Still, many researchers find IF-based estimators to\nbe opaque or overly technical, which makes their use less prevalent and their\nbenefits less available. To help foster understanding and trust in IF-based\nestimators, we present tangible, visual illustrations of when and how IF-based\nestimators can outperform standard ``plug-in'' estimators. The figures we show\nare based on connections between IFs, gradients, linear approximations, and\nNewton-Raphson. \n\n"}
{"id": "1810.04090", "contents": "Title: Statistical Convergence of the EM Algorithm on Gaussian Mixture Models Abstract: We study the convergence behavior of the Expectation Maximization (EM)\nalgorithm on Gaussian mixture models with an arbitrary number of mixture\ncomponents and mixing weights. We show that as long as the means of the\ncomponents are separated by at least $\\Omega(\\sqrt{\\min\\{M,d\\}})$, where $M$ is\nthe number of components and $d$ is the dimension, the EM algorithm converges\nlocally to the global optimum of the log-likelihood. Further, we show that the\nconvergence rate is linear and characterize the size of the basin of attraction\nto the global optimum. \n\n"}
{"id": "1810.04617", "contents": "Title: Testing Community Structures for Hypergraphs Abstract: Many complex networks in real world can be formulated as hypergraphs where\ncommunity detection has been widely used. However, the fundamental question of\nwhether communities exist or not in an observed hypergraph still remains\nunresolved. The aim of the present paper is to tackle this important problem.\nSpecifically, we study when a hypergraph with community structure can be\nsuccessfully distinguished from its Erd\\\"{o}s-Renyi counterpart, and propose\nconcrete test statistics based on hypergraph cycles when the models are\ndistinguishable. Our contributions are summarized as follows. For uniform\nhypergraphs, we show that successful testing is always impossible when average\ndegree tends to zero, might be possible when average degree is bounded, and is\npossible when average degree is growing. We obtain asymptotic distributions of\nthe proposed test statistics and analyze their power. Our results for growing\ndegree case are further extended to nonuniform hypergraphs in which a new test\ninvolving both edge and hyperedge information is proposed. The novel aspect of\nour new test is that it is provably more powerful than the classic test\ninvolving only edge information. Simulation and real data analysis support our\ntheoretical findings. The proofs rely on Janson's contiguity theory\n(\\cite{J95}) and a high-moments driven asymptotic normality result by Gao and\nWormald (\\cite{GWALD}). \n\n"}
{"id": "1810.04859", "contents": "Title: Policy Design for Active Sequential Hypothesis Testing using Deep\n  Learning Abstract: Information theory has been very successful in obtaining performance limits\nfor various problems such as communication, compression and hypothesis testing.\nLikewise, stochastic control theory provides a characterization of optimal\npolicies for Partially Observable Markov Decision Processes (POMDPs) using\ndynamic programming. However, finding optimal policies for these problems is\ncomputationally hard in general and thus, heuristic solutions are employed in\npractice. Deep learning can be used as a tool for designing better heuristics\nin such problems. In this paper, the problem of active sequential hypothesis\ntesting is considered. The goal is to design a policy that can reliably infer\nthe true hypothesis using as few samples as possible by adaptively selecting\nappropriate queries. This problem can be modeled as a POMDP and bounds on its\nvalue function exist in literature. However, optimal policies have not been\nidentified and various heuristics are used. In this paper, two new heuristics\nare proposed: one based on deep reinforcement learning and another based on a\nKL-divergence zero-sum game. These heuristics are compared with\nstate-of-the-art solutions and it is demonstrated using numerical experiments\nthat the proposed heuristics can achieve significantly better performance than\nexisting methods in some scenarios. \n\n"}
{"id": "1810.05148", "contents": "Title: Bayesian Deep Convolutional Networks with Many Channels are Gaussian\n  Processes Abstract: There is a previously identified equivalence between wide fully connected\nneural networks (FCNs) and Gaussian processes (GPs). This equivalence enables,\nfor instance, test set predictions that would have resulted from a fully\nBayesian, infinitely wide trained FCN to be computed without ever instantiating\nthe FCN, but by instead evaluating the corresponding GP. In this work, we\nderive an analogous equivalence for multi-layer convolutional neural networks\n(CNNs) both with and without pooling layers, and achieve state of the art\nresults on CIFAR10 for GPs without trainable kernels. We also introduce a Monte\nCarlo method to estimate the GP corresponding to a given neural network\narchitecture, even in cases where the analytic form has too many terms to be\ncomputationally feasible.\n  Surprisingly, in the absence of pooling layers, the GPs corresponding to CNNs\nwith and without weight sharing are identical. As a consequence, translation\nequivariance, beneficial in finite channel CNNs trained with stochastic\ngradient descent (SGD), is guaranteed to play no role in the Bayesian treatment\nof the infinite channel limit - a qualitative difference between the two\nregimes that is not present in the FCN case. We confirm experimentally, that\nwhile in some scenarios the performance of SGD-trained finite CNNs approaches\nthat of the corresponding GPs as the channel count increases, with careful\ntuning SGD-trained CNNs can significantly outperform their corresponding GPs,\nsuggesting advantages from SGD training compared to fully Bayesian parameter\nestimation. \n\n"}
{"id": "1810.05374", "contents": "Title: Limitations of \"Limitations of Bayesian leave-one-out cross-validation\n  for model selection\" Abstract: This article is an invited discussion of the article by Gronau and\nWagenmakers (2018) that can be found at\nhttps://dx.doi.org/10.1007/s42113-018-0011-7. \n\n"}
{"id": "1810.05478", "contents": "Title: Interplay of minimax estimation and minimax support recovery under\n  sparsity Abstract: In this paper, we study a new notion of scaled minimaxity for sparse\nestimation in high-dimensional linear regression model. We present more\noptimistic lower bounds than the one given by the classical minimax theory and\nhence improve on existing results. We recover sharp results for the global\nminimaxity as a consequence of our study. Fixing the scale of the\nsignal-to-noise ratio, we prove that the estimation error can be much smaller\nthan the global minimax error. We construct a new optimal estimator for the\nscaled minimax sparse estimation. An optimal adaptive procedure is also\ndescribed. \n\n"}
{"id": "1810.05679", "contents": "Title: Spherical Regression under Mismatch Corruption with Application to\n  Automated Knowledge Translation Abstract: Motivated by a series of applications in data integration, language\ntranslation, bioinformatics, and computer vision, we consider spherical\nregression with two sets of unit-length vectors when the data are corrupted by\na small fraction of mismatch in the response-predictor pairs. We propose a\nthree-step algorithm in which we initialize the parameters by solving an\northogonal Procrustes problem to estimate a translation matrix $\\mathbb{W}$\nignoring the mismatch. We then estimate a mapping matrix aiming to correct the\nmismatch using hard-thresholding to induce sparsity, while incorporating\npotential group information. We eventually obtain a refined estimate for\n$\\mathbb{W}$ by removing the estimated mismatched pairs. We derive the error\nbound for the initial estimate of $\\mathbb{W}$ in both fixed and\nhigh-dimensional setting. We demonstrate that the refined estimate of\n$\\mathbb{W}$ achieves an error rate that is as good as if no mismatch is\npresent. We show that our mapping recovery method not only correctly\ndistinguishes one-to-one and one-to-many correspondences, but also consistently\nidentifies the matched pairs and estimates the weight vector for combined\ncorrespondence. We examine the finite sample performance of the proposed method\nvia extensive simulation studies, and with application to the unsupervised\ntranslation of medical codes using electronic health records data. \n\n"}
{"id": "1810.05752", "contents": "Title: Global Convergence of EM Algorithm for Mixtures of Two Component Linear\n  Regression Abstract: The Expectation-Maximization algorithm is perhaps the most broadly used\nalgorithm for inference of latent variable problems. A theoretical\nunderstanding of its performance, however, largely remains lacking. Recent\nresults established that EM enjoys global convergence for Gaussian Mixture\nModels. For Mixed Linear Regression, however, only local convergence results\nhave been established, and those only for the high SNR regime. We show here\nthat EM converges for mixed linear regression with two components (it is known\nthat it may fail to converge for three or more), and moreover that this\nconvergence holds for random initialization. Our analysis reveals that EM\nexhibits very different behavior in Mixed Linear Regression from its behavior\nin Gaussian Mixture Models, and hence our proofs require the development of\nseveral new ideas. \n\n"}
{"id": "1810.05935", "contents": "Title: Uniform Convergence Rate of the Kernel Density Estimator Adaptive to\n  Intrinsic Volume Dimension Abstract: We derive concentration inequalities for the supremum norm of the difference\nbetween a kernel density estimator (KDE) and its point-wise expectation that\nhold uniformly over the selection of the bandwidth and under weaker conditions\non the kernel and the data generating distribution than previously used in the\nliterature. We first propose a novel concept, called the volume dimension, to\nmeasure the intrinsic dimension of the support of a probability distribution\nbased on the rates of decay of the probability of vanishing Euclidean balls.\nOur bounds depend on the volume dimension and generalize the existing bounds\nderived in the literature. In particular, when the data-generating distribution\nhas a bounded Lebesgue density or is supported on a sufficiently well-behaved\nlower-dimensional manifold, our bound recovers the same convergence rate\ndepending on the intrinsic dimension of the support as ones known in the\nliterature. At the same time, our results apply to more general cases, such as\nthe ones of distribution with unbounded densities or supported on a mixture of\nmanifolds with different dimensions. Analogous bounds are derived for the\nderivative of the KDE, of any order. Our results are generally applicable but\nare especially useful for problems in geometric inference and topological data\nanalysis, including level set estimation, density-based clustering, modal\nclustering and mode hunting, ridge estimation and persistent homology. \n\n"}
{"id": "1810.09285", "contents": "Title: Non-central limit theorems for functionals of random fields on\n  hypersurfaces Abstract: This paper derives non-central asymptotic results for non-linear integral\nfunctionals of homogeneous isotropic Gaussian random fields defined on\nhypersurfaces in $\\mathbb{R}^d$. We obtain the rate of convergence for these\nfunctionals. The results extend recent findings for solid figures. We apply the\nobtained results to the case of sojourn measures and demonstrate different\nlimit situations. \n\n"}
{"id": "1810.10122", "contents": "Title: PoPPy: A Point Process Toolbox Based on PyTorch Abstract: PoPPy is a Point Process toolbox based on PyTorch, which achieves flexible\ndesigning and efficient learning of point process models. It can be used for\ninterpretable sequential data modeling and analysis, e.g., Granger causality\nanalysis of multi-variate point processes, point process-based simulation and\nprediction of event sequences. In practice, the key points of point\nprocess-based sequential data modeling include: 1) How to design intensity\nfunctions to describe the mechanism behind observed data? 2) How to learn the\nproposed intensity functions from observed data? The goal of PoPPy is providing\na user-friendly solution to the key points above and achieving large-scale\npoint process-based sequential data analysis, simulation and prediction. \n\n"}
{"id": "1810.10659", "contents": "Title: Combinatorial Optimization with Graph Convolutional Networks and Guided\n  Tree Search Abstract: We present a learning-based approach to computing solutions for certain\nNP-hard problems. Our approach combines deep learning techniques with useful\nalgorithmic elements from classic heuristics. The central component is a graph\nconvolutional network that is trained to estimate the likelihood, for each\nvertex in a graph, of whether this vertex is part of the optimal solution. The\nnetwork is designed and trained to synthesize a diverse set of solutions, which\nenables rapid exploration of the solution space via tree search. The presented\napproach is evaluated on four canonical NP-hard problems and five datasets,\nwhich include benchmark satisfiability problems and real social network graphs\nwith up to a hundred thousand nodes. Experimental results demonstrate that the\npresented approach substantially outperforms recent deep learning work, and\nperforms on par with highly optimized state-of-the-art heuristic solvers for\nsome NP-hard problems. Experiments indicate that our approach generalizes\nacross datasets, and scales to graphs that are orders of magnitude larger than\nthose used during training. \n\n"}
{"id": "1810.11107", "contents": "Title: Adaptive Density Estimation on Bounded Domains Abstract: We study the estimation, in Lp-norm, of density functions defined on [0,1]^d.\nWe construct a new family of kernel density estimators that do not suffer from\nthe so-called boundary bias problem and we propose a data-driven procedure\nbased on the Goldenshluger and Lepski approach that jointly selects a kernel\nand a bandwidth. We derive two estimators that satisfy oracle-type\ninequalities. They are also proved to be adaptive over a scale of anisotropic\nor isotropic Sobolev-Slobodetskii classes (which are particular cases of Besov\nor Sobolev classical classes). The main interest of the isotropic procedure is\nto obtain adaptive results without any restriction on the smoothness parameter. \n\n"}
{"id": "1810.11526", "contents": "Title: Algebraic tests of general Gaussian latent tree models Abstract: We consider general Gaussian latent tree models in which the observed\nvariables are not restricted to be leaves of the tree. Extending related recent\nwork, we give a full semi-algebraic description of the set of covariance\nmatrices of any such model. In other words, we find polynomial constraints that\ncharacterize when a matrix is the covariance matrix of a distribution in a\ngiven latent tree model. However, leveraging these constraints to test a given\nsuch model is often complicated by the number of constraints being large and by\nsingularities of individual polynomials, which may invalidate standard\napproximations to relevant probability distributions. Illustrating with the\nstar tree, we propose a new testing methodology that circumvents singularity\nissues by trading off some statistical estimation efficiency and handles cases\nwith many constraints through recent advances on Gaussian approximation for\nmaxima of sums of high-dimensional random vectors. Our test avoids the need to\nmaximize the possibly multimodal likelihood function of such models and is\napplicable to models with larger number of variables. These points are\nillustrated in numerical experiments. \n\n"}
{"id": "1810.11859", "contents": "Title: Consistency of ELBO maximization for model selection Abstract: The Evidence Lower Bound (ELBO) is a quantity that plays a key role in\nvariational inference. It can also be used as a criterion in model selection.\nHowever, though extremely popular in practice in the variational Bayes\ncommunity, there has never been a general theoretic justification for selecting\nbased on the ELBO. In this paper, we show that the ELBO maximization strategy\nhas strong theoretical guarantees, and is robust to model misspecification\nwhile most works rely on the assumption that one model is correctly specified.\nWe illustrate our theoretical results by an application to the selection of the\nnumber of principal components in probabilistic PCA. \n\n"}
{"id": "1810.11896", "contents": "Title: Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of\n  Neurons Abstract: We analyze linear independence of rank one tensors produced by tensor powers\nof randomly perturbed vectors. This enables efficient decomposition of sums of\nhigh-order tensors. Our analysis builds upon [BCMV14] but allows for a wider\nrange of perturbation models, including discrete ones. We give an application\nto recovering assemblies of neurons.\n  Assemblies are large sets of neurons representing specific memories or\nconcepts. The size of the intersection of two assemblies has been shown in\nexperiments to represent the extent to which these memories co-occur or these\nconcepts are related; the phenomenon is called association of assemblies. This\nsuggests that an animal's memory is a complex web of associations, and poses\nthe problem of recovering this representation from cognitive data. Motivated by\nthis problem, we study the following more general question: Can we reconstruct\nthe Venn diagram of a family of sets, given the sizes of their $\\ell$-wise\nintersections? We show that as long as the family of sets is randomly\nperturbed, it is enough for the number of measurements to be polynomially\nlarger than the number of nonempty regions of the Venn diagram to fully\nreconstruct the diagram. \n\n"}
{"id": "1811.00781", "contents": "Title: Non-Asymptotic Guarantees For Sampling by Stochastic Gradient Descent Abstract: Sampling from various kinds of distributions is an issue of paramount\nimportance in statistics since it is often the key ingredient for constructing\nestimators, test procedures or confidence intervals. In many situations, the\nexact sampling from a given distribution is impossible or computationally\nexpensive and, therefore, one needs to resort to approximate sampling\nstrategies. However, it is only very recently that a mathematical theory\nproviding non-asymptotic guarantees for approximate sampling problem in the\nhigh-dimensional settings started to be developed. In this paper we introduce a\nnew mathematical framework that helps to analyze the Stochastic Gradient\nDescent as a method of sampling, closely related to Langevin Monte-Carlo. \n\n"}
{"id": "1811.01811", "contents": "Title: Active Deep Learning Attacks under Strict Rate Limitations for Online\n  API Calls Abstract: Machine learning has been applied to a broad range of applications and some\nof them are available online as application programming interfaces (APIs) with\neither free (trial) or paid subscriptions. In this paper, we study adversarial\nmachine learning in the form of back-box attacks on online classifier APIs. We\nstart with a deep learning based exploratory (inference) attack, which aims to\nbuild a classifier that can provide similar classification results (labels) as\nthe target classifier. To minimize the difference between the labels returned\nby the inferred classifier and the target classifier, we show that the deep\nlearning based exploratory attack requires a large number of labeled training\ndata samples. These labels can be collected by calling the online API, but\nusually there is some strict rate limitation on the number of allowed API\ncalls. To mitigate the impact of limited training data, we develop an active\nlearning approach that first builds a classifier based on a small number of API\ncalls and uses this classifier to select samples to further collect their\nlabels. Then, a new classifier is built using more training data samples. This\nupdating process can be repeated multiple times. We show that this active\nlearning approach can build an adversarial classifier with a small statistical\ndifference from the target classifier using only a limited number of training\ndata samples. We further consider evasion and causative (poisoning) attacks\nbased on the inferred classifier that is built by the exploratory attack.\nEvasion attack determines samples that the target classifier is likely to\nmisclassify, whereas causative attack provides erroneous training data samples\nto reduce the reliability of the re-trained classifier. The success of these\nattacks show that adversarial machine learning emerges as a feasible threat in\nthe realistic case with limited training data. \n\n"}
{"id": "1811.03735", "contents": "Title: A Note on the comparison of Nearest Neighbor Gaussian Process (NNGP)\n  based models Abstract: This note is devoted to the comparison between two Nearest-neighbor Gaussian\nprocesses (NNGP) based models: the response NNGP model and the latent NNGP\nmodel. We exhibit that the comparison based on the Kullback-Leibler divergence\n(KL-D) from the NNGP based models to their parent GP based model can result in\nreverse conclusions in different parameter spaces. And we suggest a heuristic\nexplanation on the phenomenon that the latent NNGP model tends to outperform\nthe response NNGP model in approximating their parent GP based model. \n\n"}
{"id": "1811.04058", "contents": "Title: Bernstein-von Mises theorems and uncertainty quantification for linear\n  inverse problems Abstract: We consider the statistical inverse problem of recovering an unknown function\n$f$ from a linear measurement corrupted by additive Gaussian white noise. We\nemploy a nonparametric Bayesian approach with standard Gaussian priors, for\nwhich the posterior-based reconstruction of $f$ corresponds to a Tikhonov\nregulariser $\\bar f$ with a reproducing kernel Hilbert space norm penalty. We\nprove a semiparametric Bernstein-von Mises theorem for a large collection of\nlinear functionals of $f$, implying that semiparametric posterior estimation\nand uncertainty quantification are valid and optimal from a frequentist point\nof view. The result is applied to study three concrete examples that cover both\nthe mildly and severely ill-posed cases: specifically, an elliptic inverse\nproblem, an elliptic boundary value problem and the heat equation. For the\nelliptic boundary value problem, we also obtain a nonparametric version of the\ntheorem that entails the convergence of the posterior distribution to a\nprior-independent infinite-dimensional Gaussian probability measure with\nminimal covariance. As a consequence, it follows that the Tikhonov regulariser\n$\\bar f$ is an efficient estimator of $f$, and we derive frequentist guarantees\nfor certain credible balls centred at $\\bar{f}$. \n\n"}
{"id": "1811.04423", "contents": "Title: When Locally Linear Embedding Hits Boundary Abstract: Based on the Riemannian manifold model, we study the asymptotic behavior of a\nwidely applied unsupervised learning algorithm, locally linear embedding (LLE),\nwhen the point cloud is sampled from a compact, smooth manifold with boundary.\nWe show several peculiar behaviors of LLE near the boundary that are different\nfrom those diffusion-based algorithms. In particular, we show that LLE\npointwisely converges to a mixed-type differential operator with degeneracy and\nwe calculate the convergence rate. The impact of the hyperbolic part of the\noperator is discussed and we propose a clipped LLE algorithm which is a\npotential approach to recover the Dirichlet Laplace-Beltrami operator. \n\n"}
{"id": "1811.06055", "contents": "Title: Minimax Rates in Network Analysis: Graphon Estimation, Community\n  Detection and Hypothesis Testing Abstract: This paper surveys some recent developments in fundamental limits and optimal\nalgorithms for network analysis. We focus on minimax optimal rates in three\nfundamental problems of network analysis: graphon estimation, community\ndetection, and hypothesis testing. For each problem, we review state-of-the-art\nresults in the literature followed by general principles behind the optimal\nprocedures that lead to minimax estimation and testing. This allows us to\nconnect problems in network analysis to other statistical inference problems\nfrom a general perspective. \n\n"}
{"id": "1811.06172", "contents": "Title: The autoregression bootstrap for kernel estimates of smooth nonlinear\n  functional time series Abstract: Functional times series have become an integral part of both functional data\nand time series analysis. This paper deals with the functional autoregressive\nmodel of order 1 and the autoregression bootstrap for smooth functions. The\nregression operator is estimated in the framework developed by Ferraty and Vieu\n[2004] and Ferraty et al. [2007] which is here extended to the double\nfunctional case under an assumption of stationary ergodic data which dates back\nto Laib and Louani [2010]. The main result of this article is the\ncharacterization of the asymptotic consistency of the bootstrapped regression\noperator. \n\n"}
{"id": "1811.07301", "contents": "Title: A conditional limit theorem for independent random variables Abstract: In this paper, we prove a conditional limit theorem for independent not\nnecessarily identically distributed random variables. Namely, we obtain the\nasymptotic distribution of a large number of them given the sum. \n\n"}
{"id": "1811.08958", "contents": "Title: Smoothed functional average variance estimation for dimension reduction Abstract: We propose an estimation method that we call functional average variance\nestimation (FAVE), for estimating the EDR space in functional semiparametric\nregression model, based on kernel estimates of density and regression.\nConsistency results are then established for the estimator of the interest\noperator, and for the directions of EDR space. A simulation study that shows\nthat the proposed approach performs as well as traditional ones is presented. \n\n"}
{"id": "1811.11124", "contents": "Title: LEASGD: an Efficient and Privacy-Preserving Decentralized Algorithm for\n  Distributed Learning Abstract: Distributed learning systems have enabled training large-scale models over\nlarge amount of data in significantly shorter time. In this paper, we focus on\ndecentralized distributed deep learning systems and aim to achieve differential\nprivacy with good convergence rate and low communication cost. To achieve this\ngoal, we propose a new learning algorithm LEASGD (Leader-Follower Elastic\nAveraging Stochastic Gradient Descent), which is driven by a novel\nLeader-Follower topology and a differential privacy model.We provide a\ntheoretical analysis of the convergence rate and the trade-off between the\nperformance and privacy in the private setting.The experimental results show\nthat LEASGD outperforms state-of-the-art decentralized learning algorithm DPSGD\nby achieving steadily lower loss within the same iterations and by reducing the\ncommunication cost by 30%. In addition, LEASGD spends less differential privacy\nbudget and has higher final accuracy result than DPSGD under private setting. \n\n"}
{"id": "1811.12239", "contents": "Title: Counterfactual Learning from Human Proofreading Feedback for Semantic\n  Parsing Abstract: In semantic parsing for question-answering, it is often too expensive to\ncollect gold parses or even gold answers as supervision signals. We propose to\nconvert model outputs into a set of human-understandable statements which allow\nnon-expert users to act as proofreaders, providing error markings as learning\nsignals to the parser. Because model outputs were suggested by a historic\nsystem, we operate in a counterfactual, or off-policy, learning setup. We\nintroduce new estimators which can effectively leverage the given feedback and\nwhich avoid known degeneracies in counterfactual learning, while still being\napplicable to stochastic gradient optimization for neural semantic parsing.\nFurthermore, we discuss how our feedback collection method can be seamlessly\nintegrated into deployed virtual personal assistants that embed a semantic\nparser. Our work is the first to show that semantic parsers can be improved\nsignificantly by counterfactual learning from logged human feedback data. \n\n"}
{"id": "1811.12500", "contents": "Title: Sequential Embedding Induced Text Clustering, a Non-parametric Bayesian\n  Approach Abstract: Current state-of-the-art nonparametric Bayesian text clustering methods model\ndocuments through multinomial distribution on bags of words. Although these\nmethods can effectively utilize the word burstiness representation of documents\nand achieve decent performance, they do not explore the sequential information\nof text and relationships among synonyms. In this paper, the documents are\nmodeled as the joint of bags of words, sequential features and word embeddings.\nWe proposed Sequential Embedding induced Dirichlet Process Mixture Model\n(SiDPMM) to effectively exploit this joint document representation in text\nclustering. The sequential features are extracted by the encoder-decoder\ncomponent. Word embeddings produced by the continuous-bag-of-words (CBOW) model\nare introduced to handle synonyms. Experimental results demonstrate the\nbenefits of our model in two major aspects: 1) improved performance across\nmultiple diverse text datasets in terms of the normalized mutual information\n(NMI); 2) more accurate inference of ground truth cluster numbers with\nregularization effect on tiny outlier clusters. \n\n"}
{"id": "1811.12593", "contents": "Title: Two-sample Test of Community Memberships of Weighted Stochastic Block\n  Models Abstract: Suppose two networks are observed for the same set of nodes, where each\nnetwork is assumed to be generated from a weighted stochastic block model. This\npaper considers the problem of testing whether the community memberships of the\ntwo networks are the same. A test statistic based on singular subspace distance\nis developed. Under the weighted stochastic block models with dense graphs, the\nlimiting distribution of the proposed test statistic is developed. Simulation\nresults show that the test has correct empirical type 1 errors under the dense\ngraphs. The test also behaves as expected in empirical power, showing gradual\nchanges when the intra-block and inter-block distributions are close and\nachieving 1 when the two distributions are not so close, where the closeness of\nthe two distributions is characterized by Renyi divergence of order 1/2. The\nEnron email networks are used to demonstrate the proposed test. \n\n"}
{"id": "1811.12799", "contents": "Title: Customer Lifetime Value in Video Games Using Deep Learning and\n  Parametric Models Abstract: Nowadays, video game developers record every virtual action performed by\ntheir players. As each player can remain in the game for years, this results in\nan exceptionally rich dataset that can be used to understand and predict player\nbehavior. In particular, this information may serve to identify the most\nvaluable players and foresee the amount of money they will spend in in-app\npurchases during their lifetime. This is crucial in free-to-play games, where\nup to 50% of the revenue is generated by just around 2% of the players, the\nso-called whales.\n  To address this challenge, we explore how deep neural networks can be used to\npredict customer lifetime value in video games, and compare their performance\nto parametric models such as Pareto/NBD. Our results suggest that convolutional\nneural network structures are the most efficient in predicting the economic\nvalue of individual players. They not only perform better in terms of accuracy,\nbut also scale to big data and significantly reduce computational time, as they\ncan work directly with raw sequential data and thus do not require any feature\nengineering process. This becomes important when datasets are very large, as is\noften the case with video game logs.\n  Moreover, convolutional neural networks are particularly well suited to\nidentify potential whales. Such an early identification is of paramount\nimportance for business purposes, as it would allow developers to implement\nin-game actions aimed at retaining big spenders and maximizing their lifetime,\nwhich would ultimately translate into increased revenue. \n\n"}
{"id": "1812.00532", "contents": "Title: Large Spectral Density Matrix Estimation by Thresholding Abstract: Spectral density matrix estimation of multivariate time series is a classical\nproblem in time series and signal processing. In modern neuroscience, spectral\ndensity based metrics are commonly used for analyzing functional connectivity\namong brain regions. In this paper, we develop a non-asymptotic theory for\nregularized estimation of high-dimensional spectral density matrices of\nGaussian and linear processes using thresholded versions of averaged\nperiodograms. Our theoretical analysis ensures that consistent estimation of\nspectral density matrix of a $p$-dimensional time series using $n$ samples is\npossible under high-dimensional regime $\\log p / n \\rightarrow 0$ as long as\nthe true spectral density is approximately sparse. A key technical component of\nour analysis is a new concentration inequality of average periodogram around\nits expectation, which is of independent interest. Our estimation consistency\nresults complement existing results for shrinkage based estimators of\nmultivariate spectral density, which require no assumption on sparsity but only\nensure consistent estimation in a regime $p^2/n \\rightarrow 0$. In addition,\nour proposed thresholding based estimators perform consistent and automatic\nedge selection when learning coherence networks among the components of a\nmultivariate time series. We demonstrate the advantage of our estimators using\nsimulation studies and a real data application on functional connectivity\nanalysis with fMRI data. \n\n"}
{"id": "1812.00855", "contents": "Title: Towards Solving Text-based Games by Producing Adaptive Action Spaces Abstract: To solve a text-based game, an agent needs to formulate valid text commands\nfor a given context and find the ones that lead to success. Recent attempts at\nsolving text-based games with deep reinforcement learning have focused on the\nlatter, i.e., learning to act optimally when valid actions are known in\nadvance. In this work, we propose to tackle the first task and train a model\nthat generates the set of all valid commands for a given context. We try three\ngenerative models on a dataset generated with Textworld. The best model can\ngenerate valid commands which were unseen at training and achieve high $F_1$\nscore on the test set. \n\n"}
{"id": "1812.01412", "contents": "Title: Necessary and Probably Sufficient Test for Finding Valid Instrumental\n  Variables Abstract: Can instrumental variables be found from data? While instrumental variable\n(IV) methods are widely used to identify causal effect, testing their validity\nfrom observed data remains a challenge. This is because validity of an IV\ndepends on two assumptions, exclusion and as-if-random, that are largely\nbelieved to be untestable from data. In this paper, we show that under certain\nconditions, testing for instrumental variables is possible. We build upon prior\nwork on necessary tests to derive a test that characterizes the odds of being a\nvalid instrument, thus yielding the name \"necessary and probably sufficient\".\nThe test works by defining the class of invalid-IV and valid-IV causal models\nas Bayesian generative models and comparing their marginal likelihood based on\nobserved data. When all variables are discrete, we also provide a method to\nefficiently compute these marginal likelihoods.\n  We evaluate the test on an extensive set of simulations for binary data,\ninspired by an open problem for IV testing proposed in past work. We find that\nthe test is most powerful when an instrument follows monotonicity---effect on\ntreatment is either non-decreasing or non-increasing---and has moderate-to-weak\nstrength; incidentally, such instruments are commonly used in observational\nstudies. Among as-if-random and exclusion, it detects exclusion violations with\nhigher power. Applying the test to IVs from two seminal studies on instrumental\nvariables and five recent studies from the American Economic Review shows that\nmany of the instruments may be flawed, at least when all variables are\ndiscretized. The proposed test opens the possibility of data-driven validation\nand search for instrumental variables. \n\n"}
{"id": "1812.01696", "contents": "Title: Learning Individualized Cardiovascular Responses from Large-scale\n  Wearable Sensors Data Abstract: We consider the problem of modeling cardiovascular responses to physical\nactivity and sleep changes captured by wearable sensors in free living\nconditions. We use an attentional convolutional neural network to learn\nparsimonious signatures of individual cardiovascular response from data\nrecorded at the minute level resolution over several months on a cohort of 80k\npeople. We demonstrate internal validity by showing that signatures generated\non an individual's 2017 data generalize to predict minute-level heart rate from\nphysical activity and sleep for the same individual in 2018, outperforming\nseveral time-series forecasting baselines. We also show external validity\ndemonstrating that signatures outperform plain resting heart rate (RHR) in\npredicting variables associated with cardiovascular functions, such as age and\nBody Mass Index (BMI). We believe that the computed cardiovascular signatures\nhave utility in monitoring cardiovascular health over time, including detecting\nabnormalities and quantifying recovery from acute events. \n\n"}
{"id": "1812.01713", "contents": "Title: FineFool: Fine Object Contour Attack via Attention Abstract: Machine learning models have been shown vulnerable to adversarial attacks\nlaunched by adversarial examples which are carefully crafted by attacker to\ndefeat classifiers. Deep learning models cannot escape the attack either. Most\nof adversarial attack methods are focused on success rate or perturbations\nsize, while we are more interested in the relationship between adversarial\nperturbation and the image itself. In this paper, we put forward a novel\nadversarial attack based on contour, named FineFool. Finefool not only has\nbetter attack performance compared with other state-of-art white-box attacks in\naspect of higher attack success rate and smaller perturbation, but also capable\nof visualization the optimal adversarial perturbation via attention on object\ncontour. To the best of our knowledge, Finefool is for the first time combines\nthe critical feature of the original clean image with the optimal perturbations\nin a visible manner. Inspired by the correlations between adversarial\nperturbations and object contour, slighter perturbations is produced via\nfocusing on object contour features, which is more imperceptible and difficult\nto be defended, especially network add-on defense methods with the trade-off\nbetween perturbations filtering and contour feature loss. Compared with\nexisting state-of-art attacks, extensive experiments are conducted to show that\nFinefool is capable of efficient attack against defensive deep models. \n\n"}
{"id": "1812.01815", "contents": "Title: Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on\n  Zero-One Loss Abstract: Uncertainty sampling, a popular active learning algorithm, is used to reduce\nthe amount of data required to learn a classifier, but it has been observed in\npractice to converge to different parameters depending on the initialization\nand sometimes to even better parameters than standard training on all the data.\nIn this work, we give a theoretical explanation of this phenomenon, showing\nthat uncertainty sampling on a convex loss can be interpreted as performing a\npreconditioned stochastic gradient step on a smoothed version of the population\nzero-one loss that converges to the population zero-one loss. Furthermore,\nuncertainty sampling moves in a descent direction and converges to stationary\npoints of the smoothed population zero-one loss. Experiments on synthetic and\nreal datasets support this connection. \n\n"}
{"id": "1812.02435", "contents": "Title: A MOM-based ensemble method for robustness, subsampling and\n  hyperparameter tuning Abstract: Hyperparameters tuning and model selection are important steps in machine\nlearning. Unfortunately, classical hyperparameter calibration and model\nselection procedures are sensitive to outliers and heavy-tailed data. In this\nwork, we construct a selection procedure which can be seen as a robust\nalternative to cross-validation and is based on a median-of-means principle.\nUsing this procedure, we also build an ensemble method which, trained with\nalgorithms and corrupted heavy-tailed data, selects an algorithm, trains it\nwith a large uncorrupted subsample and automatically tune its hyperparameters.\nThe construction relies on a divide-and-conquer methodology, making this method\neasily scalable for autoML given a corrupted database. This method is tested\nwith the LASSO which is known to be highly sensitive to outliers. \n\n"}
{"id": "1812.06209", "contents": "Title: Causal Identification under Markov Equivalence Abstract: Assessing the magnitude of cause-and-effect relations is one of the central\nchallenges found throughout the empirical sciences. The problem of\nidentification of causal effects is concerned with determining whether a causal\neffect can be computed from a combination of observational data and substantive\nknowledge about the domain under investigation, which is formally expressed in\nthe form of a causal graph. In many practical settings, however, the knowledge\navailable for the researcher is not strong enough so as to specify a unique\ncausal graph. Another line of investigation attempts to use observational data\nto learn a qualitative description of the domain called a Markov equivalence\nclass, which is the collection of causal graphs that share the same set of\nobserved features. In this paper, we marry both approaches and study the\nproblem of causal identification from an equivalence class, represented by a\npartial ancestral graph (PAG). We start by deriving a set of graphical\nproperties of PAGs that are carried over to its induced subgraphs. We then\ndevelop an algorithm to compute the effect of an arbitrary set of variables on\nan arbitrary outcome set. We show that the algorithm is strictly more powerful\nthan the current state of the art found in the literature. \n\n"}
{"id": "1812.06894", "contents": "Title: Likelihood Ratio Test in Multivariate Linear Regression: from Low to\n  High Dimension Abstract: Multivariate linear regressions are widely used statistical tools in many\napplications to model the associations between multiple related responses and a\nset of predictors. To infer such associations, it is often of interest to test\nthe structure of the regression coefficients matrix, and the likelihood ratio\ntest (LRT) is one of the most popular approaches in practice. Despite its\npopularity, it is known that the classical $\\chi^2$ approximations for LRTs\noften fail in high-dimensional settings, where the dimensions of responses and\npredictors $(m,p)$ are allowed to grow with the sample size $n$. Though various\ncorrected LRTs and other test statistics have been proposed in the literature,\nthe fundamental question of when the classic LRT starts to fail is less\nstudied, an answer to which would provide insights for practitioners,\nespecially when analyzing data with $m/n$ and $p/n$ small but not negligible.\nMoreover, the power performance of the LRT in high-dimensional data analysis\nremains underexplored. To address these issues, the first part of this work\ngives the asymptotic boundary where the classical LRT fails and develops the\ncorrected limiting distribution of the LRT for a general asymptotic regime. The\nsecond part of this work further studies the test power of the LRT in the\nhigh-dimensional setting. The result not only advances the current\nunderstanding of asymptotic behavior of the LRT under alternative hypothesis,\nbut also motivates the development of a power-enhanced LRT. The third part of\nthis work considers the setting with $p>n$, where the LRT is not well-defined.\nWe propose a two-step testing procedure by first performing dimension reduction\nand then applying the proposed LRT. Theoretical properties are developed to\nensure the validity of the proposed method. Numerical studies are also\npresented to demonstrate its good performance. \n\n"}
{"id": "1812.09041", "contents": "Title: A Multi-task Neural Approach for Emotion Attribution, Classification and\n  Summarization Abstract: Emotional content is a crucial ingredient in user-generated videos. However,\nthe sparsity of emotional expressions in the videos poses an obstacle to visual\nemotion analysis. In this paper, we propose a new neural approach, Bi-stream\nEmotion Attribution-Classification Network (BEAC-Net), to solve three related\nemotion analysis tasks: emotion recognition, emotion attribution, and\nemotion-oriented summarization, in a single integrated framework. BEAC-Net has\ntwo major constituents, an attribution network and a classification network.\nThe attribution network extracts the main emotional segment that classification\nshould focus on in order to mitigate the sparsity issue. The classification\nnetwork utilizes both the extracted segment and the original video in a\nbi-stream architecture. We contribute a new dataset for the emotion attribution\ntask with human-annotated ground-truth labels for emotion segments. Experiments\non two video datasets demonstrate superior performance of the proposed\nframework and the complementary nature of the dual classification streams. \n\n"}
{"id": "1812.09063", "contents": "Title: Efficient Calculation of the Joint Distribution of Order Statistics Abstract: We consider the problem of computing the joint distribution of order\nstatistics of stochastically independent random variables in one- and two-group\nmodels. While recursive formulas for evaluating the joint cumulative\ndistribution function of such order statistics exist in the literature for a\nlonger time, their numerical implementation remains a challenging task. We\ntackle this task by presenting novel generalizations of known recursions which\nwe utilize to obtain exact results (calculated in rational arithmetic) as well\nas faithfully rounded results. Finally, some applications in stepwise multiple\nhypothesis testing are discussed. \n\n"}
{"id": "1812.10741", "contents": "Title: On mutual information estimation for mixed-pair random variables Abstract: We study the mutual information estimation for mixed-pair random variables.\nOne random variable is discrete and the other one is continuous. We develop a\nkernel method to estimate the mutual information between the two random\nvariables. The estimates enjoy a central limit theorem under some regular\nconditions on the distributions. The theoretical results are demonstrated by\nsimulation study. \n\n"}
{"id": "1812.11026", "contents": "Title: Hybrid Wasserstein Distance and Fast Distribution Clustering Abstract: We define a modified Wasserstein distance for distribution clustering which\ninherits many of the properties of the Wasserstein distance but which can be\nestimated easily and computed quickly. The modified distance is the sum of two\nterms. The first term --- which has a closed form --- measures the\nlocation-scale differences between the distributions. The second term is an\napproximation that measures the remaining distance after accounting for\nlocation-scale differences. We consider several forms of approximation with our\nmain emphasis being a tangent space approximation that can be estimated using\nnonparametric regression. We evaluate the strengths and weaknesses of this\napproach on simulated and real examples. \n\n"}
{"id": "1812.11269", "contents": "Title: Non-Asymptotic Chernoff Lower Bound and Its Application to Community\n  Detection in Stochastic Block Model Abstract: Chernoff coefficient is an upper bound of Bayes error probability in\nclassification problem. In this paper, we will develop sharp Chernoff type\nbound on Bayes error probability. The new bound is not only an upper bound but\nalso a lower bound of Bayes error probability up to a constant in a\nnon-asymptotic setting. Moreover, we will apply this result to community\ndetection in stochastic block model. As a clustering problem, the optimal error\nrate of community detection can be characterized by our Chernoff type bound.\nThis can be formalized by deriving a minimax error rate over certain class of\nparameter space, then achieving such error rate by a feasible algorithm employ\nmultiple steps of EM type updates. \n\n"}
{"id": "1812.11377", "contents": "Title: Hessian-Aware Zeroth-Order Optimization for Black-Box Adversarial Attack Abstract: Zeroth-order optimization is an important research topic in machine learning.\nIn recent years, it has become a key tool in black-box adversarial attack to\nneural network based image classifiers. However, existing zeroth-order\noptimization algorithms rarely extract second-order information of the model\nfunction. In this paper, we utilize the second-order information of the\nobjective function and propose a novel \\textit{Hessian-aware zeroth-order\nalgorithm} called \\texttt{ZO-HessAware}. Our theoretical result shows that\n\\texttt{ZO-HessAware} has an improved zeroth-order convergence rate and query\ncomplexity under structured Hessian approximation, where we propose a few\napproximation methods for estimating Hessian. Our empirical studies on the\nblack-box adversarial attack problem validate that our algorithm can achieve\nimproved success rates with a lower query complexity. \n\n"}
{"id": "1812.11433", "contents": "Title: On the Construction of Knockoffs in Case-Control Studies Abstract: Consider a case-control study in which we have a random sample, constructed\nin such a way that the proportion of cases in our sample is different from that\nin the general population---for instance, the sample is constructed to achieve\na fixed ratio of cases to controls. Imagine that we wish to determine which of\nthe potentially many covariates under study truly influence the response by\napplying the new model-X knockoffs approach. This paper demonstrates that it\nsuffices to design knockoff variables using data that may have a different\nratio of cases to controls. For example, the knockoff variables can be\nconstructed using the distribution of the original variables under any of the\nfollowing scenarios: (1) a population of controls only; (2) a population of\ncases only; (3) a population of cases and controls mixed in an arbitrary\nproportion (irrespective of the fraction of cases in the sample at hand). The\nconsequence is that knockoff variables may be constructed using unlabeled data,\nwhich is often available more easily than labeled data, while maintaining\nType-I error guarantees. \n\n"}
{"id": "1812.11651", "contents": "Title: Multi-player Multi-armed Bandits for Stable Allocation in Heterogeneous\n  Ad-Hoc Networks Abstract: Next generation networks are expected to be ultradense and aim to explore\nspectrum sharing paradigm that allows users to communicate in licensed, shared\nas well as unlicensed spectrum. Such ultra-dense networks will incur\nsignificant signaling load at base stations leading to a negative effect on\nspectrum and energy efficiency. To minimize signaling overhead, an adhoc\napproach is being considered for users communicating in the unlicensed and\nshared spectrums. For such users, decisions need to be completely decentralized\nas: 1) No communication between users and signaling from the base station is\npossible which necessitates independent channel selection at each user. A\ncollision occurs when multiple users transmit simultaneously on the same\nchannel, 2) Channel qualities may be heterogeneous, i.e., they are not same\nacross all users, and moreover, are unknown, and 3) The network could be\ndynamic where users can enter or leave anytime. We develop a multi-armed bandit\nbased distributed algorithm for static networks and extend it for the dynamic\nnetworks. The algorithms aim to achieve stable orthogonal allocation (SOC) in\nfinite time and meet the above three constraints with two novel\ncharacteristics: 1) Low complexity narrowband radio compared to wideband radio\nin existing works, and 2) Epoch-less approach for dynamic networks. We\nestablish a convergence of our algorithms to SOC and validate via extensive\nsimulation experiments. \n\n"}
{"id": "1812.11806", "contents": "Title: An introduction to domain adaptation and transfer learning Abstract: In machine learning, if the training data is an unbiased sample of an\nunderlying distribution, then the learned classification function will make\naccurate predictions for new samples. However, if the training data is not an\nunbiased sample, then there will be differences between how the training data\nis distributed and how the test data is distributed. Standard classifiers\ncannot cope with changes in data distributions between training and test\nphases, and will not perform well. Domain adaptation and transfer learning are\nsub-fields within machine learning that are concerned with accounting for these\ntypes of changes. Here, we present an introduction to these fields, guided by\nthe question: when and how can a classifier generalize from a source to a\ntarget domain? We will start with a brief introduction into risk minimization,\nand how transfer learning and domain adaptation expand upon this framework.\nFollowing that, we discuss three special cases of data set shift, namely prior,\ncovariate and concept shift. For more complex domain shifts, there are a wide\nvariety of approaches. These are categorized into: importance-weighting,\nsubspace mapping, domain-invariant spaces, feature augmentation, minimax\nestimators and robust algorithms. A number of points will arise, which we will\ndiscuss in the last section. We conclude with the remark that many open\nquestions will have to be addressed before transfer learners and\ndomain-adaptive classifiers become practical. \n\n"}
{"id": "1901.00886", "contents": "Title: Nonparametric graphical model for counts Abstract: Although multivariate count data are routinely collected in many application\nareas, there is surprisingly little work developing flexible models for\ncharacterizing their dependence structure. This is particularly true when\ninterest focuses on inferring the conditional independence graph. In this\narticle, we propose a new class of pairwise Markov random field-type models for\nthe joint distribution of a multivariate count vector. By employing a novel\ntype of transformation, we avoid restricting to non-negative dependence\nstructures or inducing other restrictions through truncations. Taking a\nBayesian approach to inference, we choose a Dirichlet process prior for the\ndistribution of a random effect to induce great flexibility in the\nspecification. An efficient Markov chain Monte Carlo (MCMC) algorithm is\ndeveloped for posterior computation. We prove various theoretical properties,\nincluding posterior consistency, and show that our COunt Nonparametric\nGraphical Analysis (CONGA) approach has good performance relative to\ncompetitors in simulation studies. The methods are motivated by an application\nto neuron spike count data in mice. \n\n"}
{"id": "1901.01388", "contents": "Title: Extraction of digital wavefront sets using applied harmonic analysis and\n  deep neural networks Abstract: Microlocal analysis provides deep insight into singularity structures and is\noften crucial for solving inverse problems, predominately, in imaging sciences.\nOf particular importance is the analysis of wavefront sets and the correct\nextraction of those. In this paper, we introduce the first algorithmic approach\nto extract the wavefront set of images, which combines data-based and\nmodel-based methods. Based on a celebrated property of the shearlet transform\nto unravel information on the wavefront set, we extract the wavefront set of an\nimage by first applying a discrete shearlet transform and then feeding local\npatches of this transform to a deep convolutional neural network trained on\nlabeled data. The resulting algorithm outperforms all competing algorithms in\nedge-orientation and ramp-orientation detection. \n\n"}
{"id": "1901.01624", "contents": "Title: Composite optimization for robust blind deconvolution Abstract: The blind deconvolution problem seeks to recover a pair of vectors from a set\nof rank one bilinear measurements. We consider a natural nonsmooth formulation\nof the problem and show that under standard statistical assumptions, its moduli\nof weak convexity, sharpness, and Lipschitz continuity are all dimension\nindependent. This phenomenon persists even when up to half of the measurements\nare corrupted by noise. Consequently, standard algorithms, such as the\nsubgradient and prox-linear methods, converge at a rapid dimension-independent\nrate when initialized within constant relative error of the solution. We then\ncomplete the paper with a new initialization strategy, complementing the local\nsearch algorithms. The initialization procedure is both provably efficient and\nrobust to outlying measurements. Numerical experiments, on both simulated and\nreal data, illustrate the developed theory and methods. \n\n"}
{"id": "1901.04555", "contents": "Title: Music Artist Classification with Convolutional Recurrent Neural Networks Abstract: Previous attempts at music artist classification use frame level audio\nfeatures which summarize frequency content within short intervals of time.\nComparatively, more recent music information retrieval tasks take advantage of\ntemporal structure in audio spectrograms using deep convolutional and recurrent\nmodels. This paper revisits artist classification with this new framework and\nempirically explores the impacts of incorporating temporal structure in the\nfeature representation. To this end, an established classification\narchitecture, a Convolutional Recurrent Neural Network (CRNN), is applied to\nthe artist20 music artist identification dataset under a comprehensive set of\nconditions. These include audio clip length, which is a novel contribution in\nthis work, and previously identified considerations such as dataset split and\nfeature level. Our results improve upon baseline works, verify the influence of\nthe producer effect on classification performance and demonstrate the\ntrade-offs between audio length and training set size. The best performing\nmodel achieves an average F1 score of 0.937 across three independent trials\nwhich is a substantial improvement over the corresponding baseline under\nsimilar conditions. Additionally, to showcase the effectiveness of the CRNN's\nfeature extraction capabilities, we visualize audio samples at the model's\nbottleneck layer demonstrating that learned representations segment into\nclusters belonging to their respective artists. \n\n"}
{"id": "1901.05595", "contents": "Title: Model-Free Tests for Series Correlation in Multivariate Linear\n  Regression Abstract: Testing for series correlation among error terms is a basic problem in linear\nregression model diagnostics. The famous Durbin-Watson test and Durbin's h-test\nrely on certain model assumptions about the response and regressor variables.\nThe present paper proposes simple tests for series correlation that are\napplicable in both fixed and random design linear regression models. The test\nstatistics are based on the regression residuals and design matrix. The test\nprocedures are robust under different distributions of random errors. The\nasymptotic distributions of the proposed statistics are derived via a newly\nestablished joint central limit theorem for several general quadratic forms and\nthe delta method. Good performance of the proposed tests is demonstrated by\nsimulation results. \n\n"}
{"id": "1901.05639", "contents": "Title: Machine learning with neural networks Abstract: These are lecture notes for a course on machine learning with neural networks\nfor scientists and engineers that I have given at Gothenburg University and\nChalmers Technical University in Gothenburg, Sweden. The material is organised\ninto three parts: Hopfield networks, supervised learning of labeled data, and\nlearning algorithms for unlabeled data sets. Part I introduces stochastic\nrecurrent networks: Hopfield networks and Boltzmann machines. The analysis of\ntheir learning rules sets the scene for the later parts. Part II describes\nsupervised learning with multilayer perceptrons and convolutional neural\nnetworks. This part starts with a simple geometrical interpretation of the\nlearning rule and leads to the recent successes of convolutional networks in\nobject recognition, recurrent networks in language processing, and reservoir\ncomputers in time-series analysis. Part III explains what neural networks can\nlearn about data that is not labeled. This part begins with a description of\nunsupervised learning techniques for clustering of data, non-linear\nprojections, and embeddings. A section on autoencoders explains how to learn\nwithout labels using convolutional networks, and the last chapter is dedicated\nto reinforcement learning. The overall goal of the course is to explain the\nfundamental principles that allow neural networks to learn, emphasising ideas\nand concepts that are common to all three parts.\n  The present version does not contain exercises (copyright owned by Cambridge\nUniversity Press). The complete book is available at\nhttps://www.cambridge.org/gb/academic/subjects/physics/statistical-physics/machine-learning-neural-networks-introduction-scientists-and-engineers?format=HB. \n\n"}
{"id": "1901.07991", "contents": "Title: A comparative study of estimation methods in quantum tomography Abstract: As quantum tomography is becoming a key component of the quantum engineering\ntoolbox, there is a need for a deeper understanding of the multitude of\nestimation methods available. Here we investigate and compare several such\nmethods: maximum likelihood, least squares, generalised least squares, positive\nleast squares, thresholded least squares and projected least squares. The\ncommon thread of the analysis is that each estimator projects the measurement\ndata onto a parameter space with respect to a specific metric, thus allowing us\nto study the relationships between different estimators.\n  The asymptotic behaviour of the least squares and the projected least squares\nestimators is studied in detail for the case of the covariant measurement and a\nfamily of states of varying ranks. This gives insight into the rank-dependent\nrisk reduction for the projected estimator, and uncovers an interesting\nnon-monotonic behaviour of the Bures risk. These asymptotic results complement\nrecent non-asymptotic concentration bounds of \\cite{GutaKahnKungTropp} which\npoint to strong optimality properties, and high computational efficiency of the\nprojected linear estimators.\n  To illustrate the theoretical methods we present results of an extensive\nsimulation study. An app running the different estimators has been made\navailable online. \n\n"}
{"id": "cs/0702018", "contents": "Title: Estimation of the Rate-Distortion Function Abstract: Motivated by questions in lossy data compression and by theoretical\nconsiderations, we examine the problem of estimating the rate-distortion\nfunction of an unknown (not necessarily discrete-valued) source from empirical\ndata. Our focus is the behavior of the so-called \"plug-in\" estimator, which is\nsimply the rate-distortion function of the empirical distribution of the\nobserved data. Sufficient conditions are given for its consistency, and\nexamples are provided to demonstrate that in certain cases it fails to converge\nto the true rate-distortion function. The analysis of its performance is\ncomplicated by the fact that the rate-distortion function is not continuous in\nthe source distribution; the underlying mathematical problem is closely related\nto the classical problem of establishing the consistency of maximum likelihood\nestimators. General consistency results are given for the plug-in estimator\napplied to a broad class of sources, including all stationary and ergodic ones.\nA more general class of estimation problems is also considered, arising in the\ncontext of lossy data compression when the allowed class of coding\ndistributions is restricted; analogous results are developed for the plug-in\nestimator in that case. Finally, consistency theorems are formulated for\nmodified (e.g., penalized) versions of the plug-in, and for estimating the\noptimal reproduction distribution. \n\n"}
{"id": "math-ph/0212005", "contents": "Title: Why Maximum Entropy? A Non-axiomatic Approach Abstract: Ill-posed inverse problems of the form y = X p where y is J-dimensional\nvector of a data, p is m-dimensional probability vector which cannot be\nmeasured directly and matrix X of observable variables is a known J,m matrix, J\n< m, are frequently solved by Shannon's entropy maximization (MaxEnt). Several\naxiomatizations were proposed to justify the MaxEnt method (also) in this\ncontext. The main aim of the presented work is two-fold: 1) to view the concept\nof complementarity of MaxEnt and Maximum Likelihood (ML) tasks from a geometric\nperspective, and consequently 2) to provide an intuitive and non-axiomatic\nanswer to the 'Why MaxEnt?' question. \n\n"}
{"id": "math-ph/0402005", "contents": "Title: Estimators, escort probabilities, and phi-exponential families in\n  statistical physics Abstract: The lower bound of Cramer and Rao is generalized to pairs of families of\nprobability distributions, one of which is escort to the other. This bound is\noptimal for certain families, called phi-exponential in the paper. Their dual\nstructure is explored. \n\n"}
{"id": "math/0212076", "contents": "Title: Two non-regular extensions of the large deviation bound Abstract: We formulate two types of extension of the large deviation theory initiated\nby Bahadur in a non-regular setting. One can be regarded as a bound of the\npoint estimation, the other can be regarded as the limit of a bound of the\ninterval estimation. Both coincide in the regular case, but do not necessarily\ncoincide in a non-regular case. Using the limits of relative Renyi entropies,\nwe derive their upper bounds and give a necessary and sufficient condition for\nthe coincidence of the two upper bounds. We also show the attainability of\nthese two bounds in several non-regular location shift families. \n\n"}
{"id": "math/0401388", "contents": "Title: A survey of max-type recursive distributional equations Abstract: In certain problems in a variety of applied probability settings (from\nprobabilistic analysis of algorithms to statistical physics), the central\nrequirement is to solve a recursive distributional equation of the form X =^d\ng((\\xi_i,X_i),i\\geq 1). Here (\\xi_i) and g(\\cdot) are given and the X_i are\nindependent copies of the unknown distribution X. We survey this area,\nemphasizing examples where the function g(\\cdot) is essentially a ``maximum''\nor ``minimum'' function. We draw attention to the theoretical question of\nendogeny: in the associated recursive tree process X_i, are the X_i measurable\nfunctions of the innovations process (\\xi_i)? \n\n"}
{"id": "math/0409436", "contents": "Title: Causal Inference for Complex Longitudinal Data: The Continuous Time\n  g-Computation Formula Abstract: We extend Robins' theory of causal inference for complex longitudinal data to\nthe case of continuously varying as opposed to discrete covariates and\ntreatments. In particular we establish versions of the key results of the\ndiscrete theory: the g-computation formula and a collection of powerful\ncharacterizations of the g-null hypothesis of no treatment effect. This is\naccomplished under natural continuity hypotheses concerning the conditional\ndistributions of the outcome variable and of the covariates given the past. We\nalso show that our assumptions concerning counterfactual variables place no\nrestriction on the joint distribution of the observed variables: thus in a\nprecise sense, these assumptions are \"for free,\" or if you prefer, harmless. \n\n"}
{"id": "math/0409610", "contents": "Title: A rate of convergence result for the largest eigenvalue of complex white\n  Wishart matrices Abstract: It has been recently shown that if $X$ is an $n\\times N$ matrix whose entries\nare i.i.d. standard complex Gaussian and $l_1$ is the largest eigenvalue of\n$X^*X$, there exist sequences $m_{n,N}$ and $s_{n,N}$ such that\n$(l_1-m_{n,N})/s_{n,N}$ converges in distribution to $W_2$, the Tracy--Widom\nlaw appearing in the study of the Gaussian unitary ensemble. This probability\nlaw has a density which is known and computable. The cumulative distribution\nfunction of $W_2$ is denoted $F_2$. In this paper we show that, under the\nassumption that $n/N\\to \\gamma\\in(0,\\infty)$, we can find a function $M$,\ncontinuous and nonincreasing, and sequences $\\tilde{\\mu}_{n,N}$ and\n$\\tilde{\\sigma}_{n,N}$ such that, for all real $s_0$, there exists an integer\n$N(s_0,\\gamma)$ for which, if $(n\\wedge N)\\geq N(s_0,\\gamma)$, we have, with\n$l_{n,N}=(l_1-\\tilde{\\mu}_{n,N})/\\tilde{\\sigma}_{n,N}$, \\[\\forall s\\geq\ns_0\\qquad (n\\wedge N)^{2/3}|P(l_{n,N}\\leq s)-F_2(s)|\\leq M(s_0)\\exp(-s).\\] The\nsurprisingly good 2/3 rate and qualitative properties of the bounding function\nhelp explain the fact that the limiting distribution $W_2$ is a good\napproximation to the empirical distribution of $l_{n,N}$ in simulations, an\nimportant fact from the point of view of (e.g., statistical) applications. \n\n"}
{"id": "math/0410271", "contents": "Title: Statistical modeling of causal effects in continuous time Abstract: This article studies the estimation of the causal effect of a time-varying\ntreatment on time-to-an-event or on some other continuously distributed\noutcome. The paper applies to the situation where treatment is repeatedly\nadapted to time-dependent patient characteristics. The treatment effect cannot\nbe estimated by simply conditioning on these time-dependent patient\ncharacteristics, as they may themselves be indications of the treatment effect.\nThis time-dependent confounding is common in observational studies. Robins\n[(1992) Biometrika 79 321--334, (1998b) Encyclopedia of Biostatistics 6\n4372--4389] has proposed the so-called structural nested models to estimate\ntreatment effects in the presence of time-dependent confounding. In this\narticle we provide a conceptual framework and formalization for structural\nnested models in continuous time. We show that the resulting estimators are\nconsistent and asymptotically normal. Moreover, as conjectured in Robins\n[(1998b) Encyclopedia of Biostatistics 6 4372--4389], a test for whether\ntreatment affects the outcome of interest can be performed without specifying a\nmodel for treatment effect. We illustrate the ideas in this article with an\nexample. \n\n"}
{"id": "math/0510628", "contents": "Title: Towards Reconciliation between Bayesian and Frequentist Reasoning Abstract: A theory of quantitative inference about the parameters of sampling\ndistributions is constructed deductively by following very general rules,\nreferred to as the Cox-Polya-Jaynes Desiderata. The inferences are made in\nterms of probability distributions that are assigned to the parameters. The\nDesiderata, focusing primarily on consistency of the plausible reasoning, lead\nto unique assignments of these probabilities in the case of sampling\ndistributions that are invariant under Lie groups. In the scalar cases, e.g. in\nthe case of inferring a single location or scale parameter, the requirement for\nlogical consistency is equivalent to the requirement for calibration: the\nconsistent probability distributions are automatically also the ones with the\nexact calibration and vice versa. This equivalence speaks in favour of\nreconciliation between the Bayesian and Frequentist schools of reasoning. \n\n"}
{"id": "math/0512141", "contents": "Title: Asympyotic expansions for infinite weighted convolutions of light\n  subexponential distributions Abstract: We establish some asymptotic expansions for infinite weighted convolutions of\ndistributions having light subexponential tails. Examples are presented, some\nshowing that in order to obtain an expansion with two significant terms, one\nneeds to have a general way to calculate higher order expansions, due to\npossible cancellations of terms. An algebraic methodology is employed to obtain\nthe results. \n\n"}
{"id": "math/0601631", "contents": "Title: Computing maximum likelihood estimates in recursive linear models with\n  correlated errors Abstract: In recursive linear models, the multivariate normal joint distribution of all\nvariables exhibits a dependence structure induced by a recursive (or acyclic)\nsystem of linear structural equations. These linear models have a long\ntradition and appear in seemingly unrelated regressions, structural equation\nmodelling, and approaches to causal inference. They are also related to\nGaussian graphical models via a classical representation known as a path\ndiagram. Despite the models' long history, a number of problems remain open. In\nthis paper, we address the problem of computing maximum likelihood estimates in\nthe subclass of `bow-free' recursive linear models. The term `bow-free' refers\nto the condition that the errors for variables $i$ and $j$ be uncorrelated if\nvariable $i$ occurs in the structural equation for variable $j$. We introduce a\nnew algorithm, termed Residual Iterative Conditional Fitting (RICF), that can\nbe implemented using only least squares computations. In contrast to existing\nalgorithms, RICF has clear convergence properties and finds parameter estimates\nin closed form whenever possible. \n\n"}
{"id": "math/0607019", "contents": "Title: Concentration for norms of infinitely divisible vectors with independent\n  components Abstract: We obtain dimension-free concentration inequalities for $\\ell^p$-norms,\n$p\\geq2$, of infinitely divisible random vectors with independent coordinates\nand finite exponential moments. Besides such norms, the methods and results\nextend to some other classes of Lipschitz functions. \n\n"}
{"id": "math/0609418", "contents": "Title: Spectrum estimation for large dimensional covariance matrices using\n  random matrix theory Abstract: Estimating the eigenvalues of a population covariance matrix from a sample\ncovariance matrix is a problem of fundamental importance in multivariate\nstatistics; the eigenvalues of covariance matrices play a key role in many\nwidely techniques, in particular in Principal Component Analysis (PCA). In many\nmodern data analysis problems, statisticians are faced with large datasets\nwhere the sample size, n, is of the same order of magnitude as the number of\nvariables p. Random matrix theory predicts that in this context, the\neigenvalues of the sample covariance matrix are not good estimators of the\neigenvalues of the population covariance.\n  We propose to use a fundamental result in random matrix theory, the\nMarcenko-Pastur equation, to better estimate the eigenvalues of large\ndimensional covariance matrices. The Marcenko-Pastur equation holds in very\nwide generality and under weak assumptions. The estimator we obtain can be\nthought of as \"shrinking\" in a non linear fashion the eigenvalues of the sample\ncovariance matrix to estimate the population eigenvalue. Inspired by ideas of\nrandom matrix theory, we also suggest a change of point of view when thinking\nabout estimation of high-dimensional vectors: we do not try to estimate\ndirectly the vectors but rather a probability measure that describes them. We\nthink this is a theoretically more fruitful way to think statistically about\nthese problems.\n  Our estimator gives fast and good or very good results in extended\nsimulations. Our algorithmic approach is based on convex optimization. We also\nshow that the proposed estimator is consistent. \n\n"}
{"id": "math/0609678", "contents": "Title: Strong consistency of MLE for finite mixtures of location-scale\n  distributions when the ratios of the scale parameters are exponentially small Abstract: In finite mixtures of location-scale distributions, if there is no constraint\non the parameters then the maximum likelihood estimate does not exist. But when\nthe ratios of the scale parameters are restricted appropriately, the maximum\nlikelihood estimate exists. We prove that the maximum likelihood estimator\n(MLE) is strongly consistent, if the ratios of the scale parameters are\nrestricted from below by $\\exp(-n^{d}), 0 < d < 1 $, where $n$ is the sample\nsize. \n\n"}
{"id": "math/0610742", "contents": "Title: Clustering of spectra and fractals of regular graphs Abstract: We exhibit a characteristic structure of the class of all regular graphs of\ndegree d that stems from the spectra of their adjacency matrices. The structure\nhas a fractal threadlike appearance. Points with coordinates given by the mean\nand variance of the exponentials of graph eigenvalues cluster around a line\nsegment that we call a filar. Zooming-in reveals that this cluster splits into\nsmaller segments (filars) labeled by the number of triangles in graphs. Further\nzooming-in shows that the smaller filars split into subfilars labelled by the\nnumber of quadrangles in graphs, etc. We call this fractal structure,\ndiscovered in a numerical experiment, a multifilar structure. We also provide a\nmathematical explanation of this phenomenon based on the Ihara-Selberg trace\nformula, and compute the coordinates and slopes of all filars in terms of\nBessel functions of the first kind. \n\n"}
{"id": "math/0702440", "contents": "Title: Bahadur representation of sample quantiles for functional of Gaussian\n  dependent sequences under a minimal assumption Abstract: We obtain a Bahadur representation for sample quantiles of nonlinear\nfunctional of Gaussian sequences with correlation function decreasing as\n$k^{-\\alpha}$ for some $\\alpha > 0$. This representation is derived under a\nmimimal assumption. \n\n"}
{"id": "math/0702564", "contents": "Title: Convex Rank Tests and Semigraphoids Abstract: Convex rank tests are partitions of the symmetric group which have desirable\ngeometric properties. The statistical tests defined by such partitions involve\ncounting all permutations in the equivalence classes. Each class consists of\nthe linear extensions of a partially ordered set specified by data. Our methods\nrefine existing rank tests of non-parametric statistics, such as the sign test\nand the runs test, and are useful for exploratory analysis of ordinal data. We\nestablish a bijection between convex rank tests and probabilistic conditional\nindependence structures known as semigraphoids. The subclass of submodular rank\ntests is derived from faces of the cone of submodular functions, or from\nMinkowski summands of the permutohedron. We enumerate all small instances of\nsuch rank tests. Of particular interest are graphical tests, which correspond\nto both graphical models and to graph associahedra. \n\n"}
{"id": "math/0703854", "contents": "Title: Fast learning rates in statistical inference through aggregation Abstract: We develop minimax optimal risk bounds for the general learning task\nconsisting in predicting as well as the best function in a reference set G up\nto the smallest possible additive term, called the convergence rate. When the\nreference set is finite and when n denotes the size of the training data, we\nprovide minimax convergence rates of the form C ([log |G|]/n)^v with tight\nevaluation of the positive constant C and with exact v in ]0;1], the latter\nvalue depending on the convexity of the loss function and on the level of noise\nin the output distribution. The risk upper bounds are based on a sequential\nrandomized algorithm, which at each step concentrates on functions having both\nlow risk and low variance with respect to the previous step prediction\nfunction. Our analysis puts forward the links between the probabilistic and\nworst-case viewpoints, and allows to obtain risk bounds unachievable with the\nstandard statistical learning approach. One of the key idea of this work is to\nuse probabilistic inequalities with respect to appropriate (Gibbs)\ndistributions on the prediction function space instead of using them with\nrespect to the distribution generating the data. The risk lower bounds are\nbased on refinements of the Assouad's lemma taking particularly into account\nthe properties of the loss function. Our key example to illustrate the upper\nand lower bounds is to consider the L_q-regression setting for which an\nexhaustive analysis of the convergence rates is given while q describes\n[1;+infinity[. \n\n"}
{"id": "q-bio/0406048", "contents": "Title: How much can evolved characters tell us about the tree that generated\n  them? Abstract: In this paper we review some recent results that shed light on a fundamental\nquestion in molecular systematics: how much phylogenetic `signal' can we expect\nfrom characters that have evolved under some Markov process? There are many\nsides to this question and we begin by describing some explicit bounds on the\nprobability of correctly reconstructing an ancestral state from the states\nobserved at the tips. We show how this bound sets upper limits on the\nprobability of tree reconstruction from aligned sequences, and we provide some\nnew extensions that allow site-to-site rate variation or a covarion mechanism.\nWe then explore the relationship between the number of sites required for\naccurate tree reconstruction and other model parameters - such as the number of\nspecies, and substitution probabilities, and we describe a phase transition\nthat occurs when substitution probabilities exceed a critical value. In the\nremainder of this paper we turn to models of character evolution where the\nstate space is assumed to be either infinite or very large. These models have\nsome relevance to certain types of genomic data (such as gene order) and here\nwe again investigate how many characters are required for accurate tree\nreconstruction. \n\n"}
{"id": "q-bio/0410033", "contents": "Title: Four basic symmetry types in the universal 7-cluster structure of 143\n  complete bacterial genomic sequences Abstract: Coding information is the main source of heterogeneity (non-randomness) in\nthe sequences of bacterial genomes. This information can be naturally modeled\nby analysing cluster structures in the \"in-phase\" triplet distributions of\nrelatively short genomic fragments (200-400bp). We found a universal 7-cluster\nstructure in bacterial genomic sequences and explained its properties. We show\nthat codon usage of bacterial genomes is a multi-linear function of their\ngenomic G+C-content with high accuracy. Based on the analysis of 143 completely\nsequenced bacterial genomes available in Genbank in August 2004, we show that\nthere are four \"pure\" types of the 7-cluster structure observed. All 143\ncluster animated 3D-scatters are collected in a database and is made available\non our web-site: http://www.ihes.fr/~zinovyev/7clusters The finding can be\nreadily introduced into any software for gene prediction, sequence alignment or\nbacterial genomes classification. \n\n"}
{"id": "q-bio/0603031", "contents": "Title: A Mutagenetic Tree Hidden Markov Model for Longitudinal Clonal HIV\n  Sequence Data Abstract: RNA viruses provide prominent examples of measurably evolving populations. In\nHIV infection, the development of drug resistance is of particular interest,\nbecause precise predictions of the outcome of this evolutionary process are a\nprerequisite for the rational design of antiretroviral treatment protocols. We\npresent a mutagenetic tree hidden Markov model for the analysis of longitudinal\nclonal sequence data. Using HIV mutation data from clinical trials, we estimate\nthe order and rate of occurrence of seven amino acid changes that are\nassociated with resistance to the reverse transcriptase inhibitor efavirenz. \n\n"}
