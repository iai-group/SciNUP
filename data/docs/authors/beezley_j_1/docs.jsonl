{"id": "0704.0303", "contents": "Title: Measurement of the Aerosol Phase Function at the Pierre Auger\n  Observatory Abstract: Air fluorescence detectors measure the energy of ultra-high energy cosmic\nrays by collecting fluorescence light emitted from nitrogen molecules along the\nextensive air shower cascade. To ensure a reliable energy determination, the\nlight signal needs to be corrected for atmospheric effects, which not only\nattenuate the signal, but also produce a non-negligible background component\ndue to scattered Cherenkov light and multiple-scattered light. The correction\nrequires regular measurements of the aerosol attenuation length and the aerosol\nphase function, defined as the probability of light scattered in a given\ndirection. At the Pierre Auger Observatory in Malargue, Argentina, the phase\nfunction is measured on an hourly basis using two Aerosol Phase Function (APF)\nlight sources. These sources direct a UV light beam across the field of view of\nthe fluorescence detectors; the phase function can be extracted from the image\nof the shots in the fluorescence detector cameras. This paper describes the\ndesign, current status, standard operation procedure, and performance of the\nAPF system at the Pierre Auger Observatory. \n\n"}
{"id": "0705.2838", "contents": "Title: Coexistence of Weak and Strong Wave Turbulence in a Swell Propagation Abstract: By performing two parallel numerical experiments -- solving the dynamical\nHamiltonian equations and solving the Hasselmann kinetic equation -- we\nexamined the applicability of the theory of weak turbulence to the description\nof the time evolution of an ensemble of free surface waves (a swell) on deep\nwater. We observed qualitative coincidence of the results.\n  To achieve quantitative coincidence, we augmented the kinetic equation by an\nempirical dissipation term modelling the strongly nonlinear process of\nwhite-capping. Fitting the two experiments, we determined the dissipation\nfunction due to wave breaking and found that it depends very sharply on the\nparameter of nonlinearity (the surface steepness). The onset of white-capping\ncan be compared to a second-order phase transition. This result corroborates\nwith experimental observations by Banner, Babanin, Young. \n\n"}
{"id": "0705.4563", "contents": "Title: Kalman Filtering in the Presence of State Space Equality Constraints Abstract: We discuss two separate techniques for Kalman Filtering in the presence of\nstate space equality constraints. We then prove that despite the lack of\nsimilarity in their formulations, under certain conditions, the two methods\nresult in mathematically equivalent constrained estimate structures. We\nconclude that the potential benefits of using equality constraints in Kalman\nFiltering often outweigh the computational costs, and as such, equality\nconstraints, when present, should be enforced by way of one of these two\nmethods. \n\n"}
{"id": "0706.3520", "contents": "Title: On probabilities for separating sets of order statistics Abstract: Consider a set of order statistics that arise from sorting samples from two\ndifferent populations, each with their own, possibly different distribution\nfunction. The probability that these order statistics fall in disjoint, ordered\nintervals, and that of the smallest statistics, a certain number come from the\nfirst populations, are given in terms of the two distribution functions. The\nresult is applied to computing the joint probability of the number of\nrejections and the number of false rejections for the Benjamini-Hochberg false\ndiscovery rate procedure. \n\n"}
{"id": "0706.4128", "contents": "Title: A review of wildland fire spread modelling, 1990-present 2: Empirical\n  and quasi-empirical models Abstract: In recent years, advances in computational power and spatial data analysis\n(GIS, remote sensing, etc) have led to an increase in attempts to model the\nspread and behaviour of wildland fires across the landscape. This series of\nreview papers endeavours to critically and comprehensively review all types of\nsurface fire spread models developed since 1990. This paper reviews models of\nan empirical or quasi-empirical nature. These models are based solely on the\nstatistical analysis of experimentally obtained data with or without some\nphysical framework for the basis of the relations. Other papers in the series\nreview models of a physical or quasi-physical nature, and mathematical\nanalogues and simulation models. The main relations of empirical models are\nthat of wind speed and fuel moisture content with rate of forward spread.\nComparisons are made of the different functional relationships selected by\nvarious authors for these variables. \n\n"}
{"id": "0706.4130", "contents": "Title: A review of wildland fire spread modelling, 1990-present 3: Mathematical\n  analogues and simulation models Abstract: In recent years, advances in computational power and spatial data analysis\n(GIS, remote sensing, etc) have led to an increase in attempts to model the\nspread and behvaiour of wildland fires across the landscape. This series of\nreview papers endeavours to critically and comprehensively review all types of\nsurface fire spread models developed since 1990. This paper reviews models of a\nsimulation or mathematical analogue nature. Most simulation models are\nimplementations of existing empirical or quasi-empirical models and their\nprimary function is to convert these generally one dimensional models to two\ndimensions and then propagate a fire perimeter across a modelled landscape.\nMathematical analogue models are those that are based on some mathematical\nconceit (rather than a physical representation of fire spread) that\ncoincidentally simulates the spread of fire. Other papers in the series review\nmodels of an physical or quasi-physical nature and empirical or quasi-empirical\nnature. Many models are extensions or refinements of models developed before\n1990. Where this is the case, these models are also discussed but much less\ncomprehensively. \n\n"}
{"id": "0707.1161", "contents": "Title: Falsification Of The Atmospheric CO2 Greenhouse Effects Within The Frame\n  Of Physics Abstract: The atmospheric greenhouse effect, an idea that many authors trace back to\nthe traditional works of Fourier (1824), Tyndall (1861), and Arrhenius (1896),\nand which is still supported in global climatology, essentially describes a\nfictitious mechanism, in which a planetary atmosphere acts as a heat pump\ndriven by an environment that is radiatively interacting with but radiatively\nequilibrated to the atmospheric system. According to the second law of\nthermodynamics such a planetary machine can never exist. Nevertheless, in\nalmost all texts of global climatology and in a widespread secondary literature\nit is taken for granted that such mechanism is real and stands on a firm\nscientific foundation. In this paper the popular conjecture is analyzed and the\nunderlying physical principles are clarified. By showing that (a) there are no\ncommon physical laws between the warming phenomenon in glass houses and the\nfictitious atmospheric greenhouse effects, (b) there are no calculations to\ndetermine an average surface temperature of a planet, (c) the frequently\nmentioned difference of 33 degrees Celsius is a meaningless number calculated\nwrongly, (d) the formulas of cavity radiation are used inappropriately, (e) the\nassumption of a radiative balance is unphysical, (f) thermal conductivity and\nfriction must not be set to zero, the atmospheric greenhouse conjecture is\nfalsified. \n\n"}
{"id": "0707.4558", "contents": "Title: Open Problems in Algebraic Statistics Abstract: Algebraic statistics is concerned with the study of probabilistic models and\ntechniques for statistical inference using methods from algebra and geometry.\nThis article presents a list of open mathematical problems in this emerging\nfield, with main emphasis on graphical models with hidden variables, maximum\nlikelihood estimation, and multivariate Gaussian distributions. This article is\nbased on a lecture presented at the IMA in Minneapolis during the 2006/07\nprogram on Applications of Algebraic Geometry. \n\n"}
{"id": "0708.1593", "contents": "Title: Updating Probabilities with Data and Moments Abstract: We use the method of Maximum (relative) Entropy to process information in the\nform of observed data and moment constraints. The generic \"canonical\" form of\nthe posterior distribution for the problem of simultaneous updating with data\nand moments is obtained. We discuss the general problem of non-commuting\nconstraints, when they should be processed sequentially and when\nsimultaneously. As an illustration, the multinomial example of die tosses is\nsolved in detail for two superficially similar but actually very different\nproblems. \n\n"}
{"id": "0710.0317", "contents": "Title: A General Strategy for Physics-Based Model Validation Illustrated with\n  Earthquake Phenomenology, Atmospheric Radiative Transfer, and Computational\n  Fluid Dynamics Abstract: Validation is often defined as the process of determining the degree to which\na model is an accurate representation of the real world from the perspective of\nits intended uses. Validation is crucial as industries and governments depend\nincreasingly on predictions by computer models to justify their decisions. In\nthis article, we survey the model validation literature and propose to\nformulate validation as an iterative construction process that mimics the\nprocess occurring implicitly in the minds of scientists. We thus offer a formal\nrepresentation of the progressive build-up of trust in the model, and thereby\nreplace incapacitating claims on the impossibility of validating a given model\nby an adaptive process of constructive approximation. This approach is better\nadapted to the fuzzy, coarse-grained nature of validation. Our procedure\nfactors in the degree of redundancy versus novelty of the experiments used for\nvalidation as well as the degree to which the model predicts the observations.\nWe illustrate the new methodology first with the maturation of Quantum\nMechanics as the arguably best established physics theory and then with several\nconcrete examples drawn from some of our primary scientific interests: a\ncellular automaton model for earthquakes, an anomalous diffusion model for\nsolar radiation transport in the cloudy atmosphere, and a computational fluid\ndynamics code for the Richtmyer-Meshkov instability. This article is an\naugmented version of Sornette et al. [2007] that appeared in Proceedings of the\nNational Academy of Sciences in 2007 (doi: 10.1073/pnas.0611677104), with an\nelectronic supplement at URL\nhttp://www.pnas.org/cgi/content/full/0611677104/DC1. Sornette et al. [2007] is\nalso available in preprint form at physics/0511219. \n\n"}
{"id": "0712.3187", "contents": "Title: On the Korteweg-de Vries approximation for uneven bottoms Abstract: In this paper we focus on the water waves problem for uneven bottoms on a\ntwo-dimensionnal domain. Starting from the symmetric Boussinesq systems derived\nin [Chazel, Influence of topography on long water waves, 2007], we recover the\nuncoupled Korteweg-de Vries (KdV) approximation justified by Schneider and\nWayne for flat bottoms, and by Iguchi in the context of bottoms tending to zero\nat infinity at a substantial rate. The goal of this paper is to investigate the\nvalidity of this approximation for more general bathymetries. We exhibit two\nkinds of topography for which this approximation diverges from the Boussinesq\nsolutions. A topographically modified KdV approximation is then proposed to\ndeal with such bathymetries. Finally, all the models involved are numerically\ncomputed and compared. \n\n"}
{"id": "0712.3920", "contents": "Title: Asymptotic Models for Internal Waves Abstract: We derived here in a systematic way, and for a large class of scaling\nregimes, asymptotic models for the propagation of internal waves at the\ninterface between two layers of immiscible fluids of different densities, under\nthe rigid lid assumption and with a flat bottom. The full (Euler) model for\nthis situation is reduced to a system of evolution equations posed spatially on\n$\\R^d$, $d=1,2$, which involve two nonlocal operators. The different asymptotic\nmodels are obtained by expanding the nonlocal operators with respect to\nsuitable small parameters that depend variously on the amplitude, wave-lengths\nand depth ratio of the two layers. We rigorously derive classical models and\nalso some model systems that appear to be new. Furthermore, the consistency of\nthese asymptotic systems with the full Euler equations is established. \n\n"}
{"id": "0801.3875", "contents": "Title: Towards a Real-Time Data Driven Wildland Fire Model Abstract: A wildland fire model based on semi-empirical relations for the spread rate\nof a surface fire and post-frontal heat release is coupled with the Weather\nResearch and Forecasting atmospheric model (WRF). The propagation of the fire\nfront is implemented by a level set method. Data is assimilated by a morphing\nensemble Kalman filter, which provides amplitude as well as position\ncorrections. Thermal images of a fire will provide the observations and will be\ncompared to a synthetic image from the model state. \n\n"}
{"id": "0802.1218", "contents": "Title: Detecting the overlapping and hierarchical community structure of\n  complex networks Abstract: Many networks in nature, society and technology are characterized by a\nmesoscopic level of organization, with groups of nodes forming tightly\nconnected units, called communities or modules, that are only weakly linked to\neach other. Uncovering this community structure is one of the most important\nproblems in the field of complex networks. Networks often show a hierarchical\norganization, with communities embedded within other communities; moreover,\nnodes can be shared between different communities. Here we present the first\nalgorithm that finds both overlapping communities and the hierarchical\nstructure. The method is based on the local optimization of a fitness function.\nCommunity structure is revealed by peaks in the fitness histogram. The\nresolution can be tuned by a parameter enabling to investigate different\nhierarchical levels of organization. Tests on real and artificial networks give\nexcellent results. \n\n"}
{"id": "0802.1615", "contents": "Title: Real-Time Data Driven Wildland Fire Modeling Abstract: We are developing a wildland fire model based on semi-empirical relations\nthat estimate the rate of spread of a surface fire and post-frontal heat\nrelease, coupled with WRF, the Weather Research and Forecasting atmospheric\nmodel. A level set method identifies the fire front. Data are assimilated using\nboth amplitude and position corrections using a morphing ensemble Kalman\nfilter. We will use thermal images of a fire for observations that will be\ncompared to synthetic image based on the model state. \n\n"}
{"id": "0802.4324", "contents": "Title: Proof of the Atmospheric Greenhouse Effect Abstract: A recently advanced argument against the atmospheric greenhouse effect is\nrefuted. A planet without an infrared absorbing atmosphere is mathematically\nconstrained to have an average temperature less than or equal to the effective\nradiating temperature. Observed parameters for Earth prove that without\ninfrared absorption by the atmosphere, the average temperature of Earth's\nsurface would be at least 33 K lower than what is observed. \n\n"}
{"id": "0803.2298", "contents": "Title: Testing the proposed link between cosmic rays and cloud cover Abstract: A decrease in the globally averaged low level cloud cover, deduced from the\nISCCP infra red data, as the cosmic ray intensity decreased during the solar\ncycle 22 was observed by two groups. The groups went on to hypothesise that the\ndecrease in ionization due to cosmic rays causes the decrease in cloud cover,\nthereby explaining a large part of the presently observed global warming. We\nhave examined this hypothesis to look for evidence to corroborate it. None has\nbeen found and so our conclusions are to doubt it. From the absence of\ncorroborative evidence, we estimate that less than 23%, at the 95% confidence\nlevel, of the 11-year cycle change in the globally averaged cloud cover\nobserved in solar cycle 22 is due to the change in the rate of ionization from\nthe solar modulation of cosmic rays. \n\n"}
{"id": "0803.4458", "contents": "Title: Rayleigh-B\\'enard Convection as a Nambu-metriplectic problem Abstract: The traditional Hamiltonian structure of the equations governing conservative\nRayleigh-B\\'enard convection (RBC) is singular, i.e. it's Poisson bracket\npossesses nontrivial Casimir functionals. We show that a special form of one of\nthese Casimirs can be used to extend the bilinear Poisson bracket to a\ntrilinear generalised Nambu bracket. It is further shown that the equations\ngoverning dissipative RBC can be written as the superposition of the\nconservative Nambu bracket with a dissipative symmetric bracket. This leads to\na Nambu-metriplectic system, which completes the geometrical picture of RBC. \n\n"}
{"id": "0804.3207", "contents": "Title: Lookup tables to compute high energy cosmic ray induced atmospheric\n  ionization and changes in atmospheric chemistry Abstract: A variety of events such as gamma-ray bursts and supernovae may expose the\nEarth to an increased flux of high-energy cosmic rays, with potentially\nimportant effects on the biosphere. Existing atmospheric chemistry software\ndoes not have the capability of incorporating the effects of substantial cosmic\nray flux above 10 GeV . An atmospheric code, the NASA-Goddard Space Flight\nCenter two-dimensional (latitude, altitude) time-dependent atmospheric model\n(NGSFC), is used to study atmospheric chemistry changes. Using CORSIKA, we have\ncreated tables that can be used to compute high energy cosmic ray (10 GeV - 1\nPeV) induced atmospheric ionization and also, with the use of the NGSFC code,\ncan be used to simulate the resulting atmospheric chemistry changes. We discuss\nthe tables, their uses, weaknesses, and strengths. \n\n"}
{"id": "0804.3989", "contents": "Title: Maximum likelihood estimation of a multidimensional log-concave density Abstract: Let X_1, ..., X_n be independent and identically distributed random vectors\nwith a log-concave (Lebesgue) density f. We first prove that, with probability\none, there exists a unique maximum likelihood estimator of f. The use of this\nestimator is attractive because, unlike kernel density estimation, the method\nis fully automatic, with no smoothing parameters to choose. Although the\nexistence proof is non-constructive, we are able to reformulate the issue of\ncomputation in terms of a non-differentiable convex optimisation problem, and\nthus combine techniques of computational geometry with Shor's r-algorithm to\nproduce a sequence that converges to the maximum likelihood estimate. For the\nmoderate or large sample sizes in our simulations, the maximum likelihood\nestimator is shown to provide an improvement in performance compared with\nkernel-based methods, even when we allow the use of a theoretical, optimal\nfixed bandwidth for the kernel estimator that would not be available in\npractice. We also present a real data clustering example, which shows that our\nmethodology can be used in conjunction with the Expectation--Maximisation (EM)\nalgorithm to fit finite mixtures of log-concave densities. An R version of the\nalgorithm is available in the package LogConcDEAD -- Log-Concave Density\nEstimation in Arbitrary Dimensions. \n\n"}
{"id": "0805.0445", "contents": "Title: Simultaneous numerical simulation of direct and inverse cascades in wave\n  turbulence Abstract: Results of direct numerical simulation of isotropic turbulence of surface\ngravity waves in the framework of Hamiltonian equations are presented. For the\nfirst time simultaneous formation of both direct and inverse cascades was\nobserved in the framework of primordial dynamical equations. At the same time,\nstrong long waves background was developed. It was shown, that obtained\nKolmogorov spectra are very sensitive to the presence of this condensate. Such\nsituation has to be typical for experimental wave tanks, flumes, and small\nlakes. \n\n"}
{"id": "0805.3602", "contents": "Title: Marginal Likelihood Integrals for Mixtures of Independence Models Abstract: Inference in Bayesian statistics involves the evaluation of marginal\nlikelihood integrals. We present algebraic algorithms for computing such\nintegrals exactly for discrete data of small sample size. Our methods apply to\nboth uniform priors and Dirichlet priors. The underlying statistical models are\nmixtures of independent distributions, or, in geometric language, secant\nvarieties of Segre-Veronese varieties. \n\n"}
{"id": "0807.0274", "contents": "Title: Modelling of anthropogenic pollutant diffusion in the atmosphere and\n  applications to civil protection monitoring Abstract: A basic feature of fluid mechanics concerns the frictionless phase-space\ndynamics of particles in an incompressible fluid. The issue, besides its\ntheoretical interest in turbulence theory, is important in many applications,\nsuch as the pollutant dynamics in the atmosphere, a problem relevant for civil\nprotection monitoring of air quality. Actually, both the numerical simulation\nof the ABL (atmospheric boundary layer) portion of the atmosphere and that of\npollutant dynamics may generally require the correct definition of the\nLagrangian dynamics which characterizes arbitrary fluid elements of\nincompressible thermofluids. We claim that particularly important for\napplications would be to consider these trajectories as phase-space\ntrajectories. This involves, however, the unfolding of a fundamental\ntheoretical problem up to now substantially unsolved: {\\it namely the\ndetermination of the exact frictionless dynamics of tracer particles in an\nincompressible fluid, treated either as a deterministic or a turbulent (i.e.,\nstochastic) continuum.} In this paper we intend to formulate the necessary\ntheoretical framework to construct such a type of description. This is based on\na phase-space inverse kinetic theory (IKT) approach recently developed for\nincompressible fluids (Ellero \\textit{et al.}, 2004-2008). {\\it Our claim is\nthat the conditional frictionless dynamics of a tracer particles - which\ncorresponds to a prescribed velocity probability density and an arbitrary\nchoice of the relevant fluid fields - can be exactly specified}. \n\n"}
{"id": "0807.4180", "contents": "Title: Habitable Climates: The Influence of Obliquity Abstract: Extrasolar terrestrial planets with the potential to host life might have\nlarge obliquities or be subject to strong obliquity variations. We revisit the\nhabitability of oblique planets with an energy balance climate model (EBM)\nallowing for dynamical transitions to ice-covered snowball states as a result\nof ice-albedo feedback. Despite the great simplicity of our EBM, it captures\nreasonably well the seasonal cycle of global energetic fluxes at Earth's\nsurface. It also performs satisfactorily against a full-physics climate model\nof a highly oblique Earth-like planet, in an unusual regime of circulation\ndominated by heat transport from the poles to the equator. Climates on oblique\nterrestrial planets can violate global radiative balance through much of their\nseasonal cycle, which limits the usefulness of simple radiative equilibrium\narguments. High obliquity planets have severe climates, with large amplitude\nseasonal variations, but they are not necessarily more prone to global snowball\ntransitions than low obliquity planets. We find that terrestrial planets with\nmassive CO2 atmospheres, typically expected in the outer regions of habitable\nzones, can also be subject to such dynamical snowball transitions. Some of the\nsnowball climates investigated for CO2-rich atmospheres experience partial\natmospheric collapse. Since long-term CO2 atmospheric build-up acts as a\nclimatic thermostat for habitable planets, partial CO2 collapse could limit the\nhabitability of such planets. A terrestrial planet's habitability may thus\ndepend sensitively on its short-term climatic stability. \n\n"}
{"id": "0809.0869", "contents": "Title: Helicity cascades in rotating turbulence Abstract: The effect of helicity (velocity-vorticity correlations) is studied in direct\nnumerical simulations of rotating turbulence down to Rossby numbers of 0.02.\nThe results suggest that the presence of net helicity plays an important role\nin the dynamics of the flow. In particular, at small Rossby number, the energy\ncascades to large scales, as expected, but helicity then can dominate the\ncascade to small scales. A phenomenological interpretation in terms of a direct\ncascade of helicity slowed down by wave-eddy interactions leads to the\nprediction of new inertial indices for the small-scale energy and helicity\nspectra. \n\n"}
{"id": "0809.4178", "contents": "Title: Non-linear regression models for Approximate Bayesian Computation Abstract: Approximate Bayesian inference on the basis of summary statistics is\nwell-suited to complex problems for which the likelihood is either\nmathematically or computationally intractable. However the methods that use\nrejection suffer from the curse of dimensionality when the number of summary\nstatistics is increased. Here we propose a machine-learning approach to the\nestimation of the posterior density by introducing two innovations. The new\nmethod fits a nonlinear conditional heteroscedastic regression of the parameter\non the summary statistics, and then adaptively improves estimation using\nimportance sampling. The new algorithm is compared to the state-of-the-art\napproximate Bayesian methods, and achieves considerable reduction of the\ncomputational burden in two examples of inference in statistical genetics and\nin a queueing model. \n\n"}
{"id": "0811.2785", "contents": "Title: Chaos Phenotypes in Fluids Abstract: I shall briefly survey the current status on more rigorous studies of chaos\nin fluids by focusing along the line of chaos phenotypes: sensitive dependence\non initial data, and recurrence. \n\n"}
{"id": "0811.4373", "contents": "Title: Effect of helicity and rotation on the free decay of turbulent flows Abstract: The self-similar decay of energy in a turbulent flow is studied in direct\nnumerical simulations with and without rotation. Two initial conditions are\nconsidered: one non-helical (mirror-symmetric), and one with maximal helicity.\nThe results show that, while in the absence of rotation the energy in the\nhelical and non-helical cases decays with the same rate, in rotating flows the\nhelicity content has a major impact on the decay rate. These differences are\nassociated with differences in the energy and helicity cascades when rotation\nis present. Properties of the structures that arise in the flow at late times\nin each time are also discussed. \n\n"}
{"id": "0812.4846", "contents": "Title: El Nino Southern Oscillation as Sporadic Oscillations between Metastable\n  States Abstract: The main objective of this article is to establish a new mechanism of the El\nNino Southern Oscillation (ENSO), as a self-organizing and self-excitation\nsystem, with two highly coupled processes. The first is the oscillation between\nthe two metastable warm (El Nino phase) and cold events (La Nina phase), and\nthe second is the spatiotemporal oscillation of the sea surface temperature\n(SST) field. The interplay between these two processes gives rises the climate\nvariability associated with the ENSO, leads to both the random and\ndeterministic features of the ENSO, and defines a new natural feedback\nmechanism, which drives the sporadic oscillation of the ENSO. The new mechanism\nis rigorously derived using a dynamic transition theory developed recently by\nthe authors, which has also been successfully applied to a wide range of\nproblems in nonlinear sciences. \n\n"}
{"id": "0901.0401", "contents": "Title: From Physics to Economics: An Econometric Example Using Maximum Relative\n  Entropy Abstract: Econophysics, is based on the premise that some ideas and methods from\nphysics can be applied to economic situations. We intend to show in this paper\nhow a physics concept such as entropy can be applied to an economic problem. In\nso doing, we demonstrate how information in the form of observable data and\nmoment constraints are introduced into the method of Maximum relative Entropy\n(MrE). A general example of updating with data and moments is shown. Two\nspecific econometric examples are solved in detail which can then be used as\ntemplates for real world problems. A numerical example is compared to a large\ndeviation solution which illustrates some of the advantages of the MrE method. \n\n"}
{"id": "0901.0515", "contents": "Title: Solar activity and the mean global temperature Abstract: The variation with time from 1956-2002 of the globally averaged rate of\nionization produced by cosmic rays in the atmosphere is deduced and shown to\nhave a cyclic component of period roughly twice the 11 year solar cycle period.\nLong term variations in the global average surface temperature as a function of\ntime since 1956 are found to have a similar cyclic component. The cyclic\nvariations are also observed in the solar irradiance and in the mean daily sun\nspot number. The cyclic variation in the cosmic ray rate is observed to be\ndelayed by 2-4 years relative to the temperature, the solar irradiance and\ndaily sun spot variations suggesting that the origin of the correlation is more\nlikely to be direct solar activity than cosmic rays. Assuming that the\ncorrelation is caused by such solar activity, we deduce that the maximum recent\nincrease in the mean surface temperature of the Earth which can be ascribed to\nthis activity is $\\lesssim14%$ of the observed global warming. \n\n"}
{"id": "0901.1461", "contents": "Title: The climate version of the Eta regional forecast model. 2.Evaluation of\n  the Eta CCS model performance against reanalysis data and surface\n  observations Abstract: The climate version Eta CCS, prepared from the NCEP Eta forecast model, was\nintegrated over South America for the period from January 1979 to December\n1983. The model was driven by the two sets of boundary conditions derived from\nthe reanalysis and outputs of HadAM3P atmospheric global model. The mean output\nfields of precipitation, precipitation frequency, and near surface air\ntemperature, simulated by the Eta model, were compared with the observational\ndata of the CRU and GPCP projects. This comparison shows that the Eta model\nreproduces well the main patterns of the summer and winter observed\nprecipitation fields over South America. But the magnitude of precipitation is\nunderestimated by the ETA CCS model in the regions of strong convection\nactivity in summer. This underestimation of observed precipitation is larger\nfor the Eta model driven by HadAM3P than by the reanalysis. The observed number\nof wet days is overestimated by HadAM3P. The number of wet days in both runs of\nthe Eta model is closer to observations. The main summer and winter patterns of\nnear surface air temperature are reproduced well by both HadAM3P and the Eta\nmodel. The Eta model overestimates the observed surface temperature over the\ncentral part of the continent due to the lack of convective cloudiness in this\nregion. The Eta model captures observed annual cycle of precipitation in six\nselected regions over South America. On the whole, these results support the\nconclusion that the Eta model with some improvements can be used for\ndownscaling of the HadAM3P output fields. \n\n"}
{"id": "0901.3725", "contents": "Title: A Brief Tutorial on the Ensemble Kalman Filter Abstract: The ensemble Kalman filter (EnKF) is a recursive filter suitable for problems\nwith a large number of variables, such as discretizations of partial\ndifferential equations in geophysical models. The EnKF originated as a version\nof the Kalman filter for large problems (essentially, the covariance matrix is\nreplaced by the sample covariance), and it is now an important data\nassimilation component of ensemble forecasting. EnKF is related to the particle\nfilter (in this context, a particle is the same thing as an ensemble member)\nbut the EnKF makes the assumption that all probability distributions involved\nare Gaussian. This article briefly describes the derivation and practical\nimplementation of the basic version of EnKF, and reviews several extensions. \n\n"}
{"id": "0902.4596", "contents": "Title: Formation of Cooper pairs in quantum oscillations of electrons in plasma Abstract: We study low energy quantum oscillations of electron gas in plasma. It is\nshown that two electrons participating in these oscillations acquire additional\nnegative energy when they interact by means of a virtual plasmon. The\nadditional energy leads to the formation a Cooper pair and possible existence\nof the superconducting phase in the system. We suggest that this mechanism\nsupports slowly damping oscillations of electrons without any energy supply.\nBasing on our model we put forward the hypothesis the superconductivity can\noccur in a low energy ball lightning. \n\n"}
{"id": "0903.2795", "contents": "Title: Chaotic response of global climate to long-term solar forcing\n  variability Abstract: It is shown that global climate exhibits chaotic response to solar forcing\nvariability in a vast range of timescales: from annual to multi-millennium.\nUnlike linear systems, where periodic forcing leads to periodic response,\nnonlinear chaotic response to periodic forcing can result in exponentially\ndecaying broad-band power spectrum with decay rate T_e equal to the period of\nthe forcing. It is shown that power spectrum of a reconstructed time series of\nNorthern Hemisphere temperature anomaly for the past 2,000 years has an\nexponentially decaying broad-band part with T_e = 11 yr, i.e. the observed\ndecay rate T_e equals the mean period of the solar activity. It is also shown\nthat power spectrum of a reconstruction of atmospheric CO_2 time fluctuations\nfor the past 650,000 years, has an exponentially decaying broad-band part with\nT_e = 41,000 years, i.e. the observed decay rate T_e equals the period of the\nobliquity periodic forcing. A possibility of a chaotic solar forcing of the\nclimate has been also discussed. These results clarify role of solar forcing\nvariability in long-term global climate dynamics (in particular in the unsolved\nproblem of the glaciation cycles) and help in construction of adequate dynamic\nmodels of the global climate. \n\n"}
{"id": "0904.0691", "contents": "Title: Convex Optimization Methods for Dimension Reduction and Coefficient\n  Estimation in Multivariate Linear Regression Abstract: In this paper, we study convex optimization methods for computing the trace\nnorm regularized least squares estimate in multivariate linear regression. The\nso-called factor estimation and selection (FES) method, recently proposed by\nYuan et al. [22], conducts parameter estimation and factor selection\nsimultaneously and have been shown to enjoy nice properties in both large and\nfinite samples. To compute the estimates, however, can be very challenging in\npractice because of the high dimensionality and the trace norm constraint. In\nthis paper, we explore a variant of Nesterov's smooth method [20] and interior\npoint methods for computing the penalized least squares estimate. The\nperformance of these methods is then compared using a set of randomly generated\ninstances. We show that the variant of Nesterov's smooth method [20] generally\noutperforms the interior point method implemented in SDPT3 version 4.0 (beta)\n[19] substantially . Moreover, the former method is much more memory efficient. \n\n"}
{"id": "0904.0709", "contents": "Title: Lagrangian views on turbulent mixing of passive scalars Abstract: The Lagrangian view of passive scalar turbulence has recently produced\ninteresting results and interpretations. Innovations in theory, experiments,\nsimulations and data analysis of Lagrangian turbulence are reviewed here in\nbrief. Part of the review is closely related to the so-called Kraichnan model\nfor the advection of the passive scalar in synthetic turbulence. Possible\nimplications for a better understanding of the passive scalar mixing in\nNavier-Stokes turbulence are also discussed. \n\n"}
{"id": "0904.0818", "contents": "Title: Self-Similar Solutions in the Homogeneous Isotropic Turbulence Abstract: We calculate the self-similar longitudinal velocity correlation function, the\nenergy spectrum and the corresponding other properties using a theory on the\nisotropic homogeneous turbulence just presented by the author in a previous\nwork. The correlation functions correspond to steady-state solutions of the\nevolution equation under the self-similarity hypothesis introduced by von\nK\\'arm\\'an. These solutions are numerically calculated and the results\nadequately describe several properties of the isotropic turbulence. \n\n"}
{"id": "0904.2435", "contents": "Title: Computation of confidence intervals in regression utilizing uncertain\n  prior information Abstract: We consider a linear regression model with regression parameter beta\n=(beta_1, ..., beta_p) and independent and identically N(0, sigma^2)distributed\nerrors. Suppose that the parameter of interest is theta = a^T beta where a is a\nspecified vector. Define the parameter tau = c^T beta - t where the vector c\nand the number t are specified and a and c are linearly independent. Also\nsuppose that we have uncertain prior information that tau = 0. Kabaila and Giri\n(2009c) present a new frequentist 1-alpha confidence interval for theta that\nutilizes this prior information. This interval has expected length that (a) is\nrelatively small when the prior information about tau is correct and (b) has a\nmaximum value that is not too large. It coincides with the standard 1-alpha\nconfidence interval (obtained by fitting the full model to the data) when the\ndata strongly contradicts the prior information. At first sight, the\ncomputation of this new confidence interval seems to be infeasible. However, by\nthe use of the various computational devices that are presented in detail in\nthe present paper, this computation becomes feasible and practicable. \n\n"}
{"id": "0904.3488", "contents": "Title: Critical balance in magnetohydrodynamic, rotating and stratified\n  turbulence: towards a universal scaling conjecture Abstract: It is proposed that critical balance - a scale-by-scale balance between the\nlinear propagation and nonlinear interaction time scales - can be used as a\nuniversal scaling conjecture for determining the spectra of strong turbulence\nin anisotropic wave systems. Magnetohydrodynamic (MHD), rotating and stratified\nturbulence are considered under this assumption and, in particular, a novel and\nexperimentally testable energy cascade scenario and a set of scalings of the\nspectra are proposed for low-Rossby-number rotating turbulence. It is argued\nthat in neutral fluids, the critically balanced anisotropic cascade provides a\nnatural path from strong anisotropy at large scales to isotropic Kolmogorov\nturbulence at very small scales. It is also argued that the kperp^{-2} spectra\nseen in recent numerical simulations of low-Rossby-number rotating turbulence\nmay be analogous to the kperp^{-3/2} spectra of the numerical MHD turbulence in\nthe sense that they could be explained by assuming that fluctuations are\npolarised (aligned) approximately as inertial waves (Alfven waves for MHD). \n\n"}
{"id": "0905.0603", "contents": "Title: Regularized estimation of large-scale gene association networks using\n  graphical Gaussian models Abstract: Graphical Gaussian models are popular tools for the estimation of\n(undirected) gene association networks from microarray data. A key issue when\nthe number of variables greatly exceeds the number of samples is the estimation\nof the matrix of partial correlations. Since the (Moore-Penrose) inverse of the\nsample covariance matrix leads to poor estimates in this scenario, standard\nmethods are inappropriate and adequate regularization techniques are needed. In\nthis article, we investigate a general framework for combining regularized\nregression methods with the estimation of Graphical Gaussian models. This\nframework includes various existing methods as well as two new approaches based\non ridge regression and adaptive lasso, respectively. These methods are\nextensively compared both qualitatively and quantitatively within a simulation\nstudy and through an application to six diverse real data sets. In addition,\nall proposed algorithms are implemented in the R package \"parcor\", available\nfrom the R repository CRAN. \n\n"}
{"id": "0905.4602", "contents": "Title: A black box method for solving the complex exponentials approximation\n  problem Abstract: A common problem, arising in many different applied contexts, consists in\nestimating the number of exponentially damped sinusoids whose weighted sum best\nfits a finite set of noisy data and in estimating their parameters. Many\ndifferent methods exist to this purpose. The best of them are based on\napproximate Maximum Likelihood estimators, assuming to know the number of\ndamped sinusoids, which can then be estimated by an order selection procedure.\nAs the problem can be severely ill posed, a stochastic perturbation method is\nproposed which provides better results than Maximum Likelihood based methods\nwhen the signal-to-noise ratio is low. The method depends on some\nhyperparameters which turn out to be essentially independent of the\napplication. Therefore they can be fixed once and for all, giving rise to a\nblack box method. \n\n"}
{"id": "0906.1004", "contents": "Title: A Dynamic Programming Approach for Approximate Uniform Generation of\n  Binary Matrices with Specified Margins Abstract: Consider the collection of all binary matrices having a specific sequence of\nrow and column sums and consider sampling binary matrices uniformly from this\ncollection. Practical algorithms for exact uniform sampling are not known, but\nthere are practical algorithms for approximate uniform sampling. Here it is\nshown how dynamic programming and recent asymptotic enumeration results can be\nused to simplify and improve a certain class of approximate uniform samplers.\nThe dynamic programming perspective suggests interesting generalizations. \n\n"}
{"id": "0907.3521", "contents": "Title: A Numerical Approach to Performance Analysis of Quickest Change-Point\n  Detection Procedures Abstract: For the most popular sequential change detection rules such as CUSUM, EWMA,\nand the Shiryaev-Roberts test, we develop integral equations and a concise\nnumerical method to compute a number of performance metrics, including average\ndetection delay and average time to false alarm. We pay special attention to\nthe Shiryaev-Roberts procedure and evaluate its performance for various\ninitialization strategies. Regarding the randomized initialization variant\nproposed by Pollak, known to be asymptotically optimal of order-3, we offer a\nmeans for numerically computing the quasi-stationary distribution of the\nShiryaev-Roberts statistic that is the distribution of the initializing random\nvariable, thus making this test applicable in practice. A significant\nside-product of our computational technique is the observation that\ndeterministic initializations of the Shiryaev-Roberts procedure can also enjoy\nthe same order-3 optimality property as Pollak's randomized test and, after\ncareful selection, even uniformly outperform it. \n\n"}
{"id": "0907.4359", "contents": "Title: Complex networks in climate dynamics - Comparing linear and nonlinear\n  network construction methods Abstract: Complex network theory provides a powerful framework to statistically\ninvestigate the topology of local and non-local statistical interrelationships,\ni.e. teleconnections, in the climate system. Climate networks constructed from\nthe same global climatological data set using the linear Pearson correlation\ncoefficient or the nonlinear mutual information as a measure of dynamical\nsimilarity between regions, are compared systematically on local, mesoscopic\nand global topological scales. A high degree of similarity is observed on the\nlocal and mesoscopic topological scales for surface air temperature fields\ntaken from AOGCM and reanalysis data sets. We find larger differences on the\nglobal scale, particularly in the betweenness centrality field. The global\nscale view on climate networks obtained using mutual information offers\npromising new perspectives for detecting network structures based on nonlinear\nphysical processes in the climate system. \n\n"}
{"id": "0908.0132", "contents": "Title: Multifractal Detrended Cross-Correlation Analysis of Sunspot Numbers and\n  River Flow Fluctuations Abstract: We use the Detrended Cross-Correlation Analysis (DCCA) to investigate the\ninfluence of sun activity represented by sunspot numbers on one of the climate\nindicators, specifically rivers, represented by river flow fluctuation for\nDaugava, Holston, Nolichucky and French Broad rivers. The Multifractal\nDetrended Cross-Correlation Analysis (MF-DXA) shows that there exist some\ncrossovers in the cross-correlation fluctuation function versus time scale of\nthe river flow and sunspot series. One of these crossovers corresponds to the\nwell-known cycle of solar activity demonstrating a universal property of the\nmentioned rivers. The scaling exponent given by DCCA for original series at\nintermediate time scale, $(12-24)\\leq s\\leq 130$ months, is $\\lambda =\n1.17\\pm0.04$ which is almost similar for all underlying rivers at\n$1\\sigma$confidence interval showing the second universal behavior of river\nrunoffs. To remove the sinusoidal trends embedded in data sets, we apply the\nSingular Value Decomposition (SVD) method. Our results show that there exists a\nlong-range cross-correlation between the sunspot numbers and the underlying\nstreamflow records. The magnitude of the scaling exponent and the corresponding\ncross-correlation exponent are $\\lambda\\in (0.76, 0.85)$ and\n$\\gamma_{\\times}\\in(0.30, 0.48)$, respectively. Different values for scaling\nand cross-correlation exponents may be related to local and external factors\nsuch as topography, drainage network morphology, human activity and so on.\nMultifractal cross-correlation analysis demonstrates that all underlying\nfluctuations have almost weak multifractal nature which is also a universal\nproperty for data series. In addition the empirical relation between scaling\nexponent derived by DCCA and Detrended Fluctuation Analysis (DFA), $\n\\lambda\\approx(h_{\\rm sun} + h_{\\rm river})/2$ is confirmed. \n\n"}
{"id": "0908.1118", "contents": "Title: Drift wave-zonal flow dynamics Abstract: A remarkable phenomenon in turbulent flows is the spontaneous emergence of\ncoherent large spatial scale zonal jets. Geophysical examples of this\nphenomenon include the Jovian banded winds and the Earth's polar front jet. In\nthis work a comprehensive theory for the interaction of jets with turbulence,\nStochastic Structural Stability Theory, is applied to the problem of\nunderstanding the formation and maintenance of the zonal jets that are crucial\nfor enhancing plasma confinement in fusion devices. \n\n"}
{"id": "0908.3961", "contents": "Title: A simple sketching algorithm for entropy estimation Abstract: We consider the problem of approximating the empirical Shannon entropy of a\nhigh-frequency data stream under the relaxed strict-turnstile model, when space\nlimitations make exact computation infeasible. An equivalent measure of entropy\nis the Renyi entropy that depends on a constant alpha. This quantity can be\nestimated efficiently and unbiasedly from a low-dimensional synopsis called an\nalpha-stable data sketch via the method of compressed counting. An\napproximation to the Shannon entropy can be obtained from the Renyi entropy by\ntaking alpha sufficiently close to 1. However, practical guidelines for\nparameter calibration with respect to alpha are lacking. We avoid this problem\nby showing that the random variables used in estimating the Renyi entropy can\nbe transformed to have a proper distributional limit as alpha approaches 1: the\nmaximally skewed, strictly stable distribution with alpha = 1 defined on the\nentire real line. We propose a family of asymptotically unbiased log-mean\nestimators of the Shannon entropy, indexed by a constant zeta > 0, that can be\ncomputed in a single-pass algorithm to provide an additive approximation. We\nrecommend the log-mean estimator with zeta = 1 that has exponentially\ndecreasing tail bounds on the error probability, asymptotic relative efficiency\nof 0.932, and near-optimal computational complexity. \n\n"}
{"id": "0909.1272", "contents": "Title: Rotating helical turbulence. Part I. Global evolution and spectral\n  behavior Abstract: We present results from two 1536^3 direct numerical simulations of rotating\nturbulence where both energy and helicity are injected into the flow by an\nexternal forcing. The dual cascade of energy and helicity towards smaller\nscales observed in isotropic and homogeneous turbulence is broken in the\npresence of rotation, with the development of an inverse cascade of energy now\ncoexisting with direct cascades of energy and helicity. In the direct cascade\nrange, the flux of helicity dominates over that of energy at low Rossby number.\nThese cascades have several consequences for the statistics of the flow. The\nevolution of global quantities and of the energy and helicity spectra is\nstudied, and comparisons with simulations at different Reynolds and Rossby\nnumbers at lower resolution are done to identify scaling laws. \n\n"}
{"id": "0909.1957", "contents": "Title: Minimal atmospheric finite-mode models preserving symmetry and\n  generalized Hamiltonian structures Abstract: A typical problem with the conventional Galerkin approach for the\nconstruction of finite-mode models is to keep structural properties unaffected\nin the process of discretization. We present two examples of finite-mode\napproximations that in some respect preserve the geometric attributes inherited\nfrom their continuous models: a three-component model of the barotropic\nvorticity equation known as Lorenz' maximum simplification equations [Tellus,\n\\textbf{12}, 243--254 (1960)] and a six-component model of the two-dimensional\nRayleigh--B\\'{e}nard convection problem. It is reviewed that the Lorenz--1960\nmodel respects both the maximal set of admitted point symmetries and an\nextension of the noncanonical Hamiltonian form (Nambu form). In a similar\nfashion, it is proved that the famous Lorenz--1963 model violates the\nstructural properties of the Saltzman equations and hence cannot be considered\nas the maximum simplification of the Rayleigh--B\\'{e}nard convection problem.\nUsing a six-component truncation, we show that it is again possible retaining\nboth symmetries and the Nambu representation in the course of discretization.\nThe conservative part of this six-component reduction is related to the\nLagrange top equations. Dissipation is incorporated using a metric tensor. \n\n"}
{"id": "0909.2583", "contents": "Title: Field theoretical approach to the description of the coherent structures\n  in 2D fluids and plasmas Abstract: Evolving from turbulent states the 2D fluids and the plasmas reach states\ncharacterized by a high degree of order, consisting of few vortices. These\nasymptotic states represent a small subset in the space of functions and are\ncharacterised by properties that are difficult to identify in a direct\napproach. The field theoretical approach to the dynamics and to the asymptotic\nstates of fluids and plasmas in 2D provides a considerable extension of the\nusual perspective. The present works discusses a series of consequences of the\nfield theoretical approach, when it is applied to particular problems. The\ndiscussion is developed around known physical problems: the current density\nprofiles in cylindrical plasma, the density pinch in tokamak and the\nconcentration of vorticity. \n\n"}
{"id": "0910.3063", "contents": "Title: Enhanced free space beam capture by improved optical tapers Abstract: In our continuous variable quantum key distribution (QKD) scheme, the\nhomodyne detection set-up requires balancing the intensity of an incident beam\nbetween two photodiodes. Realistic lens systems are insufficient to provide a\nspatially stable focus in the presence of large spatial beam-jitter caused by\natmospheric transmission. We therefore present an improved geometry for optical\ntapers which offer up to four times the angular tolerance of a lens. The\neffective area of a photodiode can thus be increased, without decreasing its\nbandwidth. This makes them suitable for use in our free space QKD experiment\nand in free space optical communication in general. \n\n"}
{"id": "0911.0522", "contents": "Title: Can the Adaptive Metropolis Algorithm Collapse Without the Covariance\n  Lower Bound? Abstract: The Adaptive Metropolis (AM) algorithm is based on the symmetric random-walk\nMetropolis algorithm. The proposal distribution has the following\ntime-dependent covariance matrix at step $n+1$ \\[\n  S_n = Cov(X_1,...,X_n) + \\epsilon I, \\] that is, the sample covariance matrix\nof the history of the chain plus a (small) constant $\\epsilon>0$ multiple of\nthe identity matrix $I$. The lower bound on the eigenvalues of $S_n$ induced by\nthe factor $\\epsilon I$ is theoretically convenient, but practically\ncumbersome, as a good value for the parameter $\\epsilon$ may not always be easy\nto choose. This article considers variants of the AM algorithm that do not\nexplicitly bound the eigenvalues of $S_n$ away from zero. The behaviour of\n$S_n$ is studied in detail, indicating that the eigenvalues of $S_n$ do not\ntend to collapse to zero in general. \n\n"}
{"id": "0911.0741", "contents": "Title: Influence of the condensate and inverse cascade on the direct cascade in\n  wave turbulence Abstract: During direct numerical simulation of the isotropic turbulence of surface\ngravity waves in the framework of Hamiltonian equations formation of the long\nwave background or condensate was observed. Exponents of the direct cascade\nspectra at the different levels of an artificial condensate suppression show a\ntendency to become closer to the prediction of the wave turbulence theory at\nlower levels of condensate. A simple qualitative explanation of the mechanism\nof this phenomenon is proposed. \n\n"}
{"id": "0911.1164", "contents": "Title: Kernel estimators of asymptotic variance for adaptive Markov chain Monte\n  Carlo Abstract: We study the asymptotic behavior of kernel estimators of asymptotic variances\n(or long-run variances) for a class of adaptive Markov chains. The convergence\nis studied both in $L^p$ and almost surely. The results also apply to Markov\nchains and improve on the existing literature by imposing weaker conditions. We\nillustrate the results with applications to the $\\operatorname {GARCH}(1,1)$\nMarkov model and to an adaptive MCMC algorithm for Bayesian logistic\nregression. \n\n"}
{"id": "0911.1705", "contents": "Title: Simulation-based model selection for dynamical systems in systems and\n  population biology Abstract: Computer simulations have become an important tool across the biomedical\nsciences and beyond. For many important problems several different models or\nhypotheses exist and choosing which one best describes reality or observed data\nis not straightforward. We therefore require suitable statistical tools that\nallow us to choose rationally between different mechanistic models of e.g.\nsignal transduction or gene regulation networks. This is particularly\nchallenging in systems biology where only a small number of molecular species\ncan be assayed at any given time and all measurements are subject to\nmeasurement uncertainty. Here we develop such a model selection framework based\non approximate Bayesian computation and employing sequential Monte Carlo\nsampling. We show that our approach can be applied across a wide range of\nbiological scenarios, and we illustrate its use on real data describing\ninfluenza dynamics and the JAK-STAT signalling pathway. Bayesian model\nselection strikes a balance between the complexity of the simulation models and\ntheir ability to describe observed data. The present approach enables us to\nemploy the whole formal apparatus to any system that can be (efficiently)\nsimulated, even when exact likelihoods are computationally intractable. \n\n"}
{"id": "0911.1904", "contents": "Title: Improving the expected accuracy of forecasts of future climate using a\n  simple bias-variance tradeoff Abstract: We describe a simple method that utilises the standard idea of bias-variance\ntrade-off to improve the expected accuracy of numerical model forecasts of\nfuture climate. The method can be thought of as an optimal multi-model\ncombination between the forecast from a numerical model multi-model ensemble,\non one hand, and a simple statistical forecast, on the other. We apply the\nmethod to predictions for UK temperature and precipitation for the period 2010\nto 2100. The temperature predictions hardly change, while the precipitation\npredictions show large changes. \n\n"}
{"id": "0912.1772", "contents": "Title: Self-Similarity in Fully Developed Homogeneous Isotropic Turbulence\n  Using the Lyapunov Analysis Abstract: In this work, we calculate the self-similar longitudinal velocity correlation\nfunction and the statistical properties of velocity difference using the\nresults of the Lyapunov analysis of the fully developed isotropic homogeneous\nturbulence just presented by the author in a previous work (arXiv:0911.1463).\nThere, a closure of the von Karman-Howarth equation is proposed and the\nstatistics of velocity difference is determined through a specific analysis of\nthe Fourier-transformed Navier-Stokes equations.\n  The correlation functions correspond to steady-state solutions of the von\nKarman-Howarth equation under the self-similarity hypothesis introduced by von\nKarman. These solutions are numerically determined with the statistics of\nvelocity difference. The obtained results adequately describe the several\nproperties of the fully developed isotropic turbulence. \n\n"}
{"id": "0912.2380", "contents": "Title: Diffusive Nested Sampling Abstract: We introduce a general Monte Carlo method based on Nested Sampling (NS), for\nsampling complex probability distributions and estimating the normalising\nconstant. The method uses one or more particles, which explore a mixture of\nnested probability distributions, each successive distribution occupying ~e^-1\ntimes the enclosed prior mass of the previous distribution. While NS\ntechnically requires independent generation of particles, Markov Chain Monte\nCarlo (MCMC) exploration fits naturally into this technique. We illustrate the\nnew method on a test problem and find that it can achieve four times the\naccuracy of classic MCMC-based Nested Sampling, for the same computational\neffort; equivalent to a factor of 16 speedup. An additional benefit is that\nmore samples and a more accurate evidence value can be obtained simply by\ncontinuing the run for longer, as in standard MCMC. \n\n"}
{"id": "0912.4319", "contents": "Title: Empirical analysis of the solar contribution to global mean air surface\n  temperature change Abstract: The solar contribution to global mean air surface temperature change is\nanalyzed by using an empirical bi-scale climate model characterized by both\nfast and slow characteristic time responses to solar forcing: $\\tau_1 =0.4 \\pm\n0.1$ yr, and $\\tau_2= 8 \\pm 2$ yr or $\\tau_2=12 \\pm 3$ yr. Since 1980 the solar\ncontribution to climate change is uncertain because of the severe uncertainty\nof the total solar irradiance satellite composites. The sun may have caused\nfrom a slight cooling, if PMOD TSI composite is used, to a significant warming\n(up to 65% of the total observed warming) if ACRIM, or other TSI composites are\nused. The model is calibrated only on the empirical 11-year solar cycle\nsignature on the instrumental global surface temperature since 1980. The model\nreconstructs the major temperature patterns covering 400 years of solar induced\ntemperature changes, as shown in recent paleoclimate global temperature\nrecords. \n\n"}
{"id": "0912.4395", "contents": "Title: Improving Uncertain Climate Forecasts Using a New Minimum Mean Square\n  Error Estimator for the Mean of the Normal Distribution Abstract: When climate forecasts are highly uncertain, the optimal mean squared error\nstrategy is to ignore them. When climate forecasts are highly certain, the\noptimal mean squared error strategy is to use them as is. In between these two\nextremes there are climate forecasts with an intermediate level of uncertainty\nfor which the optimal mean squared error strategy is to make a compromise\nforecast. We present two new methods for making such compromise forecasts, and\nshow, using simulations, that they improve on previously published methods. \n\n"}
{"id": "1001.0151", "contents": "Title: Physical aspects of the field-theoretical description of two-dimensional\n  ideal fluids Abstract: The two-dimensional ideal (Euler) fluids can be described by the classical\nfields of streamfunction, velocity and vorticity and, in an equivalent manner,\nby a model of discrete point-like vortices interacting in plane by a\nself-generated long-range potential. This latter model can be formalized, in\nthe continuum limit, as a field theory of scalar matter in interaction with a\ngauge field, in the $su(2) $ algebra. This description has already offered the\nanalytical derivation of the \\emph{sinh}-Poisson equation, which was known to\ngovern the stationary coherent structures reached by the Euler fluid at\nrelaxation. In order this formalism to become a familiar theoretical instrument\nit is necessary to have a better understanding of the physical meaning of the\nvariables and of the operations used by the field theory. Several problems will\nbe investigated below in this respect. \n\n"}
{"id": "1001.1304", "contents": "Title: Optical turbulence vertical distribution with standard and high\n  resolution at Mt. Graham Abstract: A characterization of the optical turbulence vertical distribution (Cn2\nprofiles) and all the main integrated astroclimatic parameters derived from the\nCn2 and the wind speed profiles above the site of the Large Binocular Telescope\n(Mt. Graham, Arizona, US) is presented. The statistic includes measurements\nrelated to 43 nights done with a Generalized Scidar (GS) used in standard\nconfiguration with a vertical resolution Delta(H)~1 km on the whole 20 km and\nwith the new technique (HVR-GS) in the first kilometer. The latter achieves a\nresolution Delta(H)~20-30 m in this region of the atmosphere. Measurements done\nin different periods of the year permit us to provide a seasonal variation\nanalysis of the Cn2. A discretized distribution of Cn2 useful for the Ground\nLayer Adaptive Optics (GLAO) simulations is provided and a specific analysis\nfor the LBT Laser Guide Star system ARGOS (running in GLAO configuration) case\nis done including the calculation of the 'gray zones' for J, H and K bands. Mt.\nGraham confirms to be an excellent site with median values of the seeing\nwithout dome contribution epsilon = 0.72\", the isoplanatic angle theta0 = 2.5\"\nand the wavefront coherence time tau0= 4.8 msec. We find that the optical\nturbulence vertical distribution decreases in a much sharper way than what has\nbeen believed so far in proximity of the ground above astronomical sites. We\nfind that 50% of the whole turbulence develops in the first 80+/-15 m from the\nground. We finally prove that the error in the normalization of the\nscintillation that has been recently put in evidence in the principle of the GS\ntechnique, affects these measurements with an absolutely negligible quantity\n(0.04\"). \n\n"}
{"id": "1001.2136", "contents": "Title: An alternative marginal likelihood estimator for phylogenetic models Abstract: Bayesian phylogenetic methods are generating noticeable enthusiasm in the\nfield of molecular systematics. Many phylogenetic models are often at stake and\ndifferent approaches are used to compare them within a Bayesian framework. The\nBayes factor, defined as the ratio of the marginal likelihoods of two competing\nmodels, plays a key role in Bayesian model selection. We focus on an\nalternative estimator of the marginal likelihood whose computation is still a\nchallenging problem. Several computational solutions have been proposed none of\nwhich can be considered outperforming the others simultaneously in terms of\nsimplicity of implementation, computational burden and precision of the\nestimates. Practitioners and researchers, often led by available software, have\nprivileged so far the simplicity of the harmonic mean estimator (HM) and the\narithmetic mean estimator (AM). However it is known that the resulting\nestimates of the Bayesian evidence in favor of one model are biased and often\ninaccurate up to having an infinite variance so that the reliability of the\ncorresponding conclusions is doubtful. Our new implementation of the\ngeneralized harmonic mean (GHM) idea recycles MCMC simulations from the\nposterior, shares the computational simplicity of the original HM estimator,\nbut, unlike it, overcomes the infinite variance issue. The alternative\nestimator is applied to simulated phylogenetic data and produces fully\nsatisfactory results outperforming those simple estimators currently provided\nby most of the publicly available software. \n\n"}
{"id": "1001.3859", "contents": "Title: Strict Monotonicity and Convergence Rate of Titterington's Algorithm for\n  Computing D-optimal Designs Abstract: We study a class of multiplicative algorithms introduced by Silvey et al.\n(1978) for computing D-optimal designs. Strict monotonicity is established for\na variant considered by Titterington (1978). A formula for the rate of\nconvergence is also derived. This is used to explain why modifications\nconsidered by Titterington (1978) and Dette et al. (2008) usually converge\nfaster. \n\n"}
{"id": "1002.0764", "contents": "Title: Formation of bound states of electrons in spherically symmetric\n  oscillations of plasma Abstract: We study spherically symmetric oscillations of electrons in plasma in the\nframe of classical electrodynamics. Firstly, we analyze the electromagnetic\npotentials for the system of radially oscillating charged particles. Secondly,\nwe consider both free and forced spherically symmetric oscillations of\nelectrons. Finally, we discuss the interaction between radially oscillating\nelectrons through the exchange of ion acoustic waves. It is obtained that the\neffective potential of this interaction can be attractive and can transcend the\nDebye-Huckel potential. We suggest that oscillating electrons can form bound\nstates at the initial stages of the spherical plasma structure evolution. The\npossible applications of the obtained results for the theory of natural\nplasmoids are examined. \n\n"}
{"id": "1002.2706", "contents": "Title: Evolutionary Stochastic Search for Bayesian model exploration Abstract: Implementing Bayesian variable selection for linear Gaussian regression\nmodels for analysing high dimensional data sets is of current interest in many\nfields. In order to make such analysis operational, we propose a new sampling\nalgorithm based upon Evolutionary Monte Carlo and designed to work under the\n\"large p, small n\" paradigm, thus making fully Bayesian multivariate analysis\nfeasible, for example, in genetics/genomics experiments. Two real data examples\nin genomics are presented, demonstrating the performance of the algorithm in a\nspace of up to 10,000 covariates. Finally the methodology is compared with a\nrecently proposed search algorithms in an extensive simulation study. \n\n"}
{"id": "1002.3784", "contents": "Title: Estimation for High-Dimensional Linear Mixed-Effects Models Using\n  $\\ell_1$-Penalization Abstract: We propose an $\\ell_1$-penalized estimation procedure for high-dimensional\nlinear mixed-effects models. The models are useful whenever there is a grouping\nstructure among high-dimensional observations, i.e. for clustered data. We\nprove a consistency and an oracle optimality result and we develop an algorithm\nwith provable numerical convergence. Furthermore, we demonstrate the\nperformance of the method on simulated and a real high-dimensional data set. \n\n"}
{"id": "1003.0243", "contents": "Title: Perfect simulation using dominated coupling from the past with\n  application to area-interaction point processes and wavelet thresholding Abstract: We consider perfect simulation algorithms for locally stable point processes\nbased on dominated coupling from the past, and apply these methods in two\ndifferent contexts. A new version of the algorithm is developed which is\nfeasible for processes which are neither purely attractive nor purely\nrepulsive. Such processes include multiscale area-interaction processes, which\nare capable of modelling point patterns whose clustering structure varies\nacross scales. The other topic considered is nonparametric regression using\nwavelets, where we use a suitable area-interaction process on the discrete\nspace of indices of wavelet coefficients to model the notion that if one\nwavelet coefficient is non-zero then it is more likely that neighbouring\ncoefficients will be also. A method based on perfect simulation within this\nmodel shows promising results compared to the standard methods which threshold\ncoefficients independently. \n\n"}
{"id": "1003.1771", "contents": "Title: Data Driven Computing by the Morphing Fast Fourier Transform Ensemble\n  Kalman Filter in Epidemic Spread Simulations Abstract: The FFT EnKF data assimilation method is proposed and applied to a stochastic\ncell simulation of an epidemic, based on the S-I-R spread model. The FFT EnKF\ncombines spatial statistics and ensemble filtering methodologies into a\nlocalized and computationally inexpensive version of EnKF with a very small\nensemble, and it is further combined with the morphing EnKF to assimilate\nchanges in the position of the epidemic. \n\n"}
{"id": "1003.2942", "contents": "Title: Nonlinear and chaotic ice ages: data vs speculations Abstract: It is shown that, the wavelet regression detrended fluctuations of the\nreconstructed temperature for the past 400,000 years (Antarctic ice cores data)\nare completely dominated by one-third subharmonic resonance, presumably related\nto Earth precession effect on the energy that the intertropical regions receive\nfrom the Sun. Effects of Galactic turbulence on the temperature fluctuations\nare also discussed. Direct evidence of chaotic response of the atmospheric CO_2\ndynamics to obliquity periodic forcing has been found in a reconstruction of\natmospheric CO_2 data (deep sea proxies), for the past 650,000 years. \n\n"}
{"id": "1003.5075", "contents": "Title: Invariant measures of the 2D Euler and Vlasov equations Abstract: We discuss invariant measures of partial differential equations such as the\n2D Euler or Vlasov equations. For the 2D Euler equations, starting from the\nLiouville theorem, valid for N-dimensional approximations of the dynamics, we\ndefine the microcanonical measure as a limit measure where N goes to infinity.\nWhen only the energy and enstrophy invariants are taken into account, we give\nan explicit computation to prove the following result: the microcanonical\nmeasure is actually a Young measure corresponding to the maximization of a\nmean-field entropy. We explain why this result remains true for more general\nmicrocanonical measures, when all the dynamical invariants are taken into\naccount. We give an explicit proof that these microcanonical measures are\ninvariant measures for the dynamics of the 2D Euler equations. We describe a\nmore general set of invariant measures, and discuss briefly their stability and\ntheir consequence for the ergodicity of the 2D Euler equations. The extension\nof these results to the Vlasov equations is also discussed, together with a\nproof of the uniqueness of statistical equilibria, for Vlasov equations with\nrepulsive convex potentials. Even if we consider, in this paper, invariant\nmeasures only for Hamiltonian equations, with no fluxes of conserved\nquantities, we think this work is an important step towards the description of\nnon-equilibrium invariant measures with fluxes. \n\n"}
{"id": "1004.1950", "contents": "Title: Finite volume schemes for dispersive wave propagation and runup Abstract: Finite volume schemes are commonly used to construct approximate solutions to\nconservation laws. In this study we extend the framework of the finite volume\nmethods to dispersive water wave models, in particular to Boussinesq type\nsystems. We focus mainly on the application of the method to bidirectional\nnonlinear, dispersive wave propagation in one space dimension. Special emphasis\nis given to important nonlinear phenomena such as solitary waves interactions,\ndispersive shock wave formation and the runup of breaking and non-breaking long\nwaves. \n\n"}
{"id": "1004.3189", "contents": "Title: Short term forecasting of surface layer wind speed using a continuous\n  cascade model Abstract: This paper describes a statistical method for short-term forecasting of\nsurface layer wind velocity amplitude relying on the notion of continuous\ncascades. Inspired by recent empirical findings that suggest the existence of\nsome cascading process in the mesoscale range, we consider that wind speed can\nbe described by a seasonal component and a fluctuating part represented by a\n\"multifractal noise\" associated with a random cascade. Performances of our\nmodel are tested on hourly wind speed series gathered at various locations in\nCorsica (France) and Netherlands. The obtained results show a systematic\nimprovement of the prediction as compared to reference models like persistence\nor Artificial Neural Networks. \n\n"}
{"id": "1004.3457", "contents": "Title: General Temporal Instability Criteria For Stably Stratified Inviscid\n  Flow Abstract: The temporal instability of stably stratified flow was investigated by\nanalyzing the Taylor-Goldstein equation theoretically. According to this\nanalysis, the stable stratification $N^2\\geq0$ has a destabilization mechanism,\nand the flow instability is due to the competition of the kinetic energy with\nthe potential energy, which is dominated by the total Froude number $Fr_t^2$.\nGlobally, $Fr_t^2 \\leq 1$ implies that the total kinetic energy is smaller than\nthe total potential energy. So the potential energy might transfer to the\nkinetic energy after being disturbed, and the flow becomes unstable. On the\nother hand, when the potential energy is smaller than the kinetic energy\n($Fr_t^2>1$), the flow is stable because no potential energy could transfer to\nthe kinetic energy. The flow is more stable with the velocity profile\n$U'/U'''>0$ than that with $U'/U'''<0$. Besides, the unstable perturbation must\nbe long-wave scale. Locally, the flow is unstable as the gradient Richardson\nnumber $Ri>1/4$. These results extend the Rayleigh's, Fj{\\o}rtoft's, Sun's and\nArnol'd's criteria for the inviscid homogenous fluid, but they contradict the\nwell-known Miles-Howard theorem. It is argued here that the transform\n$F=\\phi/(U-c)^n$ is not suitable for temporal stability problem, and that it\nwill lead to contradictions with the results derived from the Taylor-Goldstein\nequation. However, such transform might be useful for the study of the\nOrr-Sommerfeld equation in viscous flows. \n\n"}
{"id": "1004.3480", "contents": "Title: Fully nonlinear weakly dispersive modelling of wave transformation,\n  breaking and runup Abstract: To describe the strongly nonlinear dynamics of waves propagating in the final\nstages of shoaling and in the surf and swash zones, fully nonlinear models are\nrequired. The ability of the Serre or Green Naghdi (S-GN) equations to\nreproduce this nonlinear processes is reviewed. Two high-order methods for\nsolving S-GN equations, based on Finite Volume approaches, are presented. The\nfirst one is based on a quasi-conservative form of the S-GN equations, and the\nsecond on a hybrid Finite Volume/Finite Difference method. We show the ability\nof these two approaches to accurately simulate nonlinear shoaling, breaking and\nrunup processes. \n\n"}
{"id": "1004.3616", "contents": "Title: Recursive Numerical Evaluation of the Cumulative Bivariate Normal\n  Distribution Abstract: We propose an algorithm for evaluation of the cumulative bivariate normal\ndistribution, building upon Marsaglia's ideas for evaluation of the cumulative\nunivariate normal distribution. The algorithm is mathematically transparent,\ndelivers competitive performance and can easily be extended to arbitrary\nprecision. \n\n"}
{"id": "1004.3830", "contents": "Title: Model Selection and Adaptive Markov chain Monte Carlo for Bayesian\n  Cointegrated VAR model Abstract: This paper develops a matrix-variate adaptive Markov chain Monte Carlo (MCMC)\nmethodology for Bayesian Cointegrated Vector Auto Regressions (CVAR). We\nreplace the popular approach to sampling Bayesian CVAR models, involving griddy\nGibbs, with an automated efficient alternative, based on the Adaptive\nMetropolis algorithm of Roberts and Rosenthal, (2009). Developing the adaptive\nMCMC framework for Bayesian CVAR models allows for efficient estimation of\nposterior parameters in significantly higher dimensional CVAR series than\npreviously possible with existing griddy Gibbs samplers. For a n-dimensional\nCVAR series, the matrix-variate posterior is in dimension $3n^2 + n$, with\nsignificant correlation present between the blocks of matrix random variables.\nWe also treat the rank of the CVAR model as a random variable and perform joint\ninference on the rank and model parameters. This is achieved with a Bayesian\nposterior distribution defined over both the rank and the CVAR model\nparameters, and inference is made via Bayes Factor analysis of rank.\nPractically the adaptive sampler also aids in the development of automated\nBayesian cointegration models for algorithmic trading systems considering\ninstruments made up of several assets, such as currency baskets. Previously the\nliterature on financial applications of CVAR trading models typically only\nconsiders pairs trading (n=2) due to the computational cost of the griddy\nGibbs. We are able to extend under our adaptive framework to $n >> 2$ and\ndemonstrate an example with n = 10, resulting in a posterior distribution with\nparameters up to dimension 310. By also considering the rank as a random\nquantity we can ensure our resulting trading models are able to adjust to\npotentially time varying market conditions in a coherent statistical framework. \n\n"}
{"id": "1004.4041", "contents": "Title: Pooling Design and Bias Correction in DNA Library Screening Abstract: We study the group test for DNA library screening based on probabilistic\napproach. Group test is a method of detecting a few positive items from among a\nlarge number of items, and has wide range of applications. In DNA library\nscreening, positive item corresponds to the clone having a specified DNA\nsegment, and it is necessary to identify and isolate the positive clones for\ncompiling the libraries. In the group test, a group of items, called pool, is\nassayed in a lump in order to save the cost of testing, and positive items are\ndetected based on the observation from each pool. It is known that the design\nof grouping, that is, pooling design is important to %reduce the estimation\nbias and achieve accurate detection. In the probabilistic approach, positive\nclones are picked up based on the posterior probability. Naive methods of\ncomputing the posterior, however, involves exponentially many sums, and thus we\nneed a device. Loopy belief propagation (loopy BP) algorithm is one of popular\nmethods to obtain approximate posterior probability efficiently. There are some\nworks investigating the relation between the accuracy of the loopy BP and the\npooling design. Based on these works, we develop pooling design with small\nestimation bias of posterior probability, and we show that the balanced\nincomplete block design (BIBD) has nice property for our purpose. Some\nnumerical experiments show that the bias correction under the BIBD is useful to\nimprove the estimation accuracy. \n\n"}
{"id": "1005.1153", "contents": "Title: Transcranial stimulability of phosphenes by long lightning\n  electromagnetic pulses Abstract: The electromagnetic pulses of rare long (order of seconds) repetitive\nlightning discharges near strike point (order of 100m) are analyzed and\ncompared to magnetic fields applied in standard clinical transcranial magnetic\nstimulation (TMS) practice. It is shown that the time-varying lightning\nmagnetic fields and locally induced potentials are in the same order of\nmagnitude and frequency as those established in TMS experiments to study\nstimulated perception phenomena, like magnetophosphenes. Lightning\nelectromagnetic pulse induced transcranial magnetic stimulation of phosphenes\nin the visual cortex is concluded to be a plausible interpretation of a large\nclass of reports on luminous perceptions during thunderstorms. APPENDIX\n(Erratum and Addendum by J. Peer, V. Cooray, G. Cooray and A. Kendl): The\ncomparison of electric fields transcranially induced by lightning discharges\nand by TMS brain stimulators via View E = - dA/dt is shown to be inappropriate.\nCorrected results with respect to evaluation of phosphene stimulability are\npresented. For average lightning parameters the correct induced electric fields\nappear more than an order of magnitude smaller. For typical ranges of stronger\nthan average lightning currents, electric fields above the threshold for\ncortical phosphene stimulation can be induced only for short distances (order\nof meters), or in medium distances (order of 50 m) only for pulses shorter than\nestablished axon excitation periods. Stimulation of retinal phosphene\nperception has much lower threshold and appears most probable for lightning\nelectromagnetic fields. \n\n"}
{"id": "1005.3907", "contents": "Title: Objective Climate Model Predictions Using Jeffreys' Prior: the General\n  Multivariate Normal Case Abstract: Objective probabilistic forecasts of future climate that include parameter\nuncertainty can be made by using the Bayesian prediction integral with the\nprior set to Jeffreys' Prior. The calculations involved in determining the\nprior can then be simplified by making parametric assumptions about the\ndistribution of the output from the climate model. The most obvious assumption\nto make is that the climate model output is normally distributed, in which case\nevaluating the prior becomes a question of evaluating gradients in the\nparameters of the normal distribution. In previous work we have considered the\nspecial cases of diagonal (but not constant) covariance matrix, and constant\n(but not diagonal) covariance matrix. We now derive expressions for the general\nmultivariate normal distribution, with non-constant non-diagonal covariance\nmatrix. The algebraic manipulation required is more complex than for the\nspecial cases, and involves some slightly esoteric matrix operations including\ntaking the expectation of a vector quadratic form and differentiating the\ndeterminants, traces and inverses of matrices. \n\n"}
{"id": "1005.4717", "contents": "Title: Smoothing proximal gradient method for general structured sparse\n  regression Abstract: We study the problem of estimating high-dimensional regression models\nregularized by a structured sparsity-inducing penalty that encodes prior\nstructural information on either the input or output variables. We consider two\nwidely adopted types of penalties of this kind as motivating examples: (1) the\ngeneral overlapping-group-lasso penalty, generalized from the group-lasso\npenalty; and (2) the graph-guided-fused-lasso penalty, generalized from the\nfused-lasso penalty. For both types of penalties, due to their nonseparability\nand nonsmoothness, developing an efficient optimization method remains a\nchallenging problem. In this paper we propose a general optimization approach,\nthe smoothing proximal gradient (SPG) method, which can solve structured sparse\nregression problems with any smooth convex loss under a wide spectrum of\nstructured sparsity-inducing penalties. Our approach combines a smoothing\ntechnique with an effective proximal gradient method. It achieves a convergence\nrate significantly faster than the standard first-order methods, subgradient\nmethods, and is much more scalable than the most widely used interior-point\nmethods. The efficiency and scalability of our method are demonstrated on both\nsimulation experiments and real genetic data sets. \n\n"}
{"id": "1006.3002", "contents": "Title: Free energy Sequential Monte Carlo, application to mixture modelling Abstract: We introduce a new class of Sequential Monte Carlo (SMC) methods, which we\ncall free energy SMC. This class is inspired by free energy methods, which\noriginate from Physics, and where one samples from a biased distribution such\nthat a given function $\\xi(\\theta)$ of the state $\\theta$ is forced to be\nuniformly distributed over a given interval. From an initial sequence of\ndistributions $(\\pi_t)$ of interest, and a particular choice of $\\xi(\\theta)$,\na free energy SMC sampler computes sequentially a sequence of biased\ndistributions $(\\tilde{\\pi}_{t})$ with the following properties: (a) the\nmarginal distribution of $\\xi(\\theta)$ with respect to $\\tilde{\\pi}_{t}$ is\napproximatively uniform over a specified interval, and (b) $\\tilde{\\pi}_{t}$\nand $\\pi_{t}$ have the same conditional distribution with respect to $\\xi$. We\napply our methodology to mixture posterior distributions, which are highly\nmultimodal. In the mixture context, forcing certain hyper-parameters to higher\nvalues greatly faciliates mode swapping, and makes it possible to recover a\nsymetric output. We illustrate our approach with univariate and bivariate\nGaussian mixtures and two real-world datasets. \n\n"}
{"id": "1008.0149", "contents": "Title: Bayesian Cointegrated Vector Autoregression models incorporating\n  Alpha-stable noise for inter-day price movements via Approximate Bayesian\n  Computation Abstract: We consider a statistical model for pairs of traded assets, based on a\nCointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR\nmodels to incorporate estimation of model parameters in the presence of price\nseries level shifts which are not accurately modeled in the standard Gaussian\nerror correction model (ECM) framework. This involves developing a novel matrix\nvariate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and\nAlpha-stable errors inter-day in the ECM framework. To achieve this we derive a\nnovel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR)\nrepresentation of Alpha-stable inter-day innovations. These results are\ngeneralized to asymmetric models for the innovation noise at inter-day\nboundaries allowing for skewed Alpha-stable models.\n  Our proposed model and sampling methodology is general, incorporating the\ncurrent literature on Gaussian models as a special subclass and also allowing\nfor price series level shifts either at random estimated time points or known a\npriori time points. We focus analysis on regularly observed non-Gaussian level\nshifts that can have significant effect on estimation performance in\nstatistical models failing to account for such level shifts, such as at the\nclose and open of markets. We compare the estimation accuracy of our model and\nestimation approach to standard frequentist and Bayesian procedures for CVAR\nmodels when non-Gaussian price series level shifts are present in the\nindividual series, such as inter-day boundaries. We fit a bi-variate\nAlpha-stable model to the inter-day jumps and model the effect of such jumps on\nestimation of matrix-variate CVAR model parameters using the likelihood based\nJohansen procedure and a Bayesian estimation. We illustrate our model and the\ncorresponding estimation procedures we develop on both synthetic and actual\ndata. \n\n"}
{"id": "1008.1355", "contents": "Title: Control Variates for Reversible MCMC Samplers Abstract: A general methodology is introduced for the construction and effective\napplication of control variates to estimation problems involving data from\nreversible MCMC samplers. We propose the use of a specific class of functions\nas control variates, and we introduce a new, consistent estimator for the\nvalues of the coefficients of the optimal linear combination of these\nfunctions. The form and proposed construction of the control variates is\nderived from our solution of the Poisson equation associated with a specific\nMCMC scenario. The new estimator, which can be applied to the same MCMC sample,\nis derived from a novel, finite-dimensional, explicit representation for the\noptimal coefficients. The resulting variance-reduction methodology is primarily\napplicable when the simulated data are generated by a conjugate random-scan\nGibbs sampler. MCMC examples of Bayesian inference problems demonstrate that\nthe corresponding reduction in the estimation variance is significant, and that\nin some cases it can be quite dramatic. Extensions of this methodology in\nseveral directions are given, including certain families of Metropolis-Hastings\nsamplers and hybrid Metropolis-within-Gibbs algorithms. Corresponding\nsimulation examples are presented illustrating the utility of the proposed\nmethods. All methodological and asymptotic arguments are rigorously justified\nunder easily verifiable and essentially minimal conditions. \n\n"}
{"id": "1008.1550", "contents": "Title: Hyper-g Priors for Generalized Linear Models Abstract: We develop an extension of the classical Zellner's g-prior to generalized\nlinear models. The prior on the hyperparameter g is handled in a flexible way,\nso that any continuous proper hyperprior f(g) can be used, giving rise to a\nlarge class of hyper-g priors. Connections with the literature are described in\ndetail. A fast and accurate integrated Laplace approximation of the marginal\nlikelihood makes inference in large model spaces feasible. For posterior\nparameter estimation we propose an efficient and tuning-free\nMetropolis-Hastings sampler. The methodology is illustrated with variable\nselection and automatic covariate transformation in the Pima Indians diabetes\ndata set. \n\n"}
{"id": "1009.1523", "contents": "Title: Point symmetry group of the barotropic vorticity equation Abstract: The complete point symmetry group of the barotropic vorticity equation on the\n$\\beta$-plane is computed using the direct method supplemented with two\ndifferent techniques. The first technique is based on the preservation of any\nmegaideal of the maximal Lie invariance algebra of a differential equation by\nthe push-forwards of point symmetries of the same equation. The second\ntechnique involves a priori knowledge on normalization properties of a class of\ndifferential equations containing the equation under consideration. Both of\nthese techniques are briefly outlined. \n\n"}
{"id": "1009.2260", "contents": "Title: Computing the confidence levels for a root-mean-square test of\n  goodness-of-fit, II Abstract: This paper extends our earlier article, \"Computing the confidence levels for\na root-mean-square test of goodness-of-fit;\" unlike in the earlier article, the\nmodels in the present paper involve parameter estimation -- both the null and\nalternative hypotheses in the associated tests are composite. We provide\nefficient black-box algorithms for calculating the asymptotic confidence levels\nof a variant on the classic chi-squared test. In some circumstances, it is also\nfeasible to compute the exact confidence levels via Monte Carlo simulation. \n\n"}
{"id": "1009.2707", "contents": "Title: Pac-bayesian bounds for sparse regression estimation with exponential\n  weights Abstract: We consider the sparse regression model where the number of parameters $p$ is\nlarger than the sample size $n$. The difficulty when considering\nhigh-dimensional problems is to propose estimators achieving a good compromise\nbetween statistical and computational performances. The BIC estimator for\ninstance performs well from the statistical point of view \\cite{BTW07} but can\nonly be computed for values of $p$ of at most a few tens. The Lasso estimator\nis solution of a convex minimization problem, hence computable for large value\nof $p$. However stringent conditions on the design are required to establish\nfast rates of convergence for this estimator. Dalalyan and Tsybakov\n\\cite{arnak} propose a method achieving a good compromise between the\nstatistical and computational aspects of the problem. Their estimator can be\ncomputed for reasonably large $p$ and satisfies nice statistical properties\nunder weak assumptions on the design. However, \\cite{arnak} proposes sparsity\noracle inequalities in expectation for the empirical excess risk only. In this\npaper, we propose an aggregation procedure similar to that of \\cite{arnak} but\nwith improved statistical performances. Our main theoretical result is a\nsparsity oracle inequality in probability for the true excess risk for a\nversion of exponential weight estimator. We also propose a MCMC method to\ncompute our estimator for reasonably large values of $p$. \n\n"}
{"id": "1009.5544", "contents": "Title: Gamma-Ray Localization of Terrestrial Gamma-Ray Flashes Abstract: Terrestrial Gamma-Ray Flashes (TGFs) are very short bursts of high energy\nphotons and electrons originating in Earth's atmosphere. We present here a\nlocalization study of TGFs carried out at gamma-ray energies above 20 MeV based\non an innovative event selection method. We use the AGILE satellite Silicon\nTracker data that for the first time have been correlated with TGFs detected by\nthe AGILE Mini-Calorimeter. We detect 8 TGFs with gamma-ray photons of energies\nabove 20 MeV localized by the AGILE gamma-ray imager with an accuracy of 5-10\ndegrees at 50 MeV. Remarkably, all TGF-associated gamma rays are compatible\nwith a terrestrial production site closer to the sub-satellite point than 400\nkm. Considering that our gamma rays reach the AGILE satellite at 540 km\naltitude with limited scattering or attenuation, our measurements provide the\nfirst precise direct localization of TGFs from space. \n\n"}
{"id": "1010.0701", "contents": "Title: Axially and spherically symmetric solitons in warm plasma Abstract: We study the existence of stable axially and spherically symmetric plasma\nstructures on the basis of the new nonlinear Schrodinger equation (NLSE)\naccounting for nonlocal electron nonlinearities. The numerical solutions of\nNLSE having the form of spatial solitions are obtained and their stability is\nanalyzed. We discuss the possible application of the obtained results to the\ntheoretical description of natural plasmoids in the atmosphere. \n\n"}
{"id": "1010.1595", "contents": "Title: Using parallel computation to improve Independent Metropolis--Hastings\n  based estimation Abstract: In this paper, we consider the implications of the fact that parallel\nraw-power can be exploited by a generic Metropolis--Hastings algorithm if the\nproposed values are independent. In particular, we present improvements to the\nindependent Metropolis--Hastings algorithm that significantly decrease the\nvariance of any estimator derived from the MCMC output, for a null computing\ncost since those improvements are based on a fixed number of target density\nevaluations. Furthermore, the techniques developed in this paper do not\njeopardize the Markovian convergence properties of the algorithm, since they\nare based on the Rao--Blackwell principles of Gelfand and Smith (1990), already\nexploited in Casella and Robert (1996), Atchade and Perron (2005) and Douc and\nRobert (2010). We illustrate those improvements both on a toy normal example\nand on a classical probit regression model, but stress the fact that they are\napplicable in any case where the independent Metropolis-Hastings is applicable. \n\n"}
{"id": "1011.0057", "contents": "Title: Discussion of \"Riemann manifold Langevin and Hamiltonian Monte Carlo\n  methods'' by M. Girolami and B. Calderhead Abstract: This technical report is the union of two contributions to the discussion of\nthe Read Paper \"Riemann manifold Langevin and Hamiltonian Monte Carlo methods\"\nby B. Calderhead and M. Girolami, presented in front of the Royal Statistical\nSociety on October 13th 2010 and to appear in the Journal of the Royal\nStatistical Society Series B. The first comment establishes a parallel and\npossible interactions with Adaptive Monte Carlo methods. The second comment\nexposes a detailed study of Riemannian Manifold Hamiltonian Monte Carlo (RMHMC)\nfor a weakly identifiable model presenting a strong ridge in its geometry. \n\n"}
{"id": "1011.2556", "contents": "Title: Oceanic rings and jets as statistical equilibrium states Abstract: Equilibrium statistical mechanics of two-dimensional flows provides an\nexplanation and a prediction for the self-organization of large scale coherent\nstructures. This theory is applied in this paper to the description of oceanic\nrings and jets, in the framework of a 1.5 layer quasi-geostrophic model. The\ntheory predicts the spontaneous formation of regions where the potential\nvorticity is homogenized, with strong and localized jets at their interface.\nMesoscale rings are shown to be close to a statistical equilibrium: the theory\naccounts for their shape, their drift, and their ubiquity in the ocean,\nindependently of the underlying generation mechanism. At basin scale, inertial\nstates presenting mid basin eastward jets (and then different from the\nclassical Fofonoff solution) are described as marginally unstable states. These\nstates are shown to be marginally unstable for the equilibrium statistical\ntheory. In that case, considering a purely inertial limit is a first step\ntoward more comprehensive out of equilibrium studies that would take into\naccount other essential aspects, such as wind forcing. \n\n"}
{"id": "1011.4722", "contents": "Title: Slice Sampling with Adaptive Multivariate Steps: The Shrinking-Rank\n  Method Abstract: The shrinking rank method is a variation of slice sampling that is efficient\nat sampling from multivariate distributions with highly correlated parameters.\nIt requires that the gradient of the log-density be computable. At each\nindividual step, it approximates the current slice with a Gaussian occupying a\nshrinking-dimension subspace. The dimension of the approximation is shrunk\northogonally to the gradient at rejected proposals, since the gradients at\npoints outside the current slice tend to point towards the slice. This causes\nthe proposal distribution to converge rapidly to an estimate of the longest\naxis of the slice, resulting in states that are less correlated than those\ngenerated by related methods. After describing the method, we compare it to two\nother methods on several distributions and obtain favorable results. \n\n"}
{"id": "1011.5038", "contents": "Title: Approximate simulation-free Bayesian inference for multiple changepoint\n  models with dependence within segments Abstract: This paper proposes approaches for the analysis of multiple changepoint\nmodels when dependency in the data is modelled through a hierarchical Gaussian\nMarkov random field. Integrated nested Laplace approximations are used to\napproximate data quantities, and an approximate filtering recursions approach\nis proposed for savings in compuational cost when detecting changepoints. All\nof these methods are simulation free. Analysis of real data demonstrates the\nusefulness of the approach in general. The new models which allow for data\ndependence are compared with conventional models where data within segments is\nassumed independent. \n\n"}
{"id": "1012.1086", "contents": "Title: Two Proposals for Robust PCA using Semidefinite Programming Abstract: The performance of principal component analysis (PCA) suffers badly in the\npresence of outliers. This paper proposes two novel approaches for robust PCA\nbased on semidefinite programming. The first method, maximum mean absolute\ndeviation rounding (MDR), seeks directions of large spread in the data while\ndamping the effect of outliers. The second method produces a low-leverage\ndecomposition (LLD) of the data that attempts to form a low-rank model for the\ndata by separating out corrupted observations. This paper also presents\nefficient computational methods for solving these SDPs. Numerical experiments\nconfirm the value of these new techniques. \n\n"}
{"id": "1012.3013", "contents": "Title: Aspects of Multi-Dimensional Modelling of Substellar Atmospheres Abstract: Theoretical arguments and observations suggest that the atmospheres of Brown\nDwarfs and planets are very dynamic on chemical and on physical time scales.\nThe modelling of such substellar atmospheres has, hence, been much more\ndemanding than initially anticipated. This Splinter\n(http://star-www.st-and.ac.uk/~ch80/CS16/MultiDSplinter_CS16.html) has combined\nnew developments in atmosphere modelling, with novel observational techniques,\nand new challenges arising from planetary and space weather observations. \n\n"}
{"id": "1101.0253", "contents": "Title: Chaos, storms and climate on Mars Abstract: Channel networks on the plateau adjacent to Juventae Chasma have the highest\ndrainage densities reported on Mars.We model frozen precipitation on the\nJuventae plateau,finding that the trigger for forming these channel networks\ncould have been ephemeral lakeshore precipitation,and that they do not require\npast temperatures higher than today.If short-lived and localized events explain\nsome dendritic channel networks on Mars, this would weaken the link between\ndendritic valley networks and surface climate conditions that could sustain\nlife. Our analysis uses MRAMS simulations and HiRISE DTMs.We model localized\nweather systems driven by water vapor release from ephemeral lakes during\noutflow channel formation.At Juventae Chasma,mean snowfall reaches a maximum of\n0.9mm/hr water equivalent on the SW rim of the chasm.Radiative effects of the\nthick cloud cover raise maximum (minimum, mean) plateau surface temperatures by\nup to 24K(9K, 17K)locally.The key result is that the area of maximum modeled\nprecipitation shows a striking correspondence to the mapped Juventae plateau\nchannel networks.Three independent methods show this fit is unlikely to be due\nto chance.We use a snowpack energy balance model to show that if the snow has\nthe albedo of dust(0.28), and for a solar luminosity of 0.8($\\equiv$3.0Gya),\nthen if the atmospheric greenhouse effect is unchanged from(6K warmer\nthan)today only 0.4%(21%)of lake-induced precipitation events produce snowpack\nthat undergoes melting.However, warming from associated dense cloud cover would\nallow melting over a wider range of conditions.In these localized precipitation\nscenarios, global temperatures need not be higher than today, and the rest of\nthe planet remains dry. \n\n"}
{"id": "1101.0387", "contents": "Title: MCMC Using Ensembles of States for Problems with Fast and Slow Variables\n  such as Gaussian Process Regression Abstract: I introduce a Markov chain Monte Carlo (MCMC) scheme in which sampling from a\ndistribution with density pi(x) is done using updates operating on an\n\"ensemble\" of states. The current state x is first stochastically mapped to an\nensemble, x^{(1)},...,x^{(K)}. This ensemble is then updated using MCMC updates\nthat leave invariant a suitable ensemble density, rho(x^{(1)},...,x^{(K)}),\ndefined in terms of pi(x^{(i)}) for i=1,...,K. Finally a single state is\nstochastically selected from the ensemble after these updates. Such ensemble\nMCMC updates can be useful when characteristics of pi and the ensemble permit\npi(x^{(i)}) for all i in {1,...,K}, to be computed in less than K times the\namount of computation time needed to compute pi(x) for a single x. One common\nsituation of this type is when changes to some \"fast\" variables allow for quick\nre-computation of the density, whereas changes to other \"slow\" variables do\nnot. Gaussian process regression models are an example of this sort of problem,\nwith an overall scaling factor for covariances and the noise variance being\nfast variables. I show that ensemble MCMC for Gaussian process regression\nmodels can indeed substantially improve sampling performance. Finally, I\ndiscuss other possible applications of ensemble MCMC, and its relationship to\nthe \"multiple-try Metropolis\" method of Liu, Liang, and Wong and the \"multiset\nsampler\" of Leman, Chen, and Lavine. \n\n"}
{"id": "1101.1136", "contents": "Title: Marginal Likelihood Computation via Arrogance Sampling Abstract: This paper describes a method for estimating the marginal likelihood or Bayes\nfactors of Bayesian models using non-parametric importance sampling (\"arrogance\nsampling\"). This method can also be used to compute the normalizing constant of\nprobability distributions. Because the required inputs are samples from the\ndistribution to be normalized and the scaled density at those samples, this\nmethod may be a convenient replacement for the harmonic mean estimator. The\nmethod has been implemented in the open source R package margLikArrogance. \n\n"}
{"id": "1101.1729", "contents": "Title: Dispersive wave runup on non-uniform shores Abstract: Historically the finite volume methods have been developed for the numerical\nintegration of conservation laws. In this study we present some recent results\non the application of such schemes to dispersive PDEs. Namely, we solve\nnumerically a representative of Boussinesq type equations in view of important\napplications to the coastal hydrodynamics. Numerical results of the runup of a\nmoderate wave onto a non-uniform beach are presented along with great lines of\nthe employed numerical method (see D. Dutykh et al. (2011) for more details). \n\n"}
{"id": "1101.4242", "contents": "Title: Efficient Bayesian inference in stochastic chemical kinetic models using\n  graphical processing units Abstract: A goal of systems biology is to understand the dynamics of intracellular\nsystems. Stochastic chemical kinetic models are often utilized to accurately\ncapture the stochastic nature of these systems due to low numbers of molecules.\nCollecting system data allows for estimation of stochastic chemical kinetic\nrate parameters. We describe a well-known, but typically impractical data\naugmentation Markov chain Monte Carlo algorithm for estimating these\nparameters. The impracticality is due to the use of rejection sampling for\nlatent trajectories with fixed initial and final endpoints which can have\ndiminutive acceptance probability. We show how graphical processing units can\nbe efficiently utilized for parameter estimation in systems that hitherto were\ninestimable. For more complex systems, we show the efficiency gain over\ntraditional CPU computing is on the order of 200. Finally, we show a Bayesian\nanalysis of a system based on Michaelis-Menton kinetics. \n\n"}
{"id": "1101.5091", "contents": "Title: Why approximate Bayesian computational (ABC) methods cannot handle model\n  choice problems Abstract: Approximate Bayesian computation (ABC), also known as likelihood-free\nmethods, have become a favourite tool for the analysis of complex stochastic\nmodels, primarily in population genetics but also in financial analyses. We\nadvocated in Grelaud et al. (2009) the use of ABC for Bayesian model choice in\nthe specific case of Gibbs random fields (GRF), relying on a sufficiency\nproperty mainly enjoyed by GRFs to show that the approach was legitimate.\nDespite having previously suggested the use of ABC for model choice in a wider\nrange of models in the DIY ABC software (Cornuet et al., 2008), we present\ntheoretical evidence that the general use of ABC for model choice is fraught\nwith danger in the sense that no amount of computation, however large, can\nguarantee a proper approximation of the posterior probabilities of the models\nunder comparison. \n\n"}
{"id": "1101.5745", "contents": "Title: An overview of the coupled atmosphere-wildland fire model WRF-Fire Abstract: We describe the coupled atmosphere-wildfire model WRF-Fire, which is\ndistributed as a part of WRF. The fire module is based on a fire-spread model,\nimplemented by the level-set method. In each time step, the fire module takes\nthe wind as input and returns the latent and sensible heat fluxes. We report on\nthe software architecture and features of the software. \n\n"}
{"id": "1101.5837", "contents": "Title: Nonasymptotic bounds on the mean square error for MCMC estimates via\n  renewal techniques Abstract: The Nummellin's split chain construction allows to decompose a Markov chain\nMonte Carlo (MCMC) trajectory into i.i.d. \"excursions\". RegenerativeMCMC\nalgorithms based on this technique use a random number of samples. They have\nbeen proposed as a promising alternative to usual fixed length simulation [25,\n33, 14]. In this note we derive nonasymptotic bounds on the mean square error\n(MSE) of regenerative MCMC estimates via techniques of renewal theory and\nsequential statistics. These results are applied to costruct confidence\nintervals. We then focus on two cases of particular interest: chains satisfying\nthe Doeblin condition and a geometric drift condition. Available explicit\nnonasymptotic results are compared for different schemes of MCMC simulation. \n\n"}
{"id": "1102.1343", "contents": "Title: Coupled atmosphere-wildland fire modeling with WRF-Fire Abstract: We describe the physical model, numerical algorithms, and software structure\nof WRF-Fire. WRF-Fire consists of a fire-spread model, implemented by the\nlevel-set method, coupled with the Weather Research and Forecasting model. In\nevery time step, the fire model inputs the surface wind, which drives the fire,\nand outputs the heat flux from the fire into the atmosphere, which in turn\ninfluences the atmosphere. The level-set method allows submesh representation\nof the burning region and flexible implementation of various ignition modes.\nWRF-Fire is distributed as a part of WRF and it uses the WRF parallel\ninfrastructure for parallel computing. \n\n"}
{"id": "1102.2409", "contents": "Title: The geometry of sound rays in a wind Abstract: We survey the close relationship between sound and light rays and geometry.\nIn the case where the medium is at rest, the geometry is the classical geometry\nof Riemann. In the case where the medium is moving, the more general geometry\nknown as Finsler geometry is needed. We develop these geometries ab initio,\nwith examples, and in particular show how sound rays in a stratified atmosphere\nwith a wind can be mapped to a problem of circles and straight lines. \n\n"}
{"id": "1102.4432", "contents": "Title: Lack of confidence in ABC model choice Abstract: Approximate Bayesian computation (ABC) have become a essential tool for the\nanalysis of complex stochastic models. Earlier, Grelaud et al. (2009) advocated\nthe use of ABC for Bayesian model choice in the specific case of Gibbs random\nfields, relying on a inter-model sufficiency property to show that the\napproximation was legitimate. Having implemented ABC-based model choice in a\nwide range of phylogenetic models in the DIY-ABC software (Cornuet et al.,\n2008), we now present theoretical background as to why a generic use of ABC for\nmodel choice is ungrounded, since it depends on an unknown amount of\ninformation loss induced by the use of insufficient summary statistics. The\napproximation error of the posterior probabilities of the models under\ncomparison may thus be unrelated with the computational effort spent in running\nan ABC algorithm. We then conclude that additional empirical verifications of\nthe performances of the ABC procedure as those available in DIYABC are\nnecessary to conduct model choice. \n\n"}
{"id": "1102.5554", "contents": "Title: Wavelet Ensemble Kalman Filters Abstract: We present a new type of the EnKF for data assimilation in spatial models\nthat uses diagonal approximation of the state covariance in the wavelet space\nto achieve adaptive localization. The efficiency of the new method is\ndemonstrated on an example. \n\n"}
{"id": "1103.0542", "contents": "Title: Optimal scaling and diffusion limits for the Langevin algorithm in high\n  dimensions Abstract: The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm\nwhich makes local moves by incorporating information about the gradient of the\nlogarithm of the target density. In this paper we study the efficiency of MALA\non a natural class of target measures supported on an infinite dimensional\nHilbert space. These natural measures have density with respect to a Gaussian\nrandom field measure and arise in many applications such as Bayesian\nnonparametric statistics and the theory of conditioned diffusions. We prove\nthat, started in stationarity, a suitably interpolated and scaled version of\nthe Markov chain corresponding to MALA converges to an infinite dimensional\ndiffusion process. Our results imply that, in stationarity, the MALA algorithm\napplied to an N-dimensional approximation of the target will take\n$\\mathcal{O}(N^{1/3})$ steps to explore the invariant measure, comparing\nfavorably with the Random Walk Metropolis which was recently shown to require\n$\\mathcal{O}(N)$ steps when applied to the same class of problems. \n\n"}
{"id": "1103.0722", "contents": "Title: Entropy production and multiple equilibria: the case of the ice-albedo\n  feedback Abstract: Nonlinear feedbacks in the Earth System provide mechanisms that can prove\nvery useful in understanding complex dynamics with relatively simple concepts.\nFor example, the temperature and the ice cover of the planet are linked in a\npositive feedback which gives birth to multiple equilibria for some values of\nthe solar constant: fully ice-covered Earth, ice-free Earth and an intermediate\nunstable solution. In this study, we show an analogy between a classical\ndynamical system approach to this problem and a Maximum Entropy Production\n(MEP) principle view, and we suggest a glimpse on how to reconcile MEP with the\ntime evolution of a variable. It enables us in particular to resolve the\nquestion of the stability of the entropy production maxima. We also compare the\nsurface heat flux obtained with MEP and with the bulk-aerodynamic formula. \n\n"}
{"id": "1103.3508", "contents": "Title: Approximating Probability Densities by Iterated Laplace Approximations Abstract: The Laplace approximation is an old, but frequently used method to\napproximate integrals for Bayesian calculations. In this paper we develop an\nextension of the Laplace approximation, by applying it iteratively to the\nresidual, i.e., the difference between the current approximation and the true\nfunction. The final approximation is thus a linear combination of multivariate\nnormal densities, where the coefficients are chosen to achieve a good fit to\nthe target distribution. We illustrate on real and artificial examples that the\nproposed procedure is a computationally efficient alternative to current\napproaches for approximation of multivariate probability densities. The\nR-package iterLap implementing the methods described in this article is\navailable from the CRAN servers. \n\n"}
{"id": "1103.4853", "contents": "Title: Arrival time and magnitude of airborne fission products from the\n  Fukushima, Japan, reactor incident as measured in Seattle, WA, USA Abstract: We report results of air monitoring started due to the recent natural\ncatastrophe on 11 March 2011 in Japan and the severe ensuing damage to the\nFukushima Dai-ichi nuclear reactor complex. On 17-18 March 2011, we registered\nthe first arrival of the airborne fission products 131-I, 132-I, 132-Te,\n134-Cs, and 137-Cs in Seattle, WA, USA, by identifying their characteristic\ngamma rays using a germanium detector. We measured the evolution of the\nactivities over a period of 23 days at the end of which the activities had\nmostly fallen below our detection limit. The highest detected activity amounted\nto 4.4 +/- 1.3 mBq/m^3 of 131-I on 19-20 March. \n\n"}
{"id": "1104.2730", "contents": "Title: Approximate deconvolution large eddy simulation of a barotropic ocean\n  circulation model Abstract: This paper puts forth a new large eddy simulation closure modeling strategy\nfor two-dimensional turbulent geophysical flows. This closure modeling approach\nutilizes approximate deconvolution, which is based solely on mathematical\napproximations and does not employ additional phenomenological arguments to the\nmodel. The new approximate deconvolution model is tested in the numerical\nsimulation of the wind-driven circulation in a shallow ocean basin, a standard\nprototype of more realistic ocean dynamics. The model employs the barotropic\nvorticity equation driven by a symmetric double-gyre wind forcing, which yields\na four-gyre circulation in the time mean. The approximate deconvolution model\nyields the correct four-gyre circulation structure predicted by a direct\nnumerical simulation, on a coarser mesh but at a fraction of the computational\ncost. This first step in the numerical assessment of the new model shows that\napproximate deconvolution could represent a viable tool for under-resolved\ncomputations in the large eddy simulation of more realistic turbulent\ngeophysical flows. \n\n"}
{"id": "1104.3975", "contents": "Title: Random matrix theory for underwater sound propagation Abstract: Ocean acoustic propagation can be formulated as a wave guide with a weakly\nrandom medium generating multiple scattering. Twenty years ago, this was\nrecognized as a quantum chaos problem, and yet random matrix theory, one pillar\nof quantum or wave chaos studies, has never been introduced into the subject.\nThe modes of the wave guide provide a representation for the propagation, which\nin the parabolic approximation is unitary. Scattering induced by the ocean's\ninternal waves leads to a power-law random banded unitary matrix ensemble for\nlong-range deep ocean acoustic propagation. The ensemble has similarities, but\ndiffers, from those introduced for studying the Anderson metal-insulator\ntransition. The resulting long-range propagation ensemble statistics agree well\nwith those of full wave propagation using the parabolic equation. \n\n"}
{"id": "1105.1449", "contents": "Title: A Hybrid (Monte-Carlo/Deterministic) Approach for Multi-Dimensional\n  Radiation Transport Abstract: A novel hybrid Monte Carlo transport scheme is demonstrated in a scene with\nsolar illumination, scattering and absorbing 2D atmosphere, a textured\nreflecting mountain, and a small detector located in the sky (mounted on a\nsatellite or a airplane). It uses a deterministic approximation of an adjoint\ntransport solution to reduce variance, computed quickly by ignoring atmospheric\ninteractions. This allows significant variance and computational cost\nreductions when the atmospheric scattering and absorption coefficient are\nsmall. When combined with an atmospheric photon-redirection scheme, significant\nvariance reduction (equivalently acceleration) is achieved in the presence of\natmospheric interactions. \n\n"}
{"id": "1105.4065", "contents": "Title: Atmospheric circulation of tidally locked exoplanets: II. Dual-band\n  radiative transfer and convective adjustment Abstract: Improving upon our purely dynamical work, we present three-dimensional\nsimulations of the atmospheric circulation on Earth-like (exo)planets and hot\nJupiters using the GFDL-Princeton Flexible Modeling System (FMS). As the first\nsteps away from the dynamical benchmarks of Heng, Menou & Phillipps (2011), we\nadd dual-band radiative transfer and dry convective adjustment schemes to our\ncomputational setup. Our treatment of radiative transfer assumes stellar\nirradiation to peak at a wavelength shorter than and distinct from that at\nwhich the exoplanet re-emits radiation (\"shortwave\" versus \"longwave\"), and\nalso uses a two-stream approximation. Convection is mimicked by adjusting\nunstable lapse rates to the dry adiabat. The bottom of the atmosphere is\nbounded by a uniform slab with a finite thermal inertia. For our models of hot\nJupiters, we include an analytical formalism for calculating\ntemperature-pressure profiles, in radiative equilibrium, which accounts for the\neffect of collision-induced absorption via a single parameter. We discuss our\nresults within the context of: the predicted temperature-pressure profiles and\nthe absence/presence of a temperature inversion; the possible maintenance, via\natmospheric circulation, of the putative high-altitude, shortwave absorber\nexpected to produce these inversions; the angular/temporal offset of the hot\nspot from the substellar point, its robustness to our ignorance of\nhyperviscosity and hence its utility in distinguishing between different hot\nJovian atmospheres; and various zonal-mean flow quantities. Our work bridges\nthe gap between three-dimensional simulations which are purely dynamical and\nthose which incorporate multi-band radiative transfer, thus contributing to the\nconstruction of a required hierarchy of three-dimensional theoretical models. \n\n"}
{"id": "1105.4409", "contents": "Title: Ionisation in atmospheres of Brown Dwarfs and extrasolar planets II\n  Dust-induced collisional ionization Abstract: Observations have shown that continuous radio emission and also sporadic\nH-alpha and X-ray emission are prominent in singular, low-mass objects later\nthan spectral class M. These activity signatures are interpreted as being\ncaused by coupling of an ionised atmosphere to the stellar magnetic field. What\nremains a puzzle, however, is the mechanism by which such a cool atmosphere can\nproduce the necessary level of ionisation. At these low temperatures, thermal\ngas processes are insufficient, but the formation of clouds sets in. Cloud\nparticles can act as seeds for electron avalanches in streamers that ionise the\nambient gas, and can lead to lightning and indirectly to magnetic field\ncoupling, a combination of processes also expected for protoplanetary disks.\nHowever, the precondition is that the cloud particles are charged.\n  We use results from Drift-Phoenix model atmospheres to investigate\ncollisional processes. We show that ionisation by turbulence-induced dust-dust\ncollisions is the most efficient kinetic process. Dust-dust collisions alone\nare not sufficient to improve the magnetic coupling of the atmosphere inside\nthe cloud layers, but the charges supplied either on grains or within the gas\nphase as separated electrons can trigger secondary non-linear processes. Cosmic\nrays are likely to increase the global level of ionisation, but their influence\ndecreases if a strong, large scale magnetic field is present as on Brown\nDwarfs.\n  We suggest that although thermal gas ionisation declines in objects across\nthe fully-convective boundary, dust charging by collisional processes can play\nan important role in the lowest mass objects. The onset of atmospheric dust may\ntherefore correlate with the anomalous X-ray and radio emission in atmospheres\nthat are cool, but charged more than expected by pure thermal ionisation. \n\n"}
{"id": "1105.5256", "contents": "Title: Parameter estimation in high dimensional Gaussian distributions Abstract: In order to compute the log-likelihood for high dimensional spatial Gaussian\nmodels, it is necessary to compute the determinant of the large, sparse,\nsymmetric positive definite precision matrix, Q. Traditional methods for\nevaluating the log-likelihood for very large models may fail due to the massive\nmemory requirements. We present a novel approach for evaluating such\nlikelihoods when the matrix-vector product, Qv, is fast to compute. In this\napproach we utilise matrix functions, Krylov subspaces, and probing vectors to\nconstruct an iterative method for computing the log-likelihood. \n\n"}
{"id": "1106.0322", "contents": "Title: Bayesian Sparsity-Path-Analysis of Genetic Association Signal using\n  Generalized t Priors Abstract: We explore the use of generalized t priors on regression coefficients to help\nunderstand the nature of association signal within \"hit regions\" of genome-wide\nassociation studies. The particular generalized t distribution we adopt is a\nStudent distribution on the absolute value of its argument. For low degrees of\nfreedom we show that the generalized t exhibits 'sparsity-prior' properties\nwith some attractive features over other common forms of sparse priors and\nincludes the well known double-exponential distribution as the degrees of\nfreedom tends to infinity. We pay particular attention to graphical\nrepresentations of posterior statistics obtained from sparsity-path-analysis\n(SPA) where we sweep over the setting of the scale (shrinkage / precision)\nparameter in the prior to explore the space of posterior models obtained over a\nrange of complexities, from very sparse models with all coefficient\ndistributions heavily concentrated around zero, to models with diffuse priors\nand coefficients distributed around their maximum likelihood estimates. The SPA\nplots are akin to LASSO plots of maximum a posteriori (MAP) estimates but they\ncharacterise the complete marginal posterior distributions of the coefficients\nplotted as a function of the precision of the prior. Generating posterior\ndistributions over a range of prior precisions is computationally challenging\nbut naturally amenable to sequential Monte Carlo (SMC) algorithms indexed on\nthe scale parameter. We show how SMC simulation on graphic-processing-units\n(GPUs) provides very efficient inference for SPA. We also present a\nscale-mixture representation of the generalized t prior that leads to an EM\nalgorithm to obtain MAP estimates should only these be required. \n\n"}
{"id": "1106.4736", "contents": "Title: Simulation of the 2009 Harmanli fire (Bulgaria) Abstract: We use a coupled atmosphere-fire model to simulate a fire that occurred on\nAugust 14--17, 2009, in the Harmanli region, Bulgaria. Data was obtained from\nGIS and satellites imagery, and from standard atmospheric data sources. Fuel\ndata was classified in the 13 Anderson categories. For correct fire behavior,\nthe spatial resolution of the models needed to be fine enough to resolve the\nessential micrometeorological effects. The simulation results are compared to\navailable incident data. The code runs faster than real time on a cluster. The\nmodel is available from openwfm.org and it extends WRF-Fire from WRF 3.3\nrelease. \n\n"}
{"id": "1106.4736", "contents": "Title: Simulation of the 2009 Harmanli fire (Bulgaria) Abstract: We use a coupled atmosphere-fire model to simulate a fire that occurred on\nAugust 14--17, 2009, in the Harmanli region, Bulgaria. Data was obtained from\nGIS and satellites imagery, and from standard atmospheric data sources. Fuel\ndata was classified in the 13 Anderson categories. For correct fire behavior,\nthe spatial resolution of the models needed to be fine enough to resolve the\nessential micrometeorological effects. The simulation results are compared to\navailable incident data. The code runs faster than real time on a cluster. The\nmodel is available from openwfm.org and it extends WRF-Fire from WRF 3.3\nrelease. \n\n"}
{"id": "1106.5850", "contents": "Title: Markov Chain Monte Carlo Based on Deterministic Transformations Abstract: In this article we propose a novel MCMC method based on deterministic\ntransformations T: X x D --> X where X is the state-space and D is some set\nwhich may or may not be a subset of X. We refer to our new methodology as\nTransformation-based Markov chain Monte Carlo (TMCMC). One of the remarkable\nadvantages of our proposal is that even if the underlying target distribution\nis very high-dimensional, deterministic transformation of a one-dimensional\nrandom variable is sufficient to generate an appropriate Markov chain that is\nguaranteed to converge to the high-dimensional target distribution. Apart from\nclearly leading to massive computational savings, this idea of\ndeterministically transforming a single random variable very generally leads to\nexcellent acceptance rates, even though all the random variables associated\nwith the high-dimensional target distribution are updated in a single block.\nSince it is well-known that joint updating of many random variables using\nMetropolis-Hastings (MH) algorithm generally leads to poor acceptance rates,\nTMCMC, in this regard, seems to provide a significant advance. We validate our\nproposal theoretically, establishing the convergence properties. Furthermore,\nwe show that TMCMC can be very effectively adopted for simulating from doubly\nintractable distributions.\n  TMCMC is compared with MH using the well-known Challenger data, demonstrating\nthe effectiveness of of the former in the case of highly correlated variables.\nMoreover, we apply our methodology to a challenging posterior simulation\nproblem associated with the geostatistical model of Diggle et al. (1998),\nupdating 160 unknown parameters jointly, using a deterministic transformation\nof a one-dimensional random variable. Remarkable computational savings as well\nas good convergence properties and acceptance rates are the results. \n\n"}
{"id": "1106.6280", "contents": "Title: On optimality of kernels for approximate Bayesian computation using\n  sequential Monte Carlo Abstract: Approximate Bayesian computation (ABC) has gained popularity over the past\nfew years for the analysis of complex models arising in population genetic,\nepidemiology and system biology. Sequential Monte Carlo (SMC) approaches have\nbecome work horses in ABC. Here we discuss how to construct the perturbation\nkernels that are required in ABC SMC approaches, in order to construct a set of\ndistributions that start out from a suitably defined prior and converge towards\nthe unknown posterior. We derive optimality criteria for different kernels,\nwhich are based on the Kullback-Leibler divergence between a distribution and\nthe distribution of the perturbed particles. We will show that for many\ncomplicated posterior distributions, locally adapted kernels tend to show the\nbest performance. In cases where it is possible to estimate the Fisher\ninformation we can construct particularly efficient perturbation kernels. We\nfind that the added moderate cost of adapting kernel functions is easily\nregained in terms of the higher acceptance rate. We demonstrate the\ncomputational efficiency gains in a range of toy-examples which illustrate some\nof the challenges faced in real-world applications of ABC, before turning to\ntwo demanding parameter inference problem in molecular biology, which highlight\nthe huge increases in efficiency that can be gained from choice of optimal\nmodels. We conclude with a general discussion of rational choice of\nperturbation kernels in ABC SMC settings. \n\n"}
{"id": "1107.1751", "contents": "Title: A diffusion-induced transition in the phase separation of binary fluid\n  mixtures subjected to a temperature ramp Abstract: Demixing of binary fluids subjected to slow temperature ramps shows repeated\nwaves of nucleation which arise as a consequence of the competition between\ngeneration of supersaturation by the temperature ramp and relaxation of\nsupersaturation by diffusive transport and flow. Here, we use an\nadvection-reaction-diffusion model to study the oscillations in the weak- and\nstrong-diffusion regime. There is a sharp transition between the two regimes,\nwhich can only be understood based on the probability distribution function of\nthe composition rather than in terms of the average composition. We argue that\nthis transition might be responsible for some yet unclear features of\nexperiments, like the appearance of secondary oscillations and bimodal droplet\nsize distributions. \n\n"}
{"id": "1107.5006", "contents": "Title: Joint downscale fluxes of energy and potential enstrophy in rotating\n  stratified Boussinesq flows Abstract: We employ a coarse-graining approach to analyze nonlinear cascades in\nBoussinesq flows using high-resolution simulation data. We derive budgets which\nresolve the evolution of energy and potential enstrophy simultaneously in space\nand in scale. We then use numerical simulations of Boussinesq flows, with\nforcing in the large-scales, and fixed rotation and stable stratification along\nthe vertical axis, to study the inter-scale flux of energy and potential\nenstrophy in three different regimes of stratification and rotation: (i) strong\nrotation and moderate stratification, (ii) moderate rotation and strong\nstratification, and (iii) equally strong stratification and rotation. In all\nthree cases, we observe constant fluxes of both global invariants, the mean\nenergy and mean potential enstrophy, from large to small scales. The existence\nof constant potential enstrophy flux ranges provides the first direct empirical\nevidence in support of the notion of a cascade of potential enstrophy. The\npersistent forward cascade of the two invariants reflects a marked departure of\nthese flows from two-dimensional turbulence. \n\n"}
{"id": "1107.5289", "contents": "Title: Planetary Atmospheres as Non-Equilibrium Condensed Matter Abstract: Planetary atmospheres, and models of them, are discussed from the viewpoint\nof condensed matter physics. Atmospheres are a form of condensed matter, and\nmany interesting phenomena of condensed matter systems are realized by them.\nThe essential physics of the general circulation is illustrated with idealized\n2-layer and 1-layer models of the atmosphere. Equilibrium and non-equilibrium\nstatistical mechanics are used to directly ascertain the statistics of these\nmodels. \n\n"}
{"id": "1109.2279", "contents": "Title: The Bayesian Bridge Abstract: We propose the Bayesian bridge estimator for regularized regression and\nclassification. Two key mixture representations for the Bayesian bridge model\nare developed: (1) a scale mixture of normals with respect to an alpha-stable\nrandom variable; and (2) a mixture of Bartlett--Fejer kernels (or triangle\ndensities) with respect to a two-component mixture of gamma random variables.\nBoth lead to MCMC methods for posterior simulation, and these methods turn out\nto have complementary domains of maximum efficiency. The first representation\nis a well known result due to West (1987), and is the better choice for\ncollinear design matrices. The second representation is new, and is more\nefficient for orthogonal problems, largely because it avoids the need to deal\nwith exponentially tilted stable random variables. It also provides insight\ninto the multimodality of the joint posterior distribution, a feature of the\nbridge model that is notably absent under ridge or lasso-type priors. We prove\na theorem that extends this representation to a wider class of densities\nrepresentable as scale mixtures of betas, and provide an explicit inversion\nformula for the mixing distribution. The connections with slice sampling and\nscale mixtures of normals are explored. On the practical side, we find that the\nBayesian bridge model outperforms its classical cousin in estimation and\nprediction across a variety of data sets, both simulated and real. We also show\nthat the MCMC for fitting the bridge model exhibits excellent mixing\nproperties, particularly for the global scale parameter. This makes for a\nfavorable contrast with analogous MCMC algorithms for other sparse Bayesian\nmodels. All methods described in this paper are implemented in the R package\nBayesBridge. An extensive set of simulation results are provided in two\nsupplemental files. \n\n"}
{"id": "1109.6090", "contents": "Title: Robust Parametric Classification and Variable Selection by a Minimum\n  Distance Criterion Abstract: We investigate a robust penalized logistic regression algorithm based on a\nminimum distance criterion. Influential outliers are often associated with the\nexplosion of parameter vector estimates, but in the context of standard\nlogistic regression, the bias due to outliers always causes the parameter\nvector to implode, that is shrink towards the zero vector. Thus, using\nLASSO-like penalties to perform variable selection in the presence of outliers\ncan result in missed detections of relevant covariates. We show that by\nchoosing a minimum distance criterion together with an Elastic Net penalty, we\ncan simultaneously find a parsimonious model and avoid estimation implosion\neven in the presence of many outliers in the important small $n$ large $p$\nsituation. Implementation using an MM algorithm is described and performance\nevaluated. \n\n"}
{"id": "1110.1805", "contents": "Title: Energy Release and Particle Acceleration in Flares: Summary and Future\n  Prospects Abstract: RHESSI measurements relevant to the fundamental processes of energy release\nand particle acceleration in flares are summarized. RHESSI's precise\nmeasurements of hard X-ray continuum spectra enable model-independent\ndeconvolution to obtain the parent electron spectrum. Taking into account the\neffects of albedo, these show that the low energy cut-off to the electron\npower-law spectrum is typically below tens of keV, confirming that the\naccelerated electrons contain a large fraction of the energy released in\nflares. RHESSI has detected a high coronal hard X-ray source that is filled\nwith accelerated electrons whose energy density is comparable to the\nmagnetic-field energy density. This suggests an efficient conversion of energy,\npreviously stored in the magnetic field, into the bulk acceleration of\nelectrons. A new, collisionless (Hall) magnetic reconnection process has been\nidentified through theory and simulations, and directly observed in space and\nin the laboratory; it should occur in the solar corona as well, with a\nreconnection rate fast enough for the energy release in flares. The\nreconnection process could result in the formation of multiple elongated\nmagnetic islands, that then collapse to bulk-accelerate the electrons, rapidly\nenough to produce the observed hard X-ray emissions. RHESSI's pioneering\n{\\gamma}-ray line imaging of energetic ions, revealing footpoints straddling a\nflare loop arcade, has provided strong evidence that ion acceleration is also\nrelated to magnetic reconnection. Flare particle acceleration is shown to have\na close relationship to impulsive Solar Energetic Particle (SEP) events\nobserved in the interplanetary medium, and also to both fast coronal mass\nejections and gradual SEP events. \n\n"}
{"id": "1110.6497", "contents": "Title: Bayesian Optimization for Adaptive MCMC Abstract: This paper proposes a new randomized strategy for adaptive MCMC using\nBayesian optimization. This approach applies to non-differentiable objective\nfunctions and trades off exploration and exploitation to reduce the number of\npotentially costly objective function evaluations. We demonstrate the strategy\nin the complex setting of sampling from constrained, discrete and densely\nconnected probabilistic graphical models where, for each variation of the\nproblem, one needs to adjust the parameters of the proposal mechanism\nautomatically to ensure efficient mixing of the Markov chains. \n\n"}
{"id": "1111.1308", "contents": "Title: Adaptive approximate Bayesian computation for complex models Abstract: Approximate Bayesian computation (ABC) is a family of computational\ntechniques in Bayesian statistics. These techniques allow to fi t a model to\ndata without relying on the computation of the model likelihood. They instead\nrequire to simulate a large number of times the model to be fi tted. A number\nof re finements to the original rejection-based ABC scheme have been proposed,\nincluding the sequential improvement of posterior distributions. This technique\nallows to de- crease the number of model simulations required, but it still\npresents several shortcomings which are particu- larly problematic for costly\nto simulate complex models. We here provide a new algorithm to perform adaptive\napproximate Bayesian computation, which is shown to perform better on both a\ntoy example and a complex social model. \n\n"}
{"id": "1111.2667", "contents": "Title: A note on the lack of symmetry in the graphical lasso Abstract: The graphical lasso (glasso) is a widely-used fast algorithm for estimating\nsparse inverse covariance matrices. The glasso solves an L1 penalized maximum\nlikelihood problem and is available as an R library on CRAN. The output from\nthe glasso, a regularized covariance matrix estimate a sparse inverse\ncovariance matrix estimate, not only identify a graphical model but can also\nserve as intermediate inputs into multivariate procedures such as PCA, LDA,\nMANOVA, and others. The glasso indeed produces a covariance matrix estimate\nwhich solves the L1 penalized optimization problem in a dual sense; however,\nthe method for producing the inverse covariance matrix estimator after this\noptimization is inexact and may produce asymmetric estimates. This problem is\nexacerbated when the amount of L1 regularization that is applied is small,\nwhich in turn is more likely to occur if the true underlying inverse covariance\nmatrix is not sparse. The lack of symmetry can potentially have consequences.\nFirst, it implies that the covariance and inverse covariance estimates are not\nnumerical inverses of one another, and second, asymmetry can possibly lead to\nnegative or complex eigenvalues,rendering many multivariate procedures which\nmay depend on the inverse covariance estimator unusable. We demonstrate this\nproblem, explain its causes, and propose possible remedies. \n\n"}
{"id": "1111.4610", "contents": "Title: A wildland fire modeling and visualization environment Abstract: We present an overview of a modeling environment, consisting of a coupled\natmosphere-wildfire model, utilities for visualization, data processing, and\ndiagnostics, open source software repositories, and a community wiki. The fire\nmodel, called SFIRE, is based on a fire-spread model, implemented by the\nlevel-set method, and it is coupled with the Weather Research Forecasting (WRF)\nmodel. A version with a subset of the features is distributed with WRF 3.3 as\nWRF-Fire. In each time step, the fire module takes the wind as input and\nreturns the latent and sensible heat fluxes. The software architecture uses WRF\nparallel infrastructure for massively parallel computing. Recent features of\nthe code include interpolation from an ideal logarithmic wind profile for\nnonhomogeneous fuels and ignition from a fire perimeter with an atmosphere and\nfire spin-up. Real runs use online sources for fuel maps, fine-scale\ntopography, and meteorological data, and can run faster than real time.\nVisualization pathways allow generating images and animations in many packages,\nincluding VisTrails, VAPOR, MayaVi, and Paraview, as well as output to Google\nEarth. The environment is available from openwfm.org. New diagnostic variables\nwere added to the code recently, including a new kind of fireline intensity,\nwhich takes into account also the speed of burning, unlike Byram's fireline\nintensity. \n\n"}
{"id": "1111.5421", "contents": "Title: Markovian stochastic approximation with expanding projections Abstract: Stochastic approximation is a framework unifying many random iterative\nalgorithms occurring in a diverse range of applications. The stability of the\nprocess is often difficult to verify in practical applications and the process\nmay even be unstable without additional stabilisation techniques. We study a\nstochastic approximation procedure with expanding projections similar to\nAndrad\\'{o}ttir [Oper. Res. 43 (1995) 1037-1048]. We focus on Markovian noise\nand show the stability and convergence under general conditions. Our framework\nalso incorporates the possibility to use a random step size sequence, which\nallows us to consider settings with a non-smooth family of Markov kernels. We\napply the theory to stochastic approximation expectation maximisation with\nparticle independent Metropolis-Hastings sampling. \n\n"}
{"id": "1112.1917", "contents": "Title: Invariant parameterization and turbulence modeling on the beta-plane Abstract: Invariant parameterization schemes for the eddy-vorticity flux in the\nbarotropic vorticity equation on the beta-plane are constructed and then\napplied to turbulence modeling. This construction is realized by the exhaustive\ndescription of differential invariants for the maximal Lie invariance\npseudogroup of this equation using the method of moving frames, which includes\nfinding functional bases of differential invariants of arbitrary order, a\nminimal generating set of differential invariants and a basis of operators of\ninvariant differentiation in an explicit form. Special attention is paid to the\nproblem of two-dimensional turbulence on the beta-plane. It is shown that\nclassical hyperdiffusion as used to initiate the energy-enstrophy cascades\nviolates the symmetries of the vorticity equation. Invariant but nonlinear\nhyperdiffusion-like terms of new types are introduced and then used in the\ncourse of numerically integrating the vorticity equation and carrying out\nfreely decaying turbulence tests. It is found that the invariant hyperdiffusion\nscheme is close to but not exactly reproducing the 1/k shape of energy spectrum\nin the enstrophy inertial range. By presenting conservative invariant\nhyperdiffusion terms, we also demonstrate that the concepts of invariant and\nconservative parameterizations are consistent. \n\n"}
{"id": "1112.3235", "contents": "Title: How can a glacial inception be predicted? Abstract: The Early Anthropogenic Hypothesis considers that greenhouse gas\nconcentrations should have declined during the Holocene in absence of humankind\nactivity, leading to glacial inception around the present. It partly relies on\nthe fact that present levels of northern summer incoming solar radiation are\nclose to those that, in the past, preceded a glacial inception phenomenon,\nassociated to declines in greenhouse gas concentrations. However, experiments\nwith various numerical models of glacial cycles show that next glacial\ninception may still be delayed by several ten thousands of years, even with the\nassumption of greenhouse gas concentration declines during the Holocene.\nFurthermore, as we show here, conceptual models designed to capture the gross\ndynamics of the climate system as a whole suggest also that small disturbances\nmay sometimes cause substantial delays in glacial events, causing a fair level\nof unpredictability on ice age dynamics. This suggests the need of a validated\nmathematical description of the climate system dynamics that allows us to\nquantify uncertainties on predictions. Here, it is proposed to organise our\nknowledge about the physics and dynamics of glacial cycles through a Bayesian\ninference network. Constraints on the physics and dynamics of climate can be\nencapsulated into a stochastic dynamical system. These constraints include, in\nparticular, estimates of the sensitivity of the components of climate to\nexternal forcings, inferred from plans of experiments with large simulators of\nthe atmosphere, oceans and ice sheets. On the other hand, palaeoclimate\nobservations are accounted for through a process of parameter calibration. We\ndiscuss promises and challenges raised by this programme. \n\n"}
{"id": "1201.0498", "contents": "Title: Invariant discretization schemes for the shallow-water equations Abstract: Invariant discretization schemes are derived for the one- and two-dimensional\nshallow-water equations with periodic boundary conditions. While originally\ndesigned for constructing invariant finite difference schemes, we extend the\nusage of difference invariants to allow constructing of invariant finite volume\nmethods as well. It is found that the classical invariant schemes converge to\nthe Lagrangian formulation of the shallow-water equations. These schemes\nrequire to redistribute the grid points according to the physical fluid\nvelocity, i.e., the mesh cannot remain fixed in the course of the numerical\nintegration. Invariant Eulerian discretization schemes are proposed for the\nshallow-water equations in computational coordinates. Instead of using the\nfluid velocity as the grid velocity, an invariant moving mesh generator is\ninvoked in order to determine the location of the grid points at the subsequent\ntime level. The numerical conservation of energy, mass and momentum is\nevaluated for both the invariant and non-invariant schemes. \n\n"}
{"id": "1201.1314", "contents": "Title: Some discussions of D. Fearnhead and D. Prangle's Read Paper\n  \"Constructing summary statistics for approximate Bayesian computation:\n  semi-automatic approximate Bayesian computation\" Abstract: This report is a collection of comments on the Read Paper of Fearnhead and\nPrangle (2011), to appear in the Journal of the Royal Statistical Society\nSeries B, along with a reply from the authors. \n\n"}
{"id": "1201.4679", "contents": "Title: Nonlinear problems of complex natural systems: Sun and climate dynamics Abstract: Universal role of the nonlinear one-third subharmonic resonance mechanism in\ngeneration of the strong fluctuations in such complex natural dynamical systems\nas global climate and global solar activity is discussed using wavelet\nregression detrended data. Role of the oceanic Rossby waves in the year-scale\nglobal temperature fluctuations and the nonlinear resonance contribution to the\nEl Nino phenomenon have been discussed in detail. The large fluctuations of the\nreconstructed temperature on the millennial time-scales (Antarctic ice cores\ndata for the past 400,000 years) are also shown to be dominated by the\none-third subharmonic resonance, presumably related to Earth precession effect\non the energy that the intertropical regions receive from the Sun. Effects of\nGalactic turbulence on the temperature fluctuations are discussed in this\ncontent. It is also shown that the one-third subharmonic resonance can be\nconsidered as a background for the 11-years solar cycle, and again the global\n(solar) rotation and chaotic propagating waves play significant role in this\nphenomenon. Finally, a multidecadal chaotic coherence between the detrended\nsolar activity and global temperature has been briefly discussed. \n\n"}
{"id": "1202.1738", "contents": "Title: INLA or MCMC? A Tutorial and Comparative Evaluation for Spatial\n  Prediction in log-Gaussian Cox Processes Abstract: We investigate two options for performing Bayesian inference on spatial\nlog-Gaussian Cox processes assuming a spatially continuous latent field: Markov\nchain Monte Carlo (MCMC) and the integrated nested Laplace approximation\n(INLA). We first describe the device of approximating a spatially continuous\nGaussian field by a Gaussian Markov random field on a discrete lattice, and\npresent a simulation study showing that, with careful choice of parameter\nvalues, small neighbourhood sizes can give excellent approximations. We then\nintroduce the spatial log-Gaussian Cox process and describe MCMC and INLA\nmethods for spatial prediction within this model class. We report the results\nof a simulation study in which we compare MALA and the technique of\napproximating the continuous latent field by a discrete one, followed by\napproximate Bayesian inference via INLA over a selection of 18 simulated\nscenarios. The results question the notion that the latter technique is both\nsignificantly faster and more robust than MCMC in this setting; 100,000\niterations of the MALA algorithm running in 20 minutes on a desktop PC\ndelivered greater predictive accuracy than the default \\verb=INLA= strategy,\nwhich ran in 4 minutes and gave comparative performance to the full Laplace\napproximation which ran in 39 minutes. \n\n"}
{"id": "1202.3345", "contents": "Title: The Influence of Atmospheric Scattering and Absorption on Ohmic\n  Dissipation in Hot Jupiters Abstract: Using semi-analytical, one-dimensional models, we elucidate the influence of\nscattering and absorption on the degree of Ohmic dissipation in hot Jovian\natmospheres. With the assumption of Saha equilibrium, the variation in\ntemperature is the main driver of the variations in the electrical\nconductivity, induced current and Ohmic power dissipated. Atmospheres\npossessing temperature inversions tend to dissipate most of the Ohmic power\nsuperficially, at high altitudes, whereas those without temperature inversions\nare capable of greater dissipation deeper down. Scattering in the optical range\nof wavelengths tends to cool the lower atmosphere, thus reducing the degree of\ndissipation at depth. Purely absorbing cloud decks (in the infrared), of a\nfinite extent in height, allow for localized reductions in dissipation and may\nreverse a temperature inversion if they are dense and thick enough, thus\ngreatly enhancing the dissipation at depth. If Ohmic dissipation is the\nmechanism for inflating hot Jupiters, then variations in the atmospheric\nopacity (which may be interpreted as arising from variations in metallicity and\ncloud/haze properties) and magnetic field strength naturally produce a scatter\nin the measured radii at a given strength of irradiation. Future work will\ndetermine if these effects are dominant over evolutionary effects, which also\ncontribute a scatter to the measured radii. \n\n"}
{"id": "1202.6403", "contents": "Title: Cosmic-muon flux and annual modulation in Borexino at 3800 m\n  water-equivalent depth Abstract: We have measured the muon flux at the underground Gran Sasso National\nLaboratory (3800 m w.e.) to be (3.41 \\pm 0.01) \\times 10-4m-2s-1 using four\nyears of Borexino data. A modulation of this signal is observed with a period\nof (366\\pm3) days and a relative amplitude of (1.29 \\pm 0.07)%. The measured\nphase is (179 \\pm 6) days, corresponding to a maximum on the 28th of June.\nUsing the most complete atmospheric data models available, muon rate\nfluctuations are shown to be positively correlated with atmospheric\ntemperature, with an effective coefficient {\\alpha}T = 0.93 \\pm 0.04. This\nresult represents the most precise study of the muon flux modulation for this\nsite and is in good agreement with expectations. \n\n"}
{"id": "1203.0258", "contents": "Title: Stable Langmuir solitons in plasma with diatomic ions Abstract: We study stable axially and spherically symmetric spatial solitons in plasma\nwith diatomic ions. The stability of a soliton against the collapse is provided\nby the interaction of induced electric dipole moments of ions with rapidly\noscillating electric field of a plasmoid. We derive the new cubic-quintic\nnonlinear Schrodinger equation which governs the soliton dynamics and\nnumerically solve it. Then we discuss the possibility of implementation of such\nplasmoids in realistic atmospheric plasma. In particular, we suggest that\nspherically symmetric Langmuir solitons, described in the present work, can be\nexcited at the formation stage of long-lived atmospheric plasma structures. The\nimplication of our model for the interpretation of the results of experiments\nfor the plasmoids generation is discussed. \n\n"}
{"id": "1203.3896", "contents": "Title: Fast and Adaptive Sparse Precision Matrix Estimation in High Dimensions Abstract: This paper proposes a new method for estimating sparse precision matrices in\nthe high dimensional setting. It has been popular to study fast computation and\nadaptive procedures for this problem. We propose a novel approach, called\nSparse Column-wise Inverse Operator, to address these two issues. We analyze an\nadaptive procedure based on cross validation, and establish its convergence\nrate under the Frobenius norm. The convergence rates under other matrix norms\nare also established. This method also enjoys the advantage of fast computation\nfor large-scale problems, via a coordinate descent algorithm. Numerical merits\nare illustrated using both simulated and real datasets. In particular, it\nperforms favorably on an HIV brain tissue dataset and an ADHD resting-state\nfMRI dataset. \n\n"}
{"id": "1203.5615", "contents": "Title: Intensity and polarization of the atmospheric emission at millimetric\n  wavelengths at Dome Concordia Abstract: Atmospheric emission is a dominant source of disturbance in ground-based\nastronomy at mm wavelengths. The Antarctic plateau is recognized to be an ideal\nsite for mm and sub-mm observations, and the French/Italian base of Dome C is\namong the best sites on Earth for these observations. In this paper we present\nmeasurements, performed using the BRAIN-pathfinder experiment, at Dome C of the\natmospheric emission in intensity and polarization at 150GHz, one of the best\nobservational frequencies for CMB observations when considering cosmic signal\nintensity, atmospheric transmission, detectors sensitivity, and foreground\nremoval. Careful characterization of the air-mass synchronous emission has been\nperformed, acquiring more that 380 elevation scans (i.e. \"skydip\") during the\nthird BRAIN-pathfinder summer campaign in December 2009/January 2010. The\nextremely high transparency of the Antarctic atmosphere over Dome Concordia is\nproven by the very low measured optical depth: <tau_I>=0.050 \\pm 0.003 \\pm\n0.011 where the first error is statistical and the second is systematic error.\nMid term stability, over the summer campaign, of the atmosphere emission has\nalso been studied. Adapting the radiative transfer atmosphere emission model\n\"am\" to the particular conditions found at Dome C, we also infer the level of\nthe PWV content of the atmosphere, notoriously the main source of disturbance\nin millimetric astronomy (<PWV>=0.77 +/- 0.06 + 0.15 - 0.12 mm). Upper limits\non the air-mass correlated polarized signal are also placed for the first time.\nThe degree of circular polarization of atmospheric emission is found to be\nlower than 0.2% (95%CL), while the degree of linear polarization is found to be\nlower than 0.1% (95%CL). These limits include signal-correlated instrumental\nspurious polarization. \n\n"}
{"id": "1204.2889", "contents": "Title: Special solutions to a compact equation for deep-water gravity waves Abstract: Recently, Dyachenko & Zakharov (2011) have derived a compact form of the well\nknown Zakharov integro-differential equation for the third order Hamiltonian\ndynamics of a potential flow of an incompressible, infinitely deep fluid with a\nfree surface. Special traveling wave solutions of this compact equation are\nnumerically constructed using the Petviashvili method. Their stability\nproperties are also investigated. Further, unstable traveling waves with\nwedge-type singularities, viz. peakons, are numerically discovered. To gain\ninsights into the properties of singular traveling waves, we consider the\nacademic case of a perturbed version of the compact equation, for which\nanalytical peakons with exponential shape are derived. Finally, by means of an\naccurate Fourier-type spectral scheme it is found that smooth solitary waves\nappear to collide elastically, suggesting the integrability of the Zakharov\nequation. \n\n"}
{"id": "1204.4449", "contents": "Title: The faint young Sun problem Abstract: For more than four decades, scientists have been trying to find an answer to\none of the most fundamental questions in paleoclimatology, the `faint young Sun\nproblem'. For the early Earth, models of stellar evolution predict a solar\nenergy input to the climate system which is about 25% lower than today. This\nwould result in a completely frozen world over the first two billion years in\nthe history of our planet, if all other parameters controlling Earth's climate\nhad been the same. Yet there is ample evidence for the presence of liquid\nsurface water and even life in the Archean (3.8 to 2.5 billion years before\npresent), so some effect (or effects) must have been compensating for the faint\nyoung Sun. A wide range of possible solutions have been suggested and explored\nduring the last four decades, with most studies focusing on higher\nconcentrations of atmospheric greenhouse gases like carbon dioxide, methane or\nammonia. All of these solutions present considerable difficulties, however, so\nthe faint young Sun problem cannot be regarded as solved. Here I review\nresearch on the subject, including the latest suggestions for solutions of the\nfaint young Sun problem and recent geochemical constraints on the composition\nof Earth's early atmosphere. Furthermore, I will outline the most promising\ndirections for future research. In particular I would argue that both improved\ngeochemical constraints on the state of the Archean climate system and\nnumerical experiments with state-of-the-art climate models are required to\nfinally assess what kept the oceans on the Archean Earth from freezing over\ncompletely. \n\n"}
{"id": "1204.5445", "contents": "Title: A recent tipping point in the Arctic sea-ice cover: abrupt and\n  persistent increase in the seasonal cycle since 2007 Abstract: There is ongoing debate over whether Arctic sea-ice has already passed a\n`tipping point', or whether it will do so in the future. Several recent studies\nargue that the loss of summer sea ice does not involve an irreversible\nbifurcation, because it is highly reversible in models. However, a broader\ndefinition of a `tipping point' also includes other abrupt, non-linear changes\nthat are neither bifurcations nor necessarily irreversible. Examination of\nsatellite data for Arctic sea-ice area reveals an abrupt increase in the\namplitude of seasonal variability in 2007 that has persisted since then. We\nidentified this abrupt transition using recently developed methods that can\ndetect multi-modality in time-series data and sometimes forewarn of\nbifurcations. When removing the mean seasonal cycle (up to 2008) from the\nsatellite data, the residual sea-ice fluctuations switch from uni-modal to\nmulti-modal behaviour around 2007. We originally interpreted this as a\nbifurcation in which a new lower ice cover attractor appears in deseasonalised\nfluctuations and is sampled in every summer-autumn from 2007 onwards. However,\nthis interpretation is clearly sensitive to how the seasonal cycle is removed\nfrom the raw data, and to the presence of continental land masses restricting\nwinter-spring ice fluctuations. Furthermore, there was no robust early warning\nsignal of critical slowing down prior to the hypothesized bifurcation. Early\nwarning indicators do however show destabilization of the summer-autumn sea-ice\ncover since 2007. Thus, the bifurcation hypothesis lacks consistent support,\nbut there was an abrupt and persistent increase in the amplitude of the\nseasonal cycle of Arctic sea-ice cover in 2007, which we describe as a\n(non-bifurcation) `tipping point'. Our statistical methods detect this `tipping\npoint' and its time of onset. \n\n"}
{"id": "1204.5459", "contents": "Title: Inference for SDE models via Approximate Bayesian Computation Abstract: Models defined by stochastic differential equations (SDEs) allow for the\nrepresentation of random variability in dynamical systems. The relevance of\nthis class of models is growing in many applied research areas and is already a\nstandard tool to model e.g. financial, neuronal and population growth dynamics.\nHowever inference for multidimensional SDE models is still very challenging,\nboth computationally and theoretically. Approximate Bayesian computation (ABC)\nallow to perform Bayesian inference for models which are sufficiently complex\nthat the likelihood function is either analytically unavailable or\ncomputationally prohibitive to evaluate. A computationally efficient ABC-MCMC\nalgorithm is proposed, halving the running time in our simulations. Focus is on\nthe case where the SDE describes latent dynamics in state-space models; however\nthe methodology is not limited to the state-space framework. Simulation studies\nfor a pharmacokinetics/pharmacodynamics model and for stochastic chemical\nreactions are considered and a MATLAB package implementing our ABC-MCMC\nalgorithm is provided. \n\n"}
{"id": "1204.6392", "contents": "Title: Statistical mechanics of quasi-geostrophic flows on a rotating sphere Abstract: Statistical mechanics provides an elegant explanation to the appearance of\ncoherent structures in two-dimensional inviscid turbulence: while the\nfine-grained vorticity field, described by the Euler equation, becomes more and\nmore filamented through time, its dynamical evolution is constrained by some\nglobal conservation laws (energy, Casimir invariants). As a consequence, the\ncoarse-grained vorticity field can be predicted through standard statistical\nmechanics arguments (relying on the Hamiltonian structure of the\ntwo-dimensional Euler flow), for any given set of the integral constraints.\n  It has been suggested that the theory applies equally well to geophysical\nturbulence; specifically in the case of the quasi-geostrophic equations, with\npotential vorticity playing the role of the advected quantity. In this study,\nwe demonstrate analytically that the Miller-Robert-Sommeria theory leads to\nnon-trivial statistical equilibria for quasi-geostrophic flows on a rotating\nsphere, with or without bottom topography. We first consider flows without\nbottom topography and with an infinite Rossby deformation radius, with and\nwithout conservation of angular momentum. When the conservation of angular\nmomentum is taken into account, we report a case of second order phase\ntransition associated with spontaneous symmetry breaking. In a second step, we\ntreat the general case of a flow with an arbitrary bottom topography and a\nfinite Rossby deformation radius. Previous studies were restricted to flows in\na planar domain with fixed or periodic boundary conditions with a beta-effect.\n  In these different cases, we are able to classify the statistical equilibria\nfor the large-scale flow through their sole macroscopic features. We build the\nphase diagrams of the system and discuss the relations of the various\nstatistical ensembles. \n\n"}
{"id": "1204.6516", "contents": "Title: Detection of additive outliers in Poisson INteger-valued AutoRegressive\n  time series Abstract: Outlying observations are commonly encountered in the analysis of time\nseries. In this paper the problem of detecting additive outliers in\ninteger-valued time series is considered. We show how Gibbs sampling can be\nused to detect outlying observations in INAR(1) processes. The methodology\nproposed is illustrated using examples as well as an observed data set. \n\n"}
{"id": "1205.0482", "contents": "Title: On the Generalized Ratio of Uniforms as a Combination of Transformed\n  Rejection and Extended Inverse of Density Sampling Abstract: In this work we investigate the relationship among three classical sampling\ntechniques: the inverse of density (Khintchine's theorem), the transformed\nrejection (TR) and the generalized ratio of uniforms (GRoU). Given a monotonic\nprobability density function (PDF), we show that the transformed area obtained\nusing the generalized ratio of uniforms method can be found equivalently by\napplying the transformed rejection sampling approach to the inverse function of\nthe target density. Then we provide an extension of the classical inverse of\ndensity idea, showing that it is completely equivalent to the GRoU method for\nmonotonic densities. Although we concentrate on monotonic probability density\nfunctions (PDFs), we also discuss how the results presented here can be\nextended to any non-monotonic PDF that can be decomposed into a collection of\nintervals where it is monotonically increasing or decreasing. In this general\ncase, we show the connections with transformations of certain random variables\nand the generalized inverse PDF with the GRoU technique. Finally, we also\nintroduce a GRoU technique to handle unbounded target densities. \n\n"}
{"id": "1205.1880", "contents": "Title: Non-Parametric Methods Applied to the N-Sample Series Comparison Abstract: Anomaly and similarity detection in multidimensional series have a long\nhistory and have found practical usage in many different fields such as\nmedicine, networks, and finance. Anomaly detection is of great appeal for many\ndifferent disciplines; for example, mathematicians searching for a unified\nmathematical formulation based on probability, statisticians searching for\nerror bound estimates, and computer scientists who are trying to design fast\nalgorithms, to name just a few. In summary, we have two contributions: First,\nwe present a self-contained survey of the most promising methods being used in\nthe fields of machine learning, statistics, and bio-informatics today. Included\nwe present discussions about conformal prediction, kernels in the Hilbert\nspace, Kolmogorov's information measure, and non-parametric cumulative\ndistribution function comparison methods (NCDF). Second, building upon this\nfoundation, we provide a powerful NCDF method for series with small\ndimensionality. Through a combination of data organization and statistical\ntests, we describe extensions that scale well with increased dimensionality. \n\n"}
{"id": "1205.2911", "contents": "Title: Factorial graphical lasso for dynamic networks Abstract: Dynamic networks models describe a growing number of important scientific\nprocesses, from cell biology and epidemiology to sociology and finance. There\nare many aspects of dynamical networks that require statistical considerations.\nIn this paper we focus on determining network structure. Estimating dynamic\nnetworks is a difficult task since the number of components involved in the\nsystem is very large. As a result, the number of parameters to be estimated is\nbigger than the number of observations. However, a characteristic of many\nnetworks is that they are sparse. For example, the molecular structure of genes\nmake interactions with other components a highly-structured and therefore\nsparse process.\n  Penalized Gaussian graphical models have been used to estimate sparse\nnetworks. However, the literature has focussed on static networks, which lack\nspecific temporal constraints. We propose a structured Gaussian dynamical\ngraphical model, where structures can consist of specific time dynamics, known\npresence or absence of links and block equality constraints on the parameters.\nThus, the number of parameters to be estimated is reduced and accuracy of the\nestimates, including the identification of the network, can be tuned up. Here,\nwe show that the constrained optimization problem can be solved by taking\nadvantage of an efficient solver, logdetPPA, developed in convex optimization.\nMoreover, model selection methods for checking the sensitivity of the inferred\nnetworks are described. Finally, synthetic and real data illustrate the\nproposed methodologies. \n\n"}
{"id": "1205.3906", "contents": "Title: Variational Inference for Generalized Linear Mixed Models Using\n  Partially Noncentered Parametrizations Abstract: The effects of different parametrizations on the convergence of Bayesian\ncomputational algorithms for hierarchical models are well explored. Techniques\nsuch as centering, noncentering and partial noncentering can be used to\naccelerate convergence in MCMC and EM algorithms but are still not well studied\nfor variational Bayes (VB) methods. As a fast deterministic approach to\nposterior approximation, VB is attracting increasing interest due to its\nsuitability for large high-dimensional data. Use of different parametrizations\nfor VB has not only computational but also statistical implications, as\ndifferent parametrizations are associated with different factorized posterior\napproximations. We examine the use of partially noncentered parametrizations in\nVB for generalized linear mixed models (GLMMs). Our paper makes four\ncontributions. First, we show how to implement an algorithm called nonconjugate\nvariational message passing for GLMMs. Second, we show that the partially\nnoncentered parametrization can adapt to the quantity of information in the\ndata and determine a parametrization close to optimal. Third, we show that\npartial noncentering can accelerate convergence and produce more accurate\nposterior approximations than centering or noncentering. Finally, we\ndemonstrate how the variational lower bound, produced as part of the\ncomputation, can be useful for model selection. \n\n"}
{"id": "1205.5658", "contents": "Title: Bayesian computation via empirical likelihood Abstract: Approximate Bayesian computation (ABC) has become an essential tool for the\nanalysis of complex stochastic models when the likelihood function is\nnumerically unavailable. However, the well-established statistical method of\nempirical likelihood provides another route to such settings that bypasses\nsimulations from the model and the choices of the ABC parameters (summary\nstatistics, distance, tolerance), while being convergent in the number of\nobservations. Furthermore, bypassing model simulations may lead to significant\ntime savings in complex models, for instance those found in population\ngenetics. The BCel algorithm we develop in this paper also provides an\nevaluation of its own performance through an associated effective sample size.\nThe method is illustrated using several examples, including estimation of\nstandard distributions, time series, and population genetics models. \n\n"}
{"id": "1206.0039", "contents": "Title: A Multi-Baseline 12 GHz Atmospheric Phase Interferometer with One Micron\n  Path Length Sensitivity Abstract: We have constructed a five station 12 GHz atmospheric phase interferometer\n(API) for the Submillimeter Array (SMA) located near the summit of Mauna Kea,\nHawaii. Operating at the base of unoccupied SMA antenna pads, each station\nemploys a commercial low noise mixing block coupled to a 0.7 m off-axis\nsatellite dish which receives a broadband, white noise-like signal from a\ngeostationary satellite. The signals are processed by an analog correlator to\nproduce the phase delays between all pairs of stations with projected baselines\nranging from 33 to 261 m. Each baseline's amplitude and phase is measured\ncontinuously at a rate of 8 kHz, processed, averaged and output at 10 Hz.\nFurther signal processing and data reduction is accomplished with a Linux\ncomputer, including the removal of the diurnal motion of the target satellite.\nThe placement of the stations below ground level with an environmental shield\ncombined with the use of low temperature coefficient, buried fiber optic cables\nprovides excellent system stability. The sensitivity in terms of rms path\nlength is 1.3 microns which corresponds to phase deviations of about 1 degree\nof phase at the highest operating frequency of the SMA. The two primary data\nproducts are: (1) standard deviations of observed phase over various time\nscales, and (2) phase structure functions. These real-time statistical data\nmeasured by the API in the direction of the satellite provide an estimate of\nthe phase front distortion experienced by the concurrent SMA astronomical\nobservations. The API data also play an important role, along with the local\nopacity measurements and weather predictions, in helping to plan the scheduling\nof science observations on the telescope. \n\n"}
{"id": "1206.0338", "contents": "Title: Poisson noise reduction with non-local PCA Abstract: Photon-limited imaging arises when the number of photons collected by a\nsensor array is small relative to the number of detector elements. Photon\nlimitations are an important concern for many applications such as spectral\nimaging, night vision, nuclear medicine, and astronomy. Typically a Poisson\ndistribution is used to model these observations, and the inherent\nheteroscedasticity of the data combined with standard noise removal methods\nyields significant artifacts. This paper introduces a novel denoising algorithm\nfor photon-limited images which combines elements of dictionary learning and\nsparse patch-based representations of images. The method employs both an\nadaptation of Principal Component Analysis (PCA) for Poisson noise and recently\ndeveloped sparsity-regularized convex optimization algorithms for\nphoton-limited images. A comprehensive empirical evaluation of the proposed\nmethod helps characterize the performance of this approach relative to other\nstate-of-the-art denoising methods. The results reveal that, despite its\nconceptual simplicity, Poisson PCA-based denoising appears to be highly\ncompetitive in very low light regimes. \n\n"}
{"id": "1206.4709", "contents": "Title: Constructing acoustic timefronts using random matrix theory Abstract: In a recent letter [Europhys. Lett. 97, 34002 (2012)], random matrix theory\nis introduced for long-range acoustic propagation in the ocean. The theory is\nexpressed in terms of unitary propagation matrices that represent the\nscattering between acoustic modes due to sound speed fluctuations induced by\nthe ocean's internal waves. The scattering exhibits a power-law decay as a\nfunction of the differences in mode numbers thereby generating a power-law,\nbanded, random unitary matrix ensemble. This work gives a more complete account\nof that approach and extends the methods to the construction of an ensemble of\nacoustic timefronts. The result is a very efficient method for studying the\nstatistical properties of timefronts at various propagation ranges that agrees\nwell with propagation based on the parabolic equation. It helps identify which\ninformation about the ocean environment survives in the timefronts and how to\nconnect features of the data to the surviving environmental information. It\nalso makes direct connections to methods used in other disordered wave guide\ncontexts where the use of random matrix theory has a multi-decade history. \n\n"}
{"id": "1206.6378", "contents": "Title: Computing the asymptotic power of a Euclidean-distance test for\n  goodness-of-fit Abstract: A natural (yet unconventional) test for goodness-of-fit measures the\ndiscrepancy between the model and empirical distributions via their Euclidean\ndistance (or, equivalently, via its square). The present paper characterizes\nthe statistical power of such a test against a family of alternative\ndistributions, in the limit that the number of observations is large, with\nevery alternative departing from the model in the same direction. Specifically,\nthe paper provides an efficient numerical method for evaluating the cumulative\ndistribution function (cdf) of the square of the Euclidean distance between the\nmodel and empirical distributions under the alternatives, in the limit that the\nnumber of observations is large. The paper illustrates the scheme by plotting\nthe asymptotic power (as a function of the significance level) for several\nexamples. \n\n"}
{"id": "1206.6532", "contents": "Title: Estimating Nuisance Parameters in Inverse Problems Abstract: Many inverse problems include nuisance parameters which, while not of direct\ninterest, are required to recover primary parameters. Structure present in\nthese problems allows efficient optimization strategies - a well known example\nis variable projection, where nonlinear least squares problems which are linear\nin some parameters can be very efficiently optimized. In this paper, we extend\nthe idea of projecting out a subset over the variables to a broad class of\nmaximum likelihood (ML) and maximum a posteriori likelihood (MAP) problems with\nnuisance parameters, such as variance or degrees of freedom. As a result, we\nare able to incorporate nuisance parameter estimation into large-scale\nconstrained and unconstrained inverse problem formulations. We apply the\napproach to a variety of problems, including estimation of unknown variance\nparameters in the Gaussian model, degree of freedom (d.o.f.) parameter\nestimation in the context of robust inverse problems, automatic calibration,\nand optimal experimental design. Using numerical examples, we demonstrate\nimprovement in recovery of primary parameters for several large- scale inverse\nproblems. The proposed approach is compatible with a wide variety of algorithms\nand formulations, and its implementation requires only minor modifications to\nexisting algorithms. \n\n"}
{"id": "1206.6919", "contents": "Title: Complete point symmetry group of the barotropic vorticity equation on a\n  rotating sphere Abstract: The complete point symmetry group of the barotropic vorticity equation on the\nsphere is determined. The method we use relies on the invariance of megaideals\nof the maximal Lie invariance algebra of a system of differential equations\nunder automorphisms generated by the associated group. A convenient set of\nmegaideals is found for the maximal Lie invariance algebra of the spherical\nvorticity equation. We prove that there are only two independent (up to\ncomposition with continuous point symmetry transformations) discrete symmetries\nfor this equation. \n\n"}
{"id": "1206.7051", "contents": "Title: Stochastic Variational Inference Abstract: We develop stochastic variational inference, a scalable algorithm for\napproximating posterior distributions. We develop this technique for a large\nclass of probabilistic models and we demonstrate it with two probabilistic\ntopic models, latent Dirichlet allocation and the hierarchical Dirichlet\nprocess topic model. Using stochastic variational inference, we analyze several\nlarge collections of documents: 300K articles from Nature, 1.8M articles from\nThe New York Times, and 3.8M articles from Wikipedia. Stochastic inference can\neasily handle data sets of this size and outperforms traditional variational\ninference, which can only handle a smaller subset. (We also show that the\nBayesian nonparametric topic model outperforms its parametric counterpart.)\nStochastic variational inference lets us apply complex Bayesian models to\nmassive data sets. \n\n"}
{"id": "1207.0105", "contents": "Title: Optimal inferential models for a Poisson mean Abstract: Statistical inference on the mean of a Poisson distribution is a\nfundamentally important problem with modern applications in, e.g., particle\nphysics. The discreteness of the Poisson distribution makes this problem\nsurprisingly challenging, even in the large-sample case. Here we propose a new\napproach, based on the recently developed framework of inferential models\n(IMs). Specifically, we construct optimal, or at least approximately optimal,\nIMs for two important classes of assertions/hypotheses about the Poisson mean.\nFor point assertions, we develop a novel recursive sorting algorithm to\nconstruct this optimal IM. Numerical comparisons of the proposed method to\nexisting methods are given, for both the mean and the more challenging\nmean-plus-background problem. \n\n"}
{"id": "1207.0258", "contents": "Title: Control of probability flow in Markov chain Monte Carlo --\n  Nonreversibility and lifting Abstract: The Markov chain Monte Carlo (MCMC) method is widely used in various fields\nas a powerful numerical integration technique for systems with many degrees of\nfreedom. In MCMC methods, probabilistic state transitions can be considered as\na random walk in state space, and random walks allow for sampling from complex\ndistributions. However, paradoxically, it is necessary to carefully suppress\nthe randomness of the random walk to improve computational efficiency. By\nbreaking detailed balance, we can create a probability flow in the state space\nand perform more efficient sampling along this flow. Motivated by this idea,\npractical and efficient nonreversible MCMC methods have been developed over the\npast ten years. In particular, the lifting technique, which introduces\nprobability flows in an extended state space, has been applied to various\nsystems and has proven more efficient than conventional reversible updates. We\nreview and discuss several practical approaches to implementing nonreversible\nMCMC methods, including the shift method in the cumulative distribution and the\ndirected-worm algorithm. \n\n"}
{"id": "1207.1708", "contents": "Title: Estimators for Archimedean copulas in high dimensions Abstract: The performance of known and new parametric estimators for Archimedean\ncopulas is investigated, with special focus on large dimensions and numerical\ndifficulties. In particular, method-of-moments-like estimators based on\npairwise Kendall's tau, a multivariate extension of Blomqvist's beta, minimum\ndistance estimators, the maximum-likelihood estimator, a simulated\nmaximum-likelihood estimator, and a maximum-likelihood estimator based on the\ncopula diagonal are studied. Their performance is compared in a large-scale\nsimulation study both under known and unknown margins (pseudo-observations), in\nsmall and high dimensions, under small and large dependencies, various\ndifferent Archimedean families and sample sizes. High dimensions up to one\nhundred are considered for the first time and computational problems arising\nfrom such large dimensions are addressed in detail. All methods are implemented\nin the open source \\R{} package \\pkg{copula} and can thus be easily accessed\nand studied. \n\n"}
{"id": "1207.3738", "contents": "Title: Ice structures, patterns, and processes: A view across the ice-fields Abstract: We look ahead from the frontiers of research on ice dynamics in its broadest\nsense; on the structures of ice, the patterns or morphologies it may assume,\nand the physical and chemical processes in which it is involved. We highlight\nopen questions in the various fields of ice research in nature; ranging from\nterrestrial and oceanic ice on Earth, to ice in the atmosphere, to ice on other\nsolar system bodies and in interstellar space. \n\n"}
{"id": "1207.5852", "contents": "Title: A framework for the evaluation of turbulence closures used in mesoscale\n  ocean large-eddy simulations Abstract: We present a methodology to determine the best turbulence closure for an\neddy-permitting ocean model through measurement of the error-landscape of the\nclosure's subgrid spectral transfers and flux. We apply this method to 6\ndifferent closures for forced-dissipative simulations of the barotropic\nvorticity equation on a f-plane (2D Navier-Stokes equation). Using a\nhigh-resolution benchmark, we compare each closure's model of energy and\nenstrophy transfer to the actual transfer observed in the benchmark run. The\nerror-landscape norms enable us to both make objective comparisons between the\nclosures and to optimize each closure's free parameter for a fair comparison.\nThe hyper-viscous closure most closely reproduces the enstrophy cascade,\nespecially at larger scales due to the concentration of its dissipative effects\nto the very smallest scales. The viscous and Leith closures perform nearly as\nwell, especially at smaller scales where all three models were dissipative. The\nSmagorinsky closure dissipates enstrophy at the wrong scales. The anticipated\npotential vorticity closure was the only model to reproduce the upscale\ntransfer of kinetic energy from the unresolved scales, but would require\nhigh-order Laplacian corrections in order to concentrate dissipation at the\nsmallest scales. The Lagrangian-averaged alpha-model closure did not perform\nsuccessfully for forced 2D isotropic Navier-Stokes: small-scale filamentation\nis only slightly reduced by the model while small-scale roll-up is prevented.\nTogether, this reduces the effects of diffusion. \n\n"}
{"id": "1208.0647", "contents": "Title: Identifying Lagrangian fronts with favourable fishery conditions Abstract: Lagrangian fronts (LF) in the ocean delineate boundaries between surface\nwaters with different Lagrangian properties. They can be accurately detected in\na given velocity field by computing synoptic maps of the drift of synthetic\ntracers and other Lagrangian indicators. Using Russian ship's catch and\nlocation data for a number of commercial fishery seasons in the region of the\nnorthwest Pacific with one of the richest fishery in the world, it is shown\nstatistically that the saury fishing grounds with maximal catches are not\nrandomly distributed over the region but located mainly along those LFs where\nproductive cold waters of the Oyashio Current, warmer waters of the southern\nbranch of the Soya Current, and waters of warm-core Kuroshio rings converge.\nComputation of those fronts with the altimetric geostrophic velocity fields\nboth in the years with the First and Second Oyashio Intrusions shows that in\nspite of different oceanographic conditions the LF locations may serve good\nindicators of potential fishing grounds. Possible reasons for saury aggregation\nnear LFs are discussed. We propose a mechanism of effective export of nutrient\nrich waters based on stretching of material lines in the vicinity of hyperbolic\nobjects in the ocean. The developed method, based on identifying LFs in any\nvelocity fields, is quite general and may be applied to forecast potential\nfishing grounds for the other pelagic fishes in different seas and the oceans. \n\n"}
{"id": "1208.1061", "contents": "Title: Data management and analysis with WRF and SFIRE Abstract: We introduce several useful utilities in development for the creation and\nanalysis of real wildland fire simulations using WRF and SFIRE. These utilities\nexist as standalone programs and scripts as well as extensions to other well\nknown software. Python web scrapers automate the process of downloading and\npreprocessing atmospheric and surface data from common sources. Other scripts\nsimplify the domain setup by creating parameter files automatically.\nIntegration with Google Earth allows users to explore the simulation in a 3D\nenvironment along with real surface imagery. Postprocessing scripts provide the\nuser with a number of output data formats compatible with many commonly used\nvisualization suites allowing for the creation of high quality 3D renderings.\nAs a whole, these improvements build toward a unified web application that\nbrings a sophisticated wildland fire modeling environment to scientists and\nusers alike. \n\n"}
{"id": "1208.1720", "contents": "Title: Mixing Coefficients Between Discrete and Real Random Variables:\n  Computation and Properties Abstract: In this paper we study the problem of estimating the alpha-, beta- and\nphi-mixing coefficients between two random variables, that can either assume\nvalues in a finite set or the set of real numbers. In either case, explicit\nclosed-form formulas for the beta-mixing coefficient are already known.\nTherefore for random variables assuming values in a finite set, our\ncontributions are two-fold: (i) In the case of the alpha-mixing coefficient, we\nshow that determining whether or not it exceeds a prespecified threshold is\nNP-complete, and provide efficiently computable upper and lower bounds. (ii) We\nderive an exact closed-form formula for the phi-mixing coefficient. Next, we\nprove analogs of the data-processing inequality from information theory for\neach of the three kinds of mixing coefficients. Then we move on to real-valued\nrandom variables, and show that by using percentile binning and allowing the\nnumber of bins to increase more slowly than the number of samples, we can\ngenerate empirical estimates that are consistent, i.e., converge to the true\nvalues as the number of samples approaches infinity. \n\n"}
{"id": "1208.1760", "contents": "Title: Indication of insensitivity of planetary weathering behavior and\n  habitable zone to surface land fraction Abstract: It is likely that unambiguous habitable zone terrestrial planets of unknown\nwater content will soon be discovered. Water content helps determine surface\nland fraction, which influences planetary weathering behavior. This is\nimportant because the silicate weathering feedback determines the width of the\nhabitable zone in space and time. Here a low-order model of weathering and\nclimate, useful for gaining qualitative understanding, is developed to examine\nclimate evolution for planets of various land-ocean fractions. It is pointed\nout that, if seafloor weathering does not depend directly on surface\ntemperature, there can be no weathering-climate feedback on a waterworld. This\nwould dramatically narrow the habitable zone of a waterworld. Results from our\nmodel indicate that weathering behavior does not depend strongly on land\nfraction for partially ocean-covered planets. This is powerful because it\nsuggests that previous habitable zone theory is robust to changes in land\nfraction, as long as there is some land. Finally, a mechanism is proposed for a\nwaterworld to prevent complete water loss during a moist greenhouse through\nrapid weathering of exposed continents. This process is named a \"waterworld\nself-arrest,\" and it implies that waterworlds can go through a moist greenhouse\nstage and end up as planets like Earth with partial ocean coverage. This work\nstresses the importance of surface and geologic effects, in addition to the\nusual incident stellar flux, for habitability. \n\n"}
{"id": "1208.2208", "contents": "Title: Pairing of charged particles in a quantum plasmoid Abstract: We study a quantum spherically symmetric object which is based on radial\nplasma oscillations. Such a plasmoid is supposed to exist in a dense plasma\ncontaining electrons, ions, and neutral particles. The method of creation and\nannihilation operators is applied to quantize the motion of charged particles\nin a self-consistent potential. We also study the effective interaction between\noscillating particles owing to the exchange of a virtual acoustic wave, which\nis excited in the neutral component of plasma. It is shown that this\ninteraction can be attractive and result in the formation of ion pairs. We\ndiscuss possible applications of this phenomenon in astrophysical and\nterrestrial plasmas. \n\n"}
{"id": "1208.2678", "contents": "Title: Nonequilibrium thermodynamics of circulation regimes in optically-thin,\n  dry atmospheres Abstract: An extensive analysis of an optically-thin, dry atmosphere at different\nvalues of the thermal Rossby number Ro and of the Taylor number Ff is per-\nformed with a general circulation model by varying the rotation rate {\\Omega}\nand the surface drag {\\tau} in a wide parametric range. By using nonequilibrium\nthermodynamics diagnostics such as material entropy production, efficiency,\nmeridional heat transport and kinetic energy dissipation we characterize in a\nnew way the different circulation regimes. Baroclinic circulations feature high\nmechanical dissipation, meridional heat transport, material entropy pro-\nduction and are fairly efficient in converting heat into mechanical work. The\nthermal dissipation associated with the sensible heat flux is found to depend\nmainly on the surface properties, almost independent from the rotation rate and\nvery low for quasi-barotropic circulations and regimes approaching equa- torial\nsuper-rotation. Slowly rotating, axisymmetric circulations have the highest\nmeridional heat transport. At high rotation rates and intermediate- high drag,\natmospheric circulations are zonostrohic with very low mechanical dissipation,\nmeridional heat transport and efficiency. When {\\tau} is interpreted as a\ntunable parameter associated with the turbulent boundary layer trans- fer of\nmomentum and sensible heat, our results confirm the possibility of using the\nMaximum Entropy Production Principle as a tuning guideline in the range of\nvalues of {\\Omega}. This study suggests the effectiveness of using fun-\ndamental nonequilibrium thermodynamics for investigating the properties of\nplanetary atmospheres and extends our knowledge of the thermodynamics of the\natmospheric circulation regimes. \n\n"}
{"id": "1208.3734", "contents": "Title: The key physical parameters governing frictional dissipation in a\n  precipitating atmosphere Abstract: Precipitation generates small-scale turbulent air flows the energy of which\nultimately dissipates to heat. The power of this process has previously been\nestimated to be around 2-4 W m-2 in the tropics: a value comparable in\nmagnitude to the dynamic power of the global circulation. Here we suggest that\nthis previous power estimate is approximately double the true figure. Our\nresult reflects a revised evaluation of the mean precipitation path length Hp.\nWe investigate the dependence of Hp on surface temperature,relative\nhumidity,temperature lapse rate and degree of condensation in the ascending\nair. We find that the degree of condensation,defined as the relative change of\nthe saturated water vapor mixing ratio in the region of condensation, is a\nmajor factor determining Hp. We estimate from theory that the mean large-scale\nrate of frictional dissipation associated with total precipitation in the\ntropics lies between 1 and 2 W m-2 and show that our estimate is supported by\nempirical evidence. We show that under terrestrial conditions frictional\ndissipation constitutes a minor fraction of the dynamic power of\ncondensation-induced atmospheric circulation,which is estimated to be at least\n2.5 times larger. However,because Hp increases with surface temperature Ts, the\nrate of frictional dissipation would exceed that of condensation-induced\ndynamics, and thus block major circulation, at Ts >~320 K in a moist adiabatic\natmosphere. \n\n"}
{"id": "1208.6275", "contents": "Title: Atmospheric aerosols at the Pierre Auger Observatory and environmental\n  implications Abstract: The Pierre Auger Observatory detects the highest energy cosmic rays.\nCalorimetric measurements of extensive air showers induced by cosmic rays are\nperformed with a fluorescence detector. Thus, one of the main challenges is the\natmospheric monitoring, especially for aerosols in suspension in the\natmosphere. Several methods are described which have been developed to measure\nthe aerosol optical depth profile and aerosol phase function, using lasers and\nother light sources as recorded by the fluorescence detector. The origin of\natmospheric aerosols traveling through the Auger site is also presented,\nhighlighting the effect of surrounding areas to atmospheric properties. In the\naim to extend the Pierre Auger Observatory to an atmospheric research platform,\na discussion about a collaborative project is presented. \n\n"}
{"id": "1209.1886", "contents": "Title: Evolution of a barotropic shear layer into elliptical vortices Abstract: When a barotropic shear layer becomes unstable, it produces the well known\nKelvin-Helmholtz instability (KH). The non-linear manifestation of KH is\nusually in the form of spiral billows. However, a piecewise linear shear layer\nproduces a different type of KH characterized by elliptical vortices of\nconstant vorticity connected via thin braids. Using direct numerical simulation\nand contour dynamics, we show that the interaction between two\ncounter-propagating vorticity waves is solely responsible for this KH\nformation. We investigate the oscillation of the vorticity wave amplitude, the\nrotation and nutation of the elliptical vortex, and straining of the braids.\nOur analysis also provides possible explanation behind the formation and\nevolution of elliptical vortices appearing in geophysical and astrophysical\nflows, e.g. meddies, Stratospheric polar vortices, Jovian vortices, Neptune's\nGreat Dark Spot and coherent vortices in the wind belts of Uranus. \n\n"}
{"id": "1209.1988", "contents": "Title: Computational information geometry: theory and practice Abstract: This paper lays the foundations for a unified framework for numerically and\ncomputationally applying methods drawn from a range of currently distinct\ngeometrical approaches to statistical modelling. In so doing, it extends\ninformation geometry from a manifold based approach to one where the simplex is\nthe fundamental geometrical object, thereby allowing applications to models\nwhich do not have a fixed dimension or support. Finally, it starts to build a\ncomputational framework which will act as a proxy for the 'space of all\ndistributions' that can be used, in particular, to investigate model selection\nand model uncertainty. A varied set of substantive running examples is used to\nillustrate theoretical and practical aspects of the discussion. Further\ndevelopments are briefly indicated. \n\n"}
{"id": "1209.3613", "contents": "Title: Robust seasonal cycle of Arctic sea ice area through tipping point in\n  amplitude Abstract: The variation in the Arctic sea ice is dominated by the seasonal cycle with\nlittle inter-annual correlation. Though the mean sea ice area has decreased\nsteadily in the period of satellite observations, a dramatic transition in the\ndynamics was initiated with the record low September ice area in 2007. The\nchange is much more pronounced in the amplitude of the seasonal cycle than in\nthe annual mean ice area. The shape of the seasonal cycle is surprisingly\nconstant for the whole observational record despite the general decline. A\nsimple explanation, independent of the increased greenhouse warming, for the\nshape of the seasonal cycle is offered. Thus the dramatic climate change in\narctic ice area is seen in the amplitude of the cycle and to a lesser extend\nthe annual mean and the summer ice extend. The reason why the climate change is\nmost pronounced in the amplitude is related to the rapid reduction in perennial\nice and thus a thinning of the ice. The analysis shows that a tipping point for\nthe arctic ice area was crossed in 2007. \n\n"}
{"id": "1209.3862", "contents": "Title: Direct Statistical Simulation of Out-of-Equilibrium Jets Abstract: We present a Direct Statistical Simulation (DSS) of jet formation on a\n\\beta-plane, solving for the statistics of a fluid flow via an expansion in\ncumulants. Here we compare an expansion truncated at second order (CE2) to\nstatistics accumulated by direct numerical simulations (DNS). We show that, for\njets near equilibrium, CE2 is capable of reproducing the jet structure\n(although some differences remain in the second cumulant). However as the\ndegree of departure from equilibrium is increased (as measured by the\nzonostrophy parameter) the jets meander more and CE2 becomes less accurate. We\ndiscuss a possible remedy by inclusion of higher cumulants. \n\n"}
{"id": "1209.4279", "contents": "Title: Conservative parameterization schemes Abstract: Parameterization (closure) schemes in numerical weather and climate\nprediction models account for the effects of physical processes that cannot be\nresolved explicitly by these models. Methods for finding physical\nparameterization schemes that preserve conservation laws of systems of\ndifferential equations are introduced. These methods rest on the possibility to\nregard the problem of finding conservative parameterization schemes as a\nconservation law classification problem for classes of differential equations.\nThe relevant classification problems can be solved using the direct or inverse\nclassification procedures. In the direct approach, one starts with a general\nfunctional form of the parameterization scheme. Specific forms are then found\nso that corresponding closed equations admit conservation laws. In the inverse\napproach, one seeks parameterization schemes that preserve one or more\npre-selected conservation laws of the initial model. The physical\ninterpretation of both classification approaches is discussed. Special\nattention is paid to the problem of finding parameterization schemes that\npreserve both conservation laws and symmetries. All methods are illustrated by\nfinding conservative and conservative invariant parameterization schemes for\nsystems of one-dimensional shallow-water equations. \n\n"}
{"id": "1210.0220", "contents": "Title: Twisted particle filters Abstract: We investigate sampling laws for particle algorithms and the influence of\nthese laws on the efficiency of particle approximations of marginal likelihoods\nin hidden Markov models. Among a broad class of candidates we characterize the\nessentially unique family of particle system transition kernels which is\noptimal with respect to an asymptotic-in-time variance growth rate criterion.\nThe sampling structure of the algorithm defined by these optimal transitions\nturns out to be only subtly different from standard algorithms and yet the\nfluctuation properties of the estimates it provides can be dramatically\ndifferent. The structure of the optimal transition suggests a new class of\nalgorithms, which we term \"twisted\" particle filters and which we validate with\nasymptotic analysis of a more traditional nature, in the regime where the\nnumber of particles tends to infinity. \n\n"}
{"id": "1210.7726", "contents": "Title: Performance Analysis of Parameter Estimation Using LASSO Abstract: The Least Absolute Shrinkage and Selection Operator (LASSO) has gained\nattention in a wide class of continuous parametric estimation problems with\npromising results. It has been a subject of research for more than a decade.\nDue to the nature of LASSO, the previous analyses have been non-parametric.\nThis ignores useful information and makes it difficult to compare LASSO to\ntraditional estimators. In particular, the role of the regularization parameter\nand super-resolution properties of LASSO have not been well-understood yet. The\nobjective of this work is to provide a new insight into this context by\nintroducing LASSO as a parametric technique of a varying order. This provides\nus theoretical expressions for the LASSO-based estimation error and false alarm\nrate in the asymptotic case of high SNR and dense grids. For this case, LASSO\nis compared to maximum likelihood and conventional beamforming. It is found\nthat LASSO loses performance due to the regularization term, but the amount of\nloss is practically negligible with a proper choice of the regularization\nparameter. Thus, we provide suggestions on the selection of the regularization\nparameter. Without loss of generality, we present the comparative numerical\nresults in the context of Direction of Arrival (DOA) estimation using a sensor\narray. \n\n"}
{"id": "1211.0805", "contents": "Title: Advances in Search and Rescue at Sea Abstract: A topical collection on \"Advances in Search and Rescue at Sea\" has appeared\nin recent issues of Ocean Dynamics following the latest in a series of\nworkshops on \"Technologies for Search and Rescue and other Emergency Marine\nOperations\" (2004, 2006, 2008 and 2011), hosted by IFREMER in Brest, France.\n  Here we give a brief overview of the history of search and rescue at sea\nbefore we summarize the main results of the papers that have appeared in the\ntopical collection.\n  Keywords: Search and rescue (SAR), Trajectory modelling, Stochastic\nLagrangian ocean models, Lagrangian measurement methods, ocean surface\ncurrents. \n\n"}
{"id": "1211.1171", "contents": "Title: Generalized Linear Gaussian Cluster-Weighted Modeling Abstract: Cluster-Weighted Modeling (CWM) is a flexible mixture approach for modeling\nthe joint probability of data coming from a heterogeneous population as a\nweighted sum of the products of marginal distributions and conditional\ndistributions. In this paper, we introduce a wide family of Cluster Weighted\nmodels in which the conditional distributions are assumed to belong to the\nexponential family with canonical links which will be referred to as\nGeneralized Linear Gaussian Cluster Weighted Models. Moreover, we show that, in\na suitable sense, mixtures of generalized linear models can be considered as\nnested in Generalized Linear Gaussian Cluster Weighted Models. The proposal is\nillustrated through many numerical studies based on both simulated and real\ndata sets. \n\n"}
{"id": "1211.3210", "contents": "Title: Fast estimation of the ICL criterion for change-point detection problems\n  with applications to Next-Generation Sequencing data Abstract: In this paper, we consider the Integrated Completed Likelihood (ICL) as a\nuseful criterion for estimating the number of changes in the underlying\ndistribution of data in problems where detecting the precise location of these\nchanges is the main goal. The exact computation of the ICL requires O(Kn2)\noperations (with K the number of segments and n the number of data-points)\nwhich is prohibitive in many practical situations with large sequences of data.\nWe describe a framework to estimate the ICL with O(Kn) complexity. Our approach\nis general in the sense that it can accommodate any given model distribution.\nWe checked the run-time and validity of our approach on simulated data and\ndemonstrate its good performance when analyzing real Next-Generation Sequencing\n(NGS) data using a negative binomial model. \n\n"}
{"id": "1211.3825", "contents": "Title: Generation of two-dimensional water waves by moving bottom disturbances Abstract: We investigate the potential and limitations of the wave generation by\ndisturbances moving at the bottom. More precisely, we assume that the wavemaker\nis composed of an underwater object of a given shape which can be displaced\naccording to a prescribed trajectory. We address the practical question of\ncomputing the wavemaker shape and trajectory generating a wave with prescribed\ncharacteristics. For the sake of simplicity we model the hydrodynamics by a\ngeneralized forced Benjamin-Bona-Mahony (BBM) equation. This practical problem\nis reformulated as a constrained nonlinear optimization problem. Additional\nconstraints are imposed in order to fulfill various practical design\nrequirements. Finally, we present some numerical results in order to\ndemonstrate the feasibility and performance of the proposed methodology. \n\n"}
{"id": "1211.6582", "contents": "Title: Emergence of large scale structure in planetary turbulence Abstract: Planetary and magnetohydrodynamic drift-wave turbulence is observed to\nself-organize into large scale structures such as zonal jets and coherent\nvortices. In this Letter we present a non-equilibrium statistical theory, the\nStochastic Structural Stability theory (SSST), that can make predictions for\nthe formation and finite amplitude equilibration of non-zonal and zonal\nstructures (lattice and stripe patterns) in homogeneous turbulence. This theory\nreveals that the emergence of large scale structure is the result of an\ninstability of the interaction between the coherent flow and the associated\nturbulent field. Comparison of the theory with nonlinear simulations of a\nbarotropic flow in a beta-plane channel with turbulence sustained by isotropic\nrandom stirring, demonstrates that SSST predicts the threshold parameters at\nwhich the coherent structures emerge as well as the characteristics of the\nemerging structures (scale, amplitude, phase speed). It is shown that non-zonal\nstructures (lattice states or zonons) emerge at lower energy input rates of the\nstirring compared to zonal flows (stripe states) and their emergence affects\nthe dynamics of jet formation. \n\n"}
{"id": "1212.1778", "contents": "Title: Hidden Markov Model Applications in Change-Point Analysis Abstract: The detection of change-points in heterogeneous sequences is a statistical\nchallenge with many applications in fields such as finance, signal analysis and\nbiology. A wide variety of literature exists for finding an ideal set of\nchange-points for characterizing the data. In this tutorial we elaborate on the\nHidden Markov Model (HMM) and present two different frameworks for applying HMM\nto change-point models. Then we provide a summary of two procedures for\ninference in change-point analysis, which are particular cases of the\nforward-backward algorithm for HMMs, and discuss common implementation\nproblems. Lastly, we provide two examples of the HMM methods on available data\nsets and we shortly discuss about the applications to current genomics studies.\nThe R code used in the examples is provided in the appendix. \n\n"}
{"id": "1212.3721", "contents": "Title: Approximate continuous-discrete filters for the estimation of diffusion\n  processes from partial and noisy observations Abstract: In this paper, an alternative approximation to the innovation method is\nintroduced for the parameter estimation of diffusion processes from partial and\nnoisy observations. This is based on a convergent approximation to the first\ntwo conditional moments of the innovation process through approximate\ncontinuous-discrete filters of minimum variance. It is shown that, for finite\nsamples, the resulting approximate estimators converge to the exact one when\nthe error of the approximate filters decreases. For an increasing number of\nobservations, the estimators are asymptotically normal distributed and their\nbias decreases when the above mentioned error does it. A simulation study is\nprovided to illustrate the performance of the new estimators. The results show\nthat, with respect to the conventional approximate estimators, the new ones\nsignificantly enhance the parameter estimation of the test equations. The\nproposed estimators are intended for the recurrent practical situation where a\nnonlinear stochastic system should be identified from a reduced number of\npartial and noisy observations distant in time. \n\n"}
{"id": "1212.5336", "contents": "Title: Percolation transition in the kinematics of nonlinear resonance\n  broadening in Charney-Hasegawa-Mima model of Rossby wave turbulence Abstract: We study the kinematics of nonlinear resonance broadening of interacting\nRossby waves as modelled by the Charney-Hasegawa-Mima equation on a biperiodic\ndomain. We focus on the set of wave modes which can interact quasi-resonantly\nat a particular level of resonance broadening and aim to characterise how the\nstructure of this set changes as the level of resonance broadening is varied.\nThe commonly held view that resonance broadening can be thought of as a\nthickening of the resonant manifold is misleading. We show that in fact the set\nof modes corresponding to a single quasi-resonant triad has a nontrivial\nstructure and that its area in fact diverges for a finite degree of broadening.\nWe also study the connectivity of the network of modes which is generated when\nquasi-resonant triads share common modes. This network has been argued to form\nthe backbone for energy transfer in Rossby wave turbulence. We show that this\nnetwork undergoes a percolation transition when the level of resonance\nbroadening exceeds a critical value. Below this critical value, the largest\nconnected component of the quasi-resonant network contains a negligible\nfraction of the total number of modes in the system whereas above this critical\nvalue a finite fraction of the total number of modes in the system are\ncontained in the largest connected component. We argue that this percolation\ntransition should correspond to the transition to turbulence in the system. \n\n"}
{"id": "1212.5783", "contents": "Title: Environmental Superstatistics Abstract: A thermodynamic device placed outdoors, or a local ecosystem, is subject to a\nvariety of different temperatures given by short-tem (daily) and long-term\n(seasonal) variations. In the long term a superstatistical description makes\nsense, with a suitable distribution function f(beta) of inverse temperature\nbeta over which ordinary statistical mechanics is averaged. We show that\nf(beta) is very different at different geographic locations, and typically\nexhibits a double-peak structure for long-term data. For some of our data sets\nwe also find a systematic drift due to global warming. For a simple\nsuperstatistical model system we show that the response to global warming is\nstronger if temperature fluctuations are taken into account. \n\n"}
{"id": "1212.5823", "contents": "Title: Symmetry analysis of a system of modified shallow-water equations Abstract: We revise the symmetry analysis of a modified system of one-dimensional\nshallow-water equations (MSWE) recently considered by Raja Sekhar and Sharma\n[Commun. Nonlinear Sci. Numer. Simulat. 20 (2012) 630-636]. Only a finite\ndimensional subalgebra of the maximal Lie invariance algebra of the MSWE, which\nin fact is infinite dimensional, was found in the aforementioned paper. The\nMSWE can be linearized using a hodograph transformation. An optimal list of\ninequivalent one-dimensional subalgebras of the maximal Lie invariance algebra\nis constructed and used for Lie reductions. Non-Lie solutions are found from\nsolutions of the linearized MSWE. \n\n"}
{"id": "1301.4168", "contents": "Title: Herded Gibbs Sampling Abstract: The Gibbs sampler is one of the most popular algorithms for inference in\nstatistical models. In this paper, we introduce a herding variant of this\nalgorithm, called herded Gibbs, that is entirely deterministic. We prove that\nherded Gibbs has an $O(1/T)$ convergence rate for models with independent\nvariables and for fully connected probabilistic graphical models. Herded Gibbs\nis shown to outperform Gibbs in the tasks of image denoising with MRFs and\nnamed entity recognition with CRFs. However, the convergence for herded Gibbs\nfor sparsely connected probabilistic graphical models is still an open problem. \n\n"}
{"id": "1301.5035", "contents": "Title: Computing Robust Leverage Diagnostics when the Design Matrix Contains\n  Coded Categorical Variables Abstract: For a robust leverage diagnostic in linear regression, Rousseeuw and van\nZomeren [1990] proposed using robust distance (Mahalanobis distance computed\nusing robust estimates of location and covariance). However, a design matrix X\nthat contains coded categorical predictor variables is often sufficiently\nsparse that robust estimates of location and covariance cannot be computed.\nSpecifically, matrices formed by taking subsets of the rows of X are likely to\nbe singular, causing algorithms that rely on subsampling to fail. Following the\nspirit of Maronna and Yohai [2000], we observe that extreme leverage points are\nextreme in the continuous predictor variables. We therefore propose a robust\nleverage diagnostic that combines a robust analysis of the continuous predictor\nvariables and the classical definition of leverage. \n\n"}
{"id": "1301.6365", "contents": "Title: Fixed effects Selection in high dimensional Linear Mixed Models Abstract: We consider linear mixed models in which the observations are grouped. A\nL1-penalization on the fixed effects coefficients of the log-likelihood\nobtained by considering the random effects as missing values is proposed. A\nmulticycle ECM algorithm is used to solve the optimization problem; it can be\ncombined with any variable selection method developed for linear models. The\nalgorithm allows the number of parameters p to be larger than the total number\nof observations n; it is faster than the lmmLasso (Schelldorfer,2011) since no\nn*n matrix has to be inverted. We show that the theoretical results of\nSchelldorfer (2011) apply for our method when the variances of both the random\neffects and the residuals are known. The combination of the algorithm with a\nvariable selection method (Rohart 2011) shows good results in estimating the\nset of relevant fixed effects coefficients as well as estimating the variances;\nit outperforms the lmmLasso both in the common case (p< n) and in the\nhigh-dimensional case (p > n). \n\n"}
{"id": "1302.0893", "contents": "Title: Probabilistic Quantitative Precipitation Forecasting Using Ensemble\n  Model Output Statistics Abstract: Statistical post-processing of dynamical forecast ensembles is an essential\ncomponent of weather forecasting. In this article, we present a post-processing\nmethod that generates full predictive probability distributions for\nprecipitation accumulations based on ensemble model output statistics (EMOS).\nWe model precipitation amounts by a generalized extreme value distribution that\nis left-censored at zero. This distribution permits modelling precipitation on\nthe original scale without prior transformation of the data. A closed form\nexpression for its continuous rank probability score can be derived and permits\ncomputationally efficient model fitting. We discuss an extension of our\napproach that incorporates further statistics characterizing the spatial\nvariability of precipitation amounts in the vicinity of the location of\ninterest. The proposed EMOS method is applied to daily 18-h forecasts of 6-h\naccumulated precipitation over Germany in 2011 using the COSMO-DE ensemble\nprediction system operated by the German Meteorological Service. It yields\ncalibrated and sharp predictive distributions and compares favourably with\nextended logistic regression and Bayesian model averaging which are state of\nthe art approaches for precipitation post-processing. The incorporation of\nneighbourhood information further improves predictive performance and turns out\nto be a useful strategy to account for displacement errors of the dynamical\nforecasts in a probabilistic forecasting framework. \n\n"}
{"id": "1302.5475", "contents": "Title: Estimation of oblique structure via penalized likelihood factor analysis Abstract: We consider the problem of sparse estimation via a lasso-type penalized\nlikelihood procedure in a factor analysis model. Typically, the model\nestimation is done under the assumption that the common factors are orthogonal\n(uncorrelated). However, the lasso-type penalization method based on the\northogonal model can often estimate a completely different model from that with\nthe true factor structure when the common factors are correlated. In order to\novercome this problem, we propose to incorporate a factor correlation into the\nmodel, and estimate the factor correlation along with parameters included in\nthe orthogonal model by maximum penalized likelihood procedure. An entire\nsolution path is computed by the EM algorithm with coordinate descent, which\npermits the application to a wide variety of convex and nonconvex penalties.\nThe proposed method can provide sufficiently sparse solutions, and be applied\nto the data where the number of variables is larger than the number of\nobservations. Monte Carlo simulations are conducted to investigate the\neffectiveness of our modeling strategies. The results show that the lasso-type\npenalization based on the orthogonal model cannot often approximate the true\nfactor structure, whereas our approach performs well in various situations. The\nusefulness of the proposed procedure is also illustrated through the analysis\nof real data. \n\n"}
{"id": "1302.5624", "contents": "Title: Semi-automatic selection of summary statistics for ABC model choice Abstract: A central statistical goal is to choose between alternative explanatory\nmodels of data. In many modern applications, such as population genetics, it is\nnot possible to apply standard methods based on evaluating the likelihood\nfunctions of the models, as these are numerically intractable. Approximate\nBayesian computation (ABC) is a commonly used alternative for such situations.\nABC simulates data x for many parameter values under each model, which is\ncompared to the observed data xobs. More weight is placed on models under which\nS(x) is close to S(xobs), where S maps data to a vector of summary statistics.\nPrevious work has shown the choice of S is crucial to the efficiency and\naccuracy of ABC. This paper provides a method to select good summary statistics\nfor model choice. It uses a preliminary step, simulating many x values from all\nmodels and fitting regressions to this with the model as response. The\nresulting model weight estimators are used as S in an ABC analysis. Theoretical\nresults are given to justify this as approximating low dimensional sufficient\nstatistics. A substantive application is presented: choosing between competing\ncoalescent models of demographic growth for Campylobacter jejuni in New Zealand\nusing multi-locus sequence typing data. \n\n"}
{"id": "1303.6435", "contents": "Title: A theory for the emergence of coherent structures in beta-plane\n  turbulence Abstract: Planetary turbulent flows are observed to self-organize into large scale\nstructures such as zonal jets and coherent vortices. One of the simplest models\nof planetary turbulence is obtained by considering a barotropic flow on a\nbeta-plane channel with turbulence sustained by random stirring. Non-linear\nintegrations of this model show that as the energy input rate of the forcing is\nincreased, the homogeneity of the flow is broken with the emergence of\nnon-zonal, coherent, westward propagating structures and at larger energy input\nrates by the emergence of zonal jets. We study the emergence of non-zonal\ncoherent structures using a non-equilibrium statistical theory, Stochastic\nStructural Stability Theory (S3T, previously referred to as SSST). S3T directly\nmodels a second order approximation to the statistical mean turbulent state and\nallows identification of statistical turbulent equilibria and study of their\nstability. Using S3T, the bifurcation properties of the homogeneous state in\nbarotropic beta-plane turbulence are determined. Analytic expressions for the\nzonal and non-zonal large scale coherent flows that emerge as a result of\nstructural instability are obtained. Through numerical integrations of the S3T\ndynamical system, it is found that the unstable structures equilibrate at\nfinite amplitude. Numerical simulations of the nonlinear equations confirm the\ncharacteristics (scale, amplitude and phase speed) of the structures predicted\nby S3T. \n\n"}
{"id": "1303.7191", "contents": "Title: Understanding the Effect of Atmospheric Density on the Cosmic Ray Flux\n  Variations at the Earth Surface Abstract: We report in this letter for the first time the numerical simulations of muon\nand neutron flux variations at the surface of the earth with varying air\ndensities in the troposphere and stratosphere. The simulated neutron and muon\nflux variations are in very good agreement with the measured neutron flux\nvariation in Oulu and the muon flux variation in Atlanta. We conclude from this\nstudy that the stratosphere air density variation dominates the effects on the\nmuon flux changes while the density variation in troposphere mainly influences\nthe neutron count variation. These results pave a new path for systematically\nstudying the global temperature evolution using worldwide cosmic ray data. \n\n"}
{"id": "1303.7318", "contents": "Title: Approximate Inference for Observation Driven Time Series Models with\n  Intractable Likelihoods Abstract: In the following article we consider approximate Bayesian parameter inference\nfor observation driven time series models. Such statistical models appear in a\nwide variety of applications, including econometrics and applied mathematics.\nThis article considers the scenario where the likelihood function cannot be\nevaluated point-wise; in such cases, one cannot perform exact statistical\ninference, including parameter estimation, which often requires advanced\ncomputational algorithms, such as Markov chain Monte Carlo (MCMC). We introduce\na new approximation based upon approximate Bayesian computation (ABC). Under\nsome conditions, we show that as $n\\rightarrow\\infty$, with $n$ the length of\nthe time series, the ABC posterior has, almost surely, a maximum \\emph{a\nposteriori} (MAP) estimator of the parameters which is different from the true\nparameter. However, a noisy ABC MAP, which perturbs the original data,\nasymptotically converges to the true parameter, almost surely. In order to draw\nstatistical inference, for the ABC approximation adopted, standard MCMC\nalgorithms can have acceptance probabilities that fall at an exponential rate\nin $n$ and slightly more advanced algorithms can mix poorly. We develop a new\nand improved MCMC kernel, which is based upon an exact approximation of a\nmarginal algorithm, whose cost per-iteration is random but the expected cost,\nfor good performance, is shown to be $\\mathcal{O}(n^2)$ per-iteration. \n\n"}
{"id": "1304.0462", "contents": "Title: Gravitational wave parameter estimation with compressed likelihood\n  evaluations Abstract: One of the main bottlenecks in gravitational wave (GW) astronomy is the high\ncost of performing parameter estimation and GW searches on the fly. We propose\na novel technique based on Reduced Order Quadratures (ROQs), an application and\ndata-specific quadrature rule, to perform fast and accurate likelihood\nevaluations. These are the dominant cost in Markov chain Monte Carlo (MCMC)\nalgorithms, which are widely employed in parameter estimation studies, and so\nROQs offer a new way to accelerate GW parameter estimation. We illustrate our\napproach using a four dimensional GW burst model embedded in noise. We build an\nROQ for this model, and perform four dimensional MCMC searches with both the\nstandard and ROQs quadrature rules, showing that, for this model, the ROQ\napproach is around 25 times faster than the standard approach with essentially\nno loss of accuracy. The speed-up from using ROQs is expected to increase for\nmore complex GW signal models and therefore has significant potential to\naccelerate parameter estimation of GW sources such as compact binary\ncoalescences. \n\n"}
{"id": "1304.0503", "contents": "Title: Nonparametric likelihood based estimation of linear filters for point\n  processes Abstract: We consider models for multivariate point processes where the intensity is\ngiven nonparametrically in terms of functions in a reproducing kernel Hilbert\nspace. The likelihood function involves a time integral and is consequently not\ngiven in terms of a finite number of kernel evaluations. The main result is a\nrepresentation of the gradient of the log-likelihood, which we use to derive\ncomputable approximations of the log-likelihood and the gradient by time\ndiscretization. These approximations are then used to minimize the approximate\npenalized log-likelihood. For time and memory efficiency the implementation\nrelies crucially on the use of sparse matrices. As an illustration we consider\nneuron network modeling, and we use this example to investigate how the\ncomputational costs of the approximations depend on the resolution of the time\ndiscretization. The implementation is available in the R package ppstat. \n\n"}
{"id": "1304.1354", "contents": "Title: Wave Extremes in the North East Atlantic from Ensemble Forecasts Abstract: A method for estimating return values from ensembles of forecasts at advanced\nlead times is presented. Return values of significant wave height in the\nNorth-East Atlantic, the Norwegian Sea and the North Sea are computed from\narchived +240-h forecasts of the ECMWF ensemble prediction system (EPS) from\n1999 to 2009.\n  We make three assumptions: First, each forecast is representative of a\nsix-hour interval and collectively the data set is then comparable to a time\nperiod of 226 years. Second, the model climate matches the observed\ndistribution, which we confirm by comparing with buoy data. Third, the ensemble\nmembers are sufficiently uncorrelated to be considered independent realizations\nof the model climate. We find anomaly correlations of 0.20, but peak events\n(>P97) are entirely uncorrelated. By comparing return values from individual\nmembers with return values of subsamples of the data set we also find that the\nestimates follow the same distribution and appear unaffected by correlations in\nthe ensemble. The annual mean and variance over the 11-year archived period\nexhibit no significant departures from stationarity compared with a recent\nreforecast, i.e., there is no spurious trend due to model upgrades.\n  EPS yields significantly higher return values than ERA-40 and ERA-Interim and\nis in good agreement with the high-resolution hindcast NORA10, except in the\nlee of unresolved islands where EPS overestimates and in enclosed seas where it\nis biased low. Confidence intervals are half the width of those found for\nERA-Interim due to the magnitude of the data set. \n\n"}
{"id": "1304.2129", "contents": "Title: A gentle introduction to the discrete Laplace method for estimating\n  Y-STR haplotype frequencies Abstract: Y-STR data simulated under a Fisher-Wright model of evolution with a\nsingle-step mutation model turns out to be well predicted by a method using\ndiscrete Laplace distributions. \n\n"}
{"id": "1304.4406", "contents": "Title: Cities as nuclei of sustainability? Abstract: We have assembled CO2 emission figures from collections of urban GHG emission\nestimates published in peer reviewed journals or reports from research\ninstitutes and non-governmental organizations. Analyzing the scaling with\npopulation size we find that the exponent is development dependent with a\ntransition from super- to sub-linear scaling. From the climate change\nmitigation point of view, the results suggest that urbanization is desirable in\ndeveloped countries and should be avoided in developing ones. Further, we\ncompare this analysis with a second scaling relation, namely the fundamental\nallometry between city population and area, and propose that density might be\nthe decisive quantity. Last, we derive the theoretical country-wide urban\nemissions by integration and obtain a dependence on the size of the largest\ncity. \n\n"}
{"id": "1304.4564", "contents": "Title: A high-dimensional two-sample test for the mean using random subspaces Abstract: A common problem in genetics is that of testing whether a set of highly\ndependent gene expressions differ between two populations, typically in a\nhigh-dimensional setting where the data dimension is larger than the sample\nsize. Most high-dimensional tests for the equality of two mean vectors rely on\nnaive diagonal or trace estimators of the covariance matrix, ignoring\ndependencies between variables. A test recently proposed by Lopes et al. (2012)\nimplicitly incorporates dependencies by using random pseudo-projections to a\nlower-dimensional space. Their test offers higher power when the variables are\ndependent, but lacks desirable invariance properties and relies on asymptotic\np-values that are too conservative. We illustrate how a permutation approach\ncan be used to obtain p-values for the Lopes et al. test and how modifying the\ntest using random subspaces leads to a test statistic that is invariant under\nlinear transformations of the marginal distributions. The resulting test does\nnot rely on assumptions about normality or the structure of the covariance\nmatrix. We show by simulation that the new test has higher power than competing\ntests in realistic settings motivated by microarray gene expression data. We\nalso discuss the computational aspects of high-dimensional permutation tests\nand provide an efficient R implementation of the proposed test. \n\n"}
{"id": "1305.0759", "contents": "Title: GPfit: An R package for Gaussian Process Model Fitting using a New\n  Optimization Algorithm Abstract: Gaussian process (GP) models are commonly used statistical metamodels for\nemulating expensive computer simulators. Fitting a GP model can be numerically\nunstable if any pair of design points in the input space are close together.\nRanjan, Haynes, and Karsten (2011) proposed a computationally stable approach\nfor fitting GP models to deterministic computer simulators. They used a genetic\nalgorithm based approach that is robust but computationally intensive for\nmaximizing the likelihood. This paper implements a slightly modified version of\nthe model proposed by Ranjan et al. (2011), as the new R package GPfit. A novel\nparameterization of the spatial correlation function and a new multi-start\ngradient based optimization algorithm yield optimization that is robust and\ntypically faster than the genetic algorithm based approach. We present two\nexamples with R codes to illustrate the usage of the main functions in GPfit.\nSeveral test functions are used for performance comparison with a popular R\npackage mlegp. GPfit is a free software and distributed under the general\npublic license, as part of the R software project (R Development Core Team\n2012). \n\n"}
{"id": "1305.5879", "contents": "Title: Statistical Significance of Clustering using Soft Thresholding Abstract: Clustering methods have led to a number of important discoveries in\nbioinformatics and beyond. A major challenge in their use is determining which\nclusters represent important underlying structure, as opposed to spurious\nsampling artifacts. This challenge is especially serious, and very few methods\nare available, when the data are very high in dimension. Statistical\nSignificance of Clustering (SigClust) is a recently developed cluster\nevaluation tool for high dimensional low sample size data. An important\ncomponent of the SigClust approach is the very definition of a single cluster\nas a subset of data sampled from a multivariate Gaussian distribution. The\nimplementation of SigClust requires the estimation of the eigenvalues of the\ncovariance matrix for the null multivariate Gaussian distribution. We show that\nthe original eigenvalue estimation can lead to a test that suffers from severe\ninflation of type-I error, in the important case where there are a few very\nlarge eigenvalues. This paper addresses this critical challenge using a novel\nlikelihood based soft thresholding approach to estimate these eigenvalues,\nwhich leads to a much improved SigClust. Major improvements in SigClust\nperformance are shown by both mathematical analysis, based on the new notion of\nTheoretical Cluster Index, and extensive simulation studies. Applications to\nsome cancer genomic data further demonstrate the usefulness of these\nimprovements. \n\n"}
{"id": "1306.2144", "contents": "Title: Importance Nested Sampling and the MultiNest Algorithm Abstract: Bayesian inference involves two main computational challenges. First, in\nestimating the parameters of some model for the data, the posterior\ndistribution may well be highly multi-modal: a regime in which the convergence\nto stationarity of traditional Markov Chain Monte Carlo (MCMC) techniques\nbecomes incredibly slow. Second, in selecting between a set of competing models\nthe necessary estimation of the Bayesian evidence for each is, by definition, a\n(possibly high-dimensional) integration over the entire parameter space; again\nthis can be a daunting computational task, although new Monte Carlo (MC)\nintegration algorithms offer solutions of ever increasing efficiency. Nested\nsampling (NS) is one such contemporary MC strategy targeted at calculation of\nthe Bayesian evidence, but which also enables posterior inference as a\nby-product, thereby allowing simultaneous parameter estimation and model\nselection. The widely-used MultiNest algorithm presents a particularly\nefficient implementation of the NS technique for multi-modal posteriors. In\nthis paper we discuss importance nested sampling (INS), an alternative\nsummation of the MultiNest draws, which can calculate the Bayesian evidence at\nup to an order of magnitude higher accuracy than `vanilla' NS with no change in\nthe way MultiNest explores the parameter space. This is accomplished by\ntreating as a (pseudo-)importance sample the totality of points collected by\nMultiNest, including those previously discarded under the constrained\nlikelihood sampling of the NS algorithm. We apply this technique to several\nchallenging test problems and compare the accuracy of Bayesian evidences\nobtained with INS against those from vanilla NS. \n\n"}
{"id": "1306.2685", "contents": "Title: Flexible sampling of discrete data correlations without the marginal\n  distributions Abstract: Learning the joint dependence of discrete variables is a fundamental problem\nin machine learning, with many applications including prediction, clustering\nand dimensionality reduction. More recently, the framework of copula modeling\nhas gained popularity due to its modular parametrization of joint\ndistributions. Among other properties, copulas provide a recipe for combining\nflexible models for univariate marginal distributions with parametric families\nsuitable for potentially high dimensional dependence structures. More\nradically, the extended rank likelihood approach of Hoff (2007) bypasses\nlearning marginal models completely when such information is ancillary to the\nlearning task at hand as in, e.g., standard dimensionality reduction problems\nor copula parameter estimation. The main idea is to represent data by their\nobservable rank statistics, ignoring any other information from the marginals.\nInference is typically done in a Bayesian framework with Gaussian copulas, and\nit is complicated by the fact this implies sampling within a space where the\nnumber of constraints increases quadratically with the number of data points.\nThe result is slow mixing when using off-the-shelf Gibbs sampling. We present\nan efficient algorithm based on recent advances on constrained Hamiltonian\nMarkov chain Monte Carlo that is simple to implement and does not require\npaying for a quadratic cost in sample size. \n\n"}
{"id": "1306.3277", "contents": "Title: Bayesian State-Space Modelling on High-Performance Hardware Using LibBi Abstract: LibBi is a software package for state-space modelling and Bayesian inference\non modern computer hardware, including multi-core central processing units\n(CPUs), many-core graphics processing units (GPUs) and distributed-memory\nclusters of such devices. The software parses a domain-specific language for\nmodel specification, then optimises, generates, compiles and runs code for the\ngiven model, inference method and hardware platform. In presenting the\nsoftware, this work serves as an introduction to state-space models and the\nspecialised methods developed for Bayesian inference with them. The focus is on\nsequential Monte Carlo (SMC) methods such as the particle filter for state\nestimation, and the particle Markov chain Monte Carlo (PMCMC) and SMC^2 methods\nfor parameter estimation. All are well-suited to current computer hardware. Two\nexamples are given and developed throughout, one a linear three-element\nwindkessel model of the human arterial system, the other a nonlinear Lorenz '96\nmodel. These are specified in the prescribed modelling language, and LibBi\ndemonstrated by performing inference with them. Empirical results are\npresented, including a performance comparison of the software with different\nhardware configurations. \n\n"}
{"id": "1306.3494", "contents": "Title: Randomized maximum-contrast selection: subagging for large-scale\n  regression Abstract: We introduce a very general method for sparse and large-scale variable\nselection. The large-scale regression settings is such that both the number of\nparameters and the number of samples are extremely large. The proposed method\nis based on careful combination of penalized estimators, each applied to a\nrandom projection of the sample space into a low-dimensional space. In one\nspecial case that we study in detail, the random projections are divided into\nnon-overlapping blocks; each consisting of only a small portion of the original\ndata. Within each block we select the projection yielding the smallest\nout-of-sample error. Our random ensemble estimator then aggregates the results\naccording to new maximal-contrast voting scheme to determine the final selected\nset. Our theoretical results illuminate the effect on performance of increasing\nthe number of non-overlapping blocks. Moreover, we demonstrate that statistical\noptimality is retained along with the computational speedup. The proposed\nmethod achieves minimax rates for approximate recovery over all estimators\nusing the full set of samples. Furthermore, our theoretical results allow the\nnumber of subsamples to grow with the subsample size and do not require\nirrepresentable condition. The estimator is also compared empirically with\nseveral other popular high-dimensional estimators via an extensive simulation\nstudy, which reveals its excellent finite-sample performance. \n\n"}
{"id": "1306.5289", "contents": "Title: Analytic Solutions for D-optimal Factorial Designs under Generalized\n  Linear Models Abstract: We develop two analytic approaches to solve D-optimal approximate designs\nunder generalized linear models. The first approach provides analytic D-optimal\nallocations for generalized linear models with two factors, which include as a\nspecial case the $2^2$ main-effects model considered by Yang, Mandal and\nMajumdar (2012). The second approach leads to explicit solutions for a class of\ngeneralized linear models with more than two factors. With the aid of the\nanalytic solutions, we provide a necessary and sufficient condition under which\na D-optimal design with two quantitative factors could be constructed on the\nboundary points only. It bridges the gap between D-optimal factorial designs\nand D-optimal designs with continuous factors. \n\n"}
{"id": "1306.6462", "contents": "Title: On the Convergence of Adaptive Sequential Monte Carlo Methods Abstract: In several implementations of Sequential Monte Carlo (SMC) methods it is\nnatural, and important in terms of algorithmic efficiency, to exploit the\ninformation of the history of the samples to optimally tune their subsequent\npropagations. In this article we provide a carefully formulated asymptotic\ntheory for a class of such \\emph{adaptive} SMC methods. The theoretical\nframework developed here will cover, under assumptions, several commonly used\nSMC algorithms. There are only limited results about the theoretical\nunderpinning of such adaptive methods: we will bridge this gap by providing a\nweak law of large numbers (WLLN) and a central limit theorem (CLT) for some of\nthese algorithms. The latter seems to be the first result of its kind in the\nliterature and provides a formal justification of algorithms used in many real\ndata context. We establish that for a general class of adaptive SMC algorithms\nthe asymptotic variance of the estimators from the adaptive SMC method is\n\\emph{identical} to a so-called `perfect' SMC algorithm which uses ideal\nproposal kernels. Our results are supported by application on a complex\nhigh-dimensional posterior distribution associated with the Navier-Stokes\nmodel, where adapting high-dimensional parameters of the proposal kernels is\ncritical for the efficiency of the algorithm. \n\n"}
{"id": "1307.0240", "contents": "Title: Continuous Forest Fire Propagation in a Local Small World Network Model Abstract: This paper presents the development of a new continuous forest fire model\nimplemented as a weighted local small-world network approach. This new approach\nwas designed to simulate fire patterns in real, heterogeneous landscapes. The\nwildland fire spread is simulated on a square lattice in which each cell\nrepresents an area of the land's surface. The interaction between burning and\nnon-burning cells, in the present work induced by flame radiation, may be\nextended well beyond nearest neighbors. It depends on local conditions of\ntopography and vegetation types. An approach based on a solid flame model is\nused to predict the radiative heat flux from the flame generated by the burning\nof each site towards its neighbors. The weighting procedure takes into account\nthe self-degradation of the tree and the ignition processes of a combustible\ncell through time. The model is tested on a field presenting a range of slopes\nand with data collected from a real wildfire scenario. The critical behavior of\nthe spreading process is investigated. \n\n"}
{"id": "1307.0515", "contents": "Title: Stabilizing Cloud Feedback Dramatically Expands the Habitable Zone of\n  Tidally Locked Planets Abstract: The habitable zone (HZ) is the circumstellar region where a planet can\nsustain surface liquid water. Searching for terrestrial planets in the HZ of\nnearby stars is the stated goal of ongoing and planned extrasolar planet\nsurveys. Previous estimates of the inner edge of the HZ were based on\none-dimensional radiative-convective models. The most serious limitation of\nthese models is the inability to predict cloud behavior. Here we use global\nclimate models with sophisticated cloud schemes to show that due to a\nstabilizing cloud feedback, tidally locked planets can be habitable at twice\nthe stellar flux found by previous studies. This dramatically expands the HZ\nand roughly doubles the frequency of habitable planets orbiting red dwarf\nstars. At high stellar flux, strong convection produces thick water clouds near\nthe substellar location that greatly increase the planetary albedo and reduce\nsurface temperatures. Higher insolation produces stronger substellar convection\nand therefore higher albedo, making this phenomenon a stabilizing climate\nfeedback. Substellar clouds also effectively block outgoing radiation from the\nsurface, reducing or even completely reversing the thermal emission contrast\nbetween dayside and nightside. The presence of substellar water clouds and the\nresulting clement surface conditions will therefore be detectable with the\nJames Webb Space Telescope. \n\n"}
{"id": "1307.2741", "contents": "Title: On the choice of ingredients for a theory of the Ice Ages Abstract: \"With five parameters one can fit an elephant\". This provocative statement\nexpresses the fact that when a theory has several adjustable parameters, an\nagreement with empirical data can be of modest value. What about a theory which\ncontains unobserved objects? This is the subject of this paper. It is motivated\nby a model of the Ice Ages of the Pleistocene, which postulates a hot planet in\nan extremely eccentric orbit. This object has many consequences. It is rather\nwell defined by the requirements, that it must not be in conflict with laws of\nnature, nor with empirical data. It must have sufficient mass to produce a\nrapid geographic pole shift on Earth after a close flyby at the end of the\nPleistocene, and also be small enough to disintegrate at this occasion and to\nevaporate during the Holocene. These requirements leave hardly any adaptable\nparameters. In this situation, the agreement with further data, in particular\nthe reverse Dansgaard-Oeschger events of the Holocene, represents a significant\nsupport of this theory. \n\n"}
{"id": "1307.3214", "contents": "Title: An Accurate Method for Determining the Pre-Change Run-Length\n  Distribution of the Generalized Shiryaev--Roberts Detection Procedure Abstract: Change-of-measure is a powerful technique used across statistics, probability\nand analysis. Particularly known as Wald's likelihood ratio identity, the\ntechnique enabled the proof of a number of exact and asymptotic optimality\nresults pertaining to the problem of quickest change-point detection. Within\nthe latter problem's context we apply the technique to develop a numerical\nmethod to compute the Generalized Shiryaev--Roberts (GSR) detection procedure's\npre-change Run-Length distribution. Specifically, the method is based on the\nintegral-equations approach and uses the collocation framework with the basis\nfunctions chosen so as to exploit a certain change-of-measure identity and a\nspecific martingale property of the GSR procedure's detection statistic. As a\nresult, the method's accuracy and robustness improve substantially, even though\nthe method's theoretical rate of convergence is shown to be merely quadratic. A\ntight upper bound on the method's error is supplied as well. The method is not\nrestricted to a particular data distribution or to a specific value of the GSR\ndetection statistic's \"headstart\". To conclude, we offer a case study to\ndemonstrate the proposed method at work, drawing particular attention to the\nmethod's accuracy and its robustness with respect to three factors: (a)\npartition size, (b) change magnitude, and (c) Average Run Length (ARL) to false\nalarm level. Specifically, assuming independent standard Gaussian observations\nundergoing a surge in the mean, we employ the method to study the GSR\nprocedure's Run-Length's pre-change distribution, its average (i.e., the usual\nARL to false alarm) and standard deviation. As expected from the theoretical\nanalysis, the method's high accuracy and robustness with respect to the\nforegoing three factors are confirmed experimentally. We also comment on\nextending the method to handle other performance measures and other procedures. \n\n"}
{"id": "1307.5558", "contents": "Title: Mixtures of Common Skew-t Factor Analyzers Abstract: A mixture of common skew-t factor analyzers model is introduced for\nmodel-based clustering of high-dimensional data. By assuming common component\nfactor loadings, this model allows clustering to be performed in the presence\nof a large number of mixture components or when the number of dimensions is too\nlarge to be well-modelled by the mixtures of factor analyzers model or a\nvariant thereof. Furthermore, assuming that the component densities follow a\nskew-t distribution allows robust clustering of skewed data. The alternating\nexpectation-conditional maximization algorithm is employed for parameter\nestimation. We demonstrate excellent clustering performance when our model is\napplied to real and simulated data.This paper marks the first time that skewed\ncommon factors have been used. \n\n"}
{"id": "1307.6701", "contents": "Title: Iterative Estimation of Solutions to Noisy Nonlinear Operator Equations\n  in Nonparametric Instrumental Regression Abstract: This paper discusses the solution of nonlinear integral equations with noisy\nintegral kernels as they appear in nonparametric instrumental regression. We\npropose a regularized Newton-type iteration and establish convergence and\nconvergence rate results. A particular emphasis is on instrumental regression\nmodels where the usual conditional mean assumption is replaced by a stronger\nindependence assumption. We demonstrate for the case of a binary instrument\nthat our approach allows the correct estimation of regression functions which\nare not identifiable with the standard model. This is illustrated in computed\nexamples with simulated data. \n\n"}
{"id": "1307.6991", "contents": "Title: Statistical significance of rising and oscillatory trends in global\n  ocean and land temperature in the past 160 years Abstract: Various interpretations of the notion of a trend in the context of global\nwarming are discussed, contrasting the difference between viewing a trend as\nthe deterministic response to an external forcing and viewing it as a slow\nvariation which can be separated from the background spectral continuum of\nlong-range persistent climate noise. The emphasis in this paper is on the\nlatter notion, and a general scheme is presented for testing a multi-parameter\ntrend model against a null hypothesis which models the observed climate record\nas an autocorrelated noise. The scheme is employed to the instrumental global\nsea-surface temperature record and the global land-temperature record. A trend\nmodel comprising a linear plus an oscillatory trend with period of\napproximately 60 yr, and the statistical significance of the trends, are tested\nagainst three different null models: first-order autoregressive process,\nfractional Gaussian noise, and fractional Brownian motion. The linear trend is\nsignificant in all cases, but the oscillatory trend is insignificant for ocean\ndata and barely significant for land data. By means of a Bayesian iteration,\nhowever, using the significance of the linear trend to formulate a sharper null\nhypothesis, the oscillatory trend in the land record appears to be\nstatistically significant. The results suggest that the global land record may\nbe better suited for detection of the global warming signal than the ocean\nrecord. \n\n"}
{"id": "1307.7147", "contents": "Title: A Simple Phenomenological Model for Grain Clustering in Turbulence Abstract: We propose a simple model for density fluctuations of aerodynamic grains,\nembedded in a turbulent, gravitating gas disk. The model combines a calculation\nfor the behavior of a group of grains encountering a single turbulent eddy,\nwith a hierarchical approximation of the eddy statistics. This makes analytic\npredictions for a range of quantities including: distributions of grain\ndensities, power spectra and correlation functions of fluctuations, and maximum\ngrain densities reached. We predict how these scale as a function of grain drag\ntime t_stop, spatial scale, grain-to-gas mass ratio, strength of turbulence\n(alpha), and detailed disk properties. We test these against numerical\nsimulations with various turbulence-driving mechanisms. The simulations agree\nwell with the predictions, spanning t_stop*Omega ~ 1e-4 - 10, alpha ~ 1e-10 -\n1e-2, and grain-to-gas mass ratio ~0-3. Results from 'turbulent concentration'\nsimulations and laboratory experiments are also predicted as a special case.\nVortices on a wide range of scales disperse and concentrate grains\nhierarchically. For small grains this is most efficient in eddies with turnover\ntime comparable to the stopping time, but fluctuations are also damped by local\ngas-grain drift. For large grains, shear and gravity lead to a much broader\nrange of eddy scales driving fluctuations, with most power on the largest\nscales. The grain density distribution has a log-Poisson shape, with\nfluctuations for large grains up to factors >1000. We provide simple analytic\nexpressions for the predictions, and discuss implications for planetesimal\nformation, grain growth, and the structure of turbulence. \n\n"}
{"id": "1307.7948", "contents": "Title: On the accuracy of the Viterbi alignment Abstract: In a hidden Markov model, the underlying Markov chain is usually hidden.\nOften, the maximum likelihood alignment (Viterbi alignment) is used as its\nestimate. Although having the biggest likelihood, the Viterbi alignment can\nbehave very untypically by passing states that are at most unexpected. To avoid\nsuch situations, the Viterbi alignment can be modified by forcing it not to\npass these states. In this article, an iterative procedure for improving the\nViterbi alignment is proposed and studied. The iterative approach is compared\nwith a simple bunch approach where a number of states with low probability are\nall replaced at the same time. It can be seen that the iterative way of\nadjusting the Viterbi alignment is more efficient and it has several advantages\nover the bunch approach. The same iterative algorithm for improving the Viterbi\nalignment can be used in the case of peeping, that is when it is possible to\nreveal hidden states. In addition, lower bounds for classification\nprobabilities of the Viterbi alignment under different conditions on the model\nparameters are studied. \n\n"}
{"id": "1308.3922", "contents": "Title: Lagrangian study of surface transport in the Kuroshio Extension area\n  based on simulation of propagation of Fukushima-derived radionuclides Abstract: Lagrangian approach is applied to study near-surface large-scale transport in\nthe Kuroshio Extension area using a simulation with synthetic particles\nadvected by AVISO altimetric velocity field. A material line technique is\napplied to find the origin of water masses in cold-core cyclonic rings pinched\noff from the jet in summer 2011. Tracking and Lagrangian maps provide the\nevidence of cross-jet transport. Fukushima derived caesium isotopes are used as\nLagrangian tracers to study transport and mixing in the area a few months after\nthe March of 2011 tsunami that caused a heavy damage of the Fukushima nuclear\npower plant (FNPP). Tracking maps are computed to trace the origin of water\nparcels with measured levels of Cs-134 and Cs-137 concentrations collected in\ntwo R/V cruises in June and July 2011 in the large area of the Northwest\nPacific. It is shown that Lagrangian simulation is useful to finding the\nsurface areas that are potentially dangerous due to the risk of radioactive\ncontamination. The results of simulation are supported by tracks of the surface\ndrifters which were deployed in the area. \n\n"}
{"id": "1308.6315", "contents": "Title: Clustering, Classification, Discriminant Analysis, and Dimension\n  Reduction via Generalized Hyperbolic Mixtures Abstract: A method for dimension reduction with clustering, classification, or\ndiscriminant analysis is introduced. This mixture model-based approach is based\non fitting generalized hyperbolic mixtures on a reduced subspace within the\nparadigm of model-based clustering, classification, or discriminant analysis. A\nreduced subspace of the data is derived by considering the extent to which\ngroup means and group covariances vary. The members of the subspace arise\nthrough linear combinations of the original data, and are ordered by importance\nvia the associated eigenvalues. The observations can be projected onto the\nsubspace, resulting in a set of variables that captures most of the clustering\ninformation available. The use of generalized hyperbolic mixtures gives a\nrobust framework capable of dealing with skewed clusters. Although dimension\nreduction is increasingly in demand across many application areas, the authors\nare most familiar with biological applications and so two of the five real data\nexamples are within that sphere. Simulated data are also used for illustration.\nThe approach introduced herein can be considered the most general such approach\navailable, and so we compare results to three special and limiting cases.\nComparisons with several well established techniques illustrate its promising\nperformance. \n\n"}
{"id": "1309.1369", "contents": "Title: Semistochastic Quadratic Bound Methods Abstract: Partition functions arise in a variety of settings, including conditional\nrandom fields, logistic regression, and latent gaussian models. In this paper,\nwe consider semistochastic quadratic bound (SQB) methods for maximum likelihood\ninference based on partition function optimization. Batch methods based on the\nquadratic bound were recently proposed for this class of problems, and\nperformed favorably in comparison to state-of-the-art techniques.\nSemistochastic methods fall in between batch algorithms, which use all the\ndata, and stochastic gradient type methods, which use small random selections\nat each iteration. We build semistochastic quadratic bound-based methods, and\nprove both global convergence (to a stationary point) under very weak\nassumptions, and linear convergence rate under stronger assumptions on the\nobjective. To make the proposed methods faster and more stable, we consider\ninexact subproblem minimization and batch-size selection schemes. The efficacy\nof SQB methods is demonstrated via comparison with several state-of-the-art\ntechniques on commonly used datasets. \n\n"}
{"id": "1309.1860", "contents": "Title: Axisymmetrically Tropical Cyclone-like Vortices with Secondary\n  Circulations Abstract: The secondary circulation of the tropical cyclone (TC) is related to its\nformation and intensification, thus becomes very important in the studies. The\nanalytical solutions have both the primary and secondary circulation in a\nthree-dimensionally nonhydrostatic and adiabatic model. We prove that there are\nthree intrinsic radiuses for the axisymmetrically ideal incompressible flow.\nThe first one is the radius of maximum primary circular velocity $r_m$. The\nsecond one is radius of the primary kernel $r_k>r_m$, across which the\nvorticity of the primary circulation changes sign and the vertical velocity\nchanges direction. The last one is the radius of the maximum primary vorticity\n$r_d$, at which the vertical flow of the secondary circulation approaches its\nmaximum, and across which the radius velocity changes sign. The first TC-like\nvortex solution has universal inflow or outflow. The relations between the\nintrinsic length scales are $r_k=\\sqrt{2}r_m$ and $r_d=2r_m$. The second one is\na multi-planar solution, periodically in $z$-coordinate. Within each layer, the\nsolution is a convection vortex. The number of the secondary circulation might\nbe one, two, three, and even more. There are also three intrinsic radiuses\n$r_m$, $r_k$ and $r_d$, but they have different values. It seems that the\nrelative stronger radius velocity could be easily found near boundaries. The\nabove solutions can be applied to study the radial structure of the tornados,\nTCs and mesoscale eddies. \n\n"}
{"id": "1309.1901", "contents": "Title: Variational Bayes Approximations for Clustering via Mixtures of Normal\n  Inverse Gaussian Distributions Abstract: Parameter estimation for model-based clustering using a finite mixture of\nnormal inverse Gaussian (NIG) distributions is achieved through variational\nBayes approximations. Univariate NIG mixtures and multivariate NIG mixtures are\nconsidered. The use of variational Bayes approximations here is a substantial\ndeparture from the traditional EM approach and alleviates some of the\nassociated computational complexities and uncertainties. Our variational\nalgorithm is applied to simulated and real data. The paper concludes with\ndiscussion and suggestions for future work. \n\n"}
{"id": "1309.2918", "contents": "Title: On the role of interaction in sequential Monte Carlo algorithms Abstract: We introduce a general form of sequential Monte Carlo algorithm defined in\nterms of a parameterized resampling mechanism. We find that a suitably\ngeneralized notion of the Effective Sample Size (ESS), widely used to monitor\nalgorithm degeneracy, appears naturally in a study of its convergence\nproperties. We are then able to phrase sufficient conditions for time-uniform\nconvergence in terms of algorithmic control of the ESS, in turn achievable by\nadaptively modulating the interaction between particles. This leads us to\nsuggest novel algorithms which are, in senses to be made precise, provably\nstable and yet designed to avoid the degree of interaction which hinders\nparallelization of standard algorithms. As a byproduct, we prove time-uniform\nconvergence of the popular adaptive resampling particle filter. \n\n"}
{"id": "1309.4289", "contents": "Title: Spherical Hamiltonian Monte Carlo for Constrained Target Distributions Abstract: We propose a new Markov Chain Monte Carlo (MCMC) method for constrained\ntarget distributions. Our method first maps the $D$-dimensional constrained\ndomain of parameters to the unit ball ${\\bf B}_0^D(1)$. Then, it augments the\nresulting parameter space to the $D$-dimensional sphere, ${\\bf S}^D$. The\nboundary of ${\\bf B}_0^D(1)$ corresponds to the equator of ${\\bf S}^D$. This\nchange of domains enables us to implicitly handle the original constraints\nbecause while the sampler moves freely on the sphere, it proposes states that\nare within the constraints imposed on the original parameter space. To improve\nthe computational efficiency of our algorithm, we split the Lagrangian dynamics\ninto several parts such that a part of the dynamics can be handled analytically\nby finding the geodesic flow on the sphere. We apply our method to several\nexamples including truncated Gaussian, Bayesian Lasso, Bayesian bridge\nregression, and a copula model for identifying synchrony among multiple\nneurons. Our results show that the proposed method can provide a natural and\nefficient framework for handling several types of constraints on target\ndistributions. \n\n"}
{"id": "1309.4332", "contents": "Title: Observations on the flow structures and transport in a warm-core ring in\n  the Gulf of Mexico Abstract: This study presents several new observations from the study of a warm-core\nring (WCR) in the Gulf of Mexico based on the ECCO2 global ocean simulation.\nUsing Lagrangian coherent structures (LCS) techniques to investigate this flow\nreveals a pattern of transversely intersecting LCS in the mixed layer of the\nWCR which experiences consistent stretching behavior over a large region of\nspace and time. A detailed analysis of this flow region leads to an analytical\nmodel velocity field which captures the essential elements that generate the\ntransversely intersecting LCS. The model parameters are determined from the WCR\nand the resulting LCS show excellent agreement with those observed in the WCR.\nThe three-dimensional transport behavior which creates these structures relies\non the small radial outflow which is present in the mixed layer and is not seen\nbelow the pycnocline, leading to a sharp change in the character of the LCS at\nthe bottom of the mixed layer. The flow behavior revealed by the LCS limits\nfluid exchange between the WCR and the surrounding ocean, contributing to the\nlong life of WCRs. Further study of these structures and their associated\ntransport behavior may lead to further insights into the development and\npersistence of such geophysical vortices as well as their transport behavior. \n\n"}
{"id": "1309.5808", "contents": "Title: Efficient goodness-of-fit tests in multi-dimensional vine copula models Abstract: We introduce a new goodness-of-fit test for regular vine (R-vine) copula\nmodels, a flexible class of multivariate copulas based on a pair-copula\nconstruction (PCC). The test arises from the information matrix ratio. The\ncorresponding test statistic is derived and its asymptotic normality is proven.\nThe test's power is investigated and compared to 14 other goodness-of-fit\ntests, adapted from the bivariate copula case, in a high dimensional setting.\nThe extensive simulation study shows the excellent performance with respect to\nsize and power as well as the superiority of the information matrix ratio based\ntest against most other goodness-of-fit tests. The best performing tests are\napplied to a portfolio of stock indices and their related volatility indices\nvalidating different R-vine specifications. \n\n"}
{"id": "1309.6897", "contents": "Title: Efficient Optimization of the Likelihood Function in Gaussian Process\n  Modelling Abstract: Gaussian Process (GP) models are popular statistical surrogates used for\nemulating computationally expensive computer simulators. The quality of a GP\nmodel fit can be assessed by a goodness of fit measure based on optimized\nlikelihood. Finding the global maximum of the likelihood function for a GP\nmodel is typically very challenging as the likelihood surface often has\nmultiple local optima, and an explicit expression for the gradient of the\nlikelihood function is typically unavailable. Previous methods for optimizing\nthe likelihood function (e.g. MacDonald et al. (2013)) have proven to be robust\nand accurate, though relatively inefficient. We propose several likelihood\noptimization techniques, including two modified multi-start local search\ntechniques, based on the method implemented by MacDonald et al. (2013), that\nare equally as reliable, and significantly more efficient. A hybridization of\nthe global search algorithm Dividing Rectangles (DIRECT) with the local\noptimization algorithm BFGS provides a comparable GP model quality for a\nfraction of the computational cost, and is the preferred optimization technique\nwhen computational resources are limited. We use several test functions and a\nreal application from an oil reservoir simulation to test and compare the\nperformance of the proposed methods with the one implemented by MacDonald et\nal. (2013) in the R library GPfit. The proposed method is implemented in a\nMatlab package, GPMfit. \n\n"}
{"id": "1309.7098", "contents": "Title: An Explicit Formulation of the Earth Movers Distance with Continuous\n  Road Map Distances Abstract: The Earth movers distance (EMD) is a measure of distance between probability\ndistributions which is at the heart of mass transportation theory. Recent\nresearch has shown that the EMD plays a crucial role in studying the potential\nimpact of Demand-Responsive Transportation (DRT) and Mobility-on-Demand (MoD)\nsystems, which are growing paradigms for one-way vehicle sharing where people\ndrive (or are driven by) shared vehicles from a point of origin to a point of\ndestination. While the ubiquitous physical transportation setting is the road\nnetwork, characterized by systems of roads connected together by interchanges,\nmost analytical works about vehicle sharing represent distances between points\nin a plane using the simple Euclidean metric. Instead, we consider the EMD when\nthe ground metric is taken from a class of one-dimensional, continuous metric\nspaces, reminiscent of road networks. We produce an explicit formulation of the\nEarth movers distance given any finite road network R. The result generalizes\nthe EMD with a Euclidean R1 ground metric, which had remained one of the only\nknown non-discrete cases with an explicit formula. Our formulation casts the\nEMD as the optimal value of a finite-dimensional, real-valued optimization\nproblem, with a convex objective function and linear constraints. In the\nspecial case that the input distributions have piece-wise uniform (constant)\ndensity, the problem reduces to one whose objective function is convex\nquadratic. Both forms are amenable to modern mathematical programming\ntechniques. \n\n"}
{"id": "1309.7626", "contents": "Title: The dynamics of technology diffusion and the impacts of climate policy\n  instruments in the decarbonisation of the global electricity sector Abstract: This paper presents an analysis of climate policy instruments for the\ndecarbonisation of the global electricity sector in a non-equilibrium economic\nand technology diffusion perspective. Energy markets are driven by innovation,\npath-dependent technology choices and diffusion. However, conventional\noptimisation models lack detail on these aspects and have limited ability to\naddress the effectiveness of policy interventions because they do not represent\ndecision-making. As a result, known effects of technology lock-ins are liable\nto be underestimated. In contrast, our approach places investor decision-making\nat the core of the analysis and investigates how it drives the diffusion of\nlow-carbon technology in a highly disaggregated, hybrid, global\nmacroeconometric model, FTT:Power-E3MG. Ten scenarios to 2050 of the\nelectricity sector in 21 regions exploring combinations of electricity policy\ninstruments are analysed, including their climate impacts. We show that in a\ndiffusion and path-dependent perspective, the impact of combinations of\npolicies does not correspond to the sum of impacts of individual instruments:\nsynergies exist between policy tools. We argue that the carbon price required\nto break the current fossil technology lock-in can be much lower when combined\nwith other policies, and that a 90% decarbonisation of the electricity sector\nby 2050 is affordable without early scrapping. \n\n"}
{"id": "1310.1022", "contents": "Title: Multivariate regression and fit function uncertainty Abstract: This article describes a multivariate polynomial regression method where the\nuncertainty of the input parameters are approximated with Gaussian\ndistributions, derived from the central limit theorem for large weighted sums,\ndirectly from the training sample. The estimated uncertainties can be\npropagated into the optimal fit function, as an alternative to the statistical\nbootstrap method. This uncertainty can be propagated further into a loss\nfunction like quantity, with which it is possible to calculate the expected\nloss function, and allows to select the optimal polynomial degree with\nstatistical significance. Combined with simple phase space splitting methods,\nit is possible to model most features of the training data even with low degree\npolynomials or constants. \n\n"}
{"id": "1310.1297", "contents": "Title: Spectral Clustering for Divide-and-Conquer Graph Matching Abstract: We present a parallelized bijective graph matching algorithm that leverages\nseeds and is designed to match very large graphs. Our algorithm combines\nspectral graph embedding with existing state-of-the-art seeded graph matching\nprocedures. We justify our approach by proving that modestly correlated, large\nstochastic block model random graphs are correctly matched utilizing very few\nseeds through our divide-and-conquer procedure. We also demonstrate the\neffectiveness of our approach in matching very large graphs in simulated and\nreal data examples, showing up to a factor of 8 improvement in runtime with\nminimal sacrifice in accuracy. \n\n"}
{"id": "1310.1905", "contents": "Title: libcloudph++ 0.2: single-moment bulk, double-moment bulk, and\n  particle-based warm-rain microphysics library in C++ Abstract: This paper introduces a library of algorithms for representing cloud\nmicrophysics in numerical models. The library is written in C++, hence the name\nlibcloudph++. In the current release, the library covers three warm-rain\nschemes: the single- and double-moment bulk schemes, and the particle-based\nscheme with Monte-Carlo coalescence. The three schemes are intended for\nmodelling frameworks of different dimensionality and complexity ranging from\nparcel models to multi-dimensional cloud-resolving (e.g. large-eddy)\nsimulations. A two-dimensional prescribed-flow framework is used in example\nsimulations presented in the paper with the aim of highlighting the library\nfeatures. The libcloudph++ and all its mandatory dependencies are free and\nopen-source software. The Boost.units library is used for zero-overhead\ndimensional analysis of the code at compile time. The particle-based scheme is\nimplemented using the Thrust library that allows to leverage the power of\ngraphics processing units (GPU), retaining the possibility to compile the\nunchanged code for execution on single or multiple standard processors (CPUs).\nThe paper includes complete description of the programming interface (API) of\nthe library and a performance analysis including comparison of GPU and CPU\nsetups. \n\n"}
{"id": "1310.2280", "contents": "Title: Nonlinear energy transfers and phase diagrams for geostrophically\n  balanced rotating--stratified flows Abstract: Equilibrium statistical mechanics tools have been developed to obtain\nindications about the natural tendencies of nonlinear energy transfers in\ntwo-dimensional and quasi two-dimensional flows like rotating and stratified\nflows in geostrophic balance. In this article, we consider a simple model of\nsuch flows with a non-trivial vertical structure, namely two-layer\nquasi-geostrophic flows, which remain amenable to analytical study. We obtain\nthe statistical equilibria of the system in the case of a linear\nvorticity-stream function relation, build the corresponding phase diagram, and\ndiscuss the most probable outcome of nonlinear energy transfers, both on the\nhorizontal and on the vertical, in the presence of stratification and rotation. \n\n"}
{"id": "1310.2750", "contents": "Title: Field theoretical prediction of a property of the tropical cyclone Abstract: The large scale atmospheric vortices (tropical cyclones, tornadoes) are\ncomplex physical systems combining thermodynamics and fluid-mechanical\nprocesses. The well known tendency of vorticity to self-organization, an\nuniversal property of the two-dimensional fluids, is part of the full dynamics,\nbut its description requires particular methods. The general framework for the\nthermodynamical and mechanical processes is based on conservation laws while\nthe vorticity self-organization needs a variational approach. It is difficult\nto estimate to what extent the vorticity self-organization (a purely kinematic\nprocess) have influenced the characteristics of the tropical cyclone at\nstationarity. If this influence is substantial it is expected that the\nstationary state of the tropical cyclone has the same nature as the vortices of\nmany other systems in nature: ideal (Euler) fluids, superconductors, Bose -\nEinstein condensate, cosmic strings, etc.\n  In previous works we have formulated a description of the $2D$ vorticity\nself-organization in terms of a classical field theory. The field theoretical\n(FT) formulation finds that the quasi-coherent form of the atmospheric vortex\n(tropical cyclone) at stationarity is an expression of the Self-Duality.\n  In the present work we examine a strong property of the tropical cyclone,\nwhich arises in the FT formulation in a natural way: the equality of the masses\nof the particles associated to the matter field and respectively to the gauge\nfield in the FT model is translated into the equality between the maximum\nradial extension of the tropical cyclone and the Rossby radius. For the cases\nwhere the FT model is a good approximation we calculate characteristic\nquantities of the tropical cyclone and find good comparison with observational\ndata. \n\n"}
{"id": "1310.7349", "contents": "Title: Wave turbulence in the two-layer ocean model Abstract: This paper looks at the two-layer ocean model from a wave turbulence\nperspective. A symmetric form of the two-layer kinetic equation for Rossby\nwaves is derived using canonical variables, allowing the turbulent cascade of\nenergy between the barotropic and baroclinic modes to be studied. It turns out\nthat energy is transferred via local triad interactions from the large-scale\nbaroclinic modes to the baroclinic and barotropic modes at the Rossby\ndeformation scale. From there it is then transferred to the large-scale\nbarotropic modes via a nonlocal inverse transfer. Using scale separation a sys-\ntem of coupled equations were obtained for the small-scale baroclinic component\nand the large-scale barotropic component. Since the total energy of the\nsmall-scale component is not conserved, but the total barotropic plus\nbaroclinic energy is conserved, the baroclinic energy loss at small scales will\nbe compensated by the growth of the barotropic energy at large scales. It is\nfound that this transfer is mostly anisotropic and mostly to the zonal\ncomponent. \n\n"}
{"id": "1311.0317", "contents": "Title: Parsimonious Shifted Asymmetric Laplace Mixtures Abstract: A family of parsimonious shifted asymmetric Laplace mixture models is\nintroduced. We extend the mixture of factor analyzers model to the shifted\nasymmetric Laplace distribution. Imposing constraints on the constitute parts\nof the resulting decomposed component scale matrices leads to a family of\nparsimonious models. An explicit two-stage parameter estimation procedure is\ndescribed, and the Bayesian information criterion and the integrated completed\nlikelihood are compared for model selection. This novel family of models is\napplied to real data, where it is compared to its Gaussian analogue within\nclustering and classification paradigms. \n\n"}
{"id": "1311.0907", "contents": "Title: Bayesian nonparametric inference on the Stiefel manifold Abstract: The Stiefel manifold $V_{p,d}$ is the space of all $d \\times p$ orthonormal\nmatrices, with the $d-1$ hypersphere and the space of all orthogonal matrices\nconstituting special cases. In modeling data lying on the Stiefel manifold,\nparametric distributions such as the matrix Langevin distribution are often\nused; however, model misspecification is a concern and it is desirable to have\nnonparametric alternatives. Current nonparametric methods are Fr\\'echet mean\nbased. We take a fully generative nonparametric approach, which relies on\nmixing parametric kernels such as the matrix Langevin. The proposed kernel\nmixtures can approximate a large class of distributions on the Stiefel\nmanifold, and we develop theory showing posterior consistency. While there\nexists work developing general posterior consistency results, extending these\nresults to this particular manifold requires substantial new theory. Posterior\ninference is illustrated on a real-world dataset of near-Earth objects. \n\n"}
{"id": "1311.1882", "contents": "Title: The complex singularity of a Stokes wave Abstract: Two-dimensional potential flow of the ideal incompressible fluid with free\nsurface and infinite depth can be described by a conformal map of the fluid\ndomain into the complex lower half-plane. Stokes wave is the fully nonlinear\ngravity wave propagating with the constant velocity. The increase of the scaled\nwave height $H/\\lambda$ from the linear limit $H/\\lambda=0$ to the critical\nvalue $H_{max}/\\lambda$ marks the transition from the limit of almost linear\nwave to a strongly nonlinear limiting Stokes wave. Here $H$ is the wave height\nand $\\lambda$ is the wavelength. We simulated fully nonlinear Euler equations,\nreformulated in terms of conformal variables, to find Stokes waves for\ndifferent wave heights. Analyzing spectra of these solutions we found in\nconformal variables, at each Stokes wave height, the distance $v_c$ from the\nlowest singularity in the upper half-plane to the real line which corresponds\nto the fluid free surface. We also identified that this singularity is the\nsquare-root branch point. The limiting Stokes wave emerges as the singularity\nreaches the fluid surface. From the analysis of data for $v_c\\to 0$ we suggest\na new power law scaling $v_c\\propto (H_{max}-H)^{3/2}$ as well as new estimate\n$H_{max}/\\lambda \\simeq 0.1410633$. \n\n"}
{"id": "1311.1890", "contents": "Title: Discrepancy estimates for variance bounding Markov chain quasi-Monte\n  Carlo Abstract: Markov chain Monte Carlo (MCMC) simulations are modeled as driven by true\nrandom numbers. We consider variance bounding Markov chains driven by a\ndeterministic sequence of numbers. The star-discrepancy provides a measure of\nefficiency of such Markov chain quasi-Monte Carlo methods. We define a\npull-back discrepancy of the driver sequence and state a close relation to the\nstar-discrepancy of the Markov chain-quasi Monte Carlo samples. We prove that\nthere exists a deterministic driver sequence such that the discrepancies\ndecrease almost with the Monte Carlo rate $n^{1/2}$. As for MCMC simulations, a\nburn-in period can also be taken into account for Markov chain quasi-Monte\nCarlo to reduce the influence of the initial state. In particular, our\ndiscrepancy bound leads to an estimate of the error for the computation of\nexpectations. To illustrate our theory we provide an example for the Metropolis\nalgorithm based on a ball walk. Furthermore, under additional assumptions we\nprove the existence of a driver sequence such that the discrepancy of the\ncorresponding deterministic Markov chain sample decreases with order\n$n^{-1+\\delta}$ for every $\\delta>0$. \n\n"}
{"id": "1311.2441", "contents": "Title: On the comparison between MASS and G-SCIDAR techniques Abstract: The Multi Aperture Scintillation Sensor (MASS) and the\nGeneralized-Scintillation Detection and Ranging (Generalized SCIDAR) are two\ninstruments conceived to measure the optical turbulence (OT) vertical\ndistribution on the whole troposphere and low stratosphere (~ 20 km) widely\nused in the astronomical context. In this paper we perform a detailed\nanalysis/comparison of measurements provided by the two instruments and taken\nduring the extended site testing campaign carried out on 2007 at Cerro Paranal\nand promoted by the European Southern Observatory (ESO). The main and final\ngoal of the study is to provide a detailed estimation of the measurements\nreliability i.e dispersion of turbulence measurements done by the two\ninstruments at different heights above the ground. This information is directly\nrelated to our ability in estimating the absolute value of the turbulence\nstratification. To better analyse the uncertainties between the MASS and the GS\nwe took advantage of the availability of measurements taken during the same\ncampaign by a third independent instrument (DIMM - Differential Imaging Motion\nMonitor) measuring the integrated turbulence extended on the whole 20 km. Such\na cross-check comparison permitted us to define the reliability of the\ninstruments and their measurements, their limits and the contexts in which\ntheir use can present some risk. \n\n"}
{"id": "1311.4780", "contents": "Title: Asymptotically Exact, Embarrassingly Parallel MCMC Abstract: Communication costs, resulting from synchronization requirements during\nlearning, can greatly slow down many parallel machine learning algorithms. In\nthis paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in\nwhich subsets of data are processed independently, with very little\ncommunication. First, we arbitrarily partition data onto multiple machines.\nThen, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be\nused to draw samples from a posterior distribution given the data subset.\nFinally, the samples from each machine are combined to form samples from the\nfull posterior. This embarrassingly parallel algorithm allows each machine to\nact independently on a subset of the data (without communication) until the\nfinal combination stage. We prove that our algorithm generates asymptotically\nexact samples and empirically demonstrate its ability to parallelize burn-in\nand sampling in several models. \n\n"}
{"id": "1311.7286", "contents": "Title: Approximate Bayesian Computation with composite score functions Abstract: Both Approximate Bayesian Computation (ABC) and composite likelihood methods\nare useful for Bayesian and frequentist inference, respectively, when the\nlikelihood function is intractable. We propose to use composite likelihood\nscore functions as summary statistics in ABC in order to obtain accurate\napproximations to the posterior distribution. This is motivated by the use of\nthe score function of the full likelihood, and extended to general unbiased\nestimating functions in complex models. Moreover, we show that if the composite\nscore is suitably standardised, the resulting ABC procedure is invariant to\nreparameterisations and automatically adjusts the curvature of the composite\nlikelihood, and of the corresponding posterior distribution. The method is\nillustrated through examples with simulated data, and an application to\nmodelling of spatial extreme rainfall data is discussed. \n\n"}
{"id": "1312.0781", "contents": "Title: Recursive maximum likelihood identification of jump Markov nonlinear\n  systems Abstract: In this contribution, we present an online method for joint state and\nparameter estimation in jump Markov non-linear systems (JMNLS). State inference\nis enabled via the use of particle filters which makes the method applicable to\na wide range of non-linear models. To exploit the inherent structure of JMNLS,\nwe design a Rao-Blackwellized particle filter (RBPF) where the discrete mode is\nmarginalized out analytically. This results in an efficient implementation of\nthe algorithm and reduces the estimation error variance. The proposed RBPF is\nthen used to compute, recursively in time, smoothed estimates of complete data\nsufficient statistics. Together with the online expectation maximization\nalgorithm, this enables recursive identification of unknown model parameters.\nThe performance of the method is illustrated in simulations and on a\nlocalization problem in wireless networks using real data. \n\n"}
{"id": "1312.2553", "contents": "Title: Hyperviscosity and statistical equilibria of Euler turbulence on the\n  torus and the sphere Abstract: Coherent structures such as jets and vortices appear in two-dimensional (2D)\nturbulence. To gain insight into both numerical simulation and equilibrium\nstatistical mechanical descriptions of 2D Euler flows, the Euler equation with\nadded hyperviscosity is integrated forward in time on the square torus and on\nthe sphere. Coherent structures that form are compared against a hierarchy of\ntruncated Miller-Robert-Sommeria equilibria. The energy-circulation-enstrophy\nMRS-2 description produces a complete condensation of energy to the largest\nscales, and in the absence of rotation correctly predicts the number and\npolarity of coherent vortices. Perturbative imposition of the quartic Casimir\nconstraint improves agreement with numerical simulation by sharpening the cores\nand transferring some energy to smaller-scale modes. MRS-2 cannot explain\nqualitative changes due to rotation, but descriptions that conserve higher\nCasimirs beyond enstrophy have the potential to do so. The result is in\nagreement with the somewhat paradoxical observation that hyperviscosity helps\nto remedy the non-conservation of the third and higher Casimirs in numerical\nsimulation. For a rotating sphere, numerical simulation also demonstrates that\ncoherent structures found at late times depend on initial conditions, limiting\nthe usefulness of statistical mechanics. \n\n"}
{"id": "1312.2556", "contents": "Title: A method for importance sampling through Markov chain Monte Carlo with\n  post sampling variational estimate Abstract: We propose a method to efficiently integrate truncated probability densities.\nThe method uses Markov chain Monte Carlo method to sample from a probability\ndensity matching the function being integrated. The required normalisation or\nequivalently the result is obtained by constructing a function with known\nintegral, through non-parametric kernel density estimation and variational\nprocedure. The method is demonstrated with numerical case studies. Possible\nenhancements to the method and limitations are discussed. \n\n"}
{"id": "1312.2923", "contents": "Title: Lagrangian Time Series Models for Ocean Surface Drifter Trajectories Abstract: This paper proposes stochastic models for the analysis of ocean surface\ntrajectories obtained from freely-drifting satellite-tracked instruments. The\nproposed time series models are used to summarise large multivariate datasets\nand infer important physical parameters of inertial oscillations and other\nocean processes. Nonstationary time series methods are employed to account for\nthe spatiotemporal variability of each trajectory. Because the datasets are\nlarge, we construct computationally efficient methods through the use of\nfrequency-domain modelling and estimation, with the data expressed as\ncomplex-valued time series. We detail how practical issues related to sampling\nand model misspecification may be addressed using semi-parametric techniques\nfor time series, and we demonstrate the effectiveness of our stochastic models\nthrough application to both real-world data and to numerical model output. \n\n"}
{"id": "1312.3250", "contents": "Title: Acceleration of raindrops formation due to tangling-clustering\n  instability in turbulent stratified atmosphere Abstract: Condensation of water vapor on active cloud condensation nuclei produces\nmicron-size water droplets. To form rain, they must grow rapidly into at least\n50-100 $\\mu$m droplets. Observations show that this process takes only 15-20\nminutes. The unexplained physical mechanism of such fast growth, is crucial for\nunderstanding and modeling of rain, and known as \"condensation-coalescence\nbottleneck in rain formation\". We show that the recently discovered phenomenon\nof the tangling clustering instability of small droplets in\ntemperature-stratified turbulence (Phys. Fluids 25, 085104, 2013) results in\nthe formation of droplet clusters with drastically increased droplet number\ndensities. The mechanism of the tangling clustering instability is much more\neffective than the previously considered by us the inertial clustering\ninstability caused by the centrifugal effect of turbulent vortices. This is the\nreason of strong enhancement of the collision-coalescence rate inside the\nclusters. The mean-field theory of the droplet growth developed in this study\ncan be useful for explanation of the observed fast growth of cloud droplets in\nwarm clouds from the initial 1 micron size droplets to 40-50 micron size\ndroplets within 15-20 minutes. \n\n"}
{"id": "1312.3434", "contents": "Title: Multifractality due to long-range correlation in the L-band ionospheric\n  scintillation S4 index time series Abstract: The earth's ionosphere is well recognized as a dynamical system and\nnon-linearly coupled with the magnetosphere above and natural atmosphere\nbelow.The shape and time variability of the ionosphere indeed shows chaos,\npattern formation, random behavior and self-organization. The present paper\nstudies the propriety of Multifractal Detrended Fluctuation Analysis (MF-DFA)\ntechnique for the ionospheric scintillation index time series. MF-DFA is used\nto identify the scaling behavior of the ionospheric scintillation time-series\ndata of two different nature.The obtained results show the robustness and the\nrelevancy of the MF-DFA technique for the ionospheric scintillation index time\nseries. The comparison of the MF-DFA results of original data to those of\nshuffled and surrogate series shows that the multifractal nature of considered\ntime series is almost due to long-range correlations. Subsequently, the Hurst\nexponents derived from two parallel methods namely Re-scaled range analysis\n(R/S) and Auto Correlation Function (ACF) are also suggesting the presence of\nlong range correlation. The presented results in this work may be of assistance\nfor future modeling and simulation studies. \n\n"}
{"id": "1401.0265", "contents": "Title: Approximate Bayesian Computation for a Class of Time Series Models Abstract: In the following article we consider approximate Bayesian computation (ABC)\nfor certain classes of time series models. In particular, we focus upon\nscenarios where the likelihoods of the observations and parameter are\nintractable, by which we mean that one cannot evaluate the likelihood even\nup-to a positive unbiased estimate. This paper reviews and develops a class of\napproximation procedures based upon the idea of ABC, but, specifically\nmaintains the probabilistic structure of the original statistical model. This\nidea is useful, in that it can facilitate an analysis of the bias of the\napproximation and the adaptation of established computational methods for\nparameter inference. Several existing results in the literature are surveyed\nand novel developments with regards to computation are given. \n\n"}
{"id": "1401.0616", "contents": "Title: Compatible finite element methods for numerical weather prediction Abstract: This article takes the form of a tutorial on the use of a particular class of\nmixed finite element methods, which can be thought of as the finite element\nextension of the C-grid staggered finite difference method. The class is often\nreferred to as compatible finite elements, mimetic finite elements, discrete\ndifferential forms or finite element exterior calculus. We provide an\nelementary introduction in the case of the one-dimensional wave equation,\nbefore summarising recent results in applications to the rotating shallow water\nequations on the sphere, before taking an outlook towards applications in\nthree-dimensional compressible dynamical cores. \n\n"}
{"id": "1401.1097", "contents": "Title: Definition of a moist-air entropy potential temperature. Application to\n  FIRE-I data flights Abstract: A moist entropy potential temperature -- denoted by ${\\theta}_{s}$ -- is\ndefined analytically in terms of the specific entropy for moist air. The\nexpression for ${\\theta}_{s}$ is valid for a general mixing of dry air, water\nvapour and possible condensed water species. It verifies the same conservative\nproperties as the moist entropy, even for varying dry air or total water\ncontent. The moist formulation for ${\\theta}_{s}$ is equal to the dry\nformulation $\\theta$ if dry air is considered and it verifies new properties\nvalid for the moist air cases, both saturated or under-saturated ones. Exact\nand approximate versions of ${\\theta}_{s}$ are evaluated for several\nStratocumulus cases, in particular by using the aircraft observations FIRE-I\nexperiment data sets. It appears that there is no (or small) jump in\n${\\theta}_{s}$ at the top of the PBL. The mixing in moist entropy is almost\ncomplete in the PBL, with the same values observed in the clear air and the\ncloudy regions, including the very top of the entrainment region. The\nRandall-Deardorff CTEI analysis may be interpreted as a mixing in moist entropy\ncriterion. The iso-${\\theta}_{s}$ lines are plotted on skew $T$-$\\ln(p)$ and\nconserved variable diagrams. All these properties could suggest some hints on\nthe use of moist entropy (or ${\\theta}_{s}$) in cloud modelling or in mixing\nprocesses, with the marine Stratocumulus considered as a paradigm of moist\nturbulence. \n\n"}
{"id": "1401.1234", "contents": "Title: Global Well-posedness of Strong Solutions to the 3D Primitive Equations\n  with Horizontal Eddy Diffusivity Abstract: In this paper, we consider the initial-boundary value problem of the 3D\nprimitive equations for oceanic and atmospheric dynamics with only horizontal\ndiffusion in the temperature equation. Global well-posedness of strong\nsolutions are established with $H^2$ initial data. \n\n"}
{"id": "1401.2103", "contents": "Title: Restricted Equilibrium and the Energy Cascade in Rotating and Stratified\n  Flows Abstract: Most of the turbulent flows appearing in nature (e.g. geophysical and\nastrophysical flows) are subjected to strong rotation and stratification. These\neffects break the symmetries of classical, homogenous isotropic turbulence. In\ndoing so, they introduce a natural decomposition of phase space in terms of\nwave modes and potential vorticity modes. The appearance of a new time scale\nassociated to the propagation of waves, in addition to the eddy turnover time,\nincreases the complexity of the energy transfers between the various scales;\nnonlinearly interacting waves may dominate at some scales while balanced motion\nmay prevail at others. In the end, it is difficult to predict \\emph{a priori}\nif the energy cascades downscale as in homogeneous isotropic turbulence,\nupscale as expected from balanced dynamics, or follows yet another\nphenomenology.\n  In this paper, we suggest a theoretical approach based on equilibrium\nstatistical mechanics for the ideal system, inspired from the restricted\npartition function formalism introduced in metastability studies. In this\nframework, we show analytically that in the presence of rotation, when the\ndynamics is restricted to the slow modes, the equilibrium energy spectrum\nfeatures an infrared divergence characteristic of an inverse cascade regime,\nwhereas this is not the case for purely stratified flows. \n\n"}
{"id": "1401.2163", "contents": "Title: Estimation of Partially Linear Regression Model under Partial\n  Consistency Property Abstract: In this paper, utilizing recent theoretical results in high dimensional\nstatistical modeling, we propose a model-free yet computationally simple\napproach to estimate the partially linear model $Y=X\\beta+g(Z)+\\varepsilon$.\nMotivated by the partial consistency phenomena, we propose to model $g(Z)$ via\nincidental parameters. Based on partitioning the support of $Z$, a simple local\naverage is used to estimate the response surface. The proposed method seeks to\nstrike a balance between computation burden and efficiency of the estimators\nwhile minimizing model bias. Computationally this approach only involves least\nsquares. We show that given the inconsistent estimator of $g(Z)$, a root $n$\nconsistent estimator of parametric component $\\beta$ of the partially linear\nmodel can be obtained with little cost in efficiency. Moreover, conditional on\nthe $\\beta$ estimates, an optimal estimator of $g(Z)$ can then be obtained\nusing classic nonparametric methods. The statistical inference problem\nregarding $\\beta$ and a two-population nonparametric testing problem regarding\n$g(Z)$ are considered. Our results show that the behavior of test statistics\nare satisfactory. To assess the performance of our method in comparison with\nother methods, three simulation studies are conducted and a real dataset about\nrisk factors of birth weights is analyzed. \n\n"}
{"id": "1401.2379", "contents": "Title: On a general definition of the squared Brunt-V\\\"{a}is\\\"{a}l\\\"{a}\n  Frequency associated with the specific moist entropy potential temperature Abstract: The squared Brunt-V\\\"{a}is\\\"{a}l\\\"{a} Frequency (BVF) is computed in terms of\nthe moist entropy potential temperature recently defined in Marquet (2011).\nBoth homogeneously saturated and non-saturated versions of $N^2$ (the squared\nBVF) are derived. The method employed for computing these special homogeneous\ncases relies on the expression of density written as a function of pressure,\ntotal water content and specific moist entropy only. The associated\nconservative variable diagrams are discussed and compared with existing ones.\nDespite being obtained without any simplification, the formulations for $N^2$\nremain nicely compact and are clearly linked with the squared BVF expressed in\nterms of the adiabatic non-saturated and saturated lapse rates. As in previous\nsimilar expressions, the extreme homogeneous solutions for $N^2$ are of course\ndifferent, but they are not analytically discontinuous. This allows us to\ndefine a simple bridging expression for a single general shape of $N^2$,\ndepending only on the basic mean atmospheric quantities and on a transition\nparameter, to be defined (or parameterized) in connection with the type of\napplication sought. This integrated result remains a linear combination (with\ncomplex but purely local weights) of two terms only, namely the environmental\ngradient of the moist entropy potential temperature and the environmental\ngradient of the total water content. Simplified versions of the various\nequations are also proposed for the case in which the moist entropy potential\ntemperature is approximated by a function of both so-called moist-conservative\nvariables of Betts (1973). \n\n"}
{"id": "1401.2490", "contents": "Title: An Online Expectation-Maximisation Algorithm for Nonnegative Matrix\n  Factorisation Models Abstract: In this paper we formulate the nonnegative matrix factorisation (NMF) problem\nas a maximum likelihood estimation problem for hidden Markov models and propose\nonline expectation-maximisation (EM) algorithms to estimate the NMF and the\nother unknown static parameters. We also propose a sequential Monte Carlo\napproximation of our online EM algorithm. We show the performance of the\nproposed method with two numerical examples. \n\n"}
{"id": "1401.2822", "contents": "Title: Approximations for two-dimensional discrete scan statistics in some\n  block-factor type dependent models Abstract: We consider the two-dimensional discrete scan statistic generated by a\nblock-factor type model obtained from i.i.d. sequence. We present an\napproximation for the distribution of the scan statistics and the corresponding\nerror bounds. A simulation study illustrates our methodology. \n\n"}
{"id": "1401.3125", "contents": "Title: On the computation of moist-air specific thermal enthalpy Abstract: The specific thermal enthalpy of a moist-air parcel is defined analytically\nfollowing a method in which specific moist entropy is derived from the Third\nLaw of thermodynamics. Specific thermal enthalpy is computed by integrating\nspecific heat content with respect to absolute temperature and including the\nimpacts of various latent heats (i.e., solid condensation, sublimation,\nmelting, and evaporation). It is assumed that thermal enthalpies can be set to\nzero at $0$ K for the solid form of the main chemically inactive components of\nthe atmosphere (solid-$\\alpha$ oxygen and nitrogen, hexagonal ice). The moist\nthermal enthalpy is compared to already existing formulations of moist static\nenergy (MSE). It is shown that the differences between thermal enthalpy and the\nthermal part of MSE may be quite large. This prevents the use of MSE to\nevaluate the enthalpy budget of a moist atmosphere accurately, a situation that\nis particularly true when dry-air and cloud parcels mix because of\nentrainment/detrainment processes along the edges of cloud. Other differences\nare observed when MSE or moist-air thermal enthalpy is plotted on a\npsychrometric diagram or when vertical profiles of surface deficit are plotted. \n\n"}
{"id": "1401.4082", "contents": "Title: Stochastic Backpropagation and Approximate Inference in Deep Generative\n  Models Abstract: We marry ideas from deep neural networks and approximate Bayesian inference\nto derive a generalised class of deep, directed generative models, endowed with\na new algorithm for scalable inference and learning. Our algorithm introduces a\nrecognition model to represent approximate posterior distributions, and that\nacts as a stochastic encoder of the data. We develop stochastic\nback-propagation -- rules for back-propagation through stochastic variables --\nand use this to develop an algorithm that allows for joint optimisation of the\nparameters of both the generative and recognition model. We demonstrate on\nseveral real-world data sets that the model generates realistic samples,\nprovides accurate imputations of missing data and is a useful tool for\nhigh-dimensional data visualisation. \n\n"}
{"id": "1401.5323", "contents": "Title: Climate of Earth-like planets with high obliquity and eccentric orbits:\n  implications for habitability conditions Abstract: We explore the effects of seasonal variability for the climate of Earth-like\nplanets as determined by the two parameters polar obliquity and orbital\neccentricity using a general circulation model of intermediate complexity. In\nthe first part of the paper we examine the consequences of different values of\nobliquity and eccentricity for the spatio-temporal patterns of radiation and\nsurface temperatures as well as for the main characteristics of the atmospheric\ncirculations. In the second part we analyse the associated implications for the\nhabitability of planets close to the outer edge of the habitable zone (HZ).\nThis part of the paper focuses in particular on the multistability property of\nclimate, i.e. the parallel existence of both an ice-free and an ice-covered\nclimate state. Our results show that seasonal variability affects both the\nexistence of and transitions between the two climate states. Moreover, our\nexperiments reveal that planets with Earth-like atmospheres and high seasonal\nvariability can have ice-free areas at much larger distance from the host star\nthan planets without seasonal variability, which leads to a substantial\nexpansion of the outer edge of the HZ. Sensitivity experiments exploring the\nrole of azimuthal obliquity and surface heat capacity test the robustness of\nour results. On circular orbits, our findings obtained with a general\ncirculation model agree well with previous studies based on one dimensional\nenergy balance models, whereas significant differences are found on eccentric\norbits. \n\n"}
{"id": "1402.1472", "contents": "Title: Parallel inference for massive distributed spatial data using low-rank\n  models Abstract: Due to rapid data growth, statistical analysis of massive datasets often has\nto be carried out in a distributed fashion, either because several datasets\nstored in separate physical locations are all relevant to a given problem, or\nsimply to achieve faster (parallel) computation through a divide-and-conquer\nscheme. In both cases, the challenge is to obtain valid inference that does not\nrequire processing all data at a single central computing node. We show that\nfor a very widely used class of spatial low-rank models, which can be written\nas a linear combination of spatial basis functions plus a fine-scale-variation\ncomponent, parallel spatial inference and prediction for massive distributed\ndata can be carried out exactly, meaning that the results are the same as for a\ntraditional, non-distributed analysis. The communication cost of our\ndistributed algorithms does not depend on the number of data points. After\nextending our results to the spatio-temporal case, we illustrate our\nmethodology by carrying out distributed spatio-temporal particle filtering\ninference on total precipitable water measured by three different satellite\nsensor systems. \n\n"}
{"id": "1402.1510", "contents": "Title: Nonlinear fast growth of water waves under wind forcing Abstract: In the wind-driven wave regime, the Miles mechanism gives an estimate of the\ngrowth rate of the waves under the effect of wind. We consider the case where\nthis growth rate, normalised with respect to the frequency of the carrier wave,\nis of the order of the wave steepness. Using the method of multiple scales, we\ncalculate the terms which appear in the nonlinear Schr\\\"odinger (NLS) equation\nin this regime of fast-growing waves. We define a coordinate transformation\nwhich maps the forced NLS equation into the standard NLS with constant\ncoefficients, that has a number of known analytical soliton solutions. Among\nthese solutions, the Peregrine and the Akhmediev solitons show an enhancement\nof both their lifetime and maximum amplitude which is in qualitative agreement\nwith the results of tank experiments and numerical simulations of dispersive\nfocusing under the action of wind. \n\n"}
{"id": "1402.2676", "contents": "Title: Ranking via Robust Binary Classification and Parallel Parameter\n  Estimation in Large-Scale Data Abstract: We propose RoBiRank, a ranking algorithm that is motivated by observing a\nclose connection between evaluation metrics for learning to rank and loss\nfunctions for robust classification. The algorithm shows a very competitive\nperformance on standard benchmark datasets against other representative\nalgorithms in the literature. On the other hand, in large scale problems where\nexplicit feature vectors and scores are not given, our algorithm can be\nefficiently parallelized across a large number of machines; for a task that\nrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\nour algorithm finds solutions that are of dramatically higher quality than that\ncan be found by a state-of-the-art competitor algorithm, given the same amount\nof wall-clock time for computation. \n\n"}
{"id": "1402.5257", "contents": "Title: Conditional Multilevel Monte Carlo Simulation of Groundwater Flow in the\n  Culebra Dolomite at the Waste Isolation Pilot Plant (WIPP) Site Abstract: We extended the multilevel Monte of Carlo (MLMC) approach to simulation of\ngroundwater flow in porous media by incorporating direct measurements of medium\nproperties. Numerical simulations of Waste Isolation Pilot Plant (WIPP)\nrepository in southeastern New Mexico are performed to test the performance of\nthe conditional MLMC technique. The log-transmissivity of WIPP site is modeled\nas the conditional random fields which honor exact field values at a few\nlocations. The conditional random fields are generated through the modified\ncirculant embedding methods in (Dietrich and Newsam, 1996). We also study\neffects of a combination of the conditional MLMC accompanied by antithetic\nvariates. The main quantity of interest is the time of radionuclides travelling\nfrom the center of repository to the site boundary. Numerical examples are\npresented to demonstrate the cost-effectiveness of the multilevel approach in\ncomparison to the standard Monte Carlo (MC) simulation. \n\n"}
{"id": "1402.5282", "contents": "Title: The Compound Class of Linear Failure Rate-Power Series Distributions:\n  Model, Properties and Applications Abstract: We introduce in this paper a new class of distributions which generalizes the\nlinear failure rate (LFR) distribution and is obtained by compounding the LFR\ndistribution and power series (PS) class of distributions. This new class of\ndistributions is called the linear failure rate-power series (LFRPS)\ndistributions and contains some new distributions such as linear failure rate\ngeometric (LFRG) distribution, linear failure rate Poisson (LFRP) distribution,\nlinear failure rate logarithmic (LFRL) distribution, linear failure rate\nbinomial (LFRB) distribution and Raylight-power series (RPS) class of\ndistributions. Some former works such as exponential-power series (EPS) class\nof distributions, exponential geometric (EG) distribution, exponential Poisson\n(EP) distribution and exponential logarithmic (EL) distribution are special\ncases of the new proposed model.\n  The ability of the LFRPS class of distributions is in covering five possible\nhazard rate function i.e., increasing, decreasing, upside-down bathtub\n(unimodal), bathtub and increasing-decreasing-increasing shaped. Several\nproperties of the LFRPS distributions such as moments, maximum likelihood\nestimation procedure via an EM-algorithm and inference for a large sample, are\ndiscussed in this paper. In order to show the flexibility and potentiality of\nthe new class of distributions, the fitted results of the new class of\ndistributions and some its submodels are compared using a real data set. \n\n"}
{"id": "1402.6602", "contents": "Title: Bayesian Inference for Hybrid Discrete-Continuous Stochastic Kinetic\n  Models Abstract: We consider the problem of efficiently performing simulation and inference\nfor stochastic kinetic models. Whilst it is possible to work directly with the\nresulting Markov jump process, computational cost can be prohibitive for\nnetworks of realistic size and complexity. In this paper, we consider an\ninference scheme based on a novel hybrid simulator that classifies reactions as\neither \"fast\" or \"slow\" with fast reactions evolving as a continuous Markov\nprocess whilst the remaining slow reaction occurrences are modelled through a\nMarkov jump process with time dependent hazards. A linear noise approximation\n(LNA) of fast reaction dynamics is employed and slow reaction events are\ncaptured by exploiting the ability to solve the stochastic differential\nequation driving the LNA. This simulation procedure is used as a proposal\nmechanism inside a particle MCMC scheme, thus allowing Bayesian inference for\nthe model parameters. We apply the scheme to a simple application and compare\nthe output with an existing hybrid approach and also a scheme for performing\ninference for the underlying discrete stochastic model. \n\n"}
{"id": "1403.2036", "contents": "Title: Straightforward Bibliography Management in R with the RefManageR Package Abstract: This work introduces the R package RefManageR, which provides tools for\nimporting and working with bibliographic references. It extends the bibentry\nclass in R in a number of useful ways, including providing R with previously\nunavailable support for BibLaTeX. BibLaTeX provides a superset of the\nfunctionality of BibTeX, including full Unicode support, no memory limitations,\nadditional fields and entry types, and more sophisticated sorting of\nreferences. RefManageR provides functions for citing and generating a\nbibliography with hyperlinks for documents prepared with RMarkdown or RHTML.\nExisting .bib files can be read into R and converted from BibTeX to BibLaTeX\nand vice versa. References can also be imported via queries to NCBI's Entrez,\nZotero libraries, Google Scholar, and CrossRef. Additionally, references can be\ncreated by reading PDFs stored on the user's machine with the help of Poppler.\nEntries stored in the reference manager can be easily searched by any field, by\ndate ranges, and by various formats for name lists (author by last names,\ntranslator by full names, etc.). Entries can also be updated, combined, sorted,\nprinted in a number of styles, and exported. \n\n"}
{"id": "1403.5207", "contents": "Title: Transdimensional Transformation based Markov Chain Monte Carlo Abstract: In this article, we propose a novel and general dimension-hopping MCMC\nmethodology that can update all the parameters as well as the number of\nparameters simultaneously using simple deterministic transformations of some\nlow-dimensional (often one-dimensional) random variable. This methodology,\nwhich has been inspired by the recent Transformation based MCMC (TMCMC) for\nupdating all the parameters simultaneously in general fixed-dimensional set-ups\nusing low-dimensional random variables, facilitates great speed in terms of\ncomputation time and provides high acceptance rates, thanks to the\nlow-dimensional random variables which effectively reduce the dimension\ndramatically. Quite importantly, our transformation based approach provides a\nnatural way to automate the move-types in the variable dimensional problems. We\nrefer to this methodology as Transdimensional Transformation based Markov Chain\nMonte Carlo (TTMCMC).\n  We develop the theory of TTMCMC, illustrating it with gamma and normal\nmixtures with unknown number of components, for both simulated and real data\nsets. Comparisons with RJMCMC demonstrates far superior performance of TTMCMC\nin terms of mixing, acceptance rate, computational speed and automation.\nFurthermore, we demonstrate good performance of TTMCMC in multivariate normal\nmixtures, even for dimension as large as 20. To our knowledge, there exists no\napplication of RJMCMC for such high-dimensional mixtures.\n  Further, we propose a novel methodology to summarize the posterior, providing\na way to obtain the mode of the posterior distribution of the densities and the\nassociated highest posterior density credible regions. Based on our method we\nalso propose a criterion to assess convergence of variable-dimensional\nalgorithms. These methods of summarization and convergence assessment are\napplicable to general problems, not just to mixtures. \n\n"}
{"id": "1403.5536", "contents": "Title: A practical sequential stopping rule for high-dimensional MCMC and its\n  application to spatial-temporal Bayesian models Abstract: A current challenge for many Bayesian analyses is determining when to\nterminate high-dimensional Markov chain Monte Carlo simulations. To this end,\nwe propose using an automated sequential stopping procedure that terminates the\nsimulation when the computational uncertainty is small relative to the\nposterior uncertainty. Such a stopping rule has previously been shown to work\nwell in settings with posteriors of moderate dimension. In this paper, we\nillustrate its utility in high-dimensional simulations while overcoming some\ncurrent computational issues. Further, we investigate the relationship between\nthe stopping rule and effective sample size. As examples, we consider two\ncomplex Bayesian analyses on spatially and temporally correlated datasets. The\nfirst involves a dynamic space-time model on weather station data and the\nsecond a spatial variable selection model on fMRI brain imaging data. Our\nresults show the sequential stopping rule is easy to implement, provides\nuncertainty estimates, and performs well in high-dimensional settings. \n\n"}
{"id": "1403.5537", "contents": "Title: Randomized pick-freeze for sparse Sobol indices estimation in high\n  dimension Abstract: This article investigates a new procedure to estimate the influence of each\nvariable of a given function defined on a high-dimensional space. More\nprecisely, we are concerned with describing a function of a large number $p$ of\nparameters that depends only on a small number $s$ of them. Our proposed method\nis an unconstrained $\\ell_{1}$-minimization based on the Sobol's method. We\nprove that, with only $\\mathcal O(s\\log p)$ evaluations of $f$, one can find\nwhich are the relevant parameters. \n\n"}
{"id": "1403.5671", "contents": "Title: The available-enthalpy (flow-exergy) cycle. Part-I: introduction and\n  basic equations Abstract: A diagnostic package is derived from the concept of specific available\nenthalpy, leading to the definition of a local and complete energy cycle. It is\nuseful to understand the transformations of energy occurring at any particular\npressure level or pressure layer of a limited area domain. The global version\nof this diagnostic tool is very similar to the cycle of Lorenz, but the local\ncounterpart contains several additional terms, with zonal, eddy and\nstatic-stability components close to definitions already given by Pearce. The\nnew cycle takes into account the flow of energy components across the vertical\nand horizontal boundaries, with additional conversion terms involving potential\nenergy, leading to accurate computations of dissipation and generation terms\nobtained as residuals. A new accurate temporal scheme is proposed in order to\nallow use of a large time interval in future numerical applications. Finally,\ncomments are made on the arbitrary choice for two constant reference values for\npressure and temperature. \n\n"}
{"id": "1403.6258", "contents": "Title: The available-enthalpy (flow-exergy) cycle. Part-II: applications to\n  idealized baroclinic waves Abstract: The local available-enthalpy cycle proposed in Part I of this paper is\napplied to document energetics of three numerical simulations, representing\nlife cycles of idealized baroclinic waves. An improved temporal numerical\nscheme defined in Part I is used in this study, together with the Arpege-IFS\nmodel using a T42 triangular truncation. A 45{\\deg}N and 200 hPa dry unstable\njet is constructed with the most unstable mode at zonal wave number 8.\nEnergetic impacts of both horizontal and vertical diffusion schemes are\ndetermined separately. The role of ageostrophic winds within the Ekman layer is\ninvestigated, leading to an explanation for large observed values for the\ndissipation terms and to a new formulation of the potential-energy conversions.\nThe magnitudes of these new conversion terms are compared with those of the\nusual barotropic and baroclinic conversions. A new version for the\navailable-enthalpy cycle is proposed. It is suitable for open systems and it\nincludes explicitly the potential-energy component as a transitional reservoir.\nFinally, some results from Intensive Observing Period 15 of the Fronts and\nAtlantic Storm-Track EXperiment (FASTEX) are compared with those from the\nidealized diabatic experiment. \n\n"}
{"id": "1404.2911", "contents": "Title: Inferring structure in bipartite networks using the latent block model\n  and exact ICL Abstract: We consider the task of simultaneous clustering of the two node sets involved\nin a bipartite network. The approach we adopt is based on use of the exact\nintegrated complete likelihood for the latent block model. Using this allows\none to infer the number of clusters as well as cluster memberships using a\ngreedy search. This gives a model-based clustering of the node sets.\nExperiments on simulated bipartite network data show that the greedy search\napproach is vastly more scalable than competing Markov chain Monte Carlo based\nmethods. Application to a number of real observed bipartite networks\ndemonstrate the algorithms discussed. \n\n"}
{"id": "1404.4178", "contents": "Title: Speeding Up MCMC by Efficient Data Subsampling Abstract: We propose Subsampling MCMC, a Markov Chain Monte Carlo (MCMC) framework\nwhere the likelihood function for $n$ observations is estimated from a random\nsubset of $m$ observations. We introduce a highly efficient unbiased estimator\nof the log-likelihood based on control variates, such that the computing cost\nis much smaller than that of the full log-likelihood in standard MCMC. The\nlikelihood estimate is bias-corrected and used in two dependent pseudo-marginal\nalgorithms to sample from a perturbed posterior, for which we derive the\nasymptotic error with respect to $n$ and $m$, respectively. We propose a\npractical estimator of the error and show that the error is negligible even for\na very small $m$ in our applications. We demonstrate that Subsampling MCMC is\nsubstantially more efficient than standard MCMC in terms of sampling efficiency\nfor a given computational budget, and that it outperforms other subsampling\nmethods for MCMC proposed in the literature. \n\n"}
{"id": "1404.6473", "contents": "Title: Quantifying Uncertainty in Random Forests via Confidence Intervals and\n  Hypothesis Tests Abstract: This work develops formal statistical inference procedures for machine\nlearning ensemble methods. Ensemble methods based on bootstrapping, such as\nbagging and random forests, have improved the predictive accuracy of individual\ntrees, but fail to provide a framework in which distributional results can be\neasily determined. Instead of aggregating full bootstrap samples, we consider\npredicting by averaging over trees built on subsamples of the training set and\ndemonstrate that the resulting estimator takes the form of a U-statistic. As\nsuch, predictions for individual feature vectors are asymptotically normal,\nallowing for confidence intervals to accompany predictions. In practice, a\nsubset of subsamples is used for computational speed; here our estimators take\nthe form of incomplete U-statistics and equivalent results are derived. We\nfurther demonstrate that this setup provides a framework for testing the\nsignificance of features. Moreover, the internal estimation method we develop\nallows us to estimate the variance parameters and perform these inference\nprocedures at no additional computational cost. Simulations and illustrations\non a real dataset are provided. \n\n"}
{"id": "1404.6909", "contents": "Title: Establishing some order amongst exact approximations of MCMCs Abstract: Exact approximations of Markov chain Monte Carlo (MCMC) algorithms are a\ngeneral emerging class of sampling algorithms. One of the main ideas behind\nexact approximations consists of replacing intractable quantities required to\nrun standard MCMC algorithms, such as the target probability density in a\nMetropolis-Hastings algorithm, with estimators. Perhaps surprisingly, such\napproximations lead to powerful algorithms which are exact in the sense that\nthey are guaranteed to have correct limiting distributions. In this paper we\ndiscover a general framework which allows one to compare, or order, performance\nmeasures of two implementations of such algorithms. In particular, we establish\nan order with respect to the mean acceptance probability, the first\nautocorrelation coefficient, the asymptotic variance and the right spectral\ngap. The key notion to guarantee the ordering is that of the convex order\nbetween estimators used to implement the algorithms. We believe that our convex\norder condition is close to optimal, and this is supported by a counter-example\nwhich shows that a weaker variance order is not sufficient. The convex order\nplays a central role by allowing us to construct a martingale coupling which\nenables the comparison of performance measures of Markov chain with differing\ninvariant distributions, contrary to existing results. We detail applications\nof our result by identifying extremal distributions within given classes of\napproximations, by showing that averaging replicas improves performance in a\nmonotonic fashion and that stratification is guaranteed to improve performance\nfor the standard implementation of the Approximate Bayesian Computation (ABC)\nMCMC method. \n\n"}
{"id": "1404.7188", "contents": "Title: Limitations of polynomial chaos expansions in the Bayesian solution of\n  inverse problems Abstract: Polynomial chaos expansions are used to reduce the computational cost in the\nBayesian solutions of inverse problems by creating a surrogate posterior that\ncan be evaluated inexpensively. We show, by analysis and example, that when the\ndata contain significant information beyond what is assumed in the prior, the\nsurrogate posterior can be very different from the posterior, and the resulting\nestimates become inaccurate. One can improve the accuracy by adaptively\nincreasing the order of the polynomial chaos, but the cost may increase too\nfast for this to be cost effective compared to Monte Carlo sampling without a\nsurrogate posterior. \n\n"}
{"id": "1405.0377", "contents": "Title: Hypothesis Testing for Parsimonious Gaussian Mixture Models Abstract: Gaussian mixture models with eigen-decomposed covariance structures make up\nthe most popular family of mixture models for clustering and classification,\ni.e., the Gaussian parsimonious clustering models (GPCM). Although the GPCM\nfamily has been used for almost 20 years, selecting the best member of the\nfamily in a given situation remains a troublesome problem. Likelihood ratio\ntests are developed to tackle this problems. These likelihood ratio tests use\nthe heteroscedastic model under the alternative hypothesis but provide much\nmore flexibility and real-world applicability than previous approaches that\ncompare the homoscedastic Gaussian mixture versus the heteroscedastic one.\nAlong the way, a novel maximum likelihood estimation procedure is developed for\ntwo members of the GPCM family. Simulations show that the $\\chi^2$ reference\ndistribution gives reasonable approximation for the LR statistics only when the\nsample size is considerable and when the mixture components are well separated;\naccordingly, following Lo (2008), a parametric bootstrap is adopted.\nFurthermore, by generalizing the idea of Greselin and Punzo (2013) to the\nclustering context, a closed testing procedure, having the defined likelihood\nratio tests as local tests, is introduced to assess a unique model in the\ngeneral family. The advantages of this likelihood ratio testing procedure are\nillustrated via an application to the well-known Iris data set. \n\n"}
{"id": "1405.1792", "contents": "Title: RAPTT: An Exact Two-Sample Test in High Dimensions Using Random\n  Projections Abstract: In high dimensions, the classical Hotelling's $T^2$ test tends to have low\npower or becomes undefined due to singularity of the sample covariance matrix.\nIn this paper, this problem is overcome by projecting the data matrix onto\nlower dimensional subspaces through multiplication by random matrices. We\npropose RAPTT (RAndom Projection T-Test), an exact test for equality of means\nof two normal populations based on projected lower dimensional data. RAPTT does\nnot require any constraints on the dimension of the data or the sample size. A\nsimulation study indicates that in high dimensions the power of this test is\noften greater than that of competing tests. The advantage of RAPTT is\nillustrated on high-dimensional gene expression data involving the\ndiscrimination of tumor and normal colon tissues. \n\n"}
{"id": "1405.4081", "contents": "Title: Sequential Monte Carlo with Highly Informative Observations Abstract: We propose sequential Monte Carlo (SMC) methods for sampling the posterior\ndistribution of state-space models under highly informative observation\nregimes, a situation in which standard SMC methods can perform poorly. A\nspecial case is simulating bridges between given initial and final values. The\nbasic idea is to introduce a schedule of intermediate weighting and resampling\ntimes between observation times, which guide particles towards the final state.\nThis can always be done for continuous-time models, and may be done for\ndiscrete-time models under sparse observation regimes; our main focus is on\ncontinuous-time diffusion processes. The methods are broadly applicable in that\nthey support multivariate models with partial observation, do not require\nsimulation of the backward transition (which is often unavailable), and, where\npossible, avoid pointwise evaluation of the forward transition. When simulating\nbridges, the last cannot be avoided entirely without concessions, and we\nsuggest an epsilon-ball approach (reminiscent of Approximate Bayesian\nComputation) as a workaround. Compared to the bootstrap particle filter, the\nnew methods deliver substantially reduced mean squared error in normalising\nconstant estimates, even after accounting for execution time. The methods are\ndemonstrated for state estimation with two toy examples, and for parameter\nestimation (within a particle marginal Metropolis--Hastings sampler) with three\napplied examples in econometrics, epidemiology and marine biogeochemistry. \n\n"}
{"id": "1405.4141", "contents": "Title: Classification using log Gaussian Cox processes Abstract: McCullagh and Yang (2006) suggest a family of classification algorithms based\non Cox processes. We further investigate the log Gaussian variant which has a\nnumber of appealing properties. Conditioned on the covariates, the distribution\nover labels is given by a type of conditional Markov random field. In the\nsupervised case, computation of the predictive probability of a single test\npoint scales linearly with the number of training points and the multiclass\ngeneralization is straightforward. We show new links between the supervised\nmethod and classical nonparametric methods. We give a detailed analysis of the\npairwise graph representable Markov random field, which we use to extend the\nmodel to semi-supervised learning problems, and propose an inference method\nbased on graph min-cuts. We give the first experimental analysis on supervised\nand semi-supervised datasets and show good empirical performance. \n\n"}
{"id": "1405.4525", "contents": "Title: Bootstrap-based model selection criteria for beta regressions Abstract: The Akaike information criterion (AIC) is a model selection criterion widely\nused in practical applications. The AIC is an estimator of the log-likelihood\nexpected value, and measures the discrepancy between the true model and the\nestimated model. In small samples the AIC is biased and tends to select\noverparameterized models. To circumvent that problem, we propose two new\nselection criteria, namely: the bootstrapped likelihood quasi-CV (BQCV) and its\n632QCV variant. We use Monte Carlo simulation to compare the finite sample\nperformances of the two proposed criteria to those of the AIC and its\nvariations that use the bootstrapped log-likelihood in the class of varying\ndispersion beta regressions. The numerical evidence shows that the proposed\nmodel selection criteria perform well in small samples. We also present and\ndiscuss and empirical application. \n\n"}
{"id": "1405.7091", "contents": "Title: Bayesian hierarchical modelling for inferring genetic interactions in\n  yeast Abstract: Identifying genetic interactions for a given microorganism such as yeast is\ndifficult. Quantitative Fitness Analysis (QFA) is a high-throughput\nexperimental and computational methodology for quantifying the fitness of\nmicrobial cultures. QFA can be used to compare between fitness observations for\ndifferent genotypes and thereby infer genetic interaction strengths. Current\n\"naive\" frequentist statistical approaches used in QFA do not model\nbetween-genotype variation or difference in genotype variation under different\nconditions. In this thesis, a Bayesian approach is introduced to evaluate\nhierarchical models that better reflect the structure or design of QFA\nexperiments. First, a two-stage approach is presented: a hierarchical logistic\nmodel is fitted to microbial culture growth curves and then a hierarchical\ninteraction model is fitted to fitness summaries inferred for each genotype.\nNext, a one-stage Bayesian approach is presented: a joint hierarchical model\nwhich does not require a univariate summary of fitness, used to pass\ninformation between models. The new hierarchical approaches are then compared\nusing a dataset examining the effect of telomere defects on yeast. By better\ndescribing the experimental structure, new evidence is found for genes and\ncomplexes which interact with the telomere cap. Various extensions of these\nmodels, including models for data transformation, batch effects, and\nintrinsically stochastic growth models are also considered. \n\n"}
{"id": "1405.7551", "contents": "Title: Origin of atmospheric aerosols at the Pierre Auger Observatory using\n  studies of air mass trajectories in South America Abstract: The Pierre Auger Observatory is making significant contributions towards\nunderstanding the nature and origin of ultra-high energy cosmic rays. One of\nits main challenges is the monitoring of the atmosphere, both in terms of its\nstate variables and its optical properties. The aim of this work is to analyze\naerosol optical depth $\\tau_{\\rm a}(z)$ values measured from 2004 to 2012 at\nthe observatory, which is located in a remote and relatively unstudied area of\nthe Pampa Amarilla, Argentina. The aerosol optical depth is in average quite\nlow - annual mean $\\tau_{\\rm a}(3.5~{\\rm km})\\sim 0.04$ - and shows a seasonal\ntrend with a winter minimum - $\\tau_{\\rm a}(3.5~{\\rm km})\\sim 0.03$ -, and a\nsummer maximum - $\\tau_{\\rm a}(3.5~{\\rm km})\\sim 0.06$ -, and an unexpected\nincrease from August to September - $\\tau_{\\rm a}(3.5~{\\rm km})\\sim 0.055$). We\ncomputed backward trajectories for the years 2005 to 2012 to interpret the air\nmass origin. Winter nights with low aerosol concentrations show air masses\noriginating from the Pacific Ocean. Average concentrations are affected by\ncontinental sources (wind-blown dust and urban pollution), while the peak\nobserved in September and October could be linked to biomass burning in the\nnorthern part of Argentina or air pollution coming from surrounding urban\nareas. \n\n"}
{"id": "1406.1533", "contents": "Title: Continuous Data Assimilation with Stochastically Noisy Data Abstract: We analyze the performance of a data-assimilation algorithm based on a linear\nfeedback control when used with observational data that contains measurement\nerrors. Our model problem consists of dynamics governed by the two-dimension\nincompressible Navier-Stokes equations, observational measurements given by\nfinite volume elements or nodal points of the velocity field and measurement\nerrors which are represented by stochastic noise. Under these assumptions, the\ndata-assimilation algorithm consists of a system of stochastically forced\nNavier-Stokes equations. The main result of this paper provides explicit\nconditions on the observation density (resolution) which guarantee explicit\nasymptotic bounds, as the time tends to infinity, on the error between the\napproximate solution and the actual solutions which is corresponding to these\nmeasurements, in terms of the variance of the noise in the measurements.\nSpecifically, such bounds are given for the the limit supremum, as the time\ntends to infinity, of the expected value of the $L^2$-norm and of the $H^1$\nSobolev norm of the difference between the approximating solution and the\nactual solution. Moreover, results on the average time error in mean are\nstated. \n\n"}
{"id": "1406.5550", "contents": "Title: ViDaExpert: user-friendly tool for nonlinear visualization and analysis\n  of multidimensional vectorial data Abstract: ViDaExpert is a tool for visualization and analysis of multidimensional\nvectorial data. ViDaExpert is able to work with data tables of \"object-feature\"\ntype that might contain numerical feature values as well as textual labels for\nrows (objects) and columns (features). ViDaExpert implements several\nstatistical methods such as standard and weighted Principal Component Analysis\n(PCA) and the method of elastic maps (non-linear version of PCA), Linear\nDiscriminant Analysis (LDA), multilinear regression, K-Means clustering, a\nvariant of decision tree construction algorithm. Equipped with several\nuser-friendly dialogs for configuring data point representations (size, shape,\ncolor) and fast 3D viewer, ViDaExpert is a handy tool allowing to construct an\ninteractive 3D-scene representing a table of data in multidimensional space and\nperform its quick and insightfull statistical analysis, from basic to advanced\nmethods. \n\n"}
{"id": "1406.6010", "contents": "Title: Forest resampling for distributed sequential Monte Carlo Abstract: This paper brings explicit considerations of distributed computing\narchitectures and data structures into the rigorous design of Sequential Monte\nCarlo (SMC) methods. A theoretical result established recently by the authors\nshows that adapting interaction between particles to suitably control the\nEffective Sample Size (ESS) is sufficient to guarantee stability of SMC\nalgorithms. Our objective is to leverage this result and devise algorithms\nwhich are thus guaranteed to work well in a distributed setting. We make three\nmain contributions to achieve this. Firstly, we study mathematical properties\nof the ESS as a function of matrices and graphs that parameterize the\ninteraction amongst particles. Secondly, we show how these graphs can be\ninduced by tree data structures which model the logical network topology of an\nabstract distributed computing environment. Thirdly, we present efficient\ndistributed algorithms that achieve the desired ESS control, perform resampling\nand operate on forests associated with these trees. \n\n"}
{"id": "1406.7648", "contents": "Title: Bayesian Network Constraint-Based Structure Learning Algorithms:\n  Parallel and Optimised Implementations in the bnlearn R Package Abstract: It is well known in the literature that the problem of learning the structure\nof Bayesian networks is very hard to tackle: its computational complexity is\nsuper-exponential in the number of nodes in the worst case and polynomial in\nmost real-world scenarios.\n  Efficient implementations of score-based structure learning benefit from past\nand current research in optimisation theory, which can be adapted to the task\nby using the network score as the objective function to maximise. This is not\ntrue for approaches based on conditional independence tests, called\nconstraint-based learning algorithms. The only optimisation in widespread use,\nbacktracking, leverages the symmetries implied by the definitions of\nneighbourhood and Markov blanket.\n  In this paper we illustrate how backtracking is implemented in recent\nversions of the bnlearn R package, and how it degrades the stability of\nBayesian network structure learning for little gain in terms of speed. As an\nalternative, we describe a software architecture and framework that can be used\nto parallelise constraint-based structure learning algorithms (also implemented\nin bnlearn) and we demonstrate its performance using four reference networks\nand two real-world data sets from genetics and systems biology. We show that on\nmodern multi-core or multiprocessor hardware parallel implementations are\npreferable over backtracking, which was developed when single-processor\nmachines were the norm. \n\n"}
{"id": "1407.1774", "contents": "Title: gamboostLSS: An R Package for Model Building and Variable Selection in\n  the GAMLSS Framework Abstract: Generalized additive models for location, scale and shape (GAMLSS) are a\nflexible class of regression models that allow to model multiple parameters of\na distribution function, such as the mean and the standard deviation,\nsimultaneously. With the R package gamboostLSS, we provide a boosting method to\nfit these models. Variable selection and model choice are naturally available\nwithin this regularized regression framework. To introduce and illustrate the R\npackage gamboostLSS and its infrastructure, we use a data set on stunted growth\nin India. In addition to the specification and application of the model itself,\nwe present a variety of convenience functions, including methods for tuning\nparameter selection, prediction and visualization of results. The package\ngamboostLSS is available from CRAN\n(http://cran.r-project.org/package=gamboostLSS). \n\n"}
{"id": "1407.2864", "contents": "Title: Asynchronous Anytime Sequential Monte Carlo Abstract: We introduce a new sequential Monte Carlo algorithm we call the particle\ncascade. The particle cascade is an asynchronous, anytime alternative to\ntraditional particle filtering algorithms. It uses no barrier synchronizations\nwhich leads to improved particle throughput and memory efficiency. It is an\nanytime algorithm in the sense that it can be run forever to emit an unbounded\nnumber of particles while keeping within a fixed memory budget. We prove that\nthe particle cascade is an unbiased marginal likelihood estimator which means\nthat it can be straightforwardly plugged into existing pseudomarginal methods. \n\n"}
{"id": "1407.4916", "contents": "Title: Extensions of stability selection using subsamples of observations and\n  covariates Abstract: We introduce extensions of stability selection, a method to stabilise\nvariable selection methods introduced by Meinshausen and B\\\"uhlmann (J R Stat\nSoc 72:417-473, 2010). We propose to apply a base selection method repeatedly\nto random observation subsamples and covariate subsets under scrutiny, and to\nselect covariates based on their selection frequency. We analyse the effects\nand benefits of these extensions. Our analysis generalizes the theoretical\nresults of Meinshausen and B\\\"uhlmann (J R Stat Soc 72:417-473, 2010) from the\ncase of half-samples to subsamples of arbitrary size. We study, in a\ntheoretical manner, the effect of taking random covariate subsets using a\nsimplified score model. Finally we validate these extensions on numerical\nexperiments on both synthetic and real datasets, and compare the obtained\nresults in detail to the original stability selection method. \n\n"}
{"id": "1407.5581", "contents": "Title: Wind and Wave Extremes over the World Oceans from Very Large Ensembles Abstract: Global return values of marine wind speed and significant wave height are\nestimated from very large aggregates of archived ensemble forecasts at +240-h\nlead time. Long lead time ensures that the forecasts represent independent\ndraws from the model climate. Compared with ERA-Interim, a reanalysis, the\nensemble yields higher return estimates for both wind speed and significant\nwave height. Confidence intervals are much tighter due to the large size of the\ndataset. The period (9 yrs) is short enough to be considered stationary even\nwith climate change. Furthermore, the ensemble is large enough for\nnon-parametric 100-yr return estimates to be made from order statistics. These\ndirect return estimates compare well with extreme value estimates outside areas\nwith tropical cyclones. Like any method employing modeled fields, it is\nsensitive to tail biases in the numerical model, but we find that the biases\nare moderate outside areas with tropical cyclones. \n\n"}
{"id": "1408.0241", "contents": "Title: Linear and Conic Programming Estimators in High-Dimensional\n  Errors-in-variables Models Abstract: We consider the linear regression model with observation error in the design.\nIn this setting, we allow the number of covariates to be much larger than the\nsample size. Several new estimation methods have been recently introduced for\nthis model. Indeed, the standard Lasso estimator or Dantzig selector turn out\nto become unreliable when only noisy regressors are available, which is quite\ncommon in practice. We show in this work that under suitable sparsity\nassumptions, the procedure introduced in Rosenbaum and Tsybakov (2013) is\nalmost optimal in a minimax sense and, despite non-convexities, can be\nefficiently computed by a single linear programming problem. Furthermore, we\nprovide an estimator attaining the minimax efficiency bound. This estimator is\nwritten as a second order cone programming minimisation problem which can be\nsolved numerically in polynomial time. \n\n"}
{"id": "1408.0365", "contents": "Title: A new SATIRE-S spectral solar irradiance reconstruction for solar cycles\n  21--23 and its implications for stratospheric ozone Abstract: We present a revised and extended total and spectral solar irradiance (SSI)\nreconstruction, which includes a wavelength-dependent uncertainty estimate,\nspanning the last three solar cycles using the SATIRE-S model. The SSI\nreconstruction covers wavelengths between 115 and 160,000 nm and all dates\nbetween August 1974 and October 2009. This represents the first full-wavelength\nSATIRE-S reconstruction to cover the last three solar cycles without data gaps\nand with an uncertainty estimate. SATIRE-S is compared with the NRLSSI model\nand SORCE/SOLSTICE ultraviolet (UV) observations. SATIRE-S displays similar\ncycle behaviour to NRLSSI for wavelengths below 242 nm and almost twice the\nvariability between 242 and 310 nm. During the decline of last solar cycle,\nbetween 2003 and 2008, SSI from SORCE/SOLSTICE version 12 and 10 typically\ndisplays more than three times the variability of SATIRE-S between 200 and 300\nnm. All three datasets are used to model changes in stratospheric ozone within\na 2D atmospheric model for a decline from high solar activity to solar minimum.\nThe different flux changes result in different modelled ozone trends. Using\nNRLSSI leads to a decline in mesospheric ozone, while SATIRE-S and\nSORCE/SOLSTICE result in an increase. Recent publications have highlighted\nincreases in mesospheric ozone when considering version 10 SORCE/SOLSTICE\nirradiances. The recalibrated SORCE/SOLSTICE version 12 irradiances result in a\nmuch smaller mesospheric ozone response than when using version 10 and now\nsimilar in magnitude to SATIRE-S. This shows that current knowledge of\nvariations in spectral irradiance is not sufficient to warrant robust\nconclusions concerning the impact of solar variability on the atmosphere and\nclimate. \n\n"}
{"id": "1408.1263", "contents": "Title: Comment on \"A test-tube model for rainfall\" by Wilkinson M., EPL 106\n  (2014) 40001 Abstract: This paper is a comment to M Wilkinson, EPL 106 (2014) 40001, arXiv:1401.4620\n[physics.ao-ph,cond-mat.soft], which draws conclusion from our data that are at\nvariance with our observations. \n\n"}
{"id": "1408.3058", "contents": "Title: Surface Shear and Persistent Wave Groups Abstract: We investigate the interaction of waves with surface flows by considering the\nfull set of conserved quantities, subtle but important surface elevation\nchanges induced by wave packets and by directly considering the necessary\nforces to prevent packet spreading in the deep water limit. Narrow surface\nshear flows are shown to exert strong localizing and stabilizing forces on\nwavepackets to maintain their strength and amplify their intensity even in the\nlinear regime. Subtle packet scale nonlinear elevation changes from wave motion\nare crucial here and it suggest that popular notions of wave stress and action\nare naive. Quantitative bounds on the surface shear flow necessary to stabilize\npackets of any wave amplitude are given. One implication of this mechanism is\nthat rogue wave stabilization must be due to a purely nonperturbative process. \n\n"}
{"id": "1408.3969", "contents": "Title: Efficient Exploration of Multi-Modal Posterior Distributions Abstract: The Markov Chain Monte Carlo (MCMC) algorithm is a widely recognised as an\nefficient method for sampling a specified posterior distribution. However, when\nthe posterior is multi-modal, conventional MCMC algorithms either tend to\nbecome stuck in one local mode, become non-Markovian or require an excessively\nlong time to explore the global properties of the distribution. We propose a\nnovel variant of MCMC, mixed MCMC, which exploits a specially designed proposal\ndensity to allow the generation candidate points from any of a number of\ndifferent modes. This new method is efficient by design, and is strictly\nMarkovian. We present our method and apply it to a toy model inference problem\nto demonstrate its validity. \n\n"}
{"id": "1408.4344", "contents": "Title: Optimal scaling for the pseudo-marginal random walk Metropolis:\n  insensitivity to the noise generating mechanism Abstract: We examine the optimal scaling and the efficiency of the pseudo-marginal\nrandom walk Metropolis algorithm using a recently-derived result on the\nlimiting efficiency as the dimension, $d\\rightarrow \\infty$. We prove that the\noptimal scaling for a given target varies by less than $20\\%$ across a wide\nrange of distributions for the noise in the estimate of the target, and that\nany scaling that is within $20\\%$ of the optimal one will be at least $70\\%$\nefficient. We demonstrate that this phenomenon occurs even outside the range of\ndistributions for which we rigorously prove it. We then conduct a simulation\nstudy on an example with $d=10$ where importance sampling is used to estimate\nthe target density; we also examine results available from an existing\nsimulations study with $d=5$ and where a particle filter was used. Our key\nconclusions are found to hold in these examples also. \n\n"}
{"id": "1408.4398", "contents": "Title: Deep-water gravity waves: theoretical estimating of wave parameters Abstract: This paper addresses deep-water gravity waves of finite amplitude generated\nby an initial disturbance to the water. It is assumed that the horizontal\ndimensions of the initially disturbed body of the water are much larger than\nthe magnitude of the free surface displacement in the origin of the waves.\nInitially the free surface has not yet been displaced from its equilibrium\nposition, but the velocity field has already become different from zero. This\nmeans that the water at rest initially is set in motion suddenly by an impulse.\nDuration of formation of the wave origin and the maximum water elevation in the\norigin are estimated using the arrival times of the waves and the maximum\nwave-heights at certain locations obtained from gauge records at the locations,\nand the distances between the centre of the origin and each of the locations.\nFor points situated at a long distance from the wave origin, forecast is made\nfor the travel time and wave height at the points. The forecast is based on the\ndata recorded by the gauges at the locations. \n\n"}
{"id": "1408.6482", "contents": "Title: Conservation Laws and Bounds on the Efficiency of Wind-Wave Growth Abstract: We examine two means by which wind can impart energy to waves: sheltering and\ndeposition of material upwards from windward surface shear. The shear driven\ndeposition is shown to be the more efficient process. Lengthening of waves to\nmatch the wind speed is shown to be very inefficient and consume a large\nfraction of the energy imparted by the wind. The surface shear provides a low\nenergy sink that absorbs most of the momentum from the wind. These produce\nbounds on the efficiency of wave growth. The results here are computed in a\nmodel independent and perturbation free fashion by a careful consideration of\nconservation laws. By combining these effects we can place bounds on the rates\nwaves can grow in a given fetch and the relative amount of shear flow versus\nthe, relatively small, Stokes drift that must arise. \n\n"}
{"id": "1409.0074", "contents": "Title: Speeding up neighborhood search in local Gaussian process prediction Abstract: Recent implementations of local approximate Gaussian process models have\npushed computational boundaries for non-linear, non-parametric prediction\nproblems, particularly when deployed as emulators for computer experiments.\nTheir flavor of spatially independent computation accommodates massive\nparallelization, meaning that they can handle designs two or more orders of\nmagnitude larger than previously. However, accomplishing that feat can still\nrequire massive supercomputing resources. Here we aim to ease that burden. We\nstudy how predictive variance is reduced as local designs are built up for\nprediction. We then observe how the exhaustive and discrete nature of an\nimportant search subroutine involved in building such local designs may be\noverly conservative. Rather, we suggest that searching the space radially,\ni.e., continuously along rays emanating from the predictive location of\ninterest, is a far thriftier alternative. Our empirical work demonstrates that\nray-based search yields predictors with accuracy comparable to exhaustive\nsearch, but in a fraction of the time - bringing a supercomputer implementation\nback onto the desktop. \n\n"}
{"id": "1409.2817", "contents": "Title: Ribbon Turbulence Abstract: We investigate the non-linear equilibration of a two-layer quasi-geostrophic\nflow in a channel forced by an imposed unstable zonal mean flow, paying\nparticular attention to the role of bottom friction. In the limit of low bottom\nfriction, classical theory of geostrophic turbulence predicts an inverse\ncascade of kinetic energy in the horizontal with condensation at the domain\nscale and barotropization on the vertical. By contrast, in the limit of large\nbottom friction, the flow is dominated by ribbons of high kinetic energy in the\nupper layer. These ribbons correspond to meandering jets separating regions of\nhomogenized potential vorticity. We interpret these result by taking advantage\nof the peculiar conservation laws satisfied by this system: the dynamics can be\nrecast in such a way that the imposed mean flow appears as an initial source of\npotential vorticity levels in the upper layer. The initial baroclinic\ninstability leads to a turbulent flow that stirs this potential vorticity field\nwhile conserving the global distribution of potential vorticity levels.\nStatistical mechanical theory of the 1-1/2 layer quasi-geostrophic model\npredict the formation of two regions of homogenized potential vorticity\nseparated by a minimal interface. We show that the dynamics of the ribbons\nresults from a competition between a tendency to reach this equilibrium state,\nand baroclinic instability that induces meanders of the interface. These\nmeanders intermittently break and induce potential vorticity mixing, but the\ninterface remains sharp throughout the flow evolution. We show that for some\nparameter regimes, the ribbons act as a mixing barrier which prevent relaxation\ntoward equilibrium, favouring the emergence of multiple zonal jets. \n\n"}
{"id": "1409.3353", "contents": "Title: Validation of Danish wind time series from a new global renewable energy\n  atlas for energy system analysis Abstract: We present a new high-resolution global renewable energy atlas ({REatlas})\nthat can be used to calculate customised hourly time series of wind and solar\nPV power generation. In this paper, the atlas is applied to produce\n32-year-long hourly model wind power time series for Denmark for each\nhistorical and future year between 1980 and 2035. These are calibrated and\nvalidated against real production data from the period 2000 to 2010. The high\nnumber of years allows us to discuss how the characteristics of Danish wind\npower generation varies between individual weather years. As an example, the\nannual energy production is found to vary by $\\pm10\\%$ from the average.\nFurthermore, we show how the production pattern change as small onshore\nturbines are gradually replaced by large onshore and offshore turbines.\nFinally, we compare our wind power time series for 2020 to corresponding data\nfrom a handful of Danish energy system models. The aim is to illustrate how\ncurrent differences in model wind may result in significant differences in\ntechnical and economical model predictions. These include up to $15\\%$\ndifferences in installed capacity and $40\\%$ differences in system reserve\nrequirements. \n\n"}
{"id": "1409.4334", "contents": "Title: Hidden Equilibration Driven Losses in Whitecapping Abstract: The role of whitecapping losses of waves is investigated in a simple model\nbased on conservation laws. It is shown that, for Airy waves, at least as much\nenergy is lost in gradual reequilibration as is lost in the whitecapping events\nthemselves. This model is based on the the notion that the waves and losses are\nsmall enough that some narrow spectrum of frequencies reappears over time. \n\n"}
{"id": "1409.4362", "contents": "Title: Bayesian inference for Markov jump processes with informative\n  observations Abstract: In this paper we consider the problem of parameter inference for Markov jump\nprocess (MJP) representations of stochastic kinetic models. Since transition\nprobabilities are intractable for most processes of interest yet forward\nsimulation is straightforward, Bayesian inference typically proceeds through\ncomputationally intensive methods such as (particle) MCMC. Such methods\nostensibly require the ability to simulate trajectories from the conditioned\njump process. When observations are highly informative, use of the forward\nsimulator is likely to be inefficient and may even preclude an exact\n(simulation based) analysis. We therefore propose three methods for improving\nthe efficiency of simulating conditioned jump processes. A conditioned hazard\nis derived based on an approximation to the jump process, and used to generate\nend-point conditioned trajectories for use inside an importance sampling\nalgorithm. We also adapt a recently proposed sequential Monte Carlo scheme to\nour problem. Essentially, trajectories are reweighted at a set of intermediate\ntime points, with more weight assigned to trajectories that are consistent with\nthe next observation. We consider two implementations of this approach, based\non two continuous approximations of the MJP. We compare these constructs for a\nsimple tractable jump process before using them to perform inference for a\nLotka-Volterra system. The best performing construct is used to infer the\nparameters governing a simple model of motility regulation in Bacillus\nsubtilis. \n\n"}
{"id": "1409.4735", "contents": "Title: Vertically Driven Waves: Energy Transfer Between Gravity Waves Revisited Abstract: We investigate the energy transfer from large waves to small ones through\nvertical acceleration and demonstrate that this is a much larger effect than\nthat of the potential energy changes of the small waves moving over the larger\nones. Rates of exponential growth for this process are given and limits on the\nstable size of small waves in the horizontal accelerations from the larger ones\nare derived. We discuss the possibility of this being a manifestation of the\nBenjamin-Feir instability. \n\n"}
{"id": "1409.5419", "contents": "Title: Footprints and footprint analysis for atmospheric dispersion problems Abstract: Footprint analysis, also known as the study of Influence areas, is a first\norder method for solving inverse atmospheric dispersion problems. We revisit\nthe concept of footprints giving a rigorous definition of the concept (denoted\nposterior footprints and posterior zero footprints) in terms of spatio-temporal\ndomains. The notion of footprints is then augmented the to the forward\ndispersion problem by defining prior footprints and prior zero footprints. We\nthen study how posterior footprints and posterior zero footprints can be\ncombined to reveal more information about the source, and how prior footprints\nand prior footprints can be combined to yield more information about the\nmeasurements. \n\n"}
{"id": "1409.7715", "contents": "Title: Parameter inference and model selection in deterministic and stochastic\n  dynamical models via approximate Bayesian computation: modeling a wildlife\n  epidemic Abstract: We consider the problem of selecting deterministic or stochastic models for a\nbiological, ecological, or environmental dynamical process. In most cases, one\nprefers either deterministic or stochastic models as candidate models based on\nexperience or subjective judgment. Due to the complex or intractable likelihood\nin most dynamical models, likelihood-based approaches for model selection are\nnot suitable. We use approximate Bayesian computation for parameter estimation\nand model selection to gain further understanding of the dynamics of two\nepidemics of chronic wasting disease in mule deer. The main novel contribution\nof this work is that under a hierarchical model framework we compare three\ntypes of dynamical models: ordinary differential equation, continuous time\nMarkov chain, and stochastic differential equation models. To our knowledge\nmodel selection between these types of models has not appeared previously.\nSince the practice of incorporating dynamical models into data models is\nbecoming more common, the proposed approach may be very useful in a variety of\napplications. \n\n"}
{"id": "1410.0524", "contents": "Title: Likelihood free inference for Markov processes: a comparison Abstract: Approaches to Bayesian inference for problems with intractable likelihoods\nhave become increasingly important in recent years. Approximate Bayesian\ncomputation (ABC) and \"likelihood free\" Markov chain Monte Carlo techniques are\npopular methods for tackling inference in these scenarios but such techniques\nare computationally expensive. In this paper we compare the two approaches to\ninference, with a particular focus on parameter inference for stochastic\nkinetic models, widely used in systems biology. Discrete time transition\nkernels for models of this type are intractable for all but the most trivial\nsystems yet forward simulation is usually straightforward. We discuss the\nrelative merits and drawbacks of each approach whilst considering the\ncomputational cost implications and efficiency of these techniques. In order to\nexplore the properties of each approach we examine a range of observation\nregimes using two example models. We use a Lotka--Volterra predator prey model\nto explore the impact of full or partial species observations using various\ntime course observations under the assumption of known and unknown measurement\nerror. Further investigation into the impact of observation error is then made\nusing a Schl\\\"ogl system, a test case which exhibits bi-modal state stability\nin some regions of parameter space. \n\n"}
{"id": "1410.1101", "contents": "Title: Sequential Monte Carlo Samplers for capital allocation under\n  copula-dependent risk models Abstract: In this paper we assume a multivariate risk model has been developed for a\nportfolio and its capital derived as a homogeneous risk measure. The Euler (or\ngradient) principle, then, states that the capital to be allocated to each\ncomponent of the portfolio has to be calculated as an expectation conditional\nto a rare event, which can be challenging to evaluate in practice. We exploit\nthe copula-dependence within the portfolio risks to design a Sequential Monte\nCarlo Samplers based estimate to the marginal conditional expectations involved\nin the problem, showing its efficiency through a series of computational\nexamples. \n\n"}
{"id": "1410.1173", "contents": "Title: Robust Orthogonal Complement Principal Component Analysis Abstract: Recently, the robustification of principal component analysis has attracted\nlots of attention from statisticians, engineers and computer scientists. In\nthis work we study the type of outliers that are not necessarily apparent in\nthe original observation space but can seriously affect the principal subspace\nestimation. Based on a mathematical formulation of such transformed outliers, a\nnovel robust orthogonal complement principal component analysis (ROC-PCA) is\nproposed. The framework combines the popular sparsity-enforcing and low rank\nregularization techniques to deal with row-wise outliers as well as\nelement-wise outliers. A non-asymptotic oracle inequality guarantees the\naccuracy and high breakdown performance of ROC-PCA in finite samples. To tackle\nthe computational challenges, an efficient algorithm is developed on the basis\nof Stiefel manifold optimization and iterative thresholding. Furthermore, a\nbatch variant is proposed to significantly reduce the cost in ultra high\ndimensions. The paper also points out a pitfall of a common practice of SVD\nreduction in robust PCA. Experiments show the effectiveness and efficiency of\nROC-PCA in both synthetic and real data. \n\n"}
{"id": "1410.2344", "contents": "Title: Lagrangian analysis of the vertical structure of eddies simulated in the\n  Japan Basin of the Japan/East Sea Abstract: The output from an eddy-resolved multi-layered circulation model is used to\nanalyze the vertical structure of simulated deep-sea eddies in the Japan Basin\nof the Japan/East Sea constrained by bottom topography. We focus on Lagrangian\nanalysis of anticyclonic eddies, generated in the model in a typical year\napproximately at the place of the mooring and the hydrographic sections, where\nsuch eddies have been regularly observed in different years (1993--1997,\n1999--2001). Using a quasi-3D computation of the finite-time Lyapunov exponents\nand displacements for a large number of synthetic tracers in each depth layer,\nwe demonstrate how the simulated feature evolves of the eddy, that does not\nreach the surface in summer, into a one reaching the surface in fall. This\nfinding is confirmed by computing deformation of the model layers across the\nsimulated eddy in zonal and meridional directions and in the corresponding\ntemperature cross sections. Computed Lagrangian tracking maps allow to trace\nthe origin and fate of water masses in different layers of the eddy. The\nresults of simulation are compared with observed temperature zonal and\nmeridional cross sections of a real anticyclonic eddy to be studied at that\nplace during the oceanographic Conductivity, Temperature, and Depth (CTD)\nhydrochemical survey in summer 1999. Both the simulated and observed eddies are\nshown to have the similar eddy core and the relief of layer interfaces and\nisotherms. \n\n"}
{"id": "1410.4070", "contents": "Title: Modulational instability in wind-forced waves Abstract: We consider the wind-forced nonlinear Schroedinger (NLS) equation obtained in\nthe potential flow framework when the Miles growth rate is of the order of the\nwave steepness. In this case, the form of the wind-forcing terms gives rise to\nthe enhancement of the modulational instability and to a band of positive gain\nwith infinite width. This regime is characterised by the fact that the ratio\nbetween wave momentum and norm is not a constant of motion, in contrast to what\nhappens in the standard case where the Miles growth rate is of the order of the\nsteepness squared. \n\n"}
{"id": "1410.4217", "contents": "Title: Sequential Importance Sampling for Two-dimensional Ising Models Abstract: In recent years, sequential importance sampling (SIS) has been well developed\nfor sampling contingency tables with linear constraints. In this paper, we\napply SIS procedure to 2-dimensional Ising models, which give observations of\n0-1 tables and include both linear and quadratic constraints. We show how to\ncompute bounds for specific cells by solving linear programming (LP) problems\nover cut polytopes to reduce rejections. The computational results, which\nincludes both simulations and real data analysis, suggest that our method\nperforms very well for sparse tables and when the 1's are spread out: the\ncomputational times are short, the acceptance rates are high, and if proper\ntests are used then in most cases our conclusions are theoretically reasonable. \n\n"}
{"id": "1410.4231", "contents": "Title: Convergence properties of weighted particle islands with application to\n  the double bootstrap algorithm Abstract: Particle island models (Verg\\'e et al., 2013) provide a means of\nparallelization of sequential Monte Carlo methods, and in this paper we present\nnovel convergence results for algorithms of this sort. In particular we\nestablish a central limit theorem - as the number of islands and the common\nsize of the islands tend jointly to infinity - of the double bootstrap\nalgorithm with possibly adaptive selection on the island level. For this\npurpose we introduce a notion of archipelagos of weighted islands and find\nconditions under which a set of convergence properties are preserved by\ndifferent operations on such archipelagos. This theory allows arbitrary\ncompositions of these operations to be straightforwardly analyzed, providing a\nvery flexible framework covering the double bootstrap algorithm as a special\ncase. Finally, we establish the long-term numerical stability of the double\nbootstrap algorithm by bounding its asymptotic variance under weak and easily\nchecked assumptions satisfied for a wide range of models with possibly\nnon-compact state space. \n\n"}
{"id": "1410.4755", "contents": "Title: Power-Law Noises over General Spatial Domains and on Non-Standard Meshes Abstract: Power-law noises abound in nature and have been observed extensively in both\ntime series and spatially varying environmental parameters. Although, recent\nyears have seen the extension of traditional stochastic partial differential\nequations to include systems driven by fractional Brownian motion, spatially\ndistributed scale-invariance has received comparatively little attention,\nespecially for parameters defined over non-standard spatial domains. This paper\ndiscusses the generalization of power-law noises to general spatial domains by\noutlining their theoretical underpinnings as well as addressing their numerical\nsimulation on arbitrary meshes. Three computational algorithms are presented\nfor efficiently generating their sample paths, accompanied by numerous\nnumerical illustrations. \n\n"}
{"id": "1410.4812", "contents": "Title: Inference and Mixture Modeling with the Elliptical Gamma Distribution Abstract: We study modeling and inference with the Elliptical Gamma Distribution (EGD).\nWe consider maximum likelihood (ML) estimation for EGD scatter matrices, a task\nfor which we develop new fixed-point algorithms. Our algorithms are efficient\nand converge to global optima despite nonconvexity. Moreover, they turn out to\nbe much faster than both a well-known iterative algorithm of Kent & Tyler\n(1991) and sophisticated manifold optimization algorithms. Subsequently, we\ninvoke our ML algorithms as subroutines for estimating parameters of a mixture\nof EGDs. We illustrate our methods by applying them to model natural image\nstatistics---the proposed EGD mixture model yields the most parsimonious model\namong several competing approaches. \n\n"}
{"id": "1410.5392", "contents": "Title: Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling\n  for Gaussian Graphical Models Abstract: Motivated by a sampling problem basic to computational statistical inference,\nwe develop a nearly optimal algorithm for a fundamental problem in spectral\ngraph theory and numerical analysis. Given an $n\\times n$ SDDM matrix ${\\bf\n\\mathbf{M}}$, and a constant $-1 \\leq p \\leq 1$, our algorithm gives efficient\naccess to a sparse $n\\times n$ linear operator $\\tilde{\\mathbf{C}}$ such that\n$${\\mathbf{M}}^{p} \\approx \\tilde{\\mathbf{C}} \\tilde{\\mathbf{C}}^\\top.$$ The\nsolution is based on factoring ${\\bf \\mathbf{M}}$ into a product of simple and\nsparse matrices using squaring and spectral sparsification. For ${\\mathbf{M}}$\nwith $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, and\npolylogarithmic depth on a parallel machine with $m$ processors. This gives the\nfirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.\nrandom univariate Gaussian samples to generate i.i.d. random samples for\n$n$-dimensional Gaussian random fields with SDDM precision matrices. For\nsampling this natural subclass of Gaussian random fields, it is optimal in the\nrandomness and nearly optimal in the work and parallel complexity. In addition,\nour sampling algorithm can be directly extended to Gaussian random fields with\nSDD precision matrices. \n\n"}
{"id": "1410.5722", "contents": "Title: Optimisation of an idealised ocean model, stochastic parameterisation of\n  sub-grid eddies Abstract: An optimisation scheme is developed to accurately represent the sub-grid\nscale forcing of a high dimensional chaotic ocean system. Using a simple\nparameterisation scheme, the velocity components of a 30km resolution shallow\nwater ocean model are optimised to have the same climatological mean and\nvariance as that of a less viscous 7.5km resolution model. The 5 day\nlag-covariance is also optimised, leading to a more accurate estimate of the\nhigh resolution response to forcing using the low resolution model.\n  The system considered is an idealised barotropic double gyre that is chaotic\nat both resolutions. Using the optimisation scheme, we find and apply the\nconstant in time, but spatially varying, forcing term that is equal to the time\nintegrated forcing of the sub-mesoscale eddies. A linear stochastic term,\nindependent of the large-scale flow, with no spatial correlation but a\nspatially varying amplitude and time scale is used to represent the transient\neddies. The climatological mean, variance and 5 day lag-covariance of the\nvelocity from a single high resolution integration is used to provide an\noptimisation target. No other high resolution statistics are required.\nAdditional programming effort, for example to build a tangent linear or adjoint\nmodel, is not required either.\n  The focus of this paper is on the optimisation scheme and the accuracy of the\noptimised flow. The method can be applied in future investigations into the\nphysical processes that govern barotropic turbulence and it can perhaps be\napplied to help understand and correct biases in the mean and variance of a\nmore realistic coarse or eddy-permitting ocean model. The method is\ncomplementary to current parameterisations and can be applied at the same time\nwithout modification. \n\n"}
{"id": "1410.6460", "contents": "Title: Markov Chain Monte Carlo and Variational Inference: Bridging the Gap Abstract: Recent advances in stochastic gradient variational inference have made it\npossible to perform variational Bayesian inference with posterior\napproximations containing auxiliary random variables. This enables us to\nexplore a new synthesis of variational inference and Monte Carlo methods where\nwe incorporate one or more steps of MCMC into our variational approximation. By\ndoing so we obtain a rich class of inference algorithms bridging the gap\nbetween variational methods and MCMC, and offering the best of both worlds:\nfast posterior approximation through the maximization of an explicit objective,\nwith the option of trading off additional computation for additional accuracy.\nWe describe the theoretical foundations that make this possible and show some\npromising first results. \n\n"}
{"id": "1410.6466", "contents": "Title: Model Selection for Topic Models via Spectral Decomposition Abstract: Topic models have achieved significant successes in analyzing large-scale\ntext corpus. In practical applications, we are always confronted with the\nchallenge of model selection, i.e., how to appropriately set the number of\ntopics. Following recent advances in topic model inference via tensor\ndecomposition, we make a first attempt to provide theoretical analysis on model\nselection in latent Dirichlet allocation. Under mild conditions, we derive the\nupper bound and lower bound on the number of topics given a text collection of\nfinite size. Experimental results demonstrate that our bounds are accurate and\ntight. Furthermore, using Gaussian mixture model as an example, we show that\nour methodology can be easily generalized to model selection analysis for other\nlatent models. \n\n"}
{"id": "1411.0416", "contents": "Title: Spatio-Temporal Analysis of Epidemic Phenomena Using the R Package\n  surveillance Abstract: The availability of geocoded health data and the inherent temporal structure\nof communicable diseases have led to an increased interest in statistical\nmodels and software for spatio-temporal data with epidemic features. The open\nsource R package surveillance can handle various levels of aggregation at which\ninfective events have been recorded: individual-level time-stamped\ngeo-referenced data (case reports) in either continuous space or discrete\nspace, as well as counts aggregated by period and region. For each of these\ndata types, the surveillance package implements tools for visualization,\nlikelihoood inference and simulation from recently developed statistical\nregression frameworks capturing endemic and epidemic dynamics. Altogether, this\npaper is a guide to the spatio-temporal modeling of epidemic phenomena,\nexemplified by analyses of public health surveillance data on measles and\ninvasive meningococcal disease. \n\n"}
{"id": "1411.0560", "contents": "Title: Multivariate response and parsimony for Gaussian cluster-weighted models Abstract: A family of parsimonious Gaussian cluster-weighted models is presented. This\nfamily concerns a multivariate extension to cluster-weighted modelling that can\naccount for correlations between multivariate responses. Parsimony is attained\nby constraining parts of an eigen-decomposition imposed on the component\ncovariance matrices. A sufficient condition for identifiability is provided and\nan expectation-maximization algorithm is presented for parameter estimation.\nModel performance is investigated on both synthetic and classical real data\nsets and compared with some popular approaches. Finally, accounting for linear\ndependencies in the presence of a linear regression structure is shown to offer\nbetter performance, vis-\\`{a}-vis clustering, over existing methodologies. \n\n"}
{"id": "1411.1314", "contents": "Title: Computation of Gaussian orthant probabilities in high dimension Abstract: We study the computation of Gaussian orthant probabilities, i.e. the\nprobability that a Gaussian falls inside a quadrant. The\nGeweke-Hajivassiliou-Keane (GHK) algorithm [Genz, 1992; Geweke, 1991;\nHajivassiliou et al., 1996; Keane, 1993], is currently used for integrals of\ndimension greater than 10. In this paper we show that for Markovian covariances\nGHK can be interpreted as the estimator of the normalizing constant of a state\nspace model using sequential importance sampling (SIS). We show for an AR(1)\nthe variance of the GHK, properly normalized, diverges exponentially fast with\nthe dimension. As an improvement we propose using a particle filter (PF). We\nthen generalize this idea to arbitrary covariance matrices using Sequential\nMonte Carlo (SMC) with properly tailored MCMC moves. We show empirically that\nthis can lead to drastic improvements on currently used algorithms. We also\nextend the framework to orthants of mixture of Gaussians (Student, Cauchy\netc.), and to the simulation of truncated Gaussians. \n\n"}
{"id": "1411.1451", "contents": "Title: Modelling extremes using approximate Bayesian Computation Abstract: By the nature of their construction, many statistical models for extremes\nresult in likelihood functions that are computationally prohibitive to\nevaluate. This is consequently problematic for the purposes of likelihood-based\ninference. With a focus on the Bayesian framework, this chapter examines the\nuse of approximate Bayesian computation (ABC) techniques for the fitting and\nanalysis of statistical models for extremes. After introducing the ideas behind\nABC algorithms and methods, we demonstrate their application to extremal models\nin stereology and spatial extremes. \n\n"}
{"id": "1411.2256", "contents": "Title: Data-Driven Prediction of Thresholded Time Series of Rainfall and SOC\n  models Abstract: We study the occurrence of events, subject to threshold, in a representative\nSOC sandpile model and in high-resolution rainfall data. The predictability in\nboth systems is analyzed by means of a decision variable sensitive to event\nclustering, and the quality of the predictions is evaluated by the receiver\noperating characteristics (ROC) method. In the case of the SOC sandpile model,\nthe scaling of quiet-time distributions with increasing threshold leads to\nincreased predictability of extreme events. A scaling theory allows us to\nunderstand all the details of the prediction procedure and to extrapolate the\nshape of the ROC curves for the most extreme events. For rainfall data, the\nquiet-time distributions do not scale for high thresholds, which means that the\ncorresponding ROC curves cannot be straightforwardly related to those for lower\nthresholds. \n\n"}
{"id": "1411.3013", "contents": "Title: Bayesian Evidence and Model Selection Abstract: In this paper we review the concepts of Bayesian evidence and Bayes factors,\nalso known as log odds ratios, and their application to model selection. The\ntheory is presented along with a discussion of analytic, approximate and\nnumerical techniques. Specific attention is paid to the Laplace approximation,\nvariational Bayes, importance sampling, thermodynamic integration, and nested\nsampling and its recent variants. Analogies to statistical physics, from which\nmany of these techniques originate, are discussed in order to provide readers\nwith deeper insights that may lead to new techniques. The utility of Bayesian\nmodel testing in the domain sciences is demonstrated by presenting four\nspecific practical examples considered within the context of signal processing\nin the areas of signal detection, sensor characterization, scientific model\nselection and molecular force characterization. \n\n"}
{"id": "1411.5519", "contents": "Title: A plethora of generalised solitary gravity-capillary water waves Abstract: The present study describes, first, an efficient algorithm for computing\ncapillary-gravity solitary waves solutions of the irrotational Euler equations\nwith a free surface and, second, provides numerical evidences of the existence\nof an infinite number of generalised solitary waves (solitary waves with\nundamped oscillatory wings). Using conformal mapping, the unknown fluid domain,\nwhich is to be determined, is mapped into a uniform strip of the complex plane.\nIn the transformed domain, a Babenko-like equation is then derived and solved\nnumerically. \n\n"}
{"id": "1411.6370", "contents": "Title: Big Learning with Bayesian Methods Abstract: Explosive growth in data and availability of cheap computing resources have\nsparked increasing interest in Big learning, an emerging subfield that studies\nscalable machine learning algorithms, systems, and applications with Big Data.\nBayesian methods represent one important class of statistic methods for machine\nlearning, with substantial recent developments on adaptive, flexible and\nscalable Bayesian learning. This article provides a survey of the recent\nadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,\nincluding nonparametric Bayesian methods for adaptively inferring model\ncomplexity, regularized Bayesian inference for improving the flexibility via\nposterior regularization, and scalable algorithms and systems based on\nstochastic subsampling and distributed computing for dealing with large-scale\napplications. \n\n"}
{"id": "1411.6902", "contents": "Title: Most probable paths in temporal weighted networks: An application to\n  ocean transport Abstract: We consider paths in weighted and directed temporal networks, introducing\ntools to compute sets of paths of high probability. We quantify the relative\nimportance of the most probable path between two nodes with respect to the\nwhole set of paths, and to a subset of highly probable paths which incorporate\nmost of the connection probability. These concepts are used to provide\nalternative definitions of betweenness centrality. We apply our formalism to a\ntransport network describing surface flow in the Mediterranean sea. Despite the\nfull transport dynamics is described by a very large number of paths we find\nthat, for realistic time scales, only a very small subset of high probability\npaths (or even a single most probable one) is enough to characterize global\nconnectivity properties of the network. \n\n"}
{"id": "1411.6927", "contents": "Title: Exact computation of the halfspace depth Abstract: For computing the exact value of the halfspace depth of a point w.r.t. a data\ncloud of $n$ points in arbitrary dimension, a theoretical framework is\nsuggested. Based on this framework a whole class of algorithms can be derived.\nIn all of these algorithms the depth is calculated as the minimum over a finite\nnumber of depth values w.r.t. proper projections of the data cloud. Three\nvariants of this class are studied in more detail. All of these algorithms are\ncapable of dealing with data that are not in general position and even with\ndata that contain ties. As is shown by simulations, all proposed algorithms\nprove to be very efficient. \n\n"}
{"id": "1411.7235", "contents": "Title: Universality of Sea Wave Growth and Its Physical Roots Abstract: Modern day studies of wind-driven sea waves are usually focused on wind\nforcing rather than on the effect of resonant nonlinear wave interactions. The\nauthors assume that these effects are dominating and propose a simple\nrelationship between instant wave steepness and time or fetch of wave\ndevelopment expressed in wave periods or lengths. This law does not contain\nwind speed explicitly and relies upon this asymptotic theory. The validity of\nthis law is illustrated by results of numerical simulations, in situ\nmeasurements of growing wind seas and wind wave tank experiments. The impact of\nthe new vision of sea wave physics is discussed in the context of conventional\napproaches to wave modeling and forecasting. \n\n"}
{"id": "1412.0250", "contents": "Title: Discussion on the spectral coherence between planetary, solar and\n  climate oscillations: a reply to some critiques Abstract: During the last few years a number of works have proposed that planetary\nharmonics regulate solar oscillations and the Earth climate. Herein I address\nsome critiques. Detailed analysis of the data do support the planetary theory\nof solar and climate variation. In particular, I show that: (1) high-resolution\ncosmogenic 10Be and 14C solar activity proxy records both during the Holocene\nand during the Marine Interglacial Stage 9.3 (MIS 9.3), 325-336 kyr ago,\npresent four common spectral peaks at about 103, 115, 130 and 150 yrs (this is\nthe frequency band that generates Maunder and Dalton like grand solar minima)\nthat can be deduced from a simple solar model based on a generic non-linear\ncoupling between planetary and solar harmonics; (2) time-frequency analysis and\nadvanced minimum variance distortion-less response (MVDR) magnitude squared\ncoherence analysis confirm the existence of persistent astronomical harmonics\nin the climate records at the decadal and multidecadal scales when used with an\nappropriate window length (110 years) to guarantee a sufficient spectral\nresolution. However, the best coherence test can be currently made only by\ncomparing directly the temperature and astronomical spectra as done in Scafetta\n(J. Atmos. Sol. Terr. Phys. 72(13), 951-970, 2010). The spectral coherence\nbetween planetary, solar and climatic oscillations is confirmed at the\nfollowing periods: 5.2 yr, 5.93 yr, 6.62 yr, 7.42 yr, 9.1 yr (main lunar tidal\ncycle), 10.4 yr (related to the 9.93-10.87-11.86 yr solar cycle harmonics),\n13.8-15.0 yr, 20 yr, 30 yr and 61 yr, 103 yr, 115 yr, 130 yr, 150 yr and about\n1000 year. This work responds to the critiques of Cauquoin et al. (Astron.\nAstrophys. 561, A132, 2014) who ignored alternative planetary theories of solar\nvariations, and of Holm (J. Atmos. Sol. Terr. Phys. 110-111, 23-27, 2014) who\nused inadequate physical and time frequency analysis of the data. \n\n"}
{"id": "1412.2325", "contents": "Title: Could the Earth's surface Ultraviolet irradiance be blamed for the\n  global warming? (II) ----Ozone layer depth reconstruction via HEWV effect Abstract: It is suggested by Chen {\\it et al.} that the Earth's surface Ultraviolet\nirradiance ($280-400$ nm) could influence the Earth's surface temperature\nvariation by \"Highly Excited Water Vapor\" (HEWV) effect. In this manuscript, we\nreconstruct the developing history of the ozone layer depth variation from 1860\nto 2011 based on the HEWV effect. It is shown that the reconstructed ozone\nlayer depth variation correlates with the observational variation from 1958 to\n2005 very well ($R=0.8422$, $P>99.9\\%$). From this reconstruction, we may limit\nthe spectra band of the surface Ultraviolet irradiance referred in HEWV effect\nto Ultraviolet B ($280-320$ nm). \n\n"}
{"id": "1412.5250", "contents": "Title: High Dimensional Forecasting via Interpretable Vector Autoregression Abstract: Vector autoregression (VAR) is a fundamental tool for modeling multivariate\ntime series. However, as the number of component series is increased, the VAR\nmodel becomes overparameterized. Several authors have addressed this issue by\nincorporating regularized approaches, such as the lasso in VAR estimation.\nTraditional approaches address overparameterization by selecting a low lag\norder, based on the assumption of short range dependence, assuming that a\nuniversal lag order applies to all components. Such an approach constrains the\nrelationship between the components and impedes forecast performance. The\nlasso-based approaches work much better in high-dimensional situations but do\nnot incorporate the notion of lag order selection.\n  We propose a new class of hierarchical lag structures (HLag) that embed the\nnotion of lag selection into a convex regularizer. The key modeling tool is a\ngroup lasso with nested groups which guarantees that the sparsity pattern of\nlag coefficients honors the VAR's ordered structure. The HLag framework offers\nthree structures, which allow for varying levels of flexibility. A simulation\nstudy demonstrates improved performance in forecasting and lag order selection\nover previous approaches, and a macroeconomic application further highlights\nforecasting improvements as well as HLag's convenient, interpretable output. \n\n"}
{"id": "1412.7392", "contents": "Title: Theoretical guarantees for approximate sampling from smooth and\n  log-concave densities Abstract: Sampling from various kinds of distributions is an issue of paramount\nimportance in statistics since it is often the key ingredient for constructing\nestimators, test procedures or confidence intervals. In many situations, the\nexact sampling from a given distribution is impossible or computationally\nexpensive and, therefore, one needs to resort to approximate sampling\nstrategies. However, there is no well-developed theory providing meaningful\nnonasymptotic guarantees for the approximate sampling procedures, especially in\nthe high-dimensional problems. This paper makes some progress in this direction\nby considering the problem of sampling from a distribution having a smooth and\nlog-concave density defined on \\(\\RR^p\\), for some integer \\(p>0\\). We\nestablish nonasymptotic bounds for the error of approximating the target\ndistribution by the one obtained by the Langevin Monte Carlo method and its\nvariants. We illustrate the effectiveness of the established guarantees with\nvarious experiments. Underlying our analysis are insights from the theory of\ncontinuous-time diffusion processes, which may be of interest beyond the\nframework of log-concave densities considered in the present work. \n\n"}
{"id": "1501.00179", "contents": "Title: A persistence landscapes toolbox for topological statistics Abstract: Topological data analysis provides a multiscale description of the geometry\nand topology of quantitative data. The persistence landscape is a topological\nsummary that can be easily combined with tools from statistics and machine\nlearning. We give efficient algorithms for calculating persistence landscapes,\ntheir averages, and distances between such averages. We discuss an\nimplementation of these algorithms and some related procedures. These are\nintended to facilitate the combination of statistics and machine learning with\ntopological data analysis. We present an experiment showing that the\nlow-dimensional persistence landscapes of points sampled from spheres (and\nboxes) of varying dimensions differ. \n\n"}
{"id": "1501.00219", "contents": "Title: Spectral diagonal ensemble Kalman filters Abstract: A new type of ensemble Kalman filter is developed, which is based on\nreplacing the sample covariance in the analysis step by its diagonal in a\nspectral basis. It is proved that this technique improves the aproximation of\nthe covariance when the covariance itself is diagonal in the spectral basis, as\nis the case, e.g., for a second-order stationary random field and the Fourier\nbasis. The method is extended by wavelets to the case when the state variables\nare random fields, which are not spatially homogeneous. Efficient\nimplementations by the fast Fourier transform (FFT) and discrete wavelet\ntransform (DWT) are presented for several types of observations, including\nhigh-dimensional data given on a part of the domain, such as radar and\nsatellite images. Computational experiments confirm that the method performs\nwell on the Lorenz 96 problem and the shallow water equations with very small\nensembles and over multiple analysis cycles. \n\n"}
{"id": "1501.02248", "contents": "Title: A Particle Multi-Target Tracker for Superpositional Measurements using\n  Labeled Random Finite Sets Abstract: In this paper we present a general solution for multi-target tracking with\nsuperpositional measurements. Measurements that are functions of the sum of the\ncontributions of the targets present in the surveillance area are called\nsuperpositional measurements. We base our modelling on Labeled Random Finite\nSet (RFS) in order to jointly estimate the number of targets and their\ntrajectories. This modelling leads to a labeled version of Mahler's\nmulti-target Bayes filter. However, a straightforward implementation of this\ntracker using Sequential Monte Carlo (SMC) methods is not feasible due to the\ndifficulties of sampling in high dimensional spaces. We propose an efficient\nmulti-target sampling strategy based on Superpositional Approximate CPHD\n(SA-CPHD) filter and the recently introduced Labeled Multi-Bernoulli (LMB) and\nVo-Vo densities. The applicability of the proposed approach is verified through\nsimulation in a challenging radar application with closely spaced targets and\nlow signal-to-noise ratio. \n\n"}
{"id": "1501.02627", "contents": "Title: A fast numerical method for max-convolution and the application to\n  efficient max-product inference in Bayesian networks Abstract: Observations depending on sums of random variables are common throughout many\nfields; however, no efficient solution is currently known for performing\nmax-product inference on these sums of general discrete distributions\n(max-product inference can be used to obtain maximum a posteriori estimates).\nThe limiting step to max-product inference is the max-convolution problem\n(sometimes presented in log-transformed form and denoted as \"infimal\nconvolution\", \"min-convolution\", or \"convolution on the tropical semiring\"),\nfor which no O(k log(k)) method is currently known. Here I present a O(k\nlog(k)) numerical method for estimating the max-convolution of two nonnegative\nvectors (e.g., two probability mass functions), where k is the length of the\nlarger vector. This numerical max-convolution method is then demonstrated by\nperforming fast max-product inference on a convolution tree, a data structure\nfor performing fast inference given information on the sum of n discrete random\nvariables in O(n k log(n k) log(n) ) steps (where each random variable has an\narbitrary prior distribution on k contiguous possible states). The numerical\nmax-convolution method can be applied to specialized classes of hidden Markov\nmodels to reduce the runtime of computing the Viterbi path from n k^2 to n k\nlog(k), and has potential application to the all-pairs shortest paths problem. \n\n"}
{"id": "1501.03291", "contents": "Title: Bayesian Optimization for Likelihood-Free Inference of Simulator-Based\n  Statistical Models Abstract: Our paper deals with inferring simulator-based statistical models given some\nobserved data. A simulator-based model is a parametrized mechanism which\nspecifies how data are generated. It is thus also referred to as generative\nmodel. We assume that only a finite number of parameters are of interest and\nallow the generative process to be very general; it may be a noisy nonlinear\ndynamical system with an unrestricted number of hidden variables. This weak\nassumption is useful for devising realistic models but it renders statistical\ninference very difficult. The main challenge is the intractability of the\nlikelihood function. Several likelihood-free inference methods have been\nproposed which share the basic idea of identifying the parameters by finding\nvalues for which the discrepancy between simulated and observed data is small.\nA major obstacle to using these methods is their computational cost. The cost\nis largely due to the need to repeatedly simulate data sets and the lack of\nknowledge about how the parameters affect the discrepancy. We propose a\nstrategy which combines probabilistic modeling of the discrepancy with\noptimization to facilitate likelihood-free inference. The strategy is\nimplemented using Bayesian optimization and is shown to accelerate the\ninference through a reduction in the number of required simulations by several\norders of magnitude. \n\n"}
{"id": "1501.05144", "contents": "Title: Lazier ABC Abstract: ABC algorithms involve a large number of simulations from the model of\ninterest, which can be very computationally costly. This paper summarises the\nlazy ABC algorithm of Prangle (2015), which reduces the computational demand by\nabandoning many unpromising simulations before completion. By using a random\nstopping decision and reweighting the output sample appropriately, the target\ndistribution is the same as for standard ABC. Lazy ABC is also extended here to\nthe case of non-uniform ABC kernels, which is shown to simplify the process of\ntuning the algorithm effectively. \n\n"}
{"id": "1501.06093", "contents": "Title: An Efficient Approach for Optical Radiative Transfer Tomography using\n  the Spherical Harmonics Discrete Ordinates Method Abstract: This paper introduces a method to preform optical tomography, using 3D\nradiative transfer as the forward model. We use an iterative approach\npredicated on the Spherical Harmonics Discrete Ordinates Method (SHDOM) to\nsolve the optimization problem in a scalable manner. We illustrate with an\napplication in remote sensing of a cloudy atmosphere. \n\n"}
{"id": "1502.02536", "contents": "Title: Nested Sequential Monte Carlo Methods Abstract: We propose nested sequential Monte Carlo (NSMC), a methodology to sample from\nsequences of probability distributions, even where the random variables are\nhigh-dimensional. NSMC generalises the SMC framework by requiring only\napproximate, properly weighted, samples from the SMC proposal distribution,\nwhile still resulting in a correct SMC algorithm. Furthermore, NSMC can in\nitself be used to produce such properly weighted samples. Consequently, one\nNSMC sampler can be used to construct an efficient high-dimensional proposal\ndistribution for another NSMC sampler, and this nesting of the algorithm can be\ndone to an arbitrary degree. This allows us to consider complex and\nhigh-dimensional models using SMC. We show results that motivate the efficacy\nof our approach on several filtering problems with dimensions in the order of\n100 to 1 000. \n\n"}
{"id": "1502.03655", "contents": "Title: Newton-based maximum likelihood estimation in nonlinear state space\n  models Abstract: Maximum likelihood (ML) estimation using Newton's method in nonlinear state\nspace models (SSMs) is a challenging problem due to the analytical\nintractability of the log-likelihood and its gradient and Hessian. We estimate\nthe gradient and Hessian using Fisher's identity in combination with a\nsmoothing algorithm. We explore two approximations of the log-likelihood and of\nthe solution of the smoothing problem. The first is a linearization\napproximation which is computationally cheap, but the accuracy typically varies\nbetween models. The second is a sampling approximation which is asymptotically\nvalid for any SSM but is more computationally costly. We demonstrate our\napproach for ML parameter estimation on simulated data from two different SSMs\nwith encouraging results. \n\n"}
{"id": "1502.03656", "contents": "Title: Quasi-Newton particle Metropolis-Hastings Abstract: Particle Metropolis-Hastings enables Bayesian parameter inference in general\nnonlinear state space models (SSMs). However, in many implementations a random\nwalk proposal is used and this can result in poor mixing if not tuned correctly\nusing tedious pilot runs. Therefore, we consider a new proposal inspired by\nquasi-Newton algorithms that may achieve similar (or better) mixing with less\ntuning. An advantage compared to other Hessian based proposals, is that it only\nrequires estimates of the gradient of the log-posterior. A possible application\nis parameter inference in the challenging class of SSMs with intractable\nlikelihoods. We exemplify this application and the benefits of the new proposal\nby modelling log-returns of future contracts on coffee by a stochastic\nvolatility model with $\\alpha$-stable observations. \n\n"}
{"id": "1502.03697", "contents": "Title: Nonlinear state space smoothing using the conditional particle filter Abstract: To estimate the smoothing distribution in a nonlinear state space model, we\napply the conditional particle filter with ancestor sampling. This gives an\niterative algorithm in a Markov chain Monte Carlo fashion, with asymptotic\nconvergence results. The computational complexity is analyzed, and our proposed\nalgorithm is successfully applied to the challenging problem of sensor fusion\nbetween ultra-wideband and accelerometer/gyroscope measurements for indoor\npositioning. It appears to be a competitive alternative to existing nonlinear\nsmoothing algorithms, in particular the forward filtering-backward simulation\nsmoother. \n\n"}
{"id": "1502.03939", "contents": "Title: Polynomial-Chaos-based Kriging Abstract: Computer simulation has become the standard tool in many engineering fields\nfor designing and optimizing systems, as well as for assessing their\nreliability. To cope with demanding analysis such as optimization and\nreliability, surrogate models (a.k.a meta-models) have been increasingly\ninvestigated in the last decade. Polynomial Chaos Expansions (PCE) and Kriging\nare two popular non-intrusive meta-modelling techniques. PCE surrogates the\ncomputational model with a series of orthonormal polynomials in the input\nvariables where polynomials are chosen in coherency with the probability\ndistributions of those input variables. On the other hand, Kriging assumes that\nthe computer model behaves as a realization of a Gaussian random process whose\nparameters are estimated from the available computer runs, i.e. input vectors\nand response values. These two techniques have been developed more or less in\nparallel so far with little interaction between the researchers in the two\nfields. In this paper, PC-Kriging is derived as a new non-intrusive\nmeta-modeling approach combining PCE and Kriging. A sparse set of orthonormal\npolynomials (PCE) approximates the global behavior of the computational model\nwhereas Kriging manages the local variability of the model output. An adaptive\nalgorithm similar to the least angle regression algorithm determines the\noptimal sparse set of polynomials. PC-Kriging is validated on various benchmark\nanalytical functions which are easy to sample for reference results. From the\nnumerical investigations it is concluded that PC-Kriging performs better than\nor at least as good as the two distinct meta-modeling techniques. A larger gain\nin accuracy is obtained when the experimental design has a limited size, which\nis an asset when dealing with demanding computational models. \n\n"}
{"id": "1502.05255", "contents": "Title: Wavelet correlations to reveal multiscale coupling in geophysical\n  systems Abstract: The interactions between climate and the environment are highly complex. Due\nto this complexity, process-based models are often preferred to estimate the\nnet magnitude and directionality of interactions in the Earth System. However,\nthese models are based on simplifications of our understanding of nature, thus\nare unavoidably imperfect. Conversely, observation-based data of climatic and\nenvironmental variables are becoming increasingly accessible over large scales\ndue to the progress of space-borne sensing technologies and data-assimilation\ntechniques. Albeit uncertain, these data enable the possibility to start\nunraveling complex multivariable, multiscale relationships if the appropriate\nstatistical methods are applied.\n  Here, we investigate the potential of the wavelet cross-correlation method as\na tool for identifying multiscale interactions, feedback and regime shifts in\ngeophysical systems. The ability of wavelet cross-correlation to resolve the\nfast and slow components of coupled systems is tested on synthetic data of\nknown directionality, and then applied to observations to study one of the most\ncritical interactions between land and atmosphere: the coupling between soil\nmoisture and near-ground air temperature. Results show that our method is not\nonly able to capture the dynamics of the soil moisture-temperature coupling\nover a wide range of temporal scales (from days to several months) and climatic\nregimes (from wet to dry), but also to consistently identify the magnitude and\ndirectionality of the coupling. Consequently, wavelet cross-correlations are\npresented as a promising tool for the study of multiscale interactions, with\nthe potential of being extended to the analysis of causal relationships in the\nEarth system. \n\n"}
{"id": "1502.05503", "contents": "Title: Classification and Bayesian Optimization for Likelihood-Free Inference Abstract: Some statistical models are specified via a data generating process for which\nthe likelihood function cannot be computed in closed form. Standard\nlikelihood-based inference is then not feasible but the model parameters can be\ninferred by finding the values which yield simulated data that resemble the\nobserved data. This approach faces at least two major difficulties: The first\ndifficulty is the choice of the discrepancy measure which is used to judge\nwhether the simulated data resemble the observed data. The second difficulty is\nthe computationally efficient identification of regions in the parameter space\nwhere the discrepancy is low. We give here an introduction to our recent work\nwhere we tackle the two difficulties through classification and Bayesian\noptimization. \n\n"}
{"id": "1502.06069", "contents": "Title: Multilevel ensemble Kalman filtering Abstract: This work embeds a multilevel Monte Carlo sampling strategy into the Monte\nCarlo step of the ensemble Kalman filter (EnKF) in the setting of finite\ndimensional signal evolution and noisy discrete-time observations. The signal\ndynamics is assumed to be governed by a stochastic differential equation (SDE),\nand a hierarchy of time grids is introduced for multilevel numerical\nintegration of that SDE. The resulting multilevel EnKF is proved to\nasymptotically outperform EnKF in terms of computational cost versus\napproximation accuracy. The theoretical results are illustrated numerically. \n\n"}
{"id": "1503.00021", "contents": "Title: Mercer kernels and integrated variance experimental design: connections\n  between Gaussian process regression and polynomial approximation Abstract: This paper examines experimental design procedures used to develop surrogates\nof computational models, exploring the interplay between experimental designs\nand approximation algorithms. We focus on two widely used approximation\napproaches, Gaussian process (GP) regression and non-intrusive polynomial\napproximation. First, we introduce algorithms for minimizing a posterior\nintegrated variance (IVAR) design criterion for GP regression. Our formulation\ntreats design as a continuous optimization problem that can be solved with\ngradient-based methods on complex input domains, without resorting to greedy\napproximations. We show that minimizing IVAR in this way yields point sets with\ngood interpolation properties, and that it enables more accurate GP regression\nthan designs based on entropy minimization or mutual information maximization.\nSecond, using a Mercer kernel/eigenfunction perspective on GP regression, we\nidentify conditions under which GP regression coincides with pseudospectral\npolynomial approximation. Departures from these conditions can be understood as\nchanges either to the kernel or to the experimental design itself. We then show\nhow IVAR-optimal designs, while sacrificing discrete orthogonality of the\nkernel eigenfunctions, can yield lower approximation error than orthogonalizing\npoint sets. Finally, we compare the performance of adaptive Gaussian process\nregression and adaptive pseudospectral approximation for several classes of\ntarget functions, identifying features that are favorable to the GP + IVAR\napproach. \n\n"}
{"id": "1503.00847", "contents": "Title: The International Workshop on Wave Hindcasting and Forecasting and the\n  Coastal Hazards Symposium Abstract: Following the 13th International Workshop on Wave Hindcasting and Forecasting\nand 4th Coastal Hazards Symposium in October 2013 in Banff, Canada, a topical\ncollection has appeared in recent issues of Ocean Dynamics. Here we give a\nbrief overview of the history of the conference since its inception in 1986 and\nof the progress made in the fields of wind-generated ocean waves and the\nmodelling of coastal hazards before we summarize the main results of the papers\nthat have appeared in the topical collection. \n\n"}
{"id": "1503.01631", "contents": "Title: Application of Sequential Quasi-Monte Carlo to Autonomous Positioning Abstract: Sequential Monte Carlo algorithms (also known as particle filters) are\npopular methods to approximate filtering (and related) distributions of\nstate-space models. However, they converge at the slow $1/\\sqrt{N}$ rate, which\nmay be an issue in real-time data-intensive scenarios. We give a brief outline\nof SQMC (Sequential Quasi-Monte Carlo), a variant of SMC based on\nlow-discrepancy point sets proposed by Gerber and Chopin (2015), which\nconverges at a faster rate, and we illustrate the greater performance of SQMC\non autonomous positioning problems. \n\n"}
{"id": "1503.03626", "contents": "Title: Integral geometry for Markov chain Monte Carlo: overcoming the curse of\n  search-subspace dimensionality Abstract: We introduce a method that uses the Cauchy-Crofton formula and a new\ncurvature formula from integral geometry to reweight the sampling probabilities\nof Metropolis-within-Gibbs algorithms in order to increase their convergence\nspeed. We consider algorithms that sample from a probability density\nconditioned on a manifold $\\mathcal{M}$. Our method exploits the symmetries of\nthe algorithms' isotropic random search-direction subspaces to analytically\naverage out the variance in the intersection volume caused by the orientation\nof the search-subspace with respect to the manifold $\\mathcal{M}$ it\nintersects. This variance can grow exponentially with the dimension of the\nsearch-subspace, greatly slowing down the algorithm. Eliminating this variance\nallows us to use search-subspaces of dimensions many times greater than would\notherwise be possible, allowing us to sample very rare events that a\nlower-dimensional search-subspace would be unlikely to intersect.\n  To extend this method to events that are rare for reasons other than their\nsupport $\\mathcal{M}$ having a lower dimension, we formulate and prove a new\ntheorem in integral geometry that makes use of the curvature form of the\nChern-Gauss-Bonnet theorem to reweight sampling probabilities. On the side, we\nalso apply our theorem to obtain new theoretical bounds for the volumes of real\nalgebraic manifolds.\n  Finally, we demonstrate the computational effectiveness and speedup of our\nmethod by numerically applying it to the conditional stochastic Airy operator\nsampling problem in random matrix theory. \n\n"}
{"id": "1503.03806", "contents": "Title: HELIOS-K: An Ultrafast, Open-source Opacity Calculator for Radiative\n  Transfer Abstract: We present an ultrafast opacity calculator that we name HELIOS-K. It takes a\nline list as an input, computes the shape of each spectral line and provides an\noption for grouping an enormous number of lines into a manageable number of\nbins. We implement a combination of Algorithm 916 and Gauss-Hermite quadrature\nto compute the Voigt profile, write the code in CUDA and optimise the\ncomputation for graphics processing units (GPUs). We restate the theory of the\nk-distribution method and use it to reduce $\\sim 10^5$ to $10^8$ lines to $\\sim\n10$ to $10^4$ wavenumber bins, which may then be used for radiative transfer,\natmospheric retrieval and general circulation models. The choice of line-wing\ncutoff for the Voigt profile is a significant source of error and affects the\nvalue of the computed flux by $\\sim 10\\%$. This is an outstanding physical\n(rather than computational) problem, due to our incomplete knowledge of\npressure broadening of spectral lines in the far line wings. We emphasize that\nthis problem remains regardless of whether one performs line-by-line\ncalculations or uses the k-distribution method and affects all calculations of\nexoplanetary atmospheres requiring the use of wavelength-dependent opacities.\nWe elucidate the correlated-k approximation and demonstrate that it applies\nequally to inhomogeneous atmospheres with a single atomic/molecular species or\nhomogeneous atmospheres with multiple species. Using a NVIDIA K20 GPU, HELIOS-K\nis capable of computing an opacity function with $\\sim 10^5$ spectral lines in\n$\\sim 1$ second and is publicly available as part of the Exoclimes Simulation\nPlatform (ESP; www.exoclime.org). \n\n"}
{"id": "1503.04123", "contents": "Title: Perturbation theory for Markov chains via Wasserstein distance Abstract: Perturbation theory for Markov chains addresses the question how small\ndifferences in the transitions of Markov chains are reflected in differences\nbetween their distributions. We prove powerful and flexible bounds on the\ndistance of the $n$th step distributions of two Markov chains when one of them\nsatisfies a Wasserstein ergodicity condition. Our work is motivated by the\nrecent interest in approximate Markov chain Monte Carlo (MCMC) methods in the\nanalysis of big data sets. By using an approach based on Lyapunov functions, we\nprovide estimates for geometrically ergodic Markov chains under weak\nassumptions. In an autoregressive model, our bounds cannot be improved in\ngeneral. We illustrate our theory by showing quantitative estimates for\napproximate versions of two prominent MCMC algorithms, the Metropolis-Hastings\nand stochastic Langevin algorithms. \n\n"}
{"id": "1503.05210", "contents": "Title: Speeding up lower bound estimation in powerlaw distributions Abstract: The traditional lower bound estimation method for powerlaw distributions\nbased on the Kolmogorov-Smirnov distance proved to perform better than other\ncompeting methods. However, if applied to very large collections of data, such\na method can be computationally demanding. In this paper, we propose two\nalternative methods with the aim to reduce the time required by the estimation\nprocedure. We apply the traditional method and the two proposed methods to\nlarge collections of data ($N = 500,000$) with varying values of the true lower\nbound. Both the proposed methods yield a significantly better performance and\naccuracy than the traditional method. \n\n"}
{"id": "1503.06058", "contents": "Title: Sequential Monte Carlo Methods for System Identification Abstract: One of the key challenges in identifying nonlinear and possibly non-Gaussian\nstate space models (SSMs) is the intractability of estimating the system state.\nSequential Monte Carlo (SMC) methods, such as the particle filter (introduced\nmore than two decades ago), provide numerical solutions to the nonlinear state\nestimation problems arising in SSMs. When combined with additional\nidentification techniques, these algorithms provide solid solutions to the\nnonlinear system identification problem. We describe two general strategies for\ncreating such combinations and discuss why SMC is a natural tool for\nimplementing these strategies. \n\n"}
{"id": "1503.07091", "contents": "Title: On nonlinearity implications and wind forcing in Hasselmann equation Abstract: We discuss several experimental and theoretical techniques historically used\nfor Hasselmann equation wind input terms derivation. We show that recently\ndeveloped ZRP technique in conjunction with high-frequency damping without\nspectral peak dissipation allows to reproduce more than a dozen of\nfetch-limited field experiments. Numerical simulation of the same Cauchy\nproblem for different wind input terms has been performed to discuss\nnonlinearity implications as well as correspondence to theoretical predictions. \n\n"}
{"id": "1503.07307", "contents": "Title: Improving the INLA approach for approximate Bayesian inference for\n  latent Gaussian models Abstract: We introduce a new copula-based correction for generalized linear mixed\nmodels (GLMMs) within the integrated nested Laplace approximation (INLA)\napproach for approximate Bayesian inference for latent Gaussian models. While\nINLA is usually very accurate, some (rather extreme) cases of GLMMs with e.g.\nbinomial or Poisson data have been seen to be problematic. Inaccuracies can\noccur when there is a very low degree of smoothing or \"borrowing strength\"\nwithin the model, and we have therefore developed a correction aiming to push\nthe boundaries of the applicability of INLA. Our new correction has been\nimplemented as part of the R-INLA package, and adds only negligible\ncomputational cost. Empirical evaluations on both real and simulated data\nindicate that the method works well. \n\n"}
{"id": "1503.07498", "contents": "Title: Zonal Flow as Pattern Formation Abstract: In this section, we examine the transition from statistically homogeneous\nturbulence to inhomogeneous turbulence with zonal flows. Statistical equations\nof motion can be derived from the quasilinear approximation to the\nHasegawa-Mima equation. We review recent work that finds a bifurcation of these\nequations and shows that the emergence of zonal flows mathematically follows a\nstandard type of pattern formation. We also show that the dispersion relation\nof modulational instability can be extracted from the statistical equations of\nmotion in a certain limit. The statistical formulation can thus be thought to\noffer a more general perspective on growth of coherent structures, namely\nthrough instability of a full turbulent spectrum. Finally, we offer a physical\nperspective on the growth of large-scale structures. \n\n"}
{"id": "1503.07677", "contents": "Title: Surface Wave Effects in the NEMO Ocean Model: Forced and Coupled\n  Experiments Abstract: The NEMO general circulation ocean model is extended to incorporate three\nphysical processes related to ocean surface waves, namely the surface stress\n(modified by growth and dissipation of the oceanic wave field), the turbulent\nkinetic energy flux from breaking waves, and the Stokes-Coriolis force.\nExperiments are done with NEMO in ocean-only (forced) mode and coupled to the\nECMWF atmospheric and wave models. Ocean-only integrations are forced with\nfields from the ERA-Interim reanalysis. All three effects are noticeable in the\nextra-tropics, but the sea-state dependent turbulent kinetic energy flux yields\nby far the largest difference. This is partly because the control run has too\nvigorous deep mixing due to an empirical mixing term in NEMO. We investigate\nthe relation between this ad hoc mixing and Langmuir turbulence and find that\nit is much more effective than the Langmuir parameterization used in NEMO. The\nbiases in sea surface temperature as well as subsurface temperature are\nreduced, and the total ocean heat content exhibits a trend closer to that\nobserved in a recent ocean reanalysis (ORAS4) when wave effects are included.\nSeasonal integrations of the coupled atmosphere-wave-ocean model consisting of\nNEMO, the wave model ECWAM and the atmospheric model of ECMWF similarly show\nthat the sea surface temperature biases are greatly reduced when the mixing is\ncontrolled by the sea state and properly weighted by the thickness of the\nuppermost level of the ocean model. These wave-related physical processes were\nrecently implemented in the operational coupled ensemble forecast system of\nECMWF. \n\n"}
{"id": "1503.08066", "contents": "Title: Scalable Bayesian Inference for the Inverse Temperature of a Hidden\n  Potts Model Abstract: The inverse temperature parameter of the Potts model governs the strength of\nspatial cohesion and therefore has a major influence over the resulting model\nfit. A difficulty arises from the dependence of an intractable normalising\nconstant on the value of this parameter and thus there is no closed-form\nsolution for sampling from the posterior distribution directly. There are a\nvariety of computational approaches for sampling from the posterior without\nevaluating the normalising constant, including the exchange algorithm and\napproximate Bayesian computation (ABC). A serious drawback of these algorithms\nis that they do not scale well for models with a large state space, such as\nimages with a million or more pixels. We introduce a parametric surrogate\nmodel, which approximates the score function using an integral curve. Our\nsurrogate model incorporates known properties of the likelihood, such as\nheteroskedasticity and critical temperature. We demonstrate this method using\nsynthetic data as well as remotely-sensed imagery from the Landsat-8 satellite.\nWe achieve up to a hundredfold improvement in the elapsed runtime, compared to\nthe exchange algorithm or ABC. An open source implementation of our algorithm\nis available in the R package \"bayesImageS.\" \n\n"}
{"id": "1503.08420", "contents": "Title: Line Strengths of Rovibrational and Rotational Transitions in the\n  X$^2\\Pi$ Ground State of OH Abstract: A new line list including positions and absolute intensities (in the form of\nEinstein $A$ values and oscillator strengths) has been produced for the OH\nground X\\DP\\ state rovibrational (Meinel system) and pure rotational\ntransitions. All possible transitions are included with v$\\primed$ and\nv$\\Dprimed$ up to 13, and $J$ up to between 9.5 and 59.5, depending on the\nband. An updated fit to determine molecular constants has been performed, which\nincludes some new rotational data and a simultaneous fitting of all molecular\nconstants. The absolute line intensities are based on a new dipole moment\nfunction, which is a combination of two high level ab initio calculations. The\ncalculations show good agreement with an experimental v=1 lifetime,\nexperimental $\\mu_\\mathrm{v}$ values, and $\\Delta$v=2 line intensity ratios\nfrom an observed spectrum. To achieve this good agreement, an alteration in the\nmethod of converting matrix elements from Hund's case (b) to (a) was made.\nPartitions sums have been calculated using the new energy levels, for the\ntemperature range 5-6000 K, which extends the previously available (in HITRAN)\n70-3000 K range. The resulting absolute intensities have been used to calculate\nO abundances in the Sun, Arcturus, and two red giants in the Galactic open and\nglobular clusters M67 and M71. Literature data based mainly on [O I] lines are\navailable for the Sun and Arcturus, and excellent agreement is found. \n\n"}
{"id": "1504.01161", "contents": "Title: Python bindings for libcloudph++ Abstract: This technical note introduces the Python bindings for libcloudph++. The\nlibcloudph++ is a C++ library of algorithms for representing atmospheric cloud\nmicrophysics in numerical models. The bindings expose the complete\nfunctionality of the library to the Python users. The bindings are implemented\nusing the Boost.Python C++ library and use NumPy arrays. This note includes\nlistings with Python scripts exemplifying the use of selected library\ncomponents. An example solution for using the Python bindings to access\nlibcloudph++ from Fortran is presented. \n\n"}
{"id": "1504.01418", "contents": "Title: Precomputing Strategy for Hamiltonian Monte Carlo Method Based on\n  Regularity in Parameter Space Abstract: Markov Chain Monte Carlo (MCMC) algorithms play an important role in\nstatistical inference problems dealing with intractable probability\ndistributions. Recently, many MCMC algorithms such as Hamiltonian Monte Carlo\n(HMC) and Riemannian Manifold HMC have been proposed to provide distant\nproposals with high acceptance rate. These algorithms, however, tend to be\ncomputationally intensive which could limit their usefulness, especially for\nbig data problems due to repetitive evaluations of functions and statistical\nquantities that depend on the data. This issue occurs in many statistic\ncomputing problems. In this paper, we propose a novel strategy that exploits\nsmoothness (regularity) of parameter space to improve computational efficiency\nof MCMC algorithms. When evaluation of functions or statistical quantities are\nneeded at a point in parameter space, interpolation from precomputed values or\nprevious computed values is used. More specifically, we focus on Hamiltonian\nMonte Carlo (HMC) algorithms that use geometric information for faster\nexploration of probability distributions. Our proposed method is based on\nprecomputing the required geometric information on a set of grids before\nrunning sampling information at nearby grids at each iteration of HMC. Sparse\ngrid interpolation method is used for high dimensional problems. Tests on\ncomputational examples are shown to illustrate the advantages of our method. \n\n"}
{"id": "1504.02661", "contents": "Title: Stochastic determination of matrix determinants Abstract: Matrix determinants play an important role in data analysis, in particular\nwhen Gaussian processes are involved. Due to currently exploding data volumes,\nlinear operations - matrices - acting on the data are often not accessible\ndirectly but are only represented indirectly in form of a computer routine.\nSuch a routine implements the transformation a data vector undergoes under\nmatrix multiplication. While efficient probing routines to estimate a matrix's\ndiagonal or trace, based solely on such computationally affordable\nmatrix-vector multiplications, are well known and frequently used in signal\ninference, there is no stochastic estimate for its determinant. We introduce a\nprobing method for the logarithm of a determinant of a linear operator. Our\nmethod rests upon a reformulation of the log-determinant by an integral\nrepresentation and the transformation of the involved terms into stochastic\nexpressions. This stochastic determinant determination enables large-size\napplications in Bayesian inference, in particular evidence calculations, model\ncomparison, and posterior determination. \n\n"}
{"id": "1504.04696", "contents": "Title: On estimation of the diagonal elements of a sparse precision matrix Abstract: In this paper, we present several estimators of the diagonal elements of the\ninverse of the covariance matrix, called precision matrix, of a sample of iid\nrandom vectors. The focus is on high dimensional vectors having a sparse\nprecision matrix. It is now well understood that when the underlying\ndistribution is Gaussian, the columns of the precision matrix can be estimated\nindependently form one another by solving linear regression problems under\nsparsity constraints. This approach leads to a computationally efficient\nstrategy for estimating the precision matrix that starts by estimating the\nregression vectors, then estimates the diagonal entries of the precision matrix\nand, in a final step, combines these estimators for getting estimators of the\noff-diagonal entries. While the step of estimating the regression vector has\nbeen intensively studied over the past decade, the problem of deriving\nstatistically accurate estimators of the diagonal entries has received much\nless attention. The goal of the present paper is to fill this gap by presenting\nfour estimators---that seem the most natural ones---of the diagonal entries of\nthe precision matrix and then performing a comprehensive empirical evaluation\nof these estimators. The estimators under consideration are the residual\nvariance, the relaxed maximum likelihood, the symmetry-enforced maximum\nlikelihood and the penalized maximum likelihood. We show, both theoretically\nand empirically, that when the aforementioned regression vectors are estimated\nwithout error, the symmetry-enforced maximum likelihood estimator has the\nsmallest estimation error. However, in a more realistic setting when the\nregression vector is estimated by a sparsity-favoring computationally efficient\nmethod, the qualities of the estimators become relatively comparable with a\nslight advantage for the residual variance estimator. \n\n"}
{"id": "1504.04722", "contents": "Title: On Robustness of the Shiryaev-Roberts Procedure for Quickest\n  Change-Point Detection under Parameter Misspecification in the Post-Change\n  Distribution Abstract: The gist of the quickest change-point detection problem is to detect the\npresence of a change in the statistical behavior of a series of sequentially\nmade observations, and do so in an optimal\ndetection-speed-vs.-\"false-positive\"-risk manner. When optimality is understood\neither in the generalized Bayesian sense or as defined in Shiryaev's\nmulti-cyclic setup, the so-called Shiryaev-Roberts (SR) detection procedure is\nknown to be the \"best one can do\", provided, however, that the observations'\npre- and post-change distributions are both fully specified. We consider a more\nrealistic setup, viz. one where the post-change distribution is assumed known\nonly up to a parameter, so that the latter may be \"misspecified\". The question\nof interest is the sensitivity (or robustness) of the otherwise \"best\" SR\nprocedure with respect to a possible misspecification of the post-change\ndistribution parameter. To answer this question, we provide a case study where,\nin a specific Gaussian scenario, we allow the SR procedure to be \"out of tune\"\nin the way of the post-change distribution parameter, and numerically assess\nthe effect of the \"mistuning\" on Shiryaev's (multi-cyclic) Stationary Average\nDetection Delay delivered by the SR procedure. The comprehensive quantitative\nrobustness characterization of the SR procedure obtained in the study can be\nused to develop the respective theory as well as to provide a rational for\npractical design of the SR procedure. The overall qualitative conclusion of the\nstudy is an expected one: the SR procedure is less (more) robust for less\n(more) contrast changes and for lower (higher) levels of the false alarm risk. \n\n"}
{"id": "1504.05285", "contents": "Title: Global well-posedness of strong solutions to a tropical climate model Abstract: In this paper, we consider the Cauchy problem to the TROPIC CLIMATE MODEL\nderived by Frierson-Majda-Pauluis in [Comm. Math. Sci, Vol. 2 (2004)] which is\na coupled system of the barotropic and the first baroclinic modes of the\nvelocity and the typical midtropospheric temperature. The system considered in\nthis paper has viscosities in the momentum equations, but no diffusivity in the\ntemperature equation. We establish here the global well-posedness of strong\nsolutions to this model. In proving the global existence of strong solutions,\nto overcome the difficulty caused by the absence of the diffusivity in the\ntemperature equation, we introduce a new velocity $w$ (called the pseudo\nbaroclinic velocity), which has more regularities than the original baroclinic\nmode of the velocity. An auxiliary function $\\phi$, which looks like the\neffective viscous flux for the compressible Navier-Stokes equations, is also\nintroduced to obtain the $L^\\infty$ bound of the temperature. Regarding the\nuniqueness, we use the idea of performing suitable energy estimates at level\none order lower than the natural basic energy estimates for the system. \n\n"}
{"id": "1504.07235", "contents": "Title: Sign Stable Random Projections for Large-Scale Learning Abstract: We study the use of \"sign $\\alpha$-stable random projections\" (where\n$0<\\alpha\\leq 2$) for building basic data processing tools in the context of\nlarge-scale machine learning applications (e.g., classification, regression,\nclustering, and near-neighbor search). After the processing by sign stable\nrandom projections, the inner products of the processed data approximate\nvarious types of nonlinear kernels depending on the value of $\\alpha$. Thus,\nthis approach provides an effective strategy for approximating nonlinear\nlearning algorithms essentially at the cost of linear learning. When $\\alpha\n=2$, it is known that the corresponding nonlinear kernel is the arc-cosine\nkernel. When $\\alpha=1$, the procedure approximates the arc-cos-$\\chi^2$ kernel\n(under certain condition). When $\\alpha\\rightarrow0+$, it corresponds to the\nresemblance kernel.\n  From practitioners' perspective, the method of sign $\\alpha$-stable random\nprojections is ready to be tested for large-scale learning applications, where\n$\\alpha$ can be simply viewed as a tuning parameter. What is missing in the\nliterature is an extensive empirical study to show the effectiveness of sign\nstable random projections, especially for $\\alpha\\neq 2$ or 1. The paper\nsupplies such a study on a wide variety of classification datasets. In\nparticular, we compare shoulder-by-shoulder sign stable random projections with\nthe recently proposed \"0-bit consistent weighted sampling (CWS)\" (Li 2015). \n\n"}
{"id": "1505.01434", "contents": "Title: Particle Gibbs algorithms for Markov jump processes Abstract: In the present paper we propose a new MCMC algorithm for sampling from the\nposterior distribution of hidden trajectory of a Markov jump process. Our\nalgorithm is based on the idea of exploiting virtual jumps, introduced by Rao\nand Teh (2013). The main novelty is that our algorithm uses particle Gibbs with\nancestor sampling to update the skeleton, while Rao and Teh use forward\nfiltering backward sampling (FFBS). In contrast to previous methods our\nalgorithm can be implemented even if the state space is infinite. In addition,\nthe cost of a single step of the proposed algorithm does not depend on the size\nof the state space. The computational cost of our methood is of order\n$\\mathcal{O}(N\\mathbb{E}(n))$, where $N$ is the number of particles used in the\nPGAS algorithm and $\\mathbb{E}(n)$ is the expected number of jumps (together\nwith virtual ones). The cost of the algorithm of Rao and Teh is of order\n$\\mathcal{O}(|\\mathcal{X}|^2\\mathbb{E}(n))$, where $|\\mathcal{X}|$ is the size\nof the state space. Simulation results show that our algorithm with PGAS\nconverges slightly slower than the algorithm with FFBS, if the size of the\nstate space is not big. However, if the size of the state space increases, the\nproposed method outperforms existing ones. We give special attention to a\nhierarchical version of our algorithm which can be applied to continuous time\nBayesian networks (CTBNs). \n\n"}
{"id": "1505.02679", "contents": "Title: Heat engines and heat pumps in a hydrostatic atmosphere: How surface\n  pressure and temperature constrain wind power output and circulation cell\n  size Abstract: The kinetic energy budget of the atmosphere's meridional circulation cells is\nanalytically assessed. In the upper atmosphere kinetic energy generation grows\nwith increasing surface temperature difference \\$\\Delta T_s\\$ between the cold\nand warm ends of a circulation cell; in the lower atmosphere it declines. A\nrequirement that kinetic energy generation is positive in the lower atmosphere\nlimits the poleward cell extension \\$L\\$ of Hadley cells via a relationship\nbetween \\$\\Delta T_s\\$ and surface pressure difference \\$\\Delta p_s\\$: an upper\nlimit exists when \\$\\Delta p_s\\$ does not grow with increasing \\$\\Delta T_s\\$.\nThis pattern is demonstrated here using monthly data from MERRA re-analysis.\nKinetic energy generation along air streamlines in the boundary layer does not\nexceed \\$40\\$~J~mol\\$^{-1}\\$; it declines with growing \\$L\\$ and reaches zero\nfor the largest observed \\$L\\$ at 2~km height. The limited meridional cell size\nnecessitates the appearance of heat pumps -- circulation cells with negative\nwork output where the low-level air moves towards colder areas. These cells\nconsume the positive work output of the heat engines -- cells where the\nlow-level air moves towards the warmer areas -- and can in theory drive the\nglobal efficiency of atmospheric circulation down to zero. Relative\ncontributions of \\$\\Delta p_s\\$ and \\$\\Delta T_s\\$ to kinetic energy generation\nare evaluated: \\$\\Delta T_s\\$ dominates in the upper atmosphere, while \\$\\Delta\np_s\\$ dominates in the lower. Analysis and empirical evidence indicate that the\nnet kinetic power output on Earth is dominated by surface pressure gradients,\nwith minor net kinetic energy generation in the upper atmosphere. The role of\ncondensation in generating surface pressure gradients is discussed. \n\n"}
{"id": "1505.02827", "contents": "Title: On Markov chain Monte Carlo methods for tall data Abstract: Markov chain Monte Carlo methods are often deemed too computationally\nintensive to be of any practical use for big data applications, and in\nparticular for inference on datasets containing a large number $n$ of\nindividual data points, also known as tall datasets. In scenarios where data\nare assumed independent, various approaches to scale up the Metropolis-Hastings\nalgorithm in a Bayesian inference context have been recently proposed in\nmachine learning and computational statistics. These approaches can be grouped\ninto two categories: divide-and-conquer approaches and, subsampling-based\nalgorithms. The aims of this article are as follows. First, we present a\ncomprehensive review of the existing literature, commenting on the underlying\nassumptions and theoretical guarantees of each method. Second, by leveraging\nour understanding of these limitations, we propose an original\nsubsampling-based approach which samples from a distribution provably close to\nthe posterior distribution of interest, yet can require less than $O(n)$ data\npoint likelihood evaluations at each iteration for certain statistical models\nin favourable scenarios. Finally, we have only been able so far to propose\nsubsampling-based methods which display good performance in scenarios where the\nBernstein-von Mises approximation of the target posterior distribution is\nexcellent. It remains an open challenge to develop such methods in scenarios\nwhere the Bernstein-von Mises approximation is poor. \n\n"}
{"id": "1505.04305", "contents": "Title: Detecting structural breaks in seasonal time series by regularized\n  optimization Abstract: Real-world systems are often complex, dynamic, and nonlinear. Understanding\nthe dynamics of a system from its observed time series is key to the prediction\nand control of the system's behavior. While most existing techniques tacitly\nassume some form of stationarity or continuity, abrupt changes, which are often\ndue to external disturbances or sudden changes in the intrinsic dynamics, are\ncommon in time series. Structural breaks, which are time points at which the\nstatistical patterns of a time series change, pose considerable challenges to\ndata analysis. Without identification of such break points, the same dynamic\nrule would be applied to the whole period of observation, whereas false\nidentification of structural breaks may lead to overfitting. In this paper, we\ncast the problem of decomposing a time series into its trend and seasonal\ncomponents as an optimization problem. This problem is ill-posed due to the\narbitrariness in the number of parameters. To overcome this difficulty, we\npropose the addition of a penalty function (i.e., a regularization term) that\naccounts for the number of parameters. Our approach simultaneously identifies\nseasonality and trend without the need of iterations, and allows the reliable\ndetection of structural breaks. The method is applied to recorded data on fish\npopulations and sea surface temperature, where it detects structural breaks\nthat would have been neglected otherwise. This suggests that our method can\nlead to a general approach for the monitoring, prediction, and prevention of\nstructural changes in real systems. \n\n"}
{"id": "1505.04321", "contents": "Title: Sequential Bayesian inference for implicit hidden Markov models and\n  current limitations Abstract: Hidden Markov models can describe time series arising in various fields of\nscience, by treating the data as noisy measurements of an arbitrarily complex\nMarkov process. Sequential Monte Carlo (SMC) methods have become standard tools\nto estimate the hidden Markov process given the observations and a fixed\nparameter value. We review some of the recent developments allowing the\ninclusion of parameter uncertainty as well as model uncertainty. The\nshortcomings of the currently available methodology are emphasised from an\nalgorithmic complexity perspective. The statistical objects of interest for\ntime series analysis are illustrated on a toy \"Lotka-Volterra\" model used in\npopulation ecology. Some open challenges are discussed regarding the\nscalability of the reviewed methodology to longer time series,\nhigher-dimensional state spaces and more flexible models. \n\n"}
{"id": "1505.04903", "contents": "Title: The impact of upper tropospheric friction and Gill-type heating on the\n  location and strength of the Tropical Easterly Jet: Idealized physics in a\n  dry Atmospheric General Circulation Model Abstract: An atmospheric general circulation model (AGCM) with idealized and complete\nphysics has been used to evaluate the Tropical Easterly Jet (TEJ) jet. In\nidealized physics, the role of upper tropospheric friction has been found to be\nimportant in getting realistic upper tropospheric zonal wind patterns in\nresponse to heating. In idealized physics, the location and strength of the TEJ\nas a response to Gill heating has been studied. Though the Gill model is\nconsidered to be widely successful in capturing the lower tropospheric\nresponse, it is found to be inadequate in explaining the location and strength\nof the upper level TEJ. Heating from the Gill model and realistic upper\ntropospheric friction does not lead to the formation of a TEJ. \n\n"}
{"id": "1505.05116", "contents": "Title: Data-driven Distributionally Robust Optimization Using the Wasserstein\n  Metric: Performance Guarantees and Tractable Reformulations Abstract: We consider stochastic programs where the distribution of the uncertain\nparameters is only observable through a finite training dataset. Using the\nWasserstein metric, we construct a ball in the space of (multivariate and\nnon-discrete) probability distributions centered at the uniform distribution on\nthe training samples, and we seek decisions that perform best in view of the\nworst-case distribution within this Wasserstein ball. The state-of-the-art\nmethods for solving the resulting distributionally robust optimization problems\nrely on global optimization techniques, which quickly become computationally\nexcruciating. In this paper we demonstrate that, under mild assumptions, the\ndistributionally robust optimization problems over Wasserstein balls can in\nfact be reformulated as finite convex programs---in many interesting cases even\nas tractable linear programs. Leveraging recent measure concentration results,\nwe also show that their solutions enjoy powerful finite-sample performance\nguarantees. Our theoretical results are exemplified in mean-risk portfolio\noptimization as well as uncertainty quantification. \n\n"}
{"id": "1505.05391", "contents": "Title: Efficient Multiple Importance Sampling Estimators Abstract: Multiple importance sampling (MIS) methods use a set of proposal\ndistributions from which samples are drawn. Each sample is then assigned an\nimportance weight that can be obtained according to different strategies. This\nwork is motivated by the trade-off between variance reduction and computational\ncomplexity of the different approaches (classical vs. deterministic mixture)\navailable for the weight calculation. A new method that achieves an efficient\ncompromise between both factors is introduced in this paper. It is based on\nforming a partition of the set of proposal distributions and computing the\nweights accordingly. Computer simulations show the excellent performance of the\nassociated \\mbox{\\emph{partial deterministic mixture} MIS estimator. \n\n"}
{"id": "1505.05770", "contents": "Title: Variational Inference with Normalizing Flows Abstract: The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference. \n\n"}
{"id": "1505.06354", "contents": "Title: Online Updating of Statistical Inference in the Big Data Setting Abstract: We present statistical methods for big data arising from online analytical\nprocessing, where large amounts of data arrive in streams and require fast\nanalysis without storage/access to the historical data. In particular, we\ndevelop iterative estimating algorithms and statistical inferences for linear\nmodels and estimating equations that update as new data arrive. These\nalgorithms are computationally efficient, minimally storage-intensive, and\nallow for possible rank deficiencies in the subset design matrices due to\nrare-event covariates. Within the linear model setting, the proposed\nonline-updating framework leads to predictive residual tests that can be used\nto assess the goodness-of-fit of the hypothesized model. We also propose a new\nonline-updating estimator under the estimating equation setting. Theoretical\nproperties of the goodness-of-fit tests and proposed estimators are examined in\ndetail. In simulation studies and real data applications, our estimator\ncompares favorably with competing approaches under the estimating equation\nsetting. \n\n"}
{"id": "1505.06475", "contents": "Title: A Fast and Flexible Algorithm for the Graph-Fused Lasso Abstract: We propose a new algorithm for solving the graph-fused lasso (GFL), a method\nfor parameter estimation that operates under the assumption that the signal\ntends to be locally constant over a predefined graph structure. Our key insight\nis to decompose the graph into a set of trails which can then each be solved\nefficiently using techniques for the ordinary (1D) fused lasso. We leverage\nthese trails in a proximal algorithm that alternates between closed form primal\nupdates and fast dual trail updates. The resulting techinque is both faster\nthan previous GFL methods and more flexible in the choice of loss function and\ngraph structure. Furthermore, we present two algorithms for constructing trail\nsets and show empirically that they offer a tradeoff between preprocessing time\nand convergence rate. \n\n"}
{"id": "1505.07071", "contents": "Title: The climatological relationships between wind and solar energy supply in\n  Britain Abstract: We use reanalysis data to investigate the daily co-variability of wind and\nsolar irradiance in Britain, and its implications for renewable energy supply\nbalancing. The joint distribution of daily-mean wind speeds and irradiances\nshows that irradiance has a much stronger seasonal cycle than wind, due to the\nrotational tilt of the Earth. Irradiance is weakly anticorrelated with wind\nspeed throughout the year ($-0.4 \\lesssim \\rho \\lesssim -0.2$): there is a weak\ntendency for windy days to be cloudier. This is particularly true in\nAtlantic-facing regions (western Scotland, south-west England). The east coast\nof Britain has the weakest anticorrelation, particularly in winter, primarily\nassociated with a relative increase in the frequency of clear-but-windy days.\nWe also consider the variability in total power output from onshore wind\nturbines and solar photovoltaic panels. In all months, daily variability in\ntotal power is always reduced by incorporating solar capacity. The scenario\nwith the least seasonal variability is approximately 70%-solar to 30%-wind.\nThis work emphasises the importance of considering the full distribution of\ndaily behaviour rather than relying on long-term average relationships or\ncorrelations. In particular, the anticorrelation between wind and solar power\nin Britain cannot solely be relied upon to produce a well-balanced energy\nsupply. \n\n"}
{"id": "1505.07856", "contents": "Title: Investigations into the impact of astronomical phenomena on the\n  terrestrial biosphere and climate Abstract: This thesis assesses the influence of astronomical phenomena on the Earth's\nbiosphere and climate. I examine in particular the relevance of both the path\nof the Sun through the Galaxy and the evolution of the Earth's orbital\nparameters in modulating non-terrestrial mechanisms. I build models to predict\nthe extinction rate of species, the temporal variation of the impact cratering\nrate and ice sheet deglaciations, and then compare these models with other\nmodels within a Bayesian framework. I find that the temporal distribution of\nmass extinction events over the past 550 Myr can be explained just as well by a\nuniform random distribution as by other models, such as variations in the\nstellar density local to the Sun arising from the Sun's orbit. Given the\nuncertainties in the Galaxy model and the Sun's current phase space\ncoordinates, as well as the errors in the geological data, it is not possible\nto draw a clear connection between terrestrial extinction and the solar motion.\nIn a separate study, I find that the solar motion, which modulates the Galactic\ntidal forces imposed on Oort cloud comets, does not significantly influence\nthis cratering rate. My dynamical models, together with the solar apex motion,\ncan explain the anisotropic perihelia of long period comets without needing to\ninvoke the existence of a Jupiter-mass solar companion. Finally, I find that\nvariations in the Earth's obliquity play a dominant role in triggering\nterrestrial deglaciations over the past 2 Myr. The precession of the equinoxes,\nin contrast, only becomes important in pacing large deglaciations after the\ntransition from the 100-kyr dominant periodicity in the ice coverage to a\n41-kyr dominant periodicity, which occurred 0.7 Myr ago. \n\n"}
{"id": "1506.00043", "contents": "Title: Sampling, feasibility, and priors in Bayesian estimation Abstract: Importance sampling algorithms are discussed in detail, with an emphasis on\nimplicit sampling, and applied to data assimilation via particle filters.\nImplicit sampling makes it possible to use the data to find high-probability\nsamples at relatively low cost, making the assimilation more efficient. A new\nanalysis of the feasibility of data assimilation is presented, showing in\ndetail why feasibility depends on the Frobenius norm of the covariance matrix\nof the noise and not on the number of variables. A discussion of the\nconvergence of particular particle filters follows. A major open problem in\nnumerical data assimilation is the determination of appropriate priors, a\nprogress report on recent work on this problem is given. The analysis\nhighlights the need for a careful attention both to the data and to the physics\nin data assimilation problems. \n\n"}
{"id": "1506.00552", "contents": "Title: Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than\n  Random Selection Abstract: There has been significant recent work on the theory and application of\nrandomized coordinate descent algorithms, beginning with the work of Nesterov\n[SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection\nrule achieves the same convergence rate as the Gauss-Southwell selection rule.\nThis result suggests that we should never use the Gauss-Southwell rule, as it\nis typically much more expensive than random selection. However, the empirical\nbehaviours of these algorithms contradict this theoretical result: in\napplications where the computational costs of the selection rules are\ncomparable, the Gauss-Southwell selection rule tends to perform substantially\nbetter than random coordinate selection. We give a simple analysis of the\nGauss-Southwell rule showing that---except in extreme cases---its convergence\nrate is faster than choosing random coordinates. Further, in this work we (i)\nshow that exact coordinate optimization improves the convergence rate for\ncertain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that\ngives an even faster convergence rate given knowledge of the Lipschitz\nconstants of the partial derivatives, (iii) analyze the effect of approximate\nGauss-Southwell rules, and (iv) analyze proximal-gradient variants of the\nGauss-Southwell rule. \n\n"}
{"id": "1506.01015", "contents": "Title: On the Late-Time Behaviour of a Bounded, Inviscid Two-Dimensional Flow Abstract: Using complementary numerical approaches at high resolution, we study the\nlate-time behaviour of an inviscid, incompressible two-dimensional flow on the\nsurface of a sphere. Starting from a random initial vorticity field comprised\nof a small set of intermediate wavenumber spherical harmonics, we find that --\ncontrary to the predictions of equilibrium statistical mechanics -- the flow\ndoes not evolve into a large-scale steady state. Instead, significant\nunsteadiness persists, characterised by a population of persistent small-scale\nvortices interacting with a large-scale oscillating quadrupolar vorticity\nfield. Moreover, the vorticity develops a stepped, staircase distribution,\nconsisting of nearly homogeneous regions separated by sharp gradients. The\npersistence of unsteadiness is explained by a simple point vortex model\ncharacterising the interactions between the four main vortices which emerge. \n\n"}
{"id": "1506.02267", "contents": "Title: Computationally Efficient Bayesian Learning of Gaussian Process State\n  Space Models Abstract: Gaussian processes allow for flexible specification of prior assumptions of\nunknown dynamics in state space models. We present a procedure for efficient\nBayesian learning in Gaussian process state space models, where the\nrepresentation is formed by projecting the problem onto a set of approximate\neigenfunctions derived from the prior covariance structure. Learning under this\nfamily of models can be conducted using a carefully crafted particle MCMC\nalgorithm. This scheme is computationally efficient and yet allows for a fully\nBayesian treatment of the problem. Compared to conventional system\nidentification tools or existing learning methods, we show competitive\nperformance and reliable quantification of uncertainties in the model. \n\n"}
{"id": "1506.03074", "contents": "Title: Variational consensus Monte Carlo Abstract: Practitioners of Bayesian statistics have long depended on Markov chain Monte\nCarlo (MCMC) to obtain samples from intractable posterior distributions.\nUnfortunately, MCMC algorithms are typically serial, and do not scale to the\nlarge datasets typical of modern machine learning. The recently proposed\nconsensus Monte Carlo algorithm removes this limitation by partitioning the\ndata and drawing samples conditional on each partition in parallel (Scott et\nal, 2013). A fixed aggregation function then combines these samples, yielding\napproximate posterior samples. We introduce variational consensus Monte Carlo\n(VCMC), a variational Bayes algorithm that optimizes over aggregation functions\nto obtain samples from a distribution that better approximates the target. The\nresulting objective contains an intractable entropy term; we therefore derive a\nrelaxation of the objective and show that the relaxed problem is blockwise\nconcave under mild conditions. We illustrate the advantages of our algorithm on\nthree inference tasks from the literature, demonstrating both the superior\nquality of the posterior approximation and the moderate overhead of the\noptimization step. Our algorithm achieves a relative error reduction (measured\nagainst serial MCMC) of up to 39% compared to consensus Monte Carlo on the task\nof estimating 300-dimensional probit regression parameter expectations;\nsimilarly, it achieves an error reduction of 92% on the task of estimating\ncluster comembership probabilities in a Gaussian mixture model with 8\ncomponents in 8 dimensions. Furthermore, these gains come at moderate cost\ncompared to the runtime of serial MCMC, achieving near-ideal speedup in some\ninstances. \n\n"}
{"id": "1506.03159", "contents": "Title: Copula variational inference Abstract: We develop a general variational inference method that preserves dependency\namong the latent variables. Our method uses copulas to augment the families of\ndistributions used in mean-field and structured approximations. Copulas model\nthe dependency that is not captured by the original variational distribution,\nand thus the augmented variational family guarantees better approximations to\nthe posterior. With stochastic optimization, inference on the augmented\ndistribution is scalable. Furthermore, our strategy is generic: it can be\napplied to any inference procedure that currently uses the mean-field or\nstructured approach. Copula variational inference has many advantages: it\nreduces bias; it is less sensitive to local optima; it is less sensitive to\nhyperparameters; and it helps characterize and interpret the dependency among\nthe latent variables. \n\n"}
{"id": "1506.03670", "contents": "Title: Spatially adaptive covariance tapering Abstract: Covariance tapering is a popular approach for reducing the computational cost\nof spatial prediction and parameter estimation for Gaussian process models.\nHowever, tapering can have poor performance when the process is sampled at\nspatially irregular locations or when non-stationary covariance models are\nused. This work introduces an adaptive tapering method in order to improve the\nperformance of tapering in these problematic cases. This is achieved by\nintroducing a computationally convenient class of compactly supported\nnon-stationary covariance functions, combined with a new method for choosing\nspatially varying taper ranges. Numerical experiments are used to show that the\nperformance of both kriging prediction and parameter estimation can be improved\nby allowing for spatially varying taper ranges. However, although adaptive\ntapering outperforms regular tapering, simply dividing the data into blocks and\nignoring the dependence between the blocks is often a better method for\nparameter estimation. \n\n"}
{"id": "1506.04137", "contents": "Title: Mixtures of Multivariate Power Exponential Distributions Abstract: An expanded family of mixtures of multivariate power exponential\ndistributions is introduced. While fitting heavy-tails and skewness has\nreceived much attention in the model-based clustering literature recently, we\ninvestigate the use of a distribution that can deal with both varying\ntail-weight and peakedness of data. A family of parsimonious models is proposed\nusing an eigen-decomposition of the scale matrix. A generalized\nexpectation-maximization algorithm is presented that combines convex\noptimization via a minorization-maximization approach and optimization based on\naccelerated line search algorithms on the Stiefel manifold. Lastly, the utility\nof this family of models is illustrated using both toy and benchmark data. \n\n"}
{"id": "1506.05860", "contents": "Title: Variational Gaussian Copula Inference Abstract: We utilize copulas to constitute a unified framework for constructing and\noptimizing variational proposals in hierarchical Bayesian models. For models\nwith continuous and non-Gaussian hidden variables, we propose a semiparametric\nand automated variational Gaussian copula approach, in which the parametric\nGaussian copula family is able to preserve multivariate posterior dependence,\nand the nonparametric transformations based on Bernstein polynomials provide\nample flexibility in characterizing the univariate marginal posteriors. \n\n"}
{"id": "1506.05863", "contents": "Title: Towards the azimuthal characteristics of ionospheric and seismic effects\n  of \"Chelyabinsk\" meteorite fall according to the data from coherent radar,\n  GPS and seismic networks Abstract: We present the results of a study of the azimuthal characteristics of\nionospheric and seismic effects of the meteorite 'Chelyabinsk', based on the\ndata from the network of GPS receivers, coherent decameter radar EKB SuperDARN\nand network of seismic stations.\n  It is shown, that 6-14 minutes after the bolide explosion, GPS network\nobserved the cone-shaped wavefront of TIDs that is interpreted as a ballistic\nacoustic wave. The typical TIDs propagation velocity were observed\n661+/-256m/s, which corresponds to the expected acoustic wave speed for 240km\nheight. 14 minutes after the bolide explosion, at distances of 200km we\nobserved the emergence and propagation of a TID with spherical wavefront, that\nis interpreted as gravitational mode of internal acoustic waves. The\npropagation velocity of this TID was 337+/-89m/s which corresponds to the\npropagation velocity of these waves in similar situations. At EKB SuperDARN\nradar, we observed TIDs in the sector of azimuthal angles close to the\nperpendicular to the meteorite trajectory. The observed TID velocity (400 m/s)\nand azimuthal properties correlate well with the model of ballistic wave\npropagating at 120-140km altitude.\n  It is shown, that the azimuthal distribution of the amplitude of vertical\nseismic oscillations can be described qualitatively by the model of vertical\nstrike-slip rupture, propagating at 1km/s along the meteorite fall trajectory\nto distance of about 40km. These parameters correspond to the direction and\nvelocity of propagation of the ballistic wave peak by the ground. It is shown,\nthat the model of ballistic wave caused by supersonic motion and burning of the\nmeteorite in the upper atmosphere can satisfactorily explain the various\nazimuthal ionospheric effects, observed by the coherent decameter radar EKB\nSuperDARN, GPS-receivers network, as well as the azimuthal characteristics of\nseismic waves at large distances. \n\n"}
{"id": "1506.05936", "contents": "Title: Sampling constrained probability distributions using Spherical\n  Augmentation Abstract: Statistical models with constrained probability distributions are abundant in\nmachine learning. Some examples include regression models with norm constraints\n(e.g., Lasso), probit, many copula models, and latent Dirichlet allocation\n(LDA). Bayesian inference involving probability distributions confined to\nconstrained domains could be quite challenging for commonly used sampling\nalgorithms. In this paper, we propose a novel augmentation technique that\nhandles a wide range of constraints by mapping the constrained domain to a\nsphere in the augmented space. By moving freely on the surface of this sphere,\nsampling algorithms handle constraints implicitly and generate proposals that\nremain within boundaries when mapped back to the original space. Our proposed\nmethod, called {Spherical Augmentation}, provides a mathematically natural and\ncomputationally efficient framework for sampling from constrained probability\ndistributions. We show the advantages of our method over state-of-the-art\nsampling algorithms, such as exact Hamiltonian Monte Carlo, using several\nexamples including truncated Gaussian distributions, Bayesian Lasso, Bayesian\nbridge regression, reconstruction of quantized stationary Gaussian process, and\nLDA for topic modeling. \n\n"}
{"id": "1506.06117", "contents": "Title: Convergence of Sequential Quasi-Monte Carlo Smoothing Algorithms Abstract: Gerber and Chopin (2015) recently introduced Sequential quasi-Monte Carlo\n(SQMC) algorithms as an efficient way to perform filtering in state-space\nmodels. The basic idea is to replace random variables with low-discrepancy\npoint sets, so as to obtain faster convergence than with standard particle\nfiltering. Gerber and Chopin (2015) describe briefly several ways to extend\nSQMC to smoothing, but do not provide supporting theory for this extension. We\ndiscuss more thoroughly how smoothing may be performed within SQMC, and derive\nconvergence results for the so-obtained smoothing algorithms. We consider in\nparticular SQMC equivalents of forward smoothing and forward filtering backward\nsampling, which are the most well-known smoothing techniques. As a preliminary\nstep, we provide a generalization of the classical result of Hlawka and M\\\"uck\n(1972) on the transformation of QMC point sets into low discrepancy point sets\nwith respect to non uniform distributions. As a corollary of the latter, we\nnote that we can slightly weaken the assumptions to prove the consistency of\nSQMC. \n\n"}
{"id": "1506.06285", "contents": "Title: The MCMC split sampler: A block Gibbs sampling scheme for latent\n  Gaussian models Abstract: A novel computationally efficient Markov chain Monte Carlo (MCMC) scheme for\nlatent Gaussian models (LGMs) is proposed in this paper. The sampling scheme is\na two block Gibbs sampling scheme designed to exploit the model structure of\nLGMs. We refer to the proposed sampling scheme as the MCMC split sampler. The\nprinciple idea behind the MCMC split sampler is to split the latent Gaussian\nparameters into two vectors. The former vector consists of latent parameters\nwhich appear in the data density function, while the latter vector consists of\nlatent parameters which do not appear in it. The former vector is placed in the\nfirst block of the proposed sampling scheme and the latter vector is placed in\nthe second block along with any potential hyperparameters. The resulting\nconditional posterior density functions within the blocks allow the MCMC split\nsampler to handle, by design, LGMs with latent models imposed on more than just\nthe mean structure of the data density function. The MCMC split sampler is also\ndesigned to be applicable for any choice of a parametric data density function.\nMoreover, it scales well in terms of computational efficiency when the\ndimension of the latent model increase. \n\n"}
{"id": "1506.07044", "contents": "Title: Monte Carlo Methods for the Ferromagnetic Potts Model Using Factor Graph\n  Duality Abstract: Normal factor graph duality offers new possibilities for Monte Carlo\nalgorithms in graphical models. Specifically, we consider the problem of\nestimating the partition function of the ferromagnetic Ising and Potts models\nby Monte Carlo methods, which are known to work well at high temperatures, but\nto fail at low temperatures. We propose Monte Carlo methods (uniform sampling\nand importance sampling) in the dual normal factor graph, and demonstrate that\nthey behave differently: they work particularly well at low temperatures. By\ncomparing the relative error in estimating the partition function, we show that\nthe proposed importance sampling algorithm significantly outperforms the\nstate-of-the-art deterministic and Monte Carlo methods. For the ferromagnetic\nIsing model in an external field, we show the equivalence between the valid\nconfigurations in the dual normal factor graph and the terms that appear in the\nhigh-temperature series expansion of the partition function. Following this\nresult, we discuss connections with Jerrum-Sinclair's polynomial randomized\napproximation scheme (the subgraphs-world process) for evaluating the partition\nfunction of ferromagnetic \n\n"}
{"id": "1506.08170", "contents": "Title: Finding Linear Structure in Large Datasets with Scalable Canonical\n  Correlation Analysis Abstract: Canonical Correlation Analysis (CCA) is a widely used spectral technique for\nfinding correlation structures in multi-view datasets. In this paper, we tackle\nthe problem of large scale CCA, where classical algorithms, usually requiring\ncomputing the product of two huge matrices and huge matrix decomposition, are\ncomputationally and storage expensive. We recast CCA from a novel perspective\nand propose a scalable and memory efficient Augmented Approximate Gradient\n(AppGrad) scheme for finding top $k$ dimensional canonical subspace which only\ninvolves large matrix multiplying a thin matrix of width $k$ and small matrix\ndecomposition of dimension $k\\times k$. Further, AppGrad achieves optimal\nstorage complexity $O(k(p_1+p_2))$, compared with classical algorithms which\nusually require $O(p_1^2+p_2^2)$ space to store two dense whitening matrices.\nThe proposed scheme naturally generalizes to stochastic optimization regime,\nespecially efficient for huge datasets where batch algorithms are prohibitive.\nThe online property of stochastic AppGrad is also well suited to the streaming\nscenario, where data comes sequentially. To the best of our knowledge, it is\nthe first stochastic algorithm for CCA. Experiments on four real data sets are\nprovided to show the effectiveness of the proposed methods. \n\n"}
{"id": "1507.00843", "contents": "Title: Optimal linear Bernoulli factories for small mean problems Abstract: Suppose a coin with unknown probability $p$ of heads can be flipped as often\nas desired. A Bernoulli factory for a function $f$ is an algorithm that uses\nflips of the coin together with auxiliary randomness to flip a single coin with\nprobability $f(p)$ of heads. Applications include near perfect sampling from\nthe stationary distribution of regenerative processes. When $f$ is analytic,\nthe problem can be reduced to a Bernoulli factory of the form $f(p) = Cp$ for\nconstant $C$. Presented here is a new algorithm where for small values of $Cp$,\nrequires roughly only $C$ coin flips to generate a $Cp$ coin. From information\ntheory considerations, this is also conjectured to be (to first order) the\nminimum number of flips needed by any such algorithm.\n  For $Cp$ large, the new algorithm can also be used to build a new Bernoulli\nfactory that uses only 80\\% of the expected coin flips of the older method, and\napplies to the more general problem of a multivariate Bernoulli factory, where\nthere are $k$ coins, the $k$th coin has unknown probability $p_k$ of heads, and\nthe goal is to simulate a coin flip with probability $C_1 p_1 + \\cdots + C_k\np_k$ of heads. \n\n"}
{"id": "1507.01571", "contents": "Title: Unified functional network and nonlinear time series analysis for\n  complex systems science: The pyunicorn package Abstract: We introduce the \\texttt{pyunicorn} (Pythonic unified complex network and\nrecurrence analysis toolbox) open source software package for applying and\ncombining modern methods of data analysis and modeling from complex network\ntheory and nonlinear time series analysis. \\texttt{pyunicorn} is a fully\nobject-oriented and easily parallelizable package written in the language\nPython. It allows for the construction of functional networks such as climate\nnetworks in climatology or functional brain networks in neuroscience\nrepresenting the structure of statistical interrelationships in large data sets\nof time series and, subsequently, investigating this structure using advanced\nmethods of complex network theory such as measures and models for spatial\nnetworks, networks of interacting networks, node-weighted statistics or network\nsurrogates. Additionally, \\texttt{pyunicorn} provides insights into the\nnonlinear dynamics of complex systems as recorded in uni- and multivariate time\nseries from a non-traditional perspective by means of recurrence quantification\nanalysis (RQA), recurrence networks, visibility graphs and construction of\nsurrogate time series. The range of possible applications of the library is\noutlined, drawing on several examples mainly from the field of climatology. \n\n"}
{"id": "1507.01944", "contents": "Title: Carbon Dioxide in Exoplanetary Atmospheres: Rarely Dominant Compared to\n  Carbon Monoxide and Water in Hot, Hydrogen-dominated Atmospheres Abstract: We present a comprehensive study of the abundance of carbon dioxide in\nexoplanetary atmospheres in hot, hydrogen-dominated atmospheres. We construct\nnovel analytical models of systems in chemical equilibrium that include carbon\nmonoxide, carbon dioxide, water, methane and acetylene and relate the\nequilibrium constants of the chemical reactions to temperature and pressure via\nthe tabulated Gibbs free energies. We prove that such chemical systems may be\ndescribed by a quintic equation for the mixing ratio of methane. By examining\nthe abundances of these molecules across a broad range of temperatures\n(spanning equilibrium temperatures from 600 to 2500 K), pressures (via\ntemperature-pressure profiles that explore albedo and opacity variations) and\ncarbon-to-oxygen ratios, we conclude that carbon dioxide is subdominant\ncompared to carbon monoxide and water. Atmospheric mixing does not alter this\nconclusion if carbon dioxide is subdominant everywhere in the atmosphere.\nCarbon dioxide and carbon monoxide may attain comparable abundances if the\nmetallicity is greatly enhanced, but this property is negated by temperatures\nabove 1000 K. For hydrogen-dominated atmospheres, our generic result has the\nimplication that retrieval studies may wish to set the subdominance of carbon\ndioxide as a prior of the calculation and not let its abundance completely roam\nfree as a fitting parameter, because it directly affects the inferred value of\nthe carbon-to-oxygen ratio and may produce unphysical conclusions. We discuss\nthe relevance of these implications for the hot Jupiter WASP-12b and suggest\nthat some of the previous results are chemically impossible. The relative\nabundance of carbon dioxide to acetylene is potentially a sensitive diagnostic\nof the carbon-to-oxygen ratio. \n\n"}
{"id": "1507.03133", "contents": "Title: Best Subset Selection via a Modern Optimization Lens Abstract: In the last twenty-five years (1990-2014), algorithmic advances in integer\noptimization combined with hardware improvements have resulted in an\nastonishing 200 billion factor speedup in solving Mixed Integer Optimization\n(MIO) problems. We present a MIO approach for solving the classical best subset\nselection problem of choosing $k$ out of $p$ features in linear regression\ngiven $n$ observations. We develop a discrete extension of modern first order\ncontinuous optimization methods to find high quality feasible solutions that we\nuse as warm starts to a MIO solver that finds provably optimal solutions. The\nresulting algorithm (a) provides a solution with a guarantee on its\nsuboptimality even if we terminate the algorithm early, (b) can accommodate\nside constraints on the coefficients of the linear regression and (c) extends\nto finding best subset solutions for the least absolute deviation loss\nfunction. Using a wide variety of synthetic and real datasets, we demonstrate\nthat our approach solves problems with $n$ in the 1000s and $p$ in the 100s in\nminutes to provable optimality, and finds near optimal solutions for $n$ in the\n100s and $p$ in the 1000s in minutes. We also establish via numerical\nexperiments that the MIO approach performs better than {\\texttt {Lasso}} and\nother popularly used sparse learning procedures, in terms of achieving sparse\nsolutions with good predictive power. \n\n"}
{"id": "1507.05990", "contents": "Title: Estimation and uncertainty of reversible Markov models Abstract: Reversibility is a key concept in Markov models and Master-equation models of\nmolecular kinetics. The analysis and interpretation of the transition matrix\nencoding the kinetic properties of the model relies heavily on the\nreversibility property. The estimation of a reversible transition matrix from\nsimulation data is therefore crucial to the successful application of the\npreviously developed theory. In this work we discuss methods for the maximum\nlikelihood estimation of transition matrices from finite simulation data and\npresent a new algorithm for the estimation if reversibility with respect to a\ngiven stationary vector is desired. We also develop new methods for the\nBayesian posterior inference of reversible transition matrices with and without\ngiven stationary vector taking into account the need for a suitable prior\ndistribution preserving the meta- stable features of the observed process\nduring posterior inference. All algorithms here are implemented in the PyEMMA\nsoftware - http://pyemma.org - as of version 2.0. \n\n"}
{"id": "1507.06244", "contents": "Title: Emulation of Higher-Order Tensors in Manifold Monte Carlo Methods for\n  Bayesian Inverse Problems Abstract: The Bayesian approach to Inverse Problems relies predominantly on Markov\nChain Monte Carlo methods for posterior inference. The typical nonlinear\nconcentration of posterior measure observed in many such Inverse Problems\npresents severe challenges to existing simulation based inference methods.\nMotivated by these challenges the exploitation of local geometric information\nin the form of covariant gradients, metric tensors, Levi-Civita connections,\nand local geodesic flows, have been introduced to more effectively locally\nexplore the configuration space of the posterior measure. However, obtaining\nsuch geometric quantities usually requires extensive computational effort and\ndespite their effectiveness affect the applicability of these\ngeometrically-based Monte Carlo methods. In this paper we explore one way to\naddress this issue by the construction of an emulator of the model from which\nall geometric objects can be obtained in a much more computationally feasible\nmanner. The main concept is to approximate the geometric quantities using a\nGaussian Process emulator which is conditioned on a carefully chosen design set\nof configuration points, which also determines the quality of the emulator. To\nthis end we propose the use of statistical experiment design methods to refine\na potentially arbitrarily initialized design online without destroying the\nconvergence of the resulting Markov chain to the desired invariant measure. The\npractical examples considered in this paper provide a demonstration of the\nsignificant improvement possible in terms of computational loading suggesting\nthis is a promising avenue of further development. \n\n"}
{"id": "1507.06336", "contents": "Title: Hessian corrections to the Metropolis Adjusted Langevin Algorithm Abstract: A natural method for the introduction of second-order derivatives of the log\nlikelihood into MCMC algorithms is introduced, based on Taylor expansion of the\nLangevin equation followed by exact solution of the truncated system. \n\n"}
{"id": "1507.06370", "contents": "Title: Sum-of-Squares Lower Bounds for Sparse PCA Abstract: This paper establishes a statistical versus computational trade-off for\nsolving a basic high-dimensional machine learning problem via a basic convex\nrelaxation method. Specifically, we consider the {\\em Sparse Principal\nComponent Analysis} (Sparse PCA) problem, and the family of {\\em\nSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well\nknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em\nin principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli)\nsamples, but all {\\em efficient} (polynomial time) algorithms known require $n\n\\approx k^2$ samples. It was also known that this quadratic gap cannot be\nimproved by the the most basic {\\em semi-definite} (SDP, aka spectral)\nrelaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also\ndegree-4 SoS algorithms cannot improve this quadratic gap. This average-case\nlower bound adds to the small collection of hardness results in machine\nlearning for this powerful family of convex relaxation algorithms. Moreover,\nour design of moments (or \"pseudo-expectations\") for this lower bound is quite\ndifferent than previous lower bounds. Establishing lower bounds for higher\ndegree SoS algorithms for remains a challenging problem. \n\n"}
{"id": "1507.06807", "contents": "Title: Bayesian inference for diffusion driven mixed-effects models Abstract: Stochastic differential equations (SDEs) provide a natural framework for\nmodelling intrinsic stochasticity inherent in many continuous-time physical\nprocesses. When such processes are observed in multiple individuals or\nexperimental units, SDE driven mixed-effects models allow the quantification of\nbetween (as well as within) individual variation. Performing Bayesian inference\nfor such models, using discrete time data that may be incomplete and subject to\nmeasurement error is a challenging problem and is the focus of this paper. We\nextend a recently proposed MCMC scheme to include the SDE driven mixed-effects\nframework. Fundamental to our approach is the development of a novel construct\nthat allows for efficient sampling of conditioned SDEs that may exhibit\nnonlinear dynamics between observation times. We apply the resulting scheme to\nsynthetic data generated from a simple SDE model of orange tree growth, and\nreal data consisting of observations on aphid numbers recorded under a variety\nof different treatment regimes. In addition, we provide a systematic comparison\nof our approach with an inference scheme based on a tractable approximation of\nthe SDE, that is, the linear noise approximation. \n\n"}
{"id": "1507.07445", "contents": "Title: Assessment of petrophysical quantities inspired by joint multifractal\n  approach Abstract: In this paper joint multifractal random walk approach is carried out to\nanalyze some petrophysical quantities for characterizing the petroleum\nreservoir. These quantities include Gamma emission (GR), sonic transient time\n(DT) and Neutron porosity (NPHI) which are collected from four wells of a\nreservoir. To quantify mutual interaction of petrophysical quantities, joint\nmultifractal random walk is implemented. This approach is based on the mutual\nmultiplicative cascade notion in the multifractal formalism and in this\napproach $L_0$ represents a benchmark to describe the nature of\ncross-correlation between two series. The analysis of the petrophysical\nquantities revealed that GR for all wells has strongly multifractal nature due\nto the considerable abundance of large fluctuations in various scales. The\nvariance of probability distribution function, $\\lambda_{\\ell}^2$, at scale\n$\\ell$ and its intercept determine the multifractal properties of the data sets\nsourced by probability density function. The value of $\\lambda_0 ^2$ for NPHI\ndata set is less than GR's, however, DT shows a nearly monofractal behavior,\nnamely $\\lambda_0 ^2\\rightarrow 0$, so we find that $\\lambda_0^2({\\rm\nGR})>\\lambda_0^2({\\rm NPHI})\\gg\\lambda_0^2({\\rm DT})$. While, the value of\nHurst exponents can not discriminate between series GR, NPHI and DT. Joint\nanalysis of the petrophysical quantities for considered wells demonstrates that\n$L_0$ has negative value for GR-NPHI confirming that finding shaly layers is in\ncompetition with finding porous medium while it takes positive value for GR-DT\ndetermining that continuum medium can be detectable by evaluating the\nstatistical properties of GR and its cross-correlation to DT signal. \n\n"}
{"id": "1507.07873", "contents": "Title: Are rogue waves really unexpected? Abstract: An unexpected wave is defined by Gemmrich & Garrett (2008) as a wave that is\nmuch taller than a set of neighboring waves. Their definition of \"unexpected\"\nrefers to a wave that is not anticipated by a casual observer. Clearly,\nunexpected waves defined in this way are predictable in a statistical sense.\nThey can occur relatively often with a small or moderate crest height, but\nlarge unexpected waves that are rogue are rare. Here, this concept is\nelaborated and statistically described based on a third-order nonlinear model.\nIn particular, the conditional return period of an unexpected wave whose crest\nexceeds a given threshold is developed. This definition leads to greater return\nperiods or on average less frequent occurrences of unexpected waves than those\nimplied by the conventional return periods not conditioned on a reference\nthreshold. Ultimately, it appears that a rogue wave that is also unexpected\nwould have a lower occurrence frequency than that of a usual rogue wave. As\nspecific applications, the Andrea and WACSIS rogue wave events are examined in\ndetail. Both waves appeared without warning and their crests were nearly\n$2$-times larger than the surrounding $O(10)$ wave crests, and thus unexpected.\nThe two crest heights are nearly the same as the\nthreshold~$h_{0.3\\cdot10^{6}}\\sim1.6H_{s}$ exceeded on average once\nevery~$0.3\\cdot 10^{6}$ waves, where $H_s$ is the significant wave height. In\ncontrast, the Andrea and WACSIS events, as both rogue and unexpected, would\noccur slightly less often and on average once every~$3\\cdot10^{6}$\nand~$0.6\\cdot10^6$ waves respectively. \n\n"}
{"id": "1507.08563", "contents": "Title: Metropolized Randomized Maximum Likelihood for sampling from multimodal\n  distributions Abstract: This article describes a method for using optimization to derive efficient\nindependent transition functions for Markov chain Monte Carlo simulations. Our\ninterest is in sampling from a posterior density $\\pi(x)$ for problems in which\nthe dimension of the model space is large, $\\pi(x)$ is multimodal with regions\nof low probability separating the modes, and evaluation of the likelihood is\nexpensive. We restrict our attention to the special case for which the target\ndensity is the product of a multivariate Gaussian prior and a likelihood\nfunction for which the errors in observations are additive and Gaussian. \n\n"}
{"id": "1508.00286", "contents": "Title: Goodness of fit of logistic models for random graphs Abstract: Logistic regression is a natural and simple tool to understand how covariates\ncontribute to explain the topology of a binary network. Once the model fitted,\nthe practitioner is interested in the goodness-of-fit of the regression in\norder to check if the covariates are sufficient to explain the whole topology\nof the network and, if they are not, to analyze the residual structure. To\naddress this problem, we introduce a generic model that combines logistic\nregression with a network-oriented residual term. This residual term takes the\nform of the graphon function of a W-graph. Using a variational Bayes framework,\nwe infer the residual graphon by averaging over a series of blockwise constant\nfunctions. This approach allows us to define a generic goodness-of-fit\ncriterion, which corresponds to the posterior probability for the residual\ngraphon to be constant. Experiments on toy data are carried out to assess the\naccuracy of the procedure. Several networks from social sciences and ecology\nare studied to illustrate the proposed methodology. \n\n"}
{"id": "1508.00635", "contents": "Title: Bayesian mixtures of spatial spline regressions Abstract: This work relates the framework of model-based clustering for spatial\nfunctional data where the data are surfaces. We first introduce a Bayesian\nspatial spline regression model with mixed-effects (BSSR) for modeling spatial\nfunction data. The BSSR model is based on Nodal basis functions for spatial\nregression and accommodates both common mean behavior for the data through a\nfixed-effects part, and variability inter-individuals thanks to a\nrandom-effects part. Then, in order to model populations of spatial functional\ndata issued from heterogeneous groups, we integrate the BSSR model into a\nmixture framework. The resulting model is a Bayesian mixture of spatial spline\nregressions with mixed-effects (BMSSR) used for density estimation and\nmodel-based surface clustering. The models, through their Bayesian formulation,\nallow to integrate possible prior knowledge on the data structure and\nconstitute a good alternative to recent mixture of spatial spline regressions\nmodel estimated in a maximum likelihood framework via the\nexpectation-maximization (EM) algorithm. The Bayesian model inference is\nperformed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs\nsampler to infer the BSSR and the BMSSR models and apply them on simulated\nsurfaces and a real problem of handwritten digit recognition using the MNIST\ndata set. The obtained results highlight the potential benefit of the proposed\nBayesian approaches for modeling surfaces possibly dispersed in particular in\nclusters. \n\n"}
{"id": "1508.02651", "contents": "Title: Sequential Monte Carlo for fractional Stochastic Volatility Models Abstract: In this paper we consider a fractional stochastic volatility model, that is a\nmodel in which the volatility may exhibit a long-range dependent or a\nrough/antipersistent behavior. We propose a dynamic sequential Monte Carlo\nmethodology that is applicable to both long memory and antipersistent processes\nin order to estimate the volatility as well as the unknown parameters of the\nmodel. We establish a central limit theorem for the state and parameter filters\nand we study asymptotic properties (consistency and asymptotic normality) for\nthe filter. We illustrate our results with a simulation study and we apply our\nmethod to estimating the volatility and the parameters of a long-range\ndependent model for S&P 500 data. \n\n"}
{"id": "1508.03884", "contents": "Title: A simple sampler for the horseshoe estimator Abstract: In this note we derive a simple Bayesian sampler for linear regression with\nthe horseshoe hierarchy. A new interpretation of the horseshoe model is\npresented, and extensions to logistic regression and alternative hierarchies,\nsuch as horseshoe$+$, are discussed. Due to the conjugacy of the proposed\nhierarchy, Chib's algorithm may be used to easily compute the marginal\nlikelihood of the model. \n\n"}
{"id": "1508.04002", "contents": "Title: Dynamical Analysis of Blocking Events: Spatial and Temporal Fluctuations\n  of Covariant Lyapunov Vectors Abstract: One of the most relevant weather regimes in the mid-latitudes atmosphere is\nthe persistent deviation from the approximately zonally symmetric jet to the\nemergence of blocking patterns. Such configurations are usually connected to\nexceptional local stability properties of the flow which come along with an\nimproved local forecast skills during the phenomenon. It is instead extremely\nhard to predict onset and decay of blockings. Covariant Lyapunov Vectors (CLVs)\noffer a suitable characterization of the linear stability of a chaotic flow,\nsince they represent the full tangent linear dynamics by a covariant basis\nwhich explores linear perturbations at all time scales. Therefore, we assess\nwhether CLVs feature a signature of the blockings. As a first step, we examine\nthe CLVs for a quasi-geostrophic beta-plane 2-layer model in a periodic channel\nbaroclinically driven by a meridional temperature gradient $\\Delta T$. An\norographic forcing enhances the emergence of localized blocked regimes. We\ndetect the blocking events with a Tibaldi-Molteni scheme adapted to the\nperiodic channel. When blocking occurs, the global growth rates of the fastest\ngrowing CLVs are significantly higher. Hence, against intuition, the\ncirculation is globally more unstable in blocked phases. Such an increase in\nthe finite time Lyapunov exponents with respect to the long term average is\nattributed to stronger barotropic and baroclinic conversion in the case of high\ntemperature gradients, while for low values of \\Delta T, the effect is only due\nto stronger barotropic instability. In order to determine the localization of\nthe CLVs we compare the meridionally averaged variance of the CLVs during\nblocked and unblocked phases. We find that on average the variance of the CLVs\nis clustered around the center of blocking. These results show that the blocked\nflow affects all time scales and processes described by the CLVs. \n\n"}
{"id": "1509.02549", "contents": "Title: Waves and vortices in the inverse cascade regime of stratified\n  turbulence with or without rotation Abstract: We study the partition of energy between waves and vortices in stratified\nturbulence, with or without rotation, for a variety of parameters, focusing on\nthe behavior of the waves and vortices in the inverse cascade of energy towards\nthe large scales. To this end, we use direct numerical simulations in a cubic\nbox at a Reynolds number Re=1000, with the ratio between the\nBrunt-V\\\"ais\\\"al\\\"a frequency N and the inertial frequency f varying from 1/4\nto 20, together with a purely stratified run. The Froude number, measuring the\nstrength of the stratification, varies within the range 0.02 < Fr < 0.32. We\nfind that the inverse cascade is dominated by the slow quasi-geostrophic modes.\nTheir energy spectra and fluxes exhibit characteristics of an inverse cascade,\neven though their energy is not conserved. Surprisingly, the slow vortices\nstill dominate when the ratio N/f increases, also in the stratified case,\nalthough less and less so. However, when N/f increases, the inverse cascade of\nthe slow modes becomes weaker and weaker, and it vanishes in the purely\nstratified case. We discuss how the disappearance of the inverse cascade of\nenergy with increasing N/f can be interpreted in terms of the waves and\nvortices, and identify three major effects that can explain this transition\nbased on inviscid invariants arguments. \n\n"}
{"id": "1509.03521", "contents": "Title: Similarity-based semi-local estimation of EMOS models Abstract: Weather forecasts are typically given in the form of forecast ensembles\nobtained from multiple runs of numerical weather prediction models with varying\ninitial conditions and physics parameterizations. Such ensemble predictions\ntend to be biased and underdispersive and thus require statistical\npostprocessing. In the ensemble model output statistics (EMOS) approach, a\nprobabilistic forecast is given by a single parametric distribution with\nparameters depending on the ensemble members. This article proposes two\nsemi-local methods for estimating the EMOS coefficients where the training data\nfor a specific observation station are augmented with corresponding forecast\ncases from stations with similar characteristics. Similarities between stations\nare determined using either distance functions or clustering based on various\nfeatures of the climatology, forecast errors, ensemble predictions and\nlocations of the observation stations. In a case study on wind speed over\nEurope with forecasts from the Grand Limited Area Model Ensemble Prediction\nSystem, the proposed similarity-based semi-local models show significant\nimprovement in predictive performance compared to standard regional and local\nestimation methods. They further allow for estimating complex models without\nnumerical stability issues and are computationally more efficient than local\nparameter estimation. \n\n"}
{"id": "1509.04214", "contents": "Title: Collective phenomena in granular and atmospheric electrification Abstract: In clouds of suspended particles (grains, droplets, spheres, crystals, etc.),\ncollisions electrify the particles and the clouds, producing large electric\npotential differences over large scales. This is seen most spectacularly in the\natmosphere as lighting in thunderstorms, thundersnow, dust storms, and volcanic\nash plumes where multi-million-volt potential differences over scales of\nkilometers can be produced, but it is a general phenomenon in granular systems\nas a whole. The electrification process is not well understood, especially for\nelectrification of insulating particles of the same material. To investigate\nthe relative importances of particle properties (material, size, etc.) and\ncollective phenomena (behaviors of systems at large scales not easily predicted\nfrom local dynamics) in granular and atmospheric electrification, we used a\ntable-top experiment that mechanically shakes particles inside a cell where we\nmeasure the macroscopic electric field between the electrically conducting end\nplates. The measured electric fields are a result of capacitive coupling and\ndirect charge transfer between the particles and the plates. Using a diverse\nrange of mono-material particle sets (plastics, ceramic, glass, and metals), we\nfound that all our particle materials electrify and show similar dynamics with\nlong time-scale temporal variation and an electric field amplitude that depends\non the particle quantity in a complex way. These results suggest that while\nparticle properties do matter like previous investigations have shown,\nmacroscopic electrification of solids is relatively material agnostic and large\nscale collective phenomena play a major role. \n\n"}
{"id": "1509.04613", "contents": "Title: Gaussian process surrogates for failure detection: a Bayesian\n  experimental design approach Abstract: An important task of uncertainty quantification is to identify {the\nprobability of} undesired events, in particular, system failures, caused by\nvarious sources of uncertainties. In this work we consider the construction of\nGaussian {process} surrogates for failure detection and failure probability\nestimation. In particular, we consider the situation that the underlying\ncomputer models are extremely expensive, and in this setting, determining the\nsampling points in the state space is of essential importance. We formulate the\nproblem as an optimal experimental design for Bayesian inferences of the limit\nstate (i.e., the failure boundary) and propose an efficient numerical scheme to\nsolve the resulting optimization problem. In particular, the proposed\nlimit-state inference method is capable of determining multiple sampling points\nat a time, and thus it is well suited for problems where multiple computer\nsimulations can be performed in parallel. The accuracy and performance of the\nproposed method is demonstrated by both academic and practical examples. \n\n"}
{"id": "1509.04752", "contents": "Title: Bayesian inference for spatio-temporal spike-and-slab priors Abstract: In this work, we address the problem of solving a series of underdetermined\nlinear inverse problems subject to a sparsity constraint. We generalize the\nspike-and-slab prior distribution to encode a priori correlation of the support\nof the solution in both space and time by imposing a transformed Gaussian\nprocess on the spike-and-slab probabilities. An expectation propagation (EP)\nalgorithm for posterior inference under the proposed model is derived. For\nlarge scale problems, the standard EP algorithm can be prohibitively slow. We\ntherefore introduce three different approximation schemes to reduce the\ncomputational complexity. Finally, we demonstrate the proposed model using\nnumerical experiments based on both synthetic and real data sets. \n\n"}
{"id": "1509.07993", "contents": "Title: Parallel Metropolis chains with cooperative adaptation Abstract: Monte Carlo methods, such as Markov chain Monte Carlo (MCMC) algorithms, have\nbecome very popular in signal processing over the last years. In this work, we\nintroduce a novel MCMC scheme where parallel MCMC chains interact, adapting\ncooperatively the parameters of their proposal functions. Furthermore, the\nnovel algorithm distributes the computational effort adaptively, rewarding the\nchains which are providing better performance and, possibly even stopping other\nones. These extinct chains can be reactivated if the algorithm considers\nnecessary. Numerical simulations shows the benefits of the novel scheme. \n\n"}
{"id": "1510.00024", "contents": "Title: Accelerating MCMC with active subspaces Abstract: The Markov chain Monte Carlo (MCMC) method is the computational workhorse for\nBayesian inverse problems. However, MCMC struggles in high-dimensional\nparameter spaces, since its iterates must sequentially explore the\nhigh-dimensional space. This struggle is compounded in physical applications\nwhen the nonlinear forward model is computationally expensive. One approach to\naccelerate MCMC is to reduce the dimension of the state space. Active subspaces\nare part of an emerging set of tools for subspace-based dimension reduction. An\nactive subspace in a given inverse problem indicates a separation between a\nlow-dimensional subspace that is informed by the data and its orthogonal\ncomplement that is constrained by the prior. With this information, one can run\nthe sequential MCMC on the active variables while sampling independently\naccording to the prior on the inactive variables. However, this approach to\nincrease efficiency may introduce bias. We provide a bound on the Hellinger\ndistance between the true posterior and its active subspace- exploiting\napproximation. And we demonstrate the active subspace-accelerated MCMC on two\ncomputational examples: (i) a two-dimensional parameter space with a quadratic\nforward model and one-dimensional active subspace and (ii) a 100-dimensional\nparameter space with a PDE-based forward model and a two-dimensional active\nsubspace. \n\n"}
{"id": "1510.00298", "contents": "Title: Statistical and Dynamical Properties of Covariant Lyapunov Vectors in a\n  Coupled Atmosphere-Ocean Model - Multiscale Effects, Geometric Degeneracy,\n  and Error Dynamics Abstract: We study a simplified coupled atmosphere-ocean model using the formalism of\ncovariant Lyapunov vectors (CLVs), which link physically-based directions of\nperturbations to growth/decay rates. The model is obtained via a severe\ntruncation of quasi-geostrophic equations for the two fluids, and includes a\nsimple yet physically meaningful representation of their\ndynamical/thermodynamical coupling. The model has 36 degrees of freedom, and\nthe parameters are chosen so that a chaotic behaviour is observed. One finds\ntwo positive Lyapunov exponents (LEs), sixteen negative LEs, and eighteen\nnear-zero LEs. The presence of many near-zero LEs results from the vast\ntime-scale separation between the characteristic time scales of the two fluids,\nand leads to nontrivial error growth properties in the tangent space spanned by\nthe corresponding CLVs, which are geometrically very degenerate. Such CLVs\ncorrespond to two different classes of ocean/atmosphere coupled modes. The\ntangent space spanned by the CLVs corresponding to the positive and negative\nLEs has, instead, a non-pathological behaviour, and one can construct robust\nlarge deviations laws for the finite time LEs, thus providing a universal model\nfor assessing predictability on long to ultra-long scales along such\ndirections. It is somewhat surprising to find that the tangent space of the\nunstable manifold has strong projection on both atmospheric and oceanic\ncomponents. Our results underline the difficulties in using hyperbolicity as a\nconceptual framework for multiscale chaotic dynamical systems, whereas the\nframework of partial hyperbolicity seems better suited, possibly indicating an\nalternative definition for the chaotic hypothesis. Our results suggest the need\nfor accurate analysis of error dynamics on different time scales and domains\nand for a careful set-up of assimilation schemes when looking at coupled\natmosphere-ocean models. \n\n"}
{"id": "1510.00967", "contents": "Title: The Proximal Robbins-Monro Method Abstract: The need for parameter estimation with massive datasets has reinvigorated\ninterest in stochastic optimization and iterative estimation procedures.\nStochastic approximations are at the forefront of this recent development as\nthey yield procedures that are simple, general, and fast. However, standard\nstochastic approximations are often numerically unstable. Deterministic\noptimization, on the other hand, increasingly uses proximal updates to achieve\nnumerical stability in a principled manner. A theoretical gap has thus emerged.\nWhile standard stochastic approximations are subsumed by the framework of\nRobbins and Monro (1951), there is no such framework for stochastic\napproximations with proximal updates. In this paper, we conceptualize a\nproximal version of the classical Robbins-Monro procedure. Our theoretical\nanalysis demonstrates that the proposed procedure has important stability\nbenefits over the classical Robbins-Monro procedure, while it retains the best\nknown convergence rates. Exact implementations of the proximal Robbins-Monro\nprocedure are challenging, but we show that approximate implementations lead to\nprocedures that are easy to implement, and still dominate classical procedures\nby achieving numerical stability, practically without tradeoffs. Moreover,\napproximate proximal Robbins-Monro procedures can be applied even when the\nobjective cannot be calculated analytically, and so they generalize stochastic\nproximal procedures currently in use. \n\n"}
{"id": "1510.04254", "contents": "Title: New singularities for Stokes waves Abstract: In 1880, Stokes famously demonstrated that the singularity that occurs at the\ncrest of the steepest possible water wave in infinite depth must correspond to\na corner of $120^\\circ$. Here, the complex velocity scales like $f^{1/3}$ where\n$f$ is the complex potential. Later in 1973, Grant showed that for any wave\naway from the steepest configuration, the singularity $f = f^*$ moves into the\ncomplex plane, and is of order $(f-f^*)^{1/2}$ (J. Fluid Mech., vol. 59, 1973,\npp. 257-262). Grant conjectured that as the highest wave is approached, other\nsingularities must coalesce at the crest so as to cancel the square-root\nbehaviour. Despite recent advances, the complete singularity structure of the\nStokes wave is still not well understood. In this work, we develop numerical\nmethods for constructing the Riemann surface that represents the extension of\nthe water wave into the complex plane. We show that a countably infinite number\nof distinct singularities exists on other branches of the solution, and that\nthese singularities coalesce as Stokes' highest wave is approached. \n\n"}
{"id": "1511.00108", "contents": "Title: Recursive computation for evaluating the exact $p$-values of temporal\n  and spatial scan statistics Abstract: Let $V$ be a finite set of indices, and let $B_i$, $i=1,\\ldots,m$, be subsets\nof $V$ such that $V=\\bigcup_{i=1}^{m}B_i$. Let $X_i$, $i\\in V$, be independent\nrandom variables, and let $X_{B_i}=(X_j)_{j\\in B_i}$. In this paper, we propose\na recursive computation method to calculate the conditional expectation\n$E\\bigl[\\prod_{i=1}^m\\chi_i(X_{B_i}) \\,|\\, N\\bigr]$ with $N=\\sum_{i\\in V}X_i$\ngiven, where $\\chi_i$ is an arbitrary function. Our method is based on the\nrecursive summation/integration technique using the Markov property in\nstatistics. To extract the Markov property, we define an undirected graph whose\ncliques are $B_j$, and obtain its chordal extension, from which we present the\nexpressions of the recursive formula. This methodology works for a class of\ndistributions including the Poisson distribution (that is, the conditional\ndistribution is the multinomial). This problem is motivated from the evaluation\nof the multiplicity-adjusted $p$-value of scan statistics in spatial\nepidemiology. As an illustration of the approach, we present the real data\nanalyses to detect temporal and spatial clustering. \n\n"}
{"id": "1511.00620", "contents": "Title: On the so-called rogue waves in the nonlinear Schr\\\"odinger equation Abstract: The mechanism of a rogue water wave is still unknown. One popular conjecture\nis that the Peregrine wave solution of the nonlinear Schr\\\"odinger equation\n(NLS) provides a mechanism. A Peregrine wave solution can be obtained by taking\nthe infinite spatial period limit to the homoclinic solutions. In this article,\nfrom the perspective of the phase space structure of these homoclinic orbits in\nthe infinite dimensional phase space where the NLS defines a dynamical system,\nwe exam the observability of these homoclinic orbits (and their\napproximations). Our conclusion is that these approximate homoclinic orbits are\nthe most observable solutions,and they should correspond to the most common\ndeep ocean waves rather than the rare rogue waves. We also discuss other\npossibilities for the mechanism of a rogue wave: rough dependence on initial\ndata or finite time blow up. \n\n"}
{"id": "1511.02543", "contents": "Title: Sandwiching the marginal likelihood using bidirectional Monte Carlo Abstract: Computing the marginal likelihood (ML) of a model requires marginalizing out\nall of the parameters and latent variables, a difficult high-dimensional\nsummation or integration problem. To make matters worse, it is often hard to\nmeasure the accuracy of one's ML estimates. We present bidirectional Monte\nCarlo, a technique for obtaining accurate log-ML estimates on data simulated\nfrom a model. This method obtains stochastic lower bounds on the log-ML using\nannealed importance sampling or sequential Monte Carlo, and obtains stochastic\nupper bounds by running these same algorithms in reverse starting from an exact\nposterior sample. The true value can be sandwiched between these two stochastic\nbounds with high probability. Using the ground truth log-ML estimates obtained\nfrom our method, we quantitatively evaluate a wide variety of existing ML\nestimators on several latent variable models: clustering, a low rank\napproximation, and a binary attributes model. These experiments yield insights\ninto how to accurately estimate marginal likelihoods. \n\n"}
{"id": "1511.03045", "contents": "Title: Influence of Atmospheric Electric Fields on the Radio Emission from\n  Extensive Air Showers Abstract: The atmospheric electric fields in thunderclouds have been shown to\nsignificantly modify the intensity and polarization patterns of the radio\nfootprint of cosmic-ray-induced extensive air showers. Simulations indicated a\nvery non-linear dependence of the signal strength in the frequency window of\n30-80 MHz on the magnitude of the atmospheric electric field. In this work we\npresent an explanation of this dependence based on Monte-Carlo simulations,\nsupported by arguments based on electron dynamics in air showers and expressed\nin terms of a simplified model. We show that by extending the frequency window\nto lower frequencies additional sensitivity to the atmospheric electric field\nis obtained. \n\n"}
{"id": "1511.04992", "contents": "Title: The Correlated Pseudo-Marginal Method Abstract: The pseudo-marginal algorithm is a popular variant of the\nMetropolis--Hastings scheme which allows us to sample asymptotically from a\ntarget probability density $\\pi$, when we are only able to estimate an\nunnormalized version of $\\pi$ pointwise unbiasedly. It has found numerous\napplications in Bayesian statistics as there are many scenarios where the\nlikelihood function is intractable but can be estimated unbiasedly using Monte\nCarlo samples. Using many samples will typically result in averages computed\nunder this chain with lower asymptotic variances than the corresponding\naverages that use fewer samples. For a fixed computing time, it has been shown\nin several recent contributions that an efficient implementation of the\npseudo-marginal method requires the variance of the log-likelihood ratio\nestimator appearing in the acceptance probability of the algorithm to be of\norder 1, which in turn usually requires scaling the number $N$ of Monte Carlo\nsamples linearly with the number $T$ of data points. We propose a modification\nof the pseudo-marginal algorithm, termed the correlated pseudo-marginal\nalgorithm, which is based on a novel log-likelihood ratio estimator computed\nusing the difference of two positively correlated log-likelihood estimators. We\nshow that the parameters of this scheme can be selected such that the variance\nof this estimator is order $1$ as $N,T\\rightarrow\\infty$ whenever\n$N/T\\rightarrow 0$. By combining these results with the Bernstein-von Mises\ntheorem, we provide an analysis of the performance of the correlated\npseudo-marginal algorithm in the large $T$ regime. In our numerical examples,\nthe efficiency of computations is increased relative to the standard\npseudo-marginal algorithm by more than 20 fold for values of $T$ of a few\nhundreds to more than 100 fold for values of $T$ of around 10,000-20,000. \n\n"}
{"id": "1511.05604", "contents": "Title: blavaan: Bayesian structural equation models via parameter expansion Abstract: This article describes blavaan, an R package for estimating Bayesian\nstructural equation models (SEMs) via JAGS and for summarizing the results. It\nalso describes a novel parameter expansion approach for estimating specific\ntypes of models with residual covariances, which facilitates estimation of\nthese models in JAGS. The methodology and software are intended to provide\nusers with a general means of estimating Bayesian SEMs, both classical and\nnovel, in a straightforward fashion. Users can estimate Bayesian versions of\nclassical SEMs with lavaan syntax, they can obtain state-of-the-art Bayesian\nfit measures associated with the models, and they can export JAGS code to\nmodify the SEMs as desired. These features and more are illustrated by example,\nand the parameter expansion approach is explained in detail. \n\n"}
{"id": "1511.06008", "contents": "Title: Glacial Cycles and Milankovitch Forcing Abstract: Using a recent conceptual model of the glacial-interglacial cycles we present\nmore evidence of Milankovitch cycles being the trigger for retreat and forming\nof ice sheets in the cycles. This model is based on a finite approximation of\nan infinite dimensional model which has three components: Budyko's energy\nbalance model describing the annual mean temperatures at latitudes, Widiasih's\nODE which describes the behavior of the edge of the ice sheet, and Walsh et al.\nwho introduced a snow line to account for glacial accumulation and ablation\nzones. Certain variables in the model are made to depend on the Milankovitch\ncycles, in particular, the obliquity of the Earth's axis and the eccentricity\nof the Earth's orbit. We see as a result that deglaciation and glaciation do\noccur mostly due to obliquity and to some extent eccentricity. \n\n"}
{"id": "1511.06286", "contents": "Title: The iterated auxiliary particle filter Abstract: We present an offline, iterated particle filter to facilitate statistical\ninference in general state space hidden Markov models. Given a model and a\nsequence of observations, the associated marginal likelihood L is central to\nlikelihood-based inference for unknown statistical parameters. We define a\nclass of \"twisted\" models: each member is specified by a sequence of positive\nfunctions psi and has an associated psi-auxiliary particle filter that provides\nunbiased estimates of L. We identify a sequence psi* that is optimal in the\nsense that the psi*-auxiliary particle filter's estimate of L has zero\nvariance. In practical applications, psi* is unknown so the psi*-auxiliary\nparticle filter cannot straightforwardly be implemented. We use an iterative\nscheme to approximate psi*, and demonstrate empirically that the resulting\niterated auxiliary particle filter significantly outperforms the bootstrap\nparticle filter in challenging settings. Applications include parameter\nestimation using a particle Markov chain Monte Carlo algorithm. \n\n"}
{"id": "1512.00933", "contents": "Title: Probabilistic Integration: A Role in Statistical Computation? Abstract: A research frontier has emerged in scientific computation, wherein numerical\nerror is regarded as a source of epistemic uncertainty that can be modelled.\nThis raises several statistical challenges, including the design of statistical\nmethods that enable the coherent propagation of probabilities through a\n(possibly deterministic) computational work-flow. This paper examines the case\nfor probabilistic numerical methods in routine statistical computation. Our\nfocus is on numerical integration, where a probabilistic integrator is equipped\nwith a full distribution over its output that reflects the presence of an\nunknown numerical error. Our main technical contribution is to establish, for\nthe first time, rates of posterior contraction for these methods. These show\nthat probabilistic integrators can in principle enjoy the \"best of both\nworlds\", leveraging the sampling efficiency of Monte Carlo methods whilst\nproviding a principled route to assess the impact of numerical error on\nscientific conclusions. Several substantial applications are provided for\nillustration and critical evaluation, including examples from statistical\nmodelling, computer graphics and a computer model for an oil reservoir. \n\n"}
{"id": "1512.00982", "contents": "Title: Bayesian non-parametric inference for $\\Lambda$-coalescents: consistency\n  and a parametric method Abstract: We investigate Bayesian non-parametric inference of the $\\Lambda$-measure of\n$\\Lambda$-coalescent processes with recurrent mutation, parametrised by\nprobability measures on the unit interval. We give verifiable criteria on the\nprior for posterior consistency when observations form a time series, and prove\nthat any non-trivial prior is inconsistent when all observations are\ncontemporaneous. We then show that the likelihood given a data set of size $n\n\\in \\mathbb{N}$ is constant across $\\Lambda$-measures whose leading $n - 2$\nmoments agree, and focus on inferring truncated sequences of moments. We\nprovide a large class of functionals which can be extremised using finite\ncomputation given a credible region of posterior truncated moment sequences,\nand a pseudo-marginal Metropolis-Hastings algorithm for sampling the posterior.\nFinally, we compare the efficiency of the exact and noisy pseudo-marginal\nalgorithms with and without delayed acceptance acceleration using a simulation\nstudy. \n\n"}
{"id": "1512.01139", "contents": "Title: Kalman-based Stochastic Gradient Method with Stop Condition and\n  Insensitivity to Conditioning Abstract: Modern proximal and stochastic gradient descent (SGD) methods are believed to\nefficiently minimize large composite objective functions, but such methods have\ntwo algorithmic challenges: (1) a lack of fast or justified stop conditions,\nand (2) sensitivity to the objective function's conditioning. In response to\nthe first challenge, modern proximal and SGD methods guarantee convergence only\nafter multiple epochs, but such a guarantee renders proximal and SGD methods\ninfeasible when the number of component functions is very large or infinite. In\nresponse to the second challenge, second order SGD methods have been developed,\nbut they are marred by the complexity of their analysis. In this work, we\naddress these challenges on the limited, but important, linear regression\nproblem by introducing and analyzing a second order proximal/SGD method based\non Kalman Filtering (kSGD). Through our analysis, we show kSGD is\nasymptotically optimal, develop a fast algorithm for very large, infinite or\nstreaming data sources with a justified stop condition, prove that kSGD is\ninsensitive to the problem's conditioning, and develop a unique approach for\nanalyzing the complex second order dynamics. Our theoretical results are\nsupported by numerical experiments on three regression problems (linear,\nnonparametric wavelet, and logistic) using three large publicly available\ndatasets. Moreover, our analysis and experiments lay a foundation for embedding\nkSGD in multiple epoch algorithms, extending kSGD to other problem classes, and\ndeveloping parallel and low memory kSGD implementations. \n\n"}
{"id": "1512.01272", "contents": "Title: CrossCat: A Fully Bayesian Nonparametric Method for Analyzing\n  Heterogeneous, High Dimensional Data Abstract: There is a widespread need for statistical methods that can analyze\nhigh-dimensional datasets with- out imposing restrictive or opaque modeling\nassumptions. This paper describes a domain-general data analysis method called\nCrossCat. CrossCat infers multiple non-overlapping views of the data, each\nconsisting of a subset of the variables, and uses a separate nonparametric\nmixture to model each view. CrossCat is based on approximately Bayesian\ninference in a hierarchical, nonparamet- ric model for data tables. This model\nconsists of a Dirichlet process mixture over the columns of a data table in\nwhich each mixture component is itself an independent Dirichlet process mixture\nover the rows; the inner mixture components are simple parametric models whose\nform depends on the types of data in the table. CrossCat combines strengths of\nmixture modeling and Bayesian net- work structure learning. Like mixture\nmodeling, CrossCat can model a broad class of distributions by positing latent\nvariables, and produces representations that can be efficiently conditioned and\nsampled from for prediction. Like Bayesian networks, CrossCat represents the\ndependencies and independencies between variables, and thus remains accurate\nwhen there are multiple statistical signals. Inference is done via a scalable\nGibbs sampling scheme; this paper shows that it works well in practice. This\npaper also includes empirical results on heterogeneous tabular data of up to 10\nmillion cells, such as hospital cost and quality measures, voting records,\nunemployment rates, gene expression measurements, and images of handwritten\ndigits. CrossCat infers structure that is consistent with accepted findings and\ncommon-sense knowledge in multiple domains and yields predictive accuracy\ncompetitive with generative, discriminative, and model-free alternatives. \n\n"}
{"id": "1512.03976", "contents": "Title: An iterative importance sampler for Bayesian parameter estimation in\n  stochastic models of multicellular clocks Abstract: We investigate a stochastic version of the synthetic multicellular clock\nmodel proposed by Garcia-Ojalvo, Elowitz and Strogatz. By introducing dynamical\nnoise in the model and assuming that the partial observations of the system can\nbe contaminated by additive noise, we enable a principled mechanism to\nrepresent experimental uncertainties in the synthesis of the multicellular\nsystem and pave the way for the design of probabilistic methods for the\nestimation of any unknowns in the model. Within this setup, we investigate the\nuse of an iterative importance sampling scheme, termed nonlinear population\nMonte Carlo (NPMC), for the Bayesian estimation of the model parameters. The\nalgorithm yields a stochastic approximation of the posterior probability\ndistribution of the unknown parameters given the available data (partial and\npossibly noisy observations). We prove a new theoretical result for this\nalgorithm, which indicates that the approximations converge almost surely to\nthe actual distributions, even when the weights in the importance sampling\nscheme cannot be computed exactly. We also provide a detailed numerical\nassessment of the stochastic multicellular model and the accuracy of the\nproposed NPMC algorithm, including a comparison with the popular particle\nMetropolis-Hastings algorithm of Andrieu {\\em et al.}, 2010, applied to the\nsame model and an approximate Bayesian computation sequential Monte Carlo\nmethod introduced by Mari\\~no {\\em et al.}, 2013. \n\n"}
{"id": "1512.06286", "contents": "Title: Compressive spectral method for the simulation of the water waves Abstract: In this paper an approach for decreasing the computational effort required\nfor the spectral simulations of the water waves is introduced. Signals with\nmajority of the components zero, are known as the sparse signals. Like majority\nof the signals in the nature it can be realized that water waves are sparse\neither in time or in the frequency domain. Using the sparsity property of the\nwater waves in the time or in the frequency domain, the compressive sampling\nalgorithm can be used as a tool for improving the performance of the spectral\nsimulation of the water waves. The methodology offered in this paper depends on\nthe idea of using a smaller number of spectral components compared to the\nclassical spectral method with a high number of components. After performing\nthe time integration with a smaller number of spectral components and using the\ncompressive sampling technique, it is shown that the water wave field can be\nreconstructed with a significantly better efficiency compared to the classical\nspectral method with a high number of spectral components, especially for long\ntime evolutions.\n  For the sparse water wave model in the time domain the well-known solitary\nwave solutions of the Korteweg-deVries (KdV) equation is considered. For the\nsparse water wave model in the frequency domain the well-known Airy (linear)\nocean waves with Jonswap spectrum is considered. Utilizing a spectral method,\nit is shown that by using a smaller number of spectral components compared to\nthe classical spectral method with a high number of components, it is possible\nto simulate the sparse water waves with negligible error in accuracy and a\ngreat efficiency especially for large time evolutions. \n\n"}
{"id": "1512.08650", "contents": "Title: An Importance Sampling Scheme for Models in a Strong External Field Abstract: We propose Monte Carlo methods to estimate the partition function of the\ntwo-dimensional Ising model in the presence of an external magnetic field. The\nestimation is done in the dual of the Forney factor graph representing the\nmodel. The proposed methods can efficiently compute an estimate of the\npartition function in a wide range of model parameters. As an example, we\nconsider models that are in a strong external field. \n\n"}
{"id": "1512.08851", "contents": "Title: Limited fetch revisited: comparison of wind input terms, in surface\n  waves modeling Abstract: Results pertaining to numerical solutions of the Hasselmann kinetic equation\n(HE), for wind driven sea spectra, in the fetch limited geometry, are\npresented. Five versions of source functions, including the recently introduced\nZRP model, have been studied, for the exact expression of Snl and\nhigh-frequency implicit dissipation, due to wave-breaking. Four of the five\nexperiments were done in the absence of spectral peak dissipation for various\nSin terms. They demonstrated the dominance of quadruplet wave-wave interaction,\nin the energy balance, and the formation of self-similar regimes, of unlimited\nwave energy growth, along the fetch. Between them was the ZRP model, which\nstrongly agreed with dozens of field observations performed in the seas and\nlakes, since 1947. The fifth, the WAM3 wind input term experiment, used\nadditional spectral peak dissipation and reproduced the results of a previous,\nsimilar, numerical simulation, but only supported the field experiments for\nmoderate fetches, demonstrating a total energy saturation at half of that of\nthe Pierson-Moscowits limit. The alternative framework for HE numerical\nsimulation is proposed, along with a set of tests, allowing one to select\nphysically-justified source terms. \n\n"}
{"id": "1601.01125", "contents": "Title: The Gibbs Sampler with Particle Efficient Importance Sampling for\n  State-Space Models Abstract: We consider Particle Gibbs (PG) as a tool for Bayesian analysis of non-linear\nnon-Gaussian state-space models. PG is a Monte Carlo (MC) approximation of the\nstandard Gibbs procedure which uses sequential MC (SMC) importance sampling\ninside the Gibbs procedure to update the latent and potentially\nhigh-dimensional state trajectories. We propose to combine PG with a generic\nand easily implementable SMC approach known as Particle Efficient Importance\nSampling (PEIS). By using SMC importance sampling densities which are\napproximately fully globally adapted to the targeted density of the states,\nPEIS can substantially improve the mixing and the efficiency of the PG draws\nfrom the posterior of the states and the parameters relative to existing PG\nimplementations. The efficiency gains achieved by PEIS are illustrated in PG\napplications to a univariate stochastic volatility model for asset returns, a\nnon-Gaussian nonlinear local-level model for interest rates, and a multivariate\nstochastic volatility model for the realized covariance matrix of asset\nreturns. \n\n"}
{"id": "1601.01178", "contents": "Title: Weakly informative reparameterisations for location-scale mixtures Abstract: While mixtures of Gaussian distributions have been studied for more than a\ncentury (Pearson, 1894), the construction of a reference Bayesian analysis of\nthose models still remains unsolved, with a general prohibition of the usage of\nimproper priors (Fruwirth-Schnatter, 2006) due to the ill-posed nature of such\nstatistical objects. This difficulty is usually bypassed by an empirical Bayes\nresolution (Richardson and Green, 1997). By creating a new parameterisation\ncantered on the mean and possibly the variance of the mixture distribution\nitself, we manage to develop here a weakly informative prior for a wide class\nof mixtures with an arbitrary number of components. We demonstrate that some\nposterior distributions associated with this prior and a minimal sample size\nare proper. We provide MCMC implementations that exhibit the expected\nexchangeability. We only study here the univariate case, the extension to\nmultivariate location-scale mixtures being currently under study. An R package\ncalled Ultimixt is associated with this paper. \n\n"}
{"id": "1601.02410", "contents": "Title: A novel approach for Markov Random Field with intractable normalising\n  constant on large lattices Abstract: The pseudo likelihood method of Besag(1974), has remained a popular method\nfor estimating Markov random field on a very large lattice, despite various\ndocumented deficiencies. This is partly because it remains the only\ncomputationally tractable method for large lattices. We introduce a novel\nmethod to estimate Markov random fields defined on a regular lattice. The\nmethod takes advantage of conditional independence structures and recursively\ndecomposes a large lattice into smaller sublattices. An approximation is made\nat each decomposition. Doing so completely avoids the need to compute the\ntroublesome normalising constant. The computational complexity is $O(N)$, where\n$N$ is the the number of pixels in lattice, making it computationally\nattractive for very large lattices. We show through simulation, that the\nproposed method performs well, even when compared to the methods using exact\nlikelihoods. \n\n"}
{"id": "1601.03050", "contents": "Title: Optical phase curves as diagnostics for aerosol composition in\n  exoplanetary atmospheres Abstract: Optical phase curves have become one of the common probes of exoplanetary\natmospheres, but the information they encode has not been fully elucidated.\nBuilding on a diverse body of work, we upgrade the Flexible Modeling System\n(FMS) to include scattering in the two-stream, dual-band approximation and\ngenerate plausible, three-dimensional structures of irradiated atmospheres to\nstudy the radiative effects of aerosols or condensates. In the optical, we\ntreat the scattering of starlight using a generalisation of Beer's law that\nallows for a finite Bond albedo to be prescribed. In the infrared, we implement\nthe two-stream solutions and include scattering via an infrared scattering\nparameter. We present a suite of four-parameter general circulation models for\nKepler-7b and demonstrate that its climatology is expected to be robust to\nvariations in optical and infrared scattering. The westward and eastward shifts\nof the optical and infrared phase curves, respectively, are shown to be robust\noutcomes of the simulations. Assuming micron-sized particles and a simplified\ntreatment of local brightness, we further show that the peak offset of the\noptical phase curve is sensitive to the composition of the aerosols or\ncondensates. However, to within the measurement uncertainties, we cannot\ndistinguish between aerosols made of silicates (enstatite or forsterite), iron,\ncorundum or titanium oxide, based on a comparison to the measured peak offset\n($41^\\circ \\pm 12^\\circ$) of the optical phase curve of Kepler-7b. Measuring\nhigh-precision optical phase curves will provide important constraints on the\natmospheres of cloudy exoplanets and reduce degeneracies in interpreting their\ninfrared spectra. \n\n"}
{"id": "1601.03704", "contents": "Title: Computationally efficient change point detection for high-dimensional\n  regression Abstract: Large-scale sequential data is often exposed to some degree of inhomogeneity\nin the form of sudden changes in the parameters of the data-generating process.\nWe consider the problem of detecting such structural changes in a\nhigh-dimensional regression setting. We propose a joint estimator of the number\nand the locations of the change points and of the parameters in the\ncorresponding segments. The estimator can be computed using dynamic programming\nor, as we emphasize here, it can be approximated using a binary search\nalgorithm with $O(n \\log(n) \\mathrm{Lasso}(n))$ computational operations while\nstill enjoying essentially the same theoretical properties; here\n$\\mathrm{Lasso}(n)$ denotes the computational cost of computing the Lasso for\nsample size $n$. We establish oracle inequalities for the estimator as well as\nfor its binary search approximation, covering also the case with a large\n(asymptotically growing) number of change points. We evaluate the performance\nof the proposed estimation algorithms on simulated data and apply the\nmethodology to real data. \n\n"}
{"id": "1601.04065", "contents": "Title: Efficient Kernel Convolution for Smooth Surfaces without Edge Effects Abstract: One of the most efficient ways to produce unconditional simulations is with\nthe kernel convolution using fast Fourier transform (FFT) [1]. However, when\ndata is located on a surface, this approach is not efficient because data needs\nto be processed in a three-dimensional enclosing box. This paper describes a\nnovel approach based on integer transformation to reduce the volume of the\nenclosing box. \n\n"}
{"id": "1601.06486", "contents": "Title: Cliffs Benchmarking Abstract: A numerical model for tsunami simulations Cliffs is exercised with the\ncomplete set of NTHMP-selected benchmark problems focused on inundation, such\nas simulating runup of a non-breaking solitary wave onto a sloping beach, runup\non a conical island, a lab experiment with a scaled model of Monai Valley, and\nthe 1993 Hokkiado tsunami and inundation of the Okushiri Island. \n\n"}
{"id": "1601.06720", "contents": "Title: The Generalized Quasilinear Approximation: Application to Zonal Jets Abstract: Quasilinear theory is often utilized to approximate the dynamics of fluids\nexhibiting significant interactions between mean flows and eddies. In this\npaper we present a generalization of quasilinear theory to include dynamic mode\ninteractions on the large scales. This generalized quasilinear (GQL)\napproximation is achieved by separating the state variables into large and\nsmall zonal scales via a spectral filter rather than by a decomposition into a\nformal mean and fluctuations. Nonlinear interactions involving only small zonal\nscales are then removed. The approximation is conservative and allows for\nscattering of energy between small-scale modes via the large scale (through\nnon-local spectral interactions). We evaluate GQL for the paradigmatic problems\nof the driving of large-scale jets on a spherical surface and on the beta-plane\nand show that it is accurate even for a small number of large-scale modes. As\nthis approximation is formally linear in the small zonal scales it allows for\nthe closure of the system and can be utilized in direct statistical simulation\nschemes that have proved an attractive alternative to direct numerical\nsimulation for many geophysical and astrophysical problems. \n\n"}
{"id": "1602.00207", "contents": "Title: Binomial and Multinomial Proportions: Accurate Estimation and Reliable\n  Assessment of Accuracy Abstract: Misestimates of $\\sigma_{P_o}$, the \\emph{uncertainty} in $P_o$ from a\n2-state Bayes equation used for binary classification, apparently arose from\n$\\hat{\\sigma}_{p_i}$, the uncertainty in underlying pdfs estimated from\nexperimental $b$-bin histograms. To address this, several Bayesian estimator\npairs $(\\hat{p}_i, \\hat{\\sigma}_{p_i})$ were compared for agreement between\nnominal confidence level ($\\xi$) and calculated coverage values ($C$). Large\n$\\xi$-to-$C$ inconsistency for large $b$ and $ p_i \\gg \\frac{1}{b}$ arises for\nall multinomial estimators since priors downweight low likelihood, high $p_i$\nvalues. To improve $\\xi$-to-$C$ matching, $(\\xi-C)^2$ was minimized against\n$\\alpha_0$ in a more general prior pdf\n($\\mathcal{B}[\\alpha_0,(b-1)\\alpha_0;x]$) to obtain\n$(\\hat{p_i})_{\\xi\\leftrightarrow C}$. This improved matching for $b=2$, but for\n$b>2$, $\\xi$-to-$C$ matching by $(\\hat{p_i})_{\\xi\\leftrightarrow C}$ required\nan effective value \"$b=2$\" and renormalization, and this reduced\n$\\hat{p}_i$-to-$p_i$ matching. Better $\\hat{p}_i$-to-$p_i$ matching came from\nthe original multinomial estimators, a new discrete-domain estimator\n$\\hat{p}(n_i,N)$, or an earlier \\emph{joint} estimator, $(\\hat{p_i})_{\\bowtie}$\nthat co-adjusted all estimates $p_i$ for James-Stein shrinkage to a mean\nvector. Best simultaneous $\\xi$-to-$C$ and $\\hat{p}_i$-to-$p_i$ matching came\nby \\emph{de-noising} initial estimates of underlying pdfs. For $b=100$,\n$N<12800$, de-noised $\\hat{p}$ needed $\\approx 10\\times$ fewer observations to\nachieve $\\hat{p}_i$-to-$p_i$ matching equivalent to that found for\n$\\hat{p}(n_i,N)$, $(\\hat{p_i})_{\\bowtie}$ or the original multinomial\n$\\hat{p}_i$. De-noising each different type of initial estimate yielded\nsimilarly high accuracy in Monte-Carlo tests. \n\n"}
{"id": "1602.04805", "contents": "Title: DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression Abstract: Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems. \n\n"}
{"id": "1602.05339", "contents": "Title: An extended Kundu-Eckhaus equation for modeling dynamics of rogue waves\n  in a chaotic wave-current field Abstract: In this paper we propose an extended Kundu-Eckhaus equation (KEE) for\nmodeling the dynamics of skewed rogue waves emerging in the vicinity of a wave\nblocking point due to opposing current. The equation we propose is a KEE with\nan additional potential term therefore the results presented in this paper can\neasily be generalized to study the quantum tunneling properties of the rogue\nwaves and ultrashort (femtosecond) pulses of the KEE. In the frame of the\nextended KEE, we numerically show that the chaotic perturbations of the ocean\ncurrent trigger the occurrence of the rogue waves on the ocean surface. We\npropose and implement a split-step scheme and show that the extended KEE that\nwe propose is unstable against random chaotic perturbations in the current\nprofile. These perturbations transform the monochromatic wave field into a\nchaotic sea state with many peaks. We numerically show that the shapes of rogue\nwaves due to perturbations in the current profile closely follow the form of\nrational rogue wave solutions, especially for the central peak. We also discuss\nthe effects of magnitude of the chaotic current perturbations on the statistics\nof the rogue wave occurrence. \n\n"}
{"id": "1602.05986", "contents": "Title: A Poisson process model for Monte Carlo Abstract: Simulating samples from arbitrary probability distributions is a major\nresearch program of statistical computing. Recent work has shown promise in an\nold idea, that sampling from a discrete distribution can be accomplished by\nperturbing and maximizing its mass function. Yet, it has not been clearly\nexplained how this research project relates to more traditional ideas in the\nMonte Carlo literature. This chapter addresses that need by identifying a\nPoisson process model that unifies the perturbation and accept-reject views of\nMonte Carlo simulation. Many existing methods can be analyzed in this\nframework. The chapter reviews Poisson processes and defines a Poisson process\nmodel for Monte Carlo methods. This model is used to generalize the\nperturbation trick to infinite spaces by constructing Gumbel processes, random\nfunctions whose maxima are located at samples over infinite spaces. The model\nis also used to analyze A* sampling and OS*, two methods from distinct Monte\nCarlo families. \n\n"}
{"id": "1602.06225", "contents": "Title: GAP Safe Screening Rules for Sparse-Group-Lasso Abstract: In high dimensional settings, sparse structures are crucial for efficiency,\neither in term of memory, computation or performance. In some contexts, it is\nnatural to handle more refined structures than pure sparsity, such as for\ninstance group sparsity. Sparse-Group Lasso has recently been introduced in the\ncontext of linear regression to enforce sparsity both at the feature level and\nat the group level. We adapt to the case of Sparse-Group Lasso recent safe\nscreening rules that discard early in the solver irrelevant features/groups.\nSuch rules have led to important speed-ups for a wide range of iterative\nmethods. Thanks to dual gap computations, we provide new safe screening rules\nfor Sparse-Group Lasso and show significant gains in term of computing time for\na coordinate descent implementation. \n\n"}
{"id": "1602.06397", "contents": "Title: High solar cycle spectral variations inconsistent with stratospheric\n  ozone observations Abstract: Some of the natural variability in climate is understood to come from changes\nin the Sun. A key route whereby the Sun may influence surface climate is\ninitiated in the tropical stratosphere by the absorption of solar ultraviolet\n(UV) radiation by ozone, leading to a modification of the temperature and wind\nstructures and consequently to the surface through changes in wave propagation\nand circulation. While changes in total, spectrally-integrated, solar\nirradiance lead to small variations in global mean surface temperature, the\n`top-down' UV effect preferentially influences on regional scales at\nmid-to-high latitudes with, in particular, a solar signal noted in the North\nAtlantic Oscillation (NAO). The amplitude of the UV variability is fundamental\nin determining the magnitude of the climate response but understanding of the\nUV variations has been challenged recently by measurements from the SOlar\nRadiation and Climate Experiment (SORCE) satellite, which show UV solar cycle\nchanges up to 10 times larger than previously thought. Indeed, climate models\nusing these larger UV variations show a much greater response, similar to NAO\nobservations. Here we present estimates of the ozone solar cycle response using\na chemistry-climate model (CCM) in which the effects of transport are\nconstrained by observations. Thus the photolytic response to different spectral\nsolar irradiance (SSI) datasets can be isolated. Comparison of the results with\nthe solar signal in ozone extracted from observational datasets yields\nsignificantly discriminable responses. According to our evaluation the SORCE UV\ndataset is not consistent with the observed ozone response whereas the smaller\nvariations suggested by earlier satellite datasets, and by UV data from\nempirical solar models, are in closer agreement with the measured stratospheric\nvariations. Determining the most appropriate SSI variability to apply in\nmodels... \n\n"}
{"id": "1602.06714", "contents": "Title: Zonal flows as statistical equilibria Abstract: Zonal jets are striking and beautiful examples of the propensity for\ngeophysical turbulent flows to spontaneously self-organize into robust, large\nscale coherent structures. There exist many dynamical mechanisms for the\nformation of zonal jets: statistical theories (kinetic approaches, second order\nor larger oder closures), deterministic approaches (modulational instability,\n$\\beta$-plumes, radiating instability, zonostrophic turbulence, and so on). A\nstriking remark is that all these different dynamical approaches, each of them\npossibly relevant in some specific regimes, lead to the same kind of final jet\nstructures. Is it then possible to have a more general explanation of why all\nthese different dynamical regimes, from fully turbulent flows to gentle\nquasilinear regime, consistently lead to the same jet attractors ? Equilibrium\nstatistical mechanics provides an answer to this general question. Here we we\npresent the salient features of this theory and review applications of this\napproach to the description of zonal jets. We show that equilibrium states on a\nbeta plane or a sphere are usually zonal or quasi-zonal, and that increasing\nthe energy leads to bifurcations breaking the zonal symmetry. \n\n"}
{"id": "1602.07079", "contents": "Title: The Cessation Threshold of Nonsuspended Sediment Transport Across\n  Aeolian and Fluvial Environments Abstract: Using particle-scale simulations of non-suspended sediment transport for a\nlarge range of Newtonian fluids driving transport, including air and water, we\ndetermine the bulk transport cessation threshold $\\Theta^r_t$ by extrapolating\nthe transport load as a function of the dimensionless fluid shear stress\n(`Shields number') $\\Theta$ to the vanishing transport limit. In this limit,\nthe simulated steady states of continuous transport can be described by simple\nanalytical model equations relating the average transport layer properties to\nthe law of the wall flow velocity profile. We use this model to calculate\n$\\Theta^r_t$ for arbitrary environments and derive a general Shields-like\nthreshold diagram in which a Stokes-like number replaces the particle Reynolds\nnumber. Despite the simplicity of our hydrodynamic description, the predicted\ncessation threshold, both from the simulations and analytical model,\nquantitatively agrees with measurements for transport in air and viscous and\nturbulent liquids despite not being fitted to these measurements. We interpret\nthe analytical model as a description of a continuous rebound motion of\ntransported particles and thus $\\Theta^r_t$ as the minimal fluid shear stress\nneeded to compensate the average energy loss of transported particles during an\naverage rebound at the bed surface. This interpretation, supported by\nsimulations near $\\Theta^r_t$, implies that entrainment mechanisms are needed\nto sustain transport above $\\Theta^r_t$. While entrainment by turbulent events\nsustains intermittent transport, entrainment by particle-bed impacts sustains\ncontinuous transport. Combining our interpretations with the critical energy\ncriterion for incipient motion by Valyrakis and coworkers, we put forward a new\nconceptual picture of sediment transport intermittency. \n\n"}
{"id": "1602.08154", "contents": "Title: Efficient Bayesian Inference for Multivariate Factor Stochastic\n  Volatility Models Abstract: We discuss efficient Bayesian estimation of dynamic covariance matrices in\nmultivariate time series through a factor stochastic volatility model. In\nparticular, we propose two interweaving strategies (Yu and Meng, Journal of\nComputational and Graphical Statistics, 20(3), 531-570, 2011) to substantially\naccelerate convergence and mixing of standard MCMC approaches. Similar to\nmarginal data augmentation techniques, the proposed acceleration procedures\nexploit non-identifiability issues which frequently arise in factor models. Our\nnew interweaving strategies are easy to implement and come at almost no extra\ncomputational cost; nevertheless, they can boost estimation efficiency by\nseveral orders of magnitude as is shown in extensive simulation studies. To\nconclude, the application of our algorithm to a 26-dimensional exchange rate\ndata set illustrates the superior performance of the new approach for\nreal-world data. \n\n"}
{"id": "1602.08734", "contents": "Title: A Structured Variational Auto-encoder for Learning Deep Hierarchies of\n  Sparse Features Abstract: In this note we present a generative model of natural images consisting of a\ndeep hierarchy of layers of latent random variables, each of which follows a\nnew type of distribution that we call rectified Gaussian. These rectified\nGaussian units allow spike-and-slab type sparsity, while retaining the\ndifferentiability necessary for efficient stochastic gradient variational\ninference. To learn the parameters of the new model, we approximate the\nposterior of the latent variables with a variational auto-encoder. Rather than\nmaking the usual mean-field assumption however, the encoder parameterizes a new\ntype of structured variational approximation that retains the prior\ndependencies of the generative model. Using this structured posterior\napproximation, we are able to perform joint training of deep models with many\nlayers of latent random variables, without having to resort to stacking or\nother layerwise training procedures. \n\n"}
{"id": "1603.00293", "contents": "Title: RWebData: A High-Level Interface to the Programmable Web Abstract: The rise of the programmable web offers new opportunities for the empirically\ndriven social sciences. The access, compilation and preparation of data from\nthe programmable web for statistical analysis can, however, involve substantial\nup-front costs for the practical researcher. The R-package RWebData provides a\nhigh-level framework that allows data to be easily collected from the\nprogrammable web in a format that can directly be used for statistical analysis\nin R (R Core Team 2013) without bothering about the data's initial format and\nnesting structure. It was developed specifically for users who have no\nexperience with web technologies and merely use R as a statistical software.\nThe core idea and methodological contribution of the package are the\ndisentangling of parsing web data and mapping them with a generic algorithm\n(independent of the initial data structure) to a flat table-like\nrepresentation. This paper provides an overview of the high-level functions for\nR-users, explains the basic architecture of the package, and illustrates the\nimplemented data mapping algorithm. \n\n"}
{"id": "1603.01897", "contents": "Title: Bias Correction of Semiparametric Long Memory Parameter Estimators via\n  the Pre-filtered Sieve Bootstrap Abstract: This paper investigates bootstrap-based bias correction of semiparametric\nestimators of the long memory parameter, $d$, in fractionally integrated\nprocesses. The re-sampling method involves the application of the sieve\nbootstrap to data pre-filtered by a preliminary semiparametric estimate of the\nlong memory parameter. Theoretical justification for using the bootstrap\ntechnique to bias adjust log periodogram and semiparametric local Whittle\nestimators of the memory parameter is provided in the case where the true value\nof $d$ lies in the range $0\\leq d<0.5$. That the bootstrap method provides\nconfidence intervals with the correct asymptotic coverage is also proven, with\nthe intervals shown to adjust explicitly for bias, as estimated via the\nbootstrap. Simulation evidence comparing the performance of the bootstrap bias\ncorrection with analytical bias-correction techniques is presented. The\nbootstrap method is shown to produce notable bias reductions, in particular\nwhen applied to an estimator for which some degree of bias reduction has\nalready been accomplished by analytical means. \n\n"}
{"id": "1603.02532", "contents": "Title: On the inconsistency of $\\ell_1$-penalised sparse precision matrix\n  estimation Abstract: Various $\\ell_1$-penalised estimation methods such as graphical lasso and\nCLIME are widely used for sparse precision matrix estimation. Many of these\nmethods have been shown to be consistent under various quantitative assumptions\nabout the underlying true covariance matrix. Intuitively, these conditions are\nrelated to situations where the penalty term will dominate the optimisation. In\nthis paper, we explore the consistency of $\\ell_1$-based methods for a class of\nsparse latent variable -like models, which are strongly motivated by several\ntypes of applications. We show that all $\\ell_1$-based methods fail\ndramatically for models with nearly linear dependencies between the variables.\nWe also study the consistency on models derived from real gene expression data\nand note that the assumptions needed for consistency never hold even for modest\nsized gene networks and $\\ell_1$-based methods also become unreliable in\npractice for larger networks. \n\n"}
{"id": "1603.02834", "contents": "Title: Inference and rare event simulation for stopped Markov processes via\n  reverse-time sequential Monte Carlo Abstract: We present a sequential Monte Carlo algorithm for Markov chain trajectories\nwith proposals constructed in reverse time, which is advantageous when paths\nare conditioned to end in a rare set. The reverse time proposal distribution is\nconstructed by approximating the ratio of Green's functions in Nagasawa's\nformula. Conditioning arguments can be used to interpret these ratios as\nlow-dimensional conditional sampling distributions of some coordinates of the\nprocess given the others. Hence the difficulty in designing SMC proposals in\nhigh dimension is greatly reduced. We illustrate our method on estimating an\noverflow probability in a queueing model, the probability that a diffusion\nfollows a narrowing corridor, and the initial location of an infection in an\nepidemic model on a network. \n\n"}
{"id": "1603.03706", "contents": "Title: Quantifying the global atmospheric power budget Abstract: The power of atmospheric circulation is a key measure of the Earth's climate\nsystem. The mismatch between predictions and observations under a warming\nclimate calls for a reassessment of how atmospheric power $W$ is defined,\nestimated and constrained. Here we review published formulations for $W$ and\nshow how they differ when applied to a moist atmosphere. Three factors, a\nnon-zero source/sink in the continuity equation, the difference between\nvelocities of gaseous air and condensate, and interaction between the gas and\ncondensate modifying the equations of motion, affect the formulation of $W$.\nStarting from the thermodynamic definition of mechanical work, we derive an\nexpression for $W$ from an explicit consideration of the equations of motion\nand continuity. Our analyses clarify how some past formulations are incomplete\nor invalid. Three caveats are identified. First, $W$ critically depends on the\nboundary condition for gaseous air velocity at the Earth's surface. Second,\nconfusion between gaseous air velocity and mean velocity of air and condensate\nin the expression for $W$ results in gross errors despite the observed\nmagnitudes of these velocities are very close. Third, $W$ expressed in terms of\nmeasurable atmospheric parameters, air pressure and velocity, is\nscale-specific; this must be taken into account when adding contributions to\n$W$ from different processes. We present a formulation of the atmospheric power\nbudget, which distinguishes three components of $W$: the kinetic power\nassociated with horizontal pressure gradients ($W_K$), the gravitational power\nof precipitation ($W_P$) and the condensate loading ($W_c$). We use MERRA and\nNCAR/NCEP re-analyses to evaluate the atmospheric power budget at different\nscales: $W_K$ increases with temporal resolution approaching our theoretical\nestimate for condensation-induced circulation when all convective motion is\nresolved. \n\n"}
{"id": "1603.05038", "contents": "Title: CoinCalc -- A new R package for quantifying simultaneities of event\n  series Abstract: We present the new R package CoinCalc for performing event coincidence\nanalysis (ECA), a novel statistical method to quantify the simultaneity of\nevents contained in two series of observations, either as simultaneous or\nlagged coincidences within a user-specific temporal tolerance window. The\npackage also provides different analytical as well as surrogate-based\nsignificance tests (valid under different assumptions about the nature of the\nobserved event series) as well as an intuitive visualization of the identified\ncoincidences. We demonstrate the usage of CoinCalc based on two typical\ngeoscientific example problems addressing the relationship between\nmeteorological extremes and plant phenology as well as that between soil\nproperties and land cover. \n\n"}
{"id": "1603.05418", "contents": "Title: Analytical Models of Exoplanetary Atmospheres. III. Gaseous C-H-O-N\n  Chemistry with 9 Molecules Abstract: We present novel, analytical, equilibrium-chemistry formulae for the\nabundances of molecules in hot exoplanetary atmospheres that include the\ncarbon, oxygen and nitrogen networks. Our hydrogen-dominated solutions involve\nacetylene (C$_2$H$_2$), ammonia (NH$_3$), carbon dioxide (CO$_2$), carbon\nmonoxide (CO), ethylene (C$_2$H$_4$), hydrogen cyanide (HCN), methane (CH$_4$),\nmolecular nitrogen (N$_2$) and water (H$_2$O). By considering only the gas\nphase, we prove that the mixing ratio of carbon monoxide is governed by a decic\nequation (polynomial equation of degree 10). We validate our solutions against\nnumerical calculations of equilibrium chemistry that perform Gibbs free energy\nminimization and demonstrate that they are accurate at the $\\sim 1\\%$ level for\ntemperatures from 500 to 3000 K. In hydrogen-dominated atmospheres, the ratio\nof abundances of HCN to CH$_4$ is nearly constant across a wide range of\ncarbon-to-oxygen ratios, which makes it a robust diagnostic of the metallicity\nin the gas phase. Our validated formulae allow for the convenient benchmarking\nof chemical kinetics codes and provide an efficient way of enforcing chemical\nequilibrium in atmospheric retrieval calculations. \n\n"}
{"id": "1603.06755", "contents": "Title: The Modular Arbitrary-Order Ocean-Atmosphere Model: MAOOAM v1.0 Abstract: This paper describes a reduced-order quasi-geostrophic coupled\nocean-atmosphere model that allows for an arbitrary number of atmospheric and\noceanic modes to be retained in the spectral decomposition. The modularity of\nthis new model allows one to easily modify the model physics. Using this new\nmodel, coined the \"Modular Arbitrary-Order Ocean-Atmosphere Model\" (MAOOAM), we\nanalyse the dependence of the model dynamics on the truncation level of the\nspectral expansion, and unveil spurious behaviour that may exist at low\nresolution by a comparison with the higher-resolution configurations. In\nparticular, we assess the robustness of the coupled low-frequency variability\nwhen the number of modes is increased. An \"optimal\" configuration is proposed\nfor which the ocean resolution is sufficiently high, while the total number of\nmodes is small enough to allow for a tractable and extensive analysis of the\ndynamics. \n\n"}
{"id": "1604.00872", "contents": "Title: Geometrically Tempered Hamiltonian Monte Carlo Abstract: Hamiltonian Monte Carlo (HMC) has become routinely used for sampling from\nposterior distributions. Its extension Riemann manifold HMC (RMHMC) modifies\nthe proposal kernel through distortion of local distances by a Riemannian\nmetric. The performance depends critically on the choice of metric, with the\nFisher information providing the standard choice. In this article, we propose a\nnew class of metrics aimed at improving HMC's performance on multi-modal target\ndistributions. We refer to the proposed approach as geometrically tempered HMC\n(GTHMC) due to its connection to other tempering methods. We establish a\ngeometric theory behind RMHMC to motivate GTHMC and characterize its\ntheoretical properties. Moreover, we develop a novel variable step size\nintegrator for simulating Hamiltonian dynamics to improve on the usual\nSt\\\"{o}rmer-Verlet integrator which suffers from numerical instability in GTHMC\nsettings. We illustrate GTHMC through simulations, demonstrating generality and\nsubstantial gains over standard HMC implementations in terms of effective\nsample sizes. \n\n"}
{"id": "1604.01250", "contents": "Title: Fast methods for training Gaussian processes on large data sets Abstract: Gaussian process regression (GPR) is a non-parametric Bayesian technique for\ninterpolating or fitting data. The main barrier to further uptake of this\npowerful tool rests in the computational costs associated with the matrices\nwhich arise when dealing with large data sets. Here, we derive some simple\nresults which we have found useful for speeding up the learning stage in the\nGPR algorithm, and especially for performing Bayesian model comparison between\ndifferent covariance functions. We apply our techniques to both synthetic and\nreal data and quantify the speed-up relative to using nested sampling to\nnumerically evaluate model evidences. \n\n"}
{"id": "1604.01443", "contents": "Title: Analysis of distributional variation through multi-scale Beta-Binomial\n  modeling Abstract: Many statistical analyses involve the comparison of multiple data sets\ncollected under different conditions in order to identify the difference in the\nunderlying distributions. A common challenge in multi-sample comparison is the\npresence of various confounders, or extraneous causes other than the conditions\nof interest that also contribute to the difference across the distributions.\nThey result in false findings, i.e., identified differences that are not\nreplicable in follow-up investigations. We consider an ANOVA approach to\naddressing this issue in multi-sample comparison---by collecting replicate data\nsets under each condition, thereby allowing the identification of the\ninteresting distributional variation from the extraneous ones. We introduce a\nmulti-scale Bayesian hierarchical model for the analysis of distributional\nvariation (ANDOVA) under this design, based on a collection of Beta-Binomial\ntests targeting variations of different scales at different locations across\nthe sample space. Instead treating the tests independently, the model employs a\ngraphical structure to introduce dependency among the individual tests thereby\nallowing borrowing of strength among them. We derive efficient inference recipe\nthrough a combination of numerical integration and message passing, and\nevaluate the ability of our method to effectively address ANDOVA through\nextensive simulation. We utilize our method to analyze a DNase-seq data set for\nidentifying differences in transcriptional factor binding. \n\n"}
{"id": "1604.01695", "contents": "Title: Recent Advances Concerning Certain Class of Geophysical Flows Abstract: This paper is devoted to reviewing several recent developments concerning\ncertain class of geophysical models, including the primitive equations (PEs) of\natmospheric and oceanic dynamics and a tropical atmosphere model. The PEs for\nlarge-scale oceanic and atmospheric dynamics are derived from the Navier-Stokes\nequations coupled to the heat convection by adopting the Boussinesq and\nhydrostatic approximations, while the tropical atmosphere model considered here\nis a nonlinear interaction system between the barotropic mode and the first\nbaroclinic mode of the tropical atmosphere with moisture.\n  We are mainly concerned with the global well-posedness of strong solutions to\nthese systems, with full or partial viscosity, as well as certain singular\nperturbation small parameter limits related to these systems, including the\nsmall aspect ratio limit from the Navier-Stokes equations to the PEs, and a\nsmall relaxation-parameter in the tropical atmosphere model. These limits\nprovide a rigorous justification to the hydrostatic balance in the PEs, and to\nthe relaxation limit of the tropical atmosphere model, respectively. Some\nconditional uniqueness of weak solutions, and the global well-posedness of weak\nsolutions with certain class of discontinuous initial data, to the PEs are also\npresented. \n\n"}
{"id": "1604.01877", "contents": "Title: Low frequency modulation of jets in quasigeostrophic turbulence Abstract: Quasigeostrophic turbulence on a beta-plane with a finite deformation radius\nis studied nu- merically, with particular emphasis on frequency and combined\nwavenumber-frequency do- main analyses. Under suitable conditions, simulations\nwith small-scale random forcing and large-scale drag exhibit a spontaneous\nformation of multiple zonal jets. The first hint of wave-like features is seen\nin the distribution of kinetic energy as a function of frequency; specifically,\nfor progressively larger deformation scales there are systematic departures in\nthe form of isolated peaks (at progressively higher frequencies) from a\npower-law scaling. Con- comitantly, there is an inverse flux of kinetic energy\nin frequency space which extends to lower frequencies for smaller deformation\nscales. The identification of these peaks as Rossby waves is made possible by\nexamining the energy spectrum in frequency-zonal wavenumber and\nfrequency-meridional wavenumber diagrams. In fact, the modified Rhines scale\nturns out to be a useful measure of the dominant meridional wavenumber of the\nmodulating Rossby waves; once this is fixed, apart from a spectral peak at the\norigin (the steady jet), almost all the energy is contained in westward\npropagating disturbances that follow the theoretical Rossby dispersion\nrelation. Quite consistently, noting that the zonal scale of the modulating\nwaves is restricted to the first few wavenumbers, the energy spectrum is almost\nentirely contained within the corresponding Rossby dispersion curves on a\nfrequency-meridional wavenumber diagram. Cases when jets do not form are also\nconsidered; once again, there is a hint of Rossby wave activity, though the\nspectral peaks are quite muted. Further, the kinetic energy scaling in\nfrequency domain follows a -5/3 power-law and is distributed much more broadly\nin frequency-wavenumber diagrams \n\n"}
{"id": "1604.04432", "contents": "Title: A climate network-based index to discriminate different types of El\n  Ni\\~no and La Ni\\~na Abstract: El Ni\\~no exhibits distinct Eastern Pacific (EP) and Central Pacific (CP)\ntypes which are commonly, but not always consistently, distinguished from each\nother by different signatures in equatorial climate variability. Here, we\npropose an index based on evolving climate networks to objectively discriminate\nbetween both flavors by utilizing a scalar-valued evolving climate network\nmeasure that quantifies spatial localization and dispersion in El Ni\\~no's\nassociated teleconnections. Our index displays a sharp peak (high localization)\nduring EP events, whereas during CP events (larger dispersion) it remains close\nto the baseline observed during normal periods. In contrast to previous\nclassification schemes, our approach specifically account for El Ni\\~no's\nglobal impacts. We confirm recent El Ni\\~no classifications for the years 1951\nto 2014 and assign types to those cases were former works yielded ambiguous\nresults. Ultimately, we study La Ni\\~na episodes and demonstrate that our index\nprovides a similar discrimination into two types. \n\n"}
{"id": "1604.04980", "contents": "Title: Potentially Predictive Variance Reducing Subsample Locations in Local\n  Gaussian Process Regression Abstract: Gaussian process models are commonly used as emulators for computer\nexperiments. However, developing a Gaussian process emulator can be\ncomputationally prohibitive when the number of experimental samples is even\nmoderately large. Local Gaussian process approximation (Gramacy and Apley,\n2015) was proposed as an accurate and computationally feasible emulation\nalternative. However, constructing local sub-designs specific to predictions at\na particular location of interest remains a substantial computational\nbottleneck to the technique. In this paper, two computationally efficient\nneighborhood search limiting techniques are proposed, a maximum distance method\nand a feature approximation method. Two examples demonstrate that the proposed\nmethods indeed save substantial computation while retaining emulation accuracy. \n\n"}
{"id": "1604.05119", "contents": "Title: The water budget of a hurricane as dependent on its movement Abstract: Despite the dangers associated with tropical cyclones and their rainfall, the\norigins of storm moisture remains unclear. Existing studies have focused on the\nregion 40-400 km from the cyclone center. It is known that the rainfall within\nthis area cannot be explained by local processes alone but requires imported\nmoisture. Nonetheless, the dynamics of this imported moisture appears unknown.\nHere, considering a region up to three thousand kilometers from storm center,\nwe analyze precipitation, atmospheric moisture and movement velocities for\nNorth Atlantic hurricanes. Our findings indicate that even over such large\nareas a hurricane's rainfall cannot be accounted for by concurrent evaporation.\nWe propose instead that a hurricane consumes pre-existing atmospheric water\nvapor as it moves. The propagation velocity of the cyclone, i.e. the difference\nbetween its movement velocity and the mean velocity of the surrounding air\n(steering flow), determines the water vapor budget. Water vapor available to\nthe hurricane through its movement makes the hurricane self-sufficient at about\n700 km from the hurricane center obviating the need to concentrate moisture\nfrom greater distances. Such hurricanes leave a dry wake, whereby rainfall is\nsuppressed by up to 40 per cent compared to its long-term mean. The inner\nradius of this dry footprint approximately coincides with the radius of\nhurricane self-sufficiency with respect to water vapor. We discuss how Carnot\nefficiency considerations do not constrain the power of such open systems that\ndeplete the pre-existing moisture. Our findings emphasize the incompletely\nunderstood role and importance of atmospheric moisture supplies, condensation\nand precipitation in hurricane dynamics. \n\n"}
{"id": "1604.07177", "contents": "Title: On the Use of Penalty MCMC for Differential Privacy Abstract: We view the penalty algorithm of Ceperley and Dewing (1999), a Markov chain\nMonte Carlo (MCMC) algorithm for Bayesian inference, in the context of data\nprivacy. Specifically, we study differential privacy of the penalty algorithm\nand advocate its use for data privacy. We show that in the simple model of\nindependent observations the algorithm has desirable convergence and privacy\nproperties that scale with data size. Two special cases are also investigated\nand privacy preserving schemes are proposed for those cases: (i) Data are\ndistributed among several data owners who are interested in the inference of a\ncommon parameter while preserving their data privacy. (ii) The data likelihood\nbelongs to an exponential family. \n\n"}
{"id": "1604.08035", "contents": "Title: Rogue Wave Spectra of the Kundu-Eckhaus Equation Abstract: In this paper we analyze the rogue wave spectra of the Kundu-Eckhaus equation\n(KEE). We compare our findings with their nonlinear Schrodinger equation (NLSE)\nanalogs and show that the spectra of the individual rogue waves significantly\ndiffer from their NLSE analogs. A remarkable difference is the one-sided\ndevelopment of the triangular spectrum before the rogue wave becomes evident in\ntime. Also we show that increasing the skewness of the rogue wave results in\nincreased asymmetry in the triangular Fourier spectra. Additionally, the\ntriangular spectra of the rogue waves of the KEE begin to develop at earlier\nstages of the their development compared to their NLSE analogs, especially for\nlarger skew angles. This feature may be used to enhance the early warning times\nof the rogue waves. However we show that in a chaotic wavefield with many\nspectral components the triangular spectra remains as the main attribute as a\nuniversal feature of the typical wavefields produced through modulation\ninstability and characteristic features of the KEE's analytical rogue wave\nspectra may be suppressed in a realistic chaotic wavefield. \n\n"}
{"id": "1605.00551", "contents": "Title: Compatible finite element spaces for geophysical fluid dynamics Abstract: Compatible finite elements provide a framework for preserving important\nstructures in equations of geophysical fluid dynamics, and are becoming\nimportant in their use for building atmosphere and ocean models. We survey the\napplication of compatible finite element spaces to geophysical fluid dynamics,\nincluding the application to the nonlinear rotating shallow water equations,\nand the three-dimensional compressible Euler equations. We summarise analytic\nresults about dispersion relations and conservation properties, and present new\nresults on approximation properties in three dimensions on the sphere, and on\nhydrostatic balance properties. \n\n"}
{"id": "1605.01521", "contents": "Title: SDDP vs. ADP: The Effect of Dimensionality in Multistage Stochastic\n  Optimization for Grid Level Energy Storage Abstract: There has been widespread interest in the use of grid-level storage to handle\nthe variability from increasing penetrations of wind and solar energy. This\nproblem setting requires optimizing energy storage and release decisions for\nanywhere from a half-dozen, to potentially hundreds of storage devices spread\naround the grid as new technologies evolve. We approach this problem using two\ncompeting algorithmic strategies. The first, developed within the stochastic\nprogramming literature, is stochastic dual dynamic programming (SDDP) which\nuses Benders decomposition to create a multidimensional value function\napproximations, which have been widely used to manage hydro reservoirs. The\nsecond approach, which has evolved using the language of approximate dynamic\nprogramming, uses separable, piecewise linear value function approximations, a\nmethod which has been successfully applied to high-dimensional fleet management\nproblems. This paper brings these two approaches together using a common\nnotational system, and contrasts the algorithmic strategies (which are both a\nform of approximate dynamic programming) used by each approach. The methods are\nthen subjected to rigorous testing using the context of optimizing grid level\nstorage. \n\n"}
{"id": "1605.01684", "contents": "Title: Fractional Brownian motion, the Matern process, and stochastic modeling\n  of turbulent dispersion Abstract: Stochastic process exhibiting power-law slopes in the frequency domain are\nfrequently well modeled by fractional Brownian motion (fBm). In particular, the\nspectral slope at high frequencies is associated with the degree of small-scale\nroughness or fractal dimension. However, a broad class of real-world signals\nhave a high-frequency slope, like fBm, but a plateau in the vicinity of zero\nfrequency. This low-frequency plateau, it is shown, implies that the temporal\nintegral of the process exhibits diffusive behavior, dispersing from its\ninitial location at a constant rate. Such processes are not well modeled by\nfBm, which has a singularity at zero frequency corresponding to an unbounded\nrate of dispersion. A more appropriate stochastic model is a much lesser-known\nrandom process called the Matern process, which is shown herein to be a damped\nversion of fractional Brownian motion. This article first provides a thorough\nintroduction to fractional Brownian motion, then examines the details of the\nMatern process and its relationship to fBm. An algorithm for the simulation of\nthe Matern process in O(N log N) operations is given. Unlike fBm, the Matern\nprocess is found to provide an excellent match to modeling velocities from\nparticle trajectories in an application to two-dimensional fluid turbulence. \n\n"}
{"id": "1605.03855", "contents": "Title: Edge States in the Climate System: Exploring Global Instabilities and\n  Critical Transitions Abstract: Multistability is a ubiquitous feature in systems of geophysical relevance\nand provides key challenges for our ability to predict a system's response to\nperturbations. Near critical transitions small causes can lead to large effects\nand - for all practical purposes - irreversible changes in the properties of\nthe system. The Earth climate is multistable: present astronomical and\nastrophysical conditions support two stable regimes, the warm climate we live\nin, and a snowball climate, characterized by global glaciation. We first\nprovide an overview of methods and ideas relevant for studying the climate\nresponse to forcings and focus on the properties of critical transitions.\nFollowing an idea developed by Eckhardt and co. for the investigation of\nmultistable turbulent flows, we study the global instability giving rise to the\nsnowball/warm multistability in the climate system by identifying the climatic\nedge state, a saddle embedded in the boundary between the two basins of\nattraction of the stable climates. The edge state attracts initial conditions\nbelonging to such a boundary and is the gate facilitating noise-induced\ntransitions between competing attractors. We use a simplified yet Earth-like\nclimate model constructed by coupling a primitive equations model of the\natmosphere with a simple diffusive ocean. We refer to the climatic edge states\nas Melancholia states. We study their dynamics, their symmetry properties, and\nwe follow a complex set of bifurcations. We find situations where the\nMelancholia state has chaotic dynamics. In these cases, the basin boundary\nbetween the two basins of attraction is a strange geometric set with a nearly\nzero codimension, and relate this feature to the time scale separation between\ninstabilities occurring on weather and climatic time scales. We also discover a\nnew stable climatic state characterized by non-trivial symmetry properties. \n\n"}
{"id": "1605.04029", "contents": "Title: Simple, Scalable and Accurate Posterior Interval Estimation Abstract: There is a lack of simple and scalable algorithms for uncertainty\nquantification. Bayesian methods quantify uncertainty through posterior and\npredictive distributions, but it is difficult to rapidly estimate summaries of\nthese distributions, such as quantiles and intervals. Variational Bayes\napproximations are widely used, but may badly underestimate posterior\ncovariance. Typically, the focus of Bayesian inference is on point and interval\nestimates for one-dimensional functionals of interest. In small scale problems,\nMarkov chain Monte Carlo algorithms remain the gold standard, but such\nalgorithms face major problems in scaling up to big data. Various modifications\nhave been proposed based on parallelization and approximations based on\nsubsamples, but such approaches are either highly complex or lack theoretical\nsupport and/or good performance outside of narrow settings. We propose a very\nsimple and general posterior interval estimation algorithm, which is based on\nrunning Markov chain Monte Carlo in parallel for subsets of the data and\naveraging quantiles estimated from each subset. We provide strong theoretical\nguarantees and illustrate performance in several applications. \n\n"}
{"id": "1605.04281", "contents": "Title: Signal Regression Models for Location, Scale and Shape with an\n  Application to Stock Returns Abstract: We discuss scalar-on-function regression models where all parameters of the\nassumed response distribution can be modeled depending on covariates. We thus\ncombine signal regression models with generalized additive models for location,\nscale and shape (GAMLSS). We compare two fundamentally different methods for\nestimation, a gradient boosting and a penalized likelihood based approach, and\naddress practically important points like identifiability and model choice.\nEstimation by a component-wise gradient boosting algorithm allows for high\ndimensional data settings and variable selection. Estimation by a penalized\nlikelihood based approach has the advantage of directly provided statistical\ninference. The motivating application is a time series of stock returns where\nit is of interest to model both the expectation and the variance depending on\nlagged response values and functional liquidity curves. \n\n"}
{"id": "1605.04382", "contents": "Title: The mixed-phase version of moist-air entropy Abstract: The aim of this note is to derive the mixed-phase version of the moist-air\nentropy potential temperature $\\theta_s$ derived in Marquet (2011).\n  This mixed-phase version is suitable to describe parcels where liquid water\nand ice are allowed to coexist, with possible under- or super-saturations, with\npossible supercooled water and with possible different temperatures for dry air\nand water vapour, on the one hand, condensed water and ice, on the other hand.\n  The impact of this new mixed-phase version for $\\theta_s$ are evaluated by\nusing high latitudes, SHEBA/FIRE-ACE vertical profiles depicted in Figure 7 of\nMorisson et al. (2011). \n\n"}
{"id": "1605.05278", "contents": "Title: Exact Simulation of Noncircular or Improper Complex-Valued Stationary\n  Gaussian Processes using Circulant Embedding Abstract: This paper provides an algorithm for simulating improper (or noncircular)\ncomplex-valued stationary Gaussian processes. The technique utilizes recently\ndeveloped methods for multivariate Gaussian processes from the circulant\nembedding literature. The method can be performed in $\\mathcal{O}(n\\log_2 n)$\noperations, where $n$ is the length of the desired sequence. The method is\nexact, except when eigenvalues of prescribed circulant matrices are negative.\nWe evaluate the performance of the algorithm empirically, and provide a\npractical example where the method is guaranteed to be exact for all $n$, with\nan improper fractional Gaussian noise process. \n\n"}
{"id": "1605.05476", "contents": "Title: Localizing the Ensemble Kalman Particle Filter Abstract: Ensemble methods such as the Ensemble Kalman Filter (EnKF) are widely used\nfor data assimilation in large-scale geophysical applications, as for example\nin numerical weather prediction (NWP). There is a growing interest for physical\nmodels with higher and higher resolution, which brings new challenges for data\nassimilation techniques because of the presence of non-linear and non-Gaussian\nfeatures that are not adequately treated by the EnKF. We propose two new\nlocalized algorithms based on the Ensemble Kalman Particle Filter (EnKPF), a\nhybrid method combining the EnKF and the Particle Filter (PF) in a way that\nmaintains scalability and sample diversity. Localization is a key element of\nthe success of EnKFs in practice, but it is much more challenging to apply to\nPFs. The algorithms that we introduce in the present paper provide a compromise\nbetween the EnKF and the PF while avoiding some of the problems of localization\nfor pure PFs. Numerical experiments with a simplified model of cumulus\nconvection based on a modified shallow water equation show that the proposed\nalgorithms perform better than the local EnKF. In particular, the PF nature of\nthe method allows to capture non-Gaussian characteristics of the estimated\nfields such as the location of wet and dry areas. \n\n"}
{"id": "1605.05537", "contents": "Title: ABC random forests for Bayesian parameter inference Abstract: This preprint has been reviewed and recommended by Peer Community In\nEvolutionary Biology (http://dx.doi.org/10.24072/pci.evolbiol.100036).\nApproximate Bayesian computation (ABC) has grown into a standard methodology\nthat manages Bayesian inference for models associated with intractable\nlikelihood functions. Most ABC implementations require the preliminary\nselection of a vector of informative statistics summarizing raw data.\nFurthermore, in almost all existing implementations, the tolerance level that\nseparates acceptance from rejection of simulated parameter values needs to be\ncalibrated. We propose to conduct likelihood-free Bayesian inferences about\nparameters with no prior selection of the relevant components of the summary\nstatistics and bypassing the derivation of the associated tolerance level. The\napproach relies on the random forest methodology of Breiman (2001) applied in a\n(non parametric) regression setting. We advocate the derivation of a new random\nforest for each component of the parameter vector of interest. When compared\nwith earlier ABC solutions, this method offers significant gains in terms of\nrobustness to the choice of the summary statistics, does not depend on any type\nof tolerance level, and is a good trade-off in term of quality of point\nestimator precision and credible interval estimations for a given computing\ntime. We illustrate the performance of our methodological proposal and compare\nit with earlier ABC methods on a Normal toy example and a population genetics\nexample dealing with human population evolution. All methods designed here have\nbeen incorporated in the R package abcrf (version 1.7) available on CRAN. \n\n"}
{"id": "1605.05798", "contents": "Title: MCMC for Imbalanced Categorical Data Abstract: Many modern applications collect highly imbalanced categorical data, with\nsome categories relatively rare. Bayesian hierarchical models combat data\nsparsity by borrowing information, while also quantifying uncertainty. However,\nposterior computation presents a fundamental barrier to routine use; a single\nclass of algorithms does not work well in all settings and practitioners waste\ntime trying different types of MCMC approaches. This article was motivated by\nan application to quantitative advertising in which we encountered extremely\npoor computational performance for common data augmentation MCMC algorithms but\nobtained excellent performance for adaptive Metropolis. To obtain a deeper\nunderstanding of this behavior, we give strong theory results on computational\ncomplexity in an infinitely imbalanced asymptotic regime. Our results show\ncomputational complexity of Metropolis is logarithmic in sample size, while\ndata augmentation is polynomial in sample size. The root cause of poor\nperformance of data augmentation is a discrepancy between the rates at which\nthe target density and MCMC step sizes concentrate. In general, MCMC algorithms\nthat have a similar discrepancy will fail in large samples - a result with\nsubstantial practical impact. \n\n"}
{"id": "1605.06376", "contents": "Title: Fast $\\epsilon$-free Inference of Simulation Models with Bayesian\n  Conditional Density Estimation Abstract: Many statistical models can be simulated forwards but have intractable\nlikelihoods. Approximate Bayesian Computation (ABC) methods are used to infer\nproperties of these models from data. Traditionally these methods approximate\nthe posterior over parameters by conditioning on data being inside an\n$\\epsilon$-ball around the observed data, which is only correct in the limit\n$\\epsilon\\!\\rightarrow\\!0$. Monte Carlo methods can then draw samples from the\napproximate posterior to approximate predictions or error bars on parameters.\nThese algorithms critically slow down as $\\epsilon\\!\\rightarrow\\!0$, and in\npractice draw samples from a broader distribution than the posterior. We\npropose a new approach to likelihood-free inference based on Bayesian\nconditional density estimation. Preliminary inferences based on limited\nsimulation data are used to guide later simulations. In some cases, learning an\naccurate parametric representation of the entire true posterior distribution\nrequires fewer model simulations than Monte Carlo ABC methods need to produce a\nsingle sample from an approximate posterior. \n\n"}
{"id": "1605.07811", "contents": "Title: Probabilistic Numerical Methods for Partial Differential Equations and\n  Bayesian Inverse Problems Abstract: This paper develops a probabilistic numerical method for solution of partial\ndifferential equations (PDEs) and studies application of that method to\nPDE-constrained inverse problems. This approach enables the solution of\nchallenging inverse problems whilst accounting, in a statistically principled\nway, for the impact of discretisation error due to numerical solution of the\nPDE. In particular, the approach confers robustness to failure of the numerical\nPDE solver, with statistical inferences driven to be more conservative in the\npresence of substantial discretisation error. Going further, the problem of\nchoosing a PDE solver is cast as a problem in the Bayesian design of\nexperiments, where the aim is to minimise the impact of solver error on\nstatistical inferences; here the challenge of non-linear PDEs is also\nconsidered. The method is applied to parameter inference problems in which\ndiscretisation error in non-negligible and must be accounted for in order to\nreach conclusions that are statistically valid. \n\n"}
{"id": "1605.07826", "contents": "Title: Asymptotically exact inference in differentiable generative models Abstract: Many generative models can be expressed as a differentiable function of\nrandom inputs drawn from some simple probability density. This framework\nincludes both deep generative architectures such as Variational Autoencoders\nand a large class of procedurally defined simulator models. We present a method\nfor performing efficient MCMC inference in such models when conditioning on\nobservations of the model output. For some models this offers an asymptotically\nexact inference method where Approximate Bayesian Computation might otherwise\nbe employed. We use the intuition that inference corresponds to integrating a\ndensity across the manifold corresponding to the set of inputs consistent with\nthe observed outputs. This motivates the use of a constrained variant of\nHamiltonian Monte Carlo which leverages the smooth geometry of the manifold to\ncoherently move between inputs exactly consistent with observations. We\nvalidate the method by performing inference tasks in a diverse set of models. \n\n"}
{"id": "1605.09107", "contents": "Title: Analysis of nonstationary modulated time series with applications to\n  oceanographic flow measurements Abstract: We propose a new class of univariate nonstationary time series models, using\nthe framework of modulated time series, which is appropriate for the analysis\nof rapidly-evolving time series as well as time series observations with\nmissing data. We extend our techniques to a class of bivariate time series that\nare isotropic. Exact inference is often not computationally viable for time\nseries analysis, and so we propose an estimation method based on the\nWhittle-likelihood, a commonly adopted pseudo-likelihood. Our inference\nprocedure is shown to be consistent under standard assumptions, as well as\nhaving considerably lower computational cost than exact likelihood in general.\nWe show the utility of this framework for the analysis of drifting instruments,\nan analysis that is key to characterising global ocean circulation and\ntherefore also for decadal to century-scale climate understanding. \n\n"}
{"id": "1605.09445", "contents": "Title: An estimator for Poisson means whose relative error distribution is\n  known Abstract: Suppose that $X_1,X_2,\\ldots$ are a stream of independent, identically\ndistributed Poisson random variables with mean $\\mu$. This work presents a new\nestimate $\\mu_k$ for $\\mu$ with the property that the distribution of the\nrelative error in the estimate ($(\\hat \\mu_k/\\mu) - 1$) is known, and does not\ndepend on $\\mu$ in any way. This enables the construction of simple exact\nconfidence intervals for the estimate, as well as a means of obtaining fast\napproximation algorithms for high dimensional integration using TPA. The new\nestimate requires a random number of Poisson draws, and so is best suited to\nMonte Carlo applications. As an example of such an application, the method is\napplied to obtain an exact confidence interval for the normalizing constant of\nthe Ising model. \n\n"}
{"id": "1606.00566", "contents": "Title: Zonal-mean circulation response to reduced air-sea momentum roughness Abstract: The impact of uncertainties in surface layer physics on the atmospheric\ngeneral circulation is comparatively unexplored. Here the sensitivity of the\nzonal-mean circulation to reduced air-sea momentum roughness ($Z_{0m}$) at low\nflow speed is investigated with the Community Atmosphere Model (CAM3). In an\naquaplanet framework with prescribed sea surface temperatures, the response to\nreduced $Z_{0m}$ resembles the La Ni$\\tilde{\\text{n}}$a minus El\nNi$\\tilde{\\text{n}}$o response to El Ni$\\tilde{\\text{n}}$o Southern Oscillation\nvariability with: i) a poleward shift of the mid-latitude westerlies extending\nall the way to the surface; ii) a weak poleward shift of the subtropical\ndescent region; and iii) a weakening of the Hadley circulation, which is\ngenerally also accompanied by a poleward shift of the inter-tropical\nconvergence zone (ITCZ) and the tropical surface easterlies. Mechanism-denial\nexperiments show this response to be initiated by the reduction of tropical\nlatent and sensible heat fluxes, effected by reducing $Z_{0m}$. The circulation\nresponse is elucidated by considering the effect of the tropical energy fluxes\non the Hadley circulation strength, the upper tropospheric critical layer\nlatitudes, and the lower-tropospheric baroclinic eddy forcing. The ITCZ shift\nis understood via moist static energy budget analysis in the tropics. The\ncirculation response to reduced $Z_{0m}$ carries over to more complex setups\nwith seasonal cycle, full complexity of atmosphere-ice-land-ocean interaction,\nand a slab ocean lower boundary condition. Hence, relatively small changes in\nthe surface parameterization parameters can lead to a significant circulation\nresponse. \n\n"}
{"id": "1606.02054", "contents": "Title: A simple multithreaded implementation of the EM algorithm for mixture\n  models Abstract: Finite mixture models have been widely used for the modelling and analysis of\ndata from heterogeneous populations. Maximum likelihood estimation of the\nparameters is typically carried out via the Expectation-Maximization (EM)\nalgorithm. The complexity of the implementation of the algorithm depends on the\nparametric distribution that is adopted as the component densities of the\nmixture model. In the case of the skew normal and skew t-distributions, for\nexample, the E-step would involve complicated expressions that are\ncomputationally expensive to evaluate. This can become quite time-consuming for\nlarge and/or high-dimensional datasets. In this paper, we develop a\nmultithreaded version of the EM algorithm for the fitting of finite mixture\nmodels. Due to the structure of the algorithm for these models, the E- and\nM-steps can be easily reformulated to be executed in parallel across multiple\nthreads to take advantage of the processing power available in modern-day\nmulticore machines. Our approach is simple and easy to implement, requiring\nonly small changes to standard code. To illustrate the approach, we focus on a\nfairly general mixture model that includes as special or limiting cases some of\nthe most commonly used mixture models including the normal, t-, skew normal,\nand skew t-mixture models. \n\n"}
{"id": "1606.02275", "contents": "Title: Measuring the reliability of MCMC inference with bidirectional Monte\n  Carlo Abstract: Markov chain Monte Carlo (MCMC) is one of the main workhorses of\nprobabilistic inference, but it is notoriously hard to measure the quality of\napproximate posterior samples. This challenge is particularly salient in black\nbox inference methods, which can hide details and obscure inference failures.\nIn this work, we extend the recently introduced bidirectional Monte Carlo\ntechnique to evaluate MCMC-based posterior inference algorithms. By running\nannealed importance sampling (AIS) chains both from prior to posterior and vice\nversa on simulated data, we upper bound in expectation the symmetrized KL\ndivergence between the true posterior distribution and the distribution of\napproximate samples. We present Bounding Divergences with REverse Annealing\n(BREAD), a protocol for validating the relevance of simulated data experiments\nto real datasets, and integrate it into two probabilistic programming\nlanguages: WebPPL and Stan. As an example of how BREAD can be used to guide the\ndesign of inference algorithms, we apply it to study the effectiveness of\ndifferent model representations in both WebPPL and Stan. \n\n"}
{"id": "1606.02782", "contents": "Title: A universal asymptotic regime in the hyperbolic nonlinear Schr\\\"odinger\n  equation Abstract: The appearance of a fundamental long-time asymptotic regime in the two space\none time dimensional hyperbolic nonlinear Schr\\\"odinger (HNLS) equation is\ndiscussed. Based on analytical and extensive numerical simulations an\napproximate self-similar solution is found for a wide range of initial\nconditions -- essentially for initial lumps of small to moderate energy. Even\nrelatively large initial amplitudes, which imply strong nonlinear effects,\neventually lead to local structures resembling those of the self-similar\nsolution, with appropriate small modifications. These modifications are\nimportant in order to properly capture the behavior of the phase of the\nsolution. This solution has aspects that suggest it is a universal attractor\nemanating from wide ranges of initial data. \n\n"}
{"id": "1606.03749", "contents": "Title: Scalable Bayesian variable selection and model averaging under block\n  orthogonal design Abstract: We propose a scalable algorithmic framework for exact Bayesian variable\nselection and model averaging in linear models under the assumption that the\nGram matrix is block-diagonal, and as a heuristic for exploring the model space\nfor general designs. In block-diagonal designs our approach returns the most\nprobable model of any given size without resorting to numerical integration.\nThe algorithm also provides a novel and efficient solution to the frequentist\nbest subset selection problem for block-diagonal designs. Posterior\nprobabilities for any number of models are obtained by evaluating a single\none-dimensional integral that can be computed upfront, and other quantities of\ninterest such as variable inclusion probabilities and model averaged regression\nestimates by carrying out an adaptive, deterministic one-dimensional numerical\nintegration. The overall computational cost scales linearly with the number of\nblocks, which can be processed in parallel, and exponentially with the block\nsize, rendering it most adequate in situations where predictors are organized\nin many moderately-sized blocks. For general designs, we approximate the Gram\nmatrix by a block-diagonal using spectral clustering and propose an iterative\nalgorithm that capitalizes on the block-diagonal algorithms to explore\nefficiently the model space. All methods proposed in this article are\nimplemented in the R library mombf. \n\n"}
{"id": "1606.05474", "contents": "Title: HELIOS: An Open-source, GPU-accelerated Radiative Transfer Code For\n  Self-consistent Exoplanetary Atmospheres Abstract: We present the open-source radiative transfer code named HELIOS, which is\nconstructed for studying exoplanetary atmospheres. In its initial version, the\nmodel atmospheres of HELIOS are one-dimensional and plane-parallel, and the\nequation of radiative transfer is solved in the two-stream approximation with\nnon-isotropic scattering. A small set of the main infrared absorbers is\nemployed, computed with the opacity calculator HELIOS-K and combined using a\ncorrelated-$k$ approximation. The molecular abundances originate from validated\nanalytical formulae for equilibrium chemistry. We compare HELIOS with the work\nof Miller-Ricci & Fortney using a model of GJ 1214b, and perform several tests,\nwhere we find: model atmospheres with single-temperature layers struggle to\nconverge to radiative equilibrium; $k$-distribution tables constructed with\n$\\gtrsim 0.01$ cm$^{-1}$ resolution in the opacity function ($ \\lesssim 10^3$\npoints per wavenumber bin) may result in errors $\\gtrsim 1$-10 % in the\nsynthetic spectra; and a diffusivity factor of 2 approximates well the exact\nradiative transfer solution in the limit of pure absorption. We construct\n\"null-hypothesis\" models (chemical equilibrium, radiative equilibrium and solar\nelement abundances) for 6 hot Jupiters. We find that the dayside emission\nspectra of HD 189733b and WASP-43b are consistent with the null hypothesis,\nwhile it consistently under-predicts the observed fluxes of WASP-8b, WASP-12b,\nWASP-14b and WASP-33b. We demonstrate that our results are somewhat insensitive\nto the choice of stellar models (blackbody, Kurucz or PHOENIX) and metallicity,\nbut are strongly affected by higher carbon-to-oxygen ratios. The code is\npublicly available as part of the Exoclimes Simulation Platform (ESP;\nexoclime.net). \n\n"}
{"id": "1606.05578", "contents": "Title: Proximity Without Consensus in Online Multi-Agent Optimization Abstract: We consider stochastic optimization problems in multi-agent settings, where a\nnetwork of agents aims to learn parameters which are optimal in terms of a\nglobal objective, while giving preference to locally observed streaming\ninformation. To do so, we depart from the canonical decentralized optimization\nframework where agreement constraints are enforced, and instead formulate a\nproblem where each agent minimizes a global objective while enforcing network\nproximity constraints. This formulation includes online consensus optimization\nas a special case, but allows for the more general hypothesis that there is\ndata heterogeneity across the network. To solve this problem, we propose using\na stochastic saddle point algorithm inspired by Arrow and Hurwicz. This method\nyields a decentralized algorithm for processing observations sequentially\nreceived at each node of the network. Using Lagrange multipliers to penalize\nthe discrepancy between them, only neighboring nodes exchange model\ninformation. We establish that under a constant step-size regime the\ntime-average suboptimality and constraint violation are contained in a\nneighborhood whose radius vanishes with increasing number of iterations. As a\nconsequence, we prove that the time-average primal vectors converge to the\noptimal objective while satisfying the network proximity constraints. We apply\nthis method to the problem of sequentially estimating a correlated random field\nin a sensor network, as well as an online source localization problem, both of\nwhich demonstrate the empirical validity of the aforementioned convergence\nresults. \n\n"}
{"id": "1606.06351", "contents": "Title: Geometric MCMC for Infinite-Dimensional Inverse Problems Abstract: Bayesian inverse problems often involve sampling posterior distributions on\ninfinite-dimensional function spaces. Traditional Markov chain Monte Carlo\n(MCMC) algorithms are characterized by deteriorating mixing times upon\nmesh-refinement, when the finite-dimensional approximations become more\naccurate. Such methods are typically forced to reduce step-sizes as the\ndiscretization gets finer, and thus are expensive as a function of dimension.\nRecently, a new class of MCMC methods with mesh-independent convergence times\nhas emerged. However, few of them take into account the geometry of the\nposterior informed by the data. At the same time, recently developed geometric\nMCMC algorithms have been found to be powerful in exploring complicated\ndistributions that deviate significantly from elliptic Gaussian laws, but are\nin general computationally intractable for models defined in infinite\ndimensions. In this work, we combine geometric methods on a finite-dimensional\nsubspace with mesh-independent infinite-dimensional approaches. Our objective\nis to speed up MCMC mixing times, without significantly increasing the\ncomputational cost per step (for instance, in comparison with the vanilla\npreconditioned Crank-Nicolson (pCN) method). This is achieved by using ideas\nfrom geometric MCMC to probe the complex structure of an intrinsic\nfinite-dimensional subspace where most data information concentrates, while\nretaining robust mixing times as the dimension grows by using pCN-like methods\nin the complementary subspace. The resulting algorithms are demonstrated in the\ncontext of three challenging inverse problems arising in subsurface flow, heat\nconduction and incompressible flow control. The algorithms exhibit up to two\norders of magnitude improvement in sampling efficiency when compared with the\npCN method. \n\n"}
{"id": "1606.06956", "contents": "Title: Statistics of topological RNA structures Abstract: In this paper we study properties of topological RNA structures, i.e.~RNA\ncontact structures with cross-serial interactions that are filtered by their\ntopological genus. RNA secondary structures within this framework are\ntopological structures having genus zero. We derive a new bivariate generating\nfunction whose singular expansion allows us to analyze the distributions of\narcs, stacks, hairpin- , interior- and multi-loops. We then extend this\nanalysis to H-type pseudoknots, kissing hairpins as well as $3$-knots and\ncompute their respective expectation values. Finally we discuss our results and\nput them into context with data obtained by uniform sampling structures of\nfixed genus. \n\n"}
{"id": "1606.07892", "contents": "Title: Large-Scale Kernel Methods for Independence Testing Abstract: Representations of probability measures in reproducing kernel Hilbert spaces\nprovide a flexible framework for fully nonparametric hypothesis tests of\nindependence, which can capture any type of departure from independence,\nincluding nonlinear associations and multivariate interactions. However, these\napproaches come with an at least quadratic computational cost in the number of\nobservations, which can be prohibitive in many applications. Arguably, it is\nexactly in such large-scale datasets that capturing any type of dependence is\nof interest, so striking a favourable tradeoff between computational efficiency\nand test performance for kernel independence tests would have a direct impact\non their applicability in practice. In this contribution, we provide an\nextensive study of the use of large-scale kernel approximations in the context\nof independence testing, contrasting block-based, Nystrom and random Fourier\nfeature approaches. Through a variety of synthetic data experiments, it is\ndemonstrated that our novel large scale methods give comparable performance\nwith existing methods whilst using significantly less computation time and\nmemory. \n\n"}
{"id": "1606.07995", "contents": "Title: Efficient data augmentation for fitting stochastic epidemic models to\n  prevalence data Abstract: Stochastic epidemic models describe the dynamics of an epidemic as a disease\nspreads through a population. Typically, only a fraction of cases are observed\nat a set of discrete times. The absence of complete information about the time\nevolution of an epidemic gives rise to a complicated latent variable problem in\nwhich the state space size of the epidemic grows large as the population size\nincreases. This makes analytically integrating over the missing data infeasible\nfor populations of even moderate size. We present a data augmentation Markov\nchain Monte Carlo (MCMC) framework for Bayesian estimation of stochastic\nepidemic model parameters, in which measurements are augmented with\nsubject-level disease histories. In our MCMC algorithm, we propose each new\nsubject-level path, conditional on the data, using a time-inhomogeneous\ncontinuous-time Markov process with rates determined by the infection histories\nof other individuals. The method is general, and may be applied, with minimal\nmodifications, to a broad class of stochastic epidemic models. We present our\nalgorithm in the context of multiple stochastic epidemic models in which the\ndata are binomially sampled prevalence counts, and apply our method to data\nfrom an outbreak of influenza in a British boarding school. \n\n"}
{"id": "1606.08088", "contents": "Title: In Situ and Ex Situ Formation Models of Kepler 11 Planets Abstract: We present formation simulations of the six Kepler 11 planets. Models assume\neither in situ or ex situ assembly, the latter with migration, and are evolved\nto the estimated age of the system, 8 Gyr. Models combine detailed calculations\nof both the gaseous envelope and the condensed core structures, including\naccretion of gas and solids, of the disk's viscous and thermal evolution,\nincluding photo-evaporation and disk-planet interactions, and of the planets'\nevaporative mass loss after disk dispersal. Planet-planet interactions are\nneglected. Both sets of simulations successfully reproduce measured radii,\nmasses, and orbital distances of the planets, except for the radius of Kepler\n11b, which loses its entire gaseous envelope shortly after formation. Gaseous\n(H+He) envelopes account for < 18% of the planet masses, and between 35 and 60%\nof the planet radii. In situ models predict a very massive inner disk, whose\nsolids' surface density (sigma_Z) varies from over 1e4 to 1e3 g/cm2 at\nstellocentric distances 0.1 < r < 0.5 AU. Initial gas densities would be in\nexcess of 1e5 g/cm2 if solids formed locally. Given the high disk temperatures\n(> 1000 K), planetary interiors can only be composed of metals and highly\nrefractory materials. Sequestration of hydrogen by the core and subsequent\noutgassing is required to account for the observed radius of Kepler 11b. Ex\nsitu models predict a relatively low-mass disk, whose initial sigma_Z varies\nfrom 10 to 5 g/cm2 at 0.5 < r < 7 AU and whose initial gas density ranges from\n1e3 to 100 g/cm2. All planetary interiors are expected to be rich in H2O, as\ncore assembly mostly occurs exterior to the ice condensation front. Kepler 11b\nis expected to have a steam atmosphere, and H2O is likely mixed with H+He in\nthe envelopes of the other planets. Results indicate that Kepler 11g may not be\nmore massive than Kepler 11e. \n\n"}
{"id": "1606.08160", "contents": "Title: Geometric ergodicity of Rao and Teh's algorithm for Markov jump\n  processes and CTBNs Abstract: Rao and Teh (2012, 2013) introduced an efficient MCMC algorithm for sampling\nfrom the posterior distribution of a hidden Markov jump process. The algorithm\nis based on the idea of sampling virtual jumps. In the present paper we show\nthat the Markov chain generated by Rao and Teh's algorithm is geometrically\nergodic. To this end we establish a geometric drift condition towards a small\nset. A similar result is also proved for a special version of the algorithm,\nused for probabilistic inference in Continuous Time Bayesian Networks. \n\n"}
{"id": "1606.08373", "contents": "Title: Which ergodic averages have finite asymptotic variance? Abstract: We show that the class of $L^2$ functions for which ergodic averages of a\nreversible Markov chain have finite asymptotic variance is determined by the\nclass of $L^2$ functions for which ergodic averages of its associated jump\nchain have finite asymptotic variance. This allows us to characterize\ncompletely which ergodic averages have finite asymptotic variance when the\nMarkov chain is an independence sampler. In addition, we obtain a simple\nsufficient condition for all ergodic averages of $L^2$ functions of the primary\nvariable in a pseudo-marginal Markov chain to have finite asymptotic variance. \n\n"}
{"id": "1607.01529", "contents": "Title: Quantitative assessment of drivers of recent climate variability: An\n  information theoretic approach Abstract: Identification and quantification of possible drivers of recent climate\nvariability remain a challenging task. This important issue is addressed\nadopting a non-parametric information theory technique, the Transfer Entropy\nand its normalized variant. It distinctly quantifies actual information\nexchanged along with the directional flow of information between any two\nvariables with no bearing on their common history or inputs, unlike\ncorrelation, mutual information etc. Measurements of greenhouse gases, CO2,\nCH4, and N2O; volcanic aerosols; solar activity: UV radiation, total solar\nirradiance (TSI ) and cosmic ray flux (CR); El Nino Southern Oscillation (ENSO)\nand Global Mean Temperature Anomaly (GMTA) made during 1984-2005 are utilized\nto distinguish driving and responding climate signals. Estimates of their\nrelative contributions reveal that CO 2 (~24%), CH 4 (~19%) and volcanic\naerosols (~23%) are the primary contributors to the observed variations in\nGMTA. While, UV (~9%) and ENSO (~12%) act as secondary drivers of variations in\nthe GMTA, the remaining play a marginal role in the observed recent climate\nvariability. Interestingly, ENSO and GMTA mutually drive each other at varied\ntime lags. This study assists future modelling efforts in climate science. \n\n"}
{"id": "1607.02188", "contents": "Title: Whole-brain substitute CT generation using Markov random field mixture\n  models Abstract: Computed tomography (CT) equivalent information is needed for attenuation\ncorrection in PET imaging and for dose planning in radiotherapy. Prior work has\nshown that Gaussian mixture models can be used to generate a substitute CT\n(s-CT) image from a specific set of MRI modalities. This work introduces a more\nflexible class of mixture models for s-CT generation, that incorporates spatial\ndependency in the data through a Markov random field prior on the latent field\nof class memberships associated with a mixture model. Furthermore, the mixture\ndistributions are extended from Gaussian to normal inverse Gaussian (NIG),\nallowing heavier tails and skewness. The amount of data needed to train a model\nfor s-CT generation is of the order of 100 million voxels. The computational\nefficiency of the parameter estimation and prediction methods are hence\nparamount, especially when spatial dependency is included in the models. A\nstochastic Expectation Maximization (EM) gradient algorithm is proposed in\norder to tackle this challenge. The advantages of the spatial model and NIG\ndistributions are evaluated with a cross-validation study based on data from 14\npatients. The study show that the proposed model enhances the predictive\nquality of the s-CT images by reducing the mean absolute error with 17.9%.\nAlso, the distribution of CT values conditioned on the MR images are better\nexplained by the proposed model as evaluated using continuous ranked\nprobability scores. \n\n"}
{"id": "1607.02472", "contents": "Title: Two Iterative Proximal-Point Algorithms for the Calculus of\n  Divergence-based Estimators with Application to Mixture Models Abstract: Estimators derived from an EM algorithm are not robust since they are based\non the maximization of the likelihood function. We propose a proximal-point\nalgorithm based on the EM algorithm which aim to minimize a divergence\ncriterion. Resulting estimators are generally robust against outliers and\nmisspecification. An EM-type proximal-point algorithm is also introduced in\norder to produce robust estimators for mixture models. Convergence properties\nof the two algorithms are treated. We relax an identifiability condition\nimposed on the proximal term in the literature; a condition which is generally\nnot fulfilled by mixture models. The convergence of the introduced algorithms\nis discussed on a two-component Weibull mixture and a two-component Gaussian\nmixture entailing a condition on the initialization of the EM algorithm in\norder for the later to converge. Simulations on mixture models using different\nstatistical divergences are provided to confirm the validity of our work and\nthe robustness of the resulting estimators against outliers in comparison to\nthe EM algorithm. \n\n"}
{"id": "1607.03195", "contents": "Title: Multi-Step Bayesian Optimization for One-Dimensional Feasibility\n  Determination Abstract: Bayesian optimization methods allocate limited sampling budgets to maximize\nexpensive-to-evaluate functions. One-step-lookahead policies are often used,\nbut computing optimal multi-step-lookahead policies remains a challenge. We\nconsider a specialized Bayesian optimization problem: finding the superlevel\nset of an expensive one-dimensional function, with a Markov process prior. We\ncompute the Bayes-optimal sampling policy efficiently, and characterize the\nsuboptimality of one-step lookahead. Our numerical experiments demonstrate that\nthe one-step lookahead policy is close to optimal in this problem, performing\nwithin 98% of optimal in the experimental settings considered. \n\n"}
{"id": "1607.04308", "contents": "Title: Comments on \"Isentropic Analysis of a Simulated Hurricane\" Abstract: This paper describes Comments to the paper of Mrowiec et al. published in the\nJ. Atmos. Sci. in May 2016 (Vol 73, Issue 5, pages 1857-1870) and entitled\n\"Isentropic analysis of a simulated hurricane\".\n  It is explained that the plotting of isentropic surfaces (namely the\nisentropes) requires a precise definition of the specific moist-air entropy,\nand that most of existing \"equivalent potential temperatures\" lead to\ninaccurate definitions of isentropes.\n  It is shown that the use of the third law of thermodynamics leads to a\ndefinition of the specific moist-air entropy (and of a corresponding potential\ntemperature) which allow the plotting of unambigous moist-air isentropes.\n  Numerical applications are shown by using a numerical simulation of the\nhurricane DUMILE. \n\n"}
{"id": "1607.04532", "contents": "Title: Should I stay or should I go? A latent threshold approach to large-scale\n  mixture innovation models Abstract: This paper proposes a straightforward algorithm to carry out inference in\nlarge time-varying parameter vector autoregressions (TVP-VARs) with mixture\ninnovation components for each coefficient in the system. We significantly\ndecrease the computational burden by approximating the latent indicators that\ndrive the time-variation in the coefficients with a latent threshold process\nthat depends on the absolute size of the shocks. The merits of our approach are\nillustrated with two applications. First, we forecast the US term structure of\ninterest rates and demonstrate forecast gains of the proposed mixture\ninnovation model relative to other benchmark models. Second, we apply our\napproach to US macroeconomic data and find significant evidence for\ntime-varying effects of a monetary policy tightening. \n\n"}
{"id": "1607.08799", "contents": "Title: Particle Filtering with Invertible Particle Flow Abstract: A key challenge when designing particle filters in high-dimensional state\nspaces is the construction of a proposal distribution that is close to the\nposterior distribution. Recent advances in particle flow filters provide a\npromising avenue to avoid weight degeneracy; particles drawn from the prior\ndistribution are migrated in the state-space to the posterior distribution by\nsolving partial differential equations. Numerous particle flow filters have\nbeen proposed based on different assumptions concerning the flow dynamics.\nApproximations are needed in the implementation of all of these filters; as a\nresult the articles do not exactly match a sample drawn from the desired\nposterior distribution. Past efforts to correct the discrepancies involve\nexpensive calculations of importance weights. In this paper, we present new\nfilters which incorporate deterministic particle flows into an encompassing\nparticle filter framework. The valuable theoretical guarantees concerning\nparticle filter performance still apply, but we can exploit the attractive\nperformance of the particle flow methods. The filters we describe involve a\ncomputationally efficient weight update step, arising because the embedded\nparticle flows we design possess an invertible mapping property. We evaluate\nthe proposed particle flow particle filters' performance through numerical\nsimulations of a challenging multi-target multi-sensor tracking scenario and\ncomplex high-dimensional filtering examples. \n\n"}
{"id": "1607.08845", "contents": "Title: Limit theorems for the Zig-Zag process Abstract: Markov chain Monte Carlo methods provide an essential tool in statistics for\nsampling from complex probability distributions. While the standard approach to\nMCMC involves constructing discrete-time reversible Markov chains whose\ntransition kernel is obtained via the Metropolis- Hastings algorithm, there has\nbeen recent interest in alternative schemes based on piecewise deterministic\nMarkov processes (PDMPs). One such approach is based on the Zig-Zag process,\nintroduced in Bierkens and Roberts (2016), which proved to provide a highly\nscalable sampling scheme for sampling in the big data regime (Bierkens,\nFearnhead and Roberts (2016)). In this paper we study the performance of the\nZig-Zag sampler, focusing on the one-dimensional case. In particular, we\nidentify conditions under which a Central limit theorem (CLT) holds and\ncharacterize the asymptotic variance. Moreover, we study the influence of the\nswitching rate on the diffusivity of the Zig-Zag process by identifying a\ndiffusion limit as the switching rate tends to infinity. Based on our results\nwe compare the performance of the Zig-Zag sampler to existing Monte Carlo\nmethods, both analytically and through simulations. \n\n"}
{"id": "1608.00489", "contents": "Title: Buoyancy driven turbulence and distributed chaos Abstract: It is shown, using results of recent direct numerical simulations, laboratory\nexperiments and atmospheric measurements, that buoyancy driven turbulence\nexhibits a broad diversity of the types of distributed chaos with its stretched\nexponential spectrum $\\exp(-k/k_{\\beta})^{\\beta}$. The distributed chaos with\n$\\beta = 1/3$ (determined by the helicity correlation integral) is the most\ncommon feature of the stably stratified turbulence (due to the strong helical\nwaves presence). These waves mostly dominate spectral properties of the\nvertical component of velocity field, while the horizontal component is\ndominated by the diffusive processes both for the weak and strong stable\nstratification ($\\beta =2/3$). For the last case influence of the low boundary\ncan overcome the wave effects and result in $\\beta =1/2$ for the vertical\ncomponent of the velocity field (the spontaneous breaking of the space\ntranslational symmetry - homogeneity). For the unstably stratified turbulence\nin the Rayleigh-Taylor mixing zone the diffusive processes ($\\beta =2/3$) are\nthe most common dominating processes in the anisotropic chaotic mixing of the\ntwo fluids under buoyancy forces. The distributed chaos in Rayleigh-B\\'{e}nard\nturbulent convection in an upright cell is determined by the strong confinement\nconditions. That is: the spontaneous breaking of the space translational\nsymmetry (homogeneity) by the finite boundaries ($\\beta = 1/2$) or by the\nnon-perfect orientation of the cell along the buoyancy direction ($\\beta\n=4/7$). In all types of turbulence appearance of an inertial range of scales\nresults in deformation of the distributed chaos and $\\beta =3/5$. \n\n"}
{"id": "1608.01274", "contents": "Title: Which Findings from the Functional Neuromaging Literature Can We Trust? Abstract: In their recent \"Cluster Failure\" paper, Eklund and colleagues cast doubt on\nthe accuracy of a widely used statistical test in functional neuroimaging.\nHere, we leverage nonparametric methods that control the false discovery rate\nto offer more nuanced, quantitative guidance about which findings in the\nexisting literature can be trusted. We show that, in the task studies examined\nby Eklund et al., most clusters originally reported to be significant are\nindeed trustworthy by the false discovery rate benchmark. \n\n"}
{"id": "1608.02797", "contents": "Title: A block EM algorithm for multivariate skew normal and skew t-mixture\n  models Abstract: Finite mixtures of skew distributions provide a flexible tool for modelling\nheterogeneous data with asymmetric distributional features. However, parameter\nestimation via the Expectation-Maximization (EM) algorithm can become very\ntime-consuming due to the complicated expressions involved in the E-step that\nare numerically expensive to evaluate. A more time-efficient implementation of\nthe EM algorithm was recently proposed which allows each component of the\nmixture model to be evaluated in parallel. In this paper, we develop a block\nimplementation of the EM algorithm that facilitates the calculations in the E-\nand M-steps to be spread across a larger number of threads. We focus on the\nfitting of finite mixtures of multivariate skew normal and skew\nt-distributions, and show that both the E- and M-steps in the EM algorithm can\nbe modified to allow the data to be split into blocks. The approach can be\neasily implemented for use by multicore and multi-processor machines. It can\nalso be applied concurrently with the recently proposed multithreaded EM\nalgorithm to achieve further reduction in computation time. The improvement in\ntime performance is illustrated on some real datasets. \n\n"}
{"id": "1608.03790", "contents": "Title: On ZRP wind input term consistency in Hasselmann equation Abstract: The new ZRP wind input source term (Zakharov et al. 2012) is checked for its\nconsistency via numerical simulation of Hasselmann equation. The results are\ncompared to field experimental data, collected at different sites around the\nworld, and theoretical predictions of self-similarity analysis. Good agreement\nis obtained for limited fetch and time domain statements \n\n"}
{"id": "1608.04770", "contents": "Title: On the Charney Conjecture of Data Assimilation Employing Temperature\n  Measurements Alone: The Paradigm of 3D Planetary Geostrophic Model Abstract: Analyzing the validity and success of a data assimilation algorithm when some\nstate variable observations are not available is an important problem in\nmeteorology and engineering. We present an improved data assimilation algorithm\nfor recovering the exact full reference solution (i.e. the velocity and\ntemperature) of the 3D Planetary Geostrophic model, at an exponential rate in\ntime, by employing coarse spatial mesh observations of the temperature alone.\nThis provides, in the case of this paradigm, a rigorous justification to an\nearlier conjecture of Charney which states that temperature history of the\natmosphere, for certain simple atmospheric models, determines all other state\nvariables. \n\n"}
{"id": "1608.05271", "contents": "Title: High-order accurate finite-volume formulations for the pressure gradient\n  force in layered ocean models Abstract: The development of a set of high-order accurate finite-volume formulations\nfor evaluation of the pressure gradient force in layered ocean models is\ndescribed. A pair of new schemes are presented, both based on an integration of\nthe contact pressure force about the perimeter of an associated momentum\ncontrol-volume. The two proposed methods differ in their choice of\ncontrol-volume geometries. High-order accurate numerical integration techniques\nare employed in both schemes to account for non-linearities in the underlying\nequation-of-state definitions and thermodynamic profiles, and details of an\nassociated vertical interpolation and quadrature scheme are discussed in\ndetail. Numerical experiments are used to confirm the consistency of the two\nformulations, and it is demonstrated that the new methods maintain hydrostatic\nand thermobaric equilibrium in the presence of strongly-sloping layer-wise\ngeometry, non-linear equation-of-state definitions and non-uniform vertical\nstratification profiles. Additionally, one scheme is shown to maintain high\nlevels of consistency in the presence of non-linear thermodynamic\nstratification. Use of the new pressure gradient force formulations for hybrid\nvertical coordinate and/or terrain-following general circulation models is\ndiscussed. \n\n"}
{"id": "1608.05373", "contents": "Title: Zonal-flow dynamics from a phase-space perspective Abstract: The wave kinetic equation (WKE) describing drift-wave (DW) turbulence is\nwidely used in studies of zonal flows (ZFs) emerging from DW turbulence.\nHowever, this formulation neglects the exchange of enstrophy between DWs and\nZFs and also ignores effects beyond the geometrical-optics limit. We derive a\nmodified theory that takes both of these effects into account, while still\ntreating DW quanta (\"driftons\") as particles in phase space. The drifton\ndynamics is described by an equation of the Wigner-Moyal type, which is\ncommonly known in the phase-space formulation of quantum mechanics. In the\ngeometrical-optics limit, this formulation features additional terms missing in\nthe traditional WKE that ensure exact conservation of the total enstrophy of\nthe system, in addition to the total energy, which is the only conserved\ninvariant in previous theories based on the WKE. Numerical simulations are\npresented to illustrate the importance of these additional terms. The proposed\nformulation can be considered as a phase-space representation of the\nsecond-order cumulant expansion, or CE2. \n\n"}
{"id": "1608.07207", "contents": "Title: Computing log-likelihood and its derivatives for restricted maximum\n  likelihood methods Abstract: Recent large scale genome wide association analysis involves large scale\nlinear mixed models. Quantifying (co)-variance parameters in the mixed models\nwith a restricted maximum likelihood method results in a score function which\nis the first derivative of a log-likelihood. To obtain a statistically\nefficient estimate of the variance parameters, one needs to find the root of\nthe score function via the Newton method. Most elements of the Jacobian matrix\nof the score involve a trace term of four parametric matrix-matrix\nmultiplications. It is computationally prohibitively for large scale data sets.\nBy a serial matrix transforms and an averaged information splitting technique,\nan approximate Jacobian matrix can be obtained by splitting the average of the\nJacobian matrix and its expected value. In the approximated Jacobian, its\nelements only involve Four matrix vector multiplications which can be\nefficiently evaluated by solving sparse linear systems with the multi-frontal\nfactorization method. \n\n"}
{"id": "1608.08666", "contents": "Title: Online state and parameter estimation in Dynamic Generalised Linear\n  Models Abstract: Inference for streaming time-series is tightly coupled with the problem of\nBayesian on-line state and parameter inference. In this paper we will introduce\nDynamic Generalised Linear Models, the class of models often chosen to model\ncontinuous and discrete time-series data. We will look at three different\napproaches which allow on-line estimation and analyse the results when applied\nto different real world datasets related to inference for streaming data.\nSufficient statistics based methods delay known problems, such as particle\nimpoverishment, especially when applied to long running time-series, while\nproviding reasonable parameter estimations when compared to exact methods, such\nas Particle Marginal Metropolis-Hastings. State and observation forecasts will\nalso be analysed as a performance metric. By benchmarking against a \"gold\nstandard\" (off-line) method, we can better understand the performance of\non-line methods in challenging real-world scenarios. \n\n"}
{"id": "1609.00048", "contents": "Title: Practical sketching algorithms for low-rank matrix approximation Abstract: This paper describes a suite of algorithms for constructing low-rank\napproximations of an input matrix from a random linear image of the matrix,\ncalled a sketch. These methods can preserve structural properties of the input\nmatrix, such as positive-semidefiniteness, and they can produce approximations\nwith a user-specified rank. The algorithms are simple, accurate, numerically\nstable, and provably correct. Moreover, each method is accompanied by an\ninformative error bound that allows users to select parameters a priori to\nachieve a given approximation quality. These claims are supported by numerical\nexperiments with real and synthetic data. \n\n"}
{"id": "1609.00770", "contents": "Title: Stochastic Bouncy Particle Sampler Abstract: We introduce a novel stochastic version of the non-reversible, rejection-free\nBouncy Particle Sampler (BPS), a Markov process whose sample trajectories are\npiecewise linear. The algorithm is based on simulating first arrival times in a\ndoubly stochastic Poisson process using the thinning method, and allows\nefficient sampling of Bayesian posteriors in big datasets. We prove that in the\nBPS no bias is introduced by noisy evaluations of the log-likelihood gradient.\nOn the other hand, we argue that efficiency considerations favor a small,\ncontrollable bias in the construction of the thinning proposals, in exchange\nfor faster mixing. We introduce a simple regression-based proposal intensity\nfor the thinning method that controls this trade-off. We illustrate the\nalgorithm in several examples in which it outperforms both unbiased, but slowly\nmixing stochastic versions of BPS, as well as biased stochastic gradient-based\nsamplers. \n\n"}
{"id": "1609.01708", "contents": "Title: Tractable Bayesian variable selection: beyond normality Abstract: Bayesian variable selection often assumes normality, but the effects of model\nmisspecification are not sufficiently understood. There are sound reasons\nbehind this assumption, particularly for large $p$: ease of interpretation,\nanalytical and computational convenience. More flexible frameworks exist,\nincluding semi- or non-parametric models, often at the cost of some\ntractability. We propose a simple extension of the Normal model that allows for\nskewness and thicker-than-normal tails but preserves tractability. It leads to\neasy interpretation and a log-concave likelihood that facilitates optimization\nand integration. We characterize asymptotically parameter estimation and Bayes\nfactor rates, in particular studying the effects of model misspecification.\nUnder suitable conditions misspecified Bayes factors are consistent and induce\nsparsity at the same asymptotic rates than under the correct model. However,\nthe rates to detect signal are altered by an exponential factor, often\nresulting in a loss of sensitivity. These deficiencies can be ameliorated by\ninferring the error distribution from the data, a simple strategy that can\nimprove inference substantially. Our work focuses on the likelihood and can\nthus be combined with any likelihood penalty or prior, but here we focus on\nnon-local priors to induce extra sparsity and ameliorate finite-sample effects\ncaused by misspecification. Our results highlight the practical importance of\nfocusing on the likelihood rather than solely on the prior, when it comes to\nBayesian variable selection. The methodology is available in R package `mombf'. \n\n"}
{"id": "1609.03317", "contents": "Title: A data driven equivariant approach to constrained Gaussian mixture\n  modeling Abstract: Maximum likelihood estimation of Gaussian mixture models with different\nclass-specific covariance matrices is known to be problematic. This is due to\nthe unboundedness of the likelihood, together with the presence of spurious\nmaximizers. Existing methods to bypass this obstacle are based on the fact that\nunboundedness is avoided if the eigenvalues of the covariance matrices are\nbounded away from zero. This can be done imposing some constraints on the\ncovariance matrices, i.e. by incorporating a priori information on the\ncovariance structure of the mixture components. The present work introduces a\nconstrained equivariant approach, where the class conditional covariance\nmatrices are shrunk towards a pre-specified matrix Psi. Data-driven choices of\nthe matrix Psi, when a priori information is not available, and the optimal\namount of shrinkage are investigated. The effectiveness of the proposal is\nevaluated on the basis of a simulation study and an empirical example. \n\n"}
{"id": "1609.06402", "contents": "Title: Exact Sampling of the Infinite Horizon Maximum of a Random Walk Over a\n  Non-linear Boundary Abstract: We present the first algorithm that samples\n$\\max_{n\\geq0}\\{S_{n}-n^{\\alpha}\\},$ where $S_n$ is a mean zero random walk,\nand $n^{\\alpha}$ with $\\alpha\\in(1/2,1)$ defines a nonliner boundary. We show\nthat our algorithm has finite expected running time. We also apply the\nalgorithm to construct the first exact simulation method for the steady-state\ndeparture process of a $GI/GI/\\infty$ queue where the service time distribution\nhas infinite mean. \n\n"}
{"id": "1609.06549", "contents": "Title: Exploring the Venus global super-rotation using a comprehensive General\n  Circulation Model Abstract: The atmospheric circulation in Venus is well known to exhibit strong\nsuper-rotation. However, the atmospheric mechanisms responsible for the\nformation of this super-rotation are still not fully understood. In this work,\nwe developed a new Venus general circulation model to study the most likely\nmechanisms driving the atmosphere to the current observed circulation. Our\nmodel includes a new radiative transfer, convection and suitably adapted\nboundary layer schemes and a dynamical core that takes into account the\ndependence of the heat capacity at constant pressure with temperature.\n  The new Venus model is able to simulate a super-rotation phenomenon in the\ncloud region quantitatively similar to the one observed. The mechanisms\nmaintaining the strong winds in the cloud region were found in the model\nresults to be a combination of zonal mean circulation, thermal tides and\ntransient waves. In this process, the semi-diurnal tide excited in the upper\nclouds has a key contribution in transporting axial angular momentum mainly\nfrom the upper atmosphere towards the cloud region. The magnitude of the\nsuper-rotation in the cloud region is sensitive to various radiative parameters\nsuch as the amount of solar radiative energy absorbed by the surface, which\ncontrols the static stability near the surface. In this work, we also discuss\nthe main difficulties in representing the flow below the cloud base in Venus\natmospheric models.\n  Our new radiative scheme is more suitable for 3D Venus climate models than\nthose used in previous work due to its easy adaptability to different\natmospheric conditions. This flexibility of the model was crucial to explore\nthe uncertainties in the lower atmospheric conditions and may also be used in\nthe future to explore, for example, dynamical-radiative-microphysical\nfeedbacks. \n\n"}
{"id": "1609.07135", "contents": "Title: Convergence of Regression Adjusted Approximate Bayesian Computation Abstract: We present asymptotic results for the regression-adjusted version of\napproximate Bayesian computation introduced by Beaumont(2002). We show that for\nan appropriate choice of the bandwidth, regression adjustment will lead to a\nposterior that, asymptotically, correctly quantifies uncertainty. Furthermore,\nfor such a choice of bandwidth we can implement an importance sampling\nalgorithm to sample from the posterior whose acceptance probability tends to\nunity as the data sample size increases. This compares favourably to results\nfor standard approximate Bayesian computation, where the only way to obtain a\nposterior that correctly quantifies uncertainty is to choose a much smaller\nbandwidth; one for which the acceptance probability tends to zero and hence for\nwhich Monte Carlo error will dominate. \n\n"}
{"id": "1609.07363", "contents": "Title: Changepoint Detection in the Presence of Outliers Abstract: Many traditional methods for identifying changepoints can struggle in the\npresence of outliers, or when the noise is heavy-tailed. Often they will infer\nadditional changepoints in order to fit the outliers. To overcome this problem,\ndata often needs to be pre-processed to remove outliers, though this is\ndifficult for applications where the data needs to be analysed online. We\npresent an approach to changepoint detection that is robust to the presence of\noutliers. The idea is to adapt existing penalised cost approaches for detecting\nchanges so that they use loss functions that are less sensitive to outliers. We\nargue that loss functions that are bounded, such as the classical biweight\nloss, are particularly suitable -- as we show that only bounded loss functions\nare robust to arbitrarily extreme outliers. We present an efficient dynamic\nprogramming algorithm that can find the optimal segmentation under our\npenalised cost criteria. Importantly, this algorithm can be used in settings\nwhere the data needs to be analysed online. We show that we can consistently\nestimate the number of changepoints, and accurately estimate their locations,\nusing the biweight loss function. We demonstrate the usefulness of our approach\nfor applications such as analysing well-log data, detecting copy number\nvariation, and detecting tampering of wireless devices. \n\n"}
{"id": "1609.09185", "contents": "Title: Indo-Pacific variability on seasonal to multidecadal timescales. Part\n  II: Multiscale atmosphere-ocean linkages Abstract: The coupled atmosphere-ocean variability of the Indo-Pacific on interannual\nto multidecadal timescales is investigated in a millennial control run of CCSM4\nand in observations using a family of modes recovered in Part~I of this work\nfrom unprocessed SST data through nonlinear Laplacian spectral analysis (NLSA).\nIt is found that ENSO and combination modes of ENSO with the annual cycle\nexhibit a seasonally synchronized southward shift of equatorial surface zonal\nwinds and thermocline adjustment consistent with terminating El Nino and La\nNina events. The surface wind patterns associated with these modes also\ngenerate teleconnections between the Pacific and Indian Oceans, leading to a\npattern of SST anomalies characteristic of the Indian Ocean dipole. Fundamental\nand combination modes representing the tropospheric biennial oscillation (TBO)\nin CCSM4 are also found to be consistent with mechanisms for seasonally\nsynchronized biennial variability of the Asian-Australian monsoon and Walker\ncirculation. On longer timescales, the leading multidecadal pattern recovered\nby NLSA from Indo-Pacific SST data in CCSM4, referred to as west Pacific\nmultidecadal mode (WPMM), is found to significantly modulate ENSO and TBO\nactivity with periods of negative SST anomalies in the western tropical Pacific\nfavoring stronger ENSO and TBO variability. Physically, this behavior is\nattributed to the fact that cold WPMM phases feature anomalous decadal\nwesterlies in the tropical central Pacific and easterlies in the tropical\nIndian Ocean, as well as an anomalously deep thermocline in the eastern Pacific\ncold tongue. Moreover, despite the relatively low SST variance explained by\nthis mode, the WPMM is found to correlate significantly with decadal\nprecipitation over Australia in CCSM4. \n\n"}
{"id": "1609.09286", "contents": "Title: Surrogate models for oscillatory systems using sparse polynomial chaos\n  expansions and stochastic time warping Abstract: Polynomial chaos expansions (PCE) have proven efficiency in a number of\nfields for propagating parametric uncertainties through computational models of\ncomplex systems, namely structural and fluid mechanics, chemical reactions and\nelectromagnetism, etc. For problems involving oscillatory, time-dependent\noutput quantities of interest, it is well-known that reasonable accuracy of\nPCE-based approaches is difficult to reach in the long term. In this paper, we\npropose a fully non-intrusive approach based on stochastic time warping to\naddress this issue: each realization (trajectory) of the model response is\nfirst rescaled to its own time scale so as to put all sampled trajectories in\nphase in a common virtual time line. Principal component analysis is introduced\nto compress the information contained in these transformed trajectories and\nsparse PCE representations using least angle regression are finally used to\napproximate the components. The approach shows remarkably small prediction\nerror for particular trajectories as well as for second-order statistics of the\nlatter. It is illustrated on different benchmark problems well known in the\nliterature on time-dependent PCE problems, ranging from rigid body dynamics,\nchemical reactions to forced oscillations of a non linear system. \n\n"}
{"id": "1610.00195", "contents": "Title: Penalized Ensemble Kalman Filters for High Dimensional Non-linear\n  Systems Abstract: The ensemble Kalman filter (EnKF) is a data assimilation technique that uses\nan ensemble of models, updated with data, to track the time evolution of a\nusually non-linear system. It does so by using an empirical approximation to\nthe well-known Kalman filter. However, its performance can suffer when the\nensemble size is smaller than the state space, as is often necessary for\ncomputationally burdensome models. This scenario means that the empirical\nestimate of the state covariance is not full rank and possibly quite noisy. To\nsolve this problem in this high dimensional regime, we propose a\ncomputationally fast and easy to implement algorithm called the penalized\nensemble Kalman filter (PEnKF). Under certain conditions, it can be\ntheoretically proven that the PEnKF will be accurate (the estimation error will\nconverge to zero) despite having fewer ensemble members than state dimensions.\nFurther, as contrasted to localization methods, the proposed approach learns\nthe covariance structure associated with the dynamical system. These\ntheoretical results are supported with simulations of several non-linear and\nhigh dimensional systems. \n\n"}
{"id": "1610.02588", "contents": "Title: Iterative proportional scaling revisited: a modern optimization\n  perspective Abstract: This paper revisits the classic iterative proportional scaling (IPS) from a\nmodern optimization perspective. In contrast to the criticisms made in the\nliterature, we show that based on a coordinate descent characterization, IPS\ncan be slightly modified to deliver coefficient estimates, and from a\nmajorization-minimization standpoint, IPS can be extended to handle log-affine\nmodels with features not necessarily binary-valued or nonnegative. Furthermore,\nsome state-of-the-art optimization techniques such as block-wise computation,\nrandomization and momentum-based acceleration can be employed to provide more\nscalable IPS algorithms, as well as some regularized variants of IPS for\nconcurrent feature selection. \n\n"}
{"id": "1610.03483", "contents": "Title: Learning in Implicit Generative Models Abstract: Generative adversarial networks (GANs) provide an algorithmic framework for\nconstructing generative models with several appealing properties: they do not\nrequire a likelihood function to be specified, only a generating procedure;\nthey provide samples that are sharp and compelling; and they allow us to\nharness our knowledge of building highly accurate neural network classifiers.\nHere, we develop our understanding of GANs with the aim of forming a rich view\nof this growing area of machine learning---to build connections to the diverse\nset of statistical thinking on this topic, of which much can be gained by a\nmutual exchange of ideas. We frame GANs within the wider landscape of\nalgorithms for learning in implicit generative models--models that only specify\na stochastic procedure with which to generate data--and relate these ideas to\nmodelling problems in related fields, such as econometrics and approximate\nBayesian computation. We develop likelihood-free inference methods and\nhighlight hypothesis testing as a principle for learning in implicit generative\nmodels, using which we are able to derive the objective function used by GANs,\nand many other related objectives. The testing viewpoint directs our focus to\nthe general problem of density ratio estimation. There are four approaches for\ndensity ratio estimation, one of which is a solution using classifiers to\ndistinguish real from generated data. Other approaches such as divergence\nminimisation and moment matching have also been explored in the GAN literature,\nand we synthesise these views to form an understanding in terms of the\nrelationships between them and the wider literature, highlighting avenues for\nfuture exploration and cross-pollination. \n\n"}
{"id": "1610.03701", "contents": "Title: Localization in High-Dimensional Monte Carlo Filtering Abstract: The high dimensionality and computational constraints associated with\nfiltering problems in large-scale geophysical applications are particularly\nchallenging for the Particle Filter (PF). Approximate but efficient methods\nsuch as the Ensemble Kalman Filter (EnKF) are therefore usually preferred. A\nkey element of these approximate methods is localization, which is in principle\na general technique to avoid the curse of dimensionality and consists in\nlimiting the influence of observations to neighboring sites. However, while it\nworks effectively with the EnKF, localization introduces harmful\ndiscontinuities in the estimated physical fields when applied blindly to the\nPF. In the present paper, we explore two possible local algorithms based on the\nEnKPF, a hybrid method combining the EnKF and the PF. A simulation study in a\nconjugate normal setup allows to highlight the trade-offs involved when\napplying localization to PF type of algorithms in the high-dimensional setting.\nExperiments with the Lorenz96 model demonstrate the ability of the local EnKPF\nalgorithms to perform well even with a small number of particles compared to\nthe problem size. \n\n"}
{"id": "1610.04272", "contents": "Title: Tensor Computation: A New Framework for High-Dimensional Problems in EDA Abstract: Many critical EDA problems suffer from the curse of dimensionality, i.e. the\nvery fast-scaling computational burden produced by large number of parameters\nand/or unknown variables. This phenomenon may be caused by multiple spatial or\ntemporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit\nsimulation), nonlinearity of devices and circuits, large number of design or\noptimization parameters (e.g. full-chip routing/placement and circuit sizing),\nor extensive process variations (e.g. variability/reliability analysis and\ndesign for manufacturability). The computational challenges generated by such\nhigh dimensional problems are generally hard to handle efficiently with\ntraditional EDA core algorithms that are based on matrix and vector\ncomputation. This paper presents \"tensor computation\" as an alternative general\nframework for the development of efficient EDA algorithms and tools. A tensor\nis a high-dimensional generalization of a matrix and a vector, and is a natural\nchoice for both storing and solving efficiently high-dimensional EDA problems.\nThis paper gives a basic tutorial on tensors, demonstrates some recent examples\nof EDA applications (e.g., nonlinear circuit modeling and high-dimensional\nuncertainty quantification), and suggests further open EDA problems where the\nuse of tensor computation could be of advantage. \n\n"}
{"id": "1610.05108", "contents": "Title: The xyz algorithm for fast interaction search in high-dimensional data Abstract: When performing regression on a dataset with $p$ variables, it is often of\ninterest to go beyond using main linear effects and include interactions as\nproducts between individual variables. For small-scale problems, these\ninteractions can be computed explicitly but this leads to a computational\ncomplexity of at least $\\mathcal{O}(p^2)$ if done naively. This cost can be\nprohibitive if $p$ is very large. We introduce a new randomised algorithm that\nis able to discover interactions with high probability and under mild\nconditions has a runtime that is subquadratic in $p$. We show that strong\ninteractions can be discovered in almost linear time, whilst finding weaker\ninteractions requires $\\mathcal{O}(p^\\alpha)$ operations for $1 < \\alpha < 2$\ndepending on their strength. The underlying idea is to transform interaction\nsearch into a closestpair problem which can be solved efficiently in\nsubquadratic time. The algorithm is called $\\mathit{xyz}$ and is implemented in\nthe language R. We demonstrate its efficiency for application to genome-wide\nassociation studies, where more than $10^{11}$ interactions can be screened in\nunder $280$ seconds with a single-core $1.2$ GHz CPU. \n\n"}
{"id": "1610.05400", "contents": "Title: Going off the Grid: Iterative Model Selection for Biclustered Matrix\n  Completion Abstract: We consider the problem of performing matrix completion with side information\non row-by-row and column-by-column similarities. We build upon recent proposals\nfor matrix estimation with smoothness constraints with respect to row and\ncolumn graphs. We present a novel iterative procedure for directly minimizing\nan information criterion in order to select an appropriate amount row and\ncolumn smoothing, namely perform model selection. We also discuss how to\nexploit the special structure of the problem to scale up the estimation and\nmodel selection procedure via the Hutchinson estimator. We present simulation\nresults and an application to predicting associations in imaging-genomics\nstudies. \n\n"}
{"id": "1610.08205", "contents": "Title: Wall modeling via function enrichment within a high-order DG method for\n  RANS simulations of incompressible flow Abstract: We present a novel approach to wall modeling for RANS within the\ndiscontinuous Galerkin method. Wall functions are not used to prescribe\nboundary conditions as usual but they are built into the function space of the\nnumerical method as a local enrichment, in addition to the standard polynomial\ncomponent. The Galerkin method then automatically finds the optimal solution\namong all shape functions available. This idea is fully consistent and gives\nthe wall model vast flexibility in separated boundary layers or high adverse\npressure gradients. The wall model is implemented in a high-order discontinuous\nGalerkin solver for incompressible flow complemented by the Spalart-Allmaras\nclosure model. As benchmark examples we present turbulent channel flow starting\nfrom $Re_{\\tau}=180$ and up to $Re_{\\tau}=100{,}000$ as well as flow past\nperiodic hills at Reynolds numbers based on the hill height of $Re_H=10{,}595$\nand $Re_{H}=19{,}000$. \n\n"}
{"id": "1610.09005", "contents": "Title: Fast and Consistent Algorithm for the Latent Block Model Abstract: The latent block model is used to simultaneously rank the rows and columns of\na matrix to reveal a block structure. The algorithms used for estimation are\noften time consuming. However, recent work shows that the log-likelihood ratios\nare equivalent under the complete and observed (with unknown labels) models and\nthe groups posterior distribution to converge as the size of the data increases\nto a Dirac mass located at the actual groups configuration. Based on these\nobservations, the algorithm $Largest$ $Gaps$ is proposed in this paper to\nperform clustering using only the marginals of the matrix, when the number of\nblocks is very small with respect to the size of the whole matrix in the case\nof binary data. In addition, a model selection method is incorporated with a\nproof of its consistency. Thus, this paper shows that studying simplistic\nconfigurations (few blocks compared to the size of the matrix or very\ncontrasting blocks) with complex algorithms is useless since the marginals\nalready give very good parameter and classification estimates. \n\n"}
{"id": "1610.09641", "contents": "Title: Auxiliary gradient-based sampling algorithms Abstract: We introduce a new family of MCMC samplers that combine auxiliary variables,\nGibbs sampling and Taylor expansions of the target density. Our approach\npermits the marginalisation over the auxiliary variables yielding marginal\nsamplers, or the augmentation of the auxiliary variables, yielding auxiliary\nsamplers. The well-known Metropolis-adjusted Langevin algorithm (MALA) and\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm are shown to be special\ncases. We prove that marginal samplers are superior in terms of asymptotic\nvariance and demonstrate cases where they are slower in computing time compared\nto auxiliary samplers. In the context of latent Gaussian models we propose new\nauxiliary and marginal samplers whose implementation requires a single tuning\nparameter, which can be found automatically during the transient phase.\nExtensive experimentation shows that the increase in efficiency (measured as\neffective sample size per unit of computing time) relative to (optimised\nimplementations of) pCNL, elliptical slice sampling and MALA ranges from\n10-fold in binary classification problems to 25-fold in log-Gaussian Cox\nprocesses to 100-fold in Gaussian process regression, and it is on par with\nRiemann manifold Hamiltonian Monte Carlo in an example where the latter has the\nsame complexity as the aforementioned algorithms. We explain this remarkable\nimprovement in terms of the way alternative samplers try to approximate the\neigenvalues of the target. We introduce a novel MCMC sampling scheme for\nhyperparameter learning that builds upon the auxiliary samplers. The MATLAB\ncode for reproducing the experiments in the article is publicly available and a\nSupplement to this article contains additional experiments and implementation\ndetails. \n\n"}
{"id": "1610.09724", "contents": "Title: Likelihood Inference for Large Scale Stochastic Blockmodels with\n  Covariates based on a Divide-and-Conquer Parallelizable Algorithm with\n  Communication Abstract: We consider a stochastic blockmodel equipped with node covariate information,\nthat is helpful in analyzing social network data. The key objective is to\nobtain maximum likelihood estimates of the model parameters. For this task, we\ndevise a fast, scalable Monte Carlo EM type algorithm based on case-control\napproximation of the log-likelihood coupled with a subsampling approach. A key\nfeature of the proposed algorithm is its parallelizability, by processing\nportions of the data on several cores, while leveraging communication of key\nstatistics across the cores during each iteration of the algorithm. The\nperformance of the algorithm is evaluated on synthetic data sets and compared\nwith competing methods for blockmodel parameter estimation. We also illustrate\nthe model on data from a Facebook derived social network enhanced with node\ncovariate information. \n\n"}
{"id": "1610.09787", "contents": "Title: Edward: A library for probabilistic modeling, inference, and criticism Abstract: Probabilistic modeling is a powerful approach for analyzing empirical\ninformation. We describe Edward, a library for probabilistic modeling. Edward's\ndesign reflects an iterative process pioneered by George Box: build a model of\na phenomenon, make inferences about the model given data, and criticize the\nmodel's fit to the data. Edward supports a broad class of probabilistic models,\nefficient algorithms for inference, and many techniques for model criticism.\nThe library builds on top of TensorFlow to support distributed training and\nhardware such as GPUs. Edward enables the development of complex probabilistic\nmodels and their algorithms at a massive scale. \n\n"}
{"id": "1610.09788", "contents": "Title: Pseudo-marginal Metropolis--Hastings using averages of unbiased\n  estimators Abstract: We consider a pseudo-marginal Metropolis--Hastings kernel $P_m$ that is\nconstructed using an average of $m$ exchangeable random variables, as well as\nan analogous kernel $P_s$ that averages $s<m$ of these same random variables.\nUsing an embedding technique to facilitate comparisons, we show that the\nasymptotic variances of ergodic averages associated with $P_m$ are lower\nbounded in terms of those associated with $P_s$. We show that the bound\nprovided is tight and disprove a conjecture that when the random variables to\nbe averaged are independent, the asymptotic variance under $P_m$ is never less\nthan $s/m$ times the variance under $P_s$. The conjecture does, however, hold\nwhen considering continuous-time Markov chains. These results imply that if the\ncomputational cost of the algorithm is proportional to $m$, it is often better\nto set $m=1$. We provide intuition as to why these findings differ so markedly\nfrom recent results for pseudo-marginal kernels employing particle filter\napproximations. Our results are exemplified through two simulation studies; in\nthe first the computational cost is effectively proportional to $m$ and in the\nsecond there is a considerable start-up cost at each iteration. \n\n"}
{"id": "1611.01069", "contents": "Title: Maxima Units Search (MUS) algorithm: methodology and applications Abstract: An algorithm for extracting identity submatrices of small rank and pivotal\nunits from large and sparse matrices is proposed. The procedure has already\nbeen satisfactorily applied for solving the label switching problem in Bayesian\nmixture models. Here we introduce it on its own and explore possible\napplications in different contexts. \n\n"}
{"id": "1611.01213", "contents": "Title: Bayesian and Variational Bayesian approaches for flows in heterogenous\n  random media Abstract: In this paper, we study porous media flows in heterogeneous stochastic media.\nWe propose an efficient forward simulation technique that is tailored for\nvariational Bayesian inversion. As a starting point, the proposed forward\nsimulation technique decomposes the solution into the sum of separable\nfunctions (with respect to randomness and the space), where each term is\ncalculated based on a variational approach. This is similar to Proper\nGeneralized Decomposition (PGD). Next, we apply a multiscale technique to solve\nfor each term and, further, decompose the random function into 1D fields. As a\nresult, our proposed method provides an approximation hierarchy for the\nsolution as we increase the number of terms in the expansion and, also,\nincrease the spatial resolution of each term. We use the hierarchical solution\ndistributions in a variational Bayesian approximation to perform uncertainty\nquantification in the inverse problem. We conduct a detailed numerical study to\nexplore the performance of the proposed uncertainty quantification technique\nand show the theoretical posterior concentration. \n\n"}
{"id": "1611.01310", "contents": "Title: Achieving Shrinkage in a Time-Varying Parameter Model Framework Abstract: Shrinkage for time-varying parameter (TVP) models is investigated within a\nBayesian framework, with the aim to automatically reduce time-varying\nparameters to static ones, if the model is overfitting. This is achieved\nthrough placing the double gamma shrinkage prior on the process variances. An\nefficient Markov chain Monte Carlo scheme is developed, exploiting boosting\nbased on the ancillarity-sufficiency interweaving strategy. The method is\napplicable both to TVP models for univariate as well as multivariate time\nseries. Applications include a TVP generalized Phillips curve for EU area\ninflation modelling and a multivariate TVP Cholesky stochastic volatility model\nfor joint modelling of the returns from the DAX-30 index. \n\n"}
{"id": "1611.01353", "contents": "Title: Information Dropout: Learning Optimal Representations Through Noisy\n  Computation Abstract: The cross-entropy loss commonly used in deep learning is closely related to\nthe defining properties of optimal representations, but does not enforce some\nof the key properties. We show that this can be solved by adding a\nregularization term, which is in turn related to injecting multiplicative noise\nin the activations of a Deep Neural Network, a special case of which is the\ncommon practice of dropout. We show that our regularized loss function can be\nefficiently minimized using Information Dropout, a generalization of dropout\nrooted in information theoretic principles that automatically adapts to the\ndata and can better exploit architectures of limited capacity. When the task is\nthe reconstruction of the input, we show that our loss function yields a\nVariational Autoencoder as a special case, thus providing a link between\nrepresentation learning, information theory and variational inference. Finally,\nwe prove that we can promote the creation of disentangled representations\nsimply by enforcing a factorized prior, a fact that has been observed\nempirically in recent work. Our experiments validate the theoretical intuitions\nbehind our method, and we find that information dropout achieves a comparable\nor better generalization performance than binary dropout, especially on smaller\nmodels, since it can automatically adapt the noise to the structure of the\nnetwork, as well as to the test sample. \n\n"}
{"id": "1611.01450", "contents": "Title: Estimating the marginal likelihood with Integrated nested Laplace\n  approximation (INLA) Abstract: The marginal likelihood is a well established model selection criterion in\nBayesian statistics. It also allows to efficiently calculate the marginal\nposterior model probabilities that can be used for Bayesian model averaging of\nquantities of interest. For many complex models, including latent modeling\napproaches, marginal likelihoods are however difficult to compute. One recent\npromising approach for approximating the marginal likelihood is Integrated\nNested Laplace Approximation (INLA), design for models with latent Gaussian\nstructures. In this study we compare the approximations obtained with INLA to\nsome alternative approaches on a number of examples of different complexity. In\nparticular we address a simple linear latent model, a Bayesian linear\nregression model, logistic Bayesian regression models with probit and logit\nlinks, and a Poisson longitudinal generalized linear mixed model. \n\n"}
{"id": "1611.02213", "contents": "Title: A Low-rank Control Variate for Multilevel Monte Carlo Simulation of\n  High-dimensional Uncertain Systems Abstract: Multilevel Monte Carlo (MLMC) is a recently proposed variation of Monte Carlo\n(MC) simulation that achieves variance reduction by simulating the governing\nequations on a series of spatial (or temporal) grids with increasing\nresolution. Instead of directly employing the fine grid solutions, MLMC\nestimates the expectation of the quantity of interest from the coarsest grid\nsolutions as well as differences between each two consecutive grid solutions.\nWhen the differences corresponding to finer grids become smaller, hence less\nvariable, fewer MC realizations of finer grid solutions are needed to compute\nthe difference expectations, thus leading to a reduction in the overall work.\nThis paper presents an extension of MLMC, referred to as multilevel control\nvariates (MLCV), where a low-rank approximation to the solution on each grid,\nobtained primarily based on coarser grid solutions, is used as a control\nvariate for estimating the expectations involved in MLMC. Cost estimates as\nwell as numerical examples are presented to demonstrate the advantage of this\nnew MLCV approach over the standard MLMC when the solution of interest admits a\nlow-rank approximation and the cost of simulating finer grids grows fast. \n\n"}
{"id": "1611.02754", "contents": "Title: Gradient-informed basis adaptation for Legendre Chaos expansions Abstract: The recently introduced basis adaptation method for Homogeneous (Wiener)\nChaos expansions is explored in a new context where the rotation/projection\nmatrices are computed by discovering the active subspace where the random input\nexhibits most of its variability. In the case where a 1-dimensional active\nsubspace exists, the methodology can be applicable to generalized Polynomial\nChaos expansions, thus enabling the projection of a high dimensional input to a\nsingle input variable and the efficient estimation of a univariate chaos\nexpansion. Attractive features of this approach, such as the significant\ncomputational savings and the high accuracy in computing statistics of interest\nare investigated. \n\n"}
{"id": "1611.03112", "contents": "Title: Multiple imputation of multilevel missing data: An introduction to the R\n  package pan Abstract: The treatment of missing data can be difficult in multilevel research because\nstate-of-the-art procedures such as multiple imputation (MI) may require\nadvanced statistical knowledge or a high degree of familiarity with certain\nstatistical software. In the missing data literature, pan has been recommended\nfor MI of multilevel data. In this article, we provide an introduction to MI of\nmultilevel missing data using the R package pan, and we discuss its\npossibilities and limitations in accommodating typical questions in multilevel\nresearch. In order to make pan more accessible to applied researchers, we make\nuse of the mitml package, which provides a user-friendly interface to the pan\npackage and several tools for managing and analyzing multiply imputed data\nsets. We illustrate the use of pan and mitml with two empirical examples that\nrepresent common applications of multilevel models, and we discuss how these\nprocedures may be used in conjunction with other software. \n\n"}
{"id": "1611.03177", "contents": "Title: A Note on Random Walks with Absorbing barriers and Sequential Monte\n  Carlo Methods Abstract: In this article we consider importance sampling (IS) and sequential Monte\nCarlo (SMC) methods in the context of 1-dimensional random walks with absorbing\nbarriers. In particular, we develop a very precise variance analysis for\nseveral IS and SMC procedures. We take advantage of some explicit spectral\nformulae available for these models to derive sharp and explicit estimates;\nthis provides stability properties of the associated normalized Feynman-Kac\nsemigroups. Our analysis allows one to compare the variance of SMC and IS\ntechniques for these models. The work in this article, is one of the few to\nconsider an in-depth analysis of an SMC method for a particular model-type as\nwell as variance comparison of SMC algorithms. \n\n"}
{"id": "1611.04416", "contents": "Title: On numerical approximation schemes for expectation propagation Abstract: Several numerical approximation strategies for the expectation-propagation\nalgorithm are studied in the context of large-scale learning: the Laplace\nmethod, a faster variant of it, Gaussian quadrature, and a deterministic\nversion of variational sampling (i.e., combining quadrature with variational\napproximation). Experiments in training linear binary classifiers show that the\nexpectation-propagation algorithm converges best using variational sampling,\nwhile it also converges well using Laplace-style methods with smooth factors\nbut tends to be unstable with non-differentiable ones. Gaussian quadrature\nyields unstable behavior or convergence to a sub-optimal solution in most\nexperiments. \n\n"}
{"id": "1611.05780", "contents": "Title: Gap Safe screening rules for sparsity enforcing penalties Abstract: In high dimensional regression settings, sparsity enforcing penalties have\nproved useful to regularize the data-fitting term. A recently introduced\ntechnique called screening rules propose to ignore some variables in the\noptimization leveraging the expected sparsity of the solutions and consequently\nleading to faster solvers. When the procedure is guaranteed not to discard\nvariables wrongly the rules are said to be safe. In this work, we propose a\nunifying framework for generalized linear models regularized with standard\nsparsity enforcing penalties such as $\\ell_1$ or $\\ell_1/\\ell_2$ norms. Our\ntechnique allows to discard safely more variables than previously considered\nsafe rules, particularly for low regularization parameters. Our proposed Gap\nSafe rules (so called because they rely on duality gap computation) can cope\nwith any iterative solver but are particularly well suited to (block)\ncoordinate descent methods. Applied to many standard learning tasks, Lasso,\nSparse-Group Lasso, multi-task Lasso, binary and multinomial logistic\nregression, etc., we report significant speed-ups compared to previously\nproposed safe rules on all tested data sets. \n\n"}
{"id": "1611.07056", "contents": "Title: The Recycling Gibbs Sampler for Efficient Learning Abstract: Monte Carlo methods are essential tools for Bayesian inference. Gibbs\nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively\nused in signal processing, machine learning, and statistics, employed to draw\nsamples from complicated high-dimensional posterior distributions. The key\npoint for the successful application of the Gibbs sampler is the ability to\ndraw efficiently samples from the full-conditional probability density\nfunctions. Since in the general case this is not possible, in order to speed up\nthe convergence of the chain, it is required to generate auxiliary samples\nwhose information is eventually disregarded. In this work, we show that these\nauxiliary samples can be recycled within the Gibbs estimators, improving their\nefficiency with no extra cost. This novel scheme arises naturally after\npointing out the relationship between the standard Gibbs sampler and the chain\nrule used for sampling purposes. Numerical simulations involving simple and\nreal inference problems confirm the excellent performance of the proposed\nscheme in terms of accuracy and computational efficiency. In particular we give\nempirical evidence of performance in a toy example, inference of Gaussian\nprocesses hyperparameters, and learning dependence graphs through regression. \n\n"}
{"id": "1611.07521", "contents": "Title: The QUESO Library, User's Manual Abstract: QUESO stands for Quantification of Uncertainty for Estimation, Simulation and\nOptimization and consists of algorithms and C++ classes intended for research\nin uncertainty quantification, including the solution of statistical inverse\nproblem and statistical forward problems, the validation of mathematical models\nunder uncertainty, and the prediction of quantities of interest from such\nmodels along with quantification of their uncertainties. QUESO is designed for\nflexibility, portability, ease of use and ease of extension. Its software\ndesign follows an object oriented approach and its code is written on C++ and\nover MPI. It can run over uniprocessor and multiprocessor environments. QUESO\ncontains two forms of documentation: a user's manual available in PDF format\nand a lower level code documentation available in web based/HTML format. The\npresent document is a user's manual which provides an overview of the\ncapabilities of QUESO, procedures for software execution, and includes example\nstudies. \n\n"}
{"id": "1611.08996", "contents": "Title: JIGSAW-GEO (1.0): locally orthogonal staggered unstructured grid\n  generation for general circulation modelling on the sphere Abstract: An algorithm for the generation of non-uniform, locally-orthogonal staggered\nunstructured spheroidal grids is described. This technique is designed to\ngenerate very high-quality staggered Voronoi/Delaunay meshes appropriate for\ngeneral circulation modelling on the sphere, including applications to\natmospheric simulation, ocean-modelling and numerical weather prediction. Using\na recently developed Frontal-Delaunay refinement technique, a method for the\nconstruction of high-quality unstructured spheroidal Delaunay triangulations is\nintroduced. A locally-orthogonal polygonal grid, derived from the associated\nVoronoi diagram, is computed as the staggered dual. It is shown that use of the\nFrontal-Delaunay refinement technique allows for the generation of very\nhigh-quality unstructured triangulations, satisfying a-priori bounds on element\nsize and shape. Grid-quality is further improved through the application of\nhill-climbing type optimisation techniques. Overall, the algorithm is shown to\nproduce grids with very high element quality and smooth grading\ncharacteristics, while imposing relatively low computational expense. A\nselection of uniform and non-uniform spheroidal grids appropriate for\nhigh-resolution, multi-scale general circulation modelling are presented. These\ngrids are shown to satisfy the geometric constraints associated with\ncontemporary unstructured C-grid type finite-volume models, including the Model\nfor Prediction Across Scales (MPAS-O). The use of user-defined mesh-spacing\nfunctions to generate smoothly graded, non-uniform grids for multi-resolution\ntype studies is discussed in detail. \n\n"}
{"id": "1611.09252", "contents": "Title: Fast Mixing Random Walks and Regularity of Incompressible Vector Fields Abstract: We show sufficient conditions under which the \\textsc{BallWalk} algorithm\nmixes fast in a bounded connected subset of $\\Real^n$. In particular, we show\nfast mixing if the space is the transformation of a convex space under a smooth\nincompressible flow. Construction of such smooth flows is in turn reduced to\nthe study of the regularity of the solution of the Dirichlet problem for\nLaplace's equation. \n\n"}
{"id": "1611.10171", "contents": "Title: Stability selection for component-wise gradient boosting in multiple\n  dimensions Abstract: We present a new algorithm for boosting generalized additive models for\nlocation, scale and shape (GAMLSS) that allows to incorporate stability\nselection, an increasingly popular way to obtain stable sets of covariates\nwhile controlling the per-family error rate (PFER). The model is fitted\nrepeatedly to subsampled data and variables with high selection frequencies are\nextracted. To apply stability selection to boosted GAMLSS, we develop a new\n\"noncyclical\" fitting algorithm that incorporates an additional selection step\nof the best-fitting distribution parameter in each iteration. This new\nalgorithms has the additional advantage that optimizing the tuning parameters\nof boosting is reduced from a multi-dimensional to a one-dimensional problem\nwith vastly decreased complexity. The performance of the novel algorithm is\nevaluated in an extensive simulation study. We apply this new algorithm to a\nstudy to estimate abundance of common eider in Massachusetts, USA, featuring\nexcess zeros, overdispersion, non-linearity and spatio-temporal structures.\nEider abundance is estimated via boosted GAMLSS, allowing both mean and\noverdispersion to be regressed on covariates. Stability selection is used to\nobtain a sparse set of stable predictors. \n\n"}
{"id": "1612.00762", "contents": "Title: Structured Filtering Abstract: A major challenge facing existing sequential Monte-Carlo methods for\nparameter estimation in physics stems from the inability of existing approaches\nto robustly deal with experiments that have different mechanisms that yield the\nresults with equivalent probability. We address this problem here by proposing\na form of particle filtering that clusters the particles that comprise the\nsequential Monte-Carlo approximation to the posterior before applying a\nresampler. Through a new graphical approach to thinking about such models, we\nare able to devise an artificial-intelligence based strategy that automatically\nlearns the shape and number of the clusters in the support of the posterior. We\ndemonstrate the power of our approach by applying it to randomized gap\nestimation and a form of low circuit-depth phase estimation where existing\nmethods from the physics literature either exhibit much worse performance or\neven fail completely. \n\n"}
{"id": "1612.00951", "contents": "Title: On the Pitfalls of Nested Monte Carlo Abstract: There is an increasing interest in estimating expectations outside of the\nclassical inference framework, such as for models expressed as probabilistic\nprograms. Many of these contexts call for some form of nested inference to be\napplied. In this paper, we analyse the behaviour of nested Monte Carlo (NMC)\nschemes, for which classical convergence proofs are insufficient. We give\nconditions under which NMC will converge, establish a rate of convergence, and\nprovide empirical data that suggests that this rate is observable in practice.\nFinally, we prove that general-purpose nested inference schemes are inherently\nbiased. Our results serve to warn of the dangers associated with naive\ncomposition of inference and models. \n\n"}
{"id": "1612.01719", "contents": "Title: Tropical Vorticity Forcing and Superrotation in the Spherical Shallow\n  Water Equations Abstract: The response of the nonlinear shallow water equations (SWE) on a sphere to\ntropical vorticity forcing is examined with an emphasis on momentum fluxes and\nthe emergence of a superrotating (SR) state. Fixing the radiative damping and\nmomentum drag timescales to be of the order of a few days, a state of SR is\nshown to emerge under steady large-scale and random small-scale vorticity\nforcing. In the first example, the stationary response to a pair of equal and\noppositely signed vortices placed on the equator is considered. Here, the\nequatorial flux budget is dominated by the eddy fluxes and these drive the\nsystem into a state of SR. Eventually, the flux associated with the momentum\ndrag increases to balance the eddy fluxes, resulting in a steady state with a\nSR jet at the equator. The initial value problem with these twin vortices also\nexhibits SR driven by eddy fluxes. Curiously, this transient solution\nspontaneously propagates westward and continually circumnavigates the globe. It\nis worth emphasizing that this SR state does not rely on any particular form of\ndissipation at large scales, and is in fact observed even in the absence of any\nlarge-scale damping mechanism. In the second example, a random small-scale\nvorticity forcing is applied across the tropics. The statistically steady state\nobtained is fairly turbulent in nature, but here too, the eddy fluxes dominate,\nand the system exhibits SR. It is important to note that in both these cases,\nthe direct forcing of the zonal mean zonal flow is zero by construction, and\nthe eddy fluxes at the equator are responsible for its eastward acceleration.\nFurther, in both these examples, the rotational part of the flow dominates the\nmomentum fluxes as well as the stationary zonal mean zonal flow itself.\nArguments based on the nature of potential vorticity and enstrophy are put\nforth to shed some light on these results. \n\n"}
{"id": "1612.01872", "contents": "Title: Simulation from quasi-stationary distributions on reducible state spaces Abstract: Quasi-stationary distributions (QSDs)arise from stochastic processes that\nexhibit transient equilibrium behaviour on the way to absorption QSDs are often\nmathematically intractable and even drawing samples from them is not\nstraightforward. In this paper the framework of Sequential Monte Carlo samplers\nis utilized to simulate QSDs and several novel resampling techniques are\nproposed to accommodate models with reducible state spaces, with particular\nfocus on preserving particle diversity on discrete spaces. Finally an approach\nis considered to estimate eigenvalues associated with QSDs, such as the decay\nparameter. \n\n"}
{"id": "1612.02195", "contents": "Title: Generalized Exponential smoothing in prediction of hierarchical time\n  series Abstract: Shang and Hyndman (2017) proposed a grouped functional time series\nforecasting approach as a combination of individual forecasts obtained using\ngeneralized least squares method. We modify their methodology using generalized\nexponential smoothing technique for the most disaggregated functional time\nseries in order to obtain more robust predictor. We discuss some properties of\nour proposals basing on results obtained via simulation studies and analysis of\nreal data related to a prediction of a demand for electricity in Australia in\n2016. \n\n"}
{"id": "1612.03243", "contents": "Title: Statistical State Dynamics of Vertically Sheared Horizontal Flows in\n  Two-Dimensional Stratified Turbulence Abstract: Simulations of strongly stratified turbulence often exhibit coherent\nlarge-scale structures called vertically sheared horizontal flows (VSHFs).\nVSHFs emerge in both two-dimensional (2D) and three-dimensional (3D) stratified\nturbulence with similar vertical structure. The mechanism responsible for VSHF\nformation is not fully understood. In this work, the formation and\nequilibration of VSHFs in a 2D Boussinesq model of stratified turbulence is\nstudied using statistical state dynamics (SSD). In SSD, equations of motion are\nexpressed directly in the statistical variables of the turbulent state.\nRestriction to 2D turbulence makes available an analytically and\ncomputationally attractive implementation of SSD referred to as S3T, in which\nthe SSD is expressed by coupling the equation for the horizontal mean structure\nwith the equation for the ensemble mean perturbation covariance. This second\norder SSD produces accurate statistics, through second order, when compared\nwith fully nonlinear simulations. In particular, S3T captures the spontaneous\nemergence of the VSHF and associated density layers seen in simulations of\nturbulence maintained by homogeneous large-scale stochastic excitation. An\nadvantage of the S3T system is that the VSHF formation mechanism, which is\nwave-mean flow interaction between the emergent VSHF and the stochastically\nexcited large-scale gravity waves, is analytically understood in the S3T\nsystem. Comparison with fully nonlinear simulations verifies that S3T solutions\naccurately predict the scale selection, dependence on stochastic excitation\nstrength, and nonlinear equilibrium structure of the VSHF. These results\nfacilitate relating VSHF theory and geophysical examples of turbulent jets such\nas the ocean's equatorial deep jets. \n\n"}
{"id": "1612.03319", "contents": "Title: Anytime Monte Carlo Abstract: Monte Carlo algorithms simulate some prescribed number of samples, taking\nsome random real time to complete the computations necessary. This work\nconsiders the converse: to impose a real-time budget on the computation, which\nresults in the number of samples simulated being random. To complicate matters,\nthe real time taken for each simulation may depend on the sample produced, so\nthat the samples themselves are not independent of their number, and a length\nbias with respect to compute time is apparent. This is especially problematic\nwhen a Markov chain Monte Carlo (MCMC) algorithm is used and the final state of\nthe Markov chain -- rather than an average over all states -- is required,\nwhich is the case in parallel tempering implementations of MCMC. The length\nbias does not diminish with the compute budget in this case. It also occurs in\nsequential Monte Carlo (SMC) algorithms, which is the focus of this paper. We\npropose an anytime framework to address the concern, using a continuous-time\nMarkov jump process to study the progress of the computation in real time. We\nfirst show that for any MCMC algorithm, the length bias of the final state's\ndistribution due to the imposed real-time computing budget can be eliminated by\nusing a multiple chain construction. The utility of this construction is then\ndemonstrated on a large-scale SMC^2 implementation, using four billion\nparticles distributed across a cluster of 128 graphics processing units on the\nAmazon EC2 service. The anytime framework imposes a real-time budget on the\nMCMC move steps within the SMC$^{2}$ algorithm, ensuring that all processors\nare simultaneously ready for the resampling step, demonstrably reducing\nidleness to due waiting times and providing substantial control over the total\ncompute budget. \n\n"}
{"id": "1612.04271", "contents": "Title: BayesBD: An R Package for Bayesian Inference on Image Boundaries Abstract: We present the BayesBD package providing Bayesian inference for boundaries of\nnoisy images. The BayesBD package implements flexible Gaussian process priors\nindexed by the circle to recover the boundary in a binary or Gaussian noised\nimage, with the benefits of guaranteed geometric restrictions on the estimated\nboundary, (nearly) minimax optimal and smoothness adaptive convergence rates,\nand convenient joint inferences under certain assumptions. The core sampling\ntasks for our model have linear complexity, and our implementation in c++ using\npackages Rcpp and RcppArmadillo is computationally efficient. Users can access\nthe full functionality of the package in both Rgui and the corresponding shiny\napplication. Additionally, the package includes numerous utility functions to\naid users in data preparation and analysis of results. We compare BayesBD with\nselected existing packages using both simulations and real data applications,\nand demonstrate the excellent performance and flexibility of BayesBD even when\nthe observation contains complicated structural information that may violate\nits assumptions. \n\n"}
{"id": "1612.05053", "contents": "Title: Expectation Propagation performs a smoothed gradient descent Abstract: Bayesian inference is a popular method to build learning algorithms but it is\nhampered by the fact that its key object, the posterior probability\ndistribution, is often uncomputable. Expectation Propagation (EP) (Minka\n(2001)) is a popular algorithm that solves this issue by computing a parametric\napproximation (e.g: Gaussian) to the density of the posterior. However, while\nit is known empirically to quickly compute fine approximations, EP is extremely\npoorly understood which prevents it from being adopted by a larger fraction of\nthe community.\n  The object of the present article is to shed intuitive light on EP, by\nrelating it to other better understood methods. More precisely, we link it to\nusing gradient descent to compute the Laplace approximation of a target\nprobability distribution. We show that EP is exactly equivalent to performing\ngradient descent on a smoothed energy landscape: i.e: the original energy\nlandscape convoluted with some smoothing kernel. This also relates EP to\nalgorithms that compute the Gaussian approximation which minimizes the reverse\nKL divergence to the target distribution, a link that has been conjectured\nbefore but has not been proved rigorously yet. These results can help\npractitioners to get a better feel for how EP works, as well as lead to other\nnew results on this important method. \n\n"}
{"id": "1612.06242", "contents": "Title: On the mathematical description of time-dependent surface water waves Abstract: This article provides a survey on some main results and recent developments\nin the mathematical theory of water waves. More precisely, we briefly discuss\nthe mathematical modeling of water waves and then we give an overview of local\nand global well-posedness results for the model equations. Moreover, we present\nreduced models in various parameter regimes for the approximate description of\nthe motion of typical wave profiles and discuss the mathematically rigorous\njustification of the validity of these models. \n\n"}
{"id": "1612.06492", "contents": "Title: Chunked-and-Averaged Estimators for Vector Parameters Abstract: A divide-and-conquer method for parameter estimation is the\nchunked-and-averaged (CA) estimator. CA estimators have been studied for\nunivariate parameters under independent and identically distributed (IID)\nsampling. We study the CA estimators of vector parameters and under non-IID\nsampling. \n\n"}
{"id": "1612.06998", "contents": "Title: Postprocessing Galerkin method applied to a data assimilation algorithm:\n  a uniform in time error estimate Abstract: We apply the Postprocessing Galerkin method to a recently introduced\ncontinuous data assimilation (downscaling) algorithm for obtaining a numerical\napproximation of the solution of the two-dimensional Navier-Stokes equations\ncorresponding to given measurements from a coarse spatial mesh. Under suitable\nconditions on the relaxation (nudging) parameter, the resolution of the coarse\nspatial mesh and the resolution of the numerical scheme, we obtain uniform in\ntime estimates for the error between the numerical approximation given by the\nPostprocessing Galerkin method and the reference solution corresponding to the\nmeasurements. Our results are valid for a large class of interpolant operators,\nincluding low Fourier modes and local averages over finite volume elements.\nNotably, we use here the 2D Navier-Stokes equations as a paradigm, but our\nresults apply equally to other evolution equations, such as the Boussinesq\nsystem of Benard convection and other oceanic and atmospheric circulation\nmodels. \n\n"}
{"id": "1612.07010", "contents": "Title: Permutation in genetic association studies with covariates: controlling\n  the familywise error rate with score tests in generalized linear models Abstract: In genome-wide association (GWA) studies the goal is to detect associations\nbetween genetic markers and a given phenotype. The number of genetic markers\ncan be large and effective methods for control of the overall error rate is a\ncentral topic when analyzing GWA data. The Bonferroni method is known to be\nconservative when the tests are dependent. Permutation methods give exact\ncontrol of the overall error rate when the assumption of exchangeability is\nsatisfied, but are computationally intensive for large datasets. For regression\nmodels the exchangeability assumption is in general not satisfied and there is\nno standard solution on how to do permutation testing, except some approximate\nmethods. In this paper we will discuss permutation methods for control of the\nfamilywise error rate in genetic association studies and present an approximate\nsolution. These methods will be compared using simulated data. \n\n"}
{"id": "1612.07223", "contents": "Title: A proof of concept for scale-adaptive parameterizations: the case of the\n  Lorenz '96 model Abstract: Constructing efficient and accurate parameterizations of sub-grid scale\nprocesses is a central area of interest in the numerical modelling of\ngeophysical fluids. Using a modified version of the two-level Lorenz '96 model,\nwe present here a proof of concept of a scale-adaptive parameterization\nconstructed using statistical mechanical arguments. By a suitable use of the\nRuelle response theory and of the Mori-Zwanzig projection method, it is\npossible to derive explicitly a parameterization for the fast variables that\ntranslates into deterministic, stochastic and non-markovian contributions to\nthe equations of motion of the variables of interest. We show that our approach\nis computationally parsimonious, has great flexibility, as it is explicitly\nscale-adaptive, and we prove that it is competitive compared to empirical\nad-hoc approaches. While the parameterization proposed here is universal and\ncan be easily analytically adapted to changes in the parameters' values by a\nsimple rescaling procedure, the parameterization constructed with the ad-hoc\napproach needs to be recomputed each time the parameters of the systems are\nchanged. The price of the higher flexibility of the method proposed here is\nhaving a lower accuracy in each individual case. \n\n"}
{"id": "1612.07717", "contents": "Title: Multilevel Monte Carlo and Improved Timestepping Methods in Atmospheric\n  Dispersion Modelling Abstract: A common way to simulate the transport and spread of pollutants in the\natmosphere is via stochastic Lagrangian dispersion models. Mathematically,\nthese models describe turbulent transport processes with stochastic\ndifferential equations (SDEs). The computational bottleneck is the Monte Carlo\nalgorithm, which simulates the motion of a large number of model particles in a\nturbulent velocity field; for each particle, a trajectory is calculated with a\nnumerical timestepping method. Choosing an efficient numerical method is\nparticularly important in operational emergency-response applications, such as\ntracking radioactive clouds from nuclear accidents or predicting the impact of\nvolcanic ash clouds on international aviation, where accurate and timely\npredictions are essential. In this paper, we investigate the application of the\nMultilevel Monte Carlo (MLMC) method to simulate the propagation of particles\nin a representative one-dimensional dispersion scenario in the atmospheric\nboundary layer. MLMC can be shown to result in asymptotically superior\ncomputational complexity and reduced computational cost when compared to the\nStandard Monte Carlo (StMC) method, which is currently used in atmospheric\ndispersion modelling. To reduce the absolute cost of the method also in the\nnon-asymptotic regime, it is equally important to choose the best possible\nnumerical timestepping method on each level. To investigate this, we also\ncompare the standard symplectic Euler method, which is used in many operational\nmodels, with two improved timestepping algorithms based on SDE splitting\nmethods. \n\n"}
{"id": "1612.08141", "contents": "Title: PLMIX: An R package for modeling and clustering partially ranked data Abstract: Ranking data represent a peculiar form of multivariate ordinal data taking\nvalues in the set of permutations. Despite the numerous methodological\ncontributions to increase the flexibility of ranked data modeling, the\napplication of more sophisticated models is limited by the related\ncomputational issues. The PLMIX package offers a comprehensive framework aimed\nat endowing the R statistical environment with some recent methodological\nadvances in modeling and clustering partially ranked data. The usefulness of\nthe novel PLMIX package can be motivated from several perspectives: (i) it\ncontributes to fill the gap concerning Bayesian estimation of ranking models in\nR, by focusing on the Plackett-Luce model and its extension within the finite\nmixture approach as the generative sampling distribution; (ii) it addresses\ncomputational complexity by combining the flexibility of R routines and the\nspeed of compiled C++ code, with possible parallel execution; (iii) it covers\nthe fundamental phases of ranking data analysis allowing for a more careful and\ncritical application of ranking models in real experiments; (iv) it provides\neffective tools for clustering heterogeneous partially ranked data. The\nfunctionality of the novel package is illustrated with several applications to\nsimulated and real datasets. \n\n"}
{"id": "1612.09123", "contents": "Title: Population and trends in the global mean temperature Abstract: The Fisher Ideal index, developed to measure price inflation, is applied to\ndefine a population-weighted temperature trend. This method has the advantages\nthat the trend is representative for the population distribution throughout the\nsample but without conflating the trend in the population distribution and the\ntrend in the temperature. I show that the trend in the global area-weighted\naverage surface air temperature is different in key details from the\npopulation-weighted trend. I extend the index to include urbanization and the\nurban heat island effect. This substantially changes the trend again. I further\nextend the index to include international migration, but this has a minor\nimpact on the trend. \n\n"}
{"id": "1701.01672", "contents": "Title: Detecting changes in slope with an $L_0$ penalty Abstract: Whilst there are many approaches to detecting changes in mean for a\nunivariate time-series, the problem of detecting multiple changes in slope has\ncomparatively been ignored. Part of the reason for this is that detecting\nchanges in slope is much more challenging. For example, simple binary\nsegmentation procedures do not work for this problem, whilst efficient dynamic\nprogramming methods that work well for the change in mean problem cannot be\ndirectly used for detecting changes in slope. We present a novel dynamic\nprogramming approach, CPOP, for finding the \"best\" continuous piecewise-linear\nfit to data. We define best based on a criterion that measures fit to data\nusing the residual sum of squares, but penalises complexity based on an $L_0$\npenalty on changes in slope. We show that using such a criterion is more\nreliable at estimating changepoint locations than approaches that penalise\ncomplexity using an $L_1$ penalty. Empirically CPOP has good computational\nproperties, and can analyse a time-series with over 10,000 observations and\nover 100 changes in a few minutes. Our method is used to analyse data on the\nmotion of bacteria, and provides fits to the data that both have substantially\nsmaller residual sum of squares and are more parsimonious than two competing\napproaches. \n\n"}
{"id": "1701.02002", "contents": "Title: Smoothing with Couplings of Conditional Particle Filters Abstract: In state space models, smoothing refers to the task of estimating a latent\nstochastic process given noisy measurements related to the process. We propose\nan unbiased estimator of smoothing expectations. The lack-of-bias property has\nmethodological benefits: independent estimators can be generated in parallel,\nand confidence intervals can be constructed from the central limit theorem to\nquantify the approximation error. To design unbiased estimators, we combine a\ngeneric debiasing technique for Markov chains with a Markov chain Monte Carlo\nalgorithm for smoothing. The resulting procedure is widely applicable and we\nshow in numerical experiments that the removal of the bias comes at a\nmanageable increase in variance. We establish the validity of the proposed\nestimators under mild assumptions. Numerical experiments are provided on toy\nmodels, including a setting of highly-informative observations, and a realistic\nLotka-Volterra model with an intractable transition density. \n\n"}
{"id": "1701.02522", "contents": "Title: Magnus expansions and pseudospectra of Master Equations Abstract: New directions in research on master equations are showcased by example.\nMagnus expansions, time-varying rates, and pseudospectra are highlighted. Exact\neigenvalues are found and contrasted with the large errors produced by standard\nnumerical methods in some cases. Isomerisation provides a running example and\nan illustrative application to chemical kinetics. We also give a brief example\nof the totally asymmetric exclusion process. \n\n"}
{"id": "1701.03405", "contents": "Title: New Flexible Compact Covariance Model on a Sphere Abstract: We discuss how the kernel convolution approach can be used to accurately\napproximate the spatial covariance model on a sphere using spherical distances\nbetween points. A detailed derivation of the required formulas is provided. The\nproposed covariance model approximation can be used for non-stationary spatial\nprediction and simulation in the case when the dataset is large and the\ncovariance model can be estimated separately in the data subsets. \n\n"}
{"id": "1701.03861", "contents": "Title: Network Inference from a Link-Traced Sample using Approximate Bayesian\n  Computation Abstract: We present a new inference method based on approximate Bayesian computation\nfor estimating parameters governing an entire network based on link-traced\nsamples of that network. To do this, we first take summary statistics from an\nobserved link-traced network sample, such as a recruitment network of subjects\nin a hard-to-reach population. Then we assume prior distributions, such as\nmultivariate uniform, for the distribution of some parameters governing the\nstructure of the network and behaviour of its nodes. Then, we draw many\nindependent and identically distributed values for these parameters. For each\nset of values, we simulate a population network, take a link-traced sample from\nthat network, and find the summary statistics for that sample. The statistics\nfrom the sample, and the parameters that eventually led to that sample, are\ncollectively treated as a single point. We take a Kernel Density estimate of\nthe points from many simulations, and observe the density across the hyperplane\ncoinciding with the statistic values of the originally observed sample. This\ndensity function is treat as a posterior estimate of the paramaters of the\nnetwork that provided the observed sample.\n  We also apply this method to a network of precedence citations between legal\ndocuments, centered around cases overseen by the Supreme Court of Canada, is\nobserved. The features of certain cases that lead to their frequent citation\nare inferred, and their effects estimated by ABC. Future work and extensions\nare also briefly discussed. \n\n"}
{"id": "1701.04247", "contents": "Title: Nonreversible Langevin Samplers: Splitting Schemes, Analysis and\n  Implementation Abstract: For a given target density, there exist an infinite number of diffusion\nprocesses which are ergodic with respect to this density. As observed in a\nnumber of papers, samplers based on nonreversible diffusion processes can\nsignificantly outperform their reversible counterparts both in terms of\nasymptotic variance and rate of convergence to equilibrium. In this paper, we\ntake advantage of this in order to construct efficient sampling algorithms\nbased on the Lie-Trotter decomposition of a nonreversible diffusion process\ninto reversible and nonreversible components. We show that samplers based on\nthis scheme can significantly outperform standard MCMC methods, at the cost of\nintroducing some controlled bias. In particular, we prove that numerical\nintegrators constructed according to this decomposition are geometrically\nergodic and characterise fully their asymptotic bias and variance, showing that\nthe sampler inherits the good mixing properties of the underlying nonreversible\ndiffusion. This is illustrated further with a number of numerical examples\nranging from highly correlated low dimensional distributions, to logistic\nregression problems in high dimensions as well as inference for spatial models\nwith many latent variables. \n\n"}
{"id": "1701.04742", "contents": "Title: Stochastic parameterization of subgrid-scale processes: A review of\n  recent physically-based approaches Abstract: We review some recent methods of subgrid-scale parameterization used in the\ncontext of climate modeling. These methods are developed to take into account\n(subgrid) processes playing an important role in the correct representation of\nthe atmospheric and climate variability. We illustrate these methods on a\nsimple stochastic triad system relevant for the atmospheric and climate\ndynamics, and we show in particular that the stability properties of the\nunderlying dynamics of the subgrid processes has a considerable impact on their\nperformances. \n\n"}
{"id": "1701.05128", "contents": "Title: A Constructive Approach to High-dimensional Regression Abstract: We develop a constructive approach to estimating sparse, high-dimensional\nlinear regression models. The approach is a computational algorithm motivated\nfrom the KKT conditions for the $\\ell_0$-penalized least squares solutions. It\ngenerates a sequence of solutions iteratively, based on support detection using\nprimal and dual information and root finding. We refer to the algorithm as SDAR\nfor brevity. Under a sparse Rieze condition on the design matrix and certain\nother conditions, we show that with high probability, the $\\ell_2$ estimation\nerror of the solution sequence decays exponentially to the minimax error bound\nin $O(\\sqrt{J}\\log(R))$ steps; and under a mutual coherence condition and\ncertain other conditions, the $\\ell_{\\infty}$ estimation error decays to the\noptimal error bound in $O(\\log(R))$ steps, where $J$ is the number of important\npredictors, $R$ is the relative magnitude of the nonzero target coefficients.\nComputational complexity analysis shows that the cost of SDAR is $O(np)$ per\niteration. Moreover the oracle least squares estimator can be exactly recovered\nwith high probability at the same cost if we know the sparsity level. We also\nconsider an adaptive version of SDAR to make it more practical in applications.\nNumerical comparisons with Lasso, MCP and greedy methods demonstrate that SDAR\nis competitive with or outperforms them in accuracy and efficiency. \n\n"}
{"id": "1701.05512", "contents": "Title: Fisher consistency for prior probability shift Abstract: We introduce Fisher consistency in the sense of unbiasedness as a desirable\nproperty for estimators of class prior probabilities. Lack of Fisher\nconsistency could be used as a criterion to dismiss estimators that are\nunlikely to deliver precise estimates in test datasets under prior probability\nand more general dataset shift. The usefulness of this unbiasedness concept is\ndemonstrated with three examples of classifiers used for quantification:\nAdjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted\nClassify & Count and EM-algorithm are Fisher consistent. A counter-example\nshows that CDE-Iterate is not Fisher consistent and, therefore, cannot be\ntrusted to deliver reliable estimates of class probabilities. \n\n"}
{"id": "1701.05892", "contents": "Title: Bayesian Static Parameter Estimation for Partially Observed Diffusions\n  via Multilevel Monte Carlo Abstract: In this article we consider static Bayesian parameter estimation for\npartially observed diffusions that are discretely observed. We work under the\nassumption that one must resort to discretizing the underlying diffusion\nprocess, for instance using the Euler-Maruyama method. Given this assumption,\nwe show how one can use Markov chain Monte Carlo (MCMC) and particularly\nparticle MCMC [Andrieu, C., Doucet, A. and Holenstein, R. (2010). Particle\nMarkov chain Monte Carlo methods (with discussion). J. R. Statist. Soc. Ser. B,\n72, 269--342] to implement a new approximation of the multilevel (ML) Monte\nCarlo (MC) collapsing sum identity. Our approach comprises constructing an\napproximate coupling of the posterior density of the joint distribution over\nparameter and hidden variables at two different discretization levels and then\ncorrecting by an importance sampling method. The variance of the weights are\nindependent of the length of the observed data set. The utility of such a\nmethod is that, for a prescribed level of mean square error, the cost of this\nMLMC method is provably less than i.i.d. sampling from the posterior associated\nto the most precise discretization. However the method here comprises using\nonly known and efficient simulation methodologies. The theoretical results are\nillustrated by inference of the parameters of two prototypical processes given\nnoisy partial observations of the process: the first is an Ornstein Uhlenbeck\nprocess and the second is a more general Langevin equation. \n\n"}
{"id": "1701.07787", "contents": "Title: Multi-locus data distinguishes between population growth and multiple\n  merger coalescents Abstract: We introduce a low dimensional function of the site frequency spectrum that\nis tailor-made for distinguishing coalescent models with multiple mergers from\nKingman coalescent models with population growth, and use this function to\nconstruct a hypothesis test between these model classes. The null and\nalternative sampling distributions of the statistic are intractable, but its\nlow dimensionality renders them amenable to Monte Carlo estimation. We\nconstruct kernel density estimates of the sampling distributions based on\nsimulated data, and show that the resulting hypothesis test dramatically\nimproves on the statistical power of a current state-of-the-art method. A key\nreason for this improvement is the use of multi-locus data, in particular\naveraging observed site frequency spectra across unlinked loci to reduce\nsampling variance. We also demonstrate the robustness of our method to nuisance\nand tuning parameters. Finally we show that the same kernel density estimates\ncan be used to conduct parameter estimation, and argue that our method is\nreadily generalisable for applications in model selection, parameter inference\nand experimental design. \n\n"}
{"id": "1702.00204", "contents": "Title: Bayesian model selection for the latent position cluster model for\n  Social Networks Abstract: The latent position cluster model is a popular model for the statistical\nanalysis of network data. This model assumes that there is an underlying latent\nspace in which the actors follow a finite mixture distribution. Moreover,\nactors which are close in this latent space are more likely to be tied by an\nedge. This is an appealing approach since it allows the model to cluster actors\nwhich consequently provides the practitioner with useful qualitative\ninformation. However, exploring the uncertainty in the number of underlying\nlatent components in the mixture distribution is a complex task. The current\nstate-of-the-art is to use an approximate form of BIC for this purpose, where\nan approximation of the log-likelihood is used instead of the true\nlog-likelihood which is unavailable. The main contribution of this paper is to\nshow that through the use of conjugate prior distributions it is possible to\nanalytically integrate out almost all of the model parameters, leaving a\nposterior distribution which depends on the allocation vector of the mixture\nmodel. This enables posterior inference over the number of components in the\nlatent mixture distribution without using trans- dimensional MCMC algorithms\nsuch as reversible jump MCMC. Our approach is compared with the\nstate-of-the-art latentnet (Krivitsky & Handcock 2015) and VBLPCM\n(Salter-Townshend & Murphy 2013) packages. \n\n"}
{"id": "1702.00317", "contents": "Title: On SGD's Failure in Practice: Characterizing and Overcoming Stalling Abstract: Stochastic Gradient Descent (SGD) is widely used in machine learning problems\nto efficiently perform empirical risk minimization, yet, in practice, SGD is\nknown to stall before reaching the actual minimizer of the empirical risk. SGD\nstalling has often been attributed to its sensitivity to the conditioning of\nthe problem; however, as we demonstrate, SGD will stall even when applied to a\nsimple linear regression problem with unity condition number for standard\nlearning rates. Thus, in this work, we numerically demonstrate and\nmathematically argue that stalling is a crippling and generic limitation of SGD\nand its variants in practice. Once we have established the problem of stalling,\nwe generalize an existing framework for hedging against its effects, which (1)\ndeters SGD and its variants from stalling, (2) still provides convergence\nguarantees, and (3) makes SGD and its variants more practical methods for\nminimization. \n\n"}
{"id": "1702.00428", "contents": "Title: Malliavin-based Multilevel Monte Carlo Estimators for Densities of\n  Max-stable Processes Abstract: We introduce a class of unbiased Monte Carlo estimators for the multivariate\ndensity of max-stable fields generated by Gaussian processes. Our estimators\ntake advantage of recent results on exact simulation of max-stable fields\ncombined with identities studied in the Malliavin calculus literature and ideas\ndeveloped in the multilevel Monte Carlo literature. Our approach allows\nestimating multivariate densities of max-stable fields with precision\n$\\varepsilon $ at a computational cost of order $O\\left( \\varepsilon ^{-2}\\log\n\\log \\log \\left( 1/\\varepsilon \\right) \\right) $. \n\n"}
{"id": "1702.00434", "contents": "Title: Efficient algorithms for Bayesian Nearest Neighbor Gaussian Processes Abstract: We consider alternate formulations of recently proposed hierarchical Nearest\nNeighbor Gaussian Process (NNGP) models (Datta et al., 2016a) for improved\nconvergence, faster computing time, and more robust and reproducible Bayesian\ninference. Algorithms are defined that improve CPU memory management and\nexploit existing high-performance numerical linear algebra libraries.\nComputational and inferential benefits are assessed for alternate NNGP\nspecifications using simulated datasets and remotely sensed light detection and\nranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit\n(TIU) in a remote portion of Interior Alaska. The resulting data product is the\nfirst statistically robust map of forest canopy for the TIU. \n\n"}
{"id": "1702.01506", "contents": "Title: A data assimilation algorithm: the paradigm of the 3D Leray-alpha model\n  of turbulence Abstract: In this paper we survey the various implementations of a new data\nassimilation (downscaling) algorithm based on spatial coarse mesh measurements.\nAs a paradigm, we demonstrate the application of this algorithm to the 3D\nLeray-$\\alpha$ subgrid scale turbulence model. Most importantly, we use this\nparadigm to show that it is not always necessary that one has to collect coarse\nmesh measurements of all the state variables, that are involved in the\nunderlying evolutionary system, in order to recover the corresponding exact\nreference solution. Specifically, we show that in the case of the 3D\nLeray$-\\alpha$ model of turbulence the solutions of the algorithm, constructed\nusing only coarse mesh observations of any two components of the\nthree-dimensional velocity field, and without any information of the third\ncomponent, converge, at an exponential rate in time, to the corresponding exact\nreference solution of the 3D Leray$-\\alpha$ model. This study serves as an\naddendum to our recent work on abridged continuous data assimilation for the 2D\nNavier-Stokes equations. Notably, similar results have also been recently\nestablished for the 3D viscous Planetary Geostrophic circulation model in which\nwe show that coarse mesh measurements of the temperature alone are sufficient\nfor recovering, through our data assimilation algorithm, the full solution;\nviz. the three components of velocity vector field and the temperature.\nConsequently, this proves the Charney conjecture for the 3D Planetary\nGeostrophic model; namely, that the history of the large spatial scales of\ntemperature is sufficient for determining all the other quantities (state\nvariables) of the model. \n\n"}
{"id": "1702.02993", "contents": "Title: Short term unpredictability of high Reynolds number turbulence --- rough\n  dependence on initial data Abstract: Short term unpredictability is discovered numerically for high Reynolds\nnumber fluid flows under periodic boundary conditions. Furthermore, the\nabundance of the short term unpredictability is also discovered. These\ndiscoveries support our theory that fully developed turbulence is constantly\ndriven by such short term unpredictability. \n\n"}
{"id": "1702.03716", "contents": "Title: A New Interpretation of Vortex-Split Sudden Stratospheric Warmings in\n  Terms of Equilibrium Statistical Mechanics Abstract: Vortex-split sudden stratospheric warmings (S-SSWs) are investigated by using\nthe Japanese 55-year Reanalysis (JRA-55), a spherical barotropic\nquasi-geostrophic (QG) model, and equilibrium statistical mechanics. The QG\nmodel reproduces well the evolution of the composite potential vorticity (PV)\nfield obtained from JRA-55 by considering a time-dependent effective topography\ngiven by the composite height field of the 550 K potential temperature surface.\nThe zonal-wavenumber-2 component of the effective topography is the most\nessential feature required to observe the vortex splitting. The\nstatistical-mechanics theory predicts a large-scale steady state as the most\nprobable outcome of turbulent stirring, and such a state can be computed\nwithout solving the QG dynamics. The theory is applied to a disk domain, which\nis modeled on the north polar cap in the stratosphere. The equilibrium state is\nobtained by computing the maximum of an entropy functional. In the range of\nparameters relevant to the winter stratosphere, this state is anticyclonic. By\ncontrast, cyclonic states are quasi-stationary states corresponding to saddle\npoints of the entropy functional. The theoretical calculations are compared\nwith the results of the quasi-static experiment in which the wavenumber-2\ntopographic amplitude is increased linearly and slowly with time. The results\nsuggest that S-SSWs can be qualitatively interpreted as the transition from the\ncyclonic quasi-stationary state toward the anticyclonic equilibrium state. The\npolar vortex splits during the transition toward the equilibrium state. Without\nany forcing such as radiative cooling, the anticyclonic equilibrium state would\nbe realized sufficiently after an S-SSW. \n\n"}
{"id": "1702.05518", "contents": "Title: Sampling strategies for fast updating of Gaussian Markov random fields Abstract: Gaussian Markov random fields (GMRFs) are popular for modeling dependence in\nlarge areal datasets due to their ease of interpretation and computational\nconvenience afforded by the sparse precision matrices needed for random\nvariable generation. Typically in Bayesian computation, GMRFs are updated\njointly in a block Gibbs sampler or componentwise in a single-site sampler via\nthe full conditional distributions. The former approach can speed convergence\nby updating correlated variables all at once, while the latter avoids solving\nlarge matrices. We consider a sampling approach in which the underlying graph\ncan be cut so that conditionally independent sites are updated simultaneously.\nThis algorithm allows a practitioner to parallelize updates of subsets of\nlocations or to take advantage of `vectorized' calculations in a high-level\nlanguage such as R. Through both simulated and real data, we demonstrate\ncomputational savings that can be achieved versus both single-site and block\nupdating, regardless of whether the data are on a regular or an irregular\nlattice. The approach provides a good compromise between statistical and\ncomputational efficiency and is accessible to statisticians without expertise\nin numerical analysis or advanced computing. \n\n"}
{"id": "1702.05593", "contents": "Title: Uncovering the Edge of the Polar Vortex Abstract: The polar vortices play a crucial role in the formation of the ozone hole and\ncan cause severe weather anomalies. Their boundaries, known as the vortex\n`edges', are typically identified via methods that are either frame-dependent\nor return non-material structures, and hence are unsuitable for assessing\nmaterial transport barriers. Using two-dimensional velocity data on isentropic\nsurfaces in the northern hemisphere, we show that elliptic Lagrangian Coherent\nStructures (LCSs) identify the correct outermost material surface dividing the\ncoherent vortex core from the surrounding incoherent surf zone. Despite the\npurely kinematic construction of LCSs, we find a remarkable contrast in\ntemperature and ozone concentration across the identified vortex boundary. We\nalso show that potential vorticity-based methods, despite their simplicity,\nmisidentify the correct extent of the vortex edge. Finally, exploiting the\nshrinkage of the vortex at various isentropic levels, we observe a trend in the\nmagnitude of vertical motion inside the vortex which is consistent with\nprevious results. \n\n"}
{"id": "1702.05698", "contents": "Title: Online Robust Principal Component Analysis with Change Point Detection Abstract: Robust PCA methods are typically batch algorithms which requires loading all\nobservations into memory before processing. This makes them inefficient to\nprocess big data. In this paper, we develop an efficient online robust\nprincipal component methods, namely online moving window robust principal\ncomponent analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can\nsuccessfully track not only slowly changing subspace but also abruptly changed\nsubspace. By embedding hypothesis testing into the algorithm, OMWRPCA can\ndetect change points of the underlying subspaces. Extensive simulation studies\ndemonstrate the superior performance of OMWRPCA compared with other\nstate-of-art approaches. We also apply the algorithm for real-time background\nsubtraction of surveillance video. \n\n"}
{"id": "1702.07007", "contents": "Title: Detecting causal associations in large nonlinear time series datasets Abstract: Identifying causal relationships from observational time series data is a key\nproblem in disciplines such as climate science or neuroscience, where\nexperiments are often not possible. Data-driven causal inference is challenging\nsince datasets are often high-dimensional and nonlinear with limited sample\nsizes. Here we introduce a novel method that flexibly combines linear or\nnonlinear conditional independence tests with a causal discovery algorithm that\nallows to reconstruct causal networks from large-scale time series datasets. We\nvalidate the method on a well-established climatic teleconnection connecting\nthe tropical Pacific with extra-tropical temperatures and using large-scale\nsynthetic datasets mimicking the typical properties of real data. The\nexperiments demonstrate that our method outperforms alternative techniques in\ndetection power from small to large-scale datasets and opens up entirely new\npossibilities to discover causal networks from time series across a range of\nresearch fields. \n\n"}
{"id": "1702.07749", "contents": "Title: Well-balanced mesh-based and meshless schemes for the shallow-water\n  equations Abstract: We formulate a general criterion for the exact preservation of the \"lake at\nrest\" solution in general mesh-based and meshless numerical schemes for the\nstrong form of the shallow-water equations with bottom topography. The main\nidea is a careful mimetic design for the spatial derivative operators in the\nmomentum flux equation that is paired with a compatible averaging rule for the\nwater column height arising in the bottom topography source term. We prove\nconsistency of the mimetic difference operators analytically and demonstrate\nthe well-balanced property numerically using finite difference and RBF-FD\nschemes in the one- and two-dimensional cases. \n\n"}
{"id": "1702.07930", "contents": "Title: Upper-Bounding the Regularization Constant for Convex Sparse Signal\n  Reconstruction Abstract: Consider reconstructing a signal $x$ by minimizing a weighted sum of a convex\ndifferentiable negative log-likelihood (NLL) (data-fidelity) term and a convex\nregularization term that imposes a convex-set constraint on $x$ and enforces\nits sparsity using $\\ell_1$-norm analysis regularization. We compute upper\nbounds on the regularization tuning constant beyond which the regularization\nterm overwhelmingly dominates the NLL term so that the set of minimum points of\nthe objective function does not change. Necessary and sufficient conditions for\nirrelevance of sparse signal regularization and a condition for the existence\nof finite upper bounds are established. We formulate an optimization problem\nfor finding these bounds when the regularization term can be globally minimized\nby a feasible $x$ and also develop an alternating direction method of\nmultipliers (ADMM) type method for their computation. Simulation examples show\nthat the derived and empirical bounds match. \n\n"}
{"id": "1702.08397", "contents": "Title: Forward Event-Chain Monte Carlo: Fast sampling by randomness control in\n  irreversible Markov chains Abstract: Irreversible and rejection-free Monte Carlo methods, recently developed in\nPhysics under the name Event-Chain and known in Statistics as Piecewise\nDeterministic Monte Carlo (PDMC), have proven to produce clear acceleration\nover standard Monte Carlo methods, thanks to the reduction of their random-walk\nbehavior. However, while applying such schemes to standard statistical models,\none generally needs to introduce an additional randomization for sake of\ncorrectness. We propose here a new class of Event-Chain Monte Carlo methods\nthat reduces this extra-randomization to a bare minimum. We compare the\nefficiency of this new methodology to standard PDMC and Monte Carlo methods.\nAccelerations up to several magnitudes and reduced dimensional scalings are\nexhibited. \n\n"}
{"id": "1702.08738", "contents": "Title: Efficient simulation of high dimensional Gaussian vectors Abstract: We describe a Markov chain Monte Carlo method to approximately simulate a\ncentered d-dimensional Gaussian vector X with given covariance matrix. The\nstandard Monte Carlo method is based on the Cholesky decomposition, which takes\ncubic time and has quadratic storage cost in d. In contrast, the storage cost\nof our algorithm is linear in d. We give a bound on the quadractic Wasserstein\ndistance between the distribution of our sample and the target distribution.\nOur method can be used to estimate the expectation of h(X), where h is a\nreal-valued function of d variables. Under certain conditions, we show that the\nmean square error of our method is inversely proportional to its running time.\nWe also prove that, under suitable conditions, our method is faster than the\nstandard Monte Carlo method by a factor nearly proportional to d. A numerical\nexample is given. \n\n"}
{"id": "1702.08849", "contents": "Title: Multi-Sensor Multi-object Tracking with the Generalized Labeled\n  Multi-Bernoulli Filter Abstract: This paper proposes an efficient implementation of the multi-sensor\ngeneralized labeled multi-Bernoulli (GLMB) filter. The solution exploits the\nGLMB joint prediction and update together with a new technique for truncating\nthe GLMB filtering density based on Gibbs sampling. The resulting algorithm has\nquadratic complexity in the number of hypothesized object and linear in the\nnumber of measurements of each individual sensors. \n\n"}
{"id": "1703.00368", "contents": "Title: Approximate Computational Approaches for Bayesian Sensor Placement in\n  High Dimensions Abstract: Since the cost of installing and maintaining sensors is usually high, sensor\nlocations are always strategically selected. For those aiming at inferring\ncertain quantities of interest (QoI), it is desirable to explore the dependency\nbetween sensor measurements and QoI. One of the most popular metric for the\ndependency is mutual information which naturally measures how much information\nabout one variable can be obtained given the other. However, computing mutual\ninformation is always challenging, and the result is unreliable in high\ndimension. In this paper, we propose an approach to find an approximate lower\nbound of mutual information and compute it in a lower dimension. Then, sensors\nare placed where highest mutual information (lower bound) is achieved and QoI\nis inferred via Bayes rule given sensor measurements. In addition, Bayesian\noptimization is introduced to provide a continuous mutual information surface\nover the domain and thus reduce the number of evaluations. A chemical release\naccident is simulated where multiple sensors are placed to locate the source of\nthe release. The result shows that the proposed approach is both effective and\nefficient in inferring QoI. \n\n"}
{"id": "1703.02998", "contents": "Title: A note on quickly sampling a sparse matrix with low rank expectation Abstract: Given matrices $X,Y \\in R^{n \\times K}$ and $S \\in R^{K \\times K}$ with\npositive elements, this paper proposes an algorithm fastRG to sample a sparse\nmatrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson\nelements. This allows for quickly sampling from a broad class of stochastic\nblockmodel graphs (degree-corrected, mixed membership, overlapping) all of\nwhich are specific parameterizations of the generalized random product graph\nmodel defined in Section 2.2. The basic idea of fastRG is to first sample the\nnumber of edges $m$ and then sample each edge. The key insight is that because\nof the the low rank expectation, it is easy to sample individual edges. The\nnaive \"element-wise\" algorithm requires $O(n^2)$ operations to generate the\n$n\\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring\nlog terms, fastRG runs in time $O(n)$. An implementation in fastRG is available\non github. A computational experiment in Section 2.4 simulates graphs up to\n$n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with\n$n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5\nGHz Intel i5. \n\n"}
{"id": "1703.03680", "contents": "Title: Strong convergence rates of probabilistic integrators for ordinary\n  differential equations Abstract: Probabilistic integration of a continuous dynamical system is a way of\nsystematically introducing model error, at scales no larger than errors\nintroduced by standard numerical discretisation, in order to enable thorough\nexploration of possible responses of the system to inputs. It is thus a\npotentially useful approach in a number of applications such as forward\nuncertainty quantification, inverse problems, and data assimilation. We extend\nthe convergence analysis of probabilistic integrators for deterministic\nordinary differential equations, as proposed by Conrad et al.\\ (\\textit{Stat.\\\nComput.}, 2017), to establish mean-square convergence in the uniform norm on\ndiscrete- or continuous-time solutions under relaxed regularity assumptions on\nthe driving vector fields and their induced flows. Specifically, we show that\nrandomised high-order integrators for globally Lipschitz flows and randomised\nEuler integrators for dissipative vector fields with polynomially-bounded local\nLipschitz constants all have the same mean-square convergence rate as their\ndeterministic counterparts, provided that the variance of the integration noise\nis not of higher order than the corresponding deterministic integrator. These\nand similar results are proven for probabilistic integrators where the random\nperturbations may be state-dependent, non-Gaussian, or non-centred random\nvariables. \n\n"}
{"id": "1703.04334", "contents": "Title: Probabilistic Matching: Causal Inference under Measurement Errors Abstract: The abundance of data produced daily from large variety of sources has\nboosted the need of novel approaches on causal inference analysis from\nobservational data. Observational data often contain noisy or missing entries.\nMoreover, causal inference studies may require unobserved high-level\ninformation which needs to be inferred from other observed attributes. In such\ncases, inaccuracies of the applied inference methods will result in noisy\noutputs. In this study, we propose a novel approach for causal inference when\none or more key variables are noisy. Our method utilizes the knowledge about\nthe uncertainty of the real values of key variables in order to reduce the bias\ninduced by noisy measurements. We evaluate our approach in comparison with\nexisting methods both on simulated and real scenarios and we demonstrate that\nour method reduces the bias and avoids false causal inference conclusions in\nmost cases. \n\n"}
{"id": "1703.05060", "contents": "Title: Online Learning for Distribution-Free Prediction Abstract: We develop an online learning method for prediction, which is important in\nproblems with large and/or streaming data sets. We formulate the learning\napproach using a covariance-fitting methodology, and show that the resulting\npredictor has desirable computational and distribution-free properties: It is\nimplemented online with a runtime that scales linearly in the number of\nsamples; has a constant memory requirement; avoids local minima problems; and\nprunes away redundant feature dimensions without relying on restrictive\nassumptions on the data distribution. In conjunction with the split conformal\napproach, it also produces distribution-free prediction confidence intervals in\na computationally efficient manner. The method is demonstrated on both real and\nsynthetic datasets. \n\n"}
{"id": "1703.05144", "contents": "Title: Bergm: Bayesian exponential random graph models in R Abstract: The Bergm package provides a comprehensive framework for Bayesian inference\nusing Markov chain Monte Carlo (MCMC) algorithms. It can also supply graphical\nBayesian goodness-of-fit procedures that address the issue of model adequacy.\nThe package is simple to use and represents an attractive way of analysing\nnetwork data as it offers the advantage of a complete probabilistic treatment\nof uncertainty. Bergm is based on the ergm package and therefore it makes use\nof the same model set-up and network simulation algorithms. The Bergm package\nhas been continually improved in terms of speed performance over the last years\nand now offers the end-user a feasible option for carrying out Bayesian\ninference for networks with several thousands of nodes. \n\n"}
{"id": "1703.05511", "contents": "Title: An Induced Natural Selection Heuristic for Finding Optimal Bayesian\n  Experimental Designs Abstract: Bayesian optimal experimental design has immense potential to inform the\ncollection of data so as to subsequently enhance our understanding of a variety\nof processes. However, a major impediment is the difficulty in evaluating\noptimal designs for problems with large, or high-dimensional, design spaces. We\npropose an efficient search heuristic suitable for general optimisation\nproblems, with a particular focus on optimal Bayesian experimental design\nproblems. The heuristic evaluates the objective (utility) function at an\ninitial, randomly generated set of input values. At each generation of the\nalgorithm, input values are \"accepted\" if their corresponding objective\n(utility) function satisfies some acceptance criteria, and new inputs are\nsampled about these accepted points. We demonstrate the new algorithm by\nevaluating the optimal Bayesian experimental designs for the previously\nconsidered death, pharmacokinetic and logistic regression models. Comparisons\nto the current \"gold-standard\" method are given to demonstrate the proposed\nalgorithm as a computationally-efficient alternative for moderately-large\ndesign problems (i.e., up to approximately 40-dimensions). \n\n"}
{"id": "1703.05984", "contents": "Title: A Tutorial on Bridge Sampling Abstract: The marginal likelihood plays an important role in many areas of Bayesian\nstatistics such as parameter estimation, model comparison, and model averaging.\nIn most applications, however, the marginal likelihood is not analytically\ntractable and must be approximated using numerical methods. Here we provide a\ntutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and\nrelatively straightforward sampling method that allows researchers to obtain\nthe marginal likelihood for models of varying complexity. First, we introduce\nbridge sampling and three related sampling methods using the beta-binomial\nmodel as a running example. We then apply bridge sampling to estimate the\nmarginal likelihood for the Expectancy Valence (EV) model---a popular model for\nreinforcement learning. Our results indicate that bridge sampling provides\naccurate estimates for both a single participant and a hierarchical version of\nthe EV model. We conclude that bridge sampling is an attractive method for\nmathematical psychologists who typically aim to approximate the marginal\nlikelihood for a limited set of possibly high-dimensional models. \n\n"}
{"id": "1703.06098", "contents": "Title: Multilevel linear models, Gibbs samplers and multigrid decompositions Abstract: We study the convergence properties of the Gibbs Sampler in the context of\nposterior distributions arising from Bayesian analysis of conditionally\nGaussian hierarchical models. We develop a multigrid approach to derive\nanalytic expressions for the convergence rates of the algorithm for various\nwidely used model structures, including nested and crossed random effects. Our\nresults apply to multilevel models with an arbitrary number of layers in the\nhierarchy, while most previous work was limited to the two-level nested case.\nThe theoretical results provide explicit and easy-to-implement guidelines to\noptimize practical implementations of the Gibbs Sampler, such as indications on\nwhich parametrization to choose (e.g. centred and non-centred), which\nconstraint to impose to guarantee statistical identifiability, and which\nparameters to monitor in the diagnostic process. Simulations suggest that the\nresults are informative also in the context of non-Gaussian distributions and\nmore general MCMC schemes, such as gradient-based ones.implementation of Gibbs\nsamplers on conditionally Gaussian hierarchical models. \n\n"}
{"id": "1703.06419", "contents": "Title: Multivariate Functional Data Visualization and Outlier Detection Abstract: This article proposes a new graphical tool, the magnitude-shape (MS) plot,\nfor visualizing both the magnitude and shape outlyingness of multivariate\nfunctional data. The proposed tool builds on the recent notion of functional\ndirectional outlyingness, which measures the centrality of functional data by\nsimultaneously considering the level and the direction of their deviation from\nthe central region. The MS-plot intuitively presents not only levels but also\ndirections of magnitude outlyingness on the horizontal axis or plane, and\ndemonstrates shape outlyingness on the vertical axis. A dividing curve or\nsurface is provided to separate non-outlying data from the outliers. Both the\nsimulated data and the practical examples confirm that the MS-plot is superior\nto existing tools for visualizing centrality and detecting outliers for\nfunctional data. \n\n"}
{"id": "1703.08504", "contents": "Title: Shingle 2.0: generalising self-consistent and automated domain\n  discretisation for multi-scale geophysical models Abstract: The approaches taken to describe and develop spatial discretisations of the\ndomains required for geophysical simulation models are commonly ad hoc, model\nor application specific and under-documented. This is particularly acute for\nsimulation models that are flexible in their use of multi-scale, anisotropic,\nfully unstructured meshes where a relatively large number of heterogeneous\nparameters are required to constrain their full description. As a consequence,\nit can be difficult to reproduce simulations, ensure a provenance in model data\nhandling and initialisation, and a challenge to conduct model intercomparisons\nrigorously. This paper takes a novel approach to spatial discretisation,\nconsidering it much like a numerical simulation model problem of its own. It\nintroduces a generalised, extensible, self-documenting approach to carefully\ndescribe, and necessarily fully, the constraints over the heterogeneous\nparameter space that determine how a domain is spatially discretised. This\nadditionally provides a method to accurately record these constraints, using\nhigh-level natural language based abstractions, that enables full accounts of\nprovenance, sharing and distribution. Together with this description, a\ngeneralised consistent approach to unstructured mesh generation for geophysical\nmodels is developed, that is automated, robust and repeatable, quick-to-draft,\nrigorously verified and consistent to the source data throughout. This\ninterprets the description above to execute a self-consistent spatial\ndiscretisation process, which is automatically validated to expected discrete\ncharacteristics and metrics. \n\n"}
{"id": "1703.08520", "contents": "Title: Augmented Ensemble MCMC sampling in Factorial Hidden Markov Models Abstract: Bayesian inference for factorial hidden Markov models is challenging due to\nthe exponentially sized latent variable space. Standard Monte Carlo samplers\ncan have difficulties effectively exploring the posterior landscape and are\noften restricted to exploration around localised regions that depend on\ninitialisation. We introduce a general purpose ensemble Markov Chain Monte\nCarlo (MCMC) technique to improve on existing poorly mixing samplers. This is\nachieved by combining parallel tempering and an auxiliary variable scheme to\nexchange information between the chains in an efficient way. The latter\nexploits a genetic algorithm within an augmented Gibbs sampler. We compare our\ntechnique with various existing samplers in a simulation study as well as in a\ncancer genomics application, demonstrating the improvements obtained by our\naugmented ensemble approach. \n\n"}
{"id": "1703.09281", "contents": "Title: Global surface temperature trends and the effect of World War II Abstract: Using parametric analysis (curve fitting) we find a persistent temperature\nbump, coincident with World War II (WW2), in eight independent time series,\nfour land- and four ocean-based. We fit the data with a Gaussian on a quadratic\nbackground. Six parameters (constant, linear and quadratic background terms and\nthe amplitude, position and width of the Gaussian) are free to vary. The mean\nfitted Gaussian amplitude is 0.339$\\pm$ 0.065$\\,^\\circ$C, non-zero by\n5.2$\\sigma$ and therefore not accidental. The area is 2.0$\\pm$0.5$\\,^\\circ$C\nyr. Temperature recovered to baseline rather quickly. Rather than coincidence,\nor systematic measuring error synchronized with WW2, we conjecture the bump is\ndue to human activity, including the greatly increased combustion (relative to\nthat era) of fossil and other fuels. Background surface temperature behavior, a\nbyproduct of our study but largely independent of the WW2 bump, is far more\nconsequential nowadays. The linear term, 0.747$\\pm$0.023$\\,^\\circ$C/century,\nagrees well with other findings, but the present-day rate of increase,\n$\\approx$2.5$\\,^\\circ$C/century, is far greater because of the quadratic term. \n\n"}
{"id": "1703.09301", "contents": "Title: Large-scale estimation of random graph models with local dependence Abstract: A class of random graph models is considered, combining features of\nexponential-family models and latent structure models, with the goal of\nretaining the strengths of both of them while reducing the weaknesses of each\nof them. An open problem is how to estimate such models from large networks. A\nnovel approach to large-scale estimation is proposed, taking advantage of the\nlocal structure of such models for the purpose of local computing. The main\nidea is that random graphs with local dependence can be decomposed into\nsubgraphs, which enables parallel computing on subgraphs and suggests a\ntwo-step estimation approach. The first step estimates the local structure\nunderlying random graphs. The second step estimates parameters given the\nestimated local structure of random graphs. Both steps can be implemented in\nparallel, which enables large-scale estimation. The advantages of the two-step\nestimation approach are demonstrated by simulation studies with up to 10,000\nnodes and an application to a large Amazon product recommendation network with\nmore than 10,000 products. \n\n"}
{"id": "1703.09382", "contents": "Title: Exact computation of GMM estimators for instrumental variable quantile\n  regression models Abstract: We show that the generalized method of moments (GMM) estimation problem in\ninstrumental variable quantile regression (IVQR) models can be equivalently\nformulated as a mixed integer quadratic programming problem. This enables exact\ncomputation of the GMM estimators for the IVQR models. We illustrate the\nusefulness of our algorithm via Monte Carlo experiments and an application to\ndemand for fish. \n\n"}
{"id": "1703.10245", "contents": "Title: Bayesian Effect Fusion for Categorical Predictors Abstract: In this paper, we propose a Bayesian approach to obtain a sparse\nrepresentation of the effect of a categorical predictor in regression type\nmodels. As the effect of a categorical predictor is captured by a group of\nlevel effects, sparsity cannot only be achieved by excluding single irrelevant\nlevel effects but also by excluding the whole group of effects associated to a\npredictor or by fusing levels which have essentially the same effect on the\nresponse. To achieve this goal, we propose a prior which allows for almost\nperfect as well as almost zero dependence between level effects a priori. We\nshow how this prior can be obtained by specifying spike and slab prior\ndistributions on all effect differences associated to one categorical predictor\nand how restricted fusion can be implemented. An efficient MCMC method for\nposterior computation is developed. The performance of the proposed method is\ninvestigated on simulated data. Finally, we illustrate its application on real\ndata from EU-SILC. \n\n"}
{"id": "1704.00520", "contents": "Title: Efficient acquisition rules for model-based approximate Bayesian\n  computation Abstract: Approximate Bayesian computation (ABC) is a method for Bayesian inference\nwhen the likelihood is unavailable but simulating from the model is possible.\nHowever, many ABC algorithms require a large number of simulations, which can\nbe costly. To reduce the computational cost, Bayesian optimisation (BO) and\nsurrogate models such as Gaussian processes have been proposed. Bayesian\noptimisation enables one to intelligently decide where to evaluate the model\nnext but common BO strategies are not designed for the goal of estimating the\nposterior distribution. Our paper addresses this gap in the literature. We\npropose to compute the uncertainty in the ABC posterior density, which is due\nto a lack of simulations to estimate this quantity accurately, and define a\nloss function that measures this uncertainty. We then propose to select the\nnext evaluation location to minimise the expected loss. Experiments show that\nthe proposed method often produces the most accurate approximations as compared\nto common BO strategies. \n\n"}
{"id": "1704.00963", "contents": "Title: Correcting boundary over-exploration deficiencies in Bayesian\n  optimization with virtual derivative sign observations Abstract: Bayesian optimization (BO) is a global optimization strategy designed to find\nthe minimum of an expensive black-box function, typically defined on a compact\nsubset of $\\mathcal{R}^d$, by using a Gaussian process (GP) as a surrogate\nmodel for the objective. Although currently available acquisition functions\naddress this goal with different degree of success, an over-exploration effect\nof the contour of the search space is typically observed. However, in problems\nlike the configuration of machine learning algorithms, the function domain is\nconservatively large and with a high probability the global minimum does not\nsit on the boundary of the domain. We propose a method to incorporate this\nknowledge into the search process by adding virtual derivative observations in\nthe \\gp at the boundary of the search space. We use the properties of GPs to\nimpose conditions on the partial derivatives of the objective. The method is\napplicable with any acquisition function, it is easy to use and consistently\nreduces the number of evaluations required to optimize the objective\nirrespective of the acquisition used. We illustrate the benefits of our\napproach in an extensive experimental comparison. \n\n"}
{"id": "1704.01445", "contents": "Title: Bayesian Inference of Log Determinants Abstract: The log-determinant of a kernel matrix appears in a variety of machine\nlearning problems, ranging from determinantal point processes and generalized\nMarkov random fields, through to the training of Gaussian processes. Exact\ncalculation of this term is often intractable when the size of the kernel\nmatrix exceeds a few thousand. In the spirit of probabilistic numerics, we\nreinterpret the problem of computing the log-determinant as a Bayesian\ninference problem. In particular, we combine prior knowledge in the form of\nbounds from matrix theory and evidence derived from stochastic trace estimation\nto obtain probabilistic estimates for the log-determinant and its associated\nuncertainty within a given computational budget. Beyond its novelty and\ntheoretic appeal, the performance of our proposal is competitive with\nstate-of-the-art approaches to approximating the log-determinant, while also\nquantifying the uncertainty due to budget-constrained evidence. \n\n"}
{"id": "1704.02719", "contents": "Title: Mechanism of mean flow generation in rotating turbulence through\n  inhomogeneous helicity Abstract: Recent numerical simulations showed that the mean flow is generated in\ninhomogeneous turbulence of an incompressible fluid accompanied with helicity\nand system rotation. In order to investigate the mechanism of this phenomenon,\nwe carry out a numerical simulation of inhomogeneous turbulence in a rotating\nsystem. In the simulation, an external force is applied to inject inhomogeneous\nturbulent helicity and the rotation axis is taken to be perpendicular to the\ninhomogeneous direction. No mean velocity is set in the initial condition of\nthe simulation. The simulation results show that only in the case with both the\nhelical forcing and the system rotation, the mean flow directed to the rotation\naxis is generated and sustained. We investigate the physical origin of this\nflow-generation phenomenon by considering the budget of the Reynolds-stress\ntransport equation. It is found that the pressure diffusion term has a large\ncontribution in the Reynolds stress equation and supports the generated mean\nflow. It is shown that a model expression for the pressure diffusion can be\nexpressed by the turbulent helicity gradient coupled with the angular velocity\nof the system rotation. This implies that inhomogeneous helicity can play a\nsignificant role for the generation of the large-scale velocity distribution in\nincompressible turbulent flows. \n\n"}
{"id": "1704.02791", "contents": "Title: Efficient SMC$^2$ schemes for stochastic kinetic models Abstract: Fitting stochastic kinetic models represented by Markov jump processes within\nthe Bayesian paradigm is complicated by the intractability of the observed data\nlikelihood. There has therefore been considerable attention given to the design\nof pseudo-marginal Markov chain Monte Carlo algorithms for such models.\nHowever, these methods are typically computationally intensive, often require\ncareful tuning and must be restarted from scratch upon receipt of new\nobservations. Sequential Monte Carlo (SMC) methods on the other hand aim to\nefficiently reuse posterior samples at each time point. Despite their appeal,\napplying SMC schemes in scenarios with both dynamic states and static\nparameters is made difficult by the problem of particle degeneracy. A\nprincipled approach for overcoming this problem is to move each parameter\nparticle through a Metropolis-Hastings kernel that leaves the target invariant.\nThis rejuvenation step is key to a recently proposed SMC$^2$ algorithm, which\ncan be seen as the pseudo-marginal analogue of an idealised scheme known as\niterated batch importance sampling. Computing the parameter weights in SMC$^2$\nrequires running a particle filter over dynamic states to unbiasedly estimate\nthe intractable observed data likelihood contributions at each time point. In\nthis paper, we propose to use an auxiliary particle filter inside the SMC$^2$\nscheme. Our method uses two recently proposed constructs for sampling\nconditioned jump processes and we find that the resulting inference schemes\ntypically require fewer state particles than when using a simple bootstrap\nfilter. Using two applications, we compare the performance of the proposed\napproach with various competing methods, including two global MCMC schemes. \n\n"}
{"id": "1704.03581", "contents": "Title: P\\'olya Urn Latent Dirichlet Allocation: a doubly sparse massively\n  parallel sampler Abstract: Latent Dirichlet Allocation (LDA) is a topic model widely used in natural\nlanguage processing and machine learning. Most approaches to training the model\nrely on iterative algorithms, which makes it difficult to run LDA on big\ncorpora that are best analyzed in parallel and distributed computational\nenvironments. Indeed, current approaches to parallel inference either don't\nconverge to the correct posterior or require storage of large dense matrices in\nmemory. We present a novel sampler that overcomes both problems, and we show\nthat this sampler is faster, both empirically and theoretically, than previous\nGibbs samplers for LDA. We do so by employing a novel P\\'olya-urn-based\napproximation in the sparse partially collapsed sampler for LDA. We prove that\nthe approximation error vanishes with data size, making our algorithm\nasymptotically exact, a property of importance for large-scale topic models. In\naddition, we show, via an explicit example, that - contrary to popular belief\nin the topic modeling literature - partially collapsed samplers can be more\nefficient than fully collapsed samplers. We conclude by comparing the\nperformance of our algorithm with that of other approaches on well-known\ncorpora. \n\n"}
{"id": "1704.04629", "contents": "Title: Metropolis Sampling Abstract: Monte Carlo (MC) sampling methods are widely applied in Bayesian inference,\nsystem simulation and optimization problems. The Markov Chain Monte Carlo\n(MCMC) algorithms are a well-known class of MC methods which generate a Markov\nchain with the desired invariant distribution. In this document, we focus on\nthe Metropolis-Hastings (MH) sampler, which can be considered as the atom of\nthe MCMC techniques, introducing the basic notions and different properties. We\ndescribe in details all the elements involved in the MH algorithm and the most\nrelevant variants. Several improvements and recent extensions proposed in the\nliterature are also briefly discussed, providing a quick but exhaustive\noverview of the current Metropolis-based sampling's world. \n\n"}
{"id": "1704.06017", "contents": "Title: PAFit: an R Package for the Non-Parametric Estimation of Preferential\n  Attachment and Node Fitness in Temporal Complex Networks Abstract: Many real-world systems are profitably described as complex networks that\ngrow over time. Preferential attachment and node fitness are two simple growth\nmechanisms that not only explain certain structural properties commonly\nobserved in real-world systems, but are also tied to a number of applications\nin modeling and inference. While there are statistical packages for estimating\nvarious parametric forms of the preferential attachment function, there is no\nsuch package implementing non-parametric estimation procedures. The\nnon-parametric approach to the estimation of the preferential attachment\nfunction allows for comparatively finer-grained investigations of the\n`rich-get-richer' phenomenon that could lead to novel insights in the search to\nexplain certain nonstandard structural properties observed in real-world\nnetworks. This paper introduces the R package PAFit, which implements\nnon-parametric procedures for estimating the preferential attachment function\nand node fitnesses in a growing network, as well as a number of functions for\ngenerating complex networks from these two mechanisms. The main computational\npart of the package is implemented in C++ with OpenMP to ensure scalability to\nlarge-scale networks. We first introduce the main functionalities of PAFit\nthrough simulated examples, and then use the package to analyze a collaboration\nnetwork between scientists in the field of complex networks. The results\nindicate the joint presence of `rich-get-richer' and `fit-get-richer' phenomena\nin the collaboration network. The estimated attachment function is observed to\nbe near-linear, which we interpret as meaning that the chance an author gets a\nnew collaborator is proportional to their current number of collaborators.\nFurthermore, the estimated author fitnesses reveal a host of familiar faces\nfrom the complex networks community among the field's topmost fittest network\nscientists. \n\n"}
{"id": "1704.06098", "contents": "Title: A Third-Law Isentropic Analysis of a Simulated Hurricane Abstract: The moist-air entropy can be used to analyze and better understand the\ngeneral circulation of the atmosphere or convective motions. Isentropic\nanalyses are commonly based on studies of different equivalent potential\ntemperatures, all of which are assumed to fully represent the entropy of moist\nair. It is, however, possible to rely either on statistical physics or the\nthird law of thermodynamics when defining and computing the absolute entropy of\nmoist air and to study the corresponding third-law potential temperature, which\nis different from the previous ones. The third law assumes that the entropy for\nthe most stable crystalline state of all substances is zero when approaching\nabsolute zero temperature.\n  This paper shows that the way all these moist-air potential temperatures are\ndefined has a large impact on: (i) the plotting of the isentropes for a\nsimulation of the Hurricane DUMILE; (ii) the changes in moist-air entropy\ncomputed for a steam cycle defined for this Hurricane; (iii) the analyses of\nisentropic stream functions computed for this Hurricane; and (iv) the\ncomputations of the heat input, the work function, and the efficiency defined\nfor this steam cycle.\n  The moist-air entropy is a state function and the isentropic analyses must be\ncompletely determined by the local moist-air conditions. The large differences\nobserved between the different formulations of moist-air entropy are\ninterpreted as proof that the isentropic analyses of moist-air atmospheric\nmotions must be achieved by using the third-law potential temperature defined\nfrom general thermodynamics. \n\n"}
{"id": "1704.06186", "contents": "Title: Enduring Lagrangian coherence of a Loop Current ring assessed using\n  independent observations Abstract: Ocean flows are routinely inferred from low-resolution satellite altimetry\nmeasurements of sea surface height assuming a geostrophic balance. Recent\nnonlinear dynamical systems techniques have revealed that surface currents\nderived from altimetry can support mesoscale eddies with material boundaries\nthat do not filament for many months, thereby representing effective transport\nmechanisms. However, the long-range Lagrangian coherence assessed for mesoscale\neddy boundaries detected from altimetry is constrained by the impossibility of\ncurrent altimeters to resolve ageostrophic submesoscale motions. These may act\nto prevent Lagrangian coherence from manifesting in the rigorous form described\nby the nonlinear dynamical systems theories. Here we use a combination of\nsatellite ocean color and surface drifter trajectory data, rarely available\nsimultaneously over an extended period of time, to provide observational\nevidence for the enduring Lagrangian coherence of a Loop Current ring detected\nfrom altimetry. We also seek indications of this behavior in the flow produced\nby a data-assimilative system which demonstrated ability to reproduce observed\nrelative dispersion statistics down into the marginally submesoscale range.\nHowever, the simulated flow, total surface and subsurface or subsampled\nemulating altimetry, is not found to support the long-lasting Lagrangian\ncoherence that characterizes the observed ring. This highlights the importance\nof the Lagrangian metrics produced by the nonlinear dynamical systems tools\nemployed here in assessing model performance. \n\n"}
{"id": "1704.07949", "contents": "Title: Reconditioning your quantile function Abstract: Monte Carlo simulation is an important tool for modeling highly nonlinear\nsystems (like particle colliders and cellular membranes), and random,\nfloating-point numbers are their fuel. These random samples are frequently\ngenerated via the inversion method, which harnesses the mapping of the quantile\nfunction Q(u) (e.g. to generate proposal variates for rejection sampling). Yet\nthe increasingly large sample size of these simulations makes them vulnerable\nto a flaw in the inversion method; Q(u) is ill-conditioned in a distribution's\ntails, stripping precision from its sample. This flaw stems from limitations in\nmachine arithmetic which are often overlooked during implementation (e.g. in\npopular C++ and Python libraries). This paper introduces a robust inversion\nmethod, which reconditions Q(u) by carefully drawing and using uniform\nvariates. pqRand, a free C++ and Python package, implements this novel method\nfor a number of popular distributions (exponential, normal, gamma, and more). \n\n"}
{"id": "1704.08742", "contents": "Title: Hybrid safe-strong rules for efficient optimization in lasso-type\n  problems Abstract: The lasso model has been widely used for model selection in data mining,\nmachine learning, and high-dimensional statistical analysis. However, with the\nultrahigh-dimensional, large-scale data sets now collected in many real-world\napplications, it is important to develop algorithms to solve the lasso that\nefficiently scale up to problems of this size. Discarding features from certain\nsteps of the algorithm is a powerful technique for increasing efficiency and\naddressing the Big Data challenge. In this paper, we propose a family of hybrid\nsafe-strong rules (HSSR) which incorporate safe screening rules into the\nsequential strong rule (SSR) to remove unnecessary computational burden. In\nparticular, we present two instances of HSSR, namely SSR-Dome and SSR-BEDPP,\nfor the standard lasso problem. We further extend SSR-BEDPP to the elastic net\nand group lasso problems to demonstrate the generalizability of the hybrid\nscreening idea. Extensive numerical experiments with synthetic and real data\nsets are conducted for both the standard lasso and the group lasso problems.\nResults show that our proposed hybrid rules can substantially outperform\nexisting state-of-the-art rules. \n\n"}
{"id": "1705.00841", "contents": "Title: Bayes Shrinkage at GWAS scale: Convergence and Approximation Theory of a\n  Scalable MCMC Algorithm for the Horseshoe Prior Abstract: The horseshoe prior is frequently employed in Bayesian analysis of\nhigh-dimensional models, and has been shown to achieve minimax optimal risk\nproperties when the truth is sparse. While optimization-based algorithms for\nthe extremely popular Lasso and elastic net procedures can scale to dimension\nin the hundreds of thousands, algorithms for the horseshoe that use Markov\nchain Monte Carlo (MCMC) for computation are limited to problems an order of\nmagnitude smaller. This is due to high computational cost per step and growth\nof the variance of time-averaging estimators as a function of dimension. We\npropose two new MCMC algorithms for computation in these models that have\nimproved performance compared to existing alternatives. One of the algorithms\nalso approximates an expensive matrix product to give orders of magnitude\nspeedup in high-dimensional applications. We prove that the exact algorithm is\ngeometrically ergodic, and give guarantees for the accuracy of the approximate\nalgorithm using perturbation theory. Versions of the approximation algorithm\nthat gradually decrease the approximation error as the chain extends are shown\nto be exact. The scalability of the algorithm is illustrated in simulations\nwith problem size as large as $N=5,000$ observations and $p=50,000$ predictors,\nand an application to a genome-wide association study with $N=2,267$ and\n$p=98,385$. The empirical results also show that the new algorithm yields\nestimates with lower mean squared error, intervals with better coverage, and\nelucidates features of the posterior that were often missed by previous\nalgorithms in high dimensions, including bimodality of posterior marginals\nindicating uncertainty about which covariates belong in the model. \n\n"}
{"id": "1705.00870", "contents": "Title: On consequences of measurements of turbulent Lewis number from\n  observations Abstract: Almost all parameterizations of turbulence in NWP models and GCM make the\nassumption of equality of exchange coefficients for heat $K_h$ and water $K_w$.\nHowever, large uncertainties exists in old papers published in the 1950s, 1960s\nand 1970s, where the turbulent Lewis number Le_t $= K_h / K_w$ have been\nevaluated from observations and then set to Le_t$=1$.\n  The aim of this note is: 1) to trust the recommendations of Richardson\n(1919), who suggested to use the moist-air entropy as a variable on which the\nturbulence is acting; 2) to compute a new exchange coefficients $K_s$ for the\nmoist-air entropy; 3) to determine the values of the new entropy-Lewis number\nLe_ts $= K_s / K_w$ from observations (M\\'et\\'eopole-Flux and Cabauw masts) and\nfrom LES and SCM outputs for the IHOP case (Couvreux et al., 2005).\n  It is shown that values of Le_ts significantly different from $1$ are\nfrequently observed and may have large consequences on the way the turbulence\nfluxes are computed in NWP models and GCMs. \n\n"}
{"id": "1705.01024", "contents": "Title: A projection pursuit framework for testing general high-dimensional\n  hypothesis Abstract: This article develops a framework for testing general hypothesis in\nhigh-dimensional models where the number of variables may far exceed the number\nof observations. Existing literature has considered less than a handful of\nhypotheses, such as testing individual coordinates of the model parameter.\nHowever, the problem of testing general and complex hypotheses remains widely\nopen. We propose a new inference method developed around the hypothesis\nadaptive projection pursuit framework, which solves the testing problems in the\nmost general case. The proposed inference is centered around a new class of\nestimators defined as $l_1$ projection of the initial guess of the unknown onto\nthe space defined by the null. This projection automatically takes into account\nthe structure of the null hypothesis and allows us to study formal inference\nfor a number of long-standing problems. For example, we can directly conduct\ninference on the sparsity level of the model parameters and the minimum signal\nstrength. This is especially significant given the fact that the former is a\nfundamental condition underlying most of the theoretical development in\nhigh-dimensional statistics, while the latter is a key condition used to\nestablish variable selection properties. Moreover, the proposed method is\nasymptotically exact and has satisfactory power properties for testing very\ngeneral functionals of the high-dimensional parameters. The simulation studies\nlend further support to our theoretical claims and additionally show excellent\nfinite-sample size and power properties of the proposed test. \n\n"}
{"id": "1705.02013", "contents": "Title: FEniCS Application to a Finite Element Quasi-Geostrophic Model. Linear\n  and non-linear analysis of Munk-like Solutions and Data Assimilation\n  Implementation Through Adjoint Method Abstract: The first aim of this work is to construct a Finite Elements model for\nquasi-geostrophic equation. This model is analyzed through FEniCS interface. As\na second purpose, it shows the potential of the control theory to treat Data\nAssimilation large scale problems. In section one the finite element\napproximation of the quasi-geostrophic model is described. In section two some\nnumerical results on different domains are shown. In section three a comparison\nbetween Navier Stokes model and quasi-geostrophic one is discussed. Last\nsection is dedicated to a more complex data assimilation/control problem based\non quasi-geostrophic equations, to test possible developments and improvements. \n\n"}
{"id": "1705.03196", "contents": "Title: Accurate Computation of the Distribution of Sums of Dependent\n  Log-Normals with Applications to the Black-Scholes Model Abstract: We present a new Monte Carlo methodology for the accurate estimation of the\ndistribution of the sum of dependent log-normal random variables. The\nmethodology delivers statistically unbiased estimators for three distributional\nquantities of significant interest in finance and risk management: the left\ntail, or cumulative distribution function, the probability density function,\nand the right tail, or complementary distribution function of the sum of\ndependent log-normal factors. In all of these three cases our methodology\ndelivers fast and highly accurate estimators in settings for which existing\nmethodology delivers estimators with large variance that tend to underestimate\nthe true quantity of interest. We provide insight into the computational\nchallenges using theory and numerical experiments, and explain their much wider\nimplications for Monte Carlo statistical estimators of rare-event\nprobabilities. In particular, we find that theoretically strongly-efficient\nestimators should be used with great caution in practice, because they may\nyield inaccurate results in the pre-limit. Further, this inaccuracy may not be\ndetectable from the output of the Monte Carlo simulation, because the\nsimulation output may severely underestimate the true variance of the\nestimator. \n\n"}
{"id": "1705.04219", "contents": "Title: A critical analysis of resampling strategies for the regularized\n  particle filter Abstract: We analyze the performance of different resampling strategies for the\nregularized particle filter regarding parameter estimation. We show in\nparticular, building on analytical insight obtained in the linear Gaussian\ncase, that resampling systematically can prevent the filtered density from\nconverging towards the true posterior distribution. We discuss several means to\novercome this limitation, including kernel bandwidth modulation, and provide\nevidence that the resulting particle filter clearly outperforms traditional\nbootstrap particle filters. Our results are supported by numerical simulations\non a linear textbook example, the logistic map and a non-linear plant growth\nmodel. \n\n"}
{"id": "1705.04374", "contents": "Title: Optimal fidelity multi-level Monte Carlo for quantification of\n  uncertainty in simulations of cloud cavitation collapse Abstract: We quantify uncertainties in the location and magnitude of extreme pressure\nspots revealed from large scale multi-phase flow simulations of cloud\ncavitation collapse. We examine clouds containing 500 cavities and quantify\nuncertainties related to their initial spatial arrangement. The resulting\n2000-dimensional space is sampled using a non-intrusive and computationally\nefficient Multi-Level Monte Carlo (MLMC) methodology. We introduce novel\noptimal control variate coefficients to enhance the variance reduction in MLMC.\nThe proposed optimal fidelity MLMC leads to more than two orders of magnitude\nspeedup when compared to standard Monte Carlo methods. We identify large\nuncertainties in the location and magnitude of the peak pressure pulse and\npresent its statistical correlations and joint probability density functions\nwith the geometrical characteristics of the cloud. Characteristic properties of\nspatial cloud structure are identified as potential causes of significant\nuncertainties in exerted collapse pressures. \n\n"}
{"id": "1705.04584", "contents": "Title: spBayesSurv: Fitting Bayesian Spatial Survival Models Using R Abstract: Spatial survival analysis has received a great deal of attention over the\nlast 20 years due to the important role that geographical information can play\nin predicting survival. This paper provides an introduction to a set of\nprograms for implementing some Bayesian spatial survival models in R using the\npackage spBayesSurv. The function survregbayes includes the three most\ncommonly-used semiparametric models: proportional hazards, proportional odds,\nand accelerated failure time. All manner of censored survival times are\nsimultaneously accommodated including uncensored, interval censored,\ncurrent-status, left and right censored, and mixtures of these. Left-truncated\ndata are also accommodated. Time-dependent covariates are allowed under the\npiecewise constant assumption. Both georeferenced and areally observed spatial\nlocations are handled via frailties. Model fit is assessed with conditional\nCox-Snell residual plots, and model choice is carried out via the log pseudo\nmarginal likelihood, the deviance information criterion and the Watanabe-Akaike\ninformation criterion. The accelerated failure time frailty model with a\ncovariate-dependent baseline is included in the function frailtyGAFT. In\naddition, the package also provides two marginal survival models: proportional\nhazards and linear dependent Dirichlet process mixture, where the spatial\ndependence is modeled via spatial copulas. Note that the package can also\nhandle non-spatial data using non-spatial versions of aforementioned models. \n\n"}
{"id": "1705.05955", "contents": "Title: ExoMol Line List XXI: Nitric Oxide (NO) Abstract: Line lists for the ground electronic ground state for six major isotopologues\nof nitric oxide are presented. The line lists are constructed using empirical\nenergy levels (and line positions) and high-level {\\it ab inito} intensities.\nThe energy levels were obtained using a combination of two approaches, from an\neffective Hamiltonian and from solving the rovibronic Schr\\\"{o}dinger equation\nvariationally. The effective hamiltonian model was obtained through a fit to\nthe experimental line positions of NO available in the literature for all six\nisotopologues using the programs SPFIT and SPCAT. The variational model was\nbuilt through a least squares fit of the \\textit{ab inito} potential and\nspin-orbit curves to the experimentally derived energies and experimental line\npositions of the main isotopologue only using the Duo program. The \\textit{ab\ninito} potential energy, spin-orbit and dipole moment curves (PEC, SOC and DMC)\nare computed using high-level {\\it ab inito} methods and the MARVEL method is\nused to obtain energies of NO from experimental transition frequencies. Each\nline list covers a wavenumber range from 0 - 40,000 \\cm with approximately\n22,000 rovibronic states and 2.3-2.6 million transitions extending to $J_{max}\n= 184.5$ and $v_{max} = 51$. Partition functions are also calculated up to a\ntemperature of 5000 K. The calculated absorption line intensities at 296 K\nusing these line lists show excellent agreement with those included in the\nHITRAN and HITEMP databases. The computed NO line lists are the most\ncomprehensive to date, covering a wider wavenumber and temperature range\ncompared to both the HITRAN and HITEMP databases. These line lists are also\nmore accurate than those used in HITEMP. The full line lists are available from\nthe CDS and ExoMol databases; data will also be available from CDMS. \n\n"}
{"id": "1705.06565", "contents": "Title: Numerical solution of fractional elliptic stochastic PDEs with spatial\n  white noise Abstract: The numerical approximation of solutions to stochastic partial differential\nequations with additive spatial white noise on bounded domains in\n$\\mathbb{R}^d$ is considered. The differential operator is given by the\nfractional power $L^\\beta$, $\\beta\\in(0,1)$, of an integer order elliptic\ndifferential operator $L$ and is therefore non-local. Its inverse $L^{-\\beta}$\nis represented by a Bochner integral from the Dunford-Taylor functional\ncalculus. By applying a quadrature formula to this integral representation, the\ninverse fractional power operator $L^{-\\beta}$ is approximated by a weighted\nsum of non-fractional resolvents $(I + t_j^2 L)^{-1}$ at certain quadrature\nnodes $t_j>0$. The resolvents are then discretized in space by a standard\nfinite element method.\n  This approach is combined with an approximation of the white noise, which is\nbased only on the mass matrix of the finite element discretization. In this\nway, an efficient numerical algorithm for computing samples of the approximate\nsolution is obtained. For the resulting approximation, the strong mean-square\nerror is analyzed and an explicit rate of convergence is derived. Numerical\nexperiments for $L=\\kappa^2-\\Delta$, $\\kappa > 0$, with homogeneous Dirichlet\nboundary conditions on the unit cube $(0,1)^d$ in $d=1,2,3$ spatial dimensions\nfor varying $\\beta\\in(0,1)$ attest the theoretical results. \n\n"}
{"id": "1705.07194", "contents": "Title: Proximal Methods for Sparse Optimal Scoring and Discriminant Analysis Abstract: Linear discriminant analysis (LDA) is a classical method for dimensionality\nreduction, where discriminant vectors are sought to project data to a lower\ndimensional space for optimal separability of classes. Several recent papers\nhave outlined strategies for exploiting sparsity for using LDA with\nhigh-dimensional data. However, many lack scalable methods for solution of the\nunderlying optimization problems. We propose three new numerical optimization\nschemes for solving the sparse optimal scoring formulation of LDA based on\nblock coordinate descent, the proximal gradient method, and the alternating\ndirection method of multipliers. We show that the per-iteration cost of these\nmethods scales linearly in the dimension of the data provided restricted\nregularization terms are employed, and cubically in the dimension of the data\nin the worst case. Furthermore, we establish that if our block coordinate\ndescent framework generates convergent subsequences of iterates, then these\nsubsequences converge to the stationary points of the sparse optimal scoring\nproblem. We demonstrate the effectiveness of our new methods with empirical\nresults for classification of Gaussian data and data sets drawn from\nbenchmarking repositories, including time-series and multispectral X-ray data,\nand provide Matlab and R implementations of our optimization schemes. \n\n"}
{"id": "1705.07598", "contents": "Title: Rao-Blackwellized Particle Smoothing as Message Passing Abstract: In this manuscript the fixed-lag smoothing problem for conditionally linear\nGaussian state-space models is investigated from a factor graph perspective.\nMore specifically, after formulating Bayesian smoothing for an arbitrary\nstate-space model as forward-backward message passing over a factor graph, we\nfocus on the above mentioned class of models and derive a novel\nRao-Blackwellized particle smoother for it. Then, we show how our technique can\nbe modified to estimate a point mass approximation of the so called joint\nsmoothing distribution. Finally, the estimation accuracy and the computational\nrequirements of our smoothing algorithms are analysed for a specific\nstate-space model. \n\n"}
{"id": "1705.07646", "contents": "Title: An approximate empirical Bayesian method for large-scale linear-Gaussian\n  inverse problems Abstract: We study Bayesian inference methods for solving linear inverse problems,\nfocusing on hierarchical formulations where the prior or the likelihood\nfunction depend on unspecified hyperparameters. In practice, these\nhyperparameters are often determined via an empirical Bayesian method that\nmaximizes the marginal likelihood function, i.e., the probability density of\nthe data conditional on the hyperparameters. Evaluating the marginal\nlikelihood, however, is computationally challenging for large-scale problems.\nIn this work, we present a method to approximately evaluate marginal likelihood\nfunctions, based on a low-rank approximation of the update from the prior\ncovariance to the posterior covariance. We show that this approximation is\noptimal in a minimax sense. Moreover, we provide an efficient algorithm to\nimplement the proposed method, based on a combination of the randomized SVD and\na spectral approximation method to compute square roots of the prior covariance\nmatrix. Several numerical examples demonstrate good performance of the proposed\nmethod. \n\n"}
{"id": "1705.07880", "contents": "Title: Reducing Reparameterization Gradient Variance Abstract: Optimization with noisy gradients has become ubiquitous in statistics and\nmachine learning. Reparameterization gradients, or gradient estimates computed\nvia the \"reparameterization trick,\" represent a class of noisy gradients often\nused in Monte Carlo variational inference (MCVI). However, when these gradient\nestimators are too noisy, the optimization procedure can be slow or fail to\nconverge. One way to reduce noise is to use more samples for the gradient\nestimate, but this can be computationally expensive. Instead, we view the noisy\ngradient as a random variable, and form an inexpensive approximation of the\ngenerating procedure for the gradient sample. This approximation has high\ncorrelation with the noisy gradient by construction, making it a useful control\nvariate for variance reduction. We demonstrate our approach on non-conjugate\nmulti-level hierarchical models and a Bayesian neural net where we observed\ngradient variance reductions of multiple orders of magnitude (20-2,000x). \n\n"}
{"id": "1705.08510", "contents": "Title: Discontinuous Hamiltonian Monte Carlo for discrete parameters and\n  discontinuous likelihoods Abstract: Hamiltonian Monte Carlo has emerged as a standard tool for posterior\ncomputation. In this article, we present an extension that can efficiently\nexplore target distributions with discontinuous densities. Our extension in\nparticular enables efficient sampling from ordinal parameters though embedding\nof probability mass functions into continuous spaces. We motivate our approach\nthrough a theory of discontinuous Hamiltonian dynamics and develop a\ncorresponding numerical solver. The proposed solver is the first of its kind,\nwith a remarkable ability to exactly preserve the Hamiltonian. We apply our\nalgorithm to challenging posterior inference problems to demonstrate its wide\napplicability and competitive performance. \n\n"}
{"id": "1705.08656", "contents": "Title: Efficient Covariance Approximations for Large Sparse Precision Matrices Abstract: The use of sparse precision (inverse covariance) matrices has become popular\nbecause they allow for efficient algorithms for joint inference in\nhigh-dimensional models. Many applications require the computation of certain\nelements of the covariance matrix, such as the marginal variances, which may be\nnon-trivial to obtain when the dimension is large. This paper introduces a fast\nRao-Blackwellized Monte Carlo sampling based method for efficiently\napproximating selected elements of the covariance matrix. The variance and\nconfidence bounds of the approximations can be precisely estimated without\nadditional computational costs. Furthermore, a method that iterates over\nsubdomains is introduced, and is shown to additionally reduce the approximation\nerrors to practically negligible levels in an application on functional\nmagnetic resonance imaging data. Both methods have low memory requirements,\nwhich is typically the bottleneck for competing direct methods. \n\n"}
{"id": "1705.08931", "contents": "Title: Proximity Variational Inference Abstract: Variational inference is a powerful approach for approximate posterior\ninference. However, it is sensitive to initialization and can be subject to\npoor local optima. In this paper, we develop proximity variational inference\n(PVI). PVI is a new method for optimizing the variational objective that\nconstrains subsequent iterates of the variational parameters to robustify the\noptimization path. Consequently, PVI is less sensitive to initialization and\noptimization quirks and finds better local optima. We demonstrate our method on\nthree proximity statistics. We study PVI on a Bernoulli factor model and\nsigmoid belief network with both real and synthetic data and compare to\ndeterministic annealing (Katahira et al., 2008). We highlight the flexibility\nof PVI by designing a proximity statistic for Bayesian deep learning models\nsuch as the variational autoencoder (Kingma and Welling, 2014; Rezende et al.,\n2014). Empirically, we show that PVI consistently finds better local optima and\ngives better predictive performance. \n\n"}
{"id": "1705.09457", "contents": "Title: Discovery of statistical equivalence classes using computer algebra Abstract: Discrete statistical models supported on labelled event trees can be\nspecified using so-called interpolating polynomials which are generalizations\nof generating functions. These admit a nested representation. A new algorithm\nexploits the primary decomposition of monomial ideals associated with an\ninterpolating polynomial to quickly compute all nested representations of that\npolynomial. It hereby determines an important subclass of all trees\nrepresenting the same statistical model. To illustrate this method we analyze\nthe full polynomial equivalence class of a staged tree representing the best\nfitting model inferred from a real-world dataset. \n\n"}
{"id": "1705.09831", "contents": "Title: A well-balanced meshless tsunami propagation and inundation model Abstract: We present a novel meshless tsunami propagation and inundation model. We\ndiscretize the nonlinear shallow-water equations using a well-balanced scheme\nrelying on radial basis function based finite differences. The inundation model\nrelies on radial basis function generated extrapolation from the wet points\nclosest to the wet-dry interface into the dry region. Numerical results against\nstandard one- and two-dimensional benchmarks are presented. \n\n"}
{"id": "1705.10376", "contents": "Title: Conducting Simulations in Causal Inference with Networks-Based\n  Structural Equation Models Abstract: The past decade has seen an increasing body of literature devoted to the\nestimation of causal effects in network-dependent data. However, the validity\nof many classical statistical methods in such data is often questioned. There\nis an emerging need for objective and practical ways to assess which causal\nmethodologies might be applicable and valid in network-dependent data. This\npaper describes a set of tools implemented in the simcausal R package that\nallow simulating data based on user-specified structural equation model for\nconnected units. Specification and simulation of counterfactual data is\nimplemented for static, dynamic and stochastic interventions. A new interface\naims to simplify the specification of network-based functional relationships\nbetween connected units. A set of examples illustrates how these simulations\nmay be applied to evaluation of different statistical methods for estimation of\ncausal effects in network-dependent data. \n\n"}
{"id": "1705.11123", "contents": "Title: Advanced Bayesian Multilevel Modeling with the R Package brms Abstract: The brms package allows R users to easily specify a wide range of Bayesian\nsingle-level and multilevel models, which are fitted with the probabilistic\nprogramming language Stan behind the scenes. Several response distributions are\nsupported, of which all parameters (e.g., location, scale, and shape) can be\npredicted at the same time thus allowing for distributional regression.\nNon-linear relationships may be specified using non-linear predictor terms or\nsemi-parametric approaches such as splines or Gaussian processes. To make all\nof these modeling options possible in a multilevel framework, brms provides an\nintuitive and powerful formula syntax, which extends the well known formula\nsyntax of lme4. The purpose of the present paper is to introduce this syntax in\ndetail and to demonstrate its usefulness with four examples, each showing other\nrelevant aspects of the syntax. \n\n"}
{"id": "1706.00098", "contents": "Title: Bayesian $l_0$-regularized Least Squares Abstract: Bayesian $l_0$-regularized least squares is a variable selection technique\nfor high dimensional predictors. The challenge is optimizing a non-convex\nobjective function via search over model space consisting of all possible\npredictor combinations. Spike-and-slab (a.k.a. Bernoulli-Gaussian) priors are\nthe gold standard for Bayesian variable selection, with a caveat of\ncomputational speed and scalability. Single Best Replacement (SBR) provides a\nfast scalable alternative. We provide a link between Bayesian regularization\nand proximal updating, which provides an equivalence between finding a\nposterior mode and a posterior mean with a different regularization prior. This\nallows us to use SBR to find the spike-and-slab estimator. To illustrate our\nmethodology, we provide simulation evidence and a real data example on the\nstatistical properties and computational efficiency of SBR versus direct\nposterior sampling using spike-and-slab priors. Finally, we conclude with\ndirections for future research. \n\n"}
{"id": "1706.01260", "contents": "Title: The Classical Complexity of Boson Sampling Abstract: We study the classical complexity of the exact Boson Sampling problem where\nthe objective is to produce provably correct random samples from a particular\nquantum mechanical distribution. The computational framework was proposed by\nAaronson and Arkhipov in 2011 as an attainable demonstration of `quantum\nsupremacy', that is a practical quantum computing experiment able to produce\noutput at a speed beyond the reach of classical (that is non-quantum) computer\nhardware. Since its introduction Boson Sampling has been the subject of intense\ninternational research in the world of quantum computing. On the face of it,\nthe problem is challenging for classical computation. Aaronson and Arkhipov\nshow that exact Boson Sampling is not efficiently solvable by a classical\ncomputer unless $P^{\\#P} = BPP^{NP}$ and the polynomial hierarchy collapses to\nthe third level.\n  The fastest known exact classical algorithm for the standard Boson Sampling\nproblem takes $O({m + n -1 \\choose n} n 2^n )$ time to produce samples for a\nsystem with input size $n$ and $m$ output modes, making it infeasible for\nanything but the smallest values of $n$ and $m$. We give an algorithm that is\nmuch faster, running in $O(n 2^n + \\operatorname{poly}(m,n))$ time and $O(m)$\nadditional space. The algorithm is simple to implement and has low constant\nfactor overheads. As a consequence our classical algorithm is able to solve the\nexact Boson Sampling problem for system sizes far beyond current photonic\nquantum computing experimentation, thereby significantly reducing the\nlikelihood of achieving near-term quantum supremacy in the context of Boson\nSampling. \n\n"}
{"id": "1706.01629", "contents": "Title: Markov Chain Monte Carlo Methods for Bayesian Data Analysis in Astronomy Abstract: Markov Chain Monte Carlo based Bayesian data analysis has now become the\nmethod of choice for analyzing and interpreting data in almost all disciplines\nof science. In astronomy, over the last decade, we have also seen a steady\nincrease in the number of papers that employ Monte Carlo based Bayesian\nanalysis. New, efficient Monte Carlo based methods are continuously being\ndeveloped and explored. In this review, we first explain the basics of Bayesian\ntheory and discuss how to set up data analysis problems within this framework.\nNext, we provide an overview of various Monte Carlo based methods for\nperforming Bayesian data analysis. Finally, we discuss advanced ideas that\nenable us to tackle complex problems and thus hold great promise for the\nfuture. We also distribute downloadable computer software (available at\nhttps://github.com/sanjibs/bmcmc/ ) that implements some of the algorithms and\nexamples discussed here. \n\n"}
{"id": "1706.02380", "contents": "Title: Multi-sample Estimation of Bacterial Composition Matrix in Metagenomics\n  Data Abstract: Metagenomics sequencing is routinely applied to quantify bacterial abundances\nin microbiome studies, where the bacterial composition is estimated based on\nthe sequencing read counts. Due to limited sequencing depth and DNA dropouts,\nmany rare bacterial taxa might not be captured in the final sequencing reads,\nwhich results in many zero counts. Naive composition estimation using count\nnormalization leads to many zero proportions, which tend to result in\ninaccurate estimates of bacterial abundance and diversity. This paper takes a\nmulti-sample approach to the estimation of bacterial abundances in order to\nborrow information across samples and across species. Empirical results from\nreal data sets suggest that the composition matrix over multiple samples is\napproximately low rank, which motivates a regularized maximum likelihood\nestimation with a nuclear norm penalty. An efficient optimization algorithm\nusing the generalized accelerated proximal gradient and Euclidean projection\nonto simplex space is developed. The theoretical upper bounds and the minimax\nlower bounds of the estimation errors, measured by the Kullback-Leibler\ndivergence and the Frobenius norm, are established. Simulation studies\ndemonstrate that the proposed estimator outperforms the naive estimators. The\nmethod is applied to an analysis of a human gut microbiome dataset. \n\n"}
{"id": "1706.02808", "contents": "Title: A randomized Halton algorithm in R Abstract: Randomized quasi-Monte Carlo (RQMC) sampling can bring orders of magnitude\nreduction in variance compared to plain Monte Carlo (MC) sampling. The extent\nof the efficiency gain varies from problem to problem and can be hard to\npredict. This article presents an R function rhalton that produces scrambled\nversions of Halton sequences. On some problems it brings efficiency gains of\nseveral thousand fold. On other problems, the efficiency gain is minor. The\ncode is designed to make it easy to determine whether a given integrand will\nbenefit from RQMC sampling. An RQMC sample of n points in $[0,1]^d$ can be\nextended later to a larger n and/or d. \n\n"}
{"id": "1706.02952", "contents": "Title: TIP: Typifying the Interpretability of Procedures Abstract: We provide a novel notion of what it means to be interpretable, looking past\nthe usual association with human understanding. Our key insight is that\ninterpretability is not an absolute concept and so we define it relative to a\ntarget model, which may or may not be a human. We define a framework that\nallows for comparing interpretable procedures by linking them to important\npractical aspects such as accuracy and robustness. We characterize many of the\ncurrent state-of-the-art interpretable methods in our framework portraying its\ngeneral applicability. Finally, principled interpretable strategies are\nproposed and empirically evaluated on synthetic data, as well as on the largest\npublic olfaction dataset that was made recently available \\cite{olfs}. We also\nexperiment on MNIST with a simple target model and different oracle models of\nvarying complexity. This leads to the insight that the improvement in the\ntarget model is not only a function of the oracle model's performance, but also\nits relative complexity with respect to the target model. Further experiments\non CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit\nof our methods over Knowledge Distillation when the target models are simple\nand the complex model is a neural network. \n\n"}
{"id": "1706.03188", "contents": "Title: Radiative Transfer for Exoplanet Atmospheres Abstract: Remote sensing of the atmospheres of distant worlds motivates a firm\nunderstanding of radiative transfer. In this review, we provide a pedagogical\ncookbook that describes the principal ingredients needed to perform a radiative\ntransfer calculation and predict the spectrum of an exoplanet atmosphere,\nincluding solving the radiative transfer equation, calculating opacities (and\nchemistry), iterating for radiative equilibrium (or not), and adapting the\noutput of the calculations to the astronomical observations. A review of the\nstate of the art is performed, focusing on selected milestone papers.\nOutstanding issues, including the need to understand aerosols or clouds and\nelucidating the assumptions and caveats behind inversion methods, are\ndiscussed. A checklist is provided to assist referees/reviewers in their\nscrutiny of works involving radiative transfer. A table summarizing the\nmethodology employed by past studies is provided. \n\n"}
{"id": "1706.04606", "contents": "Title: Nudged elastic band calculations accelerated with Gaussian process\n  regression Abstract: Minimum energy paths for transitions such as atomic and/or spin\nrearrangements in thermalized systems are the transition paths of largest\nstatistical weight. Such paths are frequently calculated using the nudged\nelastic band method, where an initial path is iteratively shifted to the\nnearest minimum energy path. The computational effort can be large, especially\nwhen ab initio or electron density functional calculations are used to evaluate\nthe energy and atomic forces. Here, we show how the number of such evaluations\ncan be reduced by an order of magnitude using a Gaussian process regression\napproach where an approximate energy surface is generated and refined in each\niteration. When the goal is to evaluate the transition rate within harmonic\ntransition state theory, the evaluation of the Hessian matrix at the initial\nand final state minima can be carried out beforehand and used as input in the\nminimum energy path calculation, thereby improving stability and reducing the\nnumber of iterations needed for convergence. A Gaussian process model also\nprovides an uncertainty estimate for the approximate energy surface, and this\ncan be used to focus the calculations on the lesser-known part of the path,\nthereby reducing the number of needed energy and force evaluations to a half in\nthe present calculations. The methodology is illustrated using the\ntwo-dimensional M\\\"uller-Brown potential surface and performance assessed on an\nestablished benchmark involving 13 rearrangement transitions of a heptamer\nisland on a solid surface. \n\n"}
{"id": "1706.04834", "contents": "Title: A coherent structure approach for parameter estimation in Lagrangian\n  Data Assimilation Abstract: We introduce a data assimilation method to estimate model parameters with\nobservations of passive tracers by directly assimilating Lagrangian Coherent\nStructures. Our approach differs from the usual Lagrangian Data Assimilation\napproach, where parameters are estimated based on tracer trajectories. We\nemploy the Approximate Bayesian Computation (ABC) framework to avoid computing\nthe likelihood function of the coherent structure, which is usually\nunavailable. We solve the ABC by a Sequential Monte Carlo (SMC) method, and use\nPrincipal Component Analysis (PCA) to identify the coherent patterns from\ntracer trajectory data. Our new method shows remarkably improved results\ncompared to the bootstrap particle filter when the physical model exhibits\nchaotic advection. \n\n"}
{"id": "1706.07564", "contents": "Title: Least Squares Polynomial Chaos Expansion: A Review of Sampling\n  Strategies Abstract: As non-institutive polynomial chaos expansion (PCE) techniques have gained\ngrowing popularity among researchers, we here provide a comprehensive review of\nmajor sampling strategies for the least squares based PCE. Traditional sampling\nmethods, such as Monte Carlo, Latin hypercube, quasi-Monte Carlo, optimal\ndesign of experiments (ODE), Gaussian quadratures, as well as more recent\ntechniques, such as coherence-optimal and randomized quadratures are discussed.\nWe also propose a hybrid sampling method, dubbed alphabetic-coherence-optimal,\nthat employs the so-called alphabetic optimality criteria used in the context\nof ODE in conjunction with coherence-optimal samples. A comparison between the\nempirical performance of the selected sampling methods applied to three\nnumerical examples, including high-order PCE's, high-dimensional problems, and\nlow oversampling ratios, is presented to provide a road map for practitioners\nseeking the most suitable sampling technique for a problem at hand. We observed\nthat the alphabetic-coherence-optimal technique outperforms other sampling\nmethods, specially when high-order ODE are employed and/or the oversampling\nratio is low. \n\n"}
{"id": "1706.07712", "contents": "Title: Asymptotics of ABC Abstract: We present an informal review of recent work on the asymptotics of\nApproximate Bayesian Computation (ABC). In particular we focus on how does the\nABC posterior, or point estimates obtained by ABC, behave in the limit as we\nhave more data? The results we review show that ABC can perform well in terms\nof point estimation, but standard implementations will over-estimate the\nuncertainty about the parameters. If we use the regression correction of\nBeaumont et al. then ABC can also accurately quantify this uncertainty. The\ntheoretical results also have practical implications for how to implement ABC. \n\n"}
{"id": "1706.08885", "contents": "Title: The primitive equations as the small aspect ratio limit of the\n  Navier-Stokes equations: rigorous justification of the hydrostatic\n  approximation Abstract: An important feature of the planetary oceanic dynamics is that the aspect\nratio (the ratio of the depth to horizontal width) is very small. As a result,\nthe hydrostatic approximation (balance), derived by performing the formal small\naspect ratio limit to the Navier-Stokes equations, is considered as a\nfundamental component in the primitive equations of the large-scale ocean. In\nthis paper, we justify rigorously the small aspect ratio limit of the\nNavier-Stokes equations to the primitive equations. Specifically, we prove that\nthe Navier-Stokes equations, after being scaled appropriately by the small\naspect ratio parameter of the physical domain, converge strongly to the\nprimitive equations, globally and uniformly in time, and the convergence rate\nis of the same order as the aspect ratio parameter. This result validates the\nhydrostatic approximation for the large-scale oceanic dynamics. Notably, only\nthe weak convergence of this small aspect ratio limit was rigorously justified\nbefore. \n\n"}
{"id": "1706.10159", "contents": "Title: Linked by dynamics: wavelet--based mutual information rate as a\n  connectivity measure and scale-specific networks Abstract: Experimentally observed networks of interacting dynamical systems are\ninferred from recorded multivariate time series by evaluating a statistical\nmeasure of dependence, usually the cross-correlation coefficient, or mutual\ninformation. These measures reflect dependence in static probability\ndistributions, generated by systems' evolution, rather than coherence of\nsystems' dynamics. Moreover, these \"static\" measures of dependence can be\nbiased due to properties of dynamics underlying the analyzed time series.\nConsequently, properties of local dynamics can be misinterpreted as properties\nof connectivity or long-range interactions. We propose the mutual information\nrate as a measure reflecting coherence or synchronization of dynamics of two\nsystems and not suffering by the bias typical for the \"static\" measures. We\ndemonstrate that a computationally accessible estimation method, derived for\nGaussian processes and adapted by using the wavelet transform, can be effective\nfor nonlinear, nonstationary and multiscale processes. The discussed problem\nand the proposed method are illustrated using numerically generated data of\ncoupled dynamical systems as well as gridded reanalysis data of surface air\ntemperature as the source for the construction of climate networks. In\nparticular, scale-specific climate networks are introduced. \n\n"}
{"id": "1707.03663", "contents": "Title: Underdamped Langevin MCMC: A non-asymptotic analysis Abstract: We study the underdamped Langevin diffusion when the log of the target\ndistribution is smooth and strongly concave. We present a MCMC algorithm based\non its discretization and show that it achieves $\\varepsilon$ error (in\n2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a\nsignificant improvement over the best known rate for overdamped Langevin MCMC,\nwhich is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same\nsmoothness/concavity assumptions.\n  The underdamped Langevin MCMC scheme can be viewed as a version of\nHamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped\nLangevin MCMC methods in a number of application areas. We provide quantitative\nrates that support this empirical wisdom. \n\n"}
{"id": "1707.04314", "contents": "Title: Bayesian Optimization for Probabilistic Programs Abstract: We present the first general purpose framework for marginal maximum a\nposteriori estimation of probabilistic program variables. By using a series of\ncode transformations, the evidence of any probabilistic program, and therefore\nof any graphical model, can be optimized with respect to an arbitrary subset of\nits sampled variables. To carry out this optimization, we develop the first\nBayesian optimization package to directly exploit the source code of its\ntarget, leading to innovations in problem-independent hyperpriors, unbounded\noptimization, and implicit constraint satisfaction; delivering significant\nperformance improvements over prominent existing packages. We present\napplications of our method to a number of tasks including engineering design\nand parameter optimization. \n\n"}
{"id": "1707.04929", "contents": "Title: Projected Power Iteration for Network Alignment Abstract: The network alignment problem asks for the best correspondence between two\ngiven graphs, so that the largest possible number of edges are matched. This\nproblem appears in many scientific problems (like the study of protein-protein\ninteractions) and it is very closely related to the quadratic assignment\nproblem which has graph isomorphism, traveling salesman and minimum bisection\nproblems as particular cases. The graph matching problem is NP-hard in general.\nHowever, under some restrictive models for the graphs, algorithms can\napproximate the alignment efficiently. In that spirit the recent work by Feizi\nand collaborators introduce EigenAlign, a fast spectral method with convergence\nguarantees for Erd\\H{o}s-Reny\\'i graphs. In this work we propose the algorithm\nProjected Power Alignment, which is a projected power iteration version of\nEigenAlign. We numerically show it improves the recovery rates of EigenAlign\nand we describe the theory that may be used to provide performance guarantees\nfor Projected Power Alignment. \n\n"}
{"id": "1707.04932", "contents": "Title: Free-space quantum links under diverse weather conditions Abstract: Free-space optical communication links are promising channels for\nestablishing secure quantum communication. Here we study the transmission of\nnonclassical light through a turbulent atmospheric link under diverse weather\nconditions, including rain or haze. To include these effects, the theory of\nlight transmission through atmospheric links in the elliptic-beam approximation\npresented by Vasylyev et al. [D. Vasylyev et al., Phys. Rev. Lett. 117, 090501\n(2016); arXiv:1604.01373] is further generalized.It is demonstrated, with good\nagreement between theory and experiment, that low-intensity rain merely\ncontributes additional deterministic losses, whereas haze also introduces\nadditional beam deformations of the transmitted light. Based on these results,\nwe study theoretically the transmission of quadrature squeezing and Gaussian\nentanglement under these weather conditions. \n\n"}
{"id": "1707.05200", "contents": "Title: A Discrete Bouncy Particle Sampler Abstract: Most Markov chain Monte Carlo methods operate in discrete time and are\nreversible with respect to the target probability. Nevertheless, it is now\nunderstood that the use of non-reversible Markov chains can be beneficial in\nmany contexts. In particular, the recently-proposed Bouncy Particle Sampler\nleverages a continuous-time and non-reversible Markov process and empirically\nshows state-of-the-art performances when used to explore certain probability\ndensities; however, its implementation typically requires the computation of\nlocal upper bounds on the gradient of the log target density.\n  We present the Discrete Bouncy Particle Sampler, a general algorithm based\nupon a guided random walk, a partial refreshment of direction, and a\ndelayed-rejection step. We show that the Bouncy Particle Sampler can be\nunderstood as a scaling limit of a special case of our algorithm. In contrast\nto the Bouncy Particle Sampler, implementing the Discrete Bouncy Particle\nSampler only requires point-wise evaluation of the target density and its\ngradient. We propose extensions of the basic algorithm for situations when the\nexact gradient of the target density is not available. In a Gaussian setting,\nwe establish a scaling limit for the radial process as dimension increases to\ninfinity. We leverage this result to obtain the theoretical efficiency of the\nDiscrete Bouncy Particle Sampler as a function of the partial-refreshment\nparameter, which leads to a simple and robust tuning criterion. A further\nanalysis in a more general setting suggests that this tuning criterion applies\nmore generally. Theoretical and empirical efficiency curves are then compared\nfor different targets and algorithm variations. \n\n"}
{"id": "1707.05659", "contents": "Title: The finite gap method and the analytic description of the exact rogue\n  wave recurrence in the periodic NLS Cauchy problem. 1 Abstract: The focusing NLS equation is the simplest universal model describing the\nmodulation instability (MI) of quasi monochromatic waves in weakly nonlinear\nmedia, considered the main physical mechanism for the appearance of rogue\n(anomalous) waves (RWs) in Nature. In this paper we study, using the finite gap\nmethod, the NLS Cauchy problem for periodic initial perturbations of the\nunstable background solution of NLS exciting just one of the unstable modes. We\ndistinguish two cases. In the case in which only the corresponding unstable gap\nis theoretically open, the solution describes an exact deterministic alternate\nrecurrence of linear and nonlinear stages of MI, and the nonlinear RW stages\nare described by the 1-breather Akhmediev solution, whose parameters, different\nat each RW appearance, are always given in terms of the initial data through\nelementary functions. If the number of unstable modes is >1, this uniform in t\ndynamics is sensibly affected by perturbations due to numerics and/or real\nexperiments, provoking O(1) corrections to the result. In the second case in\nwhich more than one unstable gap is open, a detailed investigation of all these\ngaps is necessary to get a uniform in $t$ dynamics, and this study is postponed\nto a subsequent paper. It is however possible to obtain the elementary\ndescription of the first nonlinear stage of MI, given again by the Akhmediev\n1-breather solution, and how perturbations due to numerics and/or real\nexperiments can affect this result. \n\n"}
{"id": "1707.07796", "contents": "Title: Stationary waves and slowly moving features in the night upper clouds of\n  Venus Abstract: At the cloud top level of Venus (65-70 km altitude) the atmosphere rotates 60\ntimes faster than the underlying surface, a phenomenon known as superrotation.\nWhereas on Venus's dayside the cloud top motions are well determined and Venus\ngeneral circulation models predict a mean zonal flow at the upper clouds\nsimilar on both day and nightside, the nightside circulation remains poorly\nstudied except for the polar region. Here we report global measurements of the\nnightside circulation at the upper cloud level. We tracked individual features\nin thermal emission images at 3.8 and 5.0 $\\mathrm{\\mu m}$ obtained between\n2006 and 2008 by the Visible and Infrared Thermal Imaging Spectrometer\n(VIRTIS-M) onboard Venus Express and in 2015 by ground-based measurements with\nthe Medium-Resolution 0.8-5.5 Micron Spectrograph and Imager (SpeX) at the\nNational Aeronautics and Space Administration Infrared Telescope Facility\n(NASA/IRTF). The zonal motions range from -110 to -60 m s$^{-1}$, consistent\nwith those found for the dayside but with larger dispersion. Slow motions (-50\nto -20 m s$^{-1}$) were also found and remain unexplained. In addition,\nabundant stationary wave patterns with zonal speeds from -10 to +10 m s$^{-1}$\ndominate the night upper clouds and concentrate over the regions of higher\nsurface elevation. \n\n"}
{"id": "1707.08220", "contents": "Title: Bayesian hierarchical weighting adjustment and survey inference Abstract: We combine Bayesian prediction and weighted inference as a unified approach\nto survey inference. The general principles of Bayesian analysis imply that\nmodels for survey outcomes should be conditional on all variables that affect\nthe probability of inclusion. We incorporate the weighting variables under the\nframework of multilevel regression and poststratification, as a byproduct\ngenerating model-based weights after smoothing. We investigate deep\ninteractions and introduce structured prior distributions for smoothing and\nstability of estimates. The computation is done via Stan and implemented in the\nopen source R package \"rstanarm\" ready for public use. Simulation studies\nillustrate that model-based prediction and weighting inference outperform\nclassical weighting. We apply the proposal to the New York Longitudinal Study\nof Wellbeing. The new approach generates robust weights and increases\nefficiency for finite population inference, especially for subsets of the\npopulation. \n\n"}
{"id": "1707.09705", "contents": "Title: Mini-batch Tempered MCMC Abstract: In this paper we propose a general framework of performing MCMC with only a\nmini-batch of data. We show by estimating the Metropolis-Hasting ratio with\nonly a mini-batch of data, one is essentially sampling from the true posterior\nraised to a known temperature. We show by experiments that our method,\nMini-batch Tempered MCMC (MINT-MCMC), can efficiently explore multiple modes of\na posterior distribution. Based on the Equi-Energy sampler (Kou et al. 2006),\nwe developed a new parallel MCMC algorithm based on the Equi-Energy sampler,\nwhich enables efficient sampling from high-dimensional multi-modal posteriors\nwith well separated modes. \n\n"}
{"id": "1708.00257", "contents": "Title: Robust PCA by Manifold Optimization Abstract: Robust PCA is a widely used statistical procedure to recover a underlying\nlow-rank matrix with grossly corrupted observations. This work considers the\nproblem of robust PCA as a nonconvex optimization problem on the manifold of\nlow-rank matrices, and proposes two algorithms (for two versions of\nretractions) based on manifold optimization. It is shown that, with a proper\ndesigned initialization, the proposed algorithms are guaranteed to converge to\nthe underlying low-rank matrix linearly. Compared with a previous work based on\nthe Burer-Monterio decomposition of low-rank matrices, the proposed algorithms\nreduce the dependence on the conditional number of the underlying low-rank\nmatrix theoretically. Simulations and real data examples confirm the\ncompetitive performance of our method. \n\n"}
{"id": "1708.00762", "contents": "Title: Numerical instability of the Akhmediev breather and a finite-gap model\n  of it Abstract: In this paper we study the numerical instabilities of the NLS Akhmediev\nbreather, the simplest space periodic, one-mode perturbation of the unstable\nbackground, limiting our considerations to the simplest case of one unstable\nmode. In agreement with recent theoretical findings of the authors, in the\nsituation in which the round-off errors are negligible with respect to the\nperturbations due to the discrete scheme used in the numerical experiments, the\nsplit-step Fourier method (SSFM), the numerical output is well-described by a\nsuitable genus 2 finite-gap solution of NLS. This solution can be written in\nterms of different elementary functions in different time regions and,\nultimately, it shows an exact recurrence of rogue waves described, at each\nappearance, by the Akhmediev breather. We discover a remarkable empirical\nformula connecting the recurrence time with the number of time steps used in\nthe SSFM and, via our recent theoretical findings, we establish that the SSFM\nopens up a vertical unstable gap whose length can be computed with high\naccuracy, and is proportional to the inverse of the square of the number of\ntime steps used in the SSFM. This neat picture essentially changes when the\nround-off error is sufficiently large. Indeed experiments in standard double\nprecision show serious instabilities in both the periods and phases of the\nrecurrence. In contrast with it, as predicted by the theory, replacing the\nexact Akhmediev Cauchy datum by its first harmonic approximation, we only\nslightly modify the numerical output. Let us also remark, that the first rogue\nwave appearance is completely stable in all experiments and is in perfect\nagreement with the Akhmediev formula and with the theoretical prediction in\nterms of the Cauchy data. \n\n"}
{"id": "1708.00842", "contents": "Title: Latent Parameter Estimation in Fusion Networks Using Separable\n  Likelihoods Abstract: Multi-sensor state space models underpin fusion applications in networks of\nsensors. Estimation of latent parameters in these models has the potential to\nprovide highly desirable capabilities such as network self-calibration.\nConventional solutions to the problem pose difficulties in scaling with the\nnumber of sensors due to the joint multi-sensor filtering involved when\nevaluating the parameter likelihood. In this article, we propose a separable\npseudo-likelihood which is a more accurate approximation compared to a\npreviously proposed alternative under typical operating conditions. In\naddition, we consider using separable likelihoods in the presence of many\nobjects and ambiguity in associating measurements with objects that originated\nthem. To this end, we use a state space model with a hypothesis based\nparameterisation, and, develop an empirical Bayesian perspective in order to\nevaluate separable likelihoods on this model using local filtering. Bayesian\ninference with this likelihood is carried out using belief propagation on the\nassociated pairwise Markov random field. We specify a particle algorithm for\nlatent parameter estimation in a linear Gaussian state space model and\ndemonstrate its efficacy for network self-calibration using measurements from\nnon-cooperative targets in comparison with alternatives. \n\n"}
{"id": "1708.00886", "contents": "Title: Application of a Second-order Stochastic Optimization Algorithm for\n  Fitting Stochastic Epidemiological Models Abstract: Epidemiological models have tremendous potential to forecast disease burden\nand quantify the impact of interventions. Detailed models are increasingly\npopular, however these models tend to be stochastic and very costly to\nevaluate. Fortunately, readily available high-performance cloud computing now\nmeans that these models can be evaluated many times in parallel. Here, we\nbriefly describe PSPO, an extension to Spall's second-order stochastic\noptimization algorithm, Simultaneous Perturbation Stochastic Approximation\n(SPSA), that takes full advantage of parallel computing environments. The main\nfocus of this work is on the use of PSPO to maximize the pseudo-likelihood of a\nstochastic epidemiological model to data from a 1861 measles outbreak in\nHagelloch, Germany. Results indicate that PSPO far outperforms gradient ascent\nand SPSA on this challenging likelihood maximization problem. \n\n"}
{"id": "1708.00955", "contents": "Title: Hamiltonian Monte Carlo with Energy Conserving Subsampling Abstract: Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional\nposterior distributions with proposed parameter draws obtained by iterating on\na discretized version of the Hamiltonian dynamics. The iterations make HMC\ncomputationally costly, especially in problems with large datasets, since it is\nnecessary to compute posterior densities and their derivatives with respect to\nthe parameters. Naively computing the Hamiltonian dynamics on a subset of the\ndata causes HMC to lose its key ability to generate distant parameter proposals\nwith high acceptance probability. The key insight in our article is that\nefficient subsampling HMC for the parameters is possible if both the dynamics\nand the acceptance probability are computed from the same data subsample in\neach complete HMC iteration. We show that this is possible to do in a\nprincipled way in a HMC-within-Gibbs framework where the subsample is updated\nusing a pseudo marginal MH step and the parameters are then updated using an\nHMC step, based on the current subsample. We show that our subsampling methods\nare fast and compare favorably to two popular sampling algorithms that utilize\ngradient estimates from data subsampling. We also explore the current\nlimitations of subsampling HMC algorithms by varying the quality of the\nvariance reducing control variates used in the estimators of the posterior\ndensity and its gradients. \n\n"}
{"id": "1708.02365", "contents": "Title: Indirect Inference with a Non-Smooth Criterion Function Abstract: Indirect inference requires simulating realisations of endogenous variables\nfrom the model under study. When the endogenous variables are discontinuous\nfunctions of the model parameters, the resulting indirect inference criterion\nfunction is discontinuous and does not permit the use of derivative-based\noptimisation routines. Using a change of variables technique, we propose a\nnovel simulation algorithm that alleviates the discontinuities inherent in such\nindirect inference criterion functions, and permits the application of\nderivative-based optimisation routines to estimate the unknown model\nparameters. Unlike competing approaches, this approach does not rely on kernel\nsmoothing or bandwidth parameters. Several Monte Carlo examples that have\nfeatured in the literature on indirect inference with discontinuous outcomes\nillustrate the approach, and demonstrate the superior performance of this\napproach over existing alternatives. \n\n"}
{"id": "1708.03031", "contents": "Title: Statistical state dynamics of weak jets in barotropic beta-plane\n  turbulence Abstract: Zonal jets in a barotropic setup emerge out of homogeneous turbulence through\na flow-forming instability of the homogeneous turbulent state (`zonostrophic\ninstability') which occurs as the turbulence intensity increases. This has been\ndemonstrated using the statistical state dynamics (SSD) framework with a\nclosure at second order. Furthermore, it was shown that for small\nsupercriticality the flow-forming instability follows Ginzburg-Landau (G-L)\ndynamics. Here, the SSD framework is used to study the equilibration of this\nflow-forming instability for small supercriticality. First, we compare the\npredictions of the weakly nonlinear G-L dynamics to the fully nonlinear SSD\ndynamics closed at second order for a wide ranges of parameters. A new branch\nof jet equilibria is revealed that is not contiguously connected with the G-L\nbranch. This new branch at weak supercriticalities involves jets with larger\namplitude compared to the ones of the G-L branch. Furthermore, this new branch\ncontinues even for subcritical values with respect to the linear flow-forming\ninstability. Thus, a new nonlinear flow-forming instability out of homogeneous\nturbulence is revealed. Second, we investigate how both the linear flow-forming\ninstability and the novel nonlinear flow-forming instability are equilibrated.\nWe identify the physical processes underlying the jet equilibration as well as\nthe types of eddies that contribute in each process. Third, we propose a\nmodification of the diffusion coefficient of the G-L dynamics that is able to\ncapture the asymmetric evolution for weak jets at scales other than the\nmarginal scale (side-band instabilities) for the linear flow-forming\ninstability. \n\n"}
{"id": "1708.03272", "contents": "Title: Fast and accurate Bayesian model criticism and conflict diagnostics\n  using R-INLA Abstract: Bayesian hierarchical models are increasingly popular for realistic modelling\nand analysis of complex data. This trend is accompanied by the need for\nflexible, general, and computationally efficient methods for model criticism\nand conflict detection. Usually, a Bayesian hierarchical model incorporates a\ngrouping of the individual data points, for example individuals in repeated\nmeasurement data. In such cases, the following question arises: Are any of the\ngroups \"outliers\", or in conflict with the remaining groups? Existing general\napproaches aiming to answer such questions tend to be extremely computationally\ndemanding when model fitting is based on MCMC. We show how group-level model\ncriticism and conflict detection can be done quickly and accurately through\nintegrated nested Laplace approximations (INLA). The new method is implemented\nas a part of the open source R-INLA package for Bayesian computing\n(http://r-inla.org). \n\n"}
{"id": "1708.03625", "contents": "Title: Unbiased Markov chain Monte Carlo with couplings Abstract: Markov chain Monte Carlo (MCMC) methods provide consistent of integrals as\nthe number of iterations goes to infinity. MCMC estimators are generally biased\nafter any fixed number of iterations. We propose to remove this bias by using\ncouplings of Markov chains together with a telescopic sum argument of Glynn and\nRhee (2014). The resulting unbiased estimators can be computed independently in\nparallel. We discuss practical couplings for popular MCMC algorithms. We\nestablish the theoretical validity of the proposed estimators and study their\nefficiency relative to the underlying MCMC algorithms. Finally, we illustrate\nthe performance and limitations of the method on toy examples, on an Ising\nmodel around its critical temperature, on a high-dimensional variable selection\nproblem, and on an approximation of the cut distribution arising in Bayesian\ninference for models made of multiple modules. \n\n"}
{"id": "1708.07114", "contents": "Title: Rapid Mixing of Hamiltonian Monte Carlo on Strongly Log-Concave\n  Distributions Abstract: We obtain several quantitative bounds on the mixing properties of the\nHamiltonian Monte Carlo (HMC) algorithm for a strongly log-concave target\ndistribution $\\pi$ on $\\mathbb{R}^{d}$, showing that HMC mixes quickly in this\nsetting. One of our main results is a dimension-free bound on the mixing of an\n\"ideal\" HMC chain, which is used to show that the usual leapfrog implementation\nof HMC can sample from $\\pi$ using only $\\mathcal{O}(d^{\\frac{1}{4}})$ gradient\nevaluations. This dependence on dimension is sharp, and our results\nsignificantly extend and improve previous quantitative bounds on the mixing of\nHMC. \n\n"}
{"id": "1708.07448", "contents": "Title: Seasonal forecasts of the summer 2016 Yangtze River basin rainfall Abstract: The Yangtze River has been subject to heavy flooding throughout history, and\nin recent times severe floods such as those in 1998 have resulted in heavy loss\nof life and livelihoods. Dams along the river help to manage flood waters, and\nare important sources of electricity for the region. Being able to forecast\nhigh-impact events at long lead times therefore has enormous potential benefit.\nRecent improvements in seasonal forecasting mean that dynamical climate models\ncan start to be used directly for operational services. The teleconnection from\nEl Ni\\~no to Yangtze River basin rainfall meant that the strong El Ni\\~no in\nwinter 2015/2016 provided a valuable opportunity to test the application of a\ndynamical forecast system.\n  This paper therefore presents a case study of a real time seasonal forecast\nfor the Yangtze River basin, building on previous work demonstrating the\nretrospective skill of such a forecast. A simple forecasting methodology is\npresented, in which the forecast probabilities are derived from the historical\nrelationship between hindcast and observations. Its performance for 2016 is\ndiscussed. The heavy rainfall in the May-June-July period was correctly\nforecast well in advance. August saw anomalously low rainfall, and the\nforecasts for the June-July-August period correctly showed closer to average\nlevels. The forecasts contributed to the confidence of decision-makers across\nthe Yangtze River basin. Trials of climate services such as this help to\npromote appropriate use of seasonal forecasts, and highlight areas for future\nimprovements. \n\n"}
{"id": "1708.07801", "contents": "Title: Nudging the particle filter Abstract: We investigate a new sampling scheme aimed at improving the performance of\nparticle filters whenever (a) there is a significant mismatch between the\nassumed model dynamics and the actual system, or (b) the posterior probability\ntends to concentrate in relatively small regions of the state space. The\nproposed scheme pushes some particles towards specific regions where the\nlikelihood is expected to be high, an operation known as nudging in the\ngeophysics literature. We re-interpret nudging in a form applicable to any\nparticle filtering scheme, as it does not involve any changes in the rest of\nthe algorithm. Since the particles are modified, but the importance weights do\nnot account for this modification, the use of nudging leads to additional bias\nin the resulting estimators. However, we prove analytically that nudged\nparticle filters can still attain asymptotic convergence with the same error\nrates as conventional particle methods. Simple analysis also yields an\nalternative interpretation of the nudging operation that explains its\nrobustness to model errors. Finally, we show numerical results that illustrate\nthe improvements that can be attained using the proposed scheme. In particular,\nwe present nonlinear tracking examples with synthetic data and a model\ninference example using real-world financial data. \n\n"}
{"id": "1708.07805", "contents": "Title: Dimensional Reduction of Direct Statistical Simulation Abstract: Direct Statistical Simulation (DSS) solves the equations of motion for the\nstatistics of turbulent flows in place of the traditional route of accumulating\nstatistics by Direct Numerical Simulation (DNS). That low-order statistics\nusually evolve slowly compared with instantaneous dynamics is one important\nadvantage of DSS. Depending on the symmetry of the problem and the choice of\naveraging operation, however, DSS is usually more expensive computationally\nthan DNS because even low order statistics typically have higher dimension than\nthe underlying fields. Here we show that it is possible to go much further by\nusing Proper Orthogonal Decomposition (POD) to address the \"curse of\ndimensionality.\" We apply POD directly to DSS in the form of expansions in the\nequal-time cumulants to second order (CE2). We explore two averaging operations\n(zonal and ensemble) and test the approach on two idealized barotropic models\non a rotating sphere (a jet that relaxes deterministically towards an unstable\nprofile, and a stochastically-driven flow that spontaneously organizes into\njets). Order-of-magnitude savings in computational cost are obtained in the\nreduced basis, potentially enabling access to parameter regimes beyond the\nreach of DNS. \n\n"}
{"id": "1708.08396", "contents": "Title: Controlled Sequential Monte Carlo Abstract: Sequential Monte Carlo methods, also known as particle methods, are a popular\nset of techniques for approximating high-dimensional probability distributions\nand their normalizing constants. These methods have found numerous applications\nin statistics and related fields; e.g. for inference in non-linear non-Gaussian\nstate space models, and in complex static models. Like many Monte Carlo\nsampling schemes, they rely on proposal distributions which crucially impact\ntheir performance. We introduce here a class of controlled sequential Monte\nCarlo algorithms, where the proposal distributions are determined by\napproximating the solution to an associated optimal control problem using an\niterative scheme. This method builds upon a number of existing algorithms in\neconometrics, physics, and statistics for inference in state space models, and\ngeneralizes these methods so as to accommodate complex static models. We\nprovide a theoretical analysis concerning the fluctuation and stability of this\nmethodology that also provides insight into the properties of related\nalgorithms. We demonstrate significant gains over state-of-the-art methods at a\nfixed computational complexity on a variety of applications. \n\n"}
{"id": "1709.00151", "contents": "Title: Approximately Optimal Subset Selection for Statistical Design and\n  Modelling Abstract: We study the problem of optimal subset selection from a set of correlated\nrandom variables. In particular, we consider the associated combinatorial\noptimization problem of maximizing the determinant of a symmetric positive\ndefinite matrix that characterizes the chosen subset. This problem arises in\nmany domains, such as experimental designs, regression modeling, and\nenvironmental statistics. We establish an efficient polynomial-time algorithm\nusing Determinantal Point Process for approximating the optimal solution to the\nproblem. We demonstrate the advantages of our methods by presenting\ncomputational results for both synthetic and real data sets. \n\n"}
{"id": "1709.00814", "contents": "Title: How vortices and shocks provide for a flux loop in two-dimensional\n  compressible turbulence Abstract: Large-scale turbulence in fluid layers and other quasi-two-dimensional\ncompressible systems consists of planar vortices and waves. Separately, wave\nturbulence usually produces a direct energy cascade, while solenoidal planar\nturbulence transports energy to large scales by an inverse cascade. Here, we\nconsider turbulence at finite Mach numbers when the interaction between\nacoustic waves and vortices is substantial. We employ solenoidal pumping at\nintermediate scales and show how both direct and inverse energy cascades are\nformed starting from the pumping scale. We show that there is an inverse\ncascade of kinetic energy up to a scale $\\ell$, where a typical velocity\nreaches the speed of sound; this creates shock waves, which provide for a\ncompensating direct cascade. When the system size is less than $\\ell$, the\nsteady state contains a system-size pair of long-living condensate vortices\nconnected by a system of shocks. Thus turbulence in fluid layers processes\nenergy via a loop: Most energy first goes to large scales via vortices and is\nthen transported by waves to small-scale dissipation. \n\n"}
{"id": "1709.01002", "contents": "Title: Unbiased approximations of products of expectations Abstract: We consider the problem of approximating the product of $n$ expectations with\nrespect to a common probability distribution $\\mu$. Such products routinely\narise in statistics as values of the likelihood in latent variable models.\nMotivated by pseudo-marginal Markov chain Monte Carlo schemes, we focus on\nunbiased estimators of such products. The standard approach is to sample $N$\nparticles from $\\mu$ and assign each particle to one of the expectations. This\nis wasteful and typically requires the number of particles to grow\nquadratically with the number of expectations. We propose an alternative\nestimator that approximates each expectation using most of the particles while\npreserving unbiasedness. We carefully study its properties, showing that in\nlatent variable contexts the proposed estimator needs only $\\mathcal{O}(n)$\nparticles to match the performance of the standard approach with\n$\\mathcal{O}(n^{2})$ particles. We demonstrate the procedure on two latent\nvariable examples from approximate Bayesian computation and single-cell gene\nexpression analysis, observing computational gains of the order of the number\nof expectations, i.e. data points, $n$. \n\n"}
{"id": "1709.02069", "contents": "Title: An Alternative Approach to Functional Linear Partial Quantile Regression Abstract: Functional data such as curves and surfaces have become more and more common\nwith modern technological advancements. The use of functional predictors\nremains challenging due to its inherent infinite-dimensionality. The common\npractice is to project functional data into a finite dimensional space. The\npopular partial least square (PLS) method has been well studied for the\nfunctional linear model [1]. As an alternative, quantile regression provides a\nrobust and more comprehensive picture of the conditional distribution of a\nresponse when it is non-normal, heavy-tailed, or contaminated by outliers.\nWhile partial quantile regression (PQR) was proposed in [2], no theoretical\nguarantees were provided due to the iterative nature of the algorithm and the\nnon-smoothness of quantile loss function. To address these issues, we propose\nan alternative PQR (APQR) formulation with guaranteed convergence. This novel\nformulation motivates new theories and allows us to establish asymptotic\nproperties. Numerical studies on a benchmark dataset show the superiority of\nour new approach. We also apply our novel method to a functional magnetic\nresonance imaging (fMRI) data to predict attention deficit hyperactivity\ndisorder (ADHD) and a diffusion tensor imaging (DTI) dataset to predict\nAlzheimer's disease (AD). \n\n"}
{"id": "1709.02532", "contents": "Title: Generalizing Distance Covariance to Measure and Test Multivariate Mutual\n  Dependence Abstract: We propose three measures of mutual dependence between multiple random\nvectors. All the measures are zero if and only if the random vectors are\nmutually independent. The first measure generalizes distance covariance from\npairwise dependence to mutual dependence, while the other two measures are sums\nof squared distance covariance. All the measures share similar properties and\nasymptotic distributions to distance covariance, and capture non-linear and\nnon-monotone mutual dependence between the random vectors. Inspired by complete\nand incomplete V-statistics, we define the empirical measures and simplified\nempirical measures as a trade-off between the complexity and power when testing\nmutual independence. Implementation of the tests is demonstrated by both\nsimulation results and real data examples. \n\n"}
{"id": "1709.03184", "contents": "Title: A Physics-Based Approach to Unsupervised Discovery of Coherent\n  Structures in Spatiotemporal Systems Abstract: Given that observational and numerical climate data are being produced at\never more prodigious rates, increasingly sophisticated and automated analysis\ntechniques have become essential. Deep learning is quickly becoming a standard\napproach for such analyses and, while great progress is being made, major\nchallenges remain. Unlike commercial applications in which deep learning has\nled to surprising successes, scientific data is highly complex and typically\nunlabeled. Moreover, interpretability and detecting new mechanisms are key to\nscientific discovery. To enhance discovery we present a complementary\nphysics-based, data-driven approach that exploits the causal nature of\nspatiotemporal data sets generated by local dynamics (e.g. hydrodynamic flows).\nWe illustrate how novel patterns and coherent structures can be discovered in\ncellular automata and outline the path from them to climate data. \n\n"}
{"id": "1709.03283", "contents": "Title: Principal component analysis and sparse polynomial chaos expansions for\n  global sensitivity analysis and model calibration: application to urban\n  drainage simulation Abstract: This paper presents an efficient surrogate modeling strategy for the\nuncertainty quantification and Bayesian calibration of a hydrological model. In\nparticular, a process-based dynamical urban drainage simulator that predicts\nthe discharge from a catchment area during a precipitation event is considered.\nThe goal of the case study is to perform a global sensitivity analysis and to\nidentify the unknown model parameters as well as the measurement and prediction\nerrors. These objectives can only be achieved by cheapening the incurred\ncomputational costs, that is, lowering the number of necessary model runs. With\nthis in mind, a regularity-exploiting metamodeling technique is proposed that\nenables fast uncertainty quantification. Principal component analysis is used\nfor output dimensionality reduction and sparse polynomial chaos expansions are\nused for the emulation of the reduced outputs. Sobol' sensitivity indices are\nobtained directly from the expansion coefficients by a mere post-processing.\nBayesian inference via Markov chain Monte Carlo posterior sampling is\ndrastically accelerated. \n\n"}
{"id": "1709.03312", "contents": "Title: A determinant-free method to simulate the parameters of large Gaussian\n  fields Abstract: We propose a determinant-free approach for simulation-based Bayesian\ninference in high-dimensional Gaussian models. We introduce auxiliary variables\nwith covariance equal to the inverse covariance of the model. The joint\nprobability of the auxiliary model can be computed without evaluating\ndeterminants, which are often hard to compute in high dimensions. We develop a\nMarkov chain Monte Carlo sampling scheme for the auxiliary model that requires\nno more than the application of inverse-matrix-square-roots and the solution of\nlinear systems. These operations can be performed at large scales with rational\napproximations. We provide an empirical study on both synthetic and real-world\ndata for sparse Gaussian processes and for large-scale Gaussian Markov random\nfields. \n\n"}
{"id": "1709.03757", "contents": "Title: Computation of extreme heat waves in climate models using a large\n  deviation algorithm Abstract: Studying extreme events and how they evolve in a changing climate is one of\nthe most important current scientific challenges. Starting from complex climate\nmodels, a key difficulty is to be able to run long enough simulations in order\nto observe those extremely rare events. In physics, chemistry, and biology,\nrare event algorithms have recently been developed to compute probabilities of\nevents that cannot be observed in direct numerical simulations. Here we propose\nsuch an algorithm, specifically designed for extreme heat or cold waves, based\non statistical physics approaches. This gives an improvement of more than two\norders of magnitude in the sampling efficiency. We describe the dynamics of\nevents that would not be observed otherwise. We show that European extreme heat\nwaves are related to a global teleconnection pattern involving North America\nand Asia. This tool opens a full range of studies, so far impossible, aimed at\nassessing quantitatively climate change impacts. \n\n"}
{"id": "1709.04196", "contents": "Title: Particle Filters and Data Assimilation Abstract: State-space models can be used to incorporate subject knowledge on the\nunderlying dynamics of a time series by the introduction of a latent Markov\nstate-process. A user can specify the dynamics of this process together with\nhow the state relates to partial and noisy observations that have been made.\nInference and prediction then involves solving a challenging inverse problem:\ncalculating the conditional distribution of quantities of interest given the\nobservations. This article reviews Monte Carlo algorithms for solving this\ninverse problem, covering methods based on the particle filter and the ensemble\nKalman filter. We discuss the challenges posed by models with high-dimensional\nstates, joint estimation of parameters and the state, and inference for the\nhistory of the state process. We also point out some potential new developments\nwhich will be important for tackling cutting-edge filtering applications. \n\n"}
{"id": "1709.04611", "contents": "Title: A Novel Algorithm for Clustering of Data on the Unit Sphere via Mixture\n  Models Abstract: A new maximum approximate likelihood (ML) estimation algorithm for the\nmixture of Kent distribution is proposed. The new algorithm is constructed via\nthe BSLM (block successive lower-bound maximization) framework and incorporates\nmanifold optimization procedures within it. The BSLM algorithm is iterative and\nmonotonically increases the approximate log-likelihood function in each step.\nUnder mild regularity conditions, the BSLM algorithm is proved to be convergent\nand the approximate ML estimator is proved to be consistent. A Bayesian\ninformation criterion-like (BIC-like) model selection criterion is also derive,\nfor the task of choosing the number of components in the mixture distribution.\nThe approximate ML estimator and the BIC-like criterion are both demonstrated\nto be successful via simulation studies. A model-based clustering rule is\nproposed and also assessed favorably via simulations. Example applications of\nthe developed methodology are provided via an image segmentation task and a\nneural imaging clustering problem. \n\n"}
{"id": "1709.05906", "contents": "Title: Bayesian analysis of three parameter singular Marshall-Olkin bivariate\n  Pareto distribution Abstract: This paper provides bayesian analysis of singular Marshall-Olkin bivariate\nPareto distribution. We consider three parameter singular Marshall-Olkin\nbivariate Pareto distribution. We consider two types of prior - reference prior\nand gamma prior. Bayes estimate of the parameters are calculated based on slice\ncum gibbs sampler and Lindley approximation. Credible interval is also provided\nfor all methods and all prior distributions. A data analysis is kept for\nillustrative purpose. \n\n"}
{"id": "1709.06181", "contents": "Title: On Nesting Monte Carlo Estimators Abstract: Many problems in machine learning and statistics involve nested expectations\nand thus do not permit conventional Monte Carlo (MC) estimation. For such\nproblems, one must nest estimators, such that terms in an outer estimator\nthemselves involve calculation of a separate, nested, estimation. We\ninvestigate the statistical implications of nesting MC estimators, including\ncases of multiple levels of nesting, and establish the conditions under which\nthey converge. We derive corresponding rates of convergence and provide\nempirical evidence that these rates are observed in practice. We further\nestablish a number of pitfalls that can arise from naive nesting of MC\nestimators, provide guidelines about how these can be avoided, and lay out\nnovel methods for reformulating certain classes of nested expectation problems\ninto single expectations, leading to improved convergence rates. We demonstrate\nthe applicability of our work by using our results to develop a new estimator\nfor discrete Bayesian experimental design problems and derive error bounds for\na class of variational objectives. \n\n"}
{"id": "1709.06254", "contents": "Title: BeSS: An R Package for Best Subset Selection in Linear, Logistic and\n  CoxPH Models Abstract: We introduce a new R package, BeSS, for solving the best subset selection\nproblem in linear, logistic and Cox's proportional hazard (CoxPH) models. It\nutilizes a highly efficient active set algorithm based on primal and dual\nvariables, and supports sequential and golden search strategies for best subset\nselection. We provide a C++ implementation of the algorithm using Rcpp\ninterface. We demonstrate through numerical experiments based on enormous\nsimulation and real datasets that the new BeSS package has competitive\nperformance compared to other R packages for best subset selection purpose. \n\n"}
{"id": "1709.06597", "contents": "Title: varbvs: Fast Variable Selection for Large-scale Regression Abstract: We introduce varbvs, a suite of functions written in R and MATLAB for\nregression analysis of large-scale data sets using Bayesian variable selection\nmethods. We have developed numerical optimization algorithms based on\nvariational approximation methods that make it feasible to apply Bayesian\nvariable selection to very large data sets. With a focus on examples from\ngenome-wide association studies, we demonstrate that varbvs scales well to data\nsets with hundreds of thousands of variables and thousands of samples, and has\nfeatures that facilitate rapid data analyses. Moreover, varbvs allows for\nextensive model customization, which can be used to incorporate external\ninformation into the analysis. We expect that the combination of an easy-to-use\ninterface and robust, scalable algorithms for posterior computation will\nencourage broader use of Bayesian variable selection in areas of applied\nstatistics and computational biology. The most recent R and MATLAB source code\nis available for download at Github (https://github.com/pcarbo/varbvs), and the\nR package can be installed from CRAN\n(https://cran.r-project.org/package=varbvs). \n\n"}
{"id": "1709.08266", "contents": "Title: On the wave turbulence theory for stratified flows in the ocean Abstract: After the pioneering work of Garrett and Munk, the statistics of oceanic\ninternal gravity waves has become a central subject of research in\noceanography. The time evolution of the spectral energy of internal waves in\nthe ocean can be described by a near-resonance wave turbulence equation, of\nquantum Boltzmann type. In this work, we provide the first rigorous\nmathematical study for the equation by showing the global existence and\nuniqueness of strong solutions. \n\n"}
{"id": "1709.08626", "contents": "Title: A general framework for data-driven uncertainty quantification under\n  complex input dependencies using vine copulas Abstract: Systems subject to uncertain inputs produce uncertain responses. Uncertainty\nquantification (UQ) deals with the estimation of statistics of the system\nresponse, given a computational model of the system and a probabilistic model\nof its inputs. In engineering applications it is common to assume that the\ninputs are mutually independent or coupled by a Gaussian or elliptical\ndependence structure (copula). In this paper we overcome such limitations by\nmodelling the dependence structure of multivariate inputs as vine copulas. Vine\ncopulas are models of multivariate dependence built from simpler pair-copulas.\nThe vine representation is flexible enough to capture complex dependencies.\nThis paper formalises the framework needed to build vine copula models of\nmultivariate inputs and to combine them with virtually any UQ method. The\nframework allows for a fully automated, data-driven inference of the\nprobabilistic input model on available input data. The procedure is exemplified\non two finite element models of truss structures, both subject to inputs with\nnon-Gaussian dependence structures. For each case, we analyse the moments of\nthe model response (using polynomial chaos expansions), and perform a\nstructural reliability analysis to calculate the probability of failure of the\nsystem (using the first order reliability method and importance sampling).\nReference solutions are obtained by Monte Carlo simulation. The results show\nthat, while the Gaussian assumption yields biased statistics, the vine copula\nrepresentation achieves significantly more precise estimates, even when its\nstructure needs to be fully inferred from a limited amount of observations. \n\n"}
{"id": "1709.08720", "contents": "Title: On Extreme Value Index Estimation under Random Censoring Abstract: Extreme value analysis in the presence of censoring is receiving much\nattention as it has applications in many disciplines, including survival and\nreliability studies. Estimation of extreme value index (EVI) is of primary\nimportance as it is a critical parameter needed in estimating extreme events\nsuch as quantiles and exceedance probabilities. In this paper, we review\nseveral estimators of the extreme value index when data is subject to random\ncensoring. In addition, four estimators are proposed, one based on the\nexponential regression approximation of log spacings, one based on a Zipf\nestimator and two based on variants of the moment estimator. The proposed\nestimators and the existing ones are compared under the same simulation\nconditions. The performance measures for the estimators include confidence\ninterval length and coverage probability. The simulation results show that no\nestimator is universally the best as the estimators depend on the size of the\nEVI parameter, percentage of censoring in the right tail and the underlying\ndistribution. However, certain estimators such as the proposed reduced-bias\nestimator and the adapted moment estimator are found to perform well across\nmost scenarios. Moreover, we present a bootstrap algorithm for obtaining\nsamples for extreme value analysis in the context of censoring. Some of the\nestimators that performed well in the simulation study are illustrated using a\npractical dataset from medical research \n\n"}
{"id": "1709.08723", "contents": "Title: A Simulation Comparison of Estimators of Conditional Extreme Value Index\n  under Right Random Censoring Abstract: In extreme value analysis, the extreme value index plays a vital role as it\ndetermines the tail heaviness of the underlying distribution and is the primary\nparameter required for the estimation of other extreme events. In this paper,\nwe review the estimation of the extreme value index when observations are\nsubject to right random censoring and the presence of covariate information. In\naddition, we propose some estimators of the extreme value index, including a\nmaximum likelihood estimator from a perturbed Pareto distribution. The existing\nestimators and the proposed ones are compared through a simulation study under\nidentical conditions. The results show that the performance of the estimators\ndepend on the percentage of censoring, the underlying distribution, the size of\nextreme value index and the number of top order statistics. Overall, we found\nthe proposed estimator from the perturbed Pareto distribution to be robust to\ncensoring, size of the extreme value index and the number of top order\nstatistics. \n\n"}
{"id": "1709.08725", "contents": "Title: A Machine Learning Framework to Forecast Wave Conditions Abstract: A~machine learning framework is developed to estimate ocean-wave conditions.\nBy supervised training of machine learning models on many thousands of\niterations of a physics-based wave model, accurate representations of\nsignificant wave heights and period can be used to predict ocean conditions. A\nmodel of Monterey Bay was used as the example test site; it was forced by\nmeasured wave conditions, ocean-current nowcasts, and reported winds. These\ninput data along with model outputs of spatially variable wave heights and\ncharacteristic period were aggregated into supervised learning training and\ntest data sets, which were supplied to machine learning models. These machine\nlearning models replicated wave heights with a root-mean-squared error of 9cm\nand correctly identify over 90% of the characteristic periods for the test-data\nsets. Impressively, transforming model inputs to outputs through matrix\noperations requires only a fraction (<1/1,000) of the computation time compared\nto forecasting with the physics-based model. \n\n"}
{"id": "1709.08860", "contents": "Title: A Data Driven, Zero-Dimensional Time Delay Model with Radiative Forcing\n  for Simulating Global Climate Abstract: Several complicated non-linear models exist which simulate the physical\nprocesses leading to fluctuations in global climate. Some of these more\nadvanced models use observations to constrain various parameters involved.\nHowever, they tend to be very computationally expensive. Also, the exact\nphysical processes that affect the climate variations have not been completely\ncomprehended. Therefore, to obtain an insight into global climate, we have\ndeveloped a physically motivated reduced climate model. The model utilizes a\nnovel mathematical formulation involving a non-linear delay differential\nequation to study temperature fluctuations when subjected to imposed radiative\nforcing. We have further incorporated simplified equations to test the effect\nof speculated mechanisms of climate forcing and evaluated the extent of their\ninfluence. The findings are significant in our efforts to predict climate\nchange and help in policy framing necessary to tackle it. \n\n"}
{"id": "1709.09763", "contents": "Title: Multilevel Sequential${}^2$ Monte Carlo for Bayesian Inverse Problems Abstract: The identification of parameters in mathematical models using noisy\nobservations is a common task in uncertainty quantification. We employ the\nframework of Bayesian inversion: we combine monitoring and observational data\nwith prior information to estimate the posterior distribution of a parameter.\nSpecifically, we are interested in the distribution of a diffusion coefficient\nof an elliptic PDE. In this setting, the sample space is high-dimensional, and\neach sample of the PDE solution is expensive. To address these issues we\npropose and analyse a novel Sequential Monte Carlo (SMC) sampler for the\napproximation of the posterior distribution. Classical, single-level SMC\nconstructs a sequence of measures, starting with the prior distribution, and\nfinishing with the posterior distribution. The intermediate measures arise from\na tempering of the likelihood, or, equivalently, a rescaling of the noise. The\nresolution of the PDE discretisation is fixed. In contrast, our estimator\nemploys a hierarchy of PDE discretisations to decrease the computational cost.\nWe construct a sequence of intermediate measures by decreasing the temperature\nor by increasing the discretisation level at the same time. This idea builds on\nand generalises the multi-resolution sampler proposed in [P.S. Koutsourelakis,\nJ. Comput. Phys., 228 (2009), pp. 6184-6211] where a bridging scheme is used to\ntransfer samples from coarse to fine discretisation levels. Importantly, our\nchoice between tempering and bridging is fully adaptive. We present numerical\nexperiments in 2D space, comparing our estimator to single-level SMC and the\nmulti-resolution sampler. \n\n"}
{"id": "1710.00435", "contents": "Title: Demarcating circulation regimes of synchronously rotating terrestrial\n  planets within the habitable zone Abstract: We investigate the atmospheric dynamics of terrestrial planets in synchronous\nrotation within the habitable zone of low-mass stars using the Community\nAtmosphere Model (CAM). The surface temperature contrast between day and night\nhemispheres decreases with an increase in incident stellar flux, which is\nopposite the trend seen on gas giants. We define three dynamical regimes in\nterms of the equatorial Rossby deformation radius and the Rhines length. The\nslow rotation regime has a mean zonal circulation that spans from day to night\nside, with both the Rossby deformation radius and the Rhines length exceeding\nplanetary radius, which occurs for planets around stars with effective\ntemperatures of 3300 K to 4500 K (rotation period > 20 days). Rapid rotators\nhave a mean zonal circulation that partially spans a hemisphere and with banded\ncloud formation beneath the substellar point, with the Rossby deformation\nradius is less than planetary radius, which occurs for planets orbiting stars\nwith effective temperatures of less than 3000 K (rotation period < 5 days). In\nbetween is the Rhines rotation regime, which retains a thermally-direct\ncirculation from day to night side but also features midlatitude\nturbulence-driven zonal jets. Rhines rotators occur for planets around stars in\nthe range of 3000 K to 3300 K (rotation period ~ 5 to 20 days), where the\nRhines length is greater than planetary radius but the Rossby deformation\nradius is less than planetary radius. The dynamical state can be\nobservationally inferred from comparing the morphology of the thermal emission\nphase curves of synchronously rotating planets. \n\n"}
{"id": "1710.00596", "contents": "Title: Scalable Bayesian regression in high dimensions with multiple data\n  sources Abstract: Applications of high-dimensional regression often involve multiple sources or\ntypes of covariates. We propose methodology for this setting, emphasizing the\n\"wide data\" regime with large total dimensionality p and sample size n<<p. We\nfocus on a flexible ridge-type prior with shrinkage levels that are specific to\neach data type or source and that are set automatically by empirical Bayes. All\nestimation, including setting of shrinkage levels, is formulated mainly in\nterms of inner product matrices of size n x n. This renders computation\nefficient in the wide data regime and allows scaling to problems with millions\nof features. Furthermore, the proposed procedures are free of user-set tuning\nparameters. We show how sparsity can be achieved by post-processing of the\nBayesian output via constrained minimization of a certain Kullback-Leibler\ndivergence. This yields sparse solutions with adaptive, source-specific\nshrinkage, including a closed-form variant that scales to very large p. We\npresent empirical results from a simulation study based on real data and a case\nstudy in Alzheimer's disease involving millions of features and multiple data\nsources. \n\n"}
{"id": "1710.01227", "contents": "Title: Keep It Real: Tail Probabilities of Compound Heavy-Tailed Distributions Abstract: We propose an analytical approach to the computation of tail probabilities of\ncompound distributions whose individual components have heavy tails. Our\napproach is based on the contour integration method, and gives rise to a\nrepresentation of the tail probability of a compound distribution in the form\nof a rapidly convergent one-dimensional integral involving a discontinuity of\nthe imaginary part of its moment generating function across a branch cut. The\nlatter integral can be evaluated in quadratures, or alternatively represented\nas an asymptotic expansion. Our approach thus offers a viable (especially at\nhigh percentile levels) alternative to more standard methods such as Monte\nCarlo or the Fast Fourier Transform, traditionally used for such problems. As a\npractical application, we use our method to compute the operational Value at\nRisk (VAR) of a financial institution, where individual losses are modeled as\nspliced distributions whose large loss components are given by power-law or\nlognormal distributions. Finally, we briefly discuss extensions of the present\nformalism for calculation of tail probabilities of compound distributions made\nof compound distributions with heavy tails. \n\n"}
{"id": "1710.02669", "contents": "Title: Aggregated moving functional median in robust prediction of hierarchical\n  functional time series - an application to forecasting web portal users\n  behaviors Abstract: In this article, a new nonparametric and robust method of forecasting\nhierarchical functional time series is presented. The method is compared with\nHyndman and Shang's method with respect to their unbiasedness, effectiveness,\nrobustness, and computational complexity. Taking into account results of the\nanalytical, simulation and empirical studies, we come to the conclusion that\nour proposal is superior over the proposal of Hyndman and Shang with respect to\nsome statistical criteria and especially with respect to robustness and\ncomputational complexity. An empirical usefulness of our method is presented on\nexample of management of a certain web portal divided into four subservices. An\nextensive simulation study involving hierarchical systems consisted of FAR(1)\nprocesses and Wiener processes has been conducted as well. \n\n"}
{"id": "1710.03157", "contents": "Title: Comparison of Gaussian process modeling software Abstract: Gaussian process fitting, or kriging, is often used to create a model from a\nset of data. Many available software packages do this, but we show that very\ndifferent results can be obtained from different packages even when using the\nsame data and model. We describe the parameterization, features, and\noptimization used by eight different fitting packages that run on four\ndifferent platforms. We then compare these eight packages using various data\nfunctions and data sets, revealing that there are stark differences between the\npackages. In addition to comparing the prediction accuracy, the predictive\nvariance--which is important for evaluating precision of predictions and is\noften used in stopping criteria--is also evaluated. \n\n"}
{"id": "1710.04586", "contents": "Title: Particle Filtering for Stochastic Navier-Stokes Signal Observed with\n  Linear Additive Noise Abstract: We consider a non-linear filtering problem, whereby the signal obeys the\nstochastic Navier-Stokes equations and is observed through a linear mapping\nwith additive noise. The setup is relevant to data assimilation for numerical\nweather prediction and climate modelling, where similar models are used for\nunknown ocean or wind velocities. We present a particle filtering methodology\nthat uses likelihood informed importance proposals, adaptive tempering, and a\nsmall number of appropriate Markov Chain Monte Carlo steps. We provide a\ndetailed design for each of these steps and show in our numerical examples that\nthey are all crucial in terms of achieving good performance and efficiency. \n\n"}
{"id": "1710.04977", "contents": "Title: Bayes factors for partially observed stochastic epidemic models Abstract: We consider the problem of model choice for stochastic epidemic models given\npartial observation of a disease outbreak through time. Our main focus is on\nthe use of Bayes factors. Although Bayes factors have appeared in the epidemic\nmodelling literature before, they can be hard to compute and little attention\nhas been given to fundamental questions concerning their utility. In this paper\nwe derive analytic expressions for Bayes factors given complete observation\nthrough time, which suggest practical guidelines for model choice problems. We\nextend the power posterior method for computing Bayes factors so as to account\nfor missing data and apply this approach to partially observed epidemics. For\ncomparison, we also explore the use of a deviance information criterion for\nmissing data scenarios. The methods are illustrated via examples involving both\nsimulated and real data. \n\n"}
{"id": "1710.06382", "contents": "Title: Convergence diagnostics for stochastic gradient descent with constant\n  step size Abstract: Many iterative procedures in stochastic optimization exhibit a transient\nphase followed by a stationary phase. During the transient phase the procedure\nconverges towards a region of interest, and during the stationary phase the\nprocedure oscillates in that region, commonly around a single point. In this\npaper, we develop a statistical diagnostic test to detect such phase transition\nin the context of stochastic gradient descent with constant learning rate. We\npresent theory and experiments suggesting that the region where the proposed\ndiagnostic is activated coincides with the convergence region. For a class of\nloss functions, we derive a closed-form solution describing such region.\nFinally, we suggest an application to speed up convergence of stochastic\ngradient descent by halving the learning rate each time stationarity is\ndetected. This leads to a new variant of stochastic gradient descent, which in\nmany settings is comparable to state-of-art. \n\n"}
{"id": "1710.06965", "contents": "Title: Importance sampling the union of rare events with an application to\n  power systems analysis Abstract: We consider importance sampling to estimate the probability $\\mu$ of a union\nof $J$ rare events $H_j$ defined by a random variable $\\boldsymbol{x}$. The\nsampler we study has been used in spatial statistics, genomics and\ncombinatorics going back at least to Karp and Luby (1983). It works by sampling\none event at random, then sampling $\\boldsymbol{x}$ conditionally on that event\nhappening and it constructs an unbiased estimate of $\\mu$ by multiplying an\ninverse moment of the number of occuring events by the union bound. We prove\nsome variance bounds for this sampler. For a sample size of $n$, it has a\nvariance no larger than $\\mu(\\bar\\mu-\\mu)/n$ where $\\bar\\mu$ is the union\nbound. It also has a coefficient of variation no larger than\n$\\sqrt{(J+J^{-1}-2)/(4n)}$ regardless of the overlap pattern among the $J$\nevents. Our motivating problem comes from power system reliability, where the\nphase differences between connected nodes have a joint Gaussian distribution\nand the $J$ rare events arise from unacceptably large phase differences. In the\ngrid reliability problems even some events defined by $5772$ constraints in\n$326$ dimensions, with probability below $10^{-22}$, are estimated with a\ncoefficient of variation of about $0.0024$ with only $n=10{,}000$ sample\nvalues. \n\n"}
{"id": "1710.08297", "contents": "Title: Coastal flood implications of 1.5 {\\deg}C, 2.0 {\\deg}C, and 2.5 {\\deg}C\n  temperature stabilization targets in the 21st and 22nd century Abstract: Sea-level rise (SLR) is magnifying the frequency and severity of coastal\nflooding. The rate and amount of global mean sea-level (GMSL) rise is a\nfunction of the trajectory of global mean surface temperature (GMST).\nTherefore, temperature stabilization targets (e.g., 1.5 {\\deg}C and 2.0 {\\deg}C\nof warming above pre-industrial levels, as from the Paris Agreement) have\nimportant implications for coastal flood risk. Here, we assess differences in\nthe return periods of coastal floods at a global network of tide gauges between\nscenarios that stabilize GMST warming at 1.5 {\\deg}C, 2.0 {\\deg}C, and 2.5\n{\\deg}C above pre-industrial levels. We employ probabilistic, localized SLR\nprojections and long-term hourly tide gauge records to construct estimates of\nthe return levels of current and future flood heights for the 21st and 22nd\ncenturies. By 2100, under 1.5 {\\deg}C, 2.0 {\\deg}C, and 2.5 {\\deg}C GMST\nstabilization, median GMSL is projected to rise 47 cm with a very likely range\nof 28-82 cm (90% probability), 55 cm (very likely 30-94 cm), and 58 cm (very\nlikely 36-93 cm), respectively. As an independent comparison, a semi-empirical\nsea level model calibrated to temperature and GMSL over the past two millennia\nestimates median GMSL will rise within < 13% of these projections. By 2150,\nrelative to the 2.0 {\\deg}C scenario, GMST stabilization of 1.5 {\\deg}C\ninundates roughly 5 million fewer inhabitants that currently occupy lands,\nincluding 40,000 fewer individuals currently residing in Small Island\nDeveloping States. Relative to a 2.0 {\\deg}C scenario, the reduction in the\namplification of the frequency of the 100-yr flood arising from a 1.5 {\\deg}C\nGMST stabilization is greatest in the eastern United States and in Europe, with\nflood frequency amplification being reduced by about half. \n\n"}
{"id": "1710.09890", "contents": "Title: Bayesian Nonparametric Models for Biomedical Data Analysis Abstract: In this dissertation, we develop nonparametric Bayesian models for biomedical\ndata analysis. In particular, we focus on inference for tumor heterogeneity and\ninference for missing data. First, we present a Bayesian feature allocation\nmodel for tumor subclone reconstruction using mutation pairs. The key\ninnovation lies in the use of short reads mapped to pairs of proximal single\nnucleotide variants (SNVs). In contrast, most existing methods use only\nmarginal reads for unpaired SNVs. In the same context of using mutation pairs,\nin order to recover the phylogenetic relationship of subclones, we then develop\na Bayesian treed feature allocation model. In contrast to commonly used feature\nallocation models, we allow the latent features to be dependent, using a tree\nstructure to introduce dependence. Finally, we propose a nonparametric Bayesian\napproach to monotone missing data in longitudinal studies with non-ignorable\nmissingness. In contrast to most existing methods, our method allows for\nincorporating information from auxiliary covariates and is able to capture\ncomplex structures among the response, missingness and auxiliary covariates.\nOur models are validated through simulation studies and are applied to\nreal-world biomedical datasets. \n\n"}
{"id": "1710.10951", "contents": "Title: SGDLibrary: A MATLAB library for stochastic gradient descent algorithms Abstract: We consider the problem of finding the minimizer of a function $f:\n\\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the finite-sum form $\\min f(w) =\n1/n\\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent\nyears in the field of machine learning (ML). One promising approach for\nlarge-scale data is to use a stochastic optimization algorithm to solve the\nproblem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library\nof a collection of stochastic optimization algorithms. The purpose of the\nlibrary is to provide researchers and implementers a comprehensive evaluation\nenvironment for the use of these algorithms on various ML problems. \n\n"}
{"id": "1710.11529", "contents": "Title: A 4D-Var Method with Flow-Dependent Background Covariances for the\n  Shallow-Water Equations Abstract: The 4D-Var method for filtering partially observed nonlinear chaotic\ndynamical systems consists of finding the maximum a-posteriori (MAP) estimator\nof the initial condition of the system given observations over a time window,\nand propagating it forward to the current time via the model dynamics. This\nmethod forms the basis of most currently operational weather forecasting\nsystems. In practice the optimization becomes infeasible if the time window is\ntoo long due to the non-convexity of the cost function, the effect of model\nerrors, and the limited precision of the ODE solvers. Hence the window has to\nbe kept sufficiently short, and the observations in the previous windows can be\ntaken into account via a Gaussian background (prior) distribution. The choice\nof the background covariance matrix is an important question that has received\nmuch attention in the literature. In this paper, we define the background\ncovariances in a principled manner, based on observations in the previous $b$\nassimilation windows, for a parameter $b\\ge 1$. The method is at most $b$ times\nmore computationally expensive than using fixed background covariances,\nrequires little tuning, and greatly improves the accuracy of 4D-Var. As a\nconcrete example, we focus on the shallow-water equations. The proposed method\nis compared against state-of-the-art approaches in data assimilation and is\nshown to perform favourably on simulated data. We also illustrate our approach\non data from the recent tsunami of 2011 in Fukushima, Japan. \n\n"}
{"id": "1711.00484", "contents": "Title: Spatial Statistical Downscaling for Constructing High-Resolution Nature\n  Runs in Global Observing System Simulation Experiments Abstract: Observing system simulation experiments (OSSEs) have been widely used as a\nrigorous and cost-effective way to guide development of new observing systems,\nand to evaluate the performance of new data assimilation algorithms. Nature\nruns (NRs), which are outputs from deterministic models, play an essential role\nin building OSSE systems for global atmospheric processes because they are used\nboth to create synthetic observations at high spatial resolution, and to\nrepresent the \"true\" atmosphere against which the forecasts are verified.\nHowever, most NRs are generated at resolutions coarser than actual\nobservations. Here, we propose a principled statistical downscaling framework\nto construct high-resolution NRs via conditional simulation from\ncoarse-resolution numerical model output. We use nonstationary spatial\ncovariance function models that have basis function representations. This\napproach not only explicitly addresses the change-of-support problem, but also\nallows fast computation with large volumes of numerical model output. We also\npropose a data-driven algorithm to select the required basis functions\nadaptively, in order to increase the flexibility of our nonstationary\ncovariance function models. In this article we demonstrate these techniques by\ndownscaling a coarse-resolution physical NR at a native resolution of\n$1^{\\circ} \\text{ latitude} \\times 1.25^{\\circ} \\text{ longitude}$ of global\nsurface $\\text{CO}_2$ concentrations to 655,362 equal-area hexagons. \n\n"}
{"id": "1711.00505", "contents": "Title: Novel discoveries on the mathematical foundation of linear hydrodynamic\n  stability theory Abstract: We present some new discoveries on the mathematical foundation of linear\nhydrodynamic stability theory. The new discoveries are: 1. Linearized Euler\nequations fail to provide a linear approximation on inviscid hydrodynamic\nstability. 2. Eigenvalue instability predicted by high Reynolds number\nlinearized Navier-Stokes equations cannot capture the dominant instability of\nsuper fast growth. 3. As equations for directional differentials, Rayleigh\nequation and Orr-Sommerfeld equation cannot capture the nature of the full\ndifferentials. \n\n"}
{"id": "1711.01206", "contents": "Title: Robust Decoding from 1-Bit Compressive Sampling with Least Squares Abstract: In 1-bit compressive sensing (1-bit CS) where target signal is coded into a\nbinary measurement, one goal is to recover the signal from noisy and quantized\nsamples. Mathematically, the 1-bit CS model reads: $y = \\eta \\odot\\textrm{sign}\n(\\Psi x^* + \\epsilon)$, where $x^{*}\\in \\mathcal{R}^{n}, y\\in \\mathcal{R}^{m}$,\n$\\Psi \\in \\mathcal{R}^{m\\times n}$, and $\\epsilon$ is the random error before\nquantization and $\\eta\\in \\mathcal{R}^{n}$ is a random vector modeling the sign\nflips. Due to the presence of nonlinearity, noise and sign flips, it is quite\nchallenging to decode from the 1-bit CS. In this paper, we consider least\nsquares approach under the over-determined and under-determined settings. For\n$m>n$, we show that, up to a constant $c$, with high probability, the least\nsquares solution $x_{\\textrm{ls}}$ approximates $ x^*$ with precision $\\delta$\nas long as $m \\geq\\widetilde{\\mathcal{O}}(\\frac{n}{\\delta^2})$. For $m< n$, we\nprove that, up to a constant $c$, with high probability, the\n$\\ell_1$-regularized least-squares solution $x_{\\ell_1}$ lies in the ball with\ncenter $x^*$ and radius $\\delta$ provided that $m \\geq \\mathcal{O}( \\frac{s\\log\nn}{\\delta^2})$ and $\\|x^*\\|_0 := s < m$. We introduce a Newton type method, the\nso-called primal and dual active set (PDAS) algorithm, to solve the nonsmooth\noptimization problem. The PDAS possesses the property of one-step convergence.\nIt only requires to solve a small least squares problem on the active set.\nTherefore, the PDAS is extremely efficient for recovering sparse signals\nthrough continuation. We propose a novel regularization parameter selection\nrule which does not introduce any extra computational overhead. Extensive\nnumerical experiments are presented to illustrate the robustness of our\nproposed model and the efficiency of our algorithm. \n\n"}
{"id": "1711.03954", "contents": "Title: EddyNet: A Deep Neural Network For Pixel-Wise Classification of Oceanic\n  Eddies Abstract: This work presents EddyNet, a deep learning based architecture for automated\neddy detection and classification from Sea Surface Height (SSH) maps provided\nby the Copernicus Marine and Environment Monitoring Service (CMEMS). EddyNet is\na U-Net like network that consists of a convolutional encoder-decoder followed\nby a pixel-wise classification layer. The output is a map with the same size of\nthe input where pixels have the following labels \\{'0': Non eddy, '1':\nanticyclonic eddy, '2': cyclonic eddy\\}. We investigate the use of SELU\nactivation function instead of the classical ReLU+BN and we use an overlap\nbased loss function instead of the cross entropy loss. Keras Python code, the\ntraining datasets and EddyNet weights files are open-source and freely\navailable on https://github.com/redouanelg/EddyNet. \n\n"}
{"id": "1711.04280", "contents": "Title: On the Sum of Order Statistics and Applications to Wireless\n  Communication Systems Performances Abstract: We consider the problem of evaluating the cumulative distribution function\n(CDF) of the sum of order statistics, which serves to compute outage\nprobability (OP) values at the output of generalized selection combining\nreceivers. Generally, closed-form expressions of the CDF of the sum of order\nstatistics are unavailable for many practical distributions. Moreover, the\nnaive Monte Carlo (MC) method requires a substantial computational effort when\nthe probability of interest is sufficiently small. In the region of small OP\nvalues, we propose instead two effective variance reduction techniques that\nyield a reliable estimate of the CDF with small computing cost. The first\nestimator, which can be viewed as an importance sampling estimator, has bounded\nrelative error under a certain assumption that is shown to hold for most of the\nchallenging distributions. An improvement of this estimator is then proposed\nfor the Pareto and the Weibull cases. The second is a conditional MC estimator\nthat achieves the bounded relative error property for the Generalized Gamma\ncase and the logarithmic efficiency in the Log-normal case. Finally, the\nefficiency of these estimators is compared via various numerical experiments. \n\n"}
{"id": "1711.05174", "contents": "Title: Near-Optimal Discrete Optimization for Experimental Design: A Regret\n  Minimization Approach Abstract: The experimental design problem concerns the selection of k points from a\npotentially large design pool of p-dimensional vectors, so as to maximize the\nstatistical efficiency regressed on the selected k design points. Statistical\nefficiency is measured by optimality criteria, including A(verage),\nD(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the\nT-optimality, exact optimization is NP-hard.\n  We propose a polynomial-time regret minimization framework to achieve a\n$(1+\\varepsilon)$ approximation with only $O(p/\\varepsilon^2)$ design points,\nfor all the optimality criteria above.\n  In contrast, to the best of our knowledge, before our work, no\npolynomial-time algorithm achieves $(1+\\varepsilon)$ approximations for\nD/E/G-optimality, and the best poly-time algorithm achieving\n$(1+\\varepsilon)$-approximation for A/V-optimality requires $k =\n\\Omega(p^2/\\varepsilon)$ design points. \n\n"}
{"id": "1711.05307", "contents": "Title: Neural Network Gradient Hamiltonian Monte Carlo Abstract: Hamiltonian Monte Carlo is a widely used algorithm for sampling from\nposterior distributions of complex Bayesian models. It can efficiently explore\nhigh-dimensional parameter spaces guided by simulated Hamiltonian flows.\nHowever, the algorithm requires repeated gradient calculations, and these\ncomputations become increasingly burdensome as data sets scale. We present a\nmethod to substantially reduce the computation burden by using a neural network\nto approximate the gradient. First, we prove that the proposed method still\nmaintains convergence to the true distribution though the approximated gradient\nno longer comes from a Hamiltonian system. Second, we conduct experiments on\nsynthetic examples and real data sets to validate the proposed method. \n\n"}
{"id": "1711.06719", "contents": "Title: Techniques for proving Asynchronous Convergence results for Markov Chain\n  Monte Carlo methods Abstract: Markov Chain Monte Carlo (MCMC) methods such as Gibbs sampling are finding\nwidespread use in applied statistics and machine learning. These often lead to\ndifficult computational problems, which are increasingly being solved on\nparallel and distributed systems such as compute clusters. Recent work has\nproposed running iterative algorithms such as gradient descent and MCMC in\nparallel asynchronously for increased performance, with good empirical results\nin certain problems. Unfortunately, for MCMC this parallelization technique\nrequires new convergence theory, as it has been explicitly demonstrated to lead\nto divergence on some examples. Recent theory on Asynchronous Gibbs sampling\ndescribes why these algorithms can fail, and provides a way to alter them to\nmake them converge. In this article, we describe how to apply this theory in a\ngeneric setting, to understand the asynchronous behavior of any MCMC algorithm,\nincluding those implemented using parameter servers, and those not based on\nGibbs sampling. \n\n"}
{"id": "1711.06771", "contents": "Title: Approximate Gradient Coding via Sparse Random Graphs Abstract: Distributed algorithms are often beset by the straggler effect, where the\nslowest compute nodes in the system dictate the overall running time.\nCoding-theoretic techniques have been recently proposed to mitigate stragglers\nvia algorithmic redundancy. Prior work in coded computation and gradient coding\nhas mainly focused on exact recovery of the desired output. However, slightly\ninexact solutions can be acceptable in applications that are robust to noise,\nsuch as model training via gradient-based algorithms. In this work, we present\ncomputationally simple gradient codes based on sparse graphs that guarantee\nfast and approximately accurate distributed computation. We demonstrate that\nsacrificing a small amount of accuracy can significantly increase algorithmic\nrobustness to stragglers. \n\n"}
{"id": "1711.07424", "contents": "Title: Informed proposals for local MCMC in discrete spaces Abstract: There is a lack of methodological results to design efficient Markov chain\nMonte Carlo (MCMC) algorithms for statistical models with discrete-valued\nhigh-dimensional parameters. Motivated by this consideration, we propose a\nsimple framework for the design of informed MCMC proposals (i.e.\nMetropolis-Hastings proposal distributions that appropriately incorporate local\ninformation about the target) which is naturally applicable to both discrete\nand continuous spaces. We explicitly characterize the class of optimal proposal\ndistributions under this framework, which we refer to as locally-balanced\nproposals, and prove their Peskun-optimality in high-dimensional regimes. The\nresulting algorithms are straightforward to implement in discrete spaces and\nprovide orders of magnitude improvements in efficiency compared to alternative\nMCMC schemes, including discrete versions of Hamiltonian Monte Carlo.\nSimulations are performed with both simulated and real datasets, including a\ndetailed application to Bayesian record linkage. A direct connection with\ngradient-based MCMC suggests that locally-balanced proposals may be seen as a\nnatural way to extend the latter to discrete spaces. \n\n"}
{"id": "1711.10062", "contents": "Title: Effect of turbulence on collisional growth of cloud droplets Abstract: We investigate the effect of turbulence on the collisional growth of um-sized\ndroplets through high- resolution numerical simulations with well resolved\nKolmogorov scales, assuming a collision and coalescence efficiency of unity.\nThe droplet dynamics and collisions are approximated using a superparticle\napproach. In the absence of gravity, we show that the time evolution of the\nshape of the droplet-size distribution due to turbulence-induced collisions\ndepends strongly on the turbulent energy-dissipation rate, but only weakly on\nthe Reynolds number. This can be explained through the energy dissipation rate\ndependence of the mean collision rate described by the Saffman-Turner collision\nmodel. Consistent with the Saffman-Turner collision model and its extensions,\nthe collision rate increases as the square root of the energy dissipation rate\neven when coalescence is invoked. The size distribution exhibits power law\nbehavior with a slope of -3.7 between a maximum at approximately 10 um up to\nabout 40 um. When gravity is invoked, turbulence is found to dominate the time\nevolution of an initially monodisperse droplet distribution at early times. At\nlater times, however, gravity takes over and dominates the collisional growth.\nWe find that the formation of large droplets is very sensitive to the turbulent\nenergy dissipation rate. This is due to the fact that turbulence enhances the\ncollisional growth between similar sized droplets at the early stage of\nraindrop formation. The mean collision rate grows exponentially, which is\nconsistent with the theoretical prediction of the continuous collisional growth\neven when turbulence-generated collisions are invoked. This consistency only\nreflects the mean effect of turbulence on collisional growth. \n\n"}
{"id": "1712.00182", "contents": "Title: Emulating satellite drag from large simulation experiments Abstract: Obtaining accurate estimates of satellite drag coefficients in low Earth\norbit is a crucial component in positioning and collision avoidance. Simulators\ncan produce accurate estimates, but their computational expense is much too\nlarge for real-time application. A pilot study showed that Gaussian process\n(GP) surrogate models could accurately emulate simulations. However, cubic\nruntime for training GPs means that they could only be applied to a narrow\nrange of input configurations to achieve the desired level of accuracy. In this\npaper we show how extensions to the local approximate Gaussian Process (laGP)\nmethod allow accurate full-scale emulation. The new methodological\ncontributions, which involve a multi-level global/local modeling approach, and\na set-wise approach to local subset selection, are shown to perform well in\nbenchmark and synthetic data settings. We conclude by demonstrating that our\nmethod achieves the desired level of accuracy, besting simpler viable (i.e.,\ncomputationally tractable) global and local modeling approaches, when trained\non seventy thousand core hours of drag simulations for two real-world\nsatellites: the Hubble space telescope (HST) and the gravity recovery and\nclimate experiment (GRACE). \n\n"}
{"id": "1712.02009", "contents": "Title: On the nonparametric maximum likelihood estimator for Gaussian location\n  mixture densities with application to Gaussian denoising Abstract: We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for\nestimating Gaussian location mixture densities in $d$-dimensions from\nindependent observations. Unlike usual likelihood-based methods for fitting\nmixtures, NPMLEs are based on convex optimization. We prove finite sample\nresults on the Hellinger accuracy of every NPMLE. Our results imply, in\nparticular, that every NPMLE achieves near parametric risk (up to logarithmic\nmultiplicative factors) when the true density is a discrete Gaussian mixture\nwithout any prior information on the number of mixture components. NPMLEs can\nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes\nestimator in the Gaussian denoising problem. We prove bounds for the accuracy\nof the empirical Bayes estimate as an approximation to the Oracle Bayes\nestimator. Here our results imply that the empirical Bayes estimator performs\nat nearly the optimal level (up to logarithmic multiplicative factors) for\ndenoising in clustering situations without any prior knowledge of the number of\nclusters. \n\n"}
{"id": "1712.03807", "contents": "Title: Continuous-discrete smoothing of diffusions Abstract: Suppose X is a multivariate diffusion process that is observed discretely in\ntime. At each observation time, a transformation of the state of the process is\nobserved with noise. The smoothing problem consists of recovering the path of\nthe process, consistent with the observations. We derive a novel Markov Chain\nMonte Carlo algorithm to sample from the exact smoothing distribution. The\nresulting algorithm is called the Backward Filtering Forward Guiding (BFFG)\nalgorithm. We extend the algorithm to include parameter estimation. The\nproposed method relies on guided proposals introduced in Schauer et al. (2017).\nWe illustrate its efficiency in a number of challenging problems. \n\n"}
{"id": "1712.03852", "contents": "Title: Fast nonparametric near-maximum likelihood estimation of a mixing\n  density Abstract: Mixture models are regularly used in density estimation applications, but the\nproblem of estimating the mixing distribution remains a challenge.\nNonparametric maximum likelihood produce estimates of the mixing distribution\nthat are discrete, and these may be hard to interpret when the true mixing\ndistribution is believed to have a smooth density. In this paper, we\ninvestigate an algorithm that produces a sequence of smooth estimates that has\nbeen conjectured to converge to the nonparametric maximum likelihood estimator.\nHere we give a rigorous proof of this conjecture, and propose a new data-driven\nstopping rule that produces smooth near-maximum likelihood estimates of the\nmixing density, and simulations demonstrate the quality empirical performance\nof this estimator. \n\n"}
{"id": "1712.04638", "contents": "Title: Barotropic theory for the velocity profile of Jupiter turbulent jets: an\n  example for an exact turbulent closure Abstract: We model the dynamics of Jupiter's jets by averaging the dynamics of eddies,\nin a barotropic beta-plane model, and explicitly predicting the balance between\nReynolds' stresses and dissipation, thus predicting the average velocity\nprofile explicitly.In order to obtain this result, we adopt a non-equilibrium\nstatistical mechanics approach. We consider a relevant limit for Jupiter\ntroposphere, of a time scale separation between inertial dynamics on one hand,\nand stochastic forcing and dissipation on the other hand. We assume that the\nforcing acts on scales much smaller than the jet scale, and we obtain a very\nsimple explicit relation between the Reynolds stress, the energy injection\nrate, and the average velocity shear, valid far from the jet edges (extrema of\nzonal velocity). A specific asymptotic expansion close to jet edges unravel an\nasymmetry between eastward and westward, velocity extrema. We recover Jupiter's\njet specificities: a cusp on eastward jets and a smooth parabola on westward\njets. \n\n"}
{"id": "1712.04723", "contents": "Title: Bayesian graphical compositional regression for microbiome data Abstract: An important task in microbiome studies is to test the existence of and give\ncharacterization to differences in the microbiome composition across groups of\nsamples. Important challenges of this problem include the large within-group\nheterogeneities among samples and the existence of potential confounding\nvariables that, when ignored, increase the chance of false discoveries and\nreduce the power for identifying true differences. We propose a probabilistic\nframework to overcome these issues by combining three ideas: (i) a phylogenetic\ntree-based decomposition of the cross-group comparison problem into a series of\nlocal tests, (ii) a graphical model that links the local tests to allow\ninformation sharing across taxa, and (iii) a Bayesian testing strategy that\nincorporates covariates and integrates out the within-group variation, avoiding\npotentially unstable point estimates. We derive an efficient inference\nalgorithm based on numerical integration and junction-tree message passing,\nconduct extensive simulation studies to investigate the performance of our\napproach, and compare it to state-of-the-art methods in a number of\nrepresentative settings. We then apply our method to the American Gut data to\nanalyze the association of dietary habits and human's gut microbiome\ncomposition in the presence of covariates, and illustrate the importance of\nincorporating covariates in microbiome cross-group comparison. \n\n"}
{"id": "1712.05460", "contents": "Title: Honey from the Hives: A Theoretical and Computational Exploration of\n  Combinatorial Hives Abstract: In the first half of this manuscript, we begin with a brief review of\ncombinatorial hives as introduced by Knutson and Tao, and focus on a conjecture\nby Danilov and Koshevoy for generating such a hive from Hermitian matrix pairs\nthrough an optimization scheme. We examine a proposal by Appleby and Whitehead\nin the spirit of this conjecture and analytically elucidate an obstruction in\ntheir construction for guaranteeing hive generation, while detailing stronger\nconditions under which we can produce hives with almost certain probability. We\nprovide the first mapping of this prescription onto a practical algorithmic\nspace that enables us to produce affirming computational results and open a new\narea of research into the analysis of the random geometries and curvatures of\nhive surfaces from select matrix ensembles.\n  The second part of this manuscript concerns Littlewood-Richardson\ncoefficients and methods of estimating them from the hive construction. We\nillustrate experimental confirmation of two numerical algorithms that we\nprovide as tools for the community: one as a rounded estimator on the\ncontinuous hive polytope volume following a proposal by Narayanan, and the\nother as a novel construction using a coordinate hit-and-run on the hive\nlattice itself. We compare the advantages of each, and include numerical\nresults on their accuracies for some tested cases. \n\n"}
{"id": "1712.05724", "contents": "Title: Is spontaneous generation of coherent baroclinic flows possible? Abstract: Geophysical turbulence is observed to self-organize into large-scale flows\nsuch as zonal jets and coherent vortices. Previous studies on barotropic\nbeta-plane turbulence have shown that coherent flows emerge out of a background\nof homogeneous turbulence as a bifurcation when the turbulence intensity\nincreases and the emergence of large scale flows has been attributed to a new\ntype of collective, symmetry breaking instability of the statistical state\ndynamics of the turbulent flow. In this work we extend the analysis to\nstratified flows and investigate turbulent self-organization in a two-layer\nfluid with no imposed mean north-south thermal gradient and turbulence\nsupported by an external random stirring. We use a second order closure of the\nstatistical state dynamics (S3T) with an appropriate averaging ansatz that\nallows the identification of statistical turbulent equilibria and their\nstructural stability. The bifurcation of the statistically homogeneous\nequilibrium state to inhomogeneous equilibrium states comprising of zonal jets\nand/or large scale waves when the energy input rate of the excitation passes a\ncritical threshold is analytically studied. The theory predicts that when the\nflow transitions to a statistical state with large-scale structures, these\nstates are barotropic if the scale of excitation is larger than the deformation\nradius. Mixed barotropic-baroclinic states with jets and/or waves arise when\nthe excitation is at scales shorter than the deformation radius with the\nbaroclinic component of the flow being subdominant for low energy input rates\nand non-significant for higher energy input rates. The results of the S3T\ntheory are compared to nonlinear simulations. The theory is found to accurately\npredict both the critical transition arameters and the scales of the emergent\nstructures but underestimates their amplitude. \n\n"}
{"id": "1712.08242", "contents": "Title: Exploring the Lyapunov instability properties of high-dimensional\n  atmospheric and climate models Abstract: The stability properties of intermediate-order climate models are\ninvestigated by computing their Lyapunov exponents (LEs). The two models\nconsidered are PUMA (Portable University Model of the Atmosphere), a\nprimitive-equation simple general circulation model, and MAOOAM (Modular\nArbitrary-Order Ocean-Atmosphere Model), a quasi-geostrophic coupled\nocean-atmosphere model on a beta-plane. We wish to investigate the effect of\nthe different levels of filtering on the instabilities and dynamics of the\natmospheric flows. Moreover, we assess the impact of the oceanic coupling, the\ndissipation scheme and the resolution on the spectra of LEs.\n  The PUMA Lyapunov spectrum is computed for two different values of the\nmeridional temperature gradient defining the Newtonian forcing. The increase of\nthe gradient gives rise to a higher baroclinicity and stronger instabilities,\ncorresponding to a larger dimension of the unstable manifold and a larger first\nLE. The convergence rate of the rate functional for the large deviation law of\nthe finite-time Lyapunov exponents (FTLEs) is fast for all exponents, which can\nbe interpreted as resulting from the absence of a clear-cut atmospheric\ntime-scale separation in such a model.\n  The MAOOAM spectra show that the dominant atmospheric instability is\ncorrectly represented even at low resolutions. However, the dynamics of the\ncentral manifold, which is mostly associated to the ocean dynamics, is not\nfully resolved because of its associated long time scales, even at intermediate\norders.\n  This paper highlights the need to investigate the natural variability of the\natmosphere-ocean coupled dynamics by associating rate of growth and decay of\nperturbations to the physical modes described using the formalism of the\ncovariant Lyapunov vectors and to consider long integrations in order to\ndisentangle the dynamical processes occurring at all time scales. \n\n"}
{"id": "1712.08929", "contents": "Title: Deterministic Sampling of Expensive Posteriors Using Minimum Energy\n  Designs Abstract: Markov chain Monte Carlo (MCMC) methods require a large number of samples to\napproximate a posterior distribution, which can be costly when the likelihood\nor prior is expensive to evaluate. The number of samples can be reduced if we\ncan avoid repeated samples and those that are close to each other. This is the\nidea behind deterministic sampling methods such as Quasi-Monte Carlo (QMC).\nHowever, the existing QMC methods aim at sampling from a uniform hypercube,\nwhich can miss the high probability regions of the posterior distribution and\nthus the approximation can be poor. Minimum energy design (MED) is a recently\nproposed deterministic sampling method, which makes use of the posterior\nevaluations to obtain a weighted space-filling design in the region of\ninterest. However, the existing implementation of MED is inefficient because it\nrequires several global optimizations and thus numerous evaluations of the\nposterior. In this article, we develop an efficient algorithm that can generate\nMED with few posterior evaluations. We also make several improvements to the\nMED criterion to make it perform better in high dimensions. The advantages of\nMED over MCMC and QMC are illustrated using an example of calibrating a\nfriction drilling process. \n\n"}
{"id": "1712.10204", "contents": "Title: Large-scale vorticity generation and kinetic energy budget along the\n  U.S. West Coast Abstract: We attempt to evaluate energy budget over a restricted but extremely well\nstudied oceanic region along the shorelines of Oregon and California. The\nanalysis is based on a recently updated geostrophic flow field data set\ncovering 22 years with daily resolution on a grid of\n0.25$^\\circ\\times$0.25$^\\circ$, and turbulent wind stress data from the\nERA-Interim reanalysis over the same geographic region with the same temporal\nand spatial resolutions. Integrated 2D kinetic energy, enstrophy, wind stress\nwork and { kinetic energy tendency} are determined separately for the shore-\nand open water regions. The empirical analysis is supported by 2D lattice\nBoltzmann simulations of freely decaying vortices along a rough solid wall,\nwhich permits to separate the pure shoreline effects and dissipation properties\nof surface flow fields. Comparisons clearly demonstrate that kinetic energy and\nvorticity { of the geostrophic flow field} are mostly generated along the\nshorelines and advected to the open water regions, where the net wind stress\nwork is almost negligible. Our results support that the geostrophic flow field\nis quasistationary on the timescale of a couple of days, thus total forcing is\npractically equal to total dissipation. Estimates of unknown terms in the\nequation of oceanic kinetic energy budget are based on other studies,\nnevertheless our results suggest that { an effective} eddy kinematic viscosity\nis in the order of magnitude $10^{-2}$ m$^2$/s along the shorelines, and it is\nlower { by a factor of two} in the open water region. \n\n"}
{"id": "1801.00319", "contents": "Title: An Additive Approximate Gaussian Process Model for Large Spatio-Temporal\n  Data Abstract: Motivated by a large ground-level ozone dataset, we propose a new\ncomputationally efficient additive approximate Gaussian process. The proposed\nmethod incorporates a computational-complexity-reduction method and a separable\ncovariance function, which can flexibly capture various spatio-temporal\ndependence structure. The first component is able to capture nonseparable\nspatio-temporal variability while the second component captures the separable\nvariation. Based on a hierarchical formulation of the model, we are able to\nutilize the computational advantages of both components and perform efficient\nBayesian inference. To demonstrate the inferential and computational benefits\nof the proposed method, we carry out extensive simulation studies assuming\nvarious scenarios of underlying spatio-temporal covariance structure. The\nproposed method is also applied to analyze large spatio-temporal measurements\nof ground-level ozone in the Eastern United States. \n\n"}
{"id": "1801.00718", "contents": "Title: Selective review of offline change point detection methods Abstract: This article presents a selective survey of algorithms for the offline\ndetection of multiple change points in multivariate time series. A general yet\nstructuring methodological strategy is adopted to organize this vast body of\nwork. More precisely, detection algorithms considered in this review are\ncharacterized by three elements: a cost function, a search method and a\nconstraint on the number of changes. Each of those elements is described,\nreviewed and discussed separately. Implementations of the main algorithms\ndescribed in this article are provided within a Python package called ruptures. \n\n"}
{"id": "1801.02106", "contents": "Title: Bayesian Lasso Posterior Sampling via Parallelized Measure Transport Abstract: It is well known that the Lasso can be interpreted as a Bayesian posterior\nmode estimate with a Laplacian prior. Obtaining samples from the full posterior\ndistribution, the Bayesian Lasso, confers major advantages in performance as\ncompared to having only the Lasso point estimate. Traditionally, the Bayesian\nLasso is implemented via Gibbs sampling methods which suffer from lack of\nscalability, unknown convergence rates, and generation of samples that are\nnecessarily correlated. We provide a measure transport approach to generate\ni.i.d samples from the posterior by constructing a transport map that\ntransforms a sample from the Laplacian prior into a sample from the posterior.\nWe show how the construction of this transport map can be parallelized into\nmodules that iteratively solve Lasso problems and perform closed-form linear\nalgebra updates. With this posterior sampling method, we perform maximum\nlikelihood estimation of the Lasso regularization parameter via the EM\nalgorithm. We provide comparisons to traditional Gibbs samplers using the\ndiabetes dataset of Efron et al. Lastly, we give an example implementation on a\ncomputing system that leverages parallelization, a graphics processing unit,\nwhose execution time has much less dependence on dimension as compared to a\nstandard implementation. \n\n"}
{"id": "1801.02309", "contents": "Title: Log-concave sampling: Metropolis-Hastings algorithms are fast Abstract: We consider the problem of sampling from a strongly log-concave density in\n$\\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of\nthe Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by\nsimulating a Markov chain obtained from the discretization of an appropriate\nLangevin diffusion, combined with an accept-reject step. Relative to known\nguarantees for the unadjusted Langevin algorithm (ULA), our bounds show that\nthe use of an accept-reject step in MALA leads to an exponentially improved\ndependence on the error-tolerance. Concretely, in order to obtain samples with\nTV error at most $\\delta$ for a density with condition number $\\kappa$, we show\nthat MALA requires $\\mathcal{O} \\big(\\kappa d \\log(1/\\delta) \\big)$ steps, as\ncompared to the $\\mathcal{O} \\big(\\kappa^2 d/\\delta^2 \\big)$ steps established\nin past work on ULA. We also demonstrate the gains of MALA over ULA for weakly\nlog-concave densities. Furthermore, we derive mixing time bounds for the\nMetropolized random walk (MRW) and obtain $\\mathcal{O}(\\kappa)$ mixing time\nslower than MALA. We provide numerical examples that support our theoretical\nfindings, and demonstrate the benefits of Metropolis-Hastings adjustment for\nLangevin-type sampling algorithms. \n\n"}
{"id": "1801.03612", "contents": "Title: Using probabilistic programs as proposals Abstract: Monte Carlo inference has asymptotic guarantees, but can be slow when using\ngeneric proposals. Handcrafted proposals that rely on user knowledge about the\nposterior distribution can be efficient, but are difficult to derive and\nimplement. This paper proposes to let users express their posterior knowledge\nin the form of proposal programs, which are samplers written in probabilistic\nprogramming languages. One strategy for writing good proposal programs is to\ncombine domain-specific heuristic algorithms with neural network models. The\nheuristics identify high probability regions, and the neural networks model the\nposterior uncertainty around the outputs of the algorithm. Proposal programs\ncan be used as proposal distributions in importance sampling and\nMetropolis-Hastings samplers without sacrificing asymptotic consistency, and\ncan be optimized offline using inference compilation. Support for optimizing\nand using proposal programs is easily implemented in a sampling-based\nprobabilistic programming runtime. The paper illustrates the proposed technique\nwith a proposal program that combines RANSAC and neural networks to accelerate\ninference in a Bayesian linear regression with outliers model. \n\n"}
{"id": "1801.04153", "contents": "Title: Bayesian Quadrature for Multiple Related Integrals Abstract: Bayesian probabilistic numerical methods are a set of tools providing\nposterior distributions on the output of numerical methods. The use of these\nmethods is usually motivated by the fact that they can represent our\nuncertainty due to incomplete/finite information about the continuous\nmathematical problem being approximated. In this paper, we demonstrate that\nthis paradigm can provide additional advantages, such as the possibility of\ntransferring information between several numerical methods. This allows users\nto represent uncertainty in a more faithful manner and, as a by-product,\nprovide increased numerical efficiency. We propose the first such numerical\nmethod by extending the well-known Bayesian quadrature algorithm to the case\nwhere we are interested in computing the integral of several related functions.\nWe then prove convergence rates for the method in the well-specified and\nmisspecified cases, and demonstrate its efficiency in the context of\nmulti-fidelity models for complex engineering systems and a problem of global\nillumination in computer graphics. \n\n"}
{"id": "1801.07856", "contents": "Title: Random matrix theory for an adiabatically-varying oceanic acoustic\n  waveguide Abstract: Problem of sound propagation in the ocean is considered. A novel approach of\nK. Hegewisch and S. Tomsovic for statistical modelling of acoustic wavefields\nin the random ocean is examined. The approach is based on construction of a\nwavefield propagator by means of random matrix theory. It is shown that this\napproach can be generalized onto acoustic waveguides with adiabatic\nlongitudinal variations. Efficient generalization is obtained by means of\nstepwise approximation of the propagator. Accuracy of the generalized approach\nis confirmed numerically for a model of an underwater sound channel crossing a\ncold synoptic eddy. It is found that the eddy leads to substantial suppression\nof sound scattering. \n\n"}
{"id": "1801.07873", "contents": "Title: Gaussian variational approximation for high-dimensional state space\n  models Abstract: Our article considers a Gaussian variational approximation of the posterior\ndensity in a high-dimensional state space model. The variational parameters to\nbe optimized are the mean vector and the covariance matrix of the\napproximation. The number of parameters in the covariance matrix grows as the\nsquare of the number of model parameters, so it is necessary to find simple yet\neffective parameterizations of the covariance structure when the number of\nmodel parameters is large. We approximate the joint posterior distribution over\nthe high-dimensional state vectors by a dynamic factor model, having Markovian\ntime dependence and a factor covariance structure for the states. This gives a\nreduced description of the dependence structure for the states, as well as a\ntemporal conditional independence structure similar to that in the true\nposterior. The usefulness of the approach is illustrated for prediction in two\nhigh-dimensional applications that are challenging for Markov chain Monte Carlo\nsampling. The first is a spatio-temporal model for the spread of the Eurasian\nCollared-Dove across North America; the second is a Wishart-based multivariate\nstochastic volatility model for financial returns. \n\n"}
{"id": "1801.08227", "contents": "Title: Matrix Completion with Nonconvex Regularization: Spectral Operators and\n  Scalable Algorithms Abstract: In this paper, we study the popularly dubbed matrix completion problem, where\nthe task is to \"fill in\" the unobserved entries of a matrix from a small subset\nof observed entries, under the assumption that the underlying matrix is of\nlow-rank. Our contributions herein, enhance our prior work on nuclear norm\nregularized problems for matrix completion (Mazumder et al., 2010) by\nincorporating a continuum of nonconvex penalty functions between the convex\nnuclear norm and nonconvex rank functions. Inspired by SOFT-IMPUTE (Mazumder et\nal., 2010; Hastie et al., 2016), we propose NC-IMPUTE- an EM-flavored\nalgorithmic framework for computing a family of nonconvex penalized matrix\ncompletion problems with warm-starts. We present a systematic study of the\nassociated spectral thresholding operators, which play an important role in the\noverall algorithm. We study convergence properties of the algorithm. Using\nstructured low-rank SVD computations, we demonstrate the computational\nscalability of our proposal for problems up to the Netflix size (approximately,\na $500,000 \\times 20, 000$ matrix with $10^8$ observed entries). We demonstrate\nthat on a wide range of synthetic and real data instances, our proposed\nnonconvex regularization framework leads to low-rank solutions with better\npredictive performance when compared to those obtained from nuclear norm\nproblems. Implementations of algorithms proposed herein, written in the R\nprogramming language, are made available on github. \n\n"}
{"id": "1801.08238", "contents": "Title: Bernhard Haurwitz Memorial Lecture (2017): Potential Vorticity Aspects\n  of Tropical Dynamics Abstract: This paper is the textual material accompanying the 2017 Bernhard Haurwitz\nMemorial Lecture, delivered by the author on 28 June 2017, at a joint session\nof the American Meteorological Society's 21st Conference on Atmospheric and\nOceanic Fluid Dynamics and 19th Conference on Middle Atmosphere (26-30 June\n2017, Portland, OR). \n\n"}
{"id": "1801.09141", "contents": "Title: Identification of multiple hard X-ray sources in solar flares: A\n  Bayesian analysis of the February 20 2002 event Abstract: The hard X-ray emission in a solar flare is typically characterized by a\nnumber of discrete sources, each with its own spectral, temporal, and spatial\nvariability. Establishing the relationship amongst these sources is critical to\ndetermine the role of each in the energy release and transport processes that\noccur within the flare. In this paper we present a novel method to identify and\ncharacterize each source of hard X-ray emission. The method permits a\nquantitative determination of the most likely number of subsources present, and\nof the relative probabilities that the hard X-ray emission in a given subregion\nof the flare is represented by a complicated multiple source structure or by a\nsimpler single source. We apply the method to a well-studied flare on\n2002~February~20 in order to assess competing claims as to the number of\nchromospheric footpoint sources present, and hence to the complexity of the\nunderlying magnetic geometry/toplogy. Contrary to previous claims of the need\nfor multiple sources to account for the chromospheric hard X-ray emission at\ndifferent locations and times, we find that a simple\ntwo-footpoint-plus-coronal-source model is the most probable explanation for\nthe data. We also find that one of the footpoint sources moves quite rapidly\nthroughout the event, a factor that presumably complicated previous analyses.\nThe inferred velocity of the footpoint corresponds to a very high induced\nelectric field, compatible with those in thin reconnecting current sheets. \n\n"}
{"id": "1802.02538", "contents": "Title: Yes, but Did It Work?: Evaluating Variational Inference Abstract: While it's always possible to compute a variational approximation to a\nposterior distribution, it can be difficult to discover problems with this\napproximation. We propose two diagnostic algorithms to alleviate this problem.\nThe Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of\nfit measurement for joint distributions, while simultaneously improving the\nerror in the estimate. The variational simulation-based calibration (VSBC)\nassesses the average performance of point estimates. \n\n"}
{"id": "1802.02548", "contents": "Title: Predicting Hurricane Trajectories using a Recurrent Neural Network Abstract: Hurricanes are cyclones circulating about a defined center whose closed wind\nspeeds exceed 75 mph originating over tropical and subtropical waters. At\nlandfall, hurricanes can result in severe disasters. The accuracy of predicting\ntheir trajectory paths is critical to reduce economic loss and save human\nlives. Given the complexity and nonlinearity of weather data, a recurrent\nneural network (RNN) could be beneficial in modeling hurricane behavior. We\npropose the application of a fully connected RNN to predict the trajectory of\nhurricanes. We employed the RNN over a fine grid to reduce typical truncation\nerrors. We utilized their latitude, longitude, wind speed, and pressure\npublicly provided by the National Hurricane Center (NHC) to predict the\ntrajectory of a hurricane at 6-hour intervals. Results show that this proposed\ntechnique is competitive to methods currently employed by the NHC and can\npredict up to approximately 120 hours of hurricane path. \n\n"}
{"id": "1802.02695", "contents": "Title: The physics of climate change: simple models in climate science Abstract: There is a perception that climate science can only be approached with\ncomplex computer simulations. But working climate scientists often use simple\nmodels to understand their simulations and make order-of-magnitude estimates.\nThis article presents some of these simple models with the goal of making\nclimate science more accessible and comprehensible. \n\n"}
{"id": "1802.02743", "contents": "Title: Crossroads of the mesoscale circulation Abstract: Quantifying the mechanisms of tracer dispersion in the ocean remains a\ncentral question in oceanography, for problems ranging from nutrient delivery\nto phytoplankton, to the early detection of contaminants. Most analyses have\nbeen based on Lagrangian concepts of transport, focusing on the identification\nof features minimizing fluid exchange among regions, or more recently on\nnetwork tools which focus on connectivity and transport pathways. Neither of\nthese approaches allows ranking the geographical sites of major water passage\nand selecting them so that they monitor waters coming from separate parts of\nthe ocean. These are instead key criteria when deploying an observing network.\nHere we address this issue by estimating at any point the extent of the ocean\nsurface which transits through it in a given time window. With such information\nwe are able to rank the sites with major fluxes that intercept waters\noriginating from different regions. We show that this allows us to optimize an\nobserving network, where a set of sampling sites can be chosen for monitoring\nthe largest flux of water dispersing out of a given region. When the analysis\nis performed backward in time, this method allows us to identify the major\nsources which feed a target region. The method is first applied to a\nminimalistic model of a mesoscale eddy field, and then to realistic\nsatellite-derived ocean currents in the Kerguelen area. In this region we\nidentify the optimal location of fixed stations capable of intercepting the\ntrajectories of 43 surface drifters, along with statistics on the temporal\npersistence of the stations determined in this way. We then identify possible\nhotspots of micro-nutrient enrichment for the recurrent spring phytoplanktonic\nbloom occuring here. Promising applications to other fields, such as larval\nconnectivity, marine spatial planning or contaminant detection, are then\ndiscussed. \n\n"}
{"id": "1802.03653", "contents": "Title: On Symplectic Optimization Abstract: Accelerated gradient methods have had significant impact in machine learning\n-- in particular the theoretical side of machine learning -- due to their\nability to achieve oracle lower bounds. But their heuristic construction has\nhindered their full integration into the practical machine-learning algorithmic\ntoolbox, and has limited their scope. In this paper we build on recent work\nwhich casts acceleration as a phenomenon best explained in continuous time, and\nwe augment that picture by providing a systematic methodology for converting\ncontinuous-time dynamics into discrete-time algorithms while retaining oracle\nrates. Our framework is based on ideas from Hamiltonian dynamical systems and\nsymplectic integration. These ideas have had major impact in many areas in\napplied mathematics, but have not yet been seen to have a relationship with\noptimization. \n\n"}
{"id": "1802.04686", "contents": "Title: Remote sensing of geomagnetic fields and atomic collisions in the\n  mesosphere Abstract: Magnetic-field sensing has contributed to the formulation of the\nplate-tectonics theory, the discovery and mapping of underground structures on\nEarth, and the study of magnetism in other planets. Filling the gap between\nspace-based and near-Earth observation, we demonstrate a novel method for\nremote measurement of the geomagnetic field at an altitude of 85-100 km. The\nmethod consists of optical pumping of atomic sodium in the upper mesosphere\nwith an intensity-modulated laser beam, and simultaneous ground-based\nobservation of the resultant magneto-optical resonance when driving the\natomic-sodium spins at the Larmor precession frequency. The experiment was\ncarried out at the Roque de Los Muchachos Observatory in La Palma (Canary\nIslands) where we validated this technique and remotely measured the Larmor\nprecession frequency of sodium as 260.4(1) kHz, corresponding to a mesospheric\nmagnetic field of 0.3720(1) G. We demonstrate a magnetometry accuracy level of\n0.28 mG/$\\sqrt{\\text{Hz}}$ in good atmospheric conditions. In addition, these\nobservations allow us to characterize various atomic-collision processes in the\nmesosphere. Remote detection of mesospheric magnetic fields has potential\napplications such as mapping of large-scale magnetic structures in the\nlithosphere and the study of electric-current fluctuations in the ionosphere. \n\n"}
{"id": "1802.06966", "contents": "Title: Computing the Cumulative Distribution Function and Quantiles of the\n  One-sided Kolmogorov-Smirnov Statistic Abstract: The cumulative distribution and quantile functions for the one-sided one\nsample Kolmogorov-Smirnov probability distributions are used for\ngoodness-of-fit testing. While the Smirnov-Birnbaum-Tingey formula for the CDF\nappears straight forward, its numerical evaluation generates intermediate\nresults spanning many hundreds of orders of magnitude and at times requires\nvery precise accurate representations. Computing the quantile function for any\nspecific probability may require evaluating both the CDF and its derivative,\nboth of which are computationally expensive. To work around avoid these issues,\ndifferent algorithms can be used across different parts of the domain, and\napproximations can be used to reduce the computational requirements. We show\nhere that straight forward implementation incurs accuracy loss for sample sizes\nof well under 1000. Further the approximations in use inside the open source\nSciPy python software often result in increased computation, not just reduced\naccuracy, and at times suffer catastrophic loss of accuracy for any sample\nsize. Then we provide alternate algorithms which restore accuracy and\nefficiency across the whole domain. \n\n"}
{"id": "1802.07148", "contents": "Title: Correlated pseudo-marginal schemes for time-discretised stochastic\n  kinetic models Abstract: The challenging problem of conducting fully Bayesian inference for the\nreaction rate constants governing stochastic kinetic models (SKMs) is\nconsidered. Given the challenges underlying this problem, the Markov jump\nprocess representation is routinely replaced by an approximation based on a\nsuitable time discretisation of the system of interest. Improving the accuracy\nof these schemes amounts to using an ever finer discretisation level, which in\nthe context of the inference problem, requires integrating over the uncertainty\nin the process at a predetermined number of intermediate times between\nobservations. Pseudo-marginal Metropolis-Hastings schemes are increasingly\nused, since for a given discretisation level, the observed data likelihood can\nbe unbiasedly estimated using a particle filter. When observations are\nparticularly informative an auxiliary particle filter can be implemented, by\nemploying an appropriate construct to push the state particles towards the\nobservations in a sensible way. Recent work in state-space settings has shown\nhow the pseudo-marginal approach can be made much more efficient by correlating\nthe underlying pseudo-random numbers used to form the likelihood estimate at\nthe current and proposed values of the unknown parameters. We extend this\napproach to the time-discretised SKM framework by correlating the innovations\nthat drive the auxiliary particle filter. We find that the resulting approach\noffers substantial gains in efficiency over a standard implementation. \n\n"}
{"id": "1802.08318", "contents": "Title: Proportional Volume Sampling and Approximation Algorithms for A-Optimal\n  Design Abstract: We study the optimal design problems where the goal is to choose a set of\nlinear measurements to obtain the most accurate estimate of an unknown vector\nin $d$ dimensions. We study the $A$-optimal design variant where the objective\nis to minimize the average variance of the error in the maximum likelihood\nestimate of the vector being measured. The problem also finds applications in\nsensor placement in wireless networks, sparse least squares regression, feature\nselection for $k$-means clustering, and matrix approximation. In this paper, we\nintroduce proportional volume sampling to obtain improved approximation\nalgorithms for $A$-optimal design. Our main result is to obtain improved\napproximation algorithms for the $A$-optimal design problem by introducing the\nproportional volume sampling algorithm. Our results nearly optimal bounds in\nthe asymptotic regime when the number of measurements done, $k$, is\nsignificantly more than the dimension $d$. We also give first approximation\nalgorithms when $k$ is small including when $k=d$. The proportional\nvolume-sampling algorithm also gives approximation algorithms for other optimal\ndesign objectives such as $D$-optimal design and generalized ratio objective\nmatching or improving previous best known results. Interestingly, we show that\na similar guarantee cannot be obtained for the $E$-optimal design problem. We\nalso show that the $A$-optimal design problem is NP-hard to approximate within\na fixed constant when $k=d$. \n\n"}
{"id": "1802.08798", "contents": "Title: Automatic adaptation of MCMC algorithms Abstract: Markov chain Monte Carlo (MCMC) methods are ubiquitous tools for\nsimulation-based inference in many fields but designing and identifying good\nMCMC samplers is still an open question. This paper introduces a novel MCMC\nalgorithm, namely, Auto Adapt MCMC. For sampling variables or blocks of\nvariables, we use two levels of adaptation where the inner adaptation optimizes\nthe MCMC performance within each sampler, while the outer adaptation explores\nthe valid space of kernels to find the optimal samplers. We provide a\ntheoretical foundation for our approach. To show the generality and usefulness\nof the approach, we describe a framework using only standard MCMC samplers as\ncandidate samplers and some adaptation schemes for both inner and outer\niterations. In several benchmark problems, we show that our proposed approach\nsubstantially outperforms other approaches, including an automatic blocking\nalgorithm, in terms of MCMC efficiency and computational time. \n\n"}
{"id": "1802.08898", "contents": "Title: Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo Abstract: Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from\nhigh-dimensional distributions in Statistics and Machine learning. HMC is known\nto run very efficiently in practice and its popular second-order \"leapfrog\"\nimplementation has long been conjectured to run in $d^{1/4}$ gradient\nevaluations. Here we show that this conjecture is true when sampling from\nstrongly log-concave target distributions that satisfy a weak third-order\nregularity property associated with the input data. Our regularity condition is\nweaker than the Lipschitz Hessian property and allows us to show faster\nconvergence bounds for a much larger class of distributions than would be\npossible with the usual Lipschitz Hessian constant alone. Important\ndistributions that satisfy our regularity condition include posterior\ndistributions used in Bayesian logistic regression for which the data satisfies\nan \"incoherence\" property. Our result compares favorably with the best\navailable bounds for the class of strongly log-concave distributions, which\ngrow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our\nsimulations on synthetic data suggest that, when our regularity condition is\nsatisfied, leapfrog HMC performs better than its competitors -- both in terms\nof accuracy and in terms of the number of gradient evaluations it requires. \n\n"}
{"id": "1802.08901", "contents": "Title: A quasi-physical dynamic reduced order model for thermospheric mass\n  density via Hermitian Space Dynamic Mode Decomposition Abstract: Thermospheric mass density is a major driver of satellite drag, the largest\nsource of uncertainty in accurately predicting the orbit of satellites in low\nEarth orbit (LEO) pertinent to space situational awareness. Most existing\nmodels for thermosphere are either physics-based or empirical. Physics-based\nmodels offer the potential for good predictive/forecast capabilities but\nrequire dedicated parallel resources for real-time evaluation and data\nassimilative capabilities that have yet to be developed. Empirical models are\nfast to evaluate, but offer very limited forecasting abilities. This paper\npresents a methodology of developing a reduced-order dynamic model from\nhigh-dimensional physics-based models by capturing the underlying dynamical\nbehavior. This work develops a quasi-physical reduced order model (ROM) for\nthermospheric mass density using simulated output from NCAR's\nThermosphere-Ionosphere-Electrodynamics General Circular Model (TIE-GCM). The\nROM is derived using a dynamic system formulation from a large dataset of\nTIE-GCM simulations spanning 12 years and covering a complete solar cycle.\nTowards this end, a new reduced order modeling approach, based on Dynamic Mode\nDecomposition with control (DMDc), that uses the Hermitian space of the problem\nto derive the dynamics and input matrices in a tractable manner is developed.\nResults show that the ROM performs well in serving as a reduced order surrogate\nfor TIE-GCM while almost always maintaining the forecast error to within 5\\% of\nthe simulated densities after 24 hours. \n\n"}
{"id": "1802.09188", "contents": "Title: Analysis of Langevin Monte Carlo via convex optimization Abstract: In this paper, we provide new insights on the Unadjusted Langevin Algorithm.\nWe show that this method can be formulated as a first order optimization\nalgorithm of an objective functional defined on the Wasserstein space of order\n$2$. Using this interpretation and techniques borrowed from convex\noptimization, we give a non-asymptotic analysis of this method to sample from\nlogconcave smooth target distribution on $\\mathbb{R}^d$. Based on this\ninterpretation, we propose two new methods for sampling from a non-smooth\ntarget distribution, which we analyze as well. Besides, these new algorithms\nare natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD)\nalgorithm, which is a popular extension of the Unadjusted Langevin Algorithm.\nSimilar to SGLD, they only rely on approximations of the gradient of the target\nlog density and can be used for large-scale Bayesian inference. \n\n"}
{"id": "1802.09720", "contents": "Title: Overview of Approximate Bayesian Computation Abstract: This Chapter, \"Overview of Approximate Bayesian Computation\", is to appear as\nthe first chapter in the forthcoming Handbook of Approximate Bayesian\nComputation (2018). It details the main ideas and concepts behind ABC methods\nwith many examples and illustrations. \n\n"}
{"id": "1802.09782", "contents": "Title: Potential energy surface, dipole moment surface and the intensity\n  calculations for the 10 micron, 5 micron, and 3 micron bands of ozone Abstract: Monitoring ozone concentrations in the Earth's atmosphere using spectroscopic\nmethods is a major activity which undertaken both from the ground and from\nspace. However there are long-running issues of consistency between\nmeasurements made at infrared (IR) and ultraviolet (UV) wavelengths. In\naddition, key O$_3$ IR bands at 10 \\muu, 5 \\muu\\ and 3 \\muu\\ also yield results\nwhich differ by a few percent when used for retrievals. These problems stem\nfrom the underlying laboratory measurements of the line intensities. Here we\nuse quantum chemical techniques, first principles electronic structure and\nvariational nuclear-motion calculations, to address this problem. A new\nhigh-accuracy \\ai\\ dipole moment surface (DMS) is computed. Several\nspectroscopically-determined potential energy surfaces (PESs) are constructed\nby fitting to empirical energy levels in the region below 7000 \\cm\\ starting\nfrom an \\ai\\ PES. Nuclear motion calculations using these new surfaces allow\nthe unambiguous determination of the intensities of 10 \\muu\\ band transitions,\nand the computation of the intensities of 10 \\muu\\ and 5 \\muu\\ bands within\ntheir experimental error. A decrease in intensities within the 3 \\muu\\ is\npredicted which appears consistent with atmospheric retrievals. The PES and DMS\nform a suitable starting point both for the computation of comprehensive ozone\nline lists and for future calculations of electronic transition intensities \n\n"}
{"id": "1803.00426", "contents": "Title: Computing the Cumulative Distribution Function and Quantiles of the\n  limit of the Two-sided Kolmogorov-Smirnov Statistic Abstract: The cumulative distribution and quantile functions for the two-sided one\nsample Kolmogorov-Smirnov probability distributions are used for\ngoodness-of-fit testing. The CDF is notoriously difficult to explicitly\ndescribe and to compute, and for large sample size use of the limiting\ndistribution is an attractive alternative, with its lower computational\nrequirements. No closed form solution for the computation of the quantiles is\nknown. Computing the quantile function by a numeric root-finder for any\nspecific probability may require multiple evaluations of both the CDF and its\nderivative. Approximations to both the CDF and its derivative can be used to\nreduce the computational demands. We show that the approximations in use inside\nthe open source SciPy python software result in increased computation, not just\nreduced accuracy, and cause convergence failures in the root-finding. Then we\nprovide alternate algorithms which restore accuracy and efficiency across the\nwhole domain. \n\n"}
{"id": "1803.00847", "contents": "Title: Vertically Sheared Horizontal Flow-Forming Instability in Stratified\n  Turbulence: Analytical Linear Stability Analysis of Statistical State\n  Dynamics Equilibria Abstract: Vertically banded zonal jets are frequently observed in weakly or\nnon-rotating stratified turbulence, with the quasi-biennial oscillation in the\nequatorial stratosphere and the ocean's equatorial deep jets being two\nexamples. Explaining the formation of jets in stratified turbulence is a\nfundamental problem in geophysical fluid dynamics. Statistical state dynamics\n(SSD) provides powerful methods for analyzing turbulent systems exhibiting\nemergent organization, such as banded jets. In SSD, dynamical equations are\nwritten directly for the evolution of the turbulence statistics, enabling\ndirect analysis of the statistical interactions between the incoherent\ncomponent of the turbulence and the coherent large-scale structure component\nthat underlie jet formation. A second-order closure of SSD, known as S3T, has\npreviously been applied to show that meridionally banded jets emerge in\nbarotropic beta-plane turbulence via a statistical instability referred to as\nthe zonostrophic instability. Two-dimensional Boussinesq turbulence provides a\nsimple model of non-rotating stratified turbulence analogous to the beta-plane\nmodel of planetary turbulence. Jets known as vertically sheared horizontal\nflows (VSHFs) often emerge in simulations of Boussinesq turbulence, but their\ndynamics is not yet clearly understood. In this work S3T analysis of the\nzonostrophic instability is extended to study VSHF emergence in two-dimensional\nBoussinesq turbulence using an analytical formulation of S3T amenable to\nperturbation stability analysis. VSHFs are shown to form via an instability\nthat is analogous in stratified turbulence to the zonostrophic instability in\nbeta-plane turbulence. This instability is shown to be strikingly similar to\nthe zonostrophic instability, suggesting that jet emergence in both geostrophic\nand non-rotating stratified turbulence may be understood as instances of the\nsame generic phenomenon. \n\n"}
{"id": "1803.01328", "contents": "Title: WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling Abstract: To train an inference network jointly with a deep generative topic model,\nmaking it both scalable to big corpora and fast in out-of-sample prediction, we\ndevelop Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet\nallocation, which infers posterior samples via a hybrid of stochastic-gradient\nMCMC and autoencoding variational Bayes. The generative network of WHAI has a\nhierarchy of gamma distributions, while the inference network of WHAI is a\nWeibull upward-downward variational autoencoder, which integrates a\ndeterministic-upward deep neural network, and a stochastic-downward deep\ngenerative model based on a hierarchy of Weibull distributions. The Weibull\ndistribution can be used to well approximate a gamma distribution with an\nanalytic Kullback-Leibler divergence, and has a simple reparameterization via\nthe uniform noise, which help efficiently compute the gradients of the evidence\nlower bound with respect to the parameters of the inference network. The\neffectiveness and efficiency of WHAI are illustrated with experiments on big\ncorpora. \n\n"}
{"id": "1803.02272", "contents": "Title: A geometric view of Biodiversity: scaling to metagenomics Abstract: We have designed a new efficient dimensionality reduction algorithm in order\nto investigate new ways of accurately characterizing the biodiversity, namely\nfrom a geometric point of view, scaling with large environmental sets produced\nby NGS ($\\sim 10^5$ sequences). The approach is based on Multidimensional\nScaling (MDS) that allows for mapping items on a set of $n$ points into a low\ndimensional euclidean space given the set of pairwise distances. We compute all\npairwise distances between reads in a given sample, run MDS on the distance\nmatrix, and analyze the projection on first axis, by visualization tools. We\nhave circumvented the quadratic complexity of computing pairwise distances by\nimplementing it on a hyperparallel computer (Turing, a Blue Gene Q), and the\ncubic complexity of the spectral decomposition by implementing a dense random\nprojection based algorithm. We have applied this data analysis scheme on a set\nof $10^5$ reads, which are amplicons of a diatom environmental sample from Lake\nGeneva. Analyzing the shape of the point cloud paves the way for a geometric\nanalysis of biodiversity, and for accurately building OTUs (Operational\nTaxonomic Units), when the data set is too large for implementing unsupervised,\nhierarchical, high-dimensional clustering. \n\n"}
{"id": "1803.03765", "contents": "Title: How to solve the stochastic partial differential equation that gives a\n  Mat\\'ern random field using the finite element method Abstract: This tutorial teaches parts of the finite element method (FEM), and solves a\nstochastic partial differential equation (SPDE). The contents herein are\nconsidered \"known\" in the numerics literature, but for statisticians it is very\ndifficult to find a resource for learning these ideas in a timely manner\n(without doing a year's worth of courses in numerics). The goal of this\ntutorial is to be pedagogical and explain the computations/theory to a\nstatistician. This is not a practical tutorial, there is little computer code,\nand no data analysis. \n\n"}
{"id": "1803.03858", "contents": "Title: Testing One Hypothesis Multiple Times: The Multidimensional Case Abstract: The identification of new rare signals in data, the detection of a sudden\nchange in a trend, and the selection of competing models, are among the most\nchallenging problems in statistical practice. These challenges can be tackled\nusing a test of hypothesis where a nuisance parameter is present only under the\nalternative, and a computationally efficient solution can be obtained by the\n\"Testing One Hypothesis Multiple times\" (TOHM) method. In the one-dimensional\nsetting, a fine discretization of the space of the non-identifiable parameter\nis specified, and a global p-value is obtained by approximating the\ndistribution of the supremum of the resulting stochastic process. In this\npaper, we propose a computationally efficient inferential tool to perform TOHM\nin the multidimensional setting. Here, the approximations of interest typically\ninvolve the expected Euler Characteristics (EC) of the excursion set of the\nunderlying random field. We introduce a simple algorithm to compute the EC in\nmultiple dimensions and for arbitrary large significance levels. This leads to\nan highly generalizable computational tool to perform inference under\nnon-standard regularity conditions. \n\n"}
{"id": "1803.04084", "contents": "Title: Link prediction for egocentrically sampled networks Abstract: Link prediction in networks is typically accomplished by estimating or\nranking the probabilities of edges for all pairs of nodes. In practice,\nespecially for social networks, the data are often collected by egocentric\nsampling, which means selecting a subset of nodes and recording all of their\nedges. This sampling mechanism requires different prediction tools than the\ntypical assumption of links missing at random. We propose a new computationally\nefficient link prediction algorithm for egocentrically sampled networks, which\nestimates the underlying probability matrix by estimating its row space. For\nnetworks created by sampling rows, our method outperforms many popular link\nprediction and graphon estimation techniques. \n\n"}
{"id": "1803.05554", "contents": "Title: Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models Abstract: Learning a Bayesian network (BN) from data can be useful for decision-making\nor discovering causal relationships. However, traditional methods often fail in\nmodern applications, which exhibit a larger number of observed variables than\ndata points. The resulting uncertainty about the underlying network as well as\nthe desire to incorporate prior information recommend a Bayesian approach to\nlearning the BN, but the highly combinatorial structure of BNs poses a striking\nchallenge for inference. The current state-of-the-art methods such as order\nMCMC are faster than previous methods but prevent the use of many natural\nstructural priors and still have running time exponential in the maximum\nindegree of the true directed acyclic graph (DAG) of the BN. We here propose an\nalternative posterior approximation based on the observation that, if we\nincorporate empirical conditional independence tests, we can focus on a\nhigh-probability DAG associated with each order of the vertices. We show that\nour method allows the desired flexibility in prior specification, removes\ntiming dependence on the maximum indegree and yields provably good posterior\napproximations; in addition, we show that it achieves superior accuracy,\nscalability, and sampler mixing on several datasets. \n\n"}
{"id": "1803.06277", "contents": "Title: Extreme Events: Mechanisms and Prediction Abstract: Extreme events, such as rogue waves, earthquakes and stock market crashes,\noccur spontaneously in many dynamical systems. Because of their usually adverse\nconsequences, quantification, prediction and mitigation of extreme events are\nhighly desirable. Here, we review several aspects of extreme events in\nphenomena described by high-dimensional, chaotic dynamical systems. We\nspecially focus on two pressing aspects of the problem: (i) Mechanisms\nunderlying the formation of extreme events and (ii) Real-time prediction of\nextreme events. For each aspect, we explore methods relying on models, data or\nboth. We discuss the strengths and limitations of each approach as well as\npossible future research directions. \n\n"}
{"id": "1803.06328", "contents": "Title: Nesting Probabilistic Programs Abstract: We formalize the notion of nesting probabilistic programming queries and\ninvestigate the resulting statistical implications. We demonstrate that while\nquery nesting allows the definition of models which could not otherwise be\nexpressed, such as those involving agents reasoning about other agents,\nexisting systems take approaches which lead to inconsistent estimates. We show\nhow to correct this by delineating possible ways one might want to nest queries\nand asserting the respective conditions required for convergence. We further\nintroduce a new online nested Monte Carlo estimator that makes it substantially\neasier to ensure these conditions are met, thereby providing a simple framework\nfor designing statistically correct inference engines. We prove the correctness\nof this online estimator and show that, when using the recommended setup, its\nasymptotic variance is always better than that of the equivalent fixed\nestimator, while its bias is always within a factor of two. \n\n"}
{"id": "1803.06518", "contents": "Title: Provable Convex Co-clustering of Tensors Abstract: Cluster analysis is a fundamental tool for pattern discovery of complex\nheterogeneous data. Prevalent clustering methods mainly focus on vector or\nmatrix-variate data and are not applicable to general-order tensors, which\narise frequently in modern scientific and business applications. Moreover,\nthere is a gap between statistical guarantees and computational efficiency for\nexisting tensor clustering solutions due to the nature of their non-convex\nformulations. In this work, we bridge this gap by developing a provable convex\nformulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator\nenjoys stability guarantees and its computational and storage costs are\npolynomial in the size of the data. We further establish a non-asymptotic error\nbound for the CoCo estimator, which reveals a surprising \"blessing of\ndimensionality\" phenomenon that does not exist in vector or matrix-variate\ncluster analysis. Our theoretical findings are supported by extensive simulated\nstudies. Finally, we apply the CoCo estimator to the cluster analysis of\nadvertisement click tensor data from a major online company. Our clustering\nresults provide meaningful business insights to improve advertising\neffectiveness. \n\n"}
{"id": "1803.07813", "contents": "Title: Introducing higher order correlations to marginals' subset of\n  multivariate data by means of Archimedean copulas Abstract: In this paper, we present the algorithm that alters the subset of marginals\nof multivariate standard distributed data into such modelled by an Archimedean\ncopula. Proposed algorithm leaves a correlation matrix almost unchanged, but\nintroduces a higher order correlation into a subset of marginals. Our data\ntransformation algorithm can be used to analyse whether particular machine\nlearning algorithm, especially a dimensionality reduction one, utilises higher\norder correlations or not. We present an exemplary application on two features\nselection algorithms, mention that features selection is one of the approaches\nto dimensionality reduction. To measure higher order correlation, we use\nmultivariate higher order cumulants, hence to utilises higher order\ncorrelations be to use the Joint Skewness Band Selection (JSBS) algorithm that\nuses third-order multivariate cumulant. We show the robust performance of the\nJSBS in contrary to the poor performance of the Maximum Ellipsoid Volume (MEV)\nalgorithm that does not utilise such higher order correlations. With this\nresult, we confirm the potential application of our data generation algorithm\nto analyse a performance of various dimensionality reduction algorithms. \n\n"}
{"id": "1803.07984", "contents": "Title: Online data assimilation in distributionally robust optimization Abstract: This paper considers a class of real-time decision making problems to\nminimize the expected value of a function that depends on a random variable\n$\\xi$ under an unknown distribution $\\mathbb{P}$. In this process, samples of\n$\\xi$ are collected sequentially in real time, and the decisions are made,\nusing the real-time data, to guarantee out-of-sample performance. We approach\nthis problem in a distributionally robust optimization framework and propose a\nnovel Online Data Assimilation Algorithm for this purpose. This algorithm\nguarantees the out-of-sample performance in high probability, and gradually\nimproves the quality of the data-driven decisions by incorporating the\nstreaming data. We show that the Online Data Assimilation Algorithm guarantees\nconvergence under the streaming data, and a criteria for termination of the\nalgorithm after certain number of data has been collected. \n\n"}
{"id": "1803.08021", "contents": "Title: Error Estimation for Randomized Least-Squares Algorithms via the\n  Bootstrap Abstract: Over the course of the past decade, a variety of randomized algorithms have\nbeen proposed for computing approximate least-squares (LS) solutions in\nlarge-scale settings. A longstanding practical issue is that, for any given\ninput, the user rarely knows the actual error of an approximate solution\n(relative to the exact solution). Likewise, it is difficult for the user to\nknow precisely how much computation is needed to achieve the desired error\ntolerance. Consequently, the user often appeals to worst-case error bounds that\ntend to offer only qualitative guidance. As a more practical alternative, we\npropose a bootstrap method to compute a posteriori error estimates for\nrandomized LS algorithms. These estimates permit the user to numerically assess\nthe error of a given solution, and to predict how much work is needed to\nimprove a \"preliminary\" solution. In addition, we provide theoretical\nconsistency results for the method, which are the first such results in this\ncontext (to the best of our knowledge). From a practical standpoint, the method\nalso has considerable flexibility, insofar as it can be applied to several\npopular sketching algorithms, as well as a variety of error metrics. Moreover,\nthe extra step of error estimation does not add much cost to an underlying\nsketching algorithm. Finally, we demonstrate the effectiveness of the method\nwith empirical results. \n\n"}
{"id": "1803.08424", "contents": "Title: Autoplotly - Automatic Generation of Interactive Visualizations for\n  Popular Statistical Results Abstract: The autoplotly package provides functionalities to automatically generate\ninteractive visualizations for many popular statistical results supported by\nggfortify package with plotly and ggplot2 style. The generated visualizations\ncan also be easily extended using ggplot2 and plotly syntax while staying\ninteractive. \n\n"}
{"id": "1803.08643", "contents": "Title: Numerical simulation of the geometrical-optics reduction of CE2 and\n  comparisons to quasilinear dynamics Abstract: Zonal flows have been observed to appear spontaneously from turbulence in a\nnumber of physical settings. A complete theory for their behavior is still\nlacking. Recently, a number of studies have investigated the dynamics of zonal\nflows using quasilinear theories and the statistical framework of a\nsecond-order cumulant expansion (CE2). A geometrical-optics (GO) reduction of\nCE2, derived under an assumption of separation of scales between the\nfluctuations and the zonal flow, is studied here numerically. The reduced\nmodel, CE2-GO, has a similar phase-space mathematical structure to the\ntraditional wave-kinetic equation, but that wave-kinetic equation has been\nshown to fail to preserve enstrophy conservation and to exhibit an ultraviolet\ncatastrophe. CE2-GO, in contrast, preserves nonlinear conservation of both\nenergy and enstrophy. We show here how to retain these conservation properties\nin a pseudospectral simulation of CE2-GO. We then present nonlinear simulations\nof CE2-GO and compare with direct simulations of quasilinear (QL) dynamics. We\nfind that CE2-GO retains some similarities to QL. The partitioning of energy\nthat resides in the zonal flow is in good quantitative agreement between CE2-GO\nand QL. On the other hand, the length scale of the zonal flow does not follow\nthe same qualitative trend in the two models. Overall, these simulations\nindicate that CE2-GO provides a simpler and more tractable statistical paradigm\nthan CE2, but CE2-GO is missing important physics. \n\n"}
{"id": "1803.10817", "contents": "Title: Wave kinetic equation in a nonstationary and inhomogeneous medium with a\n  weak quadratic nonlinearity Abstract: We present a systematic derivation of the wave kinetic equation describing\nthe dynamics of a statistically inhomogeneous incoherent wave field in a medium\nwith a weak quadratic nonlinearity. The medium can be nonstationary and\ninhomogeneous. Primarily based on the Weyl phase-space representation, our\nderivation assumes the standard geometrical-optics ordering and the quasinormal\napproximation for the statistical closure. The resulting wave kinetic equation\nsimultaneously captures the effects of the medium inhomogeneity (both in time\nand space) and of the nonlinear wave scattering. This general formalism can\nserve as a stepping stone for future studies of weak wave turbulence\ninteracting with mean fields in nonstationary and inhomogeneous media. \n\n"}
{"id": "1804.02274", "contents": "Title: Computationally efficient inference for latent position network models Abstract: Latent position models are widely used for the analysis of networks in a\nvariety of research fields. In fact, these models possess a number of desirable\ntheoretical properties, and are particularly easy to interpret. However,\nstatistical methodologies to fit these models generally incur a computational\ncost which grows with the square of the number of nodes in the graph. This\nmakes the analysis of large social networks impractical. In this paper, we\npropose a new method characterised by a much reduced computational complexity,\nwhich can be used to fit latent position models on networks of several tens of\nthousands nodes. Our approach relies on an approximation of the likelihood\nfunction, where the amount of noise introduced by the approximation can be\narbitrarily reduced at the expense of computational efficiency. We establish\nseveral theoretical results that show how the likelihood error propagates to\nthe invariant distribution of the Markov chain Monte Carlo sampler. In\nparticular, we demonstrate that one can achieve a substantial reduction in\ncomputing time and still obtain a good estimate of the latent structure.\nFinally, we propose applications of our method to simulated networks and to a\nlarge coauthorships network, highlighting the usefulness of our approach. \n\n"}
{"id": "1804.02655", "contents": "Title: Efficient Computational Algorithm for Optimal Continuous Experimental\n  Designs Abstract: A simple yet efficient computational algorithm for computing the continuous\noptimal experimental design for linear models is proposed. An alternative proof\nthe monotonic convergence for $D$-optimal criterion on continuous design spaces\nare provided. We further show that the proposed algorithm converges to the\n$D$-optimal design. We also provide an algorithm for the $A$-optimality and\nconjecture that the algorithm convergence monotonically on continuous design\nspaces. Different numerical examples are used to demonstrated the usefulness\nand performance of the proposed algorithms. \n\n"}
{"id": "1804.02719", "contents": "Title: Accelerating MCMC Algorithms Abstract: Markov chain Monte Carlo algorithms are used to simulate from complex\nstatistical distributions by way of a local exploration of these distributions.\nThis local feature avoids heavy requests on understanding the nature of the\ntarget, but it also potentially induces a lengthy exploration of this target,\nwith a requirement on the number of simulations that grows with the dimension\nof the problem and with the complexity of the data behind it. Several\ntechniques are available towards accelerating the convergence of these Monte\nCarlo algorithms, either at the exploration level (as in tempering, Hamiltonian\nMonte Carlo and partly deterministic methods) or at the exploitation level\n(with Rao-Blackwellisation and scalable methods). \n\n"}
{"id": "1804.03523", "contents": "Title: Hamiltonian Monte Carlo for Probabilistic Programs with Discontinuities Abstract: Hamiltonian Monte Carlo (HMC) is arguably the dominant statistical inference\nalgorithm used in most popular \"first-order differentiable\" Probabilistic\nProgramming Languages (PPLs). However, the fact that HMC uses derivative\ninformation causes complications when the target distribution is\nnon-differentiable with respect to one or more of the latent variables. In this\npaper, we show how to use extensions to HMC to perform inference in\nprobabilistic programs that contain discontinuities. To do this, we design a\nSimple first-order Probabilistic Programming Language (SPPL) that contains a\nsufficient set of language restrictions together with a compilation scheme.\nThis enables us to preserve both the statistical and syntactic interpretation\nof if-else statements in the probabilistic program, within the scope of\nfirst-order PPLs. We also provide a corresponding mathematical formalism that\nensures any joint density denoted in such a language has a suitably low measure\nof discontinuities. \n\n"}
{"id": "1804.04859", "contents": "Title: Infinite dimensional adaptive MCMC for Gaussian processes Abstract: Latent Gaussian processes are widely applied in many fields like, statistics,\ninverse problems and machine learning. A popular method for inference is\nthrough the posterior distribution, which is typically carried out by Markov\nChain Monte Carlo (MCMC) algorithms. Most Gaussian processes can be represented\nas a Gaussian measure in a infinite dimensional space. This is an issue for\nstandard algorithms as they break down in an infinite dimensional setting, thus\nthe need for appropriate infinite dimensional samplers for implementing\nprobabilistic inference in such framework. In this paper, we introduce several\nadaptive versions of the preconditioned Crank-Nicolson Langevin (pCNL)\nalgorithm, which can be viewed as an infinite dimensional version of the well\nknown Metropolis adjusted Langevin algorithm (MALA) algorithm for Gaussian\nprocesses. The basic premise for all our proposals lies in the idea of\nimplementing change of measure formulation to adapt the algorithms to greatly\nimprove their efficiency. A gradient-free version of pCNL is introduced, which\nis a hybrid of an adaptive independence sampler and an adaptive random walk\nsampler, and is shown to outperform the standard preconditioned Crank-Nicolson\n(pCN) scheme. Finally, we demonstrate the efficiency of our proposed algorithm\nfor three different statistical models. \n\n"}
{"id": "1804.05678", "contents": "Title: Sparse solutions in optimal control of PDEs with uncertain parameters:\n  the linear case Abstract: We study sparse solutions of optimal control problems governed by PDEs with\nuncertain coefficients. We propose two formulations, one where the solution is\na deterministic control optimizing the mean objective, and a formulation aiming\nat stochastic controls that share the same sparsity structure. In both\nformulations, regions where the controls do not vanish can be interpreted as\noptimal locations for placing control devices. In this paper, we focus on\nlinear PDEs with linearly entering uncertain parameters. Under these\nassumptions, the deterministic formulation reduces to a problem with known\nstructure, and thus we mainly focus on the stochastic control formulation.\nHere, shared sparsity is achieved by incorporating the $L^1$-norm of the mean\nof the pointwise squared controls in the objective. We reformulate the problem\nusing a norm reweighting function that is defined over physical space only and\nthus helps to avoid approximation of the random space using samples or\nquadrature. We show that a fixed point algorithm applied to the norm\nreweighting formulation leads to a variant of the well-studied iterative\nreweighted least squares (IRLS) algorithm, and we propose a novel\npreconditioned Newton-conjugate gradient method to speed up the IRLS algorithm.\nWe combine our algorithms with low-rank operator approximations, for which we\nprovide estimates of the truncation error. We carefully examine the\ncomputational complexity of the resulting algorithms. The sparsity structure of\nthe optimal controls and the performance of the solution algorithms are studied\nnumerically using control problems governed by the Laplace and Helmholtz\nequations. In these experiments the Newton variant clearly outperforms the IRLS\nmethod. \n\n"}
{"id": "1804.06406", "contents": "Title: nestcheck: diagnostic tests for nested sampling calculations Abstract: Nested sampling is an increasingly popular technique for Bayesian\ncomputation, in particular for multimodal, degenerate problems of moderate to\nhigh dimensionality. Without appropriate settings, however, nested sampling\nsoftware may fail to explore such posteriors correctly; for example producing\ncorrelated samples or missing important modes. This paper introduces new\ndiagnostic tests to assess the reliability both of parameter estimation and\nevidence calculations using nested sampling software, and demonstrates them\nempirically. We present two new diagnostic plots for nested sampling, and give\npractical advice for nested sampling software users in astronomy and beyond.\nOur diagnostic tests and diagrams are implemented in nestcheck: a publicly\navailable Python package for analysing nested sampling calculations, which is\ncompatible with output from MultiNest, PolyChord and dyPolyChord. \n\n"}
{"id": "1804.06742", "contents": "Title: An efficient open-source implementation to compute the Jacobian matrix\n  for the Newton-Raphson power flow algorithm Abstract: Power flow calculations for systems with a large number of buses, e.g. grids\nwith multiple voltage levels, or time series based calculations result in a\nhigh computational effort. A common power flow solver for the efficient\nanalysis of power systems is the Newton-Raphson algorithm. The main\ncomputational effort of this method results from the linearization of the\nnonlinear power flow problem and solving the resulting linear equation. This\npaper presents an algorithm for the fast linearization of the power flow\nproblem by creating the Jacobian matrix directly in CRS format. The increase in\nspeed is achieved by reducing the number of iterations over the nonzero\nelements of the sparse Jacobian matrix. This allows to efficiently create the\nJacobian matrix without having to approximate the problem. A comparison of the\ncalculation time of three power grids shows that comparable open-source\nimplementations need 3-14x the time to create the Jacobian matrix. \n\n"}
{"id": "1804.08536", "contents": "Title: Hamiltonian distributed chaos in Arctic and Antarctic Oscillations Abstract: The Arctic and Antarctic Oscillations (AO and AAO indices) are studied using\nthe Hamiltonian distributed chaos approach. Using the daily data (AO since\n1950y and AAO since 1979y) it is shown that the power spectra of the both AO\nand AAO indices exhibit the stretched exponential behaviour $E(f) \\propto\n\\exp-(f/f_0)^{3/4}$ corresponding to the Hamiltonian distributed chaos. The\ncharacteristic time scale for the both indices $T_0=1/f_0\\simeq 41$ day\ncorresponds to the well known from the numerous extratropic observations near\n40 day period. \n\n"}
{"id": "1804.08738", "contents": "Title: Bayesian Updating and Uncertainty Quantification using Sequential\n  Tempered MCMC with the Rank-One Modified Metropolis Algorithm Abstract: Bayesian methods are critical for quantifying the behaviors of systems. They\ncapture our uncertainty about a system's behavior using probability\ndistributions and update this understanding as new information becomes\navailable. Probabilistic predictions that incorporate this uncertainty can then\nbe made to evaluate system performance and make decisions. While Bayesian\nmethods are very useful, they are often computationally intensive. This\nnecessitates the development of more efficient algorithms. Here, we discuss a\ngroup of population Markov Chain Monte Carlo (MCMC) methods for Bayesian\nupdating and system reliability assessment that we call Sequential Tempered\nMCMC (ST-MCMC) algorithms. These algorithms combine 1) a notion of tempering to\ngradually transform a population of samples from the prior to the posterior\nthrough a series of intermediate distributions, 2) importance resampling, and\n3) MCMC. They are a form of Sequential Monte Carlo and include algorithms like\nTransitional Markov Chain Monte Carlo and Subset Simulation. We also introduce\na new sampling algorithm called the Rank-One Modified Metropolis Algorithm\n(ROMMA), which builds upon the Modified Metropolis Algorithm used within Subset\nSimulation to improve performance in high dimensions. Finally, we formulate a\nsingle algorithm to solve combined Bayesian updating and reliability assessment\nproblems to make posterior assessments of system reliability. The algorithms\nare then illustrated by performing prior and posterior reliability assessment\nof a water distribution system with unknown leaks and demands. \n\n"}
{"id": "1805.00397", "contents": "Title: Hamiltonian distributed chaos in the Asian-Australian Monsoons and in\n  the ENSO Abstract: Two subsystems of the Asian Monsoon: the Indian Summer Monsoon and the\nWestern North Pacific Monsoon, have been analysed using their daily indices\nISMI and WNPMI. It is shown that on the intraseasonal time scales the ISMI and\nWNPMI are dominated by the Hamiltonian distributed chaos with the stretched\nexponential spectrum: $E(f) \\propto \\exp-(f/f_0)^{\\beta}$ and analytical values\nof the parameter $\\beta =3/4$ and $\\beta =1/2$ correspondingly. The relevant\ndaily indices Ni\\~no 3 and Ni\\~no 4 (with $\\beta =1/2$) of the El\nNi\\~no-Southern Oscillation (ENSO) and the Australian Monsoon (AUSM index with\n$\\beta = 1/2$) have been also discussed in this context. \n\n"}
{"id": "1805.00541", "contents": "Title: Scalable Importance Tempering and Bayesian Variable Selection Abstract: We propose a Monte Carlo algorithm to sample from high dimensional\nprobability distributions that combines Markov chain Monte Carlo and importance\nsampling. We provide a careful theoretical analysis, including guarantees on\nrobustness to high dimensionality, explicit comparison with standard Markov\nchain Monte Carlo methods and illustrations of the potential improvements in\nefficiency. Simple and concrete intuition is provided for when the novel scheme\nis expected to outperform standard schemes. When applied to Bayesian\nvariable-selection problems, the novel algorithm is orders of magnitude more\nefficient than available alternative sampling schemes and enables fast and\nreliable fully Bayesian inferences with tens of thousand regressors. \n\n"}
{"id": "1805.00834", "contents": "Title: Reply to the Comments of Olivier Pauluis to the paper \"A Third-Law\n  Isentropic Analysis of a Simulated Hurricane\" Abstract: A careful reading of old articles puts Olivier Pauluis' criticisms concerning\nthe definition of isentropic processes in terms of a potential temperature\nclosely associated with the entropy of moist air, together with the third\nprinciple of thermodynamics, into perspective. \n\n"}
{"id": "1805.01596", "contents": "Title: Reduced-order modeling of fully turbulent buoyancy-driven flows using\n  the Green's function method Abstract: A One-Dimensional (1D) Reduced-Order Model (ROM) has been developed for a 3D\nRayleigh-B\\'enard convection system in the turbulent regime with Rayleigh\nnumber $\\mathrm{Ra}=10^6$. The state vector of the 1D ROM is horizontally\naveraged temperature. Using the Green's Function (GRF) method, which involves\napplying many localized, weak forcings to the system one at a time and\ncalculating the responses using long-time averaged Direct Numerical Simulations\n(DNS), the system's Linear Response Function (LRF) has been computed. Another\nmatrix, called the Eddy Flux Matrix (EFM), that relates changes in the\ndivergence of vertical eddy heat fluxes to changes in the state vector, has\nalso been calculated. Using various tests, it is shown that the LRF and EFM can\naccurately predict the time-mean responses of temperature and eddy heat flux to\nexternal forcings, and that the LRF can well predict the forcing needed to\nchange the mean flow in a specified way (inverse problem). The non-normality of\nthe LRF is discussed and its eigen/singular vectors are compared with the\nleading Proper Orthogonal Decomposition (POD) modes of the DNS data.\nFurthermore, it is shown that if the LRF and EFM are simply scaled by the\nsquare-root of Rayleigh number, they perform equally well for flows at other\n$\\mathrm{Ra}$, at least in the investigated range of $ 5 \\times 10^5 \\le\n\\mathrm{Ra} \\le 1.25 \\times 10^6$. The GRF method can be applied to develop 1D\nor 3D ROMs for any turbulent flow, and the calculated LRF and EFM can help with\nbetter analyzing and controlling the nonlinear system. \n\n"}
{"id": "1805.01648", "contents": "Title: Sharp convergence rates for Langevin dynamics in the nonconvex setting Abstract: We study the problem of sampling from a distribution $p^*(x) \\propto\n\\exp\\left(-U(x)\\right)$, where the function $U$ is $L$-smooth everywhere and\n$m$-strongly convex outside a ball of radius $R$, but potentially nonconvex\ninside this ball. We study both overdamped and underdamped Langevin MCMC and\nestablish upper bounds on the number of steps required to obtain a sample from\na distribution that is within $\\epsilon$ of $p^*$ in $1$-Wasserstein distance.\nFor the first-order method (overdamped Langevin MCMC), the iteration complexity\nis $\\tilde{\\mathcal{O}}\\left(e^{cLR^2}d/\\epsilon^2\\right)$, where $d$ is the\ndimension of the underlying space. For the second-order method (underdamped\nLangevin MCMC), the iteration complexity is\n$\\tilde{\\mathcal{O}}\\left(e^{cLR^2}\\sqrt{d}/\\epsilon\\right)$ for an explicit\npositive constant $c$. Surprisingly, the iteration complexity for both these\nalgorithms is only polynomial in the dimension $d$ and the target accuracy\n$\\epsilon$. It is exponential, however, in the problem parameter $LR^2$, which\nis a measure of non-log-concavity of the target distribution. \n\n"}
{"id": "1805.02990", "contents": "Title: Almost sure error bounds for data assimilation in dissipative systems\n  with unbounded observation noise Abstract: Data assimilation is uniquely challenging in weather forecasting due to the\nhigh dimensionality of the employed models and the nonlinearity of the\ngoverning equations. Although current operational schemes are used\nsuccessfully, our understanding of their long-term error behaviour is still\nincomplete. In this work, we study the error of some simple data assimilation\nschemes in the presence of unbounded (e.g. Gaussian) noise on a wide class of\ndissipative dynamical systems with certain properties, including the Lorenz\nmodels and the 2D incompressible Navier-Stokes equations. We exploit the\nproperties of the dynamics to derive analytic bounds on the long-term error for\nindividual realisations of the noise in time. These bounds are proportional to\nthe amplitude of the noise. Furthermore, we find that the error exhibits a form\nof stationary behaviour, and in particular an accumulation of error does not\noccur. This improves on previous results in which either the noise was bounded\nor the error was considered in expectation only. \n\n"}
{"id": "1805.04907", "contents": "Title: A Computational Framework for Modelling and Analyzing Ice Storms Abstract: Ice storms are extreme weather events that can have devastating implications\nfor the sustainability of natural ecosystems as well as man made\ninfrastructure. Ice storms are caused by a complex mix of atmospheric\nconditions and are among the least understood of severe weather events. Our\nability to model ice storms and characterize storm features will go a long way\ntowards both enabling support systems that offset storm impacts and increasing\nour understanding of ice storms. In this paper, we present a holistic\ncomputational framework to answer key questions of interest about ice storms.\nWe model ice storms as a function of relevant surface and atmospheric\nvariables. We learn these models by adapting and applying supervised and\nunsupervised machine learning algorithms on data with missing or incorrect\nlabels. We also include a knowledge representation module that reasons with\ndomain knowledge to revise the output of the learned models. Our models are\ntrained using reanalysis data and historical records of storm events. We\nevaluate these models on reanalyis data as well as Global Climate Model (GCM)\ndata for historical and future climate change scenarios. Furthermore, we\ndiscuss the use of appropriate bias correction approaches to run such modeling\nframeworks with GCM data. \n\n"}
{"id": "1805.05054", "contents": "Title: Consistency of Variational Bayes Inference for Estimation and Model\n  Selection in Mixtures Abstract: Mixture models are widely used in Bayesian statistics and machine learning,\nin particular in computational biology, natural language processing and many\nother fields. Variational inference, a technique for approximating intractable\nposteriors thanks to optimization algorithms, is extremely popular in practice\nwhen dealing with complex models such as mixtures. The contribution of this\npaper is two-fold. First, we study the concentration of variational\napproximations of posteriors, which is still an open problem for general\nmixtures, and we derive consistency and rates of convergence. We also tackle\nthe problem of model selection for the number of components: we study the\napproach already used in practice, which consists in maximizing a numerical\ncriterion (the Evidence Lower Bound). We prove that this strategy indeed leads\nto strong oracle inequalities. We illustrate our theoretical results by\napplications to Gaussian and multinomial mixtures. \n\n"}
{"id": "1805.05188", "contents": "Title: Essential formulae for restricted maximum likelihood and its derivatives\n  associated with the linear mixed models Abstract: The restricted maximum likelihood method enhances popularity of maximum\nlikelihood methods for variance component analysis on large scale unbalanced\ndata. As the high throughput biological data sets and the emerged science on\nuncertainty quantification, such a method receives increasing attention.\nEstimating the unknown variance parameters with restricted maximum likelihood\nmethod usually requires an nonlinear iterative method. Therefore proper\nformulae for the log-likelihood function and its derivatives play an essential\nrole in practical algorithm design. It is our aim to provide a mathematical\nintroduction to this method, and supply a self-contained derivation on some\navailable formulae used in practical algorithms. Some new proof are supplied. \n\n"}
{"id": "1805.05525", "contents": "Title: The Antarctic circumpolar wave and its seasonality: Intrinsic traveling\n  modes and ENSO teleconnections Abstract: Interannual variability in the Southern Ocean is investigated via nonlinear\nLaplacian spectral analysis (NLSA), an objective eigendecomposition technique\nfor nonlinear dynamical systems that can simultaneously recover multiple\ntimescales from data with high skill. Applied to modeled and observed sea\nsurface temperature and sea ice concentration data, NLSA recovers the\nwavenumber-2 eastward propagating signal corresponding to the Antarctic\ncircumpolar wave (ACW). During certain phases of its lifecycle, the spatial\npatterns of this mode display a structure that can explain the statistical\norigin of the Antarctic dipole pattern. Another group of modes have combination\nfrequencies consistent with the modulation of the annual cycle by the ACW.\nFurther examination of these newly identified modes reveals that they can have\neither eastward or westward propagation, combined with meridional pulsation\nreminiscent of sea ice reemergence patterns in the Arctic. Moreover, they\nexhibit smaller-scale spatial structures, and explain more Indian Ocean\nvariance than the primary ACW modes. We attribute these modes to\nteleconnections between ACW and the tropical Indo-Pacific Ocean; in particular,\nfundamental ENSO modes and their associated combination modes with the annual\ncycle recovered by NLSA. Another mode extracted from the Antarctic variables\ndisplays an eastward propagating wavenumber-3 structure over the Southern\nOcean, but exhibits no strong correlation to interannual Indo-Pacific\nvariability. \n\n"}
{"id": "1805.06328", "contents": "Title: The degree profile and Gini index of random caterpillar trees Abstract: In this paper, we investigate the degree profile and Gini index of random\ncaterpillar trees (RCTs). We consider RCTs which evolve in two different\nmanners: uniform and nonuniform. The degrees of the vertices on the central\npath (i.e., the degree profile) of a uniform RCT follow a multinomial\ndistribution. For nonuniform RCTs, we focus on those growing in the fashion of\npreferential attachment. We develop methods based on stochastic recurrences to\ncompute the exact expectations and the dispersion matrix of the degree\nvariables. A generalized P\\'{o}lya urn model is exploited to determine the\nexact joint distribution of these degree variables. We apply the methods from\ncombinatorics to prove that the asymptotic distribution is Dirichlet. In\naddition, we propose a new type of Gini index to quantitatively distinguish the\nevolutionary characteristics of the two classes of RCTs. We present the results\nvia several numerical experiments. \n\n"}
{"id": "1805.06639", "contents": "Title: Independent Component Analysis via Energy-based and Kernel-based Mutual\n  Dependence Measures Abstract: We apply both distance-based (Jin and Matteson, 2017) and kernel-based\n(Pfister et al., 2016) mutual dependence measures to independent component\nanalysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA,\nminimizing empirical dependence measures as an objective function in both\ndeflation and parallel manners. Solving this minimization problem, we introduce\nLatin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization\nmethod, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization\nof the Newton-type local optimization method. The performance of MDMICA is\nevaluated in various simulation studies and an image data example. When the ICA\nmodel is correct, MDMICA achieves competitive results compared to existing\napproaches. When the ICA model is misspecified, the estimated independent\ncomponents are less mutually dependent than the observed components using\nMDMICA, while they are prone to be even more mutually dependent than the\nobserved components using other approaches. \n\n"}
{"id": "1805.06820", "contents": "Title: Hamiltonian properties of Madden-Julian Oscillation: waves and\n  distributed chaos Abstract: The Madden-Julian Oscillation (MJO) has been studied using its daily RMM1,\nRMM2 and VPM1, VPM2 principal components' indices. A spectral analysis of the\nraw indices indicates presence of a noise presumably contributed by the random\nhigh-frequency waves. A soft simple filter - 3 day running average, has been\napplied to filter out this high-frequency noise (the filter has been chosen in\naccordance with the observationally known properties of these waves). Power\nspectrum of the smoothed indices exhibits a stretched exponential decay $E(f)\n\\propto \\exp-(f/f_0)^{1/2}$ characteristic to the Hamiltonian distributed chaos\nwith spontaneously broken time translational symmetry (with time depending\nHamiltonian and adiabatic invariance of the action). An impact of the MJO's\nHamiltonian distributed chaos on the Asian-Australian Monsoons at the\nintraseasonal time scales has been briefly discussed. \n\n"}
{"id": "1805.07011", "contents": "Title: A shadowing-based inflation scheme for ensemble data assimilation Abstract: Artificial ensemble inflation is a common technique in ensemble data\nassimilation, whereby the ensemble covariance is periodically increased in\norder to prevent deviation of the ensemble from the observations and possible\nensemble collapse. This manuscript introduces a new form of covariance\ninflation for ensemble data assimilation based upon shadowing ideas from\ndynamical systems theory. We present results from a low order nonlinear chaotic\nsystem that supports using shadowing inflation, demonstrating that shadowing\ninflation is more robust to parameter tuning than standard multiplicative\ncovariance inflation, outperforming in observation-sparse scenarios and often\nleading to longer forecast shadowing times. \n\n"}
{"id": "1805.07415", "contents": "Title: Effects of dissociation/recombination on the day-night temperature\n  contrasts of ultra-hot Jupiters Abstract: Secondary eclipse observations of ultra-hot Jupiters have found evidence that\nhydrogen is dissociated on their daysides. Additionally, full-phase light curve\nobservations of ultra-hot Jupiters show a smaller day-night emitted flux\ncontrast than that expected from previous theory. Recently, it was proposed by\nBell & Cowan (2018) that the heat intake to dissociate hydrogen and heat\nrelease due to recombination of dissociated hydrogen can affect the atmospheric\ncirculation of ultra-hot Jupiters. In this work, we add cooling/heating due to\ndissociation/recombination into the analytic theory of Komacek & Showman (2016)\nand Zhang & Showman (2017) for the dayside-nightside temperature contrasts of\nhot Jupiters. We find that at high values of incident stellar flux, the\nday-night temperature contrast of ultra-hot Jupiters may decrease with\nincreasing incident stellar flux due to dissociation/recombination, the\nopposite of that expected without including the effects of\ndissociation/recombination. We propose that a combination of a greater number\nof full-phase light curve observations of ultra-hot Jupiters and future General\nCirculation Models that include the effects of dissociation/recombination could\ndetermine in detail how the atmospheric circulation of ultra-hot Jupiters\ndiffers from that of cooler planets. \n\n"}
{"id": "1805.09091", "contents": "Title: Neural networks for post-processing ensemble weather forecasts Abstract: Ensemble weather predictions require statistical post-processing of\nsystematic errors to obtain reliable and accurate probabilistic forecasts.\nTraditionally, this is accomplished with distributional regression models in\nwhich the parameters of a predictive distribution are estimated from a training\nperiod. We propose a flexible alternative based on neural networks that can\nincorporate nonlinear relationships between arbitrary predictor variables and\nforecast distribution parameters that are automatically learned in a\ndata-driven way rather than requiring pre-specified link functions. In a case\nstudy of 2-meter temperature forecasts at surface stations in Germany, the\nneural network approach significantly outperforms benchmark post-processing\nmethods while being computationally more affordable. Key components to this\nimprovement are the use of auxiliary predictor variables and station-specific\ninformation with the help of embeddings. Furthermore, the trained neural\nnetwork can be used to gain insight into the importance of meteorological\nvariables thereby challenging the notion of neural networks as uninterpretable\nblack boxes. Our approach can easily be extended to other statistical\npost-processing and forecasting problems. We anticipate that recent advances in\ndeep learning combined with the ever-increasing amounts of model and\nobservation data will transform the post-processing of numerical weather\nforecasts in the coming decade. \n\n"}
{"id": "1805.10038", "contents": "Title: A Multi-Scan Labeled Random Finite Set Model for Multi-object State\n  Estimation Abstract: State space models in which the system state is a finite set--called the\nmulti-object state--have generated considerable interest in recent years.\nSmoothing for state space models provides better estimation performance than\nfiltering by using the full posterior rather than the filtering density. In\nmulti-object state estimation, the Bayes multi-object filtering recursion\nadmits an analytic solution known as the Generalized Labeled Multi-Bernoulli\n(GLMB) filter. In this work, we extend the analytic GLMB recursion to propagate\nthe multi-object posterior. We also propose an implementation of this so-called\nmulti-scan GLMB posterior recursion using a similar approach to the GLMB filter\nimplementation. \n\n"}
{"id": "1805.11238", "contents": "Title: Explicit construction of RIP matrices is Ramsey-hard Abstract: Matrices $\\Phi\\in\\R^{n\\times p}$ satisfying the Restricted Isometry Property\n(RIP) are an important ingredient of the compressive sensing methods. While it\nis known that random matrices satisfy the RIP with high probability even for\n$n=\\log^{O(1)}p$, the explicit construction of such matrices defied the\nrepeated efforts, and the most known approaches hit the so-called $\\sqrt{n}$\nsparsity bottleneck. The notable exception is the work by Bourgain et al\n\\cite{bourgain2011explicit} constructing an $n\\times p$ RIP matrix with\nsparsity $s=\\Theta(n^{{1\\over 2}+\\epsilon})$, but in the regime\n$n=\\Omega(p^{1-\\delta})$.\n  In this short note we resolve this open question in a sense by showing that\nan explicit construction of a matrix satisfying the RIP in the regime\n$n=O(\\log^2 p)$ and $s=\\Theta(n^{1\\over 2})$ implies an explicit construction\nof a three-colored Ramsey graph on $p$ nodes with clique sizes bounded by\n$O(\\log^2 p)$ -- a question in the extremal combinatorics which has been open\nfor decades. \n\n"}
{"id": "1805.11380", "contents": "Title: Kernel embedding of maps for sequential Bayesian inference: The\n  variational mapping particle filter Abstract: In this work, a novel sequential Monte Carlo filter is introduced which aims\nat efficient sampling of high-dimensional state spaces with a limited number of\nparticles. Particles are pushed forward from the prior to the posterior density\nusing a sequence of mappings that minimizes the Kullback-Leibler divergence\nbetween the posterior and the sequence of intermediate densities. The sequence\nof mappings represents a gradient flow. A key ingredient of the mappings is\nthat they are embedded in a reproducing kernel Hilbert space, which allows for\na practical and efficient algorithm. The embedding provides a direct means to\ncalculate the gradient of the Kullback-Leibler divergence leading to quick\nconvergence using well-known gradient-based stochastic optimization algorithms.\nEvaluation of the method is conducted in the chaotic Lorenz-63 system, the\nLorenz-96 system, which is a coarse prototype of atmospheric dynamics, and an\nepidemic model that describes cholera dynamics. No resampling is required in\nthe mapping particle filter even for long recursive sequences. The number of\neffective particles remains close to the total number of particles in all the\nexperiments. \n\n"}
{"id": "1805.11401", "contents": "Title: A Geometric Approach for Computing Tolerance Bounds for Elastic\n  Functional Data Abstract: We develop a method for constructing tolerance bounds for functional data\nwith random warping variability. In particular, we define a generative,\nprobabilistic model for the amplitude and phase components of such\nobservations, which parsimoniously characterizes variability in the baseline\ndata. Based on the proposed model, we define two different types of tolerance\nbounds that are able to measure both types of variability, and as a result,\nidentify when the data has gone beyond the bounds of amplitude and/or phase.\nThe first functional tolerance bounds are computed via a bootstrap procedure on\nthe geometric space of amplitude and phase functions. The second functional\ntolerance bounds utilize functional Principal Component Analysis to construct a\ntolerance factor. This work is motivated by two main applications: process\ncontrol and disease monitoring. The problem of statistical analysis and\nmodeling of functional data in process control is important in determining when\na production has moved beyond a baseline. Similarly, in biomedical\napplications, doctors use long, approximately periodic signals (such as the\nelectrocardiogram) to diagnose and monitor diseases. In this context, it is\ndesirable to identify abnormalities in these signals. We additionally consider\na simulated example to assess our approach and compare it to two existing\nmethods. \n\n"}
{"id": "1806.00237", "contents": "Title: Accounting for model errors in iterative ensemble smoothers Abstract: In the strong-constraint formulation of the history-matching problem, we\nassume that all the model errors relate to a selection of uncertain model input\nparameters. One does not account for additional model errors that could result\nfrom, e.g., excluded uncertain parameters, neglected physics in the model\nformulation, the use of an approximate model forcing, or discretization errors\nresulting from numerical approximations. If parameters with significant\nuncertainties are unaccounted for, there is a risk for an unphysical update, of\nsome uncertain parameters, that compensates for errors in the omitted\nparameters. This paper gives the theoretical foundation for introducing model\nerrors in ensemble methods for history matching. In particular, we explain\nprocedures for practically including model errors in iterative ensemble\nsmoothers like ESMDA and IES. Also, we demonstrate the impact of adding (or\nneglecting) model errors in the parameter-estimation problem. \n\n"}
{"id": "1806.00989", "contents": "Title: Asymptotic optimality of adaptive importance sampling Abstract: Adaptive importance sampling (AIS) uses past samples to update the\n\\textit{sampling policy} $q_t$ at each stage $t$. Each stage $t$ is formed with\ntwo steps : (i) to explore the space with $n_t$ points according to $q_t$ and\n(ii) to exploit the current amount of information to update the sampling\npolicy. The very fundamental question raised in this paper concerns the\nbehavior of empirical sums based on AIS. Without making any assumption on the\nallocation policy $n_t$, the theory developed involves no restriction on the\nsplit of computational resources between the explore (i) and the exploit (ii)\nstep. It is shown that AIS is asymptotically optimal : the asymptotic behavior\nof AIS is the same as some \"oracle\" strategy that knows the targeted sampling\npolicy from the beginning. From a practical perspective, weighted AIS is\nintroduced, a new method that allows to forget poor samples from early stages. \n\n"}
{"id": "1806.01750", "contents": "Title: Chaotic Hamiltonian dynamics of surface air temperature on daily to\n  intraseasonal time scales Abstract: The surface air temperature daily records at the land-based locations with\ndifferent climate conditions (from Arctic to Patagonia) have been studied on\nthe daily to intraseasonal time scales (low frequency annual and seasonal\nvariations have been removed by subtracting a wavelet regression from the daily\nrecords). It is shown that the power spectra of the daily time series exhibit a\nuniversal behaviour corresponding to the Hamiltonian distributed chaos. Global\naverage temperature fluctuations (land-based data) and the tropical Pacific sea\nsurface temperature fluctuations (El Ni\\~no/La Ni\\~na phenomenon) have been\nalso considered in this context. It is shown that the practical smooth\npredictability for the surface air temperature dynamics is possible at least up\nto the fundamental (pumping) period of the distributed chaos. \n\n"}
{"id": "1806.01870", "contents": "Title: Exact Simulation of the Extrema of Stable Processes Abstract: We exhibit an exact simulation algorithm for the supremum of a stable process\nover a finite time interval using dominated coupling from the past (DCFTP). We\nestablish a novel perpetuity equation for the supremum (via the representation\nof the concave majorants of L\\'evy processes) and apply it to construct a\nMarkov chain in the DCFTP algorithm. We prove that the number of steps taken\nbackwards in time before the coalescence is detected is finite. We analyse\nnumerically the performance of the algorithm (the code, written in Julia 1.0,\nis available on GitHub). \n\n"}
{"id": "1806.02068", "contents": "Title: Dynamically rescaled Hamiltonian Monte Carlo for Bayesian Hierarchical\n  Models Abstract: Dynamically rescaled Hamiltonian Monte Carlo (DRHMC) is introduced as a\ncomputationally fast and easily implemented method for performing full Bayesian\nanalysis in hierarchical statistical models. The method relies on introducing a\nmodified parameterisation so that the re-parameterised target distribution has\nclose to constant scaling properties, and thus is easily sampled using standard\n(Euclidian metric) Hamiltonian Monte Carlo. Provided that the parameterisations\nof the conditional distributions specifying the hierarchical model are\n\"constant information parameterisations\" (CIP), the relation between the\nmodified- and original parameterisation is bijective, explicitly computed and\nadmit exploitation of sparsity in the numerical linear algebra involved. CIPs\nfor a large catalogue of statistical models are presented, and from the\ncatalogue, it is clear that many CIPs are currently routinely used in\nstatistical computing. A relation between the proposed methodology and a class\nof explicitly integrated Riemann manifold Hamiltonian Monte Carlo methods is\ndiscussed. The methodology is illustrated on several example models, including\na model for inflation rates with multiple levels of non-linearly dependent\nlatent variables. \n\n"}
{"id": "1806.02670", "contents": "Title: Scalable Bayesian Nonparametric Clustering and Classification Abstract: We develop a scalable multi-step Monte Carlo algorithm for inference under a\nlarge class of nonparametric Bayesian models for clustering and classification.\nEach step is \"embarrassingly parallel\" and can be implemented using the same\nMarkov chain Monte Carlo sampler. The simplicity and generality of our approach\nmakes inference for a wide range of Bayesian nonparametric mixture models\napplicable to large datasets. Specifically, we apply the approach to inference\nunder a product partition model with regression on covariates. We show results\nfor inference with two motivating data sets: a large set of electronic health\nrecords (EHR) and a bank telemarketing dataset. We find interesting clusters\nand favorable classification performance relative to other widely used\ncompeting classifiers. \n\n"}
{"id": "1806.03756", "contents": "Title: Scalable Algorithms for the Sparse Ridge Regression Abstract: Sparse regression and variable selection for large-scale data have been\nrapidly developed in the past decades. This work focuses on sparse ridge\nregression, which enforces the sparsity by use of the L0 norm. We first prove\nthat the continuous relaxation of the mixed integer second order conic (MISOC)\nreformulation using perspective formulation is equivalent to that of the convex\ninteger formulation proposed in recent work. We also show that the convex hull\nof the constraint system of MISOC formulation is equal to its continuous\nrelaxation. Based upon these two formulations (i.e., the MISOC formulation and\nconvex integer formulation), we analyze two scalable algorithms, the greedy and\nrandomized algorithms, for sparse ridge regression with desirable theoretical\nproperties. The proposed algorithms are proved to yield near-optimal solutions\nunder mild conditions. We further propose to integrate the greedy algorithm\nwith the randomized algorithm, which can greedily search the features from the\nnonzero subset identified by the continuous relaxation of the MISOC\nformulation. The merits of the proposed methods are illustrated through\nnumerical examples in comparison with several existing ones. \n\n"}
{"id": "1806.03791", "contents": "Title: The Effect of Network Width on the Performance of Large-batch Training Abstract: Distributed implementations of mini-batch stochastic gradient descent (SGD)\nsuffer from communication overheads, attributed to the high frequency of\ngradient updates inherent in small-batch training. Training with large batches\ncan reduce these overheads; however, large batches can affect the convergence\nproperties and generalization performance of SGD. In this work, we take a first\nstep towards analyzing how the structure (width and depth) of a neural network\naffects the performance of large-batch training. We present new theoretical\nresults which suggest that--for a fixed number of parameters--wider networks\nare more amenable to fast large-batch training compared to deeper ones. We\nprovide extensive experiments on residual and fully-connected neural networks\nwhich suggest that wider networks can be trained using larger batches without\nincurring a convergence slow-down, unlike their deeper variants. \n\n"}
{"id": "1806.04505", "contents": "Title: Possible Implications of Self-Similarity for Tornadogenesis and\n  Maintenance Abstract: Self-similarity in tornadic and some non-tornadic supercell flows is studied\nand power laws relating various quantities in such flows are demonstrated.\nMagnitudes of the exponents in these power laws are related to the intensity of\nthe corresponding flow and thus the severity of the supercell storm. The\nfeatures studied in this paper include the vertical vorticity and\npseudovorticity, both obtained from radar observations and from numerical\nsimulations, the tangential velocity, and the energy spectrum as a function of\nthe wave number. Connections to fractals are highlighted and discussed. \n\n"}
{"id": "1806.04731", "contents": "Title: Deep learning to represent sub-grid processes in climate models Abstract: The representation of nonlinear sub-grid processes, especially clouds, has\nbeen a major source of uncertainty in climate models for decades.\nCloud-resolving models better represent many of these processes and can now be\nrun globally but only for short-term simulations of at most a few years because\nof computational limitations. Here we demonstrate that deep learning can be\nused to capture many advantages of cloud-resolving modeling at a fraction of\nthe computational cost. We train a deep neural network to represent all\natmospheric sub-grid processes in a climate model by learning from a\nmulti-scale model in which convection is treated explicitly. The trained neural\nnetwork then replaces the traditional sub-grid parameterizations in a global\ngeneral circulation model in which it freely interacts with the resolved\ndynamics and the surface-flux scheme. The prognostic multi-year simulations are\nstable and closely reproduce not only the mean climate of the cloud-resolving\nsimulation but also key aspects of variability, including precipitation\nextremes and the equatorial wave spectrum. Furthermore, the neural network\napproximately conserves energy despite not being explicitly instructed to.\nFinally, we show that the neural network parameterization generalizes to new\nsurface forcing patterns but struggles to cope with temperatures far outside\nits training manifold. Our results show the feasibility of using deep learning\nfor climate model parameterization. In a broader context, we anticipate that\ndata-driven Earth System Model development could play a key role in reducing\nclimate prediction uncertainty in the coming decade. \n\n"}
{"id": "1806.05982", "contents": "Title: Accelerating delayed-acceptance Markov chain Monte Carlo algorithms Abstract: Delayed-acceptance Markov chain Monte Carlo (DA-MCMC) samples from a\nprobability distribution via a two-stages version of the Metropolis-Hastings\nalgorithm, by combining the target distribution with a \"surrogate\" (i.e. an\napproximate and computationally cheaper version) of said distribution. DA-MCMC\naccelerates MCMC sampling in complex applications, while still targeting the\nexact distribution. We design a computationally faster, albeit approximate,\nDA-MCMC algorithm. We consider parameter inference in a Bayesian setting where\na surrogate likelihood function is introduced in the delayed-acceptance scheme.\nWhen the evaluation of the likelihood function is computationally intensive,\nour scheme produces a 2-4 times speed-up, compared to standard DA-MCMC.\nHowever, the acceleration is highly problem dependent. Inference results for\nthe standard delayed-acceptance algorithm and our approximated version are\nsimilar, indicating that our algorithm can return reliable Bayesian inference.\nAs a computationally intensive case study, we introduce a novel stochastic\ndifferential equation model for protein folding data. \n\n"}
{"id": "1806.06520", "contents": "Title: Stability of Conditional Sequential Monte Carlo Abstract: The particle Gibbs (PG) sampler is a Markov Chain Monte Carlo (MCMC)\nalgorithm, which uses an interacting particle system to perform the Gibbs\nsteps. Each Gibbs step consists of simulating a particle system conditioned on\none particle path. It relies on a conditional Sequential Monte Carlo (cSMC)\nmethod to create the particle system. We propose a novel interpretation of the\ncSMC algorithm as a perturbed Sequential Monte Carlo (SMC) method and apply\ntelescopic decompositions developed for the analysis of SMC algorithms\n\\cite{delmoral2004} to derive a bound for the distance between the expected\nsampled path from cSMC and the target distribution of the MCMC algorithm. This\ncan be used to get a uniform ergodicity result. In particular, we can show that\nthe mixing rate of cSMC can be kept constant by increasing the number of\nparticles linearly with the number of observations. Based on our decomposition,\nwe also prove a central limit theorem for the cSMC Algorithm, which cannot be\ndone using the approaches in \\cite{Andrieu2013} and \\cite{Lindsten2014}. \n\n"}
{"id": "1806.06784", "contents": "Title: Robust inference on the average treatment effect using the outcome\n  highly adaptive lasso Abstract: Many estimators of the average effect of a treatment on an outcome require\nestimation of the propensity score, the outcome regression, or both. It is\noften beneficial to utilize flexible techniques such as semiparametric\nregression or machine learning to estimate these quantities. However, optimal\nestimation of these regressions does not necessarily lead to optimal estimation\nof the average treatment effect, particularly in settings with strong\ninstrumental variables. A recent proposal addressed these issues via the\noutcome-adaptive lasso, a penalized regression technique for estimating the\npropensity score that seeks to minimize the impact of instrumental variables on\ntreatment effect estimators. However, a notable limitation of this approach is\nthat its application is restricted to parametric models. We propose a more\nflexible alternative that we call the outcome highly adaptive lasso. We discuss\nlarge sample theory for this estimator and propose closed form confidence\nintervals based on the proposed estimator. We show via simulation that our\nmethod offers benefits over several popular approaches. \n\n"}
{"id": "1806.06822", "contents": "Title: Exploring exomoon atmospheres with an idealized general circulation\n  model Abstract: Recent studies have shown that large exomoons can form in the accretion disks\naround super-Jovian extrasolar planets. These planets are abundant at about 1\nAU from Sun-like stars, which makes their putative moons interesting for\nstudies of habitability. Technological advances could soon make an exomoon\ndiscovery with Kepler or the upcoming CHEOPS and PLATO space missions possible.\nExomoon climates might be substantially different from exoplanet climates\nbecause the day-night cycles on moons are determined by the moon's synchronous\nrotation with its host planet. Moreover, planetary illumination at the top of\nthe moon's atmosphere and tidal heating at the moon's surface can be\nsubstantial, which can affect the redistribution of energy on exomoons. Using\nan idealized general circulation model with simplified hydrologic, radiative,\nand convective processes, we calculate surface temperature, wind speed, mean\nmeridional circulation, and energy transport on a 2.5 Mars-mass moon orbiting a\n10-Jupiter-mass at 1 AU from a Sun-like star. The strong thermal irradiation\nfrom a young giant planet causes the satellite's polar regions to warm, which\nremains consistent with the dynamically-driven polar amplification seen in\nEarth models that lack ice-albedo feedback. Thermal irradiation from young,\nluminous giant planets onto water-rich exomoons can be strong enough to induce\nwater loss on a planet, which could lead to a runaway greenhouse. Moons that\nare in synchronous rotation with their host planet and do not experience a\nrunaway greenhouse could experience substantial polar melting induced by the\npolar amplification of planetary illumination and geothermal heating from tidal\neffects. \n\n"}
{"id": "1806.09048", "contents": "Title: A classification point-of-view about conditional Kendall's tau Abstract: We show how the problem of estimating conditional Kendall's tau can be\nrewritten as a classification task. Conditional Kendall's tau is a conditional\ndependence parameter that is a characteristic of a given pair of random\nvariables. The goal is to predict whether the pair is concordant (value of $1$)\nor discordant (value of $-1$) conditionally on some covariates. We prove the\nconsistency and the asymptotic normality of a family of penalized approximate\nmaximum likelihood estimators, including the equivalent of the logit and probit\nregressions in our framework. Then, we detail specific algorithms adapting\nusual machine learning techniques, including nearest neighbors, decision trees,\nrandom forests and neural networks, to the setting of the estimation of\nconditional Kendall's tau. Finite sample properties of these estimators and\ntheir sensitivities to each component of the data-generating process are\nassessed in a simulation study. Finally, we apply all these estimators to a\ndataset of European stock indices. \n\n"}
{"id": "1806.09362", "contents": "Title: Approximate Bayesian inference for mixture cure models Abstract: Cure models in survival analysis deal with populations in which a part of the\nindividuals cannot experience the event of interest. Mixture cure models\nconsider the target population as a mixture of susceptible and non-susceptible\nindividuals. The statistical analysis of these models focuses on examining the\nprobability of cure (incidence model) and inferring on the time-to-event in the\nsusceptible subpopulation (latency model).\n  Bayesian inference on mixture cure models has typically relied upon Markov\nchain Monte Carlo (MCMC) methods. The integrated nested Laplace approximation\n(INLA) is a recent and attractive approach for doing Bayesian inference. INLA\nin its natural definition cannot fit mixture models but recent research has new\nproposals that combine INLA and MCMC methods to extend its applicability to\nthem (Bivand et al., 2014, G\\'omez-Rubio et al., 2017, G\\'omez-Rubio and Rue,\n2018}.\n  This paper focuses on the implementation of INLA in mixture cure models. A\ngeneral mixture cure survival model with covariate information for the latency\nand the incidence model within a general scenario with censored and\nnon-censored information is discussed. The fact that non-censored individuals\nundoubtedly belong to the uncured population is a valuable information that was\nincorporated in the inferential process. \n\n"}
{"id": "1806.09548", "contents": "Title: Learning dynamical systems with particle stochastic approximation EM Abstract: We present the particle stochastic approximation EM (PSAEM) algorithm for\nlearning of dynamical systems. The method builds on the EM algorithm, an\niterative procedure for maximum likelihood inference in latent variable models.\nBy combining stochastic approximation EM and particle Gibbs with ancestor\nsampling (PGAS), PSAEM obtains superior computational performance and\nconvergence properties compared to plain particle-smoothing-based\napproximations of the EM algorithm. PSAEM can be used for plain maximum\nlikelihood inference as well as for empirical Bayes learning of\nhyperparameters. Specifically, the latter point means that existing PGAS\nimplementations easily can be extended with PSAEM to estimate hyperparameters\nat almost no extra computational cost. We discuss the convergence properties of\nthe algorithm, and demonstrate it on several signal processing applications. \n\n"}
{"id": "1806.09861", "contents": "Title: The importance of ensemble techniques for operational space weather\n  forecasting Abstract: The space weather community has begun to use frontier methods such as data\nassimilation, machine learning, and ensemble modeling to advance current\noperational forecasting efforts. This was highlighted by a multi-disciplinary\nsession at the 2017 American Geophysical Union Meeting, 'Frontier\nSolar-Terrestrial Science Enabled by the Combination of Data-Driven Techniques\nand Physics-Based Understanding', with considerable discussion surrounding\nensemble techniques. Here ensemble methods are described in detail; using a set\nof predictions to improve on a single-model output, for example taking a simple\naverage of multiple models, or using more complex techniques for data\nassimilation. They have been used extensively in fields such as numerical\nweather prediction and data science, for both improving model accuracy and\nproviding a measure of model uncertainty. Researchers in the space weather\ncommunity have found them to be similarly useful, and some examples of success\nstories are highlighted in this commentary. Future developments are also\nencouraged to transition these basic research efforts to operational\nforecasting as well as providing prediction errors to aid end-user\nunderstanding. \n\n"}
{"id": "1806.10060", "contents": "Title: Large Sample Asymptotics of the Pseudo-Marginal Method Abstract: The pseudo-marginal algorithm is a variant of the Metropolis--Hastings\nalgorithm which samples asymptotically from a probability distribution when it\nis only possible to estimate unbiasedly an unnormalized version of its density.\nPractically, one has to trade-off the computational resources used to obtain\nthis estimator against the asymptotic variances of the ergodic averages\nobtained by the pseudo-marginal algorithm. Recent works optimizing this\ntrade-off rely on some strong assumptions which can cast doubts over their\npractical relevance. In particular, they all assume that the distribution of\nthe difference between the log-density and its estimate is independent of the\nparameter value at which it is evaluated. Under regularity conditions we show\nhere that, as the number of data points tends to infinity, a space-rescaled\nversion of the pseudo-marginal chain converges weakly towards another\npseudo-marginal chain for which this assumption indeed holds. A study of this\nlimiting chain allows us to provide parameter dimension-dependent guidelines on\nhow to optimally scale a normal random walk proposal and the number of Monte\nCarlo samples for the pseudo-marginal method in the large-sample regime. This\ncomplements and validates currently available results. \n\n"}
{"id": "1806.10761", "contents": "Title: Survey of multifidelity methods in uncertainty propagation, inference,\n  and optimization Abstract: In many situations across computational science and engineering, multiple\ncomputational models are available that describe a system of interest. These\ndifferent models have varying evaluation costs and varying fidelities.\nTypically, a computationally expensive high-fidelity model describes the system\nwith the accuracy required by the current application at hand, while\nlower-fidelity models are less accurate but computationally cheaper than the\nhigh-fidelity model. Outer-loop applications, such as optimization, inference,\nand uncertainty quantification, require multiple model evaluations at many\ndifferent inputs, which often leads to computational demands that exceed\navailable resources if only the high-fidelity model is used. This work surveys\nmultifidelity methods that accelerate the solution of outer-loop applications\nby combining high-fidelity and low-fidelity model evaluations, where the\nlow-fidelity evaluations arise from an explicit low-fidelity model (e.g., a\nsimplified physics approximation, a reduced model, a data-fit surrogate, etc.)\nthat approximates the same output quantity as the high-fidelity model. The\noverall premise of these multifidelity methods is that low-fidelity models are\nleveraged for speedup while the high-fidelity model is kept in the loop to\nestablish accuracy and/or convergence guarantees. We categorize multifidelity\nmethods according to three classes of strategies: adaptation, fusion, and\nfiltering. The paper reviews multifidelity methods in the outer-loop contexts\nof uncertainty propagation, inference, and optimization. \n\n"}
{"id": "1807.00420", "contents": "Title: A Piecewise Deterministic Markov Process via $(r,\\theta)$ swaps in\n  hyperspherical coordinates Abstract: Recently, a class of stochastic processes known as piecewise deterministic\nMarkov processes has been used to define continuous-time Markov chain Monte\nCarlo algorithms with a number of attractive properties, including\ncompatibility with stochastic gradients like those typically found in\noptimization and variational inference, and high efficiency on certain big data\nproblems. Not many processes in this class that are capable of targeting\narbitrary invariant distributions are currently known, and within one subclass\nall previously known processes utilize linear transition functions. In this\nwork, we derive a process whose transition function is nonlinear through\nsolving its Fokker-Planck equation in hyperspherical coordinates. We explore\nits behavior on Gaussian targets, as well as a Bayesian logistic regression\nmodel with synthetic data. We discuss implications to both the theory of\npiecewise deterministic Markov processes, and to Bayesian statisticians as well\nas physicists seeking to use them for simulation-based computation. \n\n"}
{"id": "1807.01914", "contents": "Title: A multiple-try Metropolis-Hastings algorithm with tailored proposals Abstract: We present a new multiple-try Metropolis-Hastings algorithm designed to be\nespecially beneficial when a tailored proposal distribution is available. The\nalgorithm is based on a given acyclic graph $G$, where one of the nodes in $G$,\n$k$ say, contains the current state of the Markov chain and the remaining nodes\ncontain proposed states generated by applying the tailored proposal\ndistribution. The Metropolis-Hastings algorithm alternates between two types of\nupdates. The first update type is using the tailored proposal distribution to\ngenerate new states in all nodes in $G$ except in node $k$. The second update\ntype is generating a new value for $k$, thereby changing the value of the\ncurrent state. We evaluate the effectiveness of the proposed scheme in an\nexample with previously defined target and proposal distributions. \n\n"}
{"id": "1807.01973", "contents": "Title: Surface Mixing by Geostrophic Flows in the Bay of Bengal Abstract: Mixing in the Bay of Bengal, driven by altimetry derived daily geostrophic\nsurface currents, is studied on subseasonal timescales. Hovm{\\\"o}ller and\nwavenumber-frequency diagrams with power spectra confirm the multiscale nature\nof the flow. Advection of bands immediately brings out the chaotic nature of\nmixing in the Bay via repeated straining and filamentation of the tracer field.\nA principal finding is that mixing is local, i.e., of the scale of the eddies,\nand does not span the entire basin. Indeed, Finite Time Lyapunov Exponent\n(FTLE), Relative Dispersion (RD) and Finite Size Lyapunov Exponents (FSLE) maps\nin all seasons are patchy with minima scattered through the interior of the\nBay. Non-uniform stirring of the Bay is reflected in long tailed histograms of\nFTLEs, that become more stretched for longer time intervals. Quantitatively,\nadvection for a week shows the mean FTLE lies near 0.15-0.16 $day^{-1}$, while\nextremes reach almost 0.5 $day^{-1}$. Averaged over the Bay, RD initially grows\nexponentially, this is followed by a power-law at scales between approximately\n100 and 250 $km$, which finally transitions to an eddy-diffusive regime. These\nfindings are confirmed by FSLEs; in addition, below 250 $km$, a scale dependent\ndiffusion coefficient is extracted, while above 250 $km$, eddy-diffusivities\nrange from $6 \\times 10^3$ - $10^4$ $m^2/s$. Finally, with satellite salinity\ndata, these Lagrangian tools are used in the analysis a single post-monsoonal\nfresh water mixing event. Here, FTLEs and FSLEs allow the identification of\ntransport barriers, and elucidate how eddies help preserve the identity of\nfresh water. \n\n"}
{"id": "1807.05830", "contents": "Title: Exergy in meteorology: Definition and properties of moist-air available\n  enthalpy Abstract: The exergy of the dry atmosphere can be considered as another aspect of the\nmeteorological theories of available energies. The local and global properties\nof the dry available enthalpy function, also called flow exergy, were\ninvestigated in a previous paper (Marquet, Q. J. R. Meteorol. Soc., Vol 117,\np.449-475, 1991). The concept of exergy is well defined in thermodynamics, and\nseveral generalizations to chemically reacting systems have already been made.\nSimilarly, the concept of moist available enthalpy is presented in this paper\nin order to generalize the dry available enthalpy to the case of a moist\natmosphere. It is a local exergy-like function which possesses a simple\nanalytical expression where only two unknown constants are to be determined, a\nreference temperature and a reference pressure. The moist available enthalpy,\n$a_m$, is defined in terms of a moist potential change in total entropy. The\nlocal function $a_m$ can be separated into temperature, pressure and latent\ncomponents. The latent component is a new component that is not present in the\ndry case. The moist terms have been estimated using a representative cumulus\nvertical profile. It appears that the modifications brought by the moist\nformulation are important in comparison with the dry case. Other local and\nglobal properties are also investigated and comparisons are made with some\nother available energy functions used in thermodynamics and meteorology. \n\n"}
{"id": "1807.06240", "contents": "Title: Definition of the moist-air exergy norm: a comparison with existing\n  \"moist energy norms\" Abstract: This study presents a new formulation for the norms and scalar products used\nin tangent linear or adjoint models to determine forecast errors and\nsensitivity to observations and to calculate singular vectors. The new norm is\nderived from the concept of moist-air available enthalpy, which is one of the\navailability functions referred to as exergy in general thermodynamics. It is\nshown that the sum of the kinetic energy and the moist-air available enthalpy\ncan be used to define a new moist-air squared norm which is quadratic in: 1)\nwind components; 2) temperature; 3) surface pressure; and 4) water vapor\ncontent. Preliminary numerical applications are performed to show that the new\nweighting factors for temperature and water vapor are significantly different\nfrom those used in observation impact studies, and are in better agreement with\nobserved analysis increments. These numerical applications confirm that the\nweighting factors for water vapor and temperature exhibit a large increase with\nheight (by several orders of magnitude) and a minimum in the middle\ntroposphere, respectively. \n\n"}
{"id": "1807.08735", "contents": "Title: Uniform in time error estimates for a finite element method applied to a\n  downscaling data assimilation algorithm for the Navier-Stokes equations Abstract: In this paper we analyze a finite element method applied to a continuous\ndownscaling data assimilation algorithm for the numerical approximation of the\ntwo and three dimensional Navier-Stokes equations corresponding to given\nmeasurements on a coarse spatial scale. For representing the coarse mesh\nmeasurements we consider different types of interpolation operators including a\nLagrange interpolant. We obtain uniform-in-time estimates for the error between\na finite element approximation and the reference solution corresponding to the\ncoarse mesh measurements. We consider both the case of a plain Galerkin method\nand a Galerkin method with grad-div stabilization. For the stabilized method we\nprove error bounds in which the constants do not depend on inverse powers of\nthe viscosity. Some numerical experiments illustrate the theoretical results. \n\n"}
{"id": "1807.11859", "contents": "Title: Condensational and collisional growth of cloud droplets in a turbulent\n  environment Abstract: We investigate the effect of turbulence on the combined condensational and\ncollisional growth of cloud droplets by means of high resolution direct\nnumerical simulations of turbulence and a superparticle approximation for\ndroplet dynamics and collisions. The droplets are subject to turbulence as well\nas gravity, and their collision and coalescence efficiencies are taken to be\nunity. We solve the thermodynamic equations governing temperature, water-vapor\nmixing ratio, and the resulting supersaturation fields together with the\nNavier-Stokes equation. We find that the droplet-size distribution broadens\nwith increasing Reynolds number and/or mean energy dissipation rate. Turbulence\naffects the condensational growth directly through supersaturation\nfluctuations, and it influences collisional growth indirectly through\ncondensation. Our simulations show for the first time that, in the absence of\nthe mean updraft cooling, supersaturation fluctuation-induced broadening of\ndroplet-size distributions enhances the collisional growth. This is contrary to\nclassical (non-turbulent) condensational growth, which leads to a growing mean\ndroplet size, but a narrower droplet-size distribution. Our findings, instead,\nshow that condensational growth facilitates collisional growth by broadening\nthe size distribution in the tails at an early stage of rain formation. With\nincreasing Reynolds numbers, evaporation becomes stronger. This counteracts the\nbroadening effect due to condensation at late stages of rain formation. Our\nconclusions are consistent with results of laboratory experiments and field\nobservations, and show that supersaturation fluctuations are important for\nprecipitation. \n\n"}
{"id": "1808.03230", "contents": "Title: Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal\n  densities? Abstract: Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of\nMarkov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity\nof HMC algorithms is their excellent performance as the dimension $d$ of the\ntarget becomes large: under conditions that are satisfied for many common\nstatistical models, optimally-tuned HMC algorithms have a running time that\nscales like $d^{0.25}$. In stark contrast, the running time of the usual\nRandom-Walk Metropolis (RWM) algorithm, optimally tuned, scales like $d$. This\nsuperior scaling of the HMC algorithm with dimension is attributed to the fact\nthat it, unlike RWM, incorporates the gradient information in the proposal\ndistribution. In this paper, we investigate a different scaling question: does\nHMC beat RWM for highly $\\textit{multimodal}$ targets? We find that the answer\nis often $\\textit{no}$. We compute the spectral gaps for both the algorithms\nfor a specific class of multimodal target densities, and show that they are\nidentical. The key reason is that, within one mode, the gradient is effectively\nignorant about other modes, thus negating the advantage the HMC algorithm\nenjoys in unimodal targets. We also give heuristic arguments suggesting that\nthe above observation may hold quite generally. Our main tool for answering\nthis question is a novel simple formula for the conductance of HMC using\nLiouville's theorem. This result allows us to compute the spectral gap of HMC\nalgorithms, for both the classical HMC with isotropic momentum and the recent\nRiemannian HMC, for multimodal targets. \n\n"}
{"id": "1808.03239", "contents": "Title: Simple Conditions for Metastability of Continuous Markov Chains Abstract: A family $\\{Q_{\\beta}\\}_{\\beta \\geq 0}$ of Markov chains is said to exhibit\n$\\textit{metastable mixing}$ with $\\textit{modes}$\n$S_{\\beta}^{(1)},\\ldots,S_{\\beta}^{(k)}$ if its spectral gap (or some other\nmixing property) is very close to the worst conductance\n$\\min(\\Phi_{\\beta}(S_{\\beta}^{(1)}), \\ldots, \\Phi_{\\beta}(S_{\\beta}^{(k)}))$ of\nits modes. We give simple sufficient conditions for a family of Markov chains\nto exhibit metastability in this sense, and verify that these conditions hold\nfor a prototypical Metropolis-Hastings chain targeting a mixture distribution.\nOur work differs from existing work on metastability in that, for the class of\nexamples we are interested in, it gives an asymptotically exact formula for the\nspectral gap (rather than a bound that can be very far from sharp) while at the\nsame time giving technical conditions that are easier to verify for many\nstatistical examples. Our bounds from this paper are used in a companion paper\nto compare the mixing times of the Hamiltonian Monte Carlo algorithm and a\nrandom walk algorithm for multimodal target distributions. \n\n"}
{"id": "1808.04953", "contents": "Title: On Dissipation Rate of Ocean Waves due to White Capping Abstract: We calculate the rate of ocean waves energy dissipation due to whitecapping\nby numerical simulation of deterministic phase resolving model for dynamics of\nocean surface. Two independent numerical experiments are performed. First, we\nsolve the $3D$ Hamiltonian equation that includes three- and four-wave\ninteractions. This model is valid for moderate values of surface steepness\nonly, $\\mu < 0.09$. Then we solve the exact Euler equation for non-stationary\npotential flow of an ideal fluid with a free surface in $2D$ geometry. We use\nthe conformal mapping of domain filled with fluid onto the lower half-plane.\nThis model is applicable for arbitrary high levels of steepness. The results of\nboth experiments are close. The whitecapping is the threshold process that\ntakes place if the average steepness $\\mu > \\mu_{cr} \\simeq 0.055$. The rate of\nenergy dissipation grows dramatically with increasing of steepness. Comparison\nof our results with dissipation functions used in the operational models of\nwave forecasting shows that these models overestimate the rate of wave\ndissipation by order of magnitude for typical values of steepness. \n\n"}
{"id": "1808.05653", "contents": "Title: Atomic iron and titanium in the atmosphere of the exoplanet KELT-9b Abstract: The chemical composition of an exoplanet is a key ingredient in constraining\nits formation history. Iron is the most abundant transition metal, but has\nnever been directly detected in an exoplanet due to its highly refractory\nnature. KELT-9b (HD 195689b) is the archetype of the class of ultra-hot\nJupiters that straddle the transition between stars and gas-giant exoplanets\nand serve as distinctive laboratories for studying atmospheric chemistry,\nbecause of its high equilibrium temperature of 4050 +/- 180 K. These properties\nimply that its atmosphere is a tightly constrained chemical system that is\nexpected to be nearly in chemical equilibrium and cloud-free. It was previously\npredicted that the spectral lines of iron will be detectable in the visible\nrange of wavelengths. At these high temperatures, iron and several other\ntransition metals are not sequestered in molecules or cloud particles and exist\nsolely in their atomic forms. Here, we report the direct detection of atomic\nneutral and singly-ionized iron (Fe and Fe+), and singly-ionized titanium (Ti+)\nin KELT-9b via the cross-correlation technique applied to high-resolution\nspectra obtained during the primary transit of the exoplanet. \n\n"}
{"id": "1808.07140", "contents": "Title: Multinomial Models with Linear Inequality Constraints: Overview and\n  Improvements of Computational Methods for Bayesian Inference Abstract: Many psychological theories can be operationalized as linear inequality\nconstraints on the parameters of multinomial distributions (e.g., discrete\nchoice analysis). These constraints can be described in two equivalent ways:\nEither as the solution set to a system of linear inequalities or as the convex\nhull of a set of extremal points (vertices). For both representations, we\ndescribe a general Gibbs sampler for drawing posterior samples in order to\ncarry out Bayesian analyses. We also summarize alternative sampling methods for\nestimating Bayes factors for these model representations using the encompassing\nBayes factor method. We introduce the R package multinomineq, which provides an\neasily-accessible interface to a computationally efficient implementation of\nthese techniques. \n\n"}
{"id": "1808.08592", "contents": "Title: Hypocoercivity of Piecewise Deterministic Markov Process-Monte Carlo Abstract: In this work, we establish $\\mathrm{L}^2$-exponential convergence for a broad\nclass of Piecewise Deterministic Markov Processes recently proposed in the\ncontext of Markov Process Monte Carlo methods and covering in particular the\nRandomized Hamiltonian Monte Carlo, the Zig-Zag process and the Bouncy Particle\nSampler. The kernel of the symmetric part of the generator of such processes is\nnon-trivial, and we follow the ideas recently introduced by (Dolbeault et al.,\n2009, 2015) to develop a rigorous framework for hypocoercivity in a fairly\ngeneral and unifying set-up, while deriving tractable estimates of the\nconstants involved in terms of the parameters of the dynamics. As a by-product\nwe characterize the scaling properties of these algorithms with respect to the\ndimension of classes of problems, therefore providing some theoretical evidence\nto support their practical relevance. \n\n"}
{"id": "1808.08868", "contents": "Title: An exactly solvable record model for rainfall Abstract: Daily precipitation time series are composed of null entries corresponding to\ndry days and nonzero entries that describe the rainfall amounts on wet days.\nAssuming that wet days follow a Bernoulli process with success probability $p$,\nwe show that the presence of dry days induces negative correlations between\nrecord-breaking precipitation events. The resulting non-monotonic behavior of\nthe Fano factor of the record counting process is recovered in empirical data.\nWe derive the full probability distribution $P(R,n)$ of the number of records\n$R_n$ up to time $n$, and show that for large $n$, its large deviation form\ncoincides with that of a Poisson distribution with parameter $\\ln(p\\,n)$. We\nalso study in detail the joint limit $p \\to 0$, $n \\to \\infty$, which yields a\nrandom record model in continuous time $t = pn$. \n\n"}
{"id": "1808.10507", "contents": "Title: Variational integrator for the rotating shallow-water equations on the\n  sphere Abstract: We develop a variational integrator for the shallow-water equations on a\nrotating sphere. The variational integrator is built around a discretization of\nthe continuous Euler-Poincar\\'{e} reduction framework for Eulerian\nhydrodynamics. We describe the discretization of the continuous\nEuler-Poincar\\'{e} equations on arbitrary simplicial meshes. Standard numerical\ntests are carried out to verify the accuracy and the excellent conservational\nproperties of the discrete variational integrator. \n\n"}
{"id": "1809.00027", "contents": "Title: Dimensionality-Reduction of Climate Data using Deep Autoencoders Abstract: We explore the use of deep neural networks for nonlinear dimensionality\nreduction in climate applications. We train convolutional autoencoders (CAEs)\nto encode two temperature field datasets from pre-industrial control runs in\nthe CMIP5 first ensemble, obtained with the CCSM4 model and the IPSL-CM5A-LR\nmodel, respectively. With the later dataset, consisting of 36500 96$\\times$96\nsurface temperature fields, the CAE out-performs PCA in terms of mean squared\nerror of the reconstruction from a 40 dimensional encoding. Moreover, the noise\nin the filters of the convolutional layers in the autoencoders suggests that\nthe CAE can be trained to produce better results. Our results indicate that\nconvolutional autoencoders may provide an effective platform for the\nconstruction of surrogate climate models. \n\n"}
{"id": "1809.00256", "contents": "Title: Percolation Framework of the Earth's Topography Abstract: Self-similarity and long-range correlations are the remarkable features of\nthe Earth's surface topography. Here we develop an approach based on\npercolation theory to study the geometrical features of Earth. Our analysis is\nbased on high-resolution, 1 arc min, ETOPO1 global relief records.We find some\nevidence for abrupt transitions that occurred during the evolution of the\nEarth's relief network, indicative of a continental/cluster aggregation. We\napply finite-size-scaling analysis based on a coarse-graining procedure to show\nthat the observed transition is most likely discontinuous. Furthermore, we\nstudy the percolation on two-dimensional fractional Brownian motion surfaces\nwith Hurst exponent $H$ as a model of long-range correlated topography, which\nsuggests that the long-range correlations may play a key role in the observed\ndiscontinuity on Earth. Our framework presented here provides a theoretical\nmodel to better understand the geometrical phase transition on Earth, and it\nalso identifies the critical nodes that will be more exposed to global climate\nchange in the Earth's relief network. \n\n"}
{"id": "1809.01054", "contents": "Title: Quantifying the annular mode dynamics in an idealized atmosphere Abstract: The linear response function (LRF) of an idealized GCM, the dry dynamical\ncore with Held-Suarez physics, is used to accurately compute how eddy momentum\nand heat fluxes change in response to the zonal wind and temperature anomalies\nof the annular mode at the low-frequency limit. Using these results and knowing\nthe parameterizations of surface friction and thermal radiation in Held-Suarez\nphysics, the contribution of each physical process (meridional and vertical\neddy fluxes, surface friction, thermal radiation, and meridional advection) to\nthe annular mode dynamics is quantified. Examining the quasi-geostrophic\npotential vorticity balance, it is shown that the eddy feedback is positive and\nincreases the persistence of the annular mode by a factor of more than two.\nFurthermore, how eddy fluxes change in response to only the barotropic\ncomponent of the annular mode, i.e., vertically averaged zonal wind (and no\ntemperature) anomaly, is also calculated similarly. The response of eddy fluxes\nto the barotropic-only component of the annular mode is found to be drastically\ndifferent from the response to the full (barotropic+baroclinic) annular mode\nanomaly. In the former, the barotropic governor effect significantly suppresses\nthe eddy generation leading to a negative eddy feedback that decreases the\npersistence of the annular mode by nearly a factor of three. These results\nsuggest that the baroclinic component of the annular mode anomaly, i.e., the\nincreased low-level baroclinicity, is essential for the persistence of the\nannular mode, consistent with the baroclinic mechanism but not the barotropic\nmechanism proposed in the previous studies. \n\n"}
{"id": "1809.01874", "contents": "Title: Comments on \"Is condensation-induced atmospheric dynamics a new theory\n  of the origin of the winds?\" by Jaramillo et al. (2018) Abstract: Our comments on paper \"Is Condensation-Induced Atmospheric Dynamics a New\nTheory of the Origin of the Winds?\" by A. Jaramillo, O. J. Mesa, and D. J.\nRaymond published in J. Atmos. Sci. 75, 3305 (2018). \n\n"}
{"id": "1809.02385", "contents": "Title: Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers Abstract: In recent years, data have become increasingly higher dimensional and,\ntherefore, an increased need has arisen for dimension reduction techniques for\nclustering. Although such techniques are firmly established in the literature\nfor multivariate data, there is a relative paucity in the area of matrix\nvariate, or three-way, data. Furthermore, the few methods that are available\nall assume matrix variate normality, which is not always sensible if cluster\nskewness or excess kurtosis is present. Mixtures of bilinear factor analyzers\nusing skewed matrix variate distributions are proposed. In all, four such\nmixture models are presented, based on matrix variate skew-t, generalized\nhyperbolic, variance-gamma, and normal inverse Gaussian distributions,\nrespectively. \n\n"}
{"id": "1809.02857", "contents": "Title: Computational Sufficiency, Reflection Groups, and Generalized Lasso\n  Penalties Abstract: We study estimators with generalized lasso penalties within the computational\nsufficiency framework introduced by Vu (2018, arXiv:1807.05985). By\nrepresenting these penalties as support functions of zonotopes and more\ngenerally Minkowski sums of line segments and rays, we show that there is a\nnatural reflection group associated with the underlying optimization problem. A\nconsequence of this point of view is that for large classes of estimators\nsharing the same penalty, the penalized least squares estimator is\ncomputationally minimal sufficient. This means that all such estimators can be\ncomputed by refining the output of any algorithm for the least squares case. An\ninteresting technical component is our analysis of coordinate descent on the\ndual problem. A key insight is that the iterates are obtained by reflecting and\naveraging, so they converge to an element of the dual feasible set that is\nminimal with respect to a ordering induced by the group associated with the\npenalty. Our main application is fused lasso/total variation denoising and\nisotonic regression on arbitrary graphs. In those cases the associated group is\na permutation group. \n\n"}
{"id": "1809.03388", "contents": "Title: The Coordinate Sampler: A Non-Reversible Gibbs-like MCMC Sampler Abstract: In this article, we derive a novel non-reversible, continuous-time Markov\nchain Monte Carlo (MCMC) sampler, called Coordinate Sampler, based on a\npiecewise deterministic Markov process (PDMP), which can be seen as a variant\nof the Zigzag sampler. In addition to proving a theoretical validation for this\nnew sampling algorithm, we show that the Markov chain it induces exhibits\ngeometrical ergodicity convergence, for distributions whose tails decay at\nleast as fast as an exponential distribution and at most as fast as a Gaussian\ndistribution. Several numerical examples highlight that our coordinate sampler\nis more efficient than the Zigzag sampler, in terms of effective sample size. \n\n"}
{"id": "1809.03659", "contents": "Title: New models for symbolic data analysis Abstract: Symbolic data analysis (SDA) is an emerging area of statistics concerned with\nunderstanding and modelling data that takes distributional form (i.e. symbols),\nsuch as random lists, intervals and histograms. It was developed under the\npremise that the statistical unit of interest is the symbol, and that inference\nis required at this level. Here we consider a different perspective, which\nopens a new research direction in the field of SDA. We assume that, as with a\nstandard statistical analysis, inference is required at the level of\nindividual-level data. However, the individual-level data are aggregated into\nsymbols - group-based distributional-valued summaries - prior to the analysis.\nIn this way, large and complex datasets can be reduced to a smaller number of\ndistributional summaries, that may be analysed more efficiently than the\noriginal dataset. As such, we develop SDA techniques as a new approach for the\nanalysis of big data. In particular we introduce a new general method for\nconstructing likelihood functions for symbolic data based on a desired\nprobability model for the underlying measurement-level data, while only\nobserving the distributional summaries. This approach opens the door for new\nclasses of symbol design and construction, in addition to developing SDA as a\nviable tool to enable and improve upon classical data analyses, particularly\nfor very large and complex datasets. We illustrate this new direction for SDA\nresearch through several real and simulated data analyses. \n\n"}
{"id": "1809.03986", "contents": "Title: Efficient Statistics, in High Dimensions, from Truncated Samples Abstract: We provide an efficient algorithm for the classical problem, going back to\nGalton, Pearson, and Fisher, of estimating, with arbitrary accuracy the\nparameters of a multivariate normal distribution from truncated samples.\nTruncated samples from a $d$-variate normal ${\\cal\nN}(\\mathbf{\\mu},\\mathbf{\\Sigma})$ means a samples is only revealed if it falls\nin some subset $S \\subseteq \\mathbb{R}^d$; otherwise the samples are hidden and\ntheir count in proportion to the revealed samples is also hidden. We show that\nthe mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$ can be\nestimated with arbitrary accuracy in polynomial-time, as long as we have oracle\naccess to $S$, and $S$ has non-trivial measure under the unknown $d$-variate\nnormal distribution. Additionally we show that without oracle access to $S$,\nany non-trivial estimation is impossible. \n\n"}
{"id": "1809.04000", "contents": "Title: Statistical post-processing of hydrological forecasts using Bayesian\n  model averaging Abstract: Accurate and reliable probabilistic forecasts of hydrological quantities like\nrunoff or water level are beneficial to various areas of society. Probabilistic\nstate-of-the-art hydrological ensemble prediction models are usually driven\nwith meteorological ensemble forecasts. Hence, biases and dispersion errors of\nthe meteorological forecasts cascade down to the hydrological predictions and\nadd to the errors of the hydrological models. The systematic parts of these\nerrors can be reduced by applying statistical post-processing. For a sound\nestimation of predictive uncertainty and an optimal correction of systematic\nerrors, statistical post-processing methods should be tailored to the\nparticular forecast variable at hand. Former studies have shown that it can\nmake sense to treat hydrological quantities as bounded variables. In this\npaper, a doubly truncated Bayesian model averaging (BMA) method, which allows\nfor flexible post-processing of (multi-model) ensemble forecasts of water\nlevel, is introduced. A case study based on water level for a gauge of river\nRhine, reveals a good predictive skill of doubly truncated BMA compared both to\nthe raw ensemble and the reference ensemble model output statistics approach. \n\n"}
{"id": "1809.05065", "contents": "Title: Lyapunov analysis of multiscale dynamics: The slow bundle of the\n  two-scale Lorenz 96 model Abstract: We investigate the geometrical structure of instabilities in the two-scales\nLorenz 96 model through the prism of Lyapunov analysis. Our detailed study of\nthe full spectrum of covariant Lyapunov vectors reveals the presence of a slow\nbundle in tangent space, composed by a set of vectors with a significant\nprojection on the slow degrees of freedom; they correspond to the smallest (in\nabsolute value) Lyapunov exponents and thereby to the longer time scales. We\nshow that the dimension of the slow bundle is extensive in the number of both\nslow and fast degrees of freedom, and discuss its relationship with the results\nof a finite-size analysis of instabilities, supporting the conjecture that the\nslow-variable behavior is effectively determined by a non-trivial subset of\ndegrees of freedom. More precisely, we show that the slow bundle corresponds to\nthe Lyapunov spectrum region where fast and slow instability rates overlap,\n\"mixing\" their evolution into a set of vectors which simultaneously carry\ninformation on both scales. We suggest these results may pave the way for\nfuture applications to ensemble forecasting and data assimilations in weather\nand climate models. \n\n"}
{"id": "1809.05430", "contents": "Title: Surrogate-based global sensitivity analysis for turbulence and\n  fire-spotting effects in regional-scale wildland fire modeling Abstract: In presence of strong winds, wildfires feature nonlinear behavior, possibly\ninducing fire-spotting. We present a global sensitivity analysis of a new\nsub-model for turbulence and fire-spotting included in a wildfire spread model\nbased on a stochastic representation of the fireline. To limit the number of\nmodel evaluations, fast surrogate models based on generalized Polynomial Chaos\n(gPC) and Gaussian Process are used to identify the key parameters affecting\ntopology and size of burnt area. This study investigates the application of\nthese surrogates to compute Sobol' sensitivity indices in an idealized test\ncase. The wind is known to drive the fire propagation. The results show that it\nis a more general leading factor that governs the generation of secondary\nfires. This study also compares the performance of the surrogates for varying\nsize and type of training sets as well as for varying parameterization and\nchoice of algorithms. The best performance was achieved using a gPC strategy\nbased on a sparse least-angle regression (LAR) and a low-discrepancy Halton's\nsequence. Still, the LAR-based gPC surrogate tends to filter out the\ninformation coming from parameters with large length-scale, which is not the\ncase of the cleaning-based gPC surrogate. For both algorithms, sparsity ensures\na surrogate can be built using an affordable number of forward model\nevaluations, while the model response is highly multi-scale and nonlinear.\nUsing a sparse surrogate is thus a promising strategy to analyze new models and\nits dependency on input parameters in wildfire applications. \n\n"}
{"id": "1809.06052", "contents": "Title: Parameter Estimation of absolute continuous four parameter Geometric\n  Marshall-Olkin bivariate Pareto Distribution Abstract: In this paper we formulate a four parameter absolute continuous Geometric\nMarshall-Olkin bivariate Pareto distribution and study its parameter estimation\nthrough EM algorithm and also explore the bayesian analysis through slice cum\nGibbs sampler approach. Numerical results are shown to verify the performance\nof the algorithms. We illustrate the procedures through a real life data\nanalysis. \n\n"}
{"id": "1809.06405", "contents": "Title: Bayesian analysis of absolute continuous Marshall-Olkin bivariate Pareto\n  distribution with location and scale parameters Abstract: This paper provides two different novel approaches of slice sampling to\nestimate the parameters of absolute continuous Marshall-Olkin bivariate Pareto\ndistribution with location and scale parameters. We carry out the bayesian\nanalysis taking gamma prior for shape and scale parameters and truncated normal\nfor location parameters. Credible intervals and coverage probabilities are also\nprovided for all methods. A real-life data analysis is shown for illustrative\npurpose. \n\n"}
{"id": "1809.06894", "contents": "Title: Retrieval analysis of 38 WFC3 transmission spectra and resolution of the\n  normalisation degeneracy Abstract: A comprehensive analysis of 38 previously published Wide Field Camera 3\n(WFC3) transmission spectra is performed using a hierarchy of nested-sampling\nretrievals: with versus without clouds, grey versus non-grey clouds, isothermal\nversus non-isothermal transit chords and with water, hydrogen cyanide and/or\nammonia. We revisit the \"normalisation degeneracy\": the relative abundances of\nmolecules are degenerate at the order-of-magnitude level with the absolute\nnormalisation of the transmission spectrum. Using a suite of mock retrievals,\nwe demonstrate that the normalisation degeneracy may be partially broken using\nWFC3 data alone, even in the absence of optical/visible data and without\nappealing to the presence of patchy clouds, although lower limits to the mixing\nratios may be prior-dominated depending on the measurement uncertainties. With\nJames Webb Space Telescope-like spectral resolutions, the normalisation\ndegeneracy may be completely broken from infrared spectra alone. We find no\ntrend in the retrieved water abundances across nearly two orders of magnitude\nin exoplanet mass and a factor of 5 in retrieved temperature (about 500 to 2500\nK). We further show that there is a general lack of strong Bayesian evidence to\nsupport interpretations of non-grey over grey clouds (only for WASP-69b and\nWASP-76b) and non-isothermal over isothermal atmospheres (no objects). 35 out\nof 38 WFC3 transmission spectra are well-fitted by an isothermal transit chord\nwith grey clouds and water only, while 8 are adequately explained by flat\nlines. Generally, the cloud composition is unconstrained. \n\n"}
{"id": "1809.06924", "contents": "Title: Modelling sea ice and melt ponds evolution: sensitivity to microscale\n  heat transfer mechanisms Abstract: We present a mathematical model describing the evolution of sea ice and\nmeltwater during summer. The system is described by two coupled partial\ndifferential equations for the ice thickness $h$ and pond depth $w$ fields. We\ntest the sensitivity of the model to variations of parameters controlling\nfluid-dynamic processes at the pond level, namely the variation of turbulent\nheat flux with pond depth and the lateral melting of ice enclosing a pond. We\nobserve that different heat flux scalings determine different rates of total\nsurface ablations, while the system is relatively robust in terms of\nprobability distributions of pond surface areas. Finally, we study pond\nmorphology in terms of fractal dimensions, showing that the role of lateral\nmelting is minor, whereas there is evidence of an impact from the initial sea\nice topography. \n\n"}
{"id": "1809.07763", "contents": "Title: auditor: an R Package for Model-Agnostic Visual Validation and\n  Diagnostics Abstract: Machine learning models have spread to almost every area of life. They are\nsuccessfully applied in biology, medicine, finance, physics, and other fields.\nWith modern software it is easy to train even a~complex model that fits the\ntraining data and results in high accuracy on the test set. The problem arises\nwhen models fail confronted with real-world data.\n  This paper describes methodology and tools for model-agnostic audit.\nIntroduced techniques facilitate assessing and comparing the goodness of fit\nand performance of models. In~addition, they may be used for the analysis of\nthe similarity of residuals and for identification of~outliers and influential\nobservations. The examination is carried out by diagnostic scores and visual\nverification.\n  Presented methods were implemented in the auditor package for R. Due to\nflexible and~consistent grammar, it is simple to validate models of any\nclasses. \n\n"}
{"id": "1809.07905", "contents": "Title: Subgroup Identification Using the personalized Package Abstract: A plethora of disparate statistical methods have been proposed for subgroup\nidentification to help tailor treatment decisions for patients. However a\nmajority of them do not have corresponding R packages and the few that do\npertain to particular statistical methods or provide little means of evaluating\nwhether meaningful subgroups have been found. Recently, the work of Chen, Tian,\nCai, and Yu (2017) unified many of these subgroup identification methods into\none general, consistent framework. The goal of the personalized package is to\nprovide a corresponding unified software framework for subgroup identification\nanalyses that provides not only estimation of subgroups, but evaluation of\ntreatment effects within estimated subgroups. The personalized package allows\nfor a variety of subgroup identification methods for many types of outcomes\ncommonly encountered in medical settings. The package is built to incorporate\nthe entire subgroup identification analysis pipeline including propensity score\ndiagnostics, subgroup estimation, analysis of the treatment effects within\nsubgroups, and evaluation of identified subgroups. In this framework, different\nmethods can be accessed with little change in the analysis code. Similarly, new\nmethods can easily be incorporated into the package. Besides familiar\nstatistical models, the package also allows flexible machine learning tools to\nbe leveraged in subgroup identification. Further estimation improvements can be\nobtained via efficiency augmentation. \n\n"}
{"id": "1809.08671", "contents": "Title: Jovian vortices and jets Abstract: We explore the conditions required for isolated vortices to exist in sheared\nzonal flows and the stability of the underlying zonal winds. This is done using\nthe standard 2-layer quasigeostrophic model with the lower layer depth becoming\ninfinite; however, this model differs from the usual layer model because the\nlower layer is not assumed to be motionless but has a steady configuration of\nalternating zonal flows [1]. Steady state vortices are obtained by a simulated\nannealing computational method introduced in [2], generalized and applied in\n[3] in fluid flow, and used in the context of magnetohydrodynamics in [4-6].\nVarious cases of vortices with a constant potential vorticity anomaly atop\nzonal winds and the stability of the underlying winds are considered using a\nmix of computational and analytical techniques. \n\n"}
{"id": "1809.08672", "contents": "Title: Buoyant Motion of a Turbulent Thermal Abstract: By introducing an equivalence between magnetostatics and the equations\ngoverning buoyant motion, we derive analytical expressions for the acceleration\nof isolated density anomalies, a.k.a. thermals. In particular, we investigate\nbuoyant acceleration, defined as the sum of the Archimedean buoyancy $B$ and an\nassociated perturbation pressure gradient. For the case of a uniform spherical\nthermal, the anomaly fluid accelerates at $2B/3$, extending the textbook result\nfor the induced mass of a solid sphere to the case of a fluid sphere. For a\nmore general ellipsoidal thermal, we show that the buoyant acceleration is a\nsimple analytical function of the ellipsoid's aspect ratio. The relevance of\nthese idealized uniform-density results to turbulent thermals is explored by\nanalyzing direct numerical simulations of thermals at $\\textit{Re}=6300$. We\nfind that our results fully characterize a thermal's initial motion over a\ndistance comparable to its length. Beyond this buoyancy-dominated regime, a\nthermal develops an ellipsoidal vortex circulation and begins to entrain\nenvironmental fluid. Our analytical expressions do not describe the total\nacceleration of this mature thermal, but still accurately relate the buoyant\nacceleration to the thermal's mean Archimedean buoyancy and aspect ratio. Thus,\nour analytical formulae provide a simple and direct means of estimating the\nbuoyant acceleration of turbulent thermals. \n\n"}
{"id": "1809.09238", "contents": "Title: Flexible Mixture Modeling on Constrained Spaces Abstract: This paper addresses challenges in flexibly modeling multimodal data that lie\non constrained spaces. Such data are commonly found in spatial applications,\nsuch as climatology and criminology, where measurements are restricted to a\ngeographical area. Other settings include domains where unsuitable recordings\nare discarded, such as flow-cytometry measurements. A simple approach to\nmodeling such data is through the use of mixture models, especially\nnonparametric mixture models. Mixture models, while flexible and theoretically\nwell-understood, are unsuitable for settings involving complicated constraints,\nleading to difficulties in specifying the component distributions and in\nevaluating normalization constants. Bayesian inference over the parameters of\nthese models results in posterior distributions that are doubly-intractable. We\naddress this problem via an algorithm based on rejection sampling and data\naugmentation. We view samples from a truncated distribution as outcomes of a\nrejection sampling scheme, where proposals are made from a simple mixture model\nand are rejected if they violate the constraints. Our scheme proceeds by\nimputing the rejected samples given mixture parameters and then resampling\nparameters given all samples. We study two modeling approaches: mixtures of\ntruncated Gaussians and truncated mixtures of Gaussians, along with their\nassociated Markov chain Monte Carlo sampling algorithms. We also discuss\nvariations of the models, as well as approximations to improve mixing, reduce\ncomputational cost, and lower variance. We present results on simulated data\nand apply our algorithms to two problems; one involving flow-cytometry data,\nand the other, crime recorded in the city of Chicago. \n\n"}
{"id": "1810.00118", "contents": "Title: Multilevel Optimal Transport: a Fast Approximation of Wasserstein-1\n  distances Abstract: We propose a fast algorithm for the calculation of the Wasserstein-1\ndistance, which is a particular type of optimal transport distance with\nhomogeneous of degree one transport cost.\n  Our algorithm is built on multilevel primal-dual algorithms. Several\nnumerical examples and a complexity analysis are provided to demonstrate its\ncomputational speed. On some commonly used image examples of size\n$512\\times512$, the proposed algorithm gives solutions within $0.2\\sim 1.5$\nseconds on a single CPU, which is much faster than the state-of-the-art\nalgorithms. \n\n"}
{"id": "1810.01710", "contents": "Title: Multilevel Monte Carlo Acceleration of Seismic Wave Propagation under\n  Uncertainty Abstract: We interpret uncertainty in a model for seismic wave propagation by treating\nthe model parameters as random variables, and apply the Multilevel Monte Carlo\n(MLMC) method to reduce the cost of approximating expected values of selected,\nphysically relevant, quantities of interest (QoI) with respect to the random\nvariables. Targeting source inversion problems, where the source of an\nearthquake is inferred from ground motion recordings on the Earth's surface, we\nconsider two QoI that measure the discrepancies between computed seismic\nsignals and given reference signals: one QoI, $\\hbox{QoI}_E$, is defined in\nterms of the $L^2$-misfit, which is directly related to maximum likelihood\nestimates of the source parameters; the other, $\\hbox{QoI}_W$, is based on the\nquadratic Wasserstein distance between probability distributions, and\nrepresents one possible choice in a class of such misfit functions that have\nbecome increasingly popular to solve seismic inversion in recent years. We\nsimulate seismic wave propagation, including seismic attenuation, using a\npublicly available code in widespread use, based on the spectral element\nmethod. Using random coefficients and deterministic initial and boundary data,\nwe present benchmark numerical experiments with synthetic data in a\ntwo-dimensional physical domain and a one-dimensional velocity model where the\nassumed parameter uncertainty is motivated by realistic Earth models. Here, the\ncomputational cost of the standard Monte Carlo method was reduced by up to 97%\nfor $\\hbox{QoI}_E$, and up to 78% for $\\hbox{QoI}_W$, using a relevant range of\ntolerances. Shifting to three-dimensional domains is straight-forward and will\nfurther increase the relative computational work reduction. \n\n"}
{"id": "1810.02030", "contents": "Title: Robust Estimation and Generative Adversarial Nets Abstract: Robust estimation under Huber's $\\epsilon$-contamination model has become an\nimportant topic in statistics and theoretical computer science. Statistically\noptimal procedures such as Tukey's median and other estimators based on depth\nfunctions are impractical because of their computational intractability. In\nthis paper, we establish an intriguing connection between $f$-GANs and various\ndepth functions through the lens of $f$-Learning. Similar to the derivation of\n$f$-GANs, we show that these depth functions that lead to statistically optimal\nrobust estimators can all be viewed as variational lower bounds of the total\nvariation distance in the framework of $f$-Learning. This connection opens the\ndoor of computing robust estimators using tools developed for training GANs. In\nparticular, we show in both theory and experiments that some appropriate\nstructures of discriminator networks with hidden layers in GANs lead to\nstatistically optimal robust location estimators for both Gaussian distribution\nand general elliptical distributions where first moment may not exist. \n\n"}
{"id": "1810.06391", "contents": "Title: Wind Power Persistence Characterized by Superstatistics Abstract: Mitigating climate change demands a transition towards renewable electricity\ngeneration, with wind power being a particularly promising technology. Long\nperiods either of high or of low wind therefore essentially define the\nnecessary amount of storage to balance the power system. While the general\nstatistics of wind velocities have been studied extensively, persistence\n(waiting) time statistics of wind is far from well understood. Here, we\ninvestigate the statistics of both high- and low-wind persistence. We find\nheavy tails and explain them as a superposition of different wind conditions,\nrequiring $q$-exponential distributions instead of exponential distributions.\nPersistent wind conditions are not necessarily caused by stationary atmospheric\ncirculation patterns nor by recurring individual weather types but may emerge\nas a combination of multiple weather types and circulation patterns.\nUnderstanding wind persistence statistically and synoptically, may help to\nensure a reliable and economically feasible future energy system, which uses a\nhigh share of wind generation. \n\n"}
{"id": "1810.07475", "contents": "Title: Collision fluctuations of lucky droplets with superdroplets Abstract: It was previously shown that the superdroplet algorithm for modeling the\ncollision-coalescence process can faithfully represent mean droplet growth in\nturbulent clouds. But an open question is how accurately the superdroplet\nalgorithm accounts for fluctuations in the collisional aggregation process.\nSuch fluctuations are particularly important in dilute suspensions. Even in the\nabsence of turbulence, Poisson fluctuations of collision times in dilute\nsuspensions may result in substantial variations in the growth process,\nresulting in a broad distribution of growth times to reach a certain droplet\nsize. We quantify the accuracy of the superdroplet algorithm in describing the\nfluctuating growth history of a larger droplet that settles under the effect of\ngravity in a quiescent fluid and collides with a dilute suspension of smaller\ndroplets that were initially randomly distributed in space ('lucky droplet\nmodel'). We assess the effect of fluctuations upon the growth history of the\nlucky droplet and compute the distribution of cumulative collision times. The\nlatter is shown to be sensitive enough to detect the subtle increase of\nfluctuations associated with collisions between multiple lucky droplets. The\nsuperdroplet algorithm incorporates fluctuations in two distinct ways: through\nthe random spatial distribution of superdroplets and through the Monte Carlo\ncollision algorithm involved. Using specifically designed numerical\nexperiments, we show that both on their own give an accurate representation of\nfluctuations. We conclude that the superdroplet algorithm can faithfully\nrepresent fluctuations in the coagulation of droplets driven by gravity. \n\n"}
{"id": "1810.08316", "contents": "Title: Heteroskedastic PCA: Algorithm, Optimality, and Applications Abstract: A general framework for principal component analysis (PCA) in the presence of\nheteroskedastic noise is introduced. We propose an algorithm called HeteroPCA,\nwhich involves iteratively imputing the diagonal entries of the sample\ncovariance matrix to remove estimation bias due to heteroskedasticity. This\nprocedure is computationally efficient and provably optimal under the\ngeneralized spiked covariance model. A key technical step is a deterministic\nrobust perturbation analysis on singular subspaces, which can be of independent\ninterest. The effectiveness of the proposed algorithm is demonstrated in a\nsuite of problems in high-dimensional statistics, including singular value\ndecomposition (SVD) under heteroskedastic noise, Poisson PCA, and SVD for\nheteroskedastic and incomplete data. \n\n"}
{"id": "1810.08918", "contents": "Title: Multiple Scaled Contaminated Normal Distribution and Its Application in\n  Clustering Abstract: The multivariate contaminated normal (MCN) distribution represents a simple\nheavy-tailed generalization of the multivariate normal (MN) distribution to\nmodel elliptical contoured scatters in the presence of mild outliers, referred\nto as \"bad\" points. The MCN can also automatically detect bad points. The price\nof these advantages is two additional parameters, both with specific and useful\ninterpretations: proportion of good observations and degree of contamination.\nHowever, points may be bad in some dimensions but good in others. The use of an\noverall proportion of good observations and of an overall degree of\ncontamination is limiting. To overcome this limitation, we propose a multiple\nscaled contaminated normal (MSCN) distribution with a proportion of good\nobservations and a degree of contamination for each dimension. Once the model\nis fitted, each observation has a posterior probability of being good with\nrespect to each dimension. Thanks to this probability, we have a method for\nsimultaneous directional robust estimation of the parameters of the MN\ndistribution based on down-weighting and for the automatic directional\ndetection of bad points by means of maximum a posteriori probabilities. The\nterm \"directional\" is added to specify that the method works separately for\neach dimension. Mixtures of MSCN distributions are also proposed as an\napplication of the proposed model for robust clustering. An extension of the EM\nalgorithm is used for parameter estimation based on the maximum likelihood\napproach. Real and simulated data are used to show the usefulness of our\nmixture with respect to well-established mixtures of symmetric distributions\nwith heavy tails. \n\n"}
{"id": "1810.10285", "contents": "Title: Flow structures govern particle collisions in turbulence Abstract: The role of the spatial structure of a turbulent flow in enhancing particle\ncollision rates in suspensions is an open question. We show and quantify, as a\nfunction of particle inertia, the correlation between the multiscale structures\nof turbulence and particle collisions: Straining zones contribute predominantly\nto rapid head-on collisions compared to vortical regions. We also discover the\nimportance of vortex-strain worm-rolls, which goes beyond ideas of preferential\nconcentration and may explain the rapid growth of aggregates in natural\nprocesses, such as the initiation of rain in warm clouds. \n\n"}
{"id": "1810.10883", "contents": "Title: Fast Exact Bayesian Inference for Sparse Signals in the Normal Sequence\n  Model Abstract: We consider exact algorithms for Bayesian inference with model selection\npriors (including spike-and-slab priors) in the sparse normal sequence model.\nBecause the best existing exact algorithm becomes numerically unstable for\nsample sizes over n=500, there has been much attention for alternative\napproaches like approximate algorithms (Gibbs sampling, variational Bayes,\netc.), shrinkage priors (e.g. the Horseshoe prior and the Spike-and-Slab LASSO)\nor empirical Bayesian methods. However, by introducing algorithmic ideas from\nonline sequential prediction, we show that exact calculations are feasible for\nmuch larger sample sizes: for general model selection priors we reach n=25000,\nand for certain spike-and-slab priors we can easily reach n=100000. We further\nprove a de Finetti-like result for finite sample sizes that characterizes\nexactly which model selection priors can be expressed as spike-and-slab priors.\nThe computational speed and numerical accuracy of the proposed methods are\ndemonstrated in experiments on simulated data, on a differential gene\nexpression data set, and to compare the effect of multiple hyper-parameter\nsettings in the beta-binomial prior. In our experimental evaluation we compute\nguaranteed bounds on the numerical accuracy of all new algorithms, which shows\nthat the proposed methods are numerically reliable whereas an alternative based\non long division is not. \n\n"}
{"id": "1810.11130", "contents": "Title: Robust Importance Sampling with Adaptive Winsorization Abstract: Importance sampling is a widely used technique to estimate properties of a\ndistribution. This paper investigates trading-off some bias for variance by\nadaptively winsorizing the importance sampling estimator. The novel winsorizing\nprocedure, based on the Balancing Principle (or Lepskii's Method), chooses a\nthreshold level among a pre-defined set by roughly balancing the bias and\nvariance of the estimator when winsorized at different levels. As a\nconsequence, it provides a principled way to perform winsorization with\nfinite-sample optimality guarantees under minimal assumptions. In various\nexamples, the proposed estimator is shown to have smaller mean squared error\nand mean absolute deviation than leading alternatives. \n\n"}
{"id": "1810.12161", "contents": "Title: Regularized Maximum Likelihood Estimation and Feature Selection in\n  Mixtures-of-Experts Models Abstract: Mixture of Experts (MoE) are successful models for modeling heterogeneous\ndata in many statistical learning problems including regression, clustering and\nclassification. Generally fitted by maximum likelihood estimation via the\nwell-known EM algorithm, their application to high-dimensional problems is\nstill therefore challenging. We consider the problem of fitting and feature\nselection in MoE models, and propose a regularized maximum likelihood\nestimation approach that encourages sparse solutions for heterogeneous\nregression data models with potentially high-dimensional predictors. Unlike\nstate-of-the art regularized MLE for MoE, the proposed modelings do not require\nan approximate of the penalty function. We develop two hybrid EM algorithms: an\nExpectation-Majorization-Maximization (EM/MM) algorithm, and an EM algorithm\nwith coordinate ascent algorithm. The proposed algorithms allow to\nautomatically obtaining sparse solutions without thresholding, and avoid matrix\ninversion by allowing univariate parameter updates. An experimental study shows\nthe good performance of the algorithms in terms of recovering the actual sparse\nsolutions, parameter estimation, and clustering of heterogeneous regression\ndata. \n\n"}
{"id": "1810.12451", "contents": "Title: Hurricane's maximum potential intensity and surface heat fluxes Abstract: Emanuel's concept of Maximum Potential Intensity (E-PI) relates the maximum\nvelocity $V_{\\rm max}$ of tropical storms, assumed to be in gradient wind\nbalance, to environmental parameters. Several studies suggested that the\nunbalanced flow is responsible for E-PI sometimes significantly underpredicting\n$V_{\\rm max}$. Additionally, two major modifications generated a considerable\nrange of E-PI predictions: the dissipative heating and the power expended to\nlift water were respectively suggested to increase and reduce E-PI $V_{\\rm\nmax}$ by about 20%. Here we re-derive the E-PI concept separating its dynamic\nand thermodynamic assumptions and lifting the gradient wind balance limitation.\nOur analysis reveals that E-PI formulations for a balanced and a radially\nunbalanced flow are similar, while the systematic underestimate of $V_{\\rm\nmax}$ reflects instead an incompatibility between several E-PI assumptions. We\ndiscuss how these assumptions can be modified. We further show that\nirrespective of whether dissipative heating occurs or not, E-PI uniquely\nrelates $V_{\\rm max}$ to the latent heat flux (not to the total oceanic heat\nflux as originally proposed). We clarify that, in contrast to previous\nsuggestions, lifting water has little impact on E-PI. We demonstrate that in\nE-PI the negative work of the pressure gradient in the upper atmosphere\nconsumes all the kinetic energy generated in the boundary layer. This key\ndynamic constraint is independent of other E-PI assumptions and thus can apply\nto diverse circulation patterns. Finally, we show that the E-PI maximum kinetic\nenergy per unit volume equals the local partial pressure of water vapor and\ndiscuss the implications of this finding for predicting $V_{\\rm max}$. \n\n"}
{"id": "1811.01034", "contents": "Title: Regime Transition in the Energy Cascade of Rotating Turbulence Abstract: Transition from a split to a forward kinetic energy cascade system is\nexplored in the context of rotating turbulence using direct numerical\nsimulations with a three-dimensional isotropic random force uncorrelated with\nthe velocity field. Our parametric study covers confinement effects in large\naspect ratio domains and a broad range of rotation rates. Results indicate that\nfor fixed geometrical dimensions the Rossby number acts as a control parameter,\nwhereas for a fixed Rossby number the product of the domain size along the\nrotation axis and forcing wavenumber governs the amount of energy that cascades\ninversely. The regime transition criterion hence depends on both control\nparameters. \n\n"}
{"id": "1811.02609", "contents": "Title: A Variational Inference Algorithm for BKMR in the Cross-Sectional\n  Setting Abstract: The identification of pollutant effects is an important task in environmental\nhealth. Bayesian kernel machine regression (BKMR) is a standard tool for\ninference of individual-level pollutant health-effects, and we present a mean\nfield Variational Inference (VI) algorithm for quick inference when only a\nsingle response per individual is recorded. Using simulation studies in the\ncase of informative priors, we show that VI, although fast, produces\nanti-conservative credible intervals of covariate effects and conservative\ncredible intervals for pollutant effects. To correct the coverage probabilities\nof covariate effects, we propose a simple Generalized Least Squares (GLS)\napproach that induces conservative credible intervals. We also explore using\nBKMR with flat priors and find that, while slower than the case with\ninformative priors, this approach yields uncorrected credible intervals for\ncovariate effects with coverage probabilities that are much closer to the\nnominal 95% level. We further note that fitting BKMR by VI provides a\nremarkable improvement in speed over existing MCMC methods. \n\n"}
{"id": "1811.02963", "contents": "Title: Simulation-based inference methods for partially observed Markov model\n  via the R package is2 Abstract: Partially observed Markov process (POMP) models are powerful tools for time\nseries modeling and analysis. Inherited the flexible framework of R package\npomp, the is2 package extends some useful Monte Carlo statistical methodologies\nto improve on convergence rates. A variety of efficient statistical methods for\nPOMP models have been developed including fixed lag smoothing, second-order\niterated smoothing, momentum iterated filtering, average iterated filtering,\naccelerate iterated filtering and particle iterated filtering. In this paper,\nwe show the utility of these methodologies based on two toy problems. We also\ndemonstrate the potential of some methods in a more complex model, employing a\nnonlinear epidemiological model with a discrete population, seasonality, and\nextra-demographic stochasticity. We discuss the extension beyond POMP models\nand the development of additional methods within the framework provided by is2. \n\n"}
{"id": "1811.03204", "contents": "Title: An Efficient Algorithm for High-Dimensional Log-Concave Maximum\n  Likelihood Abstract: The log-concave maximum likelihood estimator (MLE) problem answers: for a set\nof points $X_1,...X_n \\in \\mathbb R^d$, which log-concave density maximizes\ntheir likelihood? We present a characterization of the log-concave MLE that\nleads to an algorithm with runtime $poly(n,d, \\frac 1 \\epsilon,r)$ to compute a\nlog-concave distribution whose log-likelihood is at most $\\epsilon$ less than\nthat of the MLE, and $r$ is parameter of the problem that is bounded by the\n$\\ell_2$ norm of the vector of log-likelihoods the MLE evaluated at\n$X_1,...,X_n$. \n\n"}
{"id": "1811.04817", "contents": "Title: A test case for application of convolutional neural networks to\n  spatio-temporal climate data: Re-identifying clustered weather patterns Abstract: Convolutional neural networks (CNNs) can potentially provide powerful tools\nfor classifying and identifying patterns in climate and environmental data.\nHowever, because of the inherent complexities of such data, which are often\nspatio-temporal, chaotic, and non-stationary, the CNN algorithms must be\ndesigned/evaluated for each specific dataset and application. Yet to start,\nCNN, a supervised technique, requires a large labeled dataset. Labeling demands\n(human) expert time, which combined with the limited number of relevant\nexamples in this area, can discourage using CNNs for new problems. To address\nthese challenges, here we (1) Propose an effective auto-labeling strategy based\non using an unsupervised clustering algorithm and evaluating the performance of\nCNNs in re-identifying these clusters; (2) Use this approach to label thousands\nof daily large-scale weather patterns over North America in the outputs of a\nfully-coupled climate model and show the capabilities of CNNs in re-identifying\nthe 4 clustered regimes. The deep CNN trained with $1000$ samples or more per\ncluster has an accuracy of $90\\%$ or better. Accuracy scales monotonically but\nnonlinearly with the size of the training set, e.g. reaching $94\\%$ with $3000$\ntraining samples per cluster. Effects of architecture and hyperparameters on\nthe performance of CNNs are examined and discussed. \n\n"}
{"id": "1811.05073", "contents": "Title: Regularized Zero-Variance Control Variates Abstract: Zero-variance control variates (ZV-CV) are a post-processing method to reduce\nthe variance of Monte Carlo estimators of expectations using the derivatives of\nthe log target. Once the derivatives are available, the only additional\ncomputational effort lies in solving a linear regression problem. Significant\nvariance reductions have been achieved with this method in low dimensional\nexamples, but the number of covariates in the regression rapidly increases with\nthe dimension of the target. In this paper, we present compelling empirical\nevidence that the use of penalized regression techniques in the selection of\nhigh-dimensional control variates provides performance gains over the classical\nleast squares method. Another type of regularization based on using subsets of\nderivatives, or a priori regularization as we refer to it in this paper, is\nalso proposed to reduce computational and storage requirements. Several\nexamples showing the utility and limitations of regularized ZV-CV for Bayesian\ninference are given. The methods proposed in this paper are accessible through\nthe R package ZVCV. \n\n"}
{"id": "1811.06150", "contents": "Title: Effect Handling for Composable Program Transformations in Edward2 Abstract: Algebraic effects and handlers have emerged in the programming languages\ncommunity as a convenient, modular abstraction for controlling computational\neffects. They have found several applications including concurrent programming,\nmeta programming, and more recently, probabilistic programming, as part of\nPyro's Poutines library. We investigate the use of effect handlers as a\nlightweight abstraction for implementing probabilistic programming languages\n(PPLs). We interpret the existing design of Edward2 as an accidental\nimplementation of an effect-handling mechanism, and extend that design to\nsupport nested, composable transformations. We demonstrate that this enables\nstraightforward implementation of sophisticated model transformations and\ninference algorithms. \n\n"}
{"id": "1811.09469", "contents": "Title: Parallel sequential Monte Carlo for stochastic gradient-free nonconvex\n  optimization Abstract: We introduce and analyze a parallel sequential Monte Carlo methodology for\nthe numerical solution of optimization problems that involve the minimization\nof a cost function that consists of the sum of many individual components. The\nproposed scheme is a stochastic zeroth order optimization algorithm which\ndemands only the capability to evaluate small subsets of components of the cost\nfunction. It can be depicted as a bank of samplers that generate particle\napproximations of several sequences of probability measures. These measures are\nconstructed in such a way that they have associated probability density\nfunctions whose global maxima coincide with the global minima of the original\ncost function. The algorithm selects the best performing sampler and uses it to\napproximate a global minimum of the cost function. We prove analytically that\nthe resulting estimator converges to a global minimum of the cost function\nalmost surely and provide explicit convergence rates in terms of the number of\ngenerated Monte Carlo samples and the dimension of the search space. We show,\nby way of numerical examples, that the algorithm can tackle cost functions with\nmultiple minima or with broad \"flat\" regions which are hard to minimize using\ngradient-based techniques. \n\n"}
{"id": "1811.10275", "contents": "Title: Rejoinder for \"Probabilistic Integration: A Role in Statistical\n  Computation?\" Abstract: This article is the rejoinder for the paper \"Probabilistic Integration: A\nRole in Statistical Computation?\" to appear in Statistical Science with\ndiscussion. We would first like to thank the reviewers and many of our\ncolleagues who helped shape this paper, the editor for selecting our paper for\ndiscussion, and of course all of the discussants for their thoughtful,\ninsightful and constructive comments. In this rejoinder, we respond to some of\nthe points raised by the discussants and comment further on the fundamental\nquestions underlying the paper: (i) Should Bayesian ideas be used in numerical\nanalysis?, and (ii) If so, what role should such approaches have in statistical\ncomputation? \n\n"}
{"id": "1811.11733", "contents": "Title: orthoDr: Semiparametric Dimension Reduction via Orthogonality\n  Constrained Optimization Abstract: orthoDr is a package in R that solves dimension reduction problems using\northogonality constrained optimization approach. The package serves as a\nunified framework for many regression and survival analysis dimension reduction\nmodels that utilize semiparametric estimating equations. The main computational\nmachinery of orthoDr is a first-order algorithm developed by\n\\cite{wen2013feasible} for optimization within the Stiefel manifold. We\nimplement the algorithm through Rcpp and OpenMP for fast computation. In\naddition, we developed a general-purpose solver for such constrained problems\nwith user-specified objective functions, which works as a drop-in version of\noptim(). The package also serves as a platform for future methodology\ndevelopments along this line of work. \n\n"}
{"id": "1812.01553", "contents": "Title: Batch Selection for Parallelisation of Bayesian Quadrature Abstract: Integration over non-negative integrands is a central problem in machine\nlearning (e.g. for model averaging, (hyper-)parameter marginalisation, and\ncomputing posterior predictive distributions). Bayesian Quadrature is a\nprobabilistic numerical integration technique that performs promisingly when\ncompared to traditional Markov Chain Monte Carlo methods. However, in contrast\nto easily-parallelised MCMC methods, Bayesian Quadrature methods have, thus\nfar, been essentially serial in nature, selecting a single point to sample at\neach step of the algorithm. We deliver methods to select batches of points at\neach step, based upon those recently presented in the Batch Bayesian\nOptimisation literature. Such parallelisation significantly reduces computation\ntime, especially when the integrand is expensive to sample. \n\n"}
{"id": "1812.01655", "contents": "Title: A probabilistic incremental proximal gradient method Abstract: In this paper, we propose a probabilistic optimization method, named\nprobabilistic incremental proximal gradient (PIPG) method, by developing a\nprobabilistic interpretation of the incremental proximal gradient algorithm. We\nexplicitly model the update rules of the incremental proximal gradient method\nand develop a systematic approach to propagate the uncertainty of the solution\nestimate over iterations. The PIPG algorithm takes the form of Bayesian\nfiltering updates for a state-space model constructed by using the cost\nfunction. Our framework makes it possible to utilize well-known exact or\napproximate Bayesian filters, such as Kalman or extended Kalman filters, to\nsolve large-scale regularized optimization problems. \n\n"}
{"id": "1812.02609", "contents": "Title: A Framework for Adaptive MCMC Targeting Multimodal Distributions Abstract: We propose a new Monte Carlo method for sampling from multimodal\ndistributions. The idea of this technique is based on splitting the task into\ntwo: finding the modes of a target distribution $\\pi$ and sampling, given the\nknowledge of the locations of the modes. The sampling algorithm relies on steps\nof two types: local ones, preserving the mode; and jumps to regions associated\nwith different modes. Besides, the method learns the optimal parameters of the\nalgorithm while it runs, without requiring user intervention. Our technique\nshould be considered as a flexible framework, in which the design of moves can\nfollow various strategies known from the broad MCMC literature.\n  In order to design an adaptive scheme that facilitates both local and jump\nmoves, we introduce an auxiliary variable representing each mode and we define\na new target distribution $\\tilde{\\pi}$ on an augmented state space\n$\\mathcal{X}~\\times~\\mathcal{I}$, where $\\mathcal{X}$ is the original state\nspace of $\\pi$ and $\\mathcal{I}$ is the set of the modes. As the algorithm runs\nand updates its parameters, the target distribution $\\tilde{\\pi}$ also keeps\nbeing modified. This motivates a new class of algorithms, Auxiliary Variable\nAdaptive MCMC. We prove general ergodic results for the whole class before\nspecialising to the case of our algorithm. \n\n"}
{"id": "1812.04403", "contents": "Title: Encoding prior knowledge in the structure of the likelihood Abstract: The inference of deep hierarchical models is problematic due to strong\ndependencies between the hierarchies. We investigate a specific transformation\nof the model parameters based on the multivariate distributional transform.\nThis transformation is a special form of the reparametrization trick, flattens\nthe hierarchy and leads to a standard Gaussian prior on all resulting\nparameters. The transformation also transfers all the prior information into\nthe structure of the likelihood, hereby decoupling the transformed parameters a\npriori from each other. A variational Gaussian approximation in this\nstandardized space will be excellent in situations of relatively uninformative\ndata. Additionally, the curvature of the log-posterior is well-conditioned in\ndirections that are weakly constrained by the data, allowing for fast inference\nin such a scenario. In an example we perform the transformation explicitly for\nGaussian process regression with a priori unknown correlation structure. Deep\nmodels are inferred rapidly in highly and slowly in poorly informed situations.\nThe flat model show exactly the opposite performance pattern. A synthesis of\nboth, the deep and the flat perspective, provides their combined advantages and\novercomes the individual limitations, leading to a faster inference. \n\n"}
{"id": "1812.05658", "contents": "Title: Comment on \"Distinct Thresholds for the Initiation and Cessation of\n  Aeolian Saltation From Field Measurements\" by Raleigh L. Martin and Jasper F.\n  Kok: Alternative Interpretation of Measured Thresholds as two Distinct\n  Cessation Thresholds Abstract: Martin and Kok (2018a) measured two distinct aeolian saltation transport\nthresholds: a larger threshold below which continuous saltation transport\nbecomes intermittent and a smaller threshold below which intermittent saltation\ntransport ceases. In the spirit of Bagnold, they interpreted the former\nthreshold as the \\textit{fluid threshold}, associated with transport\ninitiation, and the latter threshold as the \\textit{impact threshold},\nassociated with transport cessation. Here I describe and support an alternative\ninterpretation of these two thresholds as two distinct cessation thresholds\nassociated with splash entrainment and, respectively, with compensating energy\nlosses of rebounding particles. This interpretation was recently proposed by\nP\\\"ahtz and Dur\\'an (2018a). To resolve this controversy, further field studies\nare needed. \n\n"}
{"id": "1812.06309", "contents": "Title: Extending classical surrogate modelling to high-dimensions through\n  supervised dimensionality reduction: a data-driven approach Abstract: Thanks to their versatility, ease of deployment and high-performance,\nsurrogate models have become staple tools in the arsenal of uncertainty\nquantification (UQ). From local interpolants to global spectral decompositions,\nsurrogates are characterised by their ability to efficiently emulate complex\ncomputational models based on a small set of model runs used for training. An\ninherent limitation of many surrogate models is their susceptibility to the\ncurse of dimensionality, which traditionally limits their applicability to a\nmaximum of $\\mathcal{O}(10^2)$ input dimensions. We present a novel approach at\nhigh-dimensional surrogate modelling that is model-, dimensionality reduction-\nand surrogate model- agnostic (black box), and can enable the solution of high\ndimensional (i.e. up to $\\mathcal{O}(10^4)$) problems. After introducing the\ngeneral algorithm, we demonstrate its performance by combining Kriging and\npolynomial chaos expansions surrogates and kernel principal component analysis.\nIn particular, we compare the generalisation performance that the resulting\nsurrogates achieve to the classical sequential application of dimensionality\nreduction followed by surrogate modelling on several benchmark applications,\ncomprising an analytical function and two engineering applications of\nincreasing dimensionality and complexity. \n\n"}
{"id": "1812.09063", "contents": "Title: Efficient Calculation of the Joint Distribution of Order Statistics Abstract: We consider the problem of computing the joint distribution of order\nstatistics of stochastically independent random variables in one- and two-group\nmodels. While recursive formulas for evaluating the joint cumulative\ndistribution function of such order statistics exist in the literature for a\nlonger time, their numerical implementation remains a challenging task. We\ntackle this task by presenting novel generalizations of known recursions which\nwe utilize to obtain exact results (calculated in rational arithmetic) as well\nas faithfully rounded results. Finally, some applications in stepwise multiple\nhypothesis testing are discussed. \n\n"}
{"id": "1812.09885", "contents": "Title: Model Selection for Mixture Models - Perspectives and Strategies Abstract: Determining the number G of components in a finite mixture distribution is an\nimportant and difficult inference issue. This is a most important question,\nbecause statistical inference about the resulting model is highly sensitive to\nthe value of G. Selecting an erroneous value of G may produce a poor density\nestimate. This is also a most difficult question from a theoretical perspective\nas it relates to unidentifiability issues of the mixture model. This is further\na most relevant question from a practical viewpoint since the meaning of the\nnumber of components G is strongly related to the modelling purpose of a\nmixture distribution. We distinguish in this chapter between selecting G as a\ndensity estimation problem in Section 2 and selecting G in a model-based\nclustering framework in Section 3. Both sections discuss frequentist as well as\nBayesian approaches. We present here some of the Bayesian solutions to the\ndifferent interpretations of picking the \"right\" number of components in a\nmixture, before concluding on the ill-posed nature of the question. \n\n"}
{"id": "1812.10805", "contents": "Title: Early Detection of the Draupner Wave Using Deep Learning Abstract: In this paper, we propose and apply a deep learning strategy for the early\ndetection of the Draupner rogue (freak) wave, which is also known as the New\nYear's wave. We use a long short term memory (LSTM) network and show that\nDraupner rogue wave could have been observed at least minutes before the\ncatastrophically dangerous peak has appeared in the chaotic wave field using\nthe available data. Compared to the existing early warning times scales on the\norder of seconds, this is a major step forward which will certainly enhance the\nsafety and understanding of the marine engineering. As the rogue wave data sets\nget improved in the future, our results may be enhanced to increase the early\nwarning time scales. Our results can be used to predict other rogue waves and\nextreme time-series phenomena in fields including but are not limited to\nhydrodynamics and marine engineering, optics, finance, and Bose-Einstein\ncondensation, just to name a few. \n\n"}
{"id": "1901.00533", "contents": "Title: A Simple Algorithm for Scalable Monte Carlo Inference Abstract: The methods of statistical physics are widely used for modelling complex\nnetworks. Building on the recently proposed Equilibrium Expectation approach,\nwe derive a simple and efficient algorithm for maximum likelihood estimation\n(MLE) of parameters of exponential family distributions - a family of\nstatistical models, that includes Ising model, Markov Random Field and\nExponential Random Graph models. Computational experiments and analysis of\nempirical data demonstrate that the algorithm increases by orders of magnitude\nthe size of network data amenable to Monte Carlo based inference. We report\nresults suggesting that the applicability of the algorithm may readily be\nextended to the analysis of large samples of dependent observations commonly\nfound in biology, sociology, astrophysics, and ecology. \n\n"}
{"id": "1901.02976", "contents": "Title: The square root rule for adaptive importance sampling Abstract: In adaptive importance sampling, and other contexts, we have $K>1$ unbiased\nand uncorrelated estimates $\\hat\\mu_k$ of a common quantity $\\mu$. The optimal\nunbiased linear combination weights them inversely to their variances but those\nweights are unknown and hard to estimate. A simple deterministic square root\nrule based on a working model that $\\mathrm{Var}(\\hat\\mu_k)\\propto k^{-1/2}$\ngives an unbisaed estimate of $\\mu$ that is nearly optimal under a wide range\nof alternative variance patterns. We show that if\n$\\mathrm{Var}(\\hat\\mu_k)\\propto k^{-y}$ for an unknown rate parameter $y\\in\n[0,1]$ then the square root rule yields the optimal variance rate with a\nconstant that is too large by at most $9/8$ for any $0\\le y\\le 1$ and any\nnumber $K$ of estimates. Numerical work shows that rule is similarly robust to\nsome other patterns with mildly decreasing variance as $k$ increases. \n\n"}
{"id": "1901.04816", "contents": "Title: Learning Direct and Inverse Transmission Matrices Abstract: Linear problems appear in a variety of disciplines and their application for\nthe transmission matrix recovery is one of the most stimulating challenges in\nbiomedical imaging. Its knowledge turns any random media into an optical tool\nthat can focus or transmit an image through disorder. Here, converting an\ninput-output problem into a statistical mechanical formulation, we investigate\nhow inference protocols can learn the transmission couplings by\npseudolikelihood maximization. Bridging linear regression and thermodynamics\nlet us propose an innovative framework to pursue the solution of the\nscattering-riddle. \n\n"}
{"id": "1901.04866", "contents": "Title: Practical Lossless Compression with Latent Variables using Bits Back\n  Coding Abstract: Deep latent variable models have seen recent success in many data domains.\nLossless compression is an application of these models which, despite having\nthe potential to be highly useful, has yet to be implemented in a practical\nmanner. We present `Bits Back with ANS' (BB-ANS), a scheme to perform lossless\ncompression with latent variable models at a near optimal rate. We demonstrate\nthis scheme by using it to compress the MNIST dataset with a variational\nauto-encoder model (VAE), achieving compression rates superior to standard\nmethods with only a simple VAE. Given that the scheme is highly amenable to\nparallelization, we conclude that with a sufficiently high quality generative\nmodel this scheme could be used to achieve substantial improvements in\ncompression rate with acceptable running time. We make our implementation\navailable open source at https://github.com/bits-back/bits-back . \n\n"}
{"id": "1901.06428", "contents": "Title: Unreasonable effectiveness of Monte Carlo Abstract: This is a comment on the article \"Probabilistic Integration: A Role in\nStatistical Computation?\" by F.-X. Briol, C. J. Oates, M. Girolami, M. A.\nOsborne and D. Sejdinovic to appear in Statistical Science.\n  There is a role for statistical computation in numerical integration.\nHowever, the competition from incumbent methods looks to be stiffer for this\nproblem than for some of the newer problems being handled by probabilistic\nnumerics. One of the challenges is the unreasonable effectiveness of the\ncentral limit theorem. Another is the unreasonable effectiveness of\npseudorandom number generators. A third is the common $O(n^3)$ cost of methods\nbased on Gaussian processes. Despite these advantages, the classical methods\nare weak in places where probabilistic methods could bring an improvement. \n\n"}
{"id": "1901.08115", "contents": "Title: A weighted Discrepancy Bound of quasi-Monte Carlo Importance Sampling Abstract: Importance sampling Monte-Carlo methods are widely used for the approximation\nof expectations with respect to partially known probability measures. In this\npaper we study a deterministic version of such an estimator based on\nquasi-Monte Carlo. We obtain an explicit error bound in terms of the\nstar-discrepancy for this method. \n\n"}
{"id": "1901.08606", "contents": "Title: Fast Markov Chain Monte Carlo Algorithms via Lie Groups Abstract: From basic considerations of the Lie group that preserves a target\nprobability measure, we derive the Barker, Metropolis, and ensemble Markov\nchain Monte Carlo (MCMC) algorithms, as well as variants of waste-recycling\nMetropolis-Hastings and an altogether new MCMC algorithm. We illustrate these\nconstructions with explicit numerical computations, and we empirically\ndemonstrate on a spin glass that the new algorithm converges more quickly than\nits siblings. \n\n"}
{"id": "1901.10543", "contents": "Title: A High-Dimensional Particle Filter Algorithm Abstract: Online data assimilation in time series models over a large spatial extent is\nan important problem in both geosciences and robotics. Such models are\nintrinsically high-dimensional, rendering traditional particle filter\nalgorithms ineffective. Though methods that begin to address this problem\nexist, they either rely on additional assumptions or lead to error that is\nspatially inhomogeneous. I present a novel particle-based algorithm for online\napproximation of the filtering problem on such models, using the fact that each\nlocus affects only nearby loci at the next time step. The algorithm is based on\na Metropolis-Hastings-like MCMC for creating hybrid particles at each step. I\nshow simulation results that suggest the error of this algorithm is uniform in\nboth space and time, with a lower bias, though higher variance, as compared to\na previously-proposed algorithm. \n\n"}
{"id": "1901.10568", "contents": "Title: Stochastic Gradient MCMC for Nonlinear State Space Models Abstract: State space models (SSMs) provide a flexible framework for modeling complex\ntime series via a latent stochastic process. Inference for nonlinear,\nnon-Gaussian SSMs is often tackled with particle methods that do not scale well\nto long time series. The challenge is two-fold: not only do computations scale\nlinearly with time, as in the linear case, but particle filters additionally\nsuffer from increasing particle degeneracy with longer series. Stochastic\ngradient MCMC methods have been developed to scale Bayesian inference for\nfinite-state hidden Markov models and linear SSMs using buffered stochastic\ngradient estimates to account for temporal dependencies. We extend these\nstochastic gradient estimators to nonlinear SSMs using particle methods. We\npresent error bounds that account for both buffering error and particle error\nin the case of nonlinear SSMs that are log-concave in the latent process. We\nevaluate our proposed particle buffered stochastic gradient using stochastic\ngradient MCMC for inference on both long sequential synthetic and\nminute-resolution financial returns data, demonstrating the importance of this\nclass of methods. \n\n"}
{"id": "1901.11269", "contents": "Title: Transport map accelerated adaptive importance sampling, and application\n  to inverse problems arising from multiscale stochastic reaction networks Abstract: In many applications, Bayesian inverse problems can give rise to probability\ndistributions which contain complexities due to the Hessian varying greatly\nacross parameter space. This complexity often manifests itself as lower\ndimensional manifolds on which the likelihood function is invariant, or varies\nvery little. This can be due to trying to infer unobservable parameters, or due\nto sloppiness in the model which is being used to describe the data. In such a\nsituation, standard sampling methods for characterising the posterior\ndistribution, which do not incorporate information about this structure, will\nbe highly inefficient.\n  In this paper, we seek to develop an approach to tackle this problem when\nusing adaptive importance sampling methods, by using optimal transport maps to\nsimplify posterior distributions which are concentrated on lower dimensional\nmanifolds. This approach is applicable to a whole range of problems for which\nMonte Carlo Markov chain (MCMC) methods mix slowly.\n  We demonstrate the approach by considering inverse problems arising from\npartially observed stochastic reaction networks. In particular, we consider\nsystems which exhibit multiscale behaviour, but for which only the slow\nvariables in the system are observable. We demonstrate that certain multiscale\napproximations lead to more consistent approximations of the posterior than\nothers. The use of optimal transport maps stabilises the ensemble transform\nadaptive importance sampling (ETAIS) method, and allows for efficient sampling\nwith smaller ensemble sizes. This approach allows us to take advantage of the\nlarge increases of efficiency when using adaptive importance sampling methods\nfor previously intractable Bayesian inverse problems with complex posterior\nstructure. \n\n"}
{"id": "1901.11426", "contents": "Title: The Effects of Gravity on the Climate and Circulation of a Terrestrial\n  Planet Abstract: The climate and circulation of a terrestrial planet are governed by, among\nother things, the distance to its host star, its size, rotation rate,\nobliquity, atmospheric composition and gravity. Here we explore the effects of\nthe last of these, the Newtonian gravitational acceleration, on its atmosphere\nand climate. We first demonstrate that if the atmosphere obeys the hydrostatic\nprimitive equations, which are a very good approximation for most terrestrial\natmospheres, and if the radiative forcing is unaltered, changes in gravity have\nno effect at all on the circulation except for a vertical rescaling. That is to\nsay, the effects of gravity may be completely scaled away and the circulation\nis unaltered. However, if the atmosphere contains a dilute condensible that is\nradiatively active, such as water or methane, then an increase in gravity will\ngenerally lead to a cooling of the planet because the total path length of the\ncondensible will be reduced as gravity increases, leading to a reduction in the\ngreenhouse effect. Furthermore, the specific humidity will decrease, leading to\nchanges in the moist adiabatic lapse rate, in the equator-to-pole heat\ntransport, and in the surface energy balance because of changes in the sensible\nand latent fluxes. These effects are all demonstrated both by theoretical\narguments and by numerical simulations with moist and dry general circulation\nmodels. \n\n"}
{"id": "astro-ph/0201018", "contents": "Title: Evidence for Nearby Supernova Explosions Abstract: Supernova explosions are one of the most energetic--and potentially\nlethal--phenomena in the Universe. Scientists have speculated for decades about\nthe possible consequences for life on Earth of a nearby supernova, but\nplausible candidates for such an event were lacking. Here we show that the\nScorpius-Centaurus OB association, a group of young stars currently located\nat~130 parsecs from the Sun, has generated 20 SN explosions during the last 11\nMyr, some of them probably as close as 40 pc to our planet. We find that the\ndeposition on Earth of 60Fe atoms produced by these explosions can explain the\nrecent measurements of an excess of this isotope in deep ocean crust samples.\nWe propose that ~2 Myr ago, one of the SNe exploded close enough to Earth to\nseriously damage the ozone layer, provoking or contributing to the\nPliocene-Pleistocene boundary marine extinction. \n\n"}
{"id": "astro-ph/0205286", "contents": "Title: MHD Turbulence: Scaling Laws and Astrophysical Implications Abstract: Turbulence is the most common state of astrophysical flows. In typical\nastrophysical fluids, turbulence is accompanied by strong magnetic fields,\nwhich has a large impact on the dynamics of the turbulent cascade. Recently,\nthere has been a significant breakthrough on the theory of magnetohydrodynamic\n(MHD) turbulence. For the first time we have a scaling model that is supported\nby both observations and numerical simulations. We review recent progress in\nstudies of both incompressible and compressible turbulence. We compare\nIroshnikov-Kraichnan and Goldreich-Sridhar models, and discuss scalings of\nAlfv\\'en, slow, and fast waves. We also discuss the completely new regime of\nMHD turbulence that happens below the scale at which hydrodynamic turbulent\nmotions are damped by viscosity. In the case of the partially ionized diffuse\ninterstellar gas the viscosity is due to neutrals and truncates the turbulent\ncascade at $\\sim$parsec scales. We show that below this scale magnetic\nfluctuations with a shallow spectrum persist and discuss the possibility of a\nresumption of the MHD cascade after ions and neutrals decouple. We discuss the\nimplications of this new insight into MHD turbulence for cosmic ray transport,\ngrain dynamics, etc., and how to test theoretical predictions against\nobservations. \n\n"}
{"id": "astro-ph/0505472", "contents": "Title: Gamma-Ray Bursts and the Earth: Exploration of Atmospheric, Biological,\n  Climatic and Biogeochemical Effects Abstract: Gamma-Ray Bursts (GRBs) are likely to have made a number of significant\nimpacts on the Earth during the last billion years. We have used a\ntwo-dimensional atmospheric model to investigate the effects on the Earth's\natmosphere of GRBs delivering a range of fluences, at various latitudes, at the\nequinoxes and solstices, and at different times of day. We have estimated DNA\ndamage levels caused by increased solar UVB radiation, reduction in solar\nvisible light due to $\\mathrm{NO_2}$ opacity, and deposition of nitrates\nthrough rainout of $\\mathrm{HNO_3}$. For the ``typical'' nearest burst in the\nlast billion years, we find globally averaged ozone depletion up to 38%.\nLocalized depletion reaches as much as 74%. Significant global depletion (at\nleast 10%) persists up to about 7 years after the burst. Our results depend\nstrongly on time of year and latitude over which the burst occurs. We find DNA\ndamage of up to 16 times the normal annual global average, well above lethal\nlevels for simple life forms such as phytoplankton. The greatest damage occurs\nat low to mid latitudes. We find reductions in visible sunlight of a few\npercent, primarily in the polar regions. Nitrate deposition similar to or\nslightly greater than that currently caused by lightning is also observed,\nlasting several years. We discuss how these results support the hypothesis that\nthe Late Ordovician mass extinction may have been initiated by a GRB. \n\n"}
{"id": "astro-ph/0601711", "contents": "Title: Gamma-ray bursts and terrestrial planetary atmospheres Abstract: We describe results of modeling the effects on Earth-like planets of\nlong-duration gamma-ray bursts (GRBs) within a few kiloparsecs. A primary\neffect is generation of nitrogen oxide compounds which deplete ozone. Ozone\ndepletion leads to an increase in solar UVB radiation at the surface, enhancing\nDNA damage, particularly in marine microorganisms such as phytoplankton. In\naddition, we expect increased atmospheric opacity due to buildup of nitrogen\ndioxide produced by the burst and enhanced precipitation of nitric acid. We\nreview here previous work on this subject and discuss recent developments,\nincluding further discussion of our estimates of the rates of impacting GRBs\nand the possible role of short-duration bursts. \n\n"}
{"id": "astro-ph/0602092", "contents": "Title: Do extragalactic cosmic rays induce cycles in fossil diversity? Abstract: Recent work has revealed a 62 (+/-) 3-million-year cycle in the fossil\ndiversity in the past 542 My, however no plausible mechanism has been found. We\npropose that the cycle may be caused by modulation of cosmic ray (CR) flux by\nthe Solar system vertical oscillation (64 My period) in the galaxy, the\ngalactic north-south anisotropy of CR production in the galactic\nhalo/wind/termination shock (due to the galactic motion toward the Virgo\ncluster), and the shielding by galactic magnetic fields. We revisit the\nmechanism of CR propagation and show that CR flux can vary by a factor of about\n4.6 and reach a maximum at north-most displacement of the Sun. The very high\nstatistical significance of (i) the phase agreement between Solar north-ward\nexcursions and the diversity minima and (ii) the correlation of the magnitude\nof diversity drops with CR amplitudes through all cycles provide solid support\nfor our model. Various observational predictions which can be used to confirm\nor falsify our hypothesis are presented. \n\n"}
{"id": "astro-ph/0606226", "contents": "Title: Anti-Neutrino Imprint in Solar Neutrino Flare Abstract: Future neutrino detector at Megaton mass might enlarge the neutrino telescope\nthresholds revealing cosmic supernova background and largest solar flares\nneutrino. Indeed the solar energetic flare particles while scattering among\nthemselves on Solar corona atmosphere must produce prompt charged pions, whose\nchain decays are source of solar (electron-muon) neutrino \"flare\" (at tens or\nhundreds MeV energy). These brief (minutes) neutrino \"burst\" at largest flare\npeak may overcome by three to five order of magnitude the steady atmospheric\nneutrino noise on the Earth, possibly leading to their detection above\ndetection. Moreover the birth of anti-neutrinos at a few tens MeVs is well\nloudly flaring above a null thermal \"hep\" anti-neutrino solar background and\nalso above a tiny supernova relic and atmospheric noise. The largest prompt\nsolar anti-neutrino \"burst\" may be well detected in future SuperKamikande\n(Gadolinium implemented) by anti-neutrino signatures mostly in inverse Beta\ndecay. Our estimate for the recent and exceptional October - November 2003\nsolar flares and January 20th 2005 exceptional eruption might lead to a few\nevents above or near unity for existing Super-Kamiokande and above unity for\nMegaton detectors. The neutrino spectra may reflect in a subtle way the\nneutrino flavor oscillations and mixing in flight. A comparison of the solar\nneutrino flare (at their birth place on Sun and after oscillation on the\narrival on the Earth) with other neutrino foreground is estimated: it offers an\nindependent track to disentangle the neutrino flavor puzzles and its most\nsecret mixing angles. The sharpest noise-free anti-neutrino imprint maybe its\nfirst clean voice. \n\n"}
{"id": "astro-ph/0610725", "contents": "Title: UHE Cosmic Rays and Neutrinos Showering on Planet Edges Abstract: Ultra High Energy (UHE) Cosmic Rays, UHECR, may graze high altitude\natmosphere leading to horizontal upward air-showers. Also PeVs electron\nantineutrino hitting electron in atmosphere may air-shower at W boson resonant\nmass. On the other side ultra high energy muon and electron neutrinos may also\nlead, by UHE neutrinos mass state mixing, to the rise of a corresponding UHE\nTau neutrino flavor; the consequent UHE tau neutrinos, via charge current\ninteractions in matter, may create UHE taus at horizons (Earth skimming\nneutrinos or Hor-taus) whose escape in atmosphere and whose consequent decay in\nflight, may be later amplified by upward showering on terrestrial, planetary\natmospheres. Indeed because of the finite terrestrial radius, its thin\natmosphere size its dense crust, the UHE tau cannot extend much more than 360\nkilometers in air, corresponding to an energy of about 7.2 EeV, near but below\nGZK cut-off ones; on the contrary Jupiter (or even Saturn) may offer a wider,\nless dense and thicker gaseous layer at the horizons where Tau may loose little\nenergy, travel longer before decay and rise and shower at 4-6 10^{19} eV or ZeV\nextreme energy. Titan atmosphere may open a rare window of opportunity for\nUp-ward Taus at PeVs. Also solar atmosphere may play a role, but unfortunately\ntau-showers secondaries maybe are too noisy to be disentangled, while Jupiter\natmosphere, or better, Saturn one, may offer a clearer imprint for GZK (and\nhigher Z-Burst) Tau showering, well below the horizons edges. \n\n"}
{"id": "astro-ph/0610954", "contents": "Title: Updated Z-Burst Neutrinos at Horizons Abstract: Recent homogeneous and isotropic maps of UHECR, suggest an isotropic cosmic\norigin almost uncorrelated to nearby Local Universe prescribed by GZK (tens\nMpc) cut-off. Z-Burst model based on UHE neutrino resonant scattering on light\nrelic ones in nearby Hot neutrino Dark Halo, may overcome the absence of such a\nlocal imprint and explain the recent correlation with BL Lac at distances of a\nfew hundred Mpc. Z-Burst multiple imprint, due to very possible lightest\nnon-degenerated neutrino masses, may inject energy and modulate UHECR ZeV edge\nspectra. The Z-burst (and GZK) ultra high energy neutrinos (ZeV and EeV band)\nmay also shine, by UHE neutrinos mass state mixing, and rise in corresponding\nUHE Tau neutrino flavor, whose charged current tau production and its decay in\nflight, maybe the source of UHE showering on Earth. The Radius and the\natmosphere size of our planet constrains the tau maximal distance and energy to\nmake a shower. These terrestrial tau energies are near GZK energy limit. Higher\ndistances and energies are available in bigger planets; eventual solar\natmosphere horizons may amplify the UHE tau flight allowing tau showering at\nZeV energies offering a novel way to reveal the expected Z-Burst extreme\nneutrino fluxes. \n\n"}
{"id": "astro-ph/0612745", "contents": "Title: Splitting neutrino masses and showering into Sky Abstract: Neutrino masses might be as light as a few time the atmospheric neutrino mass\nsplitting. High Energy ZeV cosmic neutrinos (in Z-Showering model) might hit\nrelic ones at each mass in different resonance energies in our nearby Universe.\nThis non-degenerated density and energy must split UHE Z-boson secondaries (in\nZ-Burst model) leading to multi injection of UHECR nucleons within future\nextreme AUGER energy. Secondaries of Z-Burst as neutral gamma, below a few tens\nEeV are better surviving local GZK cut-off and they might explain recent Hires\nBL-Lac UHECR correlations at small angles. A different high energy resonance\nmust lead to Glashow's anti-neutrino showers while hitting electrons in matter.\nIn air, Glashow's anti-neutrino showers lead to collimated and directional\nair-showers offering a new Neutrino Astronomy. At greater energy around PeV,\nTau escaping mountains and Earth and decaying in flight are effectively\nshowering in air sky. These Horizontal showering is splitting by geomagnetic\nfield in forked shapes. Such air-showers secondaries release amplified and\nbeamed gamma bursts (like observed TGF), made also by muon and electron pair\nbundles, with their accompanying rich Cherenkov flashes. Also planet' s largest\n(Saturn, Jupiter) atmosphere limbs offer an ideal screen for UHE GZK and\nZ-burst tau neutrino, because their largest sizes. Titan thick atmosphere and\nsmall radius are optimal for discovering up-going resonant Glashow resonant\nshowers. Earth detection of Neutrino showering by twin Magic Telescopes on top\nmountains, or by balloons and satellites arrays facing the limbs are the\nsimplest and cheapest way toward UHE Neutrino Astronomy . \n\n"}
{"id": "astro-ph/9805317", "contents": "Title: Anoxia duirng the Late Permian Binary Mass Extinction and Dark Matter Abstract: Recent evidence quite convincingly indicates that the Late Permian biotic\ncrisis was in fact a binary extinction with a distinct end-Guadalupian\nextinction pulse preceding the major terminal end-Permian Tartarian event by 5\nmillion years. In addition anoxia appears to be closely associated with each of\nthese end-Paleozoic binary extinctions. Most leading models cannot explain both\nanoxia and the binary characteristic of this crisis. In this paper we show that\nthe recently proposed volcanogenic dark matter scenario succeeds in doing this. \n\n"}
{"id": "chao-dyn/9802003", "contents": "Title: Noise rectification in quasigeostrophic forced turbulence Abstract: We study the appearance of large scale mean motion sustained by stochastic\nforcing on a rotating fluid (in the quasigeostrophic approximation) flowing\nover topography. As in other noise rectification phenomena, the effect requires\nnonlinearity and absence of detailed balance to occur. By application of an\nanalytical coarse graining procedure we identify the physical mechanism\nproducing such effect: It is a forcing coming from the small scales that\nmanifests in a change in the effective viscosity operator and in the effective\nnoise statistical properties. \n\n"}
{"id": "chao-dyn/9905022", "contents": "Title: Noise-induced flow in quasigeostrophic turbulence with bottom friction Abstract: Randomly-forced fluid flow in the presence of scale-unselective dissipation\ndevelops mean currents following topographic contours. Known mechanisms based\non the scale-selective action of damping processes are not at work in this\nsituation. Coarse-graining reveals that the phenomenon is a kind of\nnoise-rectification mechanism, in which lack of detailed balance and the\nsymmetry-breaking provided by topography play an important role. \n\n"}
{"id": "cond-mat/0208117", "contents": "Title: Statistical analysis of air and sea temperature anomalies Abstract: This paper presents a global air and sea temperature anomalies analysis based\nupon a combination of the wavelet multiresolution analysis and the scaling\nanalysis methods of a time series. The wavelet multiresolution analysis\ndecomposes the two temperature signals on a scale-by-scale basis. The\nscale-by-scale smooth and detail curves are compared and the correlation\ncoefficients between each couple of correspondent sets of data evaluated. The\nscaling analysis is based upon the study of the spreading and the entropy of\nthe diffusion generated by the temperature signals. Therefore, we jointly adopt\ntwo distinct methods: the Diffusion Entropy Analysis (DEA) and the Standard\nDeviation Analysis (SDA). The joint use of these two methods allows us to\nestablish with more confidence the nature of the signals, as well as their\nscaling, and it yields the discovery of a slight Levy component in the two\ntemperature data sets. Finally, the DEA and SDA are used to study the wavelet\nresiduals of the two temperature anomalies. The temporal regions of persistence\nand antipersistence of the signals are determined and the non-stationary effect\nof the 10-11 year solar cycle upon the temperature is studied. The temperature\nmonthly data cover the period from 1860 to 2000 A.D.E. \n\n"}
{"id": "nlin/0109027", "contents": "Title: Ray dynamics in ocean acoustics Abstract: Recent results relating to ray dynamics in ocean acoustics are reviewed.\nAttention is focussed on long-range propagation in deep ocean environments. For\nthis class of problems, the ray equations may be simplified by making use of a\none-way formulation in which the range variable appears as the independent\n(time-like) variable. Topics discussed include integrable and non-integrable\nray systems, action-angle variables, nonlinear resonances and the KAM theorem,\nray chaos, Lyapunov exponents, predictability, nondegeneracy violation, ray\nintensity statistics, semiclassical breakdown, wave chaos, and the connection\nbetween ray chaos and mode coupling. The Hamiltonian structure of the ray\nequations plays an important role in all of these topics. \n\n"}
{"id": "nlin/0306056", "contents": "Title: Ray chaos and ray clustering in an ocean waveguide Abstract: We consider ray propagation in a waveguide with a designed sound-speed\nprofile perturbed by a range-dependent perturbation caused by internal waves in\ndeep ocean environments. The Hamiltonian formalism in terms of the action and\nangle variables is applied to study nonlinear ray dynamics with two\nsound-channel models and three perturbation models: a single-mode perturbation,\na random-like sound-speed fluctuations, and a mixed perturbation. In the\nintegrable limit without any perturbation, we derive analytical expressions for\nray arrival times and timefronts at a given range, the main measurable\ncharacteristics in field experiments in the ocean. In the presence of a\nsingle-mode perturbation, ray chaos is shown to arise as a result of\noverlapping nonlinear ray-medium resonances. Poincar\\'e maps, plots of\nvariations of the action per a ray cycle length, and plots with rays escaping\nthe channel reveal inhomogeneous structure of the underlying phase space with\nremarkable zones of stability where stable coherent ray clusters may be formed.\nWe demonstrate the possibility of determining the wavelength of the\nperturbation mode from the arrival time distribution under conditions of ray\nchaos. It is surprising that coherent ray clusters, consisting of fans of rays\nwhich propagate over long ranges with close dynamical characteristics, can\nsurvive under a random-like multiplicative perturbation modelling sound-speed\nfluctuations caused by a wide spectrum of internal waves. \n\n"}
{"id": "nlin/0404016", "contents": "Title: Diffusion of passive scalar in a finite-scale random flow Abstract: We consider a solvable model of the decay of scalar variance in a\nsingle-scale random velocity field. We show that if there is a separation\nbetween the flow scale k_flow^{-1} and the box size k_box^{-1}, the decay rate\nlambda ~ (k_box/k_flow)^2 is determined by the turbulent diffusion of the\nbox-scale mode. Exponential decay at the rate lambda is preceded by a transient\npowerlike decay (the total scalar variance ~ t^{-5/2} if the Corrsin invariant\nis zero, t^{-3/2} otherwise) that lasts a time t~1/\\lambda. Spectra are sharply\npeaked at k=k_box. The box-scale peak acts as a slowly decaying source to a\nsecondary peak at the flow scale. The variance spectrum at scales intermediate\nbetween the two peaks (k_box<<k<<k_flow) is ~ k + a k^2 + ... (a>0). The mixing\nof the flow-scale modes by the random flow produces, for the case of large\nPeclet number, a k^{-1+delta} spectrum at k>>k_flow, where delta ~ lambda is a\nsmall correction. Our solution thus elucidates the spectral make up of the\n``strange mode,'' combining small-scale structure and a decay law set by the\nlargest scales. \n\n"}
{"id": "nlin/0408005", "contents": "Title: Small and Large Scale Fluctuations in Atmospheric Wind Speeds Abstract: Atmospheric wind speeds and their fluctuations at different locations\n(onshore and offshore) are examined. One of the most striking features is the\nmarked intermittency of probability density functions (PDF) of velocity\ndifferences -- no matter what location is considered. The shape of these PDFs\nis found to be robust over a wide range of scales which seems to contradict the\nmathematical concept of stability where a Gaussian distribution should be the\nlimiting one. Motivated by the instationarity of atmospheric winds it is shown\nthat the intermittent distributions can be understood as a superposition of\ndifferent subsets of isotropic turbulence. Thus we suggest a simple stochastic\nmodel to reproduce the measured statistics of wind speed fluctuations. \n\n"}
{"id": "nlin/0605003", "contents": "Title: Differential models for 2D turbulence Abstract: We present two phenomenological models for 2D turbulence in which the energy\nspectrum obeys a nonlinear fourth-order and a second-order differential\nequations respectively. Both equations respect the scaling properties of the\noriginal Navier-Stokes equations and it has both the -5/3 inverse-cascade and t\n-3 direct-cascade spectra. In addition, the fourth order equation has\nRaleigh-Jeans thermodynamic distributions, as exact steady state solutions. We\nuse the fourth-order model to derive a relation between the direct-cascade and\nthe inverse-cascade Kolmogorov constants which is in a good qualitative\nagreement with the laboratory and numerical experiments. We obtain a steady\nstate solution where both the enstrophy and the energy cascades are present\nsimultaneously and we discuss it in context of the Nastrom-Gage spectrum\nobserved in atmospheric turbulence. We also consider the effect of the bottom\nfriction onto the cascade solutions, and show that it leads to an additional\ndecrease and finite-wavenumber cutoffs of the respective cascade spectra. \n\n"}
{"id": "nlin/0605040", "contents": "Title: Sling effect in collisions of water droplets in turbulent clouds Abstract: We describe and evaluate the contribution of sling effect into the collision\nrate of the same-size water droplets in turbulent clouds. We show that already\nfor Stokes numbers exceeding 0.2 the sling effect gives a contribution\ncomparable to Saffman-Turner contribution, which may explain why the latter\nconsistently underestimates collision rate (even with the account of\npreferential concentration). \n\n"}
{"id": "physics/0105023", "contents": "Title: Forces at the Sea Bed using a Finite Element Solution of the Mild Slope\n  Wave Equation Abstract: An algorithm to compute forces at the sea bed from a finite element solution\nto the mild slope wave equation is devised in this work. The algorithm is best\nconsidered as consisting of two logical parts: The first is concerned with the\ncomputation of the derivatives to a finite element solution, given the\nassociated mesh; the second is a bi-quadratic least squares fit which serves to\nmodel the sea bed locally in the vicinity of a node. The force at the sea bed\ncan be quantified in terms of either lift and drag, the likes of Stokes'\nformula or traction. While the latter quantity is the most desireable, the\ndirect computation of tractions at the sea bed is controversial in the context\nof the mild slope wave equation as a result of the irrotationality implied by\nthe use of potentials. This work ultimately envisages a ``Monte Carlo''\napproach using wave induced forces to elucidate presently known heavy mineral\nplacer deposits and, consequently, to predict the existance of other deposits\nwhich remain as yet undiscovered. \n\n"}
{"id": "physics/0109005", "contents": "Title: Alternative interpretation of the sign reversal of secondary Bjerknes\n  force acting between two pulsating gas bubbles Abstract: It is known that in a certain case, the secondary Bjerknes force, which is a\nradiation force acting between pulsating bubbles, changes, e.g., from\nattraction to repulsion as the bubbles approach each other. In this paper, a\ntheoretical discussion of this phenomenon for two spherical bubbles is\ndescribed. The present theory based on analysis of the transition frequencies\nof interacting bubbles [M. Ida, Phys. Lett. A 297, 210 (2002)] provides an\ninterpretation, different from previous ones (e.g., by Doinikov and Zavtrak\n[Phys. Fluids 7, 1923 (1995)]), of the phenomenon. It is shown, for example,\nthat the reversal that occurs when one bubble is smaller and another is larger\nthan a resonance size is due to the second-highest transition frequency of the\nsmaller bubble, which cannot be obtained using traditional natural-frequency\nanalysis. \n\n"}
{"id": "physics/0205071", "contents": "Title: Problem of water vapor absorption continuum in atmospheric windows.\n  Return of dimer hypothesis Abstract: Two alternative hypotheses try to explain the origin of the continuum. The\npresent communication gives new argument in favor of the dimer hypothesis. This\nargument is based on the existence of the wide component of line in absorption\nspectrum of polyatomic molecules. \n\n"}
{"id": "physics/0207108", "contents": "Title: Richardson's pair diffusion and the stagnation point structure of\n  turbulence Abstract: DNS and laboratory experiments show that the spatial distribution of\nstraining stagnation points in homogeneous isotropic 3D turbulence has a\nfractal structure with dimension D_s = 2. In Kinematic Simulations the time\nexponent gamma in Richardson's law and the fractal dimension D_s are related by\ngamma = 6/D_s. The Richardson constant is found to be an increasing function of\nthe number of straining stagnation points in agreement with pair duffusion\noccuring in bursts when pairs meet such points in the flow. \n\n"}
{"id": "physics/0209047", "contents": "Title: Vortex line representation for flows of ideal and viscous fluids Abstract: It is shown that the Euler hydrodynamics for vortical flows of an ideal fluid\ncoincides with the equations of motion of a charged {\\it compressible} fluid\nmoving due to a self-consistent electromagnetic field. Transition to the\nLagrangian description in a new hydrodynamics is equivalent for the original\nEuler equations to the mixed Lagrangian-Eulerian description - the vortex line\nrepresentation (VLR). Due to compressibility of a \"new\" fluid the collapse of\nvortex lines can happen as the result of breaking (or overturning) of vortex\nlines. It is found that the Navier-Stokes equation in the vortex line\nrepresentation can be reduced to the equation of the diffusive type for the\nCauchy invariant with the diffusion tensor given by the metric of the VLR. \n\n"}
{"id": "physics/0212055", "contents": "Title: PARIS Altimetry with L1 Frequency Data from the Bridge 2 Experiment Abstract: A portion of 20 minutes of the GPS signals collected during the Bridge 2\nexperimental campaign, performed by ESA, have been processed. An innovative\nalgorithm called Parfait, developed by Starlab and implemented within Starlab's\nGNSS-R Software package STARLIGHT (STARLab Interferometric Gnss Toolkit), has\nbeen successfully used with this set of data. A comparison with tide values\nindependently collected and with differential GPS processed data has been\nperformed. We report a successful PARIS phase altimetric measure of the Zeeland\nBrug over the sea surface with a rapidly changing tide, with a precision better\nthan 2 cm. \n\n"}
{"id": "physics/0306043", "contents": "Title: Characteristics of Aerosol Spectral Optical Depths over Manora Peak,\n  Nainital $-$ A High Altitude Station in the Central Himalayas Abstract: We present, for the first time, spectral behaviour of aerosol optical depths\n(AODs) over Manora Peak, Nainital located at an altitude of $\\sim$ 2 km in the\nShivalik ranges of central Himalayas. The observations were carried out using a\nMulti-Wavelength solar Radiometer during January to December 2002. The main\nresults of the study are extremely low AODs during winter, a remarkable\nincrease to high values in summer and a distinct change in the spectral\ndependencies of AODs from a relatively steeper spectra during winter to a\nshallower one in summer. During transparent days, the AOD values lie usually\nbelow 0.08 while during dusty (turbid) days, it lies between 0.08 to 0.69 at\n0.5 $\\mu$m. The average AOD value at 0.5 $\\mu$m during winters, particularly in\nJanuary and February, is $\\sim 0.03\\pm0.01$. The mean aerosol extinction law at\nManora Peak during 2002 is best represented by $0.10 \\lambda^{-0.61}$. However\nduring transparent days, which almost covers 40% of the time, it is represented\nby $0.02 \\lambda^{-0.97}$. This value of wavelength exponent, representing\nreduced coarse concentration and presence of fine aerosols, indicates that the\nstation measures aerosol in the free troposphere at least during part of the\nyear. \n\n"}
{"id": "physics/0307039", "contents": "Title: Passive tracer patchiness and particle trajectory stability in\n  incompressible two-dimensional flows Abstract: Particle motion is considered in incompressible two-dimensional flows\nconsisting of a steady background gyre on which an unsteady wave-like\nperturbation is superimposed. A dynamical systems point of view that exploits\nthe action--angle formalism is adopted. It is argued and demonstrated\nnumerically that for a large class of problems one expects to observe a mixed\nphase space, i.e., the occurrence of ``regular islands'' in an otherwise\n``chaotic sea.'' This leads to patchiness in the evolution of passive tracer\ndistributions. Also, it is argued and demonstrated numerically that particle\ntrajectory stability is largely controlled by the background flow: trajectory\ninstability, quantified by various measures of the ``degree of chaos,''\nincreases on average with increasing $|\\mathrm{d}\\omega/\\mathrm{d}I|$, where\n$\\omega (I)$ is the angular frequency of the trajectory in the background flow\nand $I$ is the action. \n\n"}
{"id": "physics/0307144", "contents": "Title: Light Propagation in Turbulent Media Abstract: First, we make a revision of the up-to-date Passive Scalar Fields properties:\nalso, the refractive index is among them. Afterwards, we formulated the\nproperties that make the family of `isotropic' fractional Brownian motion (with\nparameter H) a good candidate to simulate the turbulent refractive index.\nMoreover, we obtained its fractal dimension which matches the estimated by\nConstantin for passive scalar, and thus the parameter H determines the state of\nthe turbulence.\n  Next, using a path integral velocity representation, with the Markovian\nmodel, to calculate the effects of the turbulence over a system of grids.\n  Finally, with the tools of Stochastic Calculus for fractional Brownian\nmotions we studied the ray-equation coming from the Geometric Optics in the\nturbulent case. Our analysis covers those cases where average temperature\ngradients are relevant. \n\n"}
{"id": "physics/0310079", "contents": "Title: Do medium range ensemble forecasts give useful predictions of temporal\n  correlations? Abstract: Medium range ensemble forecasts are typically used to derive predictions of\nthe conditional marginal distributions of future events on individual days. We\nassess whether they can also be used to predict the conditional correlations\nbetween different days. \n\n"}
{"id": "physics/0310112", "contents": "Title: Note on the typhoon eye trajectory Abstract: We consider a model of typhoon based on the three-dimensional baroclinic\ncompressible equations of atmosphere dynamics averaged over hight and describe\na qualitative behavior of the vortex and possible trajectories of the typhoon\neye. \n\n"}
{"id": "physics/0311138", "contents": "Title: Notes on a possible phenomenology of internal transport barriers in\n  tokamak Abstract: We propose a new phenomenology of the generation of internal transport\nbarriers, based on the exact periodic solution of the Flierl-Petvishvili\nequation. We examine the stability of this solution and compare the late stages\nof the flow with the ensemble of vortices. \n\n"}
{"id": "physics/0312133", "contents": "Title: Electromagnetic wave scattering from a random layer with rough\n  interfaces I: Coherent field Abstract: The problem of an electromagnetic wave scattered from a random medium layer\nwith rough boundaries is formulated using integral equations which involve two\nkinds of Green functions. The first one describes the wave scattered by the\nrandom medium and the rough boundaries, and the second one which corresponds to\nthe unperturbed Green functions describes the scattering by an homogeneous\nlayer with the rough boundaries. As these equations are formally similar to\nclassical equations used in scattering theory by an infinite random medium, we\nwill be able to apply standard procedures to calculate the coherent field. We\nwill use the coherent potential approximation where the correlations between\nthe particles will be taken into account under the quasi-crystalline\napproximation. \n\n"}
{"id": "physics/0401046", "contents": "Title: The problem with the Brier score Abstract: The Brier score is frequently used by meteorologists to measure the skill of\nbinary probabilistic forecasts. We show, however, that in simple idealised\ncases it gives counterintuitive results. We advocate the use of an alternative\nmeasure that has a more compelling intuitive justification. \n\n"}
{"id": "physics/0401143", "contents": "Title: Volcanic forcing improves Atmosphere-Ocean Coupled General Circulation\n  Model scaling performance Abstract: Recent Atmosphere-Ocean Coupled General Circulation Model (AOGCM) simulations\nof the twentieth century climate, which account for anthropogenic and natural\nforcings, make it possible to study the origin of long-term temperature\ncorrelations found in the observed records. We study ensemble experiments\nperformed with the NCAR PCM for 10 different historical scenarios, including no\nforcings, greenhouse gas, sulfate aerosol, ozone, solar, volcanic forcing and\nvarious combinations, such as it natural, anthropogenic and all forcings. We\ncompare the scaling exponents characterizing the long-term correlations of the\nobserved and simulated model data for 16 representative land stations and 16\nsites in the Atlantic Ocean for these scenarios. We find that inclusion of\nvolcanic forcing in the AOGCM considerably improves the PCM scaling behavior.\nThe scenarios containing volcanic forcing are able to reproduce quite well the\nobserved scaling exponents for the land with exponents around 0.65 independent\nof the station distance from the ocean. For the Atlantic Ocean, scenarios with\nthe volcanic forcing slightly underestimate the observed persistence exhibiting\nan average exponent 0.74 instead of 0.85 for reconstructed data. \n\n"}
{"id": "physics/0402027", "contents": "Title: Singular vector ensemble forecasting systems and the prediction of flow\n  dependent uncertainty Abstract: The ECMWF ensemble weather forecasts are generated by perturbing the initial\nconditions of the forecast using a subset of the singular vectors of the\nlinearised propagator. Previous results show that when creating probabilistic\nforecasts from this ensemble better forecasts are obtained if the mean of the\nspread and the variability of the spread are calibrated separately. We show\nresults from a simple linear model that suggest that this may be a generic\nproperty for all singular vector based ensemble forecasting systems based on\nonly a subset of the full set of singular vectors. \n\n"}
{"id": "physics/0402028", "contents": "Title: Atmospheric turbulence within and above an Amazon forest Abstract: In this paper, we discuss the impact of a rain forest canopy on the\nstatistical characteristics of atmospheric turbulence. This issue is of\nparticular interest for understanding on how the Amazon terrestrial biosphere\ninteract with the atmosphere. For this, we used a probability density function\nmodel of velocity and temperature differences based on Tsallis' non-extensive\nthermostatistics. We compared theoretical results with experimental data\nmeasured in a 66 m micrometeorological tower, during the wet-season campaign of\nthe Large Scale Biosphere-Atmosphere Experiment in Amazonia (LBA).\nParticularly, we investigated how the value of the entropic parameter is\naffected when one moves into the canopy, or when one passes from day/unstable\nto night/stable conditions. We show that this new approach provides interesting\ninsights on turbulence in a complex environment such as the Amazon forest. \n\n"}
{"id": "physics/0403005", "contents": "Title: Modeling turbulent wave-front phase as a fractional Brownian motion: a\n  new approach Abstract: This paper introduces a general and new formalism to model the turbulent\nwave-front phase using fractional Brownian motion processes. Moreover, it\nextends results to non-Kolmogorov turbulence. In particular, generalized\nexpressions for the Strehl ratio and the angle-of-arrival variance are\nobtained. These are dependent on the dynamic state of the turbulence. \n\n"}
{"id": "physics/0406029", "contents": "Title: Sea state monitoring using coastal GNSS-R Abstract: We report on a coastal experiment to study GPS L1 reflections. The campaign\nwas carried out at the Barcelona Port breaker and dedicated to the development\nof sea-state retrieval algorithms. An experimental system built for this\npurpose collected and processed GPS data to automatically generate a times\nseries of the interferometric complex field (ICF). The ICF was analyzed off\nline and compared to a simple developed model that relates ICF coherence time\nto the ratio of significant wave height (SWH) and mean wave period (MWP). The\nanalysis using this model showed good consistency between the ICF coherence\ntime and nearby oceanographic buoy data. Based on this result, preliminary\nconclusions are drawn on the potential of coastal GNSS-R for sea state\nmonitoring using semi-empirical modeling to relate GNSS-R ICF coherence time to\nSWH. \n\n"}
{"id": "physics/0406084", "contents": "Title: On the GNSS-R Interferometric Complex Field Coherence Time Abstract: In this paper we focus on the microwave bistatic scattering process, with the\naim of deriving an expression for the interferometric complex field\nauto-correlation function from a static platform. We start from the Fresnel\nintegral and derive the auto-correlation function in the Fraunhofer and\nModified Fraunhofer regime. The autocorrelation function at short times can be\nexpressed as a Gaussian with a direction dependent time scale. The directional\nmodulation is a function of the angle between the scattering direction and the\nwave direction. The obtained relation can be used for directional sea state\nestimation using one or more GNSS-R coastal receivers. \n\n"}
{"id": "physics/0407037", "contents": "Title: The Eddy Experiment: GNSS-R speculometry for directional sea-roughness\n  retrieval from low altitude aircraft Abstract: We report on the retrieval of directional sea surface roughness, in terms of\nits full directional mean square slope (including direction and isotropy), from\nGlobal Navigation Satellite System Reflections (GNSS-R) Delay-Doppler-Map (DDM)\ndata collected during an experimental flight at 1 km altitude. This study\nemphasizes the utilization of the entire DDM to more precisely infer ocean\nroughness directional parameters. In particular, we argue that the DDM exhibits\nthe impact of both roughness and scatterer velocity. Obtained estimates are\nanalyzed and compared to co-located Jason-1 measurements, ECMWF numerical\nweather model outputs and optical data. \n\n"}
{"id": "physics/0407082", "contents": "Title: A link between an ice age era and a rapid polar shift Abstract: The striking asymmetry of the ice cover during the Last Global Maximum\nsuggests that the North Pole was in Greenland and then rapidly shifted to its\npresent position in the Arctic See. A scenario which causes such a rapid\ngeographic polar shift is physically possible. It involves an additional\nplanet, which disappeared by evaporation within the Holocene. This is only\npossible within such a short period, if the planet was in an extremely\neccentric orbit and hot. Then, since this produced an interplanetary gas cloud,\nthe polar shift had to be preceded by a cold period with large global\ntemperature variations during several million years. \n\n"}
{"id": "physics/0411011", "contents": "Title: Water waves over a time-dependent bottom: Exact description for 2D\n  potential flows Abstract: Two-dimensional potential flows of an ideal fluid with a free surface are\nconsidered in situations when shape of the bottom depends on time due to\nexternal reasons. Exact nonlinear equations describing surface waves in terms\nof the so called conformal variables are derived for an arbitrary time-evolving\nbottom parameterized by an analytical function. An efficient numerical method\nfor the obtained equations is suggested. \n\n"}
{"id": "physics/0503155", "contents": "Title: Describing two-dimensional vortical flows : the typhoon case Abstract: We present results of a numerical study of the differential equation\ngoverning the stationary states of the two-dimensional planetary atmosphere and\nmagnetized plasma (within the Charney Hasegawa Mima model). The most strinking\nresult is that the equation appears to be able to reproduce the main features\nof the flow structure of a typhoon. \n\n"}
{"id": "physics/0503187", "contents": "Title: Reply to comment on `A simple model for the short-time evolution of\n  near-surface current and temperature profiles' Abstract: This is our response to a comment by Walter Eifler on our paper `A simple\nmodel for the short-time evolution of near-surface current and temperature\nprofiles' (arXiv:physics/0503186, accepted for publication in Deep-Sea Research\nII). Although Eifler raises genuine issues regarding our model's validity and\napplicability, we are nevertheless of the opinion that it is of value for the\nshort-term evolution of the upper-ocean profiles of current and temperature.\nThe fact that the effective eddy viscosity tends to infinity for infinite time\nunder a steady wind stress may not be surprising. It can be interpreted as a\nvertical shift of the eddy viscosity profile and an increase in the size of the\ndominant turbulent eddies under the assumed conditions of small stratification\nand infinite water depth. \n\n"}
{"id": "physics/0505103", "contents": "Title: Statistical modelling of tropical cyclone tracks: a comparison of models\n  for the variance of trajectories Abstract: We describe results from the second stage of a project to build a statistical\nmodel for hurricane tracks. In the first stage we modelled the unconditional\nmean track. We now attempt to model the unconditional variance of fluctuations\naround the mean. The variance models we describe use a semi-parametric nearest\nneighbours approach in which the optimal averaging length-scale is estimated\nusing a jack-knife out-of-sample fitting procedure. We test three different\nmodels. These models consider the variance structure of the deviations from the\nunconditional mean track to be isotropic, anisotropic but uncorrelated, and\nanisotropic and correlated, respectively. The results show that, of these\nmodels, the anisotropic correlated model gives the best predictions of the\ndistribution of future positions of hurricanes. \n\n"}
{"id": "physics/0508226", "contents": "Title: Sensitivity of ray dynamics in an underwater sound channel to vertical\n  scale of longitudinal sound-speed variations Abstract: We investigate sound ray propagation in a range-dependent underwater acoustic\nwaveguide. Our attention is focused on sensitivity of ray dynamics to the\nvertical structure of a sound-speed perturbation induced by ocean internal\nwaves. Two models of longitudinal sound-speed variations are considered: a\nperiodic inhomogeneity and a stochastic one. It is found that vertical\noscillations of a sound-speed perturbation can affect rays in a resonant\nmanner. Such resonances give rise to chaos in certain regions of phase space.\nIt is shown that stability of steep rays, being observed in experiments, is\nconnected with suppression of resonances in the case of small-scale vertical\nsound-speed oscillations. \n\n"}
{"id": "physics/0509024", "contents": "Title: Statistical modelling of tropical cyclone tracks: modelling the\n  autocorrelation in track shape Abstract: We describe results from the third stage of a project to build a statistical\nmodel for hurricane tracks. In the first stage we modelled the unconditional\nmean track. In the second stage we modelled the unconditional variance of\nfluctuations around the mean. Now we address the question of how to model the\nautocorrelations in the standardised fluctuations. We perform a thorough\ndiagnostic analysis of these fluctuations, and fit a type of AR(1) model. We\nthen assess the goodness of fit of this model in a number of ways, including an\nout-of-sample comparison with a simpler model, an in-sample residual analysis,\nand a comparison of simulated tracks from the model with the observed tracks.\nBroadly speaking, the model captures the behaviour of observed hurricane\ntracks. In detail, however, there are a number of systematic errors. \n\n"}
{"id": "physics/0510150", "contents": "Title: Current effects on scattering of surface gravity waves by bottom\n  topography Abstract: Scattering of random surface gravity waves by small amplitude topography in\nthe presence of a uniform current is investigated theoretically. This problem\nis relevant to ocean waves propagation on shallow continental shelves where\ntidal currents are often significant. A perturbation expansion of the wave\naction to second order in powers of the bottom amplitude yields an evolution\nequation for the wave action spectrum. A scattering source term gives the rate\nof exchange of the wave action spectrum between wave components, with\nconservation of the total action at each absolute frequency. With and without\ncurrent, the scattering term yields reflection coefficients for the amplitudes\nof waves that converge, to the results of previous theories for monochromatic\nwaves propagating in one dimension over sinusoidal bars. Over sandy continental\nshelves, tidal currents are known to generate sandwaves with scales comparable\nto those of surface waves. Application of the theory to such a real topography\nsuggests that scattering mainly results in a broadening of the directional wave\nspectrum, due to forward scattering, while the back-scattering is generally\nweaker. The current may strongly influence surface gravity wave scattering by\nselecting different bottom scales with widely different spectral densities due\nthe sharp bottom spectrum roll-off. \n\n"}
{"id": "physics/0511236", "contents": "Title: Efficient Data Assimilation for Spatiotemporal Chaos: a Local Ensemble\n  Transform Kalman Filter Abstract: Data assimilation is an iterative approach to the problem of estimating the\nstate of a dynamical system using both current and past observations of the\nsystem together with a model for the system's time evolution. Rather than\nsolving the problem from scratch each time new observations become available,\none uses the model to ``forecast'' the current state, using a prior state\nestimate (which incorporates information from past data) as the initial\ncondition, then uses current data to correct the prior forecast to a current\nstate estimate. This Bayesian approach is most effective when the uncertainty\nin both the observations and in the state estimate, as it evolves over time,\nare accurately quantified. In this article, we describe a practical method for\ndata assimilation in large, spatiotemporally chaotic systems. The method is a\ntype of ``ensemble Kalman filter'', in which the state estimate and its\napproximate uncertainty are represented at any given time by an ensemble of\nsystem states. We discuss both the mathematical basis of this approach and its\nimplementation; our primary emphasis is on ease of use and computational speed\nrather than improving accuracy over previously published approaches to ensemble\nKalman filtering. We include some numerical results demonstrating the\nefficiency and accuracy of our implementation for assimilating real atmospheric\ndata with the global forecast model used by the U.S. National Weather Service. \n\n"}
{"id": "physics/0512092", "contents": "Title: Year-ahead prediction of US landfalling hurricane numbers: intense\n  hurricanes Abstract: We continue with our program to derive simple practical methods that can be\nused to predict the number of US landfalling hurricanes a year in advance. We\nrepeat an earlier study, but for a slightly different definition landfalling\nhurricanes, and for intense hurricanes only. We find that the averaging lengths\nneeded for optimal predictions of numbers of intense hurricanes are longer than\nthose needed for optimal predictions of numbers of hurricanes of all strengths. \n\n"}
{"id": "physics/0512135", "contents": "Title: Statistical modelling of tropical cyclone tracks: non-normal innovations Abstract: We present results from the sixth stage of a project to build a statistical\nhurricane model. Previous papers have described our modelling of the tracks,\ngenesis, and lysis of hurricanes. In our track model we have so far employed a\nnormal distribution for the residuals when computing innovations, even though\nwe have demonstrated that their distribution is not normal. Here, we test to\nsee if the track model can be improved by including more realistic non-normal\ninnovations. The results are mixed. Some features of the model improve, but\nothers slightly worsen. \n\n"}
{"id": "physics/0602024", "contents": "Title: Piecewise continuous distribution function method and ultrasound at half\n  space Abstract: The system of hydrodynamic-type equations, derived by two-side distribution\nfunction for a stratified gas in gravity field is applied to a problem of\nultrasound propagation and attenuation. The background state and linearized\nversion of the obtained system is studied and compared with the Navier-Stokes\none at arbitrary Knudsen numbers. The WKB solutions for ultrasound in a\nstratified medium are constructed in explicit form. The problem of a generation\nby a moving plane in a rarefied gas is explored and used as a test while\ncompared with experiment. \n\n"}
{"id": "physics/0605167", "contents": "Title: Essence of inviscid shear instability: a point view of vortex dynamics Abstract: The essence of shear instability is fully revealed both mathematically and\nphysically. A general sufficient and necessary stable criterion is obtained\nanalytically within linear context. It is the analogue of Kelvin-Arnol'd\ntheorem, i.e., the stable flow minimizes the kinetic energy associated with\nvorticity. Then the mechanism of shear instability is explored by combining the\nmechanisms of both Kelvin-Helmholtz instability (K-H instability) and resonance\nof waves. It requires both concentrated vortex and resonant waves for the\ninstability. The waves, which have same phase speed with the concentrated\nvortex, have interactions with the vortex to trigger the instability. We call\nthis mechanism as \"concentrated vortex instability\". The physical explanation\nof shear instability is also sketched. Finally, some useful criteria are\nderived from the theorem. These results would intrigue future works to\ninvestigate the other hydrodynamic instabilities. \n\n"}
{"id": "physics/0609189", "contents": "Title: On the parabolic equation method in internal wave propagation Abstract: A parabolic equation for the propagation of periodic internal waves over\nvarying bottom topography is derived using the multiple-scale perturbation\nmethod. Some computational aspects of the numerical implementation are\ndiscussed. The results of numerical experiments on propagation of an incident\nplane wave over a circular-type shoal are presented in comparison with the\nanalytical result, based on Born approximation. \n\n"}
{"id": "physics/0611006", "contents": "Title: Comparing classical and Bayesian methods for predicting hurricane\n  landfall rates Abstract: We compare classical and Bayesian methods for fitting the poisson\ndistribution to the number of hurricanes making landfall on sections of the US\ncoastline. \n\n"}
{"id": "physics/0611103", "contents": "Title: Predicting hurricane regional landfall rates: comparing local and\n  basin-wide track model approaches Abstract: We compare two methods for making predictions of the climatological\ndistribution of the number of hurricanes making landfall along short sections\nof the North American coastline. The first method uses local data, and the\nsecond method uses a basin-wide track model. Using cross-validation we show\nthat the basin-wide track model gives better predictions for almost all parts\nof the coastline. This is the first time such a comparison has been made, and\nis the first rigourous justification for the use of basin-wide track models for\npredicting hurricane landfall rates and hurricane risk. \n\n"}
{"id": "physics/0611107", "contents": "Title: Change-point detection in the historical hurricane number time-series:\n  why can't we detect change-points at US landfall? Abstract: The time series of the number of hurricanes per year in the Atlantic basin\nshows a clear change of level between 1994 and 1995. The time series of the\nnumber of hurricanes that make landfall in the US, however, does not show the\nsame obvious change of level. Prima-facie this seems rather surprising, given\nthat the landfalling hurricanes are a subset of the basin hurricanes. We\ninvestigate whether it really should be considered surprising or whether there\nis a simple statistical explanation for the disappearance of this change-point\nat landfall. \n\n"}
{"id": "physics/0612042", "contents": "Title: Extreme Value Statistics of the Total Energy in an Intermediate\n  Complexity Model of the Mid-latitude Atmospheric Jet. Part II: trend\n  detection and assessment Abstract: A baroclinic model for the atmospheric jet at middle-latitudes is used as\nstochastic generator of non-stationary time series of the total energy of the\nsystem. A linear time trend is imposed on the parameter $T_E$, descriptive of\nthe forced equator-to-pole temperature gradient and responsible for setting the\naverage baroclinicity in the model. The focus lies on establishing a\ntheoretically sound framework for the detection and assessment of trend at\nextreme values of the generated time series. This problem is dealt with by\nfitting time-dependent Generalized Extreme Value (GEV) models to sequences of\nyearly maxima of the total energy. A family of GEV models is used in which the\nlocation $\\mu$ and scale parameters $\\sigma$ depend quadratically and linearly\non time, respectively, while the shape parameter $\\xi$ is kept constant. From\nthis family, a model is selected by using diagnostic graphical tools, such as\nprobability and quantile plots, and by means of the likelihood ratio test. The\ninferred location and scale parameters are found to depend in a rather smooth\nway on time and, therefore, on $T_E$. In particular, power-law dependences of\n$\\mu$ and $\\sigma$ on $T_E$ are obtained, in analogy with the results of a\nprevious work where the same baroclinic model was run with fixed values of\n$T_E$ spanning the same range as in this case. It is emphasized under which\nconditions the adopted approach is valid. \n\n"}
{"id": "physics/0701022", "contents": "Title: Humidity contribution to C_n^2 over a 600m pathlength in a tropical\n  marine environment Abstract: We present new optical turbulence structure parameter measurements, C_n^2,\nover sea water between La Parguera and Magueyes Island (17.6N 67W) on the\nsouthwest coast of Puerto Rico. The 600 meter horizontal paths were located\napproximately 1.5 m and 10 m above sea level. No data of this type has ever\nbeen made available in the literature. Based on the data, we show that the\nC_n^2 measurements are about 7 times less compared to equivalent land data.\nThis strong evidence reinforces our previous argument that humidity must be\naccounted for to better ascertain the near surface atmospheric turbulence\neffects, which current visible / near infrared C_n^2 bulk models fail to do. We\nalso explore the generalised fractal dimension of this littoral data and\ncompare it to our reference land data. We find cases that exhibit monofractal\ncharacteristics, that is to say, the effect of rising temperatures during the\ndaylight hours upon turbulence are counterbalanced by humidity, leading to a\nsingle characteristic scale for the measurements. In other words, significant\nmoisture changes in the measurement volume cancels optical turbulence increases\ndue to temperature rises. Figures available as JPG only. \n\n"}
{"id": "physics/0701166", "contents": "Title: Predicting landfalling hurricane numbers from basin hurricane numbers:\n  statistical analysis and predictions Abstract: One possible method for predicting landfalling hurricane numbers is to first\npredict the number of hurricanes in the basin and then convert that prediction\nto a prediction of landfalling hurricane numbers using an estimated proportion.\nShould this work better than just predicting landfalling hurricane numbers\ndirectly? We perform a basic statistical analysis of this question in the\ncontext of a simple abstract model, and convert some previous predictions of\nbasin numbers into landfalling numbers. \n\n"}
{"id": "physics/0701167", "contents": "Title: Predicting hurricane numbers from Sea Surface Temperature: closed form\n  expressions for the mean, variance and standard error of the number of\n  hurricanes Abstract: One way to predict hurricane numbers would be to predict sea surface\ntemperature, and then predict hurricane numbers as a function of the predicted\nsea surface temperature. For certain parametric models for sea surface\ntemperature and the relationship between sea surface temperature and hurricane\nnumbers, closed-form solutions exist for the mean and the variance of the\nnumber of predicted hurricanes, and for the standard error on the mean. We\nderive a number of such expressions. \n\n"}
{"id": "physics/0701173", "contents": "Title: Statistical Modelling of the Relationship Between Main Development\n  Region Sea Surface Temperature and \\emph{Landfalling} Atlantic Basin\n  Hurricane Numbers Abstract: We are building a hurricane number prediction scheme that relies, in part, on\nstatistical modelling of the empirical relationship between Atlantic sea\nsurface temperatures and landfalling hurricane numbers. We test out a number of\nsimple statistical models for that relationship, using data from 1900 to 2005\nand data from 1950 to 2005, and for both all hurricane numbers and intense\nhurricane numbers. The results are very different from the corresponding\nanalysis for basin hurricane numbers. \n\n"}
{"id": "physics/0701175", "contents": "Title: Correlations between hurricane numbers and sea surface temperature: why\n  does the correlation disappear at landfall? Abstract: There is significant correlation between main development region sea surface\ntemperature and the number of hurricanes that form in the Atlantic basin. The\ncorrelation between the same sea surface temperatures and the number of\n\\emph{landfalling} hurricanes is much lower, however. Why is this? Do we need\nto consider complex physical hypotheses, or is there a simple statistical\nexplanation? \n\n"}
{"id": "physics/0702034", "contents": "Title: In a book \"Tsunami and Nonlinear Waves\": Numerical Verification of the\n  Hasselmann equation Abstract: The purpose of this article is numerical verification of the thory of weak\nturbulence. We performed numerical simulation of an ensemble of nonlinearly\ninteracting free gravity waves (swell) by two different methods: solution of\nprimordial dynamical equations describing potential flow of the ideal fluid\nwith a free surface and, solution of the kinetic Hasselmann equation,\ndescribing the wave ensemble in the framework of the theory of weak turbulence.\nComparison of the results demonstrates pretty good applicability of the weak\nturbulent approach. \n\n"}
{"id": "physics/0702067", "contents": "Title: Explicit wave-averaged primitive equations using a Generalized\n  Lagrangian Mean Abstract: The generalized Langrangian mean theory provides exact equations for general\nwave-turbulence-mean flow interactions in three dimensions. For practical\napplications, these equations must be closed by specifying the wave forcing\nterms. Here an approximate closure is obtained under the hypotheses of small\nsurface slope, weak horizontal gradients of the water depth and mean current,\nand weak curvature of the mean current profile. These assumptions yield\nanalytical expressions for the mean momentum and pressure forcing terms that\ncan be expressed in terms of the wave spectrum. A vertical change of coordinate\nis then applied to obtain glm2z-RANS equations (55) and (57) with non-divergent\nmass transport in cartesian coordinates. To lowest order, agreement is found\nwith Eulerian-mean theories, and the present approximation provides an explicit\nextension of known wave-averaged equations to short-scale variations of the\nwave field, and vertically varying currents only limited to weak or localized\nprofile curvatures. Further, the underlying exact equations provide a natural\nframework for extensions to finite wave amplitudes and any realistic situation.\nThe accuracy of the approximations is discussed using comparisons with exact\nnumerical solutions for linear waves over arbitrary bottom slopes, for which\nthe equations are still exact when properly accounting for partial standing\nwaves. For finite amplitude waves it is found that the approximate solutions\nare probably accurate for ocean mixed layer modelling and shoaling waves,\nprovided that an adequate turbulent closure is designed. However, for surf zone\napplications the approximations are expected to give only qualitative results\ndue to the large influence of wave nonlinearity on the vertical profiles of\nwave forcing terms. \n\n"}
{"id": "physics/0702145", "contents": "Title: Numerical Verification of the Weak Turbulent Model for Swell Evolution Abstract: The purpose of this article is numerical verification of the theory of weak\nturbulence. We performed numerical simulation of an ensemble of nonlinearly\ninteracting free gravity waves (swell) by two different methods: solution of\nprimordial dynamical equations describing potential flow of the ideal fluid\nwith a free surface and, solution of the kinetic Hasselmann equation,\ndescribing the wave ensemble in the framework of the theory of weak turbulence.\n  In both cases we observed effects predicted by this theory: frequency\ndownshift, angular spreading and formation of Zakharov-Filonenko spectrum\n$I_{\\omega} \\sim \\omega^{-4}$. To achieve quantitative coincidence of the\nresults obtained by different methods, one has to supply the Hasselmann kinetic\nequation by an empirical dissipation term $S_{diss}$ modeling the coherent\neffects of white-capping. Using of the standard dissipation terms from\noperational wave predicting model ({\\it WAM}) leads to significant improvement\non short times, but not resolve the discrepancy completely, leaving the\nquestion about optimal choice of $S_{diss}$ open. In a long run {\\it WAM}\ndissipative terms overestimate dissipation essentially. \n\n"}
{"id": "physics/9708037", "contents": "Title: Tracking down the ENSO delayed oscillator with an adjoint OGCM Abstract: The adjoint of an ocean general circulation model is used as a tool for\ninvestigating the causes of changes in ENSO SST indices. We identify adjoint\nKelvin and Rossby waves in the sensitivities to sea level and wind stress at\nearlier times, which can be traced back for more than a year through western\nand weak eastern boundary reflections. Depending on the thermocline depth the\nfirst and second baroclinic modes are excited. The sensitivities to the heat\nflux and SST are local and decay in about a month. The sensitivities to the\nfluxes are converted into the influence of SST using the adjoint of a\nstatistical atmosphere model. Focusing on SST perturbations in the index region\nitself, we recover, up to a scale factor, the delayed oscillator concept. \n\n"}
{"id": "physics/9904002", "contents": "Title: What caused the onset of the 1997-1998 El Nino? Abstract: There has been intense debate about the causes of the 1997-1998 El Nino. One\nside sees the obvious intense westerly wind events as the main cause for the\nexceptional heating in summer 1997, the other emphasizes slower oceanic\nprocesses. We present a quantitative analysis of all factors contributing to\nthe onset of this El Nino. At six months' lead time the initial state\ncontributes about 40% of the heating compared with an average year, and the\nwind about 50%. Compared with 1996, these contributions are 30% and 90%\nrespectively. As westerly wind events are difficult to predict, this limited\nthe predictability of the onset of this El Nino. \n\n"}
