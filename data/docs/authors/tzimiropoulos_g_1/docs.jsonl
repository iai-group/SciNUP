{"id": "0912.1310", "contents": "Title: Automatic creation of urban velocity fields from aerial video Abstract: In this paper, we present a system for modelling vehicle motion in an urban\nscene from low frame-rate aerial video. In particular, the scene is modelled as\na probability distribution over velocities at every pixel in the image.\n  We describe the complete system for acquiring this model. The video is\ncaptured from a helicopter and stabilized by warping the images to match an\northorectified image of the area. A pixel classifier is applied to the\nstabilized images, and the response is segmented to determine car locations and\norientations. The results are fed in to a tracking scheme which tracks cars for\nthree frames, creating tracklets. This allows the tracker to use a combination\nof velocity, direction, appearance, and acceleration cues to keep only tracks\nlikely to be correct. Each tracklet provides a measurement of the car velocity\nat every point along the tracklet's length, and these are then aggregated to\ncreate a histogram of vehicle velocities at every pixel in the image.\n  The results demonstrate that the velocity probability distribution prior can\nbe used to infer a variety of information about road lane directions, speed\nlimits, vehicle speeds and common trajectories, and traffic bottlenecks, as\nwell as providing a means of describing environmental knowledge about traffic\nrules that can be used in tracking. \n\n"}
{"id": "1003.0776", "contents": "Title: Properties of the Discrete Pulse Transform for Multi-Dimensional Arrays Abstract: This report presents properties of the Discrete Pulse Transform on\nmulti-dimensional arrays introduced by the authors two or so years ago. The\nmain result given here in Lemma 2.1 is also formulated in a paper to appear in\nIEEE Transactions on Image Processing. However, the proof, being too technical,\nwas omitted there and hence it appears in full in this publication. \n\n"}
{"id": "1109.4909", "contents": "Title: Sparse Online Low-Rank Projection and Outlier Rejection (SOLO) for 3-D\n  Rigid-Body Motion Registration Abstract: Motivated by an emerging theory of robust low-rank matrix representation, in\nthis paper, we introduce a novel solution for online rigid-body motion\nregistration. The goal is to develop algorithmic techniques that enable a\nrobust, real-time motion registration solution suitable for low-cost, portable\n3-D camera devices. Assuming 3-D image features are tracked via a standard\ntracker, the algorithm first utilizes Robust PCA to initialize a low-rank shape\nrepresentation of the rigid body. Robust PCA finds the global optimal solution\nof the initialization, while its complexity is comparable to singular value\ndecomposition. In the online update stage, we propose a more efficient\nalgorithm for sparse subspace projection to sequentially project new feature\nobservations onto the shape subspace. The lightweight update stage guarantees\nthe real-time performance of the solution while maintaining good registration\neven when the image sequence is contaminated by noise, gross data corruption,\noutlying features, and missing data. The state-of-the-art accuracy of the\nsolution is validated through extensive simulation and a real-world experiment,\nwhile the system enjoys one to two orders of magnitude speed-up compared to\nwell-established RANSAC solutions. The new algorithm will be released online to\naid peer evaluation. \n\n"}
{"id": "1110.0957", "contents": "Title: Dictionary Learning for Deblurring and Digital Zoom Abstract: This paper proposes a novel approach to image deblurring and digital zooming\nusing sparse local models of image appearance. These models, where small image\npatches are represented as linear combinations of a few elements drawn from\nsome large set (dictionary) of candidates, have proven well adapted to several\nimage restoration tasks. A key to their success has been to learn dictionaries\nadapted to the reconstruction of small image patches. In contrast, recent works\nhave proposed instead to learn dictionaries which are not only adapted to data\nreconstruction, but also tuned for a specific task. We introduce here such an\napproach to deblurring and digital zoom, using pairs of blurry/sharp (or\nlow-/high-resolution) images for training, as well as an effective stochastic\ngradient algorithm for solving the corresponding optimization task. Although\nthis learning problem is not convex, once the dictionaries have been learned,\nthe sharp/high-resolution image can be recovered via convex optimization at\ntest time. Experiments with synthetic and real data demonstrate the\neffectiveness of the proposed approach, leading to state-of-the-art performance\nfor non-blind image deblurring and digital zoom. \n\n"}
{"id": "1202.6666", "contents": "Title: Perturbation of the Eigenvectors of the Graph Laplacian: Application to\n  Image Denoising Abstract: The original contributions of this paper are twofold: a new understanding of\nthe influence of noise on the eigenvectors of the graph Laplacian of a set of\nimage patches, and an algorithm to estimate a denoised set of patches from a\nnoisy image. The algorithm relies on the following two observations: (1) the\nlow-index eigenvectors of the diffusion, or graph Laplacian, operators are very\nrobust to random perturbations of the weights and random changes in the\nconnections of the patch-graph; and (2) patches extracted from smooth regions\nof the image are organized along smooth low-dimensional structures in the\npatch-set, and therefore can be reconstructed with few eigenvectors.\nExperiments demonstrate that our denoising algorithm outperforms the denoising\ngold-standards. \n\n"}
{"id": "1203.0781", "contents": "Title: Posterior Mean Super-Resolution with a Compound Gaussian Markov Random\n  Field Prior Abstract: This manuscript proposes a posterior mean (PM) super-resolution (SR) method\nwith a compound Gaussian Markov random field (MRF) prior. SR is a technique to\nestimate a spatially high-resolution image from observed multiple\nlow-resolution images. A compound Gaussian MRF model provides a preferable\nprior for natural images that preserves edges. PM is the optimal estimator for\nthe objective function of peak signal-to-noise ratio (PSNR). This estimator is\nnumerically determined by using variational Bayes (VB). We then solve the\nconjugate prior problem on VB and the exponential-order calculation cost\nproblem of a compound Gaussian MRF prior with simple Taylor approximations. In\nexperiments, the proposed method roughly overcomes existing methods. \n\n"}
{"id": "1208.4391", "contents": "Title: Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev\n  Descent Abstract: We present a method to track the precise shape of an object in video based on\nnew modeling and optimization on a new Riemannian manifold of parameterized\nregions.\n  Joint dynamic shape and appearance models, in which a template of the object\nis propagated to match the object shape and radiance in the next frame, are\nadvantageous over methods employing global image statistics in cases of complex\nobject radiance and cluttered background. In cases of 3D object motion and\nviewpoint change, self-occlusions and dis-occlusions of the object are\nprominent, and current methods employing joint shape and appearance models are\nunable to adapt to new shape and appearance information, leading to inaccurate\nshape detection. In this work, we model self-occlusions and dis-occlusions in a\njoint shape and appearance tracking framework.\n  Self-occlusions and the warp to propagate the template are coupled, thus a\njoint problem is formulated. We derive a coarse-to-fine optimization scheme,\nadvantageous in object tracking, that initially perturbs the template by coarse\nperturbations before transitioning to finer-scale perturbations, traversing all\nscales, seamlessly and automatically. The scheme is a gradient descent on a\nnovel infinite-dimensional Riemannian manifold that we introduce. The manifold\nconsists of planar parameterized regions, and the metric that we introduce is a\nnovel Sobolev-type metric defined on infinitesimal vector fields on regions.\nThe metric has the property of resulting in a gradient descent that\nautomatically favors coarse-scale deformations (when they reduce the energy)\nbefore moving to finer-scale deformations.\n  Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and\nbackground show that occlusion/dis-occlusion modeling leads to superior shape\naccuracy compared to recent methods employing joint shape/appearance models or\nemploying global statistics. \n\n"}
{"id": "1208.5365", "contents": "Title: A Missing and Found Recognition System for Hajj and Umrah Abstract: This note describes an integrated recognition system for identifying missing\nand found objects as well as missing, dead, and found people during Hajj and\nUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of\nSaudi Arabia. It is assumed that the total estimated number of pilgrims will\nreach 20 millions during the next decade. The ultimate goal of this system is\nto integrate facial recognition and object identification solutions into the\nHajj and Umrah rituals. The missing and found computerized system is part of\nthe CrowdSensing system for Hajj and Umrah crowd estimation, management and\nsafety. \n\n"}
{"id": "1211.5712", "contents": "Title: Detection of elliptical shapes via cross-entropy clustering Abstract: The problem of finding elliptical shapes in an image will be considered. We\ndiscuss the solution which uses cross-entropy clustering. The proposed method\nallows the search for ellipses with predefined sizes and position in the space.\nMoreover, it works well for search of ellipsoids in higher dimensions. \n\n"}
{"id": "1212.0433", "contents": "Title: Compressive Schlieren Deflectometry Abstract: Schlieren deflectometry aims at characterizing the deflections undergone by\nrefracted incident light rays at any surface point of a transparent object. For\nsmooth surfaces, each surface location is actually associated with a sparse\ndeflection map (or spectrum). This paper presents a novel method to\ncompressively acquire and reconstruct such spectra. This is achieved by\naltering the way deflection information is captured in a common Schlieren\nDeflectometer, i.e., the deflection spectra are indirectly observed by the\nprinciple of spread spectrum compressed sensing. These observations are\nrealized optically using a 2-D Spatial Light Modulator (SLM) adjusted to the\ncorresponding sensing basis and whose modulations encode the light deviation\nsubsequently recorded by a CCD camera. The efficiency of this approach is\ndemonstrated experimentally on the observation of few test objects. Further,\nusing a simple parametrization of the deflection spectra we show that relevant\nkey parameters can be directly computed using the measurements, avoiding full\nreconstruction. \n\n"}
{"id": "1212.3530", "contents": "Title: A Multi-Orientation Analysis Approach to Retinal Vessel Tracking Abstract: This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications. \n\n"}
{"id": "1302.6957", "contents": "Title: Ensemble Sparse Models for Image Analysis Abstract: Sparse representations with learned dictionaries have been successful in\nseveral image analysis applications. In this paper, we propose and analyze the\nframework of ensemble sparse models, and demonstrate their utility in image\nrestoration and unsupervised clustering. The proposed ensemble model\napproximates the data as a linear combination of approximations from multiple\n\\textit{weak} sparse models. Theoretical analysis of the ensemble model reveals\nthat even in the worst-case, the ensemble can perform better than any of its\nconstituent individual models. The dictionaries corresponding to the individual\nsparse models are obtained using either random example selection or boosted\napproaches. Boosted approaches learn one dictionary per round such that the\ndictionary learned in a particular round is optimized for the training examples\nhaving high reconstruction error in the previous round. Results with compressed\nrecovery show that the ensemble representations lead to a better performance\ncompared to using a single dictionary obtained with the conventional\nalternating minimization approach. The proposed ensemble models are also used\nfor single image superresolution, and we show that they perform comparably to\nthe recent approaches. In unsupervised clustering, experiments show that the\nproposed model performs better than baseline approaches in several standard\ndatasets. \n\n"}
{"id": "1305.6387", "contents": "Title: Higher-order Segmentation via Multicuts Abstract: Multicuts enable to conveniently represent discrete graphical models for\nunsupervised and supervised image segmentation, in the case of local energy\nfunctions that exhibit symmetries. The basic Potts model and natural extensions\nthereof to higher-order models provide a prominent class of such objectives,\nthat cover a broad range of segmentation problems relevant to image analysis\nand computer vision. We exhibit a way to systematically take into account such\nhigher-order terms for computational inference. Furthermore, we present results\nof a comprehensive and competitive numerical evaluation of a variety of\ndedicated cutting-plane algorithms. Our approach enables the globally optimal\nevaluation of a significant subset of these models, without compromising\nruntime. Polynomially solvable relaxations are studied as well, along with\nadvanced rounding schemes for post-processing. \n\n"}
{"id": "1305.6918", "contents": "Title: Video Human Segmentation using Fuzzy Object Models and its Application\n  to Body Pose Estimation of Toddlers for Behavior Studies Abstract: Video object segmentation is a challenging problem due to the presence of\ndeformable, connected, and articulated objects, intra- and inter-object\nocclusions, object motion, and poor lighting. Some of these challenges call for\nobject models that can locate a desired object and separate it from its\nsurrounding background, even when both share similar colors and textures. In\nthis work, we extend a fuzzy object model, named cloud system model (CSM), to\nhandle video segmentation, and evaluate it for body pose estimation of toddlers\nat risk of autism. CSM has been successfully used to model the parts of the\nbrain (cerebrum, left and right brain hemispheres, and cerebellum) in order to\nautomatically locate and separate them from each other, the connected brain\nstem, and the background in 3D MR-images. In our case, the objects are\narticulated parts (2D projections) of the human body, which can deform, cause\nself-occlusions, and move along the video. The proposed CSM extension handles\narticulation by connecting the individual clouds, body parts, of the system\nusing a 2D stickman model. The stickman representation naturally allows us to\nextract 2D body pose measures of arm asymmetry patterns during unsupported gait\nof toddlers, a possible behavioral marker of autism. The results show that our\nmethod can provide insightful knowledge to assist the specialist's observations\nduring real in-clinic assessments. \n\n"}
{"id": "1306.1392", "contents": "Title: PyHST2: an hybrid distributed code for high speed tomographic\n  reconstruction with iterative reconstruction and a priori knowledge\n  capabilities Abstract: We present the PyHST2 code which is in service at ESRF for phase-contrast and\nabsorption tomography. This code has been engineered to sustain the high data\nflow typical of the third generation synchrotron facilities (10 terabytes per\nexperiment) by adopting a distributed and pipelined architecture. The code\nimplements, beside a default filtered backprojection reconstruction, iterative\nreconstruction techniques with a-priori knowledge. These latter are used to\nimprove the reconstruction quality or in order to reduce the required data\nvolume and reach a given quality goal. The implemented a-priori knowledge\ntechniques are based on the total variation penalisation and a new recently\nfound convex functional which is based on overlapping patches.\n  We give details of the different methods and their implementations while the\ncode is distributed under free license.\n  We provide methods for estimating, in the absence of ground-truth data, the\noptimal parameters values for a-priori techniques. \n\n"}
{"id": "1307.1437", "contents": "Title: Toward Guaranteed Illumination Models for Non-Convex Objects Abstract: Illumination variation remains a central challenge in object detection and\nrecognition. Existing analyses of illumination variation typically pertain to\nconvex, Lambertian objects, and guarantee quality of approximation in an\naverage case sense. We show that it is possible to build V(vertex)-description\nconvex cone models with worst-case performance guarantees, for non-convex\nLambertian objects. Namely, a natural verification test based on the angle to\nthe constructed cone guarantees to accept any image which is sufficiently\nwell-approximated by an image of the object under some admissible lighting\ncondition, and guarantees to reject any image that does not have a sufficiently\ngood approximation. The cone models are generated by sampling point\nilluminations with sufficient density, which follows from a new perturbation\nbound for point images in the Lambertian model. As the number of point images\nrequired for guaranteed verification may be large, we introduce a new\nformulation for cone preserving dimensionality reduction, which leverages tools\nfrom sparse and low-rank decomposition to reduce the complexity, while\ncontrolling the approximation error with respect to the original cone. \n\n"}
{"id": "1309.0270", "contents": "Title: High-Accuracy Total Variation for Compressed Video Sensing Abstract: Numerous total variation (TV) regularizers, engaged in image restoration\nproblem, encode the gradients by means of simple $[-1,1]$ FIR filter. Despite\nits low computational processing, this filter severely deviates signal's high\nfrequency components pertinent to edge/discontinuous information and cause\nseveral deficiency issues known as texture and geometric loss. This paper\naddresses this problem by proposing an alternative model to the TV\nregularization problem via high order accuracy differential FIR filters to\npreserve rapid transitions in signal recovery. A numerical encoding scheme is\ndesigned to extend the TV model into multidimensional representation (tensorial\ndecomposition). We adopt this design to regulate the spatial and temporal\nredundancy in compressed video sensing problem to jointly recover frames from\nunder-sampled measurements. We then seek the solution via alternating direction\nmethods of multipliers and find a unique solution to quadratic minimization\nstep with capability of handling different boundary conditions. The resulting\nalgorithm uses much lower sampling rate and highly outperforms alternative\nstate-of-the-art methods. This is evaluated both in terms of restoration\naccuracy and visual quality of the recovered frames. \n\n"}
{"id": "1309.2074", "contents": "Title: Learning Transformations for Clustering and Classification Abstract: A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification. \n\n"}
{"id": "1310.4249", "contents": "Title: Mapping the stereotyped behaviour of freely-moving fruit flies Abstract: Most animals possess the ability to actuate a vast diversity of movements,\nostensibly constrained only by morphology and physics. In practice, however, a\nfrequent assumption in behavioral science is that most of an animal's\nactivities can be described in terms of a small set of stereotyped motifs. Here\nwe introduce a method for mapping the behavioral space of organisms, relying\nonly upon the underlying structure of postural movement data to organize and\nclassify behaviors. We find that six different drosophilid species each perform\na mix of non-stereotyped actions and over one hundred hierarchically-organized,\nstereotyped behaviors. Moreover, we use this approach to compare these species'\nbehavioral spaces, systematically identifying subtle behavioral differences\nbetween closely-related species. \n\n"}
{"id": "1311.5591", "contents": "Title: PANDA: Pose Aligned Networks for Deep Attribute Modeling Abstract: We propose a method for inferring human attributes (such as gender, hair\nstyle, clothes style, expression, action) from images of people under large\nvariation of viewpoint, pose, appearance, articulation and occlusion.\nConvolutional Neural Nets (CNN) have been shown to perform very well on large\nscale object recognition problems. In the context of attribute classification,\nhowever, the signal is often subtle and it may cover only a small part of the\nimage, while the image is dominated by the effects of pose and viewpoint.\nDiscounting for pose variation would require training on very large labeled\ndatasets which are not presently available. Part-based models, such as poselets\nand DPM have been shown to perform well for this problem but they are limited\nby shallow low-level features. We propose a new method which combines\npart-based models and deep learning by training pose-normalized CNNs. We show\nsubstantial improvement vs. state-of-the-art methods on challenging attribute\nclassification tasks in unconstrained settings. Experiments confirm that our\nmethod outperforms both the best part-based methods on this problem and\nconventional CNNs trained on the full bounding box of the person. \n\n"}
{"id": "1312.6034", "contents": "Title: Deep Inside Convolutional Networks: Visualising Image Classification\n  Models and Saliency Maps Abstract: This paper addresses the visualisation of image classification models, learnt\nusing deep Convolutional Networks (ConvNets). We consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The first one generates an image, which maximises the class\nscore [Erhan et al., 2009], thus visualising the notion of the class, captured\nby a ConvNet. The second technique computes a class saliency map, specific to a\ngiven image and class. We show that such maps can be employed for weakly\nsupervised object segmentation using classification ConvNets. Finally, we\nestablish the connection between the gradient-based ConvNet visualisation\nmethods and deconvolutional networks [Zeiler et al., 2013]. \n\n"}
{"id": "1312.6186", "contents": "Title: GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network\n  Training Abstract: The ability to train large-scale neural networks has resulted in\nstate-of-the-art performance in many areas of computer vision. These results\nhave largely come from computational break throughs of two forms: model\nparallelism, e.g. GPU accelerated training, which has seen quick adoption in\ncomputer vision circles, and data parallelism, e.g. A-SGD, whose large scale\nhas been used mostly in industry. We report early experiments with a system\nthat makes use of both model parallelism and data parallelism, we call GPU\nA-SGD. We show using GPU A-SGD it is possible to speed up training of large\nconvolutional neural networks useful for computer vision. We believe GPU A-SGD\nwill make it possible to train larger networks on larger training sets in a\nreasonable amount of time. \n\n"}
{"id": "1401.4221", "contents": "Title: Distortion-driven Turbulence Effect Removal using Variational Model Abstract: It remains a challenge to simultaneously remove geometric distortion and\nspace-time-varying blur in frames captured through a turbulent atmospheric\nmedium. To solve, or at least reduce these effects, we propose a new scheme to\nrecover a latent image from observed frames by integrating a new variational\nmodel and distortion-driven spatial-temporal kernel regression. The proposed\nscheme first constructs a high-quality reference image from the observed frames\nusing low-rank decomposition. Then, to generate an improved registered\nsequence, the reference image is iteratively optimized using a variational\nmodel containing a new spatial-temporal regularization. The proposed fast\nalgorithm efficiently solves this model without the use of partial differential\nequations (PDEs). Next, to reduce blur variation, distortion-driven\nspatial-temporal kernel regression is carried out to fuse the registered\nsequence into one image by introducing the concept of the near-stationary\npatch. Applying a blind deconvolution algorithm to the fused image produces the\nfinal output. Extensive experimental testing shows, both qualitatively and\nquantitatively, that the proposed method can effectively alleviate distortion\nand blur and recover details of the original scene compared to state-of-the-art\nmethods. \n\n"}
{"id": "1401.5311", "contents": "Title: Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face\n  Recognition Abstract: To perform unconstrained face recognition robust to variations in\nillumination, pose and expression, this paper presents a new scheme to extract\n\"Multi-Directional Multi-Level Dual-Cross Patterns\" (MDML-DCPs) from face\nimages. Specifically, the MDMLDCPs scheme exploits the first derivative of\nGaussian operator to reduce the impact of differences in illumination and then\ncomputes the DCP feature at both the holistic and component levels. DCP is a\nnovel face image descriptor inspired by the unique textural structure of human\nfaces. It is computationally efficient and only doubles the cost of computing\nlocal binary patterns, yet is extremely robust to pose and expression\nvariations. MDML-DCPs comprehensively yet efficiently encodes the invariant\ncharacteristics of a face image from multiple levels into patterns that are\nhighly discriminative of inter-personal differences but robust to\nintra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC\n2.0, and LFW databases indicate that DCP outperforms the state-of-the-art local\ndescriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both face\nidentification and face verification tasks. More impressively, the best\nperformance is achieved on the challenging LFW and FRGC 2.0 databases by\ndeploying MDML-DCPs in a simple recognition scheme. \n\n"}
{"id": "1401.8126", "contents": "Title: Extrinsic Methods for Coding and Dictionary Learning on Grassmann\n  Manifolds Abstract: Sparsity-based representations have recently led to notable results in\nvarious visual recognition tasks. In a separate line of research, Riemannian\nmanifolds have been shown useful for dealing with features and models that do\nnot lie in Euclidean spaces. With the aim of building a bridge between the two\nrealms, we address the problem of sparse coding and dictionary learning over\nthe space of linear subspaces, which form Riemannian structures known as\nGrassmann manifolds. To this end, we propose to embed Grassmann manifolds into\nthe space of symmetric matrices by an isometric mapping. This in turn enables\nus to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we\npropose closed-form solutions for learning a Grassmann dictionary, atom by\natom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann\nsparse coding and dictionary learning algorithms through embedding into Hilbert\nspaces.\n  Experiments on several classification tasks (gender recognition, gesture\nclassification, scene analysis, face recognition, action recognition and\ndynamic texture classification) show that the proposed approaches achieve\nconsiderable improvements in discrimination accuracy, in comparison to\nstate-of-the-art methods such as kernelized Affine Hull Method and\ngraph-embedding Grassmann discriminant analysis. \n\n"}
{"id": "1402.0240", "contents": "Title: Graph Cuts with Interacting Edge Costs - Examples, Approximations, and\n  Algorithms Abstract: We study an extension of the classical graph cut problem, wherein we replace\nthe modular (sum of edge weights) cost function by a submodular set function\ndefined over graph edges. Special cases of this problem have appeared in\ndifferent applications in signal processing, machine learning, and computer\nvision. In this paper, we connect these applications via the generic\nformulation of \"cooperative graph cuts\", for which we study complexity,\nalgorithms, and connections to polymatroidal network flows. Finally, we compare\nthe proposed algorithms empirically. \n\n"}
{"id": "1402.5074", "contents": "Title: Binary Fused Compressive Sensing: 1-Bit Compressive Sensing meets Group\n  Sparsity Abstract: We propose a new method, {\\it binary fused compressive sensing} (BFCS), to\nrecover sparse piece-wise smooth signals from 1-bit compressive measurements.\nThe proposed algorithm is a modification of the previous {\\it binary iterative\nhard thresholding} (BIHT) algorithm, where, in addition to the sparsity\nconstraint, the total-variation of the recovered signal is upper constrained.\nAs in BIHT, the data term of the objective function is an one-sided $\\ell_1$\n(or $\\ell_2$) norm. Experiments on the recovery of sparse piece-wise smooth\nsignals show that the proposed algorithm is able to take advantage of the\npiece-wise smoothness of the original signal, achieving more accurate recovery\nthan BIHT. \n\n"}
{"id": "1403.6382", "contents": "Title: CNN Features off-the-shelf: an Astounding Baseline for Recognition Abstract: Recent results indicate that the generic descriptors extracted from the\nconvolutional neural networks are very powerful. This paper adds to the\nmounting evidence that this is indeed the case. We report on a series of\nexperiments conducted for different recognition tasks using the publicly\navailable code and model of the \\overfeat network which was trained to perform\nobject classification on ILSVRC13. We use features extracted from the \\overfeat\nnetwork as a generic image representation to tackle the diverse range of\nrecognition tasks of object image classification, scene recognition, fine\ngrained recognition, attribute detection and image retrieval applied to a\ndiverse set of datasets. We selected these tasks and datasets as they gradually\nmove further away from the original task and data the \\overfeat network was\ntrained to solve. Astonishingly, we report consistent superior results compared\nto the highly tuned state-of-the-art systems in all the visual classification\ntasks on various datasets. For instance retrieval it consistently outperforms\nlow memory footprint methods except for sculptures dataset. The results are\nachieved using a linear SVM classifier (or $L2$ distance in case of retrieval)\napplied to a feature representation of size 4096 extracted from a layer in the\nnet. The representations are further modified using simple augmentation\ntechniques e.g. jittering. The results strongly suggest that features obtained\nfrom deep learning with convolutional nets should be the primary candidate in\nmost visual recognition tasks. \n\n"}
{"id": "1404.6039", "contents": "Title: The fshape framework for the variability analysis of functional shapes Abstract: This article introduces a full mathematical and numerical framework for\ntreating functional shapes (or fshapes) following the landmarks of shape spaces\nand shape analysis. Functional shapes can be described as signal functions\nsupported on varying geometrical supports. Analysing variability of fshapes'\nensembles require the modelling and quantification of joint variations in\ngeometry and signal, which have been treated separately in previous approaches.\nInstead, building on the ideas of shape spaces for purely geometrical objects,\nwe propose the extended concept of fshape bundles and define Riemannian metrics\nfor fshape metamorphoses to model geometrico-functional transformations within\nthese bundles. We also generalize previous works on data attachment terms based\non the notion of varifolds and demonstrate the utility of these distances.\nBased on these, we propose variational formulations of the atlas estimation\nproblem on populations of fshapes and prove existence of solutions for the\ndifferent models. The second part of the article examines the numerical\nimplementation of the models by detailing discrete expressions for the metrics\nand gradients and proposing an optimization scheme for the atlas estimation\nproblem. We present a few results of the methodology on a synthetic dataset as\nwell as on a population of retinal membranes with thickness maps. \n\n"}
{"id": "1406.1528", "contents": "Title: Towards building a Crowd-Sourced Sky Map Abstract: We describe a system that builds a high dynamic-range and wide-angle image of\nthe night sky by combining a large set of input images. The method makes use of\npixel-rank information in the individual input images to improve a \"consensus\"\npixel rank in the combined image. Because it only makes use of ranks and the\ncomplexity of the algorithm is linear in the number of images, the method is\nuseful for large sets of uncalibrated images that might have undergone unknown\nnon-linear tone mapping transformations for visualization or aesthetic reasons.\nWe apply the method to images of the night sky (of unknown provenance)\ndiscovered on the Web. The method permits discovery of astronomical objects or\nfeatures that are not visible in any of the input images taken individually.\nMore importantly, however, it permits scientific exploitation of a huge source\nof astronomical images that would not be available to astronomical research\nwithout our automatic system. \n\n"}
{"id": "1409.6041", "contents": "Title: Domain Adaptive Neural Networks for Object Recognition Abstract: We propose a simple neural network model to deal with the domain adaptation\nproblem in object recognition. Our model incorporates the Maximum Mean\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\nreduce the distribution mismatch between the source and target domains in the\nlatent space. From experiments, we demonstrate that the MMD regularization is\nan effective tool to provide good domain adaptation models on both SURF\nfeatures and raw image pixels of a particular image data set. We also show that\nour proposed model, preceded by the denoising auto-encoder pretraining,\nachieves better performance than recent benchmark models on the same data sets.\nThis work represents the first study of MMD measure in the context of neural\nnetworks. \n\n"}
{"id": "1409.7307", "contents": "Title: Image Classification with A Deep Network Model based on Compressive\n  Sensing Abstract: To simplify the parameter of the deep learning network, a cascaded\ncompressive sensing model \"CSNet\" is implemented for image classification.\nFirstly, we use cascaded compressive sensing network to learn feature from the\ndata. Secondly, CSNet generates the feature by binary hashing and block-wise\nhistograms. Finally, a linear SVM classifier is used to classify these\nfeatures. The experiments on the MNIST dataset indicate that higher\nclassification accuracy can be obtained by this algorithm. \n\n"}
{"id": "1409.8403", "contents": "Title: Evaluation of Output Embeddings for Fine-Grained Image Classification Abstract: Image classification has advanced significantly in recent years with the\navailability of large-scale image sets. However, fine-grained classification\nremains a major challenge due to the annotation cost of large numbers of\nfine-grained categories. This project shows that compelling classification\nperformance can be achieved on such categories even without labeled training\ndata. Given image and class embeddings, we learn a compatibility function such\nthat matching embeddings are assigned a higher score than mismatching ones;\nzero-shot classification of an image proceeds by finding the label yielding the\nhighest joint compatibility score. We use state-of-the-art image features and\nfocus on different supervised attributes and unsupervised output embeddings\neither derived from hierarchies or learned from unlabeled text corpora. We\nestablish a substantially improved state-of-the-art on the Animals with\nAttributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate\nthat purely unsupervised output embeddings (learned from Wikipedia and improved\nwith fine-grained text) achieve compelling results, even outperforming the\nprevious supervised state-of-the-art. By combining different output embeddings,\nwe further improve results. \n\n"}
{"id": "1410.5224", "contents": "Title: Supervised mid-level features for word image representation Abstract: This paper addresses the problem of learning word image representations:\ngiven the cropped image of a word, we are interested in finding a descriptive,\nrobust, and compact fixed-length representation. Machine learning techniques\ncan then be supplied with these representations to produce models useful for\nword retrieval or recognition tasks. Although many works have focused on the\nmachine learning aspect once a global representation has been produced, little\nwork has been devoted to the construction of those base image representations:\nmost works use standard coding and aggregation techniques directly on top of\nstandard computer vision features such as SIFT or HOG.\n  We propose to learn local mid-level features suitable for building word image\nrepresentations. These features are learnt by leveraging character bounding box\nannotations on a small set of training images. However, contrary to other\napproaches that use character bounding box information, our approach does not\nrely on detecting the individual characters explicitly at testing time. Our\nlocal mid-level features can then be aggregated to produce a global word image\nsignature. When pairing these features with the recent word attributes\nframework of Almaz\\'an et al., we obtain results comparable with or better than\nthe state-of-the-art on matching and recognition tasks using global descriptors\nof only 96 dimensions. \n\n"}
{"id": "1410.5263", "contents": "Title: Building pattern recognition applications with the SPARE library Abstract: This paper presents the SPARE C++ library, an open source software tool\nconceived to build pattern recognition and soft computing systems. The library\nfollows the requirement of the generality: most of the implemented algorithms\nare able to process user-defined input data types transparently, such as\nlabeled graphs and sequences of objects, as well as standard numeric vectors.\nHere we present a high-level picture of the SPARE library characteristics,\nfocusing instead on the specific practical possibility of constructing pattern\nrecognition systems for different input data types. In particular, as a proof\nof concept, we discuss two application instances involving clustering of\nreal-valued multidimensional sequences and classification of labeled graphs. \n\n"}
{"id": "1411.0022", "contents": "Title: Generalized Adaptive Dictionary Learning via Domain Shift Minimization Abstract: Visual data driven dictionaries have been successfully employed for various\nobject recognition and classification tasks. However, the task becomes more\nchallenging if the training and test data are from contrasting domains. In this\npaper, we propose a novel and generalized approach towards learning an adaptive\nand common dictionary for multiple domains. Precisely, we project the data from\ndifferent domains onto a low dimensional space while preserving the intrinsic\nstructure of data from each domain. We also minimize the domain-shift among the\ndata from each pair of domains. Simultaneously, we learn a common adaptive\ndictionary. Our algorithm can also be modified to learn class-specific\ndictionaries which can be used for classification. We additionally propose a\ndiscriminative manifold regularization which imposes the intrinsic structure of\nclass specific features onto the sparse coefficients. Experiments on image\nclassification show that our approach fares better compared to the existing\nstate-of-the-art methods. \n\n"}
{"id": "1411.3229", "contents": "Title: Multi-modal Image Registration for Correlative Microscopy Abstract: Correlative microscopy is a methodology combining the functionality of light\nmicroscopy with the high resolution of electron microscopy and other microscopy\ntechnologies. Image registration for correlative microscopy is quite\nchallenging because it is a multi-modal, multi-scale and multi-dimensional\nregistration problem. In this report, I introduce two methods of image\nregistration for correlative microscopy. The first method is based on fiducials\n(beads). I generate landmarks from the fiducials and compute the similarity\ntransformation matrix based on three pairs of nearest corresponding landmarks.\nA least-squares matching process is applied afterwards to further refine the\nregistration. The second method is inspired by the image analogies approach. I\nintroduce the sparse representation model into image analogies. I first train\nrepresentative image patches (dictionaries) for pre-registered datasets from\ntwo different modalities, and then I use the sparse coding technique to\ntransfer a given image to a predicted image from one modality to another based\non the learned dictionaries. The final image registration is between the\npredicted image and the original image corresponding to the given image in the\ndifferent modality. The method transforms a multi-modal registration problem to\na mono-modal one. I test my approaches on Transmission Electron Microscopy\n(TEM) and confocal microscopy images. Experimental results of the methods are\nalso shown in this report. \n\n"}
{"id": "1411.4894", "contents": "Title: Low-level Vision by Consensus in a Spatial Hierarchy of Regions Abstract: We introduce a multi-scale framework for low-level vision, where the goal is\nestimating physical scene values from image data---such as depth from stereo\nimage pairs. The framework uses a dense, overlapping set of image regions at\nmultiple scales and a \"local model,\" such as a slanted-plane model for stereo\ndisparity, that is expected to be valid piecewise across the visual field.\nEstimation is cast as optimization over a dichotomous mixture of variables,\nsimultaneously determining which regions are inliers with respect to the local\nmodel (binary variables) and the correct co-ordinates in the local model space\nfor each inlying region (continuous variables). When the regions are organized\ninto a multi-scale hierarchy, optimization can occur in an efficient and\nparallel architecture, where distributed computational units iteratively\nperform calculations and share information through sparse connections between\nparents and children. The framework performs well on a standard benchmark for\nbinocular stereo, and it produces a distributional scene representation that is\nappropriate for combining with higher-level reasoning and other low-level cues. \n\n"}
{"id": "1411.5140", "contents": "Title: Attentional Neural Network: Feature Selection Using Cognitive Feedback Abstract: Attentional Neural Network is a new framework that integrates top-down\ncognitive bias and bottom-up feature extraction in one coherent architecture.\nThe top-down influence is especially effective when dealing with high noise or\ndifficult segmentation problems. Our system is modular and extensible. It is\nalso easy to train and cheap to run, and yet can accommodate complex behaviors.\nWe obtain classification accuracy better than or competitive with state of art\nresults on the MNIST variation dataset, and successfully disentangle overlaid\ndigits with high success rates. We view such a general purpose framework as an\nessential foundation for a larger system emulating the cognitive abilities of\nthe whole brain. \n\n"}
{"id": "1411.5328", "contents": "Title: ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image\n  Collections Abstract: Discovering visual knowledge from weakly labeled data is crucial to scale up\ncomputer vision recognition system, since it is expensive to obtain fully\nlabeled data for a large number of concept categories. In this paper, we\npropose ConceptLearner, which is a scalable approach to discover visual\nconcepts from weakly labeled image collections. Thousands of visual concept\ndetectors are learned automatically, without human in the loop for additional\nannotation. We show that these learned detectors could be applied to recognize\nconcepts at image-level and to detect concepts at image region-level\naccurately. Under domain-specific supervision, we further evaluate the learned\nconcepts for scene recognition on SUN database and for object detection on\nPascal VOC 2007. ConceptLearner shows promising performance compared to fully\nsupervised and weakly supervised methods. \n\n"}
{"id": "1411.7466", "contents": "Title: The Treasure beneath Convolutional Layers: Cross-convolutional-layer\n  Pooling for Image Classification Abstract: A number of recent studies have shown that a Deep Convolutional Neural\nNetwork (DCNN) pretrained on a large dataset can be adopted as a universal\nimage description which leads to astounding performance in many visual\nclassification tasks. Most of these studies, if not all, adopt activations of\nthe fully-connected layer of a DCNN as the image or region representation and\nit is believed that convolutional layer activations are less discriminative.\n  This paper, however, advocates that if used appropriately convolutional layer\nactivations can be turned into a powerful image representation which enjoys\nmany advantages over fully-connected layer activations. This is achieved by\nadopting a new technique proposed in this paper called\ncross-convolutional-layer pooling. More specifically, it extracts subarrays of\nfeature maps of one convolutional layer as local features and pools the\nextracted features with the guidance of feature maps of the successive\nconvolutional layer. Compared with exising methods that apply DCNNs in the\nlocal feature setting, the proposed method is significantly faster since it\nrequires much fewer times of DCNN forward computation. Moreover, it avoids the\ndomain mismatch issue which is usually encountered when applying fully\nconnected layer activations to describe local regions. By applying our method\nto four popular visual classification tasks, it is demonstrated that the\nproposed method can achieve comparable or in some cases significantly better\nperformance than existing fully-connected layer based image representations\nwhile incurring much lower computational cost. \n\n"}
{"id": "1411.7714", "contents": "Title: Features in Concert: Discriminative Feature Selection meets Unsupervised\n  Clustering Abstract: Feature selection is an essential problem in computer vision, important for\ncategory learning and recognition. Along with the rapid development of a wide\nvariety of visual features and classifiers, there is a growing need for\nefficient feature selection and combination methods, to construct powerful\nclassifiers for more complex and higher-level recognition tasks. We propose an\nalgorithm that efficiently discovers sparse, compact representations of input\nfeatures or classifiers, from a vast sea of candidates, with important\noptimality properties, low computational cost and excellent accuracy in\npractice. Different from boosting, we start with a discriminant linear\nclassification formulation that encourages sparse solutions. Then we obtain an\nequivalent unsupervised clustering problem that jointly discovers ensembles of\ndiverse features. They are independently valuable but even more powerful when\nunited in a cluster of classifiers. We evaluate our method on the task of\nlarge-scale recognition in video and show that it significantly outperforms\nclassical selection approaches, such as AdaBoost and greedy forward-backward\nselection, and powerful classifiers such as SVMs, in speed of training and\nperformance, especially in the case of limited training data. \n\n"}
{"id": "1412.3161", "contents": "Title: Object-centric Sampling for Fine-grained Image Classification Abstract: This paper proposes to go beyond the state-of-the-art deep convolutional\nneural network (CNN) by incorporating the information from object detection,\nfocusing on dealing with fine-grained image classification. Unfortunately, CNN\nsuffers from over-fiting when it is trained on existing fine-grained image\nclassification benchmarks, which typically only consist of less than a few tens\nof thousands training images. Therefore, we first construct a large-scale\nfine-grained car recognition dataset that consists of 333 car classes with more\nthan 150 thousand training images. With this large-scale dataset, we are able\nto build a strong baseline for CNN with top-1 classification accuracy of 81.6%.\nOne major challenge in fine-grained image classification is that many classes\nare very similar to each other while having large within-class variation. One\ncontributing factor to the within-class variation is cluttered image\nbackground. However, the existing CNN training takes uniform window sampling\nover the image, acting as blind on the location of the object of interest. In\ncontrast, this paper proposes an \\emph{object-centric sampling} (OCS) scheme\nthat samples image windows based on the object location information. The\nchallenge in using the location information lies in how to design powerful\nobject detector and how to handle the imperfectness of detection results. To\nthat end, we design a saliency-aware object detection approach specific for the\nsetting of fine-grained image classification, and the uncertainty of detection\nresults are naturally handled in our OCS scheme. Our framework is demonstrated\nto be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on the\nlarge-scale fine-grained car classification dataset. \n\n"}
{"id": "1412.4044", "contents": "Title: Adaptive Stochastic Gradient Descent on the Grassmannian for Robust\n  Low-Rank Subspace Recovery and Clustering Abstract: In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradient\nfor $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm to\nrobustly recover the low-rank subspace from a large matrix. In the presence of\ncolumn outliers, we reformulate the batch mode matrix $L_{2,1}$ norm\nminimization with rank constraint problem as a stochastic optimization approach\nconstrained on Grassmann manifold. For each observed data vector, the low-rank\nsubspace $\\mathcal{S}$ is updated by taking a gradient step along the geodesic\nof Grassmannian. In order to accelerate the convergence rate of the stochastic\ngradient method, we choose to adaptively tune the constant step-size by\nleveraging the consecutive gradients. Furthermore, we demonstrate that with\nproper initialization, the K-subspaces extension, K-GASG21, can robustly\ncluster a large number of corrupted data vectors into a union of subspaces.\nNumerical experiments on synthetic and real data demonstrate the efficiency and\naccuracy of the proposed algorithms even with heavy column outliers corruption. \n\n"}
{"id": "1412.4313", "contents": "Title: Combining the Best of Graphical Models and ConvNets for Semantic\n  Segmentation Abstract: We present a two-module approach to semantic segmentation that incorporates\nConvolutional Networks (CNNs) and Graphical Models. Graphical models are used\nto generate a small (5-30) set of diverse segmentations proposals, such that\nthis set has high recall. Since the number of required proposals is so low, we\ncan extract fairly complex features to rank them. Our complex feature of choice\nis a novel CNN called SegNet, which directly outputs a (coarse) semantic\nsegmentation. Importantly, SegNet is specifically trained to optimize the\ncorpus-level PASCAL IOU loss function. To the best of our knowledge, this is\nthe first CNN specifically designed for semantic segmentation. This two-module\napproach achieves $52.5\\%$ on the PASCAL 2012 segmentation challenge. \n\n"}
{"id": "1412.5808", "contents": "Title: Minimizing the Number of Matching Queries for Object Retrieval Abstract: To increase the computational efficiency of interest-point based object\nretrieval, researchers have put remarkable research efforts into improving the\nefficiency of kNN-based feature matching, pursuing to match thousands of\nfeatures against a database within fractions of a second. However, due to the\nhigh-dimensional nature of image features that reduces the effectivity of index\nstructures (curse of dimensionality), due to the vast amount of features stored\nin image databases (images are often represented by up to several thousand\nfeatures), this ultimate goal demanded to trade query runtimes for query\nprecision. In this paper we address an approach complementary to indexing in\norder to improve the runtimes of retrieval by querying only the most promising\nkeypoint descriptors, as this affects matching runtimes linearly and can\ntherefore lead to increased efficiency. As this reduction of kNN queries\nreduces the number of tentative correspondences, a loss of query precision is\nminimized by an additional image-level correspondence generation stage with a\ncomputational performance independent of the underlying indexing structure. We\nevaluate such an adaption of the standard recognition pipeline on a variety of\ndatasets using both SIFT and state-of-the-art binary descriptors. Our results\nsuggest that decreasing the number of queried descriptors does not necessarily\nimply a reduction in the result quality as long as alternative ways of\nincreasing query recall (by thoroughly selecting k) and MAP (using image-level\ncorrespondence generation) are considered. \n\n"}
{"id": "1412.7856", "contents": "Title: Gabor wavelets combined with volumetric fractal dimension applied to\n  texture analysis Abstract: Texture analysis and classification remain as one of the biggest challenges\nfor the field of computer vision and pattern recognition. On this matter, Gabor\nwavelets has proven to be a useful technique to characterize distinctive\ntexture patterns. However, most of the approaches used to extract descriptors\nof the Gabor magnitude space usually fail in representing adequately the\nrichness of detail present into a unique feature vector. In this paper, we\npropose a new method to enhance the Gabor wavelets process extracting a fractal\nsignature of the magnitude spaces. Each signature is reduced using a canonical\nanalysis function and concatenated to form the final feature vector.\nExperiments were conducted on several texture image databases to prove the\npower and effectiveness of the proposed method. Results obtained shown that\nthis method outperforms other early proposed method, creating a more reliable\ntechnique for texture feature extraction. \n\n"}
{"id": "1412.8556", "contents": "Title: Domain-Size Pooling in Local Descriptors: DSP-SIFT Abstract: We introduce a simple modification of local image descriptors, such as SIFT,\nbased on pooling gradient orientations across different domain sizes, in\naddition to spatial locations. The resulting descriptor, which we call\nDSP-SIFT, outperforms other methods in wide-baseline matching benchmarks,\nincluding those based on convolutional neural networks, despite having the same\ndimension of SIFT and requiring no training. \n\n"}
{"id": "1501.04276", "contents": "Title: Correlation Adaptive Subspace Segmentation by Trace Lasso Abstract: This paper studies the subspace segmentation problem. Given a set of data\npoints drawn from a union of subspaces, the goal is to partition them into\ntheir underlying subspaces they were drawn from. The spectral clustering method\nis used as the framework. It requires to find an affinity matrix which is close\nto block diagonal, with nonzero entries corresponding to the data point pairs\nfrom the same subspace. In this work, we argue that both sparsity and the\ngrouping effect are important for subspace segmentation. A sparse affinity\nmatrix tends to be block diagonal, with less connections between data points\nfrom different subspaces. The grouping effect ensures that the highly corrected\ndata which are usually from the same subspace can be grouped together. Sparse\nSubspace Clustering (SSC), by using $\\ell^1$-minimization, encourages sparsity\nfor data selection, but it lacks of the grouping effect. On the contrary,\nLow-Rank Representation (LRR), by rank minimization, and Least Squares\nRegression (LSR), by $\\ell^2$-regularization, exhibit strong grouping effect,\nbut they are short in subset selection. Thus the obtained affinity matrix is\nusually very sparse by SSC, yet very dense by LRR and LSR.\n  In this work, we propose the Correlation Adaptive Subspace Segmentation\n(CASS) method by using trace Lasso. CASS is a data correlation dependent method\nwhich simultaneously performs automatic data selection and groups correlated\ndata together. It can be regarded as a method which adaptively balances SSC and\nLSR. Both theoretical and experimental results show the effectiveness of CASS. \n\n"}
{"id": "1501.07492", "contents": "Title: Weakly Supervised Learning for Salient Object Detection Abstract: Recent advances in supervised salient object detection has resulted in\nsignificant performance on benchmark datasets. Training such models, however,\nrequires expensive pixel-wise annotations of salient objects. Moreover, many\nexisting salient object detection models assume that at least one salient\nobject exists in the input image. Such an assumption often leads to less\nappealing saliency maps on the background images, which contain no salient\nobject at all. To avoid the requirement of expensive pixel-wise salient region\nannotations, in this paper, we study weakly supervised learning approaches for\nsalient object detection. Given a set of background images and salient object\nimages, we propose a solution toward jointly addressing the salient object\nexistence and detection tasks. We adopt the latent SVM framework and formulate\nthe two problems together in a single integrated objective function: saliency\nlabels of superpixels are modeled as hidden variables and involved in a\nclassification term conditioned to the salient object existence variable, which\nin turn depends on both global image and regional saliency features and\nsaliency label assignment. Experimental results on benchmark datasets validate\nthe effectiveness of our proposed approach. \n\n"}
{"id": "1502.01526", "contents": "Title: Object Proposal with Kernelized Partial Ranking Abstract: Object proposals are an ensemble of bounding boxes with high potential to\ncontain objects. In order to determine a small set of proposals with a high\nrecall, a common scheme is extracting multiple features followed by a ranking\nalgorithm which however, incurs two major challenges: {\\bf 1)} The ranking\nmodel often imposes pairwise constraints between each proposal, rendering the\nproblem away from an efficient training/testing phase; {\\bf 2)} Linear kernels\nare utilized due to the computational and memory bottleneck of training a\nkernelized model.\n  In this paper, we remedy these two issues by suggesting a {\\em kernelized\npartial ranking model}. In particular, we demonstrate that {\\bf i)} our partial\nranking model reduces the number of constraints from $O(n^2)$ to $O(nk)$ where\n$n$ is the number of all potential proposals for an image but we are only\ninterested in top-$k$ of them that has the largest overlap with the ground\ntruth; {\\bf ii)} we permit non-linear kernels in our model which is often\nsuperior to the linear classifier in terms of accuracy. For the sake of\nmitigating the computational and memory issues, we introduce a consistent\nweighted sampling~(CWS) paradigm that approximates the non-linear kernel as\nwell as facilitates an efficient learning. In fact, as we will show, training a\nlinear CWS model amounts to learning a kernelized model. Extensive experiments\ndemonstrate that equipped with the non-linear kernel and the partial ranking\nalgorithm, recall at top-$k$ proposals can be substantially improved. \n\n"}
{"id": "1502.02077", "contents": "Title: Quantum Energy Regression using Scattering Transforms Abstract: We present a novel approach to the regression of quantum mechanical energies\nbased on a scattering transform of an intermediate electron density\nrepresentation. A scattering transform is a deep convolution network computed\nwith a cascade of multiscale wavelet transforms. It possesses appropriate\ninvariant and stability properties for quantum energy regression. This new\nframework removes fundamental limitations of Coulomb matrix based energy\nregressions, and numerical experiments give state-of-the-art accuracy over\nplanar molecules. \n\n"}
{"id": "1502.04275", "contents": "Title: segDeepM: Exploiting Segmentation and Context in Deep Neural Networks\n  for Object Detection Abstract: In this paper, we propose an approach that exploits object segmentation in\norder to improve the accuracy of object detection. We frame the problem as\ninference in a Markov Random Field, in which each detection hypothesis scores\nobject appearance as well as contextual information using Convolutional Neural\nNetworks, and allows the hypothesis to choose and score a segment out of a\nlarge pool of accurate object segmentation proposals. This enables the detector\nto incorporate additional evidence when it is available and thus results in\nmore accurate detections. Our experiments show an improvement of 4.1% in mAP\nover the R-CNN baseline on PASCAL VOC 2010, and 3.4% over the current\nstate-of-the-art, demonstrating the power of our approach. \n\n"}
{"id": "1502.04824", "contents": "Title: Randomized LU decomposition: An Algorithm for Dictionaries Construction Abstract: In recent years, distinctive-dictionary construction has gained importance\ndue to his usefulness in data processing. Usually, one or more dictionaries are\nconstructed from a training data and then they are used to classify signals\nthat did not participate in the training process. A new dictionary construction\nalgorithm is introduced. It is based on a low-rank matrix factorization being\nachieved by the application of the randomized LU decomposition to a training\ndata. This method is fast, scalable, parallelizable, consumes low memory,\noutperforms SVD in these categories and works also extremely well on large\nsparse matrices. In contrast to existing methods, the randomized LU\ndecomposition constructs an under-complete dictionary, which simplifies both\nthe construction and the classification processes of newly arrived signals. The\ndictionary construction is generic and general that fits different\napplications. We demonstrate the capabilities of this algorithm for file type\nidentification, which is a fundamental task in digital security arena,\nperformed nowadays for example by sandboxing mechanism, deep packet inspection,\nfirewalls and anti-virus systems. We propose a content-based method that\ndetects file types that neither depend on file extension nor on metadata. Such\napproach is harder to deceive and we show that only a few file fragments from a\nwhole file are needed for a successful classification. Based on the constructed\ndictionaries, we show that the proposed method can effectively identify\nexecution code fragments in PDF files.\n  $\\textbf{Keywords.}$ Dictionary construction, classification, LU\ndecomposition, randomized LU decomposition, content-based file detection,\ncomputer security. \n\n"}
{"id": "1502.05243", "contents": "Title: SA-CNN: Dynamic Scene Classification using Convolutional Neural Networks Abstract: The task of classifying videos of natural dynamic scenes into appropriate\nclasses has gained lot of attention in recent years. The problem especially\nbecomes challenging when the camera used to capture the video is dynamic. In\nthis paper, we analyse the performance of statistical aggregation (SA)\ntechniques on various pre-trained convolutional neural network(CNN) models to\naddress this problem. The proposed approach works by extracting CNN activation\nfeatures for a number of frames in a video and then uses an aggregation scheme\nin order to obtain a robust feature descriptor for the video. We show through\nresults that the proposed approach performs better than the-state-of-the arts\nfor the Maryland and YUPenn dataset. The final descriptor obtained is powerful\nenough to distinguish among dynamic scenes and is even capable of addressing\nthe scenario where the camera motion is dominant and the scene dynamics are\ncomplex. Further, this paper shows an extensive study on the performance of\nvarious aggregation methods and their combinations. We compare the proposed\napproach with other dynamic scene classification algorithms on two publicly\navailable datasets - Maryland and YUPenn to demonstrate the superior\nperformance of the proposed approach. \n\n"}
{"id": "1502.07802", "contents": "Title: Modelling Local Deep Convolutional Neural Network Features to Improve\n  Fine-Grained Image Classification Abstract: We propose a local modelling approach using deep convolutional neural\nnetworks (CNNs) for fine-grained image classification. Recently, deep CNNs\ntrained from large datasets have considerably improved the performance of\nobject recognition. However, to date there has been limited work using these\ndeep CNNs as local feature extractors. This partly stems from CNNs having\ninternal representations which are high dimensional, thereby making such\nrepresentations difficult to model using stochastic models. To overcome this\nissue, we propose to reduce the dimensionality of one of the internal fully\nconnected layers, in conjunction with layer-restricted retraining to avoid\nretraining the entire network. The distribution of low-dimensional features\nobtained from the modified layer is then modelled using a Gaussian mixture\nmodel. Comparative experiments show that considerable performance improvements\ncan be achieved on the challenging Fish and UEC FOOD-100 datasets. \n\n"}
{"id": "1503.00783", "contents": "Title: Joint calibration of Ensemble of Exemplar SVMs Abstract: We present a method for calibrating the Ensemble of Exemplar SVMs model.\nUnlike the standard approach, which calibrates each SVM independently, our\nmethod optimizes their joint performance as an ensemble. We formulate joint\ncalibration as a constrained optimization problem and devise an efficient\noptimization algorithm to find its global optimum. The algorithm dynamically\ndiscards parts of the solution space that cannot contain the optimum early on,\nmaking the optimization computationally feasible. We experiment with EE-SVM\ntrained on state-of-the-art CNN descriptors. Results on the ILSVRC 2014 and\nPASCAL VOC 2007 datasets show that (i) our joint calibration procedure\noutperforms independent calibration on the task of classifying windows as\nbelonging to an object class or not; and (ii) this improved window classifier\nleads to better performance on the object detection task. \n\n"}
{"id": "1503.00949", "contents": "Title: Weakly Supervised Object Localization with Multi-fold Multiple Instance\n  Learning Abstract: Object category localization is a challenging problem in computer vision.\nStandard supervised training requires bounding box annotations of object\ninstances. This time-consuming annotation process is sidestepped in weakly\nsupervised learning. In this case, the supervised information is restricted to\nbinary labels that indicate the absence/presence of object instances in the\nimage, without their locations. We follow a multiple-instance learning approach\nthat iteratively trains the detector and infers the object locations in the\npositive training images. Our main contribution is a multi-fold multiple\ninstance learning procedure, which prevents training from prematurely locking\nonto erroneous object locations. This procedure is particularly important when\nusing high-dimensional representations, such as Fisher vectors and\nconvolutional neural network features. We also propose a window refinement\nmethod, which improves the localization accuracy by incorporating an objectness\nprior. We present a detailed experimental evaluation using the PASCAL VOC 2007\ndataset, which verifies the effectiveness of our approach. \n\n"}
{"id": "1503.01313", "contents": "Title: A Novel Performance Evaluation Methodology for Single-Target Trackers Abstract: This paper addresses the problem of single-target tracker performance\nevaluation. We consider the performance measures, the dataset and the\nevaluation system to be the most important components of tracker evaluation and\npropose requirements for each of them. The requirements are the basis of a new\nevaluation methodology that aims at a simple and easily interpretable tracker\ncomparison. The ranking-based methodology addresses tracker equivalence in\nterms of statistical significance and practical differences. A fully-annotated\ndataset with per-frame annotations with several visual attributes is\nintroduced. The diversity of its visual properties is maximized in a novel way\nby clustering a large number of videos according to their visual attributes.\nThis makes it the most sophistically constructed and annotated dataset to date.\nA multi-platform evaluation system allowing easy integration of third-party\ntrackers is presented as well. The proposed evaluation methodology was tested\non the VOT2014 challenge on the new dataset and 38 trackers, making it the\nlargest benchmark to date. Most of the tested trackers are indeed\nstate-of-the-art since they outperform the standard baselines, resulting in a\nhighly-challenging benchmark. An exhaustive analysis of the dataset from the\nperspective of tracking difficulty is carried out. To facilitate tracker\ncomparison a new performance visualization technique is proposed. \n\n"}
{"id": "1503.01563", "contents": "Title: Convex Optimization for Parallel Energy Minimization Abstract: Energy minimization has been an intensely studied core problem in computer\nvision. With growing image sizes (2D and 3D), it is now highly desirable to run\nenergy minimization algorithms in parallel. But many existing algorithms, in\nparticular, some efficient combinatorial algorithms, are difficult to\npar-allelize. By exploiting results from convex and submodular theory, we\nreformulate the quadratic energy minimization problem as a total variation\ndenoising problem, which, when viewed geometrically, enables the use of\nprojection and reflection based convex methods. The resulting min-cut algorithm\n(and code) is conceptually very simple, and solves a sequence of TV denoising\nproblems. We perform an extensive empirical evaluation comparing\nstate-of-the-art combinatorial algorithms and convex optimization techniques.\nOn small problems the iterative convex methods match the combinatorial max-flow\nalgorithms, while on larger problems they offer other flexibility and important\ngains: (a) their memory footprint is small; (b) their straightforward\nparallelizability fits multi-core platforms; (c) they can easily be\nwarm-started; and (d) they quickly reach approximately good solutions, thereby\nenabling faster \"inexact\" solutions. A key consequence of our approach based on\nsubmodularity and convexity is that it is allows to combine any arbitrary\ncombinatorial or convex methods as subroutines, which allows one to obtain\nhybrid combinatorial and convex optimization algorithms that benefit from the\nstrengths of both. \n\n"}
{"id": "1503.01868", "contents": "Title: Total Variation Regularized Tensor RPCA for Background Subtraction from\n  Compressive Measurements Abstract: Background subtraction has been a fundamental and widely studied task in\nvideo analysis, with a wide range of applications in video surveillance,\nteleconferencing and 3D modeling. Recently, motivated by compressive imaging,\nbackground subtraction from compressive measurements (BSCM) is becoming an\nactive research task in video surveillance. In this paper, we propose a novel\ntensor-based robust PCA (TenRPCA) approach for BSCM by decomposing video frames\ninto backgrounds with spatial-temporal correlations and foregrounds with\nspatio-temporal continuity in a tensor framework. In this approach, we use 3D\ntotal variation (TV) to enhance the spatio-temporal continuity of foregrounds,\nand Tucker decomposition to model the spatio-temporal correlations of video\nbackground. Based on this idea, we design a basic tensor RPCA model over the\nvideo frames, dubbed as the holistic TenRPCA model (H-TenRPCA). To characterize\nthe correlations among the groups of similar 3D patches of video background, we\nfurther design a patch-group-based tensor RPCA model (PG-TenRPCA) by joint\ntensor Tucker decompositions of 3D patch groups for modeling the video\nbackground. Efficient algorithms using alternating direction method of\nmultipliers (ADMM) are developed to solve the proposed models. Extensive\nexperiments on simulated and real-world videos demonstrate the superiority of\nthe proposed approaches over the existing state-of-the-art approaches. \n\n"}
{"id": "1503.05038", "contents": "Title: 3D Object Class Detection in the Wild Abstract: Object class detection has been a synonym for 2D bounding box localization\nfor the longest time, fueled by the success of powerful statistical learning\ntechniques, combined with robust image representations. Only recently, there\nhas been a growing interest in revisiting the promise of computer vision from\nthe early days: to precisely delineate the contents of a visual scene, object\nby object, in 3D. In this paper, we draw from recent advances in object\ndetection and 2D-3D object lifting in order to design an object class detector\nthat is particularly tailored towards 3D object class detection. Our 3D object\nclass detection method consists of several stages gradually enriching the\nobject detection output with object viewpoint, keypoints and 3D shape\nestimates. Following careful design, in each stage it constantly improves the\nperformance and achieves state-ofthe-art performance in simultaneous 2D\nbounding box and viewpoint estimation on the challenging Pascal3D+ dataset. \n\n"}
{"id": "1503.06699", "contents": "Title: Video-Based Action Recognition Using Rate-Invariant Analysis of\n  Covariance Trajectories Abstract: Statistical classification of actions in videos is mostly performed by\nextracting relevant features, particularly covariance features, from image\nframes and studying time series associated with temporal evolutions of these\nfeatures. A natural mathematical representation of activity videos is in form\nof parameterized trajectories on the covariance manifold, i.e. the set of\nsymmetric, positive-definite matrices (SPDMs). The variable execution-rates of\nactions implies variable parameterizations of the resulting trajectories, and\ncomplicates their classification. Since action classes are invariant to\nexecution rates, one requires rate-invariant metrics for comparing\ntrajectories. A recent paper represented trajectories using their transported\nsquare-root vector fields (TSRVFs), defined by parallel translating\nscaled-velocity vectors of trajectories to a reference tangent space on the\nmanifold. To avoid arbitrariness of selecting the reference and to reduce\ndistortion introduced during this mapping, we develop a purely intrinsic\napproach where SPDM trajectories are represented by redefining their TSRVFs at\nthe starting points of the trajectories, and analyzed as elements of a vector\nbundle on the manifold. Using a natural Riemannain metric on vector bundles of\nSPDMs, we compute geodesic paths and geodesic distances between trajectories in\nthe quotient space of this vector bundle, with respect to the\nre-parameterization group. This makes the resulting comparison of trajectories\ninvariant to their re-parameterization. We demonstrate this framework on two\napplications involving video classification: visual speech recognition or\nlip-reading and hand-gesture recognition. In both cases we achieve results\neither comparable to or better than the current literature. \n\n"}
{"id": "1504.04660", "contents": "Title: A spectral optical flow method for determining velocities from digital\n  imagery Abstract: We present a method for determining surface flows from solar images based\nupon optical flow techniques. We apply the method to sets of images obtained by\na variety of solar imagers to assess its performance. The {\\tt opflow3d}\nprocedure is shown to extract accurate velocity estimates when provided perfect\ntest data and quickly generates results consistent with completely distinct\nmethods when applied on global scales. We also validate it in detail by\ncomparing it to an established method when applied to high-resolution datasets\nand find that it provides comparable results without the need to tune, filter\nor otherwise preprocess the images before its application. \n\n"}
{"id": "1504.04943", "contents": "Title: Weakly Supervised Fine-Grained Image Categorization Abstract: In this paper, we categorize fine-grained images without using any object /\npart annotation neither in the training nor in the testing stage, a step\ntowards making it suitable for deployments. Fine-grained image categorization\naims to classify objects with subtle distinctions. Most existing works heavily\nrely on object / part detectors to build the correspondence between object\nparts by using object or object part annotations inside training images. The\nneed for expensive object annotations prevents the wide usage of these methods.\nInstead, we propose to select useful parts from multi-scale part proposals in\nobjects, and use them to compute a global image representation for\ncategorization. This is specially designed for the annotation-free fine-grained\ncategorization task, because useful parts have shown to play an important role\nin existing annotation-dependent works but accurate part detectors can be\nhardly acquired. With the proposed image representation, we can further detect\nand visualize the key (most discriminative) parts in objects of different\nclasses. In the experiment, the proposed annotation-free method achieves better\naccuracy than that of state-of-the-art annotation-free and most existing\nannotation-dependent methods on two challenging datasets, which shows that it\nis not always necessary to use accurate object / part annotations in\nfine-grained image categorization. \n\n"}
{"id": "1504.06603", "contents": "Title: WxBS: Wide Baseline Stereo Generalizations Abstract: We have presented a new problem -- the wide multiple baseline stereo (WxBS)\n-- which considers matching of images that simultaneously differ in more than\none image acquisition factor such as viewpoint, illumination, sensor type or\nwhere object appearance changes significantly, e.g. over time. A new dataset\nwith the ground truth for evaluation of matching algorithms has been introduced\nand will be made public.\n  We have extensively tested a large set of popular and recent detectors and\ndescriptors and show than the combination of RootSIFT and HalfRootSIFT as\ndescriptors with MSER and Hessian-Affine detectors works best for many\ndifferent nuisance factors. We show that simple adaptive thresholding improves\nHessian-Affine, DoG, MSER (and possibly other) detectors and allows to use them\non infrared and low contrast images.\n  A novel matching algorithm for addressing the WxBS problem has been\nintroduced. We have shown experimentally that the WxBS-M matcher dominantes the\nstate-of-the-art methods both on both the new and existing datasets. \n\n"}
{"id": "1504.06692", "contents": "Title: Learning like a Child: Fast Novel Visual Concept Learning from Sentence\n  Descriptions of Images Abstract: In this paper, we address the task of learning novel visual concepts, and\ntheir interactions with other concepts, from a few images with sentence\ndescriptions. Using linguistic context and visual features, our method is able\nto efficiently hypothesize the semantic meaning of new words and add them to\nits word dictionary so that they can be used to describe images which contain\nthese novel concepts. Our method has an image captioning module based on m-RNN\nwith several improvements. In particular, we propose a transposed weight\nsharing scheme, which not only improves performance on image captioning, but\nalso makes the model more suitable for the novel concept learning task. We\npropose methods to prevent overfitting the new concepts. In addition, three\nnovel concept datasets are constructed for this new task. In the experiments,\nwe show that our method effectively learns novel visual concepts from a few\nexamples without disturbing the previously learned concepts. The project page\nis http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html \n\n"}
{"id": "1504.07339", "contents": "Title: Convolutional Channel Features Abstract: Deep learning methods are powerful tools but often suffer from expensive\ncomputation and limited flexibility. An alternative is to combine light-weight\nmodels with deep representations. As successful cases exist in several visual\nproblems, a unified framework is absent. In this paper, we revisit two widely\nused approaches in computer vision, namely filtered channel features and\nConvolutional Neural Networks (CNN), and absorb merits from both by proposing\nan integrated method called Convolutional Channel Features (CCF). CCF transfers\nlow-level features from pre-trained CNN models to feed the boosting forest\nmodel. With the combination of CNN features and boosting forest, CCF benefits\nfrom the richer capacity in feature representation compared with channel\nfeatures, as well as lower cost in computation and storage compared with\nend-to-end CNN methods. We show that CCF serves as a good way of tailoring\npre-trained CNN models to diverse tasks without fine-tuning the whole network\nto each task by achieving state-of-the-art performances in pedestrian\ndetection, face detection, edge detection and object proposal generation. \n\n"}
{"id": "1504.07460", "contents": "Title: Identifying Reliable Annotations for Large Scale Image Segmentation Abstract: Challenging computer vision tasks, in particular semantic image segmentation,\nrequire large training sets of annotated images. While obtaining the actual\nimages is often unproblematic, creating the necessary annotation is a tedious\nand costly process. Therefore, one often has to work with unreliable annotation\nsources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic\ntechniques. In this work, we present a Gaussian process (GP) based technique\nfor simultaneously identifying which images of a training set have unreliable\nannotation and learning a segmentation model in which the negative effect of\nthese images is suppressed. Alternatively, the model can also just be used to\nidentify the most reliably annotated images from the training set, which can\nthen be used for training any other segmentation method. By relying on \"deep\nfeatures\" in combination with a linear covariance function, our GP can be\nlearned and its hyperparameter determined efficiently using only matrix\noperations and gradient-based optimization. This makes our method scalable even\nto large datasets with several million training instances. \n\n"}
{"id": "1504.07575", "contents": "Title: Becoming the Expert - Interactive Multi-Class Machine Teaching Abstract: Compared to machines, humans are extremely good at classifying images into\ncategories, especially when they possess prior knowledge of the categories at\nhand. If this prior information is not available, supervision in the form of\nteaching images is required. To learn categories more quickly, people should\nsee important and representative images first, followed by less important\nimages later - or not at all. However, image-importance is individual-specific,\ni.e. a teaching image is important to a student if it changes their overall\nability to discriminate between classes. Further, students keep learning, so\nwhile image-importance depends on their current knowledge, it also varies with\ntime.\n  In this work we propose an Interactive Machine Teaching algorithm that\nenables a computer to teach challenging visual concepts to a human. Our\nadaptive algorithm chooses, online, which labeled images from a teaching set\nshould be shown to the student as they learn. We show that a teaching strategy\nthat probabilistically models the student's ability and progress, based on\ntheir correct and incorrect answers, produces better 'experts'. We present\nresults using real human participants across several varied and challenging\nreal-world datasets. \n\n"}
{"id": "1505.00424", "contents": "Title: Electron Neutrino Classification in Liquid Argon Time Projection Chamber\n  Detector Abstract: Neutrinos are one of the least known elementary particles. The detection of\nneutrinos is an extremely difficult task since they are affected only by weak\nsub-atomic force or gravity. Therefore large detectors are constructed to\nreveal neutrino's properties. Among them the Liquid Argon Time Projection\nChamber (LAr-TPC) detectors provide excellent imaging and particle\nidentification ability for studying neutrinos. The computerized methods for\nautomatic reconstruction and identification of particles are needed to fully\nexploit the potential of the LAr-TPC technique. Herein, the novel method for\nelectron neutrino classification is presented. The method constructs a feature\ndescriptor from images of observed event. It characterizes the signal\ndistribution propagated from vertex of interest, where the particle interacts\nwith the detector medium. The classifier is learned with a constructed feature\ndescriptor to decide whether the images represent the electron neutrino or\ncascade produced by photons. The proposed approach assumes that the position of\nprimary interaction vertex is known. The method's performance in dependency to\nthe noise in a primary vertex position and deposited energy of particles is\nstudied. \n\n"}
{"id": "1505.00855", "contents": "Title: Large-scale Classification of Fine-Art Paintings: Learning The Right\n  Metric on The Right Feature Abstract: In the past few years, the number of fine-art collections that are digitized\nand publicly available has been growing rapidly. With the availability of such\nlarge collections of digitized artworks comes the need to develop multimedia\nsystems to archive and retrieve this pool of data. Measuring the visual\nsimilarity between artistic items is an essential step for such multimedia\nsystems, which can benefit more high-level multimedia tasks. In order to model\nthis similarity between paintings, we should extract the appropriate visual\nfeatures for paintings and find out the best approach to learn the similarity\nmetric based on these features. We investigate a comprehensive list of visual\nfeatures and metric learning approaches to learn an optimized similarity\nmeasure between paintings. We develop a machine that is able to make\naesthetic-related semantic-level judgments, such as predicting a painting's\nstyle, genre, and artist, as well as providing similarity measures optimized\nbased on the knowledge available in the domain of art historical\ninterpretation. Our experiments show the value of using this similarity measure\nfor the aforementioned prediction tasks. \n\n"}
{"id": "1505.01749", "contents": "Title: Object detection via a multi-region & semantic segmentation-aware CNN\n  model Abstract: We propose an object detection system that relies on a multi-region deep\nconvolutional neural network (CNN) that also encodes semantic\nsegmentation-aware features. The resulting CNN-based representation aims at\ncapturing a diverse set of discriminative appearance factors and exhibits\nlocalization sensitivity that is essential for accurate object localization. We\nexploit the above properties of our recognition module by integrating it on an\niterative localization mechanism that alternates between scoring a box proposal\nand refining its location with a deep CNN regression model. Thanks to the\nefficient use of our modules, we detect objects with very high localization\naccuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we\nachieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published\nwork by a significant margin. \n\n"}
{"id": "1505.04424", "contents": "Title: Improved Microaneurysm Detection using Deep Neural Networks Abstract: In this work, we propose a novel microaneurysm (MA) detection for early\ndiabetic retinopathy screening using color fundus images. Since MA usually the\nfirst lesions to appear as an indicator of diabetic retinopathy, accurate\ndetection of MA is necessary for treatment. Each pixel of the image is\nclassified as either MA or non-MA using a deep neural network with dropout\ntraining procedure using maxout activation function. No preprocessing step or\nmanual feature extraction is required. Substantial improvements over standard\nMA detection method based on the pipeline of preprocessing, feature extraction,\nclassification followed by post processing is achieved. The presented method is\nevaluated in publicly available Retinopathy Online Challenge (ROC) and\nDiaretdb1v2 database and achieved state-of-the-art accuracy. \n\n"}
{"id": "1505.05190", "contents": "Title: Image Reconstruction from Bag-of-Visual-Words Abstract: The objective of this work is to reconstruct an original image from\nBag-of-Visual-Words (BoVW). Image reconstruction from features can be a means\nof identifying the characteristics of features. Additionally, it enables us to\ngenerate novel images via features. Although BoVW is the de facto standard\nfeature for image recognition and retrieval, successful image reconstruction\nfrom BoVW has not been reported yet. What complicates this task is that BoVW\nlacks the spatial information for including visual words. As described in this\npaper, to estimate an original arrangement, we propose an evaluation function\nthat incorporates the naturalness of local adjacency and the global position,\nwith a method to obtain related parameters using an external image database. To\nevaluate the performance of our method, we reconstruct images of objects of 101\nkinds. Additionally, we apply our method to analyze object classifiers and to\ngenerate novel images via BoVW. \n\n"}
{"id": "1505.05489", "contents": "Title: A Sparse Gaussian Process Framework for Photometric Redshift Estimation Abstract: Accurate photometric redshifts are a lynchpin for many future experiments to\npin down the cosmological model and for studies of galaxy evolution. In this\nstudy, a novel sparse regression framework for photometric redshift estimation\nis presented. Simulated and real data from SDSS DR12 were used to train and\ntest the proposed models. We show that approaches which include careful data\npreparation and model design offer a significant improvement in comparison with\nseveral competing machine learning algorithms. Standard implementations of most\nregression algorithms have as the objective the minimization of the sum of\nsquared errors. For redshift inference, however, this induces a bias in the\nposterior mean of the output distribution, which can be problematic. In this\npaper we directly target minimizing $\\Delta z = (z_\\textrm{s} -\nz_\\textrm{p})/(1+z_\\textrm{s})$ and address the bias problem via a\ndistribution-based weighting scheme, incorporated as part of the optimization\nobjective. The results are compared with other machine learning algorithms in\nthe field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs)\nand sparse GPs. The proposed framework reaches a mean absolute $\\Delta z =\n0.0026(1+z_\\textrm{s})$, over the redshift range of $0 \\le z_\\textrm{s} \\le 2$\non the simulated data, and $\\Delta z = 0.0178(1+z_\\textrm{s})$ over the entire\nredshift range on the SDSS DR12 survey, outperforming the standard ANNz used in\nthe literature. We also investigate how the relative size of the training set\naffects the photometric redshift accuracy. We find that a training set of\n\\textgreater 30 per cent of total sample size, provides little additional\nconstraint on the photometric redshifts, and note that our GP formalism\nstrongly outperforms ANNz in the sparse data regime for the simulated data set. \n\n"}
{"id": "1505.06079", "contents": "Title: Robust Rotation Synchronization via Low-rank and Sparse Matrix\n  Decomposition Abstract: This paper deals with the rotation synchronization problem, which arises in\nglobal registration of 3D point-sets and in structure from motion. The problem\nis formulated in an unprecedented way as a \"low-rank and sparse\" matrix\ndecomposition that handles both outliers and missing data. A minimization\nstrategy, dubbed R-GoDec, is also proposed and evaluated experimentally against\nstate-of-the-art algorithms on simulated and real data. The results show that\nR-GoDec is the fastest among the robust algorithms. \n\n"}
{"id": "1506.00196", "contents": "Title: Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme\n  Conversion Abstract: Sequence-to-sequence translation methods based on generation with a\nside-conditioned language model have recently shown promising results in\nseveral tasks. In machine translation, models conditioned on source side words\nhave been used to produce target-language text, and in image captioning, models\nconditioned images have been used to generate caption text. Past work with this\napproach has focused on large vocabulary tasks, and measured quality in terms\nof BLEU. In this paper, we explore the applicability of such models to the\nqualitatively different grapheme-to-phoneme task. Here, the input and output\nside vocabularies are small, plain n-gram models do well, and credit is only\ngiven when the output is exactly correct. We find that the simple\nside-conditioned generation approach is able to rival the state-of-the-art, and\nwe are able to significantly advance the stat-of-the-art with bi-directional\nlong short-term memory (LSTM) neural networks that use the same alignment\ninformation that is used in conventional approaches. \n\n"}
{"id": "1506.03500", "contents": "Title: Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image\n  Generation Abstract: We introduce language-driven image generation, the task of generating an\nimage visualizing the semantic contents of a word embedding, e.g., given the\nword embedding of grasshopper, we generate a natural image of a grasshopper. We\nimplement a simple method based on two mapping functions. The first takes as\ninput a word embedding (as produced, e.g., by the word2vec toolkit) and maps it\nonto a high-level visual space (e.g., the space defined by one of the top\nlayers of a Convolutional Neural Network). The second function maps this\nabstract visual representation to pixel space, in order to generate the target\nimage. Several user studies suggest that the current system produces images\nthat capture general visual properties of the concepts encoded in the word\nembedding, such as color or typical environment, and are sufficient to\ndiscriminate between general categories of objects. \n\n"}
{"id": "1506.04757", "contents": "Title: Image-based Recommendations on Styles and Substitutes Abstract: Humans inevitably develop a sense of the relationships between objects, some\nof which are based on their appearance. Some pairs of objects might be seen as\nbeing alternatives to each other (such as two pairs of jeans), while others may\nbe seen as being complementary (such as a pair of jeans and a matching shirt).\nThis information guides many of the choices that people make, from buying\nclothes to their interactions with each other. We seek here to model this human\nsense of the relationships between objects based on their appearance. Our\napproach is not based on fine-grained modeling of user annotations but rather\non capturing the largest dataset possible and developing a scalable method for\nuncovering human notions of the visual relationships within. We cast this as a\nnetwork inference problem defined on graphs of related images, and provide a\nlarge-scale dataset for the training and evaluation of the same. The system we\ndevelop is capable of recommending which clothes and accessories will go well\ntogether (and which will not), amongst a host of other applications. \n\n"}
{"id": "1506.04954", "contents": "Title: A Tensor-Based Dictionary Learning Approach to Tomographic Image\n  Reconstruction Abstract: We consider tomographic reconstruction using priors in the form of a\ndictionary learned from training images. The reconstruction has two stages:\nfirst we construct a tensor dictionary prior from our training data, and then\nwe pose the reconstruction problem in terms of recovering the expansion\ncoefficients in that dictionary. Our approach differs from past approaches in\nthat a) we use a third-order tensor representation for our images and b) we\nrecast the reconstruction problem using the tensor formulation. The dictionary\nlearning problem is presented as a non-negative tensor factorization problem\nwith sparsity constraints. The reconstruction problem is formulated in a convex\noptimization framework by looking for a solution with a sparse representation\nin the tensor dictionary. Numerical results show that our tensor formulation\nleads to very sparse representations of both the training images and the\nreconstructions due to the ability of representing repeated features compactly\nin the dictionary. \n\n"}
{"id": "1506.05032", "contents": "Title: Histopathological Image Classification using Discriminative\n  Feature-oriented Dictionary Learning Abstract: In histopathological image analysis, feature extraction for classification is\na challenging task due to the diversity of histology features suitable for each\nproblem as well as presence of rich geometrical structures. In this paper, we\npropose an automatic feature discovery framework via learning class-specific\ndictionaries and present a low-complexity method for classification and disease\ngrading in histopathology. Essentially, our Discriminative Feature-oriented\nDictionary Learning (DFDL) method learns class-specific dictionaries such that\nunder a sparsity constraint, the learned dictionaries allow representing a new\nimage sample parsimoniously via the dictionary corresponding to the class\nidentity of the sample. At the same time, the dictionary is designed to be\npoorly capable of representing samples from other classes. Experiments on three\nchallenging real-world image databases: 1) histopathological images of\nintraductal breast lesions, 2) mammalian kidney, lung and spleen images\nprovided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University,\nand 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, reveal\nthe merits of our proposal over state-of-the-art alternatives. {Moreover, we\ndemonstrate that DFDL exhibits a more graceful decay in classification accuracy\nagainst the number of training images which is highly desirable in practice\nwhere generous training is often not available \n\n"}
{"id": "1506.06724", "contents": "Title: Aligning Books and Movies: Towards Story-like Visual Explanations by\n  Watching Movies and Reading Books Abstract: Books are a rich source of both fine-grained information, how a character, an\nobject or a scene looks like, as well as high-level semantics, what someone is\nthinking, feeling and how these states evolve through a story. This paper aims\nto align books to their movie releases in order to provide rich descriptive\nexplanations for visual content that go semantically far beyond the captions\navailable in current datasets. To align movies and books we exploit a neural\nsentence embedding that is trained in an unsupervised way from a large corpus\nof books, as well as a video-text neural embedding for computing similarities\nbetween movie clips and sentences in the book. We propose a context-aware CNN\nto combine information from multiple sources. We demonstrate good quantitative\nperformance for movie/book alignment and show several qualitative examples that\nshowcase the diversity of tasks our model can be used for. \n\n"}
{"id": "1506.08615", "contents": "Title: Coercive functions from a topological viewpoint and properties of\n  minimizing sets of convex functions appearing in image restoration Abstract: Many tasks in image processing can be tackled by modeling an appropriate data\nfidelity term $\\Phi: \\mathbb{R}^n \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ and\nthen solve one of the regularized minimization problems \\begin{align*}\n  &{}(P_{1,\\tau}) \\qquad \\mathop{\\rm argmin}_{x \\in \\mathbb R^n} \\big\\{ \\Phi(x)\n\\;{\\rm s.t.}\\; \\Psi(x) \\leq \\tau \\big\\} \\\\ &{}(P_{2,\\lambda}) \\qquad\n\\mathop{\\rm argmin}_{x \\in \\mathbb R^n} \\{ \\Phi(x) + \\lambda \\Psi(x) \\}, \\;\n\\lambda > 0 \\end{align*} with some function $\\Psi: \\mathbb{R}^n \\rightarrow\n\\mathbb{R} \\cup \\{+\\infty\\}$ and a good choice of the parameter(s). Two tasks\narise naturally here: \\begin{align*} {}& \\text{1. Study the solver sets ${\\rm\nSOL}(P_{1,\\tau})$ and\n  ${\\rm SOL}(P_{2,\\lambda})$ of the minimization problems.} \\\\ {}& \\text{2.\nEnsure that the minimization problems have solutions.} \\end{align*} This thesis\nprovides contributions to both tasks: Regarding the first task for a more\nspecial setting we prove that there are intervals $(0,c)$ and $(0,d)$ such that\nthe setvalued curves \\begin{align*}\n  \\tau \\mapsto {}& {\\rm SOL}(P_{1,\\tau}), \\; \\tau \\in (0,c) \\\\ {} \\lambda\n\\mapsto {}& {\\rm SOL}(P_{2,\\lambda}), \\; \\lambda \\in (0,d) \\end{align*} are the\nsame, besides an order reversing parameter change $g: (0,c) \\rightarrow (0,d)$.\nMoreover we show that the solver sets are changing all the time while $\\tau$\nruns from $0$ to $c$ and $\\lambda$ runs from $d$ to $0$.\n  In the presence of lower semicontinuity the second task is done if we have\nadditionally coercivity. We regard lower semicontinuity and coercivity from a\ntopological point of view and develop a new technique for proving lower\nsemicontinuity plus coercivity.\n  Dropping any lower semicontinuity assumption we also prove a theorem on the\ncoercivity of a sum of functions. \n\n"}
{"id": "1507.00913", "contents": "Title: Fine-grained Recognition Datasets for Biodiversity Analysis Abstract: In the following paper, we present and discuss challenging applications for\nfine-grained visual classification (FGVC): biodiversity and species analysis.\nWe not only give details about two challenging new datasets suitable for\ncomputer vision research with up to 675 highly similar classes, but also\npresent first results with localized features using convolutional neural\nnetworks (CNN). We conclude with a list of challenging new research directions\nin the area of visual classification for biodiversity research. \n\n"}
{"id": "1507.02380", "contents": "Title: Learning Structured Ordinal Measures for Video based Face Recognition Abstract: This paper presents a structured ordinal measure method for video-based face\nrecognition that simultaneously learns ordinal filters and structured ordinal\nfeatures. The problem is posed as a non-convex integer program problem that\nincludes two parts. The first part learns stable ordinal filters to project\nvideo data into a large-margin ordinal space. The second seeks self-correcting\nand discrete codes by balancing the projected data and a rank-one ordinal\nmatrix in a structured low-rank way. Unsupervised and supervised structures are\nconsidered for the ordinal matrix. In addition, as a complement to hierarchical\nstructures, deep feature representations are integrated into our method to\nenhance coding stability. An alternating minimization method is employed to\nhandle the discrete and low-rank constraints, yielding high-quality codes that\ncapture prior structures well. Experimental results on three commonly used face\nvideo databases show that our method with a simple voting classifier can\nachieve state-of-the-art recognition rates using fewer features and samples. \n\n"}
{"id": "1507.05699", "contents": "Title: Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians Abstract: Convolutional neural nets (CNNs) have demonstrated remarkable performance in\nrecent history. Such approaches tend to work in a unidirectional bottom-up\nfeed-forward fashion. However, practical experience and biological evidence\ntells us that feedback plays a crucial role, particularly for detailed spatial\nunderstanding tasks. This work explores bidirectional architectures that also\nreason with top-down feedback: neural units are influenced by both lower and\nhigher-level units.\n  We do so by treating units as rectified latent variables in a quadratic\nenergy function, which can be seen as a hierarchical Rectified Gaussian model\n(RGs). We show that RGs can be optimized with a quadratic program (QP), that\ncan in turn be optimized with a recurrent neural network (with rectified linear\nunits). This allows RGs to be trained with GPU-optimized gradient descent. From\na theoretical perspective, RGs help establish a connection between CNNs and\nhierarchical probabilistic models. From a practical perspective, RGs are well\nsuited for detailed spatial tasks that can benefit from top-down reasoning. We\nillustrate them on the challenging task of keypoint localization under\nocclusions, where local bottom-up evidence may be misleading. We demonstrate\nstate-of-the-art results on challenging benchmarks. \n\n"}
{"id": "1507.06550", "contents": "Title: Human Pose Estimation with Iterative Error Feedback Abstract: Hierarchical feature extractors such as Convolutional Networks (ConvNets)\nhave achieved impressive performance on a variety of classification tasks using\npurely feedforward processing. Feedforward architectures can learn rich\nrepresentations of the input space but do not explicitly model dependencies in\nthe output spaces, that are quite structured for tasks such as articulated\nhuman pose estimation or object segmentation. Here we propose a framework that\nexpands the expressive power of hierarchical feature extractors to encompass\nboth input and output spaces, by introducing top-down feedback. Instead of\ndirectly predicting the outputs in one go, we use a self-correcting model that\nprogressively changes an initial solution by feeding back error predictions, in\na process we call Iterative Error Feedback (IEF). IEF shows excellent\nperformance on the task of articulated pose estimation in the challenging MPII\nand LSP benchmarks, matching the state-of-the-art without requiring ground\ntruth scale annotation. \n\n"}
{"id": "1508.00092", "contents": "Title: Land Use Classification in Remote Sensing Images by Convolutional Neural\n  Networks Abstract: We explore the use of convolutional neural networks for the semantic\nclassification of remote sensing scenes. Two recently proposed architectures,\nCaffeNet and GoogLeNet, are adopted, with three different learning modalities.\nBesides conventional training from scratch, we resort to pre-trained networks\nthat are only fine-tuned on the target data, so as to avoid overfitting\nproblems and reduce design time. Experiments on two remote sensing datasets,\nwith markedly different characteristics, testify on the effectiveness and wide\napplicability of the proposed solution, which guarantees a significant\nperformance improvement over all state-of-the-art references. \n\n"}
{"id": "1508.01055", "contents": "Title: Estimating snow cover from publicly available images Abstract: In this paper we study the problem of estimating snow cover in mountainous\nregions, that is, the spatial extent of the earth surface covered by snow. We\nargue that publicly available visual content, in the form of user generated\nphotographs and image feeds from outdoor webcams, can both be leveraged as\nadditional measurement sources, complementing existing ground, satellite and\nairborne sensor data. To this end, we describe two content acquisition and\nprocessing pipelines that are tailored to such sources, addressing the specific\nchallenges posed by each of them, e.g., identifying the mountain peaks,\nfiltering out images taken in bad weather conditions, handling varying\nillumination conditions. The final outcome is summarized in a snow cover index,\nwhich indicates for a specific mountain and day of the year, the fraction of\nvisible area covered by snow, possibly at different elevations. We created a\nmanually labelled dataset to assess the accuracy of the image snow covered area\nestimation, achieving 90.0% precision at 91.1% recall. In addition, we show\nthat seasonal trends related to air temperature are captured by the snow cover\nindex. \n\n"}
{"id": "1508.04843", "contents": "Title: Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary\n  Detection Abstract: Efforts to automate the reconstruction of neural circuits from 3D electron\nmicroscopic (EM) brain images are critical for the field of connectomics. An\nimportant computation for reconstruction is the detection of neuronal\nboundaries. Images acquired by serial section EM, a leading 3D EM technique,\nare highly anisotropic, with inferior quality along the third dimension. For\nsuch images, the 2D max-pooling convolutional network has set the standard for\nperformance at boundary detection. Here we achieve a substantial gain in\naccuracy through three innovations. Following the trend towards deeper networks\nfor object recognition, we use a much deeper network than previously employed\nfor boundary detection. Second, we incorporate 3D as well as 2D filters, to\nenable computations that use 3D context. Finally, we adopt a recursively\ntrained architecture in which a first network generates a preliminary boundary\nmap that is provided as input along with the original image to a second network\nthat generates a final boundary map. Backpropagation training is accelerated by\nZNN, a new implementation of 3D convolutional networks that uses multicore CPU\nparallelism for speed. Our hybrid 2D-3D architecture could be more generally\napplicable to other types of anisotropic 3D images, including video, and our\nrecursive framework for any image labeling problem. \n\n"}
{"id": "1508.06073", "contents": "Title: Cooking in the kitchen: Recognizing and Segmenting Human Activities in\n  Videos Abstract: As research on action recognition matures, the focus is shifting away from\ncategorizing basic task-oriented actions using hand-segmented video datasets to\nunderstanding complex goal-oriented daily human activities in real-world\nsettings. Temporally structured models would seem obvious to tackle this set of\nproblems, but so far, cases where these models have outperformed simpler\nunstructured bag-of-word types of models are scarce. With the increasing\navailability of large human activity datasets, combined with the development of\nnovel feature coding techniques that yield more compact representations, it is\ntime to revisit structured generative approaches.\n  Here, we describe an end-to-end generative approach from the encoding of\nfeatures to the structural modeling of complex human activities by applying\nFisher vectors and temporal models for the analysis of video sequences.\n  We systematically evaluate the proposed approach on several available\ndatasets (ADL, MPIICooking, and Breakfast datasets) using a variety of\nperformance metrics. Through extensive system evaluations, we demonstrate that\ncombining compact video representations based on Fisher Vectors with HMM-based\nmodeling yields very significant gains in accuracy and when properly trained\nwith sufficient training samples, structured temporal models outperform\nunstructured bag-of-word types of models by a large margin on the tested\nperformance metric. \n\n"}
{"id": "1509.01122", "contents": "Title: Vision-Based Road Detection using Contextual Blocks Abstract: Road detection is a fundamental task in autonomous navigation systems. In\nthis paper, we consider the case of monocular road detection, where images are\nsegmented into road and non-road regions. Our starting point is the well-known\nmachine learning approach, in which a classifier is trained to distinguish road\nand non-road regions based on hand-labeled images. We proceed by introducing\nthe use of \"contextual blocks\" as an efficient way of providing contextual\ninformation to the classifier. Overall, the proposed methodology, including its\nimage feature selection and classifier, was conceived with computational cost\nin mind, leaving room for optimized implementations. Regarding experiments, we\nperform a sensible evaluation of each phase and feature subset that composes\nour system. The results show a great benefit from using contextual blocks and\ndemonstrate their computational efficiency. Finally, we submit our results to\nthe KITTI road detection benchmark achieving scores comparable with state of\nthe art methods. \n\n"}
{"id": "1509.01354", "contents": "Title: CNN Based Hashing for Image Retrieval Abstract: Along with data on the web increasing dramatically, hashing is becoming more\nand more popular as a method of approximate nearest neighbor search. Previous\nsupervised hashing methods utilized similarity/dissimilarity matrix to get\nsemantic information. But the matrix is not easy to construct for a new\ndataset. Rather than to reconstruct the matrix, we proposed a straightforward\nCNN-based hashing method, i.e. binarilizing the activations of a fully\nconnected layer with threshold 0 and taking the binary result as hash codes.\nThis method achieved the best performance on CIFAR-10 and was comparable with\nthe state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that\nthe signs of activations may carry more information than the relative values of\nactivations between samples, and that the co-adaption between feature extractor\nand hash functions is important for hashing. \n\n"}
{"id": "1509.04916", "contents": "Title: Projection Bank: From High-dimensional Data to Medium-length Binary\n  Codes Abstract: Recently, very high-dimensional feature representations, e.g., Fisher Vector,\nhave achieved excellent performance for visual recognition and retrieval.\nHowever, these lengthy representations always cause extremely heavy\ncomputational and storage costs and even become unfeasible in some large-scale\napplications. A few existing techniques can transfer very high-dimensional data\ninto binary codes, but they still require the reduced code length to be\nrelatively long to maintain acceptable accuracies. To target a better balance\nbetween computational efficiency and accuracies, in this paper, we propose a\nnovel embedding method called Binary Projection Bank (BPB), which can\neffectively reduce the very high-dimensional representations to\nmedium-dimensional binary codes without sacrificing accuracies. Instead of\nusing conventional single linear or bilinear projections, the proposed method\nlearns a bank of small projections via the max-margin constraint to optimally\npreserve the intrinsic data similarity. We have systematically evaluated the\nproposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing\ncompetitive retrieval and recognition accuracies compared with state-of-the-art\napproaches, but with a significantly smaller memory footprint and lower coding\ncomplexity. \n\n"}
{"id": "1509.06321", "contents": "Title: Evaluating the visualization of what a Deep Neural Network has learned Abstract: Deep Neural Networks (DNNs) have demonstrated impressive performance in\ncomplex machine learning tasks such as image classification or speech\nrecognition. However, due to their multi-layer nonlinear structure, they are\nnot transparent, i.e., it is hard to grasp what makes them arrive at a\nparticular classification or recognition decision given a new unseen data\nsample. Recently, several approaches have been proposed enabling one to\nunderstand and interpret the reasoning embodied in a DNN for a single test\nimage. These methods quantify the ''importance'' of individual pixels wrt the\nclassification decision and allow a visualization in terms of a heatmap in\npixel/input space. While the usefulness of heatmaps can be judged subjectively\nby a human, an objective quality measure is missing. In this paper we present a\ngeneral methodology based on region perturbation for evaluating ordered\ncollections of pixels such as heatmaps. We compare heatmaps computed by three\ndifferent methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main\nresult is that the recently proposed Layer-wise Relevance Propagation (LRP)\nalgorithm qualitatively and quantitatively provides a better explanation of\nwhat made a DNN arrive at a particular classification decision than the\nsensitivity-based approach or the deconvolution method. We provide theoretical\narguments to explain this result and discuss its practical implications.\nFinally, we investigate the use of heatmaps for unsupervised assessment of\nneural network performance. \n\n"}
{"id": "1509.07831", "contents": "Title: Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds,\n  Language and Trajectories Abstract: A robot operating in a real-world environment needs to perform reasoning over\na variety of sensor modalities such as vision, language and motion\ntrajectories. However, it is extremely challenging to manually design features\nrelating such disparate modalities. In this work, we introduce an algorithm\nthat learns to embed point-cloud, natural language, and manipulation trajectory\ndata into a shared embedding space with a deep neural network. To learn\nsemantically meaningful spaces throughout our network, we use a loss-based\nmargin to bring embeddings of relevant pairs closer together while driving\nless-relevant cases from different modalities further apart. We use this both\nto pre-train its lower layers and fine-tune our final embedding space, leading\nto a more robust representation. We test our algorithm on the task of\nmanipulating novel objects and appliances based on prior experience with other\nobjects. On a large dataset, we achieve significant improvements in both\naccuracy and inference time over the previous state of the art. We also perform\nend-to-end experiments on a PR2 robot utilizing our learned embedding space. \n\n"}
{"id": "1510.00149", "contents": "Title: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained\n  Quantization and Huffman Coding Abstract: Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency. \n\n"}
{"id": "1510.00921", "contents": "Title: Cross-convolutional-layer Pooling for Image Recognition Abstract: Recent studies have shown that a Deep Convolutional Neural Network (DCNN)\npretrained on a large image dataset can be used as a universal image\ndescriptor, and that doing so leads to impressive performance for a variety of\nimage classification tasks. Most of these studies adopt activations from a\nsingle DCNN layer, usually the fully-connected layer, as the image\nrepresentation. In this paper, we proposed a novel way to extract image\nrepresentations from two consecutive convolutional layers: one layer is\nutilized for local feature extraction and the other serves as guidance to pool\nthe extracted features. By taking different viewpoints of convolutional layers,\nwe further develop two schemes to realize this idea. The first one directly\nuses convolutional layers from a DCNN. The second one applies the pretrained\nCNN on densely sampled image regions and treats the fully-connected activations\nof each image region as convolutional feature activations. We then train\nanother convolutional layer on top of that as the pooling-guidance\nconvolutional layer. By applying our method to three popular visual\nclassification tasks, we find our first scheme tends to perform better on the\napplications which need strong discrimination on subtle object patterns within\nsmall regions while the latter excels in the cases that require discrimination\non category-level patterns. Overall, the proposed method achieves superior\nperformance over existing ways of extracting image representations from a DCNN. \n\n"}
{"id": "1510.01257", "contents": "Title: Efficient Object Detection for High Resolution Images Abstract: Efficient generation of high-quality object proposals is an essential step in\nstate-of-the-art object detection systems based on deep convolutional neural\nnetworks (DCNN) features. Current object proposal algorithms are\ncomputationally inefficient in processing high resolution images containing\nsmall objects, which makes them the bottleneck in object detection systems. In\nthis paper we present effective methods to detect objects for high resolution\nimages. We combine two complementary strategies. The first approach is to\npredict bounding boxes based on adjacent visual features. The second approach\nuses high level image features to guide a two-step search process that\nadaptively focuses on regions that are likely to contain small objects. We\nextract features required for the two strategies by utilizing a pre-trained\nDCNN model known as AlexNet. We demonstrate the effectiveness of our algorithm\nby showing its performance on a high-resolution image subset of the SUN 2012\nobject detection dataset. \n\n"}
{"id": "1510.02173", "contents": "Title: Data-Efficient Learning of Feedback Policies from Image Pixels using\n  Deep Dynamical Models Abstract: Data-efficient reinforcement learning (RL) in continuous state-action spaces\nusing very high-dimensional observations remains a key challenge in developing\nfully autonomous systems. We consider a particularly important instance of this\nchallenge, the pixels-to-torques problem, where an RL agent learns a\nclosed-loop control policy (\"torques\") from pixel information only. We\nintroduce a data-efficient, model-based reinforcement learning algorithm that\nlearns such a closed-loop policy directly from pixel information. The key\ningredient is a deep dynamical model for learning a low-dimensional feature\nembedding of images jointly with a predictive model in this low-dimensional\nfeature space. Joint learning is crucial for long-term predictions, which lie\nat the core of the adaptive nonlinear model predictive control strategy that we\nuse for closed-loop control. Compared to state-of-the-art RL methods for\ncontinuous states and actions, our approach learns quickly, scales to\nhigh-dimensional state spaces, is lightweight and an important step toward\nfully autonomous end-to-end learning from pixels to torques. \n\n"}
{"id": "1510.04609", "contents": "Title: Layer-Specific Adaptive Learning Rates for Deep Networks Abstract: The increasing complexity of deep learning architectures is resulting in\ntraining time requiring weeks or even months. This slow training is due in part\nto vanishing gradients, in which the gradients used by back-propagation are\nextremely large for weights connecting deep layers (layers near the output\nlayer), and extremely small for shallow layers (near the input layer); this\nresults in slow learning in the shallow layers. Additionally, it has also been\nshown that in highly non-convex problems, such as deep neural networks, there\nis a proliferation of high-error low curvature saddle points, which slows down\nlearning dramatically. In this paper, we attempt to overcome the two above\nproblems by proposing an optimization method for training deep neural networks\nwhich uses learning rates which are both specific to each layer in the network\nand adaptive to the curvature of the function, increasing the learning rate at\nlow curvature points. This enables us to speed up learning in the shallow\nlayers of the network and quickly escape high-error low curvature saddle\npoints. We test our method on standard image classification datasets such as\nMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy\nas well as reduces the required training time over standard algorithms. \n\n"}
{"id": "1510.06925", "contents": "Title: Confusing Deep Convolution Networks by Relabelling Abstract: Deep convolutional neural networks have become the gold standard for image\nrecognition tasks, demonstrating many current state-of-the-art results and even\nachieving near-human level performance on some tasks. Despite this fact it has\nbeen shown that their strong generalisation qualities can be fooled to\nmisclassify previously correctly classified natural images and give erroneous\nhigh confidence classifications to nonsense synthetic images. In this paper we\nextend that work, by presenting a straightforward way to perturb an image in\nsuch a way as to cause it to acquire any other label from within the dataset\nwhile leaving this perturbed image visually indistinguishable from the\noriginal. \n\n"}
{"id": "1510.09083", "contents": "Title: Deep Recurrent Regression for Facial Landmark Detection Abstract: We propose a novel end-to-end deep architecture for face landmark detection,\nbased on a deep convolutional and deconvolutional network followed by carefully\ndesigned recurrent network structures. The pipeline of this architecture\nconsists of three parts. Through the first part, we encode an input face image\nto resolution-preserved deconvolutional feature maps via a deep network with\nstacked convolutional and deconvolutional layers. Then, in the second part, we\nestimate the initial coordinates of the facial key points by an additional\nconvolutional layer on top of these deconvolutional feature maps. In the last\npart, by using the deconvolutional feature maps and the initial facial key\npoints as input, we refine the coordinates of the facial key points by a\nrecurrent network that consists of multiple Long-Short Term Memory (LSTM)\ncomponents. Extensive evaluations on several benchmark datasets show that the\nproposed deep architecture has superior performance against the\nstate-of-the-art methods. \n\n"}
{"id": "1511.02986", "contents": "Title: Experimental robustness of Fourier Ptychography phase retrieval\n  algorithms Abstract: Fourier ptychography is a new computational microscopy technique that\nprovides gigapixel-scale intensity and phase images with both wide\nfield-of-view and high resolution. By capturing a stack of low-resolution\nimages under different illumination angles, a nonlinear inverse algorithm can\nbe used to computationally reconstruct the high-resolution complex field. Here,\nwe compare and classify multiple proposed inverse algorithms in terms of\nexperimental robustness. We find that the main sources of error are noise,\naberrations and mis-calibration (i.e. model mis-match). Using simulations and\nexperiments, we demonstrate that the choice of cost function plays a critical\nrole, with amplitude-based cost functions performing better than\nintensity-based ones. The reason for this is that Fourier ptychography datasets\nconsist of images from both brightfield and darkfield illumination,\nrepresenting a large range of measured intensities. Both noise (e.g. Poisson\nnoise) and model mis-match errors are shown to scale with intensity. Hence,\nalgorithms that use an appropriate cost function will be more tolerant to both\nnoise and model mis-match. Given these insights, we propose a global Newton's\nmethod algorithm which is robust and computationally efficient. Finally, we\ndiscuss the impact of procedures for algorithmic correction of aberrations and\nmis-calibration. \n\n"}
{"id": "1511.03240", "contents": "Title: Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer Abstract: Semantic annotations are vital for training models for object recognition,\nsemantic segmentation or scene understanding. Unfortunately, pixelwise\nannotation of images at very large scale is labor-intensive and only little\nlabeled data is available, particularly at instance level and for street\nscenes. In this paper, we propose to tackle this problem by lifting the\nsemantic instance labeling task from 2D into 3D. Given reconstructions from\nstereo or laser data, we annotate static 3D scene elements with rough bounding\nprimitives and develop a model which transfers this information into the image\ndomain. We leverage our method to obtain 2D labels for a novel suburban video\ndataset which we have collected, resulting in 400k semantic and instance image\nannotations. A comparison of our method to state-of-the-art label transfer\nbaselines reveals that 3D information enables more efficient annotation while\nat the same time resulting in improved accuracy and time-coherent labels. \n\n"}
{"id": "1511.03690", "contents": "Title: Deep Multimodal Semantic Embeddings for Speech and Images Abstract: In this paper, we present a model which takes as input a corpus of images\nwith relevant spoken captions and finds a correspondence between the two\nmodalities. We employ a pair of convolutional neural networks to model visual\nobjects and speech signals at the word level, and tie the networks together\nwith an embedding and alignment model which learns a joint semantic space over\nboth modalities. We evaluate our model using image search and annotation tasks\non the Flickr8k dataset, which we augmented by collecting a corpus of 40,000\nspoken captions using Amazon Mechanical Turk. \n\n"}
{"id": "1511.04119", "contents": "Title: Action Recognition using Visual Attention Abstract: We propose a soft attention based model for the task of action recognition in\nvideos. We use multi-layered Recurrent Neural Networks (RNNs) with Long\nShort-Term Memory (LSTM) units which are deep both spatially and temporally.\nOur model learns to focus selectively on parts of the video frames and\nclassifies videos after taking a few glimpses. The model essentially learns\nwhich parts in the frames are relevant for the task at hand and attaches higher\nimportance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51\nand Hollywood2 datasets and analyze how the model focuses its attention\ndepending on the scene and the action being performed. \n\n"}
{"id": "1511.04512", "contents": "Title: Zero-Shot Learning via Joint Latent Similarity Embedding Abstract: Zero-shot recognition (ZSR) deals with the problem of predicting class labels\nfor target domain instances based on source domain side information (e.g.\nattributes) of unseen classes. We formulate ZSR as a binary prediction problem.\nOur resulting classifier is class-independent. It takes an arbitrary pair of\nsource and target domain instances as input and predicts whether or not they\ncome from the same class, i.e. whether there is a match. We model the posterior\nprobability of a match since it is a sufficient statistic and propose a latent\nprobabilistic model in this context. We develop a joint discriminative learning\nframework based on dictionary learning to jointly learn the parameters of our\nmodel for both domains, which ultimately leads to our class-independent\nclassifier. Many of the existing embedding methods can be viewed as special\ncases of our probabilistic model. On ZSR our method shows 4.90\\% improvement\nover the state-of-the-art in accuracy averaged across four benchmark datasets.\nWe also adapt ZSR method for zero-shot retrieval and show 22.45\\% improvement\naccordingly in mean average precision (mAP). \n\n"}
{"id": "1511.05261", "contents": "Title: Robust PCA via Nonconvex Rank Approximation Abstract: Numerous applications in data mining and machine learning require recovering\na matrix of minimal rank. Robust principal component analysis (RPCA) is a\ngeneral framework for handling this kind of problems. Nuclear norm based convex\nsurrogate of the rank function in RPCA is widely investigated. Under certain\nassumptions, it can recover the underlying true low rank matrix with high\nprobability. However, those assumptions may not hold in real-world\napplications. Since the nuclear norm approximates the rank by adding all\nsingular values together, which is essentially a $\\ell_1$-norm of the singular\nvalues, the resulting approximation error is not trivial and thus the resulting\nmatrix estimator can be significantly biased. To seek a closer approximation\nand to alleviate the above-mentioned limitations of the nuclear norm, we\npropose a nonconvex rank approximation. This approximation to the matrix rank\nis tighter than the nuclear norm. To solve the associated nonconvex\nminimization problem, we develop an efficient augmented Lagrange multiplier\nbased optimization algorithm. Experimental results demonstrate that our method\noutperforms current state-of-the-art algorithms in both accuracy and\nefficiency. \n\n"}
{"id": "1511.06015", "contents": "Title: Active Object Localization with Deep Reinforcement Learning Abstract: We present an active detection model for localizing objects in scenes. The\nmodel is class-specific and allows an agent to focus attention on candidate\nregions for identifying the correct location of a target object. This agent\nlearns to deform a bounding box using simple transformation actions, with the\ngoal of determining the most specific location of target objects following\ntop-down reasoning. The proposed localization agent is trained using deep\nreinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show\nthat agents guided by the proposed model are able to localize a single instance\nof an object after analyzing only between 11 and 25 regions in an image, and\nobtain the best detection results among systems that do not use object\nproposals for object localization. \n\n"}
{"id": "1511.06049", "contents": "Title: What Objective Does Self-paced Learning Indeed Optimize? Abstract: Self-paced learning (SPL) is a recently raised methodology designed through\nsimulating the learning principle of humans/animals. A variety of SPL\nrealization schemes have been designed for different computer vision and\npattern recognition tasks, and empirically substantiated to be effective in\nthese applications. However, the investigation on its theoretical insight is\nstill a blank. To this issue, this study attempts to provide some new\ntheoretical understanding under the SPL scheme. Specifically, we prove that the\nsolving strategy on SPL accords with a majorization minimization algorithm\nimplemented on a latent objective function. Furthermore, we find that the loss\nfunction contained in this latent objective has a similar configuration with\nnon-convex regularized penalty (NSPR) known in statistics and machine learning.\nSuch connection inspires us discovering more intrinsic relationship between SPL\nregimes and NSPR forms, like SCAD, LOG and EXP. The robustness insight under\nSPL can then be finely explained. We also analyze the capability of SPL on its\neasy loss prior embedding property, and provide an insightful interpretation to\nthe effectiveness mechanism under previous SPL variations. Besides, we design a\ngroup-partial-order loss prior, which is especially useful to weakly labeled\nlarge-scale data processing tasks. Through applying SPL with this loss prior to\nthe FCVID dataset, which is currently one of the biggest manually annotated\nvideo dataset, our method achieves state-of-the-art performance beyond previous\nmethods, which further helps supports the proposed theoretical arguments. \n\n"}
{"id": "1511.06067", "contents": "Title: Convolutional neural networks with low-rank regularization Abstract: Large CNNs have delivered impressive performance in various computer vision\napplications. But the storage and computation requirements make it problematic\nfor deploying these models on mobile devices. Recently, tensor decompositions\nhave been used for speeding up CNNs. In this paper, we further develop the\ntensor decomposition technique. We propose a new algorithm for computing the\nlow-rank tensor decomposition for removing the redundancy in the convolution\nkernels. The algorithm finds the exact global optimizer of the decomposition\nand is more effective than iterative methods. Based on the decomposition, we\nfurther propose a new method for training low-rank constrained CNNs from\nscratch. Interestingly, while achieving a significant speedup, sometimes the\nlow-rank constrained CNNs delivers significantly better performance than their\nnon-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank\nNIN model achieves $91.31\\%$ accuracy (without data augmentation), which also\nimproves upon state-of-the-art result. We evaluated the proposed method on\nCIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet,\nNIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is\nreduced by half while the performance is still comparable. Empirical success\nsuggests that low-rank tensor decompositions can be a very useful tool for\nspeeding up large CNNs. \n\n"}
{"id": "1511.06238", "contents": "Title: Multimodal sparse representation learning and applications Abstract: Unsupervised methods have proven effective for discriminative tasks in a\nsingle-modality scenario. In this paper, we present a multimodal framework for\nlearning sparse representations that can capture semantic correlation between\nmodalities. The framework can model relationships at a higher level by forcing\nthe shared sparse representation. In particular, we propose the use of joint\ndictionary learning technique for sparse coding and formulate the joint\nrepresentation for concision, cross-modal representations (in case of a missing\nmodality), and union of the cross-modal representations. Given the accelerated\ngrowth of multimodal data posted on the Web such as YouTube, Wikipedia, and\nTwitter, learning good multimodal features is becoming increasingly important.\nWe show that the shared representations enabled by our framework substantially\nimprove the classification performance under both unimodal and multimodal\nsettings. We further show how deep architectures built on the proposed\nframework are effective for the case of highly nonlinear correlations between\nmodalities. The effectiveness of our approach is demonstrated experimentally in\nimage denoising, multimedia event detection and retrieval on the TRECVID\ndataset (audio-video), category classification on the Wikipedia dataset\n(image-text), and sentiment classification on PhotoTweet (image-text). \n\n"}
{"id": "1511.06428", "contents": "Title: A Controller-Recognizer Framework: How necessary is recognition for\n  control? Abstract: Recently there has been growing interest in building active visual object\nrecognizers, as opposed to the usual passive recognizers which classifies a\ngiven static image into a predefined set of object categories. In this paper we\npropose to generalize these recently proposed end-to-end active visual\nrecognizers into a controller-recognizer framework. A model in the\ncontroller-recognizer framework consists of a controller, which interfaces with\nan external manipulator, and a recognizer which classifies the visual input\nadjusted by the manipulator. We describe two most recently proposed\ncontroller-recognizer models: recurrent attention model and spatial transformer\nnetwork as representative examples of controller-recognizer models. Based on\nthis description we observe that most existing end-to-end\ncontroller-recognizers tightly, or completely, couple a controller and\nrecognizer. We ask a question whether this tight coupling is necessary, and try\nto answer this empirically by building a controller-recognizer model with a\ndecoupled controller and recognizer. Our experiments revealed that it is not\nalways necessary to tightly couple them and that by decoupling a controller and\nrecognizer, there is a possibility of building a generic controller that is\npretrained and works together with any subsequent recognizer. \n\n"}
{"id": "1511.06432", "contents": "Title: Delving Deeper into Convolutional Networks for Learning Video\n  Representations Abstract: We propose an approach to learn spatio-temporal features in videos from\nintermediate visual representations we call \"percepts\" using\nGated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts\nthat are extracted from all level of a deep convolutional network trained on\nthe large ImageNet dataset. While high-level percepts contain highly\ndiscriminative information, they tend to have a low-spatial resolution.\nLow-level percepts, on the other hand, preserve a higher spatial resolution\nfrom which we can model finer motion patterns. Using low-level percepts can\nleads to high-dimensionality video representations. To mitigate this effect and\ncontrol the model number of parameters, we introduce a variant of the GRU model\nthat leverages the convolution operations to enforce sparse connectivity of the\nmodel units and share parameters across the input spatial locations.\n  We empirically validate our approach on both Human Action Recognition and\nVideo Captioning tasks. In particular, we achieve results equivalent to\nstate-of-art on the YouTube2Text dataset using a simpler text-decoder model and\nwithout extra 3D CNN features. \n\n"}
{"id": "1511.06448", "contents": "Title: Learning Representations from EEG with Deep Recurrent-Convolutional\n  Neural Networks Abstract: One of the challenges in modeling cognitive events from electroencephalogram\n(EEG) data is finding representations that are invariant to inter- and\nintra-subject differences, as well as to inherent noise associated with such\ndata. Herein, we propose a novel approach for learning such representations\nfrom multi-channel EEG time-series, and demonstrate its advantages in the\ncontext of mental load classification task. First, we transform EEG activities\ninto a sequence of topology-preserving multi-spectral images, as opposed to\nstandard EEG analysis techniques that ignore such spatial information. Next, we\ntrain a deep recurrent-convolutional network inspired by state-of-the-art video\nclassification to learn robust representations from the sequence of images. The\nproposed approach is designed to preserve the spatial, spectral, and temporal\nstructure of EEG which leads to finding features that are less sensitive to\nvariations and distortions within each dimension. Empirical evaluation on the\ncognitive load classification task demonstrated significant improvements in\nclassification accuracy over current state-of-the-art approaches in this field. \n\n"}
{"id": "1511.06653", "contents": "Title: Recurrent Semi-supervised Classification and Constrained Adversarial\n  Generation with Motion Capture Data Abstract: We explore recurrent encoder multi-decoder neural network architectures for\nsemi-supervised sequence classification and reconstruction. We find that the\nuse of multiple reconstruction modules helps models generalize in a\nclassification task when only a small amount of labeled data is available,\nwhich is often the case in practice. Such models provide useful high-level\nrepresentations of motions allowing clustering, searching and faster labeling\nof new sequences. We also propose a new, realistic partitioning of a\nwell-known, high quality motion-capture dataset for better evaluations. We\nfurther explore a novel formulation for future-predicting decoders based on\nconditional recurrent generative adversarial networks, for which we propose\nboth soft and hard constraints for transition generation derived from desired\nphysical properties of synthesized future movements and desired animation\ngoals. We find that using such constraints allow to stabilize the training of\nrecurrent adversarial architectures for animation generation. \n\n"}
{"id": "1511.06860", "contents": "Title: Convex Sparse Spectral Clustering: Single-view to Multi-view Abstract: Spectral Clustering (SC) is one of the most widely used methods for data\nclustering. It first finds a low-dimensonal embedding $U$ of data by computing\nthe eigenvectors of the normalized Laplacian matrix, and then performs k-means\non $U^\\top$ to get the final clustering result. In this work, we observe that,\nin the ideal case, $UU^\\top$ should be block diagonal and thus sparse.\nTherefore we propose the Sparse Spectral Clustering (SSC) method which extends\nSC with sparse regularization on $UU^\\top$. To address the computational issue\nof the nonconvex SSC model, we propose a novel convex relaxation of SSC based\non the convex hull of the fixed rank projection matrices. Then the convex SSC\nmodel can be efficiently solved by the Alternating Direction Method of\n\\canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse\nSpectral Clustering (PSSC) which extends SSC to boost the clustering\nperformance by using the multi-view information of data. Experimental\ncomparisons with several baselines on real-world datasets testify to the\nefficacy of our proposed methods. \n\n"}
{"id": "1511.07122", "contents": "Title: Multi-Scale Context Aggregation by Dilated Convolutions Abstract: State-of-the-art models for semantic segmentation are based on adaptations of\nconvolutional networks that had originally been designed for image\nclassification. However, dense prediction and image classification are\nstructurally different. In this work, we develop a new convolutional network\nmodule that is specifically designed for dense prediction. The presented module\nuses dilated convolutions to systematically aggregate multi-scale contextual\ninformation without losing resolution. The architecture is based on the fact\nthat dilated convolutions support exponential expansion of the receptive field\nwithout loss of resolution or coverage. We show that the presented context\nmodule increases the accuracy of state-of-the-art semantic segmentation\nsystems. In addition, we examine the adaptation of image classification\nnetworks to dense prediction and show that simplifying the adapted network can\nincrease accuracy. \n\n"}
{"id": "1512.00172", "contents": "Title: Analyzing Classifiers: Fisher Vectors and Deep Neural Networks Abstract: Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular and\nsuccessful algorithms for solving image classification problems. However, both\nare generally considered `black box' predictors as the non-linear\ntransformations involved have so far prevented transparent and interpretable\nreasoning. Recently, a principled technique, Layer-wise Relevance Propagation\n(LRP), has been developed in order to better comprehend the inherent structured\nreasoning of complex nonlinear classification models such as Bag of Feature\nmodels or DNNs. In this paper we (1) extend the LRP framework also for Fisher\nVector classifiers and then use it as analysis tool to (2) quantify the\nimportance of context for classification, (3) qualitatively compare DNNs\nagainst FV classifiers in terms of important image regions and (4) detect\npotential flaws and biases in data. All experiments are performed on the PASCAL\nVOC 2007 data set. \n\n"}
{"id": "1512.00486", "contents": "Title: Loss Functions for Top-k Error: Analysis and Insights Abstract: In order to push the performance on realistic computer vision tasks, the\nnumber of classes in modern benchmark datasets has significantly increased in\nrecent years. This increase in the number of classes comes along with increased\nambiguity between the class labels, raising the question if top-1 error is the\nright performance measure. In this paper, we provide an extensive comparison\nand evaluation of established multiclass methods comparing their top-k\nperformance both from a practical as well as from a theoretical perspective.\nMoreover, we introduce novel top-k loss functions as modifications of the\nsoftmax and the multiclass SVM losses and provide efficient optimization\nschemes for them. In the experiments, we compare on various datasets all of the\nproposed and established methods for top-k error optimization. An interesting\ninsight of this paper is that the softmax loss yields competitive top-k\nperformance for all k simultaneously. For a specific top-k error, our new top-k\nlosses lead typically to further improvements while being faster to train than\nthe softmax. \n\n"}
{"id": "1512.00907", "contents": "Title: Innovation Pursuit: A New Approach to Subspace Clustering Abstract: In subspace clustering, a group of data points belonging to a union of\nsubspaces are assigned membership to their respective subspaces. This paper\npresents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of\nsubspace clustering using a new geometrical idea whereby subspaces are\nidentified based on their relative novelties. We present two frameworks in\nwhich the idea of innovation pursuit is used to distinguish the subspaces.\nUnderlying the first framework is an iterative method that finds the subspaces\nconsecutively by solving a series of simple linear optimization problems, each\nsearching for a direction of innovation in the span of the data potentially\northogonal to all subspaces except for the one to be identified in one step of\nthe algorithm. A detailed mathematical analysis is provided establishing\nsufficient conditions for iPursuit to correctly cluster the data. The proposed\napproach can provably yield exact clustering even when the subspaces have\nsignificant intersections. It is shown that the complexity of the iterative\napproach scales only linearly in the number of data points and subspaces, and\nquadratically in the dimension of the subspaces. The second framework\nintegrates iPursuit with spectral clustering to yield a new variant of\nspectral-clustering-based algorithms. The numerical simulations with both real\nand synthetic data demonstrate that iPursuit can often outperform the\nstate-of-the-art subspace clustering algorithms, more so for subspaces with\nsignificant intersections, and that it significantly improves the\nstate-of-the-art result for subspace-segmentation-based face clustering. \n\n"}
{"id": "1512.02013", "contents": "Title: Scalable domain adaptation of convolutional neural networks Abstract: Convolutional neural networks (CNNs) tend to become a standard approach to\nsolve a wide array of computer vision problems. Besides important theoretical\nand practical advances in their design, their success is built on the existence\nof manually labeled visual resources, such as ImageNet. The creation of such\ndatasets is cumbersome and here we focus on alternatives to manual labeling. We\nhypothesize that new resources are of uttermost importance in domains which are\nnot or weakly covered by ImageNet, such as tourism photographs. We first\ncollect noisy Flickr images for tourist points of interest and apply automatic\nor weakly-supervised reranking techniques to reduce noise. Then, we learn\ndomain adapted models with a standard CNN architecture and compare them to a\ngeneric model obtained from ImageNet. Experimental validation is conducted with\npublicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred.\nResults show that low-cost domain adaptation improves results compared to the\nuse of generic models but also compared to strong non-CNN baselines such as\ntriangulation embedding. \n\n"}
{"id": "1512.02017", "contents": "Title: Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images Abstract: Image representations, from SIFT and bag of visual words to Convolutional\nNeural Networks (CNNs) are a crucial component of almost all computer vision\nsystems. However, our understanding of them remains limited. In this paper we\nstudy several landmark representations, both shallow and deep, by a number of\ncomplementary visualization techniques. These visualizations are based on the\nconcept of \"natural pre-image\", namely a natural-looking image whose\nrepresentation has some notable property. We study in particular three such\nvisualizations: inversion, in which the aim is to reconstruct an image from its\nrepresentation, activation maximization, in which we search for patterns that\nmaximally stimulate a representation component, and caricaturization, in which\nthe visual patterns that a representation detects in an image are exaggerated.\nWe pose these as a regularized energy-minimization framework and demonstrate\nits generality and effectiveness. In particular, we show that this method can\ninvert representations such as HOG more accurately than recent alternatives\nwhile being applicable to CNNs too. Among our findings, we show that several\nlayers in CNNs retain photographically accurate information about the image,\nwith different degrees of geometric and photometric invariance. \n\n"}
{"id": "1512.03385", "contents": "Title: Deep Residual Learning for Image Recognition Abstract: Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation. \n\n"}
{"id": "1512.03740", "contents": "Title: Improving Human Activity Recognition Through Ranking and Re-ranking Abstract: We propose two well-motivated ranking-based methods to enhance the\nperformance of current state-of-the-art human activity recognition systems.\nFirst, as an improvement over the classic power normalization method, we\npropose a parameter-free ranking technique called rank normalization (RaN). RaN\nnormalizes each dimension of the video features to address the sparse and\nbursty distribution problems of Fisher Vectors and VLAD. Second, inspired by\ncurriculum learning, we introduce a training-free re-ranking technique called\nmulti-class iterative re-ranking (MIR). MIR captures relationships among action\nclasses by separating easy and typical videos from difficult ones and\nre-ranking the prediction scores of classifiers accordingly. We demonstrate\nthat our methods significantly improve the performance of state-of-the-art\nmotion features on six real-world datasets. \n\n"}
{"id": "1512.04103", "contents": "Title: Deep Relative Attributes Abstract: Visual attributes are great means of describing images or scenes, in a way\nboth humans and computers understand. In order to establish a correspondence\nbetween images and to be able to compare the strength of each property between\nimages, relative attributes were introduced. However, since their introduction,\nhand-crafted and engineered features were used to learn increasingly complex\nmodels for the problem of relative attributes. This limits the applicability of\nthose methods for more realistic cases. We introduce a deep neural network\narchitecture for the task of relative attribute prediction. A convolutional\nneural network (ConvNet) is adopted to learn the features by including an\nadditional layer (ranking layer) that learns to rank the images based on these\nfeatures. We adopt an appropriate ranking loss to train the whole network in an\nend-to-end fashion. Our proposed method outperforms the baseline and\nstate-of-the-art methods in relative attribute prediction on various coarse and\nfine-grained datasets. Our qualitative results along with the visualization of\nthe saliency maps show that the network is able to learn effective features for\neach specific attribute. Source code of the proposed method is available at\nhttps://github.com/yassersouri/ghiaseddin. \n\n"}
{"id": "1512.06785", "contents": "Title: Beyond Classification: Latent User Interests Profiling from Visual\n  Contents Analysis Abstract: User preference profiling is an important task in modern online social\nnetworks (OSN). With the proliferation of image-centric social platforms, such\nas Pinterest, visual contents have become one of the most informative data\nstreams for understanding user preferences. Traditional approaches usually\ntreat visual content analysis as a general classification problem where one or\nmore labels are assigned to each image. Although such an approach simplifies\nthe process of image analysis, it misses the rich context and visual cues that\nplay an important role in people's perception of images. In this paper, we\nexplore the possibilities of learning a user's latent visual preferences\ndirectly from image contents. We propose a distance metric learning method\nbased on Deep Convolutional Neural Networks (CNN) to directly extract\nsimilarity information from visual contents and use the derived distance metric\nto mine individual users' fine-grained visual preferences. Through our\npreliminary experiments using data from 5,790 Pinterest users, we show that\neven for the images within the same category, each user possesses distinct and\nindividually-identifiable visual preferences that are consistent over their\nlifetime. Our results underscore the untapped potential of finer-grained visual\npreference profiling in understanding users' preferences. \n\n"}
{"id": "1512.09227", "contents": "Title: Denoising and Completion of 3D Data via Multidimensional Dictionary\n  Learning Abstract: In this paper a new dictionary learning algorithm for multidimensional data\nis proposed. Unlike most conventional dictionary learning methods which are\nderived for dealing with vectors or matrices, our algorithm, named KTSVD,\nlearns a multidimensional dictionary directly via a novel algebraic approach\nfor tensor factorization as proposed in [3, 12, 13]. Using this approach one\ncan define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D\ndata to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based\non the idea of sparse coding (using group-sparsity over multidimensional\ncoefficient vectors), alternates between estimating a compact representation\nand dictionary learning. We analyze our KTSVD algorithm and demonstrate its\nresult on video completion and multispectral image denoising. \n\n"}
{"id": "1601.01006", "contents": "Title: Space-Time Representation of People Based on 3D Skeletal Data: A Review Abstract: Spatiotemporal human representation based on 3D visual perception data is a\nrapidly growing research area. Based on the information sources, these\nrepresentations can be broadly categorized into two groups based on RGB-D\ninformation or 3D skeleton data. Recently, skeleton-based human representations\nhave been intensively studied and kept attracting an increasing attention, due\nto their robustness to variations of viewpoint, human body scale and motion\nspeed as well as the realtime, online performance. This paper presents a\ncomprehensive survey of existing space-time representations of people based on\n3D skeletal data, and provides an informative categorization and analysis of\nthese methods from the perspectives, including information modality,\nrepresentation encoding, structure and transition, and feature engineering. We\nalso provide a brief overview of skeleton acquisition devices and construction\nmethods, enlist a number of public benchmark datasets with skeleton data, and\ndiscuss potential future research directions. \n\n"}
{"id": "1601.01145", "contents": "Title: Image-based Vehicle Analysis using Deep Neural Network: A Systematic\n  Study Abstract: We address the vehicle detection and classification problems using Deep\nNeural Networks (DNNs) approaches. Here we answer to questions that are\nspecific to our application including how to utilize DNN for vehicle detection,\nwhat features are useful for vehicle classification, and how to extend a model\ntrained on a limited size dataset, to the cases of extreme lighting condition.\nAnswering these questions we propose our approach that outperforms\nstate-of-the-art methods, and achieves promising results on image with extreme\nlighting conditions. \n\n"}
{"id": "1601.04667", "contents": "Title: Proactive Message Passing on Memory Factor Networks Abstract: We introduce a new type of graphical model that we call a \"memory factor\nnetwork\" (MFN). We show how to use MFNs to model the structure inherent in many\ntypes of data sets. We also introduce an associated message-passing style\nalgorithm called \"proactive message passing\"' (PMP) that performs inference on\nMFNs. PMP comes with convergence guarantees and is efficient in comparison to\ncompeting algorithms such as variants of belief propagation. We specialize MFNs\nand PMP to a number of distinct types of data (discrete, continuous, labelled)\nand inference problems (interpolation, hypothesis testing), provide examples,\nand discuss approaches for efficient implementation. \n\n"}
{"id": "1601.04798", "contents": "Title: Scale-aware Pixel-wise Object Proposal Networks Abstract: Object proposal is essential for current state-of-the-art object detection\npipelines. However, the existing proposal methods generally fail in producing\nresults with satisfying localization accuracy. The case is even worse for small\nobjects which however are quite common in practice. In this paper we propose a\nnovel Scale-aware Pixel-wise Object Proposal (SPOP) network to tackle the\nchallenges. The SPOP network can generate proposals with high recall rate and\naverage best overlap (ABO), even for small objects. In particular, in order to\nimprove the localization accuracy, a fully convolutional network is employed\nwhich predicts locations of object proposals for each pixel. The produced\nensemble of pixel-wise object proposals enhances the chance of hitting the\nobject significantly without incurring heavy extra computational cost. To solve\nthe challenge of localizing objects at small scale, two localization networks\nwhich are specialized for localizing objects with different scales are\nintroduced, following the divide-and-conquer philosophy. Location outputs of\nthese two networks are then adaptively combined to generate the final proposals\nby a large-/small-size weighting network. Extensive evaluations on PASCAL VOC\n2007 show the SPOP network is superior over the state-of-the-art models. The\nhigh-quality proposals from SPOP network also significantly improve the mean\naverage precision (mAP) of object detection with Fast-RCNN framework. Finally,\nthe SPOP network (trained on PASCAL VOC) shows great generalization performance\nwhen testing it on ILSVRC 2013 validation set. \n\n"}
{"id": "1601.07883", "contents": "Title: Towards the Design of an End-to-End Automated System for Image and\n  Video-based Recognition Abstract: Over many decades, researchers working in object recognition have longed for\nan end-to-end automated system that will simply accept 2D or 3D image or videos\nas inputs and output the labels of objects in the input data. Computer vision\nmethods that use representations derived based on geometric, radiometric and\nneural considerations and statistical and structural matchers and artificial\nneural network-based methods where a multi-layer network learns the mapping\nfrom inputs to class labels have provided competing approaches for image\nrecognition problems. Over the last four years, methods based on Deep\nConvolutional Neural Networks (DCNNs) have shown impressive performance\nimprovements on object detection/recognition challenge problems. This has been\nmade possible due to the availability of large annotated data, a better\nunderstanding of the non-linear mapping between image and class labels as well\nas the affordability of GPUs. In this paper, we present a brief history of\ndevelopments in computer vision and artificial neural networks over the last\nforty years for the problem of image-based recognition. We then present the\ndesign details of a deep learning system for end-to-end unconstrained face\nverification/recognition. Some open issues regarding DCNNs for object\nrecognition problems are then discussed. We caution the readers that the views\nexpressed in this paper are from the authors and authors only! \n\n"}
{"id": "1602.00715", "contents": "Title: Algorithm-Induced Prior for Image Restoration Abstract: This paper studies a type of image priors that are constructed implicitly\nthrough the alternating direction method of multiplier (ADMM) algorithm, called\nthe algorithm-induced prior. Different from classical image priors which are\ndefined before running the reconstruction algorithm, algorithm-induced priors\nare defined by the denoising procedure used to replace one of the two modules\nin the ADMM algorithm. Since such prior is not explicitly defined, analyzing\nthe performance has been difficult in the past.\n  Focusing on the class of symmetric smoothing filters, this paper presents an\nexplicit expression of the prior induced by the ADMM algorithm. The new prior\nis reminiscent to the conventional graph Laplacian but with stronger\nreconstruction performance. It can also be shown that the overall\nreconstruction has an efficient closed-form implementation if the associated\nsymmetric smoothing filter is low rank. The results are validated with\nexperiments on image inpainting. \n\n"}
{"id": "1602.02130", "contents": "Title: Sub-cortical brain structure segmentation using F-CNN's Abstract: In this paper we propose a deep learning approach for segmenting sub-cortical\nstructures of the human brain in Magnetic Resonance (MR) image data. We draw\ninspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN)\narchitecture for semantic segmentation of objects in natural images, and adapt\nit to our task. Unlike previous CNN-based methods that operate on image\npatches, our model is applied on a full blown 2D image, without any alignment\nor registration steps at testing time. We further improve segmentation results\nby interpreting the CNN output as potentials of a Markov Random Field (MRF),\nwhose topology corresponds to a volumetric grid. Alpha-expansion is used to\nperform approximate inference imposing spatial volumetric homogeneity to the\nCNN priors. We compare the performance of the proposed pipeline with a similar\nsystem using Random Forest-based priors, as well as state-of-art segmentation\nalgorithms, and show promising results on two different brain MRI datasets. \n\n"}
{"id": "1602.03409", "contents": "Title: Deep Convolutional Neural Networks for Computer-Aided Detection: CNN\n  Architectures, Dataset Characteristics and Transfer Learning Abstract: Remarkable progress has been made in image recognition, primarily due to the\navailability of large-scale annotated datasets and the revival of deep CNN.\nCNNs enable learning data-driven, highly representative, layered hierarchical\nimage features from sufficient training data. However, obtaining datasets as\ncomprehensively annotated as ImageNet in the medical imaging domain remains a\nchallenge. There are currently three major techniques that successfully employ\nCNNs to medical image classification: training the CNN from scratch, using\noff-the-shelf pre-trained CNN features, and conducting unsupervised CNN\npre-training with supervised fine-tuning. Another effective method is transfer\nlearning, i.e., fine-tuning CNN models pre-trained from natural image dataset\nto medical image tasks. In this paper, we exploit three important, but\npreviously understudied factors of employing deep convolutional neural networks\nto computer-aided detection problems. We first explore and evaluate different\nCNN architectures. The studied models contain 5 thousand to 160 million\nparameters, and vary in numbers of layers. We then evaluate the influence of\ndataset scale and spatial image context on performance. Finally, we examine\nwhen and why transfer learning from pre-trained ImageNet (via fine-tuning) can\nbe useful. We study two specific computer-aided detection (CADe) problems,\nnamely thoraco-abdominal lymph node (LN) detection and interstitial lung\ndisease (ILD) classification. We achieve the state-of-the-art performance on\nthe mediastinal LN detection, with 85% sensitivity at 3 false positive per\npatient, and report the first five-fold cross-validation classification results\non predicting axial CT slices with ILD categories. Our extensive empirical\nevaluation, CNN model analysis and valuable insights can be extended to the\ndesign of high performance CAD systems for other medical imaging tasks. \n\n"}
{"id": "1602.07261", "contents": "Title: Inception-v4, Inception-ResNet and the Impact of Residual Connections on\n  Learning Abstract: Very deep convolutional networks have been central to the largest advances in\nimage recognition performance in recent years. One example is the Inception\narchitecture that has been shown to achieve very good performance at relatively\nlow computational cost. Recently, the introduction of residual connections in\nconjunction with a more traditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar to the\nlatest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly. There\nis also some evidence of residual Inception networks outperforming similarly\nexpensive Inception networks without residual connections by a thin margin. We\nalso present several new streamlined architectures for both residual and\nnon-residual Inception networks. These variations improve the single-frame\nrecognition performance on the ILSVRC 2012 classification task significantly.\nWe further demonstrate how proper activation scaling stabilizes the training of\nvery wide residual Inception networks. With an ensemble of three residual and\none Inception-v4, we achieve 3.08 percent top-5 error on the test set of the\nImageNet classification (CLS) challenge \n\n"}
{"id": "1602.08405", "contents": "Title: We don't need no bounding-boxes: Training object class detectors using\n  only human verification Abstract: Training object class detectors typically requires a large set of images in\nwhich objects are annotated by bounding-boxes. However, manually drawing\nbounding-boxes is very time consuming. We propose a new scheme for training\nobject detectors which only requires annotators to verify bounding-boxes\nproduced automatically by the learning algorithm. Our scheme iterates between\nre-training the detector, re-localizing objects in the training images, and\nhuman verification. We use the verification signal both to improve re-training\nand to reduce the search space for re-localisation, which makes these steps\ndifferent to what is normally done in a weakly supervised setting. Extensive\nexperiments on PASCAL VOC 2007 show that (1) using human verification to update\ndetectors and reduce the search space leads to the rapid production of\nhigh-quality bounding-box annotations; (2) our scheme delivers detectors\nperforming almost as good as those trained in a fully supervised setting,\nwithout ever drawing any bounding-box; (3) as the verification task is very\nquick, our scheme substantially reduces total annotation time by a factor\n6x-9x. \n\n"}
{"id": "1603.00550", "contents": "Title: Synthesized Classifiers for Zero-Shot Learning Abstract: Given semantic descriptions of object classes, zero-shot learning aims to\naccurately recognize objects of the unseen classes, from which no examples are\navailable at the training stage, by associating them to the seen classes, from\nwhich labeled examples are provided. We propose to tackle this problem from the\nperspective of manifold learning. Our main idea is to align the semantic space\nthat is derived from external information to the model space that concerns\nitself with recognizing visual features. To this end, we introduce a set of\n\"phantom\" object classes whose coordinates live in both the semantic space and\nthe model space. Serving as bases in a dictionary, they can be optimized from\nlabeled data such that the synthesized real object classifiers achieve optimal\ndiscriminative performance. We demonstrate superior accuracy of our approach\nover the state of the art on four benchmark datasets for zero-shot learning,\nincluding the full ImageNet Fall 2011 dataset with more than 20,000 unseen\nclasses. \n\n"}
{"id": "1603.00831", "contents": "Title: MOT16: A Benchmark for Multi-Object Tracking Abstract: Standardized benchmarks are crucial for the majority of computer vision\napplications. Although leaderboards and ranking tables should not be\nover-claimed, benchmarks often provide the most objective measure of\nperformance and are therefore important guides for reseach.\n  Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was\nlaunched with the goal of collecting existing and new data and creating a\nframework for the standardized evaluation of multiple object tracking methods.\nThe first release of the benchmark focuses on multiple people tracking, since\npedestrians are by far the most studied object in the tracking community. This\npaper accompanies a new release of the MOTChallenge benchmark. Unlike the\ninitial release, all videos of MOT16 have been carefully annotated following a\nconsistent protocol. Moreover, it not only offers a significant increase in the\nnumber of labeled boxes, but also provides multiple object classes beside\npedestrians and the level of visibility for every single object of interest. \n\n"}
{"id": "1603.01249", "contents": "Title: HyperFace: A Deep Multi-task Learning Framework for Face Detection,\n  Landmark Localization, Pose Estimation, and Gender Recognition Abstract: We present an algorithm for simultaneous face detection, landmarks\nlocalization, pose estimation and gender recognition using deep convolutional\nneural networks (CNN). The proposed method called, HyperFace, fuses the\nintermediate layers of a deep CNN using a separate CNN followed by a multi-task\nlearning algorithm that operates on the fused features. It exploits the synergy\namong the tasks which boosts up their individual performances. Additionally, we\npropose two variants of HyperFace: (1) HyperFace-ResNet that builds on the\nResNet-101 model and achieves significant improvement in performance, and (2)\nFast-HyperFace that uses a high recall fast face detector for generating region\nproposals to improve the speed of the algorithm. Extensive experiments show\nthat the proposed models are able to capture both global and local information\nin faces and performs significantly better than many competitive algorithms for\neach of these four tasks. \n\n"}
{"id": "1603.01249", "contents": "Title: HyperFace: A Deep Multi-task Learning Framework for Face Detection,\n  Landmark Localization, Pose Estimation, and Gender Recognition Abstract: We present an algorithm for simultaneous face detection, landmarks\nlocalization, pose estimation and gender recognition using deep convolutional\nneural networks (CNN). The proposed method called, HyperFace, fuses the\nintermediate layers of a deep CNN using a separate CNN followed by a multi-task\nlearning algorithm that operates on the fused features. It exploits the synergy\namong the tasks which boosts up their individual performances. Additionally, we\npropose two variants of HyperFace: (1) HyperFace-ResNet that builds on the\nResNet-101 model and achieves significant improvement in performance, and (2)\nFast-HyperFace that uses a high recall fast face detector for generating region\nproposals to improve the speed of the algorithm. Extensive experiments show\nthat the proposed models are able to capture both global and local information\nin faces and performs significantly better than many competitive algorithms for\neach of these four tasks. \n\n"}
{"id": "1603.01417", "contents": "Title: Dynamic Memory Networks for Visual and Textual Question Answering Abstract: Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision. \n\n"}
{"id": "1603.02518", "contents": "Title: A New Method to Visualize Deep Neural Networks Abstract: We present a method for visualising the response of a deep neural network to\na specific input. For image data for instance our method will highlight areas\nthat provide evidence in favor of, and against choosing a certain class. The\nmethod overcomes several shortcomings of previous methods and provides great\nadditional insight into the decision making process of convolutional networks,\nwhich is important both to improve models and to accelerate the adoption of\nsuch methods in e.g. medicine. In experiments on ImageNet data, we illustrate\nhow the method works and can be applied in different ways to understand deep\nneural nets. \n\n"}
{"id": "1603.02649", "contents": "Title: A regularization-based approach for unsupervised image segmentation Abstract: We propose a novel unsupervised image segmentation algorithm, which aims to\nsegment an image into several coherent parts. It requires no user input, no\nsupervised learning phase and assumes an unknown number of segments. It\nachieves this by first over-segmenting the image into several hundred\nsuperpixels. These are iteratively joined on the basis of a discriminative\nclassifier trained on color and texture information obtained from each\nsuperpixel. The output of the classifier is regularized by a Markov random\nfield that lends more influence to neighbouring superpixels that are more\nsimilar. In each iteration, similar superpixels fall under the same label,\nuntil only a few coherent regions remain in the image. The algorithm was tested\non a standard evaluation data set, where it performs on par with\nstate-of-the-art algorithms in term of precision and greatly outperforms the\nstate of the art by reducing the oversegmentation of the object of interest. \n\n"}
{"id": "1603.04530", "contents": "Title: Object Contour Detection with a Fully Convolutional Encoder-Decoder\n  Network Abstract: We develop a deep learning algorithm for contour detection with a fully\nconvolutional encoder-decoder network. Different from previous low-level edge\ndetection, our algorithm focuses on detecting higher-level object contours. Our\nnetwork is trained end-to-end on PASCAL VOC with refined ground truth from\ninaccurate polygon annotations, yielding much higher precision in object\ncontour detection than previous methods. We find that the learned model\ngeneralizes well to unseen object classes from the same super-categories on MS\nCOCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning.\nBy combining with the multiscale combinatorial grouping algorithm, our method\ncan generate high-quality segmented object proposals, which significantly\nadvance the state-of-the-art on PASCAL VOC (improving average recall from 0.62\nto 0.67) with a relatively small amount of candidates ($\\sim$1660 per image). \n\n"}
{"id": "1603.05027", "contents": "Title: Identity Mappings in Deep Residual Networks Abstract: Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers \n\n"}
{"id": "1603.06937", "contents": "Title: Stacked Hourglass Networks for Human Pose Estimation Abstract: This work introduces a novel convolutional network architecture for the task\nof human pose estimation. Features are processed across all scales and\nconsolidated to best capture the various spatial relationships associated with\nthe body. We show how repeated bottom-up, top-down processing used in\nconjunction with intermediate supervision is critical to improving the\nperformance of the network. We refer to the architecture as a \"stacked\nhourglass\" network based on the successive steps of pooling and upsampling that\nare done to produce a final set of predictions. State-of-the-art results are\nachieved on the FLIC and MPII benchmarks outcompeting all recent methods. \n\n"}
{"id": "1603.07057", "contents": "Title: Do We Really Need to Collect Millions of Faces for Effective Face\n  Recognition? Abstract: Face recognition capabilities have recently made extraordinary leaps. Though\nthis progress is at least partially due to ballooning training set sizes --\nhuge numbers of face images downloaded and labeled for identity -- it is not\nclear if the formidable task of collecting so many images is truly necessary.\nWe propose a far more accessible means of increasing training data sizes for\nface recognition systems. Rather than manually harvesting and labeling more\nfaces, we simply synthesize them. We describe novel methods of enriching an\nexisting dataset with important facial appearance variations by manipulating\nthe faces it contains. We further apply this synthesis approach when matching\nquery images represented using a standard convolutional neural network. The\neffect of training and testing with synthesized images is extensively tested on\nthe LFW and IJB-A (verification and identification) benchmarks and Janus CS2.\nThe performances obtained by our approach match state of the art results\nreported by systems trained on millions of downloaded images. \n\n"}
{"id": "1603.08212", "contents": "Title: Human Pose Estimation using Deep Consensus Voting Abstract: In this paper we consider the problem of human pose estimation from a single\nstill image. We propose a novel approach where each location in the image votes\nfor the position of each keypoint using a convolutional neural net. The voting\nscheme allows us to utilize information from the whole image, rather than rely\non a sparse set of keypoint locations. Using dense, multi-target votes, not\nonly produces good keypoint predictions, but also enables us to compute\nimage-dependent joint keypoint probabilities by looking at consensus voting.\nThis differs from most previous methods where joint probabilities are learned\nfrom relative keypoint locations and are independent of the image. We finally\ncombine the keypoints votes and joint probabilities in order to identify the\noptimal pose configuration. We show our competitive performance on the MPII\nHuman Pose and Leeds Sports Pose datasets. \n\n"}
{"id": "1603.08367", "contents": "Title: Sparse Activity and Sparse Connectivity in Supervised Learning Abstract: Sparseness is a useful regularizer for learning in a wide range of\napplications, in particular in neural networks. This paper proposes a model\ntargeted at classification tasks, where sparse activity and sparse connectivity\nare used to enhance classification capabilities. The tool for achieving this is\na sparseness-enforcing projection operator which finds the closest vector with\na pre-defined sparseness for any given vector. In the theoretical part of this\npaper, a comprehensive theory for such a projection is developed. In\nconclusion, it is shown that the projection is differentiable almost everywhere\nand can thus be implemented as a smooth neuronal transfer function. The entire\nmodel can hence be tuned end-to-end using gradient-based methods. Experiments\non the MNIST database of handwritten digits show that classification\nperformance can be boosted by sparse activity or sparse connectivity. With a\ncombination of both, performance can be significantly better compared to\nclassical non-sparse approaches. \n\n"}
{"id": "1603.08597", "contents": "Title: The Conditional Lucas & Kanade Algorithm Abstract: The Lucas & Kanade (LK) algorithm is the method of choice for efficient dense\nimage and object alignment. The approach is efficient as it attempts to model\nthe connection between appearance and geometric displacement through a linear\nrelationship that assumes independence across pixel coordinates. A drawback of\nthe approach, however, is its generative nature. Specifically, its performance\nis tightly coupled with how well the linear model can synthesize appearance\nfrom geometric displacement, even though the alignment task itself is\nassociated with the inverse problem. In this paper, we present a new approach,\nreferred to as the Conditional LK algorithm, which: (i) directly learns linear\nmodels that predict geometric displacement as a function of appearance, and\n(ii) employs a novel strategy for ensuring that the generative pixel\nindependence assumption can still be taken advantage of. We demonstrate that\nour approach exhibits superior performance to classical generative forms of the\nLK algorithm. Furthermore, we demonstrate its comparable performance to\nstate-of-the-art methods such as the Supervised Descent Method with\nsubstantially less training examples, as well as the unique ability to \"swap\"\ngeometric warp functions without having to retrain from scratch. Finally, from\na theoretical perspective, our approach hints at possible redundancies that\nexist in current state-of-the-art methods for alignment that could be leveraged\nin vision systems of the future. \n\n"}
{"id": "1603.08895", "contents": "Title: Latent Embeddings for Zero-shot Classification Abstract: We present a novel latent embedding model for learning a compatibility\nfunction between image and class embeddings, in the context of zero-shot\nclassification. The proposed method augments the state-of-the-art bilinear\ncompatibility model by incorporating latent variables. Instead of learning a\nsingle bilinear map, it learns a collection of maps with the selection, of\nwhich map to use, being a latent variable for the current image-class pair. We\ntrain the model with a ranking based objective function which penalizes\nincorrect rankings of the true class for a given image. We empirically\ndemonstrate that our model improves the state-of-the-art for various class\nembeddings consistently on three challenging publicly available datasets for\nthe zero-shot setting. Moreover, our method leads to visually highly\ninterpretable results with clear clusters of different fine-grained object\nproperties that correspond to different latent variable maps. \n\n"}
{"id": "1603.09065", "contents": "Title: Structured Feature Learning for Pose Estimation Abstract: In this paper, we propose a structured feature learning framework to reason\nthe correlations among body joints at the feature level in human pose\nestimation. Different from existing approaches of modelling structures on score\nmaps or predicted labels, feature maps preserve substantially richer\ndescriptions of body joints. The relationships between feature maps of joints\nare captured with the introduced geometrical transform kernels, which can be\neasily implemented with a convolution layer. Features and their relationships\nare jointly learned in an end-to-end learning system. A bi-directional tree\nstructured model is proposed, so that the feature channels at a body joint can\nwell receive information from other joints. The proposed framework improves\nfeature learning substantially. With very simple post processing, it reaches\nthe best mean PCP on the LSP and FLIC datasets. Compared with the baseline of\nlearning features at each joint separately with ConvNet, the mean PCP has been\nimproved by 18% on FLIC. The code is released to the public. \n\n"}
{"id": "1604.01093", "contents": "Title: BundleFusion: Real-time Globally Consistent 3D Reconstruction using\n  On-the-fly Surface Re-integration Abstract: Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed\nreality and robotic applications. However, scalability brings challenges of\ndrift in pose estimation, introducing significant errors in the accumulated\nmodel. Approaches often require hours of offline processing to globally correct\nmodel errors. Recent online methods demonstrate compelling results, but suffer\nfrom: (1) needing minutes to perform online correction preventing true\nreal-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation\nresulting in many tracking failures; or (3) supporting only unstructured\npoint-based representations, which limit scan quality and applicability. We\nsystematically address these issues with a novel, real-time, end-to-end\nreconstruction framework. At its core is a robust pose estimation strategy,\noptimizing per frame for a global set of camera poses by considering the\ncomplete history of RGB-D input with an efficient hierarchical approach. We\nremove the heavy reliance on temporal tracking, and continually localize to the\nglobally optimized frames instead. We contribute a parallelizable optimization\nframework, which employs correspondences based on sparse features and dense\ngeometric and photometric matching. Our approach estimates globally optimized\n(i.e., bundle adjusted) poses in real-time, supports robust tracking with\nrecovery from gross tracking failures (i.e., relocalization), and re-estimates\nthe 3D model in real-time to ensure global consistency; all within a single\nframework. Our approach outperforms state-of-the-art online systems with\nquality on par to offline methods, but with unprecedented speed and scan\ncompleteness. Our framework leads to a comprehensive online scanning solution\nfor large indoor environments, enabling ease of use and high-quality results. \n\n"}
{"id": "1604.01325", "contents": "Title: Deep Image Retrieval: Learning global representations for image search Abstract: We propose a novel approach for instance-level image retrieval. It produces a\nglobal and compact fixed-length representation for each image by aggregating\nmany region-wise descriptors. In contrast to previous works employing\npre-trained deep networks as a black box to produce features, our method\nleverages a deep architecture trained for the specific task of image retrieval.\nOur contribution is twofold: (i) we leverage a ranking framework to learn\nconvolution and projection weights that are used to build the region features;\nand (ii) we employ a region proposal network to learn which regions should be\npooled to form the final global descriptor. We show that using clean training\ndata is key to the success of our approach. To that aim, we use a large scale\nbut noisy landmark dataset and develop an automatic cleaning approach. The\nproposed architecture produces a global image representation in a single\nforward pass. Our approach significantly outperforms previous approaches based\non global descriptors on standard datasets. It even surpasses most prior works\nbased on costly local descriptor indexing and spatial verification. Additional\nmaterial is available at www.xrce.xerox.com/Deep-Image-Retrieval. \n\n"}
{"id": "1604.02376", "contents": "Title: Finding Optimal Combination of Kernels using Genetic Programming Abstract: In Computer Vision, problem of identifying or classifying the objects present\nin an image is called Object Categorization. It is a challenging problem,\nespecially when the images have clutter background, occlusions or different\nlighting conditions. Many vision features have been proposed which aid object\ncategorization even in such adverse conditions. Past research has shown that,\nemploying multiple features rather than any single features leads to better\nrecognition. Multiple Kernel Learning (MKL) framework has been developed for\nlearning an optimal combination of features for object categorization. Existing\nMKL methods use linear combination of base kernels which may not be optimal for\nobject categorization. Real-world object categorization may need to consider\ncomplex combination of kernels(non-linear) and not only linear combination.\nEvolving non-linear functions of base kernels using Genetic Programming is\nproposed in this report. Experiment results show that non-kernel generated\nusing genetic programming gives good accuracy as compared to linear combination\nof kernels. \n\n"}
{"id": "1604.02885", "contents": "Title: Semantic 3D Reconstruction with Continuous Regularization and Ray\n  Potentials Using a Visibility Consistency Constraint Abstract: We propose an approach for dense semantic 3D reconstruction which uses a data\nterm that is defined as potentials over viewing rays, combined with continuous\nsurface area penalization. Our formulation is a convex relaxation which we\naugment with a crucial non-convex constraint that ensures exact handling of\nvisibility. To tackle the non-convex minimization problem, we propose a\nmajorize-minimize type strategy which converges to a critical point. We\ndemonstrate the benefits of using the non-convex constraint experimentally. For\nthe geometry-only case, we set a new state of the art on two datasets of the\ncommonly used Middlebury multi-view stereo benchmark. Moreover, our\ngeneral-purpose formulation directly reconstructs thin objects, which are\nusually treated with specialized algorithms. A qualitative evaluation on the\ndense semantic 3D reconstruction task shows that we improve significantly over\nprevious methods. \n\n"}
{"id": "1604.03426", "contents": "Title: Sweep Distortion Removal from THz Images via Blind Demodulation Abstract: Heavy sweep distortion induced by alignments and inter-reflections of layers\nof a sample is a major burden in recovering 2D and 3D information in time\nresolved spectral imaging. This problem cannot be addressed by conventional\ndenoising and signal processing techniques as it heavily depends on the physics\nof the acquisition. Here we propose and implement an algorithmic framework\nbased on low-rank matrix recovery and alternating minimization that exploits\nthe forward model for THz acquisition. The method allows recovering the\noriginal signal in spite of the presence of temporal-spatial distortions. We\naddress a blind-demodulation problem, where based on several observations of\nthe sample texture modulated by an undesired sweep pattern, the two classes of\nsignals are separated. The performance of the method is examined in both\nsynthetic and experimental data, and the successful reconstructions are\ndemonstrated. The proposed general scheme can be implemented to advance\ninspection and imaging applications in THz and other time-resolved sensing\nmodalities. \n\n"}
{"id": "1604.04339", "contents": "Title: High-performance Semantic Segmentation Using Very Deep Fully\n  Convolutional Networks Abstract: We propose a method for high-performance semantic image segmentation (or\nsemantic pixel labelling) based on very deep residual networks, which achieves\nthe state-of-the-art performance. A few design factors are carefully considered\nto this end.\n  We make the following contributions. (i) First, we evaluate different\nvariations of a fully convolutional residual network so as to find the best\nconfiguration, including the number of layers, the resolution of feature maps,\nand the size of field-of-view. Our experiments show that further enlarging the\nfield-of-view and increasing the resolution of feature maps are typically\nbeneficial, which however inevitably leads to a higher demand for GPU memories.\nTo walk around the limitation, we propose a new method to simulate a high\nresolution network with a low resolution network, which can be applied during\ntraining and/or testing. (ii) Second, we propose an online bootstrapping method\nfor training. We demonstrate that online bootstrapping is critically important\nfor achieving good accuracy. (iii) Third we apply the traditional dropout to\nsome of the residual blocks, which further improves the performance. (iv)\nFinally, our method achieves the currently best mean intersection-over-union\n78.3\\% on the PASCAL VOC 2012 dataset, as well as on the recent dataset\nCityscapes. \n\n"}
{"id": "1604.04473", "contents": "Title: Probing the Intra-Component Correlations within Fisher Vector for\n  Material Classification Abstract: Fisher vector (FV) has become a popular image representation. One notable\nunderlying assumption of the FV framework is that local descriptors are well\ndecorrelated within each cluster so that the covariance matrix for each\nGaussian can be simplified to be diagonal. Though the FV usually relies on the\nPrincipal Component Analysis (PCA) to decorrelate local features, the PCA is\napplied to the entire training data and hence it only diagonalizes the\n\\textit{universal} covariance matrix, rather than those w.r.t. the local\ncomponents. As a result, the local decorrelation assumption is usually not\nsupported in practice.\n  To relax this assumption, this paper proposes a completed model of the Fisher\nvector, which is termed as the Completed Fisher vector (CFV). The CFV is a more\ngeneral framework of the FV, since it encodes not only the variances but also\nthe correlations of the whitened local descriptors. The CFV thus leads to\nimproved discriminative power. We take the task of material categorization as\nan example and experimentally show that: 1) the CFV outperforms the FV under\nall parameter settings; 2) the CFV is robust to the changes in the number of\ncomponents in the mixture; 3) even with a relatively small visual vocabulary\nthe CFV still works well on two challenging datasets. \n\n"}
{"id": "1604.04653", "contents": "Title: Bags of Local Convolutional Features for Scalable Instance Search Abstract: This work proposes a simple instance retrieval pipeline based on encoding the\nconvolutional features of CNN using the bag of words aggregation scheme (BoW).\nAssigning each local array of activations in a convolutional layer to a visual\nword produces an \\textit{assignment map}, a compact representation that relates\nregions of an image with a visual word. We use the assignment map for fast\nspatial reranking, obtaining object localizations that are used for query\nexpansion. We demonstrate the suitability of the BoW representation based on\nlocal CNN features for instance retrieval, achieving competitive performance on\nthe Oxford and Paris buildings benchmarks. We show that our proposed system for\nCNN feature aggregation with BoW outperforms state-of-the-art techniques using\nsum pooling at a subset of the challenging TRECVid INS benchmark. \n\n"}
{"id": "1604.06433", "contents": "Title: Walk and Learn: Facial Attribute Representation Learning from Egocentric\n  Video and Contextual Data Abstract: The way people look in terms of facial attributes (ethnicity, hair color,\nfacial hair, etc.) and the clothes or accessories they wear (sunglasses, hat,\nhoodies, etc.) is highly dependent on geo-location and weather condition,\nrespectively. This work explores, for the first time, the use of this\ncontextual information, as people with wearable cameras walk across different\nneighborhoods of a city, in order to learn a rich feature representation for\nfacial attribute classification, without the costly manual annotation required\nby previous methods. By tracking the faces of casual walkers on more than 40\nhours of egocentric video, we are able to cover tens of thousands of different\nidentities and automatically extract nearly 5 million pairs of images connected\nby or from different face tracks, along with their weather and location\ncontext, under pose and lighting variations. These image pairs are then fed\ninto a deep network that preserves similarity of images connected by the same\ntrack, in order to capture identity-related attribute features, and optimizes\nfor location and weather prediction to capture additional facial attribute\nfeatures. Finally, the network is fine-tuned with manually annotated samples.\nWe perform an extensive experimental analysis on wearable data and two standard\nbenchmark datasets based on web images (LFWA and CelebA). Our method\noutperforms by a large margin a network trained from scratch. Moreover, even\nwithout using manually annotated identity labels for pre-training as in\nprevious methods, our approach achieves results that are better than the state\nof the art. \n\n"}
{"id": "1604.06486", "contents": "Title: Humans and deep networks largely agree on which kinds of variation make\n  object recognition harder Abstract: View-invariant object recognition is a challenging problem, which has\nattracted much attention among the psychology, neuroscience, and computer\nvision communities. Humans are notoriously good at it, even if some variations\nare presumably more difficult to handle than others (e.g. 3D rotations). Humans\nare thought to solve the problem through hierarchical processing along the\nventral stream, which progressively extracts more and more invariant visual\nfeatures. This feed-forward architecture has inspired a new generation of\nbio-inspired computer vision systems called deep convolutional neural networks\n(DCNN), which are currently the best algorithms for object recognition in\nnatural images. Here, for the first time, we systematically compared human\nfeed-forward vision and DCNNs at view-invariant object recognition using the\nsame images and controlling for both the kinds of transformation as well as\ntheir magnitude. We used four object categories and images were rendered from\n3D computer models. In total, 89 human subjects participated in 10 experiments\nin which they had to discriminate between two or four categories after rapid\npresentation with backward masking. We also tested two recent DCNNs on the same\ntasks. We found that humans and DCNNs largely agreed on the relative\ndifficulties of each kind of variation: rotation in depth is by far the hardest\ntransformation to handle, followed by scale, then rotation in plane, and\nfinally position. This suggests that humans recognize objects mainly through 2D\ntemplate matching, rather than by constructing 3D object models, and that DCNNs\nare not too unreasonable models of human feed-forward vision. Also, our results\nshow that the variation levels in rotation in depth and scale strongly modulate\nboth humans' and DCNNs' recognition performances. We thus argue that these\nvariations should be controlled in the image datasets used in vision research. \n\n"}
{"id": "1604.07669", "contents": "Title: Real-time Action Recognition with Enhanced Motion Vector CNNs Abstract: The deep two-stream architecture exhibited excellent performance on video\nbased action recognition. The most computationally expensive step in this\napproach comes from the calculation of optical flow which prevents it to be\nreal-time. This paper accelerates this architecture by replacing optical flow\nwith motion vector which can be obtained directly from compressed videos\nwithout extra calculation. However, motion vector lacks fine structures, and\ncontains noisy and inaccurate motion patterns, leading to the evident\ndegradation of recognition performance. Our key insight for relieving this\nproblem is that optical flow and motion vector are inherent correlated.\nTransferring the knowledge learned with optical flow CNN to motion vector CNN\ncan significantly boost the performance of the latter. Specifically, we\nintroduce three strategies for this, initialization transfer, supervision\ntransfer and their combination. Experimental results show that our method\nachieves comparable recognition performance to the state-of-the-art, while our\nmethod can process 390.7 frames per second, which is 27 times faster than the\noriginal two-stream method. \n\n"}
{"id": "1605.00052", "contents": "Title: InterActive: Inter-Layer Activeness Propagation Abstract: An increasing number of computer vision tasks can be tackled with deep\nfeatures, which are the intermediate outputs of a pre-trained Convolutional\nNeural Network. Despite the astonishing performance, deep features extracted\nfrom low-level neurons are still below satisfaction, arguably because they\ncannot access the spatial context contained in the higher layers. In this\npaper, we present InterActive, a novel algorithm which computes the activeness\nof neurons and network connections. Activeness is propagated through a neural\nnetwork in a top-down manner, carrying high-level context and improving the\ndescriptive power of low-level and mid-level neurons. Visualization indicates\nthat neuron activeness can be interpreted as spatial-weighted neuron responses.\nWe achieve state-of-the-art classification performance on a wide range of image\ndatasets. \n\n"}
{"id": "1605.02346", "contents": "Title: Chained Predictions Using Convolutional Neural Networks Abstract: In this paper, we present an adaptation of the sequence-to-sequence model for\nstructured output prediction in vision tasks. In this model the output\nvariables for a given input are predicted sequentially using neural networks.\nThe prediction for each output variable depends not only on the input but also\non the previously predicted output variables. The model is applied to spatial\nlocalization tasks and uses convolutional neural networks (CNNs) for processing\ninput images and a multi-scale deconvolutional architecture for making spatial\npredictions at each time step. We explore the impact of weight sharing with a\nrecurrent connection matrix between consecutive predictions, and compare it to\na formulation where these weights are not tied. Untied weights are particularly\nsuited for problems with a fixed sized structure, where different classes of\noutput are predicted in different steps. We show that chained predictions\nachieve top performing results on human pose estimation from single images and\nvideos. \n\n"}
{"id": "1605.02677", "contents": "Title: Building a Large Scale Dataset for Image Emotion Recognition: The Fine\n  Print and The Benchmark Abstract: Psychological research results have confirmed that people can have different\nemotional reactions to different visual stimuli. Several papers have been\npublished on the problem of visual emotion analysis. In particular, attempts\nhave been made to analyze and predict people's emotional reaction towards\nimages. To this end, different kinds of hand-tuned features are proposed. The\nresults reported on several carefully selected and labeled small image data\nsets have confirmed the promise of such features. While the recent successes of\nmany computer vision related tasks are due to the adoption of Convolutional\nNeural Networks (CNNs), visual emotion analysis has not achieved the same level\nof success. This may be primarily due to the unavailability of confidently\nlabeled and relatively large image data sets for visual emotion analysis. In\nthis work, we introduce a new data set, which started from 3+ million weakly\nlabeled images of different emotions and ended up 30 times as large as the\ncurrent largest publicly available visual emotion data set. We hope that this\ndata set encourages further research on visual emotion analysis. We also\nperform extensive benchmarking analyses on this large data set using the state\nof the art methods including CNNs. \n\n"}
{"id": "1605.04988", "contents": "Title: Going Deeper into Action Recognition: A Survey Abstract: Understanding human actions in visual data is tied to advances in\ncomplementary research areas including object recognition, human dynamics,\ndomain adaptation and semantic segmentation. Over the last decade, human action\nanalysis evolved from earlier schemes that are often limited to controlled\nenvironments to nowadays advanced solutions that can learn from millions of\nvideos and apply to almost all daily activities. Given the broad range of\napplications from video surveillance to human-computer interaction, scientific\nmilestones in action recognition are achieved more rapidly, eventually leading\nto the demise of what used to be good in a short time. This motivated us to\nprovide a comprehensive review of the notable steps taken towards recognizing\nhuman actions. To this end, we start our discussion with the pioneering methods\nthat use handcrafted representations, and then, navigate into the realm of deep\nlearning based approaches. We aim to remain objective throughout this survey,\ntouching upon encouraging improvements as well as inevitable fallbacks, in the\nhope of raising fresh questions and motivating new research directions for the\nreader. \n\n"}
{"id": "1605.05395", "contents": "Title: Learning Deep Representations of Fine-grained Visual Descriptions Abstract: State-of-the-art methods for zero-shot visual recognition formulate learning\nas a joint embedding problem of images and side information. In these\nformulations the current best complement to visual features are attributes:\nmanually encoded vectors describing shared characteristics among categories.\nDespite good performance, attributes have limitations: (1) finer-grained\nrecognition requires commensurately more attributes, and (2) attributes do not\nprovide a natural language interface. We propose to overcome these limitations\nby training neural language models from scratch; i.e. without pre-training and\nonly consuming words and characters. Our proposed models train end-to-end to\nalign with the fine-grained and category-specific content of images. Natural\nlanguage provides a flexible and compact way of encoding only the salient\nvisual aspects for distinguishing categories. By training on raw text, our\nmodel can do inference on raw text as well, providing humans a familiar mode\nboth for annotation and retrieval. Our model achieves strong performance on\nzero-shot text-based image retrieval and significantly outperforms the\nattribute-based state-of-the-art for zero-shot classification on the Caltech\nUCSD Birds 200-2011 dataset. \n\n"}
{"id": "1605.06083", "contents": "Title: Stereotyping and Bias in the Flickr30K Dataset Abstract: An untested assumption behind the crowdsourced descriptions of the images in\nthe Flickr30K dataset (Young et al., 2014) is that they \"focus only on the\ninformation that can be obtained from the image alone\" (Hodosh et al., 2013, p.\n859). This paper presents some evidence against this assumption, and provides a\nlist of biases and unwarranted inferences that can be found in the Flickr30K\ndataset. Finally, it considers methods to find examples of these, and discusses\nhow we should deal with stereotype-driven descriptions in future applications. \n\n"}
{"id": "1605.06240", "contents": "Title: FPNN: Field Probing Neural Networks for 3D Data Abstract: Building discriminative representations for 3D data has been an important\ntask in computer graphics and computer vision research. Convolutional Neural\nNetworks (CNNs) have shown to operate on 2D images with great success for a\nvariety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a\nplausible and promising next step. Unfortunately, the computational complexity\nof 3D CNNs grows cubically with respect to voxel resolution. Moreover, since\nmost 3D geometry representations are boundary based, occupied regions do not\nincrease proportionately with the size of the discretization, resulting in\nwasted computation. In this work, we represent 3D spaces as volumetric fields,\nand propose a novel design that employs field probing filters to efficiently\nextract features from them. Each field probing filter is a set of probing\npoints --- sensors that perceive the space. Our learning algorithm optimizes\nnot only the weights associated with the probing points, but also their\nlocations, which deforms the shape of the probing filters and adaptively\ndistributes them in 3D space. The optimized probing points sense the 3D space\n\"intelligently\", rather than operating blindly over the entire domain. We show\nthat field probing is significantly more efficient than 3DCNNs, while providing\nstate-of-the-art performance, on classification tasks for 3D object recognition\nbenchmark datasets. \n\n"}
{"id": "1605.06409", "contents": "Title: R-FCN: Object Detection via Region-based Fully Convolutional Networks Abstract: We present region-based, fully convolutional networks for accurate and\nefficient object detection. In contrast to previous region-based detectors such\nas Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of\ntimes, our region-based detector is fully convolutional with almost all\ncomputation shared on the entire image. To achieve this goal, we propose\nposition-sensitive score maps to address a dilemma between\ntranslation-invariance in image classification and translation-variance in\nobject detection. Our method can thus naturally adopt fully convolutional image\nclassifier backbones, such as the latest Residual Networks (ResNets), for\nobject detection. We show competitive results on the PASCAL VOC datasets (e.g.,\n83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is\nachieved at a test-time speed of 170ms per image, 2.5-20x faster than the\nFaster R-CNN counterpart. Code is made publicly available at:\nhttps://github.com/daijifeng001/r-fcn \n\n"}
{"id": "1605.07824", "contents": "Title: Action Classification via Concepts and Attributes Abstract: Classes in natural images tend to follow long tail distributions. This is\nproblematic when there are insufficient training examples for rare classes.\nThis effect is emphasized in compound classes, involving the conjunction of\nseveral concepts, such as those appearing in action-recognition datasets. In\nthis paper, we propose to address this issue by learning how to utilize common\nvisual concepts which are readily available. We detect the presence of\nprominent concepts in images and use them to infer the target labels instead of\nusing visual features directly, combining tools from vision and\nnatural-language processing. We validate our method on the recently introduced\nHICO dataset reaching a mAP of 31.54\\% and on the Stanford-40 Actions dataset,\nwhere the proposed method outperforms that obtained by direct visual features,\nobtaining an accuracy 83.12\\%. Moreover, the method provides for each class a\nsemantically meaningful list of keywords and relevant image regions relating it\nto its constituent concepts. \n\n"}
{"id": "1605.09211", "contents": "Title: Going Deeper for Multilingual Visual Sentiment Detection Abstract: This technical report details several improvements to the visual concept\ndetector banks built on images from the Multilingual Visual Sentiment Ontology\n(MVSO). The detector banks are trained to detect a total of 9,918\nsentiment-biased visual concepts from six major languages: English, Spanish,\nItalian, French, German and Chinese. In the original MVSO release,\nadjective-noun pair (ANP) detectors were trained for the six languages using an\nAlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a\nmore extensive set of experiments, parameter tuning, and training runs, we\ndetail and release higher accuracy models for detecting ANPs across six\nlanguages from the same image pool and setting as in the original release using\na more modern architecture, GoogLeNet, providing comparable or better\nperformance with reduced network parameter cost.\n  In addition, since the image pool in MVSO can be corrupted by user noise from\nsocial interactions, we partitioned out a sub-corpus of MVSO images based on\ntag-restricted queries for higher fidelity labels. We show that as a result of\nthese higher fidelity labels, higher performing AlexNet-styled ANP detectors\ncan be trained using the tag-restricted image subset as compared to the models\nin full corpus. We release all these newly trained models for public research\nuse along with the list of tag-restricted images from the MVSO dataset. \n\n"}
{"id": "1605.09507", "contents": "Title: Deep convolutional neural networks for predominant instrument\n  recognition in polyphonic music Abstract: Identifying musical instruments in polyphonic music recordings is a\nchallenging but important problem in the field of music information retrieval.\nIt enables music search by instrument, helps recognize musical genres, or can\nmake music transcription easier and more accurate. In this paper, we present a\nconvolutional neural network framework for predominant instrument recognition\nin real-world polyphonic music. We train our network from fixed-length music\nexcerpts with a single-labeled predominant instrument and estimate an arbitrary\nnumber of predominant instruments from an audio signal with a variable length.\nTo obtain the audio-excerpt-wise result, we aggregate multiple outputs from\nsliding windows over the test audio. In doing so, we investigated two different\naggregation methods: one takes the average for each instrument and the other\ntakes the instrument-wise sum followed by normalization. In addition, we\nconducted extensive experiments on several important factors that affect the\nperformance, including analysis window size, identification threshold, and\nactivation functions for neural networks to find the optimal set of parameters.\nUsing a dataset of 10k audio excerpts from 11 instruments for evaluation, we\nfound that convolutional neural networks are more robust than conventional\nmethods that exploit spectral features and source separation with support\nvector machines. Experimental results showed that the proposed convolutional\nnetwork architecture obtained an F1 measure of 0.602 for micro and 0.503 for\nmacro, respectively, achieving 19.6% and 16.4% in performance improvement\ncompared with other state-of-the-art algorithms. \n\n"}
{"id": "1605.09653", "contents": "Title: A Systematic Evaluation and Benchmark for Person Re-Identification:\n  Features, Metrics, and Datasets Abstract: Person re-identification (re-id) is a critical problem in video analytics\napplications such as security and surveillance. The public release of several\ndatasets and code for vision algorithms has facilitated rapid progress in this\narea over the last few years. However, directly comparing re-id algorithms\nreported in the literature has become difficult since a wide variety of\nfeatures, experimental protocols, and evaluation metrics are employed. In order\nto address this need, we present an extensive review and performance evaluation\nof single- and multi-shot re-id algorithms. The experimental protocol\nincorporates the most recent advances in both feature extraction and metric\nlearning. To ensure a fair comparison, all of the approaches were implemented\nusing a unified code library that includes 11 feature extraction algorithms and\n22 metric learning and ranking techniques. All approaches were evaluated using\na new large-scale dataset that closely mimics a real-world problem setting, in\naddition to 16 other publicly available datasets: VIPeR, GRID, CAVIAR,\nDukeMTMC4ReID, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK01, CHUK02, CUHK03,\nRAiD, iLIDSVID, HDA+ and Market1501. The evaluation codebase and results will\nbe made publicly available for community use. \n\n"}
{"id": "1606.02276", "contents": "Title: Multilingual Visual Sentiment Concept Matching Abstract: The impact of culture in visual emotion perception has recently captured the\nattention of multimedia research. In this study, we pro- vide powerful\ncomputational linguistics tools to explore, retrieve and browse a dataset of\n16K multilingual affective visual concepts and 7.3M Flickr images. First, we\ndesign an effective crowdsourc- ing experiment to collect human judgements of\nsentiment connected to the visual concepts. We then use word embeddings to\nrepre- sent these concepts in a low dimensional vector space, allowing us to\nexpand the meaning around concepts, and thus enabling insight about\ncommonalities and differences among different languages. We compare a variety\nof concept representations through a novel evaluation task based on the notion\nof visual semantic relatedness. Based on these representations, we design\nclustering schemes to group multilingual visual concepts, and evaluate them\nwith novel metrics based on the crowdsourced sentiment annotations as well as\nvisual semantic relatedness. The proposed clustering framework enables us to\nanalyze the full multilingual dataset in-depth and also show an application on\na facial data subset, exploring cultural in- sights of portrait-related\naffective visual concepts. \n\n"}
{"id": "1606.02467", "contents": "Title: Point-wise mutual information-based video segmentation with high\n  temporal consistency Abstract: In this paper, we tackle the problem of temporally consistent boundary\ndetection and hierarchical segmentation in videos. While finding the best\nhigh-level reasoning of region assignments in videos is the focus of much\nrecent research, temporal consistency in boundary detection has so far only\nrarely been tackled. We argue that temporally consistent boundaries are a key\ncomponent to temporally consistent region assignment. The proposed method is\nbased on the point-wise mutual information (PMI) of spatio-temporal voxels.\nTemporal consistency is established by an evaluation of PMI-based point\naffinities in the spectral domain over space and time. Thus, the proposed\nmethod is independent of any optical flow computation or previously learned\nmotion models. The proposed low-level video segmentation method outperforms the\nlearning-based state of the art in terms of standard region metrics. \n\n"}
{"id": "1606.03647", "contents": "Title: Training Recurrent Answering Units with Joint Loss Minimization for VQA Abstract: We propose a novel algorithm for visual question answering based on a\nrecurrent deep neural network, where every module in the network corresponds to\na complete answering unit with attention mechanism by itself. The network is\noptimized by minimizing loss aggregated from all the units, which share model\nparameters while receiving different information to compute attention\nprobability. For training, our model attends to a region within image feature\nmap, updates its memory based on the question and attended image feature, and\nanswers the question based on its memory state. This procedure is performed to\ncompute loss in each step. The motivation of this approach is our observation\nthat multi-step inferences are often required to answer questions while each\nproblem may have a unique desirable number of steps, which is difficult to\nidentify in practice. Hence, we always make the first unit in the network solve\nproblems, but allow it to learn the knowledge from the rest of units by\nbackpropagation unless it degrades the model. To implement this idea, we\nearly-stop training each unit as soon as it starts to overfit. Note that, since\nmore complex models tend to overfit on easier questions quickly, the last\nanswering unit in the unfolded recurrent neural network is typically killed\nfirst while the first one remains last. We make a single-step prediction for a\nnew question using the shared model. This strategy works better than the other\noptions within our framework since the selected model is trained effectively\nfrom all units without overfitting. The proposed algorithm outperforms other\nmulti-step attention based approaches using a single step prediction in VQA\ndataset. \n\n"}
{"id": "1606.04308", "contents": "Title: Richardson-Lucy Deblurring for Moving Light Field Cameras Abstract: We generalize Richardson-Lucy (RL) deblurring to 4-D light fields by\nreplacing the convolution steps with light field rendering of motion blur. The\nmethod deals correctly with blur caused by 6-degree-of-freedom camera motion in\ncomplex 3-D scenes, without performing depth estimation. We introduce a novel\nregularization term that maintains parallax information in the light field\nwhile reducing noise and ringing. We demonstrate the method operating\neffectively on rendered scenes and scenes captured using an off-the-shelf light\nfield camera. An industrial robot arm provides repeatable and known\ntrajectories, allowing us to establish quantitative performance in complex 3-D\nscenes. Qualitative and quantitative results confirm the effectiveness of the\nmethod, including commonly occurring cases for which previously published\nmethods fail. We include mathematical proof that the algorithm converges to the\nmaximum-likelihood estimate of the unblurred scene under Poisson noise. We\nexpect extension to blind methods to be possible following the generalization\nof 2-D Richardson-Lucy to blind deconvolution. \n\n"}
{"id": "1606.04404", "contents": "Title: End-to-End Comparative Attention Networks for Person Re-identification Abstract: Person re-identification across disjoint camera views has been widely applied\nin video surveillance yet it is still a challenging problem. One of the major\nchallenges lies in the lack of spatial and temporal cues, which makes it\ndifficult to deal with large variations of lighting conditions, viewing angles,\nbody poses and occlusions. Recently, several deep learning based person\nre-identification approaches have been proposed and achieved remarkable\nperformance. However, most of those approaches extract discriminative features\nfrom the whole frame at one glimpse without differentiating various parts of\nthe persons to identify. It is essentially important to examine multiple highly\ndiscriminative local regions of the person images in details through multiple\nglimpses for dealing with the large appearance variance. In this paper, we\npropose a new soft attention based model, i.e., the end to-end Comparative\nAttention Network (CAN), specifically tailored for the task of person\nre-identification. The end-to-end CAN learns to selectively focus on parts of\npairs of person images after taking a few glimpses of them and adaptively\ncomparing their appearance. The CAN model is able to learn which parts of\nimages are relevant for discerning persons and automatically integrates\ninformation from different parts to determine whether a pair of images belongs\nto the same person. In other words, our proposed CAN model simulates the human\nperception process to verify whether two images are from the same person.\nExtensive experiments on three benchmark person re-identification datasets,\nincluding CUHK01, CHUHK03 and Market-1501, clearly demonstrate that our\nproposed end-to-end CAN for person re-identification outperforms well\nestablished baselines significantly and offer new state-of-the-art performance. \n\n"}
{"id": "1606.06266", "contents": "Title: Detection and Tracking of Liquids with Fully Convolutional Networks Abstract: Recent advances in AI and robotics have claimed many incredible results with\ndeep learning, yet no work to date has applied deep learning to the problem of\nliquid perception and reasoning. In this paper, we apply fully-convolutional\ndeep neural networks to the tasks of detecting and tracking liquids. We\nevaluate three models: a single-frame network, multi-frame network, and a LSTM\nrecurrent network. Our results show that the best liquid detection results are\nachieved when aggregating data over multiple frames, in contrast to standard\nimage segmentation. They also show that the LSTM network outperforms the other\ntwo in both tasks. This suggests that LSTM-based neural networks have the\npotential to be a key component for enabling robots to handle liquids using\nrobust, closed-loop controllers. \n\n"}
{"id": "1606.07493", "contents": "Title: Sort Story: Sorting Jumbled Images and Captions into Stories Abstract: Temporal common sense has applications in AI tasks such as QA, multi-document\nsummarization, and human-AI communication. We propose the task of sequencing --\ngiven a jumbled set of aligned image-caption pairs that belong to a story, the\ntask is to sort them such that the output sequence forms a coherent story. We\npresent multiple approaches, via unary (position) and pairwise (order)\npredictions, and their ensemble-based combinations, achieving strong results on\nthis task. We use both text-based and image-based features, which depict\ncomplementary improvements. Using qualitative examples, we demonstrate that our\nmodels have learnt interesting aspects of temporal common sense. \n\n"}
{"id": "1607.02104", "contents": "Title: Zero-Shot Visual Recognition via Bidirectional Latent Embedding Abstract: Zero-shot learning for visual recognition, e.g., object and action\nrecognition, has recently attracted a lot of attention. However, it still\nremains challenging in bridging the semantic gap between visual features and\ntheir underlying semantics and transferring knowledge to semantic categories\nunseen during learning. Unlike most of the existing zero-shot visual\nrecognition methods, we propose a stagewise bidirectional latent embedding\nframework to two subsequent learning stages for zero-shot visual recognition.\nIn the bottom-up stage, a latent embedding space is first created by exploring\nthe topological and labeling information underlying training data of known\nclasses via a proper supervised subspace learning algorithm and the latent\nembedding of training data are used to form landmarks that guide embedding\nsemantics underlying unseen classes into this learned latent space. In the\ntop-down stage, semantic representations of unseen-class labels in a given\nlabel vocabulary are then embedded to the same latent space to preserve the\nsemantic relatedness between all different classes via our proposed\nsemi-supervised Sammon mapping with the guidance of landmarks. Thus, the\nresultant latent embedding space allows for predicting the label of a test\ninstance with a simple nearest-neighbor rule. To evaluate the effectiveness of\nthe proposed framework, we have conducted extensive experiments on four\nbenchmark datasets in object and action recognition, i.e., AwA, CUB-200-2011,\nUCF101 and HMDB51. The experimental results under comparative studies\ndemonstrate that our proposed approach yields the state-of-the-art performance\nunder inductive and transductive settings. \n\n"}
{"id": "1607.03255", "contents": "Title: A Variational Model for Joint Motion Estimation and Image Reconstruction Abstract: The aim of this paper is to derive and analyze a variational model for the\njoint estimation of motion and reconstruction of image sequences, which is\nbased on a time-continuous Eulerian motion model. The model can be set up in\nterms of the continuity equation or the brightness constancy equation. The\nanalysis in this paper focuses on the latter for robust motion estimation on\nsequences of two-dimensional images. We rigorously prove the existence of a\nminimizer in a suitable function space setting. Moreover, we discuss the\nnumerical solution of the model based on primal-dual algorithms and investigate\nseveral examples. Finally, the benefits of our model compared to existing\ntechniques, such as sequential image reconstruction and motion estimation, are\nshown. \n\n"}
{"id": "1607.04433", "contents": "Title: End-to-End Learning for Image Burst Deblurring Abstract: We present a neural network model approach for multi-frame blind\ndeconvolution. The discriminative approach adopts and combines two recent\ntechniques for image deblurring into a single neural network architecture. Our\nproposed hybrid-architecture combines the explicit prediction of a\ndeconvolution filter and non-trivial averaging of Fourier coefficients in the\nfrequency domain. In order to make full use of the information contained in all\nimages in one burst, the proposed network embeds smaller networks, which\nexplicitly allow the model to transfer information between images in early\nlayers. Our system is trained end-to-end using standard backpropagation on a\nset of artificially generated training examples, enabling competitive\nperformance in multi-frame blind deconvolution, both with respect to quality\nand runtime. \n\n"}
{"id": "1607.04780", "contents": "Title: Exploiting Multi-modal Curriculum in Noisy Web Data for Large-scale\n  Concept Learning Abstract: Learning video concept detectors automatically from the big but noisy web\ndata with no additional manual annotations is a novel but challenging area in\nthe multimedia and the machine learning community. A considerable amount of\nvideos on the web are associated with rich but noisy contextual information,\nsuch as the title, which provides weak annotations or labels about the video\ncontent. To leverage the big noisy web labels, this paper proposes a novel\nmethod called WEbly-Labeled Learning (WELL), which is established on the\nstate-of-the-art machine learning algorithm inspired by the learning process of\nhuman. WELL introduces a number of novel multi-modal approaches to incorporate\nmeaningful prior knowledge called curriculum from the noisy web videos. To\ninvestigate this problem, we empirically study the curriculum constructed from\nthe multi-modal features of the videos collected from YouTube and Flickr. The\nefficacy and the scalability of WELL have been extensively demonstrated on two\npublic benchmarks, including the largest multimedia dataset and the largest\nmanually-labeled video set. The comprehensive experimental results demonstrate\nthat WELL outperforms state-of-the-art studies by a statically significant\nmargin on learning concepts from noisy web video data. In addition, the results\nalso verify that WELL is robust to the level of noisiness in the video data.\nNotably, WELL trained on sufficient noisy web labels is able to achieve a\ncomparable accuracy to supervised learning methods trained on the clean\nmanually-labeled data. \n\n"}
{"id": "1607.05836", "contents": "Title: Improved Deep Learning of Object Category using Pose Information Abstract: Despite significant recent progress, the best available computer vision\nalgorithms still lag far behind human capabilities, even for recognizing\nindividual discrete objects under various poses, illuminations, and\nbackgrounds. Here we present a new approach to using object pose information to\nimprove deep network learning. While existing large-scale datasets, e.g.\nImageNet, do not have pose information, we leverage the newly published\nturntable dataset, iLab-20M, which has ~22M images of 704 object instances shot\nunder different lightings, camera viewpoints and turntable rotations, to do\nmore controlled object recognition experiments. We introduce a new\nconvolutional neural network architecture, what/where CNN (2W-CNN), built on a\nlinear-chain feedforward CNN (e.g., AlexNet), augmented by hierarchical layers\nregularized by object poses. Pose information is only used as feedback signal\nduring training, in addition to category information; during test, the\nfeedforward network only predicts category. To validate the approach, we train\nboth 2W-CNN and AlexNet using a fraction of the dataset, and 2W-CNN achieves 6%\nperformance improvement in category prediction. We show mathematically that\n2W-CNN has inherent advantages over AlexNet under the stochastic gradient\ndescent (SGD) optimization procedure. Further more, we fine-tune object\nrecognition on ImageNet by using the pretrained 2W-CNN and AlexNet features on\niLab-20M, results show that significant improvements have been achieved,\ncompared with training AlexNet from scratch. Moreover, fine-tuning 2W-CNN\nfeatures performs even better than fine-tuning the pretrained AlexNet features.\nThese results show pretrained features on iLab- 20M generalizes well to natural\nimage datasets, and 2WCNN learns even better features for object recognition\nthan AlexNet. \n\n"}
{"id": "1607.07032", "contents": "Title: Is Faster R-CNN Doing Well for Pedestrian Detection? Abstract: Detecting pedestrian has been arguably addressed as a special topic beyond\ngeneral object detection. Although recent deep learning object detectors such\nas Fast/Faster R-CNN [1, 2] have shown excellent performance for general object\ndetection, they have limited success for detecting pedestrian, and previous\nleading pedestrian detectors were in general hybrid methods combining\nhand-crafted and deep convolutional features. In this paper, we investigate\nissues involving Faster R-CNN [2] for pedestrian detection. We discover that\nthe Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a\nstand-alone pedestrian detector, but surprisingly, the downstream classifier\ndegrades the results. We argue that two reasons account for the unsatisfactory\naccuracy: (i) insufficient resolution of feature maps for handling small\ninstances, and (ii) lack of any bootstrapping strategy for mining hard negative\nexamples. Driven by these observations, we propose a very simple but effective\nbaseline for pedestrian detection, using an RPN followed by boosted forests on\nshared, high-resolution convolutional feature maps. We comprehensively evaluate\nthis method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presenting\ncompetitive accuracy and good speed. Code will be made publicly available. \n\n"}
{"id": "1607.07220", "contents": "Title: Local- and Holistic- Structure Preserving Image Super Resolution via\n  Deep Joint Component Learning Abstract: Recently, machine learning based single image super resolution (SR)\napproaches focus on jointly learning representations for high-resolution (HR)\nand low-resolution (LR) image patch pairs to improve the quality of the\nsuper-resolved images. However, due to treat all image pixels equally without\nconsidering the salient structures, these approaches usually fail to produce\nvisual pleasant images with sharp edges and fine details. To address this\nissue, in this work we present a new novel SR approach, which replaces the main\nbuilding blocks of the classical interpolation pipeline by a flexible,\ncontent-adaptive deep neural networks. In particular, two well-designed\nstructure-aware components, respectively capturing local- and holistic- image\ncontents, are naturally incorporated into the fully-convolutional\nrepresentation learning to enhance the image sharpness and naturalness.\nExtensively evaluations on several standard benchmarks (e.g., Set5, Set14 and\nBSD200) demonstrate that our approach can achieve superior results, especially\non the image with salient structures, over many existing state-of-the-art SR\nmethods under both quantitative and qualitative measures. \n\n"}
{"id": "1607.07988", "contents": "Title: ATGV-Net: Accurate Depth Super-Resolution Abstract: In this work we present a novel approach for single depth map\nsuper-resolution. Modern consumer depth sensors, especially Time-of-Flight\nsensors, produce dense depth measurements, but are affected by noise and have a\nlow lateral resolution. We propose a method that combines the benefits of\nrecent advances in machine learning based single image super-resolution, i.e.\ndeep convolutional networks, with a variational method to recover accurate\nhigh-resolution depth maps. In particular, we integrate a variational method\nthat models the piecewise affine structures apparent in depth data via an\nanisotropic total generalized variation regularization term on top of a deep\nnetwork. We call our method ATGV-Net and train it end-to-end by unrolling the\noptimization procedure of the variational method. To train deep networks, a\nlarge corpus of training data with accurate ground-truth is required. We\ndemonstrate that it is feasible to train our method solely on synthetic data\nthat we generate in large quantities for this task. Our evaluations show that\nwe achieve state-of-the-art results on three different benchmarks, as well as\non a challenging Time-of-Flight dataset, all without utilizing an additional\nintensity image as guidance. \n\n"}
{"id": "1607.08022", "contents": "Title: Instance Normalization: The Missing Ingredient for Fast Stylization Abstract: It this paper we revisit the fast stylization method introduced in Ulyanov\net. al. (2016). We show how a small change in the stylization architecture\nresults in a significant qualitative improvement in the generated images. The\nchange is limited to swapping batch normalization with instance normalization,\nand to apply the latter both at training and testing times. The resulting\nmethod can be used to train high-performance architectures for real-time image\ngeneration. The code will is made available on github at\nhttps://github.com/DmitryUlyanov/texture_nets. Full paper can be found at\narXiv:1701.02096. \n\n"}
{"id": "1607.08085", "contents": "Title: Improving Semantic Embedding Consistency by Metric Learning for\n  Zero-Shot Classification Abstract: This paper addresses the task of zero-shot image classification. The key\ncontribution of the proposed approach is to control the semantic embedding of\nimages -- one of the main ingredients of zero-shot learning -- by formulating\nit as a metric learning problem. The optimized empirical criterion associates\ntwo types of sub-task constraints: metric discriminating capacity and accurate\nattribute prediction. This results in a novel expression of zero-shot learning\nnot requiring the notion of class in the training phase: only pairs of\nimage/attributes, augmented with a consistency indicator, are given as ground\ntruth. At test time, the learned model can predict the consistency of a test\nimage with a given set of attributes , allowing flexible ways to produce\nrecognition inferences. Despite its simplicity, the proposed approach gives\nstate-of-the-art results on four challenging datasets used for zero-shot\nrecognition evaluation. \n\n"}
{"id": "1608.00911", "contents": "Title: Modeling Spatial and Temporal Cues for Multi-label Facial Action Unit\n  Detection Abstract: Facial action units (AUs) are essential to decode human facial expressions.\nResearchers have focused on training AU detectors with a variety of features\nand classifiers. However, several issues remain. These are spatial\nrepresentation, temporal modeling, and AU correlation. Unlike most studies that\ntackle these issues separately, we propose a hybrid network architecture to\njointly address them. Specifically, spatial representations are extracted by a\nConvolutional Neural Network (CNN), which, as analyzed in this paper, is able\nto reduce person-specific biases caused by hand-crafted features (eg, SIFT and\nGabor). To model temporal dependencies, Long Short-Term Memory (LSTMs) are\nstacked on top of these representations, regardless of the lengths of input\nvideos. The outputs of CNNs and LSTMs are further aggregated into a fusion\nnetwork to produce per-frame predictions of 12 AUs. Our network naturally\naddresses the three issues, and leads to superior performance compared to\nexisting methods that consider these issues independently. Extensive\nexperiments were conducted on two large spontaneous datasets, GFT and BP4D,\ncontaining more than 400,000 frames coded with 12 AUs. On both datasets, we\nreport significant improvement over a standard multi-label CNN and\nfeature-based state-of-the-art. Finally, we provide visualization of the\nlearned AU models, which, to our best knowledge, reveal how machines see facial\nAUs for the first time. \n\n"}
{"id": "1608.01339", "contents": "Title: Retinal Vessel Segmentation Using A New Topological Method Abstract: A novel topological segmentation of retinal images represents blood vessels\nas connected regions in the continuous image plane, having shape-related\nanalytic and geometric properties. This paper presents topological segmentation\nresults from the DRIVE retinal image database. \n\n"}
{"id": "1608.01431", "contents": "Title: An efficient iterative thresholding method for image segmentation Abstract: We proposed an efficient iterative thresholding method for multi-phase image\nsegmentation. The algorithm is based on minimizing piecewise constant\nMumford-Shah functional in which the contour length (or perimeter) is\napproximated by a non-local multi-phase energy. The minimization problem is\nsolved by an iterative method. Each iteration consists of computing simple\nconvolutions followed by a thresholding step. The algorithm is easy to\nimplement and has the optimal complexity $O(N \\log N)$ per iteration. We also\nshow that the iterative algorithm has the total energy decaying property. We\npresent some numerical results to show the efficiency of our method. \n\n"}
{"id": "1608.03066", "contents": "Title: Object Detection, Tracking, and Motion Segmentation for Object-level\n  Video Segmentation Abstract: We present an approach for object segmentation in videos that combines\nframe-level object detection with concepts from object tracking and motion\nsegmentation. The approach extracts temporally consistent object tubes based on\nan off-the-shelf detector. Besides the class label for each tube, this provides\na location prior that is independent of motion. For the final video\nsegmentation, we combine this information with motion cues. The method\novercomes the typical problems of weakly supervised/unsupervised video\nsegmentation, such as scenes with no motion, dominant camera motion, and\nobjects that move as a unit. In contrast to most tracking methods, it provides\nan accurate, temporally consistent segmentation of each object. We report\nresults on four video segmentation datasets: YouTube Objects, SegTrackv2,\negoMotion, and FBMS. \n\n"}
{"id": "1608.03308", "contents": "Title: Approximate search with quantized sparse representations Abstract: This paper tackles the task of storing a large collection of vectors, such as\nvisual descriptors, and of searching in it. To this end, we propose to\napproximate database vectors by constrained sparse coding, where possible atom\nweights are restricted to belong to a finite subset. This formulation\nencompasses, as particular cases, previous state-of-the-art methods such as\nproduct or residual quantization. As opposed to traditional sparse coding\nmethods, quantized sparse coding includes memory usage as a design constraint,\nthereby allowing us to index a large collection such as the BIGANN\nbillion-sized benchmark. Our experiments, carried out on standard benchmarks,\nshow that our formulation leads to competitive solutions when considering\ndifferent trade-offs between learning/coding time, index size and search\nquality. \n\n"}
{"id": "1608.03630", "contents": "Title: Distributed-memory large deformation diffeomorphic 3D image registration Abstract: We present a parallel distributed-memory algorithm for large deformation\ndiffeomorphic registration of volumetric images that produces large isochoric\ndeformations (locally volume preserving). Image registration is a key\ntechnology in medical image analysis. Our algorithm uses a partial differential\nequation constrained optimal control formulation. Finding the optimal\ndeformation map requires the solution of a highly nonlinear problem that\ninvolves pseudo-differential operators, biharmonic operators, and pure\nadvection operators both forward and back- ward in time. A key issue is the\ntime to solution, which poses the demand for efficient optimization methods as\nwell as an effective utilization of high performance computing resources. To\naddress this problem we use a preconditioned, inexact, Gauss-Newton- Krylov\nsolver. Our algorithm integrates several components: a spectral discretization\nin space, a semi-Lagrangian formulation in time, analytic adjoints, different\nregularization functionals (including volume-preserving ones), a spectral\npreconditioner, a highly optimized distributed Fast Fourier Transform, and a\ncubic interpolation scheme for the semi-Lagrangian time-stepping. We\ndemonstrate the scalability of our algorithm on images with resolution of up to\n$1024^3$ on the \"Maverick\" and \"Stampede\" systems at the Texas Advanced\nComputing Center (TACC). The critical problem in the medical imaging\napplication domain is strong scaling, that is, solving registration problems of\na moderate size of $256^3$---a typical resolution for medical images. We are\nable to solve the registration problem for images of this size in less than\nfive seconds on 64 x86 nodes of TACC's \"Maverick\" system. \n\n"}
{"id": "1608.04489", "contents": "Title: SenTion: A framework for Sensing Facial Expressions Abstract: Facial expressions are an integral part of human cognition and communication,\nand can be applied in various real life applications. A vital precursor to\naccurate expression recognition is feature extraction. In this paper, we\npropose SenTion: A framework for sensing facial expressions. We propose a novel\nperson independent and scale invariant method of extracting Inter Vector Angles\n(IVA) as geometric features, which proves to be robust and reliable across\ndatabases. SenTion employs a novel framework of combining geometric (IVA's) and\nappearance based features (Histogram of Gradients) to create a hybrid model,\nthat achieves state of the art recognition accuracy. We evaluate the\nperformance of SenTion on two famous face expression data set, namely: CK+ and\nJAFFE; and subsequently evaluate the viability of facial expression systems by\na user study. Extensive experiments showed that SenTion framework yielded\ndramatic improvements in facial expression recognition and could be employed in\nreal-world applications with low resolution imaging and minimal computational\nresources in real-time, achieving 15-18 fps on a 2.4 GHz CPU with no GPU. \n\n"}
{"id": "1608.04493", "contents": "Title: Dynamic Network Surgery for Efficient DNNs Abstract: Deep learning has become a ubiquitous technology to improve machine\nintelligence. However, most of the existing deep models are structurally very\ncomplex, making them difficult to be deployed on the mobile platforms with\nlimited computational power. In this paper, we propose a novel network\ncompression method called dynamic network surgery, which can remarkably reduce\nthe network complexity by making on-the-fly connection pruning. Unlike the\nprevious methods which accomplish this task in a greedy way, we properly\nincorporate connection splicing into the whole process to avoid incorrect\npruning and make it as a continual network maintenance. The effectiveness of\nour method is proved with experiments. Without any accuracy loss, our method\ncan efficiently compress the number of parameters in LeNet-5 and AlexNet by a\nfactor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it\noutperforms the recent pruning method by considerable margins. Code and some\nmodels are available at https://github.com/yiwenguo/Dynamic-Network-Surgery. \n\n"}
{"id": "1608.04959", "contents": "Title: Frame- and Segment-Level Features and Candidate Pool Evaluation for\n  Video Caption Generation Abstract: We present our submission to the Microsoft Video to Language Challenge of\ngenerating short captions describing videos in the challenge dataset. Our model\nis based on the encoder--decoder pipeline, popular in image and video\ncaptioning systems. We propose to utilize two different kinds of video\nfeatures, one to capture the video content in terms of objects and attributes,\nand the other to capture the motion and action information. Using these diverse\nfeatures we train models specializing in two separate input sub-domains. We\nthen train an evaluator model which is used to pick the best caption from the\npool of candidates generated by these domain expert models. We argue that this\napproach is better suited for the current video captioning task, compared to\nusing a single model, due to the diversity in the dataset.\n  Efficacy of our method is proven by the fact that it was rated best in MSR\nVideo to Language Challenge, as per human evaluation. Additionally, we were\nranked second in the automatic evaluation metrics based table. \n\n"}
{"id": "1608.05461", "contents": "Title: We Can \"See\" You via Wi-Fi - WiFi Action Recognition via Vision-based\n  Methods Abstract: Recently, Wi-Fi has caught tremendous attention for its ubiquity, and,\nmotivated by Wi-Fi's low cost and privacy preservation, researchers have been\nputting lots of investigation into its potential on action recognition and even\nperson identification. In this paper, we offer an comprehensive overview on\nthese two topics in Wi-Fi. Also, through looking at these two topics from an\nunprecedented perspective, we could achieve generality instead of designing\nspecific ad-hoc features for each scenario. Observing the great resemblance of\nChannel State Information (CSI, a fine-grained information captured from the\nreceived Wi-Fi signal) to texture, we proposed a brand-new framework based on\ncomputer vision methods. To minimize the effect of location dependency embedded\nin CSI, we propose a novel de-noising method based on Singular Value\nDecomposition (SVD) to eliminate the background energy and effectively extract\nthe channel information of signals reflected by human bodies. From the\nexperiments conducted, we demonstrate the feasibility and efficacy of the\nproposed methods. Also, we conclude factors that would affect the performance\nand highlight a few promising issues that require further deliberation. \n\n"}
{"id": "1608.05895", "contents": "Title: VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain\n  Segmentation Abstract: Recently deep residual learning with residual units for training very deep\nneural networks advanced the state-of-the-art performance on 2D image\nrecognition tasks, e.g., object detection and segmentation. However, how to\nfully leverage contextual representations for recognition tasks from volumetric\ndata has not been well studied, especially in the field of medical image\ncomputing, where a majority of image modalities are in volumetric format. In\nthis paper we explore the deep residual learning on the task of volumetric\nbrain segmentation. There are at least two main contributions in our work.\nFirst, we propose a deep voxelwise residual network, referred as VoxResNet,\nwhich borrows the spirit of deep residual learning in 2D image recognition\ntasks, and is extended into a 3D variant for handling volumetric data. Second,\nan auto-context version of VoxResNet is proposed by seamlessly integrating the\nlow-level image appearance features, implicit shape information and high-level\ncontext together for further improving the volumetric segmentation performance.\nExtensive experiments on the challenging benchmark of brain segmentation from\nmagnetic resonance (MR) images corroborated the efficacy of our proposed method\nin dealing with volumetric data. We believe this work unravels the potential of\n3D deep learning to advance the recognition performance on volumetric image\nsegmentation. \n\n"}
{"id": "1608.06770", "contents": "Title: Automatic Synchronization of Multi-User Photo Galleries Abstract: In this paper we address the issue of photo galleries synchronization, where\npictures related to the same event are collected by different users. Existing\nsolutions to address the problem are usually based on unrealistic assumptions,\nlike time consistency across photo galleries, and often heavily rely on\nheuristics, limiting therefore the applicability to real-world scenarios. We\npropose a solution that achieves better generalization performance for the\nsynchronization task compared to the available literature. The method is\ncharacterized by three stages: at first, deep convolutional neural network\nfeatures are used to assess the visual similarity among the photos; then, pairs\nof similar photos are detected across different galleries and used to construct\na graph; eventually, a probabilistic graphical model is used to estimate the\ntemporal offset of each pair of galleries, by traversing the minimum spanning\ntree extracted from this graph. The experimental evaluation is conducted on\nfour publicly available datasets covering different types of events,\ndemonstrating the strength of our proposed method. A thorough discussion of the\nobtained results is provided for a critical assessment of the quality in\nsynchronization. \n\n"}
{"id": "1609.00129", "contents": "Title: Grid Loss: Detecting Occluded Faces Abstract: Detection of partially occluded objects is a challenging computer vision\nproblem. Standard Convolutional Neural Network (CNN) detectors fail if parts of\nthe detection window are occluded, since not every sub-part of the window is\ndiscriminative on its own. To address this issue, we propose a novel loss layer\nfor CNNs, named grid loss, which minimizes the error rate on sub-blocks of a\nconvolution layer independently rather than over the whole feature map. This\nresults in parts being more discriminative on their own, enabling the detector\nto recover if the detection window is partially occluded. By mapping our loss\nlayer back to a regular fully connected layer, no additional computational cost\nis incurred at runtime compared to standard CNNs. We demonstrate our method for\nface detection on several public face detection benchmarks and show that our\nmethod outperforms regular CNNs, is suitable for realtime applications and\nachieves state-of-the-art performance. \n\n"}
{"id": "1609.01571", "contents": "Title: Best-Buddies Similarity - Robust Template Matching using Mutual Nearest\n  Neighbors Abstract: We propose a novel method for template matching in unconstrained\nenvironments. Its essence is the Best-Buddies Similarity (BBS), a useful,\nrobust, and parameter-free similarity measure between two sets of points. BBS\nis based on counting the number of Best-Buddies Pairs (BBPs)--pairs of points\nin source and target sets, where each point is the nearest neighbor of the\nother. BBS has several key features that make it robust against complex\ngeometric deformations and high levels of outliers, such as those arising from\nbackground clutter and occlusions. We study these properties, provide a\nstatistical analysis that justifies them, and demonstrate the consistent\nsuccess of BBS on a challenging real-world dataset while using different types\nof features. \n\n"}
{"id": "1609.02770", "contents": "Title: Image and Video Mining through Online Learning Abstract: Within the field of image and video recognition, the traditional approach is\na dataset split into fixed training and test partitions. However, the labelling\nof the training set is time-consuming, especially as datasets grow in size and\ncomplexity. Furthermore, this approach is not applicable to the home user, who\nwants to intuitively group their media without tirelessly labelling the\ncontent. Our interactive approach is able to iteratively cluster classes of\nimages and video. Our approach is based around the concept of an image\nsignature which, unlike a standard bag of words model, can express\nco-occurrence statistics as well as symbol frequency. We efficiently compute\nmetric distances between signatures despite their inherent high dimensionality\nand provide discriminative feature selection, to allow common and distinctive\nelements to be identified from a small set of user labelled examples. These\nelements are then accentuated in the image signature to increase similarity\nbetween examples and pull correct classes together. By repeating this process\nin an online learning framework, the accuracy of similarity increases\ndramatically despite labelling only a few training examples. To demonstrate\nthat the approach is agnostic to media type and features used, we evaluate on\nthree image datasets (15 scene, Caltech101 and FG-NET), a mixed text and image\ndataset (ImageTag), a dataset used in active learning (Iris) and on three\naction recognition datasets (UCF11, KTH and Hollywood2). On the UCF11 video\ndataset, the accuracy is 86.7% despite using only 90 labelled examples from a\ndataset of over 1200 videos, instead of the standard 1122 training videos. The\napproach is both scalable and efficient, with a single iteration over the full\nUCF11 dataset of around 1200 videos taking approximately 1 minute on a standard\ndesktop machine. \n\n"}
{"id": "1609.03056", "contents": "Title: Sequential Deep Trajectory Descriptor for Action Recognition with\n  Three-stream CNN Abstract: Learning the spatial-temporal representation of motion information is crucial\nto human action recognition. Nevertheless, most of the existing features or\ndescriptors cannot capture motion information effectively, especially for\nlong-term motion. To address this problem, this paper proposes a long-term\nmotion descriptor called sequential Deep Trajectory Descriptor (sDTD).\nSpecifically, we project dense trajectories into two-dimensional planes, and\nsubsequently a CNN-RNN network is employed to learn an effective representation\nfor long-term motion. Unlike the popular two-stream ConvNets, the sDTD stream\nis introduced into a three-stream framework so as to identify actions from a\nvideo sequence. Consequently, this three-stream framework can simultaneously\ncapture static spatial features, short-term motion and long-term motion in the\nvideo. Extensive experiments were conducted on three challenging datasets: KTH,\nHMDB51 and UCF101. Experimental results show that our method achieves\nstate-of-the-art performance on the KTH and UCF101 datasets, and is comparable\nto the state-of-the-art methods on the HMDB51 dataset. \n\n"}
{"id": "1609.06657", "contents": "Title: The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question\n  Answering (FSVQA) Abstract: Visual Question Answering (VQA) task has showcased a new stage of interaction\nbetween language and vision, two of the most pivotal components of artificial\nintelligence. However, it has mostly focused on generating short and repetitive\nanswers, mostly single words, which fall short of rich linguistic capabilities\nof humans. We introduce Full-Sentence Visual Question Answering (FSVQA)\ndataset, consisting of nearly 1 million pairs of questions and full-sentence\nanswers for images, built by applying a number of rule-based natural language\nprocessing techniques to original VQA dataset and captions in the MS COCO\ndataset. This poses many additional complexities to conventional VQA task, and\nwe provide a baseline for approaching and evaluating the task, on top of which\nwe invite the research community to build further improvements. \n\n"}
{"id": "1609.08124", "contents": "Title: Learning Language-Visual Embedding for Movie Understanding with\n  Natural-Language Abstract: Learning a joint language-visual embedding has a number of very appealing\nproperties and can result in variety of practical application, including\nnatural language image/video annotation and search. In this work, we study\nthree different joint language-visual neural network model architectures. We\nevaluate our models on large scale LSMDC16 movie dataset for two tasks: 1)\nStandard Ranking for video annotation and retrieval 2) Our proposed movie\nmultiple-choice test. This test facilitate automatic evaluation of\nvisual-language models for natural language video annotation based on human\nactivities. In addition to original Audio Description (AD) captions, provided\nas part of LSMDC16, we collected and will make available a) manually generated\nre-phrasings of those captions obtained using Amazon MTurk b) automatically\ngenerated human activity elements in \"Predicate + Object\" (PO) phrases based on\n\"Knowlywood\", an activity knowledge mining model. Our best model archives\nRecall@10 of 19.2% on annotation and 18.9% on video retrieval tasks for subset\nof 1000 samples. For multiple-choice test, our best model achieve accuracy\n58.11% over whole LSMDC16 public test-set. \n\n"}
{"id": "1609.09545", "contents": "Title: Two-stage Convolutional Part Heatmap Regression for the 1st 3D Face\n  Alignment in the Wild (3DFAW) Challenge Abstract: This paper describes our submission to the 1st 3D Face Alignment in the Wild\n(3DFAW) Challenge. Our method builds upon the idea of convolutional part\nheatmap regression [1], extending it for 3D face alignment. Our method\ndecomposes the problem into two parts: (a) X,Y (2D) estimation and (b) Z\n(depth) estimation. At the first stage, our method estimates the X,Y\ncoordinates of the facial landmarks by producing a set of 2D heatmaps, one for\neach landmark, using convolutional part heatmap regression. Then, these\nheatmaps, alongside the input RGB image, are used as input to a very deep\nsubnetwork trained via residual learning for regressing the Z coordinate. Our\nmethod ranked 1st in the 3DFAW Challenge, surpassing the second best result by\nmore than 22%. \n\n"}
{"id": "1610.00889", "contents": "Title: Cardea: Context-Aware Visual Privacy Protection from Pervasive Cameras Abstract: The growing popularity of mobile and wearable devices with built-in cameras,\nthe bright prospect of camera related applications such as augmented reality\nand life-logging system, the increased ease of taking and sharing photos, and\nadvances in computer vision techniques have greatly facilitated people's lives\nin many aspects, but have also inevitably raised people's concerns about visual\nprivacy at the same time. Motivated by recent user studies that people's\nprivacy concerns are dependent on the context, in this paper, we propose\nCardea, a context-aware and interactive visual privacy protection framework\nthat enforces privacy protection according to people's privacy preferences. The\nframework provides people with fine-grained visual privacy protection using: i)\npersonal privacy profiles, with which people can define their context-dependent\nprivacy preferences; and ii) visual indicators: face features, for devices to\nautomatically locate individuals who request privacy protection; and iii) hand\ngestures, for people to flexibly interact with cameras to temporarily change\ntheir privacy preferences. We design and implement the framework consisting of\nthe client app on Android devices and the cloud server. Our evaluation results\nconfirm this framework is practical and effective with 86% overall accuracy,\nshowing promising future for context-aware visual privacy protection from\npervasive cameras. \n\n"}
{"id": "1610.03677", "contents": "Title: Deep Fruit Detection in Orchards Abstract: An accurate and reliable image based fruit detection system is critical for\nsupporting higher level agriculture tasks such as yield mapping and robotic\nharvesting. This paper presents the use of a state-of-the-art object detection\nframework, Faster R-CNN, in the context of fruit detection in orchards,\nincluding mangoes, almonds and apples. Ablation studies are presented to better\nunderstand the practical deployment of the detection network, including how\nmuch training data is required to capture variability in the dataset. Data\naugmentation techniques are shown to yield significant performance gains,\nresulting in a greater than two-fold reduction in the number of training images\nrequired. In contrast, transferring knowledge between orchards contributed to\nnegligible performance gain over initialising the Deep Convolutional Neural\nNetwork directly from ImageNet features. Finally, to operate over orchard data\ncontaining between 100-1000 fruit per image, a tiling approach is introduced\nfor the Faster R-CNN framework. The study has resulted in the best yet\ndetection performance for these orchards relative to previous works, with an\nF1-score of >0.9 achieved for apples and mangoes. \n\n"}
{"id": "1610.07935", "contents": "Title: PATH: Person Authentication using Trace Histories Abstract: In this paper, a solution to the problem of Active Authentication using trace\nhistories is addressed. Specifically, the task is to perform user verification\non mobile devices using historical location traces of the user as a function of\ntime. Considering the movement of a human as a Markovian motion, a modified\nHidden Markov Model (HMM)-based solution is proposed. The proposed method,\nnamely the Marginally Smoothed HMM (MSHMM), utilizes the marginal probabilities\nof location and timing information of the observations to smooth-out the\nemission probabilities while training. Hence, it can efficiently handle\nunforeseen observations during the test phase. The verification performance of\nthis method is compared to a sequence matching (SM) method , a Markov\nChain-based method (MC) and an HMM with basic Laplace Smoothing (HMM-lap).\nExperimental results using the location information of the UMD Active\nAuthentication Dataset-02 (UMDAA02) and the GeoLife dataset are presented. The\nproposed MSHMM method outperforms the compared methods in terms of equal error\nrate (EER). Additionally, the effects of different parameters on the proposed\nmethod are discussed. \n\n"}
{"id": "1610.08851", "contents": "Title: Single- and Multi-Task Architectures for Tool Presence Detection\n  Challenge at M2CAI 2016 Abstract: The tool presence detection challenge at M2CAI 2016 consists of identifying\nthe presence/absence of seven surgical tools in the images of cholecystectomy\nvideos. Here, we propose to use deep architectures that are based on our\nprevious work where we presented several architectures to perform multiple\nrecognition tasks on laparoscopic videos. In this technical report, we present\nthe tool presence detection results using two architectures: (1) a single-task\narchitecture designed to perform solely the tool presence detection task and\n(2) a multi-task architecture designed to perform jointly phase recognition and\ntool presence detection. The results show that the multi-task network only\nslightly improves the tool presence detection results. In constrast, a\nsignificant improvement is obtained when there are more data available to train\nthe networks. This significant improvement can be regarded as a call for action\nfor other institutions to start working toward publishing more datasets into\nthe community, so that better models could be generated to perform the task. \n\n"}
{"id": "1611.00471", "contents": "Title: Dual Attention Networks for Multimodal Reasoning and Matching Abstract: We propose Dual Attention Networks (DANs) which jointly leverage visual and\ntextual attention mechanisms to capture fine-grained interplay between vision\nand language. DANs attend to specific regions in images and words in text\nthrough multiple steps and gather essential information from both modalities.\nBased on this framework, we introduce two types of DANs for multimodal\nreasoning and matching, respectively. The reasoning model allows visual and\ntextual attentions to steer each other during collaborative inference, which is\nuseful for tasks such as Visual Question Answering (VQA). In addition, the\nmatching model exploits the two attention mechanisms to estimate the similarity\nbetween images and sentences by focusing on their shared semantics. Our\nextensive experiments validate the effectiveness of DANs in combining vision\nand language, achieving the state-of-the-art performance on public benchmarks\nfor VQA and image-text matching. \n\n"}
{"id": "1611.01260", "contents": "Title: Learning Identity Mappings with Residual Gates Abstract: We propose a new layer design by adding a linear gating mechanism to shortcut\nconnections. By using a scalar parameter to control each gate, we provide a way\nto learn identity mappings by optimizing only one parameter. We build upon the\nmotivation behind Residual Networks, where a layer is reformulated in order to\nmake learning identity mappings less problematic to the optimizer. The\naugmentation introduces only one extra parameter per layer, and provides easier\noptimization by making degeneration into identity mappings simpler. We propose\na new model, the Gated Residual Network, which is the result when augmenting\nResidual Networks. Experimental results show that augmenting layers provides\nbetter optimization, increased performance, and more layer independence. We\nevaluate our method on MNIST using fully-connected networks, showing empirical\nindications that our augmentation facilitates the optimization of deep models,\nand that it provides high tolerance to full layer removal: the model retains\nover 90% of its performance even after half of its layers have been randomly\nremoved. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated\nResNets, achieving 3.65% and 18.27% error, respectively. \n\n"}
{"id": "1611.02261", "contents": "Title: Memory-augmented Attention Modelling for Videos Abstract: We present a method to improve video description generation by modeling\nhigher-order interactions between video frames and described concepts. By\nstoring past visual attention in the video associated to previously generated\nwords, the system is able to decide what to look at and describe in light of\nwhat it has already looked at and described. This enables not only more\neffective local attention, but tractable consideration of the video sequence\nwhile generating each word. Evaluation on the challenging and popular MSVD and\nCharades datasets demonstrates that the proposed architecture outperforms\nprevious video description approaches without requiring external temporal video\nfeatures. \n\n"}
{"id": "1611.02639", "contents": "Title: Gradients of Counterfactuals Abstract: Gradients have been used to quantify feature importance in machine learning\nmodels. Unfortunately, in nonlinear deep networks, not only individual neurons\nbut also the whole network can saturate, and as a result an important input\nfeature can have a tiny gradient. We study various networks, and observe that\nthis phenomena is indeed widespread, across many inputs.\n  We propose to examine interior gradients, which are gradients of\ncounterfactual inputs constructed by scaling down the original input. We apply\nour method to the GoogleNet architecture for object recognition in images, as\nwell as a ligand-based virtual screening network with categorical features and\nan LSTM based language model for the Penn Treebank dataset. We visualize how\ninterior gradients better capture feature importance. Furthermore, interior\ngradients are applicable to a wide variety of deep networks, and have the\nattribution property that the feature importance scores sum to the the\nprediction score.\n  Best of all, interior gradients can be computed just as easily as gradients.\nIn contrast, previous methods are complex to implement, which hinders practical\nadoption. \n\n"}
{"id": "1611.02886", "contents": "Title: Node-Adapt, Path-Adapt and Tree-Adapt:Model-Transfer Domain Adaptation\n  for Random Forest Abstract: Random Forest (RF) is a successful paradigm for learning classifiers due to\nits ability to learn from large feature spaces and seamlessly integrate\nmulti-class classification, as well as the achieved accuracy and processing\nefficiency. However, as many other classifiers, RF requires domain adaptation\n(DA) provided that there is a mismatch between the training (source) and\ntesting (target) domains which provokes classification degradation.\nConsequently, different RF-DA methods have been proposed, which not only\nrequire target-domain samples but revisiting the source-domain ones, too. As\nnovelty, we propose three inherently different methods (Node-Adapt, Path-Adapt\nand Tree-Adapt) that only require the learned source-domain RF and a relatively\nfew target-domain samples for DA, i.e. source-domain samples do not need to be\navailable. To assess the performance of our proposals we focus on image-based\nobject detection, using the pedestrian detection problem as challenging\nproof-of-concept. Moreover, we use the RF with expert nodes because it is a\ncompetitive patch-based pedestrian model. We test our Node-, Path- and\nTree-Adapt methods in standard benchmarks, showing that DA is largely achieved. \n\n"}
{"id": "1611.04357", "contents": "Title: Selfie Detection by Synergy-Constraint Based Convolutional Neural\n  Network Abstract: Categorisation of huge amount of data on the multimedia platform is a crucial\ntask. In this work, we propose a novel approach to address the subtle problem\nof selfie detection for image database segregation on the web, given rapid rise\nin number of selfies clicked. A Convolutional Neural Network (CNN) is modeled\nto learn a synergy feature in the common subspace of head and shoulder\norientation, derived from Local Binary Pattern (LBP) and Histogram of Oriented\nGradients (HOG) features respectively. This synergy was captured by projecting\nthe aforementioned features using Canonical Correlation Analysis (CCA). We show\nthat the resulting network's convolutional activations in the neighbourhood of\nspatial keypoints captured by SIFT are discriminative for selfie-detection. In\ngeneral, proposed approach aids in capturing intricacies present in the image\ndata and has the potential for usage in other subtle image analysis scenarios\napart from just selfie detection. We investigate and analyse the performance of\npopular CNN architectures (GoogleNet, AlexNet), used for other image\nclassification tasks, when subjected to the task of detecting the selfies on\nthe multimedia platform. The results of the proposed approach are compared with\nthese popular architectures on a dataset of ninety thousand images comprising\nof roughly equal number of selfies and non-selfies. Experimental results on\nthis dataset shows the effectiveness of the proposed approach. \n\n"}
{"id": "1611.05128", "contents": "Title: Designing Energy-Efficient Convolutional Neural Networks using\n  Energy-Aware Pruning Abstract: Deep convolutional neural networks (CNNs) are indispensable to\nstate-of-the-art computer vision algorithms. However, they are still rarely\ndeployed on battery-powered mobile devices, such as smartphones and wearable\ngadgets, where vision algorithms can enable many revolutionary real-world\napplications. The key limiting factor is the high energy consumption of CNN\nprocessing due to its high computational complexity. While there are many\nprevious efforts that try to reduce the CNN model size or amount of\ncomputation, we find that they do not necessarily result in lower energy\nconsumption, and therefore do not serve as a good metric for energy cost\nestimation.\n  To close the gap between CNN design and energy consumption optimization, we\npropose an energy-aware pruning algorithm for CNNs that directly uses energy\nconsumption estimation of a CNN to guide the pruning process. The energy\nestimation methodology uses parameters extrapolated from actual hardware\nmeasurements that target realistic battery-powered system setups. The proposed\nlayer-by-layer pruning algorithm also prunes more aggressively than previously\nproposed pruning methods by minimizing the error in output feature maps instead\nof filter weights. For each layer, the weights are first pruned and then\nlocally fine-tuned with a closed-form least-square solution to quickly restore\nthe accuracy. After all layers are pruned, the entire network is further\nglobally fine-tuned using back-propagation. With the proposed pruning method,\nthe energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x,\nrespectively, with less than 1% top-5 accuracy loss. Finally, we show that\npruning the AlexNet with a reduced number of target classes can greatly\ndecrease the number of weights but the energy reduction is limited.\n  Energy modeling tool and energy-aware pruned models available at\nhttp://eyeriss.mit.edu/energy.html \n\n"}
{"id": "1611.05435", "contents": "Title: Convolutional Gated Recurrent Networks for Video Segmentation Abstract: Semantic segmentation has recently witnessed major progress, where fully\nconvolutional neural networks have shown to perform well. However, most of the\nprevious work focused on improving single image segmentation. To our knowledge,\nno prior work has made use of temporal video information in a recurrent\nnetwork. In this paper, we introduce a novel approach to implicitly utilize\ntemporal data in videos for online semantic segmentation. The method relies on\na fully convolutional network that is embedded into a gated recurrent\narchitecture. This design receives a sequence of consecutive video frames and\noutputs the segmentation of the last frame. Convolutional gated recurrent\nnetworks are used for the recurrent part to preserve spatial connectivities in\nthe image. Our proposed method can be applied in both online and batch\nsegmentation. This architecture is tested for both binary and semantic video\nsegmentation tasks. Experiments are conducted on the recent benchmarks in\nSegTrack V2, Davis, CityScapes, and Synthia. Using recurrent fully\nconvolutional networks improved the baseline network performance in all of our\nexperiments. Namely, 5% and 3% improvement of F-measure in SegTrack2 and Davis\nrespectively, 5.7% improvement in mean IoU in Synthia and 3.5% improvement in\ncategorical mean IoU in CityScapes. The performance of the RFCN network depends\non its baseline fully convolutional network. Thus RFCN architecture can be seen\nas a method to improve its baseline segmentation network by exploiting\nspatiotemporal information in videos. \n\n"}
{"id": "1611.05503", "contents": "Title: On the Exploration of Convolutional Fusion Networks for Visual\n  Recognition Abstract: Despite recent advances in multi-scale deep representations, their\nlimitations are attributed to expensive parameters and weak fusion modules.\nHence, we propose an efficient approach to fuse multi-scale deep\nrepresentations, called convolutional fusion networks (CFN). Owing to using\n1$\\times$1 convolution and global average pooling, CFN can efficiently generate\nthe side branches while adding few parameters. In addition, we present a\nlocally-connected fusion module, which can learn adaptive weights for the side\nbranches and form a discriminatively fused feature. CFN models trained on the\nCIFAR and ImageNet datasets demonstrate remarkable improvements over the plain\nCNNs. Furthermore, we generalize CFN to three new tasks, including scene\nrecognition, fine-grained recognition and image retrieval. Our experiments show\nthat it can obtain consistent improvements towards the transferring tasks. \n\n"}
{"id": "1611.05603", "contents": "Title: Weakly-supervised Learning of Mid-level Features for Pedestrian\n  Attribute Recognition and Localization Abstract: State-of-the-art methods treat pedestrian attribute recognition as a\nmulti-label image classification problem. The location information of person\nattributes is usually eliminated or simply encoded in the rigid splitting of\nwhole body in previous work. In this paper, we formulate the task in a\nweakly-supervised attribute localization framework. Based on GoogLeNet,\nfirstly, a set of mid-level attribute features are discovered by novelly\ndesigned detection layers, where a max-pooling based weakly-supervised object\ndetection technique is used to train these layers with only image-level labels\nwithout the need of bounding box annotations of pedestrian attributes.\nSecondly, attribute labels are predicted by regression of the detection\nresponse magnitudes. Finally, the locations and rough shapes of pedestrian\nattributes can be inferred by performing clustering on a fusion of activation\nmaps of the detection layers, where the fusion weights are estimated as the\ncorrelation strengths between each attribute and its relevant mid-level\nfeatures. Extensive experiments are performed on the two currently largest\npedestrian attribute datasets, i.e. the PETA dataset and the RAP dataset.\nResults show that the proposed method has achieved competitive performance on\nattribute recognition, compared to other state-of-the-art methods. Moreover,\nthe results of attribute localization are visualized to understand the\ncharacteristics of the proposed method. \n\n"}
{"id": "1611.05664", "contents": "Title: Learning to detect and localize many objects from few examples Abstract: The current trend in object detection and localization is to learn\npredictions with high capacity deep neural networks trained on a very large\namount of annotated data and using a high amount of processing power. In this\nwork, we propose a new neural model which directly predicts bounding box\ncoordinates. The particularity of our contribution lies in the local\ncomputations of predictions with a new form of local parameter sharing which\nkeeps the overall amount of trainable parameters low. Key components of the\nmodel are spatial 2D-LSTM recurrent layers which convey contextual information\nbetween the regions of the image. We show that this model is more powerful than\nthe state of the art in applications where training data is not as abundant as\nin the classical configuration of natural images and Imagenet/Pascal VOC tasks.\nWe particularly target the detection of text in document images, but our method\nis not limited to this setting. The proposed model also facilitates the\ndetection of many objects in a single image and can deal with inputs of\nvariable sizes without resizing. \n\n"}
{"id": "1611.06067", "contents": "Title: An End-to-End Spatio-Temporal Attention Model for Human Action\n  Recognition from Skeleton Data Abstract: Human action recognition is an important task in computer vision. Extracting\ndiscriminative spatial and temporal features to model the spatial and temporal\nevolutions of different actions plays a key role in accomplishing this task. In\nthis work, we propose an end-to-end spatial and temporal attention model for\nhuman action recognition from skeleton data. We build our model on top of the\nRecurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM), which\nlearns to selectively focus on discriminative joints of skeleton within each\nframe of the inputs and pays different levels of attention to the outputs of\ndifferent frames. Furthermore, to ensure effective training of the network, we\npropose a regularized cross-entropy loss to drive the model learning process\nand develop a joint training strategy accordingly. Experimental results\ndemonstrate the effectiveness of the proposed model,both on the small human\naction recognition data set of SBU and the currently largest NTU dataset. \n\n"}
{"id": "1611.06224", "contents": "Title: ModelHub: Towards Unified Data and Lifecycle Management for Deep\n  Learning Abstract: Deep learning has improved state-of-the-art results in many important fields,\nand has been the subject of much research in recent years, leading to the\ndevelopment of several systems for facilitating deep learning. Current systems,\nhowever, mainly focus on model building and training phases, while the issues\nof data management, model sharing, and lifecycle management are largely\nignored. Deep learning modeling lifecycle generates a rich set of data\nartifacts, such as learned parameters and training logs, and comprises of\nseveral frequently conducted tasks, e.g., to understand the model behaviors and\nto try out new models. Dealing with such artifacts and tasks is cumbersome and\nlargely left to the users. This paper describes our vision and implementation\nof a data and lifecycle management system for deep learning. First, we\ngeneralize model exploration and model enumeration queries from commonly\nconducted tasks by deep learning modelers, and propose a high-level domain\nspecific language (DSL), inspired by SQL, to raise the abstraction level and\naccelerate the modeling process. To manage the data artifacts, especially the\nlarge amount of checkpointed float parameters, we design a novel model\nversioning system (dlv), and a read-optimized parameter archival storage system\n(PAS) that minimizes storage footprint and accelerates query workloads without\nlosing accuracy. PAS archives versioned models using deltas in a\nmulti-resolution fashion by separately storing the less significant bits, and\nfeatures a novel progressive query (inference) evaluation algorithm. Third, we\nshow that archiving versioned models using deltas poses a new dataset\nversioning problem and we develop efficient algorithms for solving it. We\nconduct extensive experiments over several real datasets from computer vision\ndomain to show the efficiency of the proposed techniques. \n\n"}
{"id": "1611.06355", "contents": "Title: Invertible Conditional GANs for image editing Abstract: Generative Adversarial Networks (GANs) have recently demonstrated to\nsuccessfully approximate complex data distributions. A relevant extension of\nthis model is conditional GANs (cGANs), where the introduction of external\ninformation allows to determine specific representations of the generated\nimages. In this work, we evaluate encoders to inverse the mapping of a cGAN,\ni.e., mapping a real image into a latent space and a conditional\nrepresentation. This allows, for example, to reconstruct and modify real images\nof faces conditioning on arbitrary attributes. Additionally, we evaluate the\ndesign of cGANs. The combination of an encoder with a cGAN, which we call\nInvertible cGAN (IcGAN), enables to re-generate real images with deterministic\ncomplex modifications. \n\n"}
{"id": "1611.06973", "contents": "Title: RhoanaNet Pipeline: Dense Automatic Neural Annotation Abstract: Reconstructing a synaptic wiring diagram, or connectome, from electron\nmicroscopy (EM) images of brain tissue currently requires many hours of manual\nannotation or proofreading (Kasthuri and Lichtman, 2010; Lichtman and Sanes,\n2008; Seung, 2009). The desire to reconstruct ever larger and more complex\nnetworks has pushed the collection of ever larger EM datasets. A cubic\nmillimeter of raw imaging data would take up 1 PB of storage and present an\nannotation project that would be impractical without relying heavily on\nautomatic segmentation methods. The RhoanaNet image processing pipeline was\ndeveloped to automatically segment large volumes of EM data and ease the burden\nof manual proofreading and annotation. Based on (Kaynig et al., 2015), we\nupdated every stage of the software pipeline to provide better throughput\nperformance and higher quality segmentation results. We used state of the art\ndeep learning techniques to generate improved membrane probability maps, and\nGala (Nunez-Iglesias et al., 2014) was used to agglomerate 2D segments into 3D\nobjects.\n  We applied the RhoanaNet pipeline to four densely annotated EM datasets, two\nfrom mouse cortex, one from cerebellum and one from mouse lateral geniculate\nnucleus (LGN). All training and test data is made available for benchmark\ncomparisons. The best segmentation results obtained gave\n$V^\\text{Info}_\\text{F-score}$ scores of 0.9054 and 09182 for the cortex\ndatasets, 0.9438 for LGN, and 0.9150 for Cerebellum.\n  The RhoanaNet pipeline is open source software. All source code, training\ndata, test data, and annotations for all four benchmark datasets are available\nat www.rhoana.org. \n\n"}
{"id": "1611.07245", "contents": "Title: Single-View and Multi-View Depth Fusion Abstract: Dense and accurate 3D mapping from a monocular sequence is a key technology\nfor several applications and still an open research area. This paper leverages\nrecent results on single-view CNN-based depth estimation and fuses them with\nmulti-view depth estimation. Both approaches present complementary strengths.\nMulti-view depth is highly accurate but only in high-texture areas and\nhigh-parallax cases. Single-view depth captures the local structure of\nmid-level regions, including texture-less areas, but the estimated depth lacks\nglobal coherence. The single and multi-view fusion we propose is challenging in\nseveral aspects. First, both depths are related by a deformation that depends\non the image content. Second, the selection of multi-view points of high\naccuracy might be difficult for low-parallax configurations. We present\ncontributions for both problems. Our results in the public datasets of NYUv2\nand TUM shows that our algorithm outperforms the individual single and\nmulti-view approaches. A video showing the key aspects of mapping in our Single\nand Multi-view depth proposal is available at https://youtu.be/ipc5HukTb4k \n\n"}
{"id": "1611.07593", "contents": "Title: Learning Joint Feature Adaptation for Zero-Shot Recognition Abstract: Zero-shot recognition (ZSR) aims to recognize target-domain data instances of\nunseen classes based on the models learned from associated pairs of seen-class\nsource and target domain data. One of the key challenges in ZSR is the relative\nscarcity of source-domain features (e.g. one feature vector per class), which\ndo not fully account for wide variability in target-domain instances. In this\npaper we propose a novel framework of learning data-dependent feature\ntransforms for scoring similarity between an arbitrary pair of source and\ntarget data instances to account for the wide variability in target domain. Our\nproposed approach is based on optimizing over a parameterized family of local\nfeature displacements that maximize the source-target adaptive similarity\nfunctions. Accordingly we propose formulating zero-shot learning (ZSL) using\nlatent structural SVMs to learn our similarity functions from training data. As\ndemonstration we design a specific algorithm under the proposed framework\ninvolving bilinear similarity functions and regularized least squares as\npenalties for feature displacement. We test our approach on several benchmark\ndatasets for ZSR and show significant improvement over the state-of-the-art.\nFor instance, on aP&Y dataset we can achieve 80.89% in terms of recognition\naccuracy, outperforming the state-of-the-art by 11.15%. \n\n"}
{"id": "1611.07703", "contents": "Title: 'Part'ly first among equals: Semantic part-based benchmarking for\n  state-of-the-art object recognition systems Abstract: An examination of object recognition challenge leaderboards (ILSVRC,\nPASCAL-VOC) reveals that the top-performing classifiers typically exhibit small\ndifferences amongst themselves in terms of error rate/mAP. To better\ndifferentiate the top performers, additional criteria are required. Moreover,\nthe (test) images, on which the performance scores are based, predominantly\ncontain fully visible objects. Therefore, `harder' test images, mimicking the\nchallenging conditions (e.g. occlusion) in which humans routinely recognize\nobjects, need to be utilized for benchmarking. To address the concerns\nmentioned above, we make two contributions. First, we systematically vary the\nlevel of local object-part content, global detail and spatial context in images\nfrom PASCAL VOC 2010 to create a new benchmarking dataset dubbed PPSS-12.\nSecond, we propose an object-part based benchmarking procedure which quantifies\nclassifiers' robustness to a range of visibility and contextual settings. The\nbenchmarking procedure relies on a semantic similarity measure that naturally\naddresses potential semantic granularity differences between the category\nlabels in training and test datasets, thus eliminating manual mapping. We use\nour procedure on the PPSS-12 dataset to benchmark top-performing classifiers\ntrained on the ILSVRC-2012 dataset. Our results show that the proposed\nbenchmarking procedure enables additional differentiation among\nstate-of-the-art object classifiers in terms of their ability to handle missing\ncontent and insufficient object detail. Given this capability for additional\ndifferentiation, our approach can potentially supplement existing benchmarking\nprocedures used in object recognition challenge leaderboards. \n\n"}
{"id": "1611.07941", "contents": "Title: Multi-Modal Mean-Fields via Cardinality-Based Clamping Abstract: Mean Field inference is central to statistical physics. It has attracted much\ninterest in the Computer Vision community to efficiently solve problems\nexpressible in terms of large Conditional Random Fields. However, since it\nmodels the posterior probability distribution as a product of marginal\nprobabilities, it may fail to properly account for important dependencies\nbetween variables. We therefore replace the fully factorized distribution of\nMean Field by a weighted mixture of such distributions, that similarly\nminimizes the KL-Divergence to the true posterior. By introducing two new\nideas, namely, conditioning on groups of variables instead of single ones and\nusing a parameter of the conditional random field potentials, that we identify\nto the temperature in the sense of statistical physics to select such groups,\nwe can perform this minimization efficiently. Our extension of the clamping\nmethod proposed in previous works allows us to both produce a more descriptive\napproximation of the true posterior and, inspired by the diverse MAP paradigms,\nfit a mixture of Mean Field approximations. We demonstrate that this positively\nimpacts real-world algorithms that initially relied on mean fields. \n\n"}
{"id": "1611.08563", "contents": "Title: Online Real-time Multiple Spatiotemporal Action Localisation and\n  Prediction Abstract: We present a deep-learning framework for real-time multiple spatio-temporal\n(S/T) action localisation, classification and early prediction. Current\nstate-of-the-art approaches work offline and are too slow to be useful in real-\nworld settings. To overcome their limitations we introduce two major\ndevelopments. Firstly, we adopt real-time SSD (Single Shot MultiBox Detector)\nconvolutional neural networks to regress and classify detection boxes in each\nvideo frame potentially containing an action of interest. Secondly, we design\nan original and efficient online algorithm to incrementally construct and label\n`action tubes' from the SSD frame level detections. As a result, our system is\nnot only capable of performing S/T detection in real time, but can also perform\nearly action prediction in an online fashion. We achieve new state-of-the-art\nresults in both S/T action localisation and early action prediction on the\nchallenging UCF101-24 and J-HMDB-21 benchmarks, even when compared to the top\noffline competitors. To the best of our knowledge, ours is the first real-time\n(up to 40fps) system able to perform online S/T action localisation and early\naction prediction on the untrimmed videos of UCF101-24. \n\n"}
{"id": "1611.08663", "contents": "Title: Multi-Task Zero-Shot Action Recognition with Prioritised Data\n  Augmentation Abstract: Zero-Shot Learning (ZSL) promises to scale visual recognition by bypassing\nthe conventional model training requirement of annotated examples for every\ncategory. This is achieved by establishing a mapping connecting low-level\nfeatures and a semantic description of the label space, referred as\nvisual-semantic mapping, on auxiliary data. Reusing the learned mapping to\nproject target videos into an embedding space thus allows novel-classes to be\nrecognised by nearest neighbour inference. However, existing ZSL methods suffer\nfrom auxiliary-target domain shift intrinsically induced by assuming the same\nmapping for the disjoint auxiliary and target classes. This compromises the\ngeneralisation accuracy of ZSL recognition on the target data. In this work, we\nimprove the ability of ZSL to generalise across this domain shift in both\nmodel- and data-centric ways by formulating a visual-semantic mapping with\nbetter generalisation properties and a dynamic data re-weighting method to\nprioritise auxiliary data that are relevant to the target classes.\nSpecifically: (1) We introduce a multi-task visual-semantic mapping to improve\ngeneralisation by constraining the semantic mapping parameters to lie on a\nlow-dimensional manifold, (2) We explore prioritised data augmentation by\nexpanding the pool of auxiliary data with additional instances weighted by\nrelevance to the target domain. The proposed new model is applied to the\nchallenging zero-shot action recognition problem to demonstrate its advantages\nover existing ZSL models. \n\n"}
{"id": "1611.08669", "contents": "Title: Visual Dialog Abstract: We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org \n\n"}
{"id": "1611.08788", "contents": "Title: SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial\n  Networks Abstract: Autonomous driving is one of the most recent topics of interest which is\naimed at replicating human driving behavior keeping in mind the safety issues.\nWe approach the problem of learning synthetic driving using generative neural\nnetworks. The main idea is to make a controller trainer network using images\nplus key press data to mimic human learning. We used the architecture of a\nstable GAN to make predictions between driving scenes using key presses. We\ntrain our model on one video game (Road Rash) and tested the accuracy and\ncompared it by running the model on other maps in Road Rash to determine the\nextent of learning. \n\n"}
{"id": "1611.08974", "contents": "Title: Semantic Scene Completion from a Single Depth Image Abstract: This paper focuses on semantic scene completion, a task for producing a\ncomplete 3D voxel representation of volumetric occupancy and semantic labels\nfor a scene from a single-view depth map observation. Previous work has\nconsidered scene completion and semantic labeling of depth maps separately.\nHowever, we observe that these two problems are tightly intertwined. To\nleverage the coupled nature of these two tasks, we introduce the semantic scene\ncompletion network (SSCNet), an end-to-end 3D convolutional network that takes\na single depth image as input and simultaneously outputs occupancy and semantic\nlabels for all voxels in the camera view frustum. Our network uses a\ndilation-based 3D context module to efficiently expand the receptive field and\nenable 3D context learning. To train our network, we construct SUNCG - a\nmanually created large-scale dataset of synthetic 3D scenes with dense\nvolumetric annotations. Our experiments demonstrate that the joint model\noutperforms methods addressing each task in isolation and outperforms\nalternative approaches on the semantic scene completion task. \n\n"}
{"id": "1611.08991", "contents": "Title: Object Detection Free Instance Segmentation With Labeling\n  Transformations Abstract: Instance segmentation has attracted recent attention in computer vision and\nexisting methods in this domain mostly have an object detection stage. In this\npaper, we study the intrinsic challenge of the instance segmentation problem,\nthe presence of a quotient space (swapping the labels of different instances\nleads to the same result), and propose new methods that are object proposal-\nand object detection- free. We propose three alternative methods, namely\npixel-based affinity mapping, superpixel-based affinity learning, and\nboundary-based component segmentation, all focusing on performing labeling\ntransformations to cope with the quotient space problem. By adopting fully\nconvolutional neural networks (FCN) like models, our framework attains\ncompetitive results on both the PASCAL dataset (object-centric) and the Gland\ndataset (texture-centric), which the existing methods are not able to do. Our\nwork also has the advantages in its transparency, simplicity, and being all\nsegmentation based. \n\n"}
{"id": "1612.00137", "contents": "Title: RMPE: Regional Multi-person Pose Estimation Abstract: Multi-person pose estimation in the wild is challenging. Although\nstate-of-the-art human detectors have demonstrated good performance, small\nerrors in localization and recognition are inevitable. These errors can cause\nfailures for a single-person pose estimator (SPPE), especially for methods that\nsolely depend on human detection results. In this paper, we propose a novel\nregional multi-person pose estimation (RMPE) framework to facilitate pose\nestimation in the presence of inaccurate human bounding boxes. Our framework\nconsists of three components: Symmetric Spatial Transformer Network (SSTN),\nParametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals\nGenerator (PGPG). Our method is able to handle inaccurate bounding boxes and\nredundant detections, allowing it to achieve a 17% increase in mAP over the\nstate-of-the-art methods on the MPII (multi person) dataset.Our model and\nsource codes are publicly available. \n\n"}
{"id": "1612.01925", "contents": "Title: FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks Abstract: The FlowNet demonstrated that optical flow estimation can be cast as a\nlearning problem. However, the state of the art with regard to the quality of\nthe flow has still been defined by traditional methods. Particularly on small\ndisplacements and real-world data, FlowNet cannot compete with variational\nmethods. In this paper, we advance the concept of end-to-end learning of\noptical flow and make it work really well. The large improvements in quality\nand speed are caused by three major contributions: first, we focus on the\ntraining data and show that the schedule of presenting data during training is\nvery important. Second, we develop a stacked architecture that includes warping\nof the second image with intermediate optical flow. Third, we elaborate on\nsmall displacements by introducing a sub-network specializing on small motions.\nFlowNet 2.0 is only marginally slower than the original FlowNet but decreases\nthe estimation error by more than 50%. It performs on par with state-of-the-art\nmethods, while running at interactive frame rates. Moreover, we present faster\nvariants that allow optical flow computation at up to 140fps with accuracy\nmatching the original FlowNet. \n\n"}
{"id": "1612.01981", "contents": "Title: Core Sampling Framework for Pixel Classification Abstract: The intermediate map responses of a Convolutional Neural Network (CNN)\ncontain information about an image that can be used to extract contextual\nknowledge about it. In this paper, we present a core sampling framework that is\nable to use these activation maps from several layers as features to another\nneural network using transfer learning to provide an understanding of an input\nimage. Our framework creates a representation that combines features from the\ntest data and the contextual knowledge gained from the responses of a\npretrained network, processes it and feeds it to a separate Deep Belief\nNetwork. We use this representation to extract more information from an image\nat the pixel level, hence gaining understanding of the whole image. We\nexperimentally demonstrate the usefulness of our framework using a pretrained\nVGG-16 model to perform segmentation on the BAERI dataset of Synthetic Aperture\nRadar(SAR) imagery and the CAMVID dataset. \n\n"}
{"id": "1612.02184", "contents": "Title: Saliency Driven Image Manipulation Abstract: Have you ever taken a picture only to find out that an unimportant background\nobject ended up being overly salient? Or one of those team sports photos where\nyour favorite player blends with the rest? Wouldn't it be nice if you could\ntweak these pictures just a little bit so that the distractor would be\nattenuated and your favorite player will stand-out among her peers?\nManipulating images in order to control the saliency of objects is the goal of\nthis paper. We propose an approach that considers the internal color and\nsaliency properties of the image. It changes the saliency map via an\noptimization framework that relies on patch-based manipulation using only\npatches from within the same image to achieve realistic looking results.\nApplications include object enhancement, distractors attenuation and background\ndecluttering. Comparing our method to previous ones shows significant\nimprovement, both in the achieved saliency manipulation and in the realistic\nappearance of the resulting images. \n\n"}
{"id": "1612.02218", "contents": "Title: Embedded Line Scan Image Sensors: The Low Cost Alternative for High\n  Speed Imaging Abstract: In this paper we propose a low-cost high-speed imaging line scan system. We\nreplace an expensive industrial line scan camera and illumination with a\ncustom-built set-up of cheap off-the-shelf components, yielding a measurement\nsystem with comparative quality while costing about 20 times less. We use a\nlow-cost linear (1D) image sensor, cheap optics including a LED-based or\nLASER-based lighting and an embedded platform to process the images. A\nstep-by-step method to design such a custom high speed imaging system and\nselect proper components is proposed. Simulations allowing to predict the final\nimage quality to be obtained by the set-up has been developed. Finally, we\napplied our method in a lab, closely representing the real-life cases. Our\nresults shows that our simulations are very accurate and that our low-cost line\nscan set-up acquired image quality compared to the high-end commercial vision\nsystem, for a fraction of the price. \n\n"}
{"id": "1612.02954", "contents": "Title: A series of maximum entropy upper bounds of the differential entropy Abstract: We present a series of closed-form maximum entropy upper bounds for the\ndifferential entropy of a continuous univariate random variable and study the\nproperties of that series. We then show how to use those generic bounds for\nupper bounding the differential entropy of Gaussian mixture models. This\nrequires to calculate the raw moments and raw absolute moments of Gaussian\nmixtures in closed-form that may also be handy in statistical machine learning\nand information theory. We report on our experiments and discuss on the\ntightness of those bounds. \n\n"}
{"id": "1612.03144", "contents": "Title: Feature Pyramid Networks for Object Detection Abstract: Feature pyramids are a basic component in recognition systems for detecting\nobjects at different scales. But recent deep learning object detectors have\navoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale, pyramidal\nhierarchy of deep convolutional networks to construct feature pyramids with\nmarginal extra cost. A top-down architecture with lateral connections is\ndeveloped for building high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN), shows significant\nimprovement as a generic feature extractor in several applications. Using FPN\nin a basic Faster R-CNN system, our method achieves state-of-the-art\nsingle-model results on the COCO detection benchmark without bells and\nwhistles, surpassing all existing single-model entries including those from the\nCOCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale object detection.\nCode will be made publicly available. \n\n"}
{"id": "1612.03217", "contents": "Title: Automatic Lymphocyte Detection in H&E Images with Deep Neural Networks Abstract: Automatic detection of lymphocyte in H&E images is a necessary first step in\nlots of tissue image analysis algorithms. An accurate and robust automated\nlymphocyte detection approach is of great importance in both computer science\nand clinical studies. Most of the existing approaches for lymphocyte detection\nare based on traditional image processing algorithms and/or classic machine\nlearning methods. In the recent years, deep learning techniques have\nfundamentally transformed the way that a computer interprets images and have\nbecome a matchless solution in various pattern recognition problems. In this\nwork, we design a new deep neural network model which extends the fully\nconvolutional network by combining the ideas in several recent techniques, such\nas shortcut links. Also, we design a new training scheme taking the prior\nknowledge about lymphocytes into consideration. The training scheme not only\nefficiently exploits the limited amount of free-form annotations from\npathologists, but also naturally supports efficient fine-tuning. As a\nconsequence, our model has the potential of self-improvement by leveraging the\nerrors collected during real applications. Our experiments show that our deep\nneural network model achieves good performance in the images of different\nstaining conditions or different types of tissues. \n\n"}
{"id": "1612.04229", "contents": "Title: Compressive Image Recovery Using Recurrent Generative Model Abstract: Reconstruction of signals from compressively sensed measurements is an\nill-posed problem. In this paper, we leverage the recurrent generative model,\nRIDE, as an image prior for compressive image reconstruction. Recurrent\nnetworks can model long-range dependencies in images and hence are suitable to\nhandle global multiplexing in reconstruction from compressive imaging. We\nperform MAP inference with RIDE using back-propagation to the inputs and\nprojected gradient method. We propose an entropy thresholding based approach\nfor preserving texture in images well. Our approach shows superior\nreconstructions compared to recent global reconstruction approaches like D-AMP\nand TVAL3 on both simulated and real data. \n\n"}
{"id": "1612.06530", "contents": "Title: Automatic Generation of Grounded Visual Questions Abstract: In this paper, we propose the first model to be able to generate visually\ngrounded questions with diverse types for a single image. Visual question\ngeneration is an emerging topic which aims to ask questions in natural language\nbased on visual input. To the best of our knowledge, it lacks automatic methods\nto generate meaningful questions with various types for the same visual input.\nTo circumvent the problem, we propose a model that automatically generates\nvisually grounded questions with varying types. Our model takes as input both\nimages and the captions generated by a dense caption model, samples the most\nprobable question types, and generates the questions in sequel. The\nexperimental results on two real world datasets show that our model outperforms\nthe strongest baseline in terms of both correctness and diversity with a wide\nmargin. \n\n"}
{"id": "1612.07625", "contents": "Title: Hardware for Machine Learning: Challenges and Opportunities Abstract: Machine learning plays a critical role in extracting meaningful information\nout of the zetabytes of sensor data collected every day. For some applications,\nthe goal is to analyze and understand the data to identify trends (e.g.,\nsurveillance, portable/wearable electronics); in other applications, the goal\nis to take immediate action based the data (e.g., robotics/drones, self-driving\ncars, smart Internet of Things). For many of these applications, local embedded\nprocessing near the sensor is preferred over the cloud due to privacy or\nlatency concerns, or limitations in the communication bandwidth. However, at\nthe sensor there are often stringent constraints on energy consumption and cost\nin addition to throughput and accuracy requirements. Furthermore, flexibility\nis often required such that the processing can be adapted for different\napplications or environments (e.g., update the weights and model in the\nclassifier). In many applications, machine learning often involves transforming\nthe input data into a higher dimensional space, which, along with programmable\nweights, increases data movement and consequently energy consumption. In this\npaper, we will discuss how these challenges can be addressed at various levels\nof hardware design ranging from architecture, hardware-friendly algorithms,\nmixed-signal circuits, and advanced technologies (including memories and\nsensors). \n\n"}
{"id": "1612.08185", "contents": "Title: PixelCNN Models with Auxiliary Variables for Natural Image Modeling Abstract: We study probabilistic models of natural images and extend the autoregressive\nfamily of PixelCNN architectures by incorporating auxiliary variables.\nSubsequently, we describe two new generative image models that exploit\ndifferent image transformations as auxiliary variables: a quantized grayscale\nview of the image or a multi-resolution image pyramid. The proposed models\ntackle two known shortcomings of existing PixelCNN models: 1) their tendency to\nfocus on low-level image details, while largely ignoring high-level image\ninformation, such as object shapes, and 2) their computationally costly\nprocedure for image sampling. We experimentally demonstrate benefits of the\nproposed models, in particular showing that they produce much more\nrealistically looking image samples than previous state-of-the-art\nprobabilistic models. \n\n"}
{"id": "1612.09542", "contents": "Title: A Joint Speaker-Listener-Reinforcer Model for Referring Expressions Abstract: Referring expressions are natural language constructions used to identify\nparticular objects within a scene. In this paper, we propose a unified\nframework for the tasks of referring expression comprehension and generation.\nOur model is composed of three modules: speaker, listener, and reinforcer. The\nspeaker generates referring expressions, the listener comprehends referring\nexpressions, and the reinforcer introduces a reward function to guide sampling\nof more discriminative expressions. The listener-speaker modules are trained\njointly in an end-to-end learning framework, allowing the modules to be aware\nof one another during learning while also benefiting from the discriminative\nreinforcer's feedback. We demonstrate that this unified framework and training\nachieves state-of-the-art results for both comprehension and generation on\nthree referring expression datasets. Project and demo page:\nhttps://vision.cs.unc.edu/refer \n\n"}
{"id": "1701.00295", "contents": "Title: Lifting from the Deep: Convolutional 3D Pose Estimation from a Single\n  Image Abstract: We propose a unified formulation for the problem of 3D human pose estimation\nfrom a single raw RGB image that reasons jointly about 2D joint estimation and\n3D pose reconstruction to improve both tasks. We take an integrated approach\nthat fuses probabilistic knowledge of 3D human pose with a multi-stage CNN\narchitecture and uses the knowledge of plausible 3D landmark locations to\nrefine the search for better 2D locations. The entire process is trained\nend-to-end, is extremely efficient and obtains state- of-the-art results on\nHuman3.6M outperforming previous approaches both on 2D and 3D errors. \n\n"}
{"id": "1701.00485", "contents": "Title: Two-Bit Networks for Deep Learning on Resource-Constrained Embedded\n  Devices Abstract: With the rapid proliferation of Internet of Things and intelligent edge\ndevices, there is an increasing need for implementing machine learning\nalgorithms, including deep learning, on resource-constrained mobile embedded\ndevices with limited memory and computation power. Typical large Convolutional\nNeural Networks (CNNs) need large amounts of memory and computational power,\nand cannot be deployed on embedded devices efficiently. We present Two-Bit\nNetworks (TBNs) for model compression of CNNs with edge weights constrained to\n(-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the\nmemory usage and improve computational efficiency significantly while achieving\ngood performance in terms of classification accuracy, thus representing a\nreasonable tradeoff between model size and performance. \n\n"}
{"id": "1701.01833", "contents": "Title: Oriented Response Networks Abstract: Deep Convolution Neural Networks (DCNNs) are capable of learning\nunprecedentedly effective image representations. However, their ability in\nhandling significant local and global image rotations remains limited. In this\npaper, we propose Active Rotating Filters (ARFs) that actively rotate during\nconvolution and produce feature maps with location and orientation explicitly\nencoded. An ARF acts as a virtual filter bank containing the filter itself and\nits multiple unmaterialised rotated versions. During back-propagation, an ARF\nis collectively updated using errors from all its rotated versions. DCNNs using\nARFs, referred to as Oriented Response Networks (ORNs), can produce\nwithin-class rotation-invariant deep features while maintaining inter-class\ndiscrimination for classification tasks. The oriented response produced by ORNs\ncan also be used for image and object orientation estimation tasks. Over\nmultiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we\nconsistently observe that replacing regular filters with the proposed ARFs\nleads to significant reduction in the number of network parameters and\nimprovement in classification performance. We report the best results on\nseveral commonly used benchmarks. \n\n"}
{"id": "1701.02343", "contents": "Title: Information Pursuit: A Bayesian Framework for Sequential Scene Parsing Abstract: Despite enormous progress in object detection and classification, the problem\nof incorporating expected contextual relationships among object instances into\nmodern recognition systems remains a key challenge. In this work we propose\nInformation Pursuit, a Bayesian framework for scene parsing that combines prior\nmodels for the geometry of the scene and the spatial arrangement of objects\ninstances with a data model for the output of high-level image classifiers\ntrained to answer specific questions about the scene. In the proposed\nframework, the scene interpretation is progressively refined as evidence\naccumulates from the answers to a sequence of questions. At each step, we\nchoose the question to maximize the mutual information between the new answer\nand the full interpretation given the current evidence obtained from previous\ninquiries. We also propose a method for learning the parameters of the model\nfrom synthesized, annotated scenes obtained by top-down sampling from an\neasy-to-learn generative scene model. Finally, we introduce a database of\nannotated indoor scenes of dining room tables, which we use to evaluate the\nproposed approach. \n\n"}
{"id": "1701.07372", "contents": "Title: A Multi-view RGB-D Approach for Human Pose Estimation in Operating Rooms Abstract: Many approaches have been proposed for human pose estimation in single and\nmulti-view RGB images. However, some environments, such as the operating room,\nare still very challenging for state-of-the-art RGB methods. In this paper, we\npropose an approach for multi-view 3D human pose estimation from RGB-D images\nand demonstrate the benefits of using the additional depth channel for pose\nrefinement beyond its use for the generation of improved features. The proposed\nmethod permits the joint detection and estimation of the poses without knowing\na priori the number of persons present in the scene. We evaluate this approach\non a novel multi-view RGB-D dataset acquired during live surgeries and\nannotated with ground truth 3D poses. \n\n"}
{"id": "1701.07717", "contents": "Title: Unlabeled Samples Generated by GAN Improve the Person Re-identification\n  Baseline in vitro Abstract: The main contribution of this paper is a simple semi-supervised pipeline that\nonly uses the original training set without collecting extra data. It is\nchallenging in 1) how to obtain more training data only from the training set\nand 2) how to use the newly generated data. In this work, the generative\nadversarial network (GAN) is used to generate unlabeled samples. We propose the\nlabel smoothing regularization for outliers (LSRO). This method assigns a\nuniform label distribution to the unlabeled images, which regularizes the\nsupervised model and improves the baseline. We verify the proposed method on a\npractical problem: person re-identification (re-ID). This task aims to retrieve\na query person from other cameras. We adopt the deep convolutional generative\nadversarial network (DCGAN) for sample generation, and a baseline convolutional\nneural network (CNN) for representation learning. Experiments show that adding\nthe GAN-generated data effectively improves the discriminative ability of\nlearned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and\nDukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1\nprecision over the baseline CNN, respectively. We additionally apply the\nproposed method to fine-grained bird recognition and achieve a +0.6%\nimprovement over a strong baseline. The code is available at\nhttps://github.com/layumi/Person-reID_GAN. \n\n"}
{"id": "1701.08261", "contents": "Title: Exploiting saliency for object segmentation from image level labels Abstract: There have been remarkable improvements in the semantic labelling task in the\nrecent years. However, the state of the art methods rely on large-scale\npixel-level annotations. This paper studies the problem of training a\npixel-wise semantic labeller network from image-level annotations of the\npresent object classes. Recently, it has been shown that high quality seeds\nindicating discriminative object regions can be obtained from image-level\nlabels. Without additional information, obtaining the full extent of the object\nis an inherently ill-posed problem due to co-occurrences. We propose using a\nsaliency model as additional information and hereby exploit prior knowledge on\nthe object extent and image statistics. We show how to combine both information\nsources in order to recover 80% of the fully supervised performance - which is\nthe new state of the art in weakly supervised training for pixel-wise semantic\nlabelling. The code is available at https://goo.gl/KygSeb. \n\n"}
{"id": "1701.08291", "contents": "Title: Treelogy: A Novel Tree Classifier Utilizing Deep and Hand-crafted\n  Representations Abstract: We propose a novel tree classification system called Treelogy, that fuses\ndeep representations with hand-crafted features obtained from leaf images to\nperform leaf-based plant classification. Key to this system are segmentation of\nthe leaf from an untextured background, using convolutional neural networks\n(CNNs) for learning deep representations, extracting hand-crafted features with\na number of image processing techniques, training a linear SVM with feature\nvectors, merging SVM and CNN results, and identifying the species from a\ndataset of 57 trees. Our classification results show that fusion of deep\nrepresentations with hand-crafted features leads to the highest accuracy. The\nproposed algorithm is embedded in a smart-phone application, which is publicly\navailable. Furthermore, our novel dataset comprised of 5408 leaf images is also\nmade public for use of other researchers. \n\n"}
{"id": "1702.00061", "contents": "Title: Vertical Landing for Micro Air Vehicles using Event-Based Optical Flow Abstract: Small flying robots can perform landing maneuvers using bio-inspired optical\nflow by maintaining a constant divergence. However, optical flow is typically\nestimated from frame sequences recorded by standard miniature cameras. This\nrequires processing full images on-board, limiting the update rate of\ndivergence measurements, and thus the speed of the control loop and the robot.\nEvent-based cameras overcome these limitations by only measuring pixel-level\nbrightness changes at microsecond temporal accuracy, hence providing an\nefficient mechanism for optical flow estimation. This paper presents, to the\nbest of our knowledge, the first work integrating event-based optical flow\nestimation into the control loop of a flying robot. We extend an existing\n'local plane fitting' algorithm to obtain an improved and more computationally\nefficient optical flow estimation method, valid for a wide range of optical\nflow velocities. This method is validated for real event sequences. In\naddition, a method for estimating the divergence from event-based optical flow\nis introduced, which accounts for the aperture problem. The developed\nalgorithms are implemented in a constant divergence landing controller on-board\nof a quadrotor. Experiments show that, using event-based optical flow, accurate\ndivergence estimates can be obtained over a wide range of speeds. This enables\nthe quadrotor to perform very fast landing maneuvers. \n\n"}
{"id": "1702.00382", "contents": "Title: Understanding trained CNNs by indexing neuron selectivity Abstract: The impressive performance of Convolutional Neural Networks (CNNs) when\nsolving different vision problems is shadowed by their black-box nature and our\nconsequent lack of understanding of the representations they build and how\nthese representations are organized. To help understanding these issues, we\npropose to describe the activity of individual neurons by their Neuron Feature\nvisualization and quantify their inherent selectivity with two specific\nproperties. We explore selectivity indexes for: an image feature (color); and\nan image label (class membership). Our contribution is a framework to seek or\nclassify neurons by indexing on these selectivity properties. It helps to find\ncolor selective neurons, such as a red-mushroom neuron in layer Conv4 or class\nselective neurons such as dog-face neurons in layer Conv5 in VGG-M, and\nestablishes a methodology to derive other selectivity properties. Indexing on\nneuron selectivity can statistically draw how features and classes are\nrepresented through layers in a moment when the size of trained nets is growing\nand automatic tools to index neurons can be helpful. \n\n"}
{"id": "1702.00783", "contents": "Title: Pixel Recursive Super Resolution Abstract: We present a pixel recursive super resolution model that synthesizes\nrealistic details into images while enhancing their resolution. A low\nresolution image may correspond to multiple plausible high resolution images,\nthus modeling the super resolution process with a pixel independent conditional\nmodel often results in averaging different details--hence blurry edges. By\ncontrast, our model is able to represent a multimodal conditional distribution\nby properly modeling the statistical dependencies among the high resolution\nimage pixels, conditioned on a low resolution input. We employ a PixelCNN\narchitecture to define a strong prior over natural images and jointly optimize\nthis prior with a deep conditioning convolutional network. Human evaluations\nindicate that samples from our proposed model look more photo realistic than a\nstrong L2 regression baseline. \n\n"}
{"id": "1702.03041", "contents": "Title: Reconstruction-Based Disentanglement for Pose-invariant Face Recognition Abstract: Deep neural networks (DNNs) trained on large-scale datasets have recently\nachieved impressive improvements in face recognition. But a persistent\nchallenge remains to develop methods capable of handling large pose variations\nthat are relatively underrepresented in training data. This paper presents a\nmethod for learning a feature representation that is invariant to pose, without\nrequiring extensive pose coverage in training data. We first propose to\ngenerate non-frontal views from a single frontal face, in order to increase the\ndiversity of training data while preserving accurate facial details that are\ncritical for identity discrimination. Our next contribution is to seek a rich\nembedding that encodes identity features, as well as non-identity ones such as\npose and landmark locations. Finally, we propose a new feature reconstruction\nmetric learning to explicitly disentangle identity and pose, by demanding\nalignment between the feature reconstructions through various combinations of\nidentity and pose features, which is obtained from two images of the same\nsubject. Experiments on both controlled and in-the-wild face datasets, such as\nMultiPIE, 300WLP and the profile view database CFP, show that our method\nconsistently outperforms the state-of-the-art, especially on images with large\nhead pose variations. Detail results and resource are referred to\nhttps://sites.google.com/site/xipengcshomepage/iccv2017 \n\n"}
{"id": "1702.03410", "contents": "Title: ArtGAN: Artwork Synthesis with Conditional Categorical GANs Abstract: This paper proposes an extension to the Generative Adversarial Networks\n(GANs), namely as ARTGAN to synthetically generate more challenging and complex\nimages such as artwork that have abstract characteristics. This is in contrast\nto most of the current solutions that focused on generating natural images such\nas room interiors, birds, flowers and faces. The key innovation of our work is\nto allow back-propagation of the loss function w.r.t. the labels (randomly\nassigned to each generated images) to the generator from the discriminator.\nWith the feedback from the label information, the generator is able to learn\nfaster and achieve better generated image quality. Empirically, we show that\nthe proposed ARTGAN is capable to create realistic artwork, as well as generate\ncompelling real world images that globally look natural with clear shape on\nCIFAR-10. \n\n"}
{"id": "1702.03920", "contents": "Title: Cognitive Mapping and Planning for Visual Navigation Abstract: We introduce a neural architecture for navigation in novel environments. Our\nproposed architecture learns to map from first-person views and plans a\nsequence of actions towards goals in the environment. The Cognitive Mapper and\nPlanner (CMP) is based on two key ideas: a) a unified joint architecture for\nmapping and planning, such that the mapping is driven by the needs of the task,\nand b) a spatial memory with the ability to plan given an incomplete set of\nobservations about the world. CMP constructs a top-down belief map of the world\nand applies a differentiable neural net planner to produce the next action at\neach time step. The accumulated belief of the world enables the agent to track\nvisited regions of the environment. We train and test CMP on navigation\nproblems in simulation environments derived from scans of real world buildings.\nOur experiments demonstrate that CMP outperforms alternate learning-based\narchitectures, as well as, classical mapping and path planning approaches in\nmany cases. Furthermore, it naturally extends to semantically specified goals,\nsuch as 'going to a chair'. We also deploy CMP on physical robots in indoor\nenvironments, where it achieves reasonable performance, even though it is\ntrained entirely in simulation. \n\n"}
{"id": "1702.04174", "contents": "Title: FERA 2017 - Addressing Head Pose in the Third Facial Expression\n  Recognition and Analysis Challenge Abstract: The field of Automatic Facial Expression Analysis has grown rapidly in recent\nyears. However, despite progress in new approaches as well as benchmarking\nefforts, most evaluations still focus on either posed expressions, near-frontal\nrecordings, or both. This makes it hard to tell how existing expression\nrecognition approaches perform under conditions where faces appear in a wide\nrange of poses (or camera views), displaying ecologically valid expressions.\nThe main obstacle for assessing this is the availability of suitable data, and\nthe challenge proposed here addresses this limitation. The FG 2017 Facial\nExpression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to\nthe estimation of Action Units occurrence and intensity under different camera\nviews. In this paper we present the third challenge in automatic recognition of\nfacial expressions, to be held in conjunction with the 12th IEEE conference on\nFace and Gesture Recognition, May 2017, in Washington, United States. Two\nsub-challenges are defined: the detection of AU occurrence, and the estimation\nof AU intensity. In this work we outline the evaluation protocol, the data\nused, and the results of a baseline method for both sub-challenges. \n\n"}
{"id": "1702.04479", "contents": "Title: Recognizing Dynamic Scenes with Deep Dual Descriptor based on Key Frames\n  and Key Segments Abstract: Recognizing dynamic scenes is one of the fundamental problems in scene\nunderstanding, which categorizes moving scenes such as a forest fire,\nlandslide, or avalanche. While existing methods focus on reliable capturing of\nstatic and dynamic information, few works have explored frame selection from a\ndynamic scene sequence. In this paper, we propose dynamic scene recognition\nusing a deep dual descriptor based on `key frames' and `key segments.' Key\nframes that reflect the feature distribution of the sequence with a small\nnumber are used for capturing salient static appearances. Key segments, which\nare captured from the area around each key frame, provide an additional\ndiscriminative power by dynamic patterns within short time intervals. To this\nend, two types of transferred convolutional neural network features are used in\nour approach. A fully connected layer is used to select the key frames and key\nsegments, while the convolutional layer is used to describe them. We conducted\nexperiments using public datasets as well as a new dataset comprised of 23\ndynamic scene classes with 10 videos per class. The evaluation results\ndemonstrated the state-of-the-art performance of the proposed method. \n\n"}
{"id": "1702.06086", "contents": "Title: Label Distribution Learning Forests Abstract: Label distribution learning (LDL) is a general learning framework, which\nassigns to an instance a distribution over a set of labels rather than a single\nlabel or multiple labels. Current LDL methods have either restricted\nassumptions on the expression form of the label distribution or limitations in\nrepresentation learning, e.g., to learn deep features in an end-to-end manner.\nThis paper presents label distribution learning forests (LDLFs) - a novel label\ndistribution learning algorithm based on differentiable decision trees, which\nhave several advantages: 1) Decision trees have the potential to model any\ngeneral form of label distributions by a mixture of leaf node predictions. 2)\nThe learning of differentiable decision trees can be combined with\nrepresentation learning. We define a distribution-based loss function for a\nforest, enabling all the trees to be learned jointly, and show that an update\nfunction for leaf node predictions, which guarantees a strict decrease of the\nloss function, can be derived by variational bounding. The effectiveness of the\nproposed LDLFs is verified on several LDL tasks and a computer vision\napplication, showing significant improvements to the state-of-the-art LDL\nmethods. \n\n"}
{"id": "1702.08272", "contents": "Title: A Dataset for Developing and Benchmarking Active Vision Abstract: We present a new public dataset with a focus on simulating robotic vision\ntasks in everyday indoor environments using real imagery. The dataset includes\n20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely\ncaptured in 9 unique scenes. We train a fast object category detector for\ninstance detection on our data. Using the dataset we show that, although\nincreasingly accurate and fast, the state of the art for object detection is\nstill severely impacted by object scale, occlusion, and viewing direction all\nof which matter for robotics applications. We next validate the dataset for\nsimulating active vision, and use the dataset to develop and evaluate a\ndeep-network-based system for next best move prediction for object\nclassification using reinforcement learning. Our dataset is available for\ndownload at cs.unc.edu/~ammirato/active_vision_dataset_website/. \n\n"}
{"id": "1702.08423", "contents": "Title: Age Progression/Regression by Conditional Adversarial Autoencoder Abstract: \"If I provide you a face image of mine (without telling you the actual age\nwhen I took the picture) and a large amount of face images that I crawled\n(containing labeled faces of different ages but not necessarily paired), can\nyou show me what I would look like when I am 80 or what I was like when I was\n5?\" The answer is probably a \"No.\" Most existing face aging works attempt to\nlearn the transformation between age groups and thus would require the paired\nsamples as well as the labeled query image. In this paper, we look at the\nproblem from a generative modeling perspective such that no paired samples is\nrequired. In addition, given an unlabeled image, the generative model can\ndirectly produce the image with desired age attribute. We propose a conditional\nadversarial autoencoder (CAAE) that learns a face manifold, traversing on which\nsmooth age progression and regression can be realized simultaneously. In CAAE,\nthe face is first mapped to a latent vector through a convolutional encoder,\nand then the vector is projected to the face manifold conditional on age\nthrough a deconvolutional generator. The latent vector preserves personalized\nface features (i.e., personality) and the age condition controls progression\nvs. regression. Two adversarial networks are imposed on the encoder and\ngenerator, respectively, forcing to generate more photo-realistic faces.\nExperimental results demonstrate the appealing performance and flexibility of\nthe proposed framework by comparing with the state-of-the-art and ground truth. \n\n"}
{"id": "1703.00395", "contents": "Title: Lossy Image Compression with Compressive Autoencoders Abstract: We propose a new approach to the problem of optimizing autoencoders for lossy\nimage compression. New media formats, changing hardware technology, as well as\ndiverse requirements and content types create a need for compression algorithms\nwhich are more flexible than existing codecs. Autoencoders have the potential\nto address this need, but are difficult to optimize directly due to the\ninherent non-differentiabilty of the compression loss. We here show that\nminimal changes to the loss are sufficient to train deep autoencoders\ncompetitive with JPEG 2000 and outperforming recently proposed approaches based\non RNNs. Our network is furthermore computationally efficient thanks to a\nsub-pixel architecture, which makes it suitable for high-resolution images.\nThis is in contrast to previous work on autoencoders for compression using\ncoarser approximations, shallower architectures, computationally expensive\nmethods, or focusing on small images. \n\n"}
{"id": "1703.00856", "contents": "Title: Araguaia Medical Vision Lab at ISIC 2017 Skin Lesion Classification\n  Challenge Abstract: This paper describes the participation of Araguaia Medical Vision Lab at the\nInternational Skin Imaging Collaboration 2017 Skin Lesion Challenge. We\ndescribe the use of deep convolutional neural networks in attempt to classify\nimages of Melanoma and Seborrheic Keratosis lesions. With use of finetuned\nGoogleNet and AlexNet we attained results of 0.950 and 0.846 AUC on Seborrheic\nKeratosis and Melanoma respectively. \n\n"}
{"id": "1703.00986", "contents": "Title: Belief Propagation in Conditional RBMs for Structured Prediction Abstract: Restricted Boltzmann machines~(RBMs) and conditional RBMs~(CRBMs) are popular\nmodels for a wide range of applications. In previous work, learning on such\nmodels has been dominated by contrastive divergence~(CD) and its variants.\nBelief propagation~(BP) algorithms are believed to be slow for structured\nprediction on conditional RBMs~(e.g., Mnih et al. [2011]), and not as good as\nCD when applied in learning~(e.g., Larochelle et al. [2012]). In this work, we\npresent a matrix-based implementation of belief propagation algorithms on\nCRBMs, which is easily scalable to tens of thousands of visible and hidden\nunits. We demonstrate that, in both maximum likelihood and max-margin learning,\ntraining conditional RBMs with BP as the inference routine can provide\nsignificantly better results than current state-of-the-art CD methods on\nstructured prediction problems. We also include practical guidelines on\ntraining CRBMs with BP, and some insights on the interaction of learning and\ninference algorithms for CRBMs. \n\n"}
{"id": "1703.01229", "contents": "Title: Deep Collaborative Learning for Visual Recognition Abstract: Deep neural networks are playing an important role in state-of-the-art visual\nrecognition. To represent high-level visual concepts, modern networks are\nequipped with large convolutional layers, which use a large number of filters\nand contribute significantly to model complexity. For example, more than half\nof the weights of AlexNet are stored in the first fully-connected layer (4,096\nfilters).\n  We formulate the function of a convolutional layer as learning a large visual\nvocabulary, and propose an alternative way, namely Deep Collaborative Learning\n(DCL), to reduce the computational complexity. We replace a convolutional layer\nwith a two-stage DCL module, in which we first construct a couple of smaller\nconvolutional layers individually, and then fuse them at each spatial position\nto consider feature co-occurrence. In mathematics, DCL can be explained as an\nefficient way of learning compositional visual concepts, in which the\nvocabulary size increases exponentially while the model complexity only\nincreases linearly. We evaluate DCL on a wide range of visual recognition\ntasks, including a series of multi-digit number classification datasets, and\nsome generic image classification datasets such as SVHN, CIFAR and ILSVRC2012.\nWe apply DCL to several state-of-the-art network structures, improving the\nrecognition accuracy meanwhile reducing the number of parameters (16.82% fewer\nin AlexNet). \n\n"}
{"id": "1703.01383", "contents": "Title: Wavelet Domain Residual Network (WavResNet) for Low-Dose X-ray CT\n  Reconstruction Abstract: Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT\nare computationally complex because of the repeated use of the forward and\nbackward projection. Inspired by this success of deep learning in computer\nvision applications, we recently proposed a deep convolutional neural network\n(CNN) for low-dose X-ray CT and won the second place in 2016 AAPM Low-Dose CT\nGrand Challenge. However, some of the texture are not fully recovered, which\nwas unfamiliar to some radiologists. To cope with this problem, here we propose\na direct residual learning approach on directional wavelet domain to solve this\nproblem and to improve the performance against previous work. In particular,\nthe new network estimates the noise of each input wavelet transform, and then\nthe de-noised wavelet coefficients are obtained by subtracting the noise from\nthe input wavelet transform bands. The experimental results confirm that the\nproposed network has significantly improved performance, preserving the detail\ntexture of the original images. \n\n"}
{"id": "1703.02719", "contents": "Title: Large Kernel Matters -- Improve Semantic Segmentation by Global\n  Convolutional Network Abstract: One of recent trends [30, 31, 14] in network architec- ture design is\nstacking small filters (e.g., 1x1 or 3x3) in the entire network because the\nstacked small filters is more ef- ficient than a large kernel, given the same\ncomputational complexity. However, in the field of semantic segmenta- tion,\nwhere we need to perform dense per-pixel prediction, we find that the large\nkernel (and effective receptive field) plays an important role when we have to\nperform the clas- sification and localization tasks simultaneously. Following\nour design principle, we propose a Global Convolutional Network to address both\nthe classification and localization issues for the semantic segmentation. We\nalso suggest a residual-based boundary refinement to further refine the ob-\nject boundaries. Our approach achieves state-of-art perfor- mance on two public\nbenchmarks and significantly outper- forms previous results, 82.2% (vs 80.2%)\non PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset. \n\n"}
{"id": "1703.03400", "contents": "Title: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks Abstract: We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies. \n\n"}
{"id": "1703.04101", "contents": "Title: Evaluating Deep Convolutional Neural Networks for Material\n  Classification Abstract: Determining the material category of a surface from an image is a demanding\ntask in perception that is drawing increasing attention. Following the recent\nremarkable results achieved for image classification and object detection\nutilising Convolutional Neural Networks (CNNs), we empirically study material\nclassification of everyday objects employing these techniques. More\nspecifically, we conduct a rigorous evaluation of how state-of-the art CNN\narchitectures compare on a common ground over widely used material databases.\nExperimental results on three challenging material databases show that the best\nperforming CNN architectures can achieve up to 94.99\\% mean average precision\nwhen classifying materials. \n\n"}
{"id": "1703.04363", "contents": "Title: Deep Value Networks Learn to Evaluate and Iteratively Refine Structured\n  Outputs Abstract: We approach structured output prediction by optimizing a deep value network\n(DVN) to precisely estimate the task loss on different output configurations\nfor a given input. Once the model is trained, we perform inference by gradient\ndescent on the continuous relaxations of the output variables to find outputs\nwith promising scores from the value network. When applied to image\nsegmentation, the value network takes an image and a segmentation mask as\ninputs and predicts a scalar estimating the intersection over union between the\ninput and ground truth masks. For multi-label classification, the DVN's\nobjective is to correctly predict the F1 score for any potential label\nconfiguration. The DVN framework achieves the state-of-the-art results on\nmulti-label prediction and image segmentation benchmarks. \n\n"}
{"id": "1703.04824", "contents": "Title: In Search of a Dataset for Handwritten Optical Music Recognition:\n  Introducing MUSCIMA++ Abstract: Optical Music Recognition (OMR) has long been without an adequate dataset and\nground truth for evaluating OMR systems, which has been a major problem for\nestablishing a state of the art in the field. Furthermore, machine learning\nmethods require training data. We analyze how the OMR processing pipeline can\nbe expressed in terms of gradually more complex ground truth, and based on this\nanalysis, we design the MUSCIMA++ dataset of handwritten music notation that\naddresses musical symbol recognition and notation reconstruction. The MUSCIMA++\ndataset version 0.9 consists of 140 pages of handwritten music, with 91255\nmanually annotated notation symbols and 82261 explicitly marked relationships\nbetween symbol pairs. The dataset allows training and evaluating models for\nsymbol classification, symbol localization, and notation graph assembly, both\nin isolation and jointly. Open-source tools are provided for manipulating the\ndataset, visualizing the data and further annotation, and the dataset itself is\nmade available under an open license. \n\n"}
{"id": "1703.04967", "contents": "Title: Comparison of the Deep-Learning-Based Automated Segmentation Methods for\n  the Head Sectioned Images of the Virtual Korean Human Project Abstract: This paper presents an end-to-end pixelwise fully automated segmentation of\nthe head sectioned images of the Visible Korean Human (VKH) project based on\nDeep Convolutional Neural Networks (DCNNs). By converting classification\nnetworks into Fully Convolutional Networks (FCNs), a coarse prediction map,\nwith smaller size than the original input image, can be created for\nsegmentation purposes. To refine this map and to obtain a dense pixel-wise\noutput, standard FCNs use deconvolution layers to upsample the coarse map.\nHowever, upsampling based on deconvolution increases the number of network\nparameters and causes loss of detail because of interpolation. On the other\nhand, dilated convolution is a new technique introduced recently that attempts\nto capture multi-scale contextual information without increasing the network\nparameters while keeping the resolution of the prediction maps high. We used\nboth a standard FCN and a dilated convolution based FCN for semantic\nsegmentation of the head sectioned images of the VKH dataset. Quantitative\nresults showed approximately 20% improvement in the segmentation accuracy when\nusing FCNs with dilated convolutions. \n\n"}
{"id": "1703.05463", "contents": "Title: Using Human Brain Activity to Guide Machine Learning Abstract: Machine learning is a field of computer science that builds algorithms that\nlearn. In many cases, machine learning algorithms are used to recreate a human\nability like adding a caption to a photo, driving a car, or playing a game.\nWhile the human brain has long served as a source of inspiration for machine\nlearning, little effort has been made to directly use data collected from\nworking brains as a guide for machine learning algorithms. Here we demonstrate\na new paradigm of \"neurally-weighted\" machine learning, which takes fMRI\nmeasurements of human brain activity from subjects viewing images, and infuses\nthese data into the training process of an object recognition learning\nalgorithm to make it more consistent with the human brain. After training,\nthese neurally-weighted classifiers are able to classify images without\nrequiring any additional neural data. We show that our neural-weighting\napproach can lead to large performance gains when used with traditional machine\nvision features, as well as to significant improvements with already\nhigh-performing convolutional neural network features. The effectiveness of\nthis approach points to a path forward for a new class of hybrid machine\nlearning algorithms which take both inspiration and direct constraints from\nneuronal data. \n\n"}
{"id": "1703.05502", "contents": "Title: Steganographic Generative Adversarial Networks Abstract: Steganography is collection of methods to hide secret information (\"payload\")\nwithin non-secret information \"container\"). Its counterpart, Steganalysis, is\nthe practice of determining if a message contains a hidden payload, and\nrecovering it if possible. Presence of hidden payloads is typically detected by\na binary classifier. In the present study, we propose a new model for\ngenerating image-like containers based on Deep Convolutional Generative\nAdversarial Networks (DCGAN). This approach allows to generate more\nsetganalysis-secure message embedding using standard steganography algorithms.\nExperiment results demonstrate that the new model successfully deceives the\nsteganography analyzer, and for this reason, can be used in steganographic\napplications. \n\n"}
{"id": "1703.05593", "contents": "Title: Convolutional neural network architecture for geometric matching Abstract: We address the problem of determining correspondences between two images in\nagreement with a geometric model such as an affine or thin-plate spline\ntransformation, and estimating its parameters. The contributions of this work\nare three-fold. First, we propose a convolutional neural network architecture\nfor geometric matching. The architecture is based on three main components that\nmimic the standard steps of feature extraction, matching and simultaneous\ninlier detection and model parameter estimation, while being trainable\nend-to-end. Second, we demonstrate that the network parameters can be trained\nfrom synthetically generated imagery without the need for manual annotation and\nthat our matching layer significantly increases generalization capabilities to\nnever seen before images. Finally, we show that the same model can perform both\ninstance-level and category-level matching giving state-of-the-art results on\nthe challenging Proposal Flow dataset. \n\n"}
{"id": "1703.08866", "contents": "Title: Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D\n  Cameras Abstract: Visual scene understanding is an important capability that enables robots to\npurposefully act in their environment. In this paper, we propose a novel\napproach to object-class segmentation from multiple RGB-D views using deep\nlearning. We train a deep neural network to predict object-class semantics that\nis consistent from several view points in a semi-supervised way. At test time,\nthe semantics predictions of our network can be fused more consistently in\nsemantic keyframe maps than predictions of a network trained on individual\nviews. We base our network architecture on a recent single-view deep learning\napproach to RGB and depth fusion for semantic object-class segmentation and\nenhance it with multi-scale loss minimization. We obtain the camera trajectory\nusing RGB-D SLAM and warp the predictions of RGB-D images into ground-truth\nannotated frames in order to enforce multi-view consistency during training. At\ntest time, predictions from multiple views are fused into keyframes. We propose\nand analyze several methods for enforcing multi-view consistency during\ntraining and testing. We evaluate the benefit of multi-view consistency\ntraining and demonstrate that pooling of deep features and fusion over multiple\nviews outperforms single-view baselines on the NYUDv2 benchmark for semantic\nsegmentation. Our end-to-end trained network achieves state-of-the-art\nperformance on the NYUDv2 dataset in single-view segmentation as well as\nmulti-view semantic fusion. \n\n"}
{"id": "1703.08966", "contents": "Title: Mastering Sketching: Adversarial Augmentation for Structured Prediction Abstract: We present an integral framework for training sketch simplification networks\nthat convert challenging rough sketches into clean line drawings. Our approach\naugments a simplification network with a discriminator network, training both\nnetworks jointly so that the discriminator network discerns whether a line\ndrawing is a real training data or the output of the simplification network,\nwhich in turn tries to fool it. This approach has two major advantages. First,\nbecause the discriminator network learns the structure in line drawings, it\nencourages the output sketches of the simplification network to be more similar\nin appearance to the training sketches. Second, we can also train the\nsimplification network with additional unsupervised data, using the\ndiscriminator network as a substitute teacher. Thus, by adding only rough\nsketches without simplified line drawings, or only line drawings without the\noriginal rough sketches, we can improve the quality of the sketch\nsimplification. We show how our framework can be used to train models that\nsignificantly outperform the state of the art in the sketch simplification\ntask, despite using the same architecture for inference. We additionally\npresent an approach to optimize for a single image, which improves accuracy at\nthe cost of additional computation time. Finally, we show that, using the same\nframework, it is possible to train the network to perform the inverse problem,\ni.e., convert simple line sketches into pencil drawings, which is not possible\nusing the standard mean squared error loss. We validate our framework with two\nuser tests, where our approach is preferred to the state of the art in sketch\nsimplification 92.3% of the time and obtains 1.2 more points on a scale of 1 to\n5. \n\n"}
{"id": "1703.09379", "contents": "Title: Robust Guided Image Filtering Abstract: The process of using one image to guide the filtering process of another one\nis called Guided Image Filtering (GIF). The main challenge of GIF is the\nstructure inconsistency between the guidance image and the target image.\nBesides, noise in the target image is also a challenging issue especially when\nit is heavy. In this paper, we propose a general framework for Robust Guided\nImage Filtering (RGIF), which contains a data term and a smoothness term, to\nsolve the two issues mentioned above. The data term makes our model\nsimultaneously denoise the target image and perform GIF which is robust against\nthe heavy noise. The smoothness term is able to make use of the property of\nboth the guidance image and the target image which is robust against the\nstructure inconsistency. While the resulting model is highly non-convex, it can\nbe solved through the proposed Iteratively Re-weighted Least Squares (IRLS) in\nan efficient manner. For challenging applications such as guided depth map\nupsampling, we further develop a data-driven parameter optimization scheme to\nproperly determine the parameter in our model. This optimization scheme can\nhelp to preserve small structures and sharp depth edges even for a large\nupsampling factor (8x for example). Moreover, the specially designed structure\nof the data term and the smoothness term makes our model perform well in\nedge-preserving smoothing for single-image tasks (i.e., the guidance image is\nthe target image itself). This paper is an extension of our previous work [1],\n[2]. \n\n"}
{"id": "1703.09880", "contents": "Title: Novel Structured Low-rank algorithm to recover spatially smooth\n  exponential image time series Abstract: We propose a structured low rank matrix completion algorithm to recover a\ntime series of images consisting of linear combination of exponential\nparameters at every pixel, from under-sampled Fourier measurements. The spatial\nsmoothness of these parameters is exploited along with the exponential\nstructure of the time series at every pixel, to derive an annihilation relation\nin the $k-t$ domain. This annihilation relation translates into a structured\nlow rank matrix formed from the $k-t$ samples. We demonstrate the algorithm in\nthe parameter mapping setting and show significant improvement over state of\nthe art methods. \n\n"}
{"id": "1703.10664", "contents": "Title: Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos Abstract: Deep learning has been demonstrated to achieve excellent results for image\nclassification and object detection. However, the impact of deep learning on\nvideo analysis (e.g. action detection and recognition) has been limited due to\ncomplexity of video data and lack of annotations. Previous convolutional neural\nnetworks (CNN) based video action detection approaches usually consist of two\nmajor steps: frame-level action proposal detection and association of proposals\nacross frames. Also, these methods employ two-stream CNN framework to handle\nspatial and temporal feature separately. In this paper, we propose an\nend-to-end deep network called Tube Convolutional Neural Network (T-CNN) for\naction detection in videos. The proposed architecture is a unified network that\nis able to recognize and localize action based on 3D convolution features. A\nvideo is first divided into equal length clips and for each clip a set of tube\nproposals are generated next based on 3D Convolutional Network (ConvNet)\nfeatures. Finally, the tube proposals of different clips are linked together\nemploying network flow and spatio-temporal action detection is performed using\nthese linked video proposals. Extensive experiments on several video datasets\ndemonstrate the superior performance of T-CNN for classifying and localizing\nactions in both trimmed and untrimmed videos compared to state-of-the-arts. \n\n"}
{"id": "1704.01152", "contents": "Title: Pose2Instance: Harnessing Keypoints for Person Instance Segmentation Abstract: Human keypoints are a well-studied representation of people.We explore how to\nuse keypoint models to improve instance-level person segmentation. The main\nidea is to harness the notion of a distance transform of oracle provided\nkeypoints or estimated keypoint heatmaps as a prior for person instance\nsegmentation task within a deep neural network. For training and evaluation, we\nconsider all those images from COCO where both instance segmentation and human\nkeypoints annotations are available. We first show how oracle keypoints can\nboost the performance of existing human segmentation model during inference\nwithout any training. Next, we propose a framework to directly learn a deep\ninstance segmentation model conditioned on human pose. Experimental results\nshow that at various Intersection Over Union (IOU) thresholds, in a constrained\nenvironment with oracle keypoints, the instance segmentation accuracy achieves\n10% to 12% relative improvements over a strong baseline of oracle bounding\nboxes. In a more realistic environment, without the oracle keypoints, the\nproposed deep person instance segmentation model conditioned on human pose\nachieves 3.8% to 10.5% relative improvements comparing with its strongest\nbaseline of a deep network trained only for segmentation. \n\n"}
{"id": "1704.01246", "contents": "Title: Estimation of Tissue Microstructure Using a Deep Network Inspired by a\n  Sparse Reconstruction Framework Abstract: Diffusion magnetic resonance imaging (dMRI) provides a unique tool for\nnoninvasively probing the microstructure of the neuronal tissue. The NODDI\nmodel has been a popular approach to the estimation of tissue microstructure in\nmany neuroscience studies. It represents the diffusion signals with three types\nof diffusion in tissue: intra-cellular, extra-cellular, and cerebrospinal fluid\ncompartments. However, the original NODDI method uses a computationally\nexpensive procedure to fit the model and could require a large number of\ndiffusion gradients for accurate microstructure estimation, which may be\nimpractical for clinical use. Therefore, efforts have been devoted to efficient\nand accurate NODDI microstructure estimation with a reduced number of diffusion\ngradients. In this work, we propose a deep network based approach to the NODDI\nmicrostructure estimation, which is named Microstructure Estimation using a\nDeep Network (MEDN). Motivated by the AMICO algorithm which accelerates the\ncomputation of NODDI parameters, we formulate the microstructure estimation\nproblem in a dictionary-based framework. The proposed network comprises two\ncascaded stages. The first stage resembles the solution to a dictionary-based\nsparse reconstruction problem and the second stage computes the final\nmicrostructure using the output of the first stage. The weights in the two\nstages are jointly learned from training data, which is obtained from training\ndMRI scans with diffusion gradients that densely sample the q-space. The\nproposed method was applied to brain dMRI scans, where two shells each with 30\ngradient directions (60 diffusion gradients in total) were used. Estimation\naccuracy with respect to the gold standard was measured and the results\ndemonstrate that MEDN outperforms the competing algorithms. \n\n"}
{"id": "1704.01372", "contents": "Title: On the Relation between Color Image Denoising and Classification Abstract: Large amount of image denoising literature focuses on single channel images\nand often experimentally validates the proposed methods on tens of images at\nmost. In this paper, we investigate the interaction between denoising and\nclassification on large scale dataset. Inspired by classification models, we\npropose a novel deep learning architecture for color (multichannel) image\ndenoising and report on thousands of images from ImageNet dataset as well as\ncommonly used imagery. We study the importance of (sufficient) training data,\nhow semantic class information can be traded for improved denoising results. As\na result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on\naverage over state-of-the art methods on large scale dataset. We conclude that\nit is beneficial to incorporate in classification models. On the other hand, we\nalso study how noise affect classification performance. In the end, we come to\na number of interesting conclusions, some being counter-intuitive. \n\n"}
{"id": "1704.01716", "contents": "Title: Action Representation Using Classifier Decision Boundaries Abstract: Most popular deep learning based models for action recognition are designed\nto generate separate predictions within their short temporal windows, which are\noften aggregated by heuristic means to assign an action label to the full video\nsegment. Given that not all frames from a video characterize the underlying\naction, pooling schemes that impose equal importance to all frames might be\nunfavorable. In an attempt towards tackling this challenge, we propose a novel\npooling scheme, dubbed SVM pooling, based on the notion that among the bag of\nfeatures generated by a CNN on all temporal windows, there is at least one\nfeature that characterizes the action. To this end, we learn a decision\nhyperplane that separates this unknown yet useful feature from the rest.\nApplying multiple instance learning in an SVM setup, we use the parameters of\nthis separating hyperplane as a descriptor for the video. Since these\nparameters are directly related to the support vectors in a max-margin\nframework, they serve as robust representations for pooling of the CNN\nfeatures. We devise a joint optimization objective and an efficient solver that\nlearns these hyperplanes per video and the corresponding action classifiers\nover the hyperplanes. Showcased experiments on the standard HMDB and UCF101\ndatasets demonstrate state-of-the-art performance. \n\n"}
{"id": "1704.01880", "contents": "Title: A Convolution Tree with Deconvolution Branches: Exploiting Geometric\n  Relationships for Single Shot Keypoint Detection Abstract: Recently, Deep Convolution Networks (DCNNs) have been applied to the task of\nface alignment and have shown potential for learning improved feature\nrepresentations. Although deeper layers can capture abstract concepts like\npose, it is difficult to capture the geometric relationships among the\nkeypoints in DCNNs. In this paper, we propose a novel convolution-deconvolution\nnetwork for facial keypoint detection. Our model predicts the 2D locations of\nthe keypoints and their individual visibility along with 3D head pose, while\nexploiting the spatial relationships among different keypoints. Different from\nexisting approaches of modeling these relationships, we propose learnable\ntransform functions which captures the relationships between keypoints at\nfeature level. However, due to extensive variations in pose, not all of these\nrelationships act at once, and hence we propose, a pose-based routing function\nwhich implicitly models the active relationships. Both transform functions and\nthe routing function are implemented through convolutions in a multi-task\nframework. Our approach presents a single-shot keypoint detection method,\nmaking it different from many existing cascade regression-based methods. We\nalso show that learning these relationships significantly improve the accuracy\nof keypoint detections for in-the-wild face images from challenging datasets\nsuch as AFW and AFLW. \n\n"}
{"id": "1704.02117", "contents": "Title: Partial Face Detection in the Mobile Domain Abstract: Generic face detection algorithms do not perform well in the mobile domain\ndue to significant presence of occluded and partially visible faces. One\npromising technique to handle the challenge of partial faces is to design face\ndetectors based on facial segments. In this paper two different approaches of\nfacial segment-based face detection are discussed, namely, proposal-based\ndetection and detection by end-to-end regression. Methods that follow the first\napproach rely on generating face proposals that contain facial segment\ninformation. The three detectors following this approach, namely Facial\nSegment-based Face Detector (FSFD), SegFace and DeepSegFace, discussed in this\npaper, perform binary classification on each proposal based on features learned\nfrom facial segments. The process of proposal generation, however, needs to be\nhandled separately, which can be very time consuming, and is not truly\nnecessary given the nature of the active authentication problem. Hence a novel\nalgorithm, Deep Regression-based User Image Detector (DRUID) is proposed, which\nshifts from the classification to the regression paradigm, thus obviating the\nneed for proposal generation. DRUID has an unique network architecture with\ncustomized loss functions, is trained using a relatively small amount of data\nby utilizing a novel data augmentation scheme and is fast since it outputs the\nbounding boxes of a face and its segments in a single pass. Being robust to\nocclusion by design, the facial segment-based face detection methods,\nespecially DRUID show superior performance over other state-of-the-art face\ndetectors in terms of precision-recall and ROC curve on two mobile face\ndatasets. \n\n"}
{"id": "1704.02203", "contents": "Title: Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic\n  Encryption Abstract: We propose a privacy-preserving framework for learning visual classifiers by\nleveraging distributed private image data. This framework is designed to\naggregate multiple classifiers updated locally using private data and to ensure\nthat no private information about the data is exposed during and after its\nlearning procedure. We utilize a homomorphic cryptosystem that can aggregate\nthe local classifiers while they are encrypted and thus kept secret. To\novercome the high computational cost of homomorphic encryption of\nhigh-dimensional classifiers, we (1) impose sparsity constraints on local\nclassifier updates and (2) propose a novel efficient encryption scheme named\ndoubly-permuted homomorphic encryption (DPHE) which is tailored to sparse\nhigh-dimensional data. DPHE (i) decomposes sparse data into its constituent\nnon-zero values and their corresponding support indices, (ii) applies\nhomomorphic encryption only to the non-zero values, and (iii) employs double\npermutations on the support indices to make them secret. Our experimental\nevaluation on several public datasets shows that the proposed approach achieves\ncomparable performance against state-of-the-art visual recognition methods\nwhile preserving privacy and significantly outperforms other privacy-preserving\nmethods. \n\n"}
{"id": "1704.02450", "contents": "Title: Coupled Deep Learning for Heterogeneous Face Recognition Abstract: Heterogeneous face matching is a challenge issue in face recognition due to\nlarge domain difference as well as insufficient pairwise images in different\nmodalities during training. This paper proposes a coupled deep learning (CDL)\napproach for the heterogeneous face matching. CDL seeks a shared feature space\nin which the heterogeneous face matching problem can be approximately treated\nas a homogeneous face matching problem. The objective function of CDL mainly\nincludes two parts. The first part contains a trace norm and a block-diagonal\nprior as relevance constraints, which not only make unpaired images from\nmultiple modalities be clustered and correlated, but also regularize the\nparameters to alleviate overfitting. An approximate variational formulation is\nintroduced to deal with the difficulties of optimizing low-rank constraint\ndirectly. The second part contains a cross modal ranking among triplet domain\nspecific images to maximize the margin for different identities and increase\ndata for a small amount of training samples. Besides, an alternating\nminimization method is employed to iteratively update the parameters of CDL.\nExperimental results show that CDL achieves better performance on the\nchallenging CASIA NIR-VIS 2.0 face recognition database, the IIIT-D Sketch\ndatabase, the CUHK Face Sketch (CUFS), and the CUHK Face Sketch FERET (CUFSF),\nwhich significantly outperforms state-of-the-art heterogeneous face recognition\nmethods. \n\n"}
{"id": "1704.02712", "contents": "Title: Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation Abstract: Many modern computer vision and machine learning applications rely on solving\ndifficult optimization problems that involve non-differentiable objective\nfunctions and constraints. The alternating direction method of multipliers\n(ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a\ngeneralization of ADMM that often achieves better performance, but its\nefficiency depends strongly on algorithm parameters that must be chosen by an\nexpert user. We propose an adaptive method that automatically tunes the key\nalgorithm parameters to achieve optimal performance without user oversight.\nInspired by recent work on adaptivity, the proposed adaptive relaxed ADMM\n(ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A\ndetailed convergence analysis of ARADMM is provided, and numerical results on\nseveral applications demonstrate fast practical convergence. \n\n"}
{"id": "1704.02827", "contents": "Title: Learning Human Motion Models for Long-term Predictions Abstract: We propose a new architecture for the learning of predictive spatio-temporal\nmotion models from data alone. Our approach, dubbed the Dropout Autoencoder\nLSTM, is capable of synthesizing natural looking motion sequences over long\ntime horizons without catastrophic drift or motion degradation. The model\nconsists of two components, a 3-layer recurrent neural network to model\ntemporal aspects and a novel auto-encoder that is trained to implicitly recover\nthe spatial structure of the human skeleton via randomly removing information\nabout joints during training time. This Dropout Autoencoder (D-AE) is then used\nto filter each predicted pose of the LSTM, reducing accumulation of error and\nhence drift over time. Furthermore, we propose new evaluation protocols to\nassess the quality of synthetic motion sequences even for which no ground truth\ndata exists. The proposed protocols can be used to assess generated sequences\nof arbitrary length. Finally, we evaluate our proposed method on two of the\nlargest motion-capture datasets available to date and show that our model\noutperforms the state-of-the-art on a variety of actions, including cyclic and\nacyclic motion, and that it can produce natural looking sequences over longer\ntime horizons than previous methods. \n\n"}
{"id": "1704.04511", "contents": "Title: Recovery of damped exponentials using structured low rank matrix\n  completion Abstract: We introduce a structured low rank matrix completion algorithm to recover a\nseries of images from their under-sampled measurements, where the signal along\nthe parameter dimension at every pixel is described by a linear combination of\nexponentials. We exploit the exponential behavior of the signal at every pixel,\nalong with the spatial smoothness of the exponential parameters to derive an\nannihilation relation in the Fourier domain. This relation translates to a\nlow-rank property on a structured matrix constructed from the Fourier samples.\nWe enforce the low rank property of the structured matrix as a regularization\nprior to recover the images. Since the direct use of current low rank matrix\nrecovery schemes to this problem is associated with high computational\ncomplexity and memory demand, we adopt an iterative re-weighted least squares\n(IRLS) algorithm, which facilitates the exploitation of the convolutional\nstructure of the matrix. Novel approximations involving two dimensional Fast\nFourier Transforms (FFT) are introduced to drastically reduce the memory demand\nand computational complexity, which facilitates the extension of structured low\nrank methods to large scale three dimensional problems. We demonstrate our\nalgorithm in the MR parameter mapping setting and show improvement over the\nstate-of-the-art methods. \n\n"}
{"id": "1704.04516", "contents": "Title: Interpretable 3D Human Action Analysis with Temporal Convolutional\n  Networks Abstract: The discriminative power of modern deep learning models for 3D human action\nrecognition is growing ever so potent. In conjunction with the recent\nresurgence of 3D human action representation with 3D skeletons, the quality and\nthe pace of recent progress have been significant. However, the inner workings\nof state-of-the-art learning based methods in 3D human action recognition still\nremain mostly black-box. In this work, we propose to use a new class of models\nknown as Temporal Convolutional Neural Networks (TCN) for 3D human action\nrecognition. Compared to popular LSTM-based Recurrent Neural Network models,\ngiven interpretable input such as 3D skeletons, TCN provides us a way to\nexplicitly learn readily interpretable spatio-temporal representations for 3D\nhuman action recognition. We provide our strategy in re-designing the TCN with\ninterpretability in mind and how such characteristics of the model is leveraged\nto construct a powerful 3D activity recognition method. Through this work, we\nwish to take a step towards a spatio-temporal model that is easier to\nunderstand, explain and interpret. The resulting model, Res-TCN, achieves\nstate-of-the-art results on the largest 3D human action recognition dataset,\nNTU-RGBD. \n\n"}
{"id": "1704.05519", "contents": "Title: Computer Vision for Autonomous Vehicles: Problems, Datasets and State of\n  the Art Abstract: Recent years have witnessed enormous progress in AI-related fields such as\ncomputer vision, machine learning, and autonomous vehicles. As with any rapidly\ngrowing field, it becomes increasingly difficult to stay up-to-date or enter\nthe field as a beginner. While several survey papers on particular sub-problems\nhave appeared, no comprehensive survey on problems, datasets, and methods in\ncomputer vision for autonomous vehicles has been published. This book attempts\nto narrow this gap by providing a survey on the state-of-the-art datasets and\ntechniques. Our survey includes both the historically most relevant literature\nas well as the current state of the art on several specific topics, including\nrecognition, reconstruction, motion estimation, tracking, scene understanding,\nand end-to-end learning for autonomous driving. Towards this goal, we analyze\nthe performance of the state of the art on several challenging benchmarking\ndatasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open\nproblems and current research challenges. To ease accessibility and accommodate\nmissing references, we also provide a website that allows navigating topics as\nwell as methods and provides additional information. \n\n"}
{"id": "1704.05796", "contents": "Title: Network Dissection: Quantifying Interpretability of Deep Visual\n  Representations Abstract: We propose a general framework called Network Dissection for quantifying the\ninterpretability of latent representations of CNNs by evaluating the alignment\nbetween individual hidden units and a set of semantic concepts. Given any CNN\nmodel, the proposed method draws on a broad data set of visual concepts to\nscore the semantics of hidden units at each intermediate convolutional layer.\nThe units with semantics are given labels across a range of objects, parts,\nscenes, textures, materials, and colors. We use the proposed method to test the\nhypothesis that interpretability of units is equivalent to random linear\ncombinations of units, then we apply our method to compare the latent\nrepresentations of various networks when trained to solve different supervised\nand self-supervised training tasks. We further analyze the effect of training\niterations, compare networks trained with different initializations, examine\nthe impact of network depth and width, and measure the effect of dropout and\nbatch normalization on the interpretability of deep visual representations. We\ndemonstrate that the proposed method can shed light on characteristics of CNN\nmodels and training methods that go beyond measurements of their discriminative\npower. \n\n"}
{"id": "1704.05831", "contents": "Title: Learning to Generate Long-term Future via Hierarchical Prediction Abstract: We propose a hierarchical approach for making long-term predictions of future\nframes. To avoid inherent compounding errors in recursive pixel-level\nprediction, we propose to first estimate high-level structure in the input\nframes, then predict how that structure evolves in the future, and finally by\nobserving a single frame from the past and the predicted high-level structure,\nwe construct the future frames without having to observe any of the pixel-level\npredictions. Long-term video prediction is difficult to perform by recurrently\nobserving the predicted frames because the small errors in pixel space\nexponentially amplify as predictions are made deeper into the future. Our\napproach prevents pixel-level error propagation from happening by removing the\nneed to observe the predicted frames. Our model is built with a combination of\nLSTM and analogy based encoder-decoder convolutional neural networks, which\nindependently predict the video structure and generate the future frames,\nrespectively. In experiments, our model is evaluated on the Human3.6M and Penn\nAction datasets on the task of long-term pixel-level video prediction of humans\nperforming actions and demonstrate significantly better results than the\nstate-of-the-art. \n\n"}
{"id": "1704.05959", "contents": "Title: SLAM with Objects using a Nonparametric Pose Graph Abstract: Mapping and self-localization in unknown environments are fundamental\ncapabilities in many robotic applications. These tasks typically involve the\nidentification of objects as unique features or landmarks, which requires the\nobjects both to be detected and then assigned a unique identifier that can be\nmaintained when viewed from different perspectives and in different images. The\n\\textit{data association} and \\textit{simultaneous localization and mapping}\n(SLAM) problems are, individually, well-studied in the literature. But these\ntwo problems are inherently tightly coupled, and that has not been\nwell-addressed. Without accurate SLAM, possible data associations are\ncombinatorial and become intractable easily. Without accurate data association,\nthe error of SLAM algorithms diverge easily. This paper proposes a novel\nnonparametric pose graph that models data association and SLAM in a single\nframework. An algorithm is further introduced to alternate between inferring\ndata association and performing SLAM. Experimental results show that our\napproach has the new capability of associating object detections and localizing\nobjects at the same time, leading to significantly better performance on both\nthe data association and SLAM problems than achieved by considering only one\nand ignoring imperfections in the other. \n\n"}
{"id": "1704.06178", "contents": "Title: Exploring epoch-dependent stochastic residual networks Abstract: The recently proposed stochastic residual networks selectively activate or\nbypass the layers during training, based on independent stochastic choices,\neach of which following a probability distribution that is fixed in advance. In\nthis paper we present a first exploration on the use of an epoch-dependent\ndistribution, starting with a higher probability of bypassing deeper layers and\nthen activating them more frequently as training progresses. Preliminary\nresults are mixed, yet they show some potential of adding an epoch-dependent\nmanagement of distributions, worth of further investigation. \n\n"}
{"id": "1704.06369", "contents": "Title: NormFace: L2 Hypersphere Embedding for Face Verification Abstract: Thanks to the recent developments of Convolutional Neural Networks, the\nperformance of face verification methods has increased rapidly. In a typical\nface verification method, feature normalization is a critical step for boosting\nperformance. This motivates us to introduce and study the effect of\nnormalization during training. But we find this is non-trivial, despite\nnormalization being differentiable. We identify and study four issues related\nto normalization through mathematical analysis, which yields understanding and\nhelps with parameter settings. Based on this analysis we propose two strategies\nfor training using normalized features. The first is a modification of softmax\nloss, which optimizes cosine similarity instead of inner-product. The second is\na reformulation of metric learning by introducing an agent vector for each\nclass. We show that both strategies, and small variants, consistently improve\nperformance by between 0.2% to 0.4% on the LFW dataset based on two models.\nThis is significant because the performance of the two models on LFW dataset is\nclose to saturation at over 98%. Codes and models are released on\nhttps://github.com/happynear/NormFace \n\n"}
{"id": "1704.06821", "contents": "Title: Deep Learning based Isolated Arabic Scene Character Recognition Abstract: The technological advancement and sophistication in cameras and gadgets\nprompt researchers to have focus on image analysis and text understanding. The\ndeep learning techniques demonstrated well to assess the potential for\nclassifying text from natural scene images as reported in recent years. There\nare variety of deep learning approaches that prospects the detection and\nrecognition of text, effectively from images. In this work, we presented Arabic\nscene text recognition using Convolutional Neural Networks (ConvNets) as a deep\nlearning classifier. As the scene text data is slanted and skewed, thus to deal\nwith maximum variations, we employ five orientations with respect to single\noccurrence of a character. The training is formulated by keeping filter size 3\nx 3 and 5 x 5 with stride value as 1 and 2. During text classification phase,\nwe trained network with distinct learning rates. Our approach reported\nencouraging results on recognition of Arabic characters from segmented Arabic\nscene images. \n\n"}
{"id": "1704.06843", "contents": "Title: On the Two-View Geometry of Unsynchronized Cameras Abstract: We present new methods for simultaneously estimating camera geometry and time\nshift from video sequences from multiple unsynchronized cameras. Algorithms for\nsimultaneous computation of a fundamental matrix or a homography with unknown\ntime shift between images are developed. Our methods use minimal correspondence\nsets (eight for fundamental matrix and four and a half for homography) and\ntherefore are suitable for robust estimation using RANSAC. Furthermore, we\npresent an iterative algorithm that extends the applicability on sequences\nwhich are significantly unsynchronized, finding the correct time shift up to\nseveral seconds. We evaluated the methods on synthetic and wide range of real\nworld datasets and the results show a broad applicability to the problem of\ncamera synchronization. \n\n"}
{"id": "1704.07434", "contents": "Title: Paying Attention to Descriptions Generated by Image Captioning Models Abstract: To bridge the gap between humans and machines in image understanding and\ndescribing, we need further insight into how people describe a perceived scene.\nIn this paper, we study the agreement between bottom-up saliency-based visual\nattention and object referrals in scene description constructs. We investigate\nthe properties of human-written descriptions and machine-generated ones. We\nthen propose a saliency-boosted image captioning model in order to investigate\nbenefits from low-level cues in language models. We learn that (1) humans\nmention more salient objects earlier than less salient ones in their\ndescriptions, (2) the better a captioning model performs, the better attention\nagreement it has with human descriptions, (3) the proposed saliency-boosted\nmodel, compared to its baseline form, does not improve significantly on the MS\nCOCO database, indicating explicit bottom-up boosting does not help when the\ntask is well learnt and tuned on a data, (4) a better generalization is,\nhowever, observed for the saliency-boosted model on unseen data. \n\n"}
{"id": "1704.07489", "contents": "Title: Multi-Task Video Captioning with Video and Entailment Generation Abstract: Video captioning, the task of describing the content of a video, has seen\nsome promising improvements in recent years with sequence-to-sequence models,\nbut accurately learning the temporal and logical dynamics involved in the task\nstill remains a challenge, especially given the lack of sufficient annotated\ndata. We improve video captioning by sharing knowledge with two related\ndirected-generation tasks: a temporally-directed unsupervised video prediction\ntask to learn richer context-aware video encoder representations, and a\nlogically-directed language entailment generation task to learn better\nvideo-entailed caption decoder representations. For this, we present a\nmany-to-many multi-task learning model that shares parameters across the\nencoders and decoders of the three tasks. We achieve significant improvements\nand the new state-of-the-art on several standard video captioning datasets\nusing diverse automatic and human evaluations. We also show mutual multi-task\nimprovements on the entailment generation task. \n\n"}
{"id": "1704.07816", "contents": "Title: Introspective Classification with Convolutional Nets Abstract: We propose introspective convolutional networks (ICN) that emphasize the\nimportance of having convolutional neural networks empowered with generative\ncapabilities. We employ a reclassification-by-synthesis algorithm to perform\ntraining using a formulation stemmed from the Bayes theory. Our ICN tries to\niteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by\nimproving the classification. The single CNN classifier learned is at the same\ntime generative --- being able to directly synthesize new samples within its\nown discriminative model. We conduct experiments on benchmark datasets\nincluding MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures,\nand observe improved classification results. \n\n"}
{"id": "1704.08328", "contents": "Title: Face Identification and Clustering Abstract: In this thesis, we study two problems based on clustering algorithms. In the\nfirst problem, we study the role of visual attributes using an agglomerative\nclustering algorithm to whittle down the search area where the number of\nclasses is high to improve the performance of clustering. We observe that as we\nadd more attributes, the clustering performance increases overall. In the\nsecond problem, we study the role of clustering in aggregating templates in a\n1:N open set protocol using multi-shot video as a probe. We observe that by\nincreasing the number of clusters, the performance increases with respect to\nthe baseline and reaches a peak, after which increasing the number of clusters\ncauses the performance to degrade. Experiments are conducted using recently\nintroduced unconstrained IARPA Janus IJB-A, CS2, and CS3 face recognition\ndatasets. \n\n"}
{"id": "1704.08740", "contents": "Title: Improving Facial Attribute Prediction using Semantic Segmentation Abstract: Attributes are semantically meaningful characteristics whose applicability\nwidely crosses category boundaries. They are particularly important in\ndescribing and recognizing concepts where no explicit training example is\ngiven, \\textit{e.g., zero-shot learning}. Additionally, since attributes are\nhuman describable, they can be used for efficient human-computer interaction.\nIn this paper, we propose to employ semantic segmentation to improve facial\nattribute prediction. The core idea lies in the fact that many facial\nattributes describe local properties. In other words, the probability of an\nattribute to appear in a face image is far from being uniform in the spatial\ndomain. We build our facial attribute prediction model jointly with a deep\nsemantic segmentation network. This harnesses the localization cues learned by\nthe semantic segmentation to guide the attention of the attribute prediction to\nthe regions where different attributes naturally show up. As a result of this\napproach, in addition to recognition, we are able to localize the attributes,\ndespite merely having access to image level labels (weak supervision) during\ntraining. We evaluate our proposed method on CelebA and LFWA datasets and\nachieve superior results to the prior arts. Furthermore, we show that in the\nreverse problem, semantic face parsing improves when facial attributes are\navailable. That reaffirms the need to jointly model these two interconnected\ntasks. \n\n"}
{"id": "1705.00346", "contents": "Title: Deep Learning in the Automotive Industry: Applications and Tools Abstract: Deep Learning refers to a set of machine learning techniques that utilize\nneural networks with many hidden layers for tasks, such as image\nclassification, speech recognition, language understanding. Deep learning has\nbeen proven to be very effective in these domains and is pervasively used by\nmany Internet services. In this paper, we describe different automotive uses\ncases for deep learning in particular in the domain of computer vision. We\nsurveys the current state-of-the-art in libraries, tools and infrastructures\n(e.\\,g.\\ GPUs and clouds) for implementing, training and deploying deep neural\nnetworks. We particularly focus on convolutional neural networks and computer\nvision use cases, such as the visual inspection process in manufacturing plants\nand the analysis of social media data. To train neural networks, curated and\nlabeled datasets are essential. In particular, both the availability and scope\nof such datasets is typically very limited. A main contribution of this paper\nis the creation of an automotive dataset, that allows us to learn and\nautomatically recognize different vehicle properties. We describe an end-to-end\ndeep learning application utilizing a mobile app for data collection and\nprocess support, and an Amazon-based cloud backend for storage and training.\nFor training we evaluate the use of cloud and on-premises infrastructures\n(including multiple GPUs) in conjunction with different neural network\narchitectures and frameworks. We assess both the training times as well as the\naccuracy of the classifier. Finally, we demonstrate the effectiveness of the\ntrained classifier in a real world setting during manufacturing process. \n\n"}
{"id": "1705.01156", "contents": "Title: Shading Annotations in the Wild Abstract: Understanding shading effects in images is critical for a variety of vision\nand graphics problems, including intrinsic image decomposition, shadow removal,\nimage relighting, and inverse rendering. As is the case with other vision\ntasks, machine learning is a promising approach to understanding shading - but\nthere is little ground truth shading data available for real-world images. We\nintroduce Shading Annotations in the Wild (SAW), a new large-scale, public\ndataset of shading annotations in indoor scenes, comprised of multiple forms of\nshading judgments obtained via crowdsourcing, along with shading annotations\nautomatically generated from RGB-D imagery. We use this data to train a\nconvolutional neural network to predict per-pixel shading information in an\nimage. We demonstrate the value of our data and network in an application to\nintrinsic images, where we can reduce decomposition artifacts produced by\nexisting algorithms. Our database is available at\nhttp://opensurfaces.cs.cornell.edu/saw/. \n\n"}
{"id": "1705.01217", "contents": "Title: Marine Animal Classification with Correntropy Loss Based Multi-view\n  Learning Abstract: To analyze marine animals behavior, seasonal distribution and abundance,\ndigital imagery can be acquired by visual or Lidar camera. Depending on the\nquantity and properties of acquired imagery, the animals are characterized as\neither features (shape, color, texture, etc.), or dissimilarity matrices\nderived from different shape analysis methods (shape context, internal distance\nshape context, etc.). For both cases, multi-view learning is critical in\nintegrating more than one set of feature or dissimilarity matrix for higher\nclassification accuracy. This paper adopts correntropy loss as cost function in\nmulti-view learning, which has favorable statistical properties for rejecting\nnoise. For the case of features, the correntropy loss-based multi-view learning\nand its entrywise variation are developed based on the multi-view intact space\nlearning algorithm. For the case of dissimilarity matrices, the robust\nEuclidean embedding algorithm is extended to its multi-view form with the\ncorrentropy loss function. Results from simulated data and real-world marine\nanimal imagery show that the proposed algorithms can effectively enhance\nclassification rate, as well as suppress noise under different noise\nconditions. \n\n"}
{"id": "1705.01389", "contents": "Title: Learning to Estimate 3D Hand Pose from Single RGB Images Abstract: Low-cost consumer depth cameras and deep learning have enabled reasonable 3D\nhand pose estimation from single depth images. In this paper, we present an\napproach that estimates 3D hand pose from regular RGB images. This task has far\nmore ambiguities due to the missing depth information. To this end, we propose\na deep network that learns a network-implicit 3D articulation prior. Together\nwith detected keypoints in the images, this network yields good estimates of\nthe 3D pose. We introduce a large scale 3D hand pose dataset based on synthetic\nhand models for training the involved networks. Experiments on a variety of\ntest sets, including one on sign language recognition, demonstrate the\nfeasibility of 3D hand pose estimation on single color images. \n\n"}
{"id": "1705.01842", "contents": "Title: A Deep Learning Perspective on the Origin of Facial Expressions Abstract: Facial expressions play a significant role in human communication and\nbehavior. Psychologists have long studied the relationship between facial\nexpressions and emotions. Paul Ekman et al., devised the Facial Action Coding\nSystem (FACS) to taxonomize human facial expressions and model their behavior.\nThe ability to recognize facial expressions automatically, enables novel\napplications in fields like human-computer interaction, social gaming, and\npsychological research. There has been a tremendously active research in this\nfield, with several recent papers utilizing convolutional neural networks (CNN)\nfor feature extraction and inference. In this paper, we employ CNN\nunderstanding methods to study the relation between the features these\ncomputational networks are using, the FACS and Action Units (AU). We verify our\nfindings on the Extended Cohn-Kanade (CK+), NovaEmotions and FER2013 datasets.\nWe apply these models to various tasks and tests using transfer learning,\nincluding cross-dataset validation and cross-task performance. Finally, we\nexploit the nature of the FER based CNN models for the detection of\nmicro-expressions and achieve state-of-the-art accuracy using a simple\nlong-short-term-memory (LSTM) recurrent neural network (RNN). \n\n"}
{"id": "1705.01921", "contents": "Title: Recurrent Soft Attention Model for Common Object Recognition Abstract: We propose the Recurrent Soft Attention Model, which integrates the visual\nattention from the original image to a LSTM memory cell through a down-sample\nnetwork. The model recurrently transmits visual attention to the memory cells\nfor glimpse mask generation, which is a more natural way for attention\nintegration and exploitation in general object detection and recognition\nproblem. We test our model under the metric of the top-1 accuracy on the\nCIFAR-10 dataset. The experiment shows that our down-sample network and\nfeedback mechanism plays an effective role among the whole network structure. \n\n"}
{"id": "1705.02727", "contents": "Title: Automatic Recognition of Mammal Genera on Camera-Trap Images using\n  Multi-Layer Robust Principal Component Analysis and Mixture Neural Networks Abstract: The segmentation and classification of animals from camera-trap images is due\nto the conditions under which the images are taken, a difficult task. This work\npresents a method for classifying and segmenting mammal genera from camera-trap\nimages. Our method uses Multi-Layer Robust Principal Component Analysis (RPCA)\nfor segmenting, Convolutional Neural Networks (CNNs) for extracting features,\nLeast Absolute Shrinkage and Selection Operator (LASSO) for selecting features,\nand Artificial Neural Networks (ANNs) or Support Vector Machines (SVM) for\nclassifying mammal genera present in the Colombian forest. We evaluated our\nmethod with the camera-trap images from the Alexander von Humboldt Biological\nResources Research Institute. We obtained an accuracy of 92.65% classifying 8\nmammal genera and a False Positive (FP) class, using automatic-segmented\nimages. On the other hand, we reached 90.32% of accuracy classifying 10 mammal\ngenera, using ground-truth images only. Unlike almost all previous works, we\nconfront the animal segmentation and genera classification in the camera-trap\nrecognition. This method shows a new approach toward a fully-automatic\ndetection of animals from camera-trap images. \n\n"}
{"id": "1705.02966", "contents": "Title: You said that? Abstract: We present a method for generating a video of a talking face. The method\ntakes as inputs: (i) still images of the target face, and (ii) an audio speech\nsegment; and outputs a video of the target face lip synched with the audio. The\nmethod runs in real time and is applicable to faces and audio not seen at\ntraining time.\n  To achieve this we propose an encoder-decoder CNN model that uses a joint\nembedding of the face and audio to generate synthesised talking face video\nframes. The model is trained on tens of hours of unlabelled videos.\n  We also show results of re-dubbing videos using speech from a different\nperson. \n\n"}
{"id": "1705.03865", "contents": "Title: Survey of Visual Question Answering: Datasets and Techniques Abstract: Visual question answering (or VQA) is a new and exciting problem that\ncombines natural language processing and computer vision techniques. We present\na survey of the various datasets and models that have been used to tackle this\ntask. The first part of the survey details the various datasets for VQA and\ncompares them along some common factors. The second part of this survey details\nthe different approaches for VQA, classified into four types: non-deep learning\nmodels, deep learning models without attention, deep learning models with\nattention, and other models which do not fit into the first three. Finally, we\ncompare the performances of these approaches and provide some directions for\nfuture work. \n\n"}
{"id": "1705.04258", "contents": "Title: Probabilistic Image Colorization Abstract: We develop a probabilistic technique for colorizing grayscale natural images.\nIn light of the intrinsic uncertainty of this task, the proposed probabilistic\nframework has numerous desirable properties. In particular, our model is able\nto produce multiple plausible and vivid colorizations for a given grayscale\nimage and is one of the first colorization models to provide a proper\nstochastic sampling scheme. Moreover, our training procedure is supported by a\nrigorous theoretical framework that does not require any ad hoc heuristics and\nallows for efficient modeling and learning of the joint pixel color\ndistribution. We demonstrate strong quantitative and qualitative experimental\nresults on the CIFAR-10 dataset and the challenging ILSVRC 2012 dataset. \n\n"}
{"id": "1705.06524", "contents": "Title: A fully dense and globally consistent 3D map reconstruction approach for\n  GI tract to enhance therapeutic relevance of the endoscopic capsule robot Abstract: In the gastrointestinal (GI) tract endoscopy field, ingestible wireless\ncapsule endoscopy is emerging as a novel, minimally invasive diagnostic\ntechnology for inspection of the GI tract and diagnosis of a wide range of\ndiseases and pathologies. Since the development of this technology, medical\ndevice companies and many research groups have made substantial progress in\nconverting passive capsule endoscopes to robotic active capsule endoscopes with\nmost of the functionality of current active flexible endoscopes. However,\nrobotic capsule endoscopy still has some challenges. In particular, the use of\nsuch devices to generate a precise three-dimensional (3D) mapping of the entire\ninner organ remains an unsolved problem. Such global 3D maps of inner organs\nwould help doctors to detect the location and size of diseased areas more\naccurately and intuitively, thus permitting more reliable diagnoses. To our\nknowledge, this paper presents the first complete pipeline for a complete 3D\nvisual map reconstruction of the stomach. The proposed pipeline is modular and\nincludes a preprocessing module, an image registration module, and a final\nshape-from-shading-based 3D reconstruction module; the 3D map is primarily\ngenerated by a combination of image stitching and shape-from-shading\ntechniques, and is updated in a frame-by-frame iterative fashion via capsule\nmotion inside the stomach. A comprehensive quantitative analysis of the\nproposed 3D reconstruction method is performed using an esophagus gastro\nduodenoscopy simulator, three different endoscopic cameras, and a 3D optical\nscanner. \n\n"}
{"id": "1705.06566", "contents": "Title: Learning Texture Manifolds with the Periodic Spatial GAN Abstract: This paper introduces a novel approach to texture synthesis based on\ngenerative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the\nstructure of the input noise distribution by constructing tensors with\ndifferent types of dimensions. We call this technique Periodic Spatial GAN\n(PSGAN). The PSGAN has several novel abilities which surpass the current state\nof the art in texture synthesis. First, we can learn multiple textures from\ndatasets of one or more complex large images. Second, we show that the image\ngeneration with PSGANs has properties of a texture manifold: we can smoothly\ninterpolate between samples in the structured noise space and generate novel\nsamples, which lie perceptually between the textures of the original dataset.\nIn addition, we can also accurately learn periodical textures. We make multiple\nexperiments which show that PSGANs can flexibly handle diverse texture and\nimage data sources. Our method is highly scalable and it can generate output\nimages of arbitrary large size. \n\n"}
{"id": "1705.09474", "contents": "Title: Zero-Shot Learning with Generative Latent Prototype Model Abstract: Zero-shot learning, which studies the problem of object classification for\ncategories for which we have no training examples, is gaining increasing\nattention from community. Most existing ZSL methods exploit deterministic\ntransfer learning via an in-between semantic embedding space. In this paper, we\ntry to attack this problem from a generative probabilistic modelling\nperspective. We assume for any category, the observed representation, e.g.\nimages or texts, is developed from a unique prototype in a latent space, in\nwhich the semantic relationship among prototypes is encoded via linear\nreconstruction. Taking advantage of this assumption, virtual instances of\nunseen classes can be generated from the corresponding prototype, giving rise\nto a novel ZSL model which can alleviate the domain shift problem existing in\nthe way of direct transfer learning. Extensive experiments on three benchmark\ndatasets show our proposed model can achieve state-of-the-art results. \n\n"}
{"id": "1706.00130", "contents": "Title: Teaching Machines to Describe Images via Natural Language Feedback Abstract: Robots will eventually be part of every household. It is thus critical to\nenable algorithms to learn from and be guided by non-expert users. In this\npaper, we bring a human in the loop, and enable a human teacher to give\nfeedback to a learning agent in the form of natural language. We argue that a\ndescriptive sentence can provide a much stronger learning signal than a numeric\nreward in that it can easily point to where the mistakes are and how to correct\nthem. We focus on the problem of image captioning in which the quality of the\noutput can easily be judged by non-experts. We propose a hierarchical\nphrase-based captioning model trained with policy gradients, and design a\nfeedback network that provides reward to the learner by conditioning on the\nhuman-provided feedback. We show that by exploiting descriptive feedback our\nmodel learns to perform better than when given independently written human\ncaptions. \n\n"}
{"id": "1706.00556", "contents": "Title: r-BTN: Cross-domain Face Composite and Synthesis from Limited Facial\n  Patches Abstract: We start by asking an interesting yet challenging question, \"If an eyewitness\ncan only recall the eye features of the suspect, such that the forensic artist\ncan only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig.\n1), can advanced computer vision techniques help generate the whole face\nimage?\" A more generalized question is that if a large proportion (e.g., more\nthan 50%) of the face/sketch is missing, can a realistic whole face\nsketch/image still be estimated. Existing face completion and generation\nmethods either do not conduct domain transfer learning or can not handle large\nmissing area. For example, the inpainting approach tends to blur the generated\nregion when the missing area is large (i.e., more than 50%). In this paper, we\nexploit the potential of deep learning networks in filling large missing region\n(e.g., as high as 95% missing) and generating realistic faces with\nhigh-fidelity in cross domains. We propose the recursive generation by\nbidirectional transformation networks (r-BTN) that recursively generates a\nwhole face/sketch from a small sketch/face patch. The large missing area and\nthe cross domain challenge make it difficult to generate satisfactory results\nusing a unidirectional cross-domain learning structure. On the other hand, a\nforward and backward bidirectional learning between the face and sketch domains\nwould enable recursive estimation of the missing region in an incremental\nmanner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarial\nconstraint to encourage the generation of realistic faces/sketches. Extensive\nexperiments have been conducted to demonstrate the superior performance from\nr-BTN as compared to existing potential solutions. \n\n"}
{"id": "1706.00719", "contents": "Title: Automating Carotid Intima-Media Thickness Video Interpretation with\n  Convolutional Neural Networks Abstract: Cardiovascular disease (CVD) is the leading cause of mortality yet largely\npreventable, but the key to prevention is to identify at-risk individuals\nbefore adverse events. For predicting individual CVD risk, carotid intima-media\nthickness (CIMT), a noninvasive ultrasound method, has proven to be valuable,\noffering several advantages over CT coronary artery calcium score. However,\neach CIMT examination includes several ultrasound videos, and interpreting each\nof these CIMT videos involves three operations: (1) select three end-diastolic\nultrasound frames (EUF) in the video, (2) localize a region of interest (ROI)\nin each selected frame, and (3) trace the lumen-intima interface and the\nmedia-adventitia interface in each ROI to measure CIMT. These operations are\ntedious, laborious, and time consuming, a serious limitation that hinders the\nwidespread utilization of CIMT in clinical practice. To overcome this\nlimitation, this paper presents a new system to automate CIMT video\ninterpretation. Our extensive experiments demonstrate that the suggested system\nsignificantly outperforms the state-of-the-art methods. The superior\nperformance is attributable to our unified framework based on convolutional\nneural networks (CNNs) coupled with our informative image representation and\neffective post-processing of the CNN outputs, which are uniquely designed for\neach of the above three operations. \n\n"}
{"id": "1706.01000", "contents": "Title: Image Compression Based on Compressive Sensing: End-to-End Comparison\n  with JPEG Abstract: We present an end-to-end image compression system based on compressive\nsensing. The presented system integrates the conventional scheme of compressive\nsampling and reconstruction with quantization and entropy coding. The\ncompression performance, in terms of decoded image quality versus data rate, is\nshown to be comparable with JPEG and significantly better at the low rate\nrange. We study the parameters that influence the system performance, including\n(i) the choice of sensing matrix, (ii) the trade-off between quantization and\ncompression ratio, and (iii) the reconstruction algorithms. We propose an\neffective method to jointly control the quantization step and compression ratio\nin order to achieve near optimal quality at any given bit rate. Furthermore,\nour proposed image compression system can be directly used in the compressive\nsensing camera, e.g. the single pixel camera, to construct a hardware\ncompressive sampling system. \n\n"}
{"id": "1706.02179", "contents": "Title: Learning to Represent Mechanics via Long-term Extrapolation and\n  Interpolation Abstract: While the basic laws of Newtonian mechanics are well understood, explaining a\nphysical scenario still requires manually modeling the problem with suitable\nequations and associated parameters. In order to adopt such models for\nartificial intelligence, researchers have handcrafted the relevant states, and\nthen used neural networks to learn the state transitions using simulation runs\nas training data. Unfortunately, such approaches can be unsuitable for modeling\ncomplex real-world scenarios, where manually authoring relevant state spaces\ntend to be challenging. In this work, we investigate if neural networks can\nimplicitly learn physical states of real-world mechanical processes only based\non visual data, and thus enable long-term physical extrapolation. We develop a\nrecurrent neural network architecture for this task and also characterize\nresultant uncertainties in the form of evolving variance estimates. We evaluate\nour setup to extrapolate motion of a rolling ball on bowl of varying shape and\norientation using only images as input, and report competitive results with\napproaches that assume access to internal physics models and parameters. \n\n"}
{"id": "1706.02493", "contents": "Title: Learning Deep Representations for Scene Labeling with Semantic Context\n  Guided Supervision Abstract: Scene labeling is a challenging classification problem where each input image\nrequires a pixel-level prediction map. Recently, deep-learning-based methods\nhave shown their effectiveness on solving this problem. However, we argue that\nthe large intra-class variation provides ambiguous training information and\nhinders the deep models' ability to learn more discriminative deep feature\nrepresentations. Unlike existing methods that mainly utilize semantic context\nfor regularizing or smoothing the prediction map, we design novel supervisions\nfrom semantic context for learning better deep feature representations. Two\ntypes of semantic context, scene names of images and label map statistics of\nimage patches, are exploited to create label hierarchies between the original\nclasses and newly created subclasses as the learning supervisions. Such\nsubclasses show lower intra-class variation, and help CNN detect more\nmeaningful visual patterns and learn more effective deep features. Novel\ntraining strategies and network structure that take advantages of such label\nhierarchies are introduced. Our proposed method is evaluated extensively on\nfour popular datasets, Stanford Background (8 classes), SIFTFlow (33 classes),\nBarcelona (170 classes) and LM+Sun datasets (232 classes) with 3 different\nnetworks structures, and show state-of-the-art performance. The experiments\nshow that our proposed method makes deep models learn more discriminative\nfeature representations without increasing model size or complexity. \n\n"}
{"id": "1706.03581", "contents": "Title: Enriched Deep Recurrent Visual Attention Model for Multiple Object\n  Recognition Abstract: We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an\nimproved attention-based architecture for multiple object recognition. The\nproposed model is a fully differentiable unit that can be optimized end-to-end\nby using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was\nemployed as visual attention mechanism which allows to learn the geometric\ntransformation of objects within images. With the combination of the Spatial\nTransformer and the powerful recurrent architecture, the proposed EDRAM can\nlocalize and recognize objects simultaneously. EDRAM has been evaluated on two\npublicly available datasets including MNIST Cluttered (with 70K cluttered\ndigits) and SVHN (with up to 250k real world images of house numbers).\nExperiments show that it obtains superior performance as compared with the\nstate-of-the-art models. \n\n"}
{"id": "1706.04264", "contents": "Title: von Mises-Fisher Mixture Model-based Deep learning: Application to Face\n  Verification Abstract: A number of pattern recognition tasks, \\textit{e.g.}, face verification, can\nbe boiled down to classification or clustering of unit length directional\nfeature vectors whose distance can be simply computed by their angle. In this\npaper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical\nfoundation for an effective deep-learning of such directional features and\nderive a novel vMF Mixture Loss and its corresponding vMF deep features. The\nproposed vMF feature learning achieves the characteristics of discriminative\nlearning, \\textit{i.e.}, compacting the instances of the same class while\nincreasing the distance of instances from different classes. Moreover, it\nsubsumes a number of popular loss functions as well as an effective method in\ndeep learning, namely normalization. We conduct extensive experiments on face\nverification using 4 different challenging face datasets, \\textit{i.e.}, LFW,\nYouTube faces, CACD and IJB-A. Results show the effectiveness and excellent\ngeneralization ability of the proposed approach as it achieves state-of-the-art\nresults on the LFW, YouTube faces and CACD datasets and competitive results on\nthe IJB-A dataset. \n\n"}
{"id": "1706.04589", "contents": "Title: Learning without Prejudice: Avoiding Bias in Webly-Supervised Action\n  Recognition Abstract: Webly-supervised learning has recently emerged as an alternative paradigm to\ntraditional supervised learning based on large-scale datasets with manual\nannotations. The key idea is that models such as CNNs can be learned from the\nnoisy visual data available on the web. In this work we aim to exploit web data\nfor video understanding tasks such as action recognition and detection. One of\nthe main problems in webly-supervised learning is cleaning the noisy labeled\ndata from the web. The state-of-the-art paradigm relies on training a first\nclassifier on noisy data that is then used to clean the remaining dataset. Our\nkey insight is that this procedure biases the second classifier towards samples\nthat the first one understands. Here we train two independent CNNs, a RGB\nnetwork on web images and video frames and a second network using temporal\ninformation from optical flow. We show that training the networks independently\nis vastly superior to selecting the frames for the flow classifier by using our\nRGB network. Moreover, we show benefits in enriching the training set with\ndifferent data sources from heterogeneous public web databases. We demonstrate\nthat our framework outperforms all other webly-supervised methods on two public\nbenchmarks, UCF-101 and Thumos'14. \n\n"}
{"id": "1706.04970", "contents": "Title: A convolutional autoencoder approach for mining features in cellular\n  electron cryo-tomograms and weakly supervised coarse segmentation Abstract: Cellular electron cryo-tomography enables the 3D visualization of cellular\norganization in the near-native state and at submolecular resolution. However,\nthe contents of cellular tomograms are often complex, making it difficult to\nautomatically isolate different in situ cellular components. In this paper, we\npropose a convolutional autoencoder-based unsupervised approach to provide a\ncoarse grouping of 3D small subvolumes extracted from tomograms. We demonstrate\nthat the autoencoder can be used for efficient and coarse characterization of\nfeatures of macromolecular complexes and surfaces, such as membranes. In\naddition, the autoencoder can be used to detect non-cellular features related\nto sample preparation and data collection, such as carbon edges from the grid\nand tomogram boundaries. The autoencoder is also able to detect patterns that\nmay indicate spatial interactions between cellular components. Furthermore, we\ndemonstrate that our autoencoder can be used for weakly supervised semantic\nsegmentation of cellular components, requiring a very small amount of manual\nannotation. \n\n"}
{"id": "1706.05067", "contents": "Title: Face Clustering: Representation and Pairwise Constraints Abstract: Clustering face images according to their identity has two important\napplications: (i) grouping a collection of face images when no external labels\nare associated with images, and (ii) indexing for efficient large scale face\nretrieval. The clustering problem is composed of two key parts: face\nrepresentation and choice of similarity for grouping faces. We first propose a\nrepresentation based on ResNet, which has been shown to perform very well in\nimage classification problems. Given this representation, we design a\nclustering algorithm, Conditional Pairwise Clustering (ConPaC), which directly\nestimates the adjacency matrix only based on the similarity between face\nimages. This allows a dynamic selection of number of clusters and retains\npairwise similarity between faces. ConPaC formulates the clustering problem as\na Conditional Random Field (CRF) model and uses Loopy Belief Propagation to\nfind an approximate solution for maximizing the posterior probability of the\nadjacency matrix. Experimental results on two benchmark face datasets (LFW and\nIJB-B) show that ConPaC outperforms well known clustering algorithms such as\nk-means, spectral clustering and approximate rank-order. Additionally, our\nalgorithm can naturally incorporate pairwise constraints to obtain a\nsemi-supervised version that leads to improved clustering performance. We also\npropose an k-NN variant of ConPaC, which has a linear time complexity given a\nk-NN graph, suitable for large datasets. \n\n"}
{"id": "1706.05933", "contents": "Title: Ranking to Learn and Learning to Rank: On the Role of Ranking in Pattern\n  Recognition Applications Abstract: The last decade has seen a revolution in the theory and application of\nmachine learning and pattern recognition. Through these advancements, variable\nranking has emerged as an active and growing research area and it is now\nbeginning to be applied to many new problems. The rationale behind this fact is\nthat many pattern recognition problems are by nature ranking problems. The main\nobjective of a ranking algorithm is to sort objects according to some criteria,\nso that, the most relevant items will appear early in the produced result list.\nRanking methods can be analyzed from two different methodological perspectives:\nranking to learn and learning to rank. The former aims at studying methods and\ntechniques to sort objects for improving the accuracy of a machine learning\nmodel. Enhancing a model performance can be challenging at times. For example,\nin pattern classification tasks, different data representations can complicate\nand hide the different explanatory factors of variation behind the data. In\nparticular, hand-crafted features contain many cues that are either redundant\nor irrelevant, which turn out to reduce the overall accuracy of the classifier.\nIn such a case feature selection is used, that, by producing ranked lists of\nfeatures, helps to filter out the unwanted information. Moreover, in real-time\nsystems (e.g., visual trackers) ranking approaches are used as optimization\nprocedures which improve the robustness of the system that deals with the high\nvariability of the image streams that change over time. The other way around,\nlearning to rank is necessary in the construction of ranking models for\ninformation retrieval, biometric authentication, re-identification, and\nrecommender systems. In this context, the ranking model's purpose is to sort\nobjects according to their degrees of relevance, importance, or preference as\ndefined in the specific application. \n\n"}
{"id": "1706.06341", "contents": "Title: SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced\n  Learning Abstract: It is known that Boosting can be interpreted as a gradient descent technique\nto minimize an underlying loss function. Specifically, the underlying loss\nbeing minimized by the traditional AdaBoost is the exponential loss, which is\nproved to be very sensitive to random noise/outliers. Therefore, several\nBoosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to\nimprove the robustness of AdaBoost by replacing the exponential loss with some\ndesigned robust loss functions. In this work, we present a new way to robustify\nAdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning\n(SPL) into Boosting framework. Specifically, we design a new robust Boosting\nalgorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented\nby slightly modifying off-the-shelf Boosting packages. Extensive experiments\nand a theoretical characterization are also carried out to illustrate the\nmerits of the proposed SPLBoost. \n\n"}
{"id": "1706.07036", "contents": "Title: Learning Efficient Point Cloud Generation for Dense 3D Object\n  Reconstruction Abstract: Conventional methods of 3D object generative modeling learn volumetric\npredictions using deep networks with 3D convolutional operations, which are\ndirect analogies to classical 2D ones. However, these methods are\ncomputationally wasteful in attempt to predict 3D shapes, where information is\nrich only on the surfaces. In this paper, we propose a novel 3D generative\nmodeling framework to efficiently generate object shapes in the form of dense\npoint clouds. We use 2D convolutional operations to predict the 3D structure\nfrom multiple viewpoints and jointly apply geometric reasoning with 2D\nprojection optimization. We introduce the pseudo-renderer, a differentiable\nmodule to approximate the true rendering operation, to synthesize novel depth\nmaps for optimization. Experimental results for single-image 3D object\nreconstruction tasks show that we outperforms state-of-the-art methods in terms\nof shape similarity and prediction density. \n\n"}
{"id": "1706.07365", "contents": "Title: Pixels to Graphs by Associative Embedding Abstract: Graphs are a useful abstraction of image content. Not only can graphs\nrepresent details about individual objects in a scene but they can capture the\ninteractions between pairs of objects. We present a method for training a\nconvolutional neural network such that it takes in an input image and produces\na full graph definition. This is done end-to-end in a single stage with the use\nof associative embeddings. The network learns to simultaneously identify all of\nthe elements that make up a graph and piece them together. We benchmark on the\nVisual Genome dataset, and demonstrate state-of-the-art performance on the\nchallenging task of scene graph generation. \n\n"}
{"id": "1706.07567", "contents": "Title: Sampling Matters in Deep Embedding Learning Abstract: Deep embeddings answer one simple question: How similar are two images?\nLearning these embeddings is the bedrock of verification, zero-shot learning,\nand visual search. The most prominent approaches optimize a deep convolutional\nnetwork with a suitable loss function, such as contrastive loss or triplet\nloss. While a rich line of work focuses solely on the loss functions, we show\nin this paper that selecting training examples plays an equally important role.\nWe propose distance weighted sampling, which selects more informative and\nstable examples than traditional approaches. In addition, we show that a simple\nmargin based loss is sufficient to outperform all other loss functions. We\nevaluate our approach on the Stanford Online Products, CAR196, and the\nCUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset\nfor face verification. Our method achieves state-of-the-art performance on all\nof them. \n\n"}
{"id": "1706.07911", "contents": "Title: Large-Scale Mapping of Human Activity using Geo-Tagged Videos Abstract: This paper is the first work to perform spatio-temporal mapping of human\nactivity using the visual content of geo-tagged videos. We utilize a recent\ndeep-learning based video analysis framework, termed hidden two-stream\nnetworks, to recognize a range of activities in YouTube videos. This framework\nis efficient and can run in real time or faster which is important for\nrecognizing events as they occur in streaming video or for reducing latency in\nanalyzing already captured video. This is, in turn, important for using video\nin smart-city applications. We perform a series of experiments to show our\napproach is able to accurately map activities both spatially and temporally. We\nalso demonstrate the advantages of using the visual content over the\ntags/titles. \n\n"}
{"id": "1706.08098", "contents": "Title: FReLU: Flexible Rectified Linear Units for Improving Convolutional\n  Neural Networks Abstract: Rectified linear unit (ReLU) is a widely used activation function for deep\nconvolutional neural networks. However, because of the zero-hard rectification,\nReLU networks miss the benefits from negative values. In this paper, we propose\na novel activation function called \\emph{flexible rectified linear unit\n(FReLU)} to further explore the effects of negative values. By redesigning the\nrectified point of ReLU as a learnable parameter, FReLU expands the states of\nthe activation output. When the network is successfully trained, FReLU tends to\nconverge to a negative value, which improves the expressiveness and thus the\nperformance. Furthermore, FReLU is designed to be simple and effective without\nexponential functions to maintain low cost computation. For being able to\neasily used in various network architectures, FReLU does not rely on strict\nassumptions by self-adaption. We evaluate FReLU on three standard image\nclassification datasets, including CIFAR-10, CIFAR-100, and ImageNet.\nExperimental results show that the proposed method achieves fast convergence\nand higher performances on both plain and residual networks. \n\n"}
{"id": "1706.08474", "contents": "Title: Paying More Attention to Saliency: Image Captioning with Saliency and\n  Context Attention Abstract: Image captioning has been recently gaining a lot of attention thanks to the\nimpressive achievements shown by deep captioning architectures, which combine\nConvolutional Neural Networks to extract image representations, and Recurrent\nNeural Networks to generate the corresponding captions. At the same time, a\nsignificant research effort has been dedicated to the development of saliency\nprediction models, which can predict human eye fixations. Even though saliency\ninformation could be useful to condition an image captioning architecture, by\nproviding an indication of what is salient and what is not, research is still\nstruggling to incorporate these two techniques. In this work, we propose an\nimage captioning approach in which a generative recurrent neural network can\nfocus on different parts of the input image during the generation of the\ncaption, by exploiting the conditioning given by a saliency prediction model on\nwhich parts of the image are salient and which are contextual. We show, through\nextensive quantitative and qualitative experiments on large scale datasets,\nthat our model achieves superior performances with respect to captioning\nbaselines with and without saliency, and to different state of the art\napproaches combining saliency and captioning. \n\n"}
{"id": "1706.08685", "contents": "Title: Material Recognition CNNs and Hierarchical Planning for Biped Robot\n  Locomotion on Slippery Terrain Abstract: In this paper we tackle the problem of visually predicting surface friction\nfor environments with diverse surfaces, and integrating this knowledge into\nbiped robot locomotion planning. The problem is essential for autonomous robot\nlocomotion since diverse surfaces with varying friction abound in the real\nworld, from wood to ceramic tiles, grass or ice, which may cause difficulties\nor huge energy costs for robot locomotion if not considered. We propose to\nestimate friction and its uncertainty from visual estimation of material\nclasses using convolutional neural networks, together with probability\ndistribution functions of friction associated with each material. We then\nrobustly integrate the friction predictions into a hierarchical (footstep and\nfull-body) planning method using chance constraints, and optimize the same\ntrajectory costs at both levels of the planning method for consistency. Our\nsolution achieves fully autonomous perception and locomotion on slippery\nterrain, which considers not only friction and its uncertainty, but also\ncollision, stability and trajectory cost. We show promising friction prediction\nresults in real pictures of outdoor scenarios, and planning experiments on a\nreal robot facing surfaces with different friction. \n\n"}
{"id": "1706.08775", "contents": "Title: Topometric Localization with Deep Learning Abstract: Compared to LiDAR-based localization methods, which provide high accuracy but\nrely on expensive sensors, visual localization approaches only require a camera\nand thus are more cost-effective while their accuracy and reliability typically\nis inferior to LiDAR-based methods. In this work, we propose a vision-based\nlocalization approach that learns from LiDAR-based localization methods by\nusing their output as training data, thus combining a cheap, passive sensor\nwith an accuracy that is on-par with LiDAR-based localization. The approach\nconsists of two deep networks trained on visual odometry and topological\nlocalization, respectively, and a successive optimization to combine the\npredictions of these two networks. We evaluate the approach on a new\nchallenging pedestrian-based dataset captured over the course of six months in\nvarying weather conditions with a high degree of noise. The experiments\ndemonstrate that the localization errors are up to 10 times smaller than with\ntraditional vision-based localization methods. \n\n"}
{"id": "1706.08801", "contents": "Title: Detecting Approximate Reflection Symmetry in a Point Set using\n  Optimization on Manifold Abstract: We propose an algorithm to detect approximate reflection symmetry present in\na set of volumetrically distributed points belonging to $\\mathbb{R}^d$\ncontaining a distorted reflection symmetry pattern. We pose the problem of\ndetecting approximate reflection symmetry as the problem of establishing\ncorrespondences between the points which are reflections of each other and we\ndetermine the reflection symmetry transformation. We formulate an optimization\nframework in which the problem of establishing the correspondences amounts to\nsolving a linear assignment problem and the problem of determining the\nreflection symmetry transformation amounts to solving an optimization problem\non a smooth Riemannian product manifold. The proposed approach estimates the\nsymmetry from the geometry of the points and is descriptor independent. We\nevaluate the performance of the proposed approach on the standard benchmark\ndataset and achieve the state-of-the-art performance. We further show the\nrobustness of our approach by varying the amount of distortion in a perfect\nreflection symmetry pattern where we perturb each point by a different amount\nof perturbation. We demonstrate the effectiveness of the method by applying it\nto the problem of 2-D and 3-D reflection symmetry detection along with\ncomparisons. \n\n"}
{"id": "1706.09308", "contents": "Title: A New Urban Objects Detection Framework Using Weakly Annotated Sets Abstract: Urban informatics explore data science methods to address different urban\nissues intensively based on data. The large variety and quantity of data\navailable should be explored but this brings important challenges. For\ninstance, although there are powerful computer vision methods that may be\nexplored, they may require large annotated datasets. In this work we propose a\nnovel approach to automatically creating an object recognition system with\nminimal manual annotation. The basic idea behind the method is to use large\ninput datasets using available online cameras on large cities. A off-the-shelf\nweak classifier is used to detect an initial set of urban elements of interest\n(e.g. cars, pedestrians, bikes, etc.). Such initial dataset undergoes a quality\ncontrol procedure and it is subsequently used to fine tune a strong classifier.\nQuality control and comparative performance assessment are used as part of the\npipeline. We evaluate the method for detecting cars based on monitoring\ncameras. Experimental results using real data show that despite losing\ngenerality, the final detector provides better detection rates tailored to the\nselected cameras. The programmed robot gathered 770 video hours from 24 online\ncity cameras (\\~300GB), which has been fed to the proposed system. Our approach\nhas shown that the method nearly doubled the recall (93\\%) with respect to\nstate-of-the-art methods using off-the-shelf algorithms. \n\n"}
{"id": "1706.09650", "contents": "Title: Co-salient Object Detection Based on Deep Saliency Networks and Seed\n  Propagation over an Integrated Graph Abstract: This paper presents a co-salient object detection method to find common\nsalient regions in a set of images. We utilize deep saliency networks to\ntransfer co-saliency prior knowledge and better capture high-level semantic\ninformation, and the resulting initial co-saliency maps are enhanced by seed\npropagation steps over an integrated graph. The deep saliency networks are\ntrained in a supervised manner to avoid online weakly supervised learning and\nexploit them not only to extract high-level features but also to produce both\nintra- and inter-image saliency maps. Through a refinement step, the initial\nco-saliency maps can uniformly highlight co-salient regions and locate accurate\nobject boundaries. To handle input image groups inconsistent in size, we\npropose to pool multi-regional descriptors including both within-segment and\nwithin-group information. In addition, the integrated multilayer graph is\nconstructed to find the regions that the previous steps may not detect by seed\npropagation with low-level descriptors. In this work, we utilize the useful\ncomplementary components of high-, low-level information, and several\nlearning-based steps. Our experiments have demonstrated that the proposed\napproach outperforms comparable co-saliency detection methods on widely used\npublic databases and can also be directly applied to co-segmentation tasks. \n\n"}
{"id": "1707.00600", "contents": "Title: Zero-Shot Learning -- A Comprehensive Evaluation of the Good, the Bad\n  and the Ugly Abstract: Due to the importance of zero-shot learning, i.e. classifying images where\nthere is a lack of labeled training data, the number of proposed approaches has\nrecently increased steadily. We argue that it is time to take a step back and\nto analyze the status quo of the area. The purpose of this paper is three-fold.\nFirst, given the fact that there is no agreed upon zero-shot learning\nbenchmark, we first define a new benchmark by unifying both the evaluation\nprotocols and data splits of publicly available datasets used for this task.\nThis is an important contribution as published results are often not comparable\nand sometimes even flawed due to, e.g. pre-training on zero-shot test classes.\nMoreover, we propose a new zero-shot learning dataset, the Animals with\nAttributes 2 (AWA2) dataset which we make publicly available both in terms of\nimage features and the images themselves. Second, we compare and analyze a\nsignificant number of the state-of-the-art methods in depth, both in the\nclassic zero-shot setting but also in the more realistic generalized zero-shot\nsetting. Finally, we discuss in detail the limitations of the current status of\nthe area which can be taken as a basis for advancing it. \n\n"}
{"id": "1707.00600", "contents": "Title: Zero-Shot Learning -- A Comprehensive Evaluation of the Good, the Bad\n  and the Ugly Abstract: Due to the importance of zero-shot learning, i.e. classifying images where\nthere is a lack of labeled training data, the number of proposed approaches has\nrecently increased steadily. We argue that it is time to take a step back and\nto analyze the status quo of the area. The purpose of this paper is three-fold.\nFirst, given the fact that there is no agreed upon zero-shot learning\nbenchmark, we first define a new benchmark by unifying both the evaluation\nprotocols and data splits of publicly available datasets used for this task.\nThis is an important contribution as published results are often not comparable\nand sometimes even flawed due to, e.g. pre-training on zero-shot test classes.\nMoreover, we propose a new zero-shot learning dataset, the Animals with\nAttributes 2 (AWA2) dataset which we make publicly available both in terms of\nimage features and the images themselves. Second, we compare and analyze a\nsignificant number of the state-of-the-art methods in depth, both in the\nclassic zero-shot setting but also in the more realistic generalized zero-shot\nsetting. Finally, we discuss in detail the limitations of the current status of\nthe area which can be taken as a basis for advancing it. \n\n"}
{"id": "1707.00683", "contents": "Title: Modulating early visual processing by language Abstract: It is commonly assumed that language refers to high-level visual concepts\nwhile leaving low-level visual processing unaffected. This view dominates the\ncurrent literature in computational models for language-vision tasks, where\nvisual and linguistic input are mostly processed independently before being\nfused into a single representation. In this paper, we deviate from this classic\npipeline and propose to modulate the \\emph{entire visual processing} by\nlinguistic input. Specifically, we condition the batch normalization parameters\nof a pretrained residual network (ResNet) on a language embedding. This\napproach, which we call MOdulated RESnet (\\MRN), significantly improves strong\nbaselines on two visual question answering tasks. Our ablation study shows that\nmodulating from the early stages of the visual processing is beneficial. \n\n"}
{"id": "1707.00798", "contents": "Title: Deep Representation Learning with Part Loss for Person Re-Identification Abstract: Learning discriminative representations for unseen person images is critical\nfor person Re-Identification (ReID). Most of current approaches learn deep\nrepresentations in classification tasks, which essentially minimize the\nempirical classification risk on the training set. As shown in our experiments,\nsuch representations commonly focus on several body parts discriminative to the\ntraining set, rather than the entire human body. Inspired by the structural\nrisk minimization principle in SVM, we revise the traditional deep\nrepresentation learning procedure to minimize both the empirical classification\nrisk and the representation learning risk. The representation learning risk is\nevaluated by the proposed part loss, which automatically generates several\nparts for an image, and computes the person classification loss on each part\nseparately. Compared with traditional global classification loss,\nsimultaneously considering multiple part loss enforces the deep network to\nfocus on the entire human body and learn discriminative representations for\ndifferent parts. Experimental results on three datasets, i.e., Market1501,\nCUHK03, VIPeR, show that our representation outperforms the existing deep\nrepresentations. \n\n"}
{"id": "1707.01213", "contents": "Title: Data-Driven Sparse Structure Selection for Deep Neural Networks Abstract: Deep convolutional neural networks have liberated its extraordinary power on\nvarious tasks. However, it is still very challenging to deploy state-of-the-art\nmodels into real-world applications due to their high computational complexity.\nHow can we design a compact and effective network without massive experiments\nand expert knowledge? In this paper, we propose a simple and effective\nframework to learn and prune deep models in an end-to-end manner. In our\nframework, a new type of parameter -- scaling factor is first introduced to\nscale the outputs of specific structures, such as neurons, groups or residual\nblocks. Then we add sparsity regularizations on these factors, and solve this\noptimization problem by a modified stochastic Accelerated Proximal Gradient\n(APG) method. By forcing some of the factors to zero, we can safely remove the\ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing\nwith other structure selection methods that may need thousands of trials or\niterative fine-tuning, our method is trained fully end-to-end in one training\npass without bells and whistles. We evaluate our method, Sparse Structure\nSelection with several state-of-the-art CNNs, and demonstrate very promising\nresults with adaptive depth and width selection. \n\n"}
{"id": "1707.01220", "contents": "Title: DarkRank: Accelerating Deep Metric Learning via Cross Sample\n  Similarities Transfer Abstract: We have witnessed rapid evolution of deep neural network architecture design\nin the past years. These latest progresses greatly facilitate the developments\nin various areas such as computer vision and natural language processing.\nHowever, along with the extraordinary performance, these state-of-the-art\nmodels also bring in expensive computational cost. Directly deploying these\nmodels into applications with real-time requirement is still infeasible.\nRecently, Hinton etal. have shown that the dark knowledge within a powerful\nteacher model can significantly help the training of a smaller and faster\nstudent network. These knowledge are vastly beneficial to improve the\ngeneralization ability of the student model. Inspired by their work, we\nintroduce a new type of knowledge -- cross sample similarities for model\ncompression and acceleration. This knowledge can be naturally derived from deep\nmetric learning model. To transfer them, we bring the \"learning to rank\"\ntechnique into deep metric learning formulation. We test our proposed DarkRank\nmethod on various metric learning tasks including pedestrian re-identification,\nimage retrieval and image clustering. The results are quite encouraging. Our\nmethod can improve over the baseline method by a large margin. Moreover, it is\nfully compatible with other existing methods. When combined, the performance\ncan be further boosted. \n\n"}
{"id": "1707.02432", "contents": "Title: Deep Semantic Segmentation for Automated Driving: Taxonomy, Roadmap and\n  Challenges Abstract: Semantic segmentation was seen as a challenging computer vision problem few\nyears ago. Due to recent advancements in deep learning, relatively accurate\nsolutions are now possible for its use in automated driving. In this paper, the\nsemantic segmentation problem is explored from the perspective of automated\ndriving. Most of the current semantic segmentation algorithms are designed for\ngeneric images and do not incorporate prior structure and end goal for\nautomated driving. First, the paper begins with a generic taxonomic survey of\nsemantic segmentation algorithms and then discusses how it fits in the context\nof automated driving. Second, the particular challenges of deploying it into a\nsafety system which needs high level of accuracy and robustness are listed.\nThird, different alternatives instead of using an independent semantic\nsegmentation module are explored. Finally, an empirical evaluation of various\nsemantic segmentation architectures was performed on CamVid dataset in terms of\naccuracy and speed. This paper is a preliminary shorter version of a more\ndetailed survey which is work in progress. \n\n"}
{"id": "1707.02581", "contents": "Title: Class-Weighted Convolutional Features for Visual Instance Search Abstract: Image retrieval in realistic scenarios targets large dynamic datasets of\nunlabeled images. In these cases, training or fine-tuning a model every time\nnew images are added to the database is neither efficient nor scalable.\nConvolutional neural networks trained for image classification over large\ndatasets have been proven effective feature extractors for image retrieval. The\nmost successful approaches are based on encoding the activations of\nconvolutional layers, as they convey the image spatial information. In this\npaper, we go beyond this spatial information and propose a local-aware encoding\nof convolutional features based on semantic information predicted in the target\nimage. To this end, we obtain the most discriminative regions of an image using\nClass Activation Maps (CAMs). CAMs are based on the knowledge contained in the\nnetwork and therefore, our approach, has the additional advantage of not\nrequiring external information. In addition, we use CAMs to generate object\nproposals during an unsupervised re-ranking stage after a first fast search.\nOur experiments on two public available datasets for instance retrieval,\nOxford5k and Paris6k, demonstrate the competitiveness of our approach\noutperforming the current state-of-the-art when using off-the-shelf models\ntrained on ImageNet. The source code and model used in this paper are publicly\navailable at http://imatge-upc.github.io/retrieval-2017-cam/. \n\n"}
{"id": "1707.03123", "contents": "Title: SaltiNet: Scan-path Prediction on 360 Degree Images using Saliency\n  Volumes Abstract: We introduce SaltiNet, a deep neural network for scanpath prediction trained\non 360-degree images. The model is based on a temporal-aware novel\nrepresentation of saliency information named the saliency volume. The first\npart of the network consists of a model trained to generate saliency volumes,\nwhose parameters are fit by back-propagation computed from a binary cross\nentropy (BCE) loss over downsampled versions of the saliency volumes. Sampling\nstrategies over these volumes are used to generate scanpaths over the\n360-degree images. Our experiments show the advantages of using saliency\nvolumes, and how they can be used for related tasks. Our source code and\ntrained models available at\nhttps://github.com/massens/saliency-360salient-2017. \n\n"}
{"id": "1707.03296", "contents": "Title: Hierarchical Deep Recurrent Architecture for Video Understanding Abstract: This paper introduces the system we developed for the Youtube-8M Video\nUnderstanding Challenge, in which a large-scale benchmark dataset was used for\nmulti-label video classification. The proposed framework contains hierarchical\ndeep architecture, including the frame-level sequence modeling part and the\nvideo-level classification part. In the frame-level sequence modelling part, we\nexplore a set of methods including Pooling-LSTM (PLSTM), Hierarchical-LSTM\n(HLSTM), Random-LSTM (RLSTM) in order to address the problem of large amount of\nframes in a video. We also introduce two attention pooling methods, single\nattention pooling (ATT) and multiply attention pooling (Multi-ATT) so that we\ncan pay more attention to the informative frames in a video and ignore the\nuseless frames. In the video-level classification part, two methods are\nproposed to increase the classification performance, i.e.\nHierarchical-Mixture-of-Experts (HMoE) and Classifier Chains (CC). Our final\nsubmission is an ensemble consisting of 18 sub-models. In terms of the official\nevaluation metric Global Average Precision (GAP) at 20, our best submission\nachieves 0.84346 on the public 50% of test dataset and 0.84333 on the private\n50% of test data. \n\n"}
{"id": "1707.03321", "contents": "Title: A deep learning architecture for temporal sleep stage classification\n  using multivariate and multimodal time series Abstract: Sleep stage classification constitutes an important preliminary exam in the\ndiagnosis of sleep disorders. It is traditionally performed by a sleep expert\nwho assigns to each 30s of signal a sleep stage, based on the visual inspection\nof signals such as electroencephalograms (EEG), electrooculograms (EOG),\nelectrocardiograms (ECG) and electromyograms (EMG). We introduce here the first\ndeep learning approach for sleep stage classification that learns end-to-end\nwithout computing spectrograms or extracting hand-crafted features, that\nexploits all multivariate and multimodal Polysomnography (PSG) signals (EEG,\nEMG and EOG), and that can exploit the temporal context of each 30s window of\ndata. For each modality the first layer learns linear spatial filters that\nexploit the array of sensors to increase the signal-to-noise ratio, and the\nlast layer feeds the learnt representation to a softmax classifier. Our model\nis compared to alternative automatic approaches based on convolutional networks\nor decisions trees. Results obtained on 61 publicly available PSG records with\nup to 20 EEG channels demonstrate that our network architecture yields\nstate-of-the-art performance. Our study reveals a number of insights on the\nspatio-temporal distribution of the signal of interest: a good trade-off for\noptimal classification performance measured with balanced accuracy is to use 6\nEEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one\nminute of data before and after each data segment offers the strongest\nimprovement when a limited number of channels is available. As sleep experts,\nour system exploits the multivariate and multimodal nature of PSG signals in\norder to deliver state-of-the-art classification performance with a small\ncomputational cost. \n\n"}
{"id": "1707.03717", "contents": "Title: Using Transfer Learning for Image-Based Cassava Disease Detection Abstract: Cassava is the third largest source of carbohydrates for human food in the\nworld but is vulnerable to virus diseases, which threaten to destabilize food\nsecurity in sub-Saharan Africa. Novel methods of cassava disease detection are\nneeded to support improved control which will prevent this crisis. Image\nrecognition offers both a cost effective and scalable technology for disease\ndetection. New transfer learning methods offer an avenue for this technology to\nbe easily deployed on mobile devices. Using a dataset of cassava disease images\ntaken in the field in Tanzania, we applied transfer learning to train a deep\nconvolutional neural network to identify three diseases and two types of pest\ndamage (or lack thereof). The best trained model accuracies were 98% for brown\nleaf spot (BLS), 96% for red mite damage (RMD), 95% for green mite damage\n(GMD), 98% for cassava brown streak disease (CBSD), and 96% for cassava mosaic\ndisease (CMD). The best model achieved an overall accuracy of 93% for data not\nused in the training process. Our results show that the transfer learning\napproach for image recognition of field images offers a fast, affordable, and\neasily deployable strategy for digital plant disease detection. \n\n"}
{"id": "1707.03891", "contents": "Title: Unsupervised Body Part Regression via Spatially Self-ordering\n  Convolutional Neural Networks Abstract: Automatic body part recognition for CT slices can benefit various medical\nimage applications. Recent deep learning methods demonstrate promising\nperformance, with the requirement of large amounts of labeled images for\ntraining. The intrinsic structural or superior-inferior slice ordering\ninformation in CT volumes is not fully exploited. In this paper, we propose a\nconvolutional neural network (CNN) based Unsupervised Body part Regression\n(UBR) algorithm to address this problem. A novel unsupervised learning method\nand two inter-sample CNN loss functions are presented. Distinct from previous\nwork, UBR builds a coordinate system for the human body and outputs a\ncontinuous score for each axial slice, representing the normalized position of\nthe body part in the slice. The training process of UBR resembles a\nself-organization process: slice scores are learned from inter-slice\nrelationships. The training samples are unlabeled CT volumes that are abundant,\nthus no extra annotation effort is needed. UBR is simple, fast, and accurate.\nQuantitative and qualitative experiments validate its effectiveness. In\naddition, we show two applications of UBR in network initialization and anomaly\ndetection. \n\n"}
{"id": "1707.04131", "contents": "Title: Foolbox: A Python toolbox to benchmark the robustness of machine\n  learning models Abstract: Even todays most advanced machine learning models are easily fooled by almost\nimperceptible perturbations of their inputs. Foolbox is a new Python package to\ngenerate such adversarial perturbations and to quantify and compare the\nrobustness of machine learning models. It is build around the idea that the\nmost comparable robustness measure is the minimum perturbation needed to craft\nan adversarial example. To this end, Foolbox provides reference implementations\nof most published adversarial attack methods alongside some new ones, all of\nwhich perform internal hyperparameter tuning to find the minimum adversarial\nperturbation. Additionally, Foolbox interfaces with most popular deep learning\nframeworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows\ndifferent adversarial criteria such as targeted misclassification and top-k\nmisclassification as well as different distance measures. The code is licensed\nunder the MIT license and is openly available at\nhttps://github.com/bethgelab/foolbox . The most up-to-date documentation can be\nfound at http://foolbox.readthedocs.io . \n\n"}
{"id": "1707.04818", "contents": "Title: RED: Reinforced Encoder-Decoder Networks for Action Anticipation Abstract: Action anticipation aims to detect an action before it happens. Many real\nworld applications in robotics and surveillance are related to this predictive\ncapability. Current methods address this problem by first anticipating visual\nrepresentations of future frames and then categorizing the anticipated\nrepresentations to actions. However, anticipation is based on a single past\nframe's representation, which ignores the history trend. Besides, it can only\nanticipate a fixed future time. We propose a Reinforced Encoder-Decoder (RED)\nnetwork for action anticipation. RED takes multiple history representations as\ninput and learns to anticipate a sequence of future representations. One\nsalient aspect of RED is that a reinforcement module is adopted to provide\nsequence-level supervision; the reward function is designed to encourage the\nsystem to make correct predictions as early as possible. We test RED on\nTVSeries, THUMOS-14 and TV-Human-Interaction datasets for action anticipation\nand achieve state-of-the-art performance on all datasets. \n\n"}
{"id": "1707.04881", "contents": "Title: Generative Adversarial Network based on Resnet for Conditional Image\n  Restoration Abstract: The GANs promote an adversarive game to approximate complex and jointed\nexample probability. The networks driven by noise generate fake examples to\napproximate realistic data distributions. Later the conditional GAN merges\nprior-conditions as input in order to transfer attribute vectors to the\ncorresponding data. However, the CGAN is not designed to deal with the high\ndimension conditions since indirect guide of the learning is inefficiency. In\nthis paper, we proposed a network ResGAN to generate fine images in terms of\nextremely degenerated images. The coarse images aligned to attributes are\nembedded as the generator inputs and classifier labels. In generative network,\na straight path similar to the Resnet is cohered to directly transfer the\ncoarse images to the higher layers. And adversarial training is circularly\nimplemented to prevent degeneration of the generated images. Experimental\nresults of applying the ResGAN to datasets MNIST, CIFAR10/100 and CELEBA show\nits higher accuracy to the state-of-art GANs. \n\n"}
{"id": "1707.04931", "contents": "Title: Pathological OCT Retinal Layer Segmentation using Branch Residual\n  U-shape Networks Abstract: The automatic segmentation of retinal layer structures enables\nclinically-relevant quantification and monitoring of eye disorders over time in\nOCT imaging. Eyes with late-stage diseases are particularly challenging to\nsegment, as their shape is highly warped due to pathological biomarkers. In\nthis context, we propose a novel fully Convolutional Neural Network (CNN)\narchitecture which combines dilated residual blocks in an asymmetric U-shape\nconfiguration, and can segment multiple layers of highly pathological eyes in\none shot. We validate our approach on a dataset of late-stage AMD patients and\ndemonstrate lower computational costs and higher performance compared to other\nstate-of-the-art methods. \n\n"}
{"id": "1707.04960", "contents": "Title: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory\n  Network Based Approach Abstract: Recent years have witnessed a resurgence of interest in video summarization.\nHowever, one of the main obstacles to the research on video summarization is\nthe user subjectivity - users have various preferences over the summaries. The\nsubjectiveness causes at least two problems. First, no single video summarizer\nfits all users unless it interacts with and adapts to the individual users.\nSecond, it is very challenging to evaluate the performance of a video\nsummarizer.\n  To tackle the first problem, we explore the recently proposed query-focused\nvideo summarization which introduces user preferences in the form of text\nqueries about the video into the summarization process. We propose a memory\nnetwork parameterized sequential determinantal point process in order to attend\nthe user query onto different video frames and shots. To address the second\nchallenge, we contend that a good evaluation metric for video summarization\nshould focus on the semantic information that humans can perceive rather than\nthe visual features or temporal overlaps. To this end, we collect dense\nper-video-shot concept annotations, compile a new dataset, and suggest an\nefficient evaluation method defined upon the concept annotations. We conduct\nextensive experiments contrasting our video summarizer to existing ones and\npresent detailed analyses about the dataset and the new evaluation method. \n\n"}
{"id": "1707.05251", "contents": "Title: Aesthetic-Driven Image Enhancement by Adversarial Learning Abstract: We introduce EnhanceGAN, an adversarial learning based model that performs\nautomatic image enhancement. Traditional image enhancement frameworks typically\ninvolve training models in a fully-supervised manner, which require expensive\nannotations in the form of aligned image pairs. In contrast to these\napproaches, our proposed EnhanceGAN only requires weak supervision (binary\nlabels on image aesthetic quality) and is able to learn enhancement operators\nfor the task of aesthetic-based image enhancement. In particular, we show the\neffectiveness of a piecewise color enhancement module trained with weak\nsupervision, and extend the proposed EnhanceGAN framework to learning a deep\nfiltering-based aesthetic enhancer. The full differentiability of our image\nenhancement operators enables the training of EnhanceGAN in an end-to-end\nmanner. We further demonstrate the capability of EnhanceGAN in learning\naesthetic-based image cropping without any groundtruth cropping pairs. Our\nweakly-supervised EnhanceGAN reports competitive quantitative results on\naesthetic-based color enhancement as well as automatic image cropping, and a\nuser study confirms that our image enhancement results are on par with or even\npreferred over professional enhancement. \n\n"}
{"id": "1707.05956", "contents": "Title: When Unsupervised Domain Adaptation Meets Tensor Representations Abstract: Domain adaption (DA) allows machine learning methods trained on data sampled\nfrom one distribution to be applied to data sampled from another. It is thus of\ngreat practical importance to the application of such methods. Despite the fact\nthat tensor representations are widely used in Computer Vision to capture\nmulti-linear relationships that affect the data, most existing DA methods are\napplicable to vectors only. This renders them incapable of reflecting and\npreserving important structure in many problems. We thus propose here a\nlearning-based method to adapt the source and target tensor representations\ndirectly, without vectorization. In particular, a set of alignment matrices is\nintroduced to align the tensor representations from both domains into the\ninvariant tensor subspace. These alignment matrices and the tensor subspace are\nmodeled as a joint optimization problem and can be learned adaptively from the\ndata using the proposed alternative minimization scheme. Extensive experiments\nshow that our approach is capable of preserving the discriminative power of the\nsource domain, of resisting the effects of label noise, and works effectively\nfor small sample sizes, and even one-shot DA. We show that our method\noutperforms the state-of-the-art on the task of cross-domain visual recognition\nin both efficacy and efficiency, and particularly that it outperforms all\ncomparators when applied to DA of the convolutional activations of deep\nconvolutional networks. \n\n"}
{"id": "1707.06029", "contents": "Title: Supervising Neural Attention Models for Video Captioning by Human Gaze\n  Data Abstract: The attention mechanisms in deep neural networks are inspired by human's\nattention that sequentially focuses on the most relevant parts of the\ninformation over time to generate prediction output. The attention parameters\nin those models are implicitly trained in an end-to-end manner, yet there have\nbeen few trials to explicitly incorporate human gaze tracking to supervise the\nattention models. In this paper, we investigate whether attention models can\nbenefit from explicit human gaze labels, especially for the task of video\ncaptioning. We collect a new dataset called VAS, consisting of movie clips, and\ncorresponding multiple descriptive sentences along with human gaze tracking\ndata. We propose a video captioning model named Gaze Encoding Attention Network\n(GEAN) that can leverage gaze tracking information to provide the spatial and\ntemporal attention for sentence generation. Through evaluation of language\nsimilarity metrics and human assessment via Amazon mechanical Turk, we\ndemonstrate that spatial attentions guided by human gaze data indeed improve\nthe performance of multiple captioning methods. Moreover, we show that the\nproposed approach achieves the state-of-the-art performance for both gaze\nprediction and video captioning not only in our VAS dataset but also in\nstandard datasets (e.g. LSMDC and Hollywood2). \n\n"}
{"id": "1707.06427", "contents": "Title: Scalable Full Flow with Learned Binary Descriptors Abstract: We propose a method for large displacement optical flow in which local\nmatching costs are learned by a convolutional neural network (CNN) and a\nsmoothness prior is imposed by a conditional random field (CRF). We tackle the\ncomputation- and memory-intensive operations on the 4D cost volume by a\nmin-projection which reduces memory complexity from quadratic to linear and\nbinary descriptors for efficient matching. This enables evaluation of the cost\non the fly and allows to perform learning and CRF inference on high resolution\nimages without ever storing the 4D cost volume. To address the problem of\nlearning binary descriptors we propose a new hybrid learning scheme. In\ncontrast to current state of the art approaches for learning binary CNNs we can\ncompute the exact non-zero gradient within our model. We compare several\nmethods for training binary descriptors and show results on public available\nbenchmarks. \n\n"}
{"id": "1707.07256", "contents": "Title: Deeply-Learned Part-Aligned Representations for Person Re-Identification Abstract: In this paper, we address the problem of person re-identification, which\nrefers to associating the persons captured from different cameras. We propose a\nsimple yet effective human part-aligned representation for handling the body\npart misalignment problem. Our approach decomposes the human body into regions\n(parts) which are discriminative for person matching, accordingly computes the\nrepresentations over the regions, and aggregates the similarities computed\nbetween the corresponding regions of a pair of probe and gallery images as the\noverall matching score. Our formulation, inspired by attention models, is a\ndeep neural network modeling the three steps together, which is learnt through\nminimizing the triplet loss function without requiring body part labeling\ninformation. Unlike most existing deep learning algorithms that learn a global\nor spatial partition-based local representation, our approach performs human\nbody partition, and thus is more robust to pose changes and various human\nspatial distributions in the person bounding box. Our approach shows\nstate-of-the-art results over standard datasets, Market-$1501$, CUHK$03$,\nCUHK$01$ and VIPeR. \n\n"}
{"id": "1707.07381", "contents": "Title: Group-wise Deep Co-saliency Detection Abstract: In this paper, we propose an end-to-end group-wise deep co-saliency detection\napproach to address the co-salient object discovery problem based on the fully\nconvolutional network (FCN) with group input and group output. The proposed\napproach captures the group-wise interaction information for group images by\nlearning a semantics-aware image representation based on a convolutional neural\nnetwork, which adaptively learns the group-wise features for co-saliency\ndetection. Furthermore, the proposed approach discovers the collaborative and\ninteractive relationships between group-wise feature representation and\nsingle-image individual feature representation, and model this in a\ncollaborative learning framework. Finally, we set up a unified end-to-end deep\nlearning scheme to jointly optimize the process of group-wise feature\nrepresentation learning and the collaborative learning, leading to more\nreliable and robust co-saliency detection results. Experimental results\ndemonstrate the effectiveness of our approach in comparison with the\nstate-of-the-art approaches. \n\n"}
{"id": "1707.07397", "contents": "Title: Synthesizing Robust Adversarial Examples Abstract: Standard methods for generating adversarial examples for neural networks do\nnot consistently fool neural network classifiers in the physical world due to a\ncombination of viewpoint shifts, camera noise, and other natural\ntransformations, limiting their relevance to real-world systems. We demonstrate\nthe existence of robust 3D adversarial objects, and we present the first\nalgorithm for synthesizing examples that are adversarial over a chosen\ndistribution of transformations. We synthesize two-dimensional adversarial\nimages that are robust to noise, distortion, and affine transformation. We\napply our algorithm to complex three-dimensional objects, using 3D-printing to\nmanufacture the first physical adversarial objects. Our results demonstrate the\nexistence of 3D adversarial objects in the physical world. \n\n"}
{"id": "1707.07538", "contents": "Title: Infinite Latent Feature Selection: A Probabilistic Latent Graph-Based\n  Ranking Approach Abstract: Feature selection is playing an increasingly significant role with respect to\nmany computer vision applications spanning from object recognition to visual\nobject tracking. However, most of the recent solutions in feature selection are\nnot robust across different and heterogeneous set of data. In this paper, we\naddress this issue proposing a robust probabilistic latent graph-based feature\nselection algorithm that performs the ranking step while considering all the\npossible subsets of features, as paths on a graph, bypassing the combinatorial\nproblem analytically. An appealing characteristic of the approach is that it\naims to discover an abstraction behind low-level sensory data, that is,\nrelevancy. Relevancy is modelled as a latent variable in a PLSA-inspired\ngenerative process that allows the investigation of the importance of a feature\nwhen injected into an arbitrary set of cues. The proposed method has been\ntested on ten diverse benchmarks, and compared against eleven state of the art\nfeature selection methods. Results show that the proposed approach attains the\nhighest performance levels across many different scenarios and difficulties,\nthereby confirming its strong robustness while setting a new state of the art\nin feature selection domain. \n\n"}
{"id": "1707.07815", "contents": "Title: Graph-Theoretic Spatiotemporal Context Modeling for Video Saliency\n  Detection Abstract: As an important and challenging problem in computer vision, video saliency\ndetection is typically cast as a spatiotemporal context modeling problem over\nconsecutive frames. As a result, a key issue in video saliency detection is how\nto effectively capture the intrinsical properties of atomic video structures as\nwell as their associated contextual interactions along the spatial and temporal\ndimensions. Motivated by this observation, we propose a graph-theoretic video\nsaliency detection approach based on adaptive video structure discovery, which\nis carried out within a spatiotemporal atomic graph. Through graph-based\nmanifold propagation, the proposed approach is capable of effectively modeling\nthe semantically contextual interactions among atomic video structures for\nsaliency detection while preserving spatial smoothness and temporal\nconsistency. Experiments demonstrate the effectiveness of the proposed approach\nover several benchmark datasets. \n\n"}
{"id": "1707.07958", "contents": "Title: Residual Conv-Deconv Grid Network for Semantic Segmentation Abstract: This paper presents GridNet, a new Convolutional Neural Network (CNN)\narchitecture for semantic image segmentation (full scene labelling). Classical\nneural networks are implemented as one stream from the input to the output with\nsubsampling operators applied in the stream in order to reduce the feature maps\nsize and to increase the receptive field for the final prediction. However, for\nsemantic image segmentation, where the task consists in providing a semantic\nclass to each pixel of an image, feature maps reduction is harmful because it\nleads to a resolution loss in the output prediction. To tackle this problem,\nour GridNet follows a grid pattern allowing multiple interconnected streams to\nwork at different resolutions. We show that our network generalizes many well\nknown networks such as conv-deconv, residual or U-Net networks. GridNet is\ntrained from scratch and achieves competitive results on the Cityscapes\ndataset. \n\n"}
{"id": "1707.08813", "contents": "Title: A Comparative Study of the Clinical use of Motion Analysis from Kinect\n  Skeleton Data Abstract: The analysis of human motion as a clinical tool can bring many benefits such\nas the early detection of disease and the monitoring of recovery, so in turn\nhelping people to lead independent lives. However, it is currently under used.\nDevelopments in depth cameras, such as Kinect, have opened up the use of motion\nanalysis in settings such as GP surgeries, care homes and private homes. To\nprovide an insight into the use of Kinect in the healthcare domain, we present\na review of the current state of the art. We then propose a method that can\nrepresent human motions from time-series data of arbitrary length, as a single\nvector. Finally, we demonstrate the utility of this method by extracting a set\nof clinically significant features and using them to detect the age related\nchanges in the motions of a set of 54 individuals, with a high degree of\ncertainty (F1- score between 0.9 - 1.0). Indicating its potential application\nin the detection of a range of age-related motion impairments. \n\n"}
{"id": "1707.08935", "contents": "Title: Anisotropic EM Segmentation by 3D Affinity Learning and Agglomeration Abstract: The field of connectomics has recently produced neuron wiring diagrams from\nrelatively large brain regions from multiple animals. Most of these neural\nreconstructions were computed from isotropic (e.g., FIBSEM) or near isotropic\n(e.g., SBEM) data. In spite of the remarkable progress on algorithms in recent\nyears, automatic dense reconstruction from anisotropic data remains a challenge\nfor the connectomics community. One significant hurdle in the segmentation of\nanisotropic data is the difficulty in generating a suitable initial\nover-segmentation. In this study, we present a segmentation method for\nanisotropic EM data that agglomerates a 3D over-segmentation computed from the\n3D affinity prediction. A 3D U-net is trained to predict 3D affinities by the\nMALIS approach. Experiments on multiple datasets demonstrates the strength and\nrobustness of the proposed method for anisotropic EM segmentation. \n\n"}
{"id": "1707.09100", "contents": "Title: MixedPeds: Pedestrian Detection in Unannotated Videos using\n  Synthetically Generated Human-agents for Training Abstract: We present a new method for training pedestrian detectors on an unannotated\nset of images. We produce a mixed reality dataset that is composed of\nreal-world background images and synthetically generated static human-agents.\nOur approach is general, robust, and makes no other assumptions about the\nunannotated dataset regarding the number or location of pedestrians. We\nautomatically extract from the dataset: i) the vanishing point to calibrate the\nvirtual camera, and ii) the pedestrians' scales to generate a Spawn Probability\nMap, which is a novel concept that guides our algorithm to place the\npedestrians at appropriate locations. After putting synthetic human-agents in\nthe unannotated images, we use these augmented images to train a Pedestrian\nDetector, with the annotations generated along with the synthetic agents. We\nconducted our experiments using Faster R-CNN by comparing the detection results\non the unannotated dataset performed by the detector trained using our approach\nand detectors trained with other manually labeled datasets. We showed that our\napproach improves the average precision by 5-13% over these detectors. \n\n"}
{"id": "1708.00169", "contents": "Title: A Locally Weighted Fixation Density-Based Metric for Assessing the\n  Quality of Visual Saliency Predictions Abstract: With the increased focus on visual attention (VA) in the last decade, a large\nnumber of computational visual saliency methods have been developed over the\npast few years. These models are traditionally evaluated by using performance\nevaluation metrics that quantify the match between predicted saliency and\nfixation data obtained from eye-tracking experiments on human observers. Though\na considerable number of such metrics have been proposed in the literature,\nthere are notable problems in them. In this work, we discuss shortcomings in\nexisting metrics through illustrative examples and propose a new metric that\nuses local weights based on fixation density which overcomes these flaws. To\ncompare the performance of our proposed metric at assessing the quality of\nsaliency prediction with other existing metrics, we construct a ground-truth\nsubjective database in which saliency maps obtained from 17 different VA models\nare evaluated by 16 human observers on a 5-point categorical scale in terms of\ntheir visual resemblance with corresponding ground-truth fixation density maps\nobtained from eye-tracking data. The metrics are evaluated by correlating\nmetric scores with the human subjective ratings. The correlation results show\nthat the proposed evaluation metric outperforms all other popular existing\nmetrics. Additionally, the constructed database and corresponding subjective\nratings provide an insight into which of the existing metrics and future\nmetrics are better at estimating the quality of saliency prediction and can be\nused as a benchmark. \n\n"}
{"id": "1708.01015", "contents": "Title: Sensor Transformation Attention Networks Abstract: Recent work on encoder-decoder models for sequence-to-sequence mapping has\nshown that integrating both temporal and spatial attention mechanisms into\nneural networks increases the performance of the system substantially. In this\nwork, we report on the application of an attentional signal not on temporal and\nspatial regions of the input, but instead as a method of switching among inputs\nthemselves. We evaluate the particular role of attentional switching in the\npresence of dynamic noise in the sensors, and demonstrate how the attentional\nsignal responds dynamically to changing noise levels in the environment to\nachieve increased performance on both audio and visual tasks in three\ncommonly-used datasets: TIDIGITS, Wall Street Journal, and GRID. Moreover, the\nproposed sensor transformation network architecture naturally introduces a\nnumber of advantages that merit exploration, including ease of adding new\nsensors to existing architectures, attentional interpretability, and increased\nrobustness in a variety of noisy environments not seen during training.\nFinally, we demonstrate that the sensor selection attention mechanism of a\nmodel trained only on the small TIDIGITS dataset can be transferred directly to\na pre-existing larger network trained on the Wall Street Journal dataset,\nmaintaining functionality of switching between sensors to yield a dramatic\nreduction of error in the presence of noise. \n\n"}
{"id": "1708.01447", "contents": "Title: Video Salient Object Detection Using Spatiotemporal Deep Features Abstract: This paper presents a method for detecting salient objects in videos where\ntemporal information in addition to spatial information is fully taken into\naccount. Following recent reports on the advantage of deep features over\nconventional hand-crafted features, we propose a new set of SpatioTemporal Deep\n(STD) features that utilize local and global contexts over frames. We also\npropose new SpatioTemporal Conditional Random Field (STCRF) to compute saliency\nfrom STD features. STCRF is our extension of CRF to the temporal domain and\ndescribes the relationships among neighboring regions both in a frame and over\nframes. STCRF leads to temporally consistent saliency maps over frames,\ncontributing to the accurate detection of salient objects' boundaries and noise\nreduction during detection. Our proposed method first segments an input video\ninto multiple scales and then computes a saliency map at each scale level using\nSTD features with STCRF. The final saliency map is computed by fusing saliency\nmaps at different scale levels. Our experiments, using publicly available\nbenchmark datasets, confirm that the proposed method significantly outperforms\nstate-of-the-art methods. We also applied our saliency computation to the video\nobject segmentation task, showing that our method outperforms existing video\nobject segmentation methods. \n\n"}
{"id": "1708.01494", "contents": "Title: Hierarchical Metric Learning for Optical Remote Sensing Scene\n  Categorization Abstract: We address the problem of scene classification from optical remote sensing\n(RS) images based on the paradigm of hierarchical metric learning. Ideally,\nsupervised metric learning strategies learn a projection from a set of training\ndata points so as to minimize intra-class variance while maximizing inter-class\nseparability to the class label space. However, standard metric learning\ntechniques do not incorporate the class interaction information in learning the\ntransformation matrix, which is often considered to be a bottleneck while\ndealing with fine-grained visual categories. As a remedy, we propose to\norganize the classes in a hierarchical fashion by exploring their visual\nsimilarities and subsequently learn separate distance metric transformations\nfor the classes present at the non-leaf nodes of the tree. We employ an\niterative max-margin clustering strategy to obtain the hierarchical\norganization of the classes. Experiment results obtained on the large-scale\nNWPU-RESISC45 and the popular UC-Merced datasets demonstrate the efficacy of\nthe proposed hierarchical metric learning based RS scene recognition strategy\nin comparison to the standard approaches. \n\n"}
{"id": "1708.01783", "contents": "Title: Interactively Transferring CNN Patterns for Part Localization Abstract: In the scenario of one/multi-shot learning, conventional end-to-end learning\nstrategies without sufficient supervision are usually not powerful enough to\nlearn correct patterns from noisy signals. Thus, given a CNN pre-trained for\nobject classification, this paper proposes a method that first summarizes the\nknowledge hidden inside the CNN into a dictionary of latent activation\npatterns, and then builds a new model for part localization by manually\nassembling latent patterns related to the target part via human interactions.\nWe use very few (e.g., three) annotations of a semantic object part to retrieve\ncertain latent patterns from conv-layers to represent the target part. We then\nvisualize these latent patterns and ask users to further remove incorrect\npatterns, in order to refine part representation. With the guidance of human\ninteractions, our method exhibited superior performance of part localization in\nexperiments. \n\n"}
{"id": "1708.01818", "contents": "Title: Depth Adaptive Deep Neural Network for Semantic Segmentation Abstract: In this work, we present the depth-adaptive deep neural network using a depth\nmap for semantic segmentation. Typical deep neural networks receive inputs at\nthe predetermined locations regardless of the distance from the camera. This\nfixed receptive field presents a challenge to generalize the features of\nobjects at various distances in neural networks. Specifically, the\npredetermined receptive fields are too small at a short distance, and vice\nversa. To overcome this challenge, we develop a neural network which is able to\nadapt the receptive field not only for each layer but also for each neuron at\nthe spatial location. To adjust the receptive field, we propose the\ndepth-adaptive multiscale (DaM) convolution layer consisting of the adaptive\nperception neuron and the in-layer multiscale neuron. The adaptive perception\nneuron is to adjust the receptive field at each spatial location using the\ncorresponding depth information. The in-layer multiscale neuron is to apply the\ndifferent size of the receptive field at each feature space to learn features\nat multiple scales. The proposed DaM convolution is applied to two fully\nconvolutional neural networks. We demonstrate the effectiveness of the proposed\nneural networks on the publicly available RGB-D dataset for semantic\nsegmentation and the novel hand segmentation dataset for hand-object\ninteraction. The experimental results show that the proposed method outperforms\nthe state-of-the-art methods without any additional layers or\npre/post-processing. \n\n"}
{"id": "1708.02237", "contents": "Title: Image Quality Assessment Techniques Show Improved Training and\n  Evaluation of Autoencoder Generative Adversarial Networks Abstract: We propose a training and evaluation approach for autoencoder Generative\nAdversarial Networks (GANs), specifically the Boundary Equilibrium Generative\nAdversarial Network (BEGAN), based on methods from the image quality assessment\nliterature. Our approach explores a multidimensional evaluation criterion that\nutilizes three distance functions: an $l_1$ score, the Gradient Magnitude\nSimilarity Mean (GMSM) score, and a chrominance score. We show that each of the\ndifferent distance functions captures a slightly different set of properties in\nimage space and, consequently, requires its own evaluation criterion to\nproperly assess whether the relevant property has been adequately learned. We\nshow that models using the new distance functions are able to produce better\nimages than the original BEGAN model in predicted ways. \n\n"}
{"id": "1708.02337", "contents": "Title: Unconstrained Face Detection and Open-Set Face Recognition Challenge Abstract: Face detection and recognition benchmarks have shifted toward more difficult\nenvironments. The challenge presented in this paper addresses the next step in\nthe direction of automatic detection and identification of people from outdoor\nsurveillance cameras. While face detection has shown remarkable success in\nimages collected from the web, surveillance cameras include more diverse\nocclusions, poses, weather conditions and image blur. Although face\nverification or closed-set face identification have surpassed human\ncapabilities on some datasets, open-set identification is much more complex as\nit needs to reject both unknown identities and false accepts from the face\ndetector. We show that unconstrained face detection can approach high detection\nrates albeit with moderate false accept rates. By contrast, open-set face\nrecognition is currently weak and requires much more attention. \n\n"}
{"id": "1708.03276", "contents": "Title: Document Image Binarization with Fully Convolutional Neural Networks Abstract: Binarization of degraded historical manuscript images is an important\npre-processing step for many document processing tasks. We formulate\nbinarization as a pixel classification learning task and apply a novel Fully\nConvolutional Network (FCN) architecture that operates at multiple image\nscales, including full resolution. The FCN is trained to optimize a continuous\nversion of the Pseudo F-measure metric and an ensemble of FCNs outperform the\ncompetition winners on 4 of 7 DIBCO competitions. This same binarization\ntechnique can also be applied to different domains such as Palm Leaf\nManuscripts with good performance. We analyze the performance of the proposed\nmodel w.r.t. the architectural hyperparameters, size and diversity of training\ndata, and the input features chosen. \n\n"}
{"id": "1708.04006", "contents": "Title: Fast, Accurate Thin-Structure Obstacle Detection for Autonomous Mobile\n  Robots Abstract: Safety is paramount for mobile robotic platforms such as self-driving cars\nand unmanned aerial vehicles. This work is devoted to a task that is\nindispensable for safety yet was largely overlooked in the past -- detecting\nobstacles that are of very thin structures, such as wires, cables and tree\nbranches. This is a challenging problem, as thin objects can be problematic for\nactive sensors such as lidar and sonar and even for stereo cameras. In this\nwork, we propose to use video sequences for thin obstacle detection. We\nrepresent obstacles with edges in the video frames, and reconstruct them in 3D\nusing efficient edge-based visual odometry techniques. We provide both a\nmonocular camera solution and a stereo camera solution. The former incorporates\nInertial Measurement Unit (IMU) data to solve scale ambiguity, while the latter\nenjoys a novel, purely vision-based solution. Experiments demonstrated that the\nproposed methods are fast and able to detect thin obstacles robustly and\naccurately under various conditions. \n\n"}
{"id": "1708.04069", "contents": "Title: Kinship Verification from Videos using Spatio-Temporal Texture Features\n  and Deep Learning Abstract: Automatic kinship verification using facial images is a relatively new and\nchallenging research problem in computer vision. It consists in automatically\npredicting whether two persons have a biological kin relation by examining\ntheir facial attributes. While most of the existing works extract shallow\nhandcrafted features from still face images, we approach this problem from\nspatio-temporal point of view and explore the use of both shallow texture\nfeatures and deep features for characterizing faces. Promising results,\nespecially those of deep features, are obtained on the benchmark UvA-NEMO Smile\ndatabase. Our extensive experiments also show the superiority of using videos\nover still images, hence pointing out the important role of facial dynamics in\nkinship verification. Furthermore, the fusion of the two types of features\n(i.e. shallow spatio-temporal texture features and deep features) shows\nsignificant performance improvements compared to state-of-the-art methods. \n\n"}
{"id": "1708.04347", "contents": "Title: Image Augmentation using Radial Transform for Training Deep Neural\n  Networks Abstract: Deep learning models have a large number of free parameters that must be\nestimated by efficient training of the models on a large number of training\ndata samples to increase their generalization performance. In real-world\napplications, the data available to train these networks is often limited or\nimbalanced. We propose a sampling method based on the radial transform in a\npolar coordinate system for image augmentation to facilitate the training of\ndeep learning models from limited source data. This pixel-wise transform\nprovides representations of the original image in the polar coordinate system\nby generating a new image from each pixel. This technique can generate radial\ntransformed images up to the number of pixels in the original image to increase\nthe diversity of poorly represented image classes. Our experiments show\nimproved generalization performance in training deep convolutional neural\nnetworks with radial transformed images. \n\n"}
{"id": "1708.04398", "contents": "Title: Monocular Dense 3D Reconstruction of a Complex Dynamic Scene from Two\n  Perspective Frames Abstract: This paper proposes a new approach for monocular dense 3D reconstruction of a\ncomplex dynamic scene from two perspective frames. By applying superpixel\nover-segmentation to the image, we model a generically dynamic (hence\nnon-rigid) scene with a piecewise planar and rigid approximation. In this way,\nwe reduce the dynamic reconstruction problem to a \"3D jigsaw puzzle\" problem\nwhich takes pieces from an unorganized \"soup of superpixels\". We show that our\nmethod provides an effective solution to the inherent relative scale ambiguity\nin structure-from-motion. Since our method does not assume a template prior, or\nper-object segmentation, or knowledge about the rigidity of the dynamic scene,\nit is applicable to a wide range of scenarios. Extensive experiments on both\nsynthetic and real monocular sequences demonstrate the superiority of our\nmethod compared with the state-of-the-art methods. \n\n"}
{"id": "1708.04811", "contents": "Title: Language Identification Using Deep Convolutional Recurrent Neural\n  Networks Abstract: Language Identification (LID) systems are used to classify the spoken\nlanguage from a given audio sample and are typically the first step for many\nspoken language processing tasks, such as Automatic Speech Recognition (ASR)\nsystems. Without automatic language detection, speech utterances cannot be\nparsed correctly and grammar rules cannot be applied, causing subsequent speech\nrecognition steps to fail. We propose a LID system that solves the problem in\nthe image domain, rather than the audio domain. We use a hybrid Convolutional\nRecurrent Neural Network (CRNN) that operates on spectrogram images of the\nprovided audio snippets. In extensive experiments we show, that our model is\napplicable to a range of noisy scenarios and can easily be extended to\npreviously unknown languages, while maintaining its classification accuracy. We\nrelease our code and a large scale training set for LID systems to the\ncommunity. \n\n"}
{"id": "1708.04943", "contents": "Title: Stacked Deconvolutional Network for Semantic Segmentation Abstract: Recent progress in semantic segmentation has been driven by improving the\nspatial resolution under Fully Convolutional Networks (FCNs). To address this\nproblem, we propose a Stacked Deconvolutional Network (SDN) for semantic\nsegmentation. In SDN, multiple shallow deconvolutional networks, which are\ncalled as SDN units, are stacked one by one to integrate contextual information\nand guarantee the fine recovery of localization information. Meanwhile,\ninter-unit and intra-unit connections are designed to assist network training\nand enhance feature fusion since the connections improve the flow of\ninformation and gradient propagation throughout the network. Besides,\nhierarchical supervision is applied during the upsampling process of each SDN\nunit, which guarantees the discrimination of feature representations and\nbenefits the network optimization. We carry out comprehensive experiments and\nachieve the new state-of-the-art results on three datasets, including PASCAL\nVOC 2012, CamVid, GATECH. In particular, our best model without CRF\npost-processing achieves an intersection-over-union score of 86.6% in the test\nset. \n\n"}
{"id": "1708.05071", "contents": "Title: Learning spectro-temporal features with 3D CNNs for speech emotion\n  recognition Abstract: In this paper, we propose to use deep 3-dimensional convolutional networks\n(3D CNNs) in order to address the challenge of modelling spectro-temporal\ndynamics for speech emotion recognition (SER). Compared to a hybrid of\nConvolutional Neural Network and Long-Short-Term-Memory (CNN-LSTM), our\nproposed 3D CNNs simultaneously extract short-term and long-term spectral\nfeatures with a moderate number of parameters. We evaluated our proposed and\nother state-of-the-art methods in a speaker-independent manner using aggregated\ncorpora that give a large and diverse set of speakers. We found that 1) shallow\ntemporal and moderately deep spectral kernels of a homogeneous architecture are\noptimal for the task; and 2) our 3D CNNs are more effective for\nspectro-temporal feature learning compared to other methods. Finally, we\nvisualised the feature space obtained with our proposed method using\nt-distributed stochastic neighbour embedding (T-SNE) and could observe distinct\nclusters of emotions. \n\n"}
{"id": "1708.05256", "contents": "Title: Deep Learning at 15PF: Supervised and Semi-Supervised Classification for\n  Scientific Data Abstract: This paper presents the first, 15-PetaFLOP Deep Learning system for solving\nscientific pattern classification problems on contemporary HPC architectures.\nWe develop supervised convolutional architectures for discriminating signals in\nhigh-energy physics data as well as semi-supervised architectures for\nlocalizing and classifying extreme weather in climate data. Our\nIntelcaffe-based implementation obtains $\\sim$2TFLOP/s on a single Cori\nPhase-II Xeon-Phi node. We use a hybrid strategy employing synchronous\nnode-groups, while using asynchronous communication across groups. We use this\nstrategy to scale training of a single model to $\\sim$9600 Xeon-Phi nodes;\nobtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of\n11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art\nclassification accuracy on a dataset with 10M images, exceeding that achieved\nby selections on high-level physics-motivated features. Our semi-supervised\narchitecture successfully extracts weather patterns in a 15TB climate dataset.\nOur results demonstrate that Deep Learning can be optimized and scaled\neffectively on many-core, HPC systems. \n\n"}
{"id": "1708.06118", "contents": "Title: Distantly Supervised Road Segmentation Abstract: We present an approach for road segmentation that only requires image-level\nannotations at training time. We leverage distant supervision, which allows us\nto train our model using images that are different from the target domain.\nUsing large publicly available image databases as distant supervisors, we\ndevelop a simple method to automatically generate weak pixel-wise road masks.\nThese are used to iteratively train a fully convolutional neural network, which\nproduces our final segmentation model. We evaluate our method on the Cityscapes\ndataset, where we compare it with a fully supervised approach. Further, we\ndiscuss the trade-off between annotation cost and performance. Overall, our\ndistantly supervised approach achieves 93.8% of the performance of the fully\nsupervised approach, while using orders of magnitude less annotation work. \n\n"}
{"id": "1708.06320", "contents": "Title: Learning Spread-out Local Feature Descriptors Abstract: We propose a simple, yet powerful regularization technique that can be used\nto significantly improve both the pairwise and triplet losses in learning local\nfeature descriptors. The idea is that in order to fully utilize the expressive\npower of the descriptor space, good local feature descriptors should be\nsufficiently \"spread-out\" over the space. In this work, we propose a\nregularization term to maximize the spread in feature descriptor inspired by\nthe property of uniform distribution. We show that the proposed regularization\nwith triplet loss outperforms existing Euclidean distance based descriptor\nlearning techniques by a large margin. As an extension, the proposed\nregularization technique can also be used to improve image-level deep feature\nembedding. \n\n"}
{"id": "1708.06670", "contents": "Title: CNN Fixations: An unraveling approach to visualize the discriminative\n  image regions Abstract: Deep convolutional neural networks (CNN) have revolutionized various fields\nof vision research and have seen unprecedented adoption for multiple tasks such\nas classification, detection, captioning, etc. However, they offer little\ntransparency into their inner workings and are often treated as black boxes\nthat deliver excellent performance. In this work, we aim at alleviating this\nopaqueness of CNNs by providing visual explanations for the network's\npredictions. Our approach can analyze variety of CNN based models trained for\nvision applications such as object recognition and caption generation. Unlike\nexisting methods, we achieve this via unraveling the forward pass operation.\nProposed method exploits feature dependencies across the layer hierarchy and\nuncovers the discriminative image locations that guide the network's\npredictions. We name these locations CNN-Fixations, loosely analogous to human\neye fixations.\n  Our approach is a generic method that requires no architectural changes,\nadditional training or gradient computation and computes the important image\nlocations (CNN Fixations). We demonstrate through a variety of applications\nthat our approach is able to localize the discriminative image locations across\ndifferent network architectures, diverse vision tasks and data modalities. \n\n"}
{"id": "1708.07517", "contents": "Title: FacePoseNet: Making a Case for Landmark-Free Face Alignment Abstract: We show how a simple convolutional neural network (CNN) can be trained to\naccurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose,\ndirectly from image intensities. We further explain how this FacePoseNet (FPN)\ncan be used to align faces in 2D and 3D as an alternative to explicit facial\nlandmark detection for these tasks. We claim that in many cases the standard\nmeans of measuring landmark detector accuracy can be misleading when comparing\ndifferent face alignments. Instead, we compare our FPN with existing methods by\nevaluating how they affect face recognition accuracy on the IJB-A and IJB-B\nbenchmarks: using the same recognition pipeline, but varying the face alignment\nmethod. Our results show that (a) better landmark detection accuracy measured\non the 300W benchmark does not necessarily imply better face recognition\naccuracy. (b) Our FPN provides superior 2D and 3D face alignment on both\nbenchmarks. Finally, (c), FPN aligns faces at a small fraction of the\ncomputational cost of comparably accurate landmark detectors. For many\npurposes, FPN is thus a far faster and far more accurate face alignment method\nthan using facial landmark detectors. \n\n"}
{"id": "1708.08705", "contents": "Title: Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary\n  Learning Abstract: The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model,\nconsisting of a cascade of convolutional sparse layers, provides a new\ninterpretation of Convolutional Neural Networks (CNNs). Under this framework,\nthe computation of the forward pass in a CNN is equivalent to a pursuit\nalgorithm aiming to estimate the nested sparse representation vectors -- or\nfeature maps -- from a given input signal. Despite having served as a pivotal\nconnection between CNNs and sparse modeling, a deeper understanding of the\nML-CSC is still lacking: there are no pursuit algorithms that can serve this\nmodel exactly, nor are there conditions to guarantee a non-empty model. While\none can easily obtain signals that approximately satisfy the ML-CSC\nconstraints, it remains unclear how to simply sample from the model and, more\nimportantly, how one can train the convolutional filters from real data.\n  In this work, we propose a sound pursuit algorithm for the ML-CSC model by\nadopting a projection approach. We provide new and improved bounds on the\nstability of the solution of such pursuit and we analyze different practical\nalternatives to implement this in practice. We show that the training of the\nfilters is essential to allow for non-trivial signals in the model, and we\nderive an online algorithm to learn the dictionaries from real data,\neffectively resulting in cascaded sparse convolutional layers. Last, but not\nleast, we demonstrate the applicability of the ML-CSC model for several\napplications in an unsupervised setting, providing competitive results. Our\nwork represents a bridge between matrix factorization, sparse dictionary\nlearning and sparse auto-encoders, and we analyze these connections in detail. \n\n"}
{"id": "1708.09666", "contents": "Title: Generating Video Descriptions with Topic Guidance Abstract: Generating video descriptions in natural language (a.k.a. video captioning)\nis a more challenging task than image captioning as the videos are\nintrinsically more complicated than images in two aspects. First, videos cover\na broader range of topics, such as news, music, sports and so on. Second,\nmultiple topics could coexist in the same video. In this paper, we propose a\nnovel caption model, topic-guided model (TGM), to generate topic-oriented\ndescriptions for videos in the wild via exploiting topic information. In\naddition to predefined topics, i.e., category tags crawled from the web, we\nalso mine topics in a data-driven way based on training captions by an\nunsupervised topic mining model. We show that data-driven topics reflect a\nbetter topic schema than the predefined topics. As for testing video topic\nprediction, we treat the topic mining model as teacher to train the student,\nthe topic prediction model, by utilizing the full multi-modalities in the video\nespecially the speech modality. We propose a series of caption models to\nexploit topic guidance, including implicitly using the topics as input features\nto generate words related to the topic and explicitly modifying the weights in\nthe decoder with topics to function as an ensemble of topic-aware language\ndecoders. Our comprehensive experimental results on the current largest video\ncaption dataset MSR-VTT prove the effectiveness of our topic-guided model,\nwhich significantly surpasses the winning performance in the 2016 MSR video to\nlanguage challenge. \n\n"}
{"id": "1709.01077", "contents": "Title: A Nonparametric Model for Multimodal Collaborative Activities\n  Summarization Abstract: Ego-centric data streams provide a unique opportunity to reason about joint\nbehavior by pooling data across individuals. This is especially evident in\nurban environments teeming with human activities, but which suffer from\nincomplete and noisy data. Collaborative human activities exhibit common\nspatial, temporal, and visual characteristics facilitating inference across\nindividuals from multiple sensory modalities as we explore in this paper from\nthe perspective of meetings. We propose a new Bayesian nonparametric model that\nenables us to efficiently pool video and GPS data towards collaborative\nactivities analysis from multiple individuals. We demonstrate the utility of\nthis model for inference tasks such as activity detection, classification, and\nsummarization. We further demonstrate how spatio-temporal structure embedded in\nour model enables better understanding of partial and noisy observations such\nas localization and face detections based on social interactions. We show\nresults on both synthetic experiments and a new dataset of egocentric video and\nnoisy GPS data from multiple individuals. \n\n"}
{"id": "1709.01220", "contents": "Title: Multi-Modal Multi-Scale Deep Learning for Large-Scale Image Annotation Abstract: Image annotation aims to annotate a given image with a variable number of\nclass labels corresponding to diverse visual concepts. In this paper, we\naddress two main issues in large-scale image annotation: 1) how to learn a rich\nfeature representation suitable for predicting a diverse set of visual concepts\nranging from object, scene to abstract concept; 2) how to annotate an image\nwith the optimal number of class labels. To address the first issue, we propose\na novel multi-scale deep model for extracting rich and discriminative features\ncapable of representing a wide range of visual concepts. Specifically, a novel\ntwo-branch deep neural network architecture is proposed which comprises a very\ndeep main network branch and a companion feature fusion network branch designed\nfor fusing the multi-scale features computed from the main branch. The deep\nmodel is also made multi-modal by taking noisy user-provided tags as model\ninput to complement the image input. For tackling the second issue, we\nintroduce a label quantity prediction auxiliary task to the main label\nprediction task to explicitly estimate the optimal label number for a given\nimage. Extensive experiments are carried out on two large-scale image\nannotation benchmark datasets and the results show that our method\nsignificantly outperforms the state-of-the-art. \n\n"}
{"id": "1709.01421", "contents": "Title: Multi-label Class-imbalanced Action Recognition in Hockey Videos via 3D\n  Convolutional Neural Networks Abstract: Automatic analysis of the video is one of most complex problems in the fields\nof computer vision and machine learning. A significant part of this research\ndeals with (human) activity recognition (HAR) since humans, and the activities\nthat they perform, generate most of the video semantics. Video-based HAR has\napplications in various domains, but one of the most important and challenging\nis HAR in sports videos. Some of the major issues include high inter- and\nintra-class variations, large class imbalance, the presence of both group\nactions and single player actions, and recognizing simultaneous actions, i.e.,\nthe multi-label learning problem. Keeping in mind these challenges and the\nrecent success of CNNs in solving various computer vision problems, in this\nwork, we implement a 3D CNN based multi-label deep HAR system for multi-label\nclass-imbalanced action recognition in hockey videos. We test our system for\ntwo different scenarios: an ensemble of $k$ binary networks vs. a single\n$k$-output network, on a publicly available dataset. We also compare our\nresults with the system that was originally designed for the chosen dataset.\nExperimental results show that the proposed approach performs better than the\nexisting solution. \n\n"}
{"id": "1709.01467", "contents": "Title: Subspace Segmentation by Successive Approximations: A Method for\n  Low-Rank and High-Rank Data with Missing Entries Abstract: We propose a method to reconstruct and cluster incomplete high-dimensional\ndata lying in a union of low-dimensional subspaces. Exploring the sparse\nrepresentation model, we jointly estimate the missing data while imposing the\nintrinsic subspace structure. Since we have a non-convex problem, we propose an\niterative method to reconstruct the data and provide a sparse similarity\naffinity matrix. This method is robust to initialization and achieves greater\nreconstruction accuracy than current methods, which dramatically improves\nclustering performance. Extensive experiments with synthetic and real data show\nthat our approach leads to significant improvements in the reconstruction and\nsegmentation, outperforming current state of the art for both low and high-rank\ndata. \n\n"}
{"id": "1709.01784", "contents": "Title: Cross-Domain Image Retrieval with Attention Modeling Abstract: With the proliferation of e-commerce websites and the ubiquitousness of smart\nphones, cross-domain image retrieval using images taken by smart phones as\nqueries to search products on e-commerce websites is emerging as a popular\napplication. One challenge of this task is to locate the attention of both the\nquery and database images. In particular, database images, e.g. of fashion\nproducts, on e-commerce websites are typically displayed with other\naccessories, and the images taken by users contain noisy background and large\nvariations in orientation and lighting. Consequently, their attention is\ndifficult to locate. In this paper, we exploit the rich tag information\navailable on the e-commerce websites to locate the attention of database\nimages. For query images, we use each candidate image in the database as the\ncontext to locate the query attention. Novel deep convolutional neural network\narchitectures, namely TagYNet and CtxYNet, are proposed to learn the attention\nweights and then extract effective representations of the images. Experimental\nresults on public datasets confirm that our approaches have significant\nimprovement over the existing methods in terms of the retrieval accuracy and\nefficiency. \n\n"}
{"id": "1709.02228", "contents": "Title: FingerNet: An Unified Deep Network for Fingerprint Minutiae Extraction Abstract: Minutiae extraction is of critical importance in automated fingerprint\nrecognition. Previous works on rolled/slap fingerprints failed on latent\nfingerprints due to noisy ridge patterns and complex background noises. In this\npaper, we propose a new way to design deep convolutional network combining\ndomain knowledge and the representation ability of deep learning. In terms of\norientation estimation, segmentation, enhancement and minutiae extraction,\nseveral typical traditional methods performed well on rolled/slap fingerprints\nare transformed into convolutional manners and integrated as an unified plain\nnetwork. We demonstrate that this pipeline is equivalent to a shallow network\nwith fixed weights. The network is then expanded to enhance its representation\nability and the weights are released to learn complex background variance from\ndata, while preserving end-to-end differentiability. Experimental results on\nNIST SD27 latent database and FVC 2004 slap database demonstrate that the\nproposed algorithm outperforms the state-of-the-art minutiae extraction\nalgorithms. Code is made publicly available at:\nhttps://github.com/felixTY/FingerNet. \n\n"}
{"id": "1709.02482", "contents": "Title: Scalable Annotation of Fine-Grained Categories Without Experts Abstract: We present a crowdsourcing workflow to collect image annotations for visually\nsimilar synthetic categories without requiring experts. In animals, there is a\ndirect link between taxonomy and visual similarity: e.g. a collie (type of dog)\nlooks more similar to other collies (e.g. smooth collie) than a greyhound\n(another type of dog). However, in synthetic categories such as cars, objects\nwith similar taxonomy can have very different appearance: e.g. a 2011 Ford\nF-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very\ndifferent from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based\ncrowdsourcing algorithm to automatically group visually indistinguishable\nobjects together. Using our workflow, we label 712,430 images by ~1,000 Amazon\nMechanical Turk workers; resulting in the largest fine-grained visual dataset\nreported to date with 2,657 categories of cars annotated at 1/20th the cost of\nhiring experts. \n\n"}
{"id": "1709.03524", "contents": "Title: Recovering Homography from Camera Captured Documents using Convolutional\n  Neural Networks Abstract: Removing perspective distortion from hand held camera captured document\nimages is one of the primitive tasks in document analysis, but unfortunately,\nno such method exists that can reliably remove the perspective distortion from\ndocument images automatically. In this paper, we propose a convolutional neural\nnetwork based method for recovering homography from hand-held camera captured\ndocuments.\n  Our proposed method works independent of document's underlying content and is\ntrained end-to-end in a fully automatic way. Specifically, this paper makes\nfollowing three contributions: Firstly, we introduce a large scale synthetic\ndataset for recovering homography from documents images captured under\ndifferent geometric and photometric transformations; secondly, we show that a\ngeneric convolutional neural network based architecture can be successfully\nused for regressing the corners positions of documents captured under wild\nsettings; thirdly, we show that L1 loss can be reliably used for corners\nregression. Our proposed method gives state-of-the-art performance on the\ntested datasets, and has potential to become an integral part of document\nanalysis pipeline. \n\n"}
{"id": "1709.03612", "contents": "Title: Holistic, Instance-Level Human Parsing Abstract: Object parsing -- the task of decomposing an object into its semantic parts\n-- has traditionally been formulated as a category-level segmentation problem.\nConsequently, when there are multiple objects in an image, current methods\ncannot count the number of objects in the scene, nor can they determine which\npart belongs to which object. We address this problem by segmenting the parts\nof objects at an instance-level, such that each pixel in the image is assigned\na part label, as well as the identity of the object it belongs to. Moreover, we\nshow how this approach benefits us in obtaining segmentations at coarser\ngranularities as well. Our proposed network is trained end-to-end given\ndetections, and begins with a category-level segmentation module. Thereafter, a\ndifferentiable Conditional Random Field, defined over a variable number of\ninstances for every input image, reasons about the identity of each part by\nassociating it with a human detection. In contrast to other approaches, our\nmethod can handle the varying number of people in each image and our holistic\nnetwork produces state-of-the-art results in instance-level part and human\nsegmentation, together with competitive results in category-level part\nsegmentation, all achieved by a single forward-pass through our neural network. \n\n"}
{"id": "1709.03749", "contents": "Title: Deep Mean-Shift Priors for Image Restoration Abstract: In this paper we introduce a natural image prior that directly represents a\nGaussian-smoothed version of the natural image distribution. We include our\nprior in a formulation of image restoration as a Bayes estimator that also\nallows us to solve noise-blind image restoration problems. We show that the\ngradient of our prior corresponds to the mean-shift vector on the natural image\ndistribution. In addition, we learn the mean-shift vector field using denoising\nautoencoders, and use it in a gradient descent approach to perform Bayes risk\nminimization. We demonstrate competitive results for noise-blind deblurring,\nsuper-resolution, and demosaicing. \n\n"}
{"id": "1709.03966", "contents": "Title: Unsupervised Deep Homography: A Fast and Robust Homography Estimation\n  Model Abstract: Homography estimation between multiple aerial images can provide relative\npose estimation for collaborative autonomous exploration and monitoring. The\nusage on a robotic system requires a fast and robust homography estimation\nalgorithm. In this study, we propose an unsupervised learning algorithm that\ntrains a Deep Convolutional Neural Network to estimate planar homographies. We\ncompare the proposed algorithm to traditional feature-based and direct methods,\nas well as a corresponding supervised learning algorithm. Our empirical results\ndemonstrate that compared to traditional approaches, the unsupervised algorithm\nachieves faster inference speed, while maintaining comparable or better\naccuracy and robustness to illumination variation. In addition, on both a\nsynthetic dataset and representative real-world aerial dataset, our\nunsupervised method has superior adaptability and performance compared to the\nsupervised deep learning method. \n\n"}
{"id": "1709.04108", "contents": "Title: Co-training for Demographic Classification Using Deep Learning from\n  Label Proportions Abstract: Deep learning algorithms have recently produced state-of-the-art accuracy in\nmany classification tasks, but this success is typically dependent on access to\nmany annotated training examples. For domains without such data, an attractive\nalternative is to train models with light, or distant supervision. In this\npaper, we introduce a deep neural network for the Learning from Label\nProportion (LLP) setting, in which the training data consist of bags of\nunlabeled instances with associated label distributions for each bag. We\nintroduce a new regularization layer, Batch Averager, that can be appended to\nthe last layer of any deep neural network to convert it from supervised\nlearning to LLP. This layer can be implemented readily with existing deep\nlearning packages. To further support domains in which the data consist of two\nconditionally independent feature views (e.g. image and text), we propose a\nco-training algorithm that iteratively generates pseudo bags and refits the\ndeep LLP model to improve classification accuracy. We demonstrate our models on\ndemographic attribute classification (gender and race/ethnicity), which has\nmany applications in social media analysis, public health, and marketing. We\nconduct experiments to predict demographics of Twitter users based on their\ntweets and profile image, without requiring any user-level annotations for\ntraining. We find that the deep LLP approach outperforms baselines for both\ntext and image features separately. Additionally, we find that co-training\nalgorithm improves image and text classification by 4% and 8% absolute F1,\nrespectively. Finally, an ensemble of text and image classifiers further\nimproves the absolute F1 measure by 4% on average. \n\n"}
{"id": "1709.04121", "contents": "Title: Sketch-pix2seq: a Model to Generate Sketches of Multiple Categories Abstract: Sketch is an important media for human to communicate ideas, which reflects\nthe superiority of human intelligence. Studies on sketch can be roughly\nsummarized into recognition and generation. Existing models on image\nrecognition failed to obtain satisfying performance on sketch classification.\nBut for sketch generation, a recent study proposed a sequence-to-sequence\nvariational-auto-encoder (VAE) model called sketch-rnn which was able to\ngenerate sketches based on human inputs. The model achieved amazing results\nwhen asked to learn one category of object, such as an animal or a vehicle.\nHowever, the performance dropped when multiple categories were fed into the\nmodel. Here, we proposed a model called sketch-pix2seq which could learn and\ndraw multiple categories of sketches. Two modifications were made to improve\nthe sketch-rnn model: one is to replace the bidirectional recurrent neural\nnetwork (BRNN) encoder with a convolutional neural network(CNN); the other is\nto remove the Kullback-Leibler divergence from the objective function of VAE.\nExperimental results showed that models with CNN encoders outperformed those\nwith RNN encoders in generating human-style sketches. Visualization of the\nlatent space illustrated that the removal of KL-divergence made the encoder\nlearn a posterior of latent space that reflected the features of different\ncategories. Moreover, the combination of CNN encoder and removal of\nKL-divergence, i.e., the sketch-pix2seq model, had better performance in\nlearning and generating sketches of multiple categories and showed promising\nresults in creativity tasks. \n\n"}
{"id": "1709.04647", "contents": "Title: Detection of Unauthorized IoT Devices Using Machine Learning Techniques Abstract: Security experts have demonstrated numerous risks imposed by Internet of\nThings (IoT) devices on organizations. Due to the widespread adoption of such\ndevices, their diversity, standardization obstacles, and inherent mobility,\norganizations require an intelligent mechanism capable of automatically\ndetecting suspicious IoT devices connected to their networks. In particular,\ndevices not included in a white list of trustworthy IoT device types (allowed\nto be used within the organizational premises) should be detected. In this\nresearch, Random Forest, a supervised machine learning algorithm, was applied\nto features extracted from network traffic data with the aim of accurately\nidentifying IoT device types from the white list. To train and evaluate\nmulti-class classifiers, we collected and manually labeled network traffic data\nfrom 17 distinct IoT devices, representing nine types of IoT devices. Based on\nthe classification of 20 consecutive sessions and the use of majority rule, IoT\ndevice types that are not on the white list were correctly detected as unknown\nin 96% of test cases (on average), and white listed device types were correctly\nclassified by their actual types in 99% of cases. Some IoT device types were\nidentified quicker than others (e.g., sockets and thermostats were successfully\ndetected within five TCP sessions of connecting to the network). Perfect\ndetection of unauthorized IoT device types was achieved upon analyzing 110\nconsecutive sessions; perfect classification of white listed types required 346\nconsecutive sessions, 110 of which resulted in 99.49% accuracy. Further\nexperiments demonstrated the successful applicability of classifiers trained in\none location and tested on another. In addition, a discussion is provided\nregarding the resilience of our machine learning-based IoT white listing method\nto adversarial attacks. \n\n"}
{"id": "1709.04905", "contents": "Title: One-Shot Visual Imitation Learning via Meta-Learning Abstract: In order for a robot to be a generalist that can perform a wide range of\njobs, it must be able to acquire a wide variety of skills quickly and\nefficiently in complex unstructured environments. High-capacity models such as\ndeep neural networks can enable a robot to represent complex skills, but\nlearning each skill from scratch then becomes infeasible. In this work, we\npresent a meta-imitation learning method that enables a robot to learn how to\nlearn more efficiently, allowing it to acquire new skills from just a single\ndemonstration. Unlike prior methods for one-shot imitation, our method can\nscale to raw pixel inputs and requires data from significantly fewer prior\ntasks for effective learning of new skills. Our experiments on both simulated\nand real robot platforms demonstrate the ability to learn new tasks,\nend-to-end, from a single visual demonstration. \n\n"}
{"id": "1709.05424", "contents": "Title: NIMA: Neural Image Assessment Abstract: Automatically learned quality assessment for images has recently become a hot\ntopic due to its usefulness in a wide variety of applications such as\nevaluating image capture pipelines, storage techniques and sharing media.\nDespite the subjective nature of this problem, most existing methods only\npredict the mean opinion score provided by datasets such as AVA [1] and TID2013\n[2]. Our approach differs from others in that we predict the distribution of\nhuman opinion scores using a convolutional neural network. Our architecture\nalso has the advantage of being significantly simpler than other methods with\ncomparable performance. Our proposed approach relies on the success (and\nretraining) of proven, state-of-the-art deep object recognition networks. Our\nresulting network can be used to not only score images reliably and with high\ncorrelation to human perception, but also to assist with adaptation and\noptimization of photo editing/enhancement algorithms in a photographic\npipeline. All this is done without need for a \"golden\" reference image,\nconsequently allowing for single-image, semantic- and perceptually-aware,\nno-reference quality assessment. \n\n"}
{"id": "1709.05439", "contents": "Title: To Go or Not To Go? A Near Unsupervised Learning Approach For Robot\n  Navigation Abstract: It is important for robots to be able to decide whether they can go through a\nspace or not, as they navigate through a dynamic environment. This capability\ncan help them avoid injury or serious damage, e.g., as a result of running into\npeople and obstacles, getting stuck, or falling off an edge. To this end, we\npropose an unsupervised and a near-unsupervised method based on Generative\nAdversarial Networks (GAN) to classify scenarios as traversable or not based on\nvisual data. Our method is inspired by the recent success of data-driven\napproaches on computer vision problems and anomaly detection, and reduces the\nneed for vast amounts of negative examples at training time. Collecting\nnegative data indicating that a robot should not go through a space is\ntypically hard and dangerous because of collisions, whereas collecting positive\ndata can be automated and done safely based on the robot's own traveling\nexperience. We verify the generality and effectiveness of the proposed approach\non a test dataset collected in a previously unseen environment with a mobile\nrobot. Furthermore, we show that our method can be used to build costmaps (we\ncall as \"GoNoGo\" costmaps) for robot path planning using visual data only. \n\n"}
{"id": "1709.06129", "contents": "Title: When is a Convolutional Filter Easy To Learn? Abstract: We analyze the convergence of (stochastic) gradient descent algorithm for\nlearning a convolutional filter with Rectified Linear Unit (ReLU) activation\nfunction. Our analysis does not rely on any specific form of the input\ndistribution and our proofs only use the definition of ReLU, in contrast with\nprevious works that are restricted to standard Gaussian input. We show that\n(stochastic) gradient descent with random initialization can learn the\nconvolutional filter in polynomial time and the convergence rate depends on the\nsmoothness of the input distribution and the closeness of patches. To the best\nof our knowledge, this is the first recovery guarantee of gradient-based\nalgorithms for convolutional filter on non-Gaussian input distributions. Our\ntheory also justifies the two-stage learning rate strategy in deep neural\nnetworks. While our focus is theoretical, we also present experiments that\nillustrate our theoretical findings. \n\n"}
{"id": "1709.06308", "contents": "Title: Exploring Human-like Attention Supervision in Visual Question Answering Abstract: Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA. \n\n"}
{"id": "1709.07330", "contents": "Title: H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor\n  Segmentation from CT Volumes Abstract: Liver cancer is one of the leading causes of cancer death. To assist doctors\nin hepatocellular carcinoma diagnosis and treatment planning, an accurate and\nautomatic liver and tumor segmentation method is highly demanded in the\nclinical practice. Recently, fully convolutional neural networks (FCNs),\nincluding 2D and 3D FCNs, serve as the back-bone in many volumetric image\nsegmentation. However, 2D convolutions can not fully leverage the spatial\ninformation along the third dimension while 3D convolutions suffer from high\ncomputational cost and GPU memory consumption. To address these issues, we\npropose a novel hybrid densely connected UNet (H-DenseUNet), which consists of\na 2D DenseUNet for efficiently extracting intra-slice features and a 3D\ncounterpart for hierarchically aggregating volumetric contexts under the spirit\nof the auto-context algorithm for liver and tumor segmentation. We formulate\nthe learning process of H-DenseUNet in an end-to-end manner, where the\nintra-slice representations and inter-slice features can be jointly optimized\nthrough a hybrid feature fusion (HFF) layer. We extensively evaluated our\nmethod on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge\nand 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the\nsegmentation results of tumors and achieved very competitive performance for\nliver segmentation even with a single model. \n\n"}
{"id": "1709.09882", "contents": "Title: Are we done with object recognition? The iCub robot's perspective Abstract: We report on an extensive study of the benefits and limitations of current\ndeep learning approaches to object recognition in robot vision scenarios,\nintroducing a novel dataset used for our investigation. To avoid the biases in\ncurrently available datasets, we consider a natural human-robot interaction\nsetting to design a data-acquisition protocol for visual object recognition on\nthe iCub humanoid robot. Analyzing the performance of off-the-shelf models\ntrained off-line on large-scale image retrieval datasets, we show the necessity\nfor knowledge transfer. We evaluate different ways in which this last step can\nbe done, and identify the major bottlenecks affecting robotic scenarios. By\nstudying both object categorization and identification problems, we highlight\nkey differences between object recognition in robotics applications and in\nimage retrieval tasks, for which the considered deep learning approaches have\nbeen originally designed. In a nutshell, our results confirm the remarkable\nimprovements yield by deep learning in this setting, while pointing to specific\nopen challenges that need be addressed for seamless deployment in robotics. \n\n"}
{"id": "1710.00166", "contents": "Title: PCANet-II: When PCANet Meets the Second Order Pooling Abstract: PCANet, as one noticeable shallow network, employs the histogram\nrepresentation for feature pooling. However, there are three main problems\nabout this kind of pooling method. First, the histogram-based pooling method\nbinarizes the feature maps and leads to inevitable discriminative information\nloss. Second, it is difficult to effectively combine other visual cues into a\ncompact representation, because the simple concatenation of various visual cues\nleads to feature representation inefficiency. Third, the dimensionality of\nhistogram-based output grows exponentially with the number of feature maps\nused. In order to overcome these problems, we propose a novel shallow network\nmodel, named as PCANet-II. Compared with the histogram-based output, the second\norder pooling not only provides more discriminative information by preserving\nboth the magnitude and sign of convolutional responses, but also dramatically\nreduces the size of output features. Thus we combine the second order\nstatistical pooling method with the shallow network, i.e., PCANet. Moreover, it\nis easy to combine other discriminative and robust cues by using the second\norder pooling. So we introduce the binary feature difference encoding scheme\ninto our PCANet-II to further improve robustness. Experiments demonstrate the\neffectiveness and robustness of our proposed PCANet-II method. \n\n"}
{"id": "1710.00290", "contents": "Title: Translating Videos to Commands for Robotic Manipulation with Deep\n  Recurrent Neural Networks Abstract: We present a new method to translate videos to commands for robotic\nmanipulation using Deep Recurrent Neural Networks (RNN). Our framework first\nextracts deep features from the input video frames with a deep Convolutional\nNeural Networks (CNN). Two RNN layers with an encoder-decoder architecture are\nthen used to encode the visual features and sequentially generate the output\nwords as the command. We demonstrate that the translation accuracy can be\nimproved by allowing a smooth transaction between two RNN layers and using the\nstate-of-the-art feature extractor. The experimental results on our new\nchallenging dataset show that our approach outperforms recent methods by a fair\nmargin. Furthermore, we combine the proposed translation module with the vision\nand planning system to let a robot perform various manipulation tasks. Finally,\nwe demonstrate the effectiveness of our framework on a full-size humanoid robot\nWALK-MAN. \n\n"}
{"id": "1710.01115", "contents": "Title: Detection of Inferior Myocardial Infarction using Shallow Convolutional\n  Neural Networks Abstract: Myocardial Infarction is one of the leading causes of death worldwide. This\npaper presents a Convolutional Neural Network (CNN) architecture which takes\nraw Electrocardiography (ECG) signal from lead II, III and AVF and\ndifferentiates between inferior myocardial infarction (IMI) and healthy\nsignals. The performance of the model is evaluated on IMI and healthy signals\nobtained from Physikalisch-Technische Bundesanstalt (PTB) database. A\nsubject-oriented approach is taken to comprehend the generalization capability\nof the model and compared with the current state of the art. In a\nsubject-oriented approach, the network is tested on one patient and trained on\nrest of the patients. Our model achieved a superior metrics scores (accuracy=\n84.54%, sensitivity= 85.33% and specificity= 84.09%) when compared to the\nbenchmark. We also analyzed the discriminating strength of the features\nextracted by the convolutional layers by means of geometric separability index\nand euclidean distance and compared it with the benchmark model. \n\n"}
{"id": "1710.01727", "contents": "Title: Privacy-Preserving Deep Inference for Rich User Data on The Cloud Abstract: Deep neural networks are increasingly being used in a variety of machine\nlearning applications applied to rich user data on the cloud. However, this\napproach introduces a number of privacy and efficiency challenges, as the cloud\noperator can perform secondary inferences on the available data. Recently,\nadvances in edge processing have paved the way for more efficient, and private,\ndata processing at the source for simple tasks and lighter models, though they\nremain a challenge for larger, and more complicated models. In this paper, we\npresent a hybrid approach for breaking down large, complex deep models for\ncooperative, privacy-preserving analytics. We do this by breaking down the\npopular deep architectures and fine-tune them in a particular way. We then\nevaluate the privacy benefits of this approach based on the information exposed\nto the cloud service. We also asses the local inference cost of different\nlayers on a modern handset for mobile applications. Our evaluations show that\nby using certain kind of fine-tuning and embedding techniques and at a small\nprocessing costs, we can greatly reduce the level of information available to\nunintended tasks applied to the data feature on the cloud, and hence achieving\nthe desired tradeoff between privacy and performance. \n\n"}
{"id": "1710.01820", "contents": "Title: Energy-Based Spherical Sparse Coding Abstract: In this paper, we explore an efficient variant of convolutional sparse coding\nwith unit norm code vectors where reconstruction quality is evaluated using an\ninner product (cosine distance). To use these codes for discriminative\nclassification, we describe a model we term Energy-Based Spherical Sparse\nCoding (EB-SSC) in which the hypothesized class label introduces a learned\nlinear bias into the coding step. We evaluate and visualize performance of\nstacking this encoder to make a deep layered model for image classification. \n\n"}
{"id": "1710.03959", "contents": "Title: Deep learning in remote sensing: a review Abstract: Standing at the paradigm shift towards data-intensive science, machine\nlearning techniques are becoming increasingly important. In particular, as a\nmajor breakthrough in the field, deep learning has proven as an extremely\npowerful tool in many fields. Shall we embrace deep learning as the key to all?\nOr, should we resist a 'black-box' solution? There are controversial opinions\nin the remote sensing community. In this article, we analyze the challenges of\nusing deep learning for remote sensing data analysis, review the recent\nadvances, and provide resources to make deep learning in remote sensing\nridiculously simple to start with. More importantly, we advocate remote sensing\nscientists to bring their expertise into deep learning, and use it as an\nimplicit general model to tackle unprecedented large-scale influential\nchallenges, such as climate change and urbanization. \n\n"}
{"id": "1710.04112", "contents": "Title: Batch-based Activity Recognition from Egocentric Photo-Streams Revisited Abstract: Wearable cameras can gather large a\\-mounts of image data that provide rich\nvisual information about the daily activities of the wearer. Motivated by the\nlarge number of health applications that could be enabled by the automatic\nrecognition of daily activities, such as lifestyle characterization for habit\nimprovement, context-aware personal assistance and tele-rehabilitation\nservices, we propose a system to classify 21 daily activities from\nphoto-streams acquired by a wearable photo-camera. Our approach combines the\nadvantages of a Late Fusion Ensemble strategy relying on convolutional neural\nnetworks at image level with the ability of recurrent neural networks to\naccount for the temporal evolution of high level features in photo-streams\nwithout relying on event boundaries. The proposed batch-based approach achieved\nan overall accuracy of 89.85\\%, outperforming state of the art end-to-end\nmethodologies. These results were achieved on a dataset consists of 44,902\negocentric pictures from three persons captured during 26 days in average. \n\n"}
{"id": "1710.05268", "contents": "Title: Self-Supervised Visual Planning with Temporal Skip Connections Abstract: In order to autonomously learn wide repertoires of complex skills, robots\nmust be able to learn from their own autonomously collected data, without human\nsupervision. One learning signal that is always available for autonomously\ncollected data is prediction: if a robot can learn to predict the future, it\ncan use this predictive model to take actions to produce desired outcomes, such\nas moving an object to a particular location. However, in complex open-world\nscenarios, designing a representation for prediction is difficult. In this\nwork, we instead aim to enable self-supervised robotic learning through direct\nvideo prediction: instead of attempting to design a good representation, we\ndirectly predict what the robot will see next, and then use this model to\nachieve desired goals. A key challenge in video prediction for robotic\nmanipulation is handling complex spatial arrangements such as occlusions. To\nthat end, we introduce a video prediction model that can keep track of objects\nthrough occlusion by incorporating temporal skip-connections. Together with a\nnovel planning criterion and action space formulation, we demonstrate that this\nmodel substantially outperforms prior work on video prediction-based control.\nOur results show manipulation of objects not seen during training, handling\nmultiple objects, and pushing objects around obstructions. These results\nrepresent a significant advance in the range and complexity of skills that can\nbe performed entirely with self-supervised robotic learning. \n\n"}
{"id": "1710.07307", "contents": "Title: Interpretable Transformations with Encoder-Decoder Networks Abstract: Deep feature spaces have the capacity to encode complex transformations of\ntheir input data. However, understanding the relative feature-space\nrelationship between two transformed encoded images is difficult. For instance,\nwhat is the relative feature space relationship between two rotated images?\nWhat is decoded when we interpolate in feature space? Ideally, we want to\ndisentangle confounding factors, such as pose, appearance, and illumination,\nfrom object identity. Disentangling these is difficult because they interact in\nvery nonlinear ways. We propose a simple method to construct a deep feature\nspace, with explicitly disentangled representations of several known\ntransformations. A person or algorithm can then manipulate the disentangled\nrepresentation, for example, to re-render an image with explicit control over\nparameterized degrees of freedom. The feature space is constructed using a\ntransforming encoder-decoder network with a custom feature transform layer,\nacting on the hidden representations. We demonstrate the advantages of explicit\ndisentangling on a variety of datasets and transformations, and as an aid for\ntraditional tasks, such as classification. \n\n"}
{"id": "1710.07346", "contents": "Title: Be Your Own Prada: Fashion Synthesis with Structural Coherence Abstract: We present a novel and effective approach for generating new clothing on a\nwearer through generative adversarial learning. Given an input image of a\nperson and a sentence describing a different outfit, our model \"redresses\" the\nperson as desired, while at the same time keeping the wearer and her/his pose\nunchanged. Generating new outfits with precise regions conforming to a language\ndescription while retaining wearer's body structure is a new challenging task.\nExisting generative adversarial networks are not ideal in ensuring global\ncoherence of structure given both the input photograph and language description\nas conditions. We address this challenge by decomposing the complex generative\nprocess into two conditional stages. In the first stage, we generate a\nplausible semantic segmentation map that obeys the wearer's pose as a latent\nspatial arrangement. An effective spatial constraint is formulated to guide the\ngeneration of this semantic segmentation map. In the second stage, a generative\nmodel with a newly proposed compositional mapping layer is used to render the\nfinal image with precise regions and textures conditioned on this map. We\nextended the DeepFashion dataset [8] by collecting sentence descriptions for\n79K images. We demonstrate the effectiveness of our approach through both\nquantitative and qualitative evaluations. A user study is also conducted. The\ncodes and the data are available at http://mmlab.ie.cuhk.\nedu.hk/projects/FashionGAN/. \n\n"}
{"id": "1710.07477", "contents": "Title: Anticipating Daily Intention using On-Wrist Motion Triggered Sensing Abstract: Anticipating human intention by observing one's actions has many\napplications. For instance, picking up a cellphone, then a charger (actions)\nimplies that one wants to charge the cellphone (intention). By anticipating the\nintention, an intelligent system can guide the user to the closest power\noutlet. We propose an on-wrist motion triggered sensing system for anticipating\ndaily intentions, where the on-wrist sensors help us to persistently observe\none's actions. The core of the system is a novel Recurrent Neural Network (RNN)\nand Policy Network (PN), where the RNN encodes visual and motion observation to\nanticipate intention, and the PN parsimoniously triggers the process of visual\nobservation to reduce computation requirement. We jointly trained the whole\nnetwork using policy gradient and cross-entropy loss. To evaluate, we collect\nthe first daily \"intention\" dataset consisting of 2379 videos with 34\nintentions and 164 unique action sequences. Our method achieves 92.68%, 90.85%,\n97.56% accuracy on three users while processing only 29% of the visual\nobservation on average. \n\n"}
{"id": "1710.08092", "contents": "Title: VGGFace2: A dataset for recognising faces across pose and age Abstract: In this paper, we introduce a new large-scale face dataset named VGGFace2.\nThe dataset contains 3.31 million images of 9131 subjects, with an average of\n362.6 images for each subject. Images are downloaded from Google Image Search\nand have large variations in pose, age, illumination, ethnicity and profession\n(e.g. actors, athletes, politicians). The dataset was collected with three\ngoals in mind: (i) to have both a large number of identities and also a large\nnumber of images for each identity; (ii) to cover a large range of pose, age\nand ethnicity; and (iii) to minimize the label noise. We describe how the\ndataset was collected, in particular the automated and manual filtering stages\nto ensure a high accuracy for the images of each identity. To assess face\nrecognition performance using the new dataset, we train ResNet-50 (with and\nwithout Squeeze-and-Excitation blocks) Convolutional Neural Networks on\nVGGFace2, on MS- Celeb-1M, and on their union, and show that training on\nVGGFace2 leads to improved recognition performance over pose and age. Finally,\nusing the models trained on these datasets, we demonstrate state-of-the-art\nperformance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A,\nIJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin.\nDatasets and models are publicly available. \n\n"}
{"id": "1710.08585", "contents": "Title: Max-Margin Invariant Features from Transformed Unlabeled Data Abstract: The study of representations invariant to common transformations of the data\nis important to learning. Most techniques have focused on local approximate\ninvariance implemented within expensive optimization frameworks lacking\nexplicit theoretical guarantees. In this paper, we study kernels that are\ninvariant to a unitary group while having theoretical guarantees in addressing\nthe important practical issue of unavailability of transformed versions of\nlabelled data. A problem we call the Unlabeled Transformation Problem which is\na special form of semi-supervised learning and one-shot learning. We present a\ntheoretically motivated alternate approach to the invariant kernel SVM based on\nwhich we propose Max-Margin Invariant Features (MMIF) to solve this problem. As\nan illustration, we design an framework for face recognition and demonstrate\nthe efficacy of our approach on a large scale semi-synthetic dataset with\n153,000 images and a new challenging protocol on Labelled Faces in the Wild\n(LFW) while out-performing strong baselines. \n\n"}
{"id": "1710.09338", "contents": "Title: Real-Time Automatic Fetal Brain Extraction in Fetal MRI by Deep Learning Abstract: Brain segmentation is a fundamental first step in neuroimage analysis. In the\ncase of fetal MRI, it is particularly challenging and important due to the\narbitrary orientation of the fetus, organs that surround the fetal head, and\nintermittent fetal motion. Several promising methods have been proposed but are\nlimited in their performance in challenging cases and in real-time\nsegmentation. We aimed to develop a fully automatic segmentation method that\nindependently segments sections of the fetal brain in 2D fetal MRI slices in\nreal-time. To this end, we developed and evaluated a deep fully convolutional\nneural network based on 2D U-net and autocontext, and compared it to two\nalternative fast methods based on 1) a voxelwise fully convolutional network\nand 2) a method based on SIFT features, random forest and conditional random\nfield. We trained the networks with manual brain masks on 250 stacks of\ntraining images, and tested on 17 stacks of normal fetal brain images as well\nas 18 stacks of extremely challenging cases based on extreme motion, noise, and\nseverely abnormal brain shape. Experimental results show that our U-net\napproach outperformed the other methods and achieved average Dice metrics of\n96.52% and 78.83% in the normal and challenging test sets, respectively. With\nan unprecedented performance and a test run time of about 1 second, our network\ncan be used to segment the fetal brain in real-time while fetal MRI slices are\nbeing acquired. This can enable real-time motion tracking, motion detection,\nand 3D reconstruction of fetal brain MRI. \n\n"}
{"id": "1710.09820", "contents": "Title: Spiking Optical Flow for Event-based Sensors Using IBM's TrueNorth\n  Neurosynaptic System Abstract: This paper describes a fully spike-based neural network for optical flow\nestimation from Dynamic Vision Sensor data. A low power embedded implementation\nof the method which combines the Asynchronous Time-based Image Sensor with\nIBM's TrueNorth Neurosynaptic System is presented. The sensor generates spikes\nwith sub-millisecond resolution in response to scene illumination changes.\nThese spike are processed by a spiking neural network running on TrueNorth with\na 1 millisecond resolution to accurately determine the order and time\ndifference of spikes from neighboring pixels, and therefore infer the velocity.\nThe spiking neural network is a variant of the Barlow Levick method for optical\nflow estimation. The system is evaluated on two recordings for which ground\ntruth motion is available, and achieves an Average Endpoint Error of 11% at an\nestimated power budget of under 80mW for the sensor and computation. \n\n"}
{"id": "1710.10003", "contents": "Title: Deterministic Approximate Methods for Maximum Consensus Robust Fitting Abstract: Maximum consensus estimation plays a critically important role in robust\nfitting problems in computer vision. Currently, the most prevalent algorithms\nfor consensus maximization draw from the class of randomized\nhypothesize-and-verify algorithms, which are cheap but can usually deliver only\nrough approximate solutions. On the other extreme, there are exact algorithms\nwhich are exhaustive search in nature and can be costly for practical-sized\ninputs. This paper fills the gap between the two extremes by proposing\ndeterministic algorithms to approximately optimize the maximum consensus\ncriterion. Our work begins by reformulating consensus maximization with linear\ncomplementarity constraints. Then, we develop two novel algorithms: one based\non non-smooth penalty method with a Frank-Wolfe style optimization scheme, the\nother based on the Alternating Direction Method of Multipliers (ADMM). Both\nalgorithms solve convex subproblems to efficiently perform the optimization. We\ndemonstrate the capability of our algorithms to greatly improve a rough initial\nestimate, such as those obtained using least squares or a randomized algorithm.\nCompared to the exact algorithms, our approach is much more practical on\nrealistic input sizes. Further, our approach is naturally applicable to\nestimation problems with geometric residuals \n\n"}
{"id": "1710.10589", "contents": "Title: Automatic Knee Osteoarthritis Diagnosis from Plain Radiographs: A Deep\n  Learning-Based Approach Abstract: Knee osteoarthritis (OA) is the most common musculoskeletal disorder. OA\ndiagnosis is currently conducted by assessing symptoms and evaluating plain\nradiographs, but this process suffers from subjectivity. In this study, we\npresent a new transparent computer-aided diagnosis method based on the Deep\nSiamese Convolutional Neural Network to automatically score knee OA severity\naccording to the Kellgren-Lawrence grading scale. We trained our method using\nthe data solely from the Multicenter Osteoarthritis Study and validated it on\nrandomly selected 3,000 subjects (5,960 knees) from Osteoarthritis Initiative\ndataset. Our method yielded a quadratic Kappa coefficient of 0.83 and average\nmulticlass accuracy of 66.71\\% compared to the annotations given by a committee\nof clinical experts. Here, we also report a radiological OA diagnosis area\nunder the ROC curve of 0.93. We also present attention maps -- given as a class\nprobability distribution -- highlighting the radiological features affecting\nthe network decision. This information makes the decision process transparent\nfor the practitioner, which builds better trust toward automatic methods. We\nbelieve that our model is useful for clinical decision making and for OA\nresearch; therefore, we openly release our training codes and the data set\ncreated in this study. \n\n"}
{"id": "1710.10695", "contents": "Title: Multilinear Class-Specific Discriminant Analysis Abstract: There has been a great effort to transfer linear discriminant techniques that\noperate on vector data to high-order data, generally referred to as Multilinear\nDiscriminant Analysis (MDA) techniques. Many existing works focus on maximizing\nthe inter-class variances to intra-class variances defined on tensor data\nrepresentations. However, there has not been any attempt to employ\nclass-specific discrimination criteria for the tensor data. In this paper, we\npropose a multilinear subspace learning technique suitable for applications\nrequiring class-specific tensor models. The method maximizes the discrimination\nof each individual class in the feature space while retains the spatial\nstructure of the input. We evaluate the efficiency of the proposed method on\ntwo problems, i.e. facial image analysis and stock price prediction based on\nlimit order book data. \n\n"}
{"id": "1711.00489", "contents": "Title: Don't Decay the Learning Rate, Increase the Batch Size Abstract: It is common practice to decay the learning rate. Here we show one can\nusually obtain the same learning curve on both training and test sets by\ninstead increasing the batch size during training. This procedure is successful\nfor stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum,\nand Adam. It reaches equivalent test accuracies after the same number of\ntraining epochs, but with fewer parameter updates, leading to greater\nparallelism and shorter training times. We can further reduce the number of\nparameter updates by increasing the learning rate $\\epsilon$ and scaling the\nbatch size $B \\propto \\epsilon$. Finally, one can increase the momentum\ncoefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly\nreduce the test accuracy. Crucially, our techniques allow us to repurpose\nexisting training schedules for large batch training with no hyper-parameter\ntuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under\n30 minutes. \n\n"}
{"id": "1711.01467", "contents": "Title: Attentional Pooling for Action Recognition Abstract: We introduce a simple yet surprisingly powerful model to incorporate\nattention in action recognition and human object interaction tasks. Our\nproposed attention module can be trained with or without extra supervision, and\ngives a sizable boost in accuracy while keeping the network size and\ncomputational cost nearly the same. It leads to significant improvements over\nstate of the art base architecture on three standard action recognition\nbenchmarks across still images and videos, and establishes new state of the art\non MPII dataset with 12.5% relative improvement. We also perform an extensive\nanalysis of our attention module both empirically and analytically. In terms of\nthe latter, we introduce a novel derivation of bottom-up and top-down attention\nas low-rank approximations of bilinear pooling methods (typically used for\nfine-grained classification). From this perspective, our attention formulation\nsuggests a novel characterization of action recognition as a fine-grained\nrecognition problem. \n\n"}
{"id": "1711.02245", "contents": "Title: Challenges in Disentangling Independent Factors of Variation Abstract: We study the problem of building models that disentangle independent factors\nof variation. Such models could be used to encode features that can efficiently\nbe used for classification and to transfer attributes between different images\nin image synthesis. As data we use a weakly labeled training set. Our weak\nlabels indicate what single factor has changed between two data samples,\nalthough the relative value of the change is unknown. This labeling is of\nparticular interest as it may be readily available without annotation costs. To\nmake use of weak labels we introduce an autoencoder model and train it through\nconstraints on image pairs and triplets. We formally prove that without\nadditional knowledge there is no guarantee that two images with the same factor\nof variation will be mapped to the same feature. We call this issue the\nreference ambiguity. Moreover, we show the role of the feature dimensionality\nand adversarial training. We demonstrate experimentally that the proposed model\ncan successfully transfer attributes on several datasets, but show also cases\nwhen the reference ambiguity occurs. \n\n"}
{"id": "1711.02816", "contents": "Title: Multi-label Image Recognition by Recurrently Discovering Attentional\n  Regions Abstract: This paper proposes a novel deep architecture to address multi-label image\nrecognition, a fundamental and practical task towards general visual\nunderstanding. Current solutions for this task usually rely on an extra step of\nextracting hypothesis regions (i.e., region proposals), resulting in redundant\ncomputation and sub-optimal performance. In this work, we achieve the\ninterpretable and contextualized multi-label image classification by developing\na recurrent memorized-attention module. This module consists of two alternately\nperformed components: i) a spatial transformer layer to locate attentional\nregions from the convolutional feature maps in a region-proposal-free way and\nii) an LSTM (Long-Short Term Memory) sub-network to sequentially predict\nsemantic labeling scores on the located regions while capturing the global\ndependencies of these regions. The LSTM also output the parameters for\ncomputing the spatial transformer. On large-scale benchmarks of multi-label\nimage classification (e.g., MS-COCO and PASCAL VOC 07), our approach\ndemonstrates superior performances over other existing state-of-the-arts in\nboth accuracy and efficiency. \n\n"}
{"id": "1711.03677", "contents": "Title: Egocentric Hand Detection Via Dynamic Region Growing Abstract: Egocentric videos, which mainly record the activities carried out by the\nusers of the wearable cameras, have drawn much research attentions in recent\nyears. Due to its lengthy content, a large number of ego-related applications\nhave been developed to abstract the captured videos. As the users are\naccustomed to interacting with the target objects using their own hands while\ntheir hands usually appear within their visual fields during the interaction,\nan egocentric hand detection step is involved in tasks like gesture\nrecognition, action recognition and social interaction understanding. In this\nwork, we propose a dynamic region growing approach for hand region detection in\negocentric videos, by jointly considering hand-related motion and egocentric\ncues. We first determine seed regions that most likely belong to the hand, by\nanalyzing the motion patterns across successive frames. The hand regions can\nthen be located by extending from the seed regions, according to the scores\ncomputed for the adjacent superpixels. These scores are derived from four\negocentric cues: contrast, location, position consistency and appearance\ncontinuity. We discuss how to apply the proposed method in real-life scenarios,\nwhere multiple hands irregularly appear and disappear from the videos.\nExperimental results on public datasets show that the proposed method achieves\nsuperior performance compared with the state-of-the-art methods, especially in\ncomplicated scenarios. \n\n"}
{"id": "1711.03678", "contents": "Title: Self-Supervised Intrinsic Image Decomposition Abstract: Intrinsic decomposition from a single image is a highly challenging task, due\nto its inherent ambiguity and the scarcity of training data. In contrast to\ntraditional fully supervised learning approaches, in this paper we propose\nlearning intrinsic image decomposition by explaining the input image. Our\nmodel, the Rendered Intrinsics Network (RIN), joins together an image\ndecomposition pipeline, which predicts reflectance, shape, and lighting\nconditions given a single image, with a recombination function, a learned\nshading model used to recompose the original input based off of intrinsic image\npredictions. Our network can then use unsupervised reconstruction error as an\nadditional signal to improve its intermediate representations. This allows\nlarge-scale unlabeled data to be useful during training, and also enables\ntransferring learned knowledge to images of unseen object categories, lighting\nconditions, and shapes. Extensive experiments demonstrate that our method\nperforms well on both intrinsic image decomposition and knowledge transfer. \n\n"}
{"id": "1711.04661", "contents": "Title: UCT: Learning Unified Convolutional Networks for Real-time Visual\n  Tracking Abstract: Convolutional neural networks (CNN) based tracking approaches have shown\nfavorable performance in recent benchmarks. Nonetheless, the chosen CNN\nfeatures are always pre-trained in different task and individual components in\ntracking systems are learned separately, thus the achieved tracking performance\nmay be suboptimal. Besides, most of these trackers are not designed towards\nreal-time applications because of their time-consuming feature extraction and\ncomplex optimization details.In this paper, we propose an end-to-end framework\nto learn the convolutional features and perform the tracking process\nsimultaneously, namely, a unified convolutional tracker (UCT). Specifically,\nThe UCT treats feature extractor and tracking process both as convolution\noperation and trains them jointly, enabling learned CNN features are tightly\ncoupled to tracking process. In online tracking, an efficient updating method\nis proposed by introducing peak-versus-noise ratio (PNR) criterion, and scale\nchanges are handled efficiently by incorporating a scale branch into network.\nThe proposed approach results in superior tracking performance, while\nmaintaining real-time speed. The standard UCT and UCT-Lite can track generic\nobjects at 41 FPS and 154 FPS without further optimization, respectively.\nExperiments are performed on four challenging benchmark tracking datasets:\nOTB2013, OTB2015, VOT2014 and VOT2015, and our method achieves state-of-the-art\nresults on these benchmarks compared with other real-time trackers. \n\n"}
{"id": "1711.05586", "contents": "Title: People, Penguins and Petri Dishes: Adapting Object Counting Models To\n  New Visual Domains And Object Types Without Forgetting Abstract: In this paper we propose a technique to adapt a convolutional neural network\n(CNN) based object counter to additional visual domains and object types while\nstill preserving the original counting function. Domain-specific normalisation\nand scaling operators are trained to allow the model to adjust to the\nstatistical distributions of the various visual domains. The developed\nadaptation technique is used to produce a singular patch-based counting\nregressor capable of counting various object types including people, vehicles,\ncell nuclei and wildlife. As part of this study a challenging new cell counting\ndataset in the context of tissue culture and patient diagnosis is constructed.\nThis new collection, referred to as the Dublin Cell Counting (DCC) dataset, is\nthe first of its kind to be made available to the wider computer vision\ncommunity. State-of-the-art object counting performance is achieved in both the\nShanghaitech (parts A and B) and Penguins datasets while competitive\nperformance is observed on the TRANCOS and Modified Bone Marrow (MBM) datasets,\nall using a shared counting model. \n\n"}
{"id": "1711.05792", "contents": "Title: Aggregated Wasserstein Metric and State Registration for Hidden Markov\n  Models Abstract: We propose a framework, named Aggregated Wasserstein, for computing a\ndissimilarity measure or distance between two Hidden Markov Models with state\nconditional distributions being Gaussian. For such HMMs, the marginal\ndistribution at any time position follows a Gaussian mixture distribution, a\nfact exploited to softly match, aka register, the states in two HMMs. We refer\nto such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of\nstates is inspired by the intrinsic relationship of optimal transport and the\nWasserstein metric between distributions. Specifically, the components of the\nmarginal GMMs are matched by solving an optimal transport problem where the\ncost between components is the Wasserstein metric for Gaussian distributions.\nThe solution of the optimization problem is a fast approximation to the\nWasserstein metric between two GMMs. The new Aggregated Wasserstein distance is\na semi-metric and can be computed without generating Monte Carlo samples. It is\ninvariant to relabeling or permutation of states. The distance is defined\nmeaningfully even for two HMMs that are estimated from data of different\ndimensionality, a situation that can arise due to missing variables. This\ndistance quantifies the dissimilarity of GMM-HMMs by measuring both the\ndifference between the two marginal GMMs and that between the two transition\nmatrices. Our new distance is tested on tasks of retrieval, classification, and\nt-SNE visualization of time series. Experiments on both synthetic and real data\nhave demonstrated its advantages in terms of accuracy as well as efficiency in\ncomparison with existing distances based on the Kullback-Leibler divergence. \n\n"}
{"id": "1711.05861", "contents": "Title: Modal Regression based Atomic Representation for Robust Face Recognition Abstract: Representation based classification (RC) methods such as sparse RC (SRC) have\nshown great potential in face recognition in recent years. Most previous RC\nmethods are based on the conventional regression models, such as lasso\nregression, ridge regression or group lasso regression. These regression models\nessentially impose a predefined assumption on the distribution of the noise\nvariable in the query sample, such as the Gaussian or Laplacian distribution.\nHowever, the complicated noises in practice may violate the assumptions and\nimpede the performance of these RC methods. In this paper, we propose a modal\nregression based atomic representation and classification (MRARC) framework to\nalleviate such limitation. Unlike previous RC methods, the MRARC framework does\nnot require the noise variable to follow any specific predefined distributions.\nThis gives rise to the capability of MRARC in handling various complex noises\nin reality. Using MRARC as a general platform, we also develop four novel RC\nmethods for unimodal and multimodal face recognition, respectively. In\naddition, we devise a general optimization algorithm for the unified MRARC\nframework based on the alternating direction method of multipliers (ADMM) and\nhalf-quadratic theory. The experiments on real-world data validate the efficacy\nof MRARC for robust face recognition. \n\n"}
{"id": "1711.05866", "contents": "Title: Fast and Efficient Calculations of Structural Invariants of Chirality Abstract: Chirality plays an important role in physics, chemistry, biology, and other\nfields. It describes an essential symmetry in structure. However, chirality\ninvariants are usually complicated in expression or difficult to evaluate. In\nthis paper, we present five general three-dimensional chirality invariants\nbased on the generating functions. And the five chiral invariants have four\ncharacteristics:(1) They play an important role in the detection of symmetry,\nespecially in the treatment of 'false zero' problem. (2) Three of the five\nchiral invariants decode an universal chirality index. (3) Three of them are\nproposed for the first time. (4) The five chiral invariants have low order no\nbigger than 4, brief expression, low time complexity O(n) and can act as\ndescriptors of three-dimensional objects in shape analysis. The five chiral\ninvariants give a geometric view to study the chiral invariants. And the\nexperiments show that the five chirality invariants are effective and\nefficient, they can be used as a tool for symmetry detection or features in\nshape analysis. \n\n"}
{"id": "1711.06025", "contents": "Title: Learning to Compare: Relation Network for Few-Shot Learning Abstract: We present a conceptually simple, flexible, and general framework for\nfew-shot learning, where a classifier must learn to recognise new classes given\nonly few examples from each. Our method, called the Relation Network (RN), is\ntrained end-to-end from scratch. During meta-learning, it learns to learn a\ndeep distance metric to compare a small number of images within episodes, each\nof which is designed to simulate the few-shot setting. Once trained, a RN is\nable to classify images of new classes by computing relation scores between\nquery images and the few examples of each new class without further updating\nthe network. Besides providing improved performance on few-shot learning, our\nframework is easily extended to zero-shot learning. Extensive experiments on\nfive benchmarks demonstrate that our simple approach provides a unified and\neffective approach for both of these two tasks. \n\n"}
{"id": "1711.06315", "contents": "Title: SparCE: Sparsity aware General Purpose Core Extensions to Accelerate\n  Deep Neural Networks Abstract: Deep Neural Networks (DNNs) have emerged as the method of choice for solving\na wide range of machine learning tasks. The enormous computational demands\nposed by DNNs have most commonly been addressed through the design of custom\naccelerators. However, these accelerators are prohibitive in many design\nscenarios (e.g., wearable devices and IoT sensors), due to stringent area/cost\nconstraints. Accelerating DNNs on these low-power systems, comprising of mainly\nthe general-purpose processor (GPP) cores, requires new approaches. We improve\nthe performance of DNNs on GPPs by exploiting a key attribute of DNNs, i.e.,\nsparsity. We propose Sparsity aware Core Extensions (SparCE)- a set of\nmicro-architectural and ISA extensions that leverage sparsity and are minimally\nintrusive and low-overhead. We dynamically detect zero operands and skip a set\nof future instructions that use it. Our design ensures that the instructions to\nbe skipped are prevented from even being fetched, as squashing instructions\ncomes with a penalty. SparCE consists of 2 key micro-architectural\nenhancements- a Sparsity Register File (SpRF) that tracks zero registers and a\nSparsity aware Skip Address (SASA) table that indicates instructions to be\nskipped. When an instruction is fetched, SparCE dynamically pre-identifies\nwhether the following instruction(s) can be skipped and appropriately modifies\nthe program counter, thereby skipping the redundant instructions and improving\nperformance. We model SparCE using the gem5 architectural simulator, and\nevaluate our approach on 6 image-recognition DNNs in the context of both\ntraining and inference using the Caffe framework. On a scalar microprocessor,\nSparCE achieves 19%-31% reduction in application-level. We also evaluate SparCE\non a 4-way SIMD ARMv8 processor using the OpenBLAS library, and demonstrate\nthat SparCE achieves 8%-15% reduction in the application-level execution time. \n\n"}
{"id": "1711.07155", "contents": "Title: Let Features Decide for Themselves: Feature Mask Network for Person\n  Re-identification Abstract: Person re-identification aims at establishing the identity of a pedestrian\nfrom a gallery that contains images of multiple people obtained from a\nmulti-camera system. Many challenges such as occlusions, drastic lighting and\npose variations across the camera views, indiscriminate visual appearances,\ncluttered backgrounds, imperfect detections, motion blur, and noise make this\ntask highly challenging. While most approaches focus on learning features and\nmetrics to derive better representations, we hypothesize that both local and\nglobal contextual cues are crucial for an accurate identity matching. To this\nend, we propose a Feature Mask Network (FMN) that takes advantage of ResNet\nhigh-level features to predict a feature map mask and then imposes it on the\nlow-level features to dynamically reweight different object parts for a locally\naware feature representation. This serves as an effective attention mechanism\nby allowing the network to focus on local details selectively. Given the\nresemblance of person re-identification with classification and retrieval\ntasks, we frame the network training as a multi-task objective optimization,\nwhich further improves the learned feature descriptions. We conduct experiments\non Market-1501, DukeMTMC-reID and CUHK03 datasets, where the proposed approach\nrespectively achieves significant improvements of $5.3\\%$, $9.1\\%$ and $10.7\\%$\nin mAP measure relative to the state-of-the-art. \n\n"}
{"id": "1711.07974", "contents": "Title: WAYLA - Generating Images from Eye Movements Abstract: We present a method for reconstructing images viewed by observers based only\non their eye movements. By exploring the relationships between gaze patterns\nand image stimuli, the \"What Are You Looking At?\" (WAYLA) system learns to\nsynthesize photo-realistic images that are similar to the original pictures\nbeing viewed. The WAYLA approach is based on the Conditional Generative\nAdversarial Network (Conditional GAN) image-to-image translation technique of\nIsola et al. We consider two specific applications - the first, of\nreconstructing newspaper images from gaze heat maps, and the second, of\ndetailed reconstruction of images containing only text. The newspaper image\nreconstruction process is divided into two image-to-image translation\noperations, the first mapping gaze heat maps into image segmentations, and the\nsecond mapping the generated segmentation into a newspaper image. We validate\nthe performance of our approach using various evaluation metrics, along with\nhuman visual inspection. All results confirm the ability of our network to\nperform image generation tasks using eye tracking data. \n\n"}
{"id": "1711.08006", "contents": "Title: Relating Input Concepts to Convolutional Neural Network Decisions Abstract: Many current methods to interpret convolutional neural networks (CNNs) use\nvisualization techniques and words to highlight concepts of the input seemingly\nrelevant to a CNN's decision. The methods hypothesize that the recognition of\nthese concepts are instrumental in the decision a CNN reaches, but the nature\nof this relationship has not been well explored. To address this gap, this\npaper examines the quality of a concept's recognition by a CNN and the degree\nto which the recognitions are associated with CNN decisions. The study\nconsiders a CNN trained for scene recognition over the ADE20k dataset. It uses\na novel approach to find and score the strength of minimally distributed\nrepresentations of input concepts (defined by objects in scene images) across\nlate stage feature maps. Subsequent analysis finds evidence that concept\nrecognition impacts decision making. Strong recognition of concepts\nfrequently-occurring in few scenes are indicative of correct decisions, but\nrecognizing concepts common to many scenes may mislead the network. \n\n"}
{"id": "1711.08040", "contents": "Title: Identifying Most Walkable Direction for Navigation in an Outdoor\n  Environment Abstract: We present an approach for identifying the most walkable direction for\nnavigation using a hand-held camera. Our approach extracts semantically rich\ncontextual information from the scene using a custom encoder-decoder\narchitecture for semantic segmentation and models the spatial and temporal\nbehavior of objects in the scene using a spatio-temporal graph. The system\nlearns to minimize a cost function over the spatial and temporal object\nattributes to identify the most walkable direction. We construct a new\nannotated navigation dataset collected using a hand-held mobile camera in an\nunconstrained outdoor environment, which includes challenging settings such as\nhighly dynamic scenes, occlusion between objects, and distortions. Our system\nachieves an accuracy of 84% on predicting a safe direction. We also show that\nour custom segmentation network is both fast and accurate, achieving mIOU (mean\nintersection over union) scores of 81 and 44.7 on the PASCAL VOC and the PASCAL\nContext datasets, respectively, while running at about 21 frames per second. \n\n"}
{"id": "1711.08195", "contents": "Title: On the Automatic Generation of Medical Imaging Reports Abstract: Medical imaging is widely used in clinical practice for diagnosis and\ntreatment. Report-writing can be error-prone for unexperienced physicians, and\ntime- consuming and tedious for experienced physicians. To address these\nissues, we study the automatic generation of medical imaging reports. This task\npresents several challenges. First, a complete report contains multiple\nheterogeneous forms of information, including findings and tags. Second,\nabnormal regions in medical images are difficult to identify. Third, the re-\nports are typically long, containing multiple sentences. To cope with these\nchallenges, we (1) build a multi-task learning framework which jointly performs\nthe pre- diction of tags and the generation of para- graphs, (2) propose a\nco-attention mechanism to localize regions containing abnormalities and\ngenerate narrations for them, (3) develop a hierarchical LSTM model to generate\nlong paragraphs. We demonstrate the effectiveness of the proposed methods on\ntwo publicly available datasets. \n\n"}
{"id": "1711.08488", "contents": "Title: Frustum PointNets for 3D Object Detection from RGB-D Data Abstract: In this work, we study 3D object detection from RGB-D data in both indoor and\noutdoor scenes. While previous methods focus on images or 3D voxels, often\nobscuring natural 3D patterns and invariances of 3D data, we directly operate\non raw point clouds by popping up RGB-D scans. However, a key challenge of this\napproach is how to efficiently localize objects in point clouds of large-scale\nscenes (region proposal). Instead of solely relying on 3D proposals, our method\nleverages both mature 2D object detectors and advanced 3D deep learning for\nobject localization, achieving efficiency as well as high recall for even small\nobjects. Benefited from learning directly in raw point clouds, our method is\nalso able to precisely estimate 3D bounding boxes even under strong occlusion\nor with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection\nbenchmarks, our method outperforms the state of the art by remarkable margins\nwhile having real-time capability. \n\n"}
{"id": "1711.08585", "contents": "Title: Exploiting temporal information for 3D pose estimation Abstract: In this work, we address the problem of 3D human pose estimation from a\nsequence of 2D human poses. Although the recent success of deep networks has\nled many state-of-the-art methods for 3D pose estimation to train deep networks\nend-to-end to predict from images directly, the top-performing approaches have\nshown the effectiveness of dividing the task of 3D pose estimation into two\nsteps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from\nimages and then mapping them into 3D space. They also showed that a\nlow-dimensional representation like 2D locations of a set of joints can be\ndiscriminative enough to estimate 3D pose with high accuracy. However,\nestimation of 3D pose for individual frames leads to temporally incoherent\nestimates due to independent error in each frame causing jitter. Therefore, in\nthis work we utilize the temporal information across a sequence of 2D joint\nlocations to estimate a sequence of 3D poses. We designed a\nsequence-to-sequence network composed of layer-normalized LSTM units with\nshortcut connections connecting the input to the output on the decoder side and\nimposed temporal smoothness constraint during training. We found that the\nknowledge of temporal consistency improves the best reported result on\nHuman3.6M dataset by approximately $12.2\\%$ and helps our network to recover\ntemporally consistent 3D poses over a sequence of images even when the 2D pose\ndetector fails. \n\n"}
{"id": "1711.08937", "contents": "Title: Deep High Dynamic Range Imaging with Large Foreground Motions Abstract: This paper proposes the first non-flow-based deep framework for high dynamic\nrange (HDR) imaging of dynamic scenes with large-scale foreground motions. In\nstate-of-the-art deep HDR imaging, input images are first aligned using optical\nflows before merging, which are still error-prone due to occlusion and large\nmotions. In stark contrast to flow-based methods, we formulate HDR imaging as\nan image translation problem without optical flows. Moreover, our simple\ntranslation network can automatically hallucinate plausible HDR details in the\npresence of total occlusion, saturation and under-exposure, which are otherwise\nalmost impossible to recover by conventional optimization approaches. Our\nframework can also be extended for different reference images. We performed\nextensive qualitative and quantitative comparisons to show that our approach\nproduces excellent results where color artifacts and geometric distortions are\nsignificantly reduced compared to existing state-of-the-art methods, and is\nrobust across various inputs, including images without radiometric calibration. \n\n"}
{"id": "1711.09594", "contents": "Title: FuCoLoT -- A Fully-Correlational Long-Term Tracker Abstract: We propose FuCoLoT -- a Fully Correlational Long-term Tracker. It exploits\nthe novel DCF constrained filter learning method to design a detector that is\nable to re-detect the target in the whole image efficiently. FuCoLoT maintains\nseveral correlation filters trained on different time scales that act as the\ndetector components. A novel mechanism based on the correlation response is\nused for tracking failure estimation. FuCoLoT achieves state-of-the-art results\non standard short-term benchmarks and it outperforms the current\nbest-performing tracker on the long-term UAV20L benchmark by over 19%. It has\nan order of magnitude smaller memory footprint than its best-performing\ncompetitors and runs at 15fps in a single CPU thread. \n\n"}
{"id": "1711.10098", "contents": "Title: Attentive Generative Adversarial Network for Raindrop Removal from a\n  Single Image Abstract: Raindrops adhered to a glass window or camera lens can severely hamper the\nvisibility of a background scene and degrade an image considerably. In this\npaper, we address the problem by visually removing raindrops, and thus\ntransforming a raindrop degraded image into a clean one. The problem is\nintractable, since first the regions occluded by raindrops are not given.\nSecond, the information about the background scene of the occluded regions is\ncompletely lost for most part. To resolve the problem, we apply an attentive\ngenerative network using adversarial training. Our main idea is to inject\nvisual attention into both the generative and discriminative networks. During\nthe training, our visual attention learns about raindrop regions and their\nsurroundings. Hence, by injecting this information, the generative network will\npay more attention to the raindrop regions and the surrounding structures, and\nthe discriminative network will be able to assess the local consistency of the\nrestored regions. This injection of visual attention to both generative and\ndiscriminative networks is the main contribution of this paper. Our experiments\nshow the effectiveness of our approach, which outperforms the state of the art\nmethods quantitatively and qualitatively. \n\n"}
{"id": "1711.10275", "contents": "Title: 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks Abstract: Convolutional networks are the de-facto standard for analyzing\nspatio-temporal data such as images, videos, and 3D shapes. Whilst some of this\ndata is naturally dense (e.g., photos), many other data sources are inherently\nsparse. Examples include 3D point clouds that were obtained using a LiDAR\nscanner or RGB-D camera. Standard \"dense\" implementations of convolutional\nnetworks are very inefficient when applied on such sparse data. We introduce\nnew sparse convolutional operations that are designed to process\nspatially-sparse data more efficiently, and use them to develop\nspatially-sparse convolutional networks. We demonstrate the strong performance\nof the resulting models, called submanifold sparse convolutional networks\n(SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In\nparticular, our models outperform all prior state-of-the-art on the test set of\na recent semantic segmentation competition. \n\n"}
{"id": "1711.10398", "contents": "Title: DOTA: A Large-scale Dataset for Object Detection in Aerial Images Abstract: Object detection is an important and challenging problem in computer vision.\nAlthough the past decade has witnessed major advances in object detection in\nnatural scenes, such successes have been slow to aerial imagery, not only\nbecause of the huge variation in the scale, orientation and shape of the object\ninstances on the earth's surface, but also due to the scarcity of\nwell-annotated datasets of objects in aerial scenes. To advance object\ndetection research in Earth Vision, also known as Earth Observation and Remote\nSensing, we introduce a large-scale Dataset for Object deTection in Aerial\nimages (DOTA). To this end, we collect $2806$ aerial images from different\nsensors and platforms. Each image is of the size about 4000-by-4000 pixels and\ncontains objects exhibiting a wide variety of scales, orientations, and shapes.\nThese DOTA images are then annotated by experts in aerial image interpretation\nusing $15$ common object categories. The fully annotated DOTA images contains\n$188,282$ instances, each of which is labeled by an arbitrary (8 d.o.f.)\nquadrilateral To build a baseline for object detection in Earth Vision, we\nevaluate state-of-the-art object detection algorithms on DOTA. Experiments\ndemonstrate that DOTA well represents real Earth Vision applications and are\nquite challenging. \n\n"}
{"id": "1711.10872", "contents": "Title: Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density\n  Network Abstract: Learning and predicting the pose parameters of a 3D hand model given an\nimage, such as locations of hand joints, is challenging due to large viewpoint\nchanges and articulations, and severe self-occlusions exhibited particularly in\negocentric views. Both feature learning and prediction modeling have been\ninvestigated to tackle the problem. Though effective, most existing\ndiscriminative methods yield a single deterministic estimation of target poses.\nDue to their single-value mapping intrinsic, they fail to adequately handle\nself-occlusion problems, where occluded joints present multiple modes. In this\npaper, we tackle the self-occlusion issue and provide a complete description of\nobserved poses given an input depth image by a novel method called hierarchical\nmixture density networks (HMDN). The proposed method leverages the\nstate-of-the-art hand pose estimators based on Convolutional Neural Networks to\nfacilitate feature learning, while it models the multiple modes in a two-level\nhierarchy to reconcile single-valued and multi-valued mapping in its output.\nThe whole framework with a mixture of two differentiable density functions is\nnaturally end-to-end trainable. In the experiments, HMDN produces interpretable\nand diverse candidate samples, and significantly outperforms the\nstate-of-the-art methods on two benchmarks with occlusions, and performs\ncomparably on another benchmark free of occlusions. \n\n"}
{"id": "1711.11152", "contents": "Title: Optical Flow Guided Feature: A Fast and Robust Motion Representation for\n  Video Action Recognition Abstract: Motion representation plays a vital role in human action recognition in\nvideos. In this study, we introduce a novel compact motion representation for\nvideo action recognition, named Optical Flow guided Feature (OFF), which\nenables the network to distill temporal information through a fast and robust\napproach. The OFF is derived from the definition of optical flow and is\northogonal to the optical flow. The derivation also provides theoretical\nsupport for using the difference between two frames. By directly calculating\npixel-wise spatiotemporal gradients of the deep feature maps, the OFF could be\nembedded in any existing CNN based video action recognition framework with only\na slight additional cost. It enables the CNN to extract spatiotemporal\ninformation, especially the temporal information between frames simultaneously.\nThis simple but powerful idea is validated by experimental results. The network\nwith OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on\nUCF-101, which is comparable with the result obtained by two streams (RGB and\noptical flow), but is 15 times faster in speed. Experimental results also show\nthat OFF is complementary to other motion modalities such as optical flow. When\nthe proposed method is plugged into the state-of-the-art video action\nrecognition framework, it has 96:0% and 74:2% accuracy on UCF-101 and HMDB-51\nrespectively. The code for this project is available at\nhttps://github.com/kevin-ssy/Optical-Flow-Guided-Feature. \n\n"}
{"id": "1711.11155", "contents": "Title: Predicting Depression Severity by Multi-Modal Feature Engineering and\n  Fusion Abstract: We present our preliminary work to determine if patient's vocal acoustic,\nlinguistic, and facial patterns could predict clinical ratings of depression\nseverity, namely Patient Health Questionnaire depression scale (PHQ-8). We\nproposed a multi modal fusion model that combines three different modalities:\naudio, video , and text features. By training over AVEC 2017 data set, our\nproposed model outperforms each single modality prediction model, and surpasses\nthe data set baseline with ice margin. \n\n"}
{"id": "1711.11200", "contents": "Title: Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care Abstract: This paper proposes a real-time embedded fall detection system using a\nDVS(Dynamic Vision Sensor) that has never been used for traditional fall\ndetection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal\nNetwork). The first contribution is building a DVS Falls Dataset, which made\nour network to recognize a much greater variety of falls than the existing\ndatasets that existed before and solved privacy issues using the DVS. Secondly,\nwe introduce the DVS-TN : optimized deep learning network to detect falls using\nDVS. Finally, we implemented a fall detection system which can run on\nlow-computing H/W with real-time, and tested on DVS Falls Dataset that takes\ninto account various falls situations. Our approach achieved 95.5% on the\nF1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board. \n\n"}
{"id": "1711.11249", "contents": "Title: ArbiText: Arbitrary-Oriented Text Detection in Unconstrained Scene Abstract: Arbitrary-oriented text detection in the wild is a very challenging task, due\nto the aspect ratio, scale, orientation, and illumination variations. In this\npaper, we propose a novel method, namely Arbitrary-oriented Text (or ArbText\nfor short) detector, for efficient text detection in unconstrained natural\nscene images. Specifically, we first adopt the circle anchors rather than the\nrectangular ones to represent bounding boxes, which is more robust to\norientation variations. Subsequently, we incorporate a pyramid pooling module\ninto the Single Shot MultiBox Detector framework, in order to simultaneously\nexplore the local and global visual information, which can, therefore, generate\nmore confidential detection results. Experiments on established scene-text\ndatasets, such as the ICDAR 2015 and MSRA-TD500 datasets, have demonstrated the\nsupe rior performance of the proposed method, compared to the state-of-the-art\napproaches. \n\n"}
{"id": "1712.00733", "contents": "Title: Incorporating External Knowledge to Answer Open-Domain Visual Questions\n  with Dynamic Memory Networks Abstract: Visual Question Answering (VQA) has attracted much attention since it offers\ninsight into the relationships between the multi-modal analysis of images and\nnatural language. Most of the current algorithms are incapable of answering\nopen-domain questions that require to perform reasoning beyond the image\ncontents. To address this issue, we propose a novel framework which endows the\nmodel capabilities in answering more complex questions by leveraging massive\nexternal knowledge with dynamic memory networks. Specifically, the questions\nalong with the corresponding images trigger a process to retrieve the relevant\ninformation in external knowledge bases, which are embedded into a continuous\nvector space by preserving the entity-relation structures. Afterwards, we\nemploy dynamic memory networks to attend to the large body of facts in the\nknowledge graph and images, and then perform reasoning over these facts to\ngenerate corresponding answers. Extensive experiments demonstrate that our\nmodel not only achieves the state-of-the-art performance in the visual question\nanswering task, but can also answer open-domain questions effectively by\nleveraging the external knowledge. \n\n"}
{"id": "1712.01034", "contents": "Title: Towards Faster Training of Global Covariance Pooling Networks by\n  Iterative Matrix Square Root Normalization Abstract: Global covariance pooling in convolutional neural networks has achieved\nimpressive improvement over the classical first-order pooling. Recent works\nhave shown matrix square root normalization plays a central role in achieving\nstate-of-the-art performance. However, existing methods depend heavily on\neigendecomposition (EIG) or singular value decomposition (SVD), suffering from\ninefficient training due to limited support of EIG and SVD on GPU. Towards\naddressing this problem, we propose an iterative matrix square root\nnormalization method for fast end-to-end training of global covariance pooling\nnetworks. At the core of our method is a meta-layer designed with loop-embedded\ndirected graph structure. The meta-layer consists of three consecutive\nnonlinear structured layers, which perform pre-normalization, coupled matrix\niteration and post-compensation, respectively. Our method is much faster than\nEIG or SVD based ones, since it involves only matrix multiplications, suitable\nfor parallel implementation on GPU. Moreover, the proposed network with ResNet\narchitecture can converge in much less epochs, further accelerating network\ntraining. On large-scale ImageNet, we achieve competitive performance superior\nto existing counterparts. By finetuning our models pre-trained on ImageNet, we\nestablish state-of-the-art results on three challenging fine-grained\nbenchmarks. The source code and network models will be available at\nhttp://www.peihuali.org/iSQRT-COV \n\n"}
{"id": "1712.01329", "contents": "Title: Examining Cooperation in Visual Dialog Models Abstract: In this work we propose a blackbox intervention method for visual dialog\nmodels, with the aim of assessing the contribution of individual linguistic or\nvisual components. Concretely, we conduct structured or randomized\ninterventions that aim to impair an individual component of the model, and\nobserve changes in task performance. We reproduce a state-of-the-art visual\ndialog model and demonstrate that our methodology yields surprising insights,\nnamely that both dialog and image information have minimal contributions to\ntask performance. The intervention method presented here can be applied as a\nsanity check for the strength and robustness of each component in visual dialog\nsystems. \n\n"}
{"id": "1712.01358", "contents": "Title: Long-Term Visual Object Tracking Benchmark Abstract: We propose a new long video dataset (called Track Long and Prosper - TLP) and\nbenchmark for single object tracking. The dataset consists of 50 HD videos from\nreal world scenarios, encompassing a duration of over 400 minutes (676K\nframes), making it more than 20 folds larger in average duration per sequence\nand more than 8 folds larger in terms of total covered duration, as compared to\nexisting generic datasets for visual tracking. The proposed dataset paves a way\nto suitably assess long term tracking performance and train better deep\nlearning architectures (avoiding/reducing augmentation, which may not reflect\nreal world behaviour). We benchmark the dataset on 17 state of the art trackers\nand rank them according to tracking accuracy and run time speeds. We further\npresent thorough qualitative and quantitative evaluation highlighting the\nimportance of long term aspect of tracking. Our most interesting observations\nare (a) existing short sequence benchmarks fail to bring out the inherent\ndifferences in tracking algorithms which widen up while tracking on long\nsequences and (b) the accuracy of trackers abruptly drops on challenging long\nsequences, suggesting the potential need of research efforts in the direction\nof long-term tracking. \n\n"}
{"id": "1712.01537", "contents": "Title: O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis Abstract: We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D\nshape analysis. Built upon the octree representation of 3D shapes, our method\ntakes the average normal vectors of a 3D model sampled in the finest leaf\noctants as input and performs 3D CNN operations on the octants occupied by the\n3D shape surface. We design a novel octree data structure to efficiently store\nthe octant information and CNN features into the graphics memory and execute\nthe entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN\nstructures and works for 3D shapes in different representations. By restraining\nthe computations on the octants occupied by 3D surfaces, the memory and\ncomputational costs of the O-CNN grow quadratically as the depth of the octree\nincreases, which makes the 3D CNN feasible for high-resolution 3D models. We\ncompare the performance of the O-CNN with other existing 3D CNN solutions and\ndemonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks,\nincluding object classification, shape retrieval, and shape segmentation. \n\n"}
{"id": "1712.01802", "contents": "Title: R-FCN-3000 at 30fps: Decoupling Detection and Classification Abstract: We present R-FCN-3000, a large-scale real-time object detector in which\nobjectness detection and classification are decoupled. To obtain the detection\nscore for an RoI, we multiply the objectness score with the fine-grained\nclassification score. Our approach is a modification of the R-FCN architecture\nin which position-sensitive filters are shared across different object classes\nfor performing localization. For fine-grained classification, these\nposition-sensitive filters are not needed. R-FCN-3000 obtains an mAP of 34.9%\non the ImageNet detection dataset and outperforms YOLO-9000 by 18% while\nprocessing 30 images per second. We also show that the objectness learned by\nR-FCN-3000 generalizes to novel classes and the performance increases with the\nnumber of training object classes - supporting the hypothesis that it is\npossible to learn a universal objectness detector. Code will be made available. \n\n"}
{"id": "1712.01996", "contents": "Title: An analysis of incorporating an external language model into a\n  sequence-to-sequence model Abstract: Attention-based sequence-to-sequence models for automatic speech recognition\njointly train an acoustic model, language model, and alignment mechanism. Thus,\nthe language model component is only trained on transcribed audio-text pairs.\nThis leads to the use of shallow fusion with an external language model at\ninference time. Shallow fusion refers to log-linear interpolation with a\nseparately trained language model at each step of the beam search. In this\nwork, we investigate the behavior of shallow fusion across a range of\nconditions: different types of language models, different decoding units, and\ndifferent tasks. On Google Voice Search, we demonstrate that the use of shallow\nfusion with a neural LM with wordpieces yields a 9.1% relative word error rate\nreduction (WERR) over our competitive attention-based sequence-to-sequence\nmodel, obviating the need for second-pass rescoring. \n\n"}
{"id": "1712.02050", "contents": "Title: Unsupervised Multi-Domain Image Translation with Domain-Specific\n  Encoders/Decoders Abstract: Unsupervised Image-to-Image Translation achieves spectacularly advanced\ndevelopments nowadays. However, recent approaches mainly focus on one model\nwith two domains, which may face heavy burdens with large cost of $O(n^2)$\ntraining time and model parameters, under such a requirement that $n$ domains\nare freely transferred to each other in a general setting. To address this\nproblem, we propose a novel and unified framework named Domain-Bank, which\nconsists of a global shared auto-encoder and $n$ domain-specific\nencoders/decoders, assuming that a universal shared-latent sapce can be\nprojected. Thus, we yield $O(n)$ complexity in model parameters along with a\nhuge reduction of the time budgets. Besides the high efficiency, we show the\ncomparable (or even better) image translation results over state-of-the-arts on\nvarious challenging unsupervised image translation tasks, including face image\ntranslation, fashion-clothes translation and painting style translation. We\nalso apply the proposed framework to domain adaptation and achieve\nstate-of-the-art performance on digit benchmark datasets. Further, thanks to\nthe explicit representation of the domain-specific decoders as well as the\nuniversal shared-latent space, it also enables us to conduct incremental\nlearning to add a new domain encoder/decoder. Linear combination of different\ndomains' representations is also obtained by fusing the corresponding decoders. \n\n"}
{"id": "1712.02066", "contents": "Title: Automatic Segmentation and Overall Survival Prediction in Gliomas using\n  Fully Convolutional Neural Network and Texture Analysis Abstract: In this paper, we use a fully convolutional neural network (FCNN) for the\nsegmentation of gliomas from Magnetic Resonance Images (MRI). A fully\nautomatic, voxel based classification was achieved by training a 23 layer deep\nFCNN on 2-D slices extracted from patient volumes. The network was trained on\nslices extracted from 130 patients and validated on 50 patients. For the task\nof survival prediction, texture and shape based features were extracted from T1\npost contrast volume to train an XGBoost regressor. On BraTS 2017 validation\nset, the proposed scheme achieved a mean whole tumor, tumor core and active\ndice score of 0.83, 0.69 and 0.69 respectively and an accuracy of 52% for the\noverall survival prediction. \n\n"}
{"id": "1712.02249", "contents": "Title: Online and Batch Supervised Background Estimation via L1 Regression Abstract: We propose a surprisingly simple model for supervised video background\nestimation. Our model is based on $\\ell_1$ regression. As existing methods for\n$\\ell_1$ regression do not scale to high-resolution videos, we propose several\nsimple and scalable methods for solving the problem, including iteratively\nreweighted least squares, a homotopy method, and stochastic gradient descent.\nWe show through extensive experiments that our model and methods match or\noutperform the state-of-the-art online and batch methods in virtually all\nquantitative and qualitative measures. \n\n"}
{"id": "1712.02781", "contents": "Title: On Usage of Autoencoders and Siamese Networks for Online Handwritten\n  Signature Verification Abstract: In this paper, we propose a novel writer-independent global feature\nextraction framework for the task of automatic signature verification which\naims to make robust systems for automatically distinguishing negative and\npositive samples. Our method consists of an autoencoder for modeling the sample\nspace into a fixed length latent space and a Siamese Network for classifying\nthe fixed-length samples obtained from the autoencoder based on the reference\nsamples of a subject as being \"Genuine\" or \"Forged.\" During our experiments,\nusage of Attention Mechanism and applying Downsampling significantly improved\nthe accuracy of the proposed framework. We evaluated our proposed framework\nusing SigWiComp2013 Japanese and GPDSsyntheticOnLineOffLineSignature datasets.\nOn the SigWiComp2013 Japanese dataset, we achieved 8.65% EER that means 1.2%\nrelative improvement compared to the best-reported result. Furthermore, on the\nGPDSsyntheticOnLineOffLineSignature dataset, we achieved average EERs of 0.13%,\n0.12%, 0.21% and 0.25% respectively for 150, 300, 1000 and 2000 test subjects\nwhich indicates improvement of relative EER on the best-reported result by\n95.67%, 95.26%, 92.9% and 91.52% respectively. Apart from the accuracy gain,\nbecause of the nature of our proposed framework which is based on neural\nnetworks and consequently is as simple as some consecutive matrix\nmultiplications, it has less computational cost than conventional methods such\nas DTW and could be used concurrently on devices such as GPU, TPU, etc. \n\n"}
{"id": "1712.02862", "contents": "Title: MoDL: Model Based Deep Learning Architecture for Inverse Problems Abstract: We introduce a model-based image reconstruction framework with a convolution\nneural network (CNN) based regularization prior. The proposed formulation\nprovides a systematic approach for deriving deep architectures for inverse\nproblems with the arbitrary structure. Since the forward model is explicitly\naccounted for, a smaller network with fewer parameters is sufficient to capture\nthe image information compared to black-box deep learning approaches, thus\nreducing the demand for training data and training time. Since we rely on\nend-to-end training, the CNN weights are customized to the forward model, thus\noffering improved performance over approaches that rely on pre-trained\ndenoisers. The main difference of the framework from existing end-to-end\ntraining strategies is the sharing of the network weights across iterations and\nchannels. Our experiments show that the decoupling of the number of iterations\nfrom the network complexity offered by this approach provides benefits\nincluding lower demand for training data, reduced risk of overfitting, and\nimplementations with significantly reduced memory footprint. We propose to\nenforce data-consistency by using numerical optimization blocks such as\nconjugate gradients algorithm within the network; this approach offers faster\nconvergence per iteration, compared to methods that rely on proximal gradients\nsteps to enforce data consistency. Our experiments show that the faster\nconvergence translates to improved performance, especially when the available\nGPU memory restricts the number of iterations. \n\n"}
{"id": "1712.03151", "contents": "Title: Combining Deep Universal Features, Semantic Attributes, and Hierarchical\n  Classification for Zero-Shot Learning Abstract: We address zero-shot (ZS) learning, building upon prior work in hierarchical\nclassification by combining it with approaches based on semantic attribute\nestimation. For both non-novel and novel image classes we compare multiple\nformulations of the problem, starting with deep universal features in each\ncase. We investigate the effect of using different posterior probabilities as\ninputs to the hierarchical classifier, comparing the performances of posteriors\nderived from distances to SVM classifier boundaries with those of posteriors\nbased on semantic attribute estimation. Using a dataset consisting of 150\nobject classes from the ImageNet ILSVRC2012 data set, we find that the\nhierarchical classification method that maximizes expected reward for non-novel\nclasses differs from the method that maximizes expected reward for novel\nclasses. We also show that using input posteriors based on semantic attributes\nimproves the expected reward for novel classes. \n\n"}
{"id": "1712.03257", "contents": "Title: Transformational Sparse Coding Abstract: A fundamental problem faced by object recognition systems is that objects and\ntheir features can appear in different locations, scales and orientations.\nCurrent deep learning methods attempt to achieve invariance to local\ntranslations via pooling, discarding the locations of features in the process.\nOther approaches explicitly learn transformed versions of the same feature,\nleading to representations that quickly explode in size. Instead of discarding\nthe rich and useful information about feature transformations to achieve\ninvariance, we argue that models should learn object features conjointly with\ntheir transformations to achieve equivariance. We propose a new model of\nunsupervised learning based on sparse coding that can learn object features\njointly with their affine transformations directly from images. Results based\non learning from natural images indicate that our approach matches the\nreconstruction quality of traditional sparse coding but with significantly\nfewer degrees of freedom while simultaneously learning transformations from\ndata. These results open the door to scaling up unsupervised learning to allow\ndeep feature+transformation learning in a manner consistent with the\nventral+dorsal stream architecture of the primate visual cortex. \n\n"}
{"id": "1712.03382", "contents": "Title: Visual aesthetic analysis using deep neural network: model and\n  techniques to increase accuracy without transfer learning Abstract: We train a deep Convolutional Neural Network (CNN) from scratch for visual\naesthetic analysis in images and discuss techniques we adopt to improve the\naccuracy. We avoid the prevalent best transfer learning approaches of using\npretrained weights to perform the task and train a model from scratch to get\naccuracy of 78.7% on AVA2 Dataset close to the best models available (85.6%).\nWe further show that accuracy increases to 81.48% on increasing the training\nset by incremental 10 percentile of entire AVA dataset showing our algorithm\ngets better with more data. \n\n"}
{"id": "1712.04926", "contents": "Title: Object Classification using Ensemble of Local and Deep Features Abstract: In this paper we propose an ensemble of local and deep features for object\nclassification. We also compare and contrast effectiveness of feature\nrepresentation capability of various layers of convolutional neural network. We\ndemonstrate with extensive experiments for object classification that the\nrepresentation capability of features from deep networks can be complemented\nwith information captured from local features. We also find out that features\nfrom various deep convolutional networks encode distinctive characteristic\ninformation. We establish that, as opposed to conventional practice,\nintermediate layers of deep networks can augment the classification\ncapabilities of features obtained from fully connected layers. \n\n"}
{"id": "1712.05695", "contents": "Title: Lightweight Neural Networks Abstract: Most of the weights in a Lightweight Neural Network have a value of zero,\nwhile the remaining ones are either +1 or -1. These universal approximators\nrequire approximately 1.1 bits/weight of storage, posses a quick forward pass\nand achieve classification accuracies similar to conventional continuous-weight\nnetworks. Their training regimen focuses on error reduction initially, but\nlater emphasizes discretization of weights. They ignore insignificant inputs,\nremove unnecessary weights, and drop unneeded hidden neurons. We have\nsuccessfully tested them on the MNIST, credit card fraud, and credit card\ndefaults data sets using networks having 2 to 16 hidden layers and up to 4.4\nmillion weights. \n\n"}
{"id": "1712.06682", "contents": "Title: Synthesizing Novel Pairs of Image and Text Abstract: Generating novel pairs of image and text is a problem that combines computer\nvision and natural language processing. In this paper, we present strategies\nfor generating novel image and caption pairs based on existing captioning\ndatasets. The model takes advantage of recent advances in generative\nadversarial networks and sequence-to-sequence modeling. We make generalizations\nto generate paired samples from multiple domains. Furthermore, we study cycles\n-- generating from image to text then back to image and vise versa, as well as\nits connection with autoencoders. \n\n"}
{"id": "1712.06914", "contents": "Title: Bipartite Graph Matching for Keyframe Summary Evaluation Abstract: A keyframe summary, or \"static storyboard\", is a collection of frames from a\nvideo designed to summarise its semantic content. Many algorithms have been\nproposed to extract such summaries automatically. How best to evaluate these\noutputs is an important but little-discussed question. We review the current\nmethods for matching frames between two summaries in the formalism of graph\ntheory. Our analysis revealed different behaviours of these methods, which we\nillustrate with a number of case studies. Based on the results, we recommend a\ngreedy matching algorithm due to Kannappan et al. \n\n"}
{"id": "1712.07168", "contents": "Title: Real-time deep hair matting on mobile devices Abstract: Augmented reality is an emerging technology in many application domains.\nAmong them is the beauty industry, where live virtual try-on of beauty products\nis of great importance. In this paper, we address the problem of live hair\ncolor augmentation. To achieve this goal, hair needs to be segmented quickly\nand accurately. We show how a modified MobileNet CNN architecture can be used\nto segment the hair in real-time. Instead of training this network using large\namounts of accurate segmentation data, which is difficult to obtain, we use\ncrowd sourced hair segmentation data. While such data is much simpler to\nobtain, the segmentations there are noisy and coarse. Despite this, we show how\nour system can produce accurate and fine-detailed hair mattes, while running at\nover 30 fps on an iPad Pro tablet. \n\n"}
{"id": "1712.07732", "contents": "Title: Enhance Visual Recognition under Adverse Conditions via Deep Networks Abstract: Visual recognition under adverse conditions is a very important and\nchallenging problem of high practical value, due to the ubiquitous existence of\nquality distortions during image acquisition, transmission, or storage. While\ndeep neural networks have been extensively exploited in the techniques of\nlow-quality image restoration and high-quality image recognition tasks\nrespectively, few studies have been done on the important problem of\nrecognition from very low-quality images. This paper proposes a deep learning\nbased framework for improving the performance of image and video recognition\nmodels under adverse conditions, using robust adverse pre-training or its\naggressive variant. The robust adverse pre-training algorithms leverage the\npower of pre-training and generalizes conventional unsupervised pre-training\nand data augmentation methods. We further develop a transfer learning approach\nto cope with real-world datasets of unknown adverse conditions. The proposed\nframework is comprehensively evaluated on a number of image and video\nrecognition benchmarks, and obtains significant performance improvements under\nvarious single or mixed adverse conditions. Our visualization and analysis\nfurther add to the explainability of results. \n\n"}
{"id": "1712.07805", "contents": "Title: Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning\n  Applications Abstract: This paper considers security risks buried in the data processing pipeline in\ncommon deep learning applications. Deep learning models usually assume a fixed\nscale for their training and input data. To allow deep learning applications to\nhandle a wide range of input data, popular frameworks, such as Caffe,\nTensorFlow, and Torch, all provide data scaling functions to resize input to\nthe dimensions used by deep learning models. Image scaling algorithms are\nintended to preserve the visual features of an image after scaling. However,\ncommon image scaling algorithms are not designed to handle human crafted\nimages. Attackers can make the scaling outputs look dramatically different from\nthe corresponding input images.\n  This paper presents a downscaling attack that targets the data scaling\nprocess in deep learning applications. By carefully crafting input data that\nmismatches with the dimension used by deep learning models, attackers can\ncreate deceiving effects. A deep learning application effectively consumes data\nthat are not the same as those presented to users. The visual inconsistency\nenables practical evasion and data poisoning attacks to deep learning\napplications. This paper presents proof-of-concept attack samples to popular\ndeep-learning-based image classification applications. To address the\ndownscaling attacks, the paper also suggests multiple potential mitigation\nstrategies. \n\n"}
{"id": "1712.08268", "contents": "Title: Beyond saliency: understanding convolutional neural networks from\n  saliency prediction on layer-wise relevance propagation Abstract: Despite the tremendous achievements of deep convolutional neural networks\n(CNNs) in many computer vision tasks, understanding how they actually work\nremains a significant challenge. In this paper, we propose a novel two-step\nunderstanding method, namely Salient Relevance (SR) map, which aims to shed\nlight on how deep CNNs recognize images and learn features from areas, referred\nto as attention areas, therein. Our proposed method starts out with a\nlayer-wise relevance propagation (LRP) step which estimates a pixel-wise\nrelevance map over the input image. Following, we construct a context-aware\nsaliency map, SR map, from the LRP-generated map which predicts areas close to\nthe foci of attention instead of isolated pixels that LRP reveals. In human\nvisual system, information of regions is more important than of pixels in\nrecognition. Consequently, our proposed approach closely simulates human\nrecognition. Experimental results using the ILSVRC2012 validation dataset in\nconjunction with two well-established deep CNN models, AlexNet and VGG-16,\nclearly demonstrate that our proposed approach concisely identifies not only\nkey pixels but also attention areas that contribute to the underlying neural\nnetwork's comprehension of the given images. As such, our proposed SR map\nconstitutes a convenient visual interface which unveils the visual attention of\nthe network and reveals which type of objects the model has learned to\nrecognize after training. The source code is available at\nhttps://github.com/Hey1Li/Salient-Relevance-Propagation. \n\n"}
{"id": "1712.08315", "contents": "Title: Deep Hashing with Category Mask for Fast Video Retrieval Abstract: This paper proposes an end-to-end deep hashing framework with category mask\nfor fast video retrieval. We train our network in a supervised way by fully\nexploiting inter-class diversity and intra-class identity. Classification loss\nis optimized to maximize inter-class diversity, while intra-pair is introduced\nto learn representative intra-class identity. We investigate the binary bits\ndistribution related to categories and find out that the effectiveness of\nbinary bits is highly correlated with data categories, and some bits may\ndegrade classification performance of some categories. We then design hash code\ngeneration scheme with category mask to filter out bits with negative\ncontribution. Experimental results demonstrate the proposed method outperforms\nseveral state-of-the-arts under various evaluation metrics on public datasets. \n\n"}
{"id": "1712.09216", "contents": "Title: Large-Scale 3D Scene Classification With Multi-View Volumetric CNN Abstract: We introduce a method to classify imagery using a convo- lutional neural\nnetwork (CNN) on multi-view image pro- jections. The power of our method comes\nfrom using pro- jections of multiple images at multiple depth planes near the\nreconstructed surface. This enables classification of categories whose salient\naspect is appearance change un- der different viewpoints, such as water, trees,\nand other materials with complex reflection/light response proper- ties. Our\nmethod does not require boundary labelling in images and works on pixel-level\nclassification with a small (few pixels) context, which simplifies the cre-\nation of a training set. We demonstrate this application on large-scale aerial\nimagery collections, and extend the per-pixel classification to robustly create\na consistent 2D classification which can be used to fill the gaps in non-\nreconstructible water regions. We also apply our method to classify tree\nregions. In both cases, the training data can quickly be generated using a\nsmall number of manually- created polygons on a map. We show that even with a\nvery simple and standard network our CNN outperforms the state-of-the-art image\nclassification, the Inception-V3 model retrained from a large collection of\naerial images. \n\n"}
{"id": "1801.01466", "contents": "Title: A Large Dataset for Improving Patch Matching Abstract: We propose a new dataset for learning local image descriptors which can be\nused for significantly improved patch matching. Our proposed dataset consists\nof an order of magnitude more number of scenes, images, and positive and\nnegative correspondences compared to the currently available Multi-View Stereo\n(MVS) dataset from Brown et al. The new dataset also has better coverage of the\noverall viewpoint, scale, and lighting changes in comparison to the MVS\ndataset. Our dataset also provides supplementary information like RGB patches\nwith scale and rotations values, and intrinsic and extrinsic camera parameters\nwhich as shown later can be used to customize training data as per application.\nWe train an existing state-of-the-art model on our dataset and evaluate on\npublicly available benchmarks such as HPatches dataset and Strecha et\nal.\\cite{strecha} to quantify the image descriptor performance. Experimental\nevaluations show that the descriptors trained using our proposed dataset\noutperform the current state-of-the-art descriptors trained on MVS by 8%, 4%\nand 10% on matching, verification and retrieval tasks respectively on the\nHPatches dataset. Similarly on the Strecha dataset, we see an improvement of\n3-5% for the matching task in non-planar scenes. \n\n"}
{"id": "1801.01632", "contents": "Title: VSE-ens: Visual-Semantic Embeddings with Efficient Negative Sampling Abstract: Jointing visual-semantic embeddings (VSE) have become a research hotpot for\nthe task of image annotation, which suffers from the issue of semantic gap,\ni.e., the gap between images' visual features (low-level) and labels' semantic\nfeatures (high-level). This issue will be even more challenging if visual\nfeatures cannot be retrieved from images, that is, when images are only denoted\nby numerical IDs as given in some real datasets. The typical way of existing\nVSE methods is to perform a uniform sampling method for negative examples that\nviolate the ranking order against positive examples, which requires a\ntime-consuming search in the whole label space. In this paper, we propose a\nfast adaptive negative sampler that can work well in the settings of no figure\npixels available. Our sampling strategy is to choose the negative examples that\nare most likely to meet the requirements of violation according to the latent\nfactors of images. In this way, our approach can linearly scale up to large\ndatasets. The experiments demonstrate that our approach converges 5.02x faster\nthan the state-of-the-art approaches on OpenImages, 2.5x on IAPR-TCI2 and 2.06x\non NUS-WIDE datasets, as well as better ranking accuracy across datasets. \n\n"}
{"id": "1801.02765", "contents": "Title: TextBoxes++: A Single-Shot Oriented Scene Text Detector Abstract: Scene text detection is an important step of scene text recognition system\nand also a challenging problem. Different from general object detection, the\nmain challenges of scene text detection lie on arbitrary orientations, small\nsizes, and significantly variant aspect ratios of text in natural images. In\nthis paper, we present an end-to-end trainable fast scene text detector, named\nTextBoxes++, which detects arbitrary-oriented scene text with both high\naccuracy and efficiency in a single network forward pass. No post-processing\nother than an efficient non-maximum suppression is involved. We have evaluated\nthe proposed TextBoxes++ on four public datasets. In all experiments,\nTextBoxes++ outperforms competing methods in terms of text localization\naccuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of\n0.817 at 11.6fps for 1024*1024 ICDAR 2015 Incidental text images, and an\nf-measure of 0.5591 at 19.8fps for 768*768 COCO-Text images. Furthermore,\ncombined with a text recognizer, TextBoxes++ significantly outperforms the\nstate-of-the-art approaches for word spotting and end-to-end text recognition\ntasks on popular benchmarks. Code is available at:\nhttps://github.com/MhLiao/TextBoxes_plusplus \n\n"}
{"id": "1801.03454", "contents": "Title: Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters\n  in Deep Neural Networks Abstract: In an effort to understand the meaning of the intermediate representations\ncaptured by deep networks, recent papers have tried to associate specific\nsemantic concepts to individual neural network filter responses, where\ninteresting correlations are often found, largely by focusing on extremal\nfilter responses. In this paper, we show that this approach can favor\neasy-to-interpret cases that are not necessarily representative of the average\nbehavior of a representation.\n  A more realistic but harder-to-study hypothesis is that semantic\nrepresentations are distributed, and thus filters must be studied in\nconjunction. In order to investigate this idea while enabling systematic\nvisualization and quantification of multiple filter responses, we introduce the\nNet2Vec framework, in which semantic concepts are mapped to vectorial\nembeddings based on corresponding filter responses. By studying such\nembeddings, we are able to show that 1., in most cases, multiple filters are\nrequired to code for a concept, that 2., often filters are not concept specific\nand help encode multiple concepts, and that 3., compared to single filter\nactivations, filter embeddings are able to better characterize the meaning of a\nrepresentation and its relationship to other concepts. \n\n"}
{"id": "1801.04260", "contents": "Title: Conditional Probability Models for Deep Image Compression Abstract: Deep Neural Networks trained as image auto-encoders have recently emerged as\na promising direction for advancing the state-of-the-art in image compression.\nThe key challenge in learning such networks is twofold: To deal with\nquantization, and to control the trade-off between reconstruction error\n(distortion) and entropy (rate) of the latent image representation. In this\npaper, we focus on the latter challenge and propose a new technique to navigate\nthe rate-distortion trade-off for an image compression auto-encoder. The main\nidea is to directly model the entropy of the latent representation by using a\ncontext model: A 3D-CNN which learns a conditional probability model of the\nlatent distribution of the auto-encoder. During training, the auto-encoder\nmakes use of the context model to estimate the entropy of its representation,\nand the context model is concurrently updated to learn the dependencies between\nthe symbols in the latent representation. Our experiments show that this\napproach, when measured in MS-SSIM, yields a state-of-the-art image compression\nsystem based on a simple convolutional auto-encoder. \n\n"}
{"id": "1801.04261", "contents": "Title: Deep saliency: What is learnt by a deep network about saliency? Abstract: Deep convolutional neural networks have achieved impressive performance on a\nbroad range of problems, beating prior art on established benchmarks, but it\noften remains unclear what are the representations learnt by those systems and\nhow they achieve such performance. This article examines the specific problem\nof saliency detection, where benchmarks are currently dominated by CNN-based\napproaches, and investigates the properties of the learnt representation by\nvisualizing the artificial neurons' receptive fields.\n  We demonstrate that fine tuning a pre-trained network on the saliency\ndetection task lead to a profound transformation of the network's deeper\nlayers. Moreover we argue that this transformation leads to the emergence of\nreceptive fields conceptually similar to the centre-surround filters\nhypothesized by early research on visual saliency. \n\n"}
{"id": "1801.05678", "contents": "Title: Face Recognition via Centralized Coordinate Learning Abstract: Owe to the rapid development of deep neural network (DNN) techniques and the\nemergence of large scale face databases, face recognition has achieved a great\nsuccess in recent years. During the training process of DNN, the face features\nand classification vectors to be learned will interact with each other, while\nthe distribution of face features will largely affect the convergence status of\nnetwork and the face similarity computing in test stage. In this work, we\nformulate jointly the learning of face features and classification vectors, and\npropose a simple yet effective centralized coordinate learning (CCL) method,\nwhich enforces the features to be dispersedly spanned in the coordinate space\nwhile ensuring the classification vectors to lie on a hypersphere. An adaptive\nangular margin is further proposed to enhance the discrimination capability of\nface features. Extensive experiments are conducted on six face benchmarks,\nincluding those have large age gap and hard negative samples. Trained only on\nthe small-scale CASIA Webface dataset with 460K face images from about 10K\nsubjects, our CCL model demonstrates high effectiveness and generality, showing\nconsistently competitive performance across all the six benchmark databases. \n\n"}
{"id": "1801.06104", "contents": "Title: Invariants of multidimensional time series based on their\n  iterated-integral signature Abstract: We introduce a novel class of features for multidimensional time series, that\nare invariant with respect to transformations of the ambient space. The general\nlinear group, the group of rotations and the group of permutations of the axes\nare considered. The starting point for their construction is Chen's\niterated-integral signature. \n\n"}
{"id": "1801.06523", "contents": "Title: How would surround vehicles move? A Unified Framework for Maneuver\n  Classification and Motion Prediction Abstract: Reliable prediction of surround vehicle motion is a critical requirement for\npath planning for autonomous vehicles. In this paper we propose a unified\nframework for surround vehicle maneuver classification and motion prediction\nthat exploits multiple cues, namely, the estimated motion of vehicles, an\nunderstanding of typical motion patterns of freeway traffic and inter-vehicle\ninteraction. We report our results in terms of maneuver classification accuracy\nand mean and median absolute error of predicted trajectories against the ground\ntruth for real traffic data collected using vehicle mounted sensors on\nfreeways. An ablative analysis is performed to analyze the relative importance\nof each cue for trajectory prediction. Additionally, an analysis of execution\ntime for the components of the framework is presented. Finally, we present\nmultiple case studies analyzing the outputs of our model for complex traffic\nscenarios \n\n"}
{"id": "1801.06940", "contents": "Title: MRI Cross-Modality NeuroImage-to-NeuroImage Translation Abstract: We present a cross-modality generation framework that learns to generate\ntranslated modalities from given modalities in MR images without real\nacquisition. Our proposed method performs NeuroImage-to-NeuroImage translation\n(abbreviated as N2N) by means of a deep learning model that leverages\nconditional generative adversarial networks (cGANs). Our framework jointly\nexploits the low-level features (pixel-wise information) and high-level\nrepresentations (e.g. brain tumors, brain structure like gray matter, etc.)\nbetween cross modalities which are important for resolving the challenging\ncomplexity in brain structures. Our framework can serve as an auxiliary method\nin clinical diagnosis and has great application potential. Based on our\nproposed framework, we first propose a method for cross-modality registration\nby fusing the deformation fields to adopt the cross-modality information from\ntranslated modalities. Second, we propose an approach for MRI segmentation,\ntranslated multichannel segmentation (TMS), where given modalities, along with\ntranslated modalities, are segmented by fully convolutional networks (FCN) in a\nmultichannel manner. Both of these two methods successfully adopt the\ncross-modality information to improve the performance without adding any extra\ndata. Experiments demonstrate that our proposed framework advances the\nstate-of-the-art on five brain MRI datasets. We also observe encouraging\nresults in cross-modality registration and segmentation on some widely adopted\nbrain datasets. Overall, our work can serve as an auxiliary method in clinical\ndiagnosis and be applied to various tasks in medical fields.\n  Keywords: image-to-image, cross-modality, registration, segmentation, brain\nMRI \n\n"}
{"id": "1801.07455", "contents": "Title: Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action\n  Recognition Abstract: Dynamics of human body skeletons convey significant information for human\naction recognition. Conventional approaches for modeling skeletons usually rely\non hand-crafted parts or traversal rules, thus resulting in limited expressive\npower and difficulties of generalization. In this work, we propose a novel\nmodel of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks\n(ST-GCN), which moves beyond the limitations of previous methods by\nautomatically learning both the spatial and temporal patterns from data. This\nformulation not only leads to greater expressive power but also stronger\ngeneralization capability. On two large datasets, Kinetics and NTU-RGBD, it\nachieves substantial improvements over mainstream methods. \n\n"}
{"id": "1801.07637", "contents": "Title: DeepGestalt - Identifying Rare Genetic Syndromes Using Deep Learning Abstract: Facial analysis technologies have recently measured up to the capabilities of\nexpert clinicians in syndrome identification. To date, these technologies could\nonly identify phenotypes of a few diseases, limiting their role in clinical\nsettings where hundreds of diagnoses must be considered.\n  We developed a facial analysis framework, DeepGestalt, using computer vision\nand deep learning algorithms, that quantifies similarities to hundreds of\ngenetic syndromes based on unconstrained 2D images. DeepGestalt is currently\ntrained with over 26,000 patient cases from a rapidly growing\nphenotype-genotype database, consisting of tens of thousands of validated\nclinical cases, curated through a community-driven platform. DeepGestalt\ncurrently achieves 91% top-10-accuracy in identifying over 215 different\ngenetic syndromes and has outperformed clinical experts in three separate\nexperiments.\n  We suggest that this form of artificial intelligence is ready to support\nmedical genetics in clinical and laboratory practices and will play a key role\nin the future of precision medicine. \n\n"}
{"id": "1801.07698", "contents": "Title: ArcFace: Additive Angular Margin Loss for Deep Face Recognition Abstract: Recently, a popular line of research in face recognition is adopting margins\nin the well-established softmax loss function to maximize class separability.\nIn this paper, we first introduce an Additive Angular Margin Loss (ArcFace),\nwhich not only has a clear geometric interpretation but also significantly\nenhances the discriminative power. Since ArcFace is susceptible to the massive\nlabel noise, we further propose sub-center ArcFace, in which each class\ncontains $K$ sub-centers and training samples only need to be close to any of\nthe $K$ positive sub-centers. Sub-center ArcFace encourages one dominant\nsub-class that contains the majority of clean faces and non-dominant\nsub-classes that include hard or noisy faces. Based on this self-propelled\nisolation, we boost the performance through automatically purifying raw web\nfaces under massive real-world noise. Besides discriminative feature embedding,\nwe also explore the inverse problem, mapping feature vectors to face images.\nWithout training any additional generator or discriminator, the pre-trained\nArcFace model can generate identity-preserved face images for both subjects\ninside and outside the training data only by using the network gradient and\nBatch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace\ncan enhance the discriminative feature embedding as well as strengthen the\ngenerative face synthesis. \n\n"}
{"id": "1801.08747", "contents": "Title: Weakly Supervised Object Detection with Pointwise Mutual Information Abstract: In this work a novel approach for weakly supervised object detection that\nincorporates pointwise mutual information is presented. A fully convolutional\nneural network architecture is applied in which the network learns one filter\nper object class. The resulting feature map indicates the location of objects\nin an image, yielding an intuitive representation of a class activation map.\nWhile traditionally such networks are learned by a softmax or binary logistic\nregression (sigmoid cross-entropy loss), a learning approach based on a cosine\nloss is introduced. A pointwise mutual information layer is incorporated in the\nnetwork in order to project predictions and ground truth presence labels in a\nnon-categorical embedding space. Thus, the cosine loss can be employed in this\nnon-categorical representation. Besides integrating image level annotations, it\nis shown how to integrate point-wise annotations using a Spatial Pyramid\nPooling layer. The approach is evaluated on the VOC2012 dataset for\nclassification, point localization and weakly supervised bounding box\nlocalization. It is shown that the combination of pointwise mutual information\nand a cosine loss eases the learning process and thus improves the accuracy.\nThe integration of coarse point-wise localizations further improves the results\nat minimal annotation costs. \n\n"}
{"id": "1801.09414", "contents": "Title: CosFace: Large Margin Cosine Loss for Deep Face Recognition Abstract: Face recognition has made extraordinary progress owing to the advancement of\ndeep convolutional neural networks (CNNs). The central task of face\nrecognition, including face verification and identification, involves face\nfeature discrimination. However, the traditional softmax loss of deep CNNs\nusually lacks the power of discrimination. To address this problem, recently\nseveral loss functions such as center loss, large margin softmax loss, and\nangular softmax loss have been proposed. All these improved losses share the\nsame idea: maximizing inter-class variance and minimizing intra-class variance.\nIn this paper, we propose a novel loss function, namely large margin cosine\nloss (LMCL), to realize this idea from a different perspective. More\nspecifically, we reformulate the softmax loss as a cosine loss by $L_2$\nnormalizing both features and weight vectors to remove radial variations, based\non which a cosine margin term is introduced to further maximize the decision\nmargin in the angular space. As a result, minimum intra-class variance and\nmaximum inter-class variance are achieved by virtue of normalization and cosine\ndecision margin maximization. We refer to our model trained with LMCL as\nCosFace. Extensive experimental evaluations are conducted on the most popular\npublic-domain face recognition datasets such as MegaFace Challenge, Youtube\nFaces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art\nperformance on these benchmarks, which confirms the effectiveness of our\nproposed approach. \n\n"}
{"id": "1801.09749", "contents": "Title: Deep Learning based Retinal OCT Segmentation Abstract: Our objective is to evaluate the efficacy of methods that use deep learning\n(DL) for the automatic fine-grained segmentation of optical coherence\ntomography (OCT) images of the retina. OCT images from 10 patients with mild\nnon-proliferative diabetic retinopathy were used from a public (U. of Miami)\ndataset. For each patient, five images were available: one image of the fovea\ncenter, two images of the perifovea, and two images of the parafovea. For each\nimage, two expert graders each manually annotated five retinal surfaces (i.e.\nboundaries between pairs of retinal layers). The first grader's annotations\nwere used as ground truth and the second grader's annotations to compute\ninter-operator agreement. The proposed automated approach segments images using\nfully convolutional networks (FCNs) together with Gaussian process (GP)-based\nregression as a post-processing step to improve the quality of the estimates.\nUsing 10-fold cross validation, the performance of the algorithms is determined\nby computing the per-pixel unsigned error (distance) between the automated\nestimates and the ground truth annotations generated by the first manual\ngrader. We compare the proposed method against five state of the art automatic\nsegmentation techniques. The results show that the proposed methods compare\nfavorably with state of the art techniques, resulting in the smallest mean\nunsigned error values and associated standard deviations, and performance is\ncomparable with human annotation of retinal layers from OCT when there is only\nmild retinopathy. The results suggest that semantic segmentation using FCNs,\ncoupled with regression-based post-processing, can effectively solve the OCT\nsegmentation problem on par with human capabilities with mild retinopathy. \n\n"}
{"id": "1801.10068", "contents": "Title: Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation:\n  the Benefit of Target Expectation Maximization Abstract: In this paper, we make two contributions to unsupervised domain adaptation\n(UDA) using the convolutional neural network (CNN). First, our approach\ntransfers knowledge in all the convolutional layers through attention\nalignment. Most previous methods align high-level representations, e.g.,\nactivations of the fully connected (FC) layers. In these methods, however, the\nconvolutional layers which underpin critical low-level domain knowledge cannot\nbe updated directly towards reducing domain discrepancy. Specifically, we\nassume that the discriminative regions in an image are relatively invariant to\nimage style changes. Based on this assumption, we propose an attention\nalignment scheme on all the target convolutional layers to uncover the\nknowledge shared by the source domain. Second, we estimate the posterior label\ndistribution of the unlabeled data for target network training. Previous\nmethods, which iteratively update the pseudo labels by the target network and\nrefine the target network by the updated pseudo labels, are vulnerable to label\nestimation errors. Instead, our approach uses category distribution to\ncalculate the cross-entropy loss for training, thereby ameliorating the error\naccumulation of the estimated labels. The two contributions allow our approach\nto outperform the state-of-the-art methods by +2.6% on the Office-31 dataset. \n\n"}
{"id": "1801.10111", "contents": "Title: Video-based Sign Language Recognition without Temporal Segmentation Abstract: Millions of hearing impaired people around the world routinely use some\nvariants of sign languages to communicate, thus the automatic translation of a\nsign language is meaningful and important. Currently, there are two\nsub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that\nrecognizes word by word and continuous SLR that translates entire sentences.\nExisting continuous SLR methods typically utilize isolated SLRs as building\nblocks, with an extra layer of preprocessing (temporal segmentation) and\nanother layer of post-processing (sentence synthesis). Unfortunately, temporal\nsegmentation itself is non-trivial and inevitably propagates errors into\nsubsequent steps. Worse still, isolated SLR methods typically require strenuous\nlabeling of each word separately in a sentence, severely limiting the amount of\nattainable training data. To address these challenges, we propose a novel\ncontinuous sign recognition framework, the Hierarchical Attention Network with\nLatent Space (LS-HAN), which eliminates the preprocessing of temporal\nsegmentation. The proposed LS-HAN consists of three components: a two-stream\nConvolutional Neural Network (CNN) for video feature representation generation,\na Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention\nNetwork (HAN) for latent space based recognition. Experiments are carried out\non two large scale datasets. Experimental results demonstrate the effectiveness\nof the proposed framework. \n\n"}
{"id": "1801.10597", "contents": "Title: Model compression for faster structural separation of macromolecules\n  captured by Cellular Electron Cryo-Tomography Abstract: Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule\nstructure inside single cells. Macromolecule classification approaches based on\nconvolutional neural networks (CNN) were developed to separate millions of\nmacromolecules captured from ECT systematically. However, given the fast\naccumulation of ECT data, it will soon become necessary to use CNN models to\nefficiently and accurately separate substantially more macromolecules at the\nprediction stage, which requires additional computational costs. To speed up\nthe prediction, we compress classification models into compact neural networks\nwith little in accuracy for deployment. Specifically, we propose to perform\nmodel compression through knowledge distillation. Firstly, a complex teacher\nnetwork is trained to generate soft labels with better classification\nfeasibility followed by training of customized student networks with simple\narchitectures using the soft label to compress model complexity. Our tests\ndemonstrate that our compressed models significantly reduce the number of\nparameters and time cost while maintaining similar classification accuracy. \n\n"}
{"id": "1802.00153", "contents": "Title: Semantic White Balance: Semantic Color Constancy Using Convolutional\n  Neural Network Abstract: The goal of computational color constancy is to preserve the perceptive\ncolors of objects under different lighting conditions by removing the effect of\ncolor casts caused by the scene's illumination. With the rapid development of\ndeep learning based techniques, significant progress has been made in image\nsemantic segmentation. In this work, we exploit the semantic information\ntogether with the color and spatial information of the input image in order to\nremove color casts. We train a convolutional neural network (CNN) model that\nlearns to estimate the illuminant color and gamma correction parameters based\non the semantic information of the given image. Experimental results show that\nfeeding the CNN with the semantic information leads to a significant\nimprovement in the results by reducing the error by more than 40%. \n\n"}
{"id": "1802.00176", "contents": "Title: Perceptual Compressive Sensing Abstract: Compressive sensing (CS) works to acquire measurements at sub-Nyquist rate\nand recover the scene images. Existing CS methods always recover the scene\nimages in pixel level. This causes the smoothness of recovered images and lack\nof structure information, especially at a low measurement rate. To overcome\nthis drawback, in this paper, we propose perceptual CS to obtain high-level\nstructured recovery. Our task no longer focuses on pixel level. Instead, we\nwork to make a better visual effect. In detail, we employ perceptual loss,\ndefined on feature level, to enhance the structure information of the recovered\nimages. Experiments show that our method achieves better visual results with\nstronger structure information than existing CS methods at the same measurement\nrate. \n\n"}
{"id": "1802.01237", "contents": "Title: Face Destylization Abstract: Numerous style transfer methods which produce artistic styles of portraits\nhave been proposed to date. However, the inverse problem of converting the\nstylized portraits back into realistic faces is yet to be investigated\nthoroughly. Reverting an artistic portrait to its original photo-realistic face\nimage has potential to facilitate human perception and identity analysis. In\nthis paper, we propose a novel Face Destylization Neural Network (FDNN) to\nrestore the latent photo-realistic faces from the stylized ones. We develop a\nStyle Removal Network composed of convolutional, fully-connected and\ndeconvolutional layers. The convolutional layers are designed to extract facial\ncomponents from stylized face images. Consecutively, the fully-connected layer\ntransfers the extracted feature maps of stylized images into the corresponding\nfeature maps of real faces and the deconvolutional layers generate real faces\nfrom the transferred feature maps. To enforce the destylized faces to be\nsimilar to authentic face images, we employ a discriminative network, which\nconsists of convolutional and fully connected layers. We demonstrate the\neffectiveness of our network by conducting experiments on an extensive set of\nsynthetic images. Furthermore, we illustrate our network can recover faces from\nstylized portraits and real paintings for which the stylized data was\nunavailable during the training phase. \n\n"}
{"id": "1802.01532", "contents": "Title: Real-time Prediction of Intermediate-Horizon Automotive Collision Risk Abstract: Advanced collision avoidance and driver hand-off systems can benefit from the\nability to accurately predict, in real time, the probability a vehicle will be\ninvolved in a collision within an intermediate horizon of 10 to 20 seconds. The\nrarity of collisions in real-world data poses a significant challenge to\ndeveloping this capability because, as we demonstrate empirically,\nintermediate-horizon risk prediction depends heavily on high-dimensional driver\nbehavioral features. As a result, a large amount of data is required to fit an\neffective predictive model. In this paper, we assess whether simulated data can\nhelp alleviate this issue. Focusing on highway driving, we present a three-step\napproach for generating data and fitting a predictive model capable of\nreal-time prediction. First, high-risk automotive scenes are generated using\nimportance sampling on a learned Bayesian network scene model. Second,\ncollision risk is estimated through Monte Carlo simulation. Third, a neural\nnetwork domain adaptation model is trained on real and simulated data to\naddress discrepancies between the two domains. Experiments indicate that\nsimulated data can mitigate issues resulting from collision rarity, thereby\nimproving risk prediction in real-world data. \n\n"}
{"id": "1802.01666", "contents": "Title: Adviser Networks: Learning What Question to Ask for Human-In-The-Loop\n  Viewpoint Estimation Abstract: Humans have an unparalleled visual intelligence and can overcome visual\nambiguities that machines currently cannot. Recent works have shown that\nincorporating guidance from humans during inference for monocular\nviewpoint-estimation can help overcome difficult cases in which the\ncomputer-alone would have otherwise failed. These hybrid intelligence\napproaches are hence gaining traction. However, deciding what question to ask\nthe human at inference time remains an unknown for these problems.\n  We address this question by formulating it as an Adviser Problem: can we\nlearn a mapping from the input to a specific question to ask the human to\nmaximize the expected positive impact to the overall task? We formulate a\nsolution to the adviser problem for viewpoint estimation using a deep network\nwhere the question asks for the location of a keypoint in the input image. We\nshow that by using the Adviser Network's recommendations, the model and the\nhuman outperforms the previous hybrid-intelligence state-of-the-art by 3.7%,\nand the computer-only state-of-the-art by 5.28% absolute. \n\n"}
{"id": "1802.02185", "contents": "Title: Smile detection in the wild based on transfer learning Abstract: Smile detection from unconstrained facial images is a specialized and\nchallenging problem. As one of the most informative expressions, smiles convey\nbasic underlying emotions, such as happiness and satisfaction, which lead to\nmultiple applications, e.g., human behavior analysis and interactive\ncontrolling. Compared to the size of databases for face recognition, far less\nlabeled data is available for training smile detection systems. To leverage the\nlarge amount of labeled data from face recognition datasets and to alleviate\noverfitting on smile detection, an efficient transfer learning-based smile\ndetection approach is proposed in this paper. Unlike previous works which use\neither hand-engineered features or train deep convolutional networks from\nscratch, a well-trained deep face recognition model is explored and fine-tuned\nfor smile detection in the wild. Three different models are built as a result\nof fine-tuning the face recognition model with different inputs, including\naligned, unaligned and grayscale images generated from the GENKI-4K dataset.\nExperiments show that the proposed approach achieves improved state-of-the-art\nperformance. Robustness of the model to noise and blur artifacts is also\nevaluated in this paper. \n\n"}
{"id": "1802.02204", "contents": "Title: SocialML: machine learning for social media video creators Abstract: In the recent years, social media have become one of the main places where\ncreative content is being published and consumed by billions of users. Contrary\nto traditional media, social media allow the publishers to receive almost\ninstantaneous feedback regarding their creative work at an unprecedented scale.\nThis is a perfect use case for machine learning methods that can use these\nmassive amounts of data to provide content creators with inspirational ideas\nand constructive criticism of their work. In this work, we present a\ncomprehensive overview of machine learning-empowered tools we developed for\nvideo creators at Group Nine Media - one of the major social media companies\nthat creates short-form videos with over three billion views per month. Our\nmain contribution is a set of tools that allow the creators to leverage massive\namounts of data to improve their creation process, evaluate their videos before\nthe publication and improve content quality. These applications include an\ninteractive conversational bot that allows access to material archives, a\nWeb-based application for automatic selection of optimal video thumbnail, as\nwell as deep learning methods for optimizing headline and predicting video\npopularity. Our A/B tests show that deployment of our tools leads to\nsignificant increase of average video view count by 12.9%. Our additional\ncontribution is a set of considerations collected during the deployment of\nthose tools that can hel \n\n"}
{"id": "1802.02568", "contents": "Title: VISER: Visual Self-Regularization Abstract: In this work, we propose the use of large set of unlabeled images as a source\nof regularization data for learning robust visual representation. Given a\nvisual model trained by a labeled dataset in a supervised fashion, we augment\nour training samples by incorporating large number of unlabeled data and train\na semi-supervised model. We demonstrate that our proposed learning approach\nleverages an abundance of unlabeled images and boosts the visual recognition\nperformance which alleviates the need to rely on large labeled datasets for\nlearning robust representation. To increment the number of image instances\nneeded to learn robust visual models in our approach, each labeled image\npropagates its label to its nearest unlabeled image instances. These retrieved\nunlabeled images serve as local perturbations of each labeled image to perform\nVisual Self-Regularization (VISER). To retrieve such visual self regularizers,\nwe compute the cosine similarity in a semantic space defined by the penultimate\nlayer in a fully convolutional neural network. We use the publicly available\nYahoo Flickr Creative Commons 100M dataset as the source of our unlabeled image\nset and propose a distributed approximate nearest neighbor algorithm to make\nretrieval practical at that scale. Using the labeled instances and their\nregularizer samples we show that we significantly improve object categorization\nand localization performance on the MS COCO and Visual Genome datasets where\nobjects appear in context. \n\n"}
{"id": "1802.02721", "contents": "Title: Deep Image Super Resolution via Natural Image Priors Abstract: Single image super-resolution (SR) via deep learning has recently gained\nsignificant attention in the literature. Convolutional neural networks (CNNs)\nare typically learned to represent the mapping between low-resolution (LR) and\nhigh-resolution (HR) images/patches with the help of training examples. Most\nexisting deep networks for SR produce high quality results when training data\nis abundant. However, their performance degrades sharply when training is\nlimited. We propose to regularize deep structures with prior knowledge about\nthe images so that they can capture more structural information from the same\nlimited data. In particular, we incorporate in a tractable fashion within the\nCNN framework, natural image priors which have shown to have much recent\nsuccess in imaging and vision inverse problems. Experimental results show that\nthe proposed deep network with natural image priors is particularly effective\nin training starved regimes. \n\n"}
{"id": "1802.02783", "contents": "Title: Saliency-Enhanced Robust Visual Tracking Abstract: Discrete correlation filter (DCF) based trackers have shown considerable\nsuccess in visual object tracking. These trackers often make use of low to mid\nlevel features such as histogram of gradients (HoG) and mid-layer activations\nfrom convolution neural networks (CNNs). We argue that including semantically\nhigher level information to the tracked features may provide further robustness\nto challenging cases such as viewpoint changes. Deep salient object detection\nis one example of such high level features, as it make use of semantic\ninformation to highlight the important regions in the given scene. In this\nwork, we propose an improvement over DCF based trackers by combining saliency\nbased and other features based filter responses. This combination is performed\nwith an adaptive weight on the saliency based filter responses, which is\nautomatically selected according to the temporal consistency of visual\nsaliency. We show that our method consistently improves a baseline DCF based\ntracker especially in challenging cases and performs superior to the\nstate-of-the-art. Our improved tracker operates at 9.3 fps, introducing a small\ncomputational burden over the baseline which operates at 11 fps. \n\n"}
{"id": "1802.02992", "contents": "Title: Texture Segmentation Based Video Compression Using Convolutional Neural\n  Networks Abstract: There has been a growing interest in using different approaches to improve\nthe coding efficiency of modern video codec in recent years as demand for\nweb-based video consumption increases. In this paper, we propose a model-based\napproach that uses texture analysis/synthesis to reconstruct blocks in texture\nregions of a video to achieve potential coding gains using the AV1 codec\ndeveloped by the Alliance for Open Media (AOM). The proposed method uses\nconvolutional neural networks to extract texture regions in a frame, which are\nthen reconstructed using a global motion model. Our preliminary results show an\nincrease in coding efficiency while maintaining satisfactory visual quality. \n\n"}
{"id": "1802.03254", "contents": "Title: Triplet-based Deep Similarity Learning for Person Re-Identification Abstract: In recent years, person re-identification (re-id) catches great attention in\nboth computer vision community and industry. In this paper, we propose a new\nframework for person re-identification with a triplet-based deep similarity\nlearning using convolutional neural networks (CNNs). The network is trained\nwith triplet input: two of them have the same class labels and the other one is\ndifferent. It aims to learn the deep feature representation, with which the\ndistance within the same class is decreased, while the distance between the\ndifferent classes is increased as much as possible. Moreover, we trained the\nmodel jointly on six different datasets, which differs from common practice -\none model is just trained on one dataset and tested also on the same one.\nHowever, the enormous number of possible triplet data among the large number of\ntraining samples makes the training impossible. To address this challenge, a\ndouble-sampling scheme is proposed to generate triplets of images as effective\nas possible. The proposed framework is evaluated on several benchmark datasets.\nThe experimental results show that, our method is effective for the task of\nperson re-identification and it is comparable or even outperforms the\nstate-of-the-art methods. \n\n"}
{"id": "1802.03268", "contents": "Title: Efficient Neural Architecture Search via Parameter Sharing Abstract: We propose Efficient Neural Architecture Search (ENAS), a fast and\ninexpensive approach for automatic model design. In ENAS, a controller learns\nto discover neural network architectures by searching for an optimal subgraph\nwithin a large computational graph. The controller is trained with policy\ngradient to select a subgraph that maximizes the expected reward on the\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\nbetween child models, ENAS is fast: it delivers strong empirical performances\nusing much fewer GPU-hours than all existing automatic model design approaches,\nand notably, 1000x less expensive than standard Neural Architecture Search. On\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\n(Zoph et al., 2018), whose test error is 2.65%. \n\n"}
{"id": "1802.03931", "contents": "Title: Deep feature compression for collaborative object detection Abstract: Recent studies have shown that the efficiency of deep neural networks in\nmobile applications can be significantly improved by distributing the\ncomputational workload between the mobile device and the cloud. This paradigm,\ntermed collaborative intelligence, involves communicating feature data between\nthe mobile and the cloud. The efficiency of such approach can be further\nimproved by lossy compression of feature data, which has not been examined to\ndate. In this work we focus on collaborative object detection and study the\nimpact of both near-lossless and lossy compression of feature data on its\naccuracy. We also propose a strategy for improving the accuracy under lossy\nfeature compression. Experiments indicate that using this strategy, the\ncommunication overhead can be reduced by up to 70% without sacrificing\naccuracy. \n\n"}
{"id": "1802.04034", "contents": "Title: Lipschitz-Margin Training: Scalable Certification of Perturbation\n  Invariance for Deep Neural Networks Abstract: High sensitivity of neural networks against malicious perturbations on inputs\ncauses security concerns. To take a steady step towards robust classifiers, we\naim to create neural network models provably defended from perturbations. Prior\ncertification work requires strong assumptions on network structures and\nmassive computational costs, and thus the range of their applications was\nlimited. From the relationship between the Lipschitz constants and prediction\nmargins, we present a computationally efficient calculation technique to\nlower-bound the size of adversarial perturbations that can deceive networks,\nand that is widely applicable to various complicated networks. Moreover, we\npropose an efficient training procedure that robustifies networks and\nsignificantly improves the provably guarded areas around data points. In\nexperimental evaluations, our method showed its ability to provide a\nnon-trivial guarantee and enhance robustness for even large networks. \n\n"}
{"id": "1802.05763", "contents": "Title: ASP:A Fast Adversarial Attack Example Generation Framework based on\n  Adversarial Saliency Prediction Abstract: With the excellent accuracy and feasibility, the Neural Networks have been\nwidely applied into the novel intelligent applications and systems. However,\nwith the appearance of the Adversarial Attack, the NN based system performance\nbecomes extremely vulnerable:the image classification results can be\narbitrarily misled by the adversarial examples, which are crafted images with\nhuman unperceivable pixel-level perturbation. As this raised a significant\nsystem security issue, we implemented a series of investigations on the\nadversarial attack in this work: We first identify an image's pixel\nvulnerability to the adversarial attack based on the adversarial saliency\nanalysis. By comparing the analyzed saliency map and the adversarial\nperturbation distribution, we proposed a new evaluation scheme to\ncomprehensively assess the adversarial attack precision and efficiency. Then,\nwith a novel adversarial saliency prediction method, a fast adversarial example\ngeneration framework, namely \"ASP\", is proposed with significant attack\nefficiency improvement and dramatic computation cost reduction. Compared to the\nprevious methods, experiments show that ASP has at most 12 times speed-up for\nadversarial example generation, 2 times lower perturbation rate, and high\nattack success rate of 87% on both MNIST and Cifar10. ASP can be also well\nutilized to support the data-hungry NN adversarial training. By reducing the\nattack success rate as much as 90%, ASP can quickly and effectively enhance the\ndefense capability of NN based system to the adversarial attacks. \n\n"}
{"id": "1802.06454", "contents": "Title: DA-GAN: Instance-level Image Translation by Deep Attention Generative\n  Adversarial Networks (with Supplementary Materials) Abstract: Unsupervised image translation, which aims in translating two independent\nsets of images, is challenging in discovering the correct correspondences\nwithout paired data. Existing works build upon Generative Adversarial Network\n(GAN) such that the distribution of the translated images are indistinguishable\nfrom the distribution of the target set. However, such set-level constraints\ncannot learn the instance-level correspondences (e.g. aligned semantic parts in\nobject configuration task). This limitation often results in false positives\n(e.g. geometric or semantic artifacts), and further leads to mode collapse\nproblem. To address the above issues, we propose a novel framework for\ninstance-level image translation by Deep Attention GAN (DA-GAN). Such a design\nenables DA-GAN to decompose the task of translating samples from two sets into\ntranslating instances in a highly-structured latent space. Specifically, we\njointly learn a deep attention encoder, and the instancelevel correspondences\ncould be consequently discovered through attending on the learned instance\npairs. Therefore, the constraints could be exploited on both set-level and\ninstance-level. Comparisons against several state-ofthe- arts demonstrate the\nsuperiority of our approach, and the broad application capability, e.g, pose\nmorphing, data augmentation, etc., pushes the margin of domain translation\nproblem. \n\n"}
{"id": "1802.06898", "contents": "Title: EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based\n  Cameras Abstract: Event-based cameras have shown great promise in a variety of situations where\nframe based cameras suffer, such as high speed motions and high dynamic range\nscenes. However, developing algorithms for event measurements requires a new\nclass of hand crafted algorithms. Deep learning has shown great success in\nproviding model free solutions to many problems in the vision community, but\nexisting networks have been developed with frame based images in mind, and\nthere does not exist the wealth of labeled data for events as there does for\nimages for supervised training. To these points, we present EV-FlowNet, a novel\nself-supervised deep learning pipeline for optical flow estimation for event\nbased cameras. In particular, we introduce an image based representation of a\ngiven event stream, which is fed into a self-supervised neural network as the\nsole input. The corresponding grayscale images captured from the same camera at\nthe same time as the events are then used as a supervisory signal to provide a\nloss function at training time, given the estimated flow from the network. We\nshow that the resulting network is able to accurately predict optical flow from\nevents only in a variety of different scenes, with performance competitive to\nimage based networks. This method not only allows for accurate estimation of\ndense optical flow, but also provides a framework for the transfer of other\nself-supervised methods to the event-based domain. \n\n"}
{"id": "1802.07303", "contents": "Title: MoNet: Moments Embedding Network Abstract: Bilinear pooling has been recently proposed as a feature encoding layer,\nwhich can be used after the convolutional layers of a deep network, to improve\nperformance in multiple vision tasks. Different from conventional global\naverage pooling or fully connected layer, bilinear pooling gathers 2nd order\ninformation in a translation invariant fashion. However, a serious drawback of\nthis family of pooling layers is their dimensionality explosion. Approximate\npooling methods with compact properties have been explored towards resolving\nthis weakness. Additionally, recent results have shown that significant\nperformance gains can be achieved by adding 1st order information and applying\nmatrix normalization to regularize unstable higher order information. However,\ncombining compact pooling with matrix normalization and other order information\nhas not been explored until now. In this paper, we unify bilinear pooling and\nthe global Gaussian embedding layers through the empirical moment matrix. In\naddition, we propose a novel sub-matrix square-root layer, which can be used to\nnormalize the output of the convolution layer directly and mitigate the\ndimensionality problem with off-the-shelf compact pooling methods. Our\nexperiments on three widely used fine-grained classification datasets\nillustrate that our proposed architecture, MoNet, can achieve similar or better\nperformance than with the state-of-art G2DeNet. Furthermore, when combined with\ncompact pooling technique, MoNet obtains comparable performance with encoded\nfeatures with 96% less dimensions. \n\n"}
{"id": "1802.07796", "contents": "Title: Continuous Relaxation of MAP Inference: A Nonconvex Perspective Abstract: In this paper, we study a nonconvex continuous relaxation of MAP inference in\ndiscrete Markov random fields (MRFs). We show that for arbitrary MRFs, this\nrelaxation is tight, and a discrete stationary point of it can be easily\nreached by a simple block coordinate descent algorithm. In addition, we study\nthe resolution of this relaxation using popular gradient methods, and further\npropose a more effective solution using a multilinear decomposition framework\nbased on the alternating direction method of multipliers (ADMM). Experiments on\nmany real-world problems demonstrate that the proposed ADMM significantly\noutperforms other nonconvex relaxation based methods, and compares favorably\nwith state of the art MRF optimization algorithms in different settings. \n\n"}
{"id": "1802.07856", "contents": "Title: xView: Objects in Context in Overhead Imagery Abstract: We introduce a new large-scale dataset for the advancement of object\ndetection techniques and overhead object detection research. This satellite\nimagery dataset enables research progress pertaining to four key computer\nvision frontiers. We utilize a novel process for geospatial category detection\nand bounding box annotation with three stages of quality control. Our data is\ncollected from WorldView-3 satellites at 0.3m ground sample distance, providing\nhigher resolution imagery than most public satellite imagery datasets. We\ncompare xView to other object detection datasets in both natural and overhead\nimagery domains and then provide a baseline analysis using the Single Shot\nMultiBox Detector. xView is one of the largest and most diverse publicly\navailable object-detection datasets to date, with over 1 million objects across\n60 classes in over 1,400 km^2 of imagery. \n\n"}
{"id": "1802.07918", "contents": "Title: Video Person Re-identification by Temporal Residual Learning Abstract: In this paper, we propose a novel feature learning framework for video person\nre-identification (re-ID). The proposed framework largely aims to exploit the\nadequate temporal information of video sequences and tackle the poor spatial\nalignment of moving pedestrians. More specifically, for exploiting the temporal\ninformation, we design a temporal residual learning (TRL) module to\nsimultaneously extract the generic and specific features of consecutive frames.\nThe TRL module is equipped with two bi-directional LSTM (BiLSTM), which are\nrespectively responsible to describe a moving person in different aspects,\nproviding complementary information for better feature representations. To deal\nwith the poor spatial alignment in video re-ID datasets, we propose a\nspatial-temporal transformer network (ST^2N) module. Transformation parameters\nin the ST^2N module are learned by leveraging the high-level semantic\ninformation of the current frame as well as the temporal context knowledge from\nother frames. The proposed ST^2N module with less learnable parameters allows\neffective person alignments under significant appearance changes. Extensive\nexperimental results on the large-scale MARS, PRID2011, ILIDS-VID and SDU-VID\ndatasets demonstrate that the proposed method achieves consistently superior\nperformance and outperforms most of the very recent state-of-the-art methods. \n\n"}
{"id": "1802.08077", "contents": "Title: Discriminative Label Consistent Domain Adaptation Abstract: Domain adaptation (DA) is transfer learning which aims to learn an effective\npredictor on target data from source data despite data distribution mismatch\nbetween source and target. We present in this paper a novel unsupervised DA\nmethod for cross-domain visual recognition which simultaneously optimizes the\nthree terms of a theoretically established error bound. Specifically, the\nproposed DA method iteratively searches a latent shared feature subspace where\nnot only the divergence of data distributions between the source domain and the\ntarget domain is decreased as most state-of-the-art DA methods do, but also the\ninter-class distances are increased to facilitate discriminative learning.\nMoreover, the proposed DA method sparsely regresses class labels from the\nfeatures achieved in the shared subspace while minimizing the prediction errors\non the source data and ensuring label consistency between source and target.\nData outliers are also accounted for to further avoid negative knowledge\ntransfer. Comprehensive experiments and in-depth analysis verify the\neffectiveness of the proposed DA method which consistently outperforms the\nstate-of-the-art DA methods on standard DA benchmarks, i.e., 12 cross-domain\nimage classification tasks. \n\n"}
{"id": "1802.08936", "contents": "Title: A Dataset To Evaluate The Representations Learned By Video Prediction\n  Models Abstract: We present a parameterized synthetic dataset called Moving Symbols to support\nthe objective study of video prediction networks. Using several instantiations\nof the dataset in which variation is explicitly controlled, we highlight issues\nin an existing state-of-the-art approach and propose the use of a performance\nmetric with greater semantic meaning to improve experimental interpretability.\nOur dataset provides canonical test cases that will help the community better\nunderstand, and eventually improve, the representations learned by such\nnetworks in the future. Code is available at\nhttps://github.com/rszeto/moving-symbols . \n\n"}
{"id": "1802.09058", "contents": "Title: Seeing Small Faces from Robust Anchor's Perspective Abstract: This paper introduces a novel anchor design to support anchor-based face\ndetection for superior scale-invariant performance, especially on tiny faces.\nTo achieve this, we explicitly address the problem that anchor-based detectors\ndrop performance drastically on faces with tiny sizes, e.g. less than 16x16\npixels. In this paper, we investigate why this is the case. We discover that\ncurrent anchor design cannot guarantee high overlaps between tiny faces and\nanchor boxes, which increases the difficulty of training. The new Expected Max\nOverlapping (EMO) score is proposed which can theoretically explain the low\noverlapping issue and inspire several effective strategies of new anchor design\nleading to higher face overlaps, including anchor stride reduction with new\nnetwork architectures, extra shifted anchors, and stochastic face shifting.\nComprehensive experiments show that our proposed method significantly\noutperforms the baseline anchor-based detector, while consistently achieving\nstate-of-the-art results on challenging face detection datasets with\ncompetitive runtime speed. \n\n"}
{"id": "1802.09153", "contents": "Title: PBGen: Partial Binarization of Deconvolution-Based Generators for Edge\n  Intelligence Abstract: This work explores the binarization of the deconvolution-based generator in a\nGAN for memory saving and speedup of image construction. Our study suggests\nthat different from convolutional neural networks (including the discriminator)\nwhere all layers can be binarized, only some of the layers in the generator can\nbe binarized without significant performance loss. Supported by theoretical\nanalysis and verified by experiments, a direct metric based on the dimension of\ndeconvolution operations is established, which can be used to quickly decide\nwhich layers in the generator can be binarized. Our results also indicate that\nboth the generator and the discriminator should be binarized simultaneously for\nbalanced competition and better performance. Experimental results based on\nCelebA suggest that directly applying state-of-the-art binarization techniques\nto all the layers of the generator will lead to 2.83$\\times$ performance loss\nmeasured by sliced Wasserstein distance compared with the original generator,\nwhile applying them to selected layers only can yield up to 25.81$\\times$\nsaving in memory consumption, and 1.96$\\times$ and 1.32$\\times$ speedup in\ninference and training respectively with little performance loss. \n\n"}
{"id": "1802.09941", "contents": "Title: Demystifying Parallel and Distributed Deep Learning: An In-Depth\n  Concurrency Analysis Abstract: Deep Neural Networks (DNNs) are becoming an important tool in modern\ncomputing applications. Accelerating their training is a major challenge and\ntechniques range from distributed algorithms to low-level circuit design. In\nthis survey, we describe the problem from a theoretical perspective, followed\nby approaches for its parallelization. We present trends in DNN architectures\nand the resulting implications on parallelization strategies. We then review\nand model the different types of concurrency in DNNs: from the single operator,\nthrough parallelism in network inference and training, to distributed deep\nlearning. We discuss asynchronous stochastic optimization, distributed system\narchitectures, communication schemes, and neural architecture search. Based on\nthose approaches, we extrapolate potential directions for parallelism in deep\nlearning. \n\n"}
{"id": "1802.09987", "contents": "Title: Multi-View Silhouette and Depth Decomposition for High Resolution 3D\n  Object Representation Abstract: We consider the problem of scaling deep generative shape models to\nhigh-resolution. Drawing motivation from the canonical view representation of\nobjects, we introduce a novel method for the fast up-sampling of 3D objects in\nvoxel space through networks that perform super-resolution on the six\northographic depth projections. This allows us to generate high-resolution\nobjects with more efficient scaling than methods which work directly in 3D. We\ndecompose the problem of 2D depth super-resolution into silhouette and depth\nprediction to capture both structure and fine detail. This allows our method to\ngenerate sharp edges more easily than an individual network. We evaluate our\nwork on multiple experiments concerning high-resolution 3D objects, and show\nour system is capable of accurately predicting novel objects at resolutions as\nlarge as 512$\\mathbf{\\times}$512$\\mathbf{\\times}$512 -- the highest resolution\nreported for this task. We achieve state-of-the-art performance on 3D object\nreconstruction from RGB images on the ShapeNet dataset, and further demonstrate\nthe first effective 3D super-resolution method. \n\n"}
{"id": "1802.09990", "contents": "Title: Deep Learning Architectures for Face Recognition in Video Surveillance Abstract: Face recognition (FR) systems for video surveillance (VS) applications\nattempt to accurately detect the presence of target individuals over a\ndistributed network of cameras. In video-based FR systems, facial models of\ntarget individuals are designed a priori during enrollment using a limited\nnumber of reference still images or video data. These facial models are not\ntypically representative of faces being observed during operations due to large\nvariations in illumination, pose, scale, occlusion, blur, and to camera\ninter-operability. Specifically, in still-to-video FR application, a single\nhigh-quality reference still image captured with still camera under controlled\nconditions is employed to generate a facial model to be matched later against\nlower-quality faces captured with video cameras under uncontrolled conditions.\nCurrent video-based FR systems can perform well on controlled scenarios, while\ntheir performance is not satisfactory in uncontrolled scenarios mainly because\nof the differences between the source (enrollment) and the target (operational)\ndomains. Most of the efforts in this area have been toward the design of robust\nvideo-based FR systems in unconstrained surveillance environments. This chapter\npresents an overview of recent advances in still-to-video FR scenario through\ndeep convolutional neural networks (CNNs). In particular, deep learning\narchitectures proposed in the literature based on triplet-loss function (e.g.,\ncross-correlation matching CNN, trunk-branch ensemble CNN and HaarNet) and\nsupervised autoencoders (e.g., canonical face representation CNN) are reviewed\nand compared in terms of accuracy and computational complexity. \n\n"}
{"id": "1803.00197", "contents": "Title: Temporally Identity-Aware SSD with Attentional LSTM Abstract: Temporal object detection has attracted significant attention, but most\npopular detection methods cannot leverage rich temporal information in videos.\nVery recently, many algorithms have been developed for video detection task,\nyet very few approaches can achieve \\emph{real-time online} object detection in\nvideos. In this paper, based on attention mechanism and convolutional long\nshort-term memory (ConvLSTM), we propose a temporal single-shot detector (TSSD)\nfor real-world detection. Distinct from previous methods, we take aim at\ntemporally integrating pyramidal feature hierarchy using ConvLSTM, and design a\nnovel structure including a low-level temporal unit as well as a high-level one\n(LH-TU) for multi-scale feature maps. Moreover, we develop a creative temporal\nanalysis unit, namely, attentional ConvLSTM (AC-LSTM), in which a temporal\nattention mechanism is specially tailored for background suppression and scale\nsuppression while a ConvLSTM integrates attention-aware features across time.\nAn association loss and a multi-step training are designed for temporal\ncoherence. Besides, an online tubelet analysis (OTA) is exploited for\nidentification. Our framework is evaluated on ImageNet VID dataset and 2DMOT15\ndataset. Extensive comparisons on the detection and tracking capability\nvalidate the superiority of the proposed approach. Consequently, the developed\nTSSD-OTA achieves a fast speed and an overall competitive performance in terms\nof detection and tracking. Finally, a real-world maneuver is conducted for\nunderwater object grasping. The source code is publicly available at\nhttps://github.com/SeanChenxy/TSSD-OTA. \n\n"}
{"id": "1803.00386", "contents": "Title: Context-Aware Learning using Transferable Features for Classification of\n  Breast Cancer Histology Images Abstract: Convolutional neural networks (CNNs) have been recently used for a variety of\nhistology image analysis. However, availability of a large dataset is a major\nprerequisite for training a CNN which limits its use by the computational\npathology community. In previous studies, CNNs have demonstrated their\npotential in terms of feature generalizability and transferability accompanied\nwith better performance. Considering these traits of CNN, we propose a simple\nyet effective method which leverages the strengths of CNN combined with the\nadvantages of including contextual information, particularly designed for a\nsmall dataset. Our method consists of two main steps: first it uses the\nactivation features of CNN trained for a patch-based classification and then it\ntrains a separate classifier using features of overlapping patches to perform\nimage-based classification using the contextual information. The proposed\nframework outperformed the state-of-the-art method for breast cancer\nclassification. \n\n"}
{"id": "1803.00401", "contents": "Title: Unravelling Robustness of Deep Learning based Face Recognition Against\n  Adversarial Attacks Abstract: Deep neural network (DNN) architecture based models have high expressive\npower and learning capacity. However, they are essentially a black box method\nsince it is not easy to mathematically formulate the functions that are learned\nwithin its many layers of representation. Realizing this, many researchers have\nstarted to design methods to exploit the drawbacks of deep learning based\nalgorithms questioning their robustness and exposing their singularities. In\nthis paper, we attempt to unravel three aspects related to the robustness of\nDNNs for face recognition: (i) assessing the impact of deep architectures for\nface recognition in terms of vulnerabilities to attacks inspired by commonly\nobserved distortions in the real world that are well handled by shallow\nlearning methods along with learning based adversaries; (ii) detecting the\nsingularities by characterizing abnormal filter response behavior in the hidden\nlayers of deep networks; and (iii) making corrections to the processing\npipeline to alleviate the problem. Our experimental evaluation using multiple\nopen-source DNN-based face recognition networks, including OpenFace and\nVGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates\nthat the performance of deep learning based face recognition algorithms can\nsuffer greatly in the presence of such distortions. The proposed method is also\ncompared with existing detection algorithms and the results show that it is\nable to detect the attacks with very high accuracy by suitably designing a\nclassifier using the response of the hidden layers in the network. Finally, we\npresent several effective countermeasures to mitigate the impact of adversarial\nattacks and improve the overall robustness of DNN-based face recognition. \n\n"}
{"id": "1803.00758", "contents": "Title: Driving Digital Rock towards Machine Learning: predicting permeability\n  with Gradient Boosting and Deep Neural Networks Abstract: We present a research study aimed at testing of applicability of machine\nlearning techniques for prediction of permeability of digitized rock samples.\nWe prepare a training set containing 3D images of sandstone samples imaged with\nX-ray microtomography and corresponding permeability values simulated with Pore\nNetwork approach. We also use Minkowski functionals and Deep Learning-based\ndescriptors of 3D images and 2D slices as input features for predictive model\ntraining and prediction. We compare predictive power of various feature sets\nand methods. The later include Gradient Boosting and various architectures of\nDeep Neural Networks (DNN). The results demonstrate applicability of machine\nlearning for image-based permeability prediction and open a new area of Digital\nRock research. \n\n"}
{"id": "1803.01485", "contents": "Title: Totally Looks Like - How Humans Compare, Compared to Machines Abstract: Perceptual judgment of image similarity by humans relies on rich internal\nrepresentations ranging from low-level features to high-level concepts, scene\nproperties and even cultural associations. However, existing methods and\ndatasets attempting to explain perceived similarity use stimuli which arguably\ndo not cover the full breadth of factors that affect human similarity\njudgments, even those geared toward this goal. We introduce a new dataset\ndubbed Totally-Looks-Like (TLL) after a popular entertainment website, which\ncontains images paired by humans as being visually similar. The dataset\ncontains 6016 image-pairs from the wild, shedding light upon a rich and diverse\nset of criteria employed by human beings. We conduct experiments to try to\nreproduce the pairings via features extracted from state-of-the-art deep\nconvolutional neural networks, as well as additional human experiments to\nverify the consistency of the collected data. Though we create conditions to\nartificially make the matching task increasingly easier, we show that\nmachine-extracted representations perform very poorly in terms of reproducing\nthe matching selected by humans. We discuss and analyze these results,\nsuggesting future directions for improvement of learned image representations. \n\n"}
{"id": "1803.02735", "contents": "Title: Deep Back-Projection Networks For Super-Resolution Abstract: The feed-forward architectures of recently proposed deep super-resolution\nnetworks learn representations of low-resolution inputs, and the non-linear\nmapping from those to high-resolution output. However, this approach does not\nfully address the mutual dependencies of low- and high-resolution images. We\npropose Deep Back-Projection Networks (DBPN), that exploit iterative up- and\ndown-sampling layers, providing an error feedback mechanism for projection\nerrors at each stage. We construct mutually-connected up- and down-sampling\nstages each of which represents different types of image degradation and\nhigh-resolution components. We show that extending this idea to allow\nconcatenation of features across up- and down-sampling stages (Dense DBPN)\nallows us to reconstruct further improve super-resolution, yielding superior\nresults and in particular establishing new state of the art results for large\nscaling factors such as 8x across multiple data sets. \n\n"}
{"id": "1803.03095", "contents": "Title: Leveraging Unlabeled Data for Crowd Counting by Learning to Rank Abstract: We propose a novel crowd counting approach that leverages abundantly\navailable unlabeled crowd imagery in a learning-to-rank framework. To induce a\nranking of cropped images , we use the observation that any sub-image of a\ncrowded scene image is guaranteed to contain the same number or fewer persons\nthan the super-image. This allows us to address the problem of limited size of\nexisting datasets for crowd counting. We collect two crowd scene datasets from\nGoogle using keyword searches and query-by-example image retrieval,\nrespectively. We demonstrate how to efficiently learn from these unlabeled\ndatasets by incorporating learning-to-rank in a multi-task network which\nsimultaneously ranks images and estimates crowd density maps. Experiments on\ntwo of the most challenging crowd counting datasets show that our approach\nobtains state-of-the-art results. \n\n"}
{"id": "1803.04792", "contents": "Title: Testing Deep Neural Networks Abstract: Deep neural networks (DNNs) have a wide range of applications, and software\nemploying them must be thoroughly tested, especially in safety-critical\ndomains. However, traditional software test coverage metrics cannot be applied\ndirectly to DNNs. In this paper, inspired by the MC/DC coverage criterion, we\npropose a family of four novel test criteria that are tailored to structural\nfeatures of DNNs and their semantics. We validate the criteria by demonstrating\nthat the generated test inputs guided via our proposed coverage criteria are\nable to capture undesired behaviours in a DNN. Test cases are generated using a\nsymbolic approach and a gradient-based heuristic search. By comparing them with\nexisting methods, we show that our criteria achieve a balance between their\nability to find bugs (proxied using adversarial examples) and the computational\ncost of test case generation. Our experiments are conducted on state-of-the-art\nDNNs obtained using popular open source datasets, including MNIST, CIFAR-10 and\nImageNet. \n\n"}
{"id": "1803.04831", "contents": "Title: Independently Recurrent Neural Network (IndRNN): Building A Longer and\n  Deeper RNN Abstract: Recurrent neural networks (RNNs) have been widely used for processing\nsequential data. However, RNNs are commonly difficult to train due to the\nwell-known gradient vanishing and exploding problems and hard to learn\nlong-term patterns. Long short-term memory (LSTM) and gated recurrent unit\n(GRU) were developed to address these problems, but the use of hyperbolic\ntangent and the sigmoid action functions results in gradient decay over layers.\nConsequently, construction of an efficiently trainable deep network is\nchallenging. In addition, all the neurons in an RNN layer are entangled\ntogether and their behaviour is hard to interpret. To address these problems, a\nnew type of RNN, referred to as independently recurrent neural network\n(IndRNN), is proposed in this paper, where neurons in the same layer are\nindependent of each other and they are connected across layers. We have shown\nthat an IndRNN can be easily regulated to prevent the gradient exploding and\nvanishing problems while allowing the network to learn long-term dependencies.\nMoreover, an IndRNN can work with non-saturated activation functions such as\nrelu (rectified linear unit) and be still trained robustly. Multiple IndRNNs\ncan be stacked to construct a network that is deeper than the existing RNNs.\nExperimental results have shown that the proposed IndRNN is able to process\nvery long sequences (over 5000 time steps), can be used to construct very deep\nnetworks (21 layers used in the experiment) and still be trained robustly.\nBetter performances have been achieved on various tasks by using IndRNNs\ncompared with the traditional RNN and LSTM. The code is available at\nhttps://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne. \n\n"}
{"id": "1803.05494", "contents": "Title: Improving Object Counting with Heatmap Regulation Abstract: In this paper, we propose a simple and effective way to improve one-look\nregression models for object counting from images. We use class activation map\nvisualizations to illustrate the drawbacks of learning a pure one-look\nregression model for a counting task. Based on these insights, we enhance\none-look regression counting models by regulating activation maps from the\nfinal convolution layer of the network with coarse ground-truth activation maps\ngenerated from simple dot annotations. We call this strategy heatmap regulation\n(HR). We show that this simple enhancement effectively suppresses false\ndetections generated by the corresponding one-look baseline model and also\nimproves the performance in terms of false negatives. Evaluations are performed\non four different counting datasets --- two for car counting (CARPK, PUCPR+),\none for crowd counting (WorldExpo) and another for biological cell counting\n(VGG-Cells). Adding HR to a simple VGG front-end improves performance on all\nthese benchmarks compared to a simple one-look baseline model and results in\nstate-of-the-art performance for car counting. \n\n"}
{"id": "1803.05566", "contents": "Title: Advancing Acoustic-to-Word CTC Model Abstract: The acoustic-to-word model based on the connectionist temporal classification\n(CTC) criterion was shown as a natural end-to-end (E2E) model directly\ntargeting words as output units. However, the word-based CTC model suffers from\nthe out-of-vocabulary (OOV) issue as it can only model limited number of words\nin the output layer and maps all the remaining words into an OOV output node.\nHence, such a word-based CTC model can only recognize the frequent words\nmodeled by the network output nodes. Our first attempt to improve the\nacoustic-to-word model is a hybrid CTC model which consults a letter-based CTC\nwhen the word-based CTC model emits OOV tokens during testing time. Then, we\npropose a much better solution by training a mixed-unit CTC model which\ndecomposes all the OOV words into sequences of frequent words and multi-letter\nunits. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, the\nfinal acoustic-to-word solution improves the baseline word-based CTC by\nrelative 12.09% word error rate (WER) reduction when combined with our proposed\nattention CTC. Such an E2E model without using any language model (LM) or\ncomplex decoder outperforms the traditional context-dependent phoneme CTC which\nhas strong LM and decoder by relative 6.79%. \n\n"}
{"id": "1803.05759", "contents": "Title: Salient Region Segmentation Abstract: Saliency prediction is a well studied problem in computer vision. Early\nsaliency models were based on low-level hand-crafted feature derived from\ninsights gained in neuroscience and psychophysics. In the wake of deep learning\nbreakthrough, a new cohort of models were proposed based on neural network\narchitectures, allowing significantly higher gaze prediction than previous\nshallow models, on all metrics.\n  However, most models treat the saliency prediction as a \\textit{regression}\nproblem, and accurate regression of high-dimensional data is known to be a hard\nproblem. Furthermore, it is unclear that intermediate levels of saliency (ie,\nneither very high, nor very low) are meaningful: Something is either salient,\nor it is not.\n  Drawing from those two observations, we reformulate the saliency prediction\nproblem as a salient region \\textit{segmentation} problem. We demonstrate that\nthe reformulation allows for faster convergence than the classical regression\nproblem, while performance is comparable to state-of-the-art.\n  We also visualise the general features learned by the model, which are showed\nto be consistent with insights from psychophysics. \n\n"}
{"id": "1803.05982", "contents": "Title: Real-time Deep Pose Estimation with Geodesic Loss for Image-to-Template\n  Rigid Registration Abstract: With an aim to increase the capture range and accelerate the performance of\nstate-of-the-art inter-subject and subject-to-template 3D registration, we\npropose deep learning-based methods that are trained to find the 3D position of\narbitrarily oriented subjects or anatomy based on slices or volumes of medical\nimages. For this, we propose regression CNNs that learn to predict the\nangle-axis representation of 3D rotations and translations using image\nfeatures. We use and compare mean square error and geodesic loss to train\nregression CNNs for 3D pose estimation used in two different scenarios:\nslice-to-volume registration and volume-to-volume registration. Our results\nshow that in such registration applications that are amendable to learning, the\nproposed deep learning methods with geodesic loss minimization can achieve\naccurate results with a wide capture range in real-time (<100ms). We also\ntested the generalization capability of the trained CNNs on an expanded age\nrange and on images of newborn subjects with similar and different MR image\ncontrasts. We trained our models on T2-weighted fetal brain MRI scans and used\nthem to predict the 3D pose of newborn brains based on T1-weighted MRI scans.\nWe showed that the trained models generalized well for the new domain when we\nperformed image contrast transfer through a conditional generative adversarial\nnetwork. This indicates that the domain of application of the trained deep\nregression CNNs can be further expanded to image modalities and contrasts other\nthan those used in training. A combination of our proposed methods with\naccelerated optimization-based registration algorithms can dramatically enhance\nthe performance of automatic imaging devices and image processing methods of\nthe future. \n\n"}
{"id": "1803.06184", "contents": "Title: The ApolloScape Open Dataset for Autonomous Driving and its Application Abstract: Autonomous driving has attracted tremendous attention especially in the past\nfew years. The key techniques for a self-driving car include solving tasks like\n3D map construction, self-localization, parsing the driving road and\nunderstanding objects, which enable vehicles to reason and act. However, large\nscale data set for training and system evaluation is still a bottleneck for\ndeveloping robust perception models. In this paper, we present the ApolloScape\ndataset [1] and its applications for autonomous driving. Compared with existing\npublic datasets from real scenes, e.g. KITTI [2] or Cityscapes [3], ApolloScape\ncontains much large and richer labelling including holistic semantic dense\npoint cloud for each site, stereo, per-pixel semantic labelling, lanemark\nlabelling, instance segmentation, 3D car instance, high accurate location for\nevery frame in various driving videos from multiple sites, cities and daytimes.\nFor each task, it contains at lease 15x larger amount of images than SOTA\ndatasets. To label such a complete dataset, we develop various tools and\nalgorithms specified for each task to accelerate the labelling process, such as\n3D-2D segment labeling tools, active labelling in videos etc. Depend on\nApolloScape, we are able to develop algorithms jointly consider the learning\nand inference of multiple tasks. In this paper, we provide a sensor fusion\nscheme integrating camera videos, consumer-grade motion sensors (GPS/IMU), and\na 3D semantic map in order to achieve robust self-localization and semantic\nsegmentation for autonomous driving. We show that practically, sensor fusion\nand joint learning of multiple tasks are beneficial to achieve a more robust\nand accurate system. We expect our dataset and proposed relevant algorithms can\nsupport and motivate researchers for further development of multi-sensor fusion\nand multi-task learning in the field of computer vision. \n\n"}
{"id": "1803.06795", "contents": "Title: Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration Abstract: Low-rank signal modeling has been widely leveraged to capture non-local\ncorrelation in image processing applications. We propose a new method that\nemploys low-rank tensor factor analysis for tensors generated by grouped image\npatches. The low-rank tensors are fed into the alternative direction multiplier\nmethod (ADMM) to further improve image reconstruction. The motivating\napplication is compressive sensing (CS), and a deep convolutional architecture\nis adopted to approximate the expensive matrix inversion in CS applications. An\niterative algorithm based on this low-rank tensor factorization strategy,\ncalled NLR-TFA, is presented in detail. Experimental results on noiseless and\nnoisy CS measurements demonstrate the superiority of the proposed approach,\nespecially at low CS sampling rates. \n\n"}
{"id": "1803.06813", "contents": "Title: Weakly Supervised Object Localization on grocery shelves using simple\n  FCN and Synthetic Dataset Abstract: We propose a weakly supervised method using two algorithms to predict object\nbounding boxes given only an image classification dataset. First algorithm is a\nsimple Fully Convolutional Network (FCN) trained to classify object instances.\nWe use the property of FCN to return a mask for images larger than training\nimages to get a primary output segmentation mask during test time by passing an\nimage pyramid to it. We enhance the FCN output mask into final output bounding\nboxes by a Convolutional Encoder-Decoder (ConvAE) viz. the second algorithm.\nConvAE is trained to localize objects on an artificially generated dataset of\noutput segmentation masks. We demonstrate the effectiveness of this method in\nlocalizing objects in grocery shelves where annotating data for object\ndetection is hard due to variety of objects. This method can be extended to any\nproblem domain where collecting images of objects is easy and annotating their\ncoordinates is hard. \n\n"}
{"id": "1803.07212", "contents": "Title: Real-time Burst Photo Selection Using a Light-Head Adversarial Network Abstract: We present an automatic moment capture system that runs in real-time on\nmobile cameras. The system is designed to run in the viewfinder mode and\ncapture a burst sequence of frames before and after the shutter is pressed. For\neach frame, the system predicts in real-time a \"goodness\" score, based on which\nthe best moment in the burst can be selected immediately after the shutter is\nreleased, without any user interference. To solve the problem, we develop a\nhighly efficient deep neural network ranking model, which implicitly learns a\n\"latent relative attribute\" space to capture subtle visual differences within a\nsequence of burst images. Then the overall goodness is computed as a linear\naggregation of the goodnesses of all the latent attributes. The latent relative\nattributes and the aggregation function can be seamlessly integrated in one\nfully convolutional network and trained in an end-to-end fashion. To obtain a\ncompact model which can run on mobile devices in real-time, we have explored\nand evaluated a wide range of network design choices, taking into account the\nconstraints of model size, computational cost, and accuracy. Extensive studies\nshow that the best frame predicted by our model hit users' top-1 (out of 11 on\naverage) choice for $64.1\\%$ cases and top-3 choices for $86.2\\%$ cases.\nMoreover, the model(only 0.47M Bytes) can run in real time on mobile devices,\ne.g. only 13ms on iPhone 7 for one frame prediction. \n\n"}
{"id": "1803.08024", "contents": "Title: Stacked Cross Attention for Image-Text Matching Abstract: In this paper, we study the problem of image-text matching. Inferring the\nlatent semantic alignment between objects or other salient stuff (e.g. snow,\nsky, lawn) and the corresponding words in sentences allows to capture\nfine-grained interplay between vision and language, and makes image-text\nmatching more interpretable. Prior work either simply aggregates the similarity\nof all possible pairs of regions and words without attending differentially to\nmore and less important words or regions, or uses a multi-step attentional\nprocess to capture limited number of semantic alignments which is less\ninterpretable. In this paper, we present Stacked Cross Attention to discover\nthe full latent alignments using both image regions and words in a sentence as\ncontext and infer image-text similarity. Our approach achieves the\nstate-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,\nour approach outperforms the current best methods by 22.1% relatively in text\nretrieval from image query, and 18.2% relatively in image retrieval with text\nquery (based on Recall@1). On MS-COCO, our approach improves sentence retrieval\nby 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1\nusing the 5K test set). Code has been made available at:\nhttps://github.com/kuanghuei/SCAN. \n\n"}
{"id": "1803.08134", "contents": "Title: Task dependent Deep LDA pruning of neural networks Abstract: With deep learning's success, a limited number of popular deep nets have been\nwidely adopted for various vision tasks. However, this usually results in\nunnecessarily high complexities and possibly many features of low task utility.\nIn this paper, we address this problem by introducing a task-dependent deep\npruning framework based on Fisher's Linear Discriminant Analysis (LDA). The\napproach can be applied to convolutional, fully-connected, and module-based\ndeep network structures, in all cases leveraging the high decorrelation of\nneuron motifs found in the pre-decision space and cross-layer deconv\ndependency. Moreover, we examine our approach's potential in network\narchitecture search for specific tasks and analyze the influence of our pruning\non model robustness to noises and adversarial attacks. Experimental results on\ndatasets of generic objects (ImageNet, CIFAR100) as well as domain specific\ntasks (Adience, and LFWA) illustrate our framework's superior performance over\nstate-of-the-art pruning approaches and fixed compact nets (e.g. SqueezeNet,\nMobileNet). The proposed method successfully maintains comparable accuracies\neven after discarding most parameters (98%-99% for VGG16, up to 82% for the\nalready compact InceptionNet) and with significant FLOP reductions (83% for\nVGG16, up to 64% for InceptionNet). Through pruning, we can also derive\nsmaller, but more accurate and more robust models suitable for the task. \n\n"}
{"id": "1803.08607", "contents": "Title: A Quantization-Friendly Separable Convolution for MobileNets Abstract: As deep learning (DL) is being rapidly pushed to edge computing, researchers\ninvented various ways to make inference computation more efficient on\nmobile/IoT devices, such as network pruning, parameter compression, and etc.\nQuantization, as one of the key approaches, can effectively offload GPU, and\nmake it possible to deploy DL on fixed-point pipeline. Unfortunately, not all\nexisting networks design are friendly to quantization. For example, the popular\nlightweight MobileNetV1, while it successfully reduces parameter size and\ncomputation latency with separable convolution, our experiment shows its\nquantized models have large accuracy gap against its float point models. To\nresolve this, we analyzed the root cause of quantization loss and proposed a\nquantization-friendly separable convolution architecture. By evaluating the\nimage classification task on ImageNet2012 dataset, our modified MobileNetV1\nmodel can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the\ngap to the float pipeline. \n\n"}
{"id": "1803.08740", "contents": "Title: Speeding-up Object Detection Training for Robotics with FALKON Abstract: Latest deep learning methods for object detection provide remarkable\nperformance, but have limits when used in robotic applications. One of the most\nrelevant issues is the long training time, which is due to the large size and\nimbalance of the associated training sets, characterized by few positive and a\nlarge number of negative examples (i.e. background). Proposed approaches are\nbased on end-to-end learning by back-propagation [22] or kernel methods trained\nwith Hard Negatives Mining on top of deep features [8]. These solutions are\neffective, but prohibitively slow for on-line applications. In this paper we\npropose a novel pipeline for object detection that overcomes this problem and\nprovides comparable performance, with a 60x training speedup. Our pipeline\ncombines (i) the Region Proposal Network and the deep feature extractor from\n[22] to efficiently select candidate RoIs and encode them into powerful\nrepresentations, with (ii) the FALKON [23] algorithm, a novel kernel-based\nmethod that allows fast training on large scale problems (millions of points).\nWe address the size and imbalance of training data by exploiting the stochastic\nsubsampling intrinsic into the method and a novel, fast, bootstrapping\napproach. We assess the effectiveness of the approach on a standard Computer\nVision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a\nreal robotic scenario with the iCubWorld Transformations [18] dataset. \n\n"}
{"id": "1803.09025", "contents": "Title: Realtime Time Synchronized Event-based Stereo Abstract: In this work, we propose a novel event based stereo method which addresses\nthe problem of motion blur for a moving event camera. Our method uses the\nvelocity of the camera and a range of disparities to synchronize the positions\nof the events, as if they were captured at a single point in time. We represent\nthese events using a pair of novel time synchronized event disparity volumes,\nwhich we show remove motion blur for pixels at the correct disparity in the\nvolume, while further blurring pixels at the wrong disparity. We then apply a\nnovel matching cost over these time synchronized event disparity volumes, which\nboth rewards similarity between the volumes while penalizing blurriness. We\nshow that our method outperforms more expensive, smoothing based event stereo\nmethods, by evaluating on the Multi Vehicle Stereo Event Camera dataset. \n\n"}
{"id": "1803.09132", "contents": "Title: Multi-Level Factorisation Net for Person Re-Identification Abstract: Key to effective person re-identification (Re-ID) is modelling discriminative\nand view-invariant factors of person appearance at both high and low semantic\nlevels. Recently developed deep Re-ID models either learn a holistic single\nsemantic level feature representation and/or require laborious human annotation\nof these factors as attributes. We propose Multi-Level Factorisation Net\n(MLFN), a novel network architecture that factorises the visual appearance of a\nperson into latent discriminative factors at multiple semantic levels without\nmanual annotation. MLFN is composed of multiple stacked blocks. Each block\ncontains multiple factor modules to model latent factors at a specific level,\nand factor selection modules that dynamically select the factor modules to\ninterpret the content of each input image. The outputs of the factor selection\nmodules also provide a compact latent factor descriptor that is complementary\nto the conventional deeply learned features. MLFN achieves state-of-the-art\nresults on three Re-ID datasets, as well as compelling results on the general\nobject categorisation CIFAR-100 dataset. \n\n"}
{"id": "1803.09196", "contents": "Title: Learning Type-Aware Embeddings for Fashion Compatibility Abstract: Outfits in online fashion data are composed of items of many different types\n(e.g. top, bottom, shoes) that share some stylistic relationship with one\nanother. A representation for building outfits requires a method that can learn\nboth notions of similarity (for example, when two tops are interchangeable) and\ncompatibility (items of possibly different type that can go together in an\noutfit). This paper presents an approach to learning an image embedding that\nrespects item type, and jointly learns notions of item similarity and\ncompatibility in an end-to-end model. To evaluate the learned representation,\nwe crawled 68,306 outfits created by users on the Polyvore website. Our\napproach obtains 3-5% improvement over the state-of-the-art on outfit\ncompatibility prediction and fill-in-the-blank tasks using our dataset, as well\nas an established smaller dataset, while supporting a variety of useful\nqueries. \n\n"}
{"id": "1803.09359", "contents": "Title: A Face Recognition Signature Combining Patch-based Features with Soft\n  Facial Attributes Abstract: This paper focuses on improving face recognition performance with a new\nsignature combining implicit facial features with explicit soft facial\nattributes. This signature has two components: the existing patch-based\nfeatures and the soft facial attributes. A deep convolutional neural network\nadapted from state-of-the-art networks is used to learn the soft facial\nattributes. Then, a signature matcher is introduced that merges the\ncontributions of both patch-based features and the facial attributes. In this\nmatcher, the matching scores computed from patch-based features and the facial\nattributes are combined to obtain a final matching score. The matcher is also\nextended so that different weights are assigned to different facial attributes.\nThe proposed signature and matcher have been evaluated with the UR2D system on\nthe UHDB31 and IJB-A datasets. The experimental results indicate that the\nproposed signature achieve better performance than using only patch-based\nfeatures. The Rank-1 accuracy is improved significantly by 4% and 0.37% on the\ntwo datasets when compared with the UR2D system. \n\n"}
{"id": "1803.09453", "contents": "Title: CNN in MRF: Video Object Segmentation via Inference in A CNN-Based\n  Higher-Order Spatio-Temporal MRF Abstract: This paper addresses the problem of video object segmentation, where the\ninitial object mask is given in the first frame of an input video. We propose a\nnovel spatio-temporal Markov Random Field (MRF) model defined over pixels to\nhandle this problem. Unlike conventional MRF models, the spatial dependencies\namong pixels in our model are encoded by a Convolutional Neural Network (CNN).\nSpecifically, for a given object, the probability of a labeling to a set of\nspatially neighboring pixels can be predicted by a CNN trained for this\nspecific object. As a result, higher-order, richer dependencies among pixels in\nthe set can be implicitly modeled by the CNN. With temporal dependencies\nestablished by optical flow, the resulting MRF model combines both spatial and\ntemporal cues for tackling video object segmentation. However, performing\ninference in the MRF model is very difficult due to the very high-order\ndependencies. To this end, we propose a novel CNN-embedded algorithm to perform\napproximate inference in the MRF. This algorithm proceeds by alternating\nbetween a temporal fusion step and a feed-forward CNN step. When initialized\nwith an appearance-based one-shot segmentation CNN, our model outperforms the\nwinning entries of the DAVIS 2017 Challenge, without resorting to model\nensembling or any dedicated detectors. \n\n"}
{"id": "1803.09454", "contents": "Title: Fast and Accurate Single Image Super-Resolution via Information\n  Distillation Network Abstract: Recently, deep convolutional neural networks (CNNs) have been demonstrated\nremarkable progress on single image super-resolution. However, as the depth and\nwidth of the networks increase, CNN-based super-resolution methods have been\nfaced with the challenges of computational complexity and memory consumption in\npractice. In order to solve the above questions, we propose a deep but compact\nconvolutional network to directly reconstruct the high resolution image from\nthe original low resolution image. In general, the proposed model consists of\nthree parts, which are feature extraction block, stacked information\ndistillation blocks and reconstruction block respectively. By combining an\nenhancement unit with a compression unit into a distillation block, the local\nlong and short-path features can be effectively extracted. Specifically, the\nproposed enhancement unit mixes together two different types of features and\nthe compression unit distills more useful information for the sequential\nblocks. In addition, the proposed network has the advantage of fast execution\ndue to the comparatively few numbers of filters per layer and the use of group\nconvolution. Experimental results demonstrate that the proposed method is\nsuperior to the state-of-the-art methods, especially in terms of time\nperformance. \n\n"}
{"id": "1803.09867", "contents": "Title: Towards Human-Machine Cooperation: Self-supervised Sample Mining for\n  Object Detection Abstract: Though quite challenging, leveraging large-scale unlabeled or partially\nlabeled images in a cost-effective way has increasingly attracted interests for\nits great importance to computer vision. To tackle this problem, many Active\nLearning (AL) methods have been developed. However, these methods mainly define\ntheir sample selection criteria within a single image context, leading to the\nsuboptimal robustness and impractical solution for large-scale object\ndetection. In this paper, aiming to remedy the drawbacks of existing AL\nmethods, we present a principled Self-supervised Sample Mining (SSM) process\naccounting for the real challenges in object detection. Specifically, our SSM\nprocess concentrates on automatically discovering and pseudo-labeling reliable\nregion proposals for enhancing the object detector via the introduced cross\nimage validation, i.e., pasting these proposals into different labeled images\nto comprehensively measure their values under different image contexts. By\nresorting to the SSM process, we propose a new AL framework for gradually\nincorporating unlabeled or partially labeled data into the model learning while\nminimizing the annotating effort of users. Extensive experiments on two public\nbenchmarks clearly demonstrate our proposed framework can achieve the\ncomparable performance to the state-of-the-art methods with significantly fewer\nannotations. \n\n"}
{"id": "1803.10335", "contents": "Title: Adaptive Affinity Fields for Semantic Segmentation Abstract: Semantic segmentation has made much progress with increasingly powerful\npixel-wise classifiers and incorporating structural priors via Conditional\nRandom Fields (CRF) or Generative Adversarial Networks (GAN). We propose a\nsimpler alternative that learns to verify the spatial structure of segmentation\nduring training only. Unlike existing approaches that enforce semantic labels\non individual pixels and match labels between neighbouring pixels, we propose\nthe concept of Adaptive Affinity Fields (AAF) to capture and match the semantic\nrelations between neighbouring pixels in the label space. We use adversarial\nlearning to select the optimal affinity field size for each semantic category.\nIt is formulated as a minimax problem, optimizing our segmentation neural\nnetwork in a best worst-case learning scenario. AAF is versatile for\nrepresenting structures as a collection of pixel-centric relations, easier to\ntrain than GAN and more efficient than CRF without run-time inference. Our\nextensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets\ndemonstrate its above-par segmentation performance and robust generalization\nacross domains. \n\n"}
{"id": "1803.10404", "contents": "Title: Lip Movements Generation at a Glance Abstract: Cross-modality generation is an emerging topic that aims to synthesize data\nin one modality based on information in a different modality. In this paper, we\nconsider a task of such: given an arbitrary audio speech and one lip image of\narbitrary target identity, generate synthesized lip movements of the target\nidentity saying the speech. To perform well in this task, it inevitably\nrequires a model to not only consider the retention of target identity,\nphoto-realistic of synthesized images, consistency and smoothness of lip images\nin a sequence, but more importantly, learn the correlations between audio\nspeech and lip movements. To solve the collective problems, we explore the best\nmodeling of the audio-visual correlations in building and training a\nlip-movement generator network. Specifically, we devise a method to fuse audio\nand image embeddings to generate multiple lip images at once and propose a\nnovel correlation loss to synchronize lip changes and speech changes. Our final\nmodel utilizes a combination of four losses for a comprehensive consideration\nin generating lip movements; it is trained in an end-to-end fashion and is\nrobust to lip shapes, view angles and different facial characteristics.\nThoughtful experiments on three datasets ranging from lab-recorded to lips\nin-the-wild show that our model significantly outperforms other\nstate-of-the-art methods extended to this task. \n\n"}
{"id": "1803.10590", "contents": "Title: Feed-forward Uncertainty Propagation in Belief and Neural Networks Abstract: We propose a feed-forward inference method applicable to belief and neural\nnetworks. In a belief network, the method estimates an approximate factorized\nposterior of all hidden units given the input. In neural networks the method\npropagates uncertainty of the input through all the layers. In neural networks\nwith injected noise, the method analytically takes into account uncertainties\nresulting from this noise. Such feed-forward analytic propagation is\ndifferentiable in parameters and can be trained end-to-end. Compared to\nstandard NN, which can be viewed as propagating only the means, we propagate\nthe mean and variance. The method can be useful in all scenarios that require\nknowledge of the neuron statistics, e.g. when dealing with uncertain inputs,\nconsidering sigmoid activations as probabilities of Bernoulli units, training\nthe models regularized by injected noise (dropout) or estimating activation\nstatistics over the dataset (as needed for normalization methods). In the\nexperiments we show the possible utility of the method in all these tasks as\nwell as its current limitations. \n\n"}
{"id": "1803.10630", "contents": "Title: Person re-identification with fusion of hand-crafted and deep pose-based\n  body region features Abstract: Person re-identification (re-ID) aims to accurately re- trieve a person from\na large-scale database of images cap- tured across multiple cameras. Existing\nworks learn deep representations using a large training subset of unique per-\nsons. However, identifying unseen persons is critical for a good re-ID\nalgorithm. Moreover, the misalignment be- tween person crops to detection\nerrors or pose variations leads to poor feature matching. In this work, we\npresent a fusion of handcrafted features and deep feature representa- tion\nlearned using multiple body parts to complement the global body features that\nachieves high performance on un- seen test images. Pose information is used to\ndetect body regions that are passed through Convolutional Neural Net- works\n(CNN) to guide feature learning. Finally, a metric learning step enables robust\ndistance matching on a dis- criminative subspace. Experimental results on 4\npopular re-ID benchmark datasets namely VIPer, DukeMTMC-reID, Market-1501 and\nCUHK03 show that the proposed method achieves state-of-the-art performance in\nimage-based per- son re-identification. \n\n"}
{"id": "1803.10896", "contents": "Title: Deep Texture Manifold for Ground Terrain Recognition Abstract: We present a texture network called Deep Encoding Pooling Network (DEP) for\nthe task of ground terrain recognition. Recognition of ground terrain is an\nimportant task in establishing robot or vehicular control parameters, as well\nas for localization within an outdoor environment. The architecture of DEP\nintegrates orderless texture details and local spatial information and the\nperformance of DEP surpasses state-of-the-art methods for this task. The GTOS\ndatabase (comprised of over 30,000 images of 40 classes of ground terrain in\noutdoor scenes) enables supervised recognition. For evaluation under realistic\nconditions, we use test images that are not from the existing GTOS dataset, but\nare instead from hand-held mobile phone videos of similar terrain. This new\nevaluation dataset, GTOS-mobile, consists of 81 videos of 31 classes of ground\nterrain such as grass, gravel, asphalt and sand. The resultant network shows\nexcellent performance not only for GTOS-mobile, but also for more general\ndatabases (MINC and DTD). Leveraging the discriminant features learned from\nthis network, we build a new texture manifold called DEP-manifold. We learn a\nparametric distribution in feature space in a fully supervised manner, which\ngives the distance relationship among classes and provides a means to\nimplicitly represent ambiguous class boundaries. The source code and database\nare publicly available. \n\n"}
{"id": "1803.10914", "contents": "Title: Adversarial Binary Coding for Efficient Person Re-identification Abstract: Person re-identification (ReID) aims at matching persons across different\nviews/scenes. In addition to accuracy, the matching efficiency has received\nmore and more attention because of demanding applications using large-scale\ndata. Several binary coding based methods have been proposed for efficient\nReID, which either learn projections to map high-dimensional features to\ncompact binary codes, or directly adopt deep neural networks by simply\ninserting an additional fully-connected layer with tanh-like activations.\nHowever, the former approach requires time-consuming hand-crafted feature\nextraction and complicated (discrete) optimizations; the latter lacks the\nnecessary discriminative information greatly due to the straightforward\nactivation functions. In this paper, we propose a simple yet effective\nframework for efficient ReID inspired by the recent advances in adversarial\nlearning. Specifically, instead of learning explicit projections or adding\nfully-connected mapping layers, the proposed Adversarial Binary Coding (ABC)\nframework guides the extraction of binary codes implicitly and effectively. The\ndiscriminability of the extracted codes is further enhanced by equipping the\nABC with a deep triplet network for the ReID task. More importantly, the ABC\nand triplet network are simultaneously optimized in an end-to-end manner.\nExtensive experiments on three large-scale ReID benchmarks demonstrate the\nsuperiority of our approach over the state-of-the-art methods. \n\n"}
{"id": "1804.00015", "contents": "Title: ESPnet: End-to-End Speech Processing Toolkit Abstract: This paper introduces a new open source platform for end-to-end speech\nprocessing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech\nrecognition (ASR), and adopts widely-used dynamic neural network toolkits,\nChainer and PyTorch, as a main deep learning engine. ESPnet also follows the\nKaldi ASR toolkit style for data processing, feature extraction/format, and\nrecipes to provide a complete setup for speech recognition and other speech\nprocessing experiments. This paper explains a major architecture of this\nsoftware platform, several important functionalities, which differentiate\nESPnet from other open source ASR toolkits, and experimental results with major\nASR benchmarks. \n\n"}
{"id": "1804.00103", "contents": "Title: A LiDAR Point Cloud Generator: from a Virtual World to Autonomous\n  Driving Abstract: 3D LiDAR scanners are playing an increasingly important role in autonomous\ndriving as they can generate depth information of the environment. However,\ncreating large 3D LiDAR point cloud datasets with point-level labels requires a\nsignificant amount of manual annotation. This jeopardizes the efficient\ndevelopment of supervised deep learning algorithms which are often data-hungry.\nWe present a framework to rapidly create point clouds with accurate point-level\nlabels from a computer game. The framework supports data collection from both\nauto-driving scenes and user-configured scenes. Point clouds from auto-driving\nscenes can be used as training data for deep learning algorithms, while point\nclouds from user-configured scenes can be used to systematically test the\nvulnerability of a neural network, and use the falsifying examples to make the\nneural network more robust through retraining. In addition, the scene images\ncan be captured simultaneously in order for sensor fusion tasks, with a method\nproposed to do automatic calibration between the point clouds and captured\nscene images. We show a significant improvement in accuracy (+9%) in point\ncloud segmentation by augmenting the training dataset with the generated\nsynthesized data. Our experiments also show by testing and retraining the\nnetwork using point clouds from user-configured scenes, the weakness/blind\nspots of the neural network can be fixed. \n\n"}
{"id": "1804.00304", "contents": "Title: Fully automatic detection and segmentation of abdominal aortic thrombus\n  in post-operative CTA images using deep convolutional neural networks Abstract: Computerized Tomography Angiography (CTA) based follow-up of Abdominal Aortic\nAneurysms (AAA) treated with Endovascular Aneurysm Repair (EVAR) is essential\nto evaluate the progress of the patient and detect complications. In this\ncontext, accurate quantification of post-operative thrombus volume is required.\nHowever, a proper evaluation is hindered by the lack of automatic, robust and\nreproducible thrombus segmentation algorithms. We propose a new fully automatic\napproach based on Deep Convolutional Neural Networks (DCNN) for robust and\nreproducible thrombus region of interest detection and subsequent fine thrombus\nsegmentation. The DetecNet detection network is adapted to perform region of\ninterest extraction from a complete CTA and a new segmentation network\narchitecture, based on Fully Convolutional Networks and a Holistically-Nested\nEdge Detection Network, is presented. These networks are trained, validated and\ntested in 13 post-operative CTA volumes of different patients using a 4-fold\ncross-validation approach to provide more robustness to the results. Our\npipeline achieves a Dice score of more than 82% for post-operative thrombus\nsegmentation and provides a mean relative volume difference between ground\ntruth and automatic segmentation that lays within the experienced human\nobserver variance without the need of human intervention in most common cases. \n\n"}
{"id": "1804.00326", "contents": "Title: Seeing Voices and Hearing Faces: Cross-modal biometric matching Abstract: We introduce a seemingly impossible task: given only an audio clip of someone\nspeaking, decide which of two face images is the speaker. In this paper we\nstudy this, and a number of related cross-modal tasks, aimed at answering the\nquestion: how much can we infer from the voice about the face and vice versa?\nWe study this task \"in the wild\", employing the datasets that are now publicly\navailable for face recognition from static images (VGGFace) and speaker\nidentification from audio (VoxCeleb). These provide training and testing\nscenarios for both static and dynamic testing of cross-modal matching. We make\nthe following contributions: (i) we introduce CNN architectures for both binary\nand multi-way cross-modal face and audio matching, (ii) we compare dynamic\ntesting (where video information is available, but the audio is not from the\nsame video) with static testing (where only a single still image is available),\nand (iii) we use human testing as a baseline to calibrate the difficulty of the\ntask. We show that a CNN can indeed be trained to solve this task in both the\nstatic and dynamic scenarios, and is even well above chance on 10-way\nclassification of the face given the voice. The CNN matches human performance\non easy examples (e.g. different gender across faces) but exceeds human\nperformance on more challenging examples (e.g. faces with the same gender, age\nand nationality). \n\n"}
{"id": "1804.00393", "contents": "Title: Generative Spatiotemporal Modeling Of Neutrophil Behavior Abstract: Cell motion and appearance have a strong correlation with cell cycle and\ndisease progression. Many contemporary efforts in machine learning utilize\nspatio-temporal models to predict a cell's physical state and, consequently,\nthe advancement of disease. Alternatively, generative models learn the\nunderlying distribution of the data, creating holistic representations that can\nbe used in learning. In this work, we propose an aggregate model that combine\nGenerative Adversarial Networks (GANs) and Autoregressive (AR) models to\npredict cell motion and appearance in human neutrophils imaged by differential\ninterference contrast (DIC) microscopy. We bifurcate the task of learning cell\nstatistics by leveraging GANs for the spatial component and AR models for the\ntemporal component. The aggregate model learned results offer a promising\ncomputational environment for studying changes in organellar shape, quantity,\nand spatial distribution over large sequences. \n\n"}
{"id": "1804.00413", "contents": "Title: End-to-End Learning of Motion Representation for Video Understanding Abstract: Despite the recent success of end-to-end learned representations,\nhand-crafted optical flow features are still widely used in video analysis\ntasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural\nnetwork, to learn optical-flow-like features from data. TVNet subsumes a\nspecific optical flow solver, the TV-L1 method, and is initialized by unfolding\nits optimization iterations as neural layers. TVNet can therefore be used\ndirectly without any extra learning. Moreover, it can be naturally concatenated\nwith other task-specific networks to formulate an end-to-end architecture, thus\nmaking our method more efficient than current multi-stage approaches by\navoiding the need to pre-compute and store features on disk. Finally, the\nparameters of the TVNet can be further fine-tuned by end-to-end training. This\nenables TVNet to learn richer and task-specific patterns beyond exact optical\nflow. Extensive experiments on two action recognition benchmarks verify the\neffectiveness of the proposed approach. Our TVNet achieves better accuracies\nthan all compared methods, while being competitive with the fastest counterpart\nin terms of features extraction time. \n\n"}
{"id": "1804.00516", "contents": "Title: Towards Highly Accurate Coral Texture Images Classification Using Deep\n  Convolutional Neural Networks and Data Augmentation Abstract: The recognition of coral species based on underwater texture images pose a\nsignificant difficulty for machine learning algorithms, due to the three\nfollowing challenges embedded in the nature of this data: 1) datasets do not\ninclude information about the global structure of the coral; 2) several species\nof coral have very similar characteristics; and 3) defining the spatial borders\nbetween classes is difficult as many corals tend to appear together in groups.\nFor this reason, the classification of coral species has always required an aid\nfrom a domain expert. The objective of this paper is to develop an accurate\nclassification model for coral texture images. Current datasets contain a large\nnumber of imbalanced classes, while the images are subject to inter-class\nvariation. We have analyzed 1) several Convolutional Neural Network (CNN)\narchitectures, 2) data augmentation techniques and 3) transfer learning. We\nhave achieved the state-of-the art accuracies using different variations of\nResNet on the two current coral texture datasets, EILAT and RSMAS. \n\n"}
{"id": "1804.00525", "contents": "Title: DeepScores -- A Dataset for Segmentation, Detection and Classification\n  of Tiny Objects Abstract: We present the DeepScores dataset with the goal of advancing the\nstate-of-the-art in small objects recognition, and by placing the question of\nobject recognition in the context of scene understanding. DeepScores contains\nhigh quality images of musical scores, partitioned into 300,000 sheets of\nwritten music that contain symbols of different shapes and sizes. With close to\na hundred millions of small objects, this makes our dataset not only unique,\nbut also the largest public dataset. DeepScores comes with ground truth for\nobject classification, detection and semantic segmentation. DeepScores thus\nposes a relevant challenge for computer vision in general, beyond the scope of\noptical music recognition (OMR) research. We present a detailed statistical\nanalysis of the dataset, comparing it with other computer vision datasets like\nCaltech101/256, PASCAL VOC, SUN, SVHN, ImageNet, MS-COCO, smaller computer\nvision datasets, as well as with other OMR datasets. Finally, we provide\nbaseline performances for object classification and give pointers to future\nresearch based on this dataset. \n\n"}
{"id": "1804.00819", "contents": "Title: End-to-End Dense Video Captioning with Masked Transformer Abstract: Dense video captioning aims to generate text descriptions for all events in\nan untrimmed video. This involves both detecting and describing events.\nTherefore, all previous methods on dense video captioning tackle this problem\nby building two models, i.e. an event proposal and a captioning model, for\nthese two sub-problems. The models are either trained separately or in\nalternation. This prevents direct influence of the language description to the\nevent proposal, which is important for generating accurate descriptions. To\naddress this problem, we propose an end-to-end transformer model for dense\nvideo captioning. The encoder encodes the video into appropriate\nrepresentations. The proposal decoder decodes from the encoding with different\nanchors to form video event proposals. The captioning decoder employs a masking\nnetwork to restrict its attention to the proposal event over the encoding\nfeature. This masking network converts the event proposal to a differentiable\nmask, which ensures the consistency between the proposal and captioning during\ntraining. In addition, our model employs a self-attention mechanism, which\nenables the use of efficient non-recurrent structure during encoding and leads\nto performance improvements. We demonstrate the effectiveness of this\nend-to-end model on ActivityNet Captions and YouCookII datasets, where we\nachieved 10.12 and 6.58 METEOR score, respectively. \n\n"}
{"id": "1804.00887", "contents": "Title: Learning to Guide Decoding for Image Captioning Abstract: Recently, much advance has been made in image captioning, and an\nencoder-decoder framework has achieved outstanding performance for this task.\nIn this paper, we propose an extension of the encoder-decoder framework by\nadding a component called guiding network. The guiding network models the\nattribute properties of input images, and its output is leveraged to compose\nthe input of the decoder at each time step. The guiding network can be plugged\ninto the current encoder-decoder framework and trained in an end-to-end manner.\nHence, the guiding vector can be adaptively learned according to the signal\nfrom the decoder, making itself to embed information from both image and\nlanguage. Additionally, discriminative supervision can be employed to further\nimprove the quality of guidance. The advantages of our proposed approach are\nverified by experiments carried out on the MS COCO dataset. \n\n"}
{"id": "1804.00892", "contents": "Title: When will you do what? - Anticipating Temporal Occurrences of Activities Abstract: Analyzing human actions in videos has gained increased attention recently.\nWhile most works focus on classifying and labeling observed video frames or\nanticipating the very recent future, making long-term predictions over more\nthan just a few seconds is a task with many practical applications that has not\nyet been addressed. In this paper, we propose two methods to predict a\nconsiderably large amount of future actions and their durations. Both, a CNN\nand an RNN are trained to learn future video labels based on previously seen\ncontent. We show that our methods generate accurate predictions of the future\neven for long videos with a huge amount of different actions and can even deal\nwith noisy or erroneous input information. \n\n"}
{"id": "1804.01159", "contents": "Title: Crystal Loss and Quality Pooling for Unconstrained Face Verification and\n  Recognition Abstract: In recent years, the performance of face verification and recognition systems\nbased on deep convolutional neural networks (DCNNs) has significantly improved.\nA typical pipeline for face verification includes training a deep network for\nsubject classification with softmax loss, using the penultimate layer output as\nthe feature descriptor, and generating a cosine similarity score given a pair\nof face images or videos. The softmax loss function does not optimize the\nfeatures to have higher similarity score for positive pairs and lower\nsimilarity score for negative pairs, which leads to a performance gap. In this\npaper, we propose a new loss function, called Crystal Loss, that restricts the\nfeatures to lie on a hypersphere of a fixed radius. The loss can be easily\nimplemented using existing deep learning frameworks. We show that integrating\nthis simple step in the training pipeline significantly improves the\nperformance of face verification and recognition systems. We achieve\nstate-of-the-art performance for face verification and recognition on\nchallenging LFW, IJB-A, IJB-B and IJB-C datasets over a large range of false\nalarm rates (10-1 to 10-7). \n\n"}
{"id": "1804.01417", "contents": "Title: Patch-based Face Recognition using a Hierarchical Multi-label Matcher Abstract: This paper proposes a hierarchical multi-label matcher for patch-based face\nrecognition. In signature generation, a face image is iteratively divided into\nmulti-level patches. Two different types of patch divisions and signatures are\nintroduced for 2D facial image and texture-lifted image, respectively. The\nmatcher training consists of three steps. First, local classifiers are built to\nlearn the local matching of each patch. Second, the hierarchical relationships\ndefined between local patches are used to learn the global matching of each\npatch. Three ways are introduced to learn the global matching: majority voting,\nl1-regularized weighting, and decision rule. Last, the global matchings of\ndifferent levels are combined as the final matching. Experimental results on\ndifferent face recognition tasks demonstrate the effectiveness of the proposed\nmatcher at the cost of gallery generalization. Compared with the UR2D system,\nthe proposed matcher improves the Rank-1 accuracy significantly by 3% and 0.18%\non the UHDB31 dataset and IJB-A dataset, respectively. \n\n"}
{"id": "1804.01983", "contents": "Title: High-dimension Tensor Completion via Gradient-based Optimization Under\n  Tensor-train Format Abstract: Tensor train (TT) decomposition has drawn people's attention due to its\npowerful representation ability and performance stability in high-order\ntensors. In this paper, we propose a novel approach to recover the missing\nentries of incomplete data represented by higher-order tensors. We attempt to\nfind the low-rank TT decomposition of the incomplete data which captures the\nlatent features of the whole data and then reconstruct the missing entries. By\napplying gradient descent algorithms, tensor completion problem is efficiently\nsolved by optimization models. We propose two TT-based algorithms: Tensor Train\nWeighted Optimization (TT-WOPT) and Tensor Train Stochastic Gradient Descent\n(TT-SGD) to optimize TT decomposition factors. In addition, a method named\nVisual Data Tensorization (VDT) is proposed to transform visual data into\nhigher-order tensors, resulting in the performance improvement of our\nalgorithms. The experiments in synthetic data and visual data show high\nefficiency and performance of our algorithms compared to the state-of-the-art\ncompletion algorithms, especially in high-order, high missing rate, and\nlarge-scale tensor completion situations. \n\n"}
{"id": "1804.02156", "contents": "Title: OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition\n  Under Changing Conditions Abstract: Visually recognising a traversed route - regardless of whether seen during\nthe day or night, in clear or inclement conditions, or in summer or winter - is\nan important capability for navigating robots. Since SeqSLAM was introduced in\n2012, a large body of work has followed exploring how robotic systems can use\nthe algorithm to meet the challenges posed by navigation in changing\nenvironmental conditions. The following paper describes OpenSeqSLAM2.0, a fully\nopen source toolbox for visual place recognition under changing conditions.\nBeyond the benefits of open access to the source code, OpenSeqSLAM2.0 provides\na number of tools to facilitate exploration of the visual place recognition\nproblem and interactive parameter tuning. Using the new open source platform,\nit is shown for the first time how comprehensive parameter characterisations\nprovide new insights into many of the system components previously presented in\nad hoc ways and provide users with a guide to what system component options\nshould be used under what circumstances and why. \n\n"}
{"id": "1804.02201", "contents": "Title: Ensemble Manifold Segmentation for Model Distillation and\n  Semi-supervised Learning Abstract: Manifold theory has been the central concept of many learning methods.\nHowever, learning modern CNNs with manifold structures has not raised due\nattention, mainly because of the inconvenience of imposing manifold structures\nonto the architecture of the CNNs. In this paper we present ManifoldNet, a\nnovel method to encourage learning of manifold-aware representations. Our\napproach segments the input manifold into a set of fragments. By assigning the\ncorresponding segmentation id as a pseudo label to every sample, we convert the\nproblem of preserving the local manifold structure into a point-wise\nclassification task. Due to its unsupervised nature, the segmentation tends to\nbe noisy. We mitigate this by introducing ensemble manifold segmentation (EMS).\nEMS accounts for the manifold structure by dividing the training data into an\nensemble of classification training sets that contain samples of local\nproximity. CNNs are trained on these ensembles under a multi-task learning\nframework to conform to the manifold. ManifoldNet can be trained with only the\npseudo labels or together with task-specific labels. We evaluate ManifoldNet on\ntwo different tasks: network imitation (distillation) and semi-supervised\nlearning. Our experiments show that the manifold structures are effectively\nutilized for both unsupervised and semi-supervised learning. \n\n"}
{"id": "1804.02419", "contents": "Title: Image Segmentation Using Subspace Representation and Sparse\n  Decomposition Abstract: Image foreground extraction is a classical problem in image processing and\nvision, with a large range of applications. In this dissertation, we focus on\nthe extraction of text and graphics in mixed-content images, and design novel\napproaches for various aspects of this problem.\n  We first propose a sparse decomposition framework, which models the\nbackground by a subspace containing smooth basis vectors, and foreground as a\nsparse and connected component. We then formulate an optimization framework to\nsolve this problem, by adding suitable regularizations to the cost function to\npromote the desired characteristics of each component. We present two\ntechniques to solve the proposed optimization problem, one based on alternating\ndirection method of multipliers (ADMM), and the other one based on robust\nregression. Promising results are obtained for screen content image\nsegmentation using the proposed algorithm.\n  We then propose a robust subspace learning algorithm for the representation\nof the background component using training images that could contain both\nbackground and foreground components, as well as noise. With the learnt\nsubspace for the background, we can further improve the segmentation results,\ncompared to using a fixed subspace. Lastly, we investigate a different class of\nsignal/image decomposition problem, where only one signal component is active\nat each signal element. In this case, besides estimating each component, we\nneed to find their supports, which can be specified by a binary mask. We\npropose a mixed-integer programming problem, that jointly estimates the two\ncomponents and their supports through an alternating optimization scheme. We\nshow the application of this algorithm on various problems, including image\nsegmentation, video motion segmentation, and also separation of text from\ntextured images. \n\n"}
{"id": "1804.02771", "contents": "Title: Estimating Depth from RGB and Sparse Sensing Abstract: We present a deep model that can accurately produce dense depth maps given an\nRGB image with known depth at a very sparse set of pixels. The model works\nsimultaneously for both indoor/outdoor scenes and produces state-of-the-art\ndense depth maps at nearly real-time speeds on both the NYUv2 and KITTI\ndatasets. We surpass the state-of-the-art for monocular depth estimation even\nwith depth values for only 1 out of every ~10000 image pixels, and we\noutperform other sparse-to-dense depth methods at all sparsity levels. With\ndepth values for 1/256 of the image pixels, we achieve a mean absolute error of\nless than 1% of actual depth on indoor scenes, comparable to the performance of\nconsumer-grade depth sensor hardware. Our experiments demonstrate that it would\nindeed be possible to efficiently transform sparse depth measurements obtained\nusing e.g. lower-power depth sensors or SLAM systems into high-quality dense\ndepth maps. \n\n"}
{"id": "1804.03247", "contents": "Title: Fine-grained Activity Recognition in Baseball Videos Abstract: In this paper, we introduce a challenging new dataset, MLB-YouTube, designed\nfor fine-grained activity detection. The dataset contains two settings:\nsegmented video classification as well as activity detection in continuous\nvideos. We experimentally compare various recognition approaches capturing\ntemporal structure in activity videos, by classifying segmented videos and\nextending those approaches to continuous videos. We also compare models on the\nextremely difficult task of predicting pitch speed and pitch type from\nbroadcast baseball videos. We find that learning temporal structure is valuable\nfor fine-grained activity recognition. \n\n"}
{"id": "1804.03360", "contents": "Title: Reference-Conditioned Super-Resolution by Neural Texture Transfer Abstract: With the recent advancement in deep learning, we have witnessed a great\nprogress in single image super-resolution. However, due to the significant\ninformation loss of the image downscaling process, it has become extremely\nchallenging to further advance the state-of-the-art, especially for large\nupscaling factors. This paper explores a new research direction in super\nresolution, called reference-conditioned super-resolution, in which a reference\nimage containing desired high-resolution texture details is provided besides\nthe low-resolution image. We focus on transferring the high-resolution texture\nfrom reference images to the super-resolution process without the constraint of\ncontent similarity between reference and target images, which is a key\ndifference from previous example-based methods. Inspired by recent work on\nimage stylization, we address the problem via neural texture transfer. We\ndesign an end-to-end trainable deep model which generates detail enriched\nresults by adaptively fusing the content from the low-resolution image with the\ntexture patterns from the reference image. We create a benchmark dataset for\nthe general research of reference-based super-resolution, which contains\nreference images paired with low-resolution inputs with varying degrees of\nsimilarity. Both objective and subjective evaluations demonstrate the great\npotential of using reference images as well as the superiority of our results\nover other state-of-the-art methods. \n\n"}
{"id": "1804.03368", "contents": "Title: Learning Deep Gradient Descent Optimization for Image Deconvolution Abstract: As an integral component of blind image deblurring, non-blind deconvolution\nremoves image blur with a given blur kernel, which is essential but difficult\ndue to the ill-posed nature of the inverse problem. The predominant approach is\nbased on optimization subject to regularization functions that are either\nmanually designed, or learned from examples. Existing learning based methods\nhave shown superior restoration quality but are not practical enough due to\ntheir restricted and static model design. They solely focus on learning a prior\nand require to know the noise level for deconvolution. We address the gap\nbetween the optimization-based and learning-based approaches by learning a\nuniversal gradient descent optimizer. We propose a Recurrent Gradient Descent\nNetwork (RGDN) by systematically incorporating deep neural networks into a\nfully parameterized gradient descent scheme. A hyper-parameter-free update unit\nshared across steps is used to generate updates from the current estimates,\nbased on a convolutional neural network. By training on diverse examples, the\nRecurrent Gradient Descent Network learns an implicit image prior and a\nuniversal update rule through recursive supervision. The learned optimizer can\nbe repeatedly used to improve the quality of diverse degenerated observations.\nThe proposed method possesses strong interpretability and high generalization.\nExtensive experiments on synthetic benchmarks and challenging real-world images\ndemonstrate that the proposed deep optimization method is effective and robust\nto produce favorable results as well as practical for real-world image\ndeblurring applications. \n\n"}
{"id": "1804.03576", "contents": "Title: Large Field and High Resolution: Detecting Needle in Haystack Abstract: The growing use of convolutional neural networks (CNN) for a broad range of\nvisual tasks, including tasks involving fine details, raises the problem of\napplying such networks to a large field of view, since the amount of\ncomputations increases significantly with the number of pixels. To deal\neffectively with this difficulty, we develop and compare methods of using CNNs\nfor the task of small target localization in natural images, given a limited\n\"budget\" of samples to form an image. Inspired in part by human vision, we\ndevelop and compare variable sampling schemes, with peak resolution at the\ncenter and decreasing resolution with eccentricity, applied iteratively by\nre-centering the image at the previous predicted target location. The results\nindicate that variable resolution models significantly outperform constant\nresolution models. Surprisingly, variable resolution models and in particular\nmulti-channel models, outperform the optimal, \"budget-free\" full-resolution\nmodel, using only 5\\% of the samples. \n\n"}
{"id": "1804.03641", "contents": "Title: Audio-Visual Scene Analysis with Self-Supervised Multisensory Features Abstract: The thud of a bouncing ball, the onset of speech as lips open -- when visual\nand audio events occur together, it suggests that there might be a common,\nunderlying event that produced both signals. In this paper, we argue that the\nvisual and audio components of a video signal should be modeled jointly using a\nfused multisensory representation. We propose to learn such a representation in\na self-supervised way, by training a neural network to predict whether video\nframes and audio are temporally aligned. We use this learned representation for\nthree applications: (a) sound source localization, i.e. visualizing the source\nof sound in a video; (b) audio-visual action recognition; and (c) on/off-screen\naudio source separation, e.g. removing the off-screen translator's voice from a\nforeign official's speech. Code, models, and video results are available on our\nwebpage: http://andrewowens.com/multisensory \n\n"}
{"id": "1804.04192", "contents": "Title: Deep Differential Recurrent Neural Networks Abstract: Due to the special gating schemes of Long Short-Term Memory (LSTM), LSTMs\nhave shown greater potential to process complex sequential information than the\ntraditional Recurrent Neural Network (RNN). The conventional LSTM, however,\nfails to take into consideration the impact of salient spatio-temporal dynamics\npresent in the sequential input data. This problem was first addressed by the\ndifferential Recurrent Neural Network (dRNN), which uses a differential gating\nscheme known as Derivative of States (DoS). DoS uses higher orders of internal\nstate derivatives to analyze the change in information gain caused by the\nsalient motions between the successive frames. The weighted combination of\nseveral orders of DoS is then used to modulate the gates in dRNN. While each\nindividual order of DoS is good at modeling a certain level of salient\nspatio-temporal sequences, the sum of all the orders of DoS could distort the\ndetected motion patterns. To address this problem, we propose to control the\nLSTM gates via individual orders of DoS and stack multiple levels of LSTM cells\nin an increasing order of state derivatives. The proposed model progressively\nbuilds up the ability of the LSTM gates to detect salient dynamical patterns in\ndeeper stacked layers modeling higher orders of DoS, and thus the proposed LSTM\nmodel is termed deep differential Recurrent Neural Network (d2RNN). The\neffectiveness of the proposed model is demonstrated on two publicly available\nhuman activity datasets: NUS-HGA and Violent-Flows. The proposed model\noutperforms both LSTM and non-LSTM based state-of-the-art algorithms. \n\n"}
{"id": "1804.04339", "contents": "Title: Benchmark data and method for real-time people counting in cluttered\n  scenes using depth sensors Abstract: Vision-based automatic counting of people has widespread applications in\nintelligent transportation systems, security, and logistics. However, there is\ncurrently no large-scale public dataset for benchmarking approaches on this\nproblem. This work fills this gap by introducing the first real-world RGB-D\nPeople Counting DataSet (PCDS) containing over 4,500 videos recorded at the\nentrance doors of buses in normal and cluttered conditions. It also proposes an\nefficient method for counting people in real-world cluttered scenes related to\npublic transportations using depth videos. The proposed method computes a point\ncloud from the depth video frame and re-projects it onto the ground plane to\nnormalize the depth information. The resulting depth image is analyzed for\nidentifying potential human heads. The human head proposals are meticulously\nrefined using a 3D human model. The proposals in each frame of the continuous\nvideo stream are tracked to trace their trajectories. The trajectories are\nagain refined to ascertain reliable counting. People are eventually counted by\naccumulating the head trajectories leaving the scene. To enable effective head\nand trajectory identification, we also propose two different compound features.\nA thorough evaluation on PCDS demonstrates that our technique is able to count\npeople in cluttered scenes with high accuracy at 45 fps on a 1.7 GHz processor,\nand hence it can be deployed for effective real-time people counting for\nintelligent transportation systems. \n\n"}
{"id": "1804.04458", "contents": "Title: CubeNet: Equivariance to 3D Rotation and Translation Abstract: 3D Convolutional Neural Networks are sensitive to transformations applied to\ntheir input. This is a problem because a voxelized version of a 3D object, and\nits rotated clone, will look unrelated to each other after passing through to\nthe last layer of a network. Instead, an idealized model would preserve a\nmeaningful representation of the voxelized object, while explaining the\npose-difference between the two inputs. An equivariant representation vector\nhas two components: the invariant identity part, and a discernable encoding of\nthe transformation. Models that can't explain pose-differences risk \"diluting\"\nthe representation, in pursuit of optimizing a classification or regression\nloss function.\n  We introduce a Group Convolutional Neural Network with linear equivariance to\ntranslations and right angle rotations in three dimensions. We call this\nnetwork CubeNet, reflecting its cube-like symmetry. By construction, this\nnetwork helps preserve a 3D shape's global and local signature, as it is\ntransformed through successive layers. We apply this network to a variety of 3D\ninference problems, achieving state-of-the-art on the ModelNet10 classification\nchallenge, and comparable performance on the ISBI 2012 Connectome Segmentation\nBenchmark. To the best of our knowledge, this is the first 3D rotation\nequivariant CNN for voxel representations. \n\n"}
{"id": "1804.04539", "contents": "Title: Generative Visual Rationales Abstract: Interpretability and small labelled datasets are key issues in the practical\napplication of deep learning, particularly in areas such as medicine. In this\npaper, we present a semi-supervised technique that addresses both these issues\nby leveraging large unlabelled datasets to encode and decode images into a\ndense latent representation. Using chest radiography as an example, we apply\nthis encoder to other labelled datasets and apply simple models to the latent\nvectors to learn algorithms to identify heart failure.\n  For each prediction, we generate visual rationales by optimizing a latent\nrepresentation to minimize the prediction of disease while constrained by a\nsimilarity measure in image space. Decoding the resultant latent representation\nproduces an image without apparent disease. The difference between the original\ndecoding and the altered image forms an interpretable visual rationale for the\nalgorithm's prediction on that image. We also apply our method to the MNIST\ndataset and compare the generated rationales to other techniques described in\nthe literature. \n\n"}
{"id": "1804.04647", "contents": "Title: An efficient CNN for spectral reconstruction from RGB images Abstract: Recently, the example-based single image spectral reconstruction from RGB\nimages task, aka, spectral super-resolution was approached by means of deep\nlearning by Galliani et al. The proposed very deep convolutional neural network\n(CNN) achieved superior performance on recent large benchmarks. However,\nAeschbacher et al showed that comparable performance can be achieved by shallow\nlearning method based on A+, a method introduced for image super-resolution by\nTimofte et al. In this paper, we propose a moderately deep CNN model and\nsubstantially improve the reported performance on three spectral reconstruction\nstandard benchmarks: ICVL, CAVE, and NUS. \n\n"}
{"id": "1804.04784", "contents": "Title: FishEyeRecNet: A Multi-Context Collaborative Deep Network for Fisheye\n  Image Rectification Abstract: Images captured by fisheye lenses violate the pinhole camera assumption and\nsuffer from distortions. Rectification of fisheye images is therefore a crucial\npreprocessing step for many computer vision applications. In this paper, we\npropose an end-to-end multi-context collaborative deep network for removing\ndistortions from single fisheye images. In contrast to conventional approaches,\nwhich focus on extracting hand-crafted features from input images, our method\nlearns high-level semantics and low-level appearance features simultaneously to\nestimate the distortion parameters. To facilitate training, we construct a\nsynthesized dataset that covers various scenes and distortion parameter\nsettings. Experiments on both synthesized and real-world datasets show that the\nproposed model significantly outperforms current state of the art methods. Our\ncode and synthesized dataset will be made publicly available. \n\n"}
{"id": "1804.05790", "contents": "Title: Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone\n  Image Abstract: We propose a material acquisition approach to recover the spatially-varying\nBRDF and normal map of a near-planar surface from a single image captured by a\nhandheld mobile phone camera. Our method images the surface under arbitrary\nenvironment lighting with the flash turned on, thereby avoiding shadows while\nsimultaneously capturing high-frequency specular highlights. We train a CNN to\nregress an SVBRDF and surface normals from this image. Our network is trained\nusing a large-scale SVBRDF dataset and designed to incorporate physical\ninsights for material estimation, including an in-network rendering layer to\nmodel appearance and a material classifier to provide additional supervision\nduring training. We refine the results from the network using a dense CRF\nmodule whose terms are designed specifically for our task. The framework is\ntrained end-to-end and produces high quality results for a variety of\nmaterials. We provide extensive ablation studies to evaluate our network on\nboth synthetic and real data, while demonstrating significant improvements in\ncomparisons with prior works. \n\n"}
{"id": "1804.05984", "contents": "Title: A Fusion Framework for Camouflaged Moving Foreground Detection in the\n  Wavelet Domain Abstract: Detecting camouflaged moving foreground objects has been known to be\ndifficult due to the similarity between the foreground objects and the\nbackground. Conventional methods cannot distinguish the foreground from\nbackground due to the small differences between them and thus suffer from\nunder-detection of the camouflaged foreground objects. In this paper, we\npresent a fusion framework to address this problem in the wavelet domain. We\nfirst show that the small differences in the image domain can be highlighted in\ncertain wavelet bands. Then the likelihood of each wavelet coefficient being\nforeground is estimated by formulating foreground and background models for\neach wavelet band. The proposed framework effectively aggregates the\nlikelihoods from different wavelet bands based on the characteristics of the\nwavelet transform. Experimental results demonstrated that the proposed method\nsignificantly outperformed existing methods in detecting camouflaged foreground\nobjects. Specifically, the average F-measure for the proposed algorithm was\n0.87, compared to 0.71 to 0.8 for the other state-of-the-art methods. \n\n"}
{"id": "1804.06275", "contents": "Title: Network Signatures from Image Representation of Adjacency Matrices:\n  Deep/Transfer Learning for Subgraph Classification Abstract: We propose a novel subgraph image representation for classification of\nnetwork fragments with the targets being their parent networks. The graph image\nrepresentation is based on 2D image embeddings of adjacency matrices. We use\nthis image representation in two modes. First, as the input to a machine\nlearning algorithm. Second, as the input to a pure transfer learner. Our\nconclusions from several datasets are that (a) deep learning using our\nstructured image features performs the best compared to benchmark graph kernel\nand classical features based methods; and, (b) pure transfer learning works\neffectively with minimum interference from the user and is robust against small\ndata. \n\n"}
{"id": "1804.06353", "contents": "Title: Not-so-supervised: a survey of semi-supervised, multi-instance, and\n  transfer learning in medical image analysis Abstract: Machine learning (ML) algorithms have made a tremendous impact in the field\nof medical imaging. While medical imaging datasets have been growing in size, a\nchallenge for supervised ML algorithms that is frequently mentioned is the lack\nof annotated data. As a result, various methods which can learn with less/other\ntypes of supervision, have been proposed. We review semi-supervised, multiple\ninstance, and transfer learning in medical imaging, both in diagnosis/detection\nor segmentation tasks. We also discuss connections between these learning\nscenarios, and opportunities for future research. \n\n"}
{"id": "1804.06642", "contents": "Title: Superframes, A Temporal Video Segmentation Abstract: The goal of video segmentation is to turn video data into a set of concrete\nmotion clusters that can be easily interpreted as building blocks of the video.\nThere are some works on similar topics like detecting scene cuts in a video,\nbut there is few specific research on clustering video data into the desired\nnumber of compact segments. It would be more intuitive, and more efficient, to\nwork with perceptually meaningful entity obtained from a low-level grouping\nprocess which we call it superframe. This paper presents a new simple and\nefficient technique to detect superframes of similar content patterns in\nvideos. We calculate the similarity of content-motion to obtain the strength of\nchange between consecutive frames. With the help of existing optical flow\ntechnique using deep models, the proposed method is able to perform more\naccurate motion estimation efficiently. We also propose two criteria for\nmeasuring and comparing the performance of different algorithms on various\ndatabases. Experimental results on the videos from benchmark databases have\ndemonstrated the effectiveness of the proposed method. \n\n"}
{"id": "1804.06655", "contents": "Title: Deep Face Recognition: A Survey Abstract: Deep learning applies multiple processing layers to learn representations of\ndata with multiple levels of feature extraction. This emerging technique has\nreshaped the research landscape of face recognition (FR) since 2014, launched\nby the breakthroughs of DeepFace and DeepID. Since then, deep learning\ntechnique, characterized by the hierarchical architecture to stitch together\npixels into invariant face representation, has dramatically improved the\nstate-of-the-art performance and fostered successful real-world applications.\nIn this survey, we provide a comprehensive review of the recent developments on\ndeep FR, covering broad topics on algorithm designs, databases, protocols, and\napplication scenes. First, we summarize different network architectures and\nloss functions proposed in the rapid evolution of the deep FR methods. Second,\nthe related face processing methods are categorized into two classes:\n\"one-to-many augmentation\" and \"many-to-one normalization\". Then, we summarize\nand compare the commonly used databases for both model training and evaluation.\nThird, we review miscellaneous scenes in deep FR, such as cross-factor,\nheterogenous, multiple-media and industrial scenes. Finally, the technical\nchallenges and several promising directions are highlighted. \n\n"}
{"id": "1804.06870", "contents": "Title: Object Ordering with Bidirectional Matchings for Visual Reasoning Abstract: Visual reasoning with compositional natural language instructions, e.g.,\nbased on the newly-released Cornell Natural Language Visual Reasoning (NLVR)\ndataset, is a challenging task, where the model needs to have the ability to\ncreate an accurate mapping between the diverse phrases and the several objects\nplaced in complex arrangements in the image. Further, this mapping needs to be\nprocessed to answer the question in the statement given the ordering and\nrelationship of the objects across three similar images. In this paper, we\npropose a novel end-to-end neural model for the NLVR task, where we first use\njoint bidirectional attention to build a two-way conditioning between the\nvisual information and the language phrases. Next, we use an RL-based pointer\nnetwork to sort and process the varying number of unordered objects (so as to\nmatch the order of the statement phrases) in each of the three images and then\npool over the three decisions. Our model achieves strong improvements (of 4-6%\nabsolute) over the state-of-the-art on both the structured representation and\nraw image versions of the dataset. \n\n"}
{"id": "1804.08328", "contents": "Title: Taskonomy: Disentangling Task Transfer Learning Abstract: Do visual tasks have a relationship, or are they unrelated? For instance,\ncould having surface normals simplify estimating the depth of an image?\nIntuition answers these questions positively, implying existence of a structure\namong visual tasks. Knowing this structure has notable values; it is the\nconcept underlying transfer learning and provides a principled way for\nidentifying redundancies across tasks, e.g., to seamlessly reuse supervision\namong related tasks or solve many tasks in one system without piling up the\ncomplexity.\n  We proposes a fully computational approach for modeling the structure of\nspace of visual tasks. This is done via finding (first and higher-order)\ntransfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D,\nand semantic tasks in a latent space. The product is a computational taxonomic\nmap for task transfer learning. We study the consequences of this structure,\ne.g. nontrivial emerged relationships, and exploit them to reduce the demand\nfor labeled data. For example, we show that the total number of labeled\ndatapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3\n(compared to training independently) while keeping the performance nearly the\nsame. We provide a set of tools for computing and probing this taxonomical\nstructure including a solver that users can employ to devise efficient\nsupervision policies for their use cases. \n\n"}
{"id": "1804.08348", "contents": "Title: Deep Facial Expression Recognition: A Survey Abstract: With the transition of facial expression recognition (FER) from\nlaboratory-controlled to challenging in-the-wild conditions and the recent\nsuccess of deep learning techniques in various fields, deep neural networks\nhave increasingly been leveraged to learn discriminative representations for\nautomatic FER. Recent deep FER systems generally focus on two important issues:\noverfitting caused by a lack of sufficient training data and\nexpression-unrelated variations, such as illumination, head pose and identity\nbias. In this paper, we provide a comprehensive survey on deep FER, including\ndatasets and algorithms that provide insights into these intrinsic problems.\nFirst, we describe the standard pipeline of a deep FER system with the related\nbackground knowledge and suggestions of applicable implementations for each\nstage. We then introduce the available datasets that are widely used in the\nliterature and provide accepted data selection and evaluation principles for\nthese datasets. For the state of the art in deep FER, we review existing novel\ndeep neural networks and related training strategies that are designed for FER\nbased on both static images and dynamic image sequences, and discuss their\nadvantages and limitations. Competitive performances on widely used benchmarks\nare also summarized in this section. We then extend our survey to additional\nrelated issues and application scenarios. Finally, we review the remaining\nchallenges and corresponding opportunities in this field as well as future\ndirections for the design of robust deep FER systems. \n\n"}
{"id": "1804.08497", "contents": "Title: ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning Abstract: The process of aligning a pair of shapes is a fundamental operation in\ncomputer graphics. Traditional approaches rely heavily on matching\ncorresponding points or features to guide the alignment, a paradigm that\nfalters when significant shape portions are missing. These techniques generally\ndo not incorporate prior knowledge about expected shape characteristics, which\ncan help compensate for any misleading cues left by inaccuracies exhibited in\nthe input shapes. We present an approach based on a deep neural network,\nleveraging shape datasets to learn a shape-aware prior for source-to-target\nalignment that is robust to shape incompleteness. In the absence of ground\ntruth alignments for supervision, we train a network on the task of shape\nalignment using incomplete shapes generated from full shapes for\nself-supervision. Our network, called ALIGNet, is trained to warp complete\nsource shapes to incomplete targets, as if the target shapes were complete,\nthus essentially rendering the alignment partial-shape agnostic. We aim for the\nnetwork to develop specialized expertise over the common characteristics of the\nshapes in each dataset, thereby achieving a higher-level understanding of the\nexpected shape space to which a local approach would be oblivious. We constrain\nALIGNet through an anisotropic total variation identity regularization to\npromote piecewise smooth deformation fields, facilitating both partial-shape\nagnosticism and post-deformation applications. We demonstrate that ALIGNet\nlearns to align geometrically distinct shapes, and is able to infer plausible\nmappings even when the target shape is significantly incomplete. We show that\nour network learns the common expected characteristics of shape collections,\nwithout over-fitting or memorization, enabling it to produce plausible\ndeformations on unseen data during test time. \n\n"}
{"id": "1804.08529", "contents": "Title: VectorDefense: Vectorization as a Defense to Adversarial Examples Abstract: Training deep neural networks on images represented as grids of pixels has\nbrought to light an interesting phenomenon known as adversarial examples.\nInspired by how humans reconstruct abstract concepts, we attempt to codify the\ninput bitmap image into a set of compact, interpretable elements to avoid being\nfooled by the adversarial structures. We take the first step in this direction\nby experimenting with image vectorization as an input transformation step to\nmap the adversarial examples back into the natural manifold of MNIST\nhandwritten digits. We compare our method vs. state-of-the-art input\ntransformations and further discuss the trade-offs between a hand-designed and\na learned transformation defense. \n\n"}
{"id": "1804.08831", "contents": "Title: Explaining hyperspectral imaging based plant disease identification: 3D\n  CNN and saliency maps Abstract: Our overarching goal is to develop an accurate and explainable model for\nplant disease identification using hyperspectral data. Charcoal rot is a soil\nborne fungal disease that affects the yield of soybean crops worldwide.\nHyperspectral images were captured at 240 different wavelengths in the range of\n383 - 1032 nm. We developed a 3D Convolutional Neural Network model for soybean\ncharcoal rot disease identification. Our model has classification accuracy of\n95.73\\% and an infected class F1 score of 0.87. We infer the trained model\nusing saliency map and visualize the most sensitive pixel locations that enable\nclassification. The sensitivity of individual wavelengths for classification\nwas also determined using the saliency map visualization. We identify the most\nsensitive wavelength as 733 nm using the saliency map visualization. Since the\nmost sensitive wavelength is in the Near Infrared Region(700 - 1000 nm) of the\nelectromagnetic spectrum, which is also the commonly used spectrum region for\ndetermining the vegetation health of the plant, we were more confident in the\npredictions using our model. \n\n"}
{"id": "1804.09111", "contents": "Title: Structure Aware SLAM using Quadrics and Planes Abstract: Simultaneous Localization And Mapping (SLAM) is a fundamental problem in\nmobile robotics. While point-based SLAM methods provide accurate camera\nlocalization, the generated maps lack semantic information. On the other hand,\nstate of the art object detection methods provide rich information about\nentities present in the scene from a single image. This work marries the two\nand proposes a method for representing generic objects as quadrics which allows\nobject detections to be seamlessly integrated in a SLAM framework. For scene\ncoverage, additional dominant planar structures are modeled as infinite planes.\nExperiments show that the proposed points-planes-quadrics representation can\neasily incorporate Manhattan and object affordance constraints, greatly\nimproving camera localization and leading to semantically meaningful maps. The\nperformance of our SLAM system is demonstrated in https://youtu.be/dR-rB9keF8M . \n\n"}
{"id": "1804.09535", "contents": "Title: Deep Convolutional AutoEncoder-based Lossy Image Compression Abstract: Image compression has been investigated as a fundamental research topic for\nmany decades. Recently, deep learning has achieved great success in many\ncomputer vision tasks, and is gradually being used in image compression. In\nthis paper, we present a lossy image compression architecture, which utilizes\nthe advantages of convolutional autoencoder (CAE) to achieve a high coding\nefficiency. First, we design a novel CAE architecture to replace the\nconventional transforms and train this CAE using a rate-distortion loss\nfunction. Second, to generate a more energy-compact representation, we utilize\nthe principal components analysis (PCA) to rotate the feature maps produced by\nthe CAE, and then apply the quantization and entropy coder to generate the\ncodes. Experimental results demonstrate that our method outperforms traditional\nimage coding algorithms, by achieving a 13.7% BD-rate decrement on the Kodak\ndatabase images compared to JPEG2000. Besides, our method maintains a moderate\ncomplexity similar to JPEG2000. \n\n"}
{"id": "1804.10660", "contents": "Title: Large-Scale Visual Relationship Understanding Abstract: Large scale visual understanding is challenging, as it requires a model to\nhandle the widely-spread and imbalanced distribution of <subject, relation,\nobject> triples. In real-world scenarios with large numbers of objects and\nrelations, some are seen very commonly while others are barely seen. We develop\na new relationship detection model that embeds objects and relations into two\nvector spaces where both discriminative capability and semantic affinity are\npreserved. We learn both a visual and a semantic module that map features from\nthe two modalities into a shared space, where matched pairs of features have to\ndiscriminate against those unmatched, but also maintain close distances to\nsemantically similar ones. Benefiting from that, our model can achieve superior\nperformance even when the visual entity categories scale up to more than\n80,000, with extremely skewed class distribution. We demonstrate the efficacy\nof our model on a large and imbalanced benchmark based of Visual Genome that\ncomprises 53,000+ objects and 29,000+ relations, a scale at which no previous\nwork has ever been evaluated at. We show superiority of our model over\ncarefully designed baselines on the original Visual Genome dataset with 80,000+\ncategories. We also show state-of-the-art performance on the VRD dataset and\nthe scene graph dataset which is a subset of Visual Genome with 200 categories. \n\n"}
{"id": "1804.10844", "contents": "Title: CRAM: Clued Recurrent Attention Model Abstract: To overcome the poor scalability of convolutional neural network, recurrent\nattention model(RAM) selectively choose what and where to look on the image. By\ndirecting recurrent attention model how to look the image, RAM can be even more\nsuccessful in that the given clue narrow down the scope of the possible focus\nzone. In this perspective, this work proposes clued recurrent attention model\n(CRAM) which add clue or constraint on the RAM better problem solving. CRAM\nfollows encoder-decoder framework, encoder utilizes recurrent attention model\nwith spatial transformer network and decoder which varies depending on the\ntask. To ensure the performance, CRAM tackles two computer vision task. One is\nthe image classification task, with clue given as the binary image saliency\nwhich indicates the approximate location of object. The other is the inpainting\ntask, with clue given as binary mask which indicates the occluded part. In both\ntasks, CRAM shows better performance than existing methods showing the\nsuccessful extension of RAM. \n\n"}
{"id": "1804.11191", "contents": "Title: How convolutional neural network see the world - A survey of\n  convolutional neural network visualization methods Abstract: Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive\nperformance on many computer vision related tasks, such as object detection,\nimage recognition, image retrieval, etc. These achievements benefit from the\nCNNs outstanding capability to learn the input features with deep layers of\nneuron structures and iterative training process. However, these learned\nfeatures are hard to identify and interpret from a human vision perspective,\ncausing a lack of understanding of the CNNs internal working mechanism. To\nimprove the CNN interpretability, the CNN visualization is well utilized as a\nqualitative analysis method, which translates the internal features into\nvisually perceptible patterns. And many CNN visualization works have been\nproposed in the literature to interpret the CNN in perspectives of network\nstructure, operation, and semantic concept. In this paper, we expect to provide\na comprehensive survey of several representative CNN visualization methods,\nincluding Activation Maximization, Network Inversion, Deconvolutional Neural\nNetworks (DeconvNet), and Network Dissection based visualization. These methods\nare presented in terms of motivations, algorithms, and experiment results.\nBased on these visualization methods, we also discuss their practical\napplications to demonstrate the significance of the CNN interpretability in\nareas of network design, optimization, security enhancement, etc. \n\n"}
{"id": "1805.00314", "contents": "Title: Object Counts! Bringing Explicit Detections Back into Image Captioning Abstract: The use of explicit object detectors as an intermediate step to image\ncaptioning - which used to constitute an essential stage in early work - is\noften bypassed in the currently dominant end-to-end approaches, where the\nlanguage model is conditioned directly on a mid-level image embedding. We argue\nthat explicit detections provide rich semantic information, and can thus be\nused as an interpretable representation to better understand why end-to-end\nimage captioning systems work well. We provide an in-depth analysis of\nend-to-end image captioning by exploring a variety of cues that can be derived\nfrom such object detections. Our study reveals that end-to-end image captioning\nsystems rely on matching image representations to generate captions, and that\nencoding the frequency, size and position of objects are complementary and all\nplay a role in forming a good image representation. It also reveals that\ndifferent object categories contribute in different ways towards image\ncaptioning. \n\n"}
{"id": "1805.00328", "contents": "Title: 3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object\n  Deformations Abstract: The ability to interact and understand the environment is a fundamental\nprerequisite for a wide range of applications from robotics to augmented\nreality. In particular, predicting how deformable objects will react to applied\nforces in real time is a significant challenge. This is further confounded by\nthe fact that shape information about encountered objects in the real world is\noften impaired by occlusions, noise and missing regions e.g. a robot\nmanipulating an object will only be able to observe a partial view of the\nentire solid. In this work we present a framework, 3D-PhysNet, which is able to\npredict how a three-dimensional solid will deform under an applied force using\nintuitive physics modelling. In particular, we propose a new method to encode\nthe physical properties of the material and the applied force, enabling\ngeneralisation over materials. The key is to combine deep variational\nautoencoders with adversarial training, conditioned on the applied force and\nthe material properties. We further propose a cascaded architecture that takes\na single 2.5D depth view of the object and predicts its deformation. Training\ndata is provided by a physics simulator. The network is fast enough to be used\nin real-time applications from partial views. Experimental results show the\nviability and the generalisation properties of the proposed architecture. \n\n"}
{"id": "1805.00329", "contents": "Title: DeepDIVA: A Highly-Functional Python Framework for Reproducible\n  Experiments Abstract: We introduce DeepDIVA: an infrastructure designed to enable quick and\nintuitive setup of reproducible experiments with a large range of useful\nanalysis functionality. Reproducing scientific results can be a frustrating\nexperience, not only in document image analysis but in machine learning in\ngeneral. Using DeepDIVA a researcher can either reproduce a given experiment\nwith a very limited amount of information or share their own experiments with\nothers. Moreover, the framework offers a large range of functions, such as\nboilerplate code, keeping track of experiments, hyper-parameter optimization,\nand visualization of data and results. To demonstrate the effectiveness of this\nframework, this paper presents case studies in the area of handwritten document\nanalysis where researchers benefit from the integrated functionality. DeepDIVA\nis implemented in Python and uses the deep learning framework PyTorch. It is\ncompletely open source, and accessible as Web Service through DIVAServices. \n\n"}
{"id": "1805.00652", "contents": "Title: MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories\n  and head poses Abstract: Recent approaches on trajectory forecasting use tracklets to predict the\nfuture positions of pedestrians exploiting Long Short Term Memory (LSTM)\narchitectures. This paper shows that adding vislets, that is, short sequences\nof head pose estimations, allows to increase significantly the trajectory\nforecasting performance. We then propose to use vislets in a novel framework\ncalled MX-LSTM, capturing the interplay between tracklets and vislets thanks to\na joint unconstrained optimization of full covariance matrices during the LSTM\nbackpropagation. At the same time, MX-LSTM predicts the future head poses,\nincreasing the standard capabilities of the long-term trajectory forecasting\napproaches. With standard head pose estimators and an attentional-based social\npooling, MX-LSTM scores the new trajectory forecasting state-of-the-art in all\nthe considered datasets (Zara01, Zara02, UCY, and TownCentre) with a dramatic\nmargin when the pedestrians slow down, a case where most of the forecasting\napproaches struggle to provide an accurate solution. \n\n"}
{"id": "1805.01760", "contents": "Title: Facial Landmarks Localization using Cascaded Neural Networks Abstract: The accurate localization of facial landmarks is at the core of face analysis\ntasks, such as face recognition and facial expression analysis, to name a few.\nIn this work, we propose a novel localization approach based on a deep learning\narchitecture that utilizes cascaded subnetworks with convolutional neural\nnetwork units. The cascaded units of the first subnetwork estimate\nheatmap-based encodings of the landmarks locations, while the cascaded units of\nthe second subnetwork receive as input the output of the corresponding heatmap\nestimation units, and refine them through regression. The proposed scheme is\nexperimentally shown to compare favorably with contemporary state-of-the-art\nschemes, especially when applied to images depicting challenging localization\nconditions. \n\n"}
{"id": "1805.02152", "contents": "Title: Quantization Mimic: Towards Very Tiny CNN for Object Detection Abstract: In this paper, we propose a simple and general framework for training very\ntiny CNNs for object detection. Due to limited representation ability, it is\nchallenging to train very tiny networks for complicated tasks like detection.\nTo the best of our knowledge, our method, called Quantization Mimic, is the\nfirst one focusing on very tiny networks. We utilize two types of acceleration\nmethods: mimic and quantization. Mimic improves the performance of a student\nnetwork by transfering knowledge from a teacher network. Quantization converts\na full-precision network to a quantized one without large degradation of\nperformance. If the teacher network is quantized, the search scope of the\nstudent network will be smaller. Using this feature of the quantization, we\npropose Quantization Mimic. It first quantizes the large network, then mimic a\nquantized small network. The quantization operation can help student network to\nbetter match the feature maps from teacher network. To evaluate our approach,\nwe carry out experiments on various popular CNNs including VGG and Resnet, as\nwell as different detection frameworks including Faster R-CNN and R-FCN.\nExperiments on Pascal VOC and WIDER FACE verify that our Quantization Mimic\nalgorithm can be applied on various settings and outperforms state-of-the-art\nmodel acceleration methods given limited computing resouces. \n\n"}
{"id": "1805.02164", "contents": "Title: Multi-Scale Face Restoration with Sequential Gating Ensemble Network Abstract: Restoring face images from distortions is important in face recognition\napplications and is challenged by multiple scale issues, which is still not\nwell-solved in research area. In this paper, we present a Sequential Gating\nEnsemble Network (SGEN) for multi-scale face restoration issue. We first employ\nthe principle of ensemble learning into SGEN architecture design to reinforce\npredictive performance of the network. The SGEN aggregates multi-level\nbase-encoders and base-decoders into the network, which enables the network to\ncontain multiple scales of receptive field. Instead of combining these\nbase-en/decoders directly with non-sequential operations, the SGEN takes\nbase-en/decoders from different levels as sequential data. Specifically, the\nSGEN learns to sequentially extract high level information from base-encoders\nin bottom-up manner and restore low level information from base-decoders in\ntop-down manner. Besides, we propose to realize bottom-up and top-down\ninformation combination and selection with Sequential Gating Unit (SGU). The\nSGU sequentially takes two inputs from different levels and decides the output\nbased on one active input. Experiment results demonstrate that our SGEN is more\neffective at multi-scale human face restoration with more image details and\nless noise than state-of-the-art image restoration models. By using adversarial\ntraining, SGEN also produces more visually preferred results than other models\nthrough subjective evaluation. \n\n"}
{"id": "1805.02335", "contents": "Title: Skeleton-Based Action Recognition with Spatial Reasoning and Temporal\n  Stack Learning Abstract: Skeleton-based action recognition has made great progress recently, but many\nproblems still remain unsolved. For example, most of the previous methods model\nthe representations of skeleton sequences without abundant spatial structure\ninformation and detailed temporal dynamics features. In this paper, we propose\na novel model with spatial reasoning and temporal stack learning (SR-TSL) for\nskeleton based action recognition, which consists of a spatial reasoning\nnetwork (SRN) and a temporal stack learning network (TSLN). The SRN can capture\nthe high-level spatial structural information within each frame by a residual\ngraph neural network, while the TSLN can model the detailed temporal dynamics\nof skeleton sequences by a composition of multiple skip-clip LSTMs. During\ntraining, we propose a clip-based incremental loss to optimize the model. We\nperform extensive experiments on the SYSU 3D Human-Object Interaction dataset\nand NTU RGB+D dataset and verify the effectiveness of each network of our\nmodel. The comparison results illustrate that our approach achieves much better\nresults than state-of-the-art methods. \n\n"}
{"id": "1805.02397", "contents": "Title: A Review on Facial Micro-Expressions Analysis: Datasets, Features and\n  Metrics Abstract: Facial micro-expressions are very brief, spontaneous facial expressions that\nappear on the face of humans when they either deliberately or unconsciously\nconceal an emotion. Micro-expression has shorter duration than\nmacro-expression, which makes it more challenging for human and machine. Over\nthe past ten years, automatic micro-expressions recognition has attracted\nincreasing attention from researchers in psychology, computer science,\nsecurity, neuroscience and other related disciplines. The aim of this paper is\nto provide the insights of automatic micro-expressions and recommendations for\nfuture research. There has been a lot of datasets released over the last decade\nthat facilitated the rapid growth in this field. However, comparison across\ndifferent datasets is difficult due to the inconsistency in experiment\nprotocol, features used and evaluation methods. To address these issues, we\nreview the datasets, features and the performance metrics deployed in the\nliterature. Relevant challenges such as the spatial temporal settings during\ndata collection, emotional classes versus objective classes in data labelling,\nface regions in data analysis, standardisation of metrics and the requirements\nfor real-world implementation are discussed. We conclude by proposing some\npromising future directions to advancing micro-expressions research. \n\n"}
{"id": "1805.02556", "contents": "Title: Relational Network for Skeleton-Based Action Recognition Abstract: With the fast development of effective and low-cost human skeleton capture\nsystems, skeleton-based action recognition has attracted much attention\nrecently. Most existing methods use Convolutional Neural Network (CNN) and\nRecurrent Neural Network (RNN) to extract spatio-temporal information embedded\nin the skeleton sequences for action recognition. However, these approaches are\nlimited in the ability of relational modeling in a single skeleton, due to the\nloss of important structural information when converting the raw skeleton data\nto adapt to the input format of CNN or RNN. In this paper, we propose an\nAttentional Recurrent Relational Network-LSTM (ARRN-LSTM) to simultaneously\nmodel spatial configurations and temporal dynamics in skeletons for action\nrecognition. We introduce the Recurrent Relational Network to learn the spatial\nfeatures in a single skeleton, followed by a multi-layer LSTM to learn the\ntemporal features in the skeleton sequences. Between the two modules, we design\nan adaptive attentional module to focus attention on the most discriminative\nparts in the single skeleton. To exploit the complementarity from different\ngeometries in the skeleton for sufficient relational modeling, we design a\ntwo-stream architecture to learn the structural features among joints and lines\nsimultaneously. Extensive experiments are conducted on several popular skeleton\ndatasets and the results show that the proposed approach achieves better\nresults than most mainstream methods. \n\n"}
{"id": "1805.02838", "contents": "Title: A Memory Network Approach for Story-based Temporal Summarization of\n  360{\\deg} Videos Abstract: We address the problem of story-based temporal summarization of long\n360{\\deg} videos. We propose a novel memory network model named Past-Future\nMemory Network (PFMN), in which we first compute the scores of 81 normal field\nof view (NFOV) region proposals cropped from the input 360{\\deg} video, and\nthen recover a latent, collective summary using the network with two external\nmemories that store the embeddings of previously selected subshots and future\ncandidate subshots. Our major contributions are two-fold. First, our work is\nthe first to address story-based temporal summarization of 360{\\deg} videos.\nSecond, our model is the first attempt to leverage memory networks for video\nsummarization tasks. For evaluation, we perform three sets of experiments.\nFirst, we investigate the view selection capability of our model on the\nPano2Vid dataset. Second, we evaluate the temporal summarization with a newly\ncollected 360{\\deg} video dataset. Finally, we experiment our model's\nperformance in another domain, with image-based storytelling VIST dataset. We\nverify that our model achieves state-of-the-art performance on all the tasks. \n\n"}
{"id": "1805.02901", "contents": "Title: Image Ordinal Classification and Understanding: Grid Dropout with\n  Masking Label Abstract: Image ordinal classification refers to predicting a discrete target value\nwhich carries ordering correlation among image categories. The limited size of\nlabeled ordinal data renders modern deep learning approaches easy to overfit.\nTo tackle this issue, neuron dropout and data augmentation were proposed which,\nhowever, still suffer from over-parameterization and breaking spatial\nstructure, respectively. To address the issues, we first propose a grid dropout\nmethod that randomly dropout/blackout some areas of the raining image. Then we\ncombine the objective of predicting the blackout patches with classification to\ntake advantage of the spatial information. Finally we demonstrate the\neffectiveness of both approaches by visualizing the Class Activation Map (CAM)\nand discover that grid dropout is more aware of the whole facial areas and more\nrobust than neuron dropout for small training dataset. Experiments are\nconducted on a challenging age estimation dataset - Adience dataset with very\ncompetitive results compared with state-of-the-art methods. \n\n"}
{"id": "1805.03096", "contents": "Title: Fast Feature Extraction with CNNs with Pooling Layers Abstract: In recent years, many publications showed that convolutional neural network\nbased features can have a superior performance to engineered features. However,\nnot much effort was taken so far to extract local features efficiently for a\nwhole image. In this paper, we present an approach to compute patch-based local\nfeature descriptors efficiently in presence of pooling and striding layers for\nwhole images at once. Our approach is generic and can be applied to nearly all\nexisting network architectures. This includes networks for all local feature\nextraction tasks like camera calibration, Patchmatching, optical flow\nestimation and stereo matching. In addition, our approach can be applied to\nother patch-based approaches like sliding window object detection and\nrecognition. We complete our paper with a speed benchmark of popular CNN based\nfeature extraction approaches applied on a whole image, with and without our\nspeedup, and example code (for Torch) that shows how an arbitrary CNN\narchitecture can be easily converted by our approach. \n\n"}
{"id": "1805.04239", "contents": "Title: Just-in-Time Reconstruction: Inpainting Sparse Maps using Single View\n  Depth Predictors as Priors Abstract: We present ``just-in-time reconstruction\" as real-time image-guided\ninpainting of a map with arbitrary scale and sparsity to generate a fully dense\ndepth map for the image. In particular, our goal is to inpaint a sparse map ---\nobtained from either a monocular visual SLAM system or a sparse sensor ---\nusing a single-view depth prediction network as a virtual depth sensor. We\nadopt a fairly standard approach to data fusion, to produce a fused depth map\nby performing inference over a novel fully-connected Conditional Random Field\n(CRF) which is parameterized by the input depth maps and their pixel-wise\nconfidence weights. Crucially, we obtain the confidence weights that\nparameterize the CRF model in a data-dependent manner via Convolutional Neural\nNetworks (CNNs) which are trained to model the conditional depth error\ndistributions given each source of input depth map and the associated RGB\nimage. Our CRF model penalises absolute depth error in its nodes and pairwise\nscale-invariant depth error in its edges, and the confidence-based fusion\nminimizes the impact of outlier input depth values on the fused result. We\ndemonstrate the flexibility of our method by real-time inpainting of ORB-SLAM,\nKinect, and LIDAR depth maps acquired both indoors and outdoors at arbitrary\nscale and varied amount of irregular sparsity. \n\n"}
{"id": "1805.04590", "contents": "Title: The Domain Transform Solver Abstract: We present a framework for edge-aware optimization that is an order of\nmagnitude faster than the state of the art while having comparable performance.\nOur key insight is that the optimization can be formulated by leveraging\nproperties of the domain transform, a method for edge-aware filtering that\ndefines a distance-preserving 1D mapping of the input space. This enables our\nmethod to improve performance for a variety of problems including stereo, depth\nsuper-resolution, and render from defocus, while keeping the computational\ncomplexity linear in the number of pixels. Our method is highly parallelizable\nand adaptable, and it has demonstrable scalability with respect to image\nresolution. \n\n"}
{"id": "1805.06558", "contents": "Title: Recurrent Neural Network for Learning DenseDepth and Ego-Motion from\n  Video Abstract: Learning-based, single-view depth estimation often generalizes poorly to\nunseen datasets. While learning-based, two-frame depth estimation solves this\nproblem to some extent by learning to match features across frames, it performs\npoorly at large depth where the uncertainty is high. There exists few\nlearning-based, multi-view depth estimation methods. In this paper, we present\na learning-based, multi-view dense depth map and ego-motion estimation method\nthat uses Recurrent Neural Networks (RNN). Our model is designed for 3D\nreconstruction from video where the input frames are temporally correlated. It\nis generalizable to single- or two-view dense depth estimation. Compared to\nrecent single- or two-view CNN-based depth estimation methods, our model\nleverages more views and achieves more accurate results, especially at large\ndistances. Our method produces superior results to the state-of-the-art\nlearning-based, single- or two-view depth estimation methods on both indoor and\noutdoor benchmark datasets. We also demonstrate that our method can even work\non extremely difficult sequences, such as endoscopic video, where none of the\nassumptions (static scene, constant lighting, Lambertian reflection, etc.) from\ntraditional 3D reconstruction methods hold. \n\n"}
{"id": "1805.06660", "contents": "Title: Single Shot Active Learning using Pseudo Annotators Abstract: Standard myopic active learning assumes that human annotations are always\nobtainable whenever new samples are selected. This, however, is unrealistic in\nmany real-world applications where human experts are not readily available at\nall times. In this paper, we consider the single shot setting: all the required\nsamples should be chosen in a single shot and no human annotation can be\nexploited during the selection process. We propose a new method, Active\nLearning through Random Labeling (ALRL), which substitutes single human\nannotator for multiple, what we will refer to as, pseudo annotators. These\npseudo annotators always provide uniform and random labels whenever new\nunlabeled samples are queried. This random labeling enables standard active\nlearning algorithms to also exhibit the exploratory behavior needed for single\nshot active learning. The exploratory behavior is further enhanced by selecting\nthe most representative sample via minimizing nearest neighbor distance between\nunlabeled samples and queried samples. Experiments on real-world datasets\ndemonstrate that the proposed method outperforms several state-of-the-art\napproaches. \n\n"}
{"id": "1805.06725", "contents": "Title: GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training Abstract: Anomaly detection is a classical problem in computer vision, namely the\ndetermination of the normal from the abnormal when datasets are highly biased\ntowards one class (normal) due to the insufficient sample size of the other\nclass (abnormal). While this can be addressed as a supervised learning problem,\na significantly more challenging problem is that of detecting the\nunknown/unseen anomaly case that takes us instead into the space of a\none-class, semi-supervised learning paradigm. We introduce such a novel anomaly\ndetection model, by using a conditional generative adversarial network that\njointly learns the generation of high-dimensional image space and the inference\nof latent space. Employing encoder-decoder-encoder sub-networks in the\ngenerator network enables the model to map the input image to a lower dimension\nvector, which is then used to reconstruct the generated output image. The use\nof the additional encoder network maps this generated image to its latent\nrepresentation. Minimizing the distance between these images and the latent\nvectors during training aids in learning the data distribution for the normal\nsamples. As a result, a larger distance metric from this learned data\ndistribution at inference time is indicative of an outlier from that\ndistribution - an anomaly. Experimentation over several benchmark datasets,\nfrom varying domains, shows the model efficacy and superiority over previous\nstate-of-the-art approaches. \n\n"}
{"id": "1805.07193", "contents": "Title: The EuroCity Persons Dataset: A Novel Benchmark for Object Detection Abstract: Big data has had a great share in the success of deep learning in computer\nvision. Recent works suggest that there is significant further potential to\nincrease object detection performance by utilizing even bigger datasets. In\nthis paper, we introduce the EuroCity Persons dataset, which provides a large\nnumber of highly diverse, accurate and detailed annotations of pedestrians,\ncyclists and other riders in urban traffic scenes. The images for this dataset\nwere collected on-board a moving vehicle in 31 cities of 12 European countries.\nWith over 238200 person instances manually labeled in over 47300 images,\nEuroCity Persons is nearly one order of magnitude larger than person datasets\nused previously for benchmarking. The dataset furthermore contains a large\nnumber of person orientation annotations (over 211200). We optimize four\nstate-of-the-art deep learning approaches (Faster R-CNN, R-FCN, SSD and YOLOv3)\nto serve as baselines for the new object detection benchmark. In experiments\nwith previous datasets we analyze the generalization capabilities of these\ndetectors when trained with the new dataset. We furthermore study the effect of\nthe training set size, the dataset diversity (day- vs. night-time, geographical\nregion), the dataset detail (i.e. availability of object orientation\ninformation) and the annotation quality on the detector performance. Finally,\nwe analyze error sources and discuss the road ahead. \n\n"}
{"id": "1805.07281", "contents": "Title: An Unsupervised Approach to Solving Inverse Problems using Generative\n  Adversarial Networks Abstract: Solving inverse problems continues to be a challenge in a wide array of\napplications ranging from deblurring, image inpainting, source separation etc.\nMost existing techniques solve such inverse problems by either explicitly or\nimplicitly finding the inverse of the model. The former class of techniques\nrequire explicit knowledge of the measurement process which can be unrealistic,\nand rely on strong analytical regularizers to constrain the solution space,\nwhich often do not generalize well. The latter approaches have had remarkable\nsuccess in part due to deep learning, but require a large collection of\nsource-observation pairs, which can be prohibitively expensive. In this paper,\nwe propose an unsupervised technique to solve inverse problems with generative\nadversarial networks (GANs). Using a pre-trained GAN in the space of source\nsignals, we show that one can reliably recover solutions to under determined\nproblems in a `blind' fashion, i.e., without knowledge of the measurement\nprocess. We solve this by making successive estimates on the model and the\nsolution in an iterative fashion. We show promising results in three\nchallenging applications -- blind source separation, image deblurring, and\nrecovering an image from its edge map, and perform better than several\nbaselines. \n\n"}
{"id": "1805.07468", "contents": "Title: Unsupervised Learning of Neural Networks to Explain Neural Networks Abstract: This paper presents an unsupervised method to learn a neural network, namely\nan explainer, to interpret a pre-trained convolutional neural network (CNN),\ni.e., explaining knowledge representations hidden in middle conv-layers of the\nCNN. Given feature maps of a certain conv-layer of the CNN, the explainer\nperforms like an auto-encoder, which first disentangles the feature maps into\nobject-part features and then inverts object-part features back to features of\nhigher conv-layers of the CNN. More specifically, the explainer contains\ninterpretable conv-layers, where each filter disentangles the representation of\na specific object part from chaotic input feature maps. As a paraphrase of CNN\nfeatures, the disentangled representations of object parts help people\nunderstand the logic inside the CNN. We also learn the explainer to use\nobject-part features to reconstruct features of higher CNN layers, in order to\nminimize loss of information during the feature disentanglement. More\ncrucially, we learn the explainer via network distillation without using any\nannotations of sample labels, object parts, or textures for supervision. We\nhave applied our method to different types of CNNs for evaluation, and\nexplainers have significantly boosted the interpretability of CNN features. \n\n"}
{"id": "1805.07785", "contents": "Title: Conditional Inference in Pre-trained Variational Autoencoders via\n  Cross-coding Abstract: Variational Autoencoders (VAEs) are a popular generative model, but one in\nwhich conditional inference can be challenging. If the decomposition into query\nand evidence variables is fixed, conditional VAEs provide an attractive\nsolution. To support arbitrary queries, one is generally reduced to Markov\nChain Monte Carlo sampling methods that can suffer from long mixing times. In\nthis paper, we propose an idea we term cross-coding to approximate the\ndistribution over the latent variables after conditioning on an evidence\nassignment to some subset of the variables. This allows generating query\nsamples without retraining the full VAE. We experimentally evaluate three\nvariations of cross-coding showing that (i) they can be quickly optimized for\ndifferent decompositions of evidence and query and (ii) they quantitatively and\nqualitatively outperform Hamiltonian Monte Carlo. \n\n"}
{"id": "1805.07857", "contents": "Title: Parallel Transport Convolution: A New Tool for Convolutional Neural\n  Networks on Manifolds Abstract: Convolution has been playing a prominent role in various applications in\nscience and engineering for many years. It is the most important operation in\nconvolutional neural networks. There has been a recent growth of interests of\nresearch in generalizing convolutions on curved domains such as manifolds and\ngraphs. However, existing approaches cannot preserve all the desirable\nproperties of Euclidean convolutions, namely compactly supported filters,\ndirectionality, transferability across different manifolds. In this paper we\ndevelop a new generalization of the convolution operation, referred to as\nparallel transport convolution (PTC), on Riemannian manifolds and their\ndiscrete counterparts. PTC is designed based on the parallel transportation\nwhich is able to translate information along a manifold and to intrinsically\npreserve directionality. PTC allows for the construction of compactly supported\nfilters and is also robust to manifold deformations. This enables us to preform\nwavelet-like operations and to define deep convolutional neural networks on\ncurved domains. \n\n"}
{"id": "1805.08009", "contents": "Title: Object Detection in Equirectangular Panorama Abstract: We introduced a high-resolution equirectangular panorama (360-degree, virtual\nreality) dataset for object detection and propose a multi-projection variant of\nYOLO detector. The main challenge with equirectangular panorama image are i)\nthe lack of annotated training data, ii) high-resolution imagery and iii)\nsevere geometric distortions of objects near the panorama projection poles. In\nthis work, we solve the challenges by i) using training examples available in\nthe \"conventional datasets\" (ImageNet and COCO), ii) employing only\nlow-resolution images that require only moderate GPU computing power and\nmemory, and iii) our multi-projection YOLO handles projection distortions by\nmaking multiple stereographic sub-projections. In our experiments, YOLO\noutperforms the other state-of-art detector, Faster RCNN and our\nmulti-projection YOLO achieves the best accuracy with low-resolution input. \n\n"}
{"id": "1805.08105", "contents": "Title: Comparison of Semantic Segmentation Approaches for Horizon/Sky Line\n  Detection Abstract: Horizon or skyline detection plays a vital role towards mountainous visual\ngeo-localization, however most of the recently proposed visual geo-localization\napproaches rely on \\textbf{user-in-the-loop} skyline detection methods.\nDetecting such a segmenting boundary fully autonomously would definitely be a\nstep forward for these localization approaches. This paper provides a\nquantitative comparison of four such methods for autonomous horizon/sky line\ndetection on an extensive data set. Specifically, we provide the comparison\nbetween four recently proposed segmentation methods; one explicitly targeting\nthe problem of horizon detection\\cite{Ahmad15}, second focused on visual\ngeo-localization but relying on accurate detection of skyline \\cite{Saurer16}\nand other two proposed for general semantic segmentation -- Fully Convolutional\nNetworks (FCN) \\cite{Long15} and SegNet\\cite{Badrinarayanan15}. Each of the\nfirst two methods is trained on a common training set \\cite{Baatz12} comprised\nof about 200 images while models for the third and fourth method are fine tuned\nfor sky segmentation problem through transfer learning using the same data set.\nEach of the method is tested on an extensive test set (about 3K images)\ncovering various challenging geographical, weather, illumination and seasonal\nconditions. We report average accuracy and average absolute pixel error for\neach of the presented formulation. \n\n"}
{"id": "1805.08183", "contents": "Title: Constrained Sparse Subspace Clustering with Side-Information Abstract: Subspace clustering refers to the problem of segmenting high dimensional data\ndrawn from a union of subspaces into the respective subspaces. In some\napplications, partial side-information to indicate \"must-link\" or \"cannot-link\"\nin clustering is available. This leads to the task of subspace clustering with\nside-information. However, in prior work the supervision value of the\nside-information for subspace clustering has not been fully exploited. To this\nend, in this paper, we present an enhanced approach for constrained subspace\nclustering with side-information, termed Constrained Sparse Subspace Clustering\nplus (CSSC+), in which the side-information is used not only in the stage of\nlearning an affinity matrix but also in the stage of spectral clustering.\nMoreover, we propose to estimate clustering accuracy based on the partial\nside-information and theoretically justify the connection to the ground-truth\nclustering accuracy in terms of the Rand index. We conduct experiments on three\ncancer gene expression datasets to validate the effectiveness of our proposals. \n\n"}
{"id": "1805.08841", "contents": "Title: Distribution Matching Losses Can Hallucinate Features in Medical Image\n  Translation Abstract: This paper discusses how distribution matching losses, such as those used in\nCycleGAN, when used to synthesize medical images can lead to mis-diagnosis of\nmedical conditions. It seems appealing to use these new image synthesis methods\nfor translating images from a source to a target domain because they can\nproduce high quality images and some even do not require paired data. However,\nthe basis of how these image translation models work is through matching the\ntranslation output to the distribution of the target domain. This can cause an\nissue when the data provided in the target domain has an over or under\nrepresentation of some classes (e.g. healthy or sick). When the output of an\nalgorithm is a transformed image there are uncertainties whether all known and\nunknown class labels have been preserved or changed. Therefore, we recommend\nthat these translated images should not be used for direct interpretation (e.g.\nby doctors) because they may lead to misdiagnosis of patients based on\nhallucinated image features by an algorithm that matches a distribution.\nHowever there are many recent papers that seem as though this is the goal. \n\n"}
{"id": "1805.09233", "contents": "Title: Segmentation of Liver Lesions with Reduced Complexity Deep Models Abstract: We propose a computationally efficient architecture that learns to segment\nlesions from CT images of the liver. The proposed architecture uses bilinear\ninterpolation with sub-pixel convolution at the last layer to upscale the\ncourse feature in bottle neck architecture. Since bilinear interpolation and\nsub-pixel convolution do not have any learnable parameter, our overall model is\nfaster and occupies less memory footprint than the traditional U-net. We\nevaluate our proposed architecture on the highly competitive dataset of 2017\nLiver Tumor Segmentation (LiTS) Challenge. Our method achieves competitive\nresults while reducing the number of learnable parameters roughly by a factor\nof 13.8 compared to the original UNet model. \n\n"}
{"id": "1805.09662", "contents": "Title: LF-Net: Learning Local Features from Images Abstract: We present a novel deep architecture and a training strategy to learn a local\nfeature pipeline from scratch, using collections of images without the need for\nhuman supervision. To do so we exploit depth and relative camera pose cues to\ncreate a virtual target that the network should achieve on one image, provided\nthe outputs of the network for the other image. While this process is\ninherently non-differentiable, we show that we can optimize the network in a\ntwo-branch setup by confining it to one branch, while preserving\ndifferentiability in the other. We train our method on both indoor and outdoor\ndatasets, with depth data from 3D sensors for the former, and depth estimates\nfrom an off-the-shelf Structure-from-Motion solution for the latter. Our models\noutperform the state of the art on sparse feature matching on both datasets,\nwhile running at 60+ fps for QVGA images. \n\n"}
{"id": "1805.09730", "contents": "Title: Image-to-image translation for cross-domain disentanglement Abstract: Deep image translation methods have recently shown excellent results,\noutputting high-quality images covering multiple modes of the data\ndistribution. There has also been increased interest in disentangling the\ninternal representations learned by deep methods to further improve their\nperformance and achieve a finer control. In this paper, we bridge these two\nobjectives and introduce the concept of cross-domain disentanglement. We aim to\nseparate the internal representation into three parts. The shared part contains\ninformation for both domains. The exclusive parts, on the other hand, contain\nonly factors of variation that are particular to each domain. We achieve this\nthrough bidirectional image translation based on Generative Adversarial\nNetworks and cross-domain autoencoders, a novel network component. Our model\noffers multiple advantages. We can output diverse samples covering multiple\nmodes of the distributions of both domains, perform domain-specific image\ntransfer and interpolation, and cross-domain retrieval without the need of\nlabeled data, only paired images. We compare our model to the state-of-the-art\nin multi-modal image translation and achieve better results for translation on\nchallenging datasets as well as for cross-domain retrieval on realistic\ndatasets. \n\n"}
{"id": "1805.10241", "contents": "Title: SLSDeep: Skin Lesion Segmentation Based on Dilated Residual and Pyramid\n  Pooling Networks Abstract: Skin lesion segmentation (SLS) in dermoscopic images is a crucial task for\nautomated diagnosis of melanoma. In this paper, we present a robust deep\nlearning SLS model, so-called SLSDeep, which is represented as an\nencoder-decoder network. The encoder network is constructed by dilated residual\nlayers, in turn, a pyramid pooling network followed by three convolution layers\nis used for the decoder. Unlike the traditional methods employing a\ncross-entropy loss, we investigated a loss function by combining both Negative\nLog Likelihood (NLL) and End Point Error (EPE) to accurately segment the\nmelanoma regions with sharp boundaries. The robustness of the proposed model\nwas evaluated on two public databases: ISBI 2016 and 2017 for skin lesion\nanalysis towards melanoma detection challenge. The proposed model outperforms\nthe state-of-the-art methods in terms of segmentation accuracy. Moreover, it is\ncapable to segment more than $100$ images of size 384x384 per second on a\nrecent GPU. \n\n"}
{"id": "1805.10558", "contents": "Title: DPW-SDNet: Dual Pixel-Wavelet Domain Deep CNNs for Soft Decoding of\n  JPEG-Compressed Images Abstract: JPEG is one of the widely used lossy compression methods. JPEG-compressed\nimages usually suffer from compression artifacts including blocking and\nblurring, especially at low bit-rates. Soft decoding is an effective solution\nto improve the quality of compressed images without changing codec or\nintroducing extra coding bits. Inspired by the excellent performance of the\ndeep convolutional neural networks (CNNs) on both low-level and high-level\ncomputer vision problems, we develop a dual pixel-wavelet domain deep\nCNNs-based soft decoding network for JPEG-compressed images, namely DPW-SDNet.\nThe pixel domain deep network takes the four downsampled versions of the\ncompressed image to form a 4-channel input and outputs a pixel domain\nprediction, while the wavelet domain deep network uses the 1-level discrete\nwavelet transformation (DWT) coefficients to form a 4-channel input to produce\na DWT domain prediction. The pixel domain and wavelet domain estimates are\ncombined to generate the final soft decoded result. Experimental results\ndemonstrate the superiority of the proposed DPW-SDNet over several\nstate-of-the-art compression artifacts reduction algorithms. \n\n"}
{"id": "1805.10726", "contents": "Title: A Neurobiological Evaluation Metric for Neural Network Model Search Abstract: Neuroscience theory posits that the brain's visual system coarsely identifies\nbroad object categories via neural activation patterns, with similar objects\nproducing similar neural responses. Artificial neural networks also have\ninternal activation behavior in response to stimuli. We hypothesize that\nnetworks exhibiting brain-like activation behavior will demonstrate brain-like\ncharacteristics, e.g., stronger generalization capabilities. In this paper we\nintroduce a human-model similarity (HMS) metric, which quantifies the\nsimilarity of human fMRI and network activation behavior. To calculate HMS,\nrepresentational dissimilarity matrices (RDMs) are created as abstractions of\nactivation behavior, measured by the correlations of activations to stimulus\npairs. HMS is then the correlation between the fMRI RDM and the neural network\nRDM across all stimulus pairs. We test the metric on unsupervised predictive\ncoding networks, which specifically model visual perception, and assess the\nmetric for statistical significance over a large range of hyperparameters. Our\nexperiments show that networks with increased human-model similarity are\ncorrelated with better performance on two computer vision tasks: next frame\nprediction and object matching accuracy. Further, HMS identifies networks with\nhigh performance on both tasks. An unexpected secondary finding is that the\nmetric can be employed during training as an early-stopping mechanism. \n\n"}
{"id": "1805.10881", "contents": "Title: Image Distortion Detection using Convolutional Neural Network Abstract: Image distortion classification and detection is an important task in many\napplications. For example when compressing images, if we know the exact\nlocation of the distortion, then it is possible to re-compress images by\nadjusting the local compression level dynamically. In this paper, we address\nthe problem of detecting the distortion region and classifying the distortion\ntype of a given image. We show that our model significantly outperforms the\nstate-of-the-art distortion classifier, and report accurate detection results\nfor the first time. We expect that such results prove the usefulness of our\napproach in many potential applications such as image compression or distortion\nrestoration. \n\n"}
{"id": "1805.11504", "contents": "Title: Capturing Variabilities from Computed Tomography Images with Generative\n  Adversarial Networks Abstract: With the advent of Deep Learning (DL) techniques, especially Generative\nAdversarial Networks (GANs), data augmentation and generation are quickly\nevolving domains that have raised much interest recently. However, the DL\ntechniques are data demanding and since, medical data is not easily accessible,\nthey suffer from data insufficiency. To deal with this limitation, different\ndata augmentation techniques are used. Here, we propose a novel unsupervised\ndata-driven approach for data augmentation that can generate 2D Computed\nTomography (CT) images using a simple GAN. The generated CT images have good\nglobal and local features of a real CT image and can be used to augment the\ntraining datasets for effective learning. In this proof-of-concept study, we\nshow that our proposed solution using GANs is able to capture some of the\nglobal and local CT variabilities. Our network is able to generate visually\nrealistic CT images and we aim to further enhance its output by scaling it to a\nhigher resolution and potentially from 2D to 3D. \n\n"}
{"id": "1805.11592", "contents": "Title: Playing hard exploration games by watching YouTube Abstract: Deep reinforcement learning methods traditionally struggle with tasks where\nenvironment rewards are particularly sparse. One successful method of guiding\nexploration in these domains is to imitate trajectories provided by a human\ndemonstrator. However, these demonstrations are typically collected under\nartificial conditions, i.e. with access to the agent's exact environment setup\nand the demonstrator's action and reward trajectories. Here we propose a\ntwo-stage method that overcomes these limitations by relying on noisy,\nunaligned footage without access to such data. First, we learn to map unaligned\nvideos from multiple sources to a common representation using self-supervised\nobjectives constructed over both time and modality (i.e. vision and sound).\nSecond, we embed a single YouTube video in this representation to construct a\nreward function that encourages an agent to imitate human gameplay. This method\nof one-shot imitation allows our agent to convincingly exceed human-level\nperformance on the infamously hard exploration games Montezuma's Revenge,\nPitfall! and Private Eye for the first time, even if the agent is not presented\nwith any environment rewards. \n\n"}
{"id": "1805.11685", "contents": "Title: Can DNNs Learn to Lipread Full Sentences? Abstract: Finding visual features and suitable models for lipreading tasks that are\nmore complex than a well-constrained vocabulary has proven challenging. This\npaper explores state-of-the-art Deep Neural Network architectures for\nlipreading based on a Sequence to Sequence Recurrent Neural Network. We report\nresults for both hand-crafted and 2D/3D Convolutional Neural Network visual\nfront-ends, online monotonic attention, and a joint Connectionist Temporal\nClassification-Sequence-to-Sequence loss. The system is evaluated on the\npublicly available TCD-TIMIT dataset, with 59 speakers and a vocabulary of over\n6000 words. Results show a major improvement on a Hidden Markov Model\nframework. A fuller analysis of performance across visemes demonstrates that\nthe network is not only learning the language model, but actually learning to\nlipread. \n\n"}
{"id": "1805.11778", "contents": "Title: Object Detection using Domain Randomization and Generative Adversarial\n  Refinement of Synthetic Images Abstract: In this work, we present an application of domain randomization and\ngenerative adversarial networks (GAN) to train a near real-time object detector\nfor industrial electric parts, entirely in a simulated environment. Large scale\navailability of labelled real world data is typically rare and difficult to\nobtain in many industrial settings. As such here, only a few hundred of\nunlabelled real images are used to train a Cyclic-GAN network, in combination\nwith various degree of domain randomization procedures. We demonstrate that\nthis enables robust translation of synthetic images to the real world domain.\nWe show that a combination of the original synthetic (simulation) and GAN\ntranslated images, when used for training a Mask-RCNN object detection network\nachieves greater than 0.95 mean average precision in detecting and classifying\na collection of industrial electric parts. We evaluate the performance across\ndifferent combinations of training data. \n\n"}
{"id": "1805.11815", "contents": "Title: Enabling Pedestrian Safety using Computer Vision Techniques: A Case\n  Study of the 2018 Uber Inc. Self-driving Car Crash Abstract: Human lives are important. The decision to allow self-driving vehicles\noperate on our roads carries great weight. This has been a hot topic of debate\nbetween policy-makers, technologists and public safety institutions. The recent\nUber Inc. self-driving car crash, resulting in the death of a pedestrian, has\nstrengthened the argument that autonomous vehicle technology is still not ready\nfor deployment on public roads. In this work, we analyze the Uber car crash and\nshed light on the question, \"Could the Uber Car Crash have been avoided?\". We\napply state-of-the-art Computer Vision models to this highly practical\nscenario. More generally, our experimental results are an evaluation of various\nimage enhancement and object recognition techniques for enabling pedestrian\nsafety in low-lighting conditions using the Uber crash as a case study. \n\n"}
{"id": "1805.12177", "contents": "Title: Why do deep convolutional networks generalize so poorly to small image\n  transformations? Abstract: Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to\nsmall image transformations: either because of the convolutional architecture\nor because they were trained using data augmentation. Recently, several authors\nhave shown that this is not the case: small translations or rescalings of the\ninput image can drastically change the network's prediction. In this paper, we\nquantify this phenomena and ask why neither the convolutional architecture nor\ndata augmentation are sufficient to achieve the desired invariance.\nSpecifically, we show that the convolutional architecture does not give\ninvariance since architectures ignore the classical sampling theorem, and data\naugmentation does not give invariance because the CNNs learn to be invariant to\ntransformations only for images that are very similar to typical images from\nthe training set. We discuss two possible solutions to this problem: (1)\nantialiasing the intermediate representations and (2) increasing data\naugmentation and show that they provide only a partial solution at best. Taken\ntogether, our results indicate that the problem of insuring invariance to small\nimage transformations in neural networks while preserving high accuracy remains\nunsolved. \n\n"}
{"id": "1805.12234", "contents": "Title: Collaborative Human-AI (CHAI): Evidence-Based Interpretable Melanoma\n  Classification in Dermoscopic Images Abstract: Automated dermoscopic image analysis has witnessed rapid growth in diagnostic\nperformance. Yet adoption faces resistance, in part, because no evidence is\nprovided to support decisions. In this work, an approach for evidence-based\nclassification is presented. A feature embedding is learned with CNNs,\ntriplet-loss, and global average pooling, and used to classify via kNN search.\nEvidence is provided as both the discovered neighbors, as well as localized\nimage regions most relevant to measuring distance between query and neighbors.\nTo ensure that results are relevant in terms of both label accuracy and human\nvisual similarity for any skill level, a novel hierarchical triplet logic is\nimplemented to jointly learn an embedding according to disease labels and\nnon-expert similarity. Results are improved over baselines trained on disease\nlabels alone, as well as standard multiclass loss. Quantitative relevance of\nresults, according to non-expert similarity, as well as localized image\nregions, are also significantly improved. \n\n"}
{"id": "1805.12462", "contents": "Title: On GANs and GMMs Abstract: A longstanding problem in machine learning is to find unsupervised methods\nthat can learn the statistical structure of high dimensional signals. In recent\nyears, GANs have gained much attention as a possible solution to the problem,\nand in particular have shown the ability to generate remarkably realistic high\nresolution sampled images. At the same time, many authors have pointed out that\nGANs may fail to model the full distribution (\"mode collapse\") and that using\nthe learned models for anything other than generating samples may be very\ndifficult. In this paper, we examine the utility of GANs in learning\nstatistical models of images by comparing them to perhaps the simplest\nstatistical model, the Gaussian Mixture Model. First, we present a simple\nmethod to evaluate generative models based on relative proportions of samples\nthat fall into predetermined bins. Unlike previous automatic methods for\nevaluating models, our method does not rely on an additional neural network nor\ndoes it require approximating intractable computations. Second, we compare the\nperformance of GANs to GMMs trained on the same datasets. While GMMs have\npreviously been shown to be successful in modeling small patches of images, we\nshow how to train them on full sized images despite the high dimensionality.\nOur results show that GMMs can generate realistic samples (although less sharp\nthan those of GANs) but also capture the full distribution, which GANs fail to\ndo. Furthermore, GMMs allow efficient inference and explicit representation of\nthe underlying statistical structure. Finally, we discuss how GMMs can be used\nto generate sharp images. \n\n"}
{"id": "1806.00428", "contents": "Title: A Classification approach towards Unsupervised Learning of Visual\n  Representations Abstract: In this paper, we present a technique for unsupervised learning of visual\nrepresentations. Specifically, we train a model for foreground and background\nclassification task, in the process of which it learns visual representations.\nForeground and background patches for training come af- ter mining for such\npatches from hundreds and thousands of unlabelled videos available on the web\nwhich we ex- tract using a proposed patch extraction algorithm. With- out using\nany supervision, with just using 150, 000 unla- belled videos and the PASCAL\nVOC 2007 dataset, we train a object recognition model that achieves 45.3 mAP\nwhich is close to the best performing unsupervised feature learn- ing technique\nwhereas better than many other proposed al- gorithms. The code for patch\nextraction is implemented in Matlab and available open source at the following\nlink . \n\n"}
{"id": "1806.00578", "contents": "Title: SCAN: Sliding Convolutional Attention Network for Scene Text Recognition Abstract: Scene text recognition has drawn great attentions in the community of\ncomputer vision and artificial intelligence due to its challenges and wide\napplications. State-of-the-art recurrent neural networks (RNN) based models map\nan input sequence to a variable length output sequence, but are usually applied\nin a black box manner and lack of transparency for further improvement, and the\nmaintaining of the entire past hidden states prevents parallel computation in a\nsequence. In this paper, we investigate the intrinsic characteristics of text\nrecognition, and inspired by human cognition mechanisms in reading texts, we\npropose a scene text recognition method with sliding convolutional attention\nnetwork (SCAN). Similar to the eye movement during reading, the process of SCAN\ncan be viewed as an alternation between saccades and visual fixations. Compared\nto the previous recurrent models, computations over all elements of SCAN can be\nfully parallelized during training. Experimental results on several challenging\nbenchmarks, including the IIIT5k, SVT and ICDAR 2003/2013 datasets, demonstrate\nthe superiority of SCAN over state-of-the-art methods in terms of both the\nmodel interpretability and performance. \n\n"}
{"id": "1806.00738", "contents": "Title: Contextualize, Show and Tell: A Neural Visual Storyteller Abstract: We present a neural model for generating short stories from image sequences,\nwhich extends the image description model by Vinyals et al. (Vinyals et al.,\n2015). This extension relies on an encoder LSTM to compute a context vector of\neach story from the image sequence. This context vector is used as the first\nstate of multiple independent decoder LSTMs, each of which generates the\nportion of the story corresponding to each image in the sequence by taking the\nimage embedding as the first input. Our model showed competitive results with\nthe METEOR metric and human ratings in the internal track of the Visual\nStorytelling Challenge 2018. \n\n"}
{"id": "1806.00894", "contents": "Title: Infrastructure Quality Assessment in Africa using Satellite Imagery and\n  Deep Learning Abstract: The UN Sustainable Development Goals allude to the importance of\ninfrastructure quality in three of its seventeen goals. However, monitoring\ninfrastructure quality in developing regions remains prohibitively expensive\nand impedes efforts to measure progress toward these goals. To this end, we\ninvestigate the use of widely available remote sensing data for the prediction\nof infrastructure quality in Africa. We train a convolutional neural network to\npredict ground truth labels from the Afrobarometer Round 6 survey using Landsat\n8 and Sentinel 1 satellite imagery.\n  Our best models predict infrastructure quality with AUROC scores of 0.881 on\nElectricity, 0.862 on Sewerage, 0.739 on Piped Water, and 0.786 on Roads using\nLandsat 8. These performances are significantly better than models that\nleverage OpenStreetMap or nighttime light intensity on the same tasks. We also\ndemonstrate that our trained model can accurately make predictions in an unseen\ncountry after fine-tuning on a small sample of images. Furthermore, the model\ncan be deployed in regions with limited samples to predict infrastructure\noutcomes with higher performance than nearest neighbor spatial interpolation. \n\n"}
{"id": "1806.01013", "contents": "Title: Synthetic data generation for end-to-end thermal infrared tracking Abstract: The usage of both off-the-shelf and end-to-end trained deep networks have\nsignificantly improved performance of visual tracking on RGB videos. However,\nthe lack of large labeled datasets hampers the usage of convolutional neural\nnetworks for tracking in thermal infrared (TIR) images. Therefore, most state\nof the art methods on tracking for TIR data are still based on handcrafted\nfeatures. To address this problem, we propose to use image-to-image translation\nmodels. These models allow us to translate the abundantly available labeled RGB\ndata to synthetic TIR data. We explore both the usage of paired and unpaired\nimage translation models for this purpose. These methods provide us with a\nlarge labeled dataset of synthetic TIR sequences, on which we can train\nend-to-end optimal features for tracking. To the best of our knowledge we are\nthe first to train end-to-end features for TIR tracking. We perform extensive\nexperiments on VOT-TIR2017 dataset. We show that a network trained on a large\ndataset of synthetic TIR data obtains better performance than one trained on\nthe available real TIR data. Combining both data sources leads to further\nimprovement. In addition, when we combine the network with motion features we\noutperform the state of the art with a relative gain of over 10%, clearly\nshowing the efficiency of using synthetic data to train end-to-end TIR\ntrackers. \n\n"}
{"id": "1806.01023", "contents": "Title: Differential Diagnosis for Pancreatic Cysts in CT Scans Using\n  Densely-Connected Convolutional Networks Abstract: The lethal nature of pancreatic ductal adenocarcinoma (PDAC) calls for early\ndifferential diagnosis of pancreatic cysts, which are identified in up to 16%\nof normal subjects, and some of which may develop into PDAC. Previous\ncomputer-aided developments have achieved certain accuracy for classification\non segmented cystic lesions in CT. However, pancreatic cysts have a large\nvariation in size and shape, and the precise segmentation of them remains\nrather challenging, which restricts the computer-aided interpretation of CT\nimages acquired for differential diagnosis. We propose a computer-aided\nframework for early differential diagnosis of pancreatic cysts without\npre-segmenting the lesions using densely-connected convolutional networks\n(Dense-Net). The Dense-Net learns high-level features from whole abnormal\npancreas and builds mappings between medical imaging appearance to different\npathological types of pancreatic cysts. To enhance the clinical applicability,\nwe integrate saliency maps in the framework to assist the physicians to\nunderstand the decision of the deep learning method. The test on a cohort of\n206 patients with 4 pathologically confirmed subtypes of pancreatic cysts has\nachieved an overall accuracy of 72.8%, which is significantly higher than the\nbaseline accuracy of 48.1%, which strongly supports the clinical potential of\nour developed method. \n\n"}
{"id": "1806.01320", "contents": "Title: Cube Padding for Weakly-Supervised Saliency Prediction in 360{\\deg}\n  Videos Abstract: Automatic saliency prediction in 360{\\deg} videos is critical for viewpoint\nguidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal\nnetwork which is (1) weakly-supervised trained and (2) tailor-made for\n360{\\deg} viewing sphere. Note that most existing methods are less scalable\nsince they rely on annotated saliency map for training. Most importantly, they\nconvert 360{\\deg} sphere to 2D images (e.g., a single equirectangular image or\nmultiple separate Normal Field-of-View (NFoV) images) which introduces\ndistortion and image boundaries. In contrast, we propose a simple and effective\nCube Padding (CP) technique as follows. Firstly, we render the 360{\\deg} view\non six faces of a cube using perspective projection. Thus, it introduces very\nlittle distortion. Then, we concatenate all six faces while utilizing the\nconnectivity between faces on the cube for image padding (i.e., Cube Padding)\nin convolution, pooling, convolutional LSTM layers. In this way, CP introduces\nno image boundary while being applicable to almost all Convolutional Neural\nNetwork (CNN) structures. To evaluate our method, we propose Wild-360, a new\n360{\\deg} video saliency dataset, containing challenging videos with saliency\nheatmap annotations. In experiments, our method outperforms baseline methods in\nboth speed and quality. \n\n"}
{"id": "1806.01376", "contents": "Title: Factorized Adversarial Networks for Unsupervised Domain Adaptation Abstract: In this paper, we propose Factorized Adversarial Networks (FAN) to solve\nunsupervised domain adaptation problems for image classification tasks. Our\nnetworks map the data distribution into a latent feature space, which is\nfactorized into a domain-specific subspace that contains domain-specific\ncharacteristics and a task-specific subspace that retains category information,\nfor both source and target domains, respectively. Unsupervised domain\nadaptation is achieved by adversarial training to minimize the discrepancy\nbetween the distributions of two task-specific subspaces from source and target\ndomains. We demonstrate that the proposed approach outperforms state-of-the-art\nmethods on multiple benchmark datasets used in the literature for unsupervised\ndomain adaptation. Furthermore, we collect two real-world tagging datasets that\nare much larger than existing benchmark datasets, and get significant\nimprovement upon baselines, proving the practical value of our approach. \n\n"}
{"id": "1806.01954", "contents": "Title: Mining for meaning: from vision to language through multiple networks\n  consensus Abstract: Describing visual data into natural language is a very challenging task, at\nthe intersection of computer vision, natural language processing and machine\nlearning. Language goes well beyond the description of physical objects and\ntheir interactions and can convey the same abstract idea in many ways. It is\nboth about content at the highest semantic level as well as about fluent form.\nHere we propose an approach to describe videos in natural language by reaching\na consensus among multiple encoder-decoder networks. Finding such a consensual\nlinguistic description, which shares common properties with a larger group, has\na better chance to convey the correct meaning. We propose and train several\nnetwork architectures and use different types of image, audio and video\nfeatures. Each model produces its own description of the input video and the\nbest one is chosen through an efficient, two-phase consensus process. We\ndemonstrate the strength of our approach by obtaining state of the art results\non the challenging MSR-VTT dataset. \n\n"}
{"id": "1806.02323", "contents": "Title: Fast and Accurate Online Video Object Segmentation via Tracking Parts Abstract: Online video object segmentation is a challenging task as it entails to\nprocess the image sequence timely and accurately. To segment a target object\nthrough the video, numerous CNN-based methods have been developed by heavily\nfinetuning on the object mask in the first frame, which is time-consuming for\nonline applications. In this paper, we propose a fast and accurate video object\nsegmentation algorithm that can immediately start the segmentation process once\nreceiving the images. We first utilize a part-based tracking method to deal\nwith challenging factors such as large deformation, occlusion, and cluttered\nbackground. Based on the tracked bounding boxes of parts, we construct a\nregion-of-interest segmentation network to generate part masks. Finally, a\nsimilarity-based scoring function is adopted to refine these object parts by\ncomparing them to the visual information in the first frame. Our method\nperforms favorably against state-of-the-art algorithms in accuracy on the DAVIS\nbenchmark dataset, while achieving much faster runtime performance. \n\n"}
{"id": "1806.02612", "contents": "Title: Dimensionality-Driven Learning with Noisy Labels Abstract: Datasets with significant proportions of noisy (incorrect) class labels\npresent challenges for training accurate Deep Neural Networks (DNNs). We\npropose a new perspective for understanding DNN generalization for such\ndatasets, by investigating the dimensionality of the deep representation\nsubspace of training samples. We show that from a dimensionality perspective,\nDNNs exhibit quite distinctive learning styles when trained with clean labels\nversus when trained with a proportion of noisy labels. Based on this finding,\nwe develop a new dimensionality-driven learning strategy, which monitors the\ndimensionality of subspaces during training and adapts the loss function\naccordingly. We empirically demonstrate that our approach is highly tolerant to\nsignificant proportions of noisy labels, and can effectively learn\nlow-dimensional local subspaces that capture the data distribution. \n\n"}
{"id": "1806.02888", "contents": "Title: Correspondence of Deep Neural Networks and the Brain for Visual Textures Abstract: Deep convolutional neural networks (CNNs) trained on objects and scenes have\nshown intriguing ability to predict some response properties of visual cortical\nneurons. However, the factors and computations that give rise to such ability,\nand the role of intermediate processing stages in explaining changes that\ndevelop across areas of the cortical hierarchy, are poorly understood. We\nfocused on the sensitivity to textures as a paradigmatic example, since recent\nneurophysiology experiments provide rich data pointing to texture sensitivity\nin secondary but not primary visual cortex. We developed a quantitative\napproach for selecting a subset of the neural unit population from the CNN that\nbest describes the brain neural recordings. We found that the first two layers\nof the CNN showed qualitative and quantitative correspondence to the cortical\ndata across a number of metrics. This compatibility was reduced for the\narchitecture alone rather than the learned weights, for some other related\nhierarchical models, and only mildly in the absence of a nonlinear computation\nakin to local divisive normalization. Our results show that the CNN class of\nmodel is effective for capturing changes that develop across early areas of\ncortex, and has the potential to facilitate understanding of the computations\nthat give rise to hierarchical processing in the brain. \n\n"}
{"id": "1806.02892", "contents": "Title: Training Faster by Separating Modes of Variation in Batch-normalized\n  Models Abstract: Batch Normalization (BN) is essential to effectively train state-of-the-art\ndeep Convolutional Neural Networks (CNN). It normalizes inputs to the layers\nduring training using the statistics of each mini-batch. In this work, we study\nBN from the viewpoint of Fisher kernels. We show that assuming samples within a\nmini-batch are from the same probability density function, then BN is identical\nto the Fisher vector of a Gaussian distribution. That means BN can be explained\nin terms of kernels that naturally emerge from the probability density function\nof the underlying data distribution. However, given the rectifying\nnon-linearities employed in CNN architectures, distribution of inputs to the\nlayers show heavy tail and asymmetric characteristics. Therefore, we propose\napproximating underlying data distribution not with one, but a mixture of\nGaussian densities. Deriving Fisher vector for a Gaussian Mixture Model (GMM),\nreveals that BN can be improved by independently normalizing with respect to\nthe statistics of disentangled sub-populations. We refer to our proposed soft\npiecewise version of BN as Mixture Normalization (MN). Through extensive set of\nexperiments on CIFAR-10 and CIFAR-100, we show that MN not only effectively\naccelerates training image classification and Generative Adversarial networks,\nbut also reaches higher quality models. \n\n"}
{"id": "1806.03361", "contents": "Title: A Content-Based Late Fusion Approach Applied to Pedestrian Detection Abstract: The variety of pedestrians detectors proposed in recent years has encouraged\nsome works to fuse pedestrian detectors to achieve a more accurate detection.\nThe intuition behind is to combine the detectors based on its spatial\nconsensus. We propose a novel method called Content-Based Spatial Consensus\n(CSBC), which, in addition to relying on spatial consensus, considers the\ncontent of the detection windows to learn a weighted-fusion of pedestrian\ndetectors. The result is a reduction in false alarms and an enhancement in the\ndetection. In this work, we also demonstrate that there is small influence of\nthe feature used to learn the contents of the windows of each detector, which\nenables our method to be efficient even employing simple features. The CSBC\novercomes state-of-the-art fusion methods in the ETH dataset and in the Caltech\ndataset. Particularly, our method is more efficient since fewer detectors are\nnecessary to achieve expressive results. \n\n"}
{"id": "1806.03574", "contents": "Title: FMHash: Deep Hashing of In-Air-Handwriting for User Identification Abstract: Many mobile systems and wearable devices, such as Virtual Reality (VR) or\nAugmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID\nand password for signing into a virtual website. However, they are usually\nequipped with gesture capture interfaces to allow the user to interact with the\nsystem directly with hand gestures. Although gesture-based authentication has\nbeen well-studied, less attention is paid to the gesture-based user\nidentification problem, which is essentially an input method of account ID and\nan efficient searching and indexing method of a database of gesture signals. In\nthis paper, we propose FMHash (i.e., Finger Motion Hash), a user identification\nframework that can generate a compact binary hash code from a piece of\nin-air-handwriting of an ID string. This hash code enables indexing and fast\nsearch of a large account database using the in-air-handwriting by a hash\ntable. To demonstrate the effectiveness of the framework, we implemented a\nprototype and achieved >99.5% precision and >92.6% recall with exact hash code\nmatch on a dataset of 200 accounts collected by us. The ability of hashing\nin-air-handwriting pattern to binary code can be used to achieve convenient\nsign-in and sign-up with in-air-handwriting gesture ID on future mobile and\nwearable systems connected to the Internet. \n\n"}
{"id": "1806.03891", "contents": "Title: Multi-Task Deep Networks for Depth-Based 6D Object Pose and Joint\n  Registration in Crowd Scenarios Abstract: In bin-picking scenarios, multiple instances of an object of interest are\nstacked in a pile randomly, and hence, the instances are inherently subjected\nto the challenges: severe occlusion, clutter, and similar-looking distractors.\nMost existing methods are, however, for single isolated object instances, while\nsome recent methods tackle crowd scenarios as post-refinement which accounts\nmultiple object relations. In this paper, we address recovering 6D poses of\nmultiple instances in bin-picking scenarios in depth modality by multi-task\nlearning in deep neural networks. Our architecture jointly learns multiple\nsub-tasks: 2D detection, depth, and 3D pose estimation of individual objects;\nand joint registration of multiple objects. For training data generation, depth\nimages of physically plausible object pose configurations are generated by a 3D\nobject model in a physics simulation, which yields diverse occlusion patterns\nto learn. We adopt a state-of-the-art object detector, and 2D offsets are\nfurther estimated via a network to refine misaligned 2D detections. The depth\nand 3D pose estimator is designed to generate multiple hypotheses per\ndetection. This allows the joint registration network to learn occlusion\npatterns and remove physically implausible pose hypotheses. We apply our\narchitecture on both synthetic (our own and Sileane dataset) and real (a public\nBin-Picking dataset) data, showing that it significantly outperforms\nstate-of-the-art methods by 15-31% in average precision. \n\n"}
{"id": "1806.03905", "contents": "Title: Retinal Optic Disc Segmentation using Conditional Generative Adversarial\n  Network Abstract: This paper proposed a retinal image segmentation method based on conditional\nGenerative Adversarial Network (cGAN) to segment optic disc. The proposed model\nconsists of two successive networks: generator and discriminator. The generator\nlearns to map information from the observing input (i.e., retinal fundus color\nimage), to the output (i.e., binary mask). Then, the discriminator learns as a\nloss function to train this mapping by comparing the ground-truth and the\npredicted output with observing the input image as a condition.Experiments were\nperformed on two publicly available dataset; DRISHTI GS1 and RIM-ONE. The\nproposed model outperformed state-of-the-art-methods by achieving around 0.96%\nand 0.98% of Jaccard and Dice coefficients, respectively. Moreover, an image\nsegmentation is performed in less than a second on recent GPU. \n\n"}
{"id": "1806.03981", "contents": "Title: Rethinking Radiology: An Analysis of Different Approaches to BraTS Abstract: This paper discusses the deep learning architectures currently used for\npixel-wise segmentation of primary and secondary glioblastomas and low-grade\ngliomas. We implement various models such as the popular UNet architecture and\ncompare the performance of these implementations on the BRATS dataset. This\npaper will explore the different approaches and combinations, offering an in\ndepth discussion of how they perform and how we may improve upon them using\nmore recent advancements in deep learning architectures. \n\n"}
{"id": "1806.04259", "contents": "Title: Improving Whole Slide Segmentation Through Visual Context - A Systematic\n  Study Abstract: While challenging, the dense segmentation of histology images is a necessary\nfirst step to assess changes in tissue architecture and cellular morphology.\nAlthough specific convolutional neural network architectures have been applied\nwith great success to the problem, few effectively incorporate visual context\ninformation from multiple scales. With this paper, we present a systematic\ncomparison of different architectures to assess how including multi-scale\ninformation affects segmentation performance. A publicly available breast\ncancer and a locally collected prostate cancer datasets are being utilised for\nthis study. The results support our hypothesis that visual context and scale\nplay a crucial role in histology image classification problems. \n\n"}
{"id": "1806.04618", "contents": "Title: Imperfect Segmentation Labels: How Much Do They Matter? Abstract: Labeled datasets for semantic segmentation are imperfect, especially in\nmedical imaging where borders are often subtle or ill-defined. Little work has\nbeen done to analyze the effect that label errors have on the performance of\nsegmentation methodologies. Here we present a large-scale study of model\nperformance in the presence of varying types and degrees of error in training\ndata. We trained U-Net, SegNet, and FCN32 several times for liver segmentation\nwith 10 different modes of ground-truth perturbation. Our results show that for\neach architecture, performance steadily declines with boundary-localized\nerrors, however, U-Net was significantly more robust to jagged boundary errors\nthan the other architectures. We also found that each architecture was very\nrobust to non-boundary-localized errors, suggesting that boundary-localized\nerrors are fundamentally different and more challenging problem than random\nlabel errors in a classification setting. \n\n"}
{"id": "1806.04860", "contents": "Title: Learning Visual Knowledge Memory Networks for Visual Question Answering Abstract: Visual question answering (VQA) requires joint comprehension of images and\nnatural language questions, where many questions can't be directly or clearly\nanswered from visual content but require reasoning from structured human\nknowledge with confirmation from visual content. This paper proposes visual\nknowledge memory network (VKMN) to address this issue, which seamlessly\nincorporates structured human knowledge and deep visual features into memory\nnetworks in an end-to-end learning framework. Comparing to existing methods for\nleveraging external knowledge for supporting VQA, this paper stresses more on\ntwo missing mechanisms. First is the mechanism for integrating visual contents\nwith knowledge facts. VKMN handles this issue by embedding knowledge triples\n(subject, relation, target) and deep visual features jointly into the visual\nknowledge features. Second is the mechanism for handling multiple knowledge\nfacts expanding from question and answer pairs. VKMN stores joint embedding\nusing key-value pair structure in the memory networks so that it is easy to\nhandle multiple facts. Experiments show that the proposed method achieves\npromising results on both VQA v1.0 and v2.0 benchmarks, while outperforms\nstate-of-the-art methods on the knowledge-reasoning related questions. \n\n"}
{"id": "1806.04935", "contents": "Title: Convolutional sparse coding for capturing high speed video content Abstract: Video capture is limited by the trade-off between spatial and temporal\nresolution: when capturing videos of high temporal resolution, the spatial\nresolution decreases due to bandwidth limitations in the capture system.\nAchieving both high spatial and temporal resolution is only possible with\nhighly specialized and very expensive hardware, and even then the same basic\ntrade-off remains. The recent introduction of compressive sensing and sparse\nreconstruction techniques allows for the capture of single-shot high-speed\nvideo, by coding the temporal information in a single frame, and then\nreconstructing the full video sequence from this single coded image and a\ntrained dictionary of image patches. In this paper, we first analyze this\napproach, and find insights that help improve the quality of the reconstructed\nvideos. We then introduce a novel technique, based on convolutional sparse\ncoding (CSC), and show how it outperforms the state-of-the-art, patch-based\napproach in terms of flexibility and efficiency, due to the convolutional\nnature of its filter banks. The key idea for CSC high-speed video acquisition\nis extending the basic formulation by imposing an additional constraint in the\ntemporal dimension, which enforces sparsity of the first-order derivatives over\ntime. \n\n"}
{"id": "1806.05091", "contents": "Title: Estimating Achilles tendon healing progress with convolutional neural\n  networks Abstract: Quantitative assessment of a treatment progress in the Achilles tendon\nhealing process - one of the most common musculoskeletal disorder in modern\nmedical practice - is typically a long and complex process: multiple MRI\nprotocols need to be acquired and analysed by radiology experts. In this paper,\nwe propose to significantly reduce the complexity of this assessment using a\nnovel method based on a pre-trained convolutional neural network. We first\ntrain our neural network on over 500,000 2D axial cross-sections from over 3000\n3D MRI studies to classify MRI images as belonging to a healthy or injured\nclass, depending on the patient's condition. We then take the outputs of\nmodified pre-trained network and apply linear regression on the PCA-reduced\nspace of the features to assess treatment progress. Our method allows to reduce\nup to 5-fold the amount of data needed to be registered during the MRI scan\nwithout any information loss. Furthermore, we are able to predict the healing\nprocess phase with equal accuracy to human experts in 3 out of 6 main criteria.\nFinally, contrary to the current approaches to regeneration assessment that\nrely on radiologist subjective opinion, our method allows to objectively\ncompare different treatments methods which can lead to improved diagnostics and\npatient's recovery. \n\n"}
{"id": "1806.05337", "contents": "Title: Hierarchical interpretations for neural network predictions Abstract: Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise. \n\n"}
{"id": "1806.05525", "contents": "Title: EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane\n  Detection Abstract: Convolutional neural networks have been successfully applied to semantic\nsegmentation problems. However, there are many problems that are inherently not\npixel-wise classification problems but are nevertheless frequently formulated\nas semantic segmentation. This ill-posed formulation consequently necessitates\nhand-crafted scenario-specific and computationally expensive post-processing\nmethods to convert the per pixel probability maps to final desired outputs.\nGenerative adversarial networks (GANs) can be used to make the semantic\nsegmentation network output to be more realistic or better\nstructure-preserving, decreasing the dependency on potentially complex\npost-processing. In this work, we propose EL-GAN: a GAN framework to mitigate\nthe discussed problem using an embedding loss. With EL-GAN, we discriminate\nbased on learned embeddings of both the labels and the prediction at the same\ntime. This results in more stable training due to having better discriminative\ninformation, benefiting from seeing both `fake' and `real' predictions at the\nsame time. This substantially stabilizes the adversarial training process. We\nuse the TuSimple lane marking challenge to demonstrate that with our proposed\nframework it is viable to overcome the inherent anomalies of posing it as a\nsemantic segmentation problem. Not only is the output considerably more similar\nto the labels when compared to conventional methods, the subsequent\npost-processing is also simpler and crosses the competitive 96% accuracy\nthreshold. \n\n"}
{"id": "1806.05570", "contents": "Title: Direct Automated Quantitative Measurement of Spine via Cascade Amplifier\n  Regression Network Abstract: Automated quantitative measurement of the spine (i.e., multiple indices\nestimation of heights, widths, areas, and so on for the vertebral body and\ndisc) is of the utmost importance in clinical spinal disease diagnoses, such as\nosteoporosis, intervertebral disc degeneration, and lumbar disc herniation, yet\nstill an unprecedented challenge due to the variety of spine structure and the\nhigh dimensionality of indices to be estimated. In this paper, we propose a\nnovel cascade amplifier regression network (CARN), which includes the CARN\narchitecture and local shape-constrained manifold regularization (LSCMR) loss\nfunction, to achieve accurate direct automated multiple indices estimation. The\nCARN architecture is composed of a cascade amplifier network (CAN) for\nexpressive feature embedding and a linear regression model for multiple indices\nestimation. The CAN consists of cascade amplifier units (AUs), which are used\nfor selective feature reuse by stimulating effective feature and suppressing\nredundant feature during propagating feature map between adjacent layers, thus\nan expressive feature embedding is obtained. During training, the LSCMR is\nutilized to alleviate overfitting and generate realistic estimation by learning\nthe multiple indices distribution. Experiments on MR images of 195 subjects\nshow that the proposed CARN achieves impressive performance with mean absolute\nerrors of 1.2496 mm, 1.2887 mm, and 1.2692 mm for estimation of 15 heights of\ndiscs, 15 heights of vertebral bodies, and total indices respectively. The\nproposed method has great potential in clinical spinal disease diagnoses. \n\n"}
{"id": "1806.05653", "contents": "Title: HGR-Net: A Fusion Network for Hand Gesture Segmentation and Recognition Abstract: We propose a two-stage convolutional neural network (CNN) architecture for\nrobust recognition of hand gestures, called HGR-Net, where the first stage\nperforms accurate semantic segmentation to determine hand regions, and the\nsecond stage identifies the gesture. The segmentation stage architecture is\nbased on the combination of fully convolutional residual network and atrous\nspatial pyramid pooling. Although the segmentation sub-network is trained\nwithout depth information, it is particularly robust against challenges such as\nillumination variations and complex backgrounds. The recognition stage deploys\na two-stream CNN, which fuses the information from the red-green-blue and\nsegmented images by combining their deep representations in a fully connected\nlayer before classification. Extensive experiments on public datasets show that\nour architecture achieves almost as good as state-of-the-art performance in\nsegmentation and recognition of static hand gestures, at a fraction of training\ntime, run time, and model size. Our method can operate at an average of 23 ms\nper frame. \n\n"}
{"id": "1806.05978", "contents": "Title: Uncertainty Estimations by Softplus normalization in Bayesian\n  Convolutional Neural Networks with Variational Inference Abstract: We introduce a novel uncertainty estimation for classification tasks for\nBayesian convolutional neural networks with variational inference. By\nnormalizing the output of a Softplus function in the final layer, we estimate\naleatoric and epistemic uncertainty in a coherent manner. The intractable\nposterior probability distributions over weights are inferred by Bayes by\nBackprop. Firstly, we demonstrate how this reliable variational inference\nmethod can serve as a fundamental construct for various network architectures.\nOn multiple datasets in supervised learning settings (MNIST, CIFAR-10,\nCIFAR-100), this variational inference method achieves performances equivalent\nto frequentist inference in identical architectures, while the two desiderata,\na measure for uncertainty and regularization are incorporated naturally.\nSecondly, we examine how our proposed measure for aleatoric and epistemic\nuncertainties is derived and validate it on the aforementioned datasets. \n\n"}
{"id": "1806.06029", "contents": "Title: One-Shot Unsupervised Cross Domain Translation Abstract: Given a single image x from domain A and a set of images from domain B, our\ntask is to generate the analogous of x in B. We argue that this task could be a\nkey AI capability that underlines the ability of cognitive agents to act in the\nworld and present empirical evidence that the existing unsupervised domain\ntranslation methods fail on this task. Our method follows a two step process.\nFirst, a variational autoencoder for domain B is trained. Then, given the new\nsample x, we create a variational autoencoder for domain A by adapting the\nlayers that are close to the image in order to directly fit x, and only\nindirectly adapt the other layers. Our experiments indicate that the new method\ndoes as well, when trained on one sample x, as the existing domain transfer\nmethods, when these enjoy a multitude of training samples from domain A. Our\ncode is made publicly available at\nhttps://github.com/sagiebenaim/OneShotTranslation \n\n"}
{"id": "1806.06053", "contents": "Title: Deep Lip Reading: a comparison of models and an online application Abstract: The goal of this paper is to develop state-of-the-art models for lip reading\n-- visual speech recognition. We develop three architectures and compare their\naccuracy and training times: (i) a recurrent model using LSTMs; (ii) a fully\nconvolutional model; and (iii) the recently proposed transformer model. The\nrecurrent and fully convolutional models are trained with a Connectionist\nTemporal Classification loss and use an explicit language model for decoding,\nthe transformer is a sequence-to-sequence model. Our best performing model\nimproves the state-of-the-art word error rate on the challenging BBC-Oxford Lip\nReading Sentences 2 (LRS2) benchmark dataset by over 20 percent.\n  As a further contribution we investigate the fully convolutional model when\nused for online (real time) lip reading of continuous speech, and show that it\nachieves high performance with low latency. \n\n"}
{"id": "1806.06595", "contents": "Title: Uncertainty in multitask learning: joint representations for\n  probabilistic MR-only radiotherapy planning Abstract: Multi-task neural network architectures provide a mechanism that jointly\nintegrates information from distinct sources. It is ideal in the context of\nMR-only radiotherapy planning as it can jointly regress a synthetic CT (synCT)\nscan and segment organs-at-risk (OAR) from MRI. We propose a probabilistic\nmulti-task network that estimates: 1) intrinsic uncertainty through a\nheteroscedastic noise model for spatially-adaptive task loss weighting and 2)\nparameter uncertainty through approximate Bayesian inference. This allows\nsampling of multiple segmentations and synCTs that share their network\nrepresentation. We test our model on prostate cancer scans and show that it\nproduces more accurate and consistent synCTs with a better estimation in the\nvariance of the errors, state of the art results in OAR segmentation and a\nmethodology for quality assurance in radiotherapy treatment planning. \n\n"}
{"id": "1806.07185", "contents": "Title: Mixed batches and symmetric discriminators for GAN training Abstract: Generative adversarial networks (GANs) are pow- erful generative models based\non providing feed- back to a generative network via a discriminator network.\nHowever, the discriminator usually as- sesses individual samples. This prevents\nthe dis- criminator from accessing global distributional statistics of\ngenerated samples, and often leads to mode dropping: the generator models only\npart of the target distribution. We propose to feed the discriminator with\nmixed batches of true and fake samples, and train it to predict the ratio of\ntrue samples in the batch. The latter score does not depend on the order of\nsamples in a batch. Rather than learning this invariance, we introduce a\ngeneric permutation-invariant discriminator ar- chitecture. This architecture\nis provably a uni- versal approximator of all symmetric functions.\nExperimentally, our approach reduces mode col- lapse in GANs on two synthetic\ndatasets, and obtains good results on the CIFAR10 and CelebA datasets, both\nqualitatively and quantitatively. \n\n"}
{"id": "1806.07441", "contents": "Title: Wall Stress Estimation of Cerebral Aneurysm based on Zernike\n  Convolutional Neural Networks Abstract: Convolutional neural networks (ConvNets) have demonstrated an exceptional\ncapacity to discern visual patterns from digital images and signals.\nUnfortunately, such powerful ConvNets do not generalize well to\narbitrary-shaped manifolds, where data representation does not fit into a\ntensor-like grid. Hence, many fields of science and engineering, where data\npoints possess some manifold structure, cannot enjoy the full benefits of the\nrecent advances in ConvNets. The aneurysm wall stress estimation problem\nintroduced in this paper is one of many such problems. The problem is\nwell-known to be of a paramount clinical importance, but yet, traditional\nConvNets cannot be applied due to the manifold structure of the data, neither\ndoes the state-of-the-art geometric ConvNets perform well. Motivated by this,\nwe propose a new geometric ConvNet method named ZerNet, which builds upon our\nnovel mathematical generalization of convolution and pooling operations on\nmanifolds. Our study shows that the ZerNet outperforms the other\nstate-of-the-art geometric ConvNets in terms of accuracy. \n\n"}
{"id": "1806.07812", "contents": "Title: Metric-Driven Learning of Correspondence Weighting for 2-D/3-D Image\n  Registration Abstract: Registration of pre-operative 3-D volumes to intra-operative 2-D X-ray images\nis important in minimally invasive medical procedures. Rigid registration can\nbe performed by estimating a global rigid motion that optimizes the alignment\nof local correspondences. However, inaccurate correspondences challenge the\nregistration performance. To minimize their influence, we estimate optimal\nweights for correspondences using PointNet. We train the network directly with\nthe criterion to minimize the registration error. We propose an objective\nfunction which includes point-to-plane correspondence-based motion estimation\nand projection error computation, thereby enabling the learning of a weighting\nstrategy that optimally fits the underlying formulation of the registration\ntask in an end-to-end fashion. For single-vertebra registration, we achieve an\naccuracy of 0.74$\\pm$0.26 mm and highly improved robustness. The success rate\nis increased from 79.3 % to 94.3 % and the capture range from 3 mm to 13 mm. \n\n"}
{"id": "1806.07823", "contents": "Title: Unsupervised Learning of Object Landmarks through Conditional Image\n  Generation Abstract: We propose a method for learning landmark detectors for visual objects (such\nas the eyes and the nose in a face) without any manual supervision. We cast\nthis as the problem of generating images that combine the appearance of the\nobject as seen in a first example image with the geometry of the object as seen\nin a second example image, where the two examples differ by a viewpoint change\nand/or an object deformation. In order to factorize appearance and geometry, we\nintroduce a tight bottleneck in the geometry-extraction process that selects\nand distils geometry-related features. Compared to standard image generation\nproblems, which often use generative adversarial networks, our generation task\nis conditioned on both appearance and geometry and thus is significantly less\nambiguous, to the point that adopting a simple perceptual loss formulation is\nsufficient. We demonstrate that our approach can learn object landmarks from\nsynthetic image deformations or videos, all without manual supervision, while\noutperforming state-of-the-art unsupervised landmark detectors. We further show\nthat our method is applicable to a large variety of datasets - faces, people,\n3D objects, and digits - without any modifications. \n\n"}
{"id": "1806.07889", "contents": "Title: iMapper: Interaction-guided Joint Scene and Human Motion Mapping from\n  Monocular Videos Abstract: A long-standing challenge in scene analysis is the recovery of scene\narrangements under moderate to heavy occlusion, directly from monocular video.\nWhile the problem remains a subject of active research, concurrent advances\nhave been made in the context of human pose reconstruction from monocular\nvideo, including image-space feature point detection and 3D pose recovery.\nThese methods, however, start to fail under moderate to heavy occlusion as the\nproblem becomes severely under-constrained. We approach the problems\ndifferently. We observe that people interact similarly in similar scenes.\nHence, we exploit the correlation between scene object arrangement and motions\nperformed in that scene in both directions: first, typical motions performed\nwhen interacting with objects inform us about possible object arrangements; and\nsecond, object arrangements, in turn, constrain the possible motions.\n  We present iMapper, a data-driven method that focuses on identifying\nhuman-object interactions, and jointly reasons about objects and human movement\nover space-time to recover both a plausible scene arrangement and consistent\nhuman interactions. We first introduce the notion of characteristic\ninteractions as regions in space-time when an informative human-object\ninteraction happens. This is followed by a novel occlusion-aware matching\nprocedure that searches and aligns such characteristic snapshots from an\ninteraction database to best explain the input monocular video. Through\nextensive evaluations, both quantitative and qualitative, we demonstrate that\niMapper significantly improves performance over both dedicated state-of-the-art\nscene analysis and 3D human pose recovery approaches, especially under medium\nto heavy occlusion. \n\n"}
{"id": "1806.08235", "contents": "Title: Semi-supervised Seizure Prediction with Generative Adversarial Networks Abstract: In this article, we propose an approach that can make use of not only labeled\nEEG signals but also the unlabeled ones which is more accessible. We also\nsuggest the use of data fusion to further improve the seizure prediction\naccuracy. Data fusion in our vision includes EEG signals, cardiogram signals,\nbody temperature and time. We use the short-time Fourier transform on 28-s EEG\nwindows as a pre-processing step. A generative adversarial network (GAN) is\ntrained in an unsupervised manner where information of seizure onset is\ndisregarded. The trained Discriminator of the GAN is then used as feature\nextractor. Features generated by the feature extractor are classified by two\nfully-connected layers (can be replaced by any classifier) for the labeled EEG\nsignals. This semi-supervised seizure prediction method achieves area under the\noperating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp\nEEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively.\nUnsupervised training without the need of labeling is important because not\nonly it can be performed in real-time during EEG signal recording, but also it\ndoes not require feature engineering effort for each patient. \n\n"}
{"id": "1806.08409", "contents": "Title: End-to-End Audio Visual Scene-Aware Dialog using Multimodal\n  Attention-Based Video Features Abstract: Dialog systems need to understand dynamic visual scenes in order to have\nconversations with users about the objects and events around them. Scene-aware\ndialog systems for real-world applications could be developed by integrating\nstate-of-the-art technologies from multiple research areas, including:\nend-to-end dialog technologies, which generate system responses using models\ntrained from dialog data; visual question answering (VQA) technologies, which\nanswer questions about images using learned image features; and video\ndescription technologies, in which descriptions/captions are generated from\nvideos using multimodal information. We introduce a new dataset of dialogs\nabout videos of human behaviors. Each dialog is a typed conversation that\nconsists of a sequence of 10 question-and-answer(QA) pairs between two Amazon\nMechanical Turk (AMT) workers. In total, we collected dialogs on roughly 9,000\nvideos. Using this new dataset for Audio Visual Scene-aware dialog (AVSD), we\ntrained an end-to-end conversation model that generates responses in a dialog\nabout a video. Our experiments demonstrate that using multimodal features that\nwere developed for multimodal attention-based video description enhances the\nquality of generated dialog about dynamic scenes (videos). Our dataset, model\ncode and pretrained models will be publicly available for a new Video\nScene-Aware Dialog challenge. \n\n"}
{"id": "1806.08640", "contents": "Title: Towards safe deep learning: accurately quantifying biomarker uncertainty\n  in neural network predictions Abstract: Automated medical image segmentation, specifically using deep learning, has\nshown outstanding performance in semantic segmentation tasks. However, these\nmethods rarely quantify their uncertainty, which may lead to errors in\ndownstream analysis. In this work we propose to use Bayesian neural networks to\nquantify uncertainty within the domain of semantic segmentation. We also\npropose a method to convert voxel-wise segmentation uncertainty into volumetric\nuncertainty, and calibrate the accuracy and reliability of confidence intervals\nof derived measurements. When applied to a tumour volume estimation\napplication, we demonstrate that by using such modelling of uncertainty, deep\nlearning systems can be made to report volume estimates with well-calibrated\nerror-bars, making them safer for clinical use. We also show that the\nuncertainty estimates extrapolate to unseen data, and that the confidence\nintervals are robust in the presence of artificial noise. This could be used to\nprovide a form of quality control and quality assurance, and may permit further\nadoption of deep learning tools in the clinic. \n\n"}
{"id": "1806.08990", "contents": "Title: Stroke-based Character Reconstruction Abstract: Background elimination for noisy character images or character images from\nreal scene is still a challenging problem, due to the bewildering backgrounds,\nuneven illumination, low resolution and different distortions. We propose a\nstroke-based character reconstruction(SCR) method that use a weighted quadratic\nBezier curve(WQBC) to represent strokes of a character. Only training on our\nsynthetic data, our stroke extractor can achieve excellent reconstruction\neffect in real scenes. Meanwhile. It can also help achieve great ability in\ndefending adversarial attacks of character recognizers. \n\n"}
{"id": "1806.09183", "contents": "Title: A Deeper Look at Power Normalizations Abstract: Power Normalizations (PN) are very useful non-linear operators in the context\nof Bag-of-Words data representations as they tackle problems such as feature\nimbalance. In this paper, we reconsider these operators in the deep learning\nsetup by introducing a novel layer that implements PN for non-linear pooling of\nfeature maps. Specifically, by using a kernel formulation, our layer combines\nthe feature vectors and their respective spatial locations in the feature maps\nproduced by the last convolutional layer of CNN. Linearization of such a kernel\nresults in a positive definite matrix capturing the second-order statistics of\nthe feature vectors, to which PN operators are applied. We study two types of\nPN functions, namely (i) MaxExp and (ii) Gamma, addressing their role and\nmeaning in the context of nonlinear pooling. We also provide a probabilistic\ninterpretation of these operators and derive their surrogates with well-behaved\ngradients for end-to-end CNN learning. We apply our theory to practice by\nimplementing the PN layer on a ResNet-50 model and showcase experiments on four\nbenchmarks for fine-grained recognition, scene recognition, and material\nclassification. Our results demonstrate state-of-the-art performance across all\nthese tasks. \n\n"}
{"id": "1806.09764", "contents": "Title: Deep Generative Models with Learnable Knowledge Constraints Abstract: The broad set of deep generative models (DGMs) has achieved remarkable\nadvances. However, it is often difficult to incorporate rich structured domain\nknowledge with the end-to-end DGMs. Posterior regularization (PR) offers a\nprincipled framework to impose structured constraints on probabilistic models,\nbut has limited applicability to the diverse DGMs that can lack a Bayesian\nformulation or even explicit density evaluation. PR also requires constraints\nto be fully specified a priori, which is impractical or suboptimal for complex\nknowledge with learnable uncertain parts. In this paper, we establish\nmathematical correspondence between PR and reinforcement learning (RL), and,\nbased on the connection, expand PR to learn constraints as the extrinsic reward\nin RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is\nflexible to adapt arbitrary constraints with the model jointly. Experiments on\nhuman image generation and templated sentence generation show models with\nlearned knowledge constraints by our algorithm greatly improve over base\ngenerative models. \n\n"}
{"id": "1806.10050", "contents": "Title: Multi-Mapping Image-to-Image Translation with Central Biasing\n  Normalization Abstract: Recent advances in image-to-image translation have seen a rise in approaches\ngenerating diverse images through a single network. To indicate the target\ndomain for a one-to-many mapping, the latent code is injected into the\ngenerator network. However, we found that the injection method leads to mode\ncollapse because of normalization strategies. Existing normalization strategies\nmight either cause the inconsistency of feature distribution or eliminate the\neffect of the latent code. To solve these problems, we propose the consistency\nwithin diversity criteria for designing the multi-mapping model. Based on the\ncriteria, we propose central biasing normalization to inject the latent code\ninformation. Experiments show that our method can improve the quality and\ndiversity of existing image-to-image translation models, such as StarGAN,\nBicycleGAN, and pix2pix. \n\n"}
{"id": "1806.11430", "contents": "Title: Towards real-time unsupervised monocular depth estimation on CPU Abstract: Unsupervised depth estimation from a single image is a very attractive\ntechnique with several implications in robotic, autonomous navigation,\naugmented reality and so on. This topic represents a very challenging task and\nthe advent of deep learning enabled to tackle this problem with excellent\nresults. However, these architectures are extremely deep and complex. Thus,\nreal-time performance can be achieved only by leveraging power-hungry GPUs that\ndo not allow to infer depth maps in application fields characterized by\nlow-power constraints. To tackle this issue, in this paper we propose a novel\narchitecture capable to quickly infer an accurate depth map on a CPU, even of\nan embedded system, using a pyramid of features extracted from a single input\nimage. Similarly to state-of-the-art, we train our network in an unsupervised\nmanner casting depth estimation as an image reconstruction problem. Extensive\nexperimental results on the KITTI dataset show that compared to the top\nperforming approach our network has similar accuracy but a much lower\ncomplexity (about 6% of parameters) enabling to infer a depth map for a KITTI\nimage in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard\nCPU. Moreover, by trading accuracy for efficiency, our network allows to infer\nmaps at about 2 Hz and 40 Hz respectively, still being more accurate than most\nstate-of-the-art slower methods. To the best of our knowledge, it is the first\nmethod enabling such performance on CPUs paving the way for effective\ndeployment of unsupervised monocular depth estimation even on embedded systems. \n\n"}
{"id": "1807.00436", "contents": "Title: Liver Lesion Detection from Weakly-labeled Multi-phase CT Volumes with a\n  Grouped Single Shot MultiBox Detector Abstract: We present a focal liver lesion detection model leveraged by custom-designed\nmulti-phase computed tomography (CT) volumes, which reflects real-world\nclinical lesion detection practice using a Single Shot MultiBox Detector (SSD).\nWe show that grouped convolutions effectively harness richer information of the\nmulti-phase data for the object detection model, while a naive application of\nSSD suffers from a generalization gap. We trained and evaluated the modified\nSSD model and recently proposed variants with our CT dataset of 64 subjects by\nfive-fold cross validation. Our model achieved a 53.3% average precision score\nand ran in under three seconds per volume, outperforming the original model and\nstate-of-the-art variants. Results show that the one-stage object detection\nmodel is a practical solution, which runs in near real-time and can learn an\nunbiased feature representation from a large-volume real-world detection\ndataset, which requires less tedious and time consuming construction of the\nweak phase-level bounding box labels. \n\n"}
{"id": "1807.00502", "contents": "Title: Leveraging Uncertainty Estimates for Predicting Segmentation Quality Abstract: The use of deep learning for medical imaging has seen tremendous growth in\nthe research community. One reason for the slow uptake of these systems in the\nclinical setting is that they are complex, opaque and tend to fail silently.\nOutside of the medical imaging domain, the machine learning community has\nrecently proposed several techniques for quantifying model uncertainty (i.e.~a\nmodel knowing when it has failed). This is important in practical settings, as\nwe can refer such cases to manual inspection or correction by humans. In this\npaper, we aim to bring these recent results on estimating uncertainty to bear\non two important outputs in deep learning-based segmentation. The first is\nproducing spatial uncertainty maps, from which a clinician can observe where\nand why a system thinks it is failing. The second is quantifying an image-level\nprediction of failure, which is useful for isolating specific cases and\nremoving them from automated pipelines. We also show that reasoning about\nspatial uncertainty, the first output, is a useful intermediate representation\nfor generating segmentation quality predictions, the second output. We propose\na two-stage architecture for producing these measures of uncertainty, which can\naccommodate any deep learning-based medical segmentation pipeline. \n\n"}
{"id": "1807.01068", "contents": "Title: HAMLET: Hierarchical Harmonic Filters for Learning Tracts from Diffusion\n  MRI Abstract: In this work we propose HAMLET, a novel tract learning algorithm, which,\nafter training, maps raw diffusion weighted MRI directly onto an image which\nsimultaneously indicates tract direction and tract presence. The automatic\nlearning of fiber tracts based on diffusion MRI data is a rather new idea,\nwhich tries to overcome limitations of atlas-based techniques. HAMLET takes a\nsuch an approach. Unlike the current trend in machine learning, HAMLET has only\na small number of free parameters HAMLET is based on spherical tensor algebra\nwhich allows a translation and rotation covariant treatment of the problem.\nHAMLET is based on a repeated application of convolutions and non-linearities,\nwhich all respect the rotation covariance. The intrinsic treatment of such\nbasic image transformations in HAMLET allows the training and generalization of\nthe algorithm without any additional data augmentation. We demonstrate the\nperformance of our approach for twelve prominent bundles, and show that the\nobtained tract estimates are robust and reliable. It is also shown that the\nlearned models are portable from one sequence to another. \n\n"}
{"id": "1807.01394", "contents": "Title: ModaNet: A Large-Scale Street Fashion Dataset with Polygon Annotations Abstract: Understanding clothes from a single image has strong commercial and cultural\nimpacts on modern societies. However, this task remains a challenging computer\nvision problem due to wide variations in the appearance, style, brand and\nlayering of clothing items. We present a new database called ModaNet, a\nlarge-scale collection of images based on Paperdoll dataset. Our dataset\nprovides 55,176 street images, fully annotated with polygons on top of the 1\nmillion weakly annotated street images in Paperdoll. ModaNet aims to provide a\ntechnical benchmark to fairly evaluate the progress of applying the latest\ncomputer vision techniques that rely on large data for fashion understanding.\nThe rich annotation of the dataset allows to measure the performance of\nstate-of-the-art algorithms for object detection, semantic segmentation and\npolygon prediction on street fashion images in detail. The polygon-based\nannotation dataset has been released https://github.com/eBay/modanet, we also\nhost the leaderboard at EvalAI:\nhttps://evalai.cloudcv.org/featured-challenges/136/overview. \n\n"}
{"id": "1807.01438", "contents": "Title: Small-scale Pedestrian Detection Based on Somatic Topology Localization\n  and Temporal Feature Aggregation Abstract: A critical issue in pedestrian detection is to detect small-scale objects\nthat will introduce feeble contrast and motion blur in images and videos, which\nin our opinion should partially resort to deep-rooted annotation bias.\nMotivated by this, we propose a novel method integrated with somatic\ntopological line localization (TLL) and temporal feature aggregation for\ndetecting multi-scale pedestrians, which works particularly well with\nsmall-scale pedestrians that are relatively far from the camera. Moreover, a\npost-processing scheme based on Markov Random Field (MRF) is introduced to\neliminate ambiguities in occlusion cases. Applying with these methodologies\ncomprehensively, we achieve best detection performance on Caltech benchmark and\nimprove performance of small-scale objects significantly (miss rate decreases\nfrom 74.53% to 60.79%). Beyond this, we also achieve competitive performance on\nCityPersons dataset and show the existence of annotation bias in KITTI dataset. \n\n"}
{"id": "1807.01569", "contents": "Title: The SEN1-2 Dataset for Deep Learning in SAR-Optical Data Fusion Abstract: While deep learning techniques have an increasing impact on many technical\nfields, gathering sufficient amounts of training data is a challenging problem\nin remote sensing. In particular, this holds for applications involving data\nfrom multiple sensors with heterogeneous characteristics. One example for that\nis the fusion of synthetic aperture radar (SAR) data and optical imagery. With\nthis paper, we publish the SEN1-2 dataset to foster deep learning research in\nSAR-optical data fusion. SEN1-2 comprises 282,384 pairs of corresponding image\npatches, collected from across the globe and throughout all meteorological\nseasons. Besides a detailed description of the dataset, we show exemplary\nresults for several possible applications, such as SAR image colorization,\nSAR-optical image matching, and creation of artificial optical images from SAR\ninput data. Since SEN1-2 is the first large open dataset of this kind, we\nbelieve it will support further developments in the field of deep learning for\nremote sensing as well as multi-sensor data fusion. \n\n"}
{"id": "1807.01884", "contents": "Title: A Single Shot Text Detector with Scale-adaptive Anchors Abstract: Currently, most top-performing text detection networks tend to employ\nfixed-size anchor boxes to guide the search for text instances. They usually\nrely on a large amount of anchors with different scales to discover texts in\nscene images, thus leading to high computational cost. In this paper, we\npropose an end-to-end box-based text detector with scale-adaptive anchors,\nwhich can dynamically adjust the scales of anchors according to the sizes of\nunderlying texts by introducing an additional scale regression layer. The\nproposed scale-adaptive anchors allow us to use a few number of anchors to\nhandle multi-scale texts and therefore significantly improve the computational\nefficiency. Moreover, compared to discrete scales used in previous methods, the\nlearned continuous scales are more reliable, especially for small texts\ndetection. Additionally, we propose Anchor convolution to better exploit\nnecessary feature information by dynamically adjusting the sizes of receptive\nfields according to the learned scales. Extensive experiments demonstrate that\nthe proposed detector is fast, taking only $0.28$ second per image, while\noutperforming most state-of-the-art methods in accuracy. \n\n"}
{"id": "1807.01990", "contents": "Title: Transfer Learning From Synthetic To Real Images Using Variational\n  Autoencoders For Precise Position Detection Abstract: Capturing and labeling camera images in the real world is an expensive task,\nwhereas synthesizing labeled images in a simulation environment is easy for\ncollecting large-scale image data. However, learning from only synthetic images\nmay not achieve the desired performance in the real world due to a gap between\nsynthetic and real images. We propose a method that transfers learned detection\nof an object position from a simulation environment to the real world. This\nmethod uses only a significantly limited dataset of real images while\nleveraging a large dataset of synthetic images using variational autoencoders.\nAdditionally, the proposed method consistently performed well in different\nlighting conditions, in the presence of other distractor objects, and on\ndifferent backgrounds. Experimental results showed that it achieved accuracy of\n1.5mm to 3.5mm on average. Furthermore, we showed how the method can be used in\na real-world scenario like a \"pick-and-place\" robotic task. \n\n"}
{"id": "1807.02700", "contents": "Title: Towards Multi-class Object Detection in Unconstrained Remote Sensing\n  Imagery Abstract: Automatic multi-class object detection in remote sensing images in\nunconstrained scenarios is of high interest for several applications including\ntraffic monitoring and disaster management. The huge variation in object scale,\norientation, category, and complex backgrounds, as well as the different camera\nsensors pose great challenges for current algorithms. In this work, we propose\na new method consisting of a novel joint image cascade and feature pyramid\nnetwork with multi-size convolution kernels to extract multi-scale strong and\nweak semantic features. These features are fed into rotation-based region\nproposal and region of interest networks to produce object detections. Finally,\nrotational non-maximum suppression is applied to remove redundant detections.\nDuring training, we minimize joint horizontal and oriented bounding box loss\nfunctions, as well as a novel loss that enforces oriented boxes to be\nrectangular. Our method achieves 68.16% mAP on horizontal and 72.45% mAP on\noriented bounding box detection tasks on the challenging DOTA dataset,\noutperforming all published methods by a large margin (+6% and +12% absolute\nimprovement, respectively). Furthermore, it generalizes to two other datasets,\nNWPU VHR-10 and UCAS-AOD, and achieves competitive results with the baselines\neven when trained on DOTA. Our method can be deployed in multi-class object\ndetection applications, regardless of the image and object scales and\norientations, making it a great choice for unconstrained aerial and satellite\nimagery. \n\n"}
{"id": "1807.02716", "contents": "Title: A Deep-Learning-Based Geological Parameterization for History Matching\n  Complex Models Abstract: A new low-dimensional parameterization based on principal component analysis\n(PCA) and convolutional neural networks (CNN) is developed to represent complex\ngeological models. The CNN-PCA method is inspired by recent developments in\ncomputer vision using deep learning. CNN-PCA can be viewed as a generalization\nof an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA\nentail post-processing a PCA model to better honor complex geological features.\nIn CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new\nregularization involving a set of metrics for multipoint statistics is\nintroduced. The metrics are based on summary statistics of the nonlinear filter\nresponses of geological models to a pre-trained deep CNN. In addition, in the\nCNN-PCA formulation presented here, a convolutional neural network is trained\nas an explicit transform function that can post-process PCA models quickly.\nCNN-PCA is shown to provide both unconditional and conditional realizations\nthat honor the geological features present in reference SGeMS geostatistical\nrealizations for a binary channelized system. Flow statistics obtained through\nsimulation of random CNN-PCA models closely match results for random SGeMS\nmodels for a demanding case in which O-PCA models lead to significant\ndiscrepancies. Results for history matching are also presented. In this\nassessment CNN-PCA is applied with derivative-free optimization, and a subspace\nrandomized maximum likelihood method is used to provide multiple posterior\nmodels. Data assimilation and significant uncertainty reduction are achieved\nfor existing wells, and physically reasonable predictions are also obtained for\nnew wells. Finally, the CNN-PCA method is extended to a more complex\nnon-stationary bimodal deltaic fan system, and is shown to provide high-quality\nrealizations for this challenging example. \n\n"}
{"id": "1807.02740", "contents": "Title: Data-driven Upsampling of Point Clouds Abstract: High quality upsampling of sparse 3D point clouds is critically useful for a\nwide range of geometric operations such as reconstruction, rendering, meshing,\nand analysis. In this paper, we propose a data-driven algorithm that enables an\nupsampling of 3D point clouds without the need for hard-coded rules. Our\napproach uses a deep network with Chamfer distance as the loss function,\ncapable of learning the latent features in point clouds belonging to different\nobject categories. We evaluate our algorithm across different amplification\nfactors, with upsampling learned and performed on objects belonging to the same\ncategory as well as different categories. We also explore the desirable\ncharacteristics of input point clouds as a function of the distribution of the\npoint samples. Finally, we demonstrate the performance of our algorithm in\nsingle-category training versus multi-category training scenarios. The final\nproposed model is compared against a baseline, optimization-based upsampling\nmethod. Results indicate that our algorithm is capable of generating more\nuniform and accurate upsamplings. \n\n"}
{"id": "1807.02908", "contents": "Title: Partial Policy-based Reinforcement Learning for Anatomical Landmark\n  Localization in 3D Medical Images Abstract: Deploying the idea of long-term cumulative return, reinforcement learning has\nshown remarkable performance in various fields. We propose a formulation of the\nlandmark localization in 3D medical images as a reinforcement learning problem.\nWhereas value-based methods have been widely used to solve similar problems, we\nadopt an actor-critic based direct policy search method framed in a temporal\ndifference learning approach. Successful behavior learning is challenging in\nlarge state and/or action spaces, requiring many trials. We introduce a partial\npolicy-based reinforcement learning to enable solving the large problem of\nlocalization by learning the optimal policy on smaller partial domains.\nIndependent actors efficiently learn the corresponding partial policies, each\nutilizing their own independent critic. The proposed policy reconstruction from\nthe partial policies ensures a robust and efficient localization utilizing the\nsub-agents solving simple binary decision problems in their corresponding\npartial action spaces. The proposed reinforcement learning requires a small\nnumber of trials to learn the optimal behavior compared with the original\nbehavior learning scheme. \n\n"}
{"id": "1807.02941", "contents": "Title: Multi-Scale Coarse-to-Fine Segmentation for Screening Pancreatic Ductal\n  Adenocarcinoma Abstract: We propose an intuitive approach of detecting pancreatic ductal\nadenocarcinoma (PDAC), the most common type of pancreatic cancer, by checking\nabdominal CT scans. Our idea is named multi-scale\nsegmentation-for-classification, which classifies volumes by checking if at\nleast a sufficient number of voxels is segmented as tumors, by which we can\nprovide radiologists with tumor locations. In order to deal with tumors with\ndifferent scales, we train and test our volumetric segmentation networks with\nmulti-scale inputs in a coarse-to-fine flowchart. A post-processing module is\nused to filter out outliers and reduce false alarms. We collect a new dataset\ncontaining 439 CT scans, in which 136 cases were diagnosed with PDAC and 303\ncases are normal, which is the largest set for PDAC tumors to the best of our\nknowledge. To offer the best trade-off between sensitivity and specificity, our\nproposed framework reports a sensitivity of 94.1% at a specificity of 98.5%,\nwhich demonstrates the potential to make a clinical impact. \n\n"}
{"id": "1807.03089", "contents": "Title: Video Summarisation by Classification with Deep Reinforcement Learning Abstract: Most existing video summarisation methods are based on either supervised or\nunsupervised learning. In this paper, we propose a reinforcement learning-based\nweakly supervised method that exploits easy-to-obtain, video-level category\nlabels and encourages summaries to contain category-related information and\nmaintain category recognisability. Specifically, We formulate video\nsummarisation as a sequential decision-making process and train a summarisation\nnetwork with deep Q-learning (DQSN). A companion classification network is also\ntrained to provide rewards for training the DQSN. With the classification\nnetwork, we develop a global recognisability reward based on the classification\nresult. Critically, a novel dense ranking-based reward is also proposed in\norder to cope with the temporally delayed and sparse reward problems for long\nsequence reinforcement learning. Extensive experiments on two benchmark\ndatasets show that the proposed approach achieves state-of-the-art performance. \n\n"}
{"id": "1807.03094", "contents": "Title: Deep Multimodal Clustering for Unsupervised Audiovisual Learning Abstract: The seen birds twitter, the running cars accompany with noise, etc. These\nnaturally audiovisual correspondences provide the possibilities to explore and\nunderstand the outside world. However, the mixed multiple objects and sounds\nmake it intractable to perform efficient matching in the unconstrained\nenvironment. To settle this problem, we propose to adequately excavate audio\nand visual components and perform elaborate correspondence learning among them.\nConcretely, a novel unsupervised audiovisual learning model is proposed, named\nas \\Deep Multimodal Clustering (DMC), that synchronously performs sets of\nclustering with multimodal vectors of convolutional maps in different shared\nspaces for capturing multiple audiovisual correspondences. And such integrated\nmultimodal clustering network can be effectively trained with max-margin loss\nin the end-to-end fashion. Amounts of experiments in feature evaluation and\naudiovisual tasks are performed. The results demonstrate that DMC can learn\neffective unimodal representation, with which the classifier can even\noutperform human performance. Further, DMC shows noticeable performance in\nsound localization, multisource detection, and audiovisual understanding. \n\n"}
{"id": "1807.03139", "contents": "Title: Utility in Fashion with implicit feedback Abstract: Fashion preference is a fuzzy concept that depends on customer taste,\nprevailing norms in fashion product/style, henceforth used interchangeably, and\na customer's perception of utility or fashionability, yet fashion e-retail\nrelies on algorithmically generated search and recommendation systems that\nprocess structured data and images to best match customer preference. Retailers\nstudy tastes solely as a function of what sold vs what did not, and take it to\nrepresent customer preference. Such explicit modeling, however, belies the\nunderlying user preference, which is a complicated interplay of preference and\ncommercials such as brand, price point, promotions, other sale events, and\ncompetitor push/marketing. It is hard to infer a notion of utility or even\ncustomer preference by looking at sales data.\n  In search and recommendation systems for fashion e-retail, customer\npreference is implicitly derived by user-user similarity or item-item\nsimilarity. In this work, we aim to derive a metric that separates the buying\npreferences of users from the commercials of the merchandise (price,\npromotions, etc). We extend our earlier work on explicit signals to gauge\nsellability or preference with implicit signals from user behaviour. \n\n"}
{"id": "1807.03380", "contents": "Title: An Attention Model for group-level emotion recognition Abstract: In this paper we propose a new approach for classifying the global emotion of\nimages containing groups of people. To achieve this task, we consider two\ndifferent and complementary sources of information: i) a global representation\nof the entire image (ii) a local representation where only faces are\nconsidered. While the global representation of the image is learned with a\nconvolutional neural network (CNN), the local representation is obtained by\nmerging face features through an attention mechanism. The two representations\nare first learned independently with two separate CNN branches and then fused\nthrough concatenation in order to obtain the final group-emotion classifier.\nFor our submission to the EmotiW 2018 group-level emotion recognition\nchallenge, we combine several variations of the proposed model into an\nensemble, obtaining a final accuracy of 64.83% on the test set and ranking 4th\namong all challenge participants. \n\n"}
{"id": "1807.03480", "contents": "Title: Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video\n  Demonstration Abstract: Our goal is to generate a policy to complete an unseen task given just a\nsingle video demonstration of the task in a given domain. We hypothesize that\nto successfully generalize to unseen complex tasks from a single video\ndemonstration, it is necessary to explicitly incorporate the compositional\nstructure of the tasks into the model. To this end, we propose Neural Task\nGraph (NTG) Networks, which use conjugate task graph as the intermediate\nrepresentation to modularize both the video demonstration and the derived\npolicy. We empirically show NTG achieves inter-task generalization on two\ncomplex tasks: Block Stacking in BulletPhysics and Object Collection in\nAI2-THOR. NTG improves data efficiency with visual input as well as achieve\nstrong generalization without the need for dense hierarchical supervision. We\nfurther show that similar performance trends hold when applied to real-world\ndata. We show that NTG can effectively predict task structure on the JIGSAWS\nsurgical dataset and generalize to unseen tasks. \n\n"}
{"id": "1807.03528", "contents": "Title: Deep Underwater Image Enhancement Abstract: In an underwater scene, wavelength-dependent light absorption and scattering\ndegrade the visibility of images, causing low contrast and distorted color\ncasts. To address this problem, we propose a convolutional neural network based\nimage enhancement model, i.e., UWCNN, which is trained efficiently using a\nsynthetic underwater image database. Unlike the existing works that require the\nparameters of underwater imaging model estimation or impose inflexible\nframeworks applicable only for specific scenes, our model directly reconstructs\nthe clear latent underwater image by leveraging on an automatic end-to-end and\ndata-driven training mechanism. Compliant with underwater imaging models and\noptical properties of underwater scenes, we first synthesize ten different\nmarine image databases. Then, we separately train multiple UWCNN models for\neach underwater image formation type. Experimental results on real-world and\nsynthetic underwater images demonstrate that the presented method generalizes\nwell on different underwater scenes and outperforms the existing methods both\nqualitatively and quantitatively. Besides, we conduct an ablation study to\ndemonstrate the effect of each component in our network. \n\n"}
{"id": "1807.03770", "contents": "Title: Efficient identification, localization and quantification of grapevine\n  inflorescences in unprepared field images using Fully Convolutional Networks Abstract: Yield and its prediction is one of the most important tasks in grapevine\nbreeding purposes and vineyard management. Commonly, this trait is estimated\nmanually right before harvest by extrapolation, which mostly is\nlabor-intensive, destructive and inaccurate. In the present study an automated\nimage-based workflow was developed quantifying inflorescences and single\nflowers in unprepared field images of grapevines, i.e. no artificial background\nor light was applied. It is a novel approach for non-invasive, inexpensive and\nobjective phenotyping with high-throughput.\n  First, image regions depicting inflorescences were identified and localized.\nThis was done by segmenting the images into the classes \"inflorescence\" and\n\"non-inflorescence\" using a Fully Convolutional Network (FCN). Efficient image\nsegmentation hereby is the most challenging step regarding the small geometry\nand dense distribution of flowers (several hundred flowers per inflorescence),\nsimilar color of all plant organs in the fore- and background as well as the\ncircumstance that only approximately 5% of an image show inflorescences. The\ntrained FCN achieved a mean Intersection Over Union (IOU) of 87.6% on the test\ndata set. Finally, individual flowers were extracted from the\n\"inflorescence\"-areas using Circular Hough Transform. The flower extraction\nachieved a recall of 80.3% and a precision of 70.7% using the segmentation\nderived by the trained FCN model.\n  Summarized, the presented approach is a promising strategy in order to\npredict yield potential automatically in the earliest stage of grapevine\ndevelopment which is applicable for objective monitoring and evaluations of\nbreeding material, genetic repositories or commercial vineyards. \n\n"}
{"id": "1807.04355", "contents": "Title: Deepwound: Automated Postoperative Wound Assessment and Surgical Site\n  Surveillance through Convolutional Neural Networks Abstract: Postoperative wound complications are a significant cause of expense for\nhospitals, doctors, and patients. Hence, an effective method to diagnose the\nonset of wound complications is strongly desired. Algorithmically classifying\nwound images is a difficult task due to the variability in the appearance of\nwound sites. Convolutional neural networks (CNNs), a subgroup of artificial\nneural networks that have shown great promise in analyzing visual imagery, can\nbe leveraged to categorize surgical wounds. We present a multi-label CNN\nensemble, Deepwound, trained to classify wound images using only image pixels\nand corresponding labels as inputs. Our final computational model can\naccurately identify the presence of nine labels: drainage, fibrinous exudate,\ngranulation tissue, surgical site infection, open wound, staples, steri strips,\nand sutures. Our model achieves receiver operating curve (ROC) area under curve\n(AUC) scores, sensitivity, specificity, and F1 scores superior to prior work in\nthis area. Smartphones provide a means to deliver accessible wound care due to\ntheir increasing ubiquity. Paired with deep neural networks, they offer the\ncapability to provide clinical insight to assist surgeons during postoperative\ncare. We also present a mobile application frontend to Deepwound that assists\npatients in tracking their wound and surgical recovery from the comfort of\ntheir home. \n\n"}
{"id": "1807.04409", "contents": "Title: Sem-GAN: Semantically-Consistent Image-to-Image Translation Abstract: Unpaired image-to-image translation is the problem of mapping an image in the\nsource domain to one in the target domain, without requiring corresponding\nimage pairs. To ensure the translated images are realistically plausible,\nrecent works, such as Cycle-GAN, demands this mapping to be invertible. While,\nthis requirement demonstrates promising results when the domains are unimodal,\nits performance is unpredictable in a multi-modal scenario such as in an image\nsegmentation task. This is because, invertibility does not necessarily enforce\nsemantic correctness. To this end, we present a semantically-consistent GAN\nframework, dubbed Sem-GAN, in which the semantics are defined by the class\nidentities of image segments in the source domain as produced by a semantic\nsegmentation algorithm. Our proposed framework includes consistency constraints\non the translation task that, together with the GAN loss and the\ncycle-constraints, enforces that the images when translated will inherit the\nappearances of the target domain, while (approximately) maintaining their\nidentities from the source domain. We present experiments on several\nimage-to-image translation tasks and demonstrate that Sem-GAN improves the\nquality of the translated images significantly, sometimes by more than 20% on\nthe FCN score. Further, we show that semantic segmentation models, trained with\nsynthetic images translated via Sem-GAN, leads to significantly better\nsegmentation results than other variants. \n\n"}
{"id": "1807.04418", "contents": "Title: Subsampled Turbulence Removal Network Abstract: We present a deep-learning approach to restore a sequence of\nturbulence-distorted video frames from turbulent deformations and space-time\nvarying blurs. Instead of requiring a massive training sample size in deep\nnetworks, we purpose a training strategy that is based on a new data\naugmentation method to model turbulence from a relatively small dataset. Then\nwe introduce a subsampled method to enhance the restoration performance of the\npresented GAN model. The contributions of the paper is threefold: first, we\nintroduce a simple but effective data augmentation algorithm to model the\nturbulence in real life for training in the deep network; Second, we firstly\npurpose the Wasserstein GAN combined with $\\ell_1$ cost for successful\nrestoration of turbulence-corrupted video sequence; Third, we combine the\nsubsampling algorithm to filter out strongly corrupted frames to generate a\nvideo sequence with better quality. \n\n"}
{"id": "1807.04880", "contents": "Title: Effective Occlusion Handling for Fast Correlation Filter-based Trackers Abstract: Correlation filter-based trackers heavily suffer from the problem of multiple\npeaks in their response maps incurred by occlusions. Moreover, the whole\ntracking pipeline may break down due to the uncertainties brought by shifting\namong peaks, which will further lead to the degraded correlation filter model.\nTo alleviate the drift problem caused by occlusions, we propose a novel scheme\nto choose the specific filter model according to different scenarios.\nSpecifically, an effective measurement function is designed to evaluate the\nquality of filter response. A sophisticated strategy is employed to judge\nwhether occlusions occur, and then decide how to update the filter models. In\naddition, we take advantage of both log-polar method and pyramid-like approach\nto estimate the best scale of the target. We evaluate our proposed approach on\nVOT2018 challenge and OTB100 dataset, whose experimental result shows that the\nproposed tracker achieves the promising performance compared against the\nstate-of-the-art trackers. \n\n"}
{"id": "1807.04899", "contents": "Title: Analysis Dictionary Learning based Classification: Structure for\n  Robustness Abstract: A discriminative structured analysis dictionary is proposed for the\nclassification task. A structure of the union of subspaces (UoS) is integrated\ninto the conventional analysis dictionary learning to enhance the capability of\ndiscrimination. A simple classifier is also simultaneously included into the\nformulated functional to ensure a more complete consistent classification. The\nsolution of the algorithm is efficiently obtained by the linearized alternating\ndirection method of multipliers. Moreover, a distributed structured analysis\ndictionary learning is also presented to address large scale datasets. It can\ngroup-(class-) independently train the structured analysis dictionaries by\ndifferent machines/cores/threads, and therefore avoid a high computational\ncost. A consensus structured analysis dictionary and a global classifier are\njointly learned in the distributed approach to safeguard the discriminative\npower and the efficiency of classification. Experiments demonstrate that our\nmethod achieves a comparable or better performance than the state-of-the-art\nalgorithms in a variety of visual classification tasks. In addition, the\ntraining and testing computational complexity are also greatly reduced. \n\n"}
{"id": "1807.05162", "contents": "Title: Large-Scale Visual Speech Recognition Abstract: This work presents a scalable solution to open-vocabulary visual speech\nrecognition. To achieve this, we constructed the largest existing visual speech\nrecognition dataset, consisting of pairs of text and video clips of faces\nspeaking (3,886 hours of video). In tandem, we designed and trained an\nintegrated lipreading system, consisting of a video processing pipeline that\nmaps raw video to stable videos of lips and sequences of phonemes, a scalable\ndeep neural network that maps the lip videos to sequences of phoneme\ndistributions, and a production-level speech decoder that outputs sequences of\nwords. The proposed system achieves a word error rate (WER) of 40.9% as\nmeasured on a held-out set. In comparison, professional lipreaders achieve\neither 86.4% or 92.9% WER on the same dataset when having access to additional\ntypes of contextual information. Our approach significantly improves on other\nlipreading approaches, including variants of LipNet and of Watch, Attend, and\nSpell (WAS), which are only capable of 89.8% and 76.8% WER respectively. \n\n"}
{"id": "1807.05284", "contents": "Title: Survey on Deep Learning Techniques for Person Re-Identification Task Abstract: Intelligent video-surveillance is currently an active research field in\ncomputer vision and machine learning techniques. It provides useful tools for\nsurveillance operators and forensic video investigators. Person\nre-identification (PReID) is one among these tools. It consists of recognizing\nwhether an individual has already been observed over a camera in a network or\nnot. This tool can also be employed in various possible applications such as\noff-line retrieval of all the video-sequences showing an individual of interest\nwhose image is given a query, and online pedestrian tracking over multiple\ncamera views. To this aim, many techniques have been proposed to increase the\nperformance of PReID. Among the systems, many researchers utilized deep neural\nnetworks (DNNs) because of their better performance and fast execution at test\ntime. Our objective is to provide for future researchers the work being done on\nPReID to date. Therefore, we summarized state-of-the-art DNN models being used\nfor this task. A brief description of each model along with their evaluation on\na set of benchmark datasets is given. Finally, a detailed comparison is\nprovided among these models followed by some limitations that can work as\nguidelines for future research. \n\n"}
{"id": "1807.05705", "contents": "Title: ENG: End-to-end Neural Geometry for Robust Depth and Pose Estimation\n  using CNNs Abstract: Recovering structure and motion parameters given a image pair or a sequence\nof images is a well studied problem in computer vision. This is often achieved\nby employing Structure from Motion (SfM) or Simultaneous Localization and\nMapping (SLAM) algorithms based on the real-time requirements. Recently, with\nthe advent of Convolutional Neural Networks (CNNs) researchers have explored\nthe possibility of using machine learning techniques to reconstruct the 3D\nstructure of a scene and jointly predict the camera pose. In this work, we\npresent a framework that achieves state-of-the-art performance on single image\ndepth prediction for both indoor and outdoor scenes. The depth prediction\nsystem is then extended to predict optical flow and ultimately the camera pose\nand trained end-to-end. Our motion estimation framework outperforms the\nprevious motion prediction systems and we also demonstrate that the\nstate-of-the-art metric depths can be further improved using the knowledge of\npose. \n\n"}
{"id": "1807.05726", "contents": "Title: BRIEF: Backward Reduction of CNNs with Information Flow Analysis Abstract: This paper proposes BRIEF, a backward reduction algorithm that explores\ncompact CNN-model designs from the information flow perspective. This algorithm\ncan remove substantial non-zero weighting parameters (redundant neural\nchannels) of a network by considering its dynamic behavior, which traditional\nmodel-compaction techniques cannot achieve. With the aid of our proposed\nalgorithm, we achieve significant model reduction on ResNet-34 in the ImageNet\nscale (32.3% reduction), which is 3X better than the previous result (10.8%).\nEven for highly optimized models such as SqueezeNet and MobileNet, we can\nachieve additional 10.81% and 37.56% reduction, respectively, with negligible\nperformance degradation. \n\n"}
{"id": "1807.05972", "contents": "Title: Towards Single-phase Single-stage Detection of Pulmonary Nodules in\n  Chest CT Imaging Abstract: Detection of pulmonary nodules in chest CT imaging plays a crucial role in\nearly diagnosis of lung cancer. Manual examination is highly time-consuming and\nerror prone, calling for computer-aided detection, both to improve efficiency\nand reduce misdiagnosis. Over the years, a range of systems have been proposed,\nmostly following a two-phase paradigm with: 1) candidate detection, 2) false\npositive reduction. Recently, deep learning has become a dominant force in\nalgorithm development. As for candidate detection, prior art was mainly based\non the two-stage Faster R-CNN framework, which starts with an initial sub-net\nto generate a set of class-agnostic region proposals, followed by a second\nsub-net to perform classification and bounding-box regression. In contrast, we\nabandon the conventional two-phase paradigm and two-stage framework altogether\nand propose to train a single network for end-to-end nodule detection instead,\nwithout transfer learning or further post-processing. Our feature learning\nmodel is a modification of the ResNet and feature pyramid network combined,\npowered by RReLU activation. The major challenge is the condition of extreme\ninter-class and intra-class sample imbalance, where the positives are\noverwhelmed by a large negative pool, which is mostly composed of easy and a\nhandful of hard negatives. Direct training on all samples can seriously\nundermine training efficacy. We propose a patch-based sampling strategy over a\nset of regularly updating anchors, which narrows sampling scope to all\npositives and only hard negatives, effectively addressing this issue. As a\nresult, our approach substantially outperforms prior art in terms of both\naccuracy and speed. Finally, the prevailing FROC evaluation over [1/8, 1/4,\n1/2, 1, 2, 4, 8] false positives per scan, is far from ideal in real clinical\nenvironments. We suggest FROC over [1, 2, 4] false positives as a better\nmetric. \n\n"}
{"id": "1807.06160", "contents": "Title: Layer-wise Relevance Propagation for Explainable Recommendations Abstract: In this paper, we tackle the problem of explanations in a deep-learning based\nmodel for recommendations by leveraging the technique of layer-wise relevance\npropagation. We use a Deep Convolutional Neural Network to extract relevant\nfeatures from the input images before identifying similarity between the images\nin feature space. Relationships between the images are identified by the model\nand layer-wise relevance propagation is used to infer pixel-level details of\nthe images that may have significantly informed the model's choice. We evaluate\nour method on an Amazon products dataset and demonstrate the efficacy of our\napproach. \n\n"}
{"id": "1807.06416", "contents": "Title: A Dense CNN approach for skin lesion classification Abstract: This article presents a Deep CNN, based on the DenseNet architecture jointly\nwith a highly discriminating learning methodology, in order to classify seven\nkinds of skin lesions: Melanoma, Melanocytic nevus, Basal cell carcinoma,\nActinic keratosis / Bowen's disease, Benign keratosis, Dermatofibroma, Vascular\nlesion. In particular a 61 layers DenseNet, pre-trained on IMAGENET dataset,\nhas been fine-tuned on ISIC 2018 Task 3 Challenge Dataset exploiting a Center\nLoss function. \n\n"}
{"id": "1807.06699", "contents": "Title: Adaptive Neural Trees Abstract: Deep neural networks and decision trees operate on largely separate\nparadigms; typically, the former performs representation learning with\npre-specified architectures, while the latter is characterised by learning\nhierarchies over pre-specified features with data-driven architectures. We\nunite the two via adaptive neural trees (ANTs) that incorporates representation\nlearning into edges, routing functions and leaf nodes of a decision tree, along\nwith a backpropagation-based training algorithm that adaptively grows the\narchitecture from primitive modules (e.g., convolutional layers). We\ndemonstrate that, whilst achieving competitive performance on classification\nand regression datasets, ANTs benefit from (i) lightweight inference via\nconditional computation, (ii) hierarchical separation of features useful to the\ntask e.g. learning meaningful class associations, such as separating natural\nvs. man-made objects, and (iii) a mechanism to adapt the architecture to the\nsize and complexity of the training dataset. \n\n"}
{"id": "1807.07416", "contents": "Title: Image Reconstruction via Variational Network for Real-Time Hand-Held\n  Sound-Speed Imaging Abstract: Speed-of-sound is a biomechanical property for quantitative tissue\ndifferentiation, with great potential as a new ultrasound-based image modality.\nA conventional ultrasound array transducer can be used together with an\nacoustic mirror, or so-called reflector, to reconstruct sound-speed images from\ntime-of-flight measurements to the reflector collected between transducer\nelement pairs, which constitutes a challenging problem of limited-angle\ncomputed tomography. For this problem, we herein present a variational network\nbased image reconstruction architecture that is based on optimization loop\nunrolling, and provide an efficient training protocol of this network\narchitecture on fully synthetic inclusion data. Our results indicate that the\nlearned model presents good generalization ability, being able to reconstruct\nimages with significantly different statistics compared to the training set.\nComplex inclusion geometries were shown to be successfully reconstructed, also\nimproving over the prior-art by 23% in reconstruction error and by 10% in\ncontrast on synthetic data. In a phantom study, we demonstrated the detection\nof multiple inclusions that were not distinguishable by prior-art\nreconstruction, meanwhile improving the contrast by 27% for a stiff inclusion\nand by 219% for a soft inclusion. Our reconstruction algorithm takes\napproximately 10ms, enabling its use as a real-time imaging method on an\nultrasound machine, for which we are demonstrating an example preliminary setup\nherein. \n\n"}
{"id": "1807.07433", "contents": "Title: Real-Time Stereo Vision for Road Surface 3-D Reconstruction Abstract: Stereo vision techniques have been widely used in civil engineering to\nacquire 3-D road data. The two important factors of stereo vision are accuracy\nand speed. However, it is very challenging to achieve both of them\nsimultaneously and therefore the main aim of developing a stereo vision system\nis to improve the trade-off between these two factors. In this paper, we\npresent a real-time stereo vision system used for road surface 3-D\nreconstruction. The proposed system is developed from our previously published\n3-D reconstruction algorithm where the perspective view of the target image is\nfirst transformed into the reference view, which not only increases the\ndisparity accuracy but also improves the processing speed. Then, the\ncorrelation cost between each pair of blocks is computed and stored in two 3-D\ncost volumes. To adaptively aggregate the matching costs from neighbourhood\nsystems, bilateral filtering is performed on the cost volumes. This greatly\nreduces the ambiguities during stereo matching and further improves the\nprecision of the estimated disparities. Finally, the subpixel resolution is\nachieved by conducting a parabola interpolation and the subpixel disparity map\nis used to reconstruct the 3-D road surface. The proposed algorithm is\nimplemented on an NVIDIA GTX 1080 GPU for the real-time purpose. The\nexperimental results illustrate that the reconstruction accuracy is around 3\nmm. \n\n"}
{"id": "1807.07674", "contents": "Title: Bounding Box Embedding for Single Shot Person Instance Segmentation Abstract: We present a bottom-up approach for the task of object instance segmentation\nusing a single-shot model. The proposed model employs a fully convolutional\nnetwork which is trained to predict class-wise segmentation masks as well as\nthe bounding boxes of the object instances to which each pixel belongs. This\nallows us to group object pixels into individual instances. Our network\narchitecture is based on the DeepLabv3+ model, and requires only minimal extra\ncomputation to achieve pixel-wise instance assignments. We apply our method to\nthe task of person instance segmentation, a common task relevant to many\napplications. We train our model with COCO data and report competitive results\nfor the person class in the COCO instance segmentation task. \n\n"}
{"id": "1807.07688", "contents": "Title: Toward Characteristic-Preserving Image-based Virtual Try-On Network Abstract: Image-based virtual try-on systems for fitting new in-shop clothes into a\nperson image have attracted increasing research attention, yet is still\nchallenging. A desirable pipeline should not only transform the target clothes\ninto the most fitting shape seamlessly but also preserve well the clothes\nidentity in the generated image, that is, the key characteristics (e.g.\ntexture, logo, embroidery) that depict the original clothes. However, previous\nimage-conditioned generation works fail to meet these critical requirements\ntowards the plausible virtual try-on performance since they fail to handle\nlarge spatial misalignment between the input image and target clothes. Prior\nwork explicitly tackled spatial deformation using shape context matching, but\nfailed to preserve clothing details due to its coarse-to-fine strategy. In this\nwork, we propose a new fully-learnable Characteristic-Preserving Virtual Try-On\nNetwork(CP-VTON) for addressing all real-world challenges in this task. First,\nCP-VTON learns a thin-plate spline transformation for transforming the in-shop\nclothes into fitting the body shape of the target person via a new Geometric\nMatching Module (GMM) rather than computing correspondences of interest points\nas prior works did. Second, to alleviate boundary artifacts of warped clothes\nand make the results more realistic, we employ a Try-On Module that learns a\ncomposition mask to integrate the warped clothes and the rendered image to\nensure smoothness. Extensive experiments on a fashion dataset demonstrate our\nCP-VTON achieves the state-of-the-art virtual try-on performance both\nqualitatively and quantitatively. \n\n"}
{"id": "1807.07718", "contents": "Title: Efficient Facial Representations for Age, Gender and Identity\n  Recognition in Organizing Photo Albums using Multi-output CNN Abstract: This paper is focused on the automatic extraction of persons and their\nattributes (gender, year of born) from album of photos and videos. We propose\nthe two-stage approach, in which, firstly, the convolutional neural network\nsimultaneously predicts age/gender from all photos and additionally extracts\nfacial representations suitable for face identification. We modified the\nMobileNet, which is preliminarily trained to perform face recognition, in order\nto additionally recognize age and gender. In the second stage of our approach,\nextracted faces are grouped using hierarchical agglomerative clustering\ntechniques. The born year and gender of a person in each cluster are estimated\nusing aggregation of predictions for individual photos. We experimentally\ndemonstrated that our facial clustering quality is competitive with the\nstate-of-the-art neural networks, though our implementation is much\ncomputationally cheaper. Moreover, our approach is characterized by more\naccurate video-based age/gender recognition when compared to the publicly\navailable models. \n\n"}
{"id": "1807.07948", "contents": "Title: Optimize Deep Convolutional Neural Network with Ternarized Weights and\n  High Accuracy Abstract: Deep convolution neural network has achieved great success in many artificial\nintelligence applications. However, its enormous model size and massive\ncomputation cost have become the main obstacle for deployment of such powerful\nalgorithm in the low power and resource-limited embedded systems. As the\ncountermeasure to this problem, in this work, we propose statistical weight\nscaling and residual expansion methods to reduce the bit-width of the whole\nnetwork weight parameters to ternary values (i.e. -1, 0, +1), with the\nobjectives to greatly reduce model size, computation cost and accuracy\ndegradation caused by the model compression. With about 16x model compression\nrate, our ternarized ResNet-32/44/56 could outperform full-precision\ncounterparts by 0.12%, 0.24% and 0.18% on CIFAR- 10 dataset. We also test our\nternarization method with AlexNet and ResNet-18 on ImageNet dataset, which both\nachieve the best top-1 accuracy compared to recent similar works, with the same\n16x compression rate. If further incorporating our residual expansion method,\ncompared to the full-precision counterpart, our ternarized ResNet-18 even\nimproves the top-5 accuracy by 0.61% and merely degrades the top-1 accuracy\nonly by 0.42% for the ImageNet dataset, with 8x model compression rate. It\noutperforms the recent ABC-Net by 1.03% in top-1 accuracy and 1.78% in top-5\naccuracy, with around 1.25x higher compression rate and more than 6x\ncomputation reduction due to the weight sparsity. \n\n"}
{"id": "1807.08107", "contents": "Title: Person Search via A Mask-Guided Two-Stream CNN Model Abstract: In this work, we tackle the problem of person search, which is a challenging\ntask consisted of pedestrian detection and person re-identification~(re-ID).\nInstead of sharing representations in a single joint model, we find that\nseparating detector and re-ID feature extraction yields better performance. In\norder to extract more representative features for each identity, we segment out\nthe foreground person from the original image patch. We propose a simple yet\neffective re-ID method, which models foreground person and original image\npatches individually, and obtains enriched representations from two separate\nCNN streams. From the experiments on two standard person search benchmarks of\nCUHK-SYSU and PRW, we achieve mAP of $83.0\\%$ and $32.6\\%$ respectively,\nsurpassing the state of the art by a large margin (more than 5pp). \n\n"}
{"id": "1807.08368", "contents": "Title: Modeling Brain Networks with Artificial Neural Networks Abstract: In this study, we propose a neural network approach to capture the functional\nconnectivities among anatomic brain regions. The suggested approach estimates a\nset of brain networks, each of which represents the connectivity patterns of a\ncognitive process. We employ two different architectures of neural networks to\nextract directed and undirected brain networks from functional Magnetic\nResonance Imaging (fMRI) data. Then, we use the edge weights of the estimated\nbrain networks to train a classifier, namely, Support Vector Machines(SVM) to\nlabel the underlying cognitive process. We compare our brain network models\nwith popular models, which generate similar functional brain networks. We\nobserve that both undirected and directed brain networks surpass the\nperformances of the network models used in the fMRI literature. We also observe\nthat directed brain networks offer more discriminative features compared to the\nundirected ones for recognizing the cognitive processes. The representation\npower of the suggested brain networks are tested in a task-fMRI dataset of\nHuman Connectome Project and a Complex Problem Solving dataset. \n\n"}
{"id": "1807.09418", "contents": "Title: Video Storytelling: Textual Summaries for Events Abstract: Bridging vision and natural language is a longstanding goal in computer\nvision and multimedia research. While earlier works focus on generating a\nsingle-sentence description for visual content, recent works have studied\nparagraph generation. In this work, we introduce the problem of video\nstorytelling, which aims at generating coherent and succinct stories for long\nvideos. Video storytelling introduces new challenges, mainly due to the\ndiversity of the story and the length and complexity of the video. We propose\nnovel methods to address the challenges. First, we propose a context-aware\nframework for multimodal embedding learning, where we design a Residual\nBidirectional Recurrent Neural Network to leverage contextual information from\npast and future. Second, we propose a Narrator model to discover the underlying\nstoryline. The Narrator is formulated as a reinforcement learning agent which\nis trained by directly optimizing the textual metric of the generated story. We\nevaluate our method on the Video Story dataset, a new dataset that we have\ncollected to enable the study. We compare our method with multiple\nstate-of-the-art baselines, and show that our method achieves better\nperformance, in terms of quantitative measures and user study. \n\n"}
{"id": "1807.09441", "contents": "Title: Two at Once: Enhancing Learning and Generalization Capacities via\n  IBN-Net Abstract: Convolutional neural networks (CNNs) have achieved great successes in many\ncomputer vision problems. Unlike existing works that designed CNN architectures\nto improve performance on a single task of a single domain and not\ngeneralizable, we present IBN-Net, a novel convolutional architecture, which\nremarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as\nwell as its generalization capacity on another domain (e.g. GTA5) without\nfinetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch\nNormalization (BN) as building blocks, and can be wrapped into many advanced\ndeep networks to improve their performances. This work has three key\ncontributions. (1) By delving into IN and BN, we disclose that IN learns\nfeatures that are invariant to appearance changes, such as colors, styles, and\nvirtuality/reality, while BN is essential for preserving content related\ninformation. (2) IBN-Net can be applied to many advanced deep architectures,\nsuch as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their\nperformance without increasing computational cost. (3) When applying the\ntrained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves\ncomparable improvements as domain adaptation methods, even without using data\nfrom the target domain. With IBN-Net, we won the 1st place on the WAD 2018\nChallenge Drivable Area track, with an mIoU of 86.18%. \n\n"}
{"id": "1807.09607", "contents": "Title: Multi-Resolution Networks for Semantic Segmentation in Whole Slide\n  Images Abstract: Digital pathology provides an excellent opportunity for applying fully\nconvolutional networks (FCNs) to tasks, such as semantic segmentation of whole\nslide images (WSIs). However, standard FCNs face challenges with respect to\nmulti-resolution, inherited from the pyramid arrangement of WSIs. As a result,\nnetworks specifically designed to learn and aggregate information at different\nlevels are desired. In this paper, we propose two novel multi-resolution\nnetworks based on the popular `U-Net' architecture, which are evaluated on a\nbenchmark dataset for binary semantic segmentation in WSIs. The proposed\nmethods outperform the U-Net, demonstrating superior learning and\ngeneralization capabilities. \n\n"}
{"id": "1807.09993", "contents": "Title: Divide and Grow: Capturing Huge Diversity in Crowd Images with\n  Incrementally Growing CNN Abstract: Automated counting of people in crowd images is a challenging task. The major\ndifficulty stems from the large diversity in the way people appear in crowds.\nIn fact, features available for crowd discrimination largely depend on the\ncrowd density to the extent that people are only seen as blobs in a highly\ndense scene. We tackle this problem with a growing CNN which can progressively\nincrease its capacity to account for the wide variability seen in crowd scenes.\nOur model starts from a base CNN density regressor, which is trained in\nequivalence on all types of crowd images. In order to adapt with the huge\ndiversity, we create two child regressors which are exact copies of the base\nCNN. A differential training procedure divides the dataset into two clusters\nand fine-tunes the child networks on their respective specialties.\nConsequently, without any hand-crafted criteria for forming specialties, the\nchild regressors become experts on certain types of crowds. The child networks\nare again split recursively, creating two experts at every division. This\nhierarchical training leads to a CNN tree, where the child regressors are more\nfine experts than any of their parents. The leaf nodes are taken as the final\nexperts and a classifier network is then trained to predict the correct\nspecialty for a given test image patch. The proposed model achieves higher\ncount accuracy on major crowd datasets. Further, we analyse the characteristics\nof specialties mined automatically by our method. \n\n"}
{"id": "1807.10569", "contents": "Title: The Helmholtz Method: Using Perceptual Compression to Reduce Machine\n  Learning Complexity Abstract: This paper proposes a fundamental answer to a frequently asked question in\nmultimedia computing and machine learning: Do artifacts from perceptual\ncompression contribute to error in the machine learning process and if so, how\nmuch? Our approach to the problem is a reinterpretation of the Helmholtz Free\nEnergy formula from physics to explain the relationship between content and\nnoise when using sensors (such as cameras or microphones) to capture multimedia\ndata. The reinterpretation allows a bit-measurement of the noise contained in\nimages, audio, and video by combining a classifier with perceptual compression,\nsuch as JPEG or MP3. Our experiments on CIFAR-10 as well as Fraunhofer's\nIDMT-SMT-Audio-Effects dataset indicate that, at the right quality level,\nperceptual compression is actually not harmful but contributes to a significant\nreduction of complexity of the machine learning process. That is, our noise\nquantification method can be used to speed up the training of deep learning\nclassifiers significantly while maintaining, or sometimes even improving,\noverall classification accuracy. Moreover, our results provide insights into\nthe reasons for the success of deep learning. \n\n"}
{"id": "1807.10588", "contents": "Title: A Modality-Adaptive Method for Segmenting Brain Tumors and\n  Organs-at-Risk in Radiation Therapy Planning Abstract: In this paper we present a method for simultaneously segmenting brain tumors\nand an extensive set of organs-at-risk for radiation therapy planning of\nglioblastomas. The method combines a contrast-adaptive generative model for\nwhole-brain segmentation with a new spatial regularization model of tumor shape\nusing convolutional restricted Boltzmann machines. We demonstrate\nexperimentally that the method is able to adapt to image acquisitions that\ndiffer substantially from any available training data, ensuring its\napplicability across treatment sites; that its tumor segmentation accuracy is\ncomparable to that of the current state of the art; and that it captures most\norgans-at-risk sufficiently well for radiation therapy planning purposes. The\nproposed method may be a valuable step towards automating the delineation of\nbrain tumors and organs-at-risk in glioblastoma patients undergoing radiation\ntherapy. \n\n"}
{"id": "1807.10603", "contents": "Title: A Capsule Network for Traffic Speed Prediction in Complex Road Networks Abstract: This paper proposes a deep learning approach for traffic flow prediction in\ncomplex road networks. Traffic flow data from induction loop sensors are\nessentially a time series, which is also spatially related to traffic in\ndifferent road segments. The spatio-temporal traffic data can be converted into\nan image where the traffic data are expressed in a 3D space with respect to\nspace and time axes. Although convolutional neural networks (CNNs) have been\nshowing surprising performance in understanding images, they have a major\ndrawback. In the max pooling operation, CNNs are losing important information\nby locally taking the highest activation values. The inter-relationship in\ntraffic data measured by sparsely located sensors in different time intervals\nshould not be neglected in order to obtain accurate predictions. Thus, we\npropose a neural network with capsules that replaces max pooling by dynamic\nrouting. This is the first approach that employs the capsule network on a time\nseries forecasting problem, to our best knowledge. Moreover, an experiment on\nreal traffic speed data measured in the Santander city of Spain demonstrates\nthe proposed method outperforms the state-of-the-art method based on a CNN by\n13.1% in terms of root mean squared error. \n\n"}
{"id": "1807.10711", "contents": "Title: Deep Learning Methods and Applications for Region of Interest Detection\n  in Dermoscopic Images Abstract: Rapid growth in the development of medical imaging analysis technology has\nbeen propelled by the great interest in improving computer-aided diagnosis and\ndetection (CAD) systems for three popular image visualization tasks:\nclassification, segmentation, and Region of Interest (ROI) detection. However,\na limited number of datasets with ground truth annotations are available for\ndeveloping segmentation and ROI detection of lesions, as expert annotations are\nlaborious and expensive. Detecting the ROI is vital to locate lesions\naccurately. In this paper, we propose the use of two deep object detection\nmeta-architectures (Faster R-CNN Inception-V2 and SSD Inception-V2) to develop\nrobust ROI detection of skin lesions in dermoscopic datasets (2017 ISIC\nChallenge, PH2, and HAM10000), and compared the performance with\nstate-of-the-art segmentation algorithm (DeeplabV3+). To further demonstrate\nthe potential of our work, we built a smartphone application for real-time\nautomated detection of skin lesions based on this methodology. In addition, we\ndeveloped an automated natural data-augmentation method from ROI detection to\nproduce augmented copies of dermoscopic images, as a pre-processing step in the\nsegmentation of skin lesions to further improve the performance of the current\nstate-of-the-art deep learning algorithm. Our proposed ROI detection has the\npotential to more appropriately streamline dermatology referrals and reduce\nunnecessary biopsies in the diagnosis of skin cancer. \n\n"}
{"id": "1807.10889", "contents": "Title: Pairwise Body-Part Attention for Recognizing Human-Object Interactions Abstract: In human-object interactions (HOI) recognition, conventional methods consider\nthe human body as a whole and pay a uniform attention to the entire body\nregion. They ignore the fact that normally, human interacts with an object by\nusing some parts of the body. In this paper, we argue that different body parts\nshould be paid with different attention in HOI recognition, and the\ncorrelations between different body parts should be further considered. This is\nbecause our body parts always work collaboratively. We propose a new pairwise\nbody-part attention model which can learn to focus on crucial parts, and their\ncorrelations for HOI recognition. A novel attention based feature selection\nmethod and a feature representation scheme that can capture pairwise\ncorrelations between body parts are introduced in the model. Our proposed\napproach achieved 4% improvement over the state-of-the-art results in HOI\nrecognition on the HICO dataset. We will make our model and source codes\npublicly available. \n\n"}
{"id": "1807.11013", "contents": "Title: Tiny-DSOD: Lightweight Object Detection for Resource-Restricted Usages Abstract: Object detection has made great progress in the past few years along with the\ndevelopment of deep learning. However, most current object detection methods\nare resource hungry, which hinders their wide deployment to many resource\nrestricted usages such as usages on always-on devices, battery-powered low-end\ndevices, etc. This paper considers the resource and accuracy trade-off for\nresource-restricted usages during designing the whole object detection\nframework. Based on the deeply supervised object detection (DSOD) framework, we\npropose Tiny-DSOD dedicating to resource-restricted usages. Tiny-DSOD\nintroduces two innovative and ultra-efficient architecture blocks: depthwise\ndense block (DDB) based backbone and depthwise feature-pyramid-network (D-FPN)\nbased front-end. We conduct extensive experiments on three famous benchmarks\n(PASCAL VOC 2007, KITTI, and COCO), and compare Tiny-DSOD to the\nstate-of-the-art ultra-efficient object detection solutions such as Tiny-YOLO,\nMobileNet-SSD (v1 & v2), SqueezeDet, Pelee, etc. Results show that Tiny-DSOD\noutperforms these solutions in all the three metrics (parameter-size, FLOPs,\naccuracy) in each comparison. For instance, Tiny-DSOD achieves 72.1% mAP with\nonly 0.95M parameters and 1.06B FLOPs, which is by far the state-of-the-arts\nresult with such a low resource requirement. \n\n"}
{"id": "1807.11035", "contents": "Title: Texture Mixing by Interpolating Deep Statistics via Gaussian Models Abstract: Recently, enthusiastic studies have devoted to texture synthesis using deep\nneural networks, because these networks excel at handling complex patterns in\nimages. In these models, second-order statistics, such as Gram matrix, are used\nto describe textures. Despite the fact that these model have achieved promising\nresults, the structure of their parametric space is still unclear,\nconsequently, it is difficult to use them to mix textures. This paper addresses\nthe texture mixing problem by using a Gaussian scheme to interpolate deep\nstatistics computed from deep neural networks. More precisely, we first reveal\nthat the statistics used in existing deep models can be unified using a\nstationary Gaussian scheme. We then present a novel algorithm to mix these\nstatistics by interpolating between Gaussian models using optimal transport. We\nfurther apply our scheme to Neural Style Transfer, where we can create mixed\nstyles. The experiments demonstrate that our method can achieve\nstate-of-the-art results. Because all the computations are implemented in\nclosed forms, our mixing algorithm adds only negligible time to the original\ntexture synthesis procedure. \n\n"}
{"id": "1807.11037", "contents": "Title: Efficient Uncertainty Estimation for Semantic Segmentation in Videos Abstract: Uncertainty estimation in deep learning becomes more important recently. A\ndeep learning model can't be applied in real applications if we don't know\nwhether the model is certain about the decision or not. Some literature\nproposes the Bayesian neural network which can estimate the uncertainty by\nMonte Carlo Dropout (MC dropout). However, MC dropout needs to forward the\nmodel $N$ times which results in $N$ times slower. For real-time applications\nsuch as a self-driving car system, which needs to obtain the prediction and the\nuncertainty as fast as possible, so that MC dropout becomes impractical. In\nthis work, we propose the region-based temporal aggregation (RTA) method which\nleverages the temporal information in videos to simulate the sampling\nprocedure. Our RTA method with Tiramisu backbone is 10x faster than the MC\ndropout with Tiramisu backbone ($N=5$). Furthermore, the uncertainty estimation\nobtained by our RTA method is comparable to MC dropout's uncertainty estimation\non pixel-level and frame-level metrics. \n\n"}
{"id": "1807.11042", "contents": "Title: Towards Good Practices on Building Effective CNN Baseline Model for\n  Person Re-identification Abstract: Person re-identification is indeed a challenging visual recognition task due\nto the critical issues of human pose variation, human body occlusion, camera\nview variation, etc. To address this, most of the state-of-the-art approaches\nare proposed based on deep convolutional neural network (CNN), being leveraged\nby its strong feature learning power and classification boundary fitting\ncapacity. Although the vital role towards person re-identification, how to\nbuild effective CNN baseline model has not been well studied yet. To answer\nthis open question, we propose 3 good practices in this paper from the\nperspectives of adjusting CNN architecture and training procedure. In\nparticular, they are adding batch normalization after the global pooling layer,\nexecuting identity categorization directly using only one fully-connected, and\nusing Adam as optimizer. The extensive experiments on 3 widely-used benchmark\ndatasets demonstrate that, our propositions essentially facilitate the CNN\nbaseline model to achieve the state-of-the-art performance without any other\nhigh-level domain knowledge or low-level technical trick. \n\n"}
{"id": "1807.11122", "contents": "Title: Story Understanding in Video Advertisements Abstract: In order to resonate with the viewers, many video advertisements explore\ncreative narrative techniques such as \"Freytag's pyramid\" where a story begins\nwith exposition, followed by rising action, then climax, concluding with\ndenouement. In the dramatic structure of ads in particular, climax depends on\nchanges in sentiment. We dedicate our study to understand the dynamic structure\nof video ads automatically. To achieve this, we first crowdsource climax\nannotations on 1,149 videos from the Video Ads Dataset, which already provides\nsentiment annotations. We then use both unsupervised and supervised methods to\npredict the climax. Based on the predicted peak, the low-level visual and audio\ncues, and semantically meaningful context features, we build a sentiment\nprediction model that outperforms the current state-of-the-art model of\nsentiment prediction in video ads by 25%. In our ablation study, we show that\nusing our context features, and modeling dynamics with an LSTM, are both\ncrucial factors for improved performance. \n\n"}
{"id": "1807.11206", "contents": "Title: Hard-Aware Point-to-Set Deep Metric for Person Re-identification Abstract: Person re-identification (re-ID) is a highly challenging task due to large\nvariations of pose, viewpoint, illumination, and occlusion. Deep metric\nlearning provides a satisfactory solution to person re-ID by training a deep\nnetwork under supervision of metric loss, e.g., triplet loss. However, the\nperformance of deep metric learning is greatly limited by traditional sampling\nmethods. To solve this problem, we propose a Hard-Aware Point-to-Set (HAP2S)\nloss with a soft hard-mining scheme. Based on the point-to-set triplet loss\nframework, the HAP2S loss adaptively assigns greater weights to harder samples.\nSeveral advantageous properties are observed when compared with other\nstate-of-the-art loss functions: 1) Accuracy: HAP2S loss consistently achieves\nhigher re-ID accuracies than other alternatives on three large-scale benchmark\ndatasets; 2) Robustness: HAP2S loss is more robust to outliers than other\nlosses; 3) Flexibility: HAP2S loss does not rely on a specific weight function,\ni.e., different instantiations of HAP2S loss are equally effective. 4)\nGenerality: In addition to person re-ID, we apply the proposed method to\ngeneric deep metric learning benchmarks including CUB-200-2011 and Cars196, and\nalso achieve state-of-the-art results. \n\n"}
{"id": "1807.11254", "contents": "Title: Extreme Network Compression via Filter Group Approximation Abstract: In this paper we propose a novel decomposition method based on filter group\napproximation, which can significantly reduce the redundancy of deep\nconvolutional neural networks (CNNs) while maintaining the majority of feature\nrepresentation. Unlike other low-rank decomposition algorithms which operate on\nspatial or channel dimension of filters, our proposed method mainly focuses on\nexploiting the filter group structure for each layer. For several commonly used\nCNN models, including VGG and ResNet, our method can reduce over 80%\nfloating-point operations (FLOPs) with less accuracy drop than state-of-the-art\nmethods on various image classification datasets. Besides, experiments\ndemonstrate that our method is conducive to alleviating degeneracy of the\ncompressed network, which hurts the convergence and performance of the network. \n\n"}
{"id": "1808.00171", "contents": "Title: Shuffle-Then-Assemble: Learning Object-Agnostic Visual Relationship\n  Features Abstract: Due to the fact that it is prohibitively expensive to completely annotate\nvisual relationships, i.e., the (obj1, rel, obj2) triplets, relationship models\nare inevitably biased to object classes of limited pairwise patterns, leading\nto poor generalization to rare or unseen object combinations. Therefore, we are\ninterested in learning object-agnostic visual features for more generalizable\nrelationship models. By \"agnostic\", we mean that the feature is less likely\nbiased to the classes of paired objects. To alleviate the bias, we propose a\nnovel \\texttt{Shuffle-Then-Assemble} pre-training strategy. First, we discard\nall the triplet relationship annotations in an image, leaving two unpaired\nobject domains without obj1-obj2 alignment. Then, our feature learning is to\nrecover possible obj1-obj2 pairs. In particular, we design a cycle of residual\ntransformations between the two domains, to capture shared but not\nobject-specific visual patterns. Extensive experiments on two visual\nrelationship benchmarks show that by using our pre-trained features, naive\nrelationship models can be consistently improved and even outperform other\nstate-of-the-art relationship models. Code has been made available at:\n\\url{https://github.com/yangxuntu/vrd}. \n\n"}
{"id": "1808.00193", "contents": "Title: Reinforced Evolutionary Neural Architecture Search Abstract: Neural Architecture Search (NAS) is an important yet challenging task in\nnetwork design due to its high computational consumption. To address this\nissue, we propose the Reinforced Evolutionary Neural Architecture Search (RE-\nNAS), which is an evolutionary method with the reinforced mutation for NAS. Our\nmethod integrates reinforced mutation into an evolution algorithm for neural\narchitecture exploration, in which a mutation controller is introduced to learn\nthe effects of slight modifications and make mutation actions. The reinforced\nmutation controller guides the model population to evolve efficiently.\nFurthermore, as child models can inherit parameters from their parents during\nevolution, our method requires very limited computational resources. In\nexperiments, we conduct the proposed search method on CIFAR-10 and obtain a\npowerful network architecture, RENASNet. This architecture achieves a\ncompetitive result on CIFAR-10. The explored network architecture is\ntransferable to ImageNet and achieves a new state-of-the-art accuracy, i.e.,\n75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test\nits performance on semantic segmentation with DeepLabv3 on the PASCAL VOC.\nRENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83%\nmIOU without being pre-trained on COCO. \n\n"}
{"id": "1808.00495", "contents": "Title: Semantic Classification of 3D Point Clouds with Multiscale Spherical\n  Neighborhoods Abstract: This paper introduces a new definition of multiscale neighborhoods in 3D\npoint clouds. This definition, based on spherical neighborhoods and\nproportional subsampling, allows the computation of features with a consistent\ngeometrical meaning, which is not the case when using k-nearest neighbors. With\nan appropriate learning strategy, the proposed features can be used in a random\nforest to classify 3D points. In this semantic classification task, we show\nthat our multiscale features outperform state-of-the-art features using the\nsame experimental conditions. Furthermore, their classification power competes\nwith more elaborate classification approaches including Deep Learning methods. \n\n"}
{"id": "1808.00739", "contents": "Title: Deeply Self-Supervised Contour Embedded Neural Network Applied to Liver\n  Segmentation Abstract: Objective: Herein, a neural network-based liver segmentation algorithm is\nproposed, and its performance was evaluated using abdominal computed tomography\n(CT) images. Methods: A fully convolutional network was developed to overcome\nthe volumetric image segmentation problem. To guide a neural network to\naccurately delineate a target liver object, the network was deeply supervised\nby applying the adaptive self-supervision scheme to derive the essential\ncontour, which acted as a complement with the global shape. The discriminative\ncontour, shape, and deep features were internally merged for the segmentation\nresults. Results and Conclusion: 160 abdominal CT images were used for training\nand validation. The quantitative evaluation of the proposed network was\nperformed through an eight-fold cross-validation. The result showed that the\nmethod, which uses the contour feature, segmented the liver more accurately\nthan the state-of-the-art with a 2.13% improvement in the dice score.\nSignificance: In this study, a new framework was introduced to guide a neural\nnetwork and learn complementary contour features. The proposed neural network\ndemonstrates that the guided contour features can significantly improve the\nperformance of the segmentation task. \n\n"}
{"id": "1808.00928", "contents": "Title: Learning Actionable Representations from Visual Observations Abstract: In this work we explore a new approach for robots to teach themselves about\nthe world simply by observing it. In particular we investigate the\neffectiveness of learning task-agnostic representations for continuous control\ntasks. We extend Time-Contrastive Networks (TCN) that learn from visual\nobservations by embedding multiple frames jointly in the embedding space as\nopposed to a single frame. We show that by doing so, we are now able to encode\nboth position and velocity attributes significantly more accurately. We test\nthe usefulness of this self-supervised approach in a reinforcement learning\nsetting. We show that the representations learned by agents observing\nthemselves take random actions, or other agents perform tasks successfully, can\nenable the learning of continuous control policies using algorithms like\nProximal Policy Optimization (PPO) using only the learned embeddings as input.\nWe also demonstrate significant improvements on the real-world Pouring dataset\nwith a relative error reduction of 39.4% for motion attributes and 11.1% for\nstatic attributes compared to the single-frame baseline. Video results are\navailable at https://sites.google.com/view/actionablerepresentations . \n\n"}
{"id": "1808.01119", "contents": "Title: Multi-shot Person Re-identification through Set Distance with Visual\n  Distributional Representation Abstract: Person re-identification aims to identify a specific person at distinct times\nand locations. It is challenging because of occlusion, illumination, and\nviewpoint change in camera views. Recently, multi-shot person re-id task\nreceives more attention since it is closer to real-world application. A key\npoint of a good algorithm for multi-shot person re-id is the temporal\naggregation of the person appearance features. While most of the current\napproaches apply pooling strategies and obtain a fixed-size vector\nrepresentation, these may lose the matching evidence between examples. In this\nwork, we propose the idea of visual distributional representation, which\ninterprets an image set as samples drawn from an unknown distribution in\nappearance feature space. Based on the supervision signals from a downstream\ntask of interest, the method reshapes the appearance feature space and further\nlearns the unknown distribution of each image set. In the context of multi-shot\nperson re-id, we apply this novel concept along with Wasserstein distance and\nlearn a distributional set distance function between two image sets. In this\nway, the proper alignment between two image sets can be discovered naturally in\na non-parametric manner. Our experiment results on two public datasets show the\nadvantages of our proposed method compared to other state-of-the-art\napproaches. \n\n"}
{"id": "1808.01415", "contents": "Title: On Lipschitz Bounds of General Convolutional Neural Networks Abstract: Many convolutional neural networks (CNNs) have a feed-forward structure. In\nthis paper, a linear program that estimates the Lipschitz bound of such CNNs is\nproposed. Several CNNs, including the scattering networks, the AlexNet and the\nGoogleNet, are studied numerically and compared to the theoretical bounds.\nNext, concentration inequalities of the output distribution to a stationary\nrandom input signal expressed in terms of the Lipschitz bound are established.\nThe Lipschitz bound is further used to establish a nonlinear discriminant\nanalysis designed to measure the separation between features of different\nclasses. \n\n"}
{"id": "1808.02992", "contents": "Title: Controllable Image-to-Video Translation: A Case Study on Facial\n  Expression Generation Abstract: The recent advances in deep learning have made it possible to generate\nphoto-realistic images by using neural networks and even to extrapolate video\nframes from an input video clip. In this paper, for the sake of both furthering\nthis exploration and our own interest in a realistic application, we study\nimage-to-video translation and particularly focus on the videos of facial\nexpressions. This problem challenges the deep neural networks by another\ntemporal dimension comparing to the image-to-image translation. Moreover, its\nsingle input image fails most existing video generation methods that rely on\nrecurrent models. We propose a user-controllable approach so as to generate\nvideo clips of various lengths from a single face image. The lengths and types\nof the expressions are controlled by users. To this end, we design a novel\nneural network architecture that can incorporate the user input into its skip\nconnections and propose several improvements to the adversarial training method\nfor the neural network. Experiments and user studies verify the effectiveness\nof our approach. Especially, we would like to highlight that even for the face\nimages in the wild (downloaded from the Web and the authors' own photos), our\nmodel can generate high-quality facial expression videos of which about 50\\%\nare labeled as real by Amazon Mechanical Turk workers. \n\n"}
{"id": "1808.03195", "contents": "Title: Overcoming Missing and Incomplete Modalities with Generative Adversarial\n  Networks for Building Footprint Segmentation Abstract: The integration of information acquired with different modalities, spatial\nresolution and spectral bands has shown to improve predictive accuracies. Data\nfusion is therefore one of the key challenges in remote sensing. Most prior\nwork focusing on multi-modal fusion, assumes that modalities are always\navailable during inference. This assumption limits the applications of\nmulti-modal models since in practice the data collection process is likely to\ngenerate data with missing, incomplete or corrupted modalities. In this paper,\nwe show that Generative Adversarial Networks can be effectively used to\novercome the problems that arise when modalities are missing or incomplete.\nFocusing on semantic segmentation of building footprints with missing\nmodalities, our approach achieves an improvement of about 2% on the\nIntersection over Union (IoU) against the same network that relies only on the\navailable modality. \n\n"}
{"id": "1808.04325", "contents": "Title: Improving Shape Deformation in Unsupervised Image-to-Image Translation Abstract: Unsupervised image-to-image translation techniques are able to map local\ntexture between two domains, but they are typically unsuccessful when the\ndomains require larger shape change. Inspired by semantic segmentation, we\nintroduce a discriminator with dilated convolutions that is able to use\ninformation from across the entire image to train a more context-aware\ngenerator. This is coupled with a multi-scale perceptual loss that is better\nable to represent error in the underlying shape of objects. We demonstrate that\nthis design is more capable of representing shape deformation in a challenging\ntoy dataset, plus in complex mappings with significant dataset variation\nbetween humans, dolls, and anime faces, and between cats and dogs. \n\n"}
{"id": "1808.04859", "contents": "Title: GestureGAN for Hand Gesture-to-Gesture Translation in the Wild Abstract: Hand gesture-to-gesture translation in the wild is a challenging task since\nhand gestures can have arbitrary poses, sizes, locations and self-occlusions.\nTherefore, this task requires a high-level understanding of the mapping between\nthe input source gesture and the output target gesture. To tackle this problem,\nwe propose a novel hand Gesture Generative Adversarial Network (GestureGAN).\nGestureGAN consists of a single generator $G$ and a discriminator $D$, which\ntakes as input a conditional hand image and a target hand skeleton image.\nGestureGAN utilizes the hand skeleton information explicitly, and learns the\ngesture-to-gesture mapping through two novel losses, the color loss and the\ncycle-consistency loss. The proposed color loss handles the issue of \"channel\npollution\" while back-propagating the gradients. In addition, we present the\nFr\\'echet ResNet Distance (FRD) to evaluate the quality of generated images.\nExtensive experiments on two widely used benchmark datasets demonstrate that\nthe proposed GestureGAN achieves state-of-the-art performance on the\nunconstrained hand gesture-to-gesture translation task. Meanwhile, the\ngenerated images are in high-quality and are photo-realistic, allowing them to\nbe used as data augmentation to improve the performance of a hand gesture\nclassifier. Our model and code are available at\nhttps://github.com/Ha0Tang/GestureGAN. \n\n"}
{"id": "1808.04909", "contents": "Title: Vendor-independent soft tissue lesion detection using weakly supervised\n  and unsupervised adversarial domain adaptation Abstract: Computer-aided detection aims to improve breast cancer screening programs by\nhelping radiologists to evaluate digital mammography (DM) exams. DM exams are\ngenerated by devices from different vendors, with diverse characteristics\nbetween and even within vendors. Physical properties of these devices and\npostprocessing of the images can greatly influence the resulting mammogram.\nThis results in the fact that a deep learning model trained on data from one\nvendor cannot readily be applied to data from another vendor. This paper\ninvestigates the use of tailored transfer learning methods based on adversarial\nlearning to tackle this problem. We consider a database of DM exams (mostly\nbilateral and two views) generated by Hologic and Siemens vendors. We analyze\ntwo transfer learning settings: 1) unsupervised transfer, where Hologic data\nwith soft lesion annotation at pixel level and Siemens unlabelled data are used\nto annotate images in the latter data; 2) weak supervised transfer, where exam\nlevel labels for images from the Siemens mammograph are available. We propose\ntailored variants of recent state-of-the-art methods for transfer learning\nwhich take into account the class imbalance and incorporate knowledge provided\nby the annotations at exam level. Results of experiments indicate the\nbeneficial effect of transfer learning in both transfer settings. Notably, at\n0.02 false positives per image, we achieve a sensitivity of 0.37, compared to\n0.30 of a baseline with no transfer. Results indicate that using exam level\nannotations gives an additional increase in sensitivity. \n\n"}
{"id": "1808.05071", "contents": "Title: Ensemble of Convolutional Neural Networks for Dermoscopic Images\n  Classification Abstract: In this report, we are presenting our automated prediction system for disease\nclassification within dermoscopic images. The proposed solution is based on\ndeep learning, where we employed transfer learning strategy on VGG16 and\nGoogLeNet architectures. The key feature of our solution is preprocessing based\nprimarily on image augmentation and colour normalization. The solution was\nevaluated on Task 3: Lesion Diagnosis of the ISIC 2018: Skin Lesion Analysis\nTowards Melanoma Detection. \n\n"}
{"id": "1808.05517", "contents": "Title: Network Decoupling: From Regular to Depthwise Separable Convolutions Abstract: Depthwise separable convolution has shown great efficiency in network design,\nbut requires time-consuming training procedure with full training-set\navailable. This paper first analyzes the mathematical relationship between\nregular convolutions and depthwise separable convolutions, and proves that the\nformer one could be approximated with the latter one in closed form. We show\ndepthwise separable convolutions are principal components of regular\nconvolutions. And then we propose network decoupling (ND), a training-free\nmethod to accelerate convolutional neural networks (CNNs) by transferring\npre-trained CNN models into the MobileNet-like depthwise separable convolution\nstructure, with a promising speedup yet negligible accuracy loss. We further\nverify through experiments that the proposed method is orthogonal to other\ntraining-free methods like channel decomposition, spatial decomposition, etc.\nCombining the proposed method with them will bring even larger CNN speedup. For\ninstance, ND itself achieves about 2X speedup for the widely used VGG16, and\ncombined with other methods, it reaches 3.7X speedup with graceful accuracy\ndegradation. We demonstrate that ND is widely applicable to classification\nnetworks like ResNet, and object detection network like SSD300. \n\n"}
{"id": "1808.06801", "contents": "Title: Text-to-image Synthesis via Symmetrical Distillation Networks Abstract: Text-to-image synthesis aims to automatically generate images according to\ntext descriptions given by users, which is a highly challenging task. The main\nissues of text-to-image synthesis lie in two gaps: the heterogeneous and\nhomogeneous gaps. The heterogeneous gap is between the high-level concepts of\ntext descriptions and the pixel-level contents of images, while the homogeneous\ngap exists between synthetic image distributions and real image distributions.\nFor addressing these problems, we exploit the excellent capability of generic\ndiscriminative models (e.g. VGG19), which can guide the training process of a\nnew generative model on multiple levels to bridge the two gaps. The high-level\nrepresentations can teach the generative model to extract necessary visual\ninformation from text descriptions, which can bridge the heterogeneous gap. The\nmid-level and low-level representations can lead it to learn structures and\ndetails of images respectively, which relieves the homogeneous gap. Therefore,\nwe propose Symmetrical Distillation Networks (SDN) composed of a source\ndiscriminative model as \"teacher\" and a target generative model as \"student\".\nThe target generative model has a symmetrical structure with the source\ndiscriminative model, in order to transfer hierarchical knowledge accessibly.\nMoreover, we decompose the training process into two stages with different\ndistillation paradigms for promoting the performance of the target generative\nmodel. Experiments on two widely-used datasets are conducted to verify the\neffectiveness of our proposed SDN. \n\n"}
{"id": "1808.07269", "contents": "Title: A Deep Neural Network for Pixel-Level Electromagnetic Particle\n  Identification in the MicroBooNE Liquid Argon Time Projection Chamber Abstract: We have developed a convolutional neural network (CNN) that can make a\npixel-level prediction of objects in image data recorded by a liquid argon time\nprojection chamber (LArTPC) for the first time. We describe the network design,\ntraining techniques, and software tools developed to train this network. The\ngoal of this work is to develop a complete deep neural network based data\nreconstruction chain for the MicroBooNE detector. We show the first\ndemonstration of a network's validity on real LArTPC data using MicroBooNE\ncollection plane images. The demonstration is performed for stopping muon and a\n$\\nu_\\mu$ charged current neutral pion data samples. \n\n"}
{"id": "1808.07413", "contents": "Title: Manipulating Attributes of Natural Scenes via Hallucination Abstract: In this study, we explore building a two-stage framework for enabling users\nto directly manipulate high-level attributes of a natural scene. The key to our\napproach is a deep generative network which can hallucinate images of a scene\nas if they were taken at a different season (e.g. during winter), weather\ncondition (e.g. in a cloudy day) or time of the day (e.g. at sunset). Once the\nscene is hallucinated with the given attributes, the corresponding look is then\ntransferred to the input image while preserving the semantic details intact,\ngiving a photo-realistic manipulation result. As the proposed framework\nhallucinates what the scene will look like, it does not require any reference\nstyle image as commonly utilized in most of the appearance or style transfer\napproaches. Moreover, it allows to simultaneously manipulate a given scene\naccording to a diverse set of transient attributes within a single model,\neliminating the need of training multiple networks per each translation task.\nOur comprehensive set of qualitative and quantitative results demonstrate the\neffectiveness of our approach against the competing methods. \n\n"}
{"id": "1808.07416", "contents": "Title: Joint Coarse-And-Fine Reasoning for Deep Optical Flow Abstract: We propose a novel representation for dense pixel-wise estimation tasks using\nCNNs that boosts accuracy and reduces training time, by explicitly exploiting\njoint coarse-and-fine reasoning. The coarse reasoning is performed over a\ndiscrete classification space to obtain a general rough solution, while the\nfine details of the solution are obtained over a continuous regression space.\nIn our approach both components are jointly estimated, which proved to be\nbeneficial for improving estimation accuracy. Additionally, we propose a new\nnetwork architecture, which combines coarse and fine components by treating the\nfine estimation as a refinement built on top of the coarse solution, and\ntherefore adding details to the general prediction. We apply our approach to\nthe challenging problem of optical flow estimation and empirically validate it\nagainst state-of-the-art CNN-based solutions trained from scratch and tested on\nlarge optical flow datasets. \n\n"}
{"id": "1808.07503", "contents": "Title: Second-order Democratic Aggregation Abstract: Aggregated second-order features extracted from deep convolutional networks\nhave been shown to be effective for texture generation, fine-grained\nrecognition, material classification, and scene understanding. In this paper,\nwe study a class of orderless aggregation functions designed to minimize\ninterference or equalize contributions in the context of second-order features\nand we show that they can be computed just as efficiently as their first-order\ncounterparts and they have favorable properties over aggregation by summation.\nAnother line of work has shown that matrix power normalization after\naggregation can significantly improve the generalization of second-order\nrepresentations. We show that matrix power normalization implicitly equalizes\ncontributions during aggregation thus establishing a connection between matrix\nnormalization techniques and prior work on minimizing interference. Based on\nthe analysis we present {\\gamma}-democratic aggregators that interpolate\nbetween sum ({\\gamma}=1) and democratic pooling ({\\gamma}=0) outperforming both\non several classification tasks. Moreover, unlike power normalization, the\n{\\gamma}-democratic aggregations can be computed in a low dimensional space by\nsketching that allows the use of very high-dimensional second-order features.\nThis results in a state-of-the-art performance on several datasets. \n\n"}
{"id": "1808.07528", "contents": "Title: Rethinking Monocular Depth Estimation with Adversarial Training Abstract: Monocular depth estimation is an extensively studied computer vision problem\nwith a vast variety of applications. Deep learning-based methods have\ndemonstrated promise for both supervised and unsupervised depth estimation from\nmonocular images. Most existing approaches treat depth estimation as a\nregression problem with a local pixel-wise loss function. In this work, we\ninnovate beyond existing approaches by using adversarial training to learn a\ncontext-aware, non-local loss function. Such an approach penalizes the joint\nconfiguration of predicted depth values at the patch-level instead of the\npixel-level, which allows networks to incorporate more global information. In\nthis framework, the generator learns a mapping between RGB images and its\ncorresponding depth map, while the discriminator learns to distinguish depth\nmap and RGB pairs from ground truth. This conditional GAN depth estimation\nframework is stabilized using spectral normalization to prevent mode collapse\nwhen learning from diverse datasets. We test this approach using a diverse set\nof generators that include U-Net and joint CNN-CRF. We benchmark this approach\non the NYUv2, Make3D and KITTI datasets, and observe that adversarial training\nreduces relative error by several fold, achieving state-of-the-art performance. \n\n"}
{"id": "1808.07553", "contents": "Title: Predictive Image Regression for Longitudinal Studies with Missing Data Abstract: In this paper, we propose a predictive regression model for longitudinal\nimages with missing data based on large deformation diffeomorphic metric\nmapping (LDDMM) and deep neural networks. Instead of directly predicting image\nscans, our model predicts a vector momentum sequence associated with a baseline\nimage. This momentum sequence parameterizes the original image sequence in the\nLDDMM framework and lies in the tangent space of the baseline image, which is\nEuclidean. A recurrent network with long term-short memory (LSTM) units encodes\nthe time-varying changes in the vector-momentum sequence, and a convolutional\nneural network (CNN) encodes the baseline image of the vector momenta. Features\nextracted by the LSTM and CNN are fed into a decoder network to reconstruct the\nvector momentum sequence, which is used for the image sequence prediction by\ndeforming the baseline image with LDDMM shooting. To handle the missing images\nat some time points, we adopt a binary mask to ignore their reconstructions in\nthe loss calculation. We evaluate our model on synthetically generated images\nand the brain MRIs from the OASIS dataset. Experimental results demonstrate the\npromising predictions of the spatiotemporal changes in both datasets,\nirrespective of large or subtle changes in longitudinal image sequences. \n\n"}
{"id": "1808.07793", "contents": "Title: Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval Abstract: Cross-modal retrieval between visual data and natural language description\nremains a long-standing challenge in multimedia. While recent image-text\nretrieval methods offer great promise by learning deep representations aligned\nacross modalities, most of these methods are plagued by the issue of training\nwith small-scale datasets covering a limited number of images with ground-truth\nsentences. Moreover, it is extremely expensive to create a larger dataset by\nannotating millions of images with sentences and may lead to a biased model.\nInspired by the recent success of webly supervised learning in deep neural\nnetworks, we capitalize on readily-available web images with noisy annotations\nto learn robust image-text joint representation. Specifically, our main idea is\nto leverage web images and corresponding tags, along with fully annotated\ndatasets, in training for learning the visual-semantic joint embedding. We\npropose a two-stage approach for the task that can augment a typical supervised\npair-wise ranking loss based formulation with weakly-annotated web images to\nlearn a more robust visual-semantic embedding. Experiments on two standard\nbenchmark datasets demonstrate that our method achieves a significant\nperformance gain in image-text retrieval compared to state-of-the-art\napproaches. \n\n"}
{"id": "1808.08483", "contents": "Title: Painting Outside the Box: Image Outpainting with GANs Abstract: The challenging task of image outpainting (extrapolation) has received\ncomparatively little attention in relation to its cousin, image inpainting\n(completion). Accordingly, we present a deep learning approach based on Iizuka\net al. for adversarially training a network to hallucinate past image\nboundaries. We use a three-phase training schedule to stably train a DCGAN\narchitecture on a subset of the Places365 dataset. In line with Iizuka et al.,\nwe also use local discriminators to enhance the quality of our output. Once\ntrained, our model is able to outpaint $128 \\times 128$ color images relatively\nrealistically, thus allowing for recursive outpainting. Our results show that\ndeep learning approaches to image outpainting are both feasible and promising. \n\n"}
{"id": "1808.08671", "contents": "Title: Approach for Video Classification with Multi-label on YouTube-8M Dataset Abstract: Video traffic is increasing at a considerable rate due to the spread of\npersonal media and advancements in media technology. Accordingly, there is a\ngrowing need for techniques to automatically classify moving images. This paper\nuse NetVLAD and NetFV models and the Huber loss function for video\nclassification problem and YouTube-8M dataset to verify the experiment. We\ntried various attempts according to the dataset and optimize hyperparameters,\nultimately obtain a GAP score of 0.8668. \n\n"}
{"id": "1808.08993", "contents": "Title: Open Set Chinese Character Recognition using Multi-typed Attributes Abstract: Recognition of Off-line Chinese characters is still a challenging problem,\nespecially in historical documents, not only in the number of classes extremely\nlarge in comparison to contemporary image retrieval methods, but also new\nunseen classes can be expected under open learning conditions (even for CNN).\nChinese character recognition with zero or a few training samples is a\ndifficult problem and has not been studied yet. In this paper, we propose a new\nChinese character recognition method by multi-type attributes, which are based\non pronunciation, structure and radicals of Chinese characters, applied to\ncharacter recognition in historical books. This intermediate attribute code has\na strong advantage over the common `one-hot' class representation because it\nallows for understanding complex and unseen patterns symbolically using\nattributes. First, each character is represented by four groups of attribute\ntypes to cover a wide range of character possibilities: Pinyin label, layout\nstructure, number of strokes, three different input methods such as Cangjie,\nZhengma and Wubi, as well as a four-corner encoding method. A convolutional\nneural network (CNN) is trained to learn these attributes. Subsequently,\ncharacters can be easily recognized by these attributes using a distance metric\nand a complete lexicon that is encoded in attribute space. We evaluate the\nproposed method on two open data sets: printed Chinese character recognition\nfor zero-shot learning, historical characters for few-shot learning and a\nclosed set: handwritten Chinese characters. Experimental results show a good\ngeneral classification of seen classes but also a very promising generalization\nability to unseen characters. \n\n"}
{"id": "1808.09879", "contents": "Title: PanoRoom: From the Sphere to the 3D Layout Abstract: We propose a novel FCN able to work with omnidirectional images that outputs\naccurate probability maps representing the main structure of indoor scenes,\nwhich is able to generalize on different data. Our approach handles occlusions\nand recovers complex shaped rooms more faithful to the actual shape of the real\nscenes. We outperform the state of the art not only in accuracy of the 3D\nmodels but also in speed. \n\n"}
{"id": "1808.10075", "contents": "Title: Towards Effective Deep Embedding for Zero-Shot Learning Abstract: Zero-shot learning (ZSL) can be formulated as a cross-domain matching\nproblem: after being projected into a joint embedding space, a visual sample\nwill match against all candidate class-level semantic descriptions and be\nassigned to the nearest class. In this process, the embedding space underpins\nthe success of such matching and is crucial for ZSL. In this paper, we conduct\nan in-depth study on the construction of embedding space for ZSL and posit that\nan ideal embedding space should satisfy two criteria: intra-class compactness\nand inter-class separability. While the former encourages the embeddings of\nvisual samples of one class to distribute tightly close to the semantic\ndescription embedding of this class, the latter requires embeddings from\ndifferent classes to be well separated from each other. Towards this goal, we\npresent a simple but effective two-branch network to simultaneously map\nsemantic descriptions and visual samples into a joint space, on which visual\nembeddings are forced to regress to their class-level semantic embeddings and\nthe embeddings crossing classes are required to be distinguishable by a\ntrainable classifier. Furthermore, we extend our method to a transductive\nsetting to better handle the model bias problem in ZSL (i.e., samples from\nunseen classes tend to be categorized into seen classes) with minimal extra\nsupervision. Specifically, we propose a pseudo labeling strategy to\nprogressively incorporate the testing samples into the training process and\nthus balance the model between seen and unseen classes. Experimental results on\nfive standard ZSL datasets show the superior performance of the proposed method\nand its transductive extension. \n\n"}
{"id": "1809.00263", "contents": "Title: Stochastic Dynamics for Video Infilling Abstract: In this paper, we introduce a stochastic dynamics video infilling (SDVI)\nframework to generate frames between long intervals in a video. Our task\ndiffers from video interpolation which aims to produce transitional frames for\na short interval between every two frames and increase the temporal resolution.\nOur task, namely video infilling, however, aims to infill long intervals with\nplausible frame sequences. Our framework models the infilling as a constrained\nstochastic generation process and sequentially samples dynamics from the\ninferred distribution. SDVI consists of two parts: (1) a bi-directional\nconstraint propagation module to guarantee the spatial-temporal coherence among\nframes, (2) a stochastic sampling process to generate dynamics from the\ninferred distributions. Experimental results show that SDVI can generate clear\nframe sequences with varying contents. Moreover, motions in the generated\nsequence are realistic and able to transfer smoothly from the given start frame\nto the terminal frame. Our project site is\nhttps://xharlie.github.io/projects/project_sites/SDVI/video_results.html \n\n"}
{"id": "1809.00567", "contents": "Title: PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks Abstract: We introduce PathGAN, a deep neural network for visual scanpath prediction\ntrained on adversarial examples. A visual scanpath is defined as the sequence\nof fixation points over an image defined by a human observer with its gaze.\nPathGAN is composed of two parts, the generator and the discriminator. Both\nparts extract features from images using off-the-shelf networks, and train\nrecurrent layers to generate or discriminate scanpaths accordingly. In scanpath\nprediction, the stochastic nature of the data makes it very difficult to\ngenerate realistic predictions using supervised learning strategies, but we\nadopt adversarial training as a suitable alternative. Our experiments prove how\nPathGAN improves the state of the art of visual scanpath prediction on the iSUN\nand Salient360! datasets. Source code and models are available at\nhttps://imatge-upc.github.io/pathgan/ \n\n"}
{"id": "1809.00950", "contents": "Title: Compressive Hyperspectral Imaging: Fourier Transform Interferometry\n  meets Single Pixel Camera Abstract: This paper introduces a single-pixel HyperSpectral (HS) imaging framework\nbased on Fourier Transform Interferometry (FTI). By combining a space-time\ncoding of the light illumination with partial interferometric observations of a\ncollimated light beam (observed by a single pixel), our system benefits from\n(i) reduced measurement rate and light-exposure of the observed object compared\nto common (Nyquist) FTI imagers, and (ii) high spectral resolution as desirable\nin, e.g., Fluorescence Spectroscopy (FS). From the principles of compressive\nsensing with multilevel sampling, our method leverages the sparsity \"in level\"\nof FS data, both in the spectral and the spatial domains. This allows us to\noptimize the space-time light coding using time-modulated Hadamard patterns. We\nconfirm the effectiveness of our approach by a few numerical experiments. \n\n"}
{"id": "1809.00982", "contents": "Title: Wavelet based edge feature enhancement for convolutional neural networks Abstract: Convolutional neural networks are able to perform a hierarchical learning\nprocess starting with local features. However, a limited attention is paid to\nenhancing such elementary level features like edges. We propose and evaluate\ntwo wavelet-based edge feature enhancement methods to preprocess the input\nimages to convolutional neural networks. The first method develops feature\nenhanced representations by decomposing the input images using wavelet\ntransform and limited reconstructing subsequently. The second method develops\nsuch feature enhanced inputs to the network using local modulus maxima of\nwavelet coefficients. For each method, we have developed a new preprocessing\nlayer by implementing each purposed method and have appended to the network\narchitecture. Our empirical evaluations demonstrate that the proposed methods\nare outperforming the baselines and previously published work with significant\naccuracy gains. \n\n"}
{"id": "1809.01123", "contents": "Title: VideoMatch: Matching based Video Object Segmentation Abstract: Video object segmentation is challenging yet important in a wide variety of\napplications for video analysis. Recent works formulate video object\nsegmentation as a prediction task using deep nets to achieve appealing\nstate-of-the-art performance. Due to the formulation as a prediction task, most\nof these methods require fine-tuning during test time, such that the deep nets\nmemorize the appearance of the objects of interest in the given video. However,\nfine-tuning is time-consuming and computationally expensive, hence the\nalgorithms are far from real time. To address this issue, we develop a novel\nmatching based algorithm for video object segmentation. In contrast to\nmemorization based classification techniques, the proposed approach learns to\nmatch extracted features to a provided template without memorizing the\nappearance of the objects. We validate the effectiveness and the robustness of\nthe proposed method on the challenging DAVIS-16, DAVIS-17, Youtube-Objects and\nJumpCut datasets. Extensive results show that our method achieves comparable\nperformance without fine-tuning and is much more favorable in terms of\ncomputational time. \n\n"}
{"id": "1809.01438", "contents": "Title: How is Contrast Encoded in Deep Neural Networks? Abstract: Contrast is a crucial factor in visual information processing. It is desired\nfor a visual system - irrespective of being biological or artificial - to\n\"perceive\" the world robustly under large potential changes in illumination. In\nthis work, we studied the responses of deep neural networks (DNN) to identical\nimages at different levels of contrast. We analysed the activation of kernels\nin the convolutional layers of eight prominent networks with distinct\narchitectures (e.g. VGG and Inception). The results of our experiments indicate\nthat those networks with a higher tolerance to alteration of contrast have more\nthan one convolutional layer prior to the first max-pooling operator. It\nappears that the last convolutional layer before the first max-pooling acts as\na mitigator of contrast variation in input images. In our investigation,\ninterestingly, we observed many similarities between the mechanisms of these\nDNNs and biological visual systems. These comparisons allow us to understand\nmore profoundly the underlying mechanisms of a visual system that is grounded\non the basis of \"data-analysis\". \n\n"}
{"id": "1809.01465", "contents": "Title: Deep Bilevel Learning Abstract: We present a novel regularization approach to train neural networks that\nenjoys better generalization and test error than standard stochastic gradient\ndescent. Our approach is based on the principles of cross-validation, where a\nvalidation set is used to limit the model overfitting. We formulate such\nprinciples as a bilevel optimization problem. This formulation allows us to\ndefine the optimization of a cost on the validation set subject to another\noptimization on the training set. The overfitting is controlled by introducing\nweights on each mini-batch in the training set and by choosing their values so\nthat they minimize the error on the validation set. In practice, these weights\ndefine mini-batch learning rates in a gradient descent update equation that\nfavor gradients with better generalization capabilities. Because of its\nsimplicity, this approach can be integrated with other regularization methods\nand training schemes. We evaluate extensively our proposed algorithm on several\nneural network architectures and datasets, and find that it consistently\nimproves the generalization of the model, especially when labels are noisy. \n\n"}
{"id": "1809.01590", "contents": "Title: A Bayesian framework for the analog reconstruction of kymographs from\n  fluorescence microscopy data Abstract: Kymographs are widely used to represent and anal- yse spatio-temporal\ndynamics of fluorescence markers along curvilinear biological compartments.\nThese objects have a sin- gular geometry, thus kymograph reconstruction is\ninherently an analog image processing task. However, the existing approaches\nare essentially digital: the kymograph photometry is sampled directly from the\ntime-lapse images. As a result, such kymographs rely on raw image data that\nsuffer from the degradations entailed by the image formation process and the\nspatio-temporal resolution of the imaging setup. In this work, we address these\nlimitations and introduce a well-grounded Bayesian framework for the analog\nreconstruction of kymographs. To handle the movement of the object, we\nintroduce an intrinsic description of kymographs using differential geometry: a\nkymograph is a photometry defined on a parameter space that is embedded in\nphysical space by a time-varying map that follows the object geometry. We model\nthe kymograph photometry as a L\\'evy innovation process, a flexible class of\nnon-parametric signal priors. We account for the image formation process using\nthe virtual microscope framework. We formulate a computationally tractable\nrepresentation of the associated maximum a posteriori problem and solve it\nusing a class of efficient and modular algorithms based on the alternating\nsplit Bregman. We assess the performance of our Bayesian framework on synthetic\ndata and apply it to reconstruct the fluorescence dynamics along microtubules\nin vivo in the budding yeast S. cerevisiae. We demonstrate that our framework\nallows revealing patterns from single time-lapse data that are invisible on\nstandard digital kymographs. \n\n"}
{"id": "1809.01728", "contents": "Title: Attention-based Audio-Visual Fusion for Robust Automatic Speech\n  Recognition Abstract: Automatic speech recognition can potentially benefit from the lip motion\npatterns, complementing acoustic speech to improve the overall recognition\nperformance, particularly in noise. In this paper we propose an audio-visual\nfusion strategy that goes beyond simple feature concatenation and learns to\nautomatically align the two modalities, leading to enhanced representations\nwhich increase the recognition accuracy in both clean and noisy conditions. We\ntest our strategy on the TCD-TIMIT and LRS2 datasets, designed for large\nvocabulary continuous speech recognition, applying three types of noise at\ndifferent power ratios. We also exploit state of the art Sequence-to-Sequence\narchitectures, showing that our method can be easily integrated. Results show\nrelative improvements from 7% up to 30% on TCD-TIMIT over the acoustic modality\nalone, depending on the acoustic noise level. We anticipate that the fusion\nstrategy can easily generalise to many other multimodal tasks which involve\ncorrelated modalities. Code available online on GitHub:\nhttps://github.com/georgesterpu/Sigmedia-AVSR \n\n"}
{"id": "1809.01810", "contents": "Title: Interpretable Visual Question Answering by Reasoning on Dependency Trees Abstract: Collaborative reasoning for understanding image-question pairs is a very\ncritical but underexplored topic in interpretable visual question answering\nsystems. Although very recent studies have attempted to use explicit\ncompositional processes to assemble multiple subtasks embedded in questions,\ntheir models heavily rely on annotations or handcrafted rules to obtain valid\nreasoning processes, which leads to either heavy workloads or poor performance\non compositional reasoning. In this paper, to better align image and language\ndomains in diverse and unrestricted cases, we propose a novel neural network\nmodel that performs global reasoning on a dependency tree parsed from the\nquestion; thus, our model is called a parse-tree-guided reasoning network\n(PTGRN). This network consists of three collaborative modules: i) an attention\nmodule that exploits the local visual evidence of each word parsed from the\nquestion, ii) a gated residual composition module that composes the previously\nmined evidence, and iii) a parse-tree-guided propagation module that passes the\nmined evidence along the parse tree. Thus, PTGRN is capable of building an\ninterpretable visual question answering (VQA) system that gradually derives\nimage cues following question-driven parse-tree reasoning. Experiments on\nrelational datasets demonstrate the superiority of PTGRN over current\nstate-of-the-art VQA methods, and the visualization results highlight the\nexplainable capability of our reasoning system. \n\n"}
{"id": "1809.01890", "contents": "Title: Full-body High-resolution Anime Generation with Progressive\n  Structure-conditional Generative Adversarial Networks Abstract: We propose Progressive Structure-conditional Generative Adversarial Networks\n(PSGAN), a new framework that can generate full-body and high-resolution\ncharacter images based on structural information. Recent progress in generative\nadversarial networks with progressive training has made it possible to generate\nhigh-resolution images. However, existing approaches have limitations in\nachieving both high image quality and structural consistency at the same time.\nOur method tackles the limitations by progressively increasing the resolution\nof both generated images and structural conditions during training. In this\npaper, we empirically demonstrate the effectiveness of this method by showing\nthe comparison with existing approaches and video generation results of diverse\nanime characters at 1024x1024 based on target pose sequences. We also create a\nnovel dataset containing full-body 1024x1024 high-resolution images and exact\n2D pose keypoints using Unity 3D Avatar models. \n\n"}
{"id": "1809.02108", "contents": "Title: Deep Audio-Visual Speech Recognition Abstract: The goal of this work is to recognise phrases and sentences being spoken by a\ntalking face, with or without the audio. Unlike previous works that have\nfocussed on recognising a limited number of words or phrases, we tackle lip\nreading as an open-world problem - unconstrained natural language sentences,\nand in the wild videos. Our key contributions are: (1) we compare two models\nfor lip reading, one using a CTC loss, and the other using a\nsequence-to-sequence loss. Both models are built on top of the transformer\nself-attention architecture; (2) we investigate to what extent lip reading is\ncomplementary to audio speech recognition, especially when the audio signal is\nnoisy; (3) we introduce and publicly release a new dataset for audio-visual\nspeech recognition, LRS2-BBC, consisting of thousands of natural sentences from\nBritish television. The models that we train surpass the performance of all\nprevious work on a lip reading benchmark dataset by a significant margin. \n\n"}
{"id": "1809.02176", "contents": "Title: Multi-Adversarial Domain Adaptation Abstract: Recent advances in deep domain adaptation reveal that adversarial learning\ncan be embedded into deep networks to learn transferable features that reduce\ndistribution discrepancy between the source and target domains. Existing domain\nadversarial adaptation methods based on single domain discriminator only align\nthe source and target data distributions without exploiting the complex\nmultimode structures. In this paper, we present a multi-adversarial domain\nadaptation (MADA) approach, which captures multimode structures to enable\nfine-grained alignment of different data distributions based on multiple domain\ndiscriminators. The adaptation can be achieved by stochastic gradient descent\nwith the gradients computed by back-propagation in linear-time. Empirical\nevidence demonstrates that the proposed model outperforms state of the art\nmethods on standard domain adaptation datasets. \n\n"}
{"id": "1809.02786", "contents": "Title: Structure-Preserving Transformation: Generating Diverse and Transferable\n  Adversarial Examples Abstract: Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Most recent works on adversarial examples for image classification\nfocus on directly modifying pixels with minor perturbations. A common\nrequirement in all these works is that the malicious perturbations should be\nsmall enough (measured by an L_p norm for some p) so that they are\nimperceptible to humans. However, small perturbations can be unnecessarily\nrestrictive and limit the diversity of adversarial examples generated. Further,\nan L_p norm based distance metric ignores important structure patterns hidden\nin images that are important to human perception. Consequently, even the minor\nperturbation introduced in recent works often makes the adversarial examples\nless natural to humans. More importantly, they often do not transfer well and\nare therefore less effective when attacking black-box models especially for\nthose protected by a defense mechanism. In this paper, we propose a\nstructure-preserving transformation (SPT) for generating natural and diverse\nadversarial examples with extremely high transferability. The key idea of our\napproach is to allow perceptible deviation in adversarial examples while\nkeeping structure patterns that are central to a human classifier. Empirical\nresults on the MNIST and the fashion-MNIST datasets show that adversarial\nexamples generated by our approach can easily bypass strong adversarial\ntraining. Further, they transfer well to other target models with no loss or\nlittle loss of successful attack rate. \n\n"}
{"id": "1809.03322", "contents": "Title: Guiding the Creation of Deep Learning-based Object Detectors Abstract: Object detection is a computer vision field that has applications in several\ncontexts ranging from biomedicine and agriculture to security. In the last\nyears, several deep learning techniques have greatly improved object detection\nmodels. Among those techniques, we can highlight the YOLO approach, that allows\nthe construction of accurate models that can be employed in real-time\napplications. However, as most deep learning techniques, YOLO has a steep\nlearning curve and creating models using this approach might be challenging for\nnon-expert users. In this work, we tackle this problem by constructing a suite\nof Jupyter notebooks that democratizes the construction of object detection\nmodels using YOLO. The suitability of our approach has been proven with a\ndataset of stomata images where we have achieved a mAP of 90.91%. \n\n"}
{"id": "1809.03355", "contents": "Title: Learning to Zoom: a Saliency-Based Sampling Layer for Neural Networks Abstract: We introduce a saliency-based distortion layer for convolutional neural\nnetworks that helps to improve the spatial sampling of input data for a given\ntask. Our differentiable layer can be added as a preprocessing block to\nexisting task networks and trained altogether in an end-to-end fashion. The\neffect of the layer is to efficiently estimate how to sample from the original\ndata in order to boost task performance. For example, for an image\nclassification task in which the original data might range in size up to\nseveral megapixels, but where the desired input images to the task network are\nmuch smaller, our layer learns how best to sample from the underlying high\nresolution data in a manner which preserves task-relevant information better\nthan uniform downsampling. This has the effect of creating distorted,\ncaricature-like intermediate images, in which idiosyncratic elements of the\nimage that improve task performance are zoomed and exaggerated. Unlike\nalternative approaches such as spatial transformer networks, our proposed layer\nis inspired by image saliency, computed efficiently from uniformly downsampled\ndata, and degrades gracefully to a uniform sampling strategy under uncertainty.\nWe apply our layer to improve existing networks for the tasks of human gaze\nestimation and fine-grained object classification. Code for our method is\navailable in: http://github.com/recasens/Saliency-Sampler \n\n"}
{"id": "1809.03470", "contents": "Title: ViZDoom Competitions: Playing Doom from Pixels Abstract: This paper presents the first two editions of Visual Doom AI Competition,\nheld in 2016 and 2017. The challenge was to create bots that compete in a\nmulti-player deathmatch in a first-person shooter (FPS) game, Doom. The bots\nhad to make their decisions based solely on visual information, i.e., a raw\nscreen buffer. To play well, the bots needed to understand their surroundings,\nnavigate, explore, and handle the opponents at the same time. These aspects,\ntogether with the competitive multi-agent aspect of the game, make the\ncompetition a unique platform for evaluating the state of the art reinforcement\nlearning algorithms. The paper discusses the rules, solutions, results, and\nstatistics that give insight into the agents' behaviors. Best-performing agents\nare described in more detail. The results of the competition lead to the\nconclusion that, although reinforcement learning can produce capable Doom bots,\nthey still are not yet able to successfully compete against humans in this\ngame. The paper also revisits the ViZDoom environment, which is a flexible,\neasy to use, and efficient 3D platform for research for vision-based\nreinforcement learning, based on a well-recognized first-person perspective\ngame Doom. \n\n"}
{"id": "1809.03705", "contents": "Title: Bio-LSTM: A Biomechanically Inspired Recurrent Neural Network for 3D\n  Pedestrian Pose and Gait Prediction Abstract: In applications such as autonomous driving, it is important to understand,\ninfer, and anticipate the intention and future behavior of pedestrians. This\nability allows vehicles to avoid collisions and improve ride safety and\nquality. This paper proposes a biomechanically inspired recurrent neural\nnetwork (Bio-LSTM) that can predict the location and 3D articulated body pose\nof pedestrians in a global coordinate frame, given 3D poses and locations\nestimated in prior frames with inaccuracy. The proposed network is able to\npredict poses and global locations for multiple pedestrians simultaneously, for\npedestrians up to 45 meters from the cameras (urban intersection scale). The\noutputs of the proposed network are full-body 3D meshes represented in Skinned\nMulti-Person Linear (SMPL) model parameters. The proposed approach relies on a\nnovel objective function that incorporates the periodicity of human walking\n(gait), the mirror symmetry of the human body, and the change of ground\nreaction forces in a human gait cycle. This paper presents prediction results\non the PedX dataset, a large-scale, in-the-wild data set collected at real\nurban intersections with heavy pedestrian traffic. Results show that the\nproposed network can successfully learn the characteristics of pedestrian gait\nand produce accurate and consistent 3D pose predictions. \n\n"}
{"id": "1809.03721", "contents": "Title: Deep Asymmetric Networks with a Set of Node-wise Variant Activation\n  Functions Abstract: This work presents deep asymmetric networks with a set of node-wise variant\nactivation functions. The nodes' sensitivities are affected by activation\nfunction selections such that the nodes with smaller indices become\nincreasingly more sensitive. As a result, features learned by the nodes are\nsorted by the node indices in the order of their importance. Asymmetric\nnetworks not only learn input features but also the importance of those\nfeatures. Nodes of lesser importance in asymmetric networks can be pruned to\nreduce the complexity of the networks, and the pruned networks can be retrained\nwithout incurring performance losses. We validate the feature-sorting property\nusing both shallow and deep asymmetric networks as well as deep asymmetric\nnetworks transferred from famous networks. \n\n"}
{"id": "1809.03788", "contents": "Title: Convolutional Neural Networks for the segmentation of microcalcification\n  in Mammography Imaging Abstract: Cluster of microcalcifications can be an early sign of breast cancer. In this\npaper we propose a novel approach based on convolutional neural networks for\nthe detection and segmentation of microcalcification clusters. In this work we\nused 283 mammograms to train and validate our model, obtaining an accuracy of\n98.22% in the detection of preliminary suspect regions and of 97.47% in the\nsegmentation task. Our results show how deep learning could be an effective\ntool to effectively support radiologists during mammograms examination. \n\n"}
{"id": "1809.04282", "contents": "Title: Joint Segmentation and Uncertainty Visualization of Retinal Layers in\n  Optical Coherence Tomography Images using Bayesian Deep Learning Abstract: Optical coherence tomography (OCT) is commonly used to analyze retinal layers\nfor assessment of ocular diseases. In this paper, we propose a method for\nretinal layer segmentation and quantification of uncertainty based on Bayesian\ndeep learning. Our method not only performs end-to-end segmentation of retinal\nlayers, but also gives the pixel wise uncertainty measure of the segmentation\noutput. The generated uncertainty map can be used to identify erroneously\nsegmented image regions which is useful in downstream analysis. We have\nvalidated our method on a dataset of 1487 images obtained from 15 subjects (OCT\nvolumes) and compared it against the state-of-the-art segmentation algorithms\nthat does not take uncertainty into account. The proposed uncertainty based\nsegmentation method results in comparable or improved performance, and most\nimportantly is more robust against noise. \n\n"}
{"id": "1809.04624", "contents": "Title: Visual-Quality-Driven Learning for Underwater Vision Enhancement Abstract: The image processing community has witnessed remarkable advances in enhancing\nand restoring images. Nevertheless, restoring the visual quality of underwater\nimages remains a great challenge. End-to-end frameworks might fail to enhance\nthe visual quality of underwater images since in several scenarios it is not\nfeasible to provide the ground truth of the scene radiance. In this work, we\npropose a CNN-based approach that does not require ground truth data since it\nuses a set of image quality metrics to guide the restoration learning process.\nThe experiments showed that our method improved the visual quality of\nunderwater images preserving their edges and also performed well considering\nthe UCIQE metric. \n\n"}
{"id": "1809.05996", "contents": "Title: Devil in the Details: Towards Accurate Single and Multiple Human Parsing Abstract: Human parsing has received considerable interest due to its wide application\npotentials. Nevertheless, it is still unclear how to develop an accurate human\nparsing system in an efficient and elegant way. In this paper, we identify\nseveral useful properties, including feature resolution, global context\ninformation and edge details, and perform rigorous analyses to reveal how to\nleverage them to benefit the human parsing task. The advantages of these useful\nproperties finally result in a simple yet effective Context Embedding with Edge\nPerceiving (CE2P) framework for single human parsing. Our CE2P is end-to-end\ntrainable and can be easily adopted for conducting multiple human parsing.\nBenefiting the superiority of CE2P, we achieved the 1st places on all three\nhuman parsing benchmarks. Without any bells and whistles, we achieved 56.50\\%\n(mIoU), 45.31\\% (mean $AP^r$) and 33.34\\% ($AP^p_{0.5}$) in LIP, CIHP and MHP\nv2.0, which outperform the state-of-the-arts more than 2.06\\%, 3.81\\% and\n1.87\\%, respectively. We hope our CE2P will serve as a solid baseline and help\nease future research in single/multiple human parsing. Code has been made\navailable at \\url{https://github.com/liutinglt/CE2P}. \n\n"}
{"id": "1809.06006", "contents": "Title: Evaluating Merging Strategies for Sampling-based Uncertainty Techniques\n  in Object Detection Abstract: There has been a recent emergence of sampling-based techniques for estimating\nepistemic uncertainty in deep neural networks. While these methods can be\napplied to classification or semantic segmentation tasks by simply averaging\nsamples, this is not the case for object detection, where detection sample\nbounding boxes must be accurately associated and merged. A weak merging\nstrategy can significantly degrade the performance of the detector and yield an\nunreliable uncertainty measure. This paper provides the first in-depth\ninvestigation of the effect of different association and merging strategies. We\ncompare different combinations of three spatial and two semantic affinity\nmeasures with four clustering methods for MC Dropout with a Single Shot\nMulti-Box Detector. Our results show that the correct choice of\naffinity-clustering combination can greatly improve the effectiveness of the\nclassification and spatial uncertainty estimation and the resulting object\ndetection performance. We base our evaluation on a new mix of datasets that\nemulate near open-set conditions (semantically similar unknown classes),\ndistant open-set conditions (semantically dissimilar unknown classes) and the\ncommon closed-set conditions (only known classes). \n\n"}
{"id": "1809.06013", "contents": "Title: DASNet: Reducing Pixel-level Annotations for Instance and Semantic\n  Segmentation Abstract: Pixel-level annotation demands expensive human efforts and limits the\nperformance of deep networks that usually benefits from more such training\ndata. In this work we aim to achieve high quality instance and semantic\nsegmentation results over a small set of pixel-level mask annotations and a\nlarge set of box annotations. The basic idea is exploring detection models to\nsimplify the pixel-level supervised learning task and thus reduce the required\namount of mask annotations. Our architecture, named DASNet, consists of three\nmodules: detection, attention, and segmentation. The detection module detects\nall classes of objects, the attention module generates multi-scale\nclass-specific features, and the segmentation module recovers the binary masks.\nOur method demonstrates substantially improved performance compared to existing\nsemi-supervised approaches on PASCAL VOC 2012 dataset. \n\n"}
{"id": "1809.06064", "contents": "Title: Object-sensitive Deep Reinforcement Learning Abstract: Deep reinforcement learning has become popular over recent years, showing\nsuperiority on different visual-input tasks such as playing Atari games and\nrobot navigation. Although objects are important image elements, few work\nconsiders enhancing deep reinforcement learning with object characteristics. In\nthis paper, we propose a novel method that can incorporate object recognition\nprocessing to deep reinforcement learning models. This approach can be adapted\nto any existing deep reinforcement learning frameworks. State-of-the-art\nresults are shown in experiments on Atari games. We also propose a new approach\ncalled \"object saliency maps\" to visually explain the actions made by deep\nreinforcement learning agents. \n\n"}
{"id": "1809.06213", "contents": "Title: Context-Dependent Diffusion Network for Visual Relationship Detection Abstract: Visual relationship detection can bridge the gap between computer vision and\nnatural language for scene understanding of images. Different from pure object\nrecognition tasks, the relation triplets of subject-predicate-object lie on an\nextreme diversity space, such as \\textit{person-behind-person} and\n\\textit{car-behind-building}, while suffering from the problem of combinatorial\nexplosion. In this paper, we propose a context-dependent diffusion network\n(CDDN) framework to deal with visual relationship detection. To capture the\ninteractions of different object instances, two types of graphs, word semantic\ngraph and visual scene graph, are constructed to encode global context\ninterdependency. The semantic graph is built through language priors to model\nsemantic correlations across objects, whilst the visual scene graph defines the\nconnections of scene objects so as to utilize the surrounding scene\ninformation. For the graph-structured data, we design a diffusion network to\nadaptively aggregate information from contexts, which can effectively learn\nlatent representations of visual relationships and well cater to visual\nrelationship detection in view of its isomorphic invariance to graphs.\nExperiments on two widely-used datasets demonstrate that our proposed method is\nmore effective and achieves the state-of-the-art performance. \n\n"}
{"id": "1809.06226", "contents": "Title: Linear and Deformable Image Registration with 3D Convolutional Neural\n  Networks Abstract: Image registration and in particular deformable registration methods are\npillars of medical imaging. Inspired by the recent advances in deep learning,\nwe propose in this paper, a novel convolutional neural network architecture\nthat couples linear and deformable registration within a unified architecture\nendowed with near real-time performance. Our framework is modular with respect\nto the global transformation component, as well as with respect to the\nsimilarity function while it guarantees smooth displacement fields. We evaluate\nthe performance of our network on the challenging problem of MRI lung\nregistration, and demonstrate superior performance with respect to state of the\nart elastic registration methods. The proposed deformation (between inspiration\n& expiration) was considered within a clinically relevant task of interstitial\nlung disease (ILD) classification and showed promising results. \n\n"}
{"id": "1809.07217", "contents": "Title: 3D Human Pose Estimation with Siamese Equivariant Embedding Abstract: In monocular 3D human pose estimation a common setup is to first detect 2D\npositions and then lift the detection into 3D coordinates. Many algorithms\nsuffer from overfitting to camera positions in the training set. We propose a\nsiamese architecture that learns a rotation equivariant hidden representation\nto reduce the need for data augmentation. Our method is evaluated on multiple\ndatabases with different base networks and shows a consistent improvement of\nerror metrics. It achieves state-of-the-art cross-camera error rate among\nalgorithms that use estimated 2D joint coordinates only. \n\n"}
{"id": "1809.07589", "contents": "Title: DuPLO: A DUal view Point deep Learning architecture for time series\n  classificatiOn Abstract: Nowadays, modern Earth Observation systems continuously generate huge amounts\nof data. A notable example is represented by the Sentinel-2 mission, which\nprovides images at high spatial resolution (up to 10m) with high temporal\nrevisit period (every 5 days), which can be organized in Satellite Image Time\nSeries (SITS). While the use of SITS has been proved to be beneficial in the\ncontext of Land Use/Land Cover (LULC) map generation, unfortunately, machine\nlearning approaches commonly leveraged in remote sensing field fail to take\nadvantage of spatio-temporal dependencies present in such data.\n  Recently, new generation deep learning methods allowed to significantly\nadvance research in this field. These approaches have generally focused on a\nsingle type of neural network, i.e., Convolutional Neural Networks (CNNs) or\nRecurrent Neural Networks (RNNs), which model different but complementary\ninformation: spatial autocorrelation (CNNs) and temporal dependencies (RNNs).\nIn this work, we propose the first deep learning architecture for the analysis\nof SITS data, namely \\method{} (DUal view Point deep Learning architecture for\ntime series classificatiOn), that combines Convolutional and Recurrent neural\nnetworks to exploit their complementarity. Our hypothesis is that, since CNNs\nand RNNs capture different aspects of the data, a combination of both models\nwould produce a more diverse and complete representation of the information for\nthe underlying land cover classification task. Experiments carried out on two\nstudy sites characterized by different land cover characteristics (i.e., the\n\\textit{Gard} site in France and the \\textit{Reunion Island} in the Indian\nOcean), demonstrate the significance of our proposal. \n\n"}
{"id": "1809.08352", "contents": "Title: Unrestricted Adversarial Examples Abstract: We introduce a two-player contest for evaluating the safety and robustness of\nmachine learning systems, with a large prize pool. Unlike most prior work in ML\nrobustness, which studies norm-constrained adversaries, we shift our focus to\nunconstrained adversaries. Defenders submit machine learning models, and try to\nachieve high accuracy and coverage on non-adversarial data while making no\nconfident mistakes on adversarial inputs. Attackers try to subvert defenses by\nfinding arbitrary unambiguous inputs where the model assigns an incorrect label\nwith high confidence. We propose a simple unambiguous dataset (\"bird-or-\nbicycle\") to use as part of this contest. We hope this contest will help to\nmore comprehensively evaluate the worst-case adversarial risk of machine\nlearning models. \n\n"}
{"id": "1809.08440", "contents": "Title: Pose-Guided Multi-Granularity Attention Network for Text-Based Person\n  Search Abstract: Text-based person search aims to retrieve the corresponding person images in\nan image database by virtue of a describing sentence about the person, which\nposes great potential for various applications such as video surveillance.\nExtracting visual contents corresponding to the human description is the key to\nthis cross-modal matching problem. Moreover, correlated images and descriptions\ninvolve different granularities of semantic relevance, which is usually ignored\nin previous methods. To exploit the multilevel corresponding visual contents,\nwe propose a pose-guided multi-granularity attention network (PMA). Firstly, we\npropose a coarse alignment network (CA) to select the related image regions to\nthe global description by a similarity-based attention. To further capture the\nphrase-related visual body part, a fine-grained alignment network (FA) is\nproposed, which employs pose information to learn latent semantic alignment\nbetween visual body part and textual noun phrase. To verify the effectiveness\nof our model, we perform extensive experiments on the CUHK Person Description\nDataset (CUHK-PEDES) which is currently the only available dataset for\ntext-based person search. Experimental results show that our approach\noutperforms the state-of-the-art methods by 15 \\% in terms of the top-1 metric. \n\n"}
{"id": "1809.10917", "contents": "Title: Depth Reconstruction of Translucent Objects from a Single Time-of-Flight\n  Camera using Deep Residual Networks Abstract: We propose a novel approach to recovering the translucent objects from a\nsingle time-of-flight (ToF) depth camera using deep residual networks. When\nrecording the translucent objects using the ToF depth camera, their depth\nvalues are severely contaminated due to complex light interactions with the\nsurrounding environment. While existing methods suggested new capture systems\nor developed the depth distortion models, their solutions were less practical\nbecause of strict assumptions or heavy computational complexity. In this paper,\nwe adopt the deep residual networks for modeling the ToF depth distortion\ncaused by translucency. To fully utilize both the local and semantic\ninformation of objects, multi-scale patches are used to predict the depth\nvalue. Based on the quantitative and qualitative evaluation on our benchmark\ndatabase, we show the effectiveness and robustness of the proposed algorithm. \n\n"}
{"id": "1810.00602", "contents": "Title: Privado: Practical and Secure DNN Inference with Enclaves Abstract: Cloud providers are extending support for trusted hardware primitives such as\nIntel SGX. Simultaneously, the field of deep learning is seeing enormous\ninnovation as well as an increase in adoption. In this paper, we ask a timely\nquestion: \"Can third-party cloud services use Intel SGX enclaves to provide\npractical, yet secure DNN Inference-as-a-service?\" We first demonstrate that\nDNN models executing inside enclaves are vulnerable to access pattern based\nattacks. We show that by simply observing access patterns, an attacker can\nclassify encrypted inputs with 97% and 71% attack accuracy for MNIST and\nCIFAR10 datasets on models trained to achieve 99% and 79% original accuracy\nrespectively. This motivates the need for PRIVADO, a system we have designed\nfor secure, easy-to-use, and performance efficient inference-as-a-service.\nPRIVADO is input-oblivious: it transforms any deep learning framework that is\nwritten in C/C++ to be free of input-dependent access patterns thus eliminating\nthe leakage. PRIVADO is fully-automated and has a low TCB: with zero developer\neffort, given an ONNX description of a model, it generates compact and\nenclave-compatible code which can be deployed on an SGX cloud platform. PRIVADO\nincurs low performance overhead: we use PRIVADO with Torch framework and show\nits overhead to be 17.18% on average on 11 different contemporary neural\nnetworks. \n\n"}
{"id": "1810.00826", "contents": "Title: How Powerful are Graph Neural Networks? Abstract: Graph Neural Networks (GNNs) are an effective framework for representation\nlearning of graphs. GNNs follow a neighborhood aggregation scheme, where the\nrepresentation vector of a node is computed by recursively aggregating and\ntransforming representation vectors of its neighboring nodes. Many GNN variants\nhave been proposed and have achieved state-of-the-art results on both node and\ngraph classification tasks. However, despite GNNs revolutionizing graph\nrepresentation learning, there is limited understanding of their\nrepresentational properties and limitations. Here, we present a theoretical\nframework for analyzing the expressive power of GNNs to capture different graph\nstructures. Our results characterize the discriminative power of popular GNN\nvariants, such as Graph Convolutional Networks and GraphSAGE, and show that\nthey cannot learn to distinguish certain simple graph structures. We then\ndevelop a simple architecture that is provably the most expressive among the\nclass of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism\ntest. We empirically validate our theoretical findings on a number of graph\nclassification benchmarks, and demonstrate that our model achieves\nstate-of-the-art performance. \n\n"}
{"id": "1810.01032", "contents": "Title: Reinforcement Learning with Perturbed Rewards Abstract: Recent studies have shown that reinforcement learning (RL) models are\nvulnerable in various noisy scenarios. For instance, the observed reward\nchannel is often subject to noise in practice (e.g., when rewards are collected\nthrough sensors), and is therefore not credible. In addition, for applications\nsuch as robotics, a deep reinforcement learning (DRL) algorithm can be\nmanipulated to produce arbitrary errors by receiving corrupted rewards. In this\npaper, we consider noisy RL problems with perturbed rewards, which can be\napproximated with a confusion matrix. We develop a robust RL framework that\nenables agents to learn in noisy environments where only perturbed rewards are\nobserved. Our solution framework builds on existing RL/DRL algorithms and\nfirstly addresses the biased noisy reward setting without any assumptions on\nthe true distribution (e.g., zero-mean Gaussian noise as made in previous\nworks). The core ideas of our solution include estimating a reward confusion\nmatrix and defining a set of unbiased surrogate rewards. We prove the\nconvergence and sample complexity of our approach. Extensive experiments on\ndifferent DRL platforms show that trained policies based on our estimated\nsurrogate reward can achieve higher expected rewards, and converge faster than\nexisting baselines. For instance, the state-of-the-art PPO algorithm is able to\nobtain 84.6% and 80.8% improvements on average score for five Atari games, with\nerror rates as 10% and 30% respectively. \n\n"}
{"id": "1810.01069", "contents": "Title: Cloud Chaser: Real Time Deep Learning Computer Vision on Low Computing\n  Power Devices Abstract: Internet of Things(IoT) devices, mobile phones, and robotic systems are often\ndenied the power of deep learning algorithms due to their limited computing\npower. However, to provide time-critical services such as emergency response,\nhome assistance, surveillance, etc, these devices often need real-time analysis\nof their camera data. This paper strives to offer a viable approach to\nintegrate high-performance deep learning-based computer vision algorithms with\nlow-resource and low-power devices by leveraging the computing power of the\ncloud. By offloading the computation work to the cloud, no dedicated hardware\nis needed to enable deep neural networks on existing low computing power\ndevices. A Raspberry Pi based robot, Cloud Chaser, is built to demonstrate the\npower of using cloud computing to perform real-time vision tasks. Furthermore,\nto reduce latency and improve real-time performance, compression algorithms are\nproposed and evaluated for streaming real-time video frames to the cloud. \n\n"}
{"id": "1810.01185", "contents": "Title: Adversarial Examples - A Complete Characterisation of the Phenomenon Abstract: We provide a complete characterisation of the phenomenon of adversarial\nexamples - inputs intentionally crafted to fool machine learning models. We aim\nto cover all the important concerns in this field of study: (1) the conjectures\non the existence of adversarial examples, (2) the security, safety and\nrobustness implications, (3) the methods used to generate and (4) protect\nagainst adversarial examples and (5) the ability of adversarial examples to\ntransfer between different machine learning models. We provide ample background\ninformation in an effort to make this document self-contained. Therefore, this\ndocument can be used as survey, tutorial or as a catalog of attacks and\ndefences using adversarial examples. \n\n"}
{"id": "1810.01544", "contents": "Title: Image as Data: Automated Visual Content Analysis for Political Science Abstract: Image data provide unique information about political events, actors, and\ntheir interactions which are difficult to measure from or not available in text\ndata. This article introduces a new class of automated methods based on\ncomputer vision and deep learning which can automatically analyze visual\ncontent data. Scholars have already recognized the importance of visual data\nand a variety of large visual datasets have become available. The lack of\nscalable analytic methods, however, has prevented from incorporating large\nscale image data in political analysis. This article aims to offer an in-depth\noverview of automated methods for visual content analysis and explains their\nusages and implementations. We further elaborate on how these methods and\nresults can be validated and interpreted. We then discuss how these methods can\ncontribute to the study of political communication, identity and politics,\ndevelopment, and conflict, by enabling a new set of research questions at\nscale. \n\n"}
{"id": "1810.03065", "contents": "Title: Deep Model-Based 6D Pose Refinement in RGB Abstract: We present a novel approach for model-based 6D pose refinement in color data.\nBuilding on the established idea of contour-based pose tracking, we teach a\ndeep neural network to predict a translational and rotational update. At the\ncore, we propose a new visual loss that drives the pose update by aligning\nobject contours, thus avoiding the definition of any explicit appearance model.\nIn contrast to previous work our method is correspondence-free,\nsegmentation-free, can handle occlusion and is agnostic to geometrical symmetry\nas well as visual ambiguities. Additionally, we observe a strong robustness\ntowards rough initialization. The approach can run in real-time and produces\npose accuracies that come close to 3D ICP without the need for depth data.\nFurthermore, our networks are trained from purely synthetic data and will be\npublished together with the refinement code to ensure reproducibility. \n\n"}
{"id": "1810.04274", "contents": "Title: Survival prediction using ensemble tumor segmentation and transfer\n  learning Abstract: Segmenting tumors and their subregions is a challenging task as demonstrated\nby the annual BraTS challenge. Moreover, predicting the survival of the patient\nusing mainly imaging features, while being a desirable outcome to evaluate the\ntreatment of the patient, it is also a difficult task. In this paper, we\npresent a cascaded pipeline to segment the tumor and its subregions and then we\nuse these results and other clinical features together with image features\ncoming from a pretrained VGG-16 network to predict the survival of the patient.\nPreliminary results with the training and validation dataset show a promising\nstart in terms of segmentation, while the prediction values could be improved\nwith further testing on the feature extraction part of the network. \n\n"}
{"id": "1810.04452", "contents": "Title: AI Learns to Recognize Bengali Handwritten Digits: Bengali.AI Computer\n  Vision Challenge 2018 Abstract: Solving problems with Artificial intelligence in a competitive manner has\nlong been absent in Bangladesh and Bengali-speaking community. On the other\nhand, there has not been a well structured database for Bengali Handwritten\ndigits for mass public use. To bring out the best minds working in machine\nlearning and use their expertise to create a model which can easily recognize\nBengali Handwritten digits, we organized Bengali.AI Computer Vision\nChallenge.The challenge saw both local and international teams participating\nwith unprecedented efforts. \n\n"}
{"id": "1810.05358", "contents": "Title: Efficient architecture for deep neural networks with heterogeneous\n  sensitivity Abstract: This work presents a neural network that consists of nodes with heterogeneous\nsensitivity. Each node in a network is assigned a variable that determines the\nsensitivity with which it learns to perform a given task. The network is\ntrained by a constrained optimization that maximizes the sparsity of the\nsensitivity variables while ensuring the network's performance. As a result,\nthe network learns to perform a given task using only a small number of\nsensitive nodes. Insensitive nodes, the nodes with zero sensitivity, can be\nremoved from a trained network to obtain a computationally efficient network.\nRemoving zero-sensitivity nodes has no effect on the network's performance\nbecause the network has already been trained to perform the task without them.\nThe regularization parameter used to solve the optimization problem is found\nsimultaneously during the training of networks. To validate our approach, we\ndesign networks with computationally efficient architectures for various tasks\nsuch as autoregression, object recognition, facial expression recognition, and\nobject detection using various datasets. In our experiments, the networks\ndesigned by the proposed method provide the same or higher performance but with\nfar less computational complexity. \n\n"}
{"id": "1810.05438", "contents": "Title: MPTV: Matching Pursuit Based Total Variation Minimization for Image\n  Deconvolution Abstract: Total variation (TV) regularization has proven effective for a range of\ncomputer vision tasks through its preferential weighting of sharp image edges.\nExisting TV-based methods, however, often suffer from the over-smoothing issue\nand solution bias caused by the homogeneous penalization. In this paper, we\nconsider addressing these issues by applying inhomogeneous regularization on\ndifferent image components. We formulate the inhomogeneous TV minimization\nproblem as a convex quadratic constrained linear programming problem. Relying\non this new model, we propose a matching pursuit based total variation\nminimization method (MPTV), specifically for image deconvolution. The proposed\nMPTV method is essentially a cutting-plane method, which iteratively activates\na subset of nonzero image gradients, and then solves a subproblem focusing on\nthose activated gradients only. Compared to existing methods, MPTV is less\nsensitive to the choice of the trade-off parameter between data fitting and\nregularization. Moreover, the inhomogeneity of MPTV alleviates the\nover-smoothing and ringing artifacts, and improves the robustness to errors in\nblur kernel. Extensive experiments on different tasks demonstrate the\nsuperiority of the proposed method over the current state-of-the-art. \n\n"}
{"id": "1810.05852", "contents": "Title: Exploiting Semantics in Adversarial Training for Image-Level Domain\n  Adaptation Abstract: Performance achievable by modern deep learning approaches are directly\nrelated to the amount of data used at training time. Unfortunately, the\nannotation process is notoriously tedious and expensive, especially for\npixel-wise tasks like semantic segmentation. Recent works have proposed to rely\non synthetically generated imagery to ease the training set creation. However,\nmodels trained on these kind of data usually under-perform on real images due\nto the well known issue of domain shift. We address this problem by learning a\ndomain-to-domain image translation GAN to shrink the gap between real and\nsynthetic images. Peculiarly to our method, we introduce semantic constraints\ninto the generation process to both avoid artifacts and guide the synthesis. To\nprove the effectiveness of our proposal, we show how a semantic segmentation\nCNN trained on images from the synthetic GTA dataset adapted by our method can\nimprove performance by more than 16% mIoU with respect to the same model\ntrained on synthetic images. \n\n"}
{"id": "1810.06951", "contents": "Title: Deep Metric Learning with Hierarchical Triplet Loss Abstract: We present a novel hierarchical triplet loss (HTL) capable of automatically\ncollecting informative training samples (triplets) via a defined hierarchical\ntree that encodes global context information. This allows us to cope with the\nmain limitation of random sampling in training a conventional triplet loss,\nwhich is a central issue for deep metric learning. Our main contributions are\ntwo-fold. (i) we construct a hierarchical class-level tree where neighboring\nclasses are merged recursively. The hierarchical structure naturally captures\nthe intrinsic data distribution over the whole database. (ii) we formulate the\nproblem of triplet collection by introducing a new violate margin, which is\ncomputed dynamically based on the designed hierarchical tree. This allows it to\nautomatically select meaningful hard samples with the guide of global context.\nIt encourages the model to learn more discriminative features from visual\nsimilar classes, leading to faster convergence and better performance. Our\nmethod is evaluated on the tasks of image retrieval and face recognition, where\nit outperforms the standard triplet loss substantially by 1%-18%. It achieves\nnew state-of-the-art performance on a number of benchmarks, with much fewer\nlearning iterations. \n\n"}
{"id": "1810.08329", "contents": "Title: Transferrable Feature and Projection Learning with Class Hierarchy for\n  Zero-Shot Learning Abstract: Zero-shot learning (ZSL) aims to transfer knowledge from seen classes to\nunseen ones so that the latter can be recognised without any training samples.\nThis is made possible by learning a projection function between a feature space\nand a semantic space (e.g. attribute space). Considering the seen and unseen\nclasses as two domains, a big domain gap often exists which challenges ZSL.\nInspired by the fact that an unseen class is not exactly `unseen' if it belongs\nto the same superclass as a seen class, we propose a novel inductive ZSL model\nthat leverages superclasses as the bridge between seen and unseen classes to\nnarrow the domain gap. Specifically, we first build a class hierarchy of\nmultiple superclass layers and a single class layer, where the superclasses are\nautomatically generated by data-driven clustering over the semantic\nrepresentations of all seen and unseen class names. We then exploit the\nsuperclasses from the class hierarchy to tackle the domain gap challenge in two\naspects: deep feature learning and projection function learning. First, to\nnarrow the domain gap in the feature space, we integrate a recurrent neural\nnetwork (RNN) defined with the superclasses into a convolutional neural network\n(CNN), in order to enforce the superclass hierarchy. Second, to further learn a\ntransferrable projection function for ZSL, a novel projection function learning\nmethod is proposed by exploiting the superclasses to align the two domains.\nImportantly, our transferrable feature and projection learning methods can be\neasily extended to a closely related task -- few-shot learning (FSL). Extensive\nexperiments show that the proposed model significantly outperforms the\nstate-of-the-art alternatives in both ZSL and FSL tasks. \n\n"}
{"id": "1810.08452", "contents": "Title: Multitask Learning for Large-scale Semantic Change Detection Abstract: Change detection is one of the main problems in remote sensing, and is\nessential to the accurate processing and understanding of the large scale Earth\nobservation data available through programs such as Sentinel and Landsat. Most\nof the recently proposed change detection methods bring deep learning to this\ncontext, but openly available change detection datasets are still very scarce,\nwhich limits the methods that can be proposed and tested. In this paper we\npresent the first large scale high resolution semantic change detection (HRSCD)\ndataset, which enables the usage of deep learning methods for semantic change\ndetection. The dataset contains coregistered RGB image pairs, pixel-wise change\ninformation and land cover information. We then propose several methods using\nfully convolutional neural networks to perform semantic change detection. Most\nnotably, we present a network architecture that performs change detection and\nland cover mapping simultaneously, while using the predicted land cover\ninformation to help to predict changes. We also describe a sequential training\nscheme that allows this network to be trained without setting a hyperparameter\nthat balances different loss functions and achieves the best overall results. \n\n"}
{"id": "1810.08770", "contents": "Title: Sequential Context Encoding for Duplicate Removal Abstract: Duplicate removal is a critical step to accomplish a reasonable amount of\npredictions in prevalent proposal-based object detection frameworks. Albeit\nsimple and effective, most previous algorithms utilize a greedy process without\nmaking sufficient use of properties of input data. In this work, we design a\nnew two-stage framework to effectively select the appropriate proposal\ncandidate for each object. The first stage suppresses most of easy negative\nobject proposals, while the second stage selects true positives in the reduced\nproposal set. These two stages share the same network structure, \\ie, an\nencoder and a decoder formed as recurrent neural networks (RNN) with global\nattention and context gate. The encoder scans proposal candidates in a\nsequential manner to capture the global context information, which is then fed\nto the decoder to extract optimal proposals. In our extensive experiments, the\nproposed method outperforms other alternatives by a large margin. \n\n"}
{"id": "1810.09630", "contents": "Title: A Neural Compositional Paradigm for Image Captioning Abstract: Mainstream captioning models often follow a sequential structure to generate\ncaptions, leading to issues such as introduction of irrelevant semantics, lack\nof diversity in the generated captions, and inadequate generalization\nperformance. In this paper, we present an alternative paradigm for image\ncaptioning, which factorizes the captioning procedure into two stages: (1)\nextracting an explicit semantic representation from the given image; and (2)\nconstructing the caption based on a recursive compositional procedure in a\nbottom-up manner. Compared to conventional ones, our paradigm better preserves\nthe semantic content through an explicit factorization of semantics and syntax.\nBy using the compositional generation procedure, caption construction follows a\nrecursive structure, which naturally fits the properties of human language.\nMoreover, the proposed compositional procedure requires less data to train,\ngeneralizes better, and yields more diverse captions. \n\n"}
{"id": "1810.09945", "contents": "Title: Analyzing Neuroimaging Data Through Recurrent Deep Learning Models Abstract: The application of deep learning (DL) models to neuroimaging data poses\nseveral challenges, due to the high dimensionality, low sample size and complex\ntemporo-spatial dependency structure of these datasets. Even further, DL models\nact as as black-box models, impeding insight into the association of cognitive\nstate and brain activity. To approach these challenges, we introduce the\nDeepLight framework, which utilizes long short-term memory (LSTM) based DL\nmodels to analyze whole-brain functional Magnetic Resonance Imaging (fMRI)\ndata. To decode a cognitive state (e.g., seeing the image of a house),\nDeepLight separates the fMRI volume into a sequence of axial brain slices,\nwhich is then sequentially processed by an LSTM. To maintain interpretability,\nDeepLight adapts the layer-wise relevance propagation (LRP) technique. Thereby,\ndecomposing its decoding decision into the contributions of the single input\nvoxels to this decision. Importantly, the decomposition is performed on the\nlevel of single fMRI volumes, enabling DeepLight to study the associations\nbetween cognitive state and brain activity on several levels of data\ngranularity, from the level of the group down to the level of single time\npoints. To demonstrate the versatility of DeepLight, we apply it to a large\nfMRI dataset of the Human Connectome Project. We show that DeepLight\noutperforms conventional approaches of uni- and multivariate fMRI analysis in\ndecoding the cognitive states and in identifying the physiologically\nappropriate brain regions associated with these states. We further demonstrate\nDeepLight's ability to study the fine-grained temporo-spatial variability of\nbrain activity over sequences of single fMRI samples. \n\n"}
{"id": "1810.10093", "contents": "Title: Structured Domain Randomization: Bridging the Reality Gap by\n  Context-Aware Synthetic Data Abstract: We present structured domain randomization (SDR), a variant of domain\nrandomization (DR) that takes into account the structure and context of the\nscene. In contrast to DR, which places objects and distractors randomly\naccording to a uniform probability distribution, SDR places objects and\ndistractors randomly according to probability distributions that arise from the\nspecific problem at hand. In this manner, SDR-generated imagery enables the\nneural network to take the context around an object into consideration during\ndetection. We demonstrate the power of SDR for the problem of 2D bounding box\ncar detection, achieving competitive results on real data after training only\non synthetic data. On the KITTI easy, moderate, and hard tasks, we show that\nSDR outperforms other approaches to generating synthetic data (VKITTI, Sim\n200k, or DR), as well as real data collected in a different domain (BDD100K).\nMoreover, synthetic SDR data combined with real KITTI data outperforms real\nKITTI data alone. \n\n"}
{"id": "1810.10333", "contents": "Title: Memorization in Overparameterized Autoencoders Abstract: The ability of deep neural networks to generalize well in the\noverparameterized regime has become a subject of significant research interest.\nWe show that overparameterized autoencoders exhibit memorization, a form of\ninductive bias that constrains the functions learned through the optimization\nprocess to concentrate around the training examples, although the network could\nin principle represent a much larger function class. In particular, we prove\nthat single-layer fully-connected autoencoders project data onto the\n(nonlinear) span of the training examples. In addition, we show that deep\nfully-connected autoencoders learn a map that is locally contractive at the\ntraining examples, and hence iterating the autoencoder results in convergence\nto the training examples. Finally, we prove that depth is necessary and provide\nempirical evidence that it is also sufficient for memorization in convolutional\nautoencoders. Understanding this inductive bias may shed light on the\ngeneralization properties of overparametrized deep neural networks that are\ncurrently unexplained by classical statistical theory. \n\n"}
{"id": "1810.10551", "contents": "Title: Fast and accurate object detection in high resolution 4K and 8K video\n  using GPUs Abstract: Machine learning has celebrated a lot of achievements on computer vision\ntasks such as object detection, but the traditionally used models work with\nrelatively low resolution images. The resolution of recording devices is\ngradually increasing and there is a rising need for new methods of processing\nhigh resolution data. We propose an attention pipeline method which uses two\nstaged evaluation of each image or video frame under rough and refined\nresolution to limit the total number of necessary evaluations. For both stages,\nwe make use of the fast object detection model YOLO v2. We have implemented our\nmodel in code, which distributes the work across GPUs. We maintain high\naccuracy while reaching the average performance of 3-6 fps on 4K video and 2\nfps on 8K video. \n\n"}
{"id": "1810.10597", "contents": "Title: The speaker-independent lipreading play-off; a survey of lipreading\n  machines Abstract: Lipreading is a difficult gesture classification task. One problem in\ncomputer lipreading is speaker-independence. Speaker-independence means to\nachieve the same accuracy on test speakers not included in the training set as\nspeakers within the training set. Current literature is limited on\nspeaker-independent lipreading, the few independent test speaker accuracy\nscores are usually aggregated within dependent test speaker accuracies for an\naveraged performance. This leads to unclear independent results. Here we\nundertake a systematic survey of experiments with the TCD-TIMIT dataset using\nboth conventional approaches and deep learning methods to provide a series of\nwholly speaker-independent benchmarks and show that the best\nspeaker-independent machine scores 69.58% accuracy with CNN features and an SVM\nclassifier. This is less than state of the art speaker-dependent lipreading\nmachines, but greater than previously reported in independence experiments. \n\n"}
{"id": "1810.10933", "contents": "Title: Practical Shape Analysis and Segmentation Methods for Point Cloud Models Abstract: Current point cloud processing algorithms do not have the capability to\nautomatically extract semantic information from the observed scenes, except in\nvery specialized cases. Furthermore, existing mesh analysis paradigms cannot be\ndirectly employed to automatically perform typical shape analysis tasks\ndirectly on point cloud models.\n  We present a potent framework for shape analysis, similarity, and\nsegmentation of noisy point cloud models for real objects of engineering\ninterest, models that may be incomplete. The proposed framework relies on\nspectral methods and the heat diffusion kernel to construct compact shape\nsignatures, and we show that the framework supports a variety of clustering\ntechniques that have traditionally been applied only on mesh models. We\ndeveloped and implemented one practical and convergent estimate of the\nLaplace-Beltrami operator for point clouds as well as a number of clustering\ntechniques adapted to work directly on point clouds to produce geometric\nfeatures of engineering interest. The key advantage of this framework is that\nit supports practical shape analysis capabilities that operate directly on\npoint cloud models of objects without requiring surface reconstruction or\nglobal meshing. We show that the proposed technique is robust against typical\nnoise present in possibly incomplete point clouds, and segment point clouds\nscanned by depth cameras (e.g. Kinect) into semantically-meaningful sub-shapes. \n\n"}
{"id": "1810.10941", "contents": "Title: Alzheimer's Disease Diagnosis Based on Cognitive Methods in Virtual\n  Environments and Emotions Analysis Abstract: Dementia is a syndrome characterised by the decline of different cognitive\nabilities. Alzheimer's Disease (AD) is the most common dementia affecting\ncognitive domains such as memory and learning, perceptual-motion or executive\nfunction. High rate of deaths and high cost for detection, treatments and\npatient's care count amongst its consequences. Early detection of AD is\nconsidered of high importance for improving the quality of life of patients and\ntheir families. The aim of this thesis is to introduce novel non-invasive early\ndiagnosis methods in order to speed the diagnosis, reduce the associated costs\nand make them widely accessible. Novel AD's screening tests based on virtual\nenvironments using new immersive technologies combined with advanced Human\nComputer Interaction (HCI) systems are introduced. Four tests demonstrate the\nwide range of screening mechanisms based on cognitive domain impairments that\ncan be designed using virtual environments. The use of emotion recognition to\nanalyse AD symptoms has been also proposed. A novel multimodal dataset was\nspecifically created to remark the autobiographical memory deficits of AD\npatients. Data from this dataset is used to introduce novel descriptors for\nElectroencephalogram (EEG) and facial images data. EEG features are based on\nquaternions in order to keep the correlation information between sensors,\nwhereas, for facial expression recognition, a preprocessing method for motion\nmagnification and descriptors based on origami crease pattern algorithm are\nproposed to enhance facial micro-expressions. These features have been proved\non classifiers such as SVM and Adaboost for the classification of reactions to\nautobiographical stimuli such as long and short term memories. \n\n"}
{"id": "1810.11919", "contents": "Title: Text-Adaptive Generative Adversarial Networks: Manipulating Images with\n  Natural Language Abstract: This paper addresses the problem of manipulating images using natural\nlanguage description. Our task aims to semantically modify visual attributes of\nan object in an image according to the text describing the new visual\nappearance. Although existing methods synthesize images having new attributes,\nthey do not fully preserve text-irrelevant contents of the original image. In\nthis paper, we propose the text-adaptive generative adversarial network (TAGAN)\nto generate semantically manipulated images while preserving text-irrelevant\ncontents. The key to our method is the text-adaptive discriminator that creates\nword-level local discriminators according to input text to classify\nfine-grained attributes independently. With this discriminator, the generator\nlearns to generate images where only regions that correspond to the given text\nare modified. Experimental results show that our method outperforms existing\nmethods on CUB and Oxford-102 datasets, and our results were mostly preferred\non a user study. Extensive analysis shows that our method is able to\neffectively disentangle visual attributes and produce pleasing outputs. \n\n"}
{"id": "1810.12488", "contents": "Title: Re-evaluating Continual Learning Scenarios: A Categorization and Case\n  for Strong Baselines Abstract: Continual learning has received a great deal of attention recently with\nseveral approaches being proposed. However, evaluations involve a diverse set\nof scenarios making meaningful comparison difficult. This work provides a\nsystematic categorization of the scenarios and evaluates them within a\nconsistent framework including strong baselines and state-of-the-art methods.\nThe results provide an understanding of the relative difficulty of the\nscenarios and that simple baselines (Adagrad, L2 regularization, and naive\nrehearsal strategies) can surprisingly achieve similar performance to current\nmainstream methods. We conclude with several suggestions for creating harder\nevaluation scenarios and future research directions. The code is available at\nhttps://github.com/GT-RIPL/Continual-Learning-Benchmark \n\n"}
{"id": "1810.12522", "contents": "Title: Random Temporal Skipping for Multirate Video Analysis Abstract: Current state-of-the-art approaches to video understanding adopt temporal\njittering to simulate analyzing the video at varying frame rates. However, this\ndoes not work well for multirate videos, in which actions or subactions occur\nat different speeds. The frame sampling rate should vary in accordance with the\ndifferent motion speeds. In this work, we propose a simple yet effective\nstrategy, termed random temporal skipping, to address this situation. This\nstrategy effectively handles multirate videos by randomizing the sampling rate\nduring training. It is an exhaustive approach, which can potentially cover all\nmotion speed variations. Furthermore, due to the large temporal skipping, our\nnetwork can see video clips that originally cover over 100 frames. Such a time\nrange is enough to analyze most actions/events. We also introduce an\nocclusion-aware optical flow learning method that generates improved motion\nmaps for human action recognition. Our framework is end-to-end trainable, runs\nin real-time, and achieves state-of-the-art performance on six widely adopted\nvideo benchmarks. \n\n"}
{"id": "1810.13049", "contents": "Title: Cooperative Holistic Scene Understanding: Unifying 3D Object, Layout,\n  and Camera Pose Estimation Abstract: Holistic 3D indoor scene understanding refers to jointly recovering the i)\nobject bounding boxes, ii) room layout, and iii) camera pose, all in 3D. The\nexisting methods either are ineffective or only tackle the problem partially.\nIn this paper, we propose an end-to-end model that simultaneously solves all\nthree tasks in real-time given only a single RGB image. The essence of the\nproposed method is to improve the prediction by i) parametrizing the targets\n(e.g., 3D boxes) instead of directly estimating the targets, and ii)\ncooperative training across different modules in contrast to training these\nmodules individually. Specifically, we parametrize the 3D object bounding boxes\nby the predictions from several modules, i.e., 3D camera pose and object\nattributes. The proposed method provides two major advantages: i) The\nparametrization helps maintain the consistency between the 2D image and the 3D\nworld, thus largely reducing the prediction variances in 3D coordinates. ii)\nConstraints can be imposed on the parametrization to train different modules\nsimultaneously. We call these constraints \"cooperative losses\" as they enable\nthe joint training and inference. We employ three cooperative losses for 3D\nbounding boxes, 2D projections, and physical constraints to estimate a\ngeometrically consistent and physically plausible 3D scene. Experiments on the\nSUN RGB-D dataset shows that the proposed method significantly outperforms\nprior approaches on 3D object detection, 3D layout estimation, 3D camera pose\nestimation, and holistic scene understanding. \n\n"}
{"id": "1811.00201", "contents": "Title: Cogni-Net: Cognitive Feature Learning through Deep Visual Perception Abstract: Can we ask computers to recognize what we see from brain signals alone? Our\npaper seeks to utilize the knowledge learnt in the visual domain by popular\npre-trained vision models and use it to teach a recurrent model being trained\non brain signals to learn a discriminative manifold of the human brain's\ncognition of different visual object categories in response to perceived visual\ncues. For this we make use of brain EEG signals triggered from visual stimuli\nlike images and leverage the natural synchronization between images and their\ncorresponding brain signals to learn a novel representation of the cognitive\nfeature space. The concept of knowledge distillation has been used here for\ntraining the deep cognition model, CogniNet\\footnote{The source code of the\nproposed system is publicly available at\n{https://www.github.com/53X/CogniNET}}, by employing a student-teacher learning\ntechnique in order to bridge the process of inter-modal knowledge transfer. The\nproposed novel architecture obtains state-of-the-art results, significantly\nsurpassing other existing models. The experiments performed by us also suggest\nthat if visual stimuli information like brain EEG signals can be gathered on a\nlarge scale, then that would help to obtain a better understanding of the\nlargely unexplored domain of human brain cognition. \n\n"}
{"id": "1811.00342", "contents": "Title: Towards Highly Accurate and Stable Face Alignment for High-Resolution\n  Videos Abstract: In recent years, heatmap regression based models have shown their\neffectiveness in face alignment and pose estimation. However, Conventional\nHeatmap Regression (CHR) is not accurate nor stable when dealing with\nhigh-resolution facial videos, since it finds the maximum activated location in\nheatmaps which are generated from rounding coordinates, and thus leads to\nquantization errors when scaling back to the original high-resolution space. In\nthis paper, we propose a Fractional Heatmap Regression (FHR) for\nhigh-resolution video-based face alignment. The proposed FHR can accurately\nestimate the fractional part according to the 2D Gaussian function by sampling\nthree points in heatmaps. To further stabilize the landmarks among continuous\nvideo frames while maintaining the precise at the same time, we propose a novel\nstabilization loss that contains two terms to address time delay and non-smooth\nissues, respectively. Experiments on 300W, 300-VW and Talking Face datasets\nclearly demonstrate that the proposed method is more accurate and stable than\nthe state-of-the-art models. \n\n"}
{"id": "1811.00751", "contents": "Title: Show, Attend and Read: A Simple and Strong Baseline for Irregular Text\n  Recognition Abstract: Recognizing irregular text in natural scene images is challenging due to the\nlarge variance in text appearance, such as curvature, orientation and\ndistortion. Most existing approaches rely heavily on sophisticated model\ndesigns and/or extra fine-grained annotations, which, to some extent, increase\nthe difficulty in algorithm implementation and data collection. In this work,\nwe propose an easy-to-implement strong baseline for irregular scene text\nrecognition, using off-the-shelf neural network components and only word-level\nannotations. It is composed of a $31$-layer ResNet, an LSTM-based\nencoder-decoder framework and a 2-dimensional attention module. Despite its\nsimplicity, the proposed method is robust and achieves state-of-the-art\nperformance on both regular and irregular scene text recognition benchmarks.\nCode is available at: https://tinyurl.com/ShowAttendRead \n\n"}
{"id": "1811.01051", "contents": "Title: What evidence does deep learning model use to classify Skin Lesions? Abstract: Melanoma is a type of skin cancer with the most rapidly increasing incidence.\nEarly detection of melanoma using dermoscopy images significantly increases\npatients' survival rate. However, accurately classifying skin lesions by eye,\nespecially in the early stage of melanoma, is extremely challenging for the\ndermatologists. Hence, the discovery of reliable biomarkers will be meaningful\nfor melanoma diagnosis. Recent years, the value of deep learning empowered\ncomputer-assisted diagnose has been shown in biomedical imaging based decision\nmaking. However, much research focuses on improving disease detection accuracy\nbut not exploring the evidence of pathology. In this paper, we propose a method\nto interpret the deep learning classification findings. Firstly, we propose an\naccurate neural network architecture to classify skin lesions. Secondly, we\nutilize a prediction difference analysis method that examines each patch on the\nimage through patch-wised corrupting to detect the biomarkers. Lastly, we\nvalidate that our biomarker findings are corresponding to the patterns in the\nliterature. The findings can be significant and useful to guide clinical\ndiagnosis. \n\n"}
{"id": "1811.01476", "contents": "Title: Dynamic Representations Toward Efficient Inference on Deep Neural\n  Networks by Decision Gates Abstract: While deep neural networks extract rich features from the input data, the\ncurrent trade-off between depth and computational cost makes it difficult to\nadopt deep neural networks for many industrial applications, especially when\ncomputing power is limited. Here, we are inspired by the idea that, while\ndeeper embeddings are needed to discriminate difficult samples (i.e.,\nfine-grained discrimination), a large number of samples can be well\ndiscriminated via much shallower embeddings (i.e., coarse-grained\ndiscrimination). In this study, we introduce the simple yet effective concept\nof decision gates (d-gate), modules trained to decide whether a sample needs to\nbe projected into a deeper embedding or if an early prediction can be made at\nthe d-gate, thus enabling the computation of dynamic representations at\ndifferent depths. The proposed d-gate modules can be integrated with any deep\nneural network and reduces the average computational cost of the deep neural\nnetworks while maintaining modeling accuracy. The proposed d-gate framework is\nexamined via different network architectures and datasets, with experimental\nresults showing that leveraging the proposed d-gate modules led to a ~43%\nspeed-up and 44% FLOPs reduction on ResNet-101 and 55% speed-up and 39% FLOPs\nreduction on DenseNet-201 trained on the CIFAR10 dataset with only ~2% drop in\naccuracy. Furthermore, experiments where d-gate modules are integrated into\nResNet-101 trained on the ImageNet dataset demonstrate that it is possible to\nreduce the computational cost of the network by 1.5 GFLOPs without any drop in\nthe modeling accuracy. \n\n"}
{"id": "1811.01749", "contents": "Title: FUNN: Flexible Unsupervised Neural Network Abstract: Deep neural networks have demonstrated high accuracy in image classification\ntasks. However, they were shown to be weak against adversarial examples: a\nsmall perturbation in the image which changes the classification output\ndramatically. In recent years, several defenses have been proposed to solve\nthis issue in supervised classification tasks. We propose a method to obtain\nrobust features in unsupervised learning tasks against adversarial attacks. Our\nmethod differs from existing solutions by directly learning the robust features\nwithout the need to project the adversarial examples in the original examples\ndistribution space. A first auto-encoder A1 is in charge of perturbing the\ninput image to fool another auto-encoder A2 which is in charge of regenerating\nthe original image. A1 tries to find the less perturbed image under the\nconstraint that the error in the output of A2 should be at least equal to a\nthreshold. Thanks to this training, the encoder of A2 will be robust against\nadversarial attacks and could be used in different tasks like classification.\nUsing state-of-art network architectures, we demonstrate the robustness of the\nfeatures obtained thanks to this method in classification tasks. \n\n"}
{"id": "1811.02146", "contents": "Title: TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents Abstract: To safely and efficiently navigate in complex urban traffic, autonomous\nvehicles must make responsible predictions in relation to surrounding\ntraffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and\ncritical task is to explore the movement patterns of different traffic-agents\nand predict their future trajectories accurately to help the autonomous vehicle\nmake reasonable navigation decision. To solve this problem, we propose a long\nshort-term memory-based (LSTM-based) realtime traffic prediction algorithm,\nTrafficPredict. Our approach uses an instance layer to learn instances'\nmovements and interactions and has a category layer to learn the similarities\nof instances belonging to the same type to refine the prediction. In order to\nevaluate its performance, we collected trajectory datasets in a large city\nconsisting of varying conditions and traffic densities. The dataset includes\nmany challenging scenarios where vehicles, bicycles, and pedestrians move among\none another. We evaluate the performance of TrafficPredict on our new dataset\nand highlight its higher accuracy for trajectory prediction by comparing with\nprior prediction methods. \n\n"}
{"id": "1811.02234", "contents": "Title: Semantic bottleneck for computer vision tasks Abstract: This paper introduces a novel method for the representation of images that is\nsemantic by nature, addressing the question of computation intelligibility in\ncomputer vision tasks. More specifically, our proposition is to introduce what\nwe call a semantic bottleneck in the processing pipeline, which is a crossing\npoint in which the representation of the image is entirely expressed with\nnatural language , while retaining the efficiency of numerical representations.\nWe show that our approach is able to generate semantic representations that\ngive state-of-the-art results on semantic content-based image retrieval and\nalso perform very well on image classification tasks. Intelligibility is\nevaluated through user centered experiments for failure detection. \n\n"}
{"id": "1811.02642", "contents": "Title: Computational Histological Staining and Destaining of Prostate Core\n  Biopsy RGB Images with Generative Adversarial Neural Networks Abstract: Histopathology tissue samples are widely available in two states:\nparaffin-embedded unstained and non-paraffin-embedded stained whole slide RGB\nimages (WSRI). Hematoxylin and eosin stain (H&E) is one of the principal stains\nin histology but suffers from several shortcomings related to tissue\npreparation, staining protocols, slowness and human error. We report two novel\napproaches for training machine learning models for the computational H&E\nstaining and destaining of prostate core biopsy RGB images. The staining model\nuses a conditional generative adversarial network that learns hierarchical\nnon-linear mappings between whole slide RGB image (WSRI) pairs of prostate core\nbiopsy before and after H&E staining. The trained staining model can then\ngenerate computationally H&E-stained prostate core WSRIs using previously\nunseen non-stained biopsy images as input. The destaining model, by learning\nmappings between an H&E stained WSRI and a non-stained WSRI of the same biopsy,\ncan computationally destain previously unseen H&E-stained images. Structural\nand anatomical details of prostate tissue and colors, shapes, geometries,\nlocations of nuclei, stroma, vessels, glands and other cellular components were\ngenerated by both models with structural similarity indices of 0.68 (staining)\nand 0.84 (destaining). The proposed staining and destaining models can engender\ncomputational H&E staining and destaining of WSRI biopsies without additional\nequipment and devices. \n\n"}
{"id": "1811.03456", "contents": "Title: CAAD 2018: Iterative Ensemble Adversarial Attack Abstract: Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Adversarial attacks can be used to evaluate the robustness of deep\nlearning models before they are deployed. Unfortunately, most of existing\nadversarial attacks can only fool a black-box model with a low success rate. To\nimprove the success rates for black-box adversarial attacks, we proposed an\niterated adversarial attack against an ensemble of image classifiers. With this\nmethod, we won the 5th place in CAAD 2018 Targeted Adversarial Attack\ncompetition. \n\n"}
{"id": "1811.03695", "contents": "Title: Deep Learning Predicts Hip Fracture using Confounding Patient and\n  Healthcare Variables Abstract: Hip fractures are a leading cause of death and disability among older adults.\nHip fractures are also the most commonly missed diagnosis on pelvic\nradiographs. Computer-Aided Diagnosis (CAD) algorithms have shown promise for\nhelping radiologists detect fractures, but the image features underpinning\ntheir predictions are notoriously difficult to understand. In this study, we\ntrained deep learning models on 17,587 radiographs to classify fracture, five\npatient traits, and 14 hospital process variables. All 20 variables could be\npredicted from a radiograph (p < 0.05), with the best performances on scanner\nmodel (AUC=1.00), scanner brand (AUC=0.98), and whether the order was marked\n\"priority\" (AUC=0.79). Fracture was predicted moderately well from the image\n(AUC=0.78) and better when combining image features with patient data\n(AUC=0.86, p=2e-9) or patient data plus hospital process features (AUC=0.91,\np=1e-21). The model performance on a test set with matched patient variables\nwas significantly lower than a random test set (AUC=0.67, p=0.003); and when\nthe test set was matched on patient and image acquisition variables, the model\nperformed randomly (AUC=0.52, 95% CI 0.46-0.58), indicating that these\nvariables were the main source of the model's predictive ability overall. We\nalso used Naive Bayes to combine evidence from image models with patient and\nhospital data and found their inclusion improved performance, but that this\napproach was nevertheless inferior to directly modeling all variables. If CAD\nalgorithms are inexplicably leveraging patient and process variables in their\npredictions, it is unclear how radiologists should interpret their predictions\nin the context of other known patient data. Further research is needed to\nilluminate deep learning decision processes so that computers and clinicians\ncan effectively cooperate. \n\n"}
{"id": "1811.04544", "contents": "Title: Visual Saliency Maps Can Apply to Facial Expression Recognition Abstract: Human eyes concentrate different facial regions during distinct cognitive\nactivities. We study utilising facial visual saliency maps to classify\ndifferent facial expressions into different emotions. Our results show that our\nnovel method of merely using facial saliency maps can achieve a descent\naccuracy of 65\\%, much higher than the chance level of $1/7$. Furthermore, our\napproach is of semi-supervision, i.e., our facial saliency maps are generated\nfrom a general saliency prediction algorithm that is not explicitly designed\nfor face images. We also discovered that the classification accuracies of each\nemotional class using saliency maps demonstrate a strong positive correlation\nwith the accuracies produced by face images. Our work implies that humans may\nlook at different facial areas in order to perceive different emotions. \n\n"}
{"id": "1811.06498", "contents": "Title: Adjusting for Confounding in Unsupervised Latent Representations of\n  Images Abstract: Biological imaging data are often partially confounded or contain unwanted\nvariability. Examples of such phenomena include variable lighting across\nmicroscopy image captures, stain intensity variation in histological slides,\nand batch effects for high throughput drug screening assays. Therefore, to\ndevelop \"fair\" models which generalise well to unseen examples, it is crucial\nto learn data representations that are insensitive to nuisance factors of\nvariation. In this paper, we present a strategy based on adversarial training,\ncapable of learning unsupervised representations invariant to confounders. As\nan empirical validation of our method, we use deep convolutional autoencoders\nto learn unbiased cellular representations from microscopy imaging. \n\n"}
{"id": "1811.06817", "contents": "Title: Evaluating Uncertainty Quantification in End-to-End Autonomous Driving\n  Control Abstract: A rise in popularity of Deep Neural Networks (DNNs), attributed to more\npowerful GPUs and widely available datasets, has seen them being increasingly\nused within safety-critical domains. One such domain, self-driving, has\nbenefited from significant performance improvements, with millions of miles\nhaving been driven with no human intervention. Despite this, crashes and\nerroneous behaviours still occur, in part due to the complexity of verifying\nthe correctness of DNNs and a lack of safety guarantees.\n  In this paper, we demonstrate how quantitative measures of uncertainty can be\nextracted in real-time, and their quality evaluated in end-to-end controllers\nfor self-driving cars. To this end we utilise a recent method for gathering\napproximate uncertainty information from DNNs without changing the network's\narchitecture. We propose evaluation techniques for the uncertainty on two\nseparate architectures which use the uncertainty to predict crashes up to five\nseconds in advance. We find that mutual information, a measure of uncertainty\nin classification networks, is a promising indicator of forthcoming crashes. \n\n"}
{"id": "1811.07461", "contents": "Title: Indoor GeoNet: Weakly Supervised Hybrid Learning for Depth and Pose\n  Estimation Abstract: Humans naturally perceive a 3D scene in front of them through accumulation of\ninformation obtained from multiple interconnected projections of the scene and\nby interpreting their correspondence. This phenomenon has inspired artificial\nintelligence models to extract the depth and view angle of the observed scene\nby modeling the correspondence between different views of that scene. Our paper\nis built upon previous works in the field of unsupervised depth and relative\ncamera pose estimation from temporal consecutive video frames using deep\nlearning (DL) models. Our approach uses a hybrid learning framework introduced\nin a recent work called GeoNet, which leverages geometric constraints in the 3D\nscenes to synthesize a novel view from intermediate DL-based predicted depth\nand relative pose. However, the state-of-the-art unsupervised depth and pose\nestimation DL models are exclusively trained/tested on a few available outdoor\nscene datasets and we have shown they are hardly transferable to new scenes,\nespecially from indoor environments, in which estimation requires higher\nprecision and dealing with probable occlusions. This paper introduces \"Indoor\nGeoNet\", a weakly supervised depth and camera pose estimation model targeted\nfor indoor scenes. In Indoor GeoNet, we take advantage of the availability of\nindoor RGBD datasets collected by human or robot navigators, and added partial\n(i.e. weak) supervision in depth training into the model. Experimental results\nshowed that our model effectively generalizes to new scenes from different\nbuildings. Indoor GeoNet demonstrated significant depth and pose estimation\nerror reduction when compared to the original GeoNet, while showing 3 times\nmore reconstruction accuracy in synthesizing novel views in indoor\nenvironments. \n\n"}
{"id": "1811.07498", "contents": "Title: Robust Visual Tracking using Multi-Frame Multi-Feature Joint Modeling Abstract: It remains a huge challenge to design effective and efficient trackers under\ncomplex scenarios, including occlusions, illumination changes and pose\nvariations. To cope with this problem, a promising solution is to integrate the\ntemporal consistency across consecutive frames and multiple feature cues in a\nunified model. Motivated by this idea, we propose a novel correlation\nfilter-based tracker in this work, in which the temporal relatedness is\nreconciled under a multi-task learning framework and the multiple feature cues\nare modeled using a multi-view learning approach. We demonstrate the resulting\nregression model can be efficiently learned by exploiting the structure of\nblockwise diagonal matrix. A fast blockwise diagonal matrix inversion algorithm\nis developed thereafter for efficient online tracking. Meanwhile, we\nincorporate an adaptive scale estimation mechanism to strengthen the stability\nof scale variation tracking. We implement our tracker using two types of\nfeatures and test it on two benchmark datasets. Experimental results\ndemonstrate the superiority of our proposed approach when compared with other\nstate-of-the-art trackers. project homepage\nhttp://bmal.hust.edu.cn/project/KMF2JMTtracking.html \n\n"}
{"id": "1811.07630", "contents": "Title: SEIGAN: Towards Compositional Image Generation by Simultaneously\n  Learning to Segment, Enhance, and Inpaint Abstract: We present a novel approach to image manipulation and understanding by\nsimultaneously learning to segment object masks, paste objects to another\nbackground image, and remove them from original images. For this purpose, we\ndevelop a novel generative model for compositional image generation, SEIGAN\n(Segment-Enhance-Inpaint Generative Adversarial Network), which learns these\nthree operations together in an adversarial architecture with additional cycle\nconsistency losses. To train, SEIGAN needs only bounding box supervision and\ndoes not require pairing or ground truth masks. SEIGAN produces better\ngenerated images (evaluated by human assessors) than other approaches and\nproduces high-quality segmentation masks, improving over other adversarially\ntrained approaches and getting closer to the results of fully supervised\ntraining. \n\n"}
{"id": "1811.07958", "contents": "Title: Tukey-Inspired Video Object Segmentation Abstract: We investigate the problem of strictly unsupervised video object\nsegmentation, i.e., the separation of a primary object from background in video\nwithout a user-provided object mask or any training on an annotated dataset. We\nfind foreground objects in low-level vision data using a John Tukey-inspired\nmeasure of \"outlierness\". This Tukey-inspired measure also estimates the\nreliability of each data source as video characteristics change (e.g., a camera\nstarts moving). The proposed method achieves state-of-the-art results for\nstrictly unsupervised video object segmentation on the challenging DAVIS\ndataset. Finally, we use a variant of the Tukey-inspired measure to combine the\noutput of multiple segmentation methods, including those using supervision\nduring training, runtime, or both. This collectively more robust method of\nsegmentation improves the Jaccard measure of its constituent methods by as much\nas 28%. \n\n"}
{"id": "1811.07988", "contents": "Title: Automated Pain Detection from Facial Expressions using FACS: A Review Abstract: Facial pain expression is an important modality for assessing pain,\nespecially when the patient's verbal ability to communicate is impaired. The\nfacial muscle-based action units (AUs), which are defined by the Facial Action\nCoding System (FACS), have been widely studied and are highly reliable as a\nmethod for detecting facial expressions (FE) including valid detection of pain.\nUnfortunately, FACS coding by humans is a very time-consuming task that makes\nits clinical use prohibitive. Significant progress on automated facial\nexpression recognition (AFER) has led to its numerous successful applications\nin FACS-based affective computing problems. However, only a handful of studies\nhave been reported on automated pain detection (APD), and its application in\nclinical settings is still far from a reality. In this paper, we review the\nprogress in research that has contributed to automated pain detection, with\nfocus on 1) the framework-level similarity between spontaneous AFER and APD\nproblems; 2) the evolution of system design including the recent development of\ndeep learning methods; 3) the strategies and considerations in developing a\nFACS-based pain detection framework from existing research; and 4) introduction\nof the most relevant databases that are available for AFER and APD studies. We\nattempt to present key considerations in extending a general AFER framework to\nan APD framework in clinical settings. In addition, the performance metrics are\nalso highlighted in evaluating an AFER or an APD system. \n\n"}
{"id": "1811.08063", "contents": "Title: Visual Localization Under Appearance Change: Filtering Approaches Abstract: A major focus of current research on place recognition is visual localization\nfor autonomous driving. In this scenario, as cameras will be operating\ncontinuously, it is realistic to expect videos as an input to visual\nlocalization algorithms, as opposed to the single-image querying approach used\nin other visual localization works. In this paper, we show that exploiting\ntemporal continuity in the testing sequence significantly improves visual\nlocalization - qualitatively and quantitatively. Although intuitive, this idea\nhas not been fully explored in recent works. To this end, we propose two\nfiltering approaches to exploit the temporal smoothness of image sequences: i)\nfiltering on discrete domain with Hidden Markov Model, and ii) filtering on\ncontinuous domain with Monte Carlo-based visual localization. Our approaches\nrely on local features with an encoding technique to represent an image as a\nsingle vector. The experimental results on synthetic and real datasets show\nthat our proposed methods achieve better results than state of the art (i.e.,\ndeep learning-based pose regression approaches) for the task on visual\nlocalization under significant appearance change. Our synthetic dataset and\nsource code are made publicly available at\nhttps://sites.google.com/view/g2d-software/home and\nhttps://github.com/dadung/Visual-Localization-Filtering. \n\n"}
{"id": "1811.08126", "contents": "Title: Adversarial Feedback Loop Abstract: Thanks to their remarkable generative capabilities, GANs have gained great\npopularity, and are used abundantly in state-of-the-art methods and\napplications. In a GAN based model, a discriminator is trained to learn the\nreal data distribution. To date, it has been used only for training purposes,\nwhere it's utilized to train the generator to provide real-looking outputs. In\nthis paper we propose a novel method that makes an explicit use of the\ndiscriminator in test-time, in a feedback manner in order to improve the\ngenerator results. To the best of our knowledge it is the first time a\ndiscriminator is involved in test-time. We claim that the discriminator holds\nsignificant information on the real data distribution, that could be useful for\ntest-time as well, a potential that has not been explored before.\n  The approach we propose does not alter the conventional training stage. At\ntest-time, however, it transfers the output from the generator into the\ndiscriminator, and uses feedback modules (convolutional blocks) to translate\nthe features of the discriminator layers into corrections to the features of\nthe generator layers, which are used eventually to get a better generator\nresult. Our method can contribute to both conditional and unconditional GANs.\nAs demonstrated by our experiments, it can improve the results of\nstate-of-the-art networks for super-resolution, and image generation. \n\n"}
{"id": "1811.08305", "contents": "Title: IVD-Net: Intervertebral disc localization and segmentation in MRI with a\n  multi-modal UNet Abstract: Accurate localization and segmentation of intervertebral disc (IVD) is\ncrucial for the assessment of spine disease diagnosis. Despite the\ntechnological advances in medical imaging, IVD localization and segmentation\nare still manually performed, which is time-consuming and prone to errors. If,\nin addition, multi-modal imaging is considered, the burden imposed on disease\nassessments increases substantially. In this paper, we propose an architecture\nfor IVD localization and segmentation in multi-modal MRI, which extends the\nwell-known UNet. Compared to single images, multi-modal data brings\ncomplementary information, contributing to better data representation and\ndiscriminative power. Our contributions are three-fold. First, how to\neffectively integrate and fully leverage multi-modal data remains almost\nunexplored. In this work, each MRI modality is processed in a different path to\nbetter exploit their unique information. Second, inspired by HyperDenseNet, the\nnetwork is densely-connected both within each path and across different paths,\ngranting the model the freedom to learn where and how the different modalities\nshould be processed and combined. Third, we improved standard U-Net modules by\nextending inception modules with two dilated convolutions blocks of different\nscale, which helps handling multi-scale context. We report experiments over the\ndata set of the public MICCAI 2018 Challenge on Automatic Intervertebral Disc\nLocalization and Segmentation, with 13 multi-modal MRI images used for training\nand 3 for validation. We trained IVD-Net on an NVidia TITAN XP GPU with 16 GBs\nRAM, using ADAM as optimizer and a learning rate of 10e-5 during 200 epochs.\nTraining took about 5 hours, and segmentation of a whole volume about 2-3\nseconds, on average. Several baselines, with different multi-modal fusion\nstrategies, were used to demonstrate the effectiveness of the proposed\narchitecture. \n\n"}
{"id": "1811.08383", "contents": "Title: TSM: Temporal Shift Module for Efficient Video Understanding Abstract: The explosive growth in video streaming gives rise to challenges on\nperforming video understanding at high accuracy and low computation cost.\nConventional 2D CNNs are computationally cheap but cannot capture temporal\nrelationships; 3D CNN based methods can achieve good performance but are\ncomputationally intensive, making it expensive to deploy. In this paper, we\npropose a generic and effective Temporal Shift Module (TSM) that enjoys both\nhigh efficiency and high performance. Specifically, it can achieve the\nperformance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the\nchannels along the temporal dimension; thus facilitate information exchanged\namong neighboring frames. It can be inserted into 2D CNNs to achieve temporal\nmodeling at zero computation and zero parameters. We also extended TSM to\nonline setting, which enables real-time low-latency online video recognition\nand video object detection. TSM is accurate and efficient: it ranks the first\nplace on the Something-Something leaderboard upon publication; on Jetson Nano\nand Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video\nrecognition. The code is available at:\nhttps://github.com/mit-han-lab/temporal-shift-module. \n\n"}
{"id": "1811.08837", "contents": "Title: Recognizing Disguised Faces in the Wild Abstract: Research in face recognition has seen tremendous growth over the past couple\nof decades. Beginning from algorithms capable of performing recognition in\nconstrained environments, the current face recognition systems achieve very\nhigh accuracies on large-scale unconstrained face datasets. While upcoming\nalgorithms continue to achieve improved performance, a majority of the face\nrecognition systems are susceptible to failure under disguise variations, one\nof the most challenging covariate of face recognition. Most of the existing\ndisguise datasets contain images with limited variations, often captured in\ncontrolled settings. This does not simulate a real world scenario, where both\nintentional and unintentional unconstrained disguises are encountered by a face\nrecognition system. In this paper, a novel Disguised Faces in the Wild (DFW)\ndataset is proposed which contains over 11000 images of 1000 identities with\ndifferent types of disguise accessories. The dataset is collected from the\nInternet, resulting in unconstrained face images similar to real world\nsettings. This is the first-of-a-kind dataset with the availability of\nimpersonator and genuine obfuscated face images for each subject. The proposed\ndataset has been analyzed in terms of three levels of difficulty: (i) easy,\n(ii) medium, and (iii) hard in order to showcase the challenging nature of the\nproblem. It is our view that the research community can greatly benefit from\nthe DFW dataset in terms of developing algorithms robust to such adversaries.\nThe proposed dataset was released as part of the First International Workshop\nand Competition on Disguised Faces in the Wild at CVPR, 2018. This paper\npresents the DFW dataset in detail, including the evaluation protocols,\nbaseline results, performance analysis of the submissions received as part of\nthe competition, and three levels of difficulties of the DFW challenge dataset. \n\n"}
{"id": "1811.08982", "contents": "Title: Polarity Loss for Zero-shot Object Detection Abstract: Conventional object detection models require large amounts of training data.\nIn comparison, humans can recognize previously unseen objects by merely knowing\ntheir semantic description. To mimic similar behaviour, zero-shot object\ndetection aims to recognize and localize 'unseen' object instances by using\nonly their semantic information. The model is first trained to learn the\nrelationships between visual and semantic domains for seen objects, later\ntransferring the acquired knowledge to totally unseen objects. This setting\ngives rise to the need for correct alignment between visual and semantic\nconcepts, so that the unseen objects can be identified using only their\nsemantic attributes. In this paper, we propose a novel loss function called\n'Polarity loss', that promotes correct visual-semantic alignment for an\nimproved zero-shot object detection. On one hand, it refines the noisy semantic\nembeddings via metric learning on a 'Semantic vocabulary' of related concepts\nto establish a better synergy between visual and semantic domains. On the other\nhand, it explicitly maximizes the gap between positive and negative predictions\nto achieve better discrimination between seen, unseen and background objects.\nOur approach is inspired by embodiment theories in cognitive science, that\nclaim human semantic understanding to be grounded in past experiences (seen\nobjects), related linguistic concepts (word vocabulary) and visual perception\n(seen/unseen object images). We conduct extensive evaluations on MS-COCO and\nPascal VOC datasets, showing significant improvements over state of the art. \n\n"}
{"id": "1811.09178", "contents": "Title: Object-oriented Targets for Visual Navigation using Rich Semantic\n  Representations Abstract: When searching for an object humans navigate through a scene using semantic\ninformation and spatial relationships. We look for an object using our\nknowledge of its attributes and relationships with other objects to infer the\nprobable location. In this paper, we propose to tackle the visual navigation\nproblem using rich semantic representations of the observed scene and\nobject-oriented targets to train an agent. We show that both allows the agent\nto generalize to new targets and unseen scene in a short amount of training\ntime. \n\n"}
{"id": "1811.09236", "contents": "Title: Copy the Old or Paint Anew? An Adversarial Framework for (non-)\n  Parametric Image Stylization Abstract: Parametric generative deep models are state-of-the-art for photo and\nnon-photo realistic image stylization. However, learning complicated image\nrepresentations requires compute-intense models parametrized by a huge number\nof weights, which in turn requires large datasets to make learning successful.\nNon-parametric exemplar-based generation is a technique that works well to\nreproduce style from small datasets, but is also compute-intensive. These\naspects are a drawback for the practice of digital AI artists: typically one\nwants to use a small set of stylization images, and needs a fast flexible model\nin order to experiment with it. With this motivation, our work has these\ncontributions: (i) a novel stylization method called Fully Adversarial Mosaics\n(FAMOS) that combines the strengths of both parametric and non-parametric\napproaches; (ii) multiple ablations and image examples that analyze the method\nand show its capabilities; (iii) source code that will empower artists and\nmachine learning researchers to use and modify FAMOS. \n\n"}
{"id": "1811.09243", "contents": "Title: FAIM -- A ConvNet Method for Unsupervised 3D Medical Image Registration Abstract: We present a new unsupervised learning algorithm, \"FAIM\", for 3D medical\nimage registration. With a different architecture than the popular \"U-net\", the\nnetwork takes a pair of full image volumes and predicts the displacement fields\nneeded to register source to target. Compared with \"U-net\" based registration\nnetworks such as VoxelMorph, FAIM has fewer trainable parameters but can\nachieve higher registration accuracy as judged by Dice score on region labels\nin the Mindboggle-101 dataset. Moreover, with the proposed penalty loss on\nnegative Jacobian determinants, FAIM produces deformations with many fewer\n\"foldings\", i.e. regions of non-invertibility where the surface folds over\nitself. In our experiment, we varied the strength of this penalty and\ninvestigated changes in registration accuracy and non-invertibility in terms of\nnumber of \"folding\" locations. We found that FAIM is able to maintain both the\nadvantages of higher accuracy and fewer \"folding\" locations over VoxelMorph,\nover a range of hyper-parameters (with the same values used for both networks).\nFurther, when trading off registration accuracy for better invertibility, FAIM\nrequired less sacrifice of registration accuracy. Codes for this paper will be\nreleased upon publication. \n\n"}
{"id": "1811.09361", "contents": "Title: Pointwise Rotation-Invariant Network with Adaptive Sampling and 3D\n  Spherical Voxel Convolution Abstract: Point cloud analysis without pose priors is very challenging in real\napplications, as the orientations of point clouds are often unknown. In this\npaper, we propose a brand new point-set learning framework PRIN, namely,\nPointwise Rotation-Invariant Network, focusing on rotation-invariant feature\nextraction in point clouds analysis. We construct spherical signals by Density\nAware Adaptive Sampling to deal with distorted point distributions in spherical\nspace. In addition, we propose Spherical Voxel Convolution and Point\nRe-sampling to extract rotation-invariant features for each point. Our network\ncan be applied to tasks ranging from object classification, part segmentation,\nto 3D feature matching and label alignment. We show that, on the dataset with\nrandomly rotated point clouds, PRIN demonstrates better performance than\nstate-of-the-art methods without any data augmentation. We also provide\ntheoretical analysis for the rotation-invariance achieved by our methods. \n\n"}
{"id": "1811.09885", "contents": "Title: Forward Stability of ResNet and Its Variants Abstract: The residual neural network (ResNet) is a popular deep network architecture\nwhich has the ability to obtain high-accuracy results on several image\nprocessing problems. In order to analyze the behavior and structure of ResNet,\nrecent work has been on establishing connections between ResNets and\ncontinuous-time optimal control problems. In this work, we show that the\npost-activation ResNet is related to an optimal control problem with\ndifferential inclusions, and provide continuous-time stability results for the\ndifferential inclusion associated with ResNet. Motivated by the stability\nconditions, we show that alterations of either the architecture or the\noptimization problem can generate variants of ResNet which improve the\ntheoretical stability bounds. In addition, we establish stability bounds for\nthe full (discrete) network associated with two variants of ResNet, in\nparticular, bounds on the growth of the features and a measure of the\nsensitivity of the features with respect to perturbations. These results also\nhelp to show the relationship between the depth, regularization, and stability\nof the feature space. Computational experiments on the proposed variants show\nthat the accuracy of ResNet is preserved and that the accuracy seems to be\nmonotone with respect to the depth and various corruptions. \n\n"}
{"id": "1811.10203", "contents": "Title: 3D-LaneNet: End-to-End 3D Multiple Lane Detection Abstract: We introduce a network that directly predicts the 3D layout of lanes in a\nroad scene from a single image. This work marks a first attempt to address this\ntask with on-board sensing without assuming a known constant lane width or\nrelying on pre-mapped environments. Our network architecture, 3D-LaneNet,\napplies two new concepts: intra-network inverse-perspective mapping (IPM) and\nanchor-based lane representation. The intra-network IPM projection facilitates\na dual-representation information flow in both regular image-view and top-view.\nAn anchor-per-column output representation enables our end-to-end approach\nwhich replaces common heuristics such as clustering and outlier rejection,\ncasting lane estimation as an object detection problem. In addition, our\napproach explicitly handles complex situations such as lane merges and splits.\nResults are shown on two new 3D lane datasets, a synthetic and a real one. For\ncomparison with existing methods, we test our approach on the image-only\ntuSimple lane detection benchmark, achieving performance competitive with\nstate-of-the-art. \n\n"}
{"id": "1811.10427", "contents": "Title: MR-GAN: Manifold Regularized Generative Adversarial Networks Abstract: Despite the growing interest in generative adversarial networks (GANs),\ntraining GANs remains a challenging problem, both from a theoretical and a\npractical standpoint. To address this challenge, in this paper, we propose a\nnovel way to exploit the unique geometry of the real data, especially the\nmanifold information. More specifically, we design a method to regularize GAN\ntraining by adding an additional regularization term referred to as manifold\nregularizer. The manifold regularizer forces the generator to respect the\nunique geometry of the real data manifold and generate high quality data.\nFurthermore, we theoretically prove that the addition of this regularization\nterm in any class of GANs including DCGAN and Wasserstein GAN leads to improved\nperformance in terms of generalization, existence of equilibrium, and\nstability. Preliminary experiments show that the proposed manifold\nregularization helps in avoiding mode collapse and leads to stable training. \n\n"}
{"id": "1811.10519", "contents": "Title: Unsupervised 3D Shape Learning from Image Collections in the Wild Abstract: We present a method to learn the 3D surface of objects directly from a\ncollection of images. Previous work achieved this capability by exploiting\nadditional manual annotation, such as object pose, 3D surface templates,\ntemporal continuity of videos, manually selected landmarks, and\nforeground/background masks. In contrast, our method does not make use of any\nsuch annotation. Rather, it builds a generative model, a convolutional neural\nnetwork, which, given a noise vector sample, outputs the 3D surface and texture\nof an object and a background image. These 3 components combined with an\nadditional random viewpoint vector are then fed to a differential renderer to\nproduce a view of the sampled object and background. Our general principle is\nthat if the output of the renderer, the generated image, is realistic, then its\ninput, the generated 3D and texture, should also be realistic. To achieve\nrealism, the generative model is trained adversarially against a discriminator\nthat tries to distinguish between the output of the renderer and real images\nfrom the given data set. Moreover, our generative model can be paired with an\nencoder and trained as an autoencoder, to automatically extract the 3D shape,\ntexture and pose of the object in an image. Our trained generative model and\nencoder show promising results both on real and synthetic data, which\ndemonstrate for the first time that fully unsupervised 3D learning from image\ncollections is possible. \n\n"}
{"id": "1811.10597", "contents": "Title: GAN Dissection: Visualizing and Understanding Generative Adversarial\n  Networks Abstract: Generative Adversarial Networks (GANs) have recently achieved impressive\nresults for many real-world applications, and many GAN variants have emerged\nwith improvements in sample quality and training stability. However, they have\nnot been well visualized or understood. How does a GAN represent our visual\nworld internally? What causes the artifacts in GAN results? How do\narchitectural choices affect GAN learning? Answering such questions could\nenable us to develop new insights and better models.\n  In this work, we present an analytic framework to visualize and understand\nGANs at the unit-, object-, and scene-level. We first identify a group of\ninterpretable units that are closely related to object concepts using a\nsegmentation-based network dissection method. Then, we quantify the causal\neffect of interpretable units by measuring the ability of interventions to\ncontrol objects in the output. We examine the contextual relationship between\nthese units and their surroundings by inserting the discovered object concepts\ninto new images. We show several practical applications enabled by our\nframework, from comparing internal representations across different layers,\nmodels, and datasets, to improving GANs by locating and removing\nartifact-causing units, to interactively manipulating objects in a scene. We\nprovide open source interpretation tools to help researchers and practitioners\nbetter understand their GAN models. \n\n"}
{"id": "1811.11209", "contents": "Title: Iterative Transformer Network for 3D Point Cloud Abstract: 3D point cloud is an efficient and flexible representation of 3D structures.\nRecently, neural networks operating on point clouds have shown superior\nperformance on 3D understanding tasks such as shape classification and part\nsegmentation. However, performance on such tasks is evaluated on complete\nshapes aligned in a canonical frame, while real world 3D data are partial and\nunaligned. A key challenge in learning from partial, unaligned point cloud data\nis to learn features that are invariant or equivariant with respect to\ngeometric transformations. To address this challenge, we propose the Iterative\nTransformer Network (IT-Net), a network module that canonicalizes the pose of a\npartial object with a series of 3D rigid transformations predicted in an\niterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose\nestimator from partial point clouds without using complete object models.\nFurther, we show that IT-Net achieves superior performance over alternative 3D\ntransformer networks on various tasks, such as partial shape classification and\nobject part segmentation. \n\n"}
{"id": "1811.11296", "contents": "Title: Taking Control of Intra-class Variation in Conditional GANs Under Weak\n  Supervision Abstract: Generative Adversarial Networks (GANs) are able to learn mappings between\nsimple, relatively low-dimensional, random distributions and points on the\nmanifold of realistic images in image-space. The semantics of this mapping,\nhowever, are typically entangled such that meaningful image properties cannot\nbe controlled independently of one another. Conditional GANs (cGANs) provide a\npotential solution to this problem, allowing specific semantics to be enforced\nduring training. This solution, however, depends on the availability of precise\nlabels, which are sometimes difficult or near impossible to obtain, e.g. labels\nrepresenting lighting conditions or describing the background. In this paper we\nintroduce a new formulation of the cGAN that is able to learn disentangled,\nmultivariate models of semantically meaningful variation and which has the\nadvantage of requiring only the weak supervision of binary attribute labels.\nFor example, given only labels of ambient / non-ambient lighting, our method is\nable to learn multivariate lighting models disentangled from other factors such\nas the identity and pose. We coin the method intra-class variation isolation\n(IVI) and the resulting network the IVI-GAN. We evaluate IVI-GAN on the CelebA\ndataset and on synthetic 3D morphable model data, learning to disentangle\nattributes such as lighting, pose, expression, and even the background. \n\n"}
{"id": "1811.11358", "contents": "Title: Future Segmentation Using 3D Structure Abstract: Predicting the future to anticipate the outcome of events and actions is a\ncritical attribute of autonomous agents; particularly for agents which must\nrely heavily on real time visual data for decision making. Working towards this\ncapability, we address the task of predicting future frame segmentation from a\nstream of monocular video by leveraging the 3D structure of the scene. Our\nframework is based on learnable sub-modules capable of predicting pixel-wise\nscene semantic labels, depth, and camera ego-motion of adjacent frames. We\nfurther propose a recurrent neural network based model capable of predicting\nfuture ego-motion trajectory as a function of a series of past ego-motion\nsteps. Ultimately, we observe that leveraging 3D structure in the model\nfacilitates successful prediction, achieving state of the art accuracy in\nfuture semantic segmentation. \n\n"}
{"id": "1811.11823", "contents": "Title: Semantic Part Detection via Matching: Learning to Generalize to Novel\n  Viewpoints from Limited Training Data Abstract: Detecting semantic parts of an object is a challenging task in computer\nvision, particularly because it is hard to construct large annotated datasets\ndue to the difficulty of annotating semantic parts. In this paper we present an\napproach which learns from a small training dataset of annotated semantic\nparts, where the object is seen from a limited range of viewpoints, but\ngeneralizes to detect semantic parts from a much larger range of viewpoints.\nOur approach is based on a matching algorithm for finding accurate spatial\ncorrespondence between two images, which enables semantic parts annotated on\none image to be transplanted to another. In particular, this enables images in\nthe training dataset to be matched to a virtual 3D model of the object (for\nsimplicity, we assume that the object viewpoint can be estimated by standard\ntechniques). Then a clustering algorithm is used to annotate the semantic parts\nof the 3D virtual model. This virtual 3D model can be used to synthesize\nannotated images from a large range of viewpoint. These can be matched to\nimages in the test set, using the same matching algorithm, to detect semantic\nparts in novel viewpoints of the object. Our algorithm is very simple,\nintuitive, and contains very few parameters. We evaluate our approach in the\ncar subclass of the VehicleSemanticPart dataset. We show it outperforms\nstandard deep network approaches and, in particular, performs much better on\nnovel viewpoints. For facilitating the future research, code is available:\nhttps://github.com/ytongbai/SemanticPartDetection \n\n"}
{"id": "1811.12004", "contents": "Title: Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose Abstract: In this work we adapt multi-person pose estimation architecture to use it on\nedge devices. We follow the bottom-up approach from OpenPose, the winner of\nCOCO 2016 Keypoints Challenge, because of its decent quality and robustness to\nnumber of people inside the frame. With proposed network design and optimized\npost-processing code the full solution runs at 28 frames per second (fps) on\nIntel$\\unicode{xAE}$ NUC 6i7KYB mini PC and 26 fps on Core$^{TM}$ i7-6850K CPU.\nThe network model has 4.1M parameters and 9 billions floating-point operations\n(GFLOPs) complexity, which is just ~15% of the baseline 2-stage OpenPose with\nalmost the same quality. The code and model are available as a part of\nIntel$\\unicode{xAE}$ OpenVINO$^{TM}$ Toolkit. \n\n"}
{"id": "1811.12026", "contents": "Title: Attacks on State-of-the-Art Face Recognition using Attentional\n  Adversarial Attack Generative Network Abstract: With the broad use of face recognition, its weakness gradually emerges that\nit is able to be attacked. So, it is important to study how face recognition\nnetworks are subject to attacks. In this paper, we focus on a novel way to do\nattacks against face recognition network that misleads the network to identify\nsomeone as the target person not misclassify inconspicuously. Simultaneously,\nfor this purpose, we introduce a specific attentional adversarial attack\ngenerative network to generate fake face images. For capturing the semantic\ninformation of the target person, this work adds a conditional variational\nautoencoder and attention modules to learn the instance-level correspondences\nbetween faces. Unlike traditional two-player GAN, this work introduces face\nrecognition networks as the third player to participate in the competition\nbetween generator and discriminator which allows the attacker to impersonate\nthe target person better. The generated faces which are hard to arouse the\nnotice of onlookers can evade recognition by state-of-the-art networks and most\nof them are recognized as the target person. \n\n"}
{"id": "1811.12150", "contents": "Title: Parameter-Free Spatial Attention Network for Person Re-Identification Abstract: Global average pooling (GAP) allows to localize discriminative information\nfor recognition [40]. While GAP helps the convolution neural network to attend\nto the most discriminative features of an object, it may suffer if that\ninformation is missing e.g. due to camera viewpoint changes. To circumvent this\nissue, we argue that it is advantageous to attend to the global configuration\nof the object by modeling spatial relations among high-level features. We\npropose a novel architecture for Person Re-Identification, based on a novel\nparameter-free spatial attention layer introducing spatial relations among the\nfeature map activations back to the model. Our spatial attention layer\nconsistently improves the performance over the model without it. Results on\nfour benchmarks demonstrate a superiority of our model over the\nstate-of-the-art achieving rank-1 accuracy of 94.7% on Market-1501, 89.0% on\nDukeMTMC-ReID, 74.9% on CUHK03-labeled and 69.7% on CUHK03-detected. \n\n"}
{"id": "1811.12248", "contents": "Title: Discovering Spatio-Temporal Action Tubes Abstract: In this paper, we address the challenging problem of spatial and temporal\naction detection in videos. We first develop an effective approach to localize\nframe-level action regions through integrating static and kinematic information\nby the early- and late-fusion detection scheme. With the intention of exploring\nimportant temporal connections among the detected action regions, we propose a\ntracking-by-point-matching algorithm to stitch the discrete action regions into\na continuous spatio-temporal action tube. Recurrent 3D convolutional neural\nnetwork is used to predict action categories and determine temporal boundaries\nof the generated tubes. We then introduce an action footprint map to refine the\ncandidate tubes based on the action-specific spatial characteristics preserved\nin the convolutional layers of R3DCNN. In the extensive experiments, our method\nachieves superior detection results on the three public benchmark datasets:\nUCFSports, J-HMDB and UCF101. \n\n"}
{"id": "1811.12463", "contents": "Title: Fast and Flexible Indoor Scene Synthesis via Deep Convolutional\n  Generative Models Abstract: We present a new, fast and flexible pipeline for indoor scene synthesis that\nis based on deep convolutional generative models. Our method operates on a\ntop-down image-based representation, and inserts objects iteratively into the\nscene by predicting their category, location, orientation and size with\nseparate neural network modules. Our pipeline naturally supports automatic\ncompletion of partial scenes, as well as synthesis of complete scenes. Our\nmethod is significantly faster than the previous image-based method and\ngenerates result that outperforms it and other state-of-the-art deep generative\nscene models in terms of faithfulness to training data and perceived visual\nquality. \n\n"}
{"id": "1811.12673", "contents": "Title: ComDefend: An Efficient Image Compression Model to Defend Adversarial\n  Examples Abstract: Deep neural networks (DNNs) have been demonstrated to be vulnerable to\nadversarial examples. Specifically, adding imperceptible perturbations to clean\nimages can fool the well trained deep neural networks. In this paper, we\npropose an end-to-end image compression model to defend adversarial examples:\n\\textbf{ComDefend}. The proposed model consists of a compression convolutional\nneural network (ComCNN) and a reconstruction convolutional neural network\n(ResCNN). The ComCNN is used to maintain the structure information of the\noriginal image and purify adversarial perturbations. And the ResCNN is used to\nreconstruct the original image with high quality. In other words, ComDefend can\ntransform the adversarial image to its clean version, which is then fed to the\ntrained classifier. Our method is a pre-processing module, and does not modify\nthe classifier's structure during the whole process. Therefore, it can be\ncombined with other model-specific defense models to jointly improve the\nclassifier's robustness. A series of experiments conducted on MNIST, CIFAR10\nand ImageNet show that the proposed method outperforms the state-of-the-art\ndefense methods, and is consistently effective to protect classifiers against\nadversarial attacks. \n\n"}
{"id": "1812.00037", "contents": "Title: Adversarial Defense by Stratified Convolutional Sparse Coding Abstract: We propose an adversarial defense method that achieves state-of-the-art\nperformance among attack-agnostic adversarial defense methods while also\nmaintaining robustness to input resolution, scale of adversarial perturbation,\nand scale of dataset size. Based on convolutional sparse coding, we construct a\nstratified low-dimensional quasi-natural image space that faithfully\napproximates the natural image space while also removing adversarial\nperturbations. We introduce a novel Sparse Transformation Layer (STL) in\nbetween the input image and the first layer of the neural network to\nefficiently project images into our quasi-natural image space. Our experiments\nshow state-of-the-art performance of our method compared to other\nattack-agnostic adversarial defense methods in various adversarial settings. \n\n"}
{"id": "1812.00253", "contents": "Title: A Deep Learning Approach for Multi-View Engagement Estimation of\n  Children in a Child-Robot Joint Attention task Abstract: In this work we tackle the problem of child engagement estimation while\nchildren freely interact with a robot in their room. We propose a deep-based\nmulti-view solution that takes advantage of recent developments in human pose\ndetection. We extract the child's pose from different RGB-D cameras placed\nelegantly in the room, fuse the results and feed them to a deep neural network\ntrained for classifying engagement levels. The deep network contains a\nrecurrent layer, in order to exploit the rich temporal information contained in\nthe pose data. The resulting method outperforms a number of baseline\nclassifiers, and provides a promising tool for better automatic understanding\nof a child's attitude, interest and attention while cooperating with a robot.\nThe goal is to integrate this model in next generation social robots as an\nattention monitoring tool during various CRI tasks both for Typically Developed\n(TD) children and children affected by autism (ASD). \n\n"}
{"id": "1812.00303", "contents": "Title: Multi-modal Capsule Routing for Actor and Action Video Segmentation\n  Conditioned on Natural Language Queries Abstract: In this paper, we propose an end-to-end capsule network for pixel level\nlocalization of actors and actions present in a video. The localization is\nperformed based on a natural language query through which an actor and action\nare specified. We propose to encode both the video as well as textual input in\nthe form of capsules, which provide more effective representation in comparison\nwith standard convolution based features. We introduce a novel capsule based\nattention mechanism for fusion of video and text capsules for text selected\nvideo segmentation. The attention mechanism is performed via joint EM routing\nover video and text capsules for text selected actor and action localization.\nThe existing works on actor-action localization are mainly focused on\nlocalization in a single frame instead of the full video. Different from\nexisting works, we propose to perform the localization on all frames of the\nvideo. To validate the potential of the proposed network for actor and action\nlocalization on all the frames of a video, we extend an existing actor-action\ndataset (A2D) with annotations for all the frames. The experimental evaluation\ndemonstrates the effectiveness of the proposed capsule network for text\nselective actor and action localization in videos, and it also improves upon\nthe performance of the existing state-of-the art works on single frame-based\nlocalization. \n\n"}
{"id": "1812.00568", "contents": "Title: Visual Foresight: Model-Based Deep Reinforcement Learning for\n  Vision-Based Robotic Control Abstract: Deep reinforcement learning (RL) algorithms can learn complex robotic skills\nfrom raw sensory inputs, but have yet to achieve the kind of broad\ngeneralization and applicability demonstrated by deep learning methods in\nsupervised domains. We present a deep RL method that is practical for\nreal-world robotics tasks, such as robotic manipulation, and generalizes\neffectively to never-before-seen tasks and objects. In these settings, ground\ntruth reward signals are typically unavailable, and we therefore propose a\nself-supervised model-based approach, where a predictive model learns to\ndirectly predict the future from raw sensory readings, such as camera images.\nAt test time, we explore three distinct goal specification methods: designated\npixels, where a user specifies desired object manipulation tasks by selecting\nparticular pixels in an image and corresponding goal positions, goal images,\nwhere the desired goal state is specified with an image, and image classifiers,\nwhich define spaces of goal states. Our deep predictive models are trained\nusing data collected autonomously and continuously by a robot interacting with\nhundreds of objects, without human supervision. We demonstrate that visual MPC\ncan generalize to never-before-seen objects---both rigid and deformable---and\nsolve a range of user-defined object manipulation tasks using the same model. \n\n"}
{"id": "1812.00879", "contents": "Title: Image-based model parameter optimization using Model-Assisted Generative\n  Adversarial Networks Abstract: We propose and demonstrate the use of a model-assisted generative adversarial\nnetwork (GAN) to produce fake images that accurately match true images through\nthe variation of the parameters of the model that describes the features of the\nimages. The generator learns the model parameter values that produce fake\nimages that best match the true images. Two case studies show excellent\nagreement between the generated best match parameters and the true parameters.\nThe best match model parameter values can be used to retune the default\nsimulation to minimize any bias when applying image recognition techniques to\nfake and true images. In the case of a real-world experiment, the true images\nare experimental data with unknown true model parameter values, and the fake\nimages are produced by a simulation that takes the model parameters as input.\nThe model-assisted GAN uses a convolutional neural network to emulate the\nsimulation for all parameter values that, when trained, can be used as a\nconditional generator for fast fake-image production. \n\n"}
{"id": "1812.01261", "contents": "Title: Conditional Video Generation Using Action-Appearance Captions Abstract: The field of automatic video generation has received a boost thanks to the\nrecent Generative Adversarial Networks (GANs). However, most existing methods\ncannot control the contents of the generated video using a text caption, losing\ntheir usefulness to a large extent. This particularly affects human videos due\nto their great variety of actions and appearances. This paper presents\nConditional Flow and Texture GAN (CFT-GAN), a GAN-based video generation method\nfrom action-appearance captions. We propose a novel way of generating video by\nencoding a caption (e.g., \"a man in blue jeans is playing golf\") in a two-stage\ngeneration pipeline. Our CFT-GAN uses such caption to generate an optical flow\n(action) and a texture (appearance) for each frame. As a result, the output\nvideo reflects the content specified in the caption in a plausible way.\nMoreover, to train our method, we constructed a new dataset for human video\ngeneration with captions. We evaluated the proposed method qualitatively and\nquantitatively via an ablation study and a user study. The results demonstrate\nthat CFT-GAN is able to successfully generate videos containing the action and\nappearances indicated in the captions. \n\n"}
{"id": "1812.01393", "contents": "Title: TextField: Learning A Deep Direction Field for Irregular Scene Text\n  Detection Abstract: Scene text detection is an important step of scene text reading system. The\nmain challenges lie on significantly varied sizes and aspect ratios, arbitrary\norientations and shapes. Driven by recent progress in deep learning, impressive\nperformances have been achieved for multi-oriented text detection. Yet, the\nperformance drops dramatically in detecting curved texts due to the limited\ntext representation (e.g., horizontal bounding boxes, rotated rectangles, or\nquadrilaterals). It is of great interest to detect curved texts, which are\nactually very common in natural scenes. In this paper, we present a novel text\ndetector named TextField for detecting irregular scene texts. Specifically, we\nlearn a direction field pointing away from the nearest text boundary to each\ntext point. This direction field is represented by an image of two-dimensional\nvectors and learned via a fully convolutional neural network. It encodes both\nbinary text mask and direction information used to separate adjacent text\ninstances, which is challenging for classical segmentation-based approaches.\nBased on the learned direction field, we apply a simple yet effective\nmorphological-based post-processing to achieve the final detection.\nExperimental results show that the proposed TextField outperforms the\nstate-of-the-art methods by a large margin (28% and 8%) on two curved text\ndatasets: Total-Text and CTW1500, respectively, and also achieves very\ncompetitive performance on multi-oriented datasets: ICDAR 2015 and MSRA-TD500.\nFurthermore, TextField is robust in generalizing to unseen datasets. The code\nis available at https://github.com/YukangWang/TextField. \n\n"}
{"id": "1812.01690", "contents": "Title: General-to-Detailed GAN for Infrequent Class Medical Images Abstract: Deep learning has significant potential for medical imaging. However, since\nthe incident rate of each disease varies widely, the frequency of classes in a\nmedical image dataset is imbalanced, leading to poor accuracy for such\ninfrequent classes. One possible solution is data augmentation of infrequent\nclasses using synthesized images created by Generative Adversarial Networks\n(GANs), but conventional GANs also require certain amount of images to learn.\nTo overcome this limitation, here we propose General-to-detailed GAN (GDGAN),\nserially connected two GANs, one for general labels and the other for detailed\nlabels. GDGAN produced diverse medical images, and the network trained with an\naugmented dataset outperformed other networks using existing methods with\nrespect to Area-Under-Curve (AUC) of Receiver Operating Characteristic (ROC)\ncurve. \n\n"}
{"id": "1812.02019", "contents": "Title: Dynamic Spatio-temporal Graph-based CNNs for Traffic Prediction Abstract: Forecasting future traffic flows from previous ones is a challenging problem\nbecause of their complex and dynamic nature of spatio-temporal structures. Most\nexisting graph-based CNNs attempt to capture the static relations while largely\nneglecting the dynamics underlying sequential data. In this paper, we present\ndynamic spatio-temporal graph-based CNNs (DST-GCNNs) by learning expressive\nfeatures to represent spatio-temporal structures and predict future traffic\nflows from surveillance video data. In particular, DST-GCNN is a two stream\nnetwork. In the flow prediction stream, we present a novel graph-based\nspatio-temporal convolutional layer to extract features from a graph\nrepresentation of traffic flows. Then several such layers are stacked together\nto predict future flows over time. Meanwhile, the relations between traffic\nflows in the graph are often time variant as the traffic condition changes over\ntime. To capture the graph dynamics, we use the graph prediction stream to\npredict the dynamic graph structures, and the predicted structures are fed into\nthe flow prediction stream. Experiments on real datasets demonstrate that the\nproposed model achieves competitive performances compared with the other\nstate-of-the-art methods. \n\n"}
{"id": "1812.02611", "contents": "Title: OMNIA Faster R-CNN: Detection in the wild through dataset merging and\n  soft distillation Abstract: Object detectors tend to perform poorly in new or open domains, and require\nexhaustive yet costly annotations from fully labeled datasets. We aim at\nbenefiting from several datasets with different categories but without\nadditional labelling, not only to increase the number of categories detected,\nbut also to take advantage from transfer learning and to enhance domain\nindependence.\n  Our dataset merging procedure starts with training several initial Faster\nR-CNN on the different datasets while considering the complementary datasets'\nimages for domain adaptation. Similarly to self-training methods, the\npredictions of these initial detectors mitigate the missing annotations on the\ncomplementary datasets. The final OMNIA Faster R-CNN is trained with all\ncategories on the union of the datasets enriched by predictions. The joint\ntraining handles unsafe targets with a new classification loss called SoftSig\nin a softly supervised way.\n  Experimental results show that in the case of fashion detection for images in\nthe wild, merging Modanet with COCO increases the final performance from 45.5%\nto 57.4% in mAP. Applying our soft distillation to the task of detection with\ndomain shift between GTA and Cityscapes enables to beat the state-of-the-art by\n5.3 points. Our methodology could unlock object detection for real-world\napplications without immense datasets. \n\n"}
{"id": "1812.02619", "contents": "Title: Tube-CNN: Modeling temporal evolution of appearance for object detection\n  in video Abstract: Object detection in video is crucial for many applications. Compared to\nimages, video provides additional cues which can help to disambiguate the\ndetection problem. Our goal in this paper is to learn discriminative models for\nthe temporal evolution of object appearance and to use such models for object\ndetection. To model temporal evolution, we introduce space-time tubes\ncorresponding to temporal sequences of bounding boxes. We propose two CNN\narchitectures for generating and classifying tubes, respectively. Our tube\nproposal network (TPN) first generates a large number of spatio-temporal tube\nproposals maximizing object recall. The Tube-CNN then implements a tube-level\nobject detector in the video. Our method improves state of the art on two\nlarge-scale datasets for object detection in video: HollywoodHeads and ImageNet\nVID. Tube models show particular advantages in difficult dynamic scenes. \n\n"}
{"id": "1812.02707", "contents": "Title: Video Action Transformer Network Abstract: We introduce the Action Transformer model for recognizing and localizing\nhuman actions in video clips. We repurpose a Transformer-style architecture to\naggregate features from the spatiotemporal context around the person whose\nactions we are trying to classify. We show that by using high-resolution,\nperson-specific, class-agnostic queries, the model spontaneously learns to\ntrack individual people and to pick up on semantic context from the actions of\nothers. Additionally its attention mechanism learns to emphasize hands and\nfaces, which are often crucial to discriminate an action - all without explicit\nsupervision other than boxes and class labels. We train and test our Action\nTransformer network on the Atomic Visual Actions (AVA) dataset, outperforming\nthe state-of-the-art by a significant margin using only raw RGB frames as\ninput. \n\n"}
{"id": "1812.02831", "contents": "Title: Neural Image Decompression: Learning to Render Better Image Previews Abstract: A rapidly increasing portion of Internet traffic is dominated by requests\nfrom mobile devices with limited- and metered-bandwidth constraints. To satisfy\nthese requests, it has become standard practice for websites to transmit small\nand extremely compressed image previews as part of the initial page-load\nprocess. Recent work, based on an adaptive triangulation of the target image,\nhas shown the ability to generate thumbnails of full images at extreme\ncompression rates: 200 bytes or less with impressive gains (in terms of PSNR\nand SSIM) over both JPEG and WebP standards. However, qualitative assessments\nand preservation of semantic content can be less favorable. We present a novel\nmethod to significantly improve the reconstruction quality of the original\nimage with no changes to the encoded information. Our neural-based decoding not\nonly achieves higher PSNR and SSIM scores than the original methods, but also\nyields a substantial increase in semantic-level content preservation. In\naddition, by keeping the same encoding stream, our solution is completely\ninter-operable with the original decoder. The end result is suitable for a\nrange of small-device deployments, as it involves only a single forward-pass\nthrough a small, scalable network. \n\n"}
{"id": "1812.02836", "contents": "Title: High-Quality Face Capture Using Anatomical Muscles Abstract: Muscle-based systems have the potential to provide both anatomical accuracy\nand semantic interpretability as compared to blendshape models; however, a lack\nof expressivity and differentiability has limited their impact. Thus, we\npropose modifying a recently developed rather expressive muscle-based system in\norder to make it fully-differentiable; in fact, our proposed modifications\nallow this physically robust and anatomically accurate muscle model to\nconveniently be driven by an underlying blendshape basis. Our formulation is\nintuitive, natural, as well as monolithically and fully coupled such that one\ncan differentiate the model from end to end, which makes it viable for both\noptimization and learning-based approaches for a variety of applications. We\nillustrate this with a number of examples including both shape matching of\nthree-dimensional geometry as as well as the automatic determination of a\nthree-dimensional facial pose from a single two-dimensional RGB image without\nusing markers or depth information. \n\n"}
{"id": "1812.02898", "contents": "Title: TDAN: Temporally Deformable Alignment Network for Video Super-Resolution Abstract: Video super-resolution (VSR) aims to restore a photo-realistic\nhigh-resolution (HR) video frame from both its corresponding low-resolution\n(LR) frame (reference frame) and multiple neighboring frames (supporting\nframes). Due to varying motion of cameras or objects, the reference frame and\neach support frame are not aligned. Therefore, temporal alignment is a\nchallenging yet important problem for VSR. Previous VSR methods usually utilize\noptical flow between the reference frame and each supporting frame to wrap the\nsupporting frame for temporal alignment. Therefore, the performance of these\nimage-level wrapping-based models will highly depend on the prediction accuracy\nof optical flow, and inaccurate optical flow will lead to artifacts in the\nwrapped supporting frames, which also will be propagated into the reconstructed\nHR video frame. To overcome the limitation, in this paper, we propose a\ntemporal deformable alignment network (TDAN) to adaptively align the reference\nframe and each supporting frame at the feature level without computing optical\nflow. The TDAN uses features from both the reference frame and each supporting\nframe to dynamically predict offsets of sampling convolution kernels. By using\nthe corresponding kernels, TDAN transforms supporting frames to align with the\nreference frame. To predict the HR video frame, a reconstruction network taking\naligned frames and the reference frame is utilized. Experimental results\ndemonstrate the effectiveness of the proposed TDAN-based VSR model. \n\n"}
{"id": "1812.03050", "contents": "Title: Graph Cut Segmentation Methods Revisited with a Quantum Algorithm Abstract: The design and performance of computer vision algorithms are greatly\ninfluenced by the hardware on which they are implemented. CPUs, multi-core\nCPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas to\nbe realized. This is notably the case with GPUs, which has significantly\nchanged the landscape of computer vision research through deep learning. As the\nend of Moores law approaches, researchers and hardware manufacturers are\nexploring alternative hardware computing paradigms. Quantum computers are a\nvery promising alternative and offer polynomial or even exponential speed-ups\nover conventional computing for some problems. This paper presents a novel\napproach to image segmentation that uses new quantum computing hardware.\nSegmentation is formulated as a graph cut problem that can be mapped to the\nquantum approximate optimization algorithm (QAOA). This algorithm can be\nimplemented on current and near-term quantum computers. Encouraging results are\npresented on artificial and medical imaging data. This represents an important,\npractical step towards leveraging quantum computers for computer vision. \n\n"}
{"id": "1812.03282", "contents": "Title: Spatial-Temporal Person Re-identification Abstract: Most of current person re-identification (ReID) methods neglect a\nspatial-temporal constraint. Given a query image, conventional methods compute\nthe feature distances between the query image and all the gallery images and\nreturn a similarity ranked table. When the gallery database is very large in\npractice, these approaches fail to obtain a good performance due to appearance\nambiguity across different camera views. In this paper, we propose a novel\ntwo-stream spatial-temporal person ReID (st-ReID) framework that mines both\nvisual semantic information and spatial-temporal information. To this end, a\njoint similarity metric with Logistic Smoothing (LS) is introduced to integrate\ntwo kinds of heterogeneous information into a unified framework. To approximate\na complex spatial-temporal probability distribution, we develop a fast\nHistogram-Parzen (HP) method. With the help of the spatial-temporal constraint,\nthe st-ReID model eliminates lots of irrelevant images and thus narrows the\ngallery database. Without bells and whistles, our st-ReID method achieves\nrank-1 accuracy of 98.1\\% on Market-1501 and 94.4\\% on DukeMTMC-reID, improving\nfrom the baselines 91.2\\% and 83.8\\%, respectively, outperforming all previous\nstate-of-the-art methods by a large margin. \n\n"}
{"id": "1812.03944", "contents": "Title: Data Fine-tuning Abstract: In real-world applications, commercial off-the-shelf systems are utilized for\nperforming automated facial analysis including face recognition, emotion\nrecognition, and attribute prediction. However, a majority of these commercial\nsystems act as black boxes due to the inaccessibility of the model parameters\nwhich makes it challenging to fine-tune the models for specific applications.\nStimulated by the advances in adversarial perturbations, this research proposes\nthe concept of Data Fine-tuning to improve the classification accuracy of a\ngiven model without changing the parameters of the model. This is accomplished\nby modeling it as data (image) perturbation problem. A small amount of \"noise\"\nis added to the input with the objective of minimizing the classification loss\nwithout affecting the (visual) appearance. Experiments performed on three\npublicly available datasets LFW, CelebA, and MUCT, demonstrate the\neffectiveness of the proposed concept. \n\n"}
{"id": "1812.03945", "contents": "Title: A New Ensemble Learning Framework for 3D Biomedical Image Segmentation Abstract: 3D image segmentation plays an important role in biomedical image analysis.\nMany 2D and 3D deep learning models have achieved state-of-the-art segmentation\nperformance on 3D biomedical image datasets. Yet, 2D and 3D models have their\nown strengths and weaknesses, and by unifying them together, one may be able to\nachieve more accurate results. In this paper, we propose a new ensemble\nlearning framework for 3D biomedical image segmentation that combines the\nmerits of 2D and 3D models. First, we develop a fully convolutional network\nbased meta-learner to learn how to improve the results from 2D and 3D models\n(base-learners). Then, to minimize over-fitting for our sophisticated\nmeta-learner, we devise a new training method that uses the results of the\nbase-learners as multiple versions of \"ground truths\". Furthermore, since our\nnew meta-learner training scheme does not depend on manual annotation, it can\nutilize abundant unlabeled 3D image data to further improve the model.\nExtensive experiments on two public datasets (the HVSMR 2016 Challenge dataset\nand the mouse piriform cortex dataset) show that our approach is effective\nunder fully-supervised, semi-supervised, and transductive settings, and attains\nsuperior performance over state-of-the-art image segmentation methods. \n\n"}
{"id": "1812.04194", "contents": "Title: Loss Guided Activation for Action Recognition in Still Images Abstract: One significant problem of deep-learning based human action recognition is\nthat it can be easily misled by the presence of irrelevant objects or\nbackgrounds. Existing methods commonly address this problem by employing\nbounding boxes on the target humans as part of the input, in both training and\ntesting stages. This requirement of bounding boxes as part of the input is\nneeded to enable the methods to ignore irrelevant contexts and extract only\nhuman features. However, we consider this solution is inefficient, since the\nbounding boxes might not be available. Hence, instead of using a person\nbounding box as an input, we introduce a human-mask loss to automatically guide\nthe activations of the feature maps to the target human who is performing the\naction, and hence suppress the activations of misleading contexts. We propose a\nmulti-task deep learning method that jointly predicts the human action class\nand human location heatmap. Extensive experiments demonstrate our approach is\nmore robust compared to the baseline methods under the presence of irrelevant\nmisleading contexts. Our method achieves 94.06\\% and 40.65\\% (in terms of mAP)\non Stanford40 and MPII dataset respectively, which are 3.14\\% and 12.6\\%\nrelative improvements over the best results reported in the literature, and\nthus set new state-of-the-art results. Additionally, unlike some existing\nmethods, we eliminate the requirement of using a person bounding box as an\ninput during testing. \n\n"}
{"id": "1812.04204", "contents": "Title: 2.5D Visual Sound Abstract: Binaural audio provides a listener with 3D sound sensation, allowing a rich\nperceptual experience of the scene. However, binaural recordings are scarcely\navailable and require nontrivial expertise and equipment to obtain. We propose\nto convert common monaural audio into binaural audio by leveraging video. The\nkey idea is that visual frames reveal significant spatial cues that, while\nexplicitly lacking in the accompanying single-channel audio, are strongly\nlinked to it. Our multi-modal approach recovers this link from unlabeled video.\nWe devise a deep convolutional neural network that learns to decode the\nmonaural (single-channel) soundtrack into its binaural counterpart by injecting\nvisual information about object and scene configurations. We call the resulting\noutput 2.5D visual sound---the visual stream helps \"lift\" the flat single\nchannel audio into spatialized sound. In addition to sound generation, we show\nthe self-supervised representation learned by our network benefits audio-visual\nsource separation. Our video results:\nhttp://vision.cs.utexas.edu/projects/2.5D_visual_sound/ \n\n"}
{"id": "1812.04287", "contents": "Title: Deep Density-based Image Clustering Abstract: Recently, deep clustering, which is able to perform feature learning that\nfavors clustering tasks via deep neural networks, has achieved remarkable\nperformance in image clustering applications. However, the existing deep\nclustering algorithms generally need the number of clusters in advance, which\nis usually unknown in real-world tasks. In addition, the initial cluster\ncenters in the learned feature space are generated by $k$-means. This only\nworks well on spherical clusters and probably leads to unstable clustering\nresults. In this paper, we propose a two-stage deep density-based image\nclustering (DDC) framework to address these issues. The first stage is to train\na deep convolutional autoencoder (CAE) to extract low-dimensional feature\nrepresentations from high-dimensional image data, and then apply t-SNE to\nfurther reduce the data to a 2-dimensional space favoring density-based\nclustering algorithms. The second stage is to apply the developed density-based\nclustering technique on the 2-dimensional embedded data to automatically\nrecognize an appropriate number of clusters with arbitrary shapes. Concretely,\na number of local clusters are generated to capture the local structures of\nclusters, and then are merged via their density relationship to form the final\nclustering result. Experiments demonstrate that the proposed DDC achieves\ncomparable or even better clustering performance than state-of-the-art deep\nclustering methods, even though the number of clusters is not given. \n\n"}
{"id": "1812.04427", "contents": "Title: Zero-Shot Learning with Sparse Attribute Propagation Abstract: Zero-shot learning (ZSL) aims to recognize a set of unseen classes without\nany training images. The standard approach to ZSL requires a set of training\nimages annotated with seen class labels and a semantic descriptor for\nseen/unseen classes (attribute vector is the most widely used). Class\nlabel/attribute annotation is expensive; it thus severely limits the\nscalability of ZSL. In this paper, we define a new ZSL setting where only a few\nannotated images are collected from each seen class. This is clearly more\nchallenging yet more realistic than the conventional ZSL setting. To overcome\nthe resultant image-level attribute sparsity, we propose a novel inductive ZSL\nmodel termed sparse attribute propagation (SAP) by propagating attribute\nannotations to more unannotated images using sparse coding. This is followed by\nlearning bidirectional projections between features and attributes for ZSL. An\nefficient solver is provided, together with rigorous theoretic algorithm\nanalysis. With our SAP, we show that a ZSL training dataset can now be\naugmented by the abundant web images returned by image search engine, to\nfurther improve the model performance. Moreover, the general applicability of\nSAP is demonstrated on solving the social image annotation (SIA) problem.\nExtensive experiments show that our model achieves superior performance on both\nZSL and SIA. \n\n"}
{"id": "1812.04821", "contents": "Title: Efficient Super Resolution For Large-Scale Images Using Attentional GAN Abstract: Single Image Super Resolution (SISR) is a well-researched problem with broad\ncommercial relevance. However, most of the SISR literature focuses on\nsmall-size images under 500px, whereas business needs can mandate the\ngeneration of very high resolution images. At Expedia Group, we were tasked\nwith generating images of at least 2000px for display on the website, four\ntimes greater than the sizes typically reported in the literature. This\nrequirement poses a challenge that state-of-the-art models, validated on small\nimages, have not been proven to handle. In this paper, we investigate solutions\nto the problem of generating high-quality images for large-scale super\nresolution in a commercial setting. We find that training a generative\nadversarial network (GAN) with attention from scratch using a large-scale\nlodging image data set generates images with high PSNR and SSIM scores. We\ndescribe a novel attentional SISR model for large-scale images, A-SRGAN, that\nuses a Flexible Self Attention layer to enable processing of large-scale\nimages. We also describe a distributed algorithm which speeds up training by\naround a factor of five. \n\n"}
{"id": "1812.05313", "contents": "Title: When Semi-Supervised Learning Meets Transfer Learning: Training\n  Strategies, Models and Datasets Abstract: Semi-Supervised Learning (SSL) has been proved to be an effective way to\nleverage both labeled and unlabeled data at the same time. Recent\nsemi-supervised approaches focus on deep neural networks and have achieved\npromising results on several benchmarks: CIFAR10, CIFAR100 and SVHN. However,\nmost of their experiments are based on models trained from scratch instead of\npre-trained models. On the other hand, transfer learning has demonstrated its\nvalue when the target domain has limited labeled data. Here comes the intuitive\nquestion: is it possible to incorporate SSL when fine-tuning a pre-trained\nmodel? We comprehensively study how SSL methods starting from pretrained models\nperform under varying conditions, including training strategies, architecture\nchoice and datasets. From this study, we obtain several interesting and useful\nobservations.\n  While practitioners have had an intuitive understanding of these\nobservations, we do a comprehensive emperical analysis and demonstrate that:\n(1) the gains from SSL techniques over a fully-supervised baseline are smaller\nwhen trained from a pre-trained model than when trained from random\ninitialization, (2) when the domain of the source data used to train the\npre-trained model differs significantly from the domain of the target task, the\ngains from SSL are significantly higher and (3) some SSL methods are able to\nadvance fully-supervised baselines (like Pseudo-Label).\n  We hope our studies can deepen the understanding of SSL research and\nfacilitate the process of developing more effective SSL methods to utilize\npre-trained models. Code is now available at github. \n\n"}
{"id": "1812.05642", "contents": "Title: SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception Abstract: Unsupervised learning for geometric perception (depth, optical flow, etc.) is\nof great interest to autonomous systems. Recent works on unsupervised learning\nhave made considerable progress on perceiving geometry; however, they usually\nignore the coherence of objects and perform poorly under scenarios with dark\nand noisy environments. In contrast, supervised learning algorithms, which are\nrobust, require large labeled geometric dataset. This paper introduces SIGNet,\na novel framework that provides robust geometry perception without requiring\ngeometrically informative labels. Specifically, SIGNet integrates semantic\ninformation to make depth and flow predictions consistent with objects and\nrobust to low lighting conditions. SIGNet is shown to improve upon the\nstate-of-the-art unsupervised learning for depth prediction by 30% (in squared\nrelative error). In particular, SIGNet improves the dynamic object class\nperformance by 39% in depth prediction and 29% in flow prediction. Our code\nwill be made available at https://github.com/mengyuest/SIGNet \n\n"}
{"id": "1812.05806", "contents": "Title: A Self-Supervised Bootstrap Method for Single-Image 3D Face\n  Reconstruction Abstract: State-of-the-art methods for 3D reconstruction of faces from a single image\nrequire 2D-3D pairs of ground-truth data for supervision. Such data is costly\nto acquire, and most datasets available in the literature are restricted to\npairs for which the input 2D images depict faces in a near fronto-parallel\npose. Therefore, many data-driven methods for single-image 3D facial\nreconstruction perform poorly on profile and near-profile faces. We propose a\nmethod to improve the performance of single-image 3D facial reconstruction\nnetworks by utilizing the network to synthesize its own training data for\nfine-tuning, comprising: (i) single-image 3D reconstruction of faces in\nnear-frontal images without ground-truth 3D shape; (ii) application of a\nrigid-body transformation to the reconstructed face model; (iii) rendering of\nthe face model from new viewpoints; and (iv) use of the rendered image and\ncorresponding 3D reconstruction as additional data for supervised fine-tuning.\nThe new 2D-3D pairs thus produced have the same high-quality observed for near\nfronto-parallel reconstructions, thereby nudging the network towards more\nuniform performance as a function of the viewing angle of input faces.\nApplication of the proposed technique to the fine-tuning of a state-of-the-art\nsingle-image 3D-reconstruction network for faces demonstrates the usefulness of\nthe method, with particularly significant gains for profile or near-profile\nviews. \n\n"}
{"id": "1812.06190", "contents": "Title: Learning Latent Subspaces in Variational Autoencoders Abstract: Variational autoencoders (VAEs) are widely used deep generative models\ncapable of learning unsupervised latent representations of data. Such\nrepresentations are often difficult to interpret or control. We consider the\nproblem of unsupervised learning of features correlated to specific labels in a\ndataset. We propose a VAE-based generative model which we show is capable of\nextracting features correlated to binary labels in the data and structuring it\nin a latent subspace which is easy to interpret. Our model, the Conditional\nSubspace VAE (CSVAE), uses mutual information minimization to learn a\nlow-dimensional latent subspace associated with each label that can easily be\ninspected and independently manipulated. We demonstrate the utility of the\nlearned representations for attribute manipulation tasks on both the Toronto\nFace and CelebA datasets. \n\n"}
{"id": "1812.06271", "contents": "Title: PVSNet: Palm Vein Authentication Siamese Network Trained using Triplet\n  Loss and Adaptive Hard Mining by Learning Enforced Domain Specific Features Abstract: Designing an end-to-end deep learning network to match the biometric features\nwith limited training samples is an extremely challenging task. To address this\nproblem, we propose a new way to design an end-to-end deep CNN framework i.e.,\nPVSNet that works in two major steps: first, an encoder-decoder network is used\nto learn generative domain-specific features followed by a Siamese network in\nwhich convolutional layers are pre-trained in an unsupervised fashion as an\nautoencoder. The proposed model is trained via triplet loss function that is\nadjusted for learning feature embeddings in a way that minimizes the distance\nbetween embedding-pairs from the same subject and maximizes the distance with\nthose from different subjects, with a margin. In particular, a triplet Siamese\nmatching network using an adaptive margin based hard negative mining has been\nsuggested. The hyper-parameters associated with the training strategy, like the\nadaptive margin, have been tuned to make the learning more effective on\nbiometric datasets. In extensive experimentation, the proposed network\noutperforms most of the existing deep learning solutions on three type of\ntypical vein datasets which clearly demonstrates the effectiveness of our\nproposed method. \n\n"}
{"id": "1812.06576", "contents": "Title: Learning Incremental Triplet Margin for Person Re-identification Abstract: Person re-identification (ReID) aims to match people across multiple\nnon-overlapping video cameras deployed at different locations. To address this\nchallenging problem, many metric learning approaches have been proposed, among\nwhich triplet loss is one of the state-of-the-arts. In this work, we explore\nthe margin between positive and negative pairs of triplets and prove that large\nmargin is beneficial. In particular, we propose a novel multi-stage training\nstrategy which learns incremental triplet margin and improves triplet loss\neffectively. Multiple levels of feature maps are exploited to make the learned\nfeatures more discriminative. Besides, we introduce global hard identity\nsearching method to sample hard identities when generating a training batch.\nExtensive experiments on Market-1501, CUHK03, and DukeMTMCreID show that our\napproach yields a performance boost and outperforms most existing\nstate-of-the-art methods. \n\n"}
{"id": "1812.06869", "contents": "Title: BriarPatches: Pixel-Space Interventions for Inducing Demographic Parity Abstract: We introduce the BriarPatch, a pixel-space intervention that obscures\nsensitive attributes from representations encoded in pre-trained classifiers.\nThe patches encourage internal model representations not to encode sensitive\ninformation, which has the effect of pushing downstream predictors towards\nexhibiting demographic parity with respect to the sensitive information. The\nnet result is that these BriarPatches provide an intervention mechanism\navailable at user level, and complements prior research on fair representations\nthat were previously only applicable by model developers and ML experts. \n\n"}
{"id": "1812.06968", "contents": "Title: Geometric Scattering on Manifolds Abstract: The Euclidean scattering transform was introduced nearly a decade ago to\nimprove the mathematical understanding of the success of convolutional neural\nnetworks (ConvNets) in image data analysis and other tasks. Inspired by recent\ninterest in geometric deep learning, which aims to generalize ConvNets to\nmanifold and graph-structured domains, we generalize the scattering transform\nto compact manifolds. Similar to the Euclidean scattering transform, our\ngeometric scattering transform is based on a cascade of designed filters and\npointwise nonlinearities, which enables rigorous analysis of the feature\nextraction provided by scattering layers. Our main focus here is on theoretical\nunderstanding of this geometric scattering network, while setting aside\nimplementation aspects, although we remark that application of similar\ntransforms to graph data analysis has been studied recently in related work.\nOur results establish conditions under which geometric scattering provides\nlocalized isometry invariant descriptions of manifold signals, which are also\nstable to families of diffeomorphisms formulated in intrinsic manifolds terms.\nThese results not only generalize the deformation stability and local\nroto-translation invariance of Euclidean scattering, but also demonstrate the\nimportance of linking the used filter structures (e.g., in geometric deep\nlearning) to the underlying manifold geometry, or the data geometry it\nrepresents. \n\n"}
{"id": "1812.07045", "contents": "Title: EventNet: Asynchronous Recursive Event Processing Abstract: Event cameras are bio-inspired vision sensors that mimic retinas to\nasynchronously report per-pixel intensity changes rather than outputting an\nactual intensity image at regular intervals. This new paradigm of image sensor\noffers significant potential advantages; namely, sparse and non-redundant data\nrepresentation. Unfortunately, however, most of the existing artificial neural\nnetwork architectures, such as a CNN, require dense synchronous input data, and\ntherefore, cannot make use of the sparseness of the data. We propose EventNet,\na neural network designed for real-time processing of asynchronous event\nstreams in a recursive and event-wise manner. EventNet models dependence of the\noutput on tens of thousands of causal events recursively using a novel temporal\ncoding scheme. As a result, at inference time, our network operates in an\nevent-wise manner that is realized with very few sum-of-the-product\noperations---look-up table and temporal feature aggregation---which enables\nprocessing of 1 mega or more events per second on standard CPU. In experiments\nusing real data, we demonstrated the real-time performance and robustness of\nour framework. \n\n"}
{"id": "1812.07051", "contents": "Title: Unsupervised Single Image Dehazing Using Dark Channel Prior Loss Abstract: Single image dehazing is a critical stage in many modern-day autonomous\nvision applications. Early prior-based methods often involved a time-consuming\nminimization of a hand-crafted energy function. Recent learning-based\napproaches utilize the representational power of deep neural networks (DNNs) to\nlearn the underlying transformation between hazy and clear images. Due to\ninherent limitations in collecting matching clear and hazy images, these\nmethods resort to training on synthetic data; constructed from indoor images\nand corresponding depth information. This may result in a possible domain shift\nwhen treating outdoor scenes. We propose a completely unsupervised method of\ntraining via minimization of the well-known, Dark Channel Prior (DCP) energy\nfunction. Instead of feeding the network with synthetic data, we solely use\nreal-world outdoor images and tune the network's parameters by directly\nminimizing the DCP. Although our \"Deep DCP\" technique can be regarded as a fast\napproximator of DCP, it actually improves its results significantly. This\nsuggests an additional regularization obtained via the network and learning\nprocess. Experiments show that our method performs on par with large-scale\nsupervised methods. \n\n"}
{"id": "1812.07179", "contents": "Title: Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object\n  Detection for Autonomous Driving Abstract: 3D object detection is an essential task in autonomous driving. Recent\ntechniques excel with highly accurate detection rates, provided the 3D input\ndata is obtained from precise but expensive LiDAR technology. Approaches based\non cheaper monocular or stereo imagery data have, until now, resulted in\ndrastically lower accuracies --- a gap that is commonly attributed to poor\nimage-based depth estimation. However, in this paper we argue that it is not\nthe quality of the data but its representation that accounts for the majority\nof the difference. Taking the inner workings of convolutional neural networks\ninto consideration, we propose to convert image-based depth maps to\npseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With\nthis representation we can apply different existing LiDAR-based detection\nalgorithms. On the popular KITTI benchmark, our approach achieves impressive\nimprovements over the existing state-of-the-art in image-based performance ---\nraising the detection accuracy of objects within the 30m range from the\nprevious state-of-the-art of 22% to an unprecedented 74%. At the time of\nsubmission our algorithm holds the highest entry on the KITTI 3D object\ndetection leaderboard for stereo-image-based approaches. Our code is publicly\navailable at https://github.com/mileyan/pseudo_lidar. \n\n"}
{"id": "1812.07809", "contents": "Title: Found in Translation: Learning Robust Joint Representations by Cyclic\n  Translations Between Modalities Abstract: Multimodal sentiment analysis is a core research area that studies speaker\nsentiment expressed from the language, visual, and acoustic modalities. The\ncentral challenge in multimodal learning involves inferring joint\nrepresentations that can process and relate information from these modalities.\nHowever, existing work learns joint representations by requiring all modalities\nas input and as a result, the learned representations may be sensitive to noisy\nor missing modalities at test time. With the recent success of sequence to\nsequence (Seq2Seq) models in machine translation, there is an opportunity to\nexplore new ways of learning joint representations that may not require all\ninput modalities at test time. In this paper, we propose a method to learn\nrobust joint representations by translating between modalities. Our method is\nbased on the key insight that translation from a source to a target modality\nprovides a method of learning joint representations using only the source\nmodality as input. We augment modality translations with a cycle consistency\nloss to ensure that our joint representations retain maximal information from\nall modalities. Once our translation model is trained with paired multimodal\ndata, we only need data from the source modality at test time for final\nsentiment prediction. This ensures that our model remains robust from\nperturbations or missing information in the other modalities. We train our\nmodel with a coupled translation-prediction objective and it achieves new\nstate-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI,\nICT-MMMO, and YouTube. Additional experiments show that our model learns\nincreasingly discriminative joint representations with more input modalities\nwhile maintaining robustness to missing or perturbed modalities. \n\n"}
{"id": "1812.08155", "contents": "Title: Magnetic Resonance Fingerprinting using Recurrent Neural Networks Abstract: Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative\nmagnetic resonance imaging that allows simultaneous measurement of multiple\ntissue properties in a single, time-efficient acquisition. Standard MRF\nreconstructs parametric maps using dictionary matching and lacks scalability\ndue to computational inefficiency. We propose to perform MRF map reconstruction\nusing a recurrent neural network, which exploits the time-dependent information\nof the MRF signal evolution. We evaluate our method on multiparametric\nsynthetic signals and compare it to existing MRF map reconstruction approaches,\nincluding those based on neural networks. Our method achieves state-of-the-art\nestimates of T1 and T2 values. In addition, the reconstruction time is\nsignificantly reduced compared to dictionary-matching based approaches. \n\n"}
{"id": "1812.08789", "contents": "Title: Steerable $e$PCA: Rotationally Invariant Exponential Family PCA Abstract: In photon-limited imaging, the pixel intensities are affected by photon count\nnoise. Many applications, such as 3-D reconstruction using correlation analysis\nin X-ray free electron laser (XFEL) single molecule imaging, require an\naccurate estimation of the covariance of the underlying 2-D clean images.\nAccurate estimation of the covariance from low-photon count images must take\ninto account that pixel intensities are Poisson distributed, hence the\nclassical sample covariance estimator is sub-optimal. Moreover, in single\nmolecule imaging, including in-plane rotated copies of all images could further\nimprove the accuracy of covariance estimation. In this paper we introduce an\nefficient and accurate algorithm for covariance matrix estimation of count\nnoise 2-D images, including their uniform planar rotations and possibly\nreflections. Our procedure, steerable $e$PCA, combines in a novel way two\nrecently introduced innovations. The first is a methodology for principal\ncomponent analysis (PCA) for Poisson distributions, and more generally,\nexponential family distributions, called $e$PCA. The second is steerable PCA, a\nfast and accurate procedure for including all planar rotations for PCA. The\nresulting principal components are invariant to the rotation and reflection of\nthe input images. We demonstrate the efficiency and accuracy of steerable\n$e$PCA in numerical experiments involving simulated XFEL datasets and rotated\nYale B face data. \n\n"}
{"id": "1812.08852", "contents": "Title: A Scale Invariant Approach for Sparse Signal Recovery Abstract: In this paper, we study the ratio of the $L_1 $ and $L_2 $ norms, denoted as\n$L_1/L_2$, to promote sparsity. Due to the non-convexity and non-linearity,\nthere has been little attention to this scale-invariant model. Compared to\npopular models in the literature such as the $L_p$ model for $p\\in(0,1)$ and\nthe transformed $L_1$ (TL1), this ratio model is parameter free. Theoretically,\nwe present a strong null space property (sNSP) and prove that any sparse vector\nis a local minimizer of the $L_1 /L_2 $ model provided with this sNSP\ncondition. Computationally, we focus on a constrained formulation that can be\nsolved via the alternating direction method of multipliers (ADMM). Experiments\nshow that the proposed approach is comparable to the state-of-the-art methods\nin sparse recovery. In addition, a variant of the $L_1/L_2$ model to apply on\nthe gradient is also discussed with a proof-of-concept example of the MRI\nreconstruction. \n\n"}
{"id": "1812.09010", "contents": "Title: Face Hallucination Revisited: An Exploratory Study on Dataset Bias Abstract: Contemporary face hallucination (FH) models exhibit considerable ability to\nreconstruct high-resolution (HR) details from low-resolution (LR) face images.\nThis ability is commonly learned from examples of corresponding HR-LR image\npairs, created by artificially down-sampling the HR ground truth data. This\ndown-sampling (or degradation) procedure not only defines the characteristics\nof the LR training data, but also determines the type of image degradations the\nlearned FH models are eventually able to handle. If the image characteristics\nencountered with real-world LR images differ from the ones seen during\ntraining, FH models are still expected to perform well, but in practice may not\nproduce the desired results. In this paper we study this problem and explore\nthe bias introduced into FH models by the characteristics of the training data.\nWe systematically analyze the generalization capabilities of several FH models\nin various scenarios, where the image the degradation function does not match\nthe training setup and conduct experiments with synthetically downgraded as\nwell as real-life low-quality images. We make several interesting findings that\nprovide insight into existing problems with FH models and point to future\nresearch directions. \n\n"}
{"id": "1812.09280", "contents": "Title: Canonical Correlation Analysis for Misaligned Satellite Image Change\n  Detection Abstract: Canonical correlation analysis (CCA) is a statistical learning method that\nseeks to build view-independent latent representations from multi-view data.\nThis method has been successfully applied to several pattern analysis tasks\nsuch as image-to-text mapping and view-invariant object/action recognition.\nHowever, this success is highly dependent on the quality of data pairing (i.e.,\nalignments) and mispairing adversely affects the generalization ability of the\nlearned CCA representations. In this paper, we address the issue of alignment\nerrors using a new variant of canonical correlation analysis referred to as\nalignment-agnostic (AA) CCA. Starting from erroneously paired data taken from\ndifferent views, this CCA finds transformation matrices by optimizing a\nconstrained maximization problem that mixes a data correlation term with\ncontext regularization; the particular design of these two terms mitigates the\neffect of alignment errors when learning the CCA transformations. Experiments\nconducted on multi-view tasks, including multi-temporal satellite image change\ndetection, show that our AA CCA method is highly effective and resilient to\nmispairing errors. \n\n"}
{"id": "1812.09366", "contents": "Title: Wireless Software Synchronization of Multiple Distributed Cameras Abstract: We present a method for precisely time-synchronizing the capture of image\nsequences from a collection of smartphone cameras connected over WiFi. Our\nmethod is entirely software-based, has only modest hardware requirements, and\nachieves an accuracy of less than 250 microseconds on unmodified commodity\nhardware. It does not use image content and synchronizes cameras prior to\ncapture. The algorithm operates in two stages. In the first stage, we designate\none device as the leader and synchronize each client device's clock to it by\nestimating network delay. Once clocks are synchronized, the second stage\ninitiates continuous image streaming, estimates the relative phase of image\ntimestamps between each client and the leader, and shifts the streams into\nalignment. We quantitatively validate our results on a multi-camera rig imaging\na high-precision LED array and qualitatively demonstrate significant\nimprovements to multi-view stereo depth estimation and stitching of dynamic\nscenes. We release as open source 'libsoftwaresync', an Android implementation\nof our system, to inspire new types of collective capture applications. \n\n"}
{"id": "1812.09903", "contents": "Title: Adaptive Confidence Smoothing for Generalized Zero-Shot Learning Abstract: Generalized zero-shot learning (GZSL) is the problem of learning a classifier\nwhere some classes have samples and others are learned from side information,\nlike semantic attributes or text description, in a zero-shot learning fashion\n(ZSL). Training a single model that operates in these two regimes\nsimultaneously is challenging. Here we describe a probabilistic approach that\nbreaks the model into three modular components, and then combines them in a\nconsistent way. Specifically, our model consists of three classifiers: A\n\"gating\" model that makes soft decisions if a sample is from a \"seen\" class,\nand two experts: a ZSL expert, and an expert model for seen classes.\n  We address two main difficulties in this approach: How to provide an accurate\nestimate of the gating probability without any training samples for unseen\nclasses; and how to use expert predictions when it observes samples outside of\nits domain. The key insight to our approach is to pass information between the\nthree models to improve each one's accuracy, while maintaining the modular\nstructure. We test our approach, adaptive confidence smoothing (COSMO), on four\nstandard GZSL benchmark datasets and find that it largely outperforms\nstate-of-the-art GZSL models. COSMO is also the first model that closes the gap\nand surpasses the performance of generative models for GZSL, even-though it is\na light-weight model that is much easier to train and tune.\n  Notably, COSMO offers a new view for developing zero-shot models. Thanks to\nCOSMO's modular structure, instead of trying to perform well both on seen and\non unseen classes, models can focus on accurate classification of unseen\nclasses, and later consider seen class models. \n\n"}
{"id": "1812.10179", "contents": "Title: Deep Convolutional Generative Adversarial Network Based Food Recognition\n  Using Partially Labeled Data Abstract: Traditional machine learning algorithms using hand-crafted feature extraction\ntechniques (such as local binary pattern) have limited accuracy because of high\nvariation in images of the same class (or intra-class variation) for food\nrecognition task. In recent works, convolutional neural networks (CNN) have\nbeen applied to this task with better results than all previously reported\nmethods. However, they perform best when trained with large amount of annotated\n(labeled) food images. This is problematic when obtained in large volume,\nbecause they are expensive, laborious and impractical. Our work aims at\ndeveloping an efficient deep CNN learning-based method for food recognition\nalleviating these limitations by using partially labeled training data on\ngenerative adversarial networks (GANs). We make new enhancements to the\nunsupervised training architecture introduced by Goodfellow et al. (2014),\nwhich was originally aimed at generating new data by sampling a dataset. In\nthis work, we make modifications to deep convolutional GANs to make them robust\nand efficient for classifying food images. Experimental results on benchmarking\ndatasets show the superiority of our proposed method as compared to the\ncurrent-state-of-the-art methodologies even when trained with partially labeled\ntraining data. \n\n"}
{"id": "1812.10240", "contents": "Title: Studying the Plasticity in Deep Convolutional Neural Networks using\n  Random Pruning Abstract: Recently there has been a lot of work on pruning filters from deep\nconvolutional neural networks (CNNs) with the intention of reducing\ncomputations.The key idea is to rank the filters based on a certain criterion\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\nfilters are pruned away the remainder of the network is fine tuned and is shown\nto give performance comparable to the original unpruned network. In this work,\nwe report experiments which suggest that the comparable performance of the\npruned network is not due to the specific criterion chosen but due to the\ninherent plasticity of deep neural networks which allows them to recover from\nthe loss of pruned filters once the rest of the filters are fine-tuned.\nSpecifically we show counter-intuitive results wherein by randomly pruning\n25-50% filters from deep CNNs we are able to obtain the same performance as\nobtained by using state-of-the-art pruning methods. We empirically validate our\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\nneeds to be tested on only a small set of classes at test time (say, only\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\nclass specific pruning and show that even here a random pruning strategy gives\nclose to state-of-the-art performance. Unlike existing approaches which mainly\nfocus on the task of image classification, in this work we also report results\non object detection and image segmentation. We show that using a simple random\npruning strategy we can achieve significant speed up in object detection (74%\nimprovement in fps) while retaining the same accuracy as that of the original\nFaster RCNN model. Similarly we show that the performance of a pruned\nSegmentation Network (SegNet) is actually very similar to that of the original\nunpruned SegNet. \n\n"}
{"id": "1812.10265", "contents": "Title: A Survey of Deep Facial Attribute Analysis Abstract: Facial attribute analysis has received considerable attention when deep\nlearning techniques made remarkable breakthroughs in this field over the past\nfew years. Deep learning based facial attribute analysis consists of two basic\nsub-issues: facial attribute estimation (FAE), which recognizes whether facial\nattributes are present in given images, and facial attribute manipulation\n(FAM), which synthesizes or removes desired facial attributes. In this paper,\nwe provide a comprehensive survey of deep facial attribute analysis from the\nperspectives of both estimation and manipulation. First, we summarize a general\npipeline that deep facial attribute analysis follows, which comprises two\nstages: data preprocessing and model construction. Additionally, we introduce\nthe underlying theories of this two-stage pipeline for both FAE and FAM.\nSecond, the datasets and performance metrics commonly used in facial attribute\nanalysis are presented. Third, we create a taxonomy of state-of-the-art methods\nand review deep FAE and FAM algorithms in detail. Furthermore, several\nadditional facial attribute related issues are introduced, as well as relevant\nreal-world applications. Finally, we discuss possible challenges and promising\nfuture research directions. \n\n"}
{"id": "1812.10305", "contents": "Title: Spatial and Temporal Mutual Promotion for Video-based Person\n  Re-identification Abstract: Video-based person re-identification is a crucial task of matching video\nsequences of a person across multiple camera views. Generally, features\ndirectly extracted from a single frame suffer from occlusion, blur,\nillumination and posture changes. This leads to false activation or missing\nactivation in some regions, which corrupts the appearance and motion\nrepresentation. How to explore the abundant spatial-temporal information in\nvideo sequences is the key to solve this problem. To this end, we propose a\nRefining Recurrent Unit (RRU) that recovers the missing parts and suppresses\nnoisy parts of the current frame's features by referring historical frames.\nWith RRU, the quality of each frame's appearance representation is improved.\nThen we use the Spatial-Temporal clues Integration Module (STIM) to mine the\nspatial-temporal information from those upgraded features. Meanwhile, the\nmulti-level training objective is used to enhance the capability of RRU and\nSTIM. Through the cooperation of those modules, the spatial and temporal\nfeatures mutually promote each other and the final spatial-temporal feature\nrepresentation is more discriminative and robust. Extensive experiments are\nconducted on three challenging datasets, i.e., iLIDS-VID, PRID-2011 and MARS.\nThe experimental results demonstrate that our approach outperforms existing\nstate-of-the-art methods of video-based person re-identification on iLIDS-VID\nand MARS and achieves favorable results on PRID-2011. \n\n"}
{"id": "1812.10885", "contents": "Title: Coarse-to-fine Semantic Segmentation from Image-level Labels Abstract: Deep neural network-based semantic segmentation generally requires\nlarge-scale cost extensive annotations for training to obtain better\nperformance. To avoid pixel-wise segmentation annotations which are needed for\nmost methods, recently some researchers attempted to use object-level labels\n(e.g. bounding boxes) or image-level labels (e.g. image categories). In this\npaper, we propose a novel recursive coarse-to-fine semantic segmentation\nframework based on only image-level category labels. For each image, an initial\ncoarse mask is first generated by a convolutional neural network-based\nunsupervised foreground segmentation model and then is enhanced by a graph\nmodel. The enhanced coarse mask is fed to a fully convolutional neural network\nto be recursively refined. Unlike existing image-level label-based semantic\nsegmentation methods which require to label all categories for images contain\nmultiple types of objects, our framework only needs one label for each image\nand can handle images contains multi-category objects. With only trained on\nImageNet, our framework achieves comparable performance on PASCAL VOC dataset\nas other image-level label-based state-of-the-arts of semantic segmentation.\nFurthermore, our framework can be easily extended to foreground object\nsegmentation task and achieves comparable performance with the state-of-the-art\nsupervised methods on the Internet Object dataset. \n\n"}
{"id": "1812.11004", "contents": "Title: Hierarchical LSTMs with Adaptive Attention for Visual Captioning Abstract: Recent progress has been made in using attention based encoder-decoder\nframework for image and video captioning. Most existing decoders apply the\nattention mechanism to every generated word including both visual words (e.g.,\n\"gun\" and \"shooting\") and non-visual words (e.g. \"the\", \"a\"). However, these\nnon-visual words can be easily predicted using natural language model without\nconsidering visual signals or attention. Imposing attention mechanism on\nnon-visual words could mislead and decrease the overall performance of visual\ncaptioning. Furthermore, the hierarchy of LSTMs enables more complex\nrepresentation of visual data, capturing information at different scales. To\naddress these issues, we propose a hierarchical LSTM with adaptive attention\n(hLSTMat) approach for image and video captioning. Specifically, the proposed\nframework utilizes the spatial or temporal attention for selecting specific\nregions or frames to predict the related words, while the adaptive attention is\nfor deciding whether to depend on the visual information or the language\ncontext information. Also, a hierarchical LSTMs is designed to simultaneously\nconsider both low-level visual information and high-level language context\ninformation to support the caption generation. We initially design our hLSTMat\nfor video captioning task. Then, we further refine it and apply it to image\ncaptioning task. To demonstrate the effectiveness of our proposed framework, we\ntest our method on both video and image captioning tasks. Experimental results\nshow that our approach achieves the state-of-the-art performance for most of\nthe evaluation metrics on both tasks. The effect of important components is\nalso well exploited in the ablation study. \n\n"}
{"id": "1812.11207", "contents": "Title: CFA Bayer image sequence denoising and demosaicking chain Abstract: The demosaicking provokes the spatial and color correlation of noise, which\nis afterwards enhanced by the imaging pipeline. The correct removal previous or\nsimultaneously with the demosaicking process is not usually considered in the\nliterature. We present a novel imaging chain including a denoising of the Bayer\nCFA and a demosaicking method for image sequences. The proposed algorithm uses\na spatio-temporal patch method for the noise removal and demosaicking of the\nCFA. The experimentation, including real examples, illustrates the superior\nperformance of the proposed chain, avoiding the creation of artifacts and\ncolored spots in the final image. \n\n"}
{"id": "1812.11647", "contents": "Title: Path-Invariant Map Networks Abstract: Optimizing a network of maps among a collection of objects/domains (or map\nsynchronization) is a central problem across computer vision and many other\nrelevant fields. Compared to optimizing pairwise maps in isolation, the benefit\nof map synchronization is that there are natural constraints among a map\nnetwork that can improve the quality of individual maps. While such\nself-supervision constraints are well-understood for undirected map networks\n(e.g., the cycle-consistency constraint), they are under-explored for directed\nmap networks, which naturally arise when maps are given by parametric maps\n(e.g., a feed-forward neural network). In this paper, we study a natural\nself-supervision constraint for directed map networks called path-invariance,\nwhich enforces that composite maps along different paths between a fixed pair\nof source and target domains are identical. We introduce path-invariance bases\nfor efficient encoding of the path-invariance constraint and present an\nalgorithm that outputs a path-variance basis with polynomial time and space\ncomplexities. We demonstrate the effectiveness of our approach on optimizing\nobject correspondences, estimating dense image maps via neural networks, and\nsemantic segmentation of 3D scenes via map networks of diverse 3D\nrepresentations. In particular, for 3D semantic segmentation, our approach only\nrequires 8% labeled data from ScanNet to achieve the same performance as\ntraining a single 3D segmentation network with 30% to 100% labeled data. \n\n"}
{"id": "1812.11703", "contents": "Title: SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks Abstract: Siamese network based trackers formulate tracking as convolutional feature\ncross-correlation between target template and searching region. However,\nSiamese trackers still have accuracy gap compared with state-of-the-art\nalgorithms and they cannot take advantage of feature from deep networks, such\nas ResNet-50 or deeper. In this work we prove the core reason comes from the\nlack of strict translation invariance. By comprehensive theoretical analysis\nand experimental validations, we break this restriction through a simple yet\neffective spatial aware sampling strategy and successfully train a\nResNet-driven Siamese tracker with significant performance gain. Moreover, we\npropose a new model architecture to perform depth-wise and layer-wise\naggregations, which not only further improves the accuracy but also reduces the\nmodel size. We conduct extensive ablation studies to demonstrate the\neffectiveness of the proposed tracker, which obtains currently the best results\non four large tracking benchmarks, including OTB2015, VOT2018, UAV123, and\nLaSOT. Our model will be released to facilitate further studies based on this\nproblem. \n\n"}
{"id": "1901.00003", "contents": "Title: Learning Spatial Common Sense with Geometry-Aware Recurrent Networks Abstract: We integrate two powerful ideas, geometry and deep visual representation\nlearning, into recurrent network architectures for mobile visual scene\nunderstanding. The proposed networks learn to \"lift\" and integrate 2D visual\nfeatures over time into latent 3D feature maps of the scene. They are equipped\nwith differentiable geometric operations, such as projection, unprojection,\negomotion estimation and stabilization, in order to compute a\ngeometrically-consistent mapping between the world scene and their 3D latent\nfeature state. We train the proposed architectures to predict novel camera\nviews given short frame sequences as input. Their predictions strongly\ngeneralize to scenes with a novel number of objects, appearances and\nconfigurations; they greatly outperform previous works that do not consider\negomotion stabilization or a space-aware latent feature state. We train the\nproposed architectures to detect and segment objects in 3D using the latent 3D\nfeature map as input--as opposed to per frame features. The resulting object\ndetections persist over time: they continue to exist even when an object gets\noccluded or leaves the field of view. Our experiments suggest the proposed\nspace-aware latent feature memory and egomotion-stabilized convolutions are\nessential architectural choices for spatial common sense to emerge in\nartificial embodied visual agents. \n\n"}
{"id": "1901.00224", "contents": "Title: Ancient Painting to Natural Image: A New Solution for Painting\n  Processing Abstract: Collecting a large-scale and well-annotated dataset for image processing has\nbecome a common practice in computer vision. However, in the ancient painting\narea, this task is not practical as the number of paintings is limited and\ntheir style is greatly diverse. We, therefore, propose a novel solution for the\nproblems that come with ancient painting processing. This is to use domain\ntransfer to convert ancient paintings to photo-realistic natural images. By\ndoing so, the ancient painting processing problems become natural image\nprocessing problems and models trained on natural images can be directly\napplied to the transferred paintings. Specifically, we focus on Chinese ancient\nflower, bird and landscape paintings in this work. A novel Domain Style\nTransfer Network (DSTN) is proposed to transfer ancient paintings to natural\nimages which employ a compound loss to ensure that the transferred paintings\nstill maintain the color composition and content of the input paintings. The\nexperiment results show that the transferred paintings generated by the DSTN\nhave a better performance in both the human perceptual test and other image\nprocessing tasks than other state-of-art methods, indicating the authenticity\nof the transferred paintings and the superiority of the proposed method. \n\n"}
{"id": "1901.00536", "contents": "Title: Visualizing Deep Similarity Networks Abstract: For convolutional neural network models that optimize an image embedding, we\npropose a method to highlight the regions of images that contribute most to\npairwise similarity. This work is a corollary to the visualization tools\ndeveloped for classification networks, but applicable to the problem domains\nbetter suited to similarity learning. The visualization shows how similarity\nnetworks that are fine-tuned learn to focus on different features. We also\ngeneralize our approach to embedding networks that use different pooling\nstrategies and provide a simple mechanism to support image similarity searches\non objects or sub-regions in the query image. \n\n"}
{"id": "1901.00616", "contents": "Title: Volumetric Convolution: Automatic Representation Learning in Unit Ball Abstract: Convolution is an efficient technique to obtain abstract feature\nrepresentations using hierarchical layers in deep networks. Although performing\nconvolution in Euclidean geometries is fairly straightforward, its extension to\nother topological spaces---such as a sphere ($\\mathbb{S}^2$) or a unit ball\n($\\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel\n`\\emph{volumetric convolution}' operation that can effectively convolve\narbitrary functions in $\\mathbb{B}^3$. We develop a theoretical framework for\n\\emph{volumetric convolution} based on Zernike polynomials and efficiently\nimplement it as a differentiable and an easily pluggable layer for deep\nnetworks. Furthermore, our formulation leads to derivation of a novel formula\nto measure the symmetry of a function in $\\mathbb{B}^3$ around an arbitrary\naxis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of\nproposed volumetric convolution operation on a possible use-case i.e., 3D\nobject recognition task. \n\n"}
{"id": "1901.00850", "contents": "Title: CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions Abstract: Referring object detection and referring image segmentation are important\ntasks that require joint understanding of visual information and natural\nlanguage. Yet there has been evidence that current benchmark datasets suffer\nfrom bias, and current state-of-the-art models cannot be easily evaluated on\ntheir intermediate reasoning process. To address these issues and complement\nsimilar efforts in visual question answering, we build CLEVR-Ref+, a synthetic\ndiagnostic dataset for referring expression comprehension. The precise\nlocations and attributes of the objects are readily available, and the\nreferring expressions are automatically associated with functional programs.\nThe synthetic nature allows control over dataset bias (through sampling\nstrategy), and the modular programs enable intermediate reasoning ground truth\nwithout human annotators.\n  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we\nalso propose IEP-Ref, a module network approach that significantly outperforms\nother models on our dataset. In particular, we present two interesting and\nimportant findings using IEP-Ref: (1) the module trained to transform feature\nmaps into segmentation masks can be attached to any intermediate module to\nreveal the entire reasoning process step-by-step; (2) even if all training data\nhas at least one object referred, IEP-Ref can correctly predict no-foreground\nwhen presented with false-premise referring expressions. To the best of our\nknowledge, this is the first direct and quantitative proof that neural modules\nbehave in the way they are intended. \n\n"}
{"id": "1901.01493", "contents": "Title: Channel Locality Block: A Variant of Squeeze-and-Excitation Abstract: Attention mechanism is a hot spot in deep learning field. Using channel\nattention model is an effective method for improving the performance of the\nconvolutional neural network. Squeeze-and-Excitation block takes advantage of\nthe channel dependence, selectively emphasizing the important channels and\ncompressing the relatively useless channel. In this paper, we proposed a\nvariant of SE block based on channel locality. Instead of using full connection\nlayers to explore the global channel dependence, we adopt convolutional layers\nto learn the correlation between the nearby channels. We term this new\nalgorithm Channel Locality(C-Local) block. We evaluate SE block and C-Local\nblock by applying them to different CNNs architectures on cifar-10 dataset. We\nobserved that our C-Local block got higher accuracy than SE block did. \n\n"}
{"id": "1901.01570", "contents": "Title: Transductive Zero-Shot Learning with Visual Structure Constraint Abstract: To recognize objects of the unseen classes, most existing Zero-Shot\nLearning(ZSL) methods first learn a compatible projection function between the\ncommon semantic space and the visual space based on the data of source seen\nclasses, then directly apply it to the target unseen classes. However, in real\nscenarios, the data distribution between the source and target domain might not\nmatch well, thus causing the well-known \\textbf{domain shift} problem. Based on\nthe observation that visual features of test instances can be separated into\ndifferent clusters, we propose a new visual structure constraint on class\ncenters for transductive ZSL, to improve the generality of the projection\nfunction (i.e. alleviate the above domain shift problem). Specifically, three\ndifferent strategies (symmetric Chamfer-distance, Bipartite matching distance,\nand Wasserstein distance) are adopted to align the projected unseen semantic\ncenters and visual cluster centers of test instances. We also propose a new\ntraining strategy to handle the real cases where many unrelated images exist in\nthe test dataset, which is not considered in previous methods. Experiments on\nmany widely used datasets demonstrate that the proposed visual structure\nconstraint can bring substantial performance gain consistently and achieve\nstate-of-the-art results. The source code is available at\n\\url{https://github.com/raywzy/VSC}. \n\n"}
{"id": "1901.01575", "contents": "Title: Learning-Free Iris Segmentation Revisited: A First Step Toward Fast\n  Volumetric Operation Over Video Samples Abstract: Subject matching performance in iris biometrics is contingent upon fast,\nhigh-quality iris segmentation. In many cases, iris biometrics acquisition\nequipment takes a number of images in sequence and combines the segmentation\nand matching results for each image to strengthen the result. To date,\nsegmentation has occurred in 2D, operating on each image individually. But such\nmethodologies, while powerful, do not take advantage of potential gains in\nperformance afforded by treating sequential images as volumetric data. As a\nfirst step in this direction, we apply the Flexible Learning-Free\nReconstructoin of Neural Volumes (FLoRIN) framework, an open source\nsegmentation and reconstruction framework originally designed for neural\nmicroscopy volumes, to volumetric segmentation of iris videos. Further, we\nintroduce a novel dataset of near-infrared iris videos, in which each subject's\npupil rapidly changes size due to visible-light stimuli, as a test bed for\nFLoRIN. We compare the matching performance for iris masks generated by FLoRIN,\ndeep-learning-based (SegNet), and Daugman's (OSIRIS) iris segmentation\napproaches. We show that by incorporating volumetric information, FLoRIN\nachieves a factor of 3.6 to an order of magnitude increase in throughput with\nonly a minor drop in subject matching performance. We also demonstrate that\nFLoRIN-based iris segmentation maintains this speedup on low-resource hardware,\nmaking it suitable for embedded biometrics systems. \n\n"}
{"id": "1901.01620", "contents": "Title: Healthy versus pathological learning transferability in shoulder muscle\n  MRI segmentation using deep convolutional encoder-decoders Abstract: Automatic segmentation of pathological shoulder muscles in patients with\nmusculo-skeletal diseases is a challenging task due to the huge variability in\nmuscle shape, size, location, texture and injury. A reliable fully-automated\nsegmentation method from magnetic resonance images could greatly help\nclinicians to plan therapeutic interventions and predict interventional\noutcomes while eliminating time consuming manual segmentation efforts. The\npurpose of this work is three-fold. First, we investigate the feasibility of\npathological shoulder muscle segmentation using deep learning techniques, given\na very limited amount of available annotated pediatric data. Second, we address\nthe learning transferability from healthy to pathological data by comparing\ndifferent learning schemes in terms of model generalizability. Third, extended\nversions of deep convolutional encoder-decoder architectures using encoders\npre-trained on non-medical data are proposed to improve the segmentation\naccuracy. Methodological aspects are evaluated in a leave-one-out fashion on a\ndataset of 24 shoulder examinations from patients with obstetrical brachial\nplexus palsy and focus on 4 different muscles including deltoid as well as\ninfraspinatus, supraspinatus and subscapularis from the rotator cuff. The most\nrelevant segmentation model is partially pre-trained on ImageNet and jointly\nexploits inter-patient healthy and pathological annotated data. Its performance\nreaches Dice scores of 82.4%, 82.0%, 71.0% and 82.8% for deltoid,\ninfraspinatus, supraspinatus and subscapularis muscles. Absolute surface\nestimation errors are all below 83mm$^2$ except for supraspinatus with\n134.6mm$^2$. These contributions offer new perspectives for force inference in\nthe context of musculo-skeletal disorder management. \n\n"}
{"id": "1901.01760", "contents": "Title: Human Pose Estimation with Spatial Contextual Information Abstract: We explore the importance of spatial contextual information in human pose\nestimation. Most state-of-the-art pose networks are trained in a multi-stage\nmanner and produce several auxiliary predictions for deep supervision. With\nthis principle, we present two conceptually simple and yet computational\nefficient modules, namely Cascade Prediction Fusion (CPF) and Pose Graph Neural\nNetwork (PGNN), to exploit underlying contextual information. Cascade\nprediction fusion accumulates prediction maps from previous stages to extract\ninformative signals. The resulting maps also function as a prior to guide\nprediction at following stages. To promote spatial correlation among joints,\nour PGNN learns a structured representation of human pose as a graph. Direct\nmessage passing between different joints is enabled and spatial relation is\ncaptured. These two modules require very limited computational complexity.\nExperimental results demonstrate that our method consistently outperforms\nprevious methods on MPII and LSP benchmark. \n\n"}
{"id": "1901.01913", "contents": "Title: On the Global Geometry of Sphere-Constrained Sparse Blind Deconvolution Abstract: Blind deconvolution is the problem of recovering a convolutional kernel\n$\\boldsymbol a_0$ and an activation signal $\\boldsymbol x_0$ from their\nconvolution $\\boldsymbol y = \\boldsymbol a_0 \\circledast \\boldsymbol x_0$. This\nproblem is ill-posed without further constraints or priors. This paper studies\nthe situation where the nonzero entries in the activation signal are sparsely\nand randomly populated. We normalize the convolution kernel to have unit\nFrobenius norm and cast the sparse blind deconvolution problem as a nonconvex\noptimization problem over the sphere. With this spherical constraint, every\nspurious local minimum turns out to be close to some signed shift truncation of\nthe ground truth, under certain hypotheses. This benign property motivates an\neffective two stage algorithm that recovers the ground truth from the partial\ninformation offered by a suboptimal local minimum. This geometry-inspired\nalgorithm recovers the ground truth for certain microscopy problems, also\nexhibits promising performance in the more challenging image deblurring\nproblem. Our insights into the global geometry and the two stage algorithm\nextend to the convolutional dictionary learning problem, where a superposition\nof multiple convolution signals is observed. \n\n"}
{"id": "1901.01939", "contents": "Title: GASL: Guided Attention for Sparsity Learning in Deep Neural Networks Abstract: The main goal of network pruning is imposing sparsity on the neural network\nby increasing the number of parameters with zero value in order to reduce the\narchitecture size and the computational speedup. In most of the previous\nresearch works, sparsity is imposed stochastically without considering any\nprior knowledge of the weights distribution or other internal network\ncharacteristics. Enforcing too much sparsity may induce accuracy drop due to\nthe fact that a lot of important elements might have been eliminated. In this\npaper, we propose Guided Attention for Sparsity Learning (GASL) to achieve (1)\nmodel compression by having less number of elements and speed-up; (2) prevent\nthe accuracy drop by supervising the sparsity operation via a guided attention\nmechanism and (3) introduce a generic mechanism that can be adapted for any\ntype of architecture; Our work is aimed at providing a framework based on\ninterpretable attention mechanisms for imposing structured and non-structured\nsparsity in deep neural networks. For Cifar-100 experiments, we achieved the\nstate-of-the-art sparsity level and 2.91x speedup with competitive accuracy\ncompared to the best method. For MNIST and LeNet architecture we also achieved\nthe highest sparsity and speedup level. \n\n"}
{"id": "1901.02039", "contents": "Title: Spherical CNNs on Unstructured Grids Abstract: We present an efficient convolution kernel for Convolutional Neural Networks\n(CNNs) on unstructured grids using parameterized differential operators while\nfocusing on spherical signals such as panorama images or planetary signals. To\nthis end, we replace conventional convolution kernels with linear combinations\nof differential operators that are weighted by learnable parameters.\nDifferential operators can be efficiently estimated on unstructured grids using\none-ring neighbors, and learnable parameters can be optimized through standard\nback-propagation. As a result, we obtain extremely efficient neural networks\nthat match or outperform state-of-the-art network architectures in terms of\nperformance but with a significantly lower number of network parameters. We\nevaluate our algorithm in an extensive series of experiments on a variety of\ncomputer vision and climate science tasks, including shape classification,\nclimate pattern segmentation, and omnidirectional image semantic segmentation.\nOverall, we present (1) a novel CNN approach on unstructured grids using\nparameterized differential operators for spherical signals, and (2) we show\nthat our unique kernel parameterization allows our model to achieve the same or\nhigher accuracy with significantly fewer network parameters. \n\n"}
{"id": "1901.02444", "contents": "Title: Unseen Object Segmentation in Videos via Transferable Representations Abstract: In order to learn object segmentation models in videos, conventional methods\nrequire a large amount of pixel-wise ground truth annotations. However,\ncollecting such supervised data is time-consuming and labor-intensive. In this\npaper, we exploit existing annotations in source images and transfer such\nvisual information to segment videos with unseen object categories. Without\nusing any annotations in the target video, we propose a method to jointly mine\nuseful segments and learn feature representations that better adapt to the\ntarget frames. The entire process is decomposed into two tasks: 1) solving a\nsubmodular function for selecting object-like segments, and 2) learning a CNN\nmodel with a transferable module for adapting seen categories in the source\ndomain to the unseen target video. We present an iterative update scheme\nbetween two tasks to self-learn the final solution for object segmentation.\nExperimental results on numerous benchmark datasets show that the proposed\nmethod performs favorably against the state-of-the-art algorithms. \n\n"}
{"id": "1901.02446", "contents": "Title: Panoptic Feature Pyramid Networks Abstract: The recently introduced panoptic segmentation task has renewed our\ncommunity's interest in unifying the tasks of instance segmentation (for thing\nclasses) and semantic segmentation (for stuff classes). However, current\nstate-of-the-art methods for this joint task use separate and dissimilar\nnetworks for instance and semantic segmentation, without performing any shared\ncomputation. In this work, we aim to unify these methods at the architectural\nlevel, designing a single network for both tasks. Our approach is to endow Mask\nR-CNN, a popular instance segmentation method, with a semantic segmentation\nbranch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly,\nthis simple baseline not only remains effective for instance segmentation, but\nalso yields a lightweight, top-performing method for semantic segmentation. In\nthis work, we perform a detailed study of this minimally extended version of\nMask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust\nand accurate baseline for both tasks. Given its effectiveness and conceptual\nsimplicity, we hope our method can serve as a strong baseline and aid future\nresearch in panoptic segmentation. \n\n"}
{"id": "1901.02596", "contents": "Title: MSR: Multi-Scale Shape Regression for Scene Text Detection Abstract: State-of-the-art scene text detection techniques predict quadrilateral boxes\nthat are prone to localization errors while dealing with straight or curved\ntext lines of different orientations and lengths in scenes. This paper presents\na novel multi-scale shape regression network (MSR) that is capable of locating\ntext lines of different lengths, shapes and curvatures in scenes. The proposed\nMSR detects scene texts by predicting dense text boundary points that\ninherently capture the location and shape of text lines accurately and are also\nmore tolerant to the variation of text line length as compared with the state\nof the arts using proposals or segmentation. Additionally, the multi-scale\nnetwork extracts and fuses features at different scales which demonstrates\nsuperb tolerance to the text scale variation. Extensive experiments over\nseveral public datasets show that the proposed MSR obtains superior detection\nperformance for both curved and straight text lines of different lengths and\norientations. \n\n"}
{"id": "1901.02675", "contents": "Title: Low-Cost Transfer Learning of Face Tasks Abstract: Do we know what the different filters of a face network represent? Can we use\nthis filter information to train other tasks without transfer learning? For\ninstance, can age, head pose, emotion and other face related tasks be learned\nfrom face recognition network without transfer learning? Understanding the role\nof these filters allows us to transfer knowledge across tasks and take\nadvantage of large data sets in related tasks. Given a pretrained network, we\ncan infer which tasks the network generalizes for and the best way to transfer\nthe information to a new task. \n\n"}
{"id": "1901.02985", "contents": "Title: Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image\n  Segmentation Abstract: Recently, Neural Architecture Search (NAS) has successfully identified neural\nnetwork architectures that exceed human designed ones on large-scale image\nclassification. In this paper, we study NAS for semantic image segmentation.\nExisting works often focus on searching the repeatable cell structure, while\nhand-designing the outer network structure that controls the spatial resolution\nchanges. This choice simplifies the search space, but becomes increasingly\nproblematic for dense image prediction which exhibits a lot more network level\narchitectural variations. Therefore, we propose to search the network level\nstructure in addition to the cell level structure, which forms a hierarchical\narchitecture search space. We present a network level search space that\nincludes many popular designs, and develop a formulation that allows efficient\ngradient-based architecture search (3 P100 GPU days on Cityscapes images). We\ndemonstrate the effectiveness of the proposed method on the challenging\nCityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our\narchitecture searched specifically for semantic image segmentation, attains\nstate-of-the-art performance without any ImageNet pretraining. \n\n"}
{"id": "1901.03037", "contents": "Title: Image Transformation can make Neural Networks more robust against\n  Adversarial Examples Abstract: Neural networks are being applied in many tasks related to IoT with\nencouraging results. For example, neural networks can precisely detect human,\nobjects and animal via surveillance camera for security purpose. However,\nneural networks have been recently found vulnerable to well-designed input\nsamples that called adversarial examples. Such issue causes neural networks to\nmisclassify adversarial examples that are imperceptible to humans. We found\ngiving a rotation to an adversarial example image can defeat the effect of\nadversarial examples. Using MNIST number images as the original images, we\nfirst generated adversarial examples to neural network recognizer, which was\ncompletely fooled by the forged examples. Then we rotated the adversarial image\nand gave them to the recognizer to find the recognizer to regain the correct\nrecognition. Thus, we empirically confirmed rotation to images can protect\npattern recognizer based on neural networks from adversarial example attacks. \n\n"}
{"id": "1901.03470", "contents": "Title: Color Recognition for Rubik's Cube Robot Abstract: In this paper, we proposed three methods to solve color recognition of\nRubik's cube, which includes one offline method and two online methods. Scatter\nbalance \\& extreme learning machine (SB-ELM), a offline method, is proposed to\nillustrate the efficiency of training based method. We also point out the\nconception of color drifting which indicates offline methods are always\nineffectiveness and can not work well in continuous change circumstance. By\ncontrast, dynamic weight label propagation is proposed for labeling blocks\ncolor by known center blocks color of Rubik's cube. Furthermore, weak label\nhierarchic propagation, another online method, is also proposed for unknown all\ncolor information but only utilizes weak label of center block in color\nrecognition. We finally design a Rubik's cube robot and construct a dataset to\nillustrate the efficiency and effectiveness of our online methods and to\nindicate the ineffectiveness of offline method by color drifting in our\ndataset. \n\n"}
{"id": "1901.03760", "contents": "Title: Residual Pyramid FCN for Robust Follicle Segmentation Abstract: In this paper, we propose a pyramid network structure to improve the\nFCN-based segmentation solutions and apply it to label thyroid follicles in\nhistology images. Our design is based on the notion that a hierarchical\nupdating scheme, if properly implemented, can help FCNs capture the major\nobjects, as well as structure details in an image. To this end, we devise a\nresidual module to be mounted on consecutive network layers, through which\npixel labels would be propagated from the coarsest layer towards the finest\nlayer in a bottom-up fashion. We add five residual units along the decoding\npath of a modified U-Net to make our segmentation network, Res-Seg-Net.\nExperiments demonstrate that the multi-resolution set-up in our model is\neffective in producing segmentations with improved accuracy and robustness. \n\n"}
{"id": "1901.03814", "contents": "Title: Boundary-Aware Network for Fast and High-Accuracy Portrait Segmentation Abstract: Compared with other semantic segmentation tasks, portrait segmentation\nrequires both higher precision and faster inference speed. However, this\nproblem has not been well studied in previous works. In this paper, we propose\na lightweight network architecture, called Boundary-Aware Network (BANet) which\nselectively extracts detail information in boundary area to make high-quality\nsegmentation output with real-time( >25FPS) speed. In addition, we design a new\nloss function called refine loss which supervises the network with image level\ngradient information. Our model is able to produce finer segmentation results\nwhich has richer details than annotations. \n\n"}
{"id": "1901.03857", "contents": "Title: Deep-learning-based identification of odontogenic keratocysts in\n  hematoxylin- and eosin-stained jaw cyst specimens Abstract: The aim of this study was to develop a digital histopathology system for\nidentifying odontogenic keratocysts in hematoxylin- and eosin-stained tissue\nspecimens of jaw cysts. Approximately 5000 microscopy images with 400$\\times$\nmagnification were obtained from 199 odontogenic keratocysts, 208 dentigerous\ncysts, and 55 radicular cysts. A proportion of these images were used to make\ntraining patches, which were annotated as belonging to one of the following\nthree classes: keratocysts, non-keratocysts, and stroma. The patches for the\ncysts contained the complete lining epithelium, with the cyst cavity being\npresent on the upper side. The convolutional neural network (CNN) VGG16 was\nfinetuned to this dataset. The trained CNN could recognize the basal cell\npalisading pattern, which is the definitive criterion for diagnosing\nkeratocysts. Some of the remaining images were scanned and analyzed by the\ntrained CNN, whose output was then used to train another CNN for binary\nclassification (keratocyst or not). The area under the receiver operating\ncharacteristics curve for the entire algorithm was 0.997 for the test dataset.\nThus, the proposed patch classification strategy is usable for automated\nkeratocyst diagnosis. However, further optimization must be performed to make\nit suitable for practical use. \n\n"}
{"id": "1901.04111", "contents": "Title: Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views Abstract: This paper addresses the problem of 3D pose estimation for multiple people in\na few calibrated camera views. The main challenge of this problem is to find\nthe cross-view correspondences among noisy and incomplete 2D pose predictions.\nMost previous methods address this challenge by directly reasoning in 3D using\na pictorial structure model, which is inefficient due to the huge state space.\nWe propose a fast and robust approach to solve this problem. Our key idea is to\nuse a multi-way matching algorithm to cluster the detected 2D poses in all\nviews. Each resulting cluster encodes 2D poses of the same person across\ndifferent views and consistent correspondences across the keypoints, from which\nthe 3D pose of each person can be effectively inferred. The proposed convex\noptimization based multi-way matching algorithm is efficient and robust against\nmissing and false detections, without knowing the number of people in the\nscene. Moreover, we propose to combine geometric and appearance cues for\ncross-view matching. The proposed approach achieves significant performance\ngains from the state-of-the-art (96.3% vs. 90.6% and 96.9% vs. 88% on the\nCampus and Shelf datasets, respectively), while being efficient for real-time\napplications. \n\n"}
{"id": "1901.05375", "contents": "Title: DAFE-FD: Density Aware Feature Enrichment for Face Detection Abstract: Recent research on face detection, which is focused primarily on improving\naccuracy of detecting smaller faces, attempt to develop new anchor design\nstrategies to facilitate increased overlap between anchor boxes and ground\ntruth faces of smaller sizes. In this work, we approach the problem of small\nface detection with the motivation of enriching the feature maps using a\ndensity map estimation module. This module, inspired by recent crowd\ncounting/density estimation techniques, performs the task of estimating the per\npixel density of people/faces present in the image. Output of this module is\nemployed to accentuate the feature maps from the backbone network using a\nfeature enrichment module before being used for detecting smaller faces. The\nproposed approach can be used to complement recent anchor-design based novel\nmethods to further improve their results. Experiments conducted on different\ndatasets such as WIDER, FDDB and Pascal-Faces demonstrate the effectiveness of\nthe proposed approach. \n\n"}
{"id": "1901.05657", "contents": "Title: Certainty Driven Consistency Loss on Multi-Teacher Networks for\n  Semi-Supervised Learning Abstract: One of the successful approaches in semi-supervised learning is based on the\nconsistency regularization. Typically, a student model is trained to be\nconsistent with teacher prediction for the inputs under different\nperturbations. To be successful, the prediction targets given by teacher should\nhave good quality, otherwise the student can be misled by teacher.\nUnfortunately, existing methods do not assess the quality of the teacher\ntargets. In this paper, we propose a novel Certainty-driven Consistency Loss\n(CCL) that exploits the predictive uncertainty in the consistency loss to let\nthe student dynamically learn from reliable targets. Specifically, we propose\ntwo approaches, i.e. Filtering CCL and Temperature CCL to either filter out\nuncertain predictions or pay less attention on them in the consistency\nregularization. We further introduce a novel decoupled framework to encourage\nmodel difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100\ndemonstrate the advantages of our method over a few existing methods. \n\n"}
{"id": "1901.07012", "contents": "Title: Understanding the Impact of Label Granularity on CNN-based Image\n  Classification Abstract: In recent years, supervised learning using Convolutional Neural Networks\n(CNNs) has achieved great success in image classification tasks, and large\nscale labeled datasets have contributed significantly to this achievement.\nHowever, the definition of a label is often application dependent. For example,\nan image of a cat can be labeled as \"cat\" or perhaps more specifically \"Persian\ncat.\" We refer to this as label granularity. In this paper, we conduct\nextensive experiments using various datasets to demonstrate and analyze how and\nwhy training based on fine-grain labeling, such as \"Persian cat\" can improve\nCNN accuracy on classifying coarse-grain classes, in this case \"cat.\" The\nexperimental results show that training CNNs with fine-grain labels improves\nboth network's optimization and generalization capabilities, as intuitively it\nencourages the network to learn more features, and hence increases\nclassification accuracy on coarse-grain classes under all datasets considered.\nMoreover, fine-grain labels enhance data efficiency in CNN training. For\nexample, a CNN trained with fine-grain labels and only 40% of the total\ntraining data can achieve higher accuracy than a CNN trained with the full\ntraining dataset and coarse-grain labels. These results point to two possible\napplications of this work: (i) with sufficient human resources, one can improve\nCNN performance by re-labeling the dataset with fine-grain labels, and (ii)\nwith limited human resources, to improve CNN performance, rather than\ncollecting more training data, one may instead use fine-grain labels for the\ndataset. We further propose a metric called Average Confusion Ratio to\ncharacterize the effectiveness of fine-grain labeling, and show its use through\nextensive experimentation. Code is available at\nhttps://github.com/cmu-enyac/Label-Granularity. \n\n"}
{"id": "1901.07172", "contents": "Title: Efficient Image Splicing Localization via Contrastive Feature Extraction Abstract: In this work, we propose a new data visualization and clustering technique\nfor discovering discriminative structures in high-dimensional data. This\ntechnique, referred to as cPCA++, utilizes the fact that the interesting\nfeatures of a \"target\" dataset may be obscured by high variance components\nduring traditional PCA. By analyzing what is referred to as a \"background\"\ndataset (i.e., one that exhibits the high variance principal components but not\nthe interesting structures), our technique is capable of efficiently\nhighlighting the structure that is unique to the \"target\" dataset. Similar to\nanother recently proposed algorithm called \"contrastive PCA\" (cPCA), the\nproposed cPCA++ method identifies important dataset specific patterns that are\nnot detected by traditional PCA in a wide variety of settings. However, the\nproposed cPCA++ method is significantly more efficient than cPCA, because it\ndoes not require the parameter sweep in the latter approach. We applied the\ncPCA++ method to the problem of image splicing localization. In this\napplication, we utilize authentic edges as the background dataset and the\nspliced edges as the target dataset. The proposed method is significantly more\nefficient than state-of-the-art methods, as the former does not require\niterative updates of filter weights via stochastic gradient descent and\nbackpropagation, nor the training of a classifier. Furthermore, the cPCA++\nmethod is shown to provide performance scores comparable to the\nstate-of-the-art Multi-task Fully Convolutional Network (MFCN). \n\n"}
{"id": "1901.07821", "contents": "Title: Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff Abstract: Lossy compression algorithms are typically designed and analyzed through the\nlens of Shannon's rate-distortion theory, where the goal is to achieve the\nlowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate.\nHowever, in recent years, it has become increasingly accepted that \"low\ndistortion\" is not a synonym for \"high perceptual quality\", and in fact\noptimization of one often comes at the expense of the other. In light of this\nunderstanding, it is natural to seek for a generalization of rate-distortion\ntheory which takes perceptual quality into account. In this paper, we adopt the\nmathematical definition of perceptual quality recently proposed by Blau &\nMichaeli (2018), and use it to study the three-way tradeoff between rate,\ndistortion, and perception. We show that restricting the perceptual quality to\nbe high, generally leads to an elevation of the rate-distortion curve, thus\nnecessitating a sacrifice in either rate or distortion. We prove several\nfundamental properties of this triple-tradeoff, calculate it in closed form for\na Bernoulli source, and illustrate it visually on a toy MNIST example. \n\n"}
{"id": "1901.08212", "contents": "Title: Semi-Supervised Image-to-Image Translation Abstract: Image-to-image translation is a long-established and a difficult problem in\ncomputer vision. In this paper we propose an adversarial based model for\nimage-to-image translation. The regular deep neural-network based methods\nperform the task of image-to-image translation by comparing gram matrices and\nusing image segmentation which requires human intervention. Our generative\nadversarial network based model works on a conditional probability approach.\nThis approach makes the image translation independent of any local, global and\ncontent or style features. In our approach we use a bidirectional\nreconstruction model appended with the affine transform factor that helps in\nconserving the content and photorealism as compared to other models. The\nadvantage of using such an approach is that the image-to-image translation is\nsemi-supervised, independant of image segmentation and inherits the properties\nof generative adversarial networks tending to produce realistic. This method\nhas proven to produce better results than Multimodal Unsupervised\nImage-to-image translation. \n\n"}
{"id": "1901.08227", "contents": "Title: Trajectory Normalized Gradients for Distributed Optimization Abstract: Recently, researchers proposed various low-precision gradient compression,\nfor efficient communication in large-scale distributed optimization. Based on\nthese work, we try to reduce the communication complexity from a new direction.\nWe pursue an ideal bijective mapping between two spaces of gradient\ndistribution, so that the mapped gradient carries greater information entropy\nafter the compression. In our setting, all servers should share a reference\ngradient in advance, and they communicate via the normalized gradients, which\nare the subtraction or quotient, between current gradients and the reference.\nTo obtain a reference vector that yields a stronger signal-to-noise ratio,\ndynamically in each iteration, we extract and fuse information from the past\ntrajectory in hindsight, and search for an optimal reference for compression.\nWe name this to be the trajectory-based normalized gradients (TNG). It bridges\nthe research from different societies, like coding, optimization, systems, and\nlearning. It is easy to implement and can universally combine with existing\nalgorithms. Our experiments on benchmarking hard non-convex functions, convex\nproblems like logistic regression demonstrate that TNG is more\ncompression-efficient for communication of distributed optimization of general\nfunctions. \n\n"}
{"id": "1901.08292", "contents": "Title: Anomaly Detection in Road Traffic Using Visual Surveillance: A Survey Abstract: Computer vision has evolved in the last decade as a key technology for\nnumerous applications replacing human supervision. In this paper, we present a\nsurvey on relevant visual surveillance related researches for anomaly detection\nin public places, focusing primarily on roads. Firstly, we revisit the surveys\ndone in the last 10 years in this field. Since the underlying building block of\na typical anomaly detection is learning, we emphasize more on learning methods\napplied on video scenes. We then summarize the important contributions made\nduring last six years on anomaly detection primarily focusing on features,\nunderlying techniques, applied scenarios and types of anomalies using single\nstatic camera. Finally, we discuss the challenges in the computer vision\nrelated anomaly detection techniques and some of the important future\npossibilities. \n\n"}
{"id": "1901.09364", "contents": "Title: Resultant Based Incremental Recovery of Camera Pose from Pairwise\n  Matches Abstract: Incremental (online) structure from motion pipelines seek to recover the\ncamera matrix associated with an image $I_n$ given $n-1$ images,\n$I_1,...,I_{n-1}$, whose camera matrices have already been recovered. In this\npaper, we introduce a novel solution to the six-point online algorithm to\nrecover the exterior parameters associated with $I_n$. Our algorithm uses just\nsix corresponding pairs of 2D points, extracted each from $I_n$ and from\n\\textit{any} of the preceding $n-1$ images, allowing the recovery of the full\nsix degrees of freedom of the $n$'th camera, and unlike common methods, does\nnot require tracking feature points in three or more images. Our novel solution\nis based on constructing a Dixon resultant, yielding a solution method that is\nboth efficient and accurate compared to existing solutions. We further use\nBernstein's theorem to prove a tight bound on the number of complex solutions.\nOur experiments demonstrate the utility of our approach. \n\n"}
{"id": "1901.09819", "contents": "Title: Generalization of feature embeddings transferred from different video\n  anomaly detection domains Abstract: Detecting anomalous activity in video surveillance often involves using only\nnormal activity data in order to learn an accurate detector. Due to lack of\nannotated data for some specific target domain, one could employ existing data\nfrom a source domain to produce better predictions. Hence, transfer learning\npresents itself as an important tool. But how to analyze the resulting data\nspace? This paper investigates video anomaly detection, in particular feature\nembeddings of pre-trained CNN that can be used with non-fully supervised data.\nBy proposing novel cross-domain generalization measures, we study how source\nfeatures can generalize for different target video domains, as well as analyze\nunsupervised transfer learning. The proposed generalization measures are not\nonly a theorical approach, but show to be useful in practice as a way to\nunderstand which datasets can be used or transferred to describe video frames,\nwhich it is possible to better discriminate between normal and anomalous\nactivity. \n\n"}
{"id": "1901.10277", "contents": "Title: High-Quality Self-Supervised Deep Image Denoising Abstract: We describe a novel method for training high-quality image denoising models\nbased on unorganized collections of corrupted images. The training does not\nneed access to clean reference images, or explicit pairs of corrupted images,\nand can thus be applied in situations where such data is unacceptably expensive\nor impossible to acquire. We build on a recent technique that removes the need\nfor reference data by employing networks with a \"blind spot\" in the receptive\nfield, and significantly improve two key aspects: image quality and training\nefficiency. Our result quality is on par with state-of-the-art neural network\ndenoisers in the case of i.i.d. additive Gaussian noise, and not far behind\nwith Poisson and impulse noise. We also successfully handle cases where\nparameters of the noise model are variable and/or unknown in both training and\nevaluation data. \n\n"}
{"id": "1901.10747", "contents": "Title: Autonomous Cars: Vision based Steering Wheel Angle Estimation Abstract: Machine learning models, which are frequently used in self-driving cars, are\ntrained by matching the captured images of the road and the measured angle of\nthe steering wheel. The angle of the steering wheel is generally fetched from\nsteering angle sensor, which is tightly-coupled to the physical aspects of the\nvehicle at hand. Therefore, a model-agnostic autonomous car-kit is very\ndifficult to be developed and autonomous vehicles need more training data. The\nproposed vision based steering angle estimation system argues a new approach\nwhich basically matches the images of the road captured by an outdoor camera\nand the images of the steering wheel from an onboard camera, avoiding the\nburden of collecting model-dependent training data and the use of any other\nelectromechanical hardware. \n\n"}
{"id": "cs/0111054", "contents": "Title: The similarity metric Abstract: A new class of distances appropriate for measuring similarity relations\nbetween sequences, say one type of similarity per distance, is studied. We\npropose a new ``normalized information distance'', based on the noncomputable\nnotion of Kolmogorov complexity, and show that it is in this class and it\nminorizes every computable distance in the class (that is, it is universal in\nthat it discovers all computable similarities). We demonstrate that it is a\nmetric and call it the {\\em similarity metric}. This theory forms the\nfoundation for a new practical tool. To evidence generality and robustness we\ngive two distinctive applications in widely divergent areas using standard\ncompression programs like gzip and GenCompress. First, we compare whole\nmitochondrial genomes and infer their evolutionary history. This results in a\nfirst completely automatic computed whole mitochondrial phylogeny tree.\nSecondly, we fully automatically compute the language tree of 52 different\nlanguages. \n\n"}
{"id": "cs/0505058", "contents": "Title: The Cyborg Astrobiologist: Scouting Red Beds for Uncommon Features with\n  Geological Significance Abstract: The `Cyborg Astrobiologist' (CA) has undergone a second geological field\ntrial, at a red sandstone site in northern Guadalajara, Spain, near Riba de\nSantiuste. The Cyborg Astrobiologist is a wearable computer and video camera\nsystem that has demonstrated a capability to find uncommon interest points in\ngeological imagery in real-time in the field. The first (of three) geological\nstructures that we studied was an outcrop of nearly homogeneous sandstone,\nwhich exhibits oxidized-iron impurities in red and and an absence of these iron\nimpurities in white. The white areas in these ``red beds'' have turned white\nbecause the iron has been removed by chemical reduction, perhaps by a\nbiological agent. The computer vision system found in one instance several\n(iron-free) white spots to be uncommon and therefore interesting, as well as\nseveral small and dark nodules. The second geological structure contained\nwhite, textured mineral deposits on the surface of the sandstone, which were\nfound by the CA to be interesting. The third geological structure was a 50 cm\nthick paleosol layer, with fossilized root structures of some plants, which\nwere found by the CA to be interesting. A quasi-blind comparison of the Cyborg\nAstrobiologist's interest points for these images with the interest points\ndetermined afterwards by a human geologist shows that the Cyborg Astrobiologist\nconcurred with the human geologist 68% of the time (true positive rate), with a\n32% false positive rate and a 32% false negative rate.\n  (abstract has been abridged). \n\n"}
{"id": "cs/0606048", "contents": "Title: A New Quartet Tree Heuristic for Hierarchical Clustering Abstract: We consider the problem of constructing an an optimal-weight tree from the\n3*(n choose 4) weighted quartet topologies on n objects, where optimality means\nthat the summed weight of the embedded quartet topologiesis optimal (so it can\nbe the case that the optimal tree embeds all quartets as non-optimal\ntopologies). We present a heuristic for reconstructing the optimal-weight tree,\nand a canonical manner to derive the quartet-topology weights from a given\ndistance matrix. The method repeatedly transforms a bifurcating tree, with all\nobjects involved as leaves, achieving a monotonic approximation to the exact\nsingle globally optimal tree. This contrasts to other heuristic search methods\nfrom biological phylogeny, like DNAML or quartet puzzling, which, repeatedly,\nincrementally construct a solution from a random order of objects, and\nsubsequently add agreement values. \n\n"}
{"id": "quant-ph/9802028", "contents": "Title: Analogue Quantum Computers for Data Analysis Abstract: Analogue computers use continuous properties of physical system for modeling.\nIn the paper is described possibility of modeling by analogue quantum computers\nfor some model of data analysis. It is analogue associative memory and a formal\nneural network. A particularity of the models is combination of continuous\ninternal processes with discrete set of output states. The modeling of the\nsystem by classical analogue computers was offered long times ago, but now it\nis not very effectively in comparison with modern digital computers. The\napplication of quantum analogue modelling looks quite possible for modern level\nof technology and it may be more effective than digital one, because number of\nelement may be about Avogadro number (N=6.0E23). \n\n"}
